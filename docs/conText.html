<!DOCTYPE html><html><head><title>Help for package conText</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {conText}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bootstrap_contrast'><p>Bootstrap similarity and ratio computations</p></a></li>
<li><a href='#bootstrap_nns'><p>Bootstrap nearest neighbors</p></a></li>
<li><a href='#bootstrap_ols'><p>Bootstrap OLS</p></a></li>
<li><a href='#bootstrap_similarity'><p>Boostrap similarity vector</p></a></li>
<li><a href='#build_conText'><p>build a <code>conText-class</code> object</p></a></li>
<li><a href='#build_dem'><p>build a <code>dem-class</code> object</p></a></li>
<li><a href='#build_fem'><p>build a <code>fem-class</code> object</p></a></li>
<li><a href='#compute_contrast'><p>Compute similarity and similarity ratios</p></a></li>
<li><a href='#compute_similarity'><p>Compute similarity vector (sub-function of bootstrap_similarity)</p></a></li>
<li><a href='#compute_transform'><p>Compute transformation matrix A</p></a></li>
<li><a href='#conText'><p>Embedding regression</p></a></li>
<li><a href='#conText-class'><p>Virtual class &quot;conText&quot; for a conText regression output</p></a></li>
<li><a href='#contrast_nns'><p>Contrast nearest neighbors</p></a></li>
<li><a href='#cos_sim'><p>Compute the cosine similarity between one or more ALC embeddings and a set of features.</p></a></li>
<li><a href='#cr_glove_subset'><p>GloVe subset</p></a></li>
<li><a href='#cr_sample_corpus'><p>Congressional Record sample corpus</p></a></li>
<li><a href='#cr_transform'><p>Transformation matrix</p></a></li>
<li><a href='#dem'><p>Build a document-embedding matrix</p></a></li>
<li><a href='#dem_group'><p>Average document-embeddings in a dem by a grouping variable</p></a></li>
<li><a href='#dem_sample'><p>Randomly sample documents from a dem</p></a></li>
<li><a href='#dem-class'><p>Virtual class &quot;dem&quot; for a document-embedding matrix</p></a></li>
<li><a href='#embed_target'><p>Embed target using either: (a) a la carte OR (b) simple (untransformed) averaging of context embeddings</p></a></li>
<li><a href='#feature_sim'><p>Given two feature-embedding-matrices, compute &quot;parallel&quot; cosine similarities</p>
between overlapping features.</a></li>
<li><a href='#fem'><p>Create an feature-embedding matrix</p></a></li>
<li><a href='#fem-class'><p>Virtual class &quot;fem&quot; for a feature-embedding matrix</p></a></li>
<li><a href='#find_cos_sim'><p>Find cosine similarities between target and candidate words</p></a></li>
<li><a href='#find_nns'><p>Return nearest neighbors based on cosine similarity</p></a></li>
<li><a href='#get_context'><p>Get context words (words within a symmetric window around the target word/phrase)</p>
sorrounding a user defined target.</a></li>
<li><a href='#get_cos_sim'><p>Given a tokenized corpus, compute the cosine similarities</p>
of the resulting ALC embeddings and a defined set of features.</a></li>
<li><a href='#get_local_vocab'><p>Identify words common to a collection of texts and a set of pretrained embeddings.</p></a></li>
<li><a href='#get_ncs'><p>Given a set of tokenized contexts, find the top N nearest</p>
contexts.</a></li>
<li><a href='#get_nns'><p>Given a tokenized corpus and a set of candidate neighbors, find the top N nearest</p>
neighbors.</a></li>
<li><a href='#get_nns_ratio'><p>Given a corpus and a binary grouping variable, computes the ratio of cosine similarities</p>
over the union of their respective N nearest neighbors.</a></li>
<li><a href='#get_seq_cos_sim'><p>Calculate cosine similarities between target word and candidates words over</p>
sequenced variable using ALC embedding approach</a></li>
<li><a href='#ncs'><p>Given a set of embeddings and a set of tokenized contexts, find the top N nearest</p>
contexts.</a></li>
<li><a href='#nns'><p>Given a set of embeddings and a set of candidate neighbors, find the top N nearest</p>
neighbors.</a></li>
<li><a href='#nns_ratio'><p>Computes the ratio of cosine similarities for two embeddings over</p>
the union of their respective top N nearest neighbors.</a></li>
<li><a href='#permute_contrast'><p>Permute similarity and ratio computations</p></a></li>
<li><a href='#permute_ols'><p>Permute OLS</p></a></li>
<li><a href='#plot_nns_ratio'><p>Plot output of <code>get_nns_ratio()</code></p></a></li>
<li><a href='#prototypical_context'><p>Find most &quot;prototypical&quot; contexts.</p></a></li>
<li><a href='#run_ols'><p>Run OLS</p></a></li>
<li><a href='#tokens_context'><p>Get the tokens of contexts sorrounding user defined patterns</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.4.3</td>
</tr>
<tr>
<td>Title:</td>
<td>'a la Carte' on Text (ConText) Embedding Regression</td>
</tr>
<tr>
<td>Description:</td>
<td>A fast, flexible and transparent framework to estimate context-specific word and short document embeddings using the 'a la carte' 
    embeddings approach developed by Khodak et al. (2018) &lt;<a href="https://arxiv.org/abs/1805.05388">arXiv:1805.05388</a>&gt; and evaluate hypotheses about covariate effects on embeddings using 
    the regression framework developed by Rodriguez et al. (2021)<a href="https://github.com/prodriguezsosa/EmbeddingRegression">https://github.com/prodriguezsosa/EmbeddingRegression</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, Matrix (&ge; 1.3-2), quanteda (&ge; 3.0.0), text2vec (&ge;
0.6), reshape2 (&ge; 1.4.4), fastDummies (&ge; 1.6.3), stringr (&ge;
1.4.0), tidyr (&ge; 1.1.3), ggplot2, methods</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/prodriguezsosa/EmbeddingRegression">https://github.com/prodriguezsosa/EmbeddingRegression</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/prodriguezsosa/ConText/issues">https://github.com/prodriguezsosa/ConText/issues</a></td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Pedro L. Rodriguez &lt;pedro.rodriguezsosa@gmail.com&gt;</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>SnowballC (&ge; 0.7.0), hunspell, knitr, rmarkdown, formatR</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-09 20:50:29 UTC; prodriguezsosa</td>
</tr>
<tr>
<td>Author:</td>
<td>Pedro L. Rodriguez
    <a href="https://orcid.org/0000-0003-4762-4550"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre,
    cph],
  Arthur Spirling <a href="https://orcid.org/0000-0001-9959-1805"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Brandon Stewart <a href="https://orcid.org/0000-0002-7657-3089"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Christopher Barrie
    <a href="https://orcid.org/0000-0002-9156-990X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-09 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bootstrap_contrast'>Bootstrap similarity and ratio computations</h2><span id='topic+bootstrap_contrast'></span>

<h3>Description</h3>

<p>Bootstrap similarity and ratio computations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_contrast(
  target_embeddings1 = NULL,
  target_embeddings2 = NULL,
  pre_trained = NULL,
  candidates = NULL,
  norm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_contrast_+3A_target_embeddings1">target_embeddings1</code></td>
<td>
<p>ALC embeddings for group 1</p>
</td></tr>
<tr><td><code id="bootstrap_contrast_+3A_target_embeddings2">target_embeddings2</code></td>
<td>
<p>ALC embeddings for group 2</p>
</td></tr>
<tr><td><code id="bootstrap_contrast_+3A_pre_trained">pre_trained</code></td>
<td>
<p>a V x D matrix of numeric values - pretrained embeddings with V = size of vocabulary and D = embedding dimensions</p>
</td></tr>
<tr><td><code id="bootstrap_contrast_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code></p>
</td></tr>
<tr><td><code id="bootstrap_contrast_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - set to 'l2' for cosine similarity and to 'none' for inner product (see ?sim2 in text2vec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with three elements, nns for group 1, nns for group 2 and nns_ratio comparing with ratios of similarities between the two groups
</p>

<hr>
<h2 id='bootstrap_nns'>Bootstrap nearest neighbors</h2><span id='topic+bootstrap_nns'></span>

<h3>Description</h3>

<p>Uses bootstrapping &ndash;sampling of of texts with replacement&ndash;
to identify the top N nearest neighbors based on cosine or inner product
similarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_nns(
  context = NULL,
  pre_trained = NULL,
  transform = TRUE,
  transform_matrix = NULL,
  candidates = NULL,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  N = 50,
  norm = "l2"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_nns_+3A_context">context</code></td>
<td>
<p>(character) vector of texts - <code>context</code> variable in get_context output</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_transform">transform</code></td>
<td>
<p>(logical) - if TRUE (default) apply the a la carte transformation, if FALSE ouput untransformed averaged embedding.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, bootstrap similarity values - sample from texts with replacement.
Required to get std. errors.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(numeric) - number of bootstraps to use.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="bootstrap_nns_+3A_norm">norm</code></td>
<td>
<p>(character) - how to compute the similarity (see ?text2vec::sim2):
</p>

<dl>
<dt><code>"l2"</code></dt><dd><p>cosine similarity</p>
</dd>
<dt><code>"none"</code></dt><dd><p>inner product</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with the following columns:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character)  vector of feature terms corresponding to the nearest neighbors.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine/inner product similarity between
texts and feature. Average over bootstrapped samples if bootstrap = TRUE.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the similarity value. Column is dropped if bootstrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# find contexts of immigration
context_immigration &lt;- get_context(x = cr_sample_corpus,
                                   target = 'immigration',
                                   window = 6,
                                   valuetype = "fixed",
                                   case_insensitive = TRUE,
                                   hard_cut = FALSE, verbose = FALSE)

# find local vocab (use it to define the candidate of nearest neighbors)
local_vocab &lt;- get_local_vocab(context_immigration$context, pre_trained = cr_glove_subset)

set.seed(42L)
nns_immigration &lt;- bootstrap_nns(context = context_immigration$context,
                                 pre_trained = cr_glove_subset,
                                 transform_matrix = cr_transform,
                                 transform = TRUE,
                                 candidates = local_vocab,
                                 bootstrap = TRUE,
                                 num_bootstraps = 100,
                                 confidence_level = 0.95,
                                 N = 50,
                                 norm = "l2")

head(nns_immigration)

</code></pre>

<hr>
<h2 id='bootstrap_ols'>Bootstrap OLS</h2><span id='topic+bootstrap_ols'></span>

<h3>Description</h3>

<p>Bootstrap model coefficients and standard errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_ols(Y = NULL, X = NULL, stratify = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_ols_+3A_y">Y</code></td>
<td>
<p>vector of regression model's dependent variable (embedded context)</p>
</td></tr>
<tr><td><code id="bootstrap_ols_+3A_x">X</code></td>
<td>
<p>data.frame of model independent variables (covariates)</p>
</td></tr>
<tr><td><code id="bootstrap_ols_+3A_stratify">stratify</code></td>
<td>
<p>covariates to stratify when bootstrapping</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements, <code>betas</code> = list of beta_coefficients (D dimensional vectors);
<code>normed_betas</code> = tibble with the norm of the non-intercept coefficients
</p>

<hr>
<h2 id='bootstrap_similarity'>Boostrap similarity vector</h2><span id='topic+bootstrap_similarity'></span>

<h3>Description</h3>

<p>Boostrap similarity vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_similarity(
  target_embeddings = NULL,
  pre_trained = NULL,
  candidates = NULL,
  norm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_similarity_+3A_target_embeddings">target_embeddings</code></td>
<td>
<p>the target embeddings (embeddings of context)</p>
</td></tr>
<tr><td><code id="bootstrap_similarity_+3A_pre_trained">pre_trained</code></td>
<td>
<p>a V x D matrix of numeric values - pretrained embeddings with V = size of vocabulary and D = embedding dimensions</p>
</td></tr>
<tr><td><code id="bootstrap_similarity_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code></p>
</td></tr>
<tr><td><code id="bootstrap_similarity_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - set to 'l2' for cosine similarity and to 'none' for inner product (see ?sim2 in text2vec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector(s) of cosine similarities between alc embedding and nearest neighbor candidates
</p>

<hr>
<h2 id='build_conText'>build a <code>conText-class</code> object</h2><span id='topic+build_conText'></span>

<h3>Description</h3>

<p>build a <code>conText-class</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_conText(
  Class = "conText",
  x_conText,
  normed_coefficients = data.frame(),
  features = character(),
  Dimnames = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_conText_+3A_class">Class</code></td>
<td>
<p>defines the class of this object (fixed)</p>
</td></tr>
<tr><td><code id="build_conText_+3A_x_context">x_conText</code></td>
<td>
<p>a <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix</p>
</td></tr>
<tr><td><code id="build_conText_+3A_normed_coefficients">normed_coefficients</code></td>
<td>
<p>a data.frame withe the normed coefficients and
other statistics</p>
</td></tr>
<tr><td><code id="build_conText_+3A_features">features</code></td>
<td>
<p>features used in computing the embeddings</p>
</td></tr>
<tr><td><code id="build_conText_+3A_dimnames">Dimnames</code></td>
<td>
<p>row (features) and columns (NULL) names</p>
</td></tr>
</table>

<hr>
<h2 id='build_dem'>build a <code>dem-class</code> object</h2><span id='topic+build_dem'></span>

<h3>Description</h3>

<p>build a <code>dem-class</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_dem(
  Class = "em",
  x_dem,
  docvars = data.frame(),
  features = character(),
  Dimnames = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_dem_+3A_class">Class</code></td>
<td>
<p>defines tha class of this object (fixed)</p>
</td></tr>
<tr><td><code id="build_dem_+3A_x_dem">x_dem</code></td>
<td>
<p>a <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix</p>
</td></tr>
<tr><td><code id="build_dem_+3A_docvars">docvars</code></td>
<td>
<p>document covariates, inherited from dfm and corpus,
subset to embeddable documents</p>
</td></tr>
<tr><td><code id="build_dem_+3A_features">features</code></td>
<td>
<p>features used in computing the embeddings</p>
</td></tr>
<tr><td><code id="build_dem_+3A_dimnames">Dimnames</code></td>
<td>
<p>row (documents) and columns (NULL) names</p>
</td></tr>
</table>

<hr>
<h2 id='build_fem'>build a <code>fem-class</code> object</h2><span id='topic+build_fem'></span>

<h3>Description</h3>

<p>build a <code>fem-class</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_fem(
  Class = "fem",
  x_fem,
  features = character(),
  counts = numeric(),
  Dimnames = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_fem_+3A_class">Class</code></td>
<td>
<p>defines the class of this object (fixed)</p>
</td></tr>
<tr><td><code id="build_fem_+3A_x_fem">x_fem</code></td>
<td>
<p>a <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix</p>
</td></tr>
<tr><td><code id="build_fem_+3A_features">features</code></td>
<td>
<p>features used in computing the embeddings</p>
</td></tr>
<tr><td><code id="build_fem_+3A_counts">counts</code></td>
<td>
<p>counts of features used in computing embeddings</p>
</td></tr>
<tr><td><code id="build_fem_+3A_dimnames">Dimnames</code></td>
<td>
<p>row (features) and columns (NULL) names</p>
</td></tr>
</table>

<hr>
<h2 id='compute_contrast'>Compute similarity and similarity ratios</h2><span id='topic+compute_contrast'></span>

<h3>Description</h3>

<p>Compute similarity and similarity ratios
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_contrast(
  target_embeddings1 = NULL,
  target_embeddings2 = NULL,
  pre_trained = NULL,
  candidates = NULL,
  norm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_contrast_+3A_target_embeddings1">target_embeddings1</code></td>
<td>
<p>ALC embeddings for group 1</p>
</td></tr>
<tr><td><code id="compute_contrast_+3A_target_embeddings2">target_embeddings2</code></td>
<td>
<p>ALC embeddings for group 2</p>
</td></tr>
<tr><td><code id="compute_contrast_+3A_pre_trained">pre_trained</code></td>
<td>
<p>a V x D matrix of numeric values - pretrained embeddings with V = size of vocabulary and D = embedding dimensions</p>
</td></tr>
<tr><td><code id="compute_contrast_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code></p>
</td></tr>
<tr><td><code id="compute_contrast_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - set to 'l2' for cosine similarity and to 'none' for inner product (see ?sim2 in text2vec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with three elements, nns for group 1, nns for group 2 and nns_ratio comparing with ratios of similarities between the two groups
</p>

<hr>
<h2 id='compute_similarity'>Compute similarity vector (sub-function of bootstrap_similarity)</h2><span id='topic+compute_similarity'></span>

<h3>Description</h3>

<p>Compute similarity vector (sub-function of bootstrap_similarity)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_similarity(
  target_embeddings = NULL,
  pre_trained = NULL,
  candidates = NULL,
  norm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_similarity_+3A_target_embeddings">target_embeddings</code></td>
<td>
<p>the target embeddings (embeddings of context)</p>
</td></tr>
<tr><td><code id="compute_similarity_+3A_pre_trained">pre_trained</code></td>
<td>
<p>a V x D matrix of numeric values - pretrained embeddings with V = size of vocabulary and D = embedding dimensions</p>
</td></tr>
<tr><td><code id="compute_similarity_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code></p>
</td></tr>
<tr><td><code id="compute_similarity_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - set to 'l2' for cosine similarity and to 'none' for inner product (see ?sim2 in text2vec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of cosine similarities between alc embedding and nearest neighbor candidates
</p>

<hr>
<h2 id='compute_transform'>Compute transformation matrix A</h2><span id='topic+compute_transform'></span>

<h3>Description</h3>

<p>Computes a transformation matrix, given a feature-co-occurrence
matrix and corresponding pre-trained embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_transform(x, pre_trained, weighting = 500)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_transform_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>fcm-class</code> object.</p>
</td></tr>
<tr><td><code id="compute_transform_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings,
usually trained on the same corpus as that used for <code>x</code>.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding</p>
</td></tr>
<tr><td><code id="compute_transform_+3A_weighting">weighting</code></td>
<td>
<p>(character or numeric) weighting options:
</p>

<dl>
<dt><code>1</code></dt><dd><p>no weighting.</p>
</dd>
<dt><code>"log"</code></dt><dd><p>weight by the log of the frequency count.</p>
</dd>
<dt><code>numeric</code></dt><dd><p>threshold based weighting (= 1 if token count meets threshold, 0 ow).</p>
</dd>
</dl>

<p>Recommended: use <code>log</code> for small corpora, a numeric threshold for larger corpora.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>dgTMatrix-class</code> D x D non-symmetrical matrix
(D = dimensions of pre-trained embedding space) corresponding
to an 'a la carte' transformation matrix. This matrix is optimized
for the corpus and pre-trained embeddings employed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# note, cr_sample_corpus is too small to produce sensical word vectors

# tokenize
toks &lt;- tokens(cr_sample_corpus)

# construct feature-co-occurrence matrix
toks_fcm &lt;- fcm(toks, context = "window", window = 6,
count = "weighted", weights = 1 / (1:6), tri = FALSE)

# you will generally want to estimate a new (corpus-specific)
# GloVe model, we will use cr_glove_subset instead
# see the Quick Start Guide to see a full example.

# estimate transform
local_transform &lt;- compute_transform(x = toks_fcm,
pre_trained = cr_glove_subset, weighting = 'log')
</code></pre>

<hr>
<h2 id='conText'>Embedding regression</h2><span id='topic+conText'></span>

<h3>Description</h3>

<p>Estimates an embedding regression model with options to use bootstrapping to estimate confidence
intervals and a permutation test for inference (see https://github.com/prodriguezsosa/conText for details.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conText(
  formula,
  data,
  pre_trained,
  transform = TRUE,
  transform_matrix,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  stratify = FALSE,
  permute = TRUE,
  num_permutations = 100,
  window = 6L,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  hard_cut = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conText_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fitted with a target word as a DV e.g.
<code>immigrant ~ party + gender</code>. To use a phrase as a DV, place it in quotations e.g.
<code>"immigrant refugees" ~ party + gender</code>. To use all covariates included in the data,
you can use <code>.</code> on RHS, e.g.<code>immigrant ~ .</code>. If you wish to treat the full document as you DV, rather
than a single target word, use <code>.</code> on the LHS e.g. <code>. ~ party + gender</code>. If you wish to use all covariates
on the RHS use <code>immigrant ~ .</code>. Any <code>character</code> or <code>factor</code> covariates will automatically be converted
to a set of binary (<code>0/1</code>s) indicator variables for each group, leaving the first level out of the regression.</p>
</td></tr>
<tr><td><code id="conText_+3A_data">data</code></td>
<td>
<p>a quanteda <code>tokens-class</code> object with the necessary document variables. Covariates must be
either binary indicator variables or &quot;trasnformable&quot; into binary indicator variables. conText will automatically
transform any non-indicator variables into binary indicator variables (multiple if more than 2 classes),
leaving out a &quot;base&quot; category.</p>
</td></tr>
<tr><td><code id="conText_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="conText_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="conText_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="conText_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from texts with replacement and
re-run regression on each sample. Required to get std. errors.</p>
</td></tr>
<tr><td><code id="conText_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(numeric) number of bootstraps to use (at least 100)</p>
</td></tr>
<tr><td><code id="conText_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="conText_+3A_stratify">stratify</code></td>
<td>
<p>(logical) if TRUE, stratify by discrete covariates when bootstrapping.</p>
</td></tr>
<tr><td><code id="conText_+3A_permute">permute</code></td>
<td>
<p>(logical) if TRUE, compute empirical p-values using permutation test</p>
</td></tr>
<tr><td><code id="conText_+3A_num_permutations">num_permutations</code></td>
<td>
<p>(numeric) number of permutations to use</p>
</td></tr>
<tr><td><code id="conText_+3A_window">window</code></td>
<td>
<p>the number of context words to be displayed around the keyword</p>
</td></tr>
<tr><td><code id="conText_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="quanteda.html#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="conText_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="quanteda.html#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="conText_+3A_hard_cut">hard_cut</code></td>
<td>
<p>(logical) - if TRUE then a context must have <code>window</code> x 2 tokens,
if FALSE it can have <code>window</code> x 2 or fewer (e.g. if a doc begins with a target word,
then context will have <code>window</code> tokens rather than <code>window</code> x 2)</p>
</td></tr>
<tr><td><code id="conText_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the documents that had
no overlapping features with the pretrained embeddings provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>conText-class</code> object - a D x M matrix with D = dimensions
of the pre-trained feature embeddings provided and M = number of covariates
including the intercept. These represent the estimated regression coefficients.
These can be combined to compute ALC embeddings for different combinations of covariates.
The object also includes various informative attributes, importantly
a <code>data.frame</code> with the following columns:
</p>

<dl>
<dt><code>coefficient</code></dt><dd><p>(character) name of (covariate) coefficient.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) norm of the corresponding beta coefficient.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) (if bootstrap = TRUE) std. error of the norm of the beta coefficient.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
<dt><code>p.value</code></dt><dd><p>(numeric) (if permute = TRUE) empirical p.value of the norm of the coefficient.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

## given the target word "immigration"
set.seed(2021L)
model1 &lt;- conText(formula = immigration ~ party + gender,
                 data = toks,
                 pre_trained = cr_glove_subset,
                 transform = TRUE, transform_matrix = cr_transform,
                 bootstrap = TRUE,
                 num_bootstraps = 100,
                 confidence_level = 0.95,
                 stratify = FALSE,
                 permute = TRUE, num_permutations = 10,
                 window = 6, case_insensitive = TRUE,
                 verbose = FALSE)

# notice, character/factor covariates are automatically "dummified"
rownames(model1)

# the beta coefficient 'partyR' in this case corresponds to the alc embedding
# of "immigration" for Republican party speeches

# (normed) coefficient table
model1@normed_coefficients

</code></pre>

<hr>
<h2 id='conText-class'>Virtual class &quot;conText&quot; for a conText regression output</h2><span id='topic+conText-class'></span>

<h3>Description</h3>

<p>The <code>conText-class</code> is <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix corresponding to
the beta coefficients (embeddings) with additional slots:
</p>


<h3>Slots</h3>


<dl>
<dt><code>normed_coefficients</code></dt><dd><p><code>normed_betas</code> a data.frame containing the following variables:
</p>

<dl>
<dt><code>Coefficient</code></dt><dd><p>(character) non-intercept coefficient names</p>
</dd>
<dt><code>Normed_Estimate</code></dt><dd><p>(numeric) norm of non-intercept beta coefficients</p>
</dd>
<dt><code>Std.Error</code></dt><dd><p>(numeric) std errors (given boostrap)</p>
</dd>
<dt><code>Empirical_Pvalue</code></dt><dd><p>(numeric) empirical pvalue (given permute)</p>
</dd>
</dl>
</dd>
<dt><code>features</code></dt><dd><p>features used in computing the document embeddings</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code>conText</code>
</p>

<hr>
<h2 id='contrast_nns'>Contrast nearest neighbors</h2><span id='topic+contrast_nns'></span>

<h3>Description</h3>

<p>Computes the ratio of cosine similarities between group embeddings and features
&ndash;that is, for any given feature it first computes the similarity between that feature
and each group embedding, and then takes the ratio of these two similarities.
This ratio captures how &quot;discriminant&quot; a feature is of a given group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrast_nns(
  x,
  groups = NULL,
  pre_trained = NULL,
  transform = TRUE,
  transform_matrix = NULL,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  permute = TRUE,
  num_permutations = 100,
  candidates = NULL,
  N = 20,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrast_nns_+3A_x">x</code></td>
<td>
<p>(quanteda) <code>tokens-class</code> object</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_groups">groups</code></td>
<td>
<p>(numeric, factor, character) a binary variable of the same length as <code>x</code></p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from texts with replacement and
re-estimate cosine ratios for each sample. Required to get std. errors.</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(numeric) - number of bootstraps to use</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_permute">permute</code></td>
<td>
<p>(logical) - if TRUE, compute empirical p-values using a permutation test</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_num_permutations">num_permutations</code></td>
<td>
<p>(numeric) - number of permutations to use</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of candidate features for nearest neighbors</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_n">N</code></td>
<td>
<p>(numeric) - nearest neighbors are subset to the union of the N neighbors of each group (if NULL, ratio is computed for all features)</p>
</td></tr>
<tr><td><code id="contrast_nns_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the documents that had
no overlapping features with the pretrained embeddings provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with following columns:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character) vector of feature terms corresponding to the nearest neighbors.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) ratio of cosine similarities. Average over bootstrapped samples if bootstrap = TRUE.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the ratio of cosine similarties. Column is dropped if bootsrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
<dt><code>p.value</code></dt><dd><p>(numeric) empirical p-value. Column is dropped if permute = FALSE.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

cr_toks &lt;- tokens(cr_sample_corpus)

immig_toks &lt;- tokens_context(x = cr_toks,
pattern = "immigration", window = 6L, hard_cut = FALSE, verbose = TRUE)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

set.seed(42L)
party_nns &lt;- contrast_nns(x = immig_toks,
groups = docvars(immig_toks, 'party'),
pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform,
bootstrap = TRUE,
num_bootstraps = 100,
confidence_level = 0.95,
permute = TRUE, num_permutations = 10,
candidates = NULL, N = 20,
verbose = FALSE)

head(party_nns)
</code></pre>

<hr>
<h2 id='cos_sim'>Compute the cosine similarity between one or more ALC embeddings and a set of features.</h2><span id='topic+cos_sim'></span>

<h3>Description</h3>

<p>Compute the cosine similarity between one or more ALC embeddings and a set of features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cos_sim(
  x,
  pre_trained,
  features = NULL,
  stem = FALSE,
  language = "porter",
  as_list = TRUE,
  show_language = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cos_sim_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>dem-class</code> or <code>fem-class</code> object.</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_features">features</code></td>
<td>
<p>(character) features of interest.</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_stem">stem</code></td>
<td>
<p>(logical) - If TRUE, both <code>features</code> and <code>rownames(pre_trained)</code>
are stemmed and average cosine similarities are reported.
We recommend you remove misspelled words from  <code>pre_trained</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per feature.</p>
</td></tr>
<tr><td><code id="cos_sim_+3A_show_language">show_language</code></td>
<td>
<p>(logical) if TRUE print out message with language used for stemming.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings.
NA if is.null(rownames(x)).</p>
</dd>
<dt><code>feature</code></dt><dd><p>(character) feature terms defined in
the <code>features</code> argument.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code>
and feature.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get group-specific embeddings, average within party
immig_wv_party &lt;- dem_group(immig_dem, groups = immig_dem@docvars$party)

# compute the cosine similarity between each party's embedding and a specific set of features
cos_sim(x = immig_wv_party, pre_trained = cr_glove_subset,
features = c('reform', 'enforcement'), as_list = FALSE)
</code></pre>

<hr>
<h2 id='cr_glove_subset'>GloVe subset</h2><span id='topic+cr_glove_subset'></span>

<h3>Description</h3>

<p>A subset of a GloVe embeddings model trained on the top 5000 features
in the Congressional Record Record corpus covering the 111th - 114th Congresses,
and limited to speeches by Democrat and Republican representatives.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cr_glove_subset
</code></pre>


<h3>Format</h3>

<p>A matrix with 500 rows and 300 columns:
</p>

<dl>
<dt>row</dt><dd><p>each row corresponds to a word</p>
</dd>
<dt>column</dt><dd><p>each column corresponds to a dimension in the embedding space</p>
</dd>
</dl>
<p>...

</p>


<h3>Source</h3>

<p><a href="https://www.dropbox.com/s/p84wzv8bdmziog8/cr_glove.R?dl=0">https://www.dropbox.com/s/p84wzv8bdmziog8/cr_glove.R?dl=0</a>
</p>

<hr>
<h2 id='cr_sample_corpus'>Congressional Record sample corpus</h2><span id='topic+cr_sample_corpus'></span>

<h3>Description</h3>

<p>A (quanteda) corpus containing a sample of the United States Congressional Record
(daily transcripts) covering the 111th to 114th Congresses.
The raw corpus is first subset to speeches
containing the regular expression &quot;immig*&quot;. Then 100 docs from each party-gender pair
is randomly sampled. For full data and pre-processing file, see:
https://www.dropbox.com/sh/jsyrag7opfo7l7i/AAB1z7tumLuKihGu2-FDmhmKa?dl=0
For nominate scores see: https://voteview.com/data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cr_sample_corpus
</code></pre>


<h3>Format</h3>

<p>A quanteda corpus with 200 documents and 3 docvars:
</p>

<dl>
<dt>party</dt><dd><p>party of speaker, (D)emocrat or (R)epublican</p>
</dd>
<dt>gender</dt><dd><p>gender of speaker, (F)emale or (M)ale</p>
</dd>
<dt>nominate_dim1</dt><dd><p>dimension 1 of the nominate score</p>
</dd>
</dl>
<p>...

</p>


<h3>Source</h3>

<p><a href="https://data.stanford.edu/congress_text">https://data.stanford.edu/congress_text</a>
</p>

<hr>
<h2 id='cr_transform'>Transformation matrix</h2><span id='topic+cr_transform'></span>

<h3>Description</h3>

<p>A square matrix corresponding to the transformation matrix computed
using the cr_glove_subset embeddings and corresponding corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cr_transform
</code></pre>


<h3>Format</h3>

<p>A 300 by 300 matrix.
</p>


<h3>Source</h3>

<p><a href="https://www.dropbox.com/s/p84wzv8bdmziog8/cr_glove.R?dl=0">https://www.dropbox.com/s/p84wzv8bdmziog8/cr_glove.R?dl=0</a>
</p>

<hr>
<h2 id='dem'>Build a document-embedding matrix</h2><span id='topic+dem'></span>

<h3>Description</h3>

<p>Given a document-feature-matrix, for each document,
multiply its feature counts (columns) with their
corresponding pretrained word embeddings and average
(usually referred to as averaged or additive document embeddings).
If specified and a transformation matrix is provided,
multiply the document embeddings by the transformation matrix
to obtain the corresponding <code style="white-space: pre;">&#8288;a la carte&#8288;</code> document embeddings.
(see eq 2: https://arxiv.org/pdf/1805.05388.pdf)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dem(x, pre_trained, transform = TRUE, transform_matrix, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dem_+3A_x">x</code></td>
<td>
<p>a quanteda (<code>dfm-class</code>) document-feature-matrix</p>
</td></tr>
<tr><td><code id="dem_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="dem_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="dem_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="dem_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the documents that had
no overlapping features with the pretrained embeddings provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a N x D (<code>dem-class</code>) document-embedding-matrix corresponding to the ALC embeddings for each document.
N = number of documents (that could be embedded), D = dimensions of pretrained embeddings. This object
inherits the document variables in <code>x</code>, the dfm used. These can be accessed calling the attribute: <code style="white-space: pre;">&#8288;@docvars&#8288;</code>.
Note, documents with no overlapping features with the pretrained embeddings provided are automatically
dropped. For a list of the documents that were embedded call the attribute: <code style="white-space: pre;">&#8288;@Dimnames$docs&#8288;</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# construct document-feature-matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)
</code></pre>

<hr>
<h2 id='dem_group'>Average document-embeddings in a dem by a grouping variable</h2><span id='topic+dem_group'></span>

<h3>Description</h3>

<p>Average embeddings in a dem by a grouping variable, by averaging over columns within groups
and creating new &quot;documents&quot; with the group labels.
Similar in essence to <code>dfm_group</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dem_group(x, groups = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dem_group_+3A_x">x</code></td>
<td>
<p>a (<code>dem-class</code>) document-embedding-matrix</p>
</td></tr>
<tr><td><code id="dem_group_+3A_groups">groups</code></td>
<td>
<p>a character or factor variable equal in length to the number of documents</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a G x D (<code>dem-class</code>) document-embedding-matrix corresponding to the ALC embeddings for each group.
G = number of unique groups defined in the <code>groups</code> variable, D = dimensions of pretrained embeddings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get group-specific embeddings, average within party
immig_wv_party &lt;- dem_group(immig_dem,
groups = immig_dem@docvars$party)
</code></pre>

<hr>
<h2 id='dem_sample'>Randomly sample documents from a dem</h2><span id='topic+dem_sample'></span>

<h3>Description</h3>

<p>Take a random sample of documents from a <code>dem</code> with/without replacement and
with the option to group by a variable in <code>dem@docvars</code>. Note: <code>dem_sample</code> uses <code>dplyr::sample_frac</code>
underneath the hood, as such <code>size</code> refers to the fraction of total obs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dem_sample(x, size = NULL, replace = FALSE, weight = NULL, by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dem_sample_+3A_x">x</code></td>
<td>
<p>a (<code>dem-class</code>) document-embedding-matrix</p>
</td></tr>
<tr><td><code id="dem_sample_+3A_size">size</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_tidy_select">tidy-select</a></code>&gt;
For <code>sample_n()</code>, the number of rows to select.
For <code>sample_frac()</code>, the fraction of rows to select.
If <code>tbl</code> is grouped, <code>size</code> applies to each group.</p>
</td></tr>
<tr><td><code id="dem_sample_+3A_replace">replace</code></td>
<td>
<p>Sample with or without replacement?</p>
</td></tr>
<tr><td><code id="dem_sample_+3A_weight">weight</code></td>
<td>
<p>(numeric) Sampling weights. Vector of non-negative numbers of length <code>nrow(x)</code>.
Weights are automatically standardised to sum to 1 (see <code>dplyr::sample_frac</code>).
May not be applied when <code>by</code> is used.</p>
</td></tr>
<tr><td><code id="dem_sample_+3A_by">by</code></td>
<td>
<p>(character or factor vector) either of length 1 with the name of grouping variable for sampling.
Refer to the variable WITH QUOTATIONS e.g. <code>"party"</code>. Must be a variable in <code>dem@docvars</code>. OR of length
nrow(x).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>size</code> x D (<code>dem-class</code>) document-embedding-matrix corresponding to the sampled
ALC embeddings. Note, <code style="white-space: pre;">&#8288;@features&#8288;</code> in the resulting object will correspond to the original <code style="white-space: pre;">&#8288;@features&#8288;</code>,
that is, they are not subsetted to the sampled documents. For a list of the documents that were
sampled call the attribute: <code style="white-space: pre;">&#8288;@Dimnames$docs&#8288;</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get a random sample
immig_wv_party &lt;- dem_sample(immig_dem, size = 10,
replace = TRUE, by = "party")

# also works
immig_wv_party &lt;- dem_sample(immig_dem, size = 10,
replace = TRUE, by = immig_dem@docvars$party)
</code></pre>

<hr>
<h2 id='dem-class'>Virtual class &quot;dem&quot; for a document-embedding matrix</h2><span id='topic+dem-class'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;dem class&#8288;</code> is <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix with additional slots:
</p>


<h3>Slots</h3>


<dl>
<dt><code>docvars</code></dt><dd><p>document covariates, inherited from dfm and corpus
subset to embeddable documents</p>
</dd>
<dt><code>features</code></dt><dd><p>features used in computing the document embeddings</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code>dem</code>
</p>

<hr>
<h2 id='embed_target'>Embed target using either: (a) a la carte OR (b) simple (untransformed) averaging of context embeddings</h2><span id='topic+embed_target'></span>

<h3>Description</h3>

<p>For a vector of contexts (generally the context variable in get_context output), return
the transformed (or untransformed) additive embeddings, aggregated or by instance, along with
the local vocabulary. Keep track of which contexts were embedded and which were excluded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed_target(
  context,
  pre_trained,
  transform = TRUE,
  transform_matrix,
  aggregate = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embed_target_+3A_context">context</code></td>
<td>
<p>(character) vector of texts - <code>context</code> variable in get_context output</p>
</td></tr>
<tr><td><code id="embed_target_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="embed_target_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="embed_target_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="embed_target_+3A_aggregate">aggregate</code></td>
<td>
<p>(logical) - if TRUE (default) output will return one embedding (i.e. averaged over all instances of target)
if FALSE output will return one embedding per instance</p>
</td></tr>
<tr><td><code id="embed_target_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - report the observations that had no overlap the provided pre-trained embeddings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>required packages: quanteda
</p>


<h3>Value</h3>

<p>list with three elements:
</p>

<dl>
<dt><code>target_embedding</code></dt><dd><p>the target embedding(s). Values and dimensions will vary with the above settings.</p>
</dd>
<dt><code>local_vocab</code></dt><dd><p>(character) vocabulary that appears in the set of contexts provided.</p>
</dd>
<dt><code>obs_included</code></dt><dd><p>(integer) rows of the context vector that were included in the computation.
A row (context) is excluded when none of the words in the context are present in the pre-trained embeddings provided.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># find contexts for term immigration
context_immigration &lt;- get_context(x = cr_sample_corpus, target = 'immigration',
                        window = 6, valuetype = "fixed", case_insensitive = TRUE,
                        hard_cut = FALSE, verbose = FALSE)

contexts_vectors &lt;- embed_target(context = context_immigration$context,
pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform,
aggregate = FALSE, verbose = FALSE)
</code></pre>

<hr>
<h2 id='feature_sim'>Given two feature-embedding-matrices, compute &quot;parallel&quot; cosine similarities
between overlapping features.</h2><span id='topic+feature_sim'></span>

<h3>Description</h3>

<p>Efficient way of comparing two corpora along many features simultaneously.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_sim(x, y, features = character(0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_sim_+3A_x">x</code></td>
<td>
<p>a (<code>fem-class</code>) feature embedding matrix.</p>
</td></tr>
<tr><td><code id="feature_sim_+3A_y">y</code></td>
<td>
<p>a (<code>fem-class</code>) feature embedding matrix.</p>
</td></tr>
<tr><td><code id="feature_sim_+3A_features">features</code></td>
<td>
<p>(character) vector of features for which to compute
similarity scores. If not defined then all overlapping features will be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with following columns:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character) overlapping features</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between overlapping features.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# create feature co-occurrence matrix for each party (set tri = FALSE to work with fem)
fcm_D &lt;- fcm(toks[docvars(toks, 'party') == "D",],
context = "window", window = 6, count = "frequency", tri = FALSE)
fcm_R &lt;- fcm(toks[docvars(toks, 'party') == "R",],
context = "window", window = 6, count = "frequency", tri = FALSE)

# compute feature-embedding matrix
fem_D &lt;- fem(fcm_D, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)
fem_R &lt;- fem(fcm_R, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# compare "horizontal" cosine similarity
feat_comp &lt;- feature_sim(x = fem_R, y = fem_D)
</code></pre>

<hr>
<h2 id='fem'>Create an feature-embedding matrix</h2><span id='topic+fem'></span>

<h3>Description</h3>

<p>Given a featureco-occurrence matrix for each feature,
multiply its feature counts (columns) with their
corresponding pre-trained embeddings and average
(usually referred to as averaged or additive embeddings).
If specified and a transformation matrix is provided,
multiply the feature embeddings by the transformation matrix
to obtain the corresponding <code style="white-space: pre;">&#8288;a la carte&#8288;</code> embeddings.
(see eq 2: https://arxiv.org/pdf/1805.05388.pdf)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fem(x, pre_trained, transform = TRUE, transform_matrix, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fem_+3A_x">x</code></td>
<td>
<p>a quanteda (<code>fcm-class</code>) feature-co-occurrence-matrix</p>
</td></tr>
<tr><td><code id="fem_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="fem_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="fem_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="fem_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the features that had
no overlapping (co-occurring) features with the pretrained embeddings provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>fem-class</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# create feature co-occurrence matrix (set tri = FALSE to work with fem)
toks_fcm &lt;- fcm(toks, context = "window", window = 6,
count = "frequency", tri = FALSE)

# compute feature-embedding matrix
toks_fem &lt;- fem(toks_fcm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)
</code></pre>

<hr>
<h2 id='fem-class'>Virtual class &quot;fem&quot; for a feature-embedding matrix</h2><span id='topic+fem-class'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;fem class&#8288;</code> is <code style="white-space: pre;">&#8288;dgCMatrix class&#8288;</code> matrix with additional slots:
</p>


<h3>Slots</h3>


<dl>
<dt><code>features</code></dt><dd><p>features used in computing the document embeddings</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code>fem</code>
</p>

<hr>
<h2 id='find_cos_sim'>Find cosine similarities between target and candidate words</h2><span id='topic+find_cos_sim'></span>

<h3>Description</h3>

<p>Find cosine similarities between target and candidate words
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_cos_sim(target_embedding, pre_trained, candidates, norm = "l2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_cos_sim_+3A_target_embedding">target_embedding</code></td>
<td>
<p>matrix of numeric values</p>
</td></tr>
<tr><td><code id="find_cos_sim_+3A_pre_trained">pre_trained</code></td>
<td>
<p>matrix of numeric values - pretrained embeddings</p>
</td></tr>
<tr><td><code id="find_cos_sim_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining vocabulary to subset comparison to</p>
</td></tr>
<tr><td><code id="find_cos_sim_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - how to scale input matrices. If they are already scaled - use &quot;none&quot; (see ?sim2)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of cosine similarities of length candidates
</p>

<hr>
<h2 id='find_nns'>Return nearest neighbors based on cosine similarity</h2><span id='topic+find_nns'></span>

<h3>Description</h3>

<p>Return nearest neighbors based on cosine similarity
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_nns(
  target_embedding,
  pre_trained,
  N = 5,
  candidates = NULL,
  norm = "l2",
  stem = FALSE,
  language = "porter"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_nns_+3A_target_embedding">target_embedding</code></td>
<td>
<p>(numeric) 1 x D matrix. D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="find_nns_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="find_nns_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="find_nns_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of candidate features for nearest neighbors</p>
</td></tr>
<tr><td><code id="find_nns_+3A_norm">norm</code></td>
<td>
<p>(character) - how to compute similarity (see ?text2vec::sim2):
</p>

<dl>
<dt><code>"l2"</code></dt><dd><p>cosine similarity</p>
</dd>
<dt><code>"none"</code></dt><dd><p>inner product</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="find_nns_+3A_stem">stem</code></td>
<td>
<p>(logical) - whether to stem candidates when evaluating nns. Default is FALSE.
If TRUE, candidate stems are ranked by their average cosine similarity to the target.
We recommend you remove misspelled words from candidate set <code>candidates</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="find_nns_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(character) vector of nearest neighbors to target
</p>


<h3>Examples</h3>

<pre><code class='language-R'>find_nns(target_embedding = cr_glove_subset['immigration',],
         pre_trained = cr_glove_subset, N = 5,
         candidates = NULL, norm = "l2", stem = FALSE)
</code></pre>

<hr>
<h2 id='get_context'>Get context words (words within a symmetric window around the target word/phrase)
sorrounding a user defined target.</h2><span id='topic+get_context'></span>

<h3>Description</h3>

<p>A wrapper function for quanteda's <code>kwic()</code> function that subsets documents to where
target is present before tokenizing to speed up processing, and concatenates
kwic's pre/post variables into a <code>context</code> column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_context(
  x,
  target,
  window = 6L,
  valuetype = "fixed",
  case_insensitive = TRUE,
  hard_cut = FALSE,
  what = "word",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_context_+3A_x">x</code></td>
<td>
<p>(character) vector - this is the set of documents (corpus) of interest.</p>
</td></tr>
<tr><td><code id="get_context_+3A_target">target</code></td>
<td>
<p>(character) vector - these are the target words whose contexts we want to evaluate
This vector may include a single token, a phrase or multiple tokens and/or phrases.</p>
</td></tr>
<tr><td><code id="get_context_+3A_window">window</code></td>
<td>
<p>(numeric) - defines the size of a context (words around the target).</p>
</td></tr>
<tr><td><code id="get_context_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="quanteda.html#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="get_context_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="quanteda.html#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="get_context_+3A_hard_cut">hard_cut</code></td>
<td>
<p>(logical) - if TRUE then a context must have <code>window</code> x 2 tokens,
if FALSE it can have <code>window</code> x 2 or fewer (e.g. if a doc begins with a target word,
then context will have <code>window</code> tokens rather than <code>window</code> x 2)</p>
</td></tr>
<tr><td><code id="get_context_+3A_what">what</code></td>
<td>
<p>(character) defines which quanteda tokenizer to use. You will rarely want to change this.
For chinese text you may want to set <code>what = 'fastestword'</code>.</p>
</td></tr>
<tr><td><code id="get_context_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the total number of target instances found.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with the following columns:
</p>

<dl>
<dt><code>docname</code></dt><dd><p> (character) document name to which instances belong to.</p>
</dd>
<dt><code>target</code></dt><dd><p>(character) targets.</p>
</dd>
<dt><code>context</code></dt><dd><p>(numeric) pre/post variables in <code>kwic()</code> output concatenated.</p>
</dd>
</dl>



<h3>Note</h3>

<p><code>target</code> in the return data.frame is equivalent to <code>kwic()</code>'s <code>keyword</code> output variable,
so it may not match the user-defined target exactly if <code>valuetype</code> is not fixed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># get context words sorrounding the term immigration
context_immigration &lt;- get_context(x = cr_sample_corpus, target = 'immigration',
                                   window = 6, valuetype = "fixed", case_insensitive = FALSE,
                                   hard_cut = FALSE, verbose = FALSE)
</code></pre>

<hr>
<h2 id='get_cos_sim'>Given a tokenized corpus, compute the cosine similarities
of the resulting ALC embeddings and a defined set of features.</h2><span id='topic+get_cos_sim'></span>

<h3>Description</h3>

<p>This is a wrapper function for <code>cos_sim()</code> that allows users to go from a
tokenized corpus to results with the option to bootstrap cosine similarities
and get the corresponding std. errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cos_sim(
  x,
  groups = NULL,
  features = character(0),
  pre_trained,
  transform = TRUE,
  transform_matrix,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  stem = FALSE,
  language = "porter",
  as_list = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_cos_sim_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>tokens-class</code> object</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_groups">groups</code></td>
<td>
<p>(numeric, factor, character) a binary variable of the same length as <code>x</code></p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_features">features</code></td>
<td>
<p>(character) features of interest</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from texts with replacement and
re-estimate cosine similarities for each sample. Required to get std. errors.
If <code>groups</code> defined, sampling is automatically stratified.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(integer) number of bootstraps to use.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_stem">stem</code></td>
<td>
<p>(logical) - If TRUE, both <code>features</code> and <code>rownames(pre_trained)</code>
are stemmed and average cosine similarities are reported.
We recommend you remove misspelled words from  <code>pre_trained</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="get_cos_sim_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per feature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings.</p>
</dd>
<dt><code>feature</code></dt><dd><p>(character) feature terms defined in
the <code>features</code> argument.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code>
and feature. Average over bootstrapped samples if bootstrap = TRUE.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the similarity value.
Column is dropped if bootstrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigration", window = 6L)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

# compute the cosine similarity between each group's embedding
# and a specific set of features
set.seed(2021L)
get_cos_sim(x = immig_toks,
            groups = docvars(immig_toks, 'party'),
            features = c("reform", "enforce"),
            pre_trained = cr_glove_subset,
            transform = TRUE,
            transform_matrix = cr_transform,
            bootstrap = TRUE,
            num_bootstraps = 100,
            confidence_level = 0.95,
            stem = TRUE,
            as_list = FALSE)
</code></pre>

<hr>
<h2 id='get_local_vocab'>Identify words common to a collection of texts and a set of pretrained embeddings.</h2><span id='topic+get_local_vocab'></span>

<h3>Description</h3>

<p>Local vocab consists of the intersect between the set of pretrained embeddings
and the collection of texts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_local_vocab(context, pre_trained)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_local_vocab_+3A_context">context</code></td>
<td>
<p>(character) vector of contexts (usually <code>context</code> in <code>get_context()</code> output)</p>
</td></tr>
<tr><td><code id="get_local_vocab_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(character) vector of words common to the texts and pretrained embeddings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># find local vocab (use it to define the candidate of nearest neighbors)
local_vocab &lt;- get_local_vocab(cr_sample_corpus, pre_trained = cr_glove_subset)
</code></pre>

<hr>
<h2 id='get_ncs'>Given a set of tokenized contexts, find the top N nearest
contexts.</h2><span id='topic+get_ncs'></span>

<h3>Description</h3>

<p>This is a wrapper function for <code>ncs()</code> that allows users to go from a
tokenized corpus to results with the option to bootstrap cosine similarities
and get the corresponding std. errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ncs(
  x,
  N = 5,
  groups = NULL,
  pre_trained,
  transform = TRUE,
  transform_matrix,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  as_list = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_ncs_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>tokens-class</code> object</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest contexts to return</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_groups">groups</code></td>
<td>
<p>a character or factor variable equal in length to the number of documents</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from <code>x</code> with replacement and
re-estimate cosine similarities for each sample. Required to get std. errors.
If <code>groups</code> defined, sampling is automatically stratified.</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(integer) number of bootstraps to use.</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="get_ncs_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per embedding</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings. <code>NA</code> if <code>is.null(rownames(x))</code>.</p>
</dd>
<dt><code>context</code></dt><dd><p>(character) contexts collapsed into single documents (i.e. untokenized).</p>
</dd>
<dt><code>rank</code></dt><dd><p>(character) rank of context in terms of similarity with <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code> and context.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the similarity value.
Column is dropped if bootstrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigration",
window = 6L, rm_keyword = FALSE)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

# compare nearest contexts between groups
set.seed(2021L)
immig_party_ncs &lt;- get_ncs(x = immig_toks,
                           N = 10,
                           groups = docvars(immig_toks, 'party'),
                           pre_trained = cr_glove_subset,
                           transform = TRUE,
                           transform_matrix = cr_transform,
                           bootstrap = TRUE,
                           num_bootstraps = 100,
                           confidence_level = 0.95,
                           as_list = TRUE)

# nearest neighbors of "immigration" for Republican party
immig_party_ncs[["D"]]
</code></pre>

<hr>
<h2 id='get_nns'>Given a tokenized corpus and a set of candidate neighbors, find the top N nearest
neighbors.</h2><span id='topic+get_nns'></span>

<h3>Description</h3>

<p>This is a wrapper function for <code>nns()</code> that allows users to go from a
tokenized corpus to results with the option to bootstrap cosine similarities
and get the corresponding std. errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_nns(
  x,
  N = 10,
  groups = NULL,
  candidates = character(0),
  pre_trained,
  transform = TRUE,
  transform_matrix,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  stem = FALSE,
  language = "porter",
  as_list = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_nns_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>tokens-class</code> object</p>
</td></tr>
<tr><td><code id="get_nns_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return</p>
</td></tr>
<tr><td><code id="get_nns_+3A_groups">groups</code></td>
<td>
<p>a character or factor variable equal in length to the number of documents</p>
</td></tr>
<tr><td><code id="get_nns_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of features to consider as candidates to be nearest neighbor
You may for example want to only consider features that meet a certain count threshold
or exclude stop words etc. To do so you can simply identify the set of features you
want to consider and supply these as a character vector in the <code>candidates</code> argument.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from <code>x</code> with replacement and
re-estimate cosine similarities for each sample. Required to get std. errors.
If <code>groups</code> defined, sampling is automatically stratified.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(integer) number of bootstraps to use.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="get_nns_+3A_stem">stem</code></td>
<td>
<p>(logical) - whether to stem candidates when evaluating nns. Default is FALSE.
If TRUE, candidate stems are ranked by their average cosine similarity to the target.
We recommend you remove misspelled words from candidate set <code>candidates</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="get_nns_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="get_nns_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per group.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings. <code>NA</code> if <code>is.null(rownames(x))</code>.</p>
</dd>
<dt><code>feature</code></dt><dd><p>(character) features identified as nearest neighbors.</p>
</dd>
<dt><code>rank</code></dt><dd><p>(character) rank of feature in terms of similarity with <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code>
and feature. Average over bootstrapped samples if bootstrap = TRUE.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the similarity value.
Column is dropped if bootstrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigration", window = 6L)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

# we limit candidates to features in our corpus
feats &lt;- featnames(dfm(immig_toks))

# compare nearest neighbors between groups
set.seed(2021L)
immig_party_nns &lt;- get_nns(x = immig_toks, N = 10,
                           groups = docvars(immig_toks, 'party'),
                           candidates = feats,
                           pre_trained = cr_glove_subset,
                           transform = TRUE,
                           transform_matrix = cr_transform,
                           bootstrap = TRUE,
                           num_bootstraps = 100,
                           stem = TRUE,
                           as_list = TRUE)

# nearest neighbors of "immigration" for Republican party
immig_party_nns[["R"]]
</code></pre>

<hr>
<h2 id='get_nns_ratio'>Given a corpus and a binary grouping variable, computes the ratio of cosine similarities
over the union of their respective N nearest neighbors.</h2><span id='topic+get_nns_ratio'></span>

<h3>Description</h3>

<p>This is a wrapper function for <code>nns_ratio()</code> that allows users to go from a
tokenized corpus to results with the option to: (1) bootstrap cosine similarity ratios
and get the corresponding std. errors. (2) use a permutation test to get empirical
p-values for inference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_nns_ratio(
  x,
  N = 10,
  groups,
  numerator = NULL,
  candidates = character(0),
  pre_trained,
  transform = TRUE,
  transform_matrix,
  bootstrap = TRUE,
  num_bootstraps = 100,
  confidence_level = 0.95,
  permute = TRUE,
  num_permutations = 100,
  stem = FALSE,
  language = "porter",
  verbose = TRUE,
  show_language = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_nns_ratio_+3A_x">x</code></td>
<td>
<p>a (quanteda) tokens object</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return. Nearest neighbors
consist of the union of the top N nearest neighbors of the embeddings in <code>x</code>.
If these overlap, then resulting N will be smaller than 2*N.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_groups">groups</code></td>
<td>
<p>a character or factor variable equal in length to the number of documents</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_numerator">numerator</code></td>
<td>
<p>(character) defines which group is the nuemerator in the ratio.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of features to consider as candidates to be nearest neighbor
You may for example want to only consider features that meet a certian count threshold
or exclude stop words etc. To do so you can simply identify the set of features you
want to consider and supply these as a character vector in the <code>candidates</code> argument.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_transform">transform</code></td>
<td>
<p>(logical) if TRUE (default) apply the 'a la carte' transformation,
if FALSE ouput untransformed averaged embeddings.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(logical) if TRUE, use bootstrapping &ndash; sample from texts with replacement and
re-estimate cosine similarity ratios for each sample. Required to get std. errors.
If <code>groups</code> defined, sampling is automatically stratified.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_num_bootstraps">num_bootstraps</code></td>
<td>
<p>(integer) number of bootstraps to use.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(numeric in (0,1)) confidence level e.g. 0.95</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_permute">permute</code></td>
<td>
<p>(logical) if TRUE, compute empirical p-values using permutation test</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_num_permutations">num_permutations</code></td>
<td>
<p>(numeric) number of permutations to use.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_stem">stem</code></td>
<td>
<p>(logical) - whether to stem candidates when evaluating nns. Default is FALSE.
If TRUE, candidate stems are ranked by their average cosine similarity to the target.
We recommend you remove misspelled words from candidate set <code>candidates</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_verbose">verbose</code></td>
<td>
<p>provide information on which group is the numerator</p>
</td></tr>
<tr><td><code id="get_nns_ratio_+3A_show_language">show_language</code></td>
<td>
<p>(logical) if TRUE print out message with language used for stemming.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with following columns:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character) features in <code>candidates</code>
(or all features if <code>candidates</code> not defined), one instance for each embedding in <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity ratio between <code>x</code>
and feature. Average over bootstrapped samples if bootstrap = TRUE.</p>
</dd>
<dt><code>std.error</code></dt><dd><p>(numeric) std. error of the similarity value.
Column is dropped if bootstrap = FALSE.</p>
</dd>
<dt><code>lower.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) lower bound of the confidence interval.</p>
</dd>
<dt><code>upper.ci</code></dt><dd><p>(numeric) (if bootstrap = TRUE) upper bound of the confidence interval.</p>
</dd>
<dt><code>p.value</code></dt><dd><p>(numeric) empirical p-value of bootstrapped ratio
of cosine similarities if permute = TRUE, if FALSE, column is dropped.</p>
</dd>
<dt><code>group</code></dt><dd><p>(character) group in <code>groups</code> for which feature belongs
to the top N nearest neighbors. If &quot;shared&quot;, the feature appeared as
top nearest neighbor for both groups.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigration", window = 6L)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

# we limit candidates to features in our corpus
feats &lt;- featnames(dfm(immig_toks))

# compute ratio
set.seed(2021L)
immig_nns_ratio &lt;- get_nns_ratio(x = immig_toks,
                                 N = 10,
                                 groups = docvars(immig_toks, 'party'),
                                 numerator = "R",
                                 candidates = feats,
                                 pre_trained = cr_glove_subset,
                                 transform = TRUE,
                                 transform_matrix = cr_transform,
                                 bootstrap = FALSE,
                                 num_bootstraps = 100,
                                 permute = FALSE,
                                 num_permutations = 5,
                                 verbose = FALSE)

head(immig_nns_ratio)
</code></pre>

<hr>
<h2 id='get_seq_cos_sim'>Calculate cosine similarities between target word and candidates words over
sequenced variable using ALC embedding approach</h2><span id='topic+get_seq_cos_sim'></span>

<h3>Description</h3>

<p>Calculate cosine similarities between target word and candidates words over
sequenced variable using ALC embedding approach
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_seq_cos_sim(
  x,
  seqvar,
  target,
  candidates,
  pre_trained,
  transform_matrix,
  window = 6,
  valuetype = "fixed",
  case_insensitive = TRUE,
  hard_cut = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_seq_cos_sim_+3A_x">x</code></td>
<td>
<p>(character) vector - this is the set of documents (corpus) of interest</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_seqvar">seqvar</code></td>
<td>
<p>ordered variable such as list of dates or ordered iseology scores</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_target">target</code></td>
<td>
<p>(character) vector - target word</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of features of interest</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_window">window</code></td>
<td>
<p>(numeric) - defines the size of a context (words around the target).</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="quanteda.html#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="quanteda.html#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_hard_cut">hard_cut</code></td>
<td>
<p>(logical) - if TRUE then a context must have <code>window</code> x 2 tokens,
if FALSE it can have <code>window</code> x 2 or fewer (e.g. if a doc begins with a target word,
then context will have <code>window</code> tokens rather than <code>window</code> x 2)</p>
</td></tr>
<tr><td><code id="get_seq_cos_sim_+3A_verbose">verbose</code></td>
<td>
<p>(logical) - if TRUE, report the total number of target instances found.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with one column for
each candidate term with corresponding cosine similarity values
and one column for seqvar.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# gen sequence var (here: year)
docvars(cr_sample_corpus, 'year') &lt;- rep(2011:2014, each = 50)
cos_simsdf &lt;- get_seq_cos_sim(x = cr_sample_corpus,
seqvar = docvars(cr_sample_corpus, 'year'),
target = "equal",
candidates = c("immigration", "immigrants"),
pre_trained = cr_glove_subset,
transform_matrix = cr_transform)
</code></pre>

<hr>
<h2 id='ncs'>Given a set of embeddings and a set of tokenized contexts, find the top N nearest
contexts.</h2><span id='topic+ncs'></span>

<h3>Description</h3>

<p>Given a set of embeddings and a set of tokenized contexts, find the top N nearest
contexts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncs(x, contexts_dem, contexts = NULL, N = 5, as_list = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncs_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>dem-class</code> or <code>fem-class</code> object.</p>
</td></tr>
<tr><td><code id="ncs_+3A_contexts_dem">contexts_dem</code></td>
<td>
<p>a <code>dem-class</code> object corresponding to the ALC
embeddings of candidate contexts.</p>
</td></tr>
<tr><td><code id="ncs_+3A_contexts">contexts</code></td>
<td>
<p>a (quanteda) <code>tokens-class</code> object of
tokenized candidate contexts. Note, these must correspond to the same
contexts in <code>contexts_dem</code>. If NULL, then the context (document) ids
will be output instead of the text.</p>
</td></tr>
<tr><td><code id="ncs_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest contexts to return</p>
</td></tr>
<tr><td><code id="ncs_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per embedding</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings. <code>NA</code> if <code>is.null(rownames(x))</code>.</p>
</dd>
<dt><code>context</code></dt><dd><p>(character) contexts collapsed into single documents (i.e. untokenized).
If <code>contexts</code> is NULL then this variable will show the context (document) ids which
you can use to merge.</p>
</dd>
<dt><code>rank</code></dt><dd><p>(character) rank of context in terms of similarity with <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code> and context.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*",
window = 6L, rm_keyword = FALSE)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get group-specific embeddings, average within party
immig_wv_party &lt;- dem_group(immig_dem, groups = immig_dem@docvars$party)

# find nearest contexts by party
# setting as_list = FALSE combines each group's
# results into a single data.frame (useful for joint plotting)
ncs(x = immig_wv_party, contexts_dem = immig_dem,
contexts = immig_toks, N = 5, as_list = TRUE)
</code></pre>

<hr>
<h2 id='nns'>Given a set of embeddings and a set of candidate neighbors, find the top N nearest
neighbors.</h2><span id='topic+nns'></span>

<h3>Description</h3>

<p>Given a set of embeddings and a set of candidate neighbors, find the top N nearest
neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nns(
  x,
  N = 10,
  candidates = character(0),
  pre_trained,
  stem = FALSE,
  language = "porter",
  as_list = TRUE,
  show_language = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nns_+3A_x">x</code></td>
<td>
<p>a <code>dem-class</code> or <code>fem-class</code> object.</p>
</td></tr>
<tr><td><code id="nns_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return</p>
</td></tr>
<tr><td><code id="nns_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of features to consider as candidates to be nearest neighbor
You may for example want to only consider features that meet a certain count threshold
or exclude stop words etc. To do so you can simply identify the set of features you
want to consider and supply these as a character vector in the <code>candidates</code> argument.</p>
</td></tr>
<tr><td><code id="nns_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="nns_+3A_stem">stem</code></td>
<td>
<p>(logical) - whether to stem candidates when evaluating nns. Default is FALSE.
If TRUE, candidate stems are ranked by their average cosine similarity to the target.
We recommend you remove misspelled words from candidate set <code>candidates</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="nns_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="nns_+3A_as_list">as_list</code></td>
<td>
<p>(logical) if FALSE all results are combined into a single data.frame
If TRUE, a list of data.frames is returned with one data.frame per group.</p>
</td></tr>
<tr><td><code id="nns_+3A_show_language">show_language</code></td>
<td>
<p>(logical) if TRUE print out message with language used for stemming.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> or list of data.frames (one for each target)
with the following columns:
</p>

<dl>
<dt><code>target</code></dt><dd><p> (character) rownames of <code>x</code>,
the labels of the ALC embeddings. <code>NA</code> if <code>is.null(rownames(x))</code>.</p>
</dd>
<dt><code>feature</code></dt><dd><p>(character) features identified as nearest neighbors.</p>
</dd>
<dt><code>rank</code></dt><dd><p>(character) rank of feature in terms of similarity with <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) cosine similarity between <code>x</code> and feature.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get group-specific embeddings, average within party
immig_wv_party &lt;- dem_group(immig_dem, groups = immig_dem@docvars$party)

# find nearest neighbors by party
# setting as_list = FALSE combines each group's
# results into a single tibble (useful for joint plotting)
immig_nns &lt;- nns(immig_wv_party, pre_trained = cr_glove_subset,
N = 5, candidates = immig_wv_party@features, stem = FALSE, as_list = TRUE)
</code></pre>

<hr>
<h2 id='nns_ratio'>Computes the ratio of cosine similarities for two embeddings over
the union of their respective top N nearest neighbors.</h2><span id='topic+nns_ratio'></span>

<h3>Description</h3>

<p>Computes the ratio of cosine similarities between group embeddings and features
&ndash;that is, for any given feature it first computes the similarity between that feature
and each group embedding, and then takes the ratio of these two similarities.
This ratio captures how &quot;discriminant&quot; a feature is of a given group.
Values larger (smaller) than 1 mean the feature is more (less)
discriminant of the group in the numerator (denominator).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nns_ratio(
  x,
  N = 10,
  numerator = NULL,
  candidates = character(0),
  pre_trained,
  stem = FALSE,
  language = "porter",
  verbose = TRUE,
  show_language = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nns_ratio_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>dem-class</code> or <code>fem-class</code> object.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_n">N</code></td>
<td>
<p>(numeric) number of nearest neighbors to return. Nearest neighbors
consist of the union of the top N nearest neighbors of the embeddings in <code>x</code>.
If these overlap, then resulting N will be smaller than 2*N.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_numerator">numerator</code></td>
<td>
<p>(character) defines which group is the nuemerator in the ratio</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_candidates">candidates</code></td>
<td>
<p>(character) vector of features to consider as candidates to be nearest neighbor
You may for example want to only consider features that meet a certian count threshold
or exclude stop words etc. To do so you can simply identify the set of features you
want to consider and supply these as a character vector in the <code>candidates</code> argument.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_stem">stem</code></td>
<td>
<p>(logical) - whether to stem candidates when evaluating nns. Default is FALSE.
If TRUE, candidate stems are ranked by their average cosine similarity to the target.
We recommend you remove misspelled words from candidate set <code>candidates</code> as these can
significantly influence the average.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>, or a two- or three-letter ISO-639
code corresponding to one of these languages (see references for
the list of codes).
</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_verbose">verbose</code></td>
<td>
<p>report which group is the numerator and which group is the denominator.</p>
</td></tr>
<tr><td><code id="nns_ratio_+3A_show_language">show_language</code></td>
<td>
<p>(logical) if TRUE print out message with language used for stemming.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with following columns:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character) features in <code>candidates</code>
(or all features if <code>candidates</code> not defined), one instance for each embedding in <code>x</code>.</p>
</dd>
<dt><code>value</code></dt><dd><p>(numeric) ratio of cosine similarities.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)

# build document-feature matrix
immig_dfm &lt;- dfm(immig_toks)

# construct document-embedding-matrix
immig_dem &lt;- dem(immig_dfm, pre_trained = cr_glove_subset,
transform = TRUE, transform_matrix = cr_transform, verbose = FALSE)

# to get group-specific embeddings, average within party
immig_wv_party &lt;- dem_group(immig_dem, groups = immig_dem@docvars$party)

# compute the cosine similarity between each party's
# embedding and a specific set of features
nns_ratio(x = immig_wv_party, N = 10, numerator = "R",
candidates = immig_wv_party@features,
pre_trained = cr_glove_subset, verbose = FALSE)

# with stemming
nns_ratio(x = immig_wv_party, N = 10, numerator = "R",
candidates = immig_wv_party@features,
pre_trained = cr_glove_subset, stem = TRUE, verbose = FALSE)
</code></pre>

<hr>
<h2 id='permute_contrast'>Permute similarity and ratio computations</h2><span id='topic+permute_contrast'></span>

<h3>Description</h3>

<p>Permute similarity and ratio computations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permute_contrast(
  target_embeddings1 = NULL,
  target_embeddings2 = NULL,
  pre_trained = NULL,
  candidates = NULL,
  norm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permute_contrast_+3A_target_embeddings1">target_embeddings1</code></td>
<td>
<p>ALC embeddings for group 1</p>
</td></tr>
<tr><td><code id="permute_contrast_+3A_target_embeddings2">target_embeddings2</code></td>
<td>
<p>ALC embeddings for group 2</p>
</td></tr>
<tr><td><code id="permute_contrast_+3A_pre_trained">pre_trained</code></td>
<td>
<p>a V x D matrix of numeric values - pretrained embeddings with V = size of vocabulary and D = embedding dimensions</p>
</td></tr>
<tr><td><code id="permute_contrast_+3A_candidates">candidates</code></td>
<td>
<p>character vector defining the candidates for nearest neighbors - e.g. output from <code>get_local_vocab</code></p>
</td></tr>
<tr><td><code id="permute_contrast_+3A_norm">norm</code></td>
<td>
<p>character = c(&quot;l2&quot;, &quot;none&quot;) - set to 'l2' for cosine similarity and to 'none' for inner product (see ?sim2 in text2vec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with three elements, nns for group 1, nns for group 2 and nns_ratio comparing with ratios of similarities between the two groups
</p>

<hr>
<h2 id='permute_ols'>Permute OLS</h2><span id='topic+permute_ols'></span>

<h3>Description</h3>

<p>Estimate empirical p-value using permutated regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permute_ols(Y = NULL, X = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permute_ols_+3A_y">Y</code></td>
<td>
<p>vector of regression model's dependent variable (embedded context)</p>
</td></tr>
<tr><td><code id="permute_ols_+3A_x">X</code></td>
<td>
<p>data.frame of model independent variables (covariates)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements, <code>betas</code> = list of beta_coefficients (D dimensional vectors);
<code>normed_betas</code> = tibble with the norm of the non-intercept coefficients
</p>

<hr>
<h2 id='plot_nns_ratio'>Plot output of <code>get_nns_ratio()</code></h2><span id='topic+plot_nns_ratio'></span>

<h3>Description</h3>

<p>A way of visualizing the top nearest neighbors of a pair of ALC embeddings that captures
how &quot;discriminant&quot; each feature is of each embedding (group).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_nns_ratio(x, alpha = 0.01, horizontal = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_nns_ratio_+3A_x">x</code></td>
<td>
<p>output of get_nns_ratio</p>
</td></tr>
<tr><td><code id="plot_nns_ratio_+3A_alpha">alpha</code></td>
<td>
<p>(numerical) betwee 0 and 1. Significance threshold to identify significant values.
These are denoted by a <code>*</code> on the plot.</p>
</td></tr>
<tr><td><code id="plot_nns_ratio_+3A_horizontal">horizontal</code></td>
<td>
<p>(logical) defines the type of plot. if TRUE results are plotted on 1 dimension.
If FALSE, results are plotted on 2 dimensions, with the second dimension catpuring the ranking
of cosine ratio similarties.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ggplot-class</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigration", window = 6L)

# sample 100 instances of the target term, stratifying by party (only for example purposes)
set.seed(2022L)
immig_toks &lt;- tokens_sample(immig_toks, size = 100, by = docvars(immig_toks, 'party'))

# we limit candidates to features in our corpus
feats &lt;- featnames(dfm(immig_toks))

# compute ratio
set.seed(2022L)
immig_nns_ratio &lt;- get_nns_ratio(x = immig_toks,
                                 N = 10,
                                 groups = docvars(immig_toks, 'party'),
                                 numerator = "R",
                                 candidates = feats,
                                 pre_trained = cr_glove_subset,
                                 transform = TRUE,
                                 transform_matrix = cr_transform,
                                 bootstrap = FALSE,
                                 num_bootstraps = 100,
                                 permute = FALSE,
                                 num_permutations = 10,
                                 verbose = FALSE)

plot_nns_ratio(x = immig_nns_ratio, alpha = 0.01, horizontal = TRUE)
</code></pre>

<hr>
<h2 id='prototypical_context'>Find most &quot;prototypical&quot; contexts.</h2><span id='topic+prototypical_context'></span>

<h3>Description</h3>

<p>Contexts most similar on average to the full set of contexts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prototypical_context(
  context,
  pre_trained,
  transform = TRUE,
  transform_matrix,
  N = 3,
  norm = "l2"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prototypical_context_+3A_context">context</code></td>
<td>
<p>(character) vector of texts - <code>context</code> variable in get_context output</p>
</td></tr>
<tr><td><code id="prototypical_context_+3A_pre_trained">pre_trained</code></td>
<td>
<p>(numeric) a F x D matrix corresponding to pretrained embeddings.
F = number of features and D = embedding dimensions.
rownames(pre_trained) = set of features for which there is a pre-trained embedding.</p>
</td></tr>
<tr><td><code id="prototypical_context_+3A_transform">transform</code></td>
<td>
<p>(logical) - if TRUE (default) apply the a la carte transformation, if FALSE ouput untransformed averaged embedding.</p>
</td></tr>
<tr><td><code id="prototypical_context_+3A_transform_matrix">transform_matrix</code></td>
<td>
<p>(numeric) a D x D 'a la carte' transformation matrix.
D = dimensions of pretrained embeddings.</p>
</td></tr>
<tr><td><code id="prototypical_context_+3A_n">N</code></td>
<td>
<p>(numeric) number of most &quot;prototypical&quot; contexts to return.</p>
</td></tr>
<tr><td><code id="prototypical_context_+3A_norm">norm</code></td>
<td>
<p>(character) - how to compute similarity (see ?text2vec::sim2):
</p>

<dl>
<dt><code>"l2"</code></dt><dd><p>cosine similarity</p>
</dd>
<dt><code>"none"</code></dt><dd><p>inner product</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code> with the following columns:
</p>

<dl>
<dt><code>doc_id</code></dt><dd><p> (integer) document id.</p>
</dd>
<dt><code>typicality_score</code></dt><dd><p>(numeric) average similarity score to all other contexts</p>
</dd>
<dt><code>context</code></dt><dd><p>(character) contexts</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# find contexts of immigration
context_immigration &lt;- get_context(x = cr_sample_corpus, target = 'immigration',
                                   window = 6, valuetype = "fixed", case_insensitive = TRUE,
                                   hard_cut = FALSE, verbose = FALSE)

# identify top N prototypical contexts and compute typicality score
pt_context &lt;- prototypical_context(context = context_immigration$context,
pre_trained = cr_glove_subset,
transform = TRUE,
transform_matrix = cr_transform,
N = 3, norm = 'l2')
</code></pre>

<hr>
<h2 id='run_ols'>Run OLS</h2><span id='topic+run_ols'></span>

<h3>Description</h3>

<p>Bootstrap model coefficients and standard errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_ols(Y = NULL, X = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_ols_+3A_y">Y</code></td>
<td>
<p>vector of regression model's dependent variable (embedded context)</p>
</td></tr>
<tr><td><code id="run_ols_+3A_x">X</code></td>
<td>
<p>data.frame of model independent variables (covariates)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements, <code>betas</code> = list of beta_coefficients (D dimensional vectors);
<code>normed_betas</code> = tibble with the norm of the non-intercept coefficients
</p>

<hr>
<h2 id='tokens_context'>Get the tokens of contexts sorrounding user defined patterns</h2><span id='topic+tokens_context'></span>

<h3>Description</h3>

<p>This function uses quanteda's <code>kwic()</code> function to find the contexts
around user defined patterns (i.e. target words/phrases) and return a tokens object
with the tokenized contexts and corresponding document variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_context(
  x,
  pattern,
  window = 6L,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  hard_cut = FALSE,
  rm_keyword = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_context_+3A_x">x</code></td>
<td>
<p>a (quanteda) <code>tokens-class</code> object</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="quanteda.html#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="quanteda.html#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_window">window</code></td>
<td>
<p>the number of context words to be displayed around the keyword</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="quanteda.html#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="quanteda.html#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_hard_cut">hard_cut</code></td>
<td>
<p>(logical) - if TRUE then a context must have <code>window</code> x 2 tokens,
if FALSE it can have <code>window</code> x 2 or fewer (e.g. if a doc begins with a target word,
then context will have <code>window</code> tokens rather than <code>window</code> x 2)</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_rm_keyword">rm_keyword</code></td>
<td>
<p>(logical) if FALSE, keyword matching pattern is included in the tokenized contexts</p>
</td></tr>
<tr><td><code id="tokens_context_+3A_verbose">verbose</code></td>
<td>
<p>(logical) if TRUE, report the total number of instances per pattern found</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a (quanteda) <code>tokens-class</code>. Each document in the output tokens object
inherits the document variables (<code>docvars</code>) of the document from whence it came,
along with a column registering corresponding the pattern used.
This information can be retrieved using <code>docvars()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)

# tokenize corpus
toks &lt;- tokens(cr_sample_corpus)

# build a tokenized corpus of contexts sorrounding a target term
immig_toks &lt;- tokens_context(x = toks, pattern = "immigr*", window = 6L)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
