<!DOCTYPE html><html><head><title>Help for package psych</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {psych}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#00.psych'><p>A package for personality, psychometric, and psychological research</p></a></li>
<li><a href='#alpha'><p>Find two estimates of reliability: Cronbach's alpha and Guttman's Lambda 6.</p></a></li>
<li><a href='#anova.psych'><p>Model comparison for regression, mediation, and factor analysis</p></a></li>
<li><a href='#AUC'><p>Decision Theory measures of specificity, sensitivity, and d prime</p></a></li>
<li><a href='#bassAckward'><p>The Bass-Ackward factoring algorithm discussed by Goldberg</p></a></li>
<li><a href='#Bechtoldt'><p>Seven data sets showing a bifactor solution.</p></a></li>
<li><a href='#bestScales'><p>A bootstrap aggregation function for choosing most predictive unit weighted items</p></a></li>
<li><a href='#bfi'><p>25 Personality items representing 5 factors</p></a></li>
<li><a href='#bi.bars'><p>Draw pairs of bargraphs based on two groups</p></a></li>
<li><a href='#bigCor'><p>Find large correlation matrices by stitching together smaller ones found more rapidly</p></a></li>
<li><a href='#biplot.psych'><p>Draw biplots of factor or component scores by factor or component loadings</p></a></li>
<li><a href='#block.random'><p>Create a block randomized structure for n independent variables</p></a></li>
<li><a href='#bock'><p>Bock and Liberman (1970) data set of 1000 observations of the LSAT</p></a></li>
<li><a href='#cattell'><p>12 cognitive variables from Cattell (1963)</p></a></li>
<li><a href='#circ.tests'><p> Apply four tests of circumplex versus simple structure</p></a></li>
<li><a href='#cluster.fit'><p> cluster Fit:  fit of the cluster model to a correlation matrix</p></a></li>
<li><a href='#cluster.loadings'><p> Find item by cluster correlations, corrected for overlap and reliability</p></a></li>
<li><a href='#cluster.plot'><p>Plot factor/cluster loadings and assign items to clusters by their highest loading.</p></a></li>
<li><a href='#cluster2keys'><p>Convert a cluster vector (from e.g., kmeans) to a keys matrix suitable for scoring item clusters.</p></a></li>
<li><a href='#cohen.d'><p>Find Cohen d and confidence intervals</p></a></li>
<li><a href='#cohen.kappa'><p>Find Cohen's kappa and weighted kappa coefficients for correlation of two raters</p></a></li>
<li><a href='#comorbidity'><p> Convert base rates of two diagnoses and their comorbidity into phi, Yule, and tetrachorics</p></a></li>
<li><a href='#congruence'><p>Matrix and profile congruences and distances</p></a></li>
<li><a href='#cor.smooth'><p>Smooth a non-positive definite correlation matrix to make it positive definite</p></a></li>
<li><a href='#cor.wt'><p>The sample size weighted correlation may be used in correlating aggregated data</p></a></li>
<li><a href='#cor2dist'><p>Convert correlations to distances (necessary to do multidimensional scaling of correlation data)</p></a></li>
<li><a href='#corCi'><p>Bootstrapped and normal confidence intervals for raw and composite correlations</p></a></li>
<li><a href='#corFiml'><p>Find a Full Information Maximum Likelihood (FIML) correlation or covariance matrix from a data matrix with missing data</p></a></li>
<li><a href='#corPlot'><p>Create an image plot for a correlation or factor matrix</p></a></li>
<li><a href='#correct.cor'><p> Find dis-attenuated correlations given correlations and reliabilities</p></a></li>
<li><a href='#cortest'><p>Chi square tests of whether a single matrix is an identity matrix, or a pair of matrices are equal.</p></a></li>
<li><a href='#corTest'><p>Find the correlations, sample sizes, and probability values between elements of a matrix or data.frame.</p></a></li>
<li><a href='#cortest.bartlett'><p>Bartlett's test that a correlation matrix is an identity matrix</p></a></li>
<li><a href='#cosinor'><p>Functions for analysis of circadian or diurnal data</p></a></li>
<li><a href='#cta'><p>Simulate the C(ues) T(endency) A(ction) model of motivation</p></a></li>
<li><a href='#densityBy'><p>Create a 'violin plot' or density plot of the distribution of a set of variables</p></a></li>
<li><a href='#describe'><p>  Basic descriptive statistics useful for psychometrics</p></a></li>
<li><a href='#describeBy'><p> Basic summary statistics by group</p></a></li>
<li><a href='#diagram'><p>Helper functions for drawing path model diagrams</p></a></li>
<li><a href='#draw.tetra'><p>Draw a correlation ellipse and two normal curves to demonstrate tetrachoric correlation</p></a></li>
<li><a href='#dummy.code'><p>Create dummy coded variables</p></a></li>
<li><a href='#Dwyer'><p>8 cognitive variables used by Dwyer for an example.</p></a></li>
<li><a href='#eigen.loadings'><p>Convert eigen vectors and eigen values to the more normal (for psychologists) component loadings</p></a></li>
<li><a href='#ellipses'><p>Plot data and 1 and 2 sigma correlation ellipses</p></a></li>
<li><a href='#error.bars'><p>Plot means and confidence intervals</p></a></li>
<li><a href='#error.bars.by'><p> Plot means and confidence intervals for multiple groups</p></a></li>
<li><a href='#error.crosses'><p> Plot x and y error bars</p></a></li>
<li><a href='#error.dots'><p>Show a  dot.chart with error bars for different groups or variables</p></a></li>
<li><a href='#errorCircles'><p>Two way plots of means, error bars, and sample sizes</p></a></li>
<li><a href='#esem'><p>Perform and Exploratory Structural Equation Model (ESEM) by using factor extension techniques</p></a></li>
<li><a href='#fa'><p>Exploratory Factor analysis using MinRes (minimum residual) as well as EFA by Principal Axis, Weighted Least Squares or Maximum Likelihood</p></a></li>
<li><a href='#fa.diagram'><p> Graph factor loading matrices</p></a></li>
<li><a href='#fa.extension'><p>Apply Dwyer's factor extension to find factor loadings for extended variables</p></a></li>
<li><a href='#fa.lookup'><p>A set of functions for factorial and empirical scale construction</p></a></li>
<li><a href='#fa.multi'><p>Multi level (hierarchical) factor analysis</p></a></li>
<li><a href='#fa.parallel'><p>Scree plots of data or correlation matrix compared to random &ldquo;parallel&quot; matrices</p></a></li>
<li><a href='#fa.poly'><p>Deprecated Exploratory Factor analysis functions.  Please use fa</p></a></li>
<li><a href='#fa.random'><p>A first approximation to Random Effects Exploratory Factor Analysis</p></a></li>
<li><a href='#fa.sort'><p>Sort factor analysis or principal components analysis loadings</p></a></li>
<li><a href='#faCor'><p>Correlations between two factor analysis solutions</p></a></li>
<li><a href='#factor.congruence'><p>Coefficient of factor congruence</p></a></li>
<li><a href='#factor.fit'><p>   How well does the factor model fit a correlation matrix. Part of the VSS package</p></a></li>
<li><a href='#factor.model'><p> Find R =  F F' + U2 is the basic factor model</p></a></li>
<li><a href='#factor.residuals'><p> R* =  R- F F'</p></a></li>
<li><a href='#factor.rotate'><p>&ldquo;Hand&quot; rotate a factor loading matrix</p></a></li>
<li><a href='#factor.scores'><p>Various ways to estimate factor scores for the factor analysis model</p></a></li>
<li><a href='#factor.stats'><p>Find various goodness of fit statistics for factor analysis and principal components</p></a></li>
<li><a href='#factor2cluster'><p> Extract cluster definitions from factor loadings</p></a></li>
<li><a href='#faRotations'><p>Multiple rotations of factor loadings to find local minima</p></a></li>
<li><a href='#fisherz'><p>Transformations of r, d, and t  including Fisher r to z and z to r and confidence intervals</p></a></li>
<li><a href='#fparse'>
<p>Parse and exten  formula input from a model and return the DV, IV, and associated terms.</p></a></li>
<li><a href='#Garcia'><p>Data from the sexism (protest) study of Garcia, Schmitt, Branscome, and Ellemers (2010)</p></a></li>
<li><a href='#geometric.mean'><p> Find the geometric mean of a vector or columns of a data.frame.</p></a></li>
<li><a href='#glb.algebraic'><p>Find the greatest lower bound to reliability.</p></a></li>
<li><a href='#Gleser'>
<p>Example data from Gleser, Cronbach and Rajaratnam (1965) to show basic principles of generalizability theory.</p></a></li>
<li><a href='#Gorsuch'><p>Example data set from Gorsuch (1997) for an example factor extension.</p></a></li>
<li><a href='#Harman'><p>Five data sets from Harman (1967). 9 cognitive variables from Holzinger and 8 emotional variables from Burt</p></a></li>
<li><a href='#harmonic.mean'><p> Find the harmonic mean of a vector, matrix, or columns of a data.frame</p></a></li>
<li><a href='#headTail'><p>Combine calls to head and tail</p></a></li>
<li><a href='#ICC'><p> Intraclass Correlations (ICC1, ICC2, ICC3 from Shrout and Fleiss)</p></a></li>
<li><a href='#iclust'><p>iclust: Item Cluster Analysis   &ndash; Hierarchical cluster analysis using psychometric principles</p></a></li>
<li><a href='#ICLUST.cluster'><p>Function to form hierarchical cluster analysis of items</p></a></li>
<li><a href='#iclust.diagram'><p> Draw an ICLUST hierarchical cluster structure diagram</p></a></li>
<li><a href='#ICLUST.graph'><p> create control code for ICLUST graphical output</p></a></li>
<li><a href='#ICLUST.rgraph'><p> Draw an ICLUST graph using the Rgraphviz package</p></a></li>
<li><a href='#ICLUST.sort'><p>Sort items by absolute size of cluster loadings</p></a></li>
<li><a href='#interp.median'><p>Find the interpolated sample median, quartiles, or specific quantiles for a vector, matrix, or data frame</p></a></li>
<li><a href='#irt.1p'><p>Item Response Theory estimate of theta (ability) using a Rasch (like) model</p></a></li>
<li><a href='#irt.fa'><p>Item Response Analysis by Exploratory Factor Analysis of tetrachoric/polychoric correlations</p></a></li>
<li><a href='#irt.item.diff.rasch'><p>Simple function to estimate item difficulties using IRT concepts</p></a></li>
<li><a href='#irt.responses'><p>Plot probability of multiple choice responses as a function of a latent trait</p></a></li>
<li><a href='#kaiser'><p>Apply the Kaiser normalization when rotating factors</p></a></li>
<li><a href='#KMO'><p>Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy</p></a></li>
<li><a href='#lmCor'><p>Multiple Regression, Canonical and Set Correlation from matrix or raw input</p></a></li>
<li><a href='#logistic'><p>Logistic transform from x to p and logit transform from p to x</p></a></li>
<li><a href='#lowerUpper'><p>Combine two square matrices to have a lower off diagonal for one, upper off diagonal for the other</p></a></li>
<li><a href='#make.keys'><p> Create a keys matrix for use by score.items or cluster.cor</p></a></li>
<li><a href='#manhattan'><p>&quot;Manhattan&quot; plots of correlations with a set of criteria.</p></a></li>
<li><a href='#mardia'><p> Calculate univariate or multivariate (Mardia's test) skew and kurtosis for a vector, matrix, or data.frame</p></a></li>
<li><a href='#mat.sort'><p>Sort the elements of a correlation matrix to reflect factor loadings</p></a></li>
<li><a href='#matrix.addition'><p>A function to add two vectors or matrices</p></a></li>
<li><a href='#mediate'><p>Estimate and display direct and indirect effects of mediators and  moderator in path models</p></a></li>
<li><a href='#mixedCor'><p>Find correlations for mixtures of continuous, polytomous, and dichotomous variables</p></a></li>
<li><a href='#mssd'><p>Find von Neuman's Mean Square of Successive Differences</p></a></li>
<li><a href='#multi.hist'><p>  Multiple histograms with density and normal fits on one page</p></a></li>
<li><a href='#multilevel.reliability'><p>Find and plot various reliability/gneralizability coefficients for multilevel data</p></a></li>
<li><a href='#omega'><p>  Calculate McDonald's  omega estimates of general and total factor saturation</p></a></li>
<li><a href='#omega.graph'><p>Graph hierarchical factor structures</p></a></li>
<li><a href='#outlier'><p>Find and graph Mahalanobis squared distances to detect outliers</p></a></li>
<li><a href='#p.rep'><p>Find the probability of replication for an F, t, or r and estimate effect size</p></a></li>
<li><a href='#paired.r'><p> Test the difference between (un)paired correlations</p></a></li>
<li><a href='#pairs.panels'><p>SPLOM, histograms and correlations for a data matrix</p></a></li>
<li><a href='#pairwiseCount'><p>Count number of pairwise cases for a data set with missing (NA) data and impute values.</p></a></li>
<li><a href='#parcels'><p>Find miniscales (parcels) of size 2 or 3 from a set of items</p></a></li>
<li><a href='#partial.r'><p> Find the partial correlations for a set (x) of variables with set (y) removed.</p></a></li>
<li><a href='#phi'><p> Find the phi coefficient of correlation between two dichotomous variables</p></a></li>
<li><a href='#phi.demo'><p> A simple demonstration of the Pearson, phi, and polychoric corelation</p></a></li>
<li><a href='#phi2tetra'><p> Convert a phi coefficient to a tetrachoric correlation</p></a></li>
<li><a href='#Pinv'><p>Compute the  Moore-Penrose Pseudo Inverse of a matrix</p></a></li>
<li><a href='#plot.psych'><p>Plotting functions for the psych package of class &ldquo;psych&quot;</p></a></li>
<li><a href='#polar'><p>Convert Cartesian factor loadings into polar coordinates</p></a></li>
<li><a href='#polychor.matrix'><p>Phi or Yule coefficient matrix  to polychoric coefficient matrix</p></a></li>
<li><a href='#predict.psych'><p>Prediction function for factor analysis, principal components (pca), bestScales</p></a></li>
<li><a href='#predicted.validity'><p>Find the predicted validities of a set of scales based on item statistics</p></a></li>
<li><a href='#principal'><p> Principal components analysis (PCA)</p></a></li>
<li><a href='#print.psych'><p> Print and summary functions for the psych class</p></a></li>
<li><a href='#Promax'><p> Perform Procustes,bifactor, promax or targeted rotations and return the inter factor angles.</p></a></li>
<li><a href='#psych.misc'><p>Miscellaneous helper functions for the psych package</p></a></li>
<li><a href='#r.test'><p>Tests of significance for correlations</p></a></li>
<li><a href='#rangeCorrection'><p>Correct correlations for restriction of range. (Thorndike Case 2)</p></a></li>
<li><a href='#reliability'><p>Reports 7 different estimates of scale reliabity including alpha, omega, split half</p></a></li>
<li><a href='#rescale'><p>Function to convert scores to &ldquo;conventional</p>
&quot; metrics</a></li>
<li><a href='#residuals.psych'><p>Extract residuals from various psych objects</p></a></li>
<li><a href='#reverse.code'><p>Reverse the coding of selected items prior to scale analysis</p></a></li>
<li><a href='#RMSEA'><p>Root Mean Squared Error of Approximation from chisq, df, and n</p></a></li>
<li><a href='#sat.act'><p>3 Measures of ability: SATV, SATQ, ACT</p></a></li>
<li><a href='#scaling.fits'><p> Test the adequacy of simple choice, logistic, or Thurstonian scaling.</p></a></li>
<li><a href='#scatterHist'><p>Draw a scatter plot with associated X and Y histograms, densities and correlation</p></a></li>
<li><a href='#schmid'><p>Apply the Schmid Leiman transformation to a correlation matrix</p></a></li>
<li><a href='#Schmid'><p>12 variables created by Schmid and Leiman to show the Schmid-Leiman Transformation</p></a></li>
<li><a href='#score.alpha'><p> Score scales and find Cronbach's alpha as well as associated statistics</p></a></li>
<li><a href='#score.multiple.choice'><p>Score multiple choice items and provide basic test statistics</p></a></li>
<li><a href='#scoreIrt'><p>Find Item Response Theory (IRT) based scores for dichotomous or polytomous items</p></a></li>
<li><a href='#scoreItems'><p> Score item composite scales and find Cronbach's alpha, Guttman lambda 6 and item whole correlations</p></a></li>
<li><a href='#scoreOverlap'><p>Find correlations of composite variables (corrected for overlap) from a larger matrix.</p></a></li>
<li><a href='#scoreWtd'><p>Score items using regression or correlation based weights</p></a></li>
<li><a href='#scrub'><p>A utility for basic data cleaning and recoding.  Changes values outside of minimum and maximum limits to NA.</p></a></li>
<li><a href='#SD'><p> Find the Standard deviation for a vector, matrix, or data.frame - do not return error if there are no cases</p></a></li>
<li><a href='#sim'><p>Functions to simulate psychological/psychometric data.</p></a></li>
<li><a href='#sim.anova'><p>Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures.</p></a></li>
<li><a href='#sim.congeneric'><p> Simulate a congeneric data set with or without minor factors</p></a></li>
<li><a href='#sim.hierarchical'><p>Create a population or sample correlation matrix, perhaps with hierarchical structure.</p></a></li>
<li><a href='#sim.irt'><p>Functions to simulate psychological/psychometric data.</p></a></li>
<li><a href='#sim.item'><p>Generate simulated data structures for circumplex, spherical, or simple structure</p></a></li>
<li><a href='#sim.multilevel'><p>Simulate multilevel data with specified within group and between group correlations</p></a></li>
<li><a href='#sim.omega'><p>Further functions to simulate psychological/psychometric data.</p></a></li>
<li><a href='#sim.structure'><p>Create correlation matrices or data matrices with a particular measurement and structural model</p></a></li>
<li><a href='#sim.VSS'><p> create VSS like data</p></a></li>
<li><a href='#simulation.circ'><p> Simulations of circumplex and simple structure</p></a></li>
<li><a href='#smc'><p>Find the Squared Multiple Correlation (SMC) of each variable with the remaining variables in a matrix</p></a></li>
<li><a href='#spider'><p>Make &quot;radar&quot; or &quot;spider&quot; plots.</p></a></li>
<li><a href='#splitHalf'><p>Alternative estimates of test reliabiity</p></a></li>
<li><a href='#statsBy'><p>Find statistics (including correlations) within and between groups for basic multilevel analyses</p></a></li>
<li><a href='#structure.diagram'><p>Draw a structural equation model specified by two measurement models and a structural model</p></a></li>
<li><a href='#structure.list'><p>Create factor model matrices from an input list</p></a></li>
<li><a href='#superMatrix'><p>Form a super matrix from two sub matrices.</p></a></li>
<li><a href='#table2matrix'><p> Convert a table with counts to a matrix or data.frame representing those counts.</p></a></li>
<li><a href='#Tal_Or'>
<p>Data set testing causal direction in presumed media influence</p></a></li>
<li><a href='#test.irt'><p>A simple demonstration (and test) of various IRT scoring algorthims.</p></a></li>
<li><a href='#test.psych'><p> Testing of functions in the psych package</p></a></li>
<li><a href='#testRetest'><p>Find various test-retest statistics, including test, person and item reliability</p></a></li>
<li><a href='#tetrachoric'><p>Tetrachoric, polychoric, biserial and polyserial correlations from various types of input</p></a></li>
<li><a href='#thurstone'><p>Thurstone Case V scaling</p></a></li>
<li><a href='#tr'><p>Find the trace of a square matrix</p></a></li>
<li><a href='#Tucker'>
<p>9 Cognitive variables discussed by Tucker and Lewis (1973)</p></a></li>
<li><a href='#unidim'><p>Several indices of the unidimensionality of a set of variables.</p></a></li>
<li><a href='#VSS'><p> Apply the Very Simple Structure, MAP, and other criteria to determine the appropriate number of factors.</p></a></li>
<li><a href='#VSS.parallel'><p>Compare real and random VSS solutions</p></a></li>
<li><a href='#VSS.plot'><p>Plot VSS fits</p></a></li>
<li><a href='#VSS.scree'><p>Plot the successive eigen values for a scree test</p></a></li>
<li><a href='#winsor'><p>Find the Winsorized scores, means, sds or variances  for a vector, matrix, or data.frame</p></a></li>
<li><a href='#withinBetween'><p>An example of the distinction between within group and between group correlations</p></a></li>
<li><a href='#Yule'><p>From a two by two table, find the Yule coefficients of association, convert to phi, or tetrachoric, recreate table the table to create the Yule coefficient.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.4.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-17</td>
</tr>
<tr>
<td>Title:</td>
<td>Procedures for Psychological, Psychometric, and Personality
Research</td>
</tr>
<tr>
<td>Description:</td>
<td>A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> web page.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>mnormt,parallel,stats,graphics,grDevices,methods,lattice,nlme</td>
</tr>
<tr>
<td>Suggests:</td>
<td>psychTools, GPArotation, lavaan, lme4, Rcsdp, graph, knitr,
Rgraphviz</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://personality-project.org/r/psych/">https://personality-project.org/r/psych/</a>
<a href="https://personality-project.org/r/psych-manual.pdf">https://personality-project.org/r/psych-manual.pdf</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-18 22:01:00 UTC; WR</td>
</tr>
<tr>
<td>Author:</td>
<td>William Revelle <a href="https://orcid.org/0000-0003-4880-9610"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>William Revelle &lt;revelle@northwestern.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-18 23:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='00.psych'>A package for personality, psychometric, and psychological research</h2><span id='topic+psych'></span><span id='topic+psych-package'></span>

<h3>Description</h3>

<p>Overview of the psych package.
</p>
<p>The psych package has been developed at Northwestern University to include functions most useful for personality and psychological research.  Some of the functions (e.g., <code><a href="psychTools.html#topic+read.file">read.file</a></code>, <code><a href="psychTools.html#topic+read.clipboard">read.clipboard</a></code>, <code><a href="#topic+describe">describe</a></code>,  <code><a href="#topic+pairs.panels">pairs.panels</a></code>, <code><a href="#topic+error.bars">error.bars</a></code> and <code><a href="#topic+error.dots">error.dots</a></code>) are useful for basic data entry and descriptive analyses. Use help(package=&quot;psych&quot;) or objects(&quot;package:psych&quot;) for a list of all functions.  Two vignettes are included as part of the package.  The intro vignette tells how to install psych and overview vignette provides examples of using psych in many applications.  In addition, there are a growing set of tutorials available on the <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> webpages. 
</p>
<p>A companion package <code><a href="psychTools.html#topic+psychTools">psychTools</a></code> includes larger data set examples and four more vignette. 
</p>
<p>Psychometric applications include routines (<code><a href="#topic+fa">fa</a></code> for  maximum likelihood (fm=&quot;mle&quot;), minimum residual (fm=&quot;minres&quot;), minimum rank (fm=minrank)  principal axes (fm=&quot;pa&quot;) and weighted least squares (fm=&quot;wls&quot;)  factor analysis  as well as functions to do Schmid Leiman transformations (<code><a href="#topic+schmid">schmid</a></code>) to transform a hierarchical factor structure into a bifactor solution. Principal Components Analysis (<code><a href="#topic+pca">pca</a></code>) is also available.  Rotations  may be done using factor or components transformations to a target matrix include the standard Promax transformation (<code><a href="#topic+Promax">Promax</a></code>), a transformation to a cluster target, or to any simple target matrix (<code><a href="#topic+target.rot">target.rot</a></code>) as well as the ability to call many of the GPArotation functions (e.g., oblimin, quartimin, varimax, geomin, ...). Functions for determining the number of factors in a data matrix include Very Simple Structure (<code><a href="#topic+VSS">VSS</a></code>) and Minimum Average Partial correlation (<code><a href="#topic+MAP">MAP</a></code>). 
</p>
<p>An alternative approach to factor analysis is Item Cluster Analysis (<code><a href="#topic+ICLUST">ICLUST</a></code>). This function is particularly appropriate for exploratory scale construction.
</p>
<p>There are a number of functions for finding various reliability coefficients (see Revelle and Condon, 2019). These include the traditional  <code><a href="#topic+alpha">alpha</a></code>  (found for multiple scales and with more useful output by <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code>), beta (<code><a href="#topic+ICLUST">ICLUST</a></code>) and  both of McDonald's omega coefficients (<code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+omegaSem">omegaSem</a></code> and  <code><a href="#topic+omega.diagram">omega.diagram</a></code>) as well as Guttman's six estimates of internal consistency reliability (<code><a href="#topic+guttman">guttman</a></code>) and the six measures of Intraclass correlation coefficients (<code><a href="#topic+ICC">ICC</a></code>) discussed by Shrout and Fleiss are also available.  
</p>
<p>Multilevel analyses may be done by <code><a href="#topic+statsBy">statsBy</a></code> and  <code><a href="#topic+multilevel.reliability">multilevel.reliability</a></code>.
</p>
<p>The <code><a href="#topic+scoreItems">scoreItems</a></code>, and <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code> functions may be used to form single or multiple scales from sets of dichotomous, multilevel, or multiple choice items by specifying scoring keys.  <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> correct interscale correlations for overlapping items, so that it is possible to examine hierarchical or nested structures.
</p>
<p>Scales can be formed that best predict (after cross validation) particular criteria using <code><a href="#topic+bestScales">bestScales</a></code> using unit weighted or correlation weights. This procedure, also called  the <code><a href="#topic+BISCUIT">BISCUIT</a></code> algorithm (Best Items Scales that are Cross validated, Unit weighted, Informative, and Transparent) is a simple alternative to more complicated supervised machine learning algorithms.
</p>
<p>Additional functions make for more convenient descriptions of item characteristics  include 1 and 2 parameter Item Response measures.  The <code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+polychoric">polychoric</a></code> and <code><a href="#topic+irt.fa">irt.fa</a></code> functions are used to find 2 parameter descriptions of item functioning. <code><a href="#topic+scoreIrt">scoreIrt</a></code>, <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code> and <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> do basic IRT based scoring.
</p>
<p>A number of procedures have been developed as part of the Synthetic Aperture Personality Assessment (SAPA <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project.  These routines facilitate forming and analyzing composite scales equivalent to using the raw data but doing so by adding within and between cluster/scale item correlations. These functions include extracting clusters from factor loading matrices (<code><a href="#topic+factor2cluster">factor2cluster</a></code>), synthetically forming clusters from correlation matrices (<code><a href="#topic+cluster.cor">cluster.cor</a></code>), and finding multiple  ((<code><a href="#topic+lmCor">lmCor</a></code>) and partial ((<code><a href="#topic+partial.r">partial.r</a></code>) correlations from correlation matrices.
</p>
<p>If forming empirical scales, or testing out multiple regressions, it is important to cross validate the results.  <code><a href="#topic+crossValidation">crossValidation</a></code> will do this on a different data set.
</p>
<p><code><a href="#topic+lmCor">lmCor</a></code> and <code><a href="#topic+mediate">mediate</a></code> meet the desire to do regressions and mediation analysis from either raw data or from correlation matrices.  If raw data are provided, these functions can also do  moderation analyses.
</p>
<p>Functions to generate simulated data with particular structures include <code><a href="#topic+sim.circ">sim.circ</a></code> (for circumplex structures), <code><a href="#topic+sim.item">sim.item</a></code> (for general structures) and <code><a href="#topic+sim.congeneric">sim.congeneric</a></code> (for a specific demonstration of congeneric measurement).  The functions <code><a href="#topic+sim.congeneric">sim.congeneric</a></code>  and <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> can be used to create data sets with particular structural properties. A more general form for all of these is <code><a href="#topic+sim.structural">sim.structural</a></code> for generating general structural models.  These are discussed in more detail in the vignette (psych_for_sem).
</p>
<p>Functions to apply various standard statistical tests include <code><a href="#topic+p.rep">p.rep</a></code> and its variants for testing the probability of replication, <code><a href="#topic+r.con">r.con</a></code> for the confidence intervals of a correlation, and <code><a href="#topic+r.test">r.test</a></code> to test single, paired, or sets of correlations. 
</p>
<p>In order to study diurnal or circadian variations in mood, it is helpful to use circular statistics.  Functions to find the circular mean (<code><a href="#topic+circadian.mean">circadian.mean</a></code>), circular (phasic) correlations (<code><a href="#topic+circadian.cor">circadian.cor</a></code>) and the correlation between linear variables and circular variables (<code><a href="#topic+circadian.linear.cor">circadian.linear.cor</a></code>) supplement a function to find the best fitting phase angle (<code><a href="#topic+cosinor">cosinor</a></code>) for measures taken with a fixed period (e.g., 24 hours).
</p>
<p>A dynamic model of personality and motivation (the Cues-Tendency-Actions model) is include as <code><a href="#topic+cta">cta</a></code>.
</p>
<p>A number of useful helper functions allow for data input (<code><a href="psychTools.html#topic+read.file">read.file</a></code>), and data manipulation <code><a href="#topic+cs">cs</a></code> and <code><a href="psychTools.html#topic+dfOrder">dfOrder</a></code>,
</p>
<p>The most recent development version of the package is always available for download as a <em>source</em> file from the repository at the PMC lab:
</p>
<p>install.packages(&quot;psych&quot;, repos = &quot;https://personality-project.org/r/&quot;, type=&quot;source&quot;).
</p>
<p>This will provide the most recent version for PCs and Macs. 
</p>


<h3>Details</h3>

<p>Two vignettes (intro.pdf and scoring.pdf) are useful introductions to the package. They may be found as vignettes in R or may be downloaded from 
<a href="https://personality-project.org/r/psych/intro.pdf">https://personality-project.org/r/psych/intro.pdf</a> <a href="https://personality-project.org/r/psych/overview.pdf">https://personality-project.org/r/psych/overview.pdf</a> and <a href="https://personality-project.org/r/psych/psych_for_sem.pdf">https://personality-project.org/r/psych/psych_for_sem.pdf</a>.  In addition, there are a number of &quot;HowTo&quot;s available at <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> 
</p>
<p>The more important functions in the package are for the analysis of multivariate data, with an emphasis upon those functions useful in scale construction of item composites. However, there are a number of very useful functions for basic data manipulation including  
<code><a href="psychTools.html#topic+read.file">read.file</a></code>, <code><a href="psychTools.html#topic+read.clipboard">read.clipboard</a></code>, <code><a href="#topic+describe">describe</a></code>,  <code><a href="#topic+pairs.panels">pairs.panels</a></code>, <code><a href="#topic+error.bars">error.bars</a></code> and <code><a href="#topic+error.dots">error.dots</a></code>) which are useful for basic data entry and descriptive analyses.
</p>
<p>When given a set of items from a personality inventory, one goal is to combine these into higher level item composites. This leads to several questions:
</p>
<p>1) What are the basic properties of the data?  <code><a href="#topic+describe">describe</a></code> reports basic summary statistics (mean, sd, median, mad, range,  minimum, maximum, skew, kurtosis, standard error) for vectors, columns of matrices, or data.frames. <code><a href="#topic+describeBy">describeBy</a></code> provides descriptive statistics, organized by one or more grouping variables. <code><a href="#topic+statsBy">statsBy</a></code> provides even more detail for data structured by groups including within and between correlation matrices, ICCs for group differences, as well as basic descriptive statistics organized by group.
</p>
<p><code><a href="#topic+pairs.panels">pairs.panels</a></code> shows scatter plot matrices (SPLOMs) as well as histograms and the Pearson correlation for scales or items. <code><a href="#topic+error.bars">error.bars</a></code> will plot variable means with associated confidence intervals. <code><a href="#topic+errorCircles">errorCircles</a></code> will plot confidence intervals for both the x and y coordinates.  <code><a href="#topic+corr.test">corr.test</a></code> will find the significance values for a matrix of correlations. <code><a href="#topic+error.dots">error.dots</a></code> creates a dot chart with confidence intervals.
</p>
<p>2) What is the most appropriate number of item composites to form? After finding  either standard Pearson correlations, or finding tetrachoric or polychoric correlations,  the dimensionality of the correlation matrix may be examined. The number of factors/components problem is a standard question of factor analysis, cluster analysis, or principal components analysis. Unfortunately, there is no agreed upon answer. The Very Simple Structure (<code><a href="#topic+VSS">VSS</a></code>) set of procedures has been proposed as on answer to the question of the optimal number of factors.  Other procedures (<code><a href="#topic+VSS.scree">VSS.scree</a></code>,  <code><a href="#topic+VSS.parallel">VSS.parallel</a></code>,  <code><a href="#topic+fa.parallel">fa.parallel</a></code>, and <code><a href="#topic+MAP">MAP</a></code>)  also address this question.  <code><a href="#topic+nfactors">nfactors</a></code> combine several of these approaches into one convenient function. Unfortunately, there is no best answer to the problem.
</p>
<p>3) What are the best composites to form?  Although this may be answered using principal components  (<code><a href="#topic+principal">principal</a></code>, aka <code><a href="#topic+pca">pca</a></code>), principal axis (<code><a href="#topic+factor.pa">factor.pa</a></code>) or minimum residual (<code><a href="#topic+factor.minres">factor.minres</a></code>)  factor analysis (all part of the <code><a href="#topic+fa">fa</a></code> function) and to show the results graphically (<code><a href="#topic+fa.diagram">fa.diagram</a>)</code>, it is sometimes more useful to address this question using cluster analytic techniques.  Previous versions of <code><a href="#topic+ICLUST">ICLUST</a></code> (e.g., Revelle, 1979)  have been shown to be particularly successful at forming maximally consistent and independent item composites.  Graphical output from <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code> uses the Graphviz dot language and allows one to write files suitable for Graphviz.  If Rgraphviz is available, these graphs can be done in R.
</p>
<p>Graphical organizations of cluster and factor analysis output can be done using  <code><a href="#topic+cluster.plot">cluster.plot</a></code> which plots items by cluster/factor loadings and assigns items to that dimension with the highest loading.  
</p>
<p>4) How well does a particular item composite reflect a single construct?  This is a question of reliability and general factor saturation.  Multiple solutions for this problem result in (Cronbach's) alpha (<code><a href="#topic+alpha">alpha</a></code>, <code><a href="#topic+scoreItems">scoreItems</a></code>), (Revelle's) Beta (<code><a href="#topic+ICLUST">ICLUST</a></code>), and (McDonald's) <code><a href="#topic+omega">omega</a></code> (both omega hierarchical and omega total). Additional reliability estimates may be found in the <code><a href="#topic+guttman">guttman</a></code> function.
</p>
<p>This can also be examined by applying <code><a href="#topic+irt.fa">irt.fa</a></code> Item Response Theory techniques using factor analysis of the <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> correlation matrices and converting the results into the standard two parameter parameterization of item difficulty and item discrimination.  Information functions for the items suggest where they are most effective.
</p>
<p>5) For some applications, data matrices are synthetically combined from sampling different items for different people.  So called Synthetic Aperture Personality Assessement (SAPA) techniques allow the formation of large correlation or covariance matrices even though no one person has taken all of the items. To analyze such data sets, it is easy to form item composites based upon the covariance matrix of the items, rather than original data set.  These matrices may then be analyzed using a number of functions (e.g., <code><a href="#topic+cluster.cor">cluster.cor</a></code>,   <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+pca">pca</a></code>, <code><a href="#topic+mat.regress">mat.regress</a></code>, and <code><a href="#topic+factor2cluster">factor2cluster</a></code>.
</p>
<p>6) More typically, one has a raw data set to analyze. <code><a href="#topic+alpha">alpha</a></code> will report several reliablity estimates as well as item-whole correlations for items forming a single scale, <code><a href="#topic+score.items">score.items</a></code> will score data sets on multiple scales, reporting the scale scores, item-scale and scale-scale correlations, as well as coefficient alpha,  alpha-1 and G6+. Using a &lsquo;keys&rsquo; matrix (created by <code><a href="#topic+make.keys">make.keys</a></code> or by hand), scales can have overlapping or independent items. <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code> scores multiple choice items or converts multiple choice items to dichtomous (0/1) format for other functions. 
</p>
<p>If the scales have overlapping items, then <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> will give similar statistics, but correcting for the item overlap.
</p>
<p>7) The <code><a href="#topic+reliability">reliability</a></code> function combines the output from several different ways to estimate reliability including <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+splitHalf">splitHalf</a></code>.
</p>
<p>8) In addition to classical test theory (CTT) based scores of either totals or averages, 1 and 2 parameter IRT based scores may be found with <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>, <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> or more generally <code><a href="#topic+scoreIrt">scoreIrt</a></code>. Although highly correlated with CTT estimates, these scores take advantage of different item difficulties and are particularly appropriate for the problem of missing data. 
</p>
<p>9) If the data has a multilevel structure (e.g, items nested within time nested within subjects) the <code><a href="#topic+multilevel.reliability">multilevel.reliability</a></code> aka <code><a href="#topic+mlr">mlr</a></code> function will estimate generalizability coefficients for data over subjects, subjects over time, etc. <code><a href="#topic+mlPlot">mlPlot</a></code> will provide plots for each subject of items over time. <code><a href="#topic+mlArrange">mlArrange</a></code> takes the conventional wide output format and converts it to the long format necessary for some multilevel functions. Other functions useful for multilevel data include <code><a href="#topic+statsBy">statsBy</a></code> and <code><a href="#topic+faBy">faBy</a></code>.    
</p>
<p>An additional set of functions generate simulated data to meet certain structural properties. <code><a href="#topic+sim.anova">sim.anova</a></code> produces data simulating a 3 way analysis of variance (ANOVA) or linear model with or with out repeated measures. <code><a href="#topic+sim.item">sim.item</a></code> creates simple structure data,  <code><a href="#topic+sim.circ">sim.circ</a></code> will produce circumplex structured data,  <code><a href="#topic+sim.dichot">sim.dichot</a></code> produces circumplex or simple structured data for dichotomous items.  These item structures are useful for understanding the effects of skew, differential item endorsement on factor and cluster analytic soutions.  <code><a href="#topic+sim.structural">sim.structural</a></code> will produce correlation matrices and data matrices to match general structural models. (See the vignette).
</p>
<p>When examining personality items, some people like to discuss them as representing items in a two dimensional space with a circumplex structure.  Tests of circumplex fit <code><a href="#topic+circ.tests">circ.tests</a></code> have been developed.  When representing items in a circumplex, it is convenient to view them in <code><a href="#topic+polar">polar</a></code> coordinates. 
</p>
<p>Additional functions for testing the difference between two independent or dependent correlation <code><a href="#topic+r.test">r.test</a></code>, to find the <code><a href="#topic+phi">phi</a></code> or <code><a href="#topic+Yule">Yule</a></code> coefficients from a two by table, or to find the confidence interval of a correlation coefficient.    
</p>
<p>Many data sets are included: <code><a href="psychTools.html#topic+bfi">bfi</a></code> represents 25 personality items thought to represent five factors of personality, <code><a href="psychTools.html#topic+ability">ability</a></code> has 14 multiple choice iq items. <code><a href="#topic+sat.act">sat.act</a></code> has data on self reported test scores by age and gender. <code><a href="psychTools.html#topic+galton">galton</a> </code>  Galton's data set of the heights of parents and their children. <code><a href="psychTools.html#topic+peas">peas</a></code> recreates the original Galton data set of the genetics of sweet peas.  <code><a href="psychTools.html#topic+heights">heights</a></code> and
<code><a href="psychTools.html#topic+cubits">cubits</a></code> provide even more Galton data, <code><a href="psychTools.html#topic+vegetables">vegetables</a></code> provides the Guilford preference matrix of vegetables.  <code><a href="psychTools.html#topic+cities">cities</a></code> provides airline miles between 11 US cities (demo data for multidimensional scaling).
</p>
<p>Partial Index (to see the entire index, see the link at the bottom of every help page)
</p>
<p><a href="#topic+psych">psych</a>      A package for personality, psychometric, and psychological research.<br />
</p>
<p>Useful data entry and descriptive statistics<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+read.file">read.file</a>       </td><td style="text-align: left;">      search for, find, and read from file</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+read.clipboard">read.clipboard</a>       </td><td style="text-align: left;">      shortcut for reading from the clipboard</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+read.clipboard.csv">read.clipboard.csv</a>   </td><td style="text-align: left;">      shortcut for reading comma delimited files from clipboard </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+read.clipboard.lower">read.clipboard.lower</a>  </td><td style="text-align: left;">            shortcut for reading lower triangular matrices from the clipboard</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+read.clipboard.upper">read.clipboard.upper</a>   </td><td style="text-align: left;">           shortcut for reading upper triangular matrices from the clipboard</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+describe">describe</a>        </td><td style="text-align: left;">           Basic descriptive statistics useful for psychometrics</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+describe.by">describe.by</a>    </td><td style="text-align: left;">            Find summary statistics by groups</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+statsBy">statsBy</a>       </td><td style="text-align: left;">            Find summary statistics by a grouping variable,  including within and between correlation matrices. </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+mlArrange">mlArrange</a>        </td><td style="text-align: left;">          Change multilevel data from wide to long format</td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+headtail">headtail</a>        </td><td style="text-align: left;">           combines the head and tail functions for showing data sets</td>
</tr>
<tr>
 <td style="text-align: left;">

<a href="#topic+pairs.panels">pairs.panels</a>    </td><td style="text-align: left;">          SPLOM and correlations for a data matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+corr.test">corr.test</a>    </td><td style="text-align: left;">             Correlations, sample sizes, and p values  for a data matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cor.plot">cor.plot</a>    </td><td style="text-align: left;">             graphically show the size of correlations in a correlation matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+multi.hist">multi.hist</a>      </td><td style="text-align: left;">         Histograms and densities of multiple variables arranged in matrix form</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+skew">skew</a>           </td><td style="text-align: left;">          Calculate skew for a vector, each column of a matrix, or data.frame</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+kurtosi">kurtosi</a>        </td><td style="text-align: left;">          Calculate kurtosis for a vector, each column of a matrix or dataframe</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+geometric.mean">geometric.mean</a>  </td><td style="text-align: left;">        Find the geometric mean of a vector or columns of a data.frame </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+harmonic.mean">harmonic.mean</a>  </td><td style="text-align: left;">         Find the harmonic mean of a vector or columns of a data.frame </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+error.bars">error.bars</a>      </td><td style="text-align: left;">         Plot means and error bars </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+error.bars.by">error.bars.by</a>   </td><td style="text-align: left;">           Plot means and error bars for separate groups</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+error.crosses">error.crosses</a>   </td><td style="text-align: left;">         Two way error bars </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+interp.median">interp.median</a>  </td><td style="text-align: left;">           Find the interpolated median, quartiles, or general quantiles. </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+rescale">rescale</a>        </td><td style="text-align: left;">           Rescale data to specified mean and standard deviation </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+table2df">table2df</a>      </td><td style="text-align: left;">            Convert a two dimensional table of counts to a matrix or data frame </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Data reduction through cluster and factor analysis<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa">fa</a>              </td><td style="text-align: left;">         Combined function for principal axis, minimum 
residual,  weighted least squares, </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> and maximum likelihood factor analysis</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.pa">factor.pa</a>        </td><td style="text-align: left;">        Do a principal Axis factor analysis   (deprecated)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.minres">factor.minres</a>    </td><td style="text-align: left;">        Do a minimum residual factor analysis (deprecated)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.wls">factor.wls</a>    </td><td style="text-align: left;">        Do a weighted least squares factor analysis (deprecated)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.graph">fa.graph</a>        </td><td style="text-align: left;">         Show the results of a factor analysis or principal components analysis graphically</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.diagram">fa.diagram</a>     </td><td style="text-align: left;">          Show the results of a factor analysis without using Rgraphviz </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.sort">fa.sort</a>        </td><td style="text-align: left;">          Sort a factor or principal components output </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.extension">fa.extension</a>   </td><td style="text-align: left;">          Apply the Dwyer extension for factor loadingss </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+principal">principal</a>       </td><td style="text-align: left;">         Do an eigen value decomposition to find the principal components of a matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.parallel">fa.parallel</a>     </td><td style="text-align: left;">         Scree test and Parallel analysis </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.parallel.poly">fa.parallel.poly</a>     </td><td style="text-align: left;">     Scree test and Parallel analysis for polychoric matrices </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.scores">factor.scores</a>     </td><td style="text-align: left;">           Estimate factor scores given a data matrix and factor loadings </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+guttman">guttman</a>         </td><td style="text-align: left;">          8 different measures of reliability (6 from Guttman (1945) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+irt.fa">irt.fa</a></code>   </td><td style="text-align: left;">          Apply factor analysis to dichotomous items to get IRT parameters </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+iclust">iclust</a></code>   </td><td style="text-align: left;">          Apply the ICLUST algorithm</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+ICLUST.diagram">ICLUST.diagram</a>    </td><td style="text-align: left;">         The base R graphics output function called by  <code><a href="#topic+iclust">iclust</a></code> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+ICLUST.graph">ICLUST.graph</a>    </td><td style="text-align: left;">          Graph the output from ICLUST using the dot language</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a>   </td><td style="text-align: left;">          Graph the output from ICLUST using rgraphviz </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+kaiser">kaiser</a>     </td><td style="text-align: left;">           Apply kaiser normalization before rotating </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+reliability">reliability</a> </td><td style="text-align: left;">    A wrapper function to find alpha, omega, split half. etc. </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+polychoric">polychoric</a>        </td><td style="text-align: left;">          Find the polychoric correlations for items  and find item thresholds</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+poly.mat">poly.mat</a>        </td><td style="text-align: left;">          Find the polychoric correlations for items (uses J. Fox's hetcor) </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega">omega</a>           </td><td style="text-align: left;">          Calculate the omega estimate of factor saturation (requires the GPArotation package)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega.graph">omega.graph</a>      </td><td style="text-align: left;">         Draw a hierarchical or Schmid Leiman orthogonalized solution (uses Rgraphviz) </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+partial.r">partial.r</a>        </td><td style="text-align: left;">         Partial variables from a correlation matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="stats.html#topic+predict">predict</a>          </td><td style="text-align: left;">         Predict factor/component scores for new data </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+schmid">schmid</a>            </td><td style="text-align: left;">        Apply the Schmid Leiman transformation to a correlation matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+scoreItems">scoreItems</a>       </td><td style="text-align: left;">        Combine items into multiple scales and find alpha</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+score.multiple.choice">score.multiple.choice</a>  </td><td style="text-align: left;">   Combine items into multiple scales and find alpha and basic scale statistics</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+scoreOverlap">scoreOverlap</a> </td><td style="text-align: left;">   Find item and scale statistics (similar to <a href="#topic+score.items">score.items</a>) but correct for item overlap </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+lmCor">lmCor</a>             </td><td style="text-align: left;">       Find Cohen's set correlation between two sets of variables (see also <a href="#topic+lmCor">lmCor</a> for the latest version)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+smc">smc</a>             </td><td style="text-align: left;">          Find the Squared Multiple Correlation (used for initial communality estimates)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+tetrachoric">tetrachoric</a>            </td><td style="text-align: left;">    Find tetrachoric correlations and item thresholds </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+polyserial">polyserial</a>            </td><td style="text-align: left;">    Find polyserial and biserial correlations for item validity studies </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+mixed.cor">mixed.cor</a>          </td><td style="text-align: left;">       Form a correlation matrix from continuous, polytomous, and dichotomous items </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+VSS">VSS</a>            </td><td style="text-align: left;">           Apply the Very Simple Structure criterion to determine the appropriate number of factors.</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+VSS.parallel">VSS.parallel</a>    </td><td style="text-align: left;">          Do a parallel analysis to determine the number of factors for a random matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+VSS.plot">VSS.plot</a>       </td><td style="text-align: left;">           Plot VSS output</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+VSS.scree">VSS.scree</a>      </td><td style="text-align: left;">           Show the scree plot of the factor/principal components</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+MAP">MAP</a>             </td><td style="text-align: left;">         Apply the Velicer Minimum Absolute Partial criterion for number of factors </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Functions for reliability analysis (some are listed above as well).
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+alpha">alpha</a>      </td><td style="text-align: left;">         Find coefficient alpha and Guttman Lambda 6 for a scale (see also <a href="#topic+score.items">score.items</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+guttman">guttman</a>         </td><td style="text-align: left;">          8 different measures of reliability (6 from Guttman (1945) </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega">omega</a>           </td><td style="text-align: left;">          Calculate the omega estimates of reliability (requires the GPArotation package)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omegaSem">omegaSem</a>           </td><td style="text-align: left;">       Calculate the omega estimates of reliability  using a Confirmatory model (requires the sem package)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+ICC">ICC</a>          </td><td style="text-align: left;">             Intraclass correlation coefficients  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+score.items">score.items</a>       </td><td style="text-align: left;">        Combine items into multiple scales and find alpha</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+glb.algebraic">glb.algebraic</a> </td><td style="text-align: left;">  The greates lower bound found by an algebraic solution (requires Rcsdp).  Written by  Andreas Moeltner </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Procedures particularly useful for Synthetic Aperture Personality Assessment<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+alpha">alpha</a>      </td><td style="text-align: left;">         Find coefficient alpha and Guttman Lambda 6 for a scale (see also <a href="#topic+score.items">score.items</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+bestScales">bestScales</a> </td><td style="text-align: left;"> A bootstrap aggregation function for choosing most predictive unit weighted items </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+make.keys">make.keys</a>        </td><td style="text-align: left;">         Create the keys file for score.items or cluster.cor            </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+correct.cor">correct.cor</a>      </td><td style="text-align: left;">        Correct a correlation matrix for unreliability</td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+count.pairwise">count.pairwise</a>   </td><td style="text-align: left;">        Count the number of complete cases when doing pair wise correlations</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cluster.cor">cluster.cor</a>       </td><td style="text-align: left;">        find correlations of composite variables from larger matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cluster.loadings">cluster.loadings</a>  </td><td style="text-align: left;">        find correlations of items with  composite variables from a larger matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+eigen.loadings">eigen.loadings</a>    </td><td style="text-align: left;">        Find the loadings when doing an eigen value decomposition</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa">fa</a>        </td><td style="text-align: left;">        Do a minimal residual or principal axis factor analysis and estimate factor scores</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.extension">fa.extension</a>        </td><td style="text-align: left;">      Extend a factor analysis to a set of new variables</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.pa">factor.pa</a>        </td><td style="text-align: left;">        Do a Principal Axis factor analysis and estimate factor scores</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor2cluster">factor2cluster</a>    </td><td style="text-align: left;">       extract cluster definitions from factor loadings</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.congruence">factor.congruence</a>  </td><td style="text-align: left;">      Factor congruence coefficient</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.fit">factor.fit</a>       </td><td style="text-align: left;">        How well does a factor model fit a correlation matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.model">factor.model</a>    </td><td style="text-align: left;">          Reproduce a correlation matrix based upon the factor model</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.residuals">factor.residuals</a> </td><td style="text-align: left;">        Fit = data - model</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+factor.rotate">factor.rotate</a>   </td><td style="text-align: left;">         ``hand rotate" factors</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+guttman">guttman</a>         </td><td style="text-align: left;">         8 different measures of reliability</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+lmCor">lmCor</a>           </td><td style="text-align: left;">        standardized multiple regression from raw or correlation matrix input Formerly called <a href="#topic+lmCor">lmCor</a></td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+mat.regress">mat.regress</a>     </td><td style="text-align: left;">         standardized multiple regression from raw or correlation matrix input</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+polyserial">polyserial</a>       </td><td style="text-align: left;">      polyserial and biserial correlations with massive missing data</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+tetrachoric">tetrachoric</a>      </td><td style="text-align: left;">    Find tetrachoric correlations and item thresholds </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Functions for generating simulated data sets <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim">sim</a>               </td><td style="text-align: left;">       The basic simulation functions </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.anova">sim.anova</a>         </td><td style="text-align: left;">       Generate 3 independent variables and 1 or more 
dependent variables for demonstrating ANOVA </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;">  and lm designs </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.circ">sim.circ</a>         </td><td style="text-align: left;">        Generate a two dimensional circumplex item structure </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.item">sim.item</a>         </td><td style="text-align: left;">        Generate a two dimensional simple structure with 
particular item characteristics </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.congeneric">sim.congeneric</a>   </td><td style="text-align: left;">        Generate a one factor congeneric reliability structure </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.minor">sim.minor</a>       </td><td style="text-align: left;">         Simulate nfact major and nvar/2 minor factors  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.structural">sim.structural</a>   </td><td style="text-align: left;">        Generate a multifactorial structural model  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.irt">sim.irt</a>          </td><td style="text-align: left;">        Generate data for a 1, 2, 3 or 4 parameter logistic model</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.VSS">sim.VSS</a>          </td><td style="text-align: left;">          Generate simulated data for the factor model</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+phi.demo">phi.demo</a>          </td><td style="text-align: left;">       Create artificial data matrices for teaching purposes</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sim.hierarchical">sim.hierarchical</a> </td><td style="text-align: left;">        Generate simulated correlation matrices with hierarchical or any structure</td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+sim.spherical">sim.spherical</a>    </td><td style="text-align: left;">        Generate three dimensional spherical data (generalization of circumplex to 3 space)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Graphical functions (require Rgraphviz) &ndash; deprecated <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+structure.graph">structure.graph</a>  </td><td style="text-align: left;">           Draw a sem or regression graph </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.graph">fa.graph</a>        </td><td style="text-align: left;">           Draw the factor structure from a factor or principal components analysis </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega.graph">omega.graph</a>      </td><td style="text-align: left;">           Draw the factor structure from an omega analysis(either with or without the Schmid Leiman transformation) </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+ICLUST.graph">ICLUST.graph</a>     </td><td style="text-align: left;">           Draw the tree diagram from ICLUST  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Graphical functions that do not require Rgraphviz <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+diagram">diagram</a>           </td><td style="text-align: left;">           A general set of diagram functions. </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+structure.diagram">structure.diagram</a>  </td><td style="text-align: left;">           Draw a sem or regression graph </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.diagram">fa.diagram</a>        </td><td style="text-align: left;">           Draw the factor structure from a factor or principal components analysis </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega.diagram">omega.diagram</a>      </td><td style="text-align: left;">           Draw the factor structure from an omega analysis(either with or without the Schmid Leiman transformation) </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<a href="#topic+ICLUST.diagram">ICLUST.diagram</a>     </td><td style="text-align: left;">           Draw the tree diagram from ICLUST  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+plot.psych">plot.psych</a>        </td><td style="text-align: left;">           A call to plot various types of output (e.g. from irt.fa, fa, omega, iclust </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cor.plot">cor.plot</a>        </td><td style="text-align: left;">            A heat map display of correlations </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+scatterHist">scatterHist</a>     </td><td style="text-align: left;">            Bivariate scatter plot and histograms </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+spider">spider</a>          </td><td style="text-align: left;">            Spider and radar plots (circular displays of correlations)
</td>
</tr>

</table>

<p>Circular statistics (for circadian data analysis) <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+circadian.cor">circadian.cor</a>     </td><td style="text-align: left;">      Find the correlation with e.g., mood and time of day </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+circadian.linear.cor">circadian.linear.cor</a> </td><td style="text-align: left;">  Correlate a circular value with a linear value </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+circadian.mean">circadian.mean</a>     </td><td style="text-align: left;">      Find the circular mean of each column of a a data set  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cosinor">cosinor</a>       </td><td style="text-align: left;">            Find the best fitting phase angle for a circular data set </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Miscellaneous functions<br />
<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+comorbidity">comorbidity</a>   </td><td style="text-align: left;">             Convert base rate and comorbity to phi, Yule and tetrachoric</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+df2latex">df2latex</a>   </td><td style="text-align: left;">               Convert a data.frame or matrix to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+dummy.code">dummy.code</a>     </td><td style="text-align: left;">          Convert categorical data to dummy codes </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fisherz">fisherz</a>      </td><td style="text-align: left;">             Apply the Fisher r to z transform</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fisherz2r">fisherz2r</a>    </td><td style="text-align: left;">             Apply the Fisher z to r transform</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+ICC">ICC</a>          </td><td style="text-align: left;">             Intraclass correlation coefficients  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cortest.mat">cortest.mat</a>     </td><td style="text-align: left;">         Test for equality of two matrices (see also cortest.normal, cortest.jennrich ) </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+cortest.bartlett">cortest.bartlett</a> </td><td style="text-align: left;">         Test whether a matrix is an identity matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+paired.r">paired.r</a>       </td><td style="text-align: left;">          Test for the difference of two paired or two independent correlations</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+r.con">r.con</a>          </td><td style="text-align: left;">            Confidence intervals for correlation coefficients </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+r.test">r.test</a>         </td><td style="text-align: left;">           Test of significance of r, differences between rs. </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+p.rep">p.rep</a>          </td><td style="text-align: left;">           The probability of replication given a p, r, t, or F </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+phi">phi</a>             </td><td style="text-align: left;">          Find the phi coefficient of correlation from a 2 x 2 table </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+phi.demo">phi.demo</a>        </td><td style="text-align: left;">          Demonstrate the problem of phi coefficients with varying cut points </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+phi2poly">phi2poly</a>         </td><td style="text-align: left;">         Given a phi coefficient, what is the polychoric correlation</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+phi2poly.matrix">phi2poly.matrix</a>  </td><td style="text-align: left;">         Given a phi coefficient, what is the polychoric correlation (works on matrices)</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+polar">polar</a>           </td><td style="text-align: left;">          Convert 2 dimensional factor loadings to polar coordinates.</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+scaling.fits">scaling.fits</a>     </td><td style="text-align: left;">         Compares alternative scaling solutions and gives goodness of fits </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+scrub">scrub</a>   </td><td style="text-align: left;">            Basic data cleaning </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+tetrachor">tetrachor</a>   </td><td style="text-align: left;">            Finds tetrachoric correlations </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+thurstone">thurstone</a>       </td><td style="text-align: left;">          Thurstone Case V scaling </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+tr">tr</a>               </td><td style="text-align: left;">         Find the trace of a square matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+wkappa">wkappa</a>          </td><td style="text-align: left;">          weighted and unweighted versions of Cohen's kappa </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Yule">Yule</a>            </td><td style="text-align: left;">           Find the Yule Q coefficient of correlation </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Yule.inv">Yule.inv</a>         </td><td style="text-align: left;">          What is the two by two table that produces a Yule Q with set marginals? </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Yule2phi">Yule2phi</a>         </td><td style="text-align: left;">          What is the phi coefficient corresponding to a Yule Q with set marginals? </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Yule2tetra">Yule2tetra</a>        </td><td style="text-align: left;">          Convert one or a matrix of Yule coefficients to tetrachoric coefficients. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Functions that are under development and not recommended for casual use <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+irt.item.diff.rasch">irt.item.diff.rasch</a>  </td><td style="text-align: left;">    IRT estimate of item difficulty with assumption that theta = 0</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+irt.person.rasch">irt.person.rasch</a>	   </td><td style="text-align: left;">     Item Response Theory estimates of theta (ability) using a Rasch like model</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Data sets included in the psych or psychTools package <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+bfi">bfi</a>          </td><td style="text-align: left;">           represents 25 personality items thought to represent five factors of personality </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Thurstone">Thurstone</a>     </td><td style="text-align: left;">           8 different data sets with a bifactor structure  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+cities">cities</a>        </td><td style="text-align: left;">          The airline distances between 11 cities (used to demonstrate MDS) </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+epi.bfi">epi.bfi</a>        </td><td style="text-align: left;">         13 personality scales </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+iqitems">iqitems</a>        </td><td style="text-align: left;">         14 multiple choice iq items </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+msq">msq</a>            </td><td style="text-align: left;">         75 mood items  </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+sat.act">sat.act</a>        </td><td style="text-align: left;">          Self reported ACT and SAT Verbal and Quantitative scores by age and gender</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+Tucker">Tucker</a>         </td><td style="text-align: left;">          Correlation matrix from Tucker </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+galton">galton</a>        </td><td style="text-align: left;">            Galton's data set of the heights of parents and their children </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+heights">heights</a>       </td><td style="text-align: left;">            Galton's data set of the relationship between height and forearm (cubit) length </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+cubits">cubits</a>        </td><td style="text-align: left;">             Galton's data table of height and forearm length </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+peas">peas</a>          </td><td style="text-align: left;">            Galton`s data set of the diameters of 700 parent and offspring sweet peas </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="psychTools.html#topic+vegetables">vegetables</a>     </td><td style="text-align: left;">           Guilford`s preference matrix of vegetables (used for thurstone) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>A debugging function that may also be used as a demonstration of psych.
</p>

<table>
<tr>
 <td style="text-align: left;">
<a href="#topic+test.psych">test.psych</a>   </td><td style="text-align: left;">          Run a test of the major functions on 5 different data 
sets.  Primarily for development purposes.</td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;">  Although the output can be used as a demo of the various functions.
</td>
</tr>

</table>



<h3>Note</h3>

<p>Development versions (source code) of this package are maintained at the  repository <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> along with further documentation.   Specify that you are downloading a source package.  
<br />
Some functions require other packages. Specifically, omega and schmid require the GPArotation package,  ICLUST.rgraph and fa.graph require Rgraphviz but have alternatives using the diagram functions.  i.e.:
<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
function  </td><td style="text-align: left;">     requires</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+omega">omega</a>    </td><td style="text-align: left;">     GPArotation </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+schmid">schmid</a>    </td><td style="text-align: left;">     GPArotation</td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a> </td><td style="text-align: left;">   Rgraphviz </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+fa.graph">fa.graph</a>  </td><td style="text-align: left;">      Rgraphviz </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+structure.graph">structure.graph</a> </td><td style="text-align: left;"> Rgraphviz </td>
</tr>
<tr>
 <td style="text-align: left;">
<a href="#topic+glb.algebraic">glb.algebraic</a> </td><td style="text-align: left;"> Rcsdp </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>A general guide to personality theory and research may be found at the personality-project <a href="https://personality-project.org/">https://personality-project.org/</a>. See also the short guide to R at <a href="https://personality-project.org/r/">https://personality-project.org/r/</a>.
In addition, see 
</p>
<p>Revelle, W. (in preparation) An Introduction to Psychometric Theory with applications in R. Springer. at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>   
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.   <a href="https://osf.io/preprints/psyarxiv/2y3w9/">https://osf.io/preprints/psyarxiv/2y3w9/</a> Preprint available from PsyArxiv  </p>


<h3>Examples</h3>

<pre><code class='language-R'>#See the separate man pages and the complete index.
#to test most of the psych package run the following
#test.psych()   
</code></pre>

<hr>
<h2 id='alpha'>Find two estimates of reliability: Cronbach's alpha and Guttman's Lambda 6. </h2><span id='topic+alpha'></span><span id='topic+alpha.scale'></span><span id='topic+alpha.ci'></span><span id='topic+alpha2r'></span>

<h3>Description</h3>

<p>Internal consistency measures of reliability range from <code class="reqn">\omega_h</code> to <code class="reqn">\alpha</code> to <code class="reqn">\omega_t</code>.  This function reports two estimates: Cronbach's coefficient <code class="reqn">\alpha</code> and Guttman's <code class="reqn">\lambda_6</code>.  Also reported are item - whole correlations, <code class="reqn">\alpha</code> if an item is omitted, and item means and standard deviations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alpha(x, keys=NULL,cumulative=FALSE, title=NULL, max=10,na.rm = TRUE,
   check.keys=FALSE,n.iter=1,delete=TRUE,use="pairwise",warnings=TRUE,
   n.obs=NULL,impute=NULL, discrete=TRUE
   )
alpha.ci(alpha,n.obs,n.var=NULL,p.val=.05,digits=2) #returns an invisible object
alpha2r(alpha, n.var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha_+3A_x">x</code></td>
<td>
<p>A data.frame or matrix of data, or a covariance or correlation matrix </p>
</td></tr>
<tr><td><code id="alpha_+3A_keys">keys</code></td>
<td>
<p>If some items are to be reversed keyed, then either specify the direction of all items or just a vector of which items to reverse </p>
</td></tr>
<tr><td><code id="alpha_+3A_title">title</code></td>
<td>
<p>Any text string to identify this run</p>
</td></tr>
<tr><td><code id="alpha_+3A_cumulative">cumulative</code></td>
<td>
<p>should means reflect the sum of items or the mean of the items.  The default value is means.</p>
</td></tr>
<tr><td><code id="alpha_+3A_max">max</code></td>
<td>
<p>the number of categories/item to consider if reporting category frequencies.  Defaults to 10, passed to <code>link{response.frequencies}</code> </p>
</td></tr>
<tr><td><code id="alpha_+3A_na.rm">na.rm</code></td>
<td>
<p>The default is to remove missing values and find pairwise correlations</p>
</td></tr>
<tr><td><code id="alpha_+3A_check.keys">check.keys</code></td>
<td>
<p>if TRUE, then find the first principal component and reverse key items with negative loadings.  Give a warning if this happens. </p>
</td></tr>
<tr><td><code id="alpha_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of iterations if bootstrapped confidence intervals are desired</p>
</td></tr>
<tr><td><code id="alpha_+3A_delete">delete</code></td>
<td>
<p>Delete items with no variance and issue a warning</p>
</td></tr>
<tr><td><code id="alpha_+3A_use">use</code></td>
<td>
<p>Options to pass to the cor function: &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot;. The default is &quot;pairwise&quot;</p>
</td></tr>
<tr><td><code id="alpha_+3A_warnings">warnings</code></td>
<td>
<p>By default print a warning and a message that items were reversed. Suppress the message if warnings = FALSE</p>
</td></tr>
<tr><td><code id="alpha_+3A_alpha">alpha</code></td>
<td>
<p>The value to use for confidence intervals</p>
</td></tr>
<tr><td><code id="alpha_+3A_n.obs">n.obs</code></td>
<td>
<p>If using correlation matrices as input, by specify the number of observations, we can find confidence intervals</p>
</td></tr>
<tr><td><code id="alpha_+3A_impute">impute</code></td>
<td>
<p>How should we impute missing data? Not at all, medians, or means</p>
</td></tr>
<tr><td><code id="alpha_+3A_discrete">discrete</code></td>
<td>
<p>If TRUE, then report frequencies by categories.</p>
</td></tr>
<tr><td><code id="alpha_+3A_n.var">n.var</code></td>
<td>
<p>Number of items in the scale (to find r.bar)</p>
</td></tr>
<tr><td><code id="alpha_+3A_p.val">p.val</code></td>
<td>
<p>width of confidence interval (pval/2 to 1-p.val/2) </p>
</td></tr>
<tr><td><code id="alpha_+3A_digits">digits</code></td>
<td>
<p>How many digits to use for alpha.ci</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Alpha is one of several estimates of the internal consistency reliability of a test.
</p>
<p>Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's <code class="reqn">\alpha</code>  (1951) underestimates the reliability of a test and over estimates the first factor saturation.
</p>
<p><code class="reqn">\alpha</code> (Cronbach, 1951) is the same as Guttman's  <code class="reqn">\lambda</code>3 (Guttman, 1945) and may be found by
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_3 =  \frac{n}{n-1}\Bigl(1 - \frac{tr(\vec{V})_x}{V_x}\Bigr) = \frac{n}{n-1} \frac{V_x - tr(\vec{V}_x)}{V_x} = \alpha
</code>
</p>

<p>Perhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is &ldquo;lumpy&quot;) coefficients <code class="reqn">\beta</code> (Revelle, 1979; see <code><a href="#topic+ICLUST">ICLUST</a></code>) and <code class="reqn">\omega_h</code> (see <code><a href="#topic+omega">omega</a></code>)  are more appropriate estimates of the general factor saturation.  <code class="reqn">\omega_t</code> (see <code><a href="#topic+omega">omega</a></code>) is a better estimate of the reliability of the total test.  
</p>
<p>Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, <code class="reqn">e_j^2</code>,  and is
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_6 = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1-r_{smc}^2)}{V_x}
.</code>
</p>

<p>The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.
</p>
<p>G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha &gt; G6, but if the loadings are unequal or if there is a general factor, G6 &gt; alpha. alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep).
</p>
<p>Alpha and G6 are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  
</p>
<p>A useful index of the quality of the test that is linear with the number of items and the average correlation is the Signal/Noise ratio where </p>
<p style="text-align: center;"><code class="reqn">s/n = \frac{n \bar{r}}{1-\bar{r}}</code>
</p>
<p>  (Cronbach and Gleser, 1964; Revelle and Condon (2019)).
</p>
<p>More complete reliability analyses of a single scale can be done using the <code><a href="#topic+omega">omega</a></code> function which finds <code class="reqn">\omega_h</code> and <code class="reqn">\omega_t</code> based upon a hierarchical factor analysis.  
</p>
<p>Alternative functions <code><a href="#topic+score.items">score.items</a></code> and  <code><a href="#topic+cluster.cor">cluster.cor</a></code> will also score multiple scales and report more useful statistics. &ldquo;Standardized&quot; alpha is calculated from the inter-item correlations and will differ from raw alpha. 
</p>
<p>Four alternative item-whole correlations are reported, three are conventional, one unique.  raw.r is the correlation of the item with the entire scale, not correcting for item overlap. std.r is the correlation of the item with the entire scale, if each item were standardized.  r.drop is the correlation of the item with the scale composed of the remaining items.  Although each of these are conventional statistics, they have the disadvantage that a) item overlap inflates the first and b) the scale is different for each item when an item is dropped. Thus, the fourth alternative, r.cor, corrects for the item overlap by subtracting the item variance but then replaces this with the best estimate of common variance, the smc. This is similar to a suggestion by Cureton (1966).
</p>
<p>If some items are to be reversed keyed then they can be specified by either item name or by item location.  (Look at the 3rd and 4th examples.)  Automatic reversal can also be done, and this is based upon the sign of the loadings on the first principal component (Example 5).  This requires the check.keys option to be TRUE.  Previous versions defaulted to have check.keys=TRUE, but some users complained that this made it too easy to find alpha without realizing that some items had been reversed (even though a warning was issued!).  Thus, I have set the default to be check.keys=FALSE with a warning that some items need to be reversed (if this is the case).  To suppress these warnings, set warnings=FALSE.  
</p>
<p>Scores are based upon the simple averages (or totals) of the items scored. Thus, if some items are missing, the scores reflect just the items answered.  This is particularly problematic if using total scores (with the cumulative=TRUE option).  To impute missing data using either means or medians, use the <code><a href="#topic+scoreItems">scoreItems</a></code> function.   Reversed items are subtracted from the maximum + minimum item response for all the items.
</p>
<p>When using raw data, standard errors for the raw alpha are calculated using equation 2 and 3 from Duhhachek and Iacobucci (2004).  This is problematic because some simulations suggest these values are too small.  It is probably better to use bootstrapped values.
</p>
<p><code><a href="#topic+alpha.ci">alpha.ci</a></code> finds confidence intervals using  the Feldt et al. (1987) procedure.  This procedure does not consider the internal structure of the test the way that the Duhachek and Iacobucci (2004) procedure does.  That is, the latter considers the variance of the covariances, while the Feldt procedure is based upon just the mean covariance.  In March, 2022, alpha.ci was finally fixed to follow the Feldt procedure. The confidence intervals reported by alpha use both the Feld and the Duhaceck and Iabocucci precedures.  Note that these match for large values of N, but differ for smaller values.  
</p>
<p>Because both of these procedures use normal theory, if you really care about confidence intervals, using the boot option (n.iter &gt; 1) is recommended.
</p>
<p>Bootstrapped resamples are found if n.iter &gt; 1.  These are returned as the boot object.  They may be plotted or described.  The 2.5% and 97.5% values are returned in the boot.ci object.
</p>


<h3>Value</h3>

<table>
<tr><td><code>total</code></td>
<td>
<p>a list containing</p>
</td></tr>
<tr><td><code>raw_alpha</code></td>
<td>
<p>alpha based upon the covariances</p>
</td></tr>
<tr><td><code>std.alpha</code></td>
<td>
<p>The standarized alpha based upon the correlations</p>
</td></tr>
<tr><td><code>G6(smc)</code></td>
<td>
<p>Guttman's Lambda 6 reliability</p>
</td></tr>
<tr><td><code>average_r</code></td>
<td>
<p>The average interitem correlation</p>
</td></tr>
<tr><td><code>median_r</code></td>
<td>
<p>The median interitem correlation</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>For data matrices, the mean of the scale formed by averaging or summing the items (depending upon the cumulative option)</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>For data matrices, the standard deviation of the total score</p>
</td></tr>
<tr><td><code>alpha.drop</code></td>
<td>
<p>A data frame with all of the above for the case of each item being removed one by one.</p>
</td></tr>
<tr><td><code>item.stats</code></td>
<td>
<p>A data frame including</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of complete cases for the item</p>
</td></tr>
<tr><td><code>raw.r</code></td>
<td>
<p>The correlation of each item with the total score, not corrected for item overlap.</p>
</td></tr>
<tr><td><code>std.r</code></td>
<td>
<p>The correlation of each item with the total score (not corrected for item overlap) if the items were all standardized</p>
</td></tr>
<tr><td><code>r.cor</code></td>
<td>
<p>Item whole correlation corrected for item overlap and scale reliability</p>
</td></tr>
<tr><td><code>r.drop</code></td>
<td>
<p>Item whole correlation for this item against the scale without this item</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>for data matrices, the mean of each item</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>For data matrices, the standard deviation of each item</p>
</td></tr>
<tr><td><code>response.freq</code></td>
<td>
<p>For data matrices, the frequency of each item response (if less than 20)  May be suppressed by specifying discretet=FALSE.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>Scores are by default simply the average response for all items that a participant took. If cumulative=TRUE, then these are sum scores. Note, this is dangerous if there are lots of missing values.</p>
</td></tr>  
<tr><td><code>boot.ci</code></td>
<td>
<p>The lower, median, and upper ranges of the 95% confidence interval based upon the bootstrap.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>a 6 column by n.iter matrix of boot strapped resampled values</p>
</td></tr>
<tr><td><code>Unidim</code></td>
<td>
<p>An index of unidimensionality</p>
</td></tr>
<tr><td><code>Fit</code></td>
<td>
<p>The fit of the off diagonal matrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p>By default, items that correlate negatively with the overall scale will be reverse coded.  This option may be turned off by setting check.keys = FALSE.  If items are reversed, then each item is subtracted from the  minimum item response + maximum item response where  min and max are taken over all items.  Thus, if the items intentionally differ in range, the scores will be off by a constant.  See <code><a href="#topic+scoreItems">scoreItems</a></code> for a solution. 
</p>
<p>Two item level statistics are worth comparing: the mean interitem r and the median interitem r.  If these differ very much, that is a sign that the scale is not particularly homogeneous.  
</p>
<p>Variables without variance do not contribute to reliability but do contribute to total score. They are dropped with a warning that they had no variance and were thus dropped.  However the scores found still include these values in the calculations.
</p>
<p>If the data have been preprocessed by the dplyr package, a strange error can occur.  alpha expects either data.frames or matrix input. data.frames returned by dplyr have had three extra classes added to them which causes alpha to break.  The solution is merely to change the class of the input to &quot;data.frame&quot;.  
</p>
<p>Two experimental measures of Goodness of Fit are returned in the output: Unidim and Fit. They are not printed or displayed, but are available for analysis. The first is an index of how well the modeled average correlations actually reproduce the original correlation matrix.  The second is how well the modeled correlations reproduce the off diagonal elements of the matrix. Both are indices of squared residuals compared to the squared original correlations.  These two measures are under development and might well be modified or dropped in subsequent versions.</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Cronbach, L.J. (1951) Coefficient alpha and the internal strucuture of tests.  Psychometrika, 16, 297-334.
</p>
<p>Cureton, E. (1966). Corrected item-test correlations. Psychometrika, 31(1):93-96.
</p>
<p>Cronbach, L.J. and Gleser G.C.  (1964)The signal/noise ratio in the comparison of reliability coefficients. Educational and Psychological Measurement, 24 (3) 467-480. 
</p>
<p>Duhachek, A. and Iacobucci, D. (2004). Alpha's standard error (ase): An accurate and precise confidence interval estimate. Journal of Applied Psychology, 89(5):792-808.
</p>
<p>Feldt, L. S., Woodruff, D. J., &amp; Salih, F. A. (1987). Statistical inference for coefficient alpha. Applied Psychological Measurement (11) 93-103.
</p>
<p>Guttman, L. (1945). A basis for analyzing test-retest reliability. Psychometrika, 10 (4), 255-282. 
</p>
<p>Revelle, W.  (in preparation) An introduction to psychometric theory with applications in R. Springer.  (Available online at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>). 
</p>
<p>Revelle, W. Hierarchical Cluster Analysis and the Internal Structure of Tests. Multivariate Behavioral Research, 1979, 14, 57-74.
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.  <a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Condon, D.M. (2018) Reliability.  In Irwing, P., Booth, T. and Hughes, D. (Eds). the Wiley-Blackwell Handbook of Psychometric Testing: A multidisciplinary reference on survey, scale, and test development.
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma. Psychometrika, 74 (1) 1145-154. 
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+guttman">guttman</a></code>, <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42) #keep the same starting values
#four congeneric measures
r4 &lt;- sim.congeneric()
alpha(r4)
#nine hierarchical measures -- should actually use omega
r9 &lt;- sim.hierarchical()
alpha(r9)

# examples of two independent factors that produce reasonable alphas
#this is a case where alpha is a poor indicator of unidimensionality

two.f &lt;- sim.item(8)
#specify which items to reverse key by name
 alpha(two.f,keys=c("V3","V4","V5","V6"))
 cov.two &lt;- cov(two.f)
 alpha(cov.two,check.keys=TRUE)
 #automatic reversal base upon first component
alpha(two.f,check.keys=TRUE)    #note that the median is much less than the average R
#this suggests (correctly) that the 1 factor model is probably wrong 
#an example with discrete item responses  -- show the frequencies
items &lt;- sim.congeneric(N=500,short=FALSE,low=-2,high=2,
        categorical=TRUE) #500 responses to 4 discrete items with 5 categories
a4 &lt;- alpha(items$observed)  #item response analysis of congeneric measures
a4
#summary just gives Alpha
summary(a4)

alpha2r(alpha = .74,n.var=4)

#because alpha.ci returns an invisible object, you need to print it
print(alpha.ci(.74, 100,p.val=.05,n.var=4))
</code></pre>

<hr>
<h2 id='anova.psych'>Model comparison for regression, mediation, and factor analysis</h2><span id='topic+anova.psych'></span>

<h3>Description</h3>

<p>When doing regressions from the data or from a correlation matrix using <code><a href="#topic+setCor">setCor</a></code> or doing a mediation analysis using <code>link{mediate}</code>, it is useful to compare alternative models.  Since these are both regression models, the appropriate test is an Analysis of Variance.  Similar tests, using Chi Square may be done for factor analytic models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
anova(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.psych_+3A_object">object</code></td>
<td>
<p>An object from <code><a href="#topic+setCor">setCor</a></code>,  <code><a href="#topic+mediate">mediate</a></code>, <code><a href="#topic+omega">omega</a></code>, or <code><a href="#topic+fa">fa</a></code>.
</p>
</td></tr>
<tr><td><code id="anova.psych_+3A_...">...</code></td>
<td>
<p>More objects of the same type may be supplied here</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+setCor">setCor</a></code> returns the SE.residual and degrees of freedom.  These are converted to SSR and then an analysis of variance is used to compare two (or more) models. For <code><a href="#topic+omega">omega</a></code> or <code><a href="#topic+fa">fa</a></code> the change in the ML chisquare statistic as a function of change in df is reported.
</p>


<h3>Value</h3>

<p>An ANOVA table comparing the models.</p>


<h3>Note</h3>

<p>The code has been adapted from the anova.lm function in stats and the anova.sem by John Fox.
</p>


<h3>Author(s)</h3>

<p>Wiliam Revelle
</p>


<h3>See Also</h3>

<p><code><a href="#topic+setCor">setCor</a></code>, <code><a href="#topic+mediate">mediate</a></code>, <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+fa">fa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("psychTools")) {
m1 &lt;- lmCor(reaction ~ import, data = Tal_Or,std=FALSE)
m2 &lt;- lmCor(reaction ~ import+pmi, data = Tal_Or,std=FALSE)
m3 &lt;- lmCor(reaction ~ import+pmi + cond, data = Tal_Or,std=FALSE)
anova(m1,m2,m3)
}


#Several interesting test cases are taken from analyses of the Spengler data set
#Although the sample sizes are actually very large in the first wave,  I use the
#sample sizes from the last wave 
#This data set is actually in psychTools but is copied here until we can update psychTools
#We set the n.iter to be 50 instead of the default value of 5,000
if(require("psychTools")) {

 mod1 &lt;- mediate(Income.50 ~ IQ + Parental+ (Ed.11) ,data=Spengler,
    n.obs = 1952, n.iter=50)
 mod2 &lt;- mediate(Income.50 ~ IQ + Parental+ (Ed.11)  + (Income.11)
  ,data=Spengler,n.obs = 1952, n.iter=50)

#Now, compare these models
anova(mod1,mod2)
}

f3 &lt;- fa(Thurstone,3,n.obs=213)  #we need to specifiy the n.obs for the test to work
f2 &lt;- fa(Thurstone,2, n.obs=213)
anova(f2,f3)
</code></pre>

<hr>
<h2 id='AUC'>Decision Theory measures of specificity, sensitivity, and d prime</h2><span id='topic+AUC'></span><span id='topic+auc'></span><span id='topic+Specificity'></span><span id='topic+Sensitivity'></span>

<h3>Description</h3>

<p>In many fields, decisions and outcomes are categorical even though the underlying phenomenon are probably continuous.  E.g. students are accepted to graduate school or not, they finish or not. X-Rays are diagnosed as patients having cancer or not.   Outcomes of such decisions are usually labeled as Valid Positives, Valid Negatives, False Positives and False Negatives. In hypothesis testing, False Positives are known as Type I errors, while False Negatives are Type II errors.  The relationship between these four cells depends upon the correlation between the decision rule and the outcome as well as the level of evidence needed for a decision (the criterion).  Signal Detection Theory and Decision Theory have a number of related measures of performance (accuracy = VP + VN), Sensitivity (VP/(VP + FN)), Specificity (1 - FP), d prime (d'), and the area under the Response Operating Characteristic Curve (AUC). More generally, these are examples of correlations based upon dichotomous data.  <code><a href="#topic+AUC">AUC</a></code> addresses some of these questions.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AUC(t=NULL,BR=NULL,SR=NULL,Phi=NULL,VP=NULL,labels=NULL,plot="b",zero=TRUE,correct=.5, 
     col=c("blue","red"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AUC_+3A_t">t</code></td>
<td>
<p>a 4 x 1 vector or a 2 x2 table of TP, FP, FN, TN values (see below) May be 
counts or proportions.</p>
</td></tr> 
<tr><td><code id="AUC_+3A_br">BR</code></td>
<td>
<p>Base Rate of successful outcomes or actual symptom (if t is not specified)</p>
</td></tr> 
<tr><td><code id="AUC_+3A_sr">SR</code></td>
<td>
<p>Selection Rate for  candidates or diagnoses (if t is not specified)</p>
</td></tr>
<tr><td><code id="AUC_+3A_phi">Phi</code></td>
<td>
<p>The Phi correlation coefficient between the predictor and the outcome variable
(if t is not specified)</p>
</td></tr>
<tr><td><code id="AUC_+3A_vp">VP</code></td>
<td>
<p>The number of Valid Positives (selected applicants who succeed; correct 
diagnoses).(if t and Phi are not specified)</p>
</td></tr>
<tr><td><code id="AUC_+3A_labels">labels</code></td>
<td>
<p>Names of variables  1 and 2</p>
</td></tr>
<tr><td><code id="AUC_+3A_plot">plot</code></td>
<td>
<p>&quot;b&quot; (both), &quot;d&quot; (decision theory), &quot;a&quot; (auc), or &quot;n&quot; neither</p>
</td></tr>
<tr><td><code id="AUC_+3A_zero">zero</code></td>
<td>
<p>If True, then the noise distribution is centered at zero</p>
</td></tr>
<tr><td><code id="AUC_+3A_correct">correct</code></td>
<td>
<p>Cell values of 0 are replaced with correct. (See <code><a href="#topic+tetrachoric">tetrachoric</a></code> for a discussion of why this is needed.)</p>
</td></tr>
<tr><td><code id="AUC_+3A_col">col</code></td>
<td>
<p>The color choice for the VP and FP, defaults to =c(&quot;blue&quot;,&quot;red&quot;) but could be c(&quot;grey&quot;,&quot;black&quot;) if we want to avoid colors </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The problem of making binary decisions about the state of the world is ubiquitous.  We see this in Null Hypothesis Significance Testing (NHST), medical diagnoses, and selection for occupations.  Variously known as NHST, Signal Detection Theory, clinical Assessment, or college admissions, all of these domains share the same two x two decision task.
</p>
<p>Although the underlying phenomena are probably continuous, a typical decision or  diagnostic situation makes dichotomous decisions: Accept or Reject, correctly identified, incorrectly identified.   In Signal Detection Theory, the world has two states: Noise versus Signal + Noise.  The decision is whether there is  a signal or not.  
</p>
<p>In diagnoses, it is whether to diagnose an illness or not given some noisy signal (e.g., an X-Ray, a set of diagnostic tests).
</p>
<p>In college admissions, we accept some students and reject others.  Four-Five years later we observe who &quot;succeeds&quot; or graduates. 
</p>
<p>All of these decisions lead to four cells based upon a two x two categorization.  Given the true state of the world is Positive or Negative, and a rater assigns positive or negative ratings, then the resulting two by two table has True (Valid) Positives and True (Valid) Negatives on the diagonal and False Positives and False Negatives off the diagonal. 
</p>
<p>When expressed as percentages of the total, then  Base Rates (BR) depend upon the state of the world, but Selection Ratios (SR) are  under the control of the person making the decision and affect the number of False Positives and the number of Valid Positives.
</p>
<p>Given a two x two table of counts or percentages <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;">       </td><td style="text-align: left;"> Decide +  </td><td style="text-align: left;"> Decide - </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> True + </td><td style="text-align: left;"> Valid Positive  </td><td style="text-align: left;"> False Negative  </td><td style="text-align: left;"> Base Rate % </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> True - </td><td style="text-align: left;"> False Positive </td><td style="text-align: left;"> Valid Negative   </td><td style="text-align: left;"> 1- Base Rate </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td><td style="text-align: left;">   Selection ratio </td><td style="text-align: left;">  1 - Selection ratio  </td><td style="text-align: left;">  (Total N)
</td>
</tr>

</table>

<p>Unfortunately, although this way of categorizing the data is typical in assessment (e.g., Wiggins 1973), and everything is expressed as percentages of the total, in some decision papers, VP are expressed as the ratio of VP to total positive decisions (e.g., Wickens, 1984).  This requires dividing through by the column totals (and represented as VP* and FP* in the table below).
</p>
<p>The relationships implied by these data can be summarized as a <code><a href="#topic+phi">phi</a></code> or <code><a href="#topic+tetrachoric">tetrachoric</a></code>  correlation between the raters and the world, or as a decision process with several alternative measures.  If we make the assumption that the two dimensions are continuous and were artificially dichotomised, then the <code><a href="#topic+tetrachoric">tetrachoric</a></code> correlation is an estimate of the continuous correlation between these two latent dimensions.  If we think of the data as truly representing two states e.g., vaccinated or not vaccinanated, dead or alive, then the <code><a href="#topic+phi">phi</a></code> coefficient is more appropriate.  
</p>
<p>Sensitivity, Specificity, Accuracy, Area Under the Curve, and d' (d prime). These measures  may be defined as <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> Measure    </td><td style="text-align: left;"> Definition   </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> Sensitivity </td><td style="text-align: left;"> VP/(VP+ FN) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> Specificity</td><td style="text-align: left;"> VN/(FP + VN)  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> Accuracy </td><td style="text-align: left;">  VP + VN  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> VP* </td><td style="text-align: left;"> VP/(VP + FP) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> FP* </td><td style="text-align: left;"> (FP/(VP + FP </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> d'  </td><td style="text-align: left;"> z(VP*) - z(FP*) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> d' </td><td style="text-align: left;"> sqrt(2) z(AUC) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> beta </td><td style="text-align: left;"> prob(X/S)/(prob(X/N)) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Although only one point is found, we can form a graphical display of VP versus FP as a smooth curve as a function of the decision criterion. The smooth curve assumes normality whereas the other merely are the two line segments between the points (0,0), (FP,VP), (1,1). The resulting correlation between the inferred continuous state of the world and the dichotomous decision process is a biserial correlation.
</p>
<p>When using table input, the values can be counts and thus greater than 1 or merely probabilities which should add up to 1. Base Rates and Selection Ratios are proportions and thus less than 1. </p>


<h3>Value</h3>

<table>
<tr><td><code>phi</code></td>
<td>
<p>Phi coefficient of the two by two table</p>
</td></tr>
<tr><td><code>tetra</code></td>
<td>
<p>Tetrachoric (latent) coefficient inferred from the two by two table</p>
</td></tr>
<tr><td><code>r.bis</code></td>
<td>
<p>Biserial correlation of continuous state of world with decision</p>
</td></tr>
<tr><td><code>observed</code></td>
<td>
<p>The observed input (as a check)</p>
</td></tr>
<tr><td><code>probabilities</code></td>
<td>
<p>Observed values/ total number of observations</p>
</td></tr>
<tr><td><code>conditional</code></td>
<td>
<p>prob / rowSums(prob)</p>
</td></tr>
<tr><td><code>Accuracy</code></td>
<td>
<p>percentage of True Positives + True Negatives</p>
</td></tr>
<tr><td><code>Sensitivity</code></td>
<td>
<p>VP/(VP + FN)</p>
</td></tr>
<tr><td><code>Specificity</code></td>
<td>
<p>VN/(FP + VN)</p>
</td></tr>
<tr><td><code>d.prime</code></td>
<td>
<p>difference of True Positives versus True Negatives</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>ratio of ordinates at the decision point</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Metz, C.E. (1978) Basic principles of ROC analysis. Seminars in Nuclear Medicine, 8, 283-298.
</p>
<p>Wiggins, Jerry S. (1973) Personality and Prediction: Principles of Personality Assessment. Addison-Wesley.
</p>
<p>Wickens, Christopher D. (1984) Engineering Psychology and Human Performance.  Merrill.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+phi">phi</a></code>, <code><a href="#topic+phi2tetra">phi2tetra</a></code> ,<code><a href="#topic+Yule">Yule</a></code>, <code><a href="#topic+Yule.inv">Yule.inv</a></code> <code><a href="#topic+Yule2phi">Yule2phi</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code> and <code><a href="#topic+polychoric">polychoric</a></code>, <code><a href="#topic+comorbidity">comorbidity</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>AUC(c(30,20,20,30))  #specify the table input
AUC(c(140,60,100,900)) #Metz example with colors
AUC(c(140,60,100,900),col=c("grey","black"))  #Metz example 1 no colors
AUC(c(80,120,40, 960)) #Metz example 2  Note how the accuracies are the same but d's differ
AUC(c(49,40,79,336)) #Wiggins p 249
AUC(BR=.05,SR=.254,Phi = .317) #Wiggins 251 extreme Base Rates
</code></pre>

<hr>
<h2 id='bassAckward'>The Bass-Ackward factoring algorithm discussed by Goldberg
</h2><span id='topic+bassAckward'></span><span id='topic+bassAckward.diagram'></span>

<h3>Description</h3>

<p>Goldberg (2006) described a hierarchical factor structure organization from the &ldquo;top down&quot;.  The original idea was to do successive factor analyses from 1 to nf factors organized by factor score correlations from  one level to the next.  Waller (2007) discussed a simple way of doing this for components without finding the scores.  Using the factor correlations (from Gorsuch) to organize factors hierarchically results may be organized at many different levels. The algorithm may be applied to principal components (pca) or to true factor analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bassAckward(r, nfactors = 1, fm = "minres", rotate = "oblimin", scores = "tenBerge",
   adjust=TRUE, plot=TRUE,cut=.3, use = "pairwise", cor = "cor", weight = NULL,
   correct = 0.5,...)
bassAckward.diagram(x,digits=2,cut = .3,labels=NULL,marg=c(1.5,.5,1.0,.5),
   main="BassAckward",items=TRUE,sort=TRUE,lr=TRUE,curves=FALSE,organize=TRUE,
   values=FALSE,...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bassAckward_+3A_r">r</code></td>
<td>
<p>A correlation matrix or a data matrix suitable for factoring</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_nfactors">nfactors</code></td>
<td>
<p>Factors from 1 to nfactors will be extracted. If nfactors is a a vector, then just the number of factors specified in the vector will be extracted. (See examples).
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_fm">fm</code></td>
<td>
<p>Factor method.  The default is 'minres' factoring.  Although to be consistent with the original Goldberg article, we can also do principal components (fm =&quot;pca&quot;).
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_rotate">rotate</code></td>
<td>
<p>What type of rotation to apply.  The default for factors is oblimin.  Unlike the normal call to pca where the default is varimax, in bassAckward the default for pca is oblimin.</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_scores">scores</code></td>
<td>
<p>What factor scoring algorithm should be used. The default is &quot;tenBerge&quot;, other possibilities include &quot;regression&quot;, or &quot;bartlett&quot;</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_adjust">adjust</code></td>
<td>
<p>If using any other scoring proceure that &quot;tenBerge&quot; should we adjust the correlations for the lack of factor score fit?</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_plot">plot</code></td>
<td>
<p>By default draw a bassAckward diagram</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_use">use</code></td>
<td>
<p>How to treat missing data.  Use='pairwise&quot; finds pairwise complete correlations.
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_cor">cor</code></td>
<td>
<p>What kind of correlation to find.  The default is Pearson.
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_weight">weight</code></td>
<td>
<p>Should cases be weighted?  Default, no.
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_correct">correct</code></td>
<td>
<p>If finding tetrachoric or polychoric correlations, what correction should be applied to empty cells (defaults to .5)
</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_x">x</code></td>
<td>
<p>The object returned by bassAckward</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_digits">digits</code></td>
<td>
<p>Number of digits to display on each path</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_cut">cut</code></td>
<td>
<p>Values greater than the abs(cut) will be displayed in a path diagram.</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_labels">labels</code></td>
<td>
<p>Labels may be taken from the output of the bassAckward function or can be specified as a list.</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_marg">marg</code></td>
<td>
<p>Margins are set to be slightly bigger than normal to allow for a cleaner diagram</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_main">main</code></td>
<td>
<p>The main title for the figure</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_items">items</code></td>
<td>
<p>if TRUE, show the items associated with the factors</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_sort">sort</code></td>
<td>
<p>if TRUE, sort the items by factor loadings</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_lr">lr</code></td>
<td>
<p>Should the graphic be drawn left to right or top to bottom</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_curves">curves</code></td>
<td>
<p>Should we show the correlations between factors at the same level</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_organize">organize</code></td>
<td>
<p>Rename and sort the factors at two lowest levels for a more pleasing figure</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_values">values</code></td>
<td>
<p>If TRUE, then show the percent variance accounted for by this factor.</p>
</td></tr>
<tr><td><code id="bassAckward_+3A_...">...</code></td>
<td>
<p>Other graphic parameters (e.g., cex)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is essentially a wrapper to the <code><a href="#topic+fa">fa</a></code> and <code><a href="#topic+pca">pca</a></code> combined with the <code><a href="#topic+faCor">faCor</a></code> functions.  They are called repeatedly and then the weights from the resulting solutions are used to find the factor/component correlations. 
</p>
<p>Although the default is do all factor solutions from 1 to the nfactors, this can be simplified by specifying just some of the factor solutions. Thus, for the 135 items of the spi, it is more reasonable to ask for 3,5, and 27 item solutions. 
</p>
<p>The function <code><a href="#topic+bassAckward.diagram">bassAckward.diagram</a></code> may be called using the <code><a href="#topic+diagram">diagram</a></code> function or may be called directly.
</p>
<p>The output from <code><a href="#topic+bassAckward.diagram">bassAckward.diagram</a></code> is the sorted factor structure suitable for using <code><a href="#topic+fa.lookup">fa.lookup</a></code>.
</p>
<p>Although not particularly pretty, it is possible to do Schmid-Leiman rotations at each level.  Specify the rotation as rotate=&quot;schmid&quot;.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Call</code></td>
<td>
<p>Echo the call</p>
</td></tr>
<tr><td><code>fm</code></td>
<td>
<p>Echos the factor method used</p>
</td></tr>
<tr><td><code>fa</code></td>
<td>
<p>A list of the factor loadings at each level</p>
</td></tr>
<tr><td><code>bass.ack</code></td>
<td>
<p>A list of the factor correlations at each level</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>The factors at each level</p>
</td></tr>
<tr><td><code>sumnames</code></td>
<td>
<p>Summary of the factor names</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>Factor labels including items for each level</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The correlation matrix analyzed</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>The factor correlations at each level</p>
</td></tr>
<tr><td><code>fa</code></td>
<td>
<p>The factor analysis loadings at each level, now includes Phi values</p>
</td></tr>
<tr><td><code>fa.vac</code></td>
<td>
<p>The variance accounted for by each factor at each level</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Goldberg calculated factor/component scores and then correlated these.  Waller suggests just looking at the unrotated components and then examining the correlations when rotating  different numbers of components.  I do not follow the Waller procedure, but rather find successive factors and then  find factor/component correlations following Gorsuch. 
</p>
<p>It is important to note that the BassAckward solution is not a hierarchical solution in the standard meaning. The factors are not factors of factors as is found in a hierarchical model (e.g. <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> or <code><a href="#topic+omega">omega</a></code>, but is merely way of organising solutions with a different number of factors. In each case, unlike <code><a href="#topic+omega">omega</a></code> the factors are of the original variables, not the lower level factors.  Thus, detailed statistics for any level of the hierarchy may be found by doing a factoring with that particular number of factors.  
</p>
<p>To find basic statistics for the multiple factorings, examine the fa object.  For more detail just do the factoring at that level. 
</p>
<p>To see the items associated with the lowest level factors, use <code><a href="#topic+fa.lookup">fa.lookup</a></code>. For the items associated with other levels, use <code><a href="#topic+fa.lookup">fa.lookup</a></code> specifying the level.  (See examples.)
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Goldberg, L.R. (2006) Doing it all Bass-Ackwards: The development of hierarchical factor structures from the top down. Journal of Research in Personality, 40, 4, 347-358.
</p>
<p>Gorsuch, Richard, (1983) Factor Analysis. Lawrence Erlebaum Associates.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Waller, N. (2007), A general method for computing hierarchical component structures by Goldberg's Bass-Ackwards method, Journal of Research in Personality, 41, 4, 745-752,
DOI: 10.1016/j.jrp.2006.08.005
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+pca">pca</a></code>,  <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+iclust">iclust</a></code>  for alternative hierarchical solutions. 
<code>link{fa.lookup}</code> to show the items in the lowest level of the solution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bassAckward(Thurstone,4,main="Thurstone data set")
f.labels &lt;- list(level1=cs(Approach,Avoid),level2=cs(PosAffect,NegAffect,Constraint), 
level3 = cs(Extraversion,Agreeableness,Neuroticism,Conscientiousness,Openness))

ba &lt;- bassAckward(psychTools::bfi[1:25],c(2,3,5),labels=f.labels,
        main="bfi data set from psychTools", values=TRUE)
print(ba,short=FALSE)
#show the items associated with the 5 level solution
fa.lookup(ba,dictionary=psychTools::bfi.dictionary)
#now show the items associated with the 3 level solution
fa.lookup(ba$fa[[2]],dictionary=psychTools::bfi.dictionary)
# compare the 3 factor solution to what get by extracting 3 factors directly
f3 &lt;- fa(psychTools::bfi[1:25],3)
f3$loadings - ba$fa[[2]]$loadings   # these are the same
 #do pca instead of factors  just summarize, don't plot
summary(bassAckward(psychTools::bfi[1:25],c(1,3,5,7),fm="pca",
       main="Components",plot=FALSE))
##not run, but useful example
  
f.labels &lt;- list(level1 = cs(Neg,Pos,Constrain),level2 = cs(Extra,Neuro,Cons,Open,Agree),
level3 = cs(attnseeking,sociability,impulsivity,
   charisma,sensationseek,emotexpr,humor,anxiety,
   emotstab,irritability,wellbeing,industry,order,author,honesty,perfect,easygoing,
   selfcontrol,conservatism,creativity,introspect,art,
   intellect,conform,adaptability,compassion,trust))
 
sp5 &lt;- bassAckward(psychTools::spi[11:145], c(3,5,27),labels=f.labels,
           main="spi data set from psychTools")

</code></pre>

<hr>
<h2 id='Bechtoldt'>Seven data sets showing a bifactor solution.</h2><span id='topic+Bechtoldt.1'></span><span id='topic+Bechtoldt.2'></span><span id='topic+Bechtoldt'></span><span id='topic+Holzinger'></span><span id='topic+Holzinger.9'></span><span id='topic+Reise'></span><span id='topic+Thurstone'></span><span id='topic+Thurstone.33'></span><span id='topic+Thurstone.33G'></span><span id='topic+Thurstone.9'></span>

<h3>Description</h3>

<p>Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Thurstone)
data(Thurstone.33)
data(Thurstone.33G)
data(Thurstone.9)
data(Holzinger)
data(Holzinger.9)
data(Bechtoldt)
data(Bechtoldt.1)
data(Bechtoldt.2)
data(Reise)
</code></pre>


<h3>Details</h3>

<p>Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the <code><a href="#topic+omega">omega</a></code> function or using sem. The bifactor model is typically used in measures of cognitive ability.
</p>
<p>There are several ways to analyze such data.  One is to use the <code><a href="#topic+omega">omega</a></code> function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using <code><a href="#topic+omegaSem">omegaSem</a></code>. Another way is to do a regular factor analysis and use either a <code><a href="#topic+bifactor">bifactor</a></code> or  <code><a href="#topic+biquartimin">biquartimin</a></code> rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The <code><a href="#topic+bifactor">bifactor</a></code> rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. 
</p>
<p>The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.
</p>
<p>Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  &quot;t01_visperc&quot;  &quot;t02_cubes&quot;    &quot;t04_lozenges&quot; &quot;t06_paracomp&quot; &quot;t07_sentcomp&quot; &quot;t09_wordmean&quot; &quot;t10_addition&quot; &quot;t12_countdot&quot; and  &quot;t13_sccaps&quot; and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.
</p>
<p>Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.
</p>
<p>Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 male students reported by Carl Brigham of Princeton to the College Entrance Examination Board. (Brigham, 1932, Table VIII p 352.) This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.  On page 352, we are given reliability estimates of .86, .73, .83, .97,.94, .85, .92, .92, and .90.  These are based upon sampling  743 boys.
</p>
<p>A parallel set (also from Brigham) is the correlation matrix of the same 9 variables for 2899 girls. (Thurstone.33G)
</p>
<p>Tucker (1958) uses 9 variables from Thurstone and Thurstone (1941) for his example of <code><a href="#topic+interbattery">interbattery</a></code> factor analysis.  
</p>
<p>More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon &gt;35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    
</p>
<p>The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). 
</p>
<p>The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.
</p>
<p>Two more data sets with  similar structures are found in the <code><a href="#topic+Harman">Harman</a></code> data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman <code>link{Harman.Holzinger}</code> as well as 8 affective variables from <code>link{burt}</code>. 
</p>
<p>Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package.
</p>

<ul>
<li><p> Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212.
</p>
</li>
<li><p> Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213.
</p>
</li>
<li><p> Holzinger: 14 x 14 correlation matrix of ability tests, N = 355
</p>
</li>
<li><p> Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145
</p>
</li>
<li><p> Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000
</p>
</li>
<li><p> Thurstone: 9 x 9 correlation matrix of ability tests, N = 213
</p>
</li>
<li><p> Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 
</p>
</li>
<li><p> Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
</p>
</li></ul>



<h3>Note</h3>

<p>Note that these are tests, not items. Thus, it was possible to find the reliabilities of each test.
</p>
<p>For the Holzinger 14 tests these were found from 1- t2 where t = c(.332, .517, .360, .382, .354,.249, .444, .393, .455, .424, .393, .487, .534, .382) (page  53) and thus the reliabilities were 0.890, 0.733, 0.870, 0.854, 0.875, 0.938, 0.803, 0.846, 0.793, 0.820, 0.846, 0.763, 0.715, 0.854. 
</p>
<p>For the Holzinger.9 tests, the reliabilities for the Grant-White tests were: .76, .57, .94, .65, .75, .87, .95, .84 and .89 (Keith Widaman, personal communication, 2020),
</p>


<h3>Source</h3>

<p>Holzinger:  Holzinger and Swineford (1937) <br />
Reise: Steve Reise (personal communication) <br />
sem help page (for Thurstone) 
Brigham (for Thurstone.33)
</p>


<h3>References</h3>

 
<p>Bechtoldt, Harold, (1961). An empirical study of the factor analysis stability hypothesis. Psychometrika, 26, 405-432.
</p>
<p>Brigham, Carl C. (1932) A study of errors.  College Entrance Examination Board. 
</p>
<p>Holzinger, Karl and Swineford, Frances (1937) The Bi-factor method.  Psychometrika, 2, 41-54
</p>
<p>Holzinger, K., &amp; Swineford, F. (1939). A study in factor analysis: The stability of a bifactor solution. Supplementary Educational Monograph, no. 48. Chicago: University of Chicago Press.
</p>
<p>McDonald, Roderick P. (1999) Test theory: A unified treatment. L. Erlbaum Associates. Mahwah, N.J.
</p>
<p>Mansolf, Maxwell  and   Reise, Steven P. (2016) Exploratory Bifactor Analysis: The Schmid-Leiman Orthogonalization and Jennrich-Bentler Analytic Rotations, Multivariate Behavioral Research, 51:5, 698-717, DOI: 10.1080/00273171.2016.1215898
</p>
<p>Reise, Steven and Morizot, Julien and Hays, Ron (2007) The role of the bifactor model in resolving dimensionality issues in health outcomes measures. Quality of Life Research. 16, 19-31.
</p>
<p>Thurstone, Louis Leon (1933) The theory of multiple factors. Edwards Brothers, Inc.  Ann Arbor.
</p>
<p>Thurstone, Louis Leon and Thurstone, Thelma (Gwinn). (1941) Factorial studies of intelligence. The University of Chicago Press. Chicago, Il.
</p>
<p>Tucker, Ledyard (1958) An inter-battery method of factor analysis, Psychometrika, 23, 111-136.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(!require(GPArotation)) {message("I am sorry, to run omega requires GPArotation") 
        } else {
#holz &lt;- omega(Holzinger,4, title = "14 ability tests from Holzinger-Swineford")
#bf &lt;- omega(Reise,5,title="16 health items from Reise") 
#omega(Reise,5,labels=colnames(Reise),title="16 health items from Reise")
thur.om &lt;- omega(Thurstone,title="9 variables from Thurstone") #compare with
thur.bf   &lt;- fa(Thurstone,3,rotate="biquartimin")
factor.congruence(thur.om,thur.bf)
}
</code></pre>

<hr>
<h2 id='bestScales'>A bootstrap aggregation function for choosing most predictive unit weighted items</h2><span id='topic+bestItems'></span><span id='topic+bestScales'></span><span id='topic+BISCUIT'></span><span id='topic+biscuit'></span><span id='topic+BISCWIT'></span><span id='topic+biscwit'></span>

<h3>Description</h3>

<p><code><a href="#topic+bestScales">bestScales</a></code> forms scales from the items/scales most correlated with a particular criterion and then cross validates on a hold out sample using unit weighted scales.  This may be repeated n.iter times using either basic bootstrap aggregation (bagging) techniques or K-fold cross validation. Thus, the technique is known as <code><a href="#topic+BISCUIT">BISCUIT</a></code> (Best Items Scales that are Cross validated, Unit weighted, Informative, and Transparent).  Given a dictionary of item content, <code><a href="#topic+bestScales">bestScales</a></code> will sort by criteria correlations and display the item content. Options for bagging (bootstrap aggregation) are included. An alternative to unit weighting is to weight items by their zero order correlations (cross validated) with the criteria. This weighted version is called <code><a href="#topic+BISCWIT">BISCWIT</a></code> and is an optional output. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bestScales(x,criteria,min.item=NULL,max.item=NULL, delta = 0,
           cut=.1, n.item =10, wtd.cut=0, wtd.n=10, 
          n.iter =1, folds=1, p.keyed=.9,
          overlap=FALSE, dictionary=NULL, check=TRUE, impute="none",log.p=FALSE,digits=2)

bestItems(x,criteria=1,cut=.1, n.item=10, abs=TRUE, 
   dictionary=NULL,check=FALSE,digits=2,use="pairwise",method="pearson") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bestScales_+3A_x">x</code></td>
<td>
<p>A data matrix or data frame depending upon the function.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_criteria">criteria</code></td>
<td>
<p>Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. </p>
</td></tr>
<tr><td><code id="bestScales_+3A_min.item">min.item</code></td>
<td>
<p>Find unit weighted and correlation weighted scales from min.item to max.item</p>
</td></tr>
<tr><td><code id="bestScales_+3A_max.item">max.item</code></td>
<td>
<p>These are all summarized in the final.multi.valid object</p>
</td></tr>
<tr><td><code id="bestScales_+3A_delta">delta</code></td>
<td>
<p>Return items where the predicted r + delta * se of r &lt; max value</p>
</td></tr>
<tr><td><code id="bestScales_+3A_cut">cut</code></td>
<td>
<p>Return all values in abs(x[,c1]) &gt; cut.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_wtd.cut">wtd.cut</code></td>
<td>
<p>When finding the weighted scales, use all items with zero 
order correlations &gt; wtd.cut</p>
</td></tr>
<tr><td><code id="bestScales_+3A_wtd.n">wtd.n</code></td>
<td>
<p>When finding the weighted scales, use the  wtd.n items that
are &gt; than wtd.cut</p>
</td></tr>
<tr><td><code id="bestScales_+3A_abs">abs</code></td>
<td>
<p>if TRUE, sort by absolute value in bestItems</p>
</td></tr>
<tr><td><code id="bestScales_+3A_dictionary">dictionary</code></td>
<td>
<p>a data.frame with rownames corresponding to rownames in the f$loadings 
matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple
columns) of item content.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_check">check</code></td>
<td>
<p>if TRUE, delete items with no variance</p>
</td></tr>
<tr><td><code id="bestScales_+3A_n.item">n.item</code></td>
<td>
<p>How many items make up an empirical scale, or (bestItems, show the best
n.items) </p>
</td></tr>
<tr><td><code id="bestScales_+3A_overlap">overlap</code></td>
<td>
<p>Are the correlations with other criteria fair game for bestScales</p>
</td></tr>
<tr><td><code id="bestScales_+3A_impute">impute</code></td>
<td>
<p>When finding the best scales, and thus the correlations with the criteria,  how should we handle missing data?  The default is to drop missing items. (That is to say, to use pairwise complete correlations.)</p>
</td></tr>
<tr><td><code id="bestScales_+3A_n.iter">n.iter</code></td>
<td>
<p>How many times to perform a bootstrap estimate. Replicate the best item function n.iter times, sampling roughly 1-1/e  of the cases each time, and validating on the remaining 1/e of the cases for each iteration.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_folds">folds</code></td>
<td>
<p>If folds &gt; 1, this is k-folds validation.  Note, set  n.iter &gt; 1  to do bootstrap aggregation, or set folds &gt; 1  to do k-folds.   </p>
</td></tr>
<tr><td><code id="bestScales_+3A_p.keyed">p.keyed</code></td>
<td>
<p>The proportion of replications needed to include items in the final best keys.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_log.p">log.p</code></td>
<td>
<p>Select items based upon the log of the probability of the correlations.  This will only have an effect if the number of pairwise cases differs drastically from pair to pair.  </p>
</td></tr>
<tr><td><code id="bestScales_+3A_digits">digits</code></td>
<td>
<p>round to digits when showing output.</p>
</td></tr>
<tr><td><code id="bestScales_+3A_use">use</code></td>
<td>
<p>How to handle missing data.  Defaults to &quot;pairwise&quot;</p>
</td></tr>
<tr><td><code id="bestScales_+3A_method">method</code></td>
<td>
<p>Which correlation to find.  Defaults to &quot;pearson&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are a number of procedures that can be used for predicting criteria from a set of predictors.  The generic term for this is &quot;machine learning&quot; or &quot;statistical learning&quot;.  The basic logic of these procedures is to find a set of items that best predict a criteria according to some fit statistic and then cross validate these items numerous times.  &quot;lasso&quot; regression (least absolute shrinkage and selection) is one such example. <code><a href="#topic+bestScales">bestScales</a></code> differs from these procedures by unit weighting items chosen from their zero order correlations with the criteria rather than weighting the partial correlations ala regression.  This is an admittedly simple procedure that takes into account the well known finding (Wilks, 1938;  Wainer, 1976; Dawes, 1979; Waller, 2008) that although regression weights are optimal for any particular data set, unit weights are almost as good (fungible) and more robust across sample variation.  
</p>
<p>Following some suggestions, we have added the ability to find scales where items are weighted by their zero order correlations with the criteria.   This is effectively a comprimise between unit weighting and regression weights (where the weights are the zero order correlations times the inverse of the correlation matrix). This weighted version may be thought of as <code><a href="#topic+BISCWIT">BISCWIT</a></code> in contrast to the unit weighted version <code><a href="#topic+BISCUIT">BISCUIT</a></code>.
</p>
<p>To be comparable to other ML algorithms, we now consider multiple solutions (for number of items &gt;= min.item to max.item).  The final scale consists of the number  items which maximize  the validity or at least are within delta * standard error of r of the maximum.  
</p>
<p>Thus, <code><a href="#topic+bestScales">bestScales</a></code> will find up to n.items per criterion that have absolute correlations with a criterion greater than cut.  If the overlap option is FALSE (default) the other criteria are not used.  This is an example of &ldquo;dust bowl empiricism&quot; in that there is no latent construct being measured, just those items that most correlate with a set of criteria. The empirically identified items are then formed into scales (ignoring concepts of internal consistency) which are then correlated with the criteria.  
</p>
<p>Clearly, <code><a href="#topic+bestScales">bestScales</a></code> is capitalizing on chance associations.  Thus, we should validate the empirical scales by deriving them on a fraction of the total number of subjects, and cross validating on the remaining subjects. (This is known both as K-fold cross validation and bagging.  Both may be done).  If folds &gt; 1, then a k-fold cross validation is done.  This removes 1/k (a fold) from the sample for the derivation sample and validates on that remaining fold. This is done k-folds times.  Traditional cross validation would thus be a k-fold with k =2.  More modern applications seem to prefer k =10 to have 90% derivation sample and a 10% cross validation sample.
</p>
<p>The alternative, known as 'bagging' is to do a bootstrap sample (which because it is sampling with replacement will typically extract 1- 1/e = 63.2% of the sample) for the derivation sample (the bag) and then validate it on the remaining 1/e of the sample (the out of bag).  This is done n.iter times. This should be repeated multiple times (n.iter &gt; 1, e.g. n.iter=1000) to get stable cross validations.
</p>
<p>One can compare the validity of these two approaches by  trying each.  The average predictability of the n.iter samples are shown as are the average validity of the cross validations.   This can only be done if x is a data matrix/ data.frame, not a correlation matrix.  For very large data sets (e.g., those from SAPA) these scales seem very stable. 
</p>
<p><code><a href="#topic+bestScales">bestScales</a></code> is effectively a straight forward application of 'bagging' (bootstrap aggregation) and machine learning as well as k-fold validation. 
</p>
<p>The criteria can be the colnames of elements of x, or can be a separate data.frame. 
</p>
<p><code><a href="#topic+bestItems">bestItems</a></code> and <code><a href="#topic+lookup">lookup</a></code> are simple helper functions to summarize correlation matrices or factor loading matrices.  <code><a href="#topic+bestItems">bestItems</a></code> will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values &gt; cut.   If there is a dictionary of item content and item names, then include the contents as a two column matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary <code><a href="psychTools.html#topic+bfi.dictionary">bfi.dictionary</a></code>).
</p>
<p>The derived model can be further validated against yet another hold out sample using the <code><a href="#topic+predict.psych">predict.psych</a></code> function if given the best scale object and the new data set. 
</p>


<h3>Value</h3>

<p><code><a href="#topic+bestScales">bestScales</a></code> returns the correlation of the empirically constructed scale with each criteria and the items used in the scale.  If a dictionary is specified, it also returns a list (value) that shows the item content. Also returns the keys.list so that scales can be found using <code><a href="#topic+cluster.cor">cluster.cor</a></code> or <code><a href="#topic+scoreItems">scoreItems</a></code>.  If using replications (bagging or kfold) then it also returns the best.keys, a list suitable for scoring.
</p>
<p>There are actually four keys lists reported.
</p>
<p>best.keys are all the items used to form unit weighted scales with the restriction of n.item.  
</p>
<p>weights may be used in the <code><a href="#topic+scoreWtd">scoreWtd</a></code> function to find scales based upon the raw correlation weights.
</p>
<p>If the min.item and max.item options are used, then two more sets of weights are provided.
</p>
<p>optimal.keys are a subset of the best.keys, taking just those items that increase the cross validation values up to the delta * se of the maximum.  This is a slightly more parsimonious set.
</p>
<p>optimal.weights is analogous to optimal keys, but for supplies weights for just those items that are used to predict cross validation values up to delta * se of the maximum.
</p>
<p>The best.keys object is a list of items (with keying information) that may be used in subsequent analyses.  These &ldquo;best.keys&quot; are formed into scale scores for the &ldquo;final.valid&quot; object which reports how well the best.keys work on the entire sample.  This is, of course, not cross validated.  Further cross validation can be done using the <code><a href="#topic+predict.psych">predict.psych</a></code> function.
</p>
<table>
<tr><td><code>scores</code></td>
<td>
<p> Are the unit weighted scores from the original items</p>
</td></tr>
<tr><td><code>best.keys</code></td>
<td>
<p> A key list of those items that were used in the unit weighting.</p>
</td></tr>
<tr><td><code>wtd.scores</code></td>
<td>
<p>Are the zero-order correlation based scores.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p> the scoring weights used</p>
</td></tr>
<tr><td><code>final.multi.valid</code></td>
<td>
<p>An object with the unit weighted and correlation weighted correlations from low.step to high.step</p>
</td></tr>
</table>
<p>The print and summary output list a number of summary statistics for each criteria.  This is given for the default case (number of items fixed) and then if requested, the optimal values chosen from min.item to max.item:
</p>
<p>The default statistics:
</p>

<dl>
<dt>derivation mean</dt><dd><p>Mean correlation of fixed length scale with the criteria, derivation sample</p>
</dd>
<dt>derivation.sd</dt><dd><p>The standard deviation of these estimates</p>
</dd>
<dt>validation.m</dt><dd><p>The mean cross validated correlations with the criteria</p>
</dd>
<dt>validation.sd</dt><dd><p>The standard deviations of these estimates</p>
</dd>
<dt>final.valid</dt><dd><p>The correlation of the pooled models with all the subjects</p>
</dd>
<dt>final.wtd</dt><dd><p>The correlation of the pooled weighted model with all subjects</p>
</dd>
<dt>N.wtd</dt><dd><p>Number of items used in the final weighted model</p>
</dd>
</dl>

<p>The optimal number of items statistics:
</p>

<dl>
<dt>n</dt><dd><p>The mean number of items meeting the criteria</p>
</dd>
<dt>unit</dt><dd><p>The mean derivation predictive valididy</p>
</dd>
<dt>n.wtd</dt><dd><p>the mean number of items used in the wtd scales</p>
</dd>
<dt>wtd</dt><dd><p>The mean derivation wtd correlaton</p>
</dd>
<dt>valid.n</dt><dd><p>the mean number of items in the cross validation sample</p>
</dd>
<dt>valid.unit</dt><dd><p>The mean cross validated unit weighted correlations</p>
</dd>
<dt>valid.wtd.n</dt><dd><p>The mean number of items used in the cross validated correlated weighed scale</p>
</dd>
<dt>valid.wtd</dt><dd><p>The mean cross validated weighted correlation with criteria</p>
</dd>
<dt>n.final</dt><dd><p>The optimal number of items on the final cross validation sample</p>
</dd>
<dt>n.wtd.final</dt><dd><p>The optimal number of weighted items on the final cross validation.</p>
</dd>
<dt>derviation.mean</dt><dd></dd>
</dl>

<p><code><a href="#topic+bestItems">bestItems</a></code> returns a sorted list of factor loadings or correlations with the labels as provided in the dictionary. If given raw data, then the correlations with the criteria variables are found first according to &quot;use&quot; and &quot;method&quot;.
</p>
<p>The stats object can be used to create <code><a href="#topic+error.dots">error.dots</a></code> plots to show the mean estimate and the standard error of the estimates.   See  the examples.
</p>
<p>The resulting best scales can be cross validated on a different hold out sample using <code><a href="#topic+crossValidation">crossValidation</a></code>.  See the last example. 
</p>


<h3>Note</h3>

<p>Although <code><a href="#topic+bestScales">bestScales</a></code> was designed to form the best unit weighted scales, for large data sets, there seems to be some additional information in weighting by the average zero-order correlation. 
</p>
<p>To create a dictionary, create an object with row names as the item numbers, and the columns as the item content.  See the <code>link{bfi.dictionary}</code> as an example.
</p>
<p>This is a very computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package. The number of cores to use when doing bestScales may be specified using the options command. The greatest step in speed is going from 1 core to 2. This is about a 50
</p>


<h3>Note</h3>

<p>Although empirical scale construction is appealing, it has the basic problem of capitalizing on chance.  Thus, be careful of over interpreting the results unless working with large samples.  Iteration and bootstrapping aggregation (bagging) gives information on the stability of the solutions.
</p>
<p>It is also important to realize that the variables that are most related to the criterion might be because of other, trivial reasons (e.g. height predicts gender)
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Dawes, R.M. (1979) The robust beauty of improper linear models in decision making, American Psychologist, 34, 571-582.
</p>
<p>Elleman, L. G., McDougald, S. K., Condon, D. M., &amp; Revelle, W. 2020 (in press). That takes the BISCUIT: A comparative study of predictive accuracy and parsimony of four statistical learning techniques in personality data, with data missingness conditions.  European Journal of Psychological Assessment. (Preprint available at https://psyarxiv.com/tuqap/)
</p>
<p>Revelle, W.  (in preparation) An introduction to psychometric theory with applications in R. Springer.  (Available online at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>). 
</p>
<p>Wainer, H. (1979) Estimating coefficients in linear models: It don't make no nevermind. Psychological Buletin, 83, 213-217.
</p>
<p>Waller, N.G. (2008), Fungible weights in multiple regression.  Psychometrica, 73, 691-703. 
</p>
<p>Wilks, S. S. (1938), Weighting systems for linear functions of correlated variables when there is no dependent variable. Psychometrika. 3. 23-40.	
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+iclust">iclust</a></code>,<code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+error.dots">error.dots</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#This is an example of 'bagging' (bootstrap aggregation)
#not run in order to pass the timing tests for Debian at CRAN
#bestboot &lt;- bestScales(psychTools::bfi,criteria=cs(gender,age,education), 
# n.iter=10,dictionary=psychTools::bfi.dictionary[1:3])
#bestboot
#compare with 10 fold cross validation 
#don't test for purposes of speed in installation to pass the debian CRAN test
tenfold &lt;- bestScales(psychTools::bfi,criteria=cs(gender,age,education),fold=10,
           dictionary= psychTools::bfi.dictionary[1:3])
tenfold

#Then, to display the results graphically
#Note that we scale the two graphs with the same x.lim values

 error.dots(tenfold,eyes=TRUE,pch=16,xlim=c(0,.4))
 # error.dots(bestboot,add=TRUE,xlim=c(0,.4))
#do this again, but this time display the scale fits from 1 to 15 Items
# tenfold &lt;- bestScales(psychTools::bfi,criteria=cs(gender,age,education),fold=10,
# dictionary= psychTools::bfi.dictionary[1:3],min.item=1,max.item=15)
# matplot(tenfold$multi.validities$wtd.deriv,typ="b",
# xlab="Number of Items",main="Fit as a function of number of items")   #the wtd weights
# matpoints(tenfold$multi.validities$unit.deriv,typ="b",lwd=2) #the unit weights 

#an example of using crossValidation with bestScales
set.seed(42)
ss &lt;- sample(1:2800,1400)
model &lt;- bestScales(bfi[ss,],criteria=cs(gender,education,age))
original.fit &lt;- crossValidation(model,bfi[ss,]) #the derivation set
cross.fit &lt;- crossValidation(model,bfi[-ss,])  #the cross validation set
summary(original.fit)
summary(cross.fit)

</code></pre>

<hr>
<h2 id='bfi'>25 Personality items representing 5 factors</h2><span id='topic+bfi'></span><span id='topic+bfi.keys'></span>

<h3>Description</h3>

<p>25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis.  Three additional demographic variables (sex, education, and age) are also included. This data set is deprecated and users are encouraged to use <code><a href="psychTools.html#topic+bfi">bfi</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bfi)


</code></pre>


<h3>Format</h3>

<p>A data frame with 2800 observations on the following 28 variables. (The q numbers are the SAPA item numbers).
</p>

<dl>
<dt><code>A1</code></dt><dd><p>Am indifferent to the feelings of others. (q_146)</p>
</dd>
<dt><code>A2</code></dt><dd><p>Inquire about others' well-being. (q_1162)</p>
</dd>
<dt><code>A3</code></dt><dd><p>Know how to comfort others. (q_1206) </p>
</dd>
<dt><code>A4</code></dt><dd><p>Love children. (q_1364)</p>
</dd>
<dt><code>A5</code></dt><dd><p>Make people feel at ease. (q_1419)</p>
</dd>
<dt><code>C1</code></dt><dd><p>Am exacting in my work. (q_124)</p>
</dd>
<dt><code>C2</code></dt><dd><p>Continue until everything is perfect. (q_530)</p>
</dd>
<dt><code>C3</code></dt><dd><p>Do things according to a plan. (q_619)</p>
</dd>
<dt><code>C4</code></dt><dd><p>Do things in a half-way manner. (q_626)</p>
</dd>
<dt><code>C5</code></dt><dd><p>Waste my time. (q_1949)</p>
</dd>
<dt><code>E1</code></dt><dd><p>Don't talk a lot. (q_712)</p>
</dd>
<dt><code>E2</code></dt><dd><p>Find it difficult to approach others. (q_901)</p>
</dd>
<dt><code>E3</code></dt><dd><p>Know how to captivate people. (q_1205)</p>
</dd>
<dt><code>E4</code></dt><dd><p>Make friends easily. (q_1410)</p>
</dd>
<dt><code>E5</code></dt><dd><p>Take charge. (q_1768)</p>
</dd>
<dt><code>N1</code></dt><dd><p>Get angry easily. (q_952)</p>
</dd>
<dt><code>N2</code></dt><dd><p>Get irritated easily. (q_974)</p>
</dd>
<dt><code>N3</code></dt><dd><p>Have frequent mood swings. (q_1099</p>
</dd>
<dt><code>N4</code></dt><dd><p>Often feel blue. (q_1479)</p>
</dd>
<dt><code>N5</code></dt><dd><p>Panic easily. (q_1505)</p>
</dd>
<dt><code>O1</code></dt><dd><p>Am full of ideas. (q_128)</p>
</dd>
<dt><code>O2</code></dt><dd><p>Avoid difficult reading material.(q_316)</p>
</dd>
<dt><code>O3</code></dt><dd><p>Carry the conversation to a higher level. (q_492)</p>
</dd>
<dt><code>O4</code></dt><dd><p>Spend time reflecting on things. (q_1738)</p>
</dd>
<dt><code>O5</code></dt><dd><p>Will not probe deeply into a subject. (q_1964)</p>
</dd>
<dt><code>gender</code></dt><dd><p>Males = 1, Females =2</p>
</dd>
<dt><code>education</code></dt><dd><p>1 = HS, 2 = finished HS, 3 = some college, 4 = college graduate 5 = graduate degree</p>
</dd>
<dt><code>age</code></dt><dd><p>age in years</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data set is deprecated and users are encouraged to use <code><a href="psychTools.html#topic+bfi">bfi</a></code>.It is kept here backward compatability for one more release.
</p>
<p>The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness.  The scoring key is created using  <code><a href="#topic+make.keys">make.keys</a></code>, the scores are found using  <code><a href="#topic+score.items">score.items</a></code>.
</p>
<p>These five factors are a useful example of using <code><a href="#topic+irt.fa">irt.fa</a></code> to do Item Response Theory based latent factor analysis of the <code><a href="#topic+polychoric">polychoric</a></code> correlation matrix.  The endorsement plots for each item, as well as the item information functions reveal that the items differ in their quality.
</p>
<p>The item data were collected using a 6 point response scale: 
1 Very Inaccurate
2 Moderately Inaccurate
3 Slightly Inaccurate
4 Slightly Accurate
5 Moderately Accurate
6 Very Accurate
</p>
<p>as part of the Synthetic Apeture Personality Assessment (SAPA <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project.  To see an example of the data collection technique, visit <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a> or the International Cognitive Ability Resource at <a href="https://icar-project.org/">https://icar-project.org/</a>.  The items given were sampled from the International Personality Item Pool of Lewis Goldberg using the sampling technique of SAPA.  This is a sample data set taken from the much larger SAPA data bank.
</p>


<h3>Note</h3>

<p>This data set is deprecated and users are encouraged to use <code><a href="psychTools.html#topic+bfi">bfi</a></code>.It is kept here backward compatability for one more release.
</p>
<p>The bfi data set and items should not be confused with the BFI (Big Five Inventory) of Oliver John and colleagues (John, O. P., Donahue, E. M., &amp; Kentle, R. L. (1991). The Big Five Inventory&ndash;Versions 4a and 54. Berkeley, CA: University of California,Berkeley, Institute of Personality and Social Research.)
</p>


<h3>Source</h3>

<p>The items are from the ipip (Goldberg, 1999).  The data are from the SAPA project (Revelle, Wilt and Rosenthal, 2010) , collected Spring, 2010 ( <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>).
</p>


<h3>References</h3>

<p>Goldberg, L.R. (1999) A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In Mervielde, I. and Deary, I. and De Fruyt, F. and Ostendorf, F. (eds) Personality psychology in Europe. 7. Tilburg University Press. Tilburg, The Netherlands.
</p>
<p>Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer.
</p>
<p>Revelle, W,  Condon, D.M.,  Wilt, J.,  French, J.A., Brown, A.,  and  Elleman, L.G. (2016) Web and phone based data collection using planned missing designs. In  Fielding, N.G.,  Lee, R.M. and  Blank, G. (Eds). SAGE Handbook of Online Research Methods (2nd Ed), Sage Publcations. </p>


<h3>See Also</h3>

<p><code><a href="#topic+bi.bars">bi.bars</a></code> to show the data by age and gender, <code><a href="#topic+irt.fa">irt.fa</a></code> for item factor analysis applying the irt model.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bfi)
psych::describe(bfi)
# create the bfi.keys (actually already saved in the data file)
keys &lt;-
  list(agree=c("-A1","A2","A3","A4","A5"),conscientious=c("C1","C2","C3","-C4","-C5"),
extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
openness = c("O1","-O2","O3","O4","-O5")) 

 scores &lt;- psych::scoreItems(keys,bfi,min=1,max=6) #specify the minimum and maximum values
 scores
 #show the use of the fa.lookup with a dictionary
 #psych::keys.lookup(bfi.keys,bfi.dictionary[,1:4])   #deprecated  -- use psychTools
 
</code></pre>

<hr>
<h2 id='bi.bars'>Draw pairs of bargraphs based on two groups</h2><span id='topic+bi.bars'></span>

<h3>Description</h3>

<p>When showing e.g., age or education distributions for two groups, it is convenient to plot them back to back.  bi.bars will do so.</p>


<h3>Usage</h3>

<pre><code class='language-R'>bi.bars(x,var=NULL,grp=NULL,horiz,color,label=NULL,zero=FALSE,xlab,ylab,...) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bi.bars_+3A_x">x</code></td>
<td>
<p>The data frame or matrix from which we specify the data</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_var">var</code></td>
<td>
<p>The variable to plot</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_grp">grp</code></td>
<td>
<p>a grouping variable.</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_horiz">horiz</code></td>
<td>
<p>horizontal (default) or vertical bars</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_color">color</code></td>
<td>
<p>colors for the two groups &ndash; defaults to blue and red</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_label">label</code></td>
<td>
<p>If specified, labels  for the dependent axis </p>
</td></tr> 
<tr><td><code id="bi.bars_+3A_zero">zero</code></td>
<td>
<p>If TRUE, subtract the minimum value to make the numbers range from 0 to max -min.  This is useful if  showing heights</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_xlab">xlab</code></td>
<td>
<p>xaxis label if appropriate</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_ylab">ylab</code></td>
<td>
<p>y axis label otherwise</p>
</td></tr>
<tr><td><code id="bi.bars_+3A_...">...</code></td>
<td>
<p>Further parameters to pass to the graphing program</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A trivial, if useful, function to draw back to back histograms/barplots. One for each group.</p>


<h3>Value</h3>

<p>a graphic</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

  <p><code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+describeBy">describeBy</a></code> and  <code><a href="#topic+statsBy">statsBy</a></code> for descriptive statistics and <code><a href="#topic+error.bars">error.bars</a></code> <code><a href="#topic+error.bars.by">error.bars.by</a></code> and  <code><a href="#topic+densityBy">densityBy</a></code>  <code><a href="#topic+violinBy">violinBy</a></code> for graphic displays </p>


<h3>Examples</h3>

<pre><code class='language-R'>#data(bfi)
bi.bars(psychTools::bfi,"age","gender" ,ylab="Age",main="Age by males and females")
 bi.bars(psychTools::bfi,"education","gender",xlab="Education",
     main="Education by gender",horiz=FALSE)
</code></pre>

<hr>
<h2 id='bigCor'>Find large correlation matrices by stitching together smaller ones found more rapidly</h2><span id='topic+bigCor'></span>

<h3>Description</h3>

<p>When analyzing many subjects (ie. 100,000 or more) with many variables (i.e. 1000 or more) core R can take a long time and sometime exceed  memory limits (i.e. with 600K subjects and 6K variables).  bigCor runs (in parallel if multicores are available) by breaking the variables into subsets (of size=size), finding all subset correlations, and then stitches the resulting matrices into one large matrix.   Noticeable improvements in speed compared to cor.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigCor(x, size = NULL, use = "pairwise",cor="pearson",correct=.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigCor_+3A_x">x</code></td>
<td>
<p>A data set of numeric variables</p>
</td></tr>
<tr><td><code id="bigCor_+3A_size">size</code></td>
<td>
<p>What should the size of the subsets be? Defaults to NCOL (x)/20  
</p>
</td></tr>
<tr><td><code id="bigCor_+3A_use">use</code></td>
<td>
<p>The standard correlation option.  &quot;pairwise&quot; allows for missing data
</p>
</td></tr>
<tr><td><code id="bigCor_+3A_cor">cor</code></td>
<td>
<p>Defaults to Pearson correlations, alteratives are polychoric and spearman  </p>
</td></tr>
<tr><td><code id="bigCor_+3A_correct">correct</code></td>
<td>
<p>Correction for continuity for polychoric correlations. (see <code><a href="#topic+polychoric">polychoric</a></code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data are divided into subsets of size=size.  Correlations are then found for each subset and pairs of subsets. 
</p>
<p>Time is roughly linear with the number of cases and increases by the square of the number of variables.  The benefit of more cores is noticeable. It seems as if with 4 cores, we should use sizes to split it into 8 or 12 sets.  Otherwise we don't actually use all cores efficiently. 
</p>
<p>There is some overhead in using multicores.  So for smaller problems (e.g. the 4,000 cases of the  145 items of the psychTools::spi data set, the timings are roughly .14 seconds for bigCor (default size) and .10 for normal cor. For small problems, this actually  gets worse as we use more cores.    The cross over point seems to be at roughly 5K subjects.  (updated these timings to recognize the M1 Max chip.  An increase of 4x in speed!  They had been .44 and .36.)
</p>
<p>The basic loop loops over the subsets. When the size is a integer subset of the number of variables and is a multiple of the number of cores, the multiple cores will be used more.  Notice the benefit of 660/80 versus 660/100.  But this breaks down if we try 660/165.  Further notice the benefit when using a smaller subset (55) which led to the 4 cores being used more.  
</p>
<p>The following timings are included to help users tinker with parameters:
</p>
<p>Timings (in seconds) for various problems with 645K subjects on an 8 core Mac Book Pro with a 2.4 GHZ Intell core i9.
</p>
<p>options(mc.cores=4) (Because we have 8 we can work at the same time as we test this.)
</p>
<p>First test it with 644,495 subjects and 1/10 of the number of possible variables.  Then test it for somewhat fewer variables.  
</p>

<table>
<tr>
 <td style="text-align: left;">
Variables  </td><td style="text-align: left;"> size         </td><td style="text-align: left;"> 2 cores     </td><td style="text-align: left;"> 4 cores    </td><td style="text-align: left;"> compared to normal cor function </td>
</tr>
<tr>
 <td style="text-align: left;">


660  </td><td style="text-align: left;"> 100  </td><td style="text-align: left;">  430   </td><td style="text-align: left;"> 434 </td><td style="text-align: left;">  430 </td>
</tr>
<tr>
 <td style="text-align: left;">

660  </td><td style="text-align: left;"> 80   </td><td style="text-align: left;">  600 </td><td style="text-align: left;">   348  </td><td style="text-align: left;">  notice the improvement with 8ths  </td>
</tr>
<tr>
 <td style="text-align: left;">

660  </td><td style="text-align: left;"> 165  </td><td style="text-align: left;"> </td><td style="text-align: left;">    666  </td><td style="text-align: left;"> (Stitching seems to have been very slow) </td>
</tr>
<tr>
 <td style="text-align: left;">

660  </td><td style="text-align: left;"> 55   </td><td style="text-align: left;"> </td><td style="text-align: left;">    303  </td><td style="text-align: left;">  Even better if we break it into 12ths! </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>
<tr>
 <td style="text-align: left;">

 500  </td><td style="text-align: left;"> 100  </td><td style="text-align: left;">  </td><td style="text-align: left;"> 332   </td><td style="text-align: left;"> 322 secs  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>
<tr>
 <td style="text-align: left;">

480 </td><td style="text-align: left;"> 120    </td><td style="text-align: left;"> 408 </td><td style="text-align: left;">  365   </td><td style="text-align: left;"> 315    Better to change the size </td>
</tr>
<tr>
 <td style="text-align: left;"> 

480 </td><td style="text-align: left;"> 60    </td><td style="text-align: left;">  358 </td><td style="text-align: left;">   206  </td><td style="text-align: left;"> This leads to 8 splits </td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td>
</tr>

</table>

<p>We also test it with fewer subjects.  Time is roughly linear with number of subjects.
</p>

<table>
<tr>
 <td style="text-align: left;">
Variables  </td><td style="text-align: left;"> size      </td><td style="text-align: left;"> 2 cores     </td><td style="text-align: left;"> 4 cores    </td><td style="text-align: left;"> compared to normal cor function 
Further comparisons with fewer subjects (100K) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>
<tr>
 <td style="text-align: left;"> 
480  </td><td style="text-align: left;">  60  </td><td style="text-align: left;"> 57 </td><td style="text-align: left;">    31 </td><td style="text-align: left;"> 47 with normal cor.  Note the effect of n subjects!  </td>
</tr>
<tr>
 <td style="text-align: left;">
200 </td><td style="text-align: left;"> 50  </td><td style="text-align: left;">  19.9   </td><td style="text-align: left;">  13.6 </td><td style="text-align: left;"> 27.13 </td>
</tr>
<tr>
 <td style="text-align: left;">
100  </td><td style="text-align: left;"> 25    </td><td style="text-align: left;"> 4.6  </td><td style="text-align: left;"> 3.5   </td><td style="text-align: left;"> 5.85  </td>
</tr>
<tr>
 <td style="text-align: left;">
 </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td>
</tr>

</table>
 
<p>One last comparison, 10,000 subjects, showing the effect of getting the proper size value. You can tune on these smaller sets of subjects before trying large problems.
</p>

<table>
<tr>
 <td style="text-align: left;">
Variables  </td><td style="text-align: left;"> size       </td><td style="text-align: left;"> 2 cores     </td><td style="text-align: left;"> 4 cores    </td><td style="text-align: left;"> compared to normal cor function 
</td>
</tr>
<tr>
 <td style="text-align: left;">
480  </td><td style="text-align: left;"> 120 </td><td style="text-align: left;"> 5.2    </td><td style="text-align: left;">   5.1  </td><td style="text-align: left;">  4.51 </td>
</tr>
<tr>
 <td style="text-align: left;">
480  </td><td style="text-align: left;"> 60 </td><td style="text-align: left;"> 2.9    </td><td style="text-align: left;">   2.88  </td><td style="text-align: left;">  4.51 </td>
</tr>
<tr>
 <td style="text-align: left;">
480  </td><td style="text-align: left;">   30  </td><td style="text-align: left;"> 2.65  </td><td style="text-align: left;">  2.691  </td>
</tr>
<tr>
 <td style="text-align: left;">
480  </td><td style="text-align: left;"> 20   </td><td style="text-align: left;"> 2.73 </td><td style="text-align: left;">    2.77 </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
480   </td><td style="text-align: left;">   10   </td><td style="text-align: left;"> 2.82  </td><td style="text-align: left;">    2.97 </td><td style="text-align: left;"> too many splits? </td>
</tr>
<tr>
 <td style="text-align: left;">

200  </td><td style="text-align: left;">  50   </td><td style="text-align: left;"> 2.18  </td><td style="text-align: left;"> 1.39 </td><td style="text-align: left;"> 2.47 for normal cor (1.44 with 8 cores 2.99 with 1 core) </td>
</tr>
<tr>
 <td style="text-align: left;">
200  </td><td style="text-align: left;">  25   </td><td style="text-align: left;"> 1.2  </td><td style="text-align: left;"> 1.17 </td><td style="text-align: left;"> 2.47 for normal cor </td>
</tr>
<tr>
 <td style="text-align: left;"> (1.16 with 8 cores,  1.17 with 1 core) </td>
</tr>
<tr>
 <td style="text-align: left;">
100  </td><td style="text-align: left;"> 25    </td><td style="text-align: left;"> .64  </td><td style="text-align: left;"> .52   </td><td style="text-align: left;"> .56 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
 
<p>Timings updated in 2/23 using a MacBook Pro with M1 max chip
10,000 subjects 953 variables  suggests that a very small size (e.g. 20) is probably optimal  
</p>

<table>
<tr>
 <td style="text-align: left;">
Variables  </td><td style="text-align: left;"> size     </td><td style="text-align: left;"> 2 cores     </td><td style="text-align: left;"> 4 cores  </td><td style="text-align: left;"> 8 cores   </td><td style="text-align: left;"> compared to normal cor function 
</td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 20 </td><td style="text-align: left;"> 7.92    </td><td style="text-align: left;"> 4.55  </td><td style="text-align: left;"> 2.88 </td><td style="text-align: left;"> 11.04 </td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 30 </td><td style="text-align: left;"> 7.98    </td><td style="text-align: left;"> 4.88   </td><td style="text-align: left;"> 3.15 </td><td style="text-align: left;"> 11.04 </td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 40 </td><td style="text-align: left;"> 8.22 </td><td style="text-align: left;"> 5.14 </td><td style="text-align: left;"> 3.63 </td><td style="text-align: left;"> 11.16  </td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 60 </td><td style="text-align: left;"> 8.51 </td><td style="text-align: left;"> 5.59 </td><td style="text-align: left;"> 3.93 </td><td style="text-align: left;"> 11.16  </td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 80  </td><td style="text-align: left;"> 8.31 </td><td style="text-align: left;"> 5.59 </td><td style="text-align: left;"> 4.14 </td><td style="text-align: left;"> 11.16  </td>
</tr>
<tr>
 <td style="text-align: left;">
953 </td><td style="text-align: left;"> 120  </td><td style="text-align: left;"> 8.33 </td><td style="text-align: left;"> 6.22  </td><td style="text-align: left;"> 4.75 </td><td style="text-align: left;"> 11.16  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>The correlation matrix
</p>


<h3>Note</h3>

<p>Does not seem to work with data.tables
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Examples of large data sets with massively missing data are taken from the SAPA project. e.g.,
</p>
<p>William Revelle, Elizabeth M. Dworak, and David M. Condon (2021) Exploring the persome: The power of the item in understanding personality structure. Personality and Individual Differences, 169, <a href="https://doi.org/10.1016/j.paid.2020.109905">doi:10.1016/j.paid.2020.109905</a>
</p>
<p>David Condon (2018)The SAPA Personality Inventory: an empirically-derived, hierarchically-organized self-report personality assessment model.  PsyArXiv /sc4p9/ <a href="https://doi.org/10.31234/osf.io/sc4p9">doi:10.31234/osf.io/sc4p9</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pairwiseCountBig">pairwiseCountBig</a></code> which will do the same, but find the count of observations per cell.</p>


<h3>Examples</h3>

<pre><code class='language-R'>R &lt;- bigCor(psychTools::bfi,10)
#compare the results with 
r.bfi &lt;- cor(psychTools::bfi,use="pairwise")
all.equal(R,r.bfi)
</code></pre>

<hr>
<h2 id='biplot.psych'>Draw biplots of factor or component scores by factor or component loadings</h2><span id='topic+biplot.psych'></span>

<h3>Description</h3>

<p>Extends the biplot function to the output of <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+fa.poly">fa.poly</a></code>  or <code><a href="#topic+principal">principal</a></code>. Will plot factor scores and factor loadings in the same graph.  If the number of factors &gt; 2, then all pairs of factors are plotted. Factor score histograms are plotted on the diagonal. The input is the resulting object from <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code>, or <code><a href="#topic+fa.poly">fa.poly</a></code> with the scores=TRUE option. Points may be colored according to other criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
biplot(x, labels=NULL,cex=c(.75,1),main="Biplot from fa",
hist.col="cyan",xlim.s=c(-3,3),ylim.s=c(-3,3),xlim.f=c(-1,1),ylim.f=c(-1,1),
maxpoints=100,adjust=1.2,col,pos, arrow.len = 0.1,pch=16,choose=NULL,
cuts=1,cutl=.0,group=NULL,smoother=FALSE,vars=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biplot.psych_+3A_x">x</code></td>
<td>
<p>The output from <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+fa.poly">fa.poly</a></code> or <code><a href="#topic+principal">principal</a></code>  with the scores=TRUE option</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_labels">labels</code></td>
<td>
<p>if NULL, draw the points with the plot character (pch) specified. 
To identify the data points, specify labels= 1:n  where n is the number of 
observations, or labels =rownames(data) where data was the data set analyzed
by the factor analysis.</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_cex">cex</code></td>
<td>
<p>A vector of plot sizes of the data labels and of the factor labels</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_main">main</code></td>
<td>
<p>A main title for a two factor biplot</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_hist.col">hist.col</code></td>
<td>
<p>If plotting more than two factors, the color of the histogram of the factor scores</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_xlim.s">xlim.s</code></td>
<td>
<p>x limits of the scores. Defaults to plus/minus three sigma</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_ylim.s">ylim.s</code></td>
<td>
<p>y limits of the scores.Defaults to plus/minus three sigma</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_xlim.f">xlim.f</code></td>
<td>
<p>x limits of the factor loadings.Defaults to plus/minus 1.0</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_ylim.f">ylim.f</code></td>
<td>
<p>y limits of the factor loadings.Defaults to plus/minus 1.0</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_maxpoints">maxpoints</code></td>
<td>
<p>When plotting 3 (or more) dimensions, at what size should we switch 
from plotting &quot;o&quot; to plotting &quot;.&quot;</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_adjust">adjust</code></td>
<td>
<p>an adjustment factor in the histogram</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_col">col</code></td>
<td>
<p>a vector of colors for the data points and for the factor loading labels</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_pos">pos</code></td>
<td>
<p>If plotting labels, what position should they be in? 1=below, 2=left,
3 top, 4 right.
If missing, then the assumption is that labels should be printed instead of data points.</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_arrow.len">arrow.len</code></td>
<td>
<p> the length of the arrow head</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_pch">pch</code></td>
<td>
<p>The plotting character to use.  pch=16 gives reasonable size dots.
pch=&quot;.&quot; gives tiny points.  If adding colors, use pch between 21 and 25. 
(see examples).</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_choose">choose</code></td>
<td>
<p>Plot just the specified factors</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_cuts">cuts</code></td>
<td>
<p>Do not label cases with abs(factor scores) &lt; cuts) (Actually,
the  distance of the x and y scores from 0) </p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_cutl">cutl</code></td>
<td>
<p>Do not label variables with communalities in the two space &lt; cutl</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_group">group</code></td>
<td>
<p>A vector of a grouping variable for the scores.  Show a different color 
and symbol for each group.</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_smoother">smoother</code></td>
<td>
<p>If TRUE then do a smooth scatter plot (which shows the density rather than the data points). Only useful for large data sets.</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_vars">vars</code></td>
<td>
<p>If TRUE, draw arrows for the variables, and plot the scores.  If FALSE, then draw arrows for the scores and plot the variables.</p>
</td></tr>
<tr><td><code id="biplot.psych_+3A_...">...</code></td>
<td>
<p>more options for graphics</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the generic biplot function to take the output of a factor analysis <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+fa.poly">fa.poly</a></code> or principal components analysis <code><a href="#topic+principal">principal</a></code> and plot the factor/component scores along with the factor/component loadings.
</p>
<p>This is an extension of the generic biplot function to allow more control over plotting points in a two space and also to plot three or more factors (two at time).  
</p>
<p>This will work for objects produced by <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+fa.poly">fa.poly</a></code> or <code><a href="#topic+principal">principal</a></code>  if they applied to the original data matrix.  If however, one has a correlation matrix (e.g., based upon the output from <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code>), and has done either <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+principal">principal</a></code> on the correlations, then obviously, we can not do a biplot.  
</p>
<p>However, both of those functions produce a weights matrix, which, in combination with the original data can be used to find the scores by using <code><a href="#topic+factor.scores">factor.scores</a></code>.  Since biplot.psych is looking for two elements of the x object: x$loadings and x$scores, you can create the appropriate object to plot, or add it to the factor object See the third and 	fourth examples.
</p>
<p>In order to just plot the loadings, use <code><a href="#topic+fa.plot">fa.plot</a></code>.  Or, if we want to show the loadings as vectors, use pch = &quot;&quot;. 
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+fa.poly">fa.poly</a></code>,  <code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+fa.plot">fa.plot</a></code>, <code><a href="#topic+pairs.panels">pairs.panels</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#the standard example
data(USArrests)
fa2 &lt;- fa(USArrests,2,scores=TRUE)
biplot(fa2,labels=rownames(USArrests))

# plot the 3 factor solution
#data(bfi)
fa3 &lt;- fa(psychTools::bfi[1:200,1:15],3,scores=TRUE)
biplot(fa3)
#just plot factors 1 and 3 from that solution
biplot(fa3,choose=c(1,3))

#
fa2 &lt;- fa(psychTools::bfi[16:25],2)  #factor analysis
fa2$scores &lt;- fa2$scores[1:100,]  #just take the first 100
#now plot with different colors and shapes for males and females
biplot(fa2,pch=c(24,21)[psychTools::bfi[1:100,"gender"]],
group =psychTools::bfi[1:100,"gender"],
   main="Biplot of Openness and Neuroticism by gender")

## An example from the correlation matrix
r &lt;- cor(psychTools::bfi[1:200,1:10], use="pairwise") #find the correlations
f2 &lt;- fa(r,2) 
#biplot(f2)  #this throws an error    (not run)

#f2 does not have scores, but we can find them
f2$scores &lt;- factor.scores(psychTools::bfi[1:200,1:10],f2)

biplot(f2,main="biplot from correlation matrix and factor scores")


#or create a new object with the scores
 #find the correlations for all subjects
r &lt;- cor(psychTools::bfi[1:10], use="pairwise") 
f2 &lt;- fa(r,2) 
x &lt;- list() 
#find the scores for just the first 200 subjects
x$scores &lt;- factor.scores(psychTools::bfi[1:200,1:10],f2)
x$loadings &lt;- f2$loadings
class(x) &lt;- c('psych','fa')
biplot(x,main="biplot from correlation matrix combined with factor scores")

</code></pre>

<hr>
<h2 id='block.random'>Create a block randomized structure for n independent variables</h2><span id='topic+block.random'></span>

<h3>Description</h3>

<p>Random assignment of n subjects with an equal number in all of N conditions may  done by block randomization, where the block size is the number of experimental conditions. The number of Independent Variables and the number of levels in each IV are specified as input. The output is a the block randomized design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>block.random(n, ncond = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="block.random_+3A_n">n</code></td>
<td>
<p>The number of subjects to randomize.  Must be a multiple of the number of experimental conditions</p>
</td></tr>
<tr><td><code id="block.random_+3A_ncond">ncond</code></td>
<td>
<p>The number of conditions for each IV.  Defaults to 2 levels for one IV.  If more than one IV, specify as a vector. If names are provided, they are used, otherwise the IVs are labeled as IV1 ... IVn</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>blocks</code></td>
<td>
<p>A matrix of subject numbers, block number, and randomized levels for each IV</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Prepared for a course on Research Methods in Psychology <a href="https://personality-project.org/courses/205/205.syllabus.html">https://personality-project.org/courses/205/205.syllabus.html</a> </p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>Examples</h3>

<pre><code class='language-R'>br &lt;- block.random(n=24,c(2,3))
pairs.panels(br)
br &lt;- block.random(96,c(time=4,drug=3,sex=2))
pairs.panels(br)
</code></pre>

<hr>
<h2 id='bock'>Bock and Liberman (1970) data set of 1000 observations of the LSAT
</h2><span id='topic+bock'></span><span id='topic+bock.table'></span><span id='topic+lsat6'></span><span id='topic+lsat7'></span><span id='topic+bock.lsat'></span>

<h3>Description</h3>

<p>An example data set used by McDonald (1999) as well as other discussions of Item Response Theory makes use of a data table on 10 items (two sets of 5) from the Law School Admissions Test (LSAT).  Included in this data set is the original table as well as the reponses for 1000 subjects on the first set (Figure Classification) and second set (Debate). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bock)</code></pre>


<h3>Format</h3>

<p>A data frame with 32 observations on the following 8 variables.
</p>

<dl>
<dt><code>index</code></dt><dd><p>32 response patterns</p>
</dd>
<dt><code>Q1</code></dt><dd><p>Responses to item 1</p>
</dd>
<dt><code>Q2</code></dt><dd><p>Responses to item 2</p>
</dd>
<dt><code>Q3</code></dt><dd><p>Responses to item 3</p>
</dd>
<dt><code>Q4</code></dt><dd><p>Responses to item 4</p>
</dd>
<dt><code>Q5</code></dt><dd><p>Responses to item 5</p>
</dd>
<dt><code>Ob6</code></dt><dd><p>count of observations for the section 6 test</p>
</dd>
<dt><code>Ob7</code></dt><dd><p>count of observations for the section 7 test</p>
</dd>
</dl>

<p>Two other data sets are derived from the bock dataset. These are converted using the <code><a href="#topic+table2df">table2df</a></code> function.
</p>

<dl>
<dt>lsat6</dt><dd><p>reponses to 5 items for 1000 subjects on section 6</p>
</dd>
<dt>lsat7</dt><dd><p>reponses to 5 items for 1000 subjects on section 7</p>
</dd>
</dl>



<h3>Details</h3>

<p>The lsat6 data set is analyzed in the ltm package as well as by McDonald (1999). lsat7 is another 1000 subjects on part 7 of the LSAT. Both sets are described by Bock and Lieberman (1970). Both sets are useful examples of testing out IRT procedures and showing the use of <code><a href="#topic+tetrachoric">tetrachoric</a></code> correlations and item factor analysis using the <code><a href="#topic+irt.fa">irt.fa</a></code> function.
</p>


<h3>Source</h3>

<p>R. Darrell Bock and M. Lieberman (1970). Fitting a response model for dichotomously scored items. Psychometrika, 35(2):179-197.
</p>


<h3>References</h3>

<p>R.P. McDonald. Test theory: A unified treatment. L. Erlbaum Associates, Mahwah, N.J., 1999.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bock)
responses &lt;- table2df(bock.table[,2:6],count=bock.table[,7],
        labs= paste("lsat6.",1:5,sep=""))
describe(responses)
## maybe str(bock.table) ; plot(bock.table) ...
</code></pre>

<hr>
<h2 id='cattell'>12 cognitive variables from Cattell (1963) </h2><span id='topic+cattell'></span>

<h3>Description</h3>

<p>Rindskopf and Rose (1988) use this data set to demonstrate confirmatory second order factor models.  It is a nice example data set to explore hierarchical structure and alternative factor solutions. It contains measures of fluid and crystallized intelligence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("cattell")</code></pre>


<h3>Format</h3>

<p>A correlation matrix of the following 12 variables from 277 7th and 8th graders
</p>

<dl>
<dt>Verbal</dt><dd><p>A verbal ability test from Thurstone</p>
</dd>
<dt>Verbal2</dt><dd><p>A verbal ability test from Thurstone</p>
</dd>
<dt>Space1</dt><dd><p>A Spatial ability test from Thurstone</p>
</dd>
<dt>Space2</dt><dd><p>A Spatial ability test from Thurstone</p>
</dd>
<dt>Reason1</dt><dd><p>A reasoning test from Thurstone</p>
</dd>
<dt>Reason2</dt><dd><p>A reasoning  test from Thurstone</p>
</dd>
<dt>Number1</dt><dd><p>A Numerical ability test from Thurstone</p>
</dd>
<dt>Number2</dt><dd><p>A Numerical ability test from Thurstone</p>
</dd>
<dt>IPATSer</dt><dd><p>A &quot;culture fair&quot;  series from the IPAT</p>
</dd>
<dt>IPATCLAS</dt><dd><p>A &quot;culture fair&quot; classification test from the IPAT</p>
</dd>
<dt>IPATMatr</dt><dd><p>A &quot;culture fair&quot; matrix reasoning  test from the IPAT</p>
</dd>
<dt>IPATTop</dt><dd><p>A &quot;culture fair&quot; topology test from the IPAT</p>
</dd>
</dl>



<h3>Details</h3>

<p>Cattell (1963) reported on 8 cognitive variables from Thurstone and four from the Institute for Personality Assessment Test (IPAT).  Rindskopf and Rose (1988) use this data set as an example of second order factor analysis. It is thus a nice set for examining alternative solutions such as bifactor rotation, <code><a href="#topic+omega">omega</a></code> hierarchical, as well as <code><a href="#topic+esem">esem</a></code> and <code><a href="#topic+interbattery">interbattery</a></code> factor analysis.
</p>


<h3>Source</h3>

<p>David Rindskopf and Tedd Rose, (1988) Some Theory and Applications of Confirmatory Second- Order Factor Analysis, Multivariate Behavioral Research, 23, 51-67.</p>


<h3>References</h3>

<p>Cattell, R. B. (1963).Theory of fluid and crystallized intelligence: A critical experiment. Journal of Educational Psychology, 54, 1-22.
</p>
<p>David Rindskopf and Tedd Rose, (1988) Some Theory and Applications of Confirmatory Second- Order Factor Analysis, Multivariate Behavioral Research, 23, 51-67.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cattell)
corPlot(cattell,numbers=TRUE,upper=FALSE,diag=FALSE,
             main="12 cognitive variables from Cattell (1963)",xlas=2)
</code></pre>

<hr>
<h2 id='circ.tests'> Apply four tests of circumplex versus simple structure </h2><span id='topic+circ.tests'></span>

<h3>Description</h3>

<p>Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>circ.tests(loads, loading = TRUE, sorting = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="circ.tests_+3A_loads">loads</code></td>
<td>
<p> A matrix of loadings <code>loads</code> here </p>
</td></tr>
<tr><td><code id="circ.tests_+3A_loading">loading</code></td>
<td>
<p> Are these loadings or a correlation matrix <code>loading</code> </p>
</td></tr>
<tr><td><code id="circ.tests_+3A_sorting">sorting</code></td>
<td>
<p> Should the variables be sorted <code>sorting</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>&ldquo;A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle &amp; Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.
</p>
<p>&ldquo;A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, &amp; Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte &amp; Plutchik, 1981; Larsen &amp; Diener, 1992). (Acton and Revelle, 2004)
</p>
<p>Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:
</p>
<p>1	The Gap test of equal spacing
</p>
<p>2	Fisher's test of equality of axes 
</p>
<p>3	A test of indifference to Rotation
</p>
<p>4	A test of equal Variance of squared factor loadings across arbitrary rotations.
</p>
<p>To interpret the values of these various tests, it is useful to compare the particular solution to simulated solutions representing pure cases of circumplex and simple structure.  See the example output from <code><a href="#topic+circ.simulation">circ.simulation</a></code> and compare these plots with the results of the circ.test.
</p>


<h3>Value</h3>

<p>A list of four items is returned.  These are the gap, fisher, rotation and variance test results.
</p>
<table>
<tr><td><code>gaps</code></td>
<td>
<p>gap.test</p>
</td></tr>
<tr><td><code>fisher</code></td>
<td>
<p>fisher.test</p>
</td></tr>
<tr><td><code>RT</code></td>
<td>
<p>rotation.test</p>
</td></tr>
<tr><td><code>VT</code></td>
<td>
<p>variance.test</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Of the 10 criterion discussed in Acton and Revelle (2004), these tests operationalize the four most useful.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p> Acton, G. S. and Revelle, W. (2004) Evaluation of Ten Psychometric Criteria for Circumplex Structure.  Methods of Psychological Research Online, Vol. 9, No. 1 <a href="https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf">https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf</a> </p>


<h3>See Also</h3>

<p>To understand the results of the circ.tests it it best to compare it to simulated values.  Thus, see  <code><a href="#topic+circ.simulation">circ.simulation</a></code>, <code><a href="#topic+sim.circ">sim.circ</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>circ.data &lt;- circ.sim(24,500)
circ.fa &lt;- fa(circ.data,2)
plot(circ.fa,title="Circumplex Structure")
ct &lt;- circ.tests(circ.fa)
#compare with non-circumplex data
simp.data &lt;- item.sim(24,500)
simp.fa &lt;- fa(simp.data,2)
plot(simp.fa,title="Simple Structure")
st &lt;- circ.tests(simp.fa)
res &lt;- rbind(ct[1:4],st[1:4])
rownames(res) &lt;- c("circumplex","Simple")
print(res,digits=2)

</code></pre>

<hr>
<h2 id='cluster.fit'> cluster Fit:  fit of the cluster model to a correlation matrix </h2><span id='topic+cluster.fit'></span>

<h3>Description</h3>

<p>How well does the cluster model found by <code><a href="#topic+ICLUST">ICLUST</a></code> fit the original correlation matrix?  A similar algorithm  <code><a href="#topic+factor.fit">factor.fit</a></code> is found in <code><a href="#topic+VSS">VSS</a></code>. This function is internal to ICLUST but has more general use as well.
</p>
<p>In general, the cluster model is a Very Simple Structure model of complexity one.  That is, every item is assumed to represent only one factor/cluster. Cluster fit is an analysis of how well this model reproduces a correlation matrix.  Two measures of fit are given: cluster fit and factor fit.  Cluster fit assumes that variables that define different clusters are orthogonal.  Factor fit takes the loadings generated by a cluster model, finds the cluster loadings on all clusters, and measures the degree of fit of this somewhat more complicated model.  Because the cluster loadings are similar to, but not identical to factor loadings, the factor fits found here and by  <code><a href="#topic+factor.fit">factor.fit</a></code> will be similar.   </p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.fit(original, load, clusters, diagonal = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.fit_+3A_original">original</code></td>
<td>
<p>The original correlation matrix being fit </p>
</td></tr>
<tr><td><code id="cluster.fit_+3A_load">load</code></td>
<td>
<p> Cluster loadings &ndash; that is, the correlation of individual items with the clusters, corrected for item overlap </p>
</td></tr>
<tr><td><code id="cluster.fit_+3A_clusters">clusters</code></td>
<td>
<p>The cluster structure </p>
</td></tr>
<tr><td><code id="cluster.fit_+3A_diagonal">diagonal</code></td>
<td>
<p> Should we fit the diagonal as well?  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster model is similar to the factor model: R is fitted by C'C. Where C &lt;- Cluster definition matrix x the loading matrix.  How well does this model approximate the original correlation matrix and how does this compare to a factor model?
</p>
<p>The fit statistic is a comparison of the original (squared) correlations to the residual correlations.  Fit = 1 - r*2/r2 where r* is the residual correlation of data - model and model = C'C.
</p>


<h3>Value</h3>

<table>
<tr><td><code>clusterfit</code></td>
<td>
<p>The cluster model is a reduced form of the factor loading matrix.  That is, it is the product of the elements of the cluster matrix * the loading matrix. </p>
</td></tr>
<tr><td><code>factorfit</code></td>
<td>
<p>How well does the complete loading matrix reproduce the correlation matrix?</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a> </p>


<h3>See Also</h3>

  <p><code><a href="#topic+VSS">VSS</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>,  <code><a href="#topic+factor2cluster">factor2cluster</a></code>,  <code><a href="#topic+cluster.cor">cluster.cor</a></code>,  <code><a href="#topic+factor.fit">factor.fit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> r.mat&lt;- Harman74.cor$cov
 iq.clus &lt;- ICLUST(r.mat,nclusters =2)
 fit &lt;- cluster.fit(r.mat,iq.clus$loadings,iq.clus$clusters)
 fit
 

</code></pre>

<hr>
<h2 id='cluster.loadings'> Find item by cluster correlations, corrected for overlap and reliability </h2><span id='topic+cluster.loadings'></span>

<h3>Description</h3>

<p>Given a n x n correlation matrix and a n x c matrix of -1,0,1 cluster weights for those n items on  c clusters, find the correlation of each item with each cluster.  If the item is part of the cluster, correct for item overlap.  Part of the <code><a href="#topic+ICLUST">ICLUST</a></code> set of functions, but useful for many item analysis problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.loadings(keys, r.mat, correct = TRUE,SMC=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.loadings_+3A_keys">keys</code></td>
<td>
<p> Cluster keys: a matrix of -1,0,1 cluster weights</p>
</td></tr>
<tr><td><code id="cluster.loadings_+3A_r.mat">r.mat</code></td>
<td>
<p> A correlation matrix </p>
</td></tr>
<tr><td><code id="cluster.loadings_+3A_correct">correct</code></td>
<td>
<p>Correct for reliability</p>
</td></tr>
<tr><td><code id="cluster.loadings_+3A_smc">SMC</code></td>
<td>
<p>Use the squared multiple correlation as a communality estimate, otherwise use the greatest correlation for each variable</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a set of items to be scored as (perhaps overlapping) clusters and the intercorrelation matrix of the items, find the clusters and then the correlations of each item with each cluster.  Correct for item overlap by replacing the item variance with its average within cluster inter-item correlation.  
</p>
<p>Although part of ICLUST, this may be used in any SAPA (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) 
application where we are interested in item-whole correlations of items and composite scales.
</p>
<p>For information about SAPA see Revelle et al, 2010, 2016.  For information about SAPA based measures of ability, see <a href="https://icar-project.org">https://icar-project.org</a>.
</p>
<p>These loadings are particularly interpretable when sorted by absolute magnitude for each cluster (see <code><a href="#topic+ICLUST.sort">ICLUST.sort</a></code>). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>loadings</code></td>
<td>
<p>A matrix of item-cluster correlations (loadings)</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>Correlation matrix of the clusters</p>
</td></tr>
<tr><td><code>corrected</code></td>
<td>
<p>Correlation matrix of the clusters, raw correlations below the diagonal, alpha on diagonal, corrected for reliability above the diagonal</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>Cluster standard deviations</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>alpha reliabilities of the clusters</p>
</td></tr>
<tr><td><code>G6</code></td>
<td>
<p>G6* Modified estimated of Guttman Lambda 6</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>Number of items in the cluster</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Although part of ICLUST, this may be used in any SAPA application where we are interested in item- whole correlations of items and composite scales.</p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

<p> ICLUST: <a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a>
</p>
<p>Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer.
</p>
<p>Revelle, W,  Condon, D.M.,  Wilt, J.,  French, J.A., Brown, A.,  and  Elleman, L.G. (2016) Web and phone based data collection using planned missing designs. In  Fielding, N.G.,  Lee, R.M. and  Blank, G. (Eds). SAGE Handbook of Online Research Methods (2nd Ed), Sage Publcations.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+factor2cluster">factor2cluster</a></code>,  <code><a href="#topic+cluster.cor">cluster.cor</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
 r.mat&lt;- Harman74.cor$cov
 clusters &lt;- matrix(c(1,1,1,rep(0,24),1,1,1,1,rep(0,17)),ncol=2)
 cluster.loadings(clusters,r.mat)

 

</code></pre>

<hr>
<h2 id='cluster.plot'>Plot factor/cluster loadings and assign items to clusters by their highest loading.</h2><span id='topic+cluster.plot'></span><span id='topic+fa.plot'></span><span id='topic+factor.plot'></span>

<h3>Description</h3>

<p>Cluster analysis and factor analysis are procedures for grouping items in terms of a smaller number of (latent) factors or (observed) clusters.  Graphical presentations of clusters typically show tree structures, although they can be represented in terms of item by cluster correlations.  
</p>
<p>Cluster.plot plots items by their cluster loadings (taken, e.g., from <code><a href="#topic+ICLUST">ICLUST</a></code>) or factor loadings (taken, eg., from <code><a href="#topic+fa">fa</a></code>).  Cluster membership may be assigned apriori or may be determined in terms of the highest (absolute) cluster loading for each item.  
</p>
<p>If the input is an object of class &quot;kmeans&quot;, then the cluster centers are plotted. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>

cluster.plot(ic.results, cluster = NULL, cut = 0, labels=NULL,
          title = "Cluster plot",pch=18,pos,show.points=TRUE,choose=NULL,...)
fa.plot(ic.results, cluster = NULL, cut = 0, labels=NULL,title, 
    jiggle=FALSE,amount=.02,pch=18,pos,show.points=TRUE,choose=NULL,main=NULL,...)
factor.plot(ic.results, cluster = NULL, cut = 0, labels=NULL,title,jiggle=FALSE,
                  amount=.02,pch=18,pos,show.points=TRUE,...)  #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.plot_+3A_ic.results">ic.results</code></td>
<td>
<p>A factor analysis or cluster analysis output including the loadings, or a matrix of item by cluster correlations. Or the output from a kmeans cluster analysis. </p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_cluster">cluster</code></td>
<td>
<p> A vector of cluster membership </p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_cut">cut</code></td>
<td>
<p> Assign items to clusters if the absolute loadings are &gt; cut </p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_labels">labels</code></td>
<td>
<p>If row.names exist they will be added to the plot, or, if they don't, labels can be specified.  If labels =NULL, and there are no row names, then variables are labeled by row number.)</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_title">title</code></td>
<td>
<p> Any title</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_jiggle">jiggle</code></td>
<td>
<p>When plotting with factor loadings that are almost identical, it is sometimes useful to &quot;jiggle&quot; the points by jittering them. The default is to not jiggle.</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_amount">amount</code></td>
<td>
<p>if jiggle=TRUE, then how much should the points be jittered?</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_pch">pch</code></td>
<td>
<p>factor and clusters are shown with different pch values, starting at pch+1</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_pos">pos</code></td>
<td>
<p>Position of the text for labels for two dimensional plots. 1=below, 2 = left, 3 = above, 4= right</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_show.points">show.points</code></td>
<td>
<p>When adding labels to the points, should we show the points as well as the labels.  For many points, better to not show them, just the labels.</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_choose">choose</code></td>
<td>
<p>Specify the factor/clusters to plot</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_main">main</code></td>
<td>
<p>Any title &ndash; redundant with title</p>
</td></tr>
<tr><td><code id="cluster.plot_+3A_...">...</code></td>
<td>
<p>Further options to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Results of either a factor analysis or cluster analysis are plotted.  Each item is assigned to its highest loading factor, and then identified by variable name as well as cluster (by color). 
The cluster assignments can be specified to override the automatic clustering by loading.
Both of these functions may be called directly or by calling the generic plot function.  (see example).
</p>


<h3>Value</h3>

<p>Graphical output is presented.
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>, <code><a href="#topic+fa.graph">fa.graph</a></code>, <code><a href="#topic+plot.psych">plot.psych</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>circ.data &lt;- circ.sim(24,500)
circ.fa &lt;- fa(circ.data,2)
plot(circ.fa,cut=.5)
f5 &lt;- fa(psychTools::bfi[1:25],5) 
plot(f5,labels=colnames(psychTools::bfi)[1:25],show.points=FALSE)
plot(f5,labels=colnames(psychTools::bfi)[1:25],show.points=FALSE,choose=c(1,2,4))
</code></pre>

<hr>
<h2 id='cluster2keys'>Convert a cluster vector (from e.g., kmeans) to a keys matrix suitable for scoring item clusters. </h2><span id='topic+cluster2keys'></span>

<h3>Description</h3>

<p>The output of the kmeans clustering function produces a vector of cluster membership.  The <code><a href="#topic+score.items">score.items</a></code> and <code><a href="#topic+cluster.cor">cluster.cor</a></code> functions require a matrix of keys.  cluster2keys does this.
</p>
<p>May also be used to take the output of an <code><a href="#topic+ICLUST">ICLUST</a></code> analysis and find a keys matrix.  (By doing a call to the <code><a href="#topic+factor2cluster">factor2cluster</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster2keys(c)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster2keys_+3A_c">c</code></td>
<td>
<p>A vector of cluster assignments or an object of class &ldquo;kmeans&quot; that contains a vector of clusters. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that because kmeans will not reverse score items, the clusters defined by kmeans will not necessarily match those of ICLUST with the same number of clusters extracted. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>keys</code></td>
<td>
<p>A matrix of keys suitable for score.items or cluster.cor</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+cluster.cor">cluster.cor</a></code>,<code><a href="#topic+score.items">score.items</a></code>, <code><a href="#topic+factor2cluster">factor2cluster</a></code>, <code><a href="#topic+make.keys">make.keys</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>test.data &lt;- Harman74.cor$cov
kc &lt;- kmeans(test.data,4)
keys &lt;- cluster2keys(kc)
keys  #these match those found by ICLUST
cluster.cor(keys,test.data)
</code></pre>

<hr>
<h2 id='cohen.d'>Find Cohen d and confidence intervals</h2><span id='topic+cohen.d'></span><span id='topic+d.robust'></span><span id='topic+cohen.d.ci'></span><span id='topic+d.ci'></span><span id='topic+cohen.d.by'></span><span id='topic+d2r'></span><span id='topic+r2d'></span><span id='topic+d2t'></span><span id='topic+t2d'></span><span id='topic+m2t'></span><span id='topic+m2d'></span><span id='topic+d2OVL'></span><span id='topic+d2OVL2'></span><span id='topic+d2CL'></span><span id='topic+d2U3'></span><span id='topic+cd.validity'></span>

<h3>Description</h3>

<p>Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cohen.d(x, group,alpha=.05,std=TRUE,sort=NULL,dictionary=NULL,MD=TRUE,data=NULL)
d.robust(x,group,trim=.2)
cohen.d.ci(d,n=NULL,n2=NULL,n1=NULL,alpha=.05)
d.ci(d,n=NULL,n2=NULL,n1=NULL,alpha=.05)
cohen.d.by(x,group,group2,alpha=.05,MD=TRUE)
d2r(d)
r2d(rho)
d2t(d,n=NULL,n2=NULL,n1=NULL)
t2d(t,n=NULL,n2=NULL,n1=NULL)
m2t(m1,m2,s1,s2,n1=NULL,n2=NULL,n=NULL,pooled=TRUE)  #returns d invisibily
m2d(m1,m2,s1,s2,n1=NULL,n2=NULL,n=NULL,pooled=TRUE)
d2OVL(d)  #Percent overlap for 1 distribtion
d2OVL2(d)  #Percent overlap joint distribution
d2CL(d)   #Common language effect size
d2U3(d)   #Proportion in higher group exceedding median of lower group
cd.validity(d, keys, abs=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cohen.d_+3A_x">x</code></td>
<td>
<p>A data frame or matrix  (can be specified in formula mode) </p>
</td></tr>
<tr><td><code id="cohen.d_+3A_group">group</code></td>
<td>
<p>Some dichotomous grouping variable (may be specified using formula input (see example))</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_group2">group2</code></td>
<td>
<p>Apply cohen.d for each of the subgroups defined by group2 (may be specified by formula as well)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_data">data</code></td>
<td>
<p>If using formula mode and specifying a particular variable (see example)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_d">d</code></td>
<td>
<p>An effect size</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_keys">keys</code></td>
<td>
<p>A list of scoring keys (similar to scoreItems)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_abs">abs</code></td>
<td>
<p>When finding average cd validities, should we take absolute values (TRUE)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_trim">trim</code></td>
<td>
<p>The amount of trimming used in finding the means and sds in d.robust</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_n">n</code></td>
<td>
<p>Total sample size (of groups 1 and 2)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_n1">n1</code></td>
<td>
<p>Sample size of group 1 (if only one group)</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_n2">n2</code></td>
<td>
<p>Sample size of group 2 </p>
</td></tr>
<tr><td><code id="cohen.d_+3A_pooled">pooled</code></td>
<td>
<p>Pool the two variances</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_t">t</code></td>
<td>
<p>Student's t statistic</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha is the width of the confidence interval</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_std">std</code></td>
<td>
<p>Find the correlation rather covariance matrix</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_rho">rho</code></td>
<td>
<p>A correlation to be converted to an effect size</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_m1">m1</code></td>
<td>
<p>Mean of group 1</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_m2">m2</code></td>
<td>
<p>Mean of group 2</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_s1">s1</code></td>
<td>
<p>Standard deviation of group 1</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_s2">s2</code></td>
<td>
<p>Standard deviation of group 2</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_sort">sort</code></td>
<td>
<p>Should we sort (and if so, in which direction), the results of cohen.d? 
Directions are &quot;decreasing&quot; or  &quot;increasing&quot;.  If TRUE, sorts in a decreasing order.</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_dictionary">dictionary</code></td>
<td>
<p>What are the items being described?</p>
</td></tr>
<tr><td><code id="cohen.d_+3A_md">MD</code></td>
<td>
<p>Find Mahalanobis distance in cohen.d.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   
</p>
<p style="text-align: center;"><code class="reqn">d  = \frac{M2 - M1}{Sp}</code>
</p>
  
<p>where Sp is the pooled standard deviation.  
</p>
<p style="text-align: center;"><code class="reqn">Sp  = \sqrt{\frac{(n1-1)*s1^2 + (n2-1)* s2^2}{N}  }  </code>
</p>

<p>Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  
</p>
<p>Confidence intervals for Cohen's d are found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.
</p>
<p>The results of <code><a href="#topic+cohen.d">cohen.d</a></code> may be displayed using the <code><a href="#topic+error.dots">error.dots</a></code> function.  This will include the labels provided in the dictionary.  
</p>
<p>In the case of finding the confidence interval (using <code><a href="#topic+cohen.d.ci">cohen.d.ci</a></code> for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.
</p>
<p>Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.
</p>
<p>It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  
</p>
<p><code><a href="#topic+cohen.d.by">cohen.d.by</a></code> will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. 
</p>
<p><code><a href="#topic+d.robust">d.robust</a></code> follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.
</p>
<p><code><a href="#topic+m2t">m2t</a></code> reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.
</p>
<p>The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  <code class="reqn">D = \sqrt{d' R^{-1}d}</code>.
By default, <code><a href="#topic+cohen.d">cohen.d</a></code> will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.
</p>
<p>Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the <code><a href="#topic+d2OVL">d2OVL</a></code> (percent overlap for 1 distribution), <code><a href="#topic+d2OVL2">d2OVL2</a></code> percent overlap of joint distributions, <code><a href="#topic+d2CL">d2CL</a></code> (the common language effect size), and <code><a href="#topic+d2U3">d2U3</a></code> (proportion in higher group exceeding median of the lower group).
</p>
<p style="text-align: center;"><code class="reqn">OVL  = 2\phi(-d/2)</code>
</p>
<p>  is the proportion of overlap (and gets smaller the larger the d).
where  Phi  is the cumulative density function of the normal distribution.
</p>
<p style="text-align: center;"><code class="reqn">OVL_2  = \frac{OVL}{2-OVL}</code>
</p>

<p>The proportion of individuals in one group above the median of the other group is U3
</p>
<p style="text-align: center;"><code class="reqn">U_3 = \phi_d</code>
</p>
<p>.
</p>
<p>The Common Language Effect size 
</p>
<p style="text-align: center;"><code class="reqn">CL  = \phi (d *\sqrt{2})</code>
</p>

<p>These last two get larger with (abs (d)).  
For graphic displays of Cohen's d and Mahalanobis D, see the <code><a href="#topic+scatterHist">scatterHist</a></code> examples, or the example from the psychTools::GERAS data set. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>d</code></td>
<td>
<p>Cohen's d statistic, including the upper and lower confidence levels</p>
</td></tr>
<tr><td><code>hedges.g</code></td>
<td>
<p>Hedge's g statistic, including the upper and lower confidence levels</p>
</td></tr>
<tr><td><code>M.dist</code></td>
<td>
<p>Mahalanobis distance between the two groups</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>Student's t statistic</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The point biserial r equivalent of d</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>sample size used for each analysis</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The probability of abs(t)&gt;0</p>
</td></tr>
<tr><td><code>descriptive</code></td>
<td>
<p>The descriptive statistics for each group.  This is useful to show means and then the d values.</p>
</td></tr>
<tr><td><code>OVL</code></td>
<td>
<p>etc. some of the measures of overlap discussed by DelGiudice, 2009</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Cohen and Hedges differ in they way they calculate the pooled within group standard deviation. I find the treatment by McGrath and Meyer to be most helpful in understanding the differences.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Cohen, Jackob (1988) Statistical Power Analysis for the Behavioral Sciences. 2nd Edition, Lawrence Erlbaum Associates. 
</p>
<p>Algina, James and Keselman, H. J. and Penfield, Randall D. (2005) An Alternative to Cohen's Standardized Mean Difference Effect Size: A Robust Parameter and Confidence Interval in the Two Independent Groups Case. Psychological Methods.  10, 317-328.
</p>
<p>Goulet-Pelletier, Jean-Christophe and Cousineau, Denis.(2018) A review of effect sizes and their confidence intervals, Part I: The Cohen's d family. The Quantitative Methods for Psychology, 14, 242-265.
</p>
<p>Marco Del Giudice (2019) Measuring Sex Differences and Similarities, (in VanderLaan and Wong (ed. ) Gender and sexuality development: Contemporary theory and research.)
</p>
<p>McGrath, Robert E and Meyer, Gregory J. (2006) When effect sizes disagree: the case of r and d. Psychological methods, 11, 4, 386-401.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+describeBy">describeBy</a></code>, <code><a href="#topic+describe">describe</a></code> <code><a href="#topic+error.dots">error.dots</a></code> to display the results.  <code><a href="#topic+scatterHist">scatterHist</a></code> to show d and MD for pairs of variables. (See in particular the use of <code><a href="#topic+scatterHist">scatterHist</a></code> on psychTools::GERAS daa set.)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cohen.d(sat.act,"gender")
#robust version
round(d.robust(sat.act,"gender")$robust.d,2)

#formula input is nicer
cohen.d(sat.act ~ gender) #formula input version
#if we want to report the group means, we grab the data in descriptive
cd &lt;- cohen.d(sat.act ~ gender)
cd.df &lt;- data.frame(d = cd$cohen.d[,"effect"], male = cd$descriptive$mean[1,cd$order[-1]],
      female = cd$descriptive$mean[2, cd$order[-1]])
#report cohen.d by another group
cd &lt;- cohen.d.by(sat.act,"gender","education")
cohen.d(SATV + SATQ ~ gender, data=sat.act) #just choose two variables
summary(cd)  #summarize the output

#formula version combines these functions
cd &lt;- cohen.d(sat.act ~ gender + education)  #find d by gender for each level of education
summary(cd)

#now show several examples of confidence intervals
#one group (d vs 0)
#consider the t from the cushny data set
t2d( -4.0621,n1=10)
d.ci(-1.284549,n1=10)  #the confidence interval of the effect of drug on sleep
#two groups
d.ci(.62,n=64)  #equal group size
d.ci(.62,n1=35,n2=29) #unequal group size
#several examples of d and t from data
m2d(52.58,-70.65,49.9,47.5) #Terman and Miles 1936

#graphically show the various overlap statistics
curve(d2OVL2(x),0,3,xlab="d",ylab="",lty="dashed",
       main="Four representations of effect size (d) ")
curve(d2OVL(x),0,3,xlab="d",add=TRUE,)
curve(d2CL(x),0,3,add=TRUE)
curve(d2U3(x), add=TRUE,lty="dotted")
text(1,.37,"OVL2")
text(2,.37,"OVL")
text(1,.88,"U3")
text(2, .88,"CL")
</code></pre>

<hr>
<h2 id='cohen.kappa'>Find Cohen's kappa and weighted kappa coefficients for correlation of two raters</h2><span id='topic+wkappa'></span><span id='topic+cohen.kappa'></span>

<h3>Description</h3>

<p>Cohen's kappa (Cohen, 1960) and weighted kappa (Cohen, 1968) may be used to find the agreement of two raters when using nominal scores.  Light's kappa is just the average cohen.kappa if using more than 2 raters. 
</p>
<p>weighted.kappa is (probability of observed matches - probability of expected matches)/(1 - probability of expected matches).  Kappa just considers the matches on the main diagonal.  Weighted kappa considers off diagonal elements as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cohen.kappa(x, w=NULL,n.obs=NULL,alpha=.05,levels=NULL,w.exp=2)  
wkappa(x, w = NULL)    #deprectated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cohen.kappa_+3A_x">x</code></td>
<td>
<p>Either a two by n data with categorical values from 1 to p or a p x p table.  If a data array, a table will be found.</p>
</td></tr>
<tr><td><code id="cohen.kappa_+3A_w">w</code></td>
<td>
<p>A p x p matrix of weights.  If not specified, they are set to be 0 (on the diagonal) and (distance from diagonal) off the diagonal)^2. </p>
</td></tr>
<tr><td><code id="cohen.kappa_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations (if input is a square matrix.</p>
</td></tr>
<tr><td><code id="cohen.kappa_+3A_alpha">alpha</code></td>
<td>
<p>Probability level for confidence intervals</p>
</td></tr>
<tr><td><code id="cohen.kappa_+3A_levels">levels</code></td>
<td>
<p>Specify the levels if some levels of x or y are completely missing.  See Examples</p>
</td></tr>
<tr><td><code id="cohen.kappa_+3A_w.exp">w.exp</code></td>
<td>
<p>Exponent to apply to weights matrix &ndash; see examples</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When cateogorical judgments are made with two cateories, a measure of relationship is the phi coefficient.  However, some categorical judgments are made using more than two outcomes.  For example, two diagnosticians might be asked to categorize patients three ways (e.g., Personality disorder, Neurosis, Psychosis) or to categorize the stages of a disease.  Just as base rates affect observed cell frequencies in a two by two table, they need to be considered in the n-way table (Cohen, 1960). 
</p>
<p>Kappa considers the matches on the main diagonal.  A penalty function (weight) may be applied to the off diagonal matches.  If the weights increase by the square of the distance from the diagonal, weighted kappa is similar to an Intra Class Correlation (<code><a href="#topic+ICC">ICC</a></code>).
</p>
<p>Derivations of weighted kappa are sometimes expressed in terms of similarities, and sometimes in terms of dissimilarities. In the latter case, the weights on the diagonal are 1 and the weights off the diagonal are less than one. In this  case, if the weights are 1 - squared distance from the diagonal / k, then the result is similar to the ICC (for any positive k).  
</p>
<p>cohen.kappa may use either similarity weighting (diagonal = 0) or dissimilarity weighting (diagonal = 1) in order to match various published examples. 
</p>
<p>The input may be a two column data.frame or matrix with columns representing the two judges and rows the subjects being rated. Alternatively, the input may be a square n x n matrix of counts or proportion of matches.  If proportions are used, it is necessary to specify the number of observations (n.obs) in order to correctly find the confidence intervals.
</p>
<p>The confidence intervals are based upon the variance estimates discussed by Fleiss, Cohen, and Everitt who corrected the formulae of Cohen (1968) and Blashfield.
</p>
<p>Some data sets will include data with numeric categories with some category values missing completely.  In the sense that kappa is a measure of category relationship, this should not matter.  But when finding weighted kappa, the number of categories weighted will be less than the number of categories potentially in the data.  This can be remedied by specifying the levels parameter.  This is a vector of the levels potentially in the data (even if some are missing).   See the examples.
</p>
<p>If there are more than 2 raters, then the average of all raters is known as Light's kappa. (Conger, 1980). 
</p>
<p>Following a request from Julius Pfadt, the weights matrix may be formed by squared distances (the default) or linear distances (w.exp=1)
</p>


<h3>Value</h3>

<table>
<tr><td><code>kappa</code></td>
<td>
<p>Unweighted kappa</p>
</td></tr>
<tr><td><code>weighted.kappa</code></td>
<td>
<p>The default weights are quadratric.</p>
</td></tr>
<tr><td><code>var.kappa</code></td>
<td>
<p>Variance of kappa</p>
</td></tr>
<tr><td><code>var.weighted</code></td>
<td>
<p>Variance of weighted kappa</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code>weight</code></td>
<td>
<p>The weights used in the estimation of weighted kappa</p>
</td></tr>
<tr><td><code>confid</code></td>
<td>
<p>The alpha/2 confidence intervals for unweighted and weighted kappa</p>
</td></tr>
<tr><td><code>plevel</code></td>
<td>
<p>The alpha level used in determining the confidence limits</p>
</td></tr>
</table>


<h3>Note</h3>

<p>As is true of many R functions, there are alternatives in other packages.  The Kappa function in the vcd package estimates unweighted and weighted kappa and reports the variance of the estimate. The input is a square matrix. The ckappa and wkappa functions in the psy package take raw data matrices. The kappam.light function from the irr package finds Light's average kappa.
</p>
<p>To avoid confusion with Kappa (from vcd) or the kappa function from base, the function was originally named wkappa. With additional features modified from psy::ckappa to allow input with a different number of categories, the function has been renamed cohen.kappa.  
</p>
<p>Unfortunately, to make it more confusing, the weights described by Cohen are a function of the reciprocals of those discucssed by Fleiss and Cohen. The cohen.kappa function uses the appropriate formula for Cohen or Fleiss-Cohen weights. 
</p>
<p>There are some cases where the large sample size approximation of Fleiss et al. will produce confidence intervals exceeding +/- 1.  Clearly, for these cases, the upper (or lower for negative values) should be set to 1.  Boot strap resampling shows the problem is that the values are not symmetric.  See the last (unrun) example.
</p>
<p>It is also possible to have more than 2 raters.  In this case, cohen.kappa is reported for all pairs of raters (e.g. R1 and R2, R1 and R3,  ... R3 and R4).  To see the confidence intervals for these cohen.kappas, use the print command with the all=TRUE option. (See the exmaple of multiple raters.)
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Banerjee, M., Capozzoli, M., McSweeney, L and Sinha, D. (1999) Beyond Kappa: A review of interrater agreement measures The Canadian Journal of Statistics / La Revue Canadienne de Statistique, 27, 3-23
</p>
<p>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20 37-46
</p>
<p>Cohen, J. (1968). Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological Bulletin, 70, 213-220.
</p>
<p>Conger, A. J. (1980) Integration and generalization of kappas for multiple raters, Psychological Bulletin,, 88, 322-328.
</p>
<p>Fleiss, J. L., Cohen, J.  and Everitt, B.S. (1969) Large sample standard errors of kappa and weighted kappa. Psychological Bulletin, 72, 332-327.
</p>
<p>Light, R. J. (12971) Measures of response agreement for qualitative data: Some generalizations and alternatives, Psychological Bulletin, 76, 365-377.
</p>
<p>Zwick, R.  (1988) Another look at interrater agreement. Psychological Bulletin, 103, 374 - 378.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#rating data (with thanks to Tim Bates)
rater1 = c(1,2,3,4,5,6,7,8,9) # rater one's ratings
rater2 = c(1,3,1,6,1,5,5,6,7) # rater two's ratings
cohen.kappa(x=cbind(rater1,rater2))

#data matrix taken from Cohen
cohen &lt;- matrix(c(
0.44, 0.07, 0.09,
0.05, 0.20, 0.05,
0.01, 0.03, 0.06),ncol=3,byrow=TRUE)

#cohen.weights  weight differences
cohen.weights &lt;- matrix(c(
0,1,3,
1,0,6,
3,6,0),ncol=3)


cohen.kappa(cohen,cohen.weights,n.obs=200)
#cohen reports .492 and .348 

#another set of weights
#what if the weights are non-symmetric
wc &lt;- matrix(c(
0,1,4,
1,0,6,
2,2,0),ncol=3,byrow=TRUE)
cohen.kappa(cohen,wc)  
#Cohen reports kw = .353

cohen.kappa(cohen,n.obs=200)  #this uses the squared weights

fleiss.cohen &lt;- 1 - cohen.weights/9
cohen.kappa(cohen,fleiss.cohen,n.obs=200)

#however, Fleiss, Cohen and Everitt weight similarities
fleiss &lt;- matrix(c(
106, 10,4,
22,28, 10,
2, 12,  6),ncol=3,byrow=TRUE)

#Fleiss weights the similarities
weights &lt;- matrix(c(
 1.0000, 0.0000, 0.4444,
 0.0000, 1.0000, 0.6667,
 0.4444, 0.6667, 1.0000),ncol=3)
 
 cohen.kappa(fleiss,weights,n.obs=200)
 
 #another example is comparing the scores of two sets of twins
 #data may be a 2 column matrix
 #compare weighted and unweighted
 #also look at the ICC for this data set.
 twins &lt;- matrix(c(
    1, 2, 
    2, 3,
    3, 4,
    5, 6,
    6, 7), ncol=2,byrow=TRUE)
  cohen.kappa(twins)
  
#data may be explicitly categorical
x &lt;- c("red","yellow","blue","red")
y &lt;- c("red",  "blue", "blue" ,"red") 
xy.df &lt;- data.frame(x,y)
ck &lt;- cohen.kappa(xy.df)
ck
ck$agree

#Example for specifying levels
#The problem of missing categories (from Amy Finnegan)
#We need to specify all the categories possible using the levels option
numbers &lt;- data.frame(rater1=c(6,3,7,8,7),
                      rater2=c(6,1,8,5,10))
cohen.kappa(numbers)  #compare with the next analysis
cohen.kappa(numbers,levels=1:10)  #specify the number of levels 
              #   these leads to slightly higher weighted kappa
  
#finally, input can be a data.frame of ratings from more than two raters
ratings &lt;- matrix(rep(1:5,4),ncol=4)
ratings[1,2] &lt;- ratings[2,3] &lt;- ratings[3,4] &lt;- NA
ratings[2,1] &lt;- ratings[3,2] &lt;- ratings[4,3] &lt;- 1
ck &lt;- cohen.kappa(ratings)
ck  #just show the raw and weighted kappas
print(ck, all=TRUE)  #show the confidence intervals as well

 
 #In the case of confidence intervals being artificially truncated to +/- 1, it is 
 #helpful to compare the results of a boot strap resample
 #ck.boot &lt;-function(x,s=1:nrow(x)) {cohen.kappa(x[s,])$kappa}
 #library(boot)
 #ckb &lt;- boot(x,ck.boot,R=1000)
 #hist(ckb$t)
 
</code></pre>

<hr>
<h2 id='comorbidity'> Convert base rates of two diagnoses and their comorbidity into phi, Yule, and tetrachorics </h2><span id='topic+comorbidity'></span>

<h3>Description</h3>

<p>In medicine and clinical psychology, diagnoses tend to be categorical (someone is depressed or not, someone has an anxiety disorder or not).  Cooccurrence  of both of these symptoms is called comorbidity.   Diagnostic categories vary in their degree of comorbidity with other diagnostic categories.  From the point of view of correlation, comorbidity is just a name applied to one cell in a four fold table.  It is thus possible to analyze comorbidity rates by considering the probability of the separate diagnoses and the probability of the joint diagnosis.  This gives the two by two table needed for a phi, Yule, or tetrachoric correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comorbidity(d1, d2, com, labels = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comorbidity_+3A_d1">d1</code></td>
<td>
<p>Proportion of diagnostic category 1</p>
</td></tr>
<tr><td><code id="comorbidity_+3A_d2">d2</code></td>
<td>
<p>Proportion of diganostic category 2 </p>
</td></tr>
<tr><td><code id="comorbidity_+3A_com">com</code></td>
<td>
<p>Proportion of comorbidity (diagnostic category 1 and 2) </p>
</td></tr>
<tr><td><code id="comorbidity_+3A_labels">labels</code></td>
<td>
<p>Names of categories 1 and 2</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>twobytwo</code></td>
<td>
<p>The two by two table implied by the input </p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>Phi coefficient of the two by two table</p>
</td></tr>
<tr><td><code>Yule</code></td>
<td>
<p>Yule coefficient of the two by two table</p>
</td></tr>
<tr><td><code>tetra</code></td>
<td>
<p>Tetrachoric coefficient of the two by two table</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>See Also</h3>

 <p><code><a href="#topic+phi">phi</a></code>, <code><a href="#topic+phi2tetra">phi2tetra</a></code> ,<code><a href="#topic+Yule">Yule</a></code>, <code><a href="#topic+Yule.inv">Yule.inv</a></code> <code><a href="#topic+Yule2phi">Yule2phi</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code> and <code><a href="#topic+polychoric">polychoric</a></code>, as well as <code><a href="#topic+AUC">AUC</a></code> for graphical displays</p>


<h3>Examples</h3>

<pre><code class='language-R'>comorbidity(.2,.15,.1,c("Anxiety","Depression")) 
</code></pre>

<hr>
<h2 id='congruence'>Matrix and profile congruences and distances</h2><span id='topic+congruence'></span><span id='topic+cohen.profile'></span><span id='topic+distance'></span>

<h3>Description</h3>

<p>The congruence coefficient two matrices is just the cross product of their respective values divided by the square root of their sums of squares. If the columns are zero centered, this is just the correlation. If the columns are centered around the scale neutral point, this is Cohen's profile correlation. A set of distances (city block, euclidean, Minkowski) may be found by the distance function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>congruence(x,y=NULL)
cohen.profile(x,y=NULL ,M=NULL)
distance(x,y=NULL,r=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="congruence_+3A_x">x</code></td>
<td>
<p> A matrix of factor loadings or a list of matrices of factor loadings</p>
</td></tr>
<tr><td><code id="congruence_+3A_y">y</code></td>
<td>
<p> A second matrix of factor loadings (if x is a list, then y may be empty)</p>
</td></tr>
<tr><td><code id="congruence_+3A_m">M</code></td>
<td>
<p>The midpoint of the items which should be used for reflection. NULL if to be found from the data. </p>
</td></tr>
<tr><td><code id="congruence_+3A_r">r</code></td>
<td>
<p>The exponent for the generalized distance. (r=1 is city block, r=2 is euclidian, r=100 or larger emphasize the largest distance )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Congruences are the cosines of pairs of vectors defined by a matrix and based at the origin.  Thus, for values that differ only by a scaler the congruence will be 1.
</p>
<p>For two matrices, F1 and F2, the measure of  congruence, phi, is 
</p>
<p style="text-align: center;"><code class="reqn">
\phi = \frac{\sum F_1 F_2}{\sqrt{\sum(F_1^2)\sum(F_2^2)}} 
.</code>
</p>

<p>It is an interesting exercise to compare  congruences with the correlations of the respective values. Congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of items are cosines of the vectors based at the mean loading for each column.   
</p>
<p style="text-align: center;"><code class="reqn">
\phi = \frac{\sum (F_1-a) (F_2 - b)}{\sqrt{\sum((F_1-a)^2)\sum((F_2-b)^2)}} 
.</code>
</p>
<p>.
</p>
<p>For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.
</p>
<p>Input may either be one or two matrices.
</p>
<p>For congruences of factor or component loading matrices, use <code><a href="#topic+factor.congruence">factor.congruence</a></code>.
</p>
<p>Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.
</p>
<p>If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.
</p>
<p><code><a href="#topic+cohen.profile">cohen.profile</a></code> applies the <code><a href="#topic+congruence">congruence</a></code> function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). 
</p>
<p><code><a href="#topic+distance">distance</a></code> finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r &gt;2).
</p>


<h3>Value</h3>

<p>A matrix of  congruences or distances.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Burt, Cyril (1948) Methods of factor-analysis with and without successive approximation.  British Journal of Educational Psychology, 7 (2) 172-195.
</p>
<p>Burt, Cyril (1948) The factorial study of temperamental traits. British Journal of Statistical Psychology, 1(3) 178-203.
</p>
<p>Cohen, Jacob (1969), rc: A profile similarity coefficient invariant over variable reflection. Psychological Bulletin, 71 (4) 281-284.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+factor.congruence">factor.congruence</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#cohen's example
# a and b have reversed one item around the midpoint
co &lt;-  data.frame(ira=c(2,6,5,6,4),
       jim=c(1,3,5,4,4),
      a=c(5,6,5,6,4),b=c(6,3,5,4,4))

lowerMat(congruence(co-3.5)) # congruence around the midpoint is insensitive to reflection
lowerCor(co)   #the correlation is not
lowerMat(congruence(scale(co,scale=FALSE))) #zero centered congruence is  r
cohen.profile(co)
</code></pre>

<hr>
<h2 id='cor.smooth'>Smooth a non-positive definite correlation matrix to make it positive definite</h2><span id='topic+cor.smooth'></span><span id='topic+cor.smoother'></span>

<h3>Description</h3>

<p>Factor analysis requires positive definite correlation matrices.  Unfortunately, with pairwise deletion of missing data or if using <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> correlations, not all correlation matrices are positive definite.  cor.smooth does a eigenvector (principal components) smoothing.  Negative eigen values are replaced with 100  * eig.tol, the matrix is reproduced and forced to a correlation matrix using cov2cor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.smooth(x,eig.tol=10^-12)
cor.smoother(x,cut=.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor.smooth_+3A_x">x</code></td>
<td>
<p>A correlation matrix or a raw data matrix.</p>
</td></tr>
<tr><td><code id="cor.smooth_+3A_eig.tol">eig.tol</code></td>
<td>
<p>the minimum acceptable eigenvalue</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="cor.smooth_+3A_cut">cut</code></td>
<td>
<p>Report all abs(residuals)  &gt; cut</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The smoothing is done by eigen value decomposition.  eigen values &lt; eig.tol are changed to 100  * eig.tol.  The positive eigen values are rescaled to sum to the number of items.  The matrix is recomputed (eigen.vectors %*% diag(eigen.values) %*% t(eigen.vectors) and forced to a correlation matrix using cov2cor. (See Bock, Gibbons and Muraki, 1988 and Wothke, 1993). 
</p>
<p>This does not implement the Knol and ten Berge (1989) solution, nor do nearcor and posdefify in sfmsmisc, not does nearPD in Matrix. As Martin Maechler puts it in the posdedify function, &quot;there are more sophisticated algorithms to solve this and related problems.&quot;  
</p>
<p>cor.smoother examines all of nvar minors of rank nvar-1 by systematically dropping one variable at a time and finding the eigen value decomposition.  It reports those variables, which, when dropped, produce a positive definite matrix.  It also reports the number of negative eigenvalues when each variable is dropped.  Finally, it compares the original correlation matrix to the smoothed correlation matrix and reports those items with absolute deviations great than cut.  These are all hints as to what might be wrong with a correlation matrix.  
</p>


<h3>Value</h3>

<p>The smoothed matrix with a warning reporting that smoothing was necessary (if smoothing was in fact necessary).
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>R. Darrell Bock, Robert Gibbons and Eiji Muraki (1988) Full-Information Item Factor Analysis. Applied Psychological Measurement, 12 (3), 261-280.
</p>
<p>Werner Wothke (1993), Nonpositive definite matrices in structural modeling. In Kenneth A. Bollen and J. Scott Long (Editors),Testing structural equation models, Sage Publications, Newbury Park. 
</p>
<p>D.L. Knol and JMF ten Berge (1989) Least squares approximation of an improper correlation matrix by a proper one.  Psychometrika, 54, 53-61.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+polychoric">polychoric</a></code>, <code><a href="#topic+fa">fa</a></code> and <code><a href="#topic+irt.fa">irt.fa</a></code>, and the <code><a href="psychTools.html#topic+burt">burt</a></code> data set.
</p>
<p>See also nearcor and posdefify in the sfsmisc package and nearPD in the Matrix package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>burt &lt;- psychTools::burt
bs &lt;- cor.smooth(psychTools::burt)  #burt data set is not positive definite
plot(burt[lower.tri(burt)],bs[lower.tri(bs)],ylab="smoothed values",xlab="original values")
abline(0,1,lty="dashed")

round(burt - bs,3) 
fa(burt,2) #this throws a warning that the matrix yields an improper solution
#Smoothing first throws a warning that the matrix was improper, 
#but produces a better solution 
fa(cor.smooth(burt),2)  

#this next example is a correlation matrix from DeLeuw used as an example 
#in Knol and ten Berge.  
#the example is also used in the nearcor documentation
 cat("pr is the example matrix used in Knol DL, ten Berge (1989)\n")
 pr &lt;- matrix(c(1,     0.477, 0.644, 0.478, 0.651, 0.826,
		0.477, 1,     0.516, 0.233, 0.682, 0.75,
		0.644, 0.516, 1,     0.599, 0.581, 0.742,
		0.478, 0.233, 0.599, 1,     0.741, 0.8,
		0.651, 0.682, 0.581, 0.741, 1,     0.798,
		0.826, 0.75,  0.742, 0.8,   0.798, 1),
	      nrow = 6, ncol = 6)
	      
sm &lt;- cor.smooth(pr)
resid &lt;- pr - sm
# several goodness of fit tests
# from Knol and ten Berge
tr(resid %*% t(resid)) /2

# from nearPD
sum(resid^2)/2

</code></pre>

<hr>
<h2 id='cor.wt'>The sample size weighted correlation may be used in correlating aggregated data</h2><span id='topic+cor.wt'></span>

<h3>Description</h3>

<p>If using aggregated data, the correlation of the means does not reflect the sample size used for each mean. cov.wt in RCore does this and returns a covariance matrix or the correlation matrix.  The cor.wt function weights by sample size or by standard errors and by default return correlations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.wt(data,vars=NULL, w=NULL,sds=NULL, cor=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor.wt_+3A_data">data</code></td>
<td>
<p>A matrix or data frame</p>
</td></tr>
<tr><td><code id="cor.wt_+3A_vars">vars</code></td>
<td>
<p>Variables to analyze</p>
</td></tr>
<tr><td><code id="cor.wt_+3A_w">w</code></td>
<td>
<p>A set of weights (e.g., the sample sizes)</p>
</td></tr>
<tr><td><code id="cor.wt_+3A_sds">sds</code></td>
<td>
<p>Standard deviations of the samples (used if weighting by standard errors)</p>
</td></tr>
<tr><td><code id="cor.wt_+3A_cor">cor</code></td>
<td>
<p>Report correlations (the default) or covariances</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A weighted correlation is just <code class="reqn"> r_{ij} = \frac{\sum(wt_{k} (x_{ik} - x_{jk})}{\sqrt{wt_{ik} \sum(x_{ik}^2) wt_jk \sum(x_{jk}^2)}}    </code>  where <code class="reqn">x_{ik}</code> is a deviation from the weighted mean.  
</p>
<p>The weighted correlation is appropriate for correlating aggregated data, where individual data points might reflect the means of a number of observations.  In this case, each point is weighted by its sample size (or alternatively, by the standard error).  If the weights are all equal, the correlation is just a normal Pearson correlation. 
</p>
<p>Used when finding correlations of group means found using <code><a href="#topic+statsBy">statsBy</a></code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cor</code></td>
<td>
<p>The weighted correlation</p>
</td></tr>
<tr><td><code>xwt</code></td>
<td>
<p>The data as weighted deviations from the weighted mean </p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>The weights used (calculated from the sample sizes).</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>The weighted means</p>
</td></tr>
<tr><td><code>xc</code></td>
<td>
<p>Unweighted, centered deviation scores from the weighted mean</p>
</td></tr>
<tr><td><code>xs</code></td>
<td>
<p>Deviation scores weighted by the standard error of each sample mean</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A generalization of <code><a href="stats.html#topic+cov.wt">cov.wt</a></code> in core R</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="stats.html#topic+cov.wt">cov.wt</a></code>, <code><a href="#topic+statsBy">statsBy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>means.by.age &lt;- statsBy(sat.act,"age")
wt.cors &lt;- cor.wt(means.by.age)
lowerMat(wt.cors$r)  #show the weighted correlations
unwt &lt;- lowerCor(means.by.age$mean)
mixed &lt;- lowerUpper(unwt,wt.cors$r)  #combine both results
cor.plot(mixed,TRUE,main="weighted versus unweighted correlations")
diff &lt;- lowerUpper(unwt,wt.cors$r,TRUE)
cor.plot(diff,TRUE,main="differences of weighted versus unweighted correlations")
</code></pre>

<hr>
<h2 id='cor2dist'>Convert correlations to distances (necessary to do multidimensional scaling of correlation data)</h2><span id='topic+cor2dist'></span>

<h3>Description</h3>

<p>A minor helper function to convert correlations (ranging from -1 to 1) to distances (ranging from 0 to 2). <code class="reqn">d = \sqrt{(2(1-r))}</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor2dist(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor2dist_+3A_x">x</code></td>
<td>
<p>If square, then assumed to be a correlation matrix, otherwise the correlations are found first.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dist: a square matrix of distances.
</p>


<h3>Note</h3>

<p>For an example of doing multidimensional scaling on data that are normally factored, see Revelle (in prep)
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>

<hr>
<h2 id='corCi'>Bootstrapped and normal confidence intervals for raw and composite correlations</h2><span id='topic+corCi'></span><span id='topic+cor.ci'></span>

<h3>Description</h3>

<p>Although normal theory provides confidence intervals for correlations, this is particularly problematic with Synthetic Aperture Personality Assessment (SAPA) data where the individual items are Massively Missing at Random.  Bootstrapped confidence intervals are found for Pearson, Spearman, Kendall, tetrachoric, or polychoric correlations and for scales made from those correlations. If given a correlation matrix and sample size(s), normal theory confidence intervals are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corCi(x, keys = NULL, n.iter = 100,  p = 0.05,overlap = FALSE, 
 poly = FALSE, method = "pearson", plot=TRUE,minlength=5,n=NULL,...)
 
cor.ci(x, keys = NULL, n.iter = 100,  p = 0.05,overlap = FALSE, 
 poly = FALSE, method = "pearson", plot=TRUE,minlength=5,n=NULL,...)
 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corCi_+3A_x">x</code></td>
<td>
<p>The raw data, or a correlation matrix if not doing bootstrapping</p>
</td></tr>
<tr><td><code id="corCi_+3A_keys">keys</code></td>
<td>
<p>If NULL, then the confidence intervals of the raw correlations are found.  Otherwise, composite scales are formed from the keys applied to the correlation matrix (in a logic similar to <code><a href="#topic+cluster.cor">cluster.cor</a></code> but without the bells and whistles) and the confidence of those composite scales intercorrelations. </p>
</td></tr>
<tr><td><code id="corCi_+3A_n.iter">n.iter</code></td>
<td>
<p>The number of iterations to bootstrap over. This will be very slow if using tetrachoric/or polychoric correlations. </p>
</td></tr>
<tr><td><code id="corCi_+3A_p">p</code></td>
<td>
<p>The upper and lower confidence region will include 1-p of the distribution.</p>
</td></tr>
<tr><td><code id="corCi_+3A_overlap">overlap</code></td>
<td>
<p>If true, the correlation between overlapping scales is corrected for item overlap.</p>
</td></tr>
<tr><td><code id="corCi_+3A_poly">poly</code></td>
<td>
<p>if FALSE, then find the correlations using the method specified (defaults to Pearson).  If TRUE, the polychoric correlations will be found (slowly).  Because the polychoric function uses multicores (if available), and corCi does as well, the number of cores used is options(&quot;mc.cores&quot;)^2. </p>
</td></tr>
<tr><td><code id="corCi_+3A_method">method</code></td>
<td>
<p>&quot;pearson&quot;,&quot;spearman&quot;, &quot;kendall&quot;</p>
</td></tr>
<tr><td><code id="corCi_+3A_plot">plot</code></td>
<td>
<p>Show the correlation plot with correlations scaled by the probability values.  To show the matrix in terms of the confidence intervals, use <code><a href="#topic+cor.plot.upperLowerCi">cor.plot.upperLowerCi</a></code>.</p>
</td></tr>
<tr><td><code id="corCi_+3A_minlength">minlength</code></td>
<td>
<p>What is the minlength to use in abbreviations of the cis? Defaults to 5</p>
</td></tr>
<tr><td><code id="corCi_+3A_n">n</code></td>
<td>
<p>If finding confidence intervals from a correlation matrix, specify the n</p>
</td></tr>
<tr><td><code id="corCi_+3A_...">...</code></td>
<td>
<p>Other parameters for axis (e.g., cex.axis to change the font size, srt to rotate the numbers in the plot)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If given a correlation matrix, then confidence intervals are found based upon the sample sizes using the conventional r2z fisher transformation (<code><a href="#topic+fisherz">fisherz</a></code> and the normal distribution.
</p>
<p>If given raw data, correlations are found.  If keys are specified (the normal case), then composite scales based upon the correlations are found and reported.  This is the same procedure as done using <code><a href="#topic+cluster.cor">cluster.cor</a></code> or <code><a href="#topic+scoreItems">scoreItems</a></code>.
</p>
<p>Then (with raw data) the data are recreated n.iter times by sampling subjects (rows) with replacement and the correlations (and composite scales) are found again (and again and again).  Mean and standard deviations of these values are calculated based upon the Fisher Z transform of the correlations.  Summary statistics include the original correlations and their confidence intervals.  For those who want the complete set of replications, those are available as an object in the resulting output.
</p>
<p>Although particularly useful for SAPA (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) type data where we have lots of missing data, this will work for any normal data set as well. 
</p>
<p>Although the correlations are shown automatically as a <code><a href="#topic+cor.plot">cor.plot</a></code>, it is possible to show the upper and lower confidence intervals by using <code><a href="#topic+cor.plot.upperLowerCi">cor.plot.upperLowerCi</a></code>. This will also return, invisibly, a matrix for printing with the lower and upper bounds of the correlations shown below and above the diagonal (see the first example).
</p>


<h3>Value</h3>

<table>
<tr><td><code>rho</code></td>
<td>
<p>The original (composite) correlation matrix. </p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>Mean (of Fisher transformed) correlation retransformed back to the r units</p>
</td></tr>
<tr><td><code>sds</code></td>
<td>
<p>Standard deviation of Fisher transformed correlations</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Mean +/- alpha/2 of the z scores as well as the alpha/2 and 1-alpha/2 quantiles. These are labeled as lower.emp(ircal), lower.norm(al), upper.norm and upper.emp.</p>
</td></tr>
<tr><td><code>replicates</code></td>
<td>
<p>The observed replication values so one can do one's own estimates</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>For SAPA type data, see Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Personality and Cognition: The Personality-Cognition Link. In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.keys">make.keys</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code>, and <code><a href="#topic+scoreItems">scoreItems</a></code> for forming synthetic correlation matrices from composites of item correlations.  See <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> for correcting for item overlap in scales. See also <code><a href="#topic+corr.test">corr.test</a></code> for standard significance testing of correlation matrices.  See also <code><a href="#topic+lowerCor">lowerCor</a></code> for finding and printing correlation matrices, as well as <code><a href="#topic+lowerMat">lowerMat</a></code> for displaying them. Also see <code><a href="#topic+cor.plot.upperLowerCi">cor.plot.upperLowerCi</a></code> for displaying the confidence intervals graphically.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#find confidence intervals of a correlation matrix with specified sample size
ci &lt;- corCi(Thurstone[1:6,1:6],n=213)
ci  #show them
R &lt;- cor.plot.upperLowerCi(ci)  #show them graphically
R #show them as a matrix 


#confidence intervals by bootstrapping requires raw data
corCi(psychTools::bfi[1:200,1:10])  # just the first 10 variables
#The keys have overlapping scales
keys &lt;- list(agree=c("-A1","A2","A3","A4","A5"), conscientious= c("C1", 
  "C2","C3","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"), neuroticism= 
  c("N1", "N2", "N3","N4","N5"), openness = c("O1","-O2","O3","O4","-O5"), 
  alpha=c("-A1","A2","A3","A4","A5","C1","C2","C3","-C4","-C5","N1","N2","N3","N4","N5"),
beta = c("-E1","-E2","E3","E4","E5","O1","-O2","O3","O4","-O5") )

  
#do not correct for item overlap
rci &lt;-  corCi(psychTools::bfi[1:200,],keys,n.iter=10,main="correlation with overlapping scales") 
#also shows the graphic -note the overlap
#correct for overlap
rci &lt;-  cor.ci(psychTools::bfi[1:200,],keys,overlap=TRUE, n.iter=10,main="Correct for overlap") 
#show the confidence intervals
ci &lt;- cor.plot.upperLowerCi(rci)  #to show the upper and lower confidence intervals
ci   #print the confidence intervals in matrix form
</code></pre>

<hr>
<h2 id='corFiml'>Find a Full Information Maximum Likelihood (FIML) correlation or covariance matrix from a data matrix with missing data </h2><span id='topic+corFiml'></span>

<h3>Description</h3>

<p>Makes use of functions adapted from the lavaan package to find FIML covariance/correlation matrices.  FIML can be much slower than the normal pairwise deletion option of cor, but provides slightly more precise estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corFiml(x, covar = FALSE,show=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corFiml_+3A_x">x</code></td>
<td>
<p>A data.frame or data matrix</p>
</td></tr>
<tr><td><code id="corFiml_+3A_covar">covar</code></td>
<td>
<p>By default, just return the correlation matrix.  If covar is TRUE, return a list containing the covariance matrix and the ML fit function.</p>
</td></tr>
<tr><td><code id="corFiml_+3A_show">show</code></td>
<td>
<p>If show=TRUE, then just show the patterns of missingness, but don't do the FIML.  Useful for understanding the process of fiml.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the presence of missing data, Full Information Maximum Likelihood (FIML) is an alternative to simply using the pairwise correlations. The implementation in the lavaan package for structural equation modeling has been adapted for the simpler case of just finding the correlations or covariances.  
</p>
<p>The pairwise solution for any pair of variables is insensitive to other variables included in the matrix.  On the other hand, the ML solution depends upon the entire set of items being correlated.  This will lead to slightly different solutions for different subsets of variables.  
</p>
<p>The basic FIML algorithm is to find the pairwise ML solution for covariances and means for every pattern of missingness and then to weight the solution by the size of every unique pattern of missingness.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>cor</code></td>
<td>
<p>The correlation matrix found using FIML</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p>The covariance matrix found using FIML</p>
</td></tr>
<tr><td><code>fx</code></td>
<td>
<p>The ML fit function</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The functions used in lavaan are not exported and so have been copied (and simplified) to the psych package.
</p>


<h3>Author(s)</h3>

<p>Wiliam Revelle</p>


<h3>See Also</h3>

<p> To use the resulting correlations, see <code><a href="#topic+fa">fa</a></code>.  To see the pairwise pattern of missingness, see <code><a href="#topic+count.pairwise">count.pairwise</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
rML &lt;- corFiml(psychTools::bfi[20:27])
rpw &lt;- cor(psychTools::bfi[20:27],use="pairwise") 
round(rML - rpw,3)
mp &lt;- corFiml(psychTools::bfi[20:27],show=TRUE)
mp
</code></pre>

<hr>
<h2 id='corPlot'>Create an image plot for a correlation or factor matrix</h2><span id='topic+cor.plot'></span><span id='topic+corPlot'></span><span id='topic+corPlotUpperLowerCi'></span><span id='topic+cor.plot.upperLowerCi'></span>

<h3>Description</h3>

<p>Correlation matrices may be shown graphically by using the image function to emphasize structure.  This is a particularly useful tool for showing the structure of  correlation matrices with a clear structure.  Partially meant for the pedagogical value of the graphic for teaching or discussing factor analysis and other multivariate techniques. The sort option uses iclust to sort the matrix before plotting. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corPlot(r,numbers=TRUE,colors=TRUE,n=51,main=NULL,zlim=c(-1,1),
  show.legend=TRUE, labels=NULL,n.legend=10,keep.par=TRUE,select=NULL, pval=NULL, 
  digits=2, trailing=TRUE, cuts=c(.001,.01),scale=TRUE,cex,MAR,upper=TRUE,diag=TRUE, 
  symmetric=TRUE,stars=FALSE, adjust="holm",xaxis=1, xlas=0, ylas=2,ysrt=0,xsrt=0, 
   gr=NULL, alpha=.75,  min.length=NULL,sort=FALSE,n.obs=NULL, ...)

corPlotUpperLowerCi(R,numbers=TRUE,cuts=c(.001,.01,.05),select=NULL,
      main="Upper and lower confidence intervals of correlations",adjust=FALSE,...)
 
 cor.plot(r,numbers=TRUE,colors=TRUE,n=51,main=NULL,zlim=c(-1,1),
 show.legend=TRUE, labels=NULL,n.legend=10,keep.par=TRUE,select=NULL, pval=NULL, 
  digits=2, trailing=TRUE, cuts=c(.001,.01),scale=TRUE,cex,MAR,upper=TRUE,diag=TRUE, 
  symmetric=TRUE,stars=FALSE, adjust="holm",xaxis=1, xlas=0, ylas=2,ysrt=0,xsrt=0,
     gr=NULL, alpha=.75, min.length=NULL, sort=FALSE, n.obs=NULL,...)     #deprecated
           
cor.plot.upperLowerCi(R,numbers=TRUE,cuts=c(.001,.01,.05),select=NULL,
      main="Upper and lower confidence intervals of correlations",adjust=FALSE,...)
      #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corPlot_+3A_r">r</code></td>
<td>
<p>A correlation matrix or the output of  <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code> or <code><a href="#topic+omega">omega</a></code>, or a raw data matrix. </p>
</td></tr>
<tr><td><code id="corPlot_+3A_r">R</code></td>
<td>
<p>The object returned from  <code><a href="#topic+cor.ci">cor.ci</a></code>  </p>
</td></tr>
<tr><td><code id="corPlot_+3A_numbers">numbers</code></td>
<td>
<p>Display the numeric value of the correlations. (As of September, 2019)  Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_colors">colors</code></td>
<td>
<p>Defaults to TRUE and colors use colors from the colorRampPalette from red through white to blue, but colors=FALSE will use a grey scale</p>
</td></tr>
<tr><td><code id="corPlot_+3A_n">n</code></td>
<td>
<p>The number of levels of shading to use.  Defaults to 51</p>
</td></tr>
<tr><td><code id="corPlot_+3A_main">main</code></td>
<td>
<p>A title. Defaults to &quot;correlation plot&quot;</p>
</td></tr>
<tr><td><code id="corPlot_+3A_zlim">zlim</code></td>
<td>
<p>The range of values to color &ndash; defaults to -1 to 1.  If specified as NULL, then defaults to min and max observed correlation.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_show.legend">show.legend</code></td>
<td>
<p>A legend (key) to the colors is shown on the right hand side</p>
</td></tr>
<tr><td><code id="corPlot_+3A_labels">labels</code></td>
<td>
<p>if NULL, use column and row names, otherwise use labels</p>
</td></tr>
<tr><td><code id="corPlot_+3A_n.legend">n.legend</code></td>
<td>
<p>How many categories should be labelled in the legend?</p>
</td></tr>
<tr><td><code id="corPlot_+3A_sort">sort</code></td>
<td>
<p>If true, then sort the variables using the iclust algorithm</p>
</td></tr>
<tr><td><code id="corPlot_+3A_keep.par">keep.par</code></td>
<td>
<p>restore the graphic parameters when exiting</p>
</td></tr>
<tr><td><code id="corPlot_+3A_pval">pval</code></td>
<td>
<p>scale the numbers by their pvals, categorizing them based upon the values of cuts</p>
</td></tr>
<tr><td><code id="corPlot_+3A_digits">digits</code></td>
<td>
<p>Round off to digits.  Defaults to 2.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_trailing">trailing</code></td>
<td>
<p>Show trailing zeros.  </p>
</td></tr>
<tr><td><code id="corPlot_+3A_cuts">cuts</code></td>
<td>
<p>Scale the numbers by the categories defined by pval &lt; cuts</p>
</td></tr>
<tr><td><code id="corPlot_+3A_scale">scale</code></td>
<td>
<p>Should the size of the numbers be scaled by the significance level?</p>
</td></tr>
<tr><td><code id="corPlot_+3A_select">select</code></td>
<td>
<p>Select the subset of variables to plot</p>
</td></tr>
<tr><td><code id="corPlot_+3A_cex">cex</code></td>
<td>
<p>Character size.  Should be reduced a bit for large numbers of variables.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_mar">MAR</code></td>
<td>
<p>Allows for adjustment .of the margins if using really long labels or big fonts</p>
</td></tr>
<tr><td><code id="corPlot_+3A_upper">upper</code></td>
<td>
<p>Should the upper off diagonal matrix be drawn, or left blank?</p>
</td></tr>
<tr><td><code id="corPlot_+3A_diag">diag</code></td>
<td>
<p>Should we show the diagonal?</p>
</td></tr>
<tr><td><code id="corPlot_+3A_symmetric">symmetric</code></td>
<td>
<p>By default, if given a non-symmetric matrix, we find the correlations using pair.wise complete and then show them.  If wanting to display a non-symmetric matrix, then specify that symmetric is FALSE</p>
</td></tr>
<tr><td><code id="corPlot_+3A_stars">stars</code></td>
<td>
<p>For those people who like to show the 'significance' of correlations by using magic astricks, set stars=TRUE</p>
</td></tr>
<tr><td><code id="corPlot_+3A_n.obs">n.obs</code></td>
<td>
<p>If you want to show &quot;stars&quot; for symmetric input matrices (i.e. correlations), specify the number of observations</p>
</td></tr>
<tr><td><code id="corPlot_+3A_adjust">adjust</code></td>
<td>
<p>If showing significance, should we adjust for multiple tests?  The default is to show zero order probabilities below the diagonal and adjust these using the 'holm' correction above the diagonal. Use adjust = &quot;none&quot; if no adjustment is desired.
adjust is also used in corPlotUpperLowerCI to show the nominal alpha confidence intervals (adjust =FALSE) or the Bonferonni adjusted confidence intervals (adjust=TRUE).</p>
</td></tr>
<tr><td><code id="corPlot_+3A_xlas">xlas</code></td>
<td>
<p>Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)</p>
</td></tr>
<tr><td><code id="corPlot_+3A_ylas">ylas</code></td>
<td>
<p>Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)</p>
</td></tr>
<tr><td><code id="corPlot_+3A_ysrt">ysrt</code></td>
<td>
<p>Rotation of y labels in degrees</p>
</td></tr>
<tr><td><code id="corPlot_+3A_xsrt">xsrt</code></td>
<td>
<p>Rotation of x labels in degrees</p>
</td></tr>
<tr><td><code id="corPlot_+3A_xaxis">xaxis</code></td>
<td>
<p>By default, draw this below the figure.  If xaxis=3, then it wil be drawn above the figure</p>
</td></tr>
<tr><td><code id="corPlot_+3A_gr">gr</code></td>
<td>
<p>A color gradient: e.g.,  gr &lt;- colorRampPalette(c(&quot;#B52127&quot;, &quot;white&quot;, &quot;#2171B5&quot;))  will produce slightly more pleasing (to some) colors. See next to last example.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_alpha">alpha</code></td>
<td>
<p>The degree of transparency (0 = completely, 1= not). Default value of .75 makes somewhat moreor pleasing plots when using numbers.</p>
</td></tr>
<tr><td><code id="corPlot_+3A_min.length">min.length</code></td>
<td>
<p>If not NULL, then the maximum number of characters to use in 
row/column labels</p>
</td></tr>
<tr><td><code id="corPlot_+3A_...">...</code></td>
<td>
<p>Other parameters for axis (e.g., cex.axis to change the font size, srt to   
rotate the numbers in the plot)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When summarizing the correlations of large data bases or when teaching about factor analysis or cluster analysis, it is useful to graphically display the structure of correlation matrices.  This is a simple graphical display using the image function. 
</p>
<p>The difference between mat.plot with a regular image plot is that the primary diagonal goes from the top left to the lower right. 
zlim defines how to treat the range of possible values. -1 to 1 and the color choice is more reasonable.  Setting it as c(0,1) will lead to negative correlations  treated as zero.  This is advantageous when showing general factor structures, because it makes the 0 white.  
</p>
<p>There is an interesting case when plotting correlations corrected for attenuation.  Some of these might exceed 1.  In this case, either set zlim = NULL (to use the observed maximum and minimum values) or all values above 1 will be given a slightly darker shade than 1, but do not differ.
</p>
<p>The default shows a legend for the color coding on the right hand side of the figure.
</p>
<p>Inspired, in part, by a paper by S. Dray (2008)  on the number of components problem. 
</p>
<p>Modified following suggestions by David Condon and Josh Wilt to use a more meaningful color choice ranging from dark red (-1) through white (0) to dark blue (1). Further modified to allow for color choices using the gr option (suggested by Lorien Elleman). Further modified to include the numerical value of the correlation.  (Inspired by the corrplot package).  These values may be scaled according the the probability values found in <code><a href="#topic+cor.ci">cor.ci</a></code> or <code><a href="#topic+corTest">corTest</a></code>.
</p>
<p>Unless specified, the font size is dynamically scaled to have a cex =  10/max(nrow(r),ncol(r).  This can produce fairly small fonts for large problems.  
The font size of the labels may be adjusted using cex.axis which defaults to one.  
</p>
<p>By default <code><a href="#topic+cor.ci">cor.ci</a></code> calls corPlotUpperLowerCi and scales the correlations based upon &quot;significance&quot; values.  The correlations plotted are the upper and lower confidence boundaries.  To show the correlations themselves, call corPlot directly.
</p>
<p>If using the output of <code><a href="#topic+corTest">corTest</a></code>, the upper off diagonal will be scaled by the corrected probability, the lower off diagonal the scaling is the uncorrected probabilities.
</p>
<p>If given raw data or  correlation matrix, <code><a href="#topic+corPlotUpperLowerCi">corPlotUpperLowerCi</a></code> will automatically call <code><a href="#topic+corTest">corTest</a></code> or <code><a href="#topic+cor.ci">cor.ci</a></code>.  
</p>
<p>If using the output of <code><a href="#topic+corTest">corTest</a></code> or <code><a href="#topic+cor.ci">cor.ci</a></code> as input to <code><a href="#topic+corPlotUpperLowerCi">corPlotUpperLowerCi</a></code>, the upper off diagonal will be the upper bounds and the lower off diagonal the lower bounds of the confidence intervals.  If adjust=TRUE, these will use the Holm or Bonferroni adjusted values (depending upon corTest). 
</p>
<p>To compare the elements of two correlation matrices, <code><a href="#topic+corPlot">corPlot</a></code> the results from <code><a href="#topic+lowerUpper">lowerUpper</a></code>.  
</p>
<p>To do multiple <code><a href="#topic+corPlot">corPlot</a></code> on the same plot, specify that show.legend=FALSE and keep.par=FALSE.  See the last examples.  
</p>
<p>Care should be taken when selecting rows and columns from a non-symmetric matrix (e.g., the corrected correlations from <code><a href="#topic+scoreItems">scoreItems</a></code> or <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>).  
</p>
<p>To show a factor loading matrix (or any non-symmetric matrix), set symmetric=FALSE.  Otherwise the input will be treated as raw data and correlations will be found.
</p>
<p>The sort option will sort the matrix using the output from <code><a href="#topic+iclust">iclust</a></code>. To sort the matrix use another order, use  <code><a href="#topic+mat.sort">mat.sort</a></code> first.  To find correlations other than Pearson, plot the output from e.g., <code><a href="#topic+mixed.cor">mixed.cor</a></code>. 
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Dray, Stephane (2008) On the number of principal components: A test of dimensionality based on measurements of similarity between matrices. Computational Statistics &amp; Data Analysis. 52, 4, 2228-2237.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+mat.sort">mat.sort</a></code>, <code><a href="#topic+cor.ci">cor.ci</a></code>, <code><a href="#topic+corTest">corTest</a></code> <code><a href="#topic+lowerUpper">lowerUpper</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corPlot(Thurstone,main="9 cognitive variables from Thurstone") 
#just blue implies positive manifold
#select just some variables to plot
corPlot(Thurstone, zlim=c(0,1),main="9 cognitive variables from Thurstone",select=c(1:3,7:9))
#now show a non-symmetric plot
corPlot(Thurstone[4:9,1:3], zlim=c(0,1),main="9 cognitive variables
 from Thurstone",numbers=TRUE,symmetric=FALSE)

#Two ways of including stars to show significance
#From the raw data
corPlot(sat.act,numbers=TRUE,stars=TRUE)
#from a correlation matrix with pvals
cp &lt;- corTest(sat.act)  #find the correlations and pvals
r&lt;- cp$r
p &lt;- cp$p
corPlot(r,numbers=TRUE,diag=FALSE,stars=TRUE, pval = p,main="Correlation plot
 with Holm corrected 'significance'")

#now red means less than .5
corPlot(mat.sort(Thurstone),TRUE,zlim=c(0,1), 
       main="9 cognitive variables from Thurstone (sorted by factor loading) ")
simp &lt;- sim.circ(24)
corPlot(cor(simp),main="24 variables in a circumplex")

#scale by raw and adjusted probabilities
rs &lt;- corTest(sat.act[1:200,] ) #find the probabilities of the correlations
corPlot(r=rs$r,numbers=TRUE,pval=rs$p,main="Correlations scaled by probability values") 
 #Show the upper and lower confidence intervals
corPlotUpperLowerCi(R=rs,numbers=TRUE) 

#now do this again, but with lighter colors
gr &lt;- colorRampPalette(c("#B52127", "white", "#2171B5"))
corPlot(r=rs$r,numbers=TRUE,pval=rs$p,main="Correlations scaled by probability values",gr=gr) 

corPlotUpperLowerCi(R=rs,numbers=TRUE,gr=gr) 




#do multiple plots 
#Also show the xaxis option
op &lt;- par(mfrow=c(2,2))
corPlot(psychTools::ability,show.legend=FALSE,keep.par=FALSE,upper=FALSE)
f4 &lt;- fa(psychTools::ability,4)
corPlot(f4,show.legend=FALSE,keep.par=FALSE,numbers=TRUE,xlas=3)
om &lt;- omega(psychTools::ability,4)
corPlot(om,show.legend=FALSE,keep.par=FALSE,numbers=TRUE,xaxis=3)
par(op)


corPlotUpperLowerCi(rs,adjust=TRUE,main="Holm adjusted confidence intervals",gr=gr)

</code></pre>

<hr>
<h2 id='correct.cor'> Find dis-attenuated correlations given correlations and reliabilities </h2><span id='topic+correct.cor'></span>

<h3>Description</h3>

<p>Given a raw correlation matrix and a vector of reliabilities, report the disattenuated correlations above the diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct.cor(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct.cor_+3A_x">x</code></td>
<td>
<p> A raw correlation matrix </p>
</td></tr>
<tr><td><code id="correct.cor_+3A_y">y</code></td>
<td>
<p> Vector of reliabilities </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Disattenuated correlations may be thought of as  correlations between the latent variables measured by a set of observed variables. That is, what would the correlation be between two (unreliable) variables be if both variables were measured perfectly reliably.
</p>
<p>This function is mainly used if importing correlations and reliabilities from somewhere else.  If the raw data are available, use <code><a href="#topic+score.items">score.items</a></code>, or  <code><a href="#topic+cluster.loadings">cluster.loadings</a></code> or <code><a href="#topic+cluster.cor">cluster.cor</a></code>.
</p>
<p>Examples of the output of this function are seen in <code><a href="#topic+cluster.loadings">cluster.loadings</a></code> and <code><a href="#topic+cluster.cor">cluster.cor</a></code>
</p>


<h3>Value</h3>

<p>Raw correlations below the diagonal, reliabilities on the diagonal, disattenuated above the diagonal.
</p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

<p> Revelle, W. (in preparation) An Introduction to Psychometric Theory with applications in R. Springer. at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+cluster.loadings">cluster.loadings</a></code> and <code><a href="#topic+cluster.cor">cluster.cor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# attitude from the datasets package
#example 1 is a rather clunky way of doing things

a1 &lt;- attitude[,c(1:3)]
a2 &lt;- attitude[,c(4:7)]
x1 &lt;- rowSums(a1)  #find the sum of the first 3 attitudes
x2 &lt;- rowSums(a2)   #find the sum of the last 4 attitudes
alpha1 &lt;- alpha(a1)
alpha2 &lt;- alpha(a2)
x &lt;- matrix(c(x1,x2),ncol=2)
x.cor &lt;- cor(x)
alpha &lt;- c(alpha1$total$raw_alpha,alpha2$total$raw_alpha)
round(correct.cor(x.cor,alpha),2)
#
#much better - although uses standardized alpha 
clusters &lt;- matrix(c(rep(1,3),rep(0,7),rep(1,4)),ncol=2)
cluster.loadings(clusters,cor(attitude))
# or 
clusters &lt;- matrix(c(rep(1,3),rep(0,7),rep(1,4)),ncol=2)
cluster.cor(clusters,cor(attitude))
#
#best
keys &lt;- make.keys(attitude,list(first=1:3,second=4:7))
scores &lt;- scoreItems(keys,attitude)
scores$corrected

#However, to do the more general case of correcting correlations for reliabilty
#corrected &lt;- cor2cov(x.cor,1/alpha)
#diag(corrected) &lt;- 1


</code></pre>

<hr>
<h2 id='cortest'>Chi square tests of whether a single matrix is an identity matrix, or a pair of matrices are equal. </h2><span id='topic+cortest.mat'></span><span id='topic+cortest.normal'></span><span id='topic+cortest'></span><span id='topic+cortest.jennrich'></span>

<h3>Description</h3>

<p>Steiger (1980) pointed out that the sum of the squared elements of a correlation matrix, or the Fisher z score equivalents, is distributed as chi square under the null hypothesis that the values are zero (i.e., elements of the identity matrix).  This is particularly useful for examining whether correlations in a single matrix differ from zero or for comparing two matrices. Jennrich (1970) also examined tests of differences between matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cortest(R1,R2=NULL,n1=NULL,n2 = NULL, fisher = TRUE,cor=TRUE,method="pearson",
           use ="pairwise") #same as cortest.normal this does the steiger test
cortest.normal(R1, R2 = NULL, n1 = NULL, n2 = NULL, fisher = TRUE)      #the steiger test

cortest.jennrich(R1,R2,n1=NULL, n2=NULL)  #the Jennrich test
cortest.mat(R1,R2=NULL,n1=NULL,n2 = NULL) #an alternative test

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cortest_+3A_r1">R1</code></td>
<td>
<p>A correlation matrix. (If R1 is not rectangular, and cor=TRUE, the correlations are found). </p>
</td></tr>
<tr><td><code id="cortest_+3A_r2">R2</code></td>
<td>
<p>A correlation matrix.  If R2 is not rectangular, and cor=TRUE, the correlations are found. If R2 is NULL, then the test is just whether R1 is an identity matrix. </p>
</td></tr>
<tr><td><code id="cortest_+3A_n1">n1</code></td>
<td>
<p>Sample size of R1 </p>
</td></tr>
<tr><td><code id="cortest_+3A_n2">n2</code></td>
<td>
<p>Sample size of R2 </p>
</td></tr>
<tr><td><code id="cortest_+3A_fisher">fisher</code></td>
<td>
<p>Fisher z transform the correlations? </p>
</td></tr>
<tr><td><code id="cortest_+3A_cor">cor</code></td>
<td>
<p>By default, if the input matrices are not symmetric, they are converted to correlation matrices.  That is, they are treated as if they were the raw data.  If cor=FALSE, then the input matrices are taken to be correlation matrices.</p>
</td></tr> 
<tr><td><code id="cortest_+3A_method">method</code></td>
<td>
<p>Which type of correlation to find (&quot;pearson&quot;, &quot;spearman&quot;,&quot;kendall&quot;)</p>
</td></tr>
<tr><td><code id="cortest_+3A_use">use</code></td>
<td>
<p>How to handle missing data (defaults to pairwise)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several ways to test if a matrix is the identity matrix. The most well known is the chi square test  of Bartlett (1951) and Box (1949). A very straightforward test, discussed by Steiger (1980) is to find the sum of the squared correlations or the sum of the squared Fisher transformed correlations.  Under the null hypothesis that all the correlations are equal, this sum is distributed as chi square.  This is implemented in 
<code><a href="#topic+cortest">cortest</a></code> and <code><a href="#topic+cortest.normal">cortest.normal</a></code>
</p>
<p>Yet another test, is the Jennrich(1970) test of the equality of two matrices. This compares the differences between two matrices to the averages of two matrices using a chi square test. This is implemented in <code><a href="#topic+cortest.jennrich">cortest.jennrich</a></code>.
</p>
<p>Yet another option <code><a href="#topic+cortest.mat">cortest.mat</a></code> is to compare the two matrices using an approach analogous to that used in evaluating the adequacy of a factor model.  In factor analysis, the maximum likelihood fit statistic is 
<br />
<code class="reqn">f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^{-1} R|) - n.items</code>. 
</p>
<p>This in turn is converted to a chi square 
</p>
<p><code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> (see <code><a href="#topic+fa">fa</a></code>.)
</p>
<p>That is, the model (M = FF' + U2) is compared to the original correlation matrix (R) by a function of <code class="reqn">M^{-1} R</code>.  By analogy, in the case of two matrices, A and B, <code><a href="#topic+cortest.mat">cortest.mat</a></code> finds the chi squares associated with <code class="reqn">A^{-1}B</code> and <code class="reqn">A B^{-1}</code>.  The sum of these two <code class="reqn">\chi^2</code> will also be a <code class="reqn">\chi^2</code> but with twice the degrees of freedom.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>chi2</code></td>
<td>
<p>The chi square statistic</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom for the Chi Square</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>The probability of observing the Chi Square under the null hypothesis.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Both the  cortest.jennrich  and cortest.normal  are probably overly stringent.  The ChiSquare values for pairs of random samples from the same population are larger than would be expected.  This is a good test for rejecting the null of no differences. 
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p> Steiger, James H. (1980) Testing pattern hypotheses on correlation matrices: alternative statistics and some empirical results. Multivariate Behavioral Research, 15, 335-352.
</p>
<p>Jennrich, Robert I. (1970) An Asymptotic <code class="reqn">\chi^2</code> Test for the Equality of Two Correlation Matrices. Journal of the American Statistical Association, 65, 904-912.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code>  <code><a href="#topic+corr.test">corr.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
x &lt;- matrix(rnorm(1000),ncol=10)
cortest.normal(x)  #just test if this matrix is an identity
#now create two correlation matrices that should be equal
x &lt;- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)
y &lt;- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)

cortest(x$r,y$r,n1=1000,n2=1000) #The Steiger test
cortest.jennrich(x$r,y$r,n1=100,n2=1000) # The Jennrich test

cortest.mat(x$r,y$r,n1=1000,n2=1000)   #twice the degrees of freedom as the Jennrich

#create a new matrix that is different
z &lt;- sim.congeneric(loads=c(.8,.8,.7,.7, .6), N= 1000, short=FALSE)
cortest(x$r,z$r,n1=1000)  #these should be different

</code></pre>

<hr>
<h2 id='corTest'>Find the correlations, sample sizes, and probability values between elements of a matrix or data.frame.   </h2><span id='topic+corTest'></span><span id='topic+corr.test'></span><span id='topic+corr.p'></span>

<h3>Description</h3>

<p>Although the cor function finds the correlations for a matrix,  it does not report probability values.  cor.test does, but for only one pair of variables at a time.  corr.test uses cor to find the correlations for either complete or pairwise data and reports the sample sizes and probability values as well. For symmetric matrices, raw probabilites are reported below the diagonal and correlations adjusted for multiple comparisons above the diagonal. In the case of different x and ys, the default is to adjust the probabilities for multiple tests. Both corr.test and corr.p return raw and adjusted confidence intervals for each correlation. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corTest(x, y = NULL, use = "pairwise",method="pearson",adjust="holm", 
    alpha=.05,ci=TRUE,minlength=5,normal=TRUE)
corr.test(x, y = NULL, use = "pairwise",method="pearson",adjust="holm", 
    alpha=.05,ci=TRUE,minlength=5,normal=TRUE)
corr.p(r,n,adjust="holm",alpha=.05,minlength=5,ci=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corTest_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe </p>
</td></tr>
<tr><td><code id="corTest_+3A_y">y</code></td>
<td>
<p>A second matrix or dataframe with the same number of rows as x </p>
</td></tr>
<tr><td><code id="corTest_+3A_use">use</code></td>
<td>
<p>use=&quot;pairwise&quot; is the default value and will do pairwise deletion of cases. use=&quot;complete&quot; will select just complete cases. </p>
</td></tr>
<tr><td><code id="corTest_+3A_method">method</code></td>
<td>
<p>method=&quot;pearson&quot; is the default value.  The alternatives to be passed to cor are &quot;spearman&quot; and &quot;kendall&quot;.  These last two are much slower, particularly for big data sets. </p>
</td></tr>
<tr><td><code id="corTest_+3A_adjust">adjust</code></td>
<td>
<p>What adjustment for multiple tests should be used? (&quot;holm&quot;, &quot;hochberg&quot;, &quot;hommel&quot;, &quot;bonferroni&quot;, &quot;BH&quot;, &quot;BY&quot;, &quot;fdr&quot;, &quot;none&quot;). See <code><a href="stats.html#topic+p.adjust">p.adjust</a></code> for details about why to use &quot;holm&quot; rather than &quot;bonferroni&quot;). </p>
</td></tr>
<tr><td><code id="corTest_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of confidence intervals</p>
</td></tr>
<tr><td><code id="corTest_+3A_r">r</code></td>
<td>
<p>A correlation matrix</p>
</td></tr>
<tr><td><code id="corTest_+3A_n">n</code></td>
<td>
<p>Number of observations if using corr.p. May be either a matrix (as returned from corr.test, or a scaler. Set to n - np if finding the significance of partial correlations. (See below). </p>
</td></tr>
<tr><td><code id="corTest_+3A_ci">ci</code></td>
<td>
<p>By default, confidence intervals are found.  However, this leads to a noticable slowdown of speed, particularly for large problems.  So, for just the rs, ts and ps, set ci=FALSE</p>
</td></tr>
<tr><td><code id="corTest_+3A_minlength">minlength</code></td>
<td>
<p>What is the minimum length for abbreviations.  Defaults to 5.</p>
</td></tr>
<tr><td><code id="corTest_+3A_normal">normal</code></td>
<td>
<p>By default, probabilities for method=&quot;spearman&quot; and method=&quot;kendall&quot; are found by normal theory.  If normal==&quot;FALSE&quot;, then repetitive calls are made to cor.test.  This is much slower, but gives more accurate p values. exact is set to be FALSE which means that exact p values for small samples are not found given the problem of ties. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>corr.test uses the <code><a href="stats.html#topic+cor">cor</a></code> function to find the correlations, and then applies a t-test to the individual correlations using the formula
</p>
<p style="text-align: center;"><code class="reqn">t = \frac{r * \sqrt(n-2)}{\sqrt(1-r^2)}
</code>
</p>

<p style="text-align: center;"><code class="reqn">se = \sqrt(\frac{1-r^2}{n-2}) </code>
</p>

<p>The t and Standard Errors are returned as objects in the result, but are not normally displayed. Confidence intervals are found and printed if using the print(short=FALSE) option.  These are found by using the fisher z transform of the correlation and then taking the range r +/- qnorm(alpha/2) *  se and the standard error of the z transforms is </p>
<p style="text-align: center;"><code class="reqn">se = \sqrt(\frac {1}{n-3}) </code>
</p>
<p>.   These values are then back transformed to be in correlation units. They are returned in the ci object. 
</p>
<p>Note that in the case of method==&quot;kendall&quot; since these are the normal theory confidence intervals they are slightly too big.
</p>
<p>The probability values may be adjusted using the Holm (or other) correction.  If the matrix is symmetric (no y data), then the original p values are reported below the diagonal and the adjusted above the diagonal.  Otherwise, all probabilities are adjusted (unless adjust=&quot;none&quot;).  This is made explicit in the output. Confidence intervals are shown for raw and adjusted probabilities in the ci object.
</p>
<p>For those who like the conventional use of &quot;magic asterisks&quot; to show (stars) to represent conventional levels of significance, the object stars is returned (but not shown)).  See the examples.
</p>
<p><code><a href="#topic+corr.p">corr.p</a></code> may be applied to the results of <code><a href="#topic+partial.r">partial.r</a></code> if n is set to n - s (where s is the number of variables partialed out)  Fisher, 1924. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>r</code></td>
<td>
<p>The matrix of correlations</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Number of cases per correlation</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>value of t-test for each correlation</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>two tailed probability of t for each correlation.  For symmetric matrices, p values adjusted for multiple tests are reported above the diagonal. </p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard error of the correlation</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>the alpha/2 lower and upper values.</p>
</td></tr>
<tr><td><code>ci2</code></td>
<td>
<p>ci but with the adjusted pvalues as well.  This was added after tests showed we were breaking some packages that were calling the ci object without bothering to check for its dimensions.</p>
</td></tr>
<tr><td><code>ci.adj</code></td>
<td>
<p>These are the adjusted ((Holm or Bonferroni) confidence intervals. If asking to not adjust, the Holm adjustments for the confidence intervals are shown anyway, but the probability values are not adjusted and the appropriate confidence intervals are shown in the ci object.  </p>
</td></tr>
<tr><td><code>stars</code></td>
<td>
<p>For those people who like to show magic asterisks denoting &ldquo;statistical significance&quot; the stars object flags those correlation values that are unlikely given normal theory. See the last example for  how to print these neatly.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For very large matrices (&gt; 200 x 200), there is a noticeable speed improvement if confidence intervals are not found.
</p>
<p>That adjusted confidence intervals are shown even when asking for no adjustment might be confusing.  If you don't want adjusted intervals, just use the ci object.  The adjusted values are given in the ci.adj object. </p>


<h3>See Also</h3>

  <p><code><a href="stats.html#topic+cor.test">cor.test</a></code> for tests of a single correlation,  Hmisc::rcorr for an equivalant function, <code><a href="#topic+r.test">r.test</a></code> to test the difference between correlations, and <code><a href="#topic+cortest.mat">cortest.mat</a></code> to test for equality of two correlation matrices. 
</p>
<p>Also see <code><a href="#topic+cor.ci">cor.ci</a></code> for bootstrapped confidence intervals of Pearson, Spearman, Kendall, tetrachoric or polychoric correlations.  In addition <code><a href="#topic+cor.ci">cor.ci</a></code> will find bootstrapped estimates of composite scales based upon a set of correlations (ala <code><a href="#topic+cluster.cor">cluster.cor</a></code>). 
</p>
<p>In particular, see <code><a href="stats.html#topic+p.adjust">p.adjust</a></code> for a discussion of p values associated with multiple tests.
</p>
<p>Other useful functions related to finding and displaying correlations include <code>link{corPlot}</code> to graphically display the correlation matrix,  and <code><a href="#topic+lowerCor">lowerCor</a></code> for finding the correlations and then displaying the lower off diagonal using the <code><a href="#topic+lowerMat">lowerMat</a></code> function. <code><a href="#topic+lowerUpper">lowerUpper</a></code> to compare two correlation matrices.  Also see <code><a href="#topic+pairs.panels">pairs.panels</a></code> to show the correlations and scatter plots.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ct  &lt;- corTest(attitude)
#ct &lt;- corr.test(attitude)  #find the correlations and give the probabilities
ct #show the results


cts &lt;- corr.test(attitude[1:3],attitude[4:6]) #reports all values corrected for multiple tests

#corr.test(sat.act[1:3],sat.act[4:6],adjust="none")  #don't adjust the probabilities

#take correlations and show the probabilities as well as the confidence intervals
print(corr.p(cts$r,n=30),short=FALSE)  

#don't adjust the probabilities
print(corr.test(sat.act[1:3],sat.act[4:6],adjust="none"),short=FALSE)  

#print out the stars object without showing quotes
print(corr.test(attitude)$stars,quote=FALSE)  #note that the adjusted ps are given as well


kendall.r &lt;- corr.test(bfi[1:40,4:6], method="kendall", normal=FALSE)
#compare with 
cor.test(x=bfi[1:40,4],y=bfi[1:40,6],method="kendall", exact=FALSE)
print(kendall.r,digits=6)
</code></pre>

<hr>
<h2 id='cortest.bartlett'>Bartlett's test that a correlation matrix is an identity matrix </h2><span id='topic+cortest.bartlett'></span>

<h3>Description</h3>

<p>Bartlett (1951) proposed that -ln(det(R)*(N-1 - (2p+5)/6) was distributed as chi square if R were an identity matrix.  A useful test that residuals correlations are all zero. Contrast to the Kaiser-Meyer-Olkin test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cortest.bartlett(R, n = NULL,diag=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cortest.bartlett_+3A_r">R</code></td>
<td>
<p>A correlation matrix. (If R is not square, correlations are found and a warning is issued. </p>
</td></tr>
<tr><td><code id="cortest.bartlett_+3A_n">n</code></td>
<td>
<p>Sample size (if not specified, 100 is assumed).</p>
</td></tr>
<tr><td><code id="cortest.bartlett_+3A_diag">diag</code></td>
<td>
<p>Will replace the diagonal of the matrix with 1s to make it a correlation matrix.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>More useful for pedagogical purposes than actual applications. The Bartlett test is asymptotically chi square distributed.
</p>
<p>Note that if applied to residuals from factor analysis (<code><a href="#topic+fa">fa</a></code>) or principal components analysis (<code><a href="#topic+principal">principal</a></code>) that the diagonal must be replaced with 1s. This is done automatically if diag=TRUE. (See examples.)  
</p>
<p>An Alternative way of testing whether a correlation matrix is factorable (i.e., the correlations differ from 0) is the Kaiser-Meyer-Olkin <code><a href="#topic+KMO">KMO</a></code> test of factorial adequacy. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>chisq</code></td>
<td>
<p>Assymptotically chisquare</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>Of chi square</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The degrees of freedom</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

 
<p>Bartlett, M. S., (1951), The Effect of Standardization on a chi square Approximation in Factor Analysis, Biometrika, 38, 337-344.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+cortest.mat">cortest.mat</a></code>, <code><a href="#topic+cortest.normal">cortest.normal</a></code>, <code><a href="#topic+cortest.jennrich">cortest.jennrich</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)   
x &lt;- matrix(rnorm(1000),ncol=10)
r &lt;- cor(x)
cortest.bartlett(r)      #random data don't differ from an identity matrix
#data(bfi)
cortest.bartlett(psychTools::bfi[1:200,1:10])    #not an identity matrix
f3 &lt;- fa(Thurstone,3)
f3r &lt;- f3$resid
cortest.bartlett(f3r,n=213,diag=FALSE)  #incorrect

cortest.bartlett(f3r,n=213,diag=TRUE)  #correct (by default)

</code></pre>

<hr>
<h2 id='cosinor'>Functions for analysis of circadian or diurnal data</h2><span id='topic+cosinor'></span><span id='topic+circadian.phase'></span><span id='topic+cosinor.plot'></span><span id='topic+cosinor.period'></span><span id='topic+circadian.mean'></span><span id='topic+circadian.sd'></span><span id='topic+circadian.cor'></span><span id='topic+circadian.linear.cor'></span><span id='topic+circadian.stats'></span><span id='topic+circadian.F'></span><span id='topic+circadian.reliability'></span><span id='topic+circular.mean'></span><span id='topic+circular.cor'></span>

<h3>Description</h3>

<p>Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosinor(angle,x=NULL,code=NULL,data=NULL,hours=TRUE,period=24,
            plot=FALSE,opti=FALSE,na.rm=TRUE)
cosinor.plot(angle,x=NULL,data = NULL, IDloc=NULL, ID=NULL,hours=TRUE, period=24,
 na.rm=TRUE,ylim=NULL,ylab="observed",xlab="Time (double plotted)",
 main="Cosine fit",add=FALSE,multi=FALSE,typ="l",...)
 cosinor.period(angle,x=NULL,code=NULL,data=NULL,hours=TRUE,period=seq(23,26,1),
            plot=FALSE,opti=FALSE,na.rm=TRUE)          
circadian.phase(angle,x=NULL,code=NULL,data=NULL,hours=TRUE,period=24,
            plot=FALSE,opti=FALSE,na.rm=TRUE)
circadian.mean(angle,data=NULL, hours=TRUE,na.rm=TRUE)
circadian.sd(angle,data=NULL,hours=TRUE,na.rm=TRUE)
circadian.stats(angle,data=NULL,hours=TRUE,na.rm=TRUE)
circadian.F(angle,group,data=NULL,hours=TRUE,na.rm=TRUE)
circadian.reliability(angle,x=NULL,code=NULL,data = NULL,min=16,   
         oddeven=FALSE, hours=TRUE,period=24,plot=FALSE,opti=FALSE,na.rm=TRUE) 
circular.mean(angle,na.rm=TRUE) #angles in radians
circadian.cor(angle,data=NULL,hours=TRUE,na.rm=TRUE)  #angles in radians
circular.cor(angle,na.rm=TRUE) #angles in radians
circadian.linear.cor(angle,x=NULL,data=NULL,hours=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cosinor_+3A_angle">angle</code></td>
<td>
<p>A data frame or matrix of observed values with the time of day as the first value (unless specified in code) angle can be specified either as hours or as radians)</p>
</td></tr>
<tr><td><code id="cosinor_+3A_code">code</code></td>
<td>
<p>A subject identification variable</p>
</td></tr>
<tr><td><code id="cosinor_+3A_data">data</code></td>
<td>
<p>A matrix or data frame of data.  If specified, then angle and code are variable names (or locations).  See examples.</p>
</td></tr>
<tr><td><code id="cosinor_+3A_group">group</code></td>
<td>
<p>If doing comparisons by groups, specify the group code</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="cosinor_+3A_min">min</code></td>
<td>
<p>The minimum number of observations per subject to use when finding split half reliabilities.</p>
</td></tr>
<tr><td><code id="cosinor_+3A_oddeven">oddeven</code></td>
<td>
<p>Reliabilities are based upon odd and even items (TRUE) or first vs. last half (FALSE).  Default is first and last half.</p>
</td></tr>
<tr><td><code id="cosinor_+3A_period">period</code></td>
<td>
<p>Although time of day is assumed to have a 24 hour rhythm, other rhythms may be fit. If calling cosinor.period, a range may be specified.</p>
</td></tr>
<tr><td><code id="cosinor_+3A_idloc">IDloc</code></td>
<td>
<p>Which column number is the ID field</p>
</td></tr>
<tr><td><code id="cosinor_+3A_id">ID</code></td>
<td>
<p>What specific subject number should be plotted for one variable</p>
</td></tr>
<tr><td><code id="cosinor_+3A_plot">plot</code></td>
<td>
<p>if TRUE, then plot the first variable (angle)</p>
</td></tr> 
<tr><td><code id="cosinor_+3A_opti">opti</code></td>
<td>
<p>opti=TRUE: iterative optimization (slow) or opti=FALSE: linear fitting (fast)</p>
</td></tr>    
<tr><td><code id="cosinor_+3A_hours">hours</code></td>
<td>
<p>If TRUE, measures are in 24 hours to the day, otherwise, radians</p>
</td></tr>
<tr><td><code id="cosinor_+3A_x">x</code></td>
<td>
<p>A set of external variables to correlate with the phase angles</p>
</td></tr>
<tr><td><code id="cosinor_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing data be removed?</p>
</td></tr>
<tr><td><code id="cosinor_+3A_ylim">ylim</code></td>
<td>
<p>Specify the range of the y axis if the defaults don't work</p>
</td></tr>
<tr><td><code id="cosinor_+3A_ylab">ylab</code></td>
<td>
<p>The label of the yaxis</p>
</td></tr>
<tr><td><code id="cosinor_+3A_xlab">xlab</code></td>
<td>
<p>Labels for the x axis</p>
</td></tr>
<tr><td><code id="cosinor_+3A_main">main</code></td>
<td>
<p>the title of the graphic</p>
</td></tr>
<tr><td><code id="cosinor_+3A_add">add</code></td>
<td>
<p>If doing multiple (spagetti) plots, set add = TRUE for the second and beyond plots</p>
</td></tr>
<tr><td><code id="cosinor_+3A_multi">multi</code></td>
<td>
<p>If doing multiple (spagetti) plots, set multi=TRUE for the first and subsequent plots</p>
</td></tr>
<tr><td><code id="cosinor_+3A_typ">typ</code></td>
<td>
<p>Pass the line type to graphics</p>
</td></tr>
<tr><td><code id="cosinor_+3A_...">...</code></td>
<td>
<p>any other graphic parameters to pass</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  
</p>
<p>The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in <code><a href="#topic+circadian.cor">circadian.cor</a></code> here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  
</p>
<p>The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  
</p>
<p>The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t &lt;- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is <code class="reqn">sign(beta.c) * acos(beta.c/\sqrt(beta.c^2 + beta.s^2)) * 12/pi</code>
</p>
<p>Simulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].
</p>
<p>The <code><a href="#topic+circadian.reliability">circadian.reliability</a></code> function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using <code><a href="#topic+circadian.cor">circadian.cor</a></code>) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.
</p>
<p><code><a href="#topic+circular.mean">circular.mean</a></code> and <code><a href="#topic+circular.cor">circular.cor</a></code> are just <code><a href="#topic+circadian.mean">circadian.mean</a></code> and <code><a href="#topic+circadian.cor">circadian.cor</a></code> but with input given in radians rather than hours.
</p>
<p>The <code><a href="#topic+circadian.linear.cor">circadian.linear.cor</a></code> function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  
</p>
<p>The <code><a href="#topic+circadian.F">circadian.F</a></code> will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>phase</code></td>
<td>
<p>The phase angle that best fits the data (expressed in hours if hours=TRUE).</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>Value of the correlation of the fit.  This is just the correlation of the data with the phase adjusted cosine.</p>
</td></tr>
<tr><td><code>mean.angle</code></td>
<td>
<p>A vector of mean angles</p>
</td></tr>
<tr><td><code>n</code>, <code>mean</code>, <code>sd</code></td>
<td>
<p>The appropriate circular statistic.</p>
</td></tr>
<tr><td><code>correl</code></td>
<td>
<p>A matrix of circular correlations or linear by circular correlations</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>R is the vector length (0-1) of the mean vector when finding circadian statistics using <code><a href="#topic+circadian.stats">circadian.stats</a></code>   </p>
</td></tr>
<tr><td><code>z</code>, <code>p</code></td>
<td>
<p>z is the number of observations x R^2.  p is the probability of a z.</p>
</td></tr>
<tr><td><code>phase.rel</code></td>
<td>
<p>The reliability of the phase measures.  This is the circular correlation between the two halves adjusted using the Spearman-Brown correction.</p>
</td></tr>
<tr><td><code>fit.rel</code></td>
<td>
<p>The split half reliability of the fit statistic.</p>
</td></tr>
<tr><td><code>split.F</code></td>
<td>
<p>Do the two halves differ from each other?  One would hope not.</p>
</td></tr>
<tr><td><code>group1</code>, <code>group2</code></td>
<td>
<p>The statistics from each half</p>
</td></tr>
<tr><td><code>splits</code></td>
<td>
<p>The individual data from each half.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>These functions have been adapted from the circular package to allow for ease of use with circadian data, particularly for data sets with missing data and multiple variables of interest.</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p> See circular statistics 
Jammalamadaka, Sreenivasa and Lund, Ulric (2006),The effect of wind direction on ozone levels: a case study, Environmental and Ecological Statistics, 13, 287-298.	
</p>


<h3>See Also</h3>

<p>See the circular and CircStats packages. </p>


<h3>Examples</h3>

<pre><code class='language-R'>time &lt;- seq(1:24) #create a 24 hour time
pure &lt;- matrix(time,24,18) 
colnames(pure) &lt;- paste0("H",1:18)
pure &lt;- data.frame(time,cos((pure - col(pure))*pi/12)*3 + 3)
    #18 different phases but scaled to 0-6  match mood data
matplot(pure[-1],type="l",main="Pure circadian arousal rhythms",
    xlab="time of day",ylab="Arousal") 
op &lt;- par(mfrow=c(2,2))
 cosinor.plot(1,3,pure)
 cosinor.plot(1,5,pure)
 cosinor.plot(1,8,pure)
 cosinor.plot(1,12,pure)

p &lt;- cosinor(pure) #find the acrophases (should match the input)

#now, test finding the acrophases for  different subjects on 3 variables
#They should be the first 3, second 3, etc. acrophases of pure
pp &lt;- matrix(NA,nrow=6*24,ncol=4)
pure &lt;- as.matrix(pure)
pp[,1] &lt;- rep(pure[,1],6)
pp[1:24,2:4] &lt;- pure[1:24,2:4] 
pp[25:48,2:4] &lt;- pure[1:24,5:7] *2   #to test different variances
pp[49:72,2:4] &lt;- pure[1:24,8:10] *3
pp[73:96,2:4] &lt;- pure[1:24,11:13]
pp[97:120,2:4] &lt;- pure[1:24,14:16]
pp[121:144,2:4] &lt;- pure[1:24,17:19]
pure.df &lt;- data.frame(ID = rep(1:6,each=24),pp)
colnames(pure.df) &lt;- c("ID","Time",paste0("V",1:3))
cosinor("Time",3:5,"ID",pure.df)

op &lt;- par(mfrow=c(2,2))
 cosinor.plot(2,3,pure.df,IDloc=1,ID="1")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="2")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="3")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="4")
 
 #now, show those in one panel as spagetti plots
op &lt;- par(mfrow=c(1,1))
cosinor.plot(2,3,pure.df,IDloc=1,ID="1",multi=TRUE,ylim=c(0,20),ylab="Modeled")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="2",multi=TRUE,add=TRUE,lty="dotdash")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="3",multi=TRUE,add=TRUE,lty="dashed")
 cosinor.plot(2,3,pure.df,IDloc=1,ID="4",multi=TRUE,add=TRUE,lty="dotted")

set.seed(42)   #what else?
noisy &lt;- pure
noisy[,2:19]&lt;- noisy[,2:19] + rnorm(24*18,0,.2)

n &lt;- cosinor(time,noisy) #add a bit of noise

small.pure &lt;- pure[c(8,11,14,17,20,23),]
small.noisy &lt;- noisy[c(8,11,14,17,20,23),]
small.time &lt;- c(8,11,14,17,20,23)

cosinor.plot(1,3,small.pure,multi=TRUE)
cosinor.plot(1,3,small.noisy,multi=TRUE,add=TRUE,lty="dashed")

         
# sp &lt;- cosinor(small.pure)
# spo &lt;- cosinor(small.pure,opti=TRUE) #iterative fit
# sn &lt;- cosinor(small.noisy) #linear
# sno &lt;- cosinor(small.noisy,opti=TRUE) #iterative
# sum.df &lt;- data.frame(pure=p,noisy = n, small=sp,small.noise = sn, 
#         small.opt=spo,small.noise.opt=sno)
# round(sum.df,2)
# round(circadian.cor(sum.df[,c(1,3,5,7,9,11)]),2)  #compare alternatives 
# 
# #now, lets form three "subjects" and show how the grouping variable works
# mixed.df &lt;- rbind(small.pure,small.noisy,noisy)
# mixed.df &lt;- data.frame(ID=c(rep(1,6),rep(2,6),rep(3,24)),
#           time=c(rep(c(8,11,14,17,20,23),2),1:24),mixed.df)
# group.df &lt;- cosinor(angle="time",x=2:20,code="ID",data=mixed.df)
# round(group.df,2)  #compare these values to the sp,sn,and n values done separately


</code></pre>

<hr>
<h2 id='cta'>Simulate the C(ues) T(endency) A(ction) model of motivation</h2><span id='topic+cta'></span><span id='topic+cta.15'></span>

<h3>Description</h3>

<p>Dynamic motivational models such as the Dynamics of Action (Atkinson and Birch, 1970, Revelle, 1986) may be reparameterized as a simple pair of differential (matrix) equations (Revelle, 1986, 2008). This function simulates the dynamic aspects of the CTA.  The CTA model is discussed in detail in Revelle and Condon (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cta (n=3,t=5000, cues = NULL, act=NULL, inhibit=NULL,expect = NULL, consume = NULL, 
tendency = NULL,tstrength=NULL, type="both", fast=2,compare=FALSE,learn=TRUE,reward=NULL) 
cta.15(n = 3, t = 5000, cues = NULL,stim=NULL, act = NULL, inhibit = NULL, consume = NULL, 
   ten = NULL,  type = "both", fast = 2)


</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cta_+3A_n">n</code></td>
<td>
<p>number of actions to simuate </p>
</td></tr>
<tr><td><code id="cta_+3A_t">t</code></td>
<td>
<p>length of time to simulate</p>
</td></tr>
<tr><td><code id="cta_+3A_cues">cues</code></td>
<td>
<p>a vector of cue strengths</p>
</td></tr>
<tr><td><code id="cta_+3A_stim">stim</code></td>
<td>
<p>A vector of the environmental stimuli</p>
</td></tr>
<tr><td><code id="cta_+3A_act">act</code></td>
<td>
<p>matrix of associations between cues and action tendencies</p>
</td></tr>
<tr><td><code id="cta_+3A_inhibit">inhibit</code></td>
<td>
<p>inhibition matrix </p>
</td></tr>
<tr><td><code id="cta_+3A_consume">consume</code></td>
<td>
<p> Consummation matrix </p>
</td></tr>
<tr><td><code id="cta_+3A_ten">ten</code></td>
<td>
<p>Initial values of action tendencies </p>
</td></tr>
<tr><td><code id="cta_+3A_type">type</code></td>
<td>
<p>show actions, tendencies, both, or state diagrams </p>
</td></tr>
<tr><td><code id="cta_+3A_fast">fast</code></td>
<td>
<p>display every fast time (skips  </p>
</td></tr>
<tr><td><code id="cta_+3A_expect">expect</code></td>
<td>
<p>A matrix of expectations</p>
</td></tr>
<tr><td><code id="cta_+3A_tendency">tendency</code></td>
<td>
<p>starting values of tendencies</p>
</td></tr>
<tr><td><code id="cta_+3A_tstrength">tstrength</code></td>
<td>
<p>a vector of starting value of tendencies</p>
</td></tr>
<tr><td><code id="cta_+3A_compare">compare</code></td>
<td>
<p>Allows a two x two graph to compare two plots</p>
</td></tr>
<tr><td><code id="cta_+3A_learn">learn</code></td>
<td>
<p>Allow the system to learn (self reinforce) over time</p>
</td></tr>
<tr><td><code id="cta_+3A_reward">reward</code></td>
<td>
<p>The strength of the reward for doing an action</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A very thorough discussion of the CTA model is available from Revelle (2008). An application of the model is discussed in Revelle and Condon (2015). 
</p>
<p><code><a href="#topic+cta.15">cta.15</a></code> is the version used to produce the figures and analysis in Revelle and Condon (2015).  <code><a href="#topic+cta">cta</a></code> is the most recent version and includes a learning function developed in collaboration with Luke Smillie at the University of Melbourne.
</p>
<p>The dynamics of action (Atkinson and Birch, 1970)  was a model of how instigating forces elicited action tendencies which in turn elicited actions.  The basic concept was that action tendencies had inertia.  That is, a wish (action tendency) would persist until satisfied and would not change without an instigating force.  The consummatory strength of doing an action was thought  in turn to reduce the action tendency.  Forces could either be instigating or inhibitory (leading to &quot;negaction&quot;).
</p>
<p>Perhaps the simplest example is the action tendency (T)  to eat a pizza.  The instigating forces (F) to eat the pizza include the smell and look of the pizza, and once eating it, the flavor and texture.  However, if eating the pizza, there is also a consummatory force (C) which was thought to reflect both the strength (gusto) of eating the pizza as well as some constant consummatory value of the activity (c).  If not eating the pizza, but in a pizza parlor, the smells and visual cues combine to increase the tendency to eat the pizza.  Once eating it, however, the consummatory effect is no longer zero, and the change in action tendency will be a function of both the instigating forces and the consummatory forces.  These will achieve a balance when instigating forces are equal to the consummatory forces.  The asymptotic strength of eating the pizza reflects this balance and does not require a &ldquo;set point&quot; or &ldquo;comparator&quot;. 
</p>
<p>To avoid the problems of instigating and consummatory lags and the need for a decision mechanism, it is possible to reparameterize the original DOA model in terms of action tendencies and actions (Revelle, 1986).   Rather than specifying inertia for action tendencies and a choice rule of always expressing the dominant action tendency, it is useful to distinguish between action tendencies (t) and the actions (a) themselves and to  have actions as well as tendencies  having inertial properties. By separating tendencies from actions, and giving them both inertial properties, we avoid the necessity  of a  lag parameter, and by making the decision rule one of mutual inhibition, the process is perhaps easier to understand.  In an environment which affords cues for action (c), cues enhance action tendencies (t) which in turn strengthen actions (a).  This leads to two differential equations, one describing the growth and decay of action tendencies (t), the other of the actions themselves (a). 
</p>
<p style="text-align: center;"><code class="reqn">d{t} = {Sc} - { Ca} </code>
</p>
<p>   and
</p>
<p style="text-align: center;"><code class="reqn">d{a} = {Et} - {Ia}</code>
</p>
<p>.  
(See Revelle and Condon (2015) for an extensive discussion of this model.)
</p>
<p><code><a href="#topic+cta">cta</a></code> simulates this model, with the addition of a learning parameter such that  activities strengthen the connection between cues and tendencies.  The learning part of the cta model is still under development.  <code><a href="#topic+cta.15">cta.15</a></code> represents the state of the cta model as described in the Revelle and Condon (2015) article.
</p>


<h3>Value</h3>

<p>graphical output unless type=&quot;none&quot;
</p>
<table>
<tr><td><code>cues</code></td>
<td>
<p>echo back the cue input</p>
</td></tr>
<tr><td><code>inhibition</code></td>
<td>
<p>echo back the inhibitory matrix</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>time spent in each activity</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>Frequency of each activity</p>
</td></tr>
<tr><td><code>tendencies</code></td>
<td>
<p>average tendency strengths</p>
</td></tr>
<tr><td><code>actions</code></td>
<td>
<p>average action strength</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Atkinson, John W. and Birch, David (1970) The dynamics of action. John Wiley, New York, N.Y.
</p>
<p>Revelle, William (1986)  Motivation and efficiency of cognitive performance  in Brown, Donald R. and Veroff, Joe (ed). Frontiers of Motivational Psychology: Essays in honor of J. W. Atkinson. Springer.  (Available as a pdf at <a href="https://personality-project.org/revelle/publications/dynamicsofmotivation.pdf">https://personality-project.org/revelle/publications/dynamicsofmotivation.pdf</a>.)
</p>
<p>Revelle, W. (2008)  Cues, Tendencies and Actions. The Dynamics of Action revisted. <a href="https://personality-project.org/revelle/publications/cta.pdf">https://personality-project.org/revelle/publications/cta.pdf</a> 
</p>
<p>Revelle, W. and Condon, D. (2015) A model for personality at three levels.  Journal of Research in Personality <a href="https://www.sciencedirect.com/science/article/pii/S0092656615000318">https://www.sciencedirect.com/science/article/pii/S0092656615000318</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#not run 
#cta()   #default values, running over time 
#cta(type="state") #default values, in a state space  of tendency 1 versus tendency 2
#these next are examples without graphic output
#not run
#two introverts
#c2i &lt;- c(.95,1.05)
#cta(n=2,t=10000,cues=c2i,type="none")
#two extraverts
#c2e &lt;- c(3.95,4.05)
#cta(n=2,t=10000,cues=c2e,type="none")
#three introverts
#c3i &lt;-  c(.95,1,1.05)
#cta(3,t=10000,cues=c3i,type="none")
#three extraverts
#c3i &lt;- c(3.95,4, 4.05)
#cta(3,10000,c3i,type="none")
#mixed
#c3 &lt;- c(1,2.5,4)
#cta(3,10000,c3,type="none")
</code></pre>

<hr>
<h2 id='densityBy'>Create a 'violin plot' or density plot of the distribution of a set of variables</h2><span id='topic+densityBy'></span><span id='topic+violinBy'></span><span id='topic+violin'></span>

<h3>Description</h3>

<p>Among the many ways to describe a data set, one is a density plot for each value of a grouping variable and another is violin plot of multiple variables.  A density plot shows the density for different groups to show effect sizes. A violin plot is similar 
to a box plot but shows the actual distribution.
Median and 25th and 75th percentile lines are added to the display. If a grouping variable is specified, violinBy will draw violin plots for each variable and for each group. Data points may be drawn as well in what is known as a &quot;raincloud plot&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
violin(x,data=NULL, var=NULL, grp=NULL, grp.name=NULL, xlab=NULL, ylab=NULL,
	main="Density plot", vertical=TRUE, dots=FALSE, rain=FALSE, jitter=.05, alpha=1,
	errors=FALSE, eyes=TRUE, adjust=1, restrict=TRUE, xlim=NULL, add=FALSE, 
	col=NULL, pch=20, scale=NULL,...) 
 
violinBy(x, var=NULL, grp=NULL, data=NULL, grp.name=NULL, xlab=NULL, ylab=NULL,
	main="Density plot", vertical=TRUE, dots=FALSE, rain=FALSE, jitter=.05, alpha= 1,
	errors=FALSE, eyes=TRUE, adjust=1, restrict=TRUE, xlim=NULL, add=FALSE, 
	col=NULL, pch=20, scale=NULL,...) 
 
densityBy(x, var=NULL, grp=NULL,data=NULL, freq=FALSE, col=c("blue","red","black"), 
	alpha=.5, adjust=1, ylim=NULL, xlim=NULL, xlab="Variable", ylab="Density",
     main="Density Plot",legend=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="densityBy_+3A_x">x</code></td>
<td>
<p>A matrix or data.frame (can be expressed in formula input)</p>
</td></tr>
<tr><td><code id="densityBy_+3A_var">var</code></td>
<td>
<p>The variable(s) to display</p>
</td></tr>
<tr><td><code id="densityBy_+3A_grp">grp</code></td>
<td>
<p>The grouping variable(s) </p>
</td></tr>
<tr><td><code id="densityBy_+3A_data">data</code></td>
<td>
<p>The name of the data object if using formula input</p>
</td></tr>
<tr><td><code id="densityBy_+3A_grp.name">grp.name</code></td>
<td>
<p>If the grouping variable is specified, then what names should be give to the group? Defaults to 1:ngrp</p>
</td></tr>
<tr><td><code id="densityBy_+3A_ylab">ylab</code></td>
<td>
<p>The y label</p>
</td></tr>
<tr><td><code id="densityBy_+3A_xlab">xlab</code></td>
<td>
<p>The x label</p>
</td></tr>
<tr><td><code id="densityBy_+3A_main">main</code></td>
<td>
<p>Figure title</p>
</td></tr>
<tr><td><code id="densityBy_+3A_vertical">vertical</code></td>
<td>
<p>If TRUE, plot the violins vertically, otherwise, horizontonally</p>
</td></tr>
<tr><td><code id="densityBy_+3A_dots">dots</code></td>
<td>
<p>if TRUE, add a stripchart with the data points</p>
</td></tr>
<tr><td><code id="densityBy_+3A_rain">rain</code></td>
<td>
<p>If TRUE, draw a half violin with rain drops</p>
</td></tr>
<tr><td><code id="densityBy_+3A_jitter">jitter</code></td>
<td>
<p>If doing a stripchart, then jitter the points this much</p>
</td></tr>
<tr><td><code id="densityBy_+3A_errors">errors</code></td>
<td>
<p>If TRUE, add error bars or cats eyes to the violins</p>
</td></tr>
<tr><td><code id="densityBy_+3A_eyes">eyes</code></td>
<td>
<p>if TRUE and errors=TRUE, then draw cats eyes</p>
</td></tr>
<tr><td><code id="densityBy_+3A_alpha">alpha</code></td>
<td>
<p>A degree of transparency (0=transparent ... 1 not transparent)</p>
</td></tr>
<tr><td><code id="densityBy_+3A_adjust">adjust</code></td>
<td>
<p>Allows smoothing of density histograms when plotting variables like height</p>
</td></tr>
<tr><td><code id="densityBy_+3A_freq">freq</code></td>
<td>
<p>if TRUE, then plot frequencies (n * density)</p>
</td></tr>
<tr><td><code id="densityBy_+3A_restrict">restrict</code></td>
<td>
<p>Restrict the density to the observed max and min of the data</p>
</td></tr>
<tr><td><code id="densityBy_+3A_xlim">xlim</code></td>
<td>
<p>if not specified, will be .5 beyond the number of variables</p>
</td></tr>
<tr><td><code id="densityBy_+3A_ylim">ylim</code></td>
<td>
<p>If not specified, determined by the data</p>
</td></tr>
<tr><td><code id="densityBy_+3A_add">add</code></td>
<td>
<p>Allows overplotting</p>
</td></tr>
<tr><td><code id="densityBy_+3A_col">col</code></td>
<td>
<p>Allows for specification of colours.  The default for 2 groups is 
blue and red, for more group levels, rainbows.</p>
</td></tr>
<tr><td><code id="densityBy_+3A_pch">pch</code></td>
<td>
<p>The plot character for the mean is by default a small filled circle.  To not show the mean, use pch=NA </p>
</td></tr>
<tr><td><code id="densityBy_+3A_scale">scale</code></td>
<td>
<p>If NULL, scale the widths by the square root of sample size, otherwise scale by the value supplied.</p>
</td></tr>
<tr><td><code id="densityBy_+3A_legend">legend</code></td>
<td>
<p>If not NULL, draw a legend at c(topleft,topright,top,left,right) </p>
</td></tr>
<tr><td><code id="densityBy_+3A_...">...</code></td>
<td>
<p>Other graphic parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Describe the data using a violin plot. Change alpha to modify the shading.  
The grp variable may be used to draw separate violin plots for each of multiple groups.
</p>
<p>For relatively smallish data sets (&lt; 500-1000), it is informative to also show the actual data points. This done with the dots=TRUE option. The jitter value is arbitrarily set to .05, but making it larger (say .1 or .2) will display more points.
</p>
<p>Perhaps even prettier, is draw &quot;raincloud&quot; plots (half violins with rain drops)
</p>


<h3>Value</h3>

<p>The density (y axis) by value (x axis)  of the data  (for densityBy) or a violin plot for each variable (perhaps broken down by groups)
</p>


<h3>Note</h3>

<p>Formula input added July 12, 2020</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+describeBy">describeBy</a></code> and  <code><a href="#topic+statsBy">statsBy</a></code> for descriptive statistics and <code><a href="#topic+error.bars">error.bars</a></code>, <code><a href="#topic+error.bars.by">error.bars.by</a></code> and  <code><a href="#topic+bi.bars">bi.bars</a></code>, <code><a href="#topic+histBy">histBy</a></code> and <code><a href="#topic+scatterHist">scatterHist</a></code> for other graphic displays </p>


<h3>Examples</h3>

<pre><code class='language-R'>violin(psychTools::bfi[4:8])
violin(SATV + SATQ ~ gender, data=sat.act, grp.name =cs(MV,FV,MQ,FQ)) #formula input
violinBy(psychTools::bfi,var=4:7,grp ="gender",grp.name=c("M","F"))
#rain does not work for multiple DVS, just up to 2 IVs 
violinBy(SATV  ~ education,data =sat.act, rain=TRUE, pch=".", vertical=FALSE)  #rain 

densityBy(SATV  ~ gender,data =sat.act,legend=1)  #formula input 

  
</code></pre>

<hr>
<h2 id='describe'>  Basic descriptive statistics useful for psychometrics </h2><span id='topic+describe'></span><span id='topic+describeData'></span><span id='topic+describeFast'></span>

<h3>Description</h3>

<p>There are many summary statistics available in R; this function
provides the ones most useful for scale construction and item analysis in classic psychometrics. 
Range is most useful for the first pass in a data set, to check for coding errors. Parallelizes if multiple cores are available. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>describe(x, na.rm = TRUE, interp=FALSE,skew = TRUE, ranges = TRUE,trim=.1,
              type=3,check=TRUE,fast=NULL,quant=NULL,IQR=FALSE,omit=FALSE,data=NULL,
              size=50)
describeData(x,head=4,tail=4)
describeFast(x) 
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="describe_+3A_x">x</code></td>
<td>
<p> A data frame or matrix</p>
</td></tr>
<tr><td><code id="describe_+3A_na.rm">na.rm</code></td>
<td>
<p>The default is to delete missing data.  na.rm=FALSE will delete the case. </p>
</td></tr>
<tr><td><code id="describe_+3A_interp">interp</code></td>
<td>
<p>Should the median be standard or interpolated</p>
</td></tr>
<tr><td><code id="describe_+3A_skew">skew</code></td>
<td>
<p> Should the skew and kurtosis be calculated? </p>
</td></tr>
<tr><td><code id="describe_+3A_ranges">ranges</code></td>
<td>
<p> Should the range be calculated? </p>
</td></tr>
<tr><td><code id="describe_+3A_trim">trim</code></td>
<td>
<p>trim=.1 &ndash; trim means by dropping the top and bottom trim fraction</p>
</td></tr>
<tr><td><code id="describe_+3A_type">type</code></td>
<td>
<p>Which estimate of skew and kurtosis should be used?  (See details.) </p>
</td></tr>
<tr><td><code id="describe_+3A_check">check</code></td>
<td>
<p>Should we check for non-numeric variables?  Slower but helpful.</p>
</td></tr>
<tr><td><code id="describe_+3A_fast">fast</code></td>
<td>
<p>if TRUE, will do n, means, sds, min, max, ranges for an improvement in speed.  If 
NULL, will switch to fast mode for large (ncol * nrow &gt; 10^7) problems, otherwise 
defaults to fast = FALSE</p>
</td></tr>
<tr><td><code id="describe_+3A_quant">quant</code></td>
<td>
<p>if not NULL, will find the specified quantiles (e.g. quant=c(.25,.75) will 
find the 25th and 75th percentiles)</p>
</td></tr>
<tr><td><code id="describe_+3A_iqr">IQR</code></td>
<td>
<p>If TRUE, show the interquartile range</p>
</td></tr>
<tr><td><code id="describe_+3A_omit">omit</code></td>
<td>
<p>Do not convert non-numerical variables to numeric, omit them instead</p>
</td></tr>
<tr><td><code id="describe_+3A_head">head</code></td>
<td>
<p>show the first 1:head cases for each variable in describeData</p>
</td></tr>
<tr><td><code id="describe_+3A_tail">tail</code></td>
<td>
<p>Show the last nobs-tail cases for each variable in describeData</p>
</td></tr>
<tr><td><code id="describe_+3A_data">data</code></td>
<td>
<p>Allows formula input for specific grouping variables</p>
</td></tr>
<tr><td><code id="describe_+3A_size">size</code></td>
<td>
<p>For very big problems (in terms of nvar) set size to some reasonable value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In basic data analysis it is vital to get basic descriptive statistics. 
Procedures such as <code><a href="base.html#topic+summary">summary</a></code> and Hmisc::describe do so.  The describe function in the <code><a href="#topic+psych">psych</a></code> package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call <code><a href="#topic+describeBy">describeBy</a></code> to the processing. The results from describe can be used in graphics functions (e.g., <code><a href="#topic+error.crosses">error.crosses</a></code>).
</p>
<p>The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data.  
</p>
<p>Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not  for alphanumeric data). 
</p>
<p>If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described.  These variables are marked with an * in the row name.  This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected.  For instance, if education is coded &quot;high school&quot;, &quot;some college&quot; , &quot;finished college&quot;, then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all).
</p>
<p>In a typical study, one might read the data in from the clipboard (<code><a href="psychTools.html#topic+read.clipboard">read.clipboard</a></code>), show the splom plot of the correlations (<code><a href="#topic+pairs.panels">pairs.panels</a></code>), and then describe the data. 
</p>
<p>na.rm=FALSE is equivalent to describe(na.omit(x))  
</p>
<p>When finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). 
</p>
<p>If we define <code class="reqn">m_r = [\sum(X- mx)^r]/n</code> then 
</p>
<p>Type 1 finds skewness and kurtosis by <code class="reqn">g_1 = m_3/(m_2)^{3/2} </code> and <code class="reqn">g_2 = m_4/(m_2)^2 -3</code>.  
</p>
<p>Type 2 is <code class="reqn">G1 = g1 * \sqrt{n *(n-1)}/(n-2)</code> and <code class="reqn">G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3))</code>.  
</p>
<p>Type 3 is <code class="reqn">b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2}</code> and <code class="reqn">b2 =  [(n-1)/n]^{3/2} m_4/m_2^2)</code>.
</p>
<p>The additional helper function <code><a href="#topic+describeData">describeData</a></code> just scans the data array and reports on whether the data are all numerical, logical/factorial, or categorical.  This is a useful check to run if trying to get descriptive statistics on very large data sets where to improve the speed, the check option is FALSE.  
</p>
<p>An even faster overview of the data is <code><a href="#topic+describeFast">describeFast</a></code> which reports the number of total cases, number of complete cases, number of numeric variables and the number which are factors. 
</p>
<p>The fast=TRUE option will lead to a speed up of about 50% for larger problems by not finding all of the statistics (see NOTE)
</p>
<p>To describe the data for different groups, see <code><a href="#topic+describeBy">describeBy</a></code> or specify the grouping variable(s) in formula mode (see the examples). 
</p>


<h3>Value</h3>

<p>A data.frame of the relevant statistics: <br />
item name <br />
item number <br />
number of valid cases<br />
mean<br />
standard deviation<br />
trimmed mean (with trim defaulting to .1) <br />
median (standard or interpolated<br />
mad: median absolute deviation (from the median). <br />
minimum<br />
maximum<br />
skew<br />
kurtosis<br />
standard error<br />
</p>


<h3>Note</h3>

<p>For very large data sets that are data.frames, <code><a href="#topic+describe">describe</a></code> can be rather slow. Converting the data to a matrix first is recommended.  However, if the data are of different types, (factors or logical), this is  not possible.  If the data set includes columns of character data, it is also not possible.  Thus, a quick pass with <code><a href="#topic+describeData">describeData</a></code> is recommended.  Even faster is a quick pass with <code><a href="#topic+describeFast">describeFast</a></code> which just counts number of observations per variable and reports the type of data (numerical, factor, logical).
</p>
<p>For the greatest speed, at the cost of losing information, do not ask for ranges or for skew and turn off check.  This is done automatically if the fast option is TRUE or for large data sets. 
</p>
<p>Note that by default, fast=NULL.  But if the number of cases x number of variables exceeds (ncol * nrow &gt; 10^7), fast will be set to TRUE.  This will provide just n, mean, sd, min, max, range, and standard errors. To get all of the statistics (but at a cost of greater time) set fast=FALSE. 
</p>
<p>The problem seems to be a memory limitation in that the time taken is an accelerating function of nvars * nobs.  Thus, for a largish problem (72,000 cases with 1680 variables) which might take 330 seconds, doing it as two sets of 840 variable cuts the time down to 80 seconds.   
</p>
<p>The object returned is a data frame with the normal precision of R.  However, to control the number of digits displayed, you can set digits in a print command, rather than losing precision at the descriptive stats level.  See the last two examples.  One just sets the number of digits, one gives uses signif  to make 'prettier'  output where all numbers are displayed to the same number of digits.
</p>
<p>The MAD (median absolute deviation from the median) is calculated using the mad function from the stats package in Core-R.  Note that by default, the MAD is adjusted by a scaling factor (1.4826) that will give the expectation of the MAD to be the same as the standard deviation for normal data.  
</p>
<p>An interesting problem with describe is that a function with the same name is in the Hmisc package.  HMisc is loaded by qqgraph which in turn is loaded by SemPlot.  So even if not directly loading HMisc, if you load  SemPlot after loading psych, describe will not work, but the reverse order for loading should work.
</p>


<h3>Author(s)</h3>

 
<p><a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
</p>
<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
</p>


<h3>References</h3>

<p>Joanes, D.N. and Gill, C.A (1998).  Comparing measures of sample skewness and kurtosis.  The Statistician, 47, 183-189.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+describeBy">describeBy</a></code>, <code><a href="#topic+skew">skew</a></code>, <code><a href="#topic+kurtosi">kurtosi</a></code> <code><a href="#topic+interp.median">interp.median</a></code>, <code><a href="psychTools.html#topic+read.clipboard">read.clipboard</a></code>.  Then, for graphic output, see <code><a href="#topic+error.crosses">error.crosses</a></code>, <code><a href="#topic+pairs.panels">pairs.panels</a></code>, <code><a href="#topic+error.bars">error.bars</a></code>, <code><a href="#topic+error.bars.by">error.bars.by</a></code> and <code><a href="#topic+densityBy">densityBy</a></code>, or <code><a href="#topic+violinBy">violinBy</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sat.act)
describe(sat.act)
describe(sat.act ~ gender) #formula mode option calls describeBy for the entire data frame
describe(SATV + SATQ ~ gender, data=sat.act) #formula mode specifies just two variables

describe(sat.act,skew=FALSE)
describe(sat.act,IQR=TRUE) #show the interquartile Range
describe(sat.act,quant=c(.1,.25,.5,.75,.90) ) #find the 10th, 25th, 50th, 
                   #75th and 90th percentiles
                   
                   
 
describeData(sat.act) #the fast version just  gives counts and head and tail

print(describeFast(sat.act),short=FALSE)  #even faster is just counts  (just less information)  

#now show how to adjust the displayed number of digits
 des &lt;- describe(sat.act)  #find the descriptive statistics.  Keep the original accuracy
 des  #show the normal output, which is rounded to 2 decimals
 print(des,digits=3)  #show the output, but round to 3 (trailing) digits
 print(des, signif=3) #round all numbers to the 3 significant digits 

</code></pre>

<hr>
<h2 id='describeBy'> Basic summary statistics by group</h2><span id='topic+describeBy'></span><span id='topic+describe.by'></span>

<h3>Description</h3>

<p>Report basic summary statistics by a grouping variable.  Useful if the grouping variable is some experimental variable and data are to be aggregated for plotting.  Partly a wrapper for by and <code><a href="#topic+describe">describe</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>describeBy(x, group=NULL,mat=FALSE,type=3,digits=15,data,...)
describe.by(x, group=NULL,mat=FALSE,type=3,...)  # deprecated

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="describeBy_+3A_x">x</code></td>
<td>
<p>a data.frame or matrix. See note for statsBy.  </p>
</td></tr>
<tr><td><code id="describeBy_+3A_group">group</code></td>
<td>
<p>a grouping variable or a list of grouping variables.  (may be ignored if calling using the formula mode.)</p>
</td></tr>
<tr><td><code id="describeBy_+3A_mat">mat</code></td>
<td>
<p>provide a matrix output rather than a list</p>
</td></tr>
<tr><td><code id="describeBy_+3A_type">type</code></td>
<td>
<p>Which type of skew and kurtosis should be found</p>
</td></tr>
<tr><td><code id="describeBy_+3A_digits">digits</code></td>
<td>
<p>When giving matrix output, how many digits should be reported?</p>
</td></tr>
<tr><td><code id="describeBy_+3A_data">data</code></td>
<td>
<p>Needed if using formula input</p>
</td></tr>
<tr><td><code id="describeBy_+3A_...">...</code></td>
<td>
<p>parameters to be passed to describe</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To get descriptive statistics for several different grouping variables, make sure that group is a list.  In the case of matrix output with multiple grouping variables, the grouping variable values are added to the output.
</p>
<p>As of July, 2020, the grouping variable(s) may be specified in formula mode (see the examples).
</p>
<p>The type parameter specifies which version of skew and kurtosis should be found.  See <code><a href="#topic+describe">describe</a></code> for more details. 
</p>
<p>An alternative function (<code><a href="#topic+statsBy">statsBy</a></code>) returns a list of means, n, and standard deviations for each group.  This is particularly useful if finding weighted correlations of group means using <code><a href="#topic+cor.wt">cor.wt</a></code>. More importantly, it does a proper within and between group decomposition of the correlation.
</p>
<p><code><a href="#topic+cohen.d">cohen.d</a></code>  will work for two groups. It converts the data into mean differences and pools the within group standard deviations.  Returns cohen.d statistic as well as the multivariate generalization (Mahalanobis D). 
</p>


<h3>Value</h3>

<p>A data.frame of the relevant statistics broken down by group: <br />
item name <br />
item number <br />
number of valid cases<br />
mean<br />
standard deviation<br />
median<br />
mad: median absolute deviation (from the median) <br />
minimum<br />
maximum<br />
skew<br />
standard error<br />
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

  <p><code><a href="#topic+describe">describe</a></code>,  <code><a href="#topic+statsBy">statsBy</a></code>, <code><a href="#topic+densityBy">densityBy</a></code> and <code><a href="#topic+violinBy">violinBy</a></code>,  <code><a href="#topic+cohen.d">cohen.d</a></code>,  <code><a href="#topic+cohen.d.by">cohen.d.by</a></code>, and  <code><a href="#topic+cohen.d.ci">cohen.d.ci</a></code>
as well as <code><a href="#topic+error.bars">error.bars</a></code> and <code><a href="#topic+error.bars.by">error.bars.by</a></code> for other graphical displays. </p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(sat.act)
describeBy(sat.act,sat.act$gender) #just one grouping variable
describeBy(sat.act ~ gender)   #describe the entire set	formula input
describeBy(SATV + SATQ ~ gender,data =sat.act)  #specify the data set if using formula
#describeBy(sat.act,list(sat.act$gender,sat.act$education))  #two grouping variables
describeBy(sat.act ~ gender +  education) #two grouping variables
des.mat &lt;- describeBy(age ~ education,mat=TRUE,data = sat.act) #matrix (data.frame) output 
des.mat &lt;- describeBy(age ~ education + gender, data=sat.act,
               mat=TRUE,digits=2)  #matrix output  rounded to 2 decimals            
             


</code></pre>

<hr>
<h2 id='diagram'>Helper functions for drawing path model diagrams</h2><span id='topic+diagram'></span><span id='topic+dia.rect'></span><span id='topic+dia.ellipse'></span><span id='topic+dia.ellipse1'></span><span id='topic+dia.arrow'></span><span id='topic+dia.curve'></span><span id='topic+dia.curved.arrow'></span><span id='topic+dia.self'></span><span id='topic+dia.shape'></span><span id='topic+dia.triangle'></span><span id='topic+dia.cone'></span><span id='topic+multi.rect'></span><span id='topic+multi.arrow'></span><span id='topic+multi.curved.arrow'></span><span id='topic+multi.self'></span>

<h3>Description</h3>

<p>Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagram(fit,...)
dia.rect(x, y = NULL, labels = NULL,  cex = 1,  xlim = c(0, 1), ylim = c(0, 1),
    draw=TRUE, ...)
dia.ellipse(x, y = NULL, labels = NULL, cex=1,e.size=.05, xlim=c(0,1), 
       ylim=c(0,1),draw=TRUE,  ...) 
dia.triangle(x, y = NULL, labels =NULL,  cex = 1, xlim=c(0,1),ylim=c(0,1),...)
dia.ellipse1(x,y,e.size=.05,xlim=c(0,1),ylim=c(0,1),draw=TRUE,...)
dia.shape(x, y = NULL, labels = NULL, cex = 1, 
         e.size=.05, xlim=c(0,1), ylim=c(0,1), shape=1, ...)
dia.arrow(from,to,labels=NULL,scale=1,cex=1,adj=2,both=FALSE,pos=NULL,l.cex,
        gap.size,draw=TRUE,col="black",lty="solid",...)
dia.curve(from,to,labels=NULL,scale=1,...)
dia.curved.arrow(from,to,labels=NULL,scale=1,both=FALSE,dir=NULL,draw=TRUE,...)
dia.self(location,labels=NULL,scale=.8,side=2,draw=TRUE,...)
dia.cone(x=0, y=-2, theta=45, arrow=TRUE,curves=TRUE,add=FALSE,labels=NULL,
      xlim = c(-1, 1), ylim=c(-1,1),... ) 
multi.self(self.list,...)
multi.arrow(arrows.list,...)
multi.curved.arrow(curved.list,l.cex=NULL,...)
multi.rect(rect.list,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diagram_+3A_fit">fit</code></td>
<td>
<p>The results from a factor analysis <code><a href="#topic+fa">fa</a></code>, components analysis  <code><a href="#topic+principal">principal</a></code>, omega reliability analysis, <code><a href="#topic+omega">omega</a></code>, cluster analysis  <code><a href="#topic+iclust">iclust</a></code>, topdown (bassAckward) <code><a href="#topic+bassAckward">bassAckward</a></code> or confirmatory factor analysis, cfa, or structural equation model,sem, using the lavaan package.</p>
</td></tr>
<tr><td><code id="diagram_+3A_x">x</code></td>
<td>
<p>x coordinate of a rectangle or ellipse</p>
</td></tr>
<tr><td><code id="diagram_+3A_y">y</code></td>
<td>
<p>y coordinate of a rectangle or ellipse</p>
</td></tr>
<tr><td><code id="diagram_+3A_e.size">e.size</code></td>
<td>
<p>The size of the ellipse (scaled by the number of variables</p>
</td></tr>
<tr><td><code id="diagram_+3A_labels">labels</code></td>
<td>
<p>Text to insert in rectangle, ellipse, or arrow</p>
</td></tr>
<tr><td><code id="diagram_+3A_cex">cex</code></td>
<td>
<p>adjust the text size</p>
</td></tr>
<tr><td><code id="diagram_+3A_col">col</code></td>
<td>
<p>line color  (normal meaning for plot figures)</p>
</td></tr>
<tr><td><code id="diagram_+3A_lty">lty</code></td>
<td>
<p>line type</p>
</td></tr>
<tr><td><code id="diagram_+3A_l.cex">l.cex</code></td>
<td>
<p>Adjust the text size in arrows, defaults to 
cex which in turn defaults to 1</p>
</td></tr>
<tr><td><code id="diagram_+3A_gap.size">gap.size</code></td>
<td>
<p>Tweak the gap in an arrow to be allow the label to be in a gap</p>
</td></tr>
<tr><td><code id="diagram_+3A_adj">adj</code></td>
<td>
<p>Where to put the label along the arrows (values are then divided by 4)</p>
</td></tr>
<tr><td><code id="diagram_+3A_both">both</code></td>
<td>
<p>Should the arrows have arrow heads on both ends?</p>
</td></tr>
<tr><td><code id="diagram_+3A_scale">scale</code></td>
<td>
<p>modifies size of rectangle and ellipse as well as the curvature of curves.  (For curvature, positive numbers are concave down and to the left</p>
</td></tr>
<tr><td><code id="diagram_+3A_from">from</code></td>
<td>
<p>arrows and curves go from </p>
</td></tr>
<tr><td><code id="diagram_+3A_to">to</code></td>
<td>
<p>arrows and curves go to</p>
</td></tr>
<tr><td><code id="diagram_+3A_location">location</code></td>
<td>
<p>where is the rectangle?</p>
</td></tr>
<tr><td><code id="diagram_+3A_shape">shape</code></td>
<td>
<p>Which shape to draw</p>
</td></tr>
<tr><td><code id="diagram_+3A_xlim">xlim</code></td>
<td>
<p>default ranges</p>
</td></tr>
<tr><td><code id="diagram_+3A_ylim">ylim</code></td>
<td>
<p>default ranges</p>
</td></tr>
<tr><td><code id="diagram_+3A_draw">draw</code></td>
<td>
<p>Draw the text box</p>
</td></tr>
<tr><td><code id="diagram_+3A_side">side</code></td>
<td>
<p>Which side of boxes should errors appear</p>
</td></tr>
<tr><td><code id="diagram_+3A_theta">theta</code></td>
<td>
<p>Angle in degrees of vectors</p>
</td></tr>
<tr><td><code id="diagram_+3A_arrow">arrow</code></td>
<td>
<p>draw arrows for edges in dia.cone</p>
</td></tr>
<tr><td><code id="diagram_+3A_add">add</code></td>
<td>
<p>if TRUE, plot on previous plot</p>
</td></tr>
<tr><td><code id="diagram_+3A_curves">curves</code></td>
<td>
<p>if TRUE, draw curves between arrows in dia.cone</p>
</td></tr>
<tr><td><code id="diagram_+3A_pos">pos</code></td>
<td>
<p>The position of the text in . Follows the text positions of 1, 2, 3, 4 or NULL</p>
</td></tr>
<tr><td><code id="diagram_+3A_dir">dir</code></td>
<td>
<p>Should the direction of the curve be calculated dynamically, or set as &quot;up&quot; or &quot;left&quot;</p>
</td></tr>
<tr><td><code id="diagram_+3A_...">...</code></td>
<td>
<p>Most graphic parameters may be passed here</p>
</td></tr>
<tr><td><code id="diagram_+3A_self.list">self.list</code></td>
<td>
<p>list saved from dia.self</p>
</td></tr>
<tr><td><code id="diagram_+3A_arrows.list">arrows.list</code></td>
<td>
<p>lst saved from dia.arrow</p>
</td></tr>
<tr><td><code id="diagram_+3A_curved.list">curved.list</code></td>
<td>
<p>list saved from dia.curved.arrow</p>
</td></tr>
<tr><td><code id="diagram_+3A_rect.list">rect.list</code></td>
<td>
<p>list saved from dia.rect</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagram function calls  <code><a href="#topic+fa.diagram">fa.diagram</a></code>, <code><a href="#topic+omega.diagram">omega.diagram</a></code>,  <code><a href="#topic+ICLUST.diagram">ICLUST.diagram</a></code>, <code><a href="#topic+lavaan.diagram">lavaan.diagram</a></code> or <code><a href="#topic+bassAckward">bassAckward</a></code>.diagram depending upon the class of the fit input.  See those functions for particular parameter values.
</p>
<p>The remaining functions are the graphic primitives used by <code><a href="#topic+fa.diagram">fa.diagram</a></code>, <code><a href="#topic+structure.diagram">structure.diagram</a></code>, <code><a href="#topic+omega.diagram">omega.diagram</a></code>, <code><a href="#topic+ICLUST.diagram">ICLUST.diagram</a></code> and <code><a href="#topic+het.diagram">het.diagram</a></code>
</p>
<p>They create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. 
</p>
<p>To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. 
</p>
<p>The functions <code><a href="#topic+multi.rect">multi.rect</a></code>, <code><a href="#topic+multi.self">multi.self</a></code>, <code><a href="#topic+multi.arrow">multi.arrow</a></code> and <code><a href="#topic+multi.curved.arrow">multi.curved.arrow</a></code>  will take the saved output from the appropriate primitives and then draw them all at once. 
</p>
<p>Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. 
</p>
<p>Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir=&quot;up&quot; for left right curvature.
</p>
<p>The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. 
</p>
<p>These functions form the core of <code><a href="#topic+fa.diagram">fa.diagram</a></code>,<code><a href="#topic+het.diagram">het.diagram</a></code>. 
</p>
<p>Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.
</p>
<p>dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
</p>


<h3>Value</h3>

<p>Graphic output</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p>The diagram functions that use the dia functions:  <code><a href="#topic+fa.diagram">fa.diagram</a></code>, <code><a href="#topic+structure.diagram">structure.diagram</a></code>, <code><a href="#topic+omega.diagram">omega.diagram</a></code>, and <code><a href="#topic+ICLUST.diagram">ICLUST.diagram</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#first, show the primitives
xlim=c(-2,10)
ylim=c(0,10)
plot(NA,xlim=xlim,ylim=ylim,main="Demonstration of diagram functions",axes=FALSE,xlab="",ylab="")
ul &lt;- dia.rect(1,9,labels="upper left",xlim=xlim,ylim=ylim)
ml &lt;- dia.rect(1,6,"middle left",xlim=xlim,ylim=ylim)
ll &lt;- dia.rect(1,3,labels="lower left",xlim=xlim,ylim=ylim)
bl &lt;- dia.rect(1,1,"bottom left",xlim=xlim,ylim=ylim)
lr &lt;- dia.ellipse(7,3,"lower right",xlim=xlim,ylim=ylim,e.size=.07)
ur &lt;- dia.ellipse(7,9,"upper right",xlim=xlim,ylim=ylim,e.size=.07)
mr &lt;- dia.ellipse(7,6,"middle right",xlim=xlim,ylim=ylim,e.size=.07)
lm &lt;- dia.triangle(4,1,"Lower Middle",xlim=xlim,ylim=ylim)
br &lt;- dia.rect(9,1,"bottom right",xlim=xlim,ylim=ylim) 
dia.curve(from=ul$left,to=bl$left,"double headed",scale=-1)

dia.arrow(from=lr,to=ul,labels="right to left")
dia.arrow(from=ul,to=ur,labels="left to right")
dia.curved.arrow(from=lr,to=ll,labels ="right to left")
dia.curved.arrow(to=ur,from=ul,labels ="left to right")
dia.curve(ll$top,ul$bottom,"right")  #for rectangles, specify where to point 

dia.curve(ll$top,ul$bottom,"left",scale=-1)  #for rectangles, specify where to point 
dia.curve(mr,ur,"up")  #but for ellipses, you may just point to it.
dia.curve(mr,lr,"down")
dia.curve(mr,ur,"up")
dia.curved.arrow(mr,ur,"up")  #but for ellipses, you may just point to it.
dia.curved.arrow(mr,lr,"down")  #but for ellipses, you may just point to it.

dia.curved.arrow(ur$right,mr$right,"3")
dia.curve(ml,mr,"across")
dia.curve(ur$right,lr$right,"top down",scale =2)
dia.curved.arrow(br$top,lr$right,"up")
dia.curved.arrow(bl,br,"left to right")
dia.curved.arrow(br,bl,"right to left",scale=-1)
dia.arrow(bl,ll$bottom)
dia.curved.arrow(ml,ll$right)
dia.curved.arrow(mr,lr$top)

#now, put them together in a factor analysis diagram
v9 &lt;- sim.hierarchical()
f3 &lt;- fa(v9,3,rotate="cluster")
fa.diagram(f3,error=TRUE,side=3) 
</code></pre>

<hr>
<h2 id='draw.tetra'>Draw a correlation ellipse and two normal curves to demonstrate tetrachoric correlation</h2><span id='topic+draw.tetra'></span><span id='topic+draw.cor'></span>

<h3>Description</h3>

<p>A graphic of a correlation ellipse divided into 4 regions based upon x and y cutpoints on two normal distributions.  This is also an example of using the layout function. Draw a bivariate density plot to show how tetrachorics work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>draw.tetra(r, t1, t2,shade=TRUE)
draw.cor(r=.5,expand=10,theta=30,phi=30,N=101,nbcol=30,box=TRUE,
main="Bivariate density  rho = ",cuts=NULL,all=TRUE,ellipses=TRUE,ze=.15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="draw.tetra_+3A_r">r</code></td>
<td>
<p>the underlying Pearson correlation defines the shape of the ellipse</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_t1">t1</code></td>
<td>
<p>X is cut at tau</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_t2">t2</code></td>
<td>
<p>Y is cut at Tau</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_shade">shade</code></td>
<td>
<p>shade the diagram (default is TRUE)</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_expand">expand</code></td>
<td>
<p>The relative height of the z axis</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_theta">theta</code></td>
<td>
<p>The angle to rotate the x-y plane</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_phi">phi</code></td>
<td>
<p>The angle above the plane to view the graph</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_n">N</code></td>
<td>
<p>The grid resolution</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_nbcol">nbcol</code></td>
<td>
<p>The color resolution</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_box">box</code></td>
<td>
<p>Draw the axes</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_main">main</code></td>
<td>
<p>The main title</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_cuts">cuts</code></td>
<td>
<p>Should the graphic show cuts (e.g., cuts=c(0,0))</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_all">all</code></td>
<td>
<p>Show all four parts of the tetrachoric</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_ellipses">ellipses</code></td>
<td>
<p>Draw a correlation ellipse</p>
</td></tr>
<tr><td><code id="draw.tetra_+3A_ze">ze</code></td>
<td>
<p>height of the ellipse if requested</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A graphic demonstration of the <code><a href="#topic+tetrachoric">tetrachoric</a></code> correlation. Used for teaching purposes.  The default values are for a correlation of .5 with cuts at 1 and 1. Any other values are possible.  The code is also a demonstration of how to use the <code><a href="graphics.html#topic+layout">layout</a></code> function for complex graphics using base graphics. 
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tetrachoric">tetrachoric</a></code> to find tetrachoric correlations, <code><a href="#topic+irt.fa">irt.fa</a></code> and <code><a href="#topic+fa.poly">fa.poly</a></code> to use them in factor analyses, <code><a href="#topic+scatter.hist">scatter.hist</a></code> to show correlations and histograms. </p>


<h3>Examples</h3>

<pre><code class='language-R'>#if(require(mvtnorm)) {
#draw.tetra(.5,1,1)
#draw.tetra(.8,2,1)} else {print("draw.tetra requires the mvtnorm package")
#draw.cor(.5,cuts=c(0,0))}

draw.tetra(.5,1,1)
draw.tetra(.8,2,1)
draw.cor(.5,cuts=c(0,0))
</code></pre>

<hr>
<h2 id='dummy.code'>Create dummy coded variables</h2><span id='topic+dummy.code'></span>

<h3>Description</h3>

<p>Given a variable x with n distinct values, create n new dummy coded variables coded 0/1 for presence (1) or absence (0) of each variable.  A typical application would be to create dummy coded college majors from a vector of college majors. Can also combine categories by group.  By default, NA values of x are returned as NA (added 10/20/17)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy.code(x,group=NULL,na.rm=TRUE,top=NULL,min=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummy.code_+3A_x">x</code></td>
<td>
<p>A vector to be transformed into dummy codes</p>
</td></tr>
<tr><td><code id="dummy.code_+3A_group">group</code></td>
<td>
<p>A vector of categories to be coded as 1, all others coded as 0.</p>
</td></tr>
<tr><td><code id="dummy.code_+3A_na.rm">na.rm</code></td>
<td>
<p>If TRUE, return NA for all codes with NA in x</p>
</td></tr>
<tr><td><code id="dummy.code_+3A_top">top</code></td>
<td>
<p>If specified, then just dummy code the top values, and make the rest NA</p>
</td></tr>
<tr><td><code id="dummy.code_+3A_min">min</code></td>
<td>
<p>If specified, then dummy code all values &gt;= min</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When coding demographic information, it is typical to create one variable with multiple categorical values (e.g., ethnicity, college major, occupation). <code><a href="#topic+dummy.code">dummy.code</a></code> will convert these categories into n distinct dummy coded variables.
</p>
<p>If there are many possible values (e.g., country in the SAPA data set) then specifying top will assign dummy codes to just a subset of the data.
</p>
<p>If using dummy coded variables as predictors, remember to use n-1 variables.
</p>
<p>If group is specified, then all values of x that are in group are given the value of 1, otherwise, 0. (Useful for combining a range of science majors into STEM or not. The example forms a dummy code of any smoking at all.)
</p>


<h3>Value</h3>

<p>A matrix of dummy coded variables</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>new &lt;- dummy.code(sat.act$education)
new.sat &lt;- data.frame(new,sat.act)
round(cor(new.sat,use="pairwise"),2)
#dum.smoke &lt;- dummy.code(spi$smoke,group=2:9)
#table(dum.smoke,spi$smoke)
#dum.age &lt;- dummy.code(round(spi$age/5)*5,top=5)  #the most frequent five year blocks
</code></pre>

<hr>
<h2 id='Dwyer'>8 cognitive variables used by Dwyer for an example.
</h2><span id='topic+Dwyer'></span>

<h3>Description</h3>

<p>Dwyer (1937) introduced a technique for factor extension and used 8 cognitive variables from Thurstone.  This is the example data set used in his paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Dwyer)</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:8, 1:8] 1 0.58 -0.28 0.01 0.36 0.38 0.61 0.15 0.58 1 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : chr [1:8] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
..$ : chr [1:8] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
</p>


<h3>Source</h3>

<p>Data matrix retyped from the original publication. 
</p>


<h3>References</h3>

<p>Dwyer, Paul S. (1937), The determination of the factor loadings of a given test from the known factor loadings of other tests. Psychometrika, 3, 173-178
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Dwyer)
Ro &lt;- Dwyer[1:7,1:7]
Roe &lt;- Dwyer[1:7,8]
fo &lt;- fa(Ro,2,rotate="none")
fa.extension(Roe,fo)
</code></pre>

<hr>
<h2 id='eigen.loadings'>Convert eigen vectors and eigen values to the more normal (for psychologists) component loadings</h2><span id='topic+eigen.loadings'></span>

<h3>Description</h3>

<p> The default procedures for principal component returns values not immediately equivalent to the loadings from a factor analysis.  eigen.loadings translates them into the more typical metric of eigen vectors multiplied by the squareroot of the eigenvalues.   This lets us find pseudo factor loadings if we have used princomp  or eigen. <br />
If we use <code><a href="#topic+principal">principal</a></code> to do our principal components analysis, then we do not need this routine.</p>


<h3>Usage</h3>

<pre><code class='language-R'>eigen.loadings(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eigen.loadings_+3A_x">x</code></td>
<td>
<p>the output from eigen or a list of class princomp derived from princomp</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of Principal Component loadings more typical for what is expected in psychometrics.  That is, they are scaled by the square root of the eigenvalues.
</p>


<h3>Note</h3>

<p>Useful for SAPA analyses</p>


<h3>Author(s)</h3>

<p><a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
<a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- eigen(Harman74.cor$cov)
x$vectors[1:8,1:4]  #as they appear from eigen
y &lt;- princomp(covmat=Harman74.cor$cov) 
y$loadings[1:8,1:4] #as they appear from princomp
eigen.loadings(x)[1:8,1:4] # rescaled by the eigen values
z &lt;- pca(Harman74.cor$cov,4,rotate="none")
z$loadings[1:8,1:4]  #as they appear in pca
</code></pre>

<hr>
<h2 id='ellipses'>Plot data and 1 and 2 sigma correlation ellipses</h2><span id='topic+ellipses'></span><span id='topic+minkowski'></span>

<h3>Description</h3>

<p>For teaching correlation, it is useful to draw ellipses around the mean to reflect the correlation.  This variation of the ellipse function from John Fox's car package does so.  Input may be either two vectors or a matrix or data.frame.  In the latter cases, if the number of variables &gt;2, then the ellipses are done in the <code><a href="#topic+pairs.panels">pairs.panels</a></code> function. Ellipses may be added to existing plots. 
The minkowski function is included as a generalized ellipse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ellipses(x, y = NULL, add = FALSE, smooth=TRUE, lm=FALSE,data=TRUE, n = 2,
 span=2/3, iter=3, col = "red", xlab =NULL,ylab= NULL,size=c(1,2), ...)
minkowski(r=2,add=FALSE,main=NULL,xl=1,yl=1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ellipses_+3A_x">x</code></td>
<td>
<p>a vector,matrix, or data.frame </p>
</td></tr>
<tr><td><code id="ellipses_+3A_y">y</code></td>
<td>
<p>Optional second vector </p>
</td></tr>
<tr><td><code id="ellipses_+3A_add">add</code></td>
<td>
<p>Should a new plot be created, or should it be added to?</p>
</td></tr>
<tr><td><code id="ellipses_+3A_smooth">smooth</code></td>
<td>
<p>smooth = TRUE -&gt; draw a loess fit</p>
</td></tr>
<tr><td><code id="ellipses_+3A_lm">lm</code></td>
<td>
<p>lm=TRUE -&gt; draw the linear fit</p>
</td></tr>
<tr><td><code id="ellipses_+3A_data">data</code></td>
<td>
<p>data=TRUE implies draw the data points</p>
</td></tr>
<tr><td><code id="ellipses_+3A_n">n</code></td>
<td>
<p>Should 1 or 2 ellipses be drawn </p>
</td></tr>
<tr><td><code id="ellipses_+3A_span">span</code></td>
<td>
<p>averaging window parameter for the lowess fit</p>
</td></tr>
<tr><td><code id="ellipses_+3A_iter">iter</code></td>
<td>
<p>iteration parameter for lowess</p>
</td></tr>
<tr><td><code id="ellipses_+3A_col">col</code></td>
<td>
<p>color of ellipses (default is red</p>
</td></tr>
<tr><td><code id="ellipses_+3A_xlab">xlab</code></td>
<td>
<p>label for the x axis</p>
</td></tr>
<tr><td><code id="ellipses_+3A_ylab">ylab</code></td>
<td>
<p>label for the y axis</p>
</td></tr>
<tr><td><code id="ellipses_+3A_size">size</code></td>
<td>
<p>The size of ellipses in sd units (defaults to 1 and 2)</p>
</td></tr>
<tr><td><code id="ellipses_+3A_...">...</code></td>
<td>
<p>Other parameters for plotting</p>
</td></tr>
<tr><td><code id="ellipses_+3A_r">r</code></td>
<td>
<p>r=1 draws a city block, r=2 is a Euclidean circle, r &gt; 2 tends towards a square</p>
</td></tr>
<tr><td><code id="ellipses_+3A_main">main</code></td>
<td>
<p>title to use when drawing Minkowski circles</p>
</td></tr>
<tr><td><code id="ellipses_+3A_xl">xl</code></td>
<td>
<p>stretch the x axis</p>
</td></tr>
<tr><td><code id="ellipses_+3A_yl">yl</code></td>
<td>
<p>stretch the y axis</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ellipse dimensions are calculated from the correlation between the x and y variables and are scaled as sqrt(1+r) and sqrt(1-r). They are then scaled as size[1] and size[2] standard deviation units.   To scale  for 95 and 99 percent confidence use c(1.64,2.32)
</p>


<h3>Value</h3>

<p> A single plot (for 2 vectors or data frames with fewer than 3 variables.  Otherwise a call is made to <code><a href="#topic+pairs.panels">pairs.panels</a></code>. 
</p>


<h3>Note</h3>

<p>Adapted from John Fox's ellipse and data.ellipse functions.
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>  Galton, Francis (1888), Co-relations and their measurement. Proceedings of the Royal Society. London Series, 45, 135-145.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+pairs.panels">pairs.panels</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(psychTools::galton)
galton &lt;- psychTools::galton
ellipses(galton,lm=TRUE)

ellipses(galton$parent,galton$child,xlab="Mid Parent Height",
                  ylab="Child Height") #input are two vectors
data(sat.act)
ellipses(sat.act)  #shows the pairs.panels ellipses
minkowski(2,main="Minkowski circles")
minkowski(1,TRUE)
minkowski(4,TRUE)

</code></pre>

<hr>
<h2 id='error.bars'>Plot means and confidence intervals</h2><span id='topic+error.bars'></span><span id='topic+error.bars.tab'></span>

<h3>Description</h3>

<p>One of the many functions in R to plot means and confidence intervals. Can be done using barplots if desired.  Can also be combined with such functions as boxplot or violin to summarize distributions.  Means and standard errors are calculated from the raw data using <code><a href="#topic+describe">describe</a></code>. Alternatively, plots of means +/- one standard deviation may be drawn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error.bars(x,stats=NULL,data=NULL,group=NULL, ylab = "Dependent Variable",
	xlab="Independent Variable", main=NULL,eyes=TRUE, ylim = NULL, xlim=NULL,
	alpha=.05, sd=FALSE, labels = NULL, pos = NULL,  arrow.len = 0.05,
	arrow.col="black",add = FALSE,bars=FALSE, within=FALSE, 
	col=c("black","blue","red"), density=-10,...)

error.bars.tab(t,way="columns",raw=FALSE,col=c('blue','red'),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error.bars_+3A_x">x</code></td>
<td>
<p> A data frame or matrix of raw data OR, a formula of the form DV ~ IV.  If formula input is specified, error.bars.by is called. </p>
</td></tr>
<tr><td><code id="error.bars_+3A_t">t</code></td>
<td>
<p>A table of frequencies</p>
</td></tr>
<tr><td><code id="error.bars_+3A_stats">stats</code></td>
<td>
<p>Alternatively, a data.frame of descriptive stats from (e.g., describe). if specified, the means, sd, n and perhaps of se of a data set to be plotted</p>
</td></tr>
<tr><td><code id="error.bars_+3A_data">data</code></td>
<td>
<p>If using formula input, specify the object where the data may found</p>
</td></tr>
<tr><td><code id="error.bars_+3A_group">group</code></td>
<td>
<p>If not null, then do error.bars.by syntax</p>
</td></tr>
<tr><td><code id="error.bars_+3A_ylab">ylab</code></td>
<td>
<p>y label</p>
</td></tr>
<tr><td><code id="error.bars_+3A_xlab">xlab</code></td>
<td>
<p>x label</p>
</td></tr>
<tr><td><code id="error.bars_+3A_main">main</code></td>
<td>
<p>title for figure</p>
</td></tr>
<tr><td><code id="error.bars_+3A_ylim">ylim</code></td>
<td>
<p>if specified, the limits for the plot, otherwise based upon the data</p>
</td></tr>
<tr><td><code id="error.bars_+3A_xlim">xlim</code></td>
<td>
<p>if specified, the x limits for the plot, otherwise c(.5,nvar + .5)</p>
</td></tr>
<tr><td><code id="error.bars_+3A_eyes">eyes</code></td>
<td>
<p>should 'cats eyes' plots be drawn</p>
</td></tr>
<tr><td><code id="error.bars_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of confidence interval  &ndash; defaults to 95% confidence interval</p>
</td></tr>
<tr><td><code id="error.bars_+3A_sd">sd</code></td>
<td>
<p>if TRUE, draw one  standard deviation instead of standard errors at the alpha level</p>
</td></tr>
<tr><td><code id="error.bars_+3A_labels">labels</code></td>
<td>
<p> X axis label </p>
</td></tr>
<tr><td><code id="error.bars_+3A_pos">pos</code></td>
<td>
<p>where to place text: below, left, above, right</p>
</td></tr>
<tr><td><code id="error.bars_+3A_arrow.len">arrow.len</code></td>
<td>
<p> How long should the top of the error bars be?</p>
</td></tr>
<tr><td><code id="error.bars_+3A_arrow.col">arrow.col</code></td>
<td>
<p>What color should the error bars be?</p>
</td></tr>
<tr><td><code id="error.bars_+3A_add">add</code></td>
<td>
<p> add=FALSE, new plot, add=TRUE, just points and error bars</p>
</td></tr>
<tr><td><code id="error.bars_+3A_bars">bars</code></td>
<td>
<p>bars=TRUE will draw a bar graph if you really want to do that</p>
</td></tr>
<tr><td><code id="error.bars_+3A_within">within</code></td>
<td>
<p>should the error variance of a variable be corrected by 1-SMC?</p>
</td></tr>
<tr><td><code id="error.bars_+3A_col">col</code></td>
<td>
<p>color(s) of the catseyes.  Defaults to blue.</p>
</td></tr> 
<tr><td><code id="error.bars_+3A_density">density</code></td>
<td>
<p>If negative, solid colors, if positive, how many lines to draw</p>
</td></tr>
<tr><td><code id="error.bars_+3A_way">way</code></td>
<td>
<p>Percentages are based upon the row totals (default) column totals, or grand total of the data Table</p>
</td></tr>
<tr><td><code id="error.bars_+3A_raw">raw</code></td>
<td>
<p>If raw is FALSE, display the graphs in terms of probability, raw TRUE displays the data in terms of raw counts</p>
</td></tr>
<tr><td><code id="error.bars_+3A_...">...</code></td>
<td>
<p>other parameters to pass to the plot function, e.g., typ=&quot;b&quot; to draw lines, lty=&quot;dashed&quot; to draw dashed lines</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Drawing the mean +/- a confidence interval is a frequently used function when reporting experimental results. By default, the confidence interval is 1.96 standard errors of the t-distribution.  
</p>
<p>If within=TRUE, the error bars are corrected for the correlation with the other variables by reducing the variance by a factor of (1-smc).  This allows for comparisons between variables.
</p>
<p>The error bars are normally calculated from the data using the <code><a href="#topic+describe">describe</a></code> function.  If, alternatively, a matrix of statistics is provided with column headings of values, means, and se, then those values will be used for the plot (using the stats option).  If n is included in the matrix of statistics, then the distribution is drawn for a t distribution for n-1 df.  If n is omitted (NULL) or is NA, then the distribution will be a normal distribution.  
</p>
<p>If sd is TRUE, then the error bars will represent one standard deviation from the mean rather than be a function of alpha and the standard errors.
</p>
<p>See the last two examples for the  case of plotting data with statistics from another function. 
</p>
<p>Alternatively, <code><a href="#topic+error.bars.tab">error.bars.tab</a></code> will take tabulated data and convert to either row, column or overall percentages, and then plot these as percentages with the equivalent standard error (based upon sqrt(pq/N)).
</p>
<p>In August, 2018, the functionality of <code><a href="#topic+error.bars">error.bars</a></code> and <code><a href="#topic+error.bars.by">error.bars.by</a></code> were  combined so that if groups are specified, then the error bars are done by group.  Furthermore, if the x variable is a formula of the form DV ~ IV, then <code><a href="#topic+error.bars.by">error.bars.by</a></code> is called to do the plotting.
</p>


<h3>Value</h3>

<p>Graphic output showing the means + x
</p>
<p>These confidence regions are based upon normal theory and do not take into account any skew in the variables.  More accurate confidence intervals could be found by resampling.
</p>
<p>The error.bars.tab function will return (invisibly) the cell means and standard errors.
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+error.crosses">error.crosses</a></code> for two way error bars, <code><a href="#topic+error.bars.by">error.bars.by</a></code> for error bars for different groups as well as <code><a href="#topic+error.dots">error.dots</a></code>.
</p>
<p>The  <code><a href="#topic+error.bars.by">error.bars.by</a></code> is useful for showing the results of one or two way ANOVAs in that it will display means and CIs for one or more DVs for one or two IVs.
</p>
<p>In addition, as pointed out by Jim Lemon on the R-help news group, error bars or confidence intervals may be drawn using 
</p>

<table>
<tr>
 <td style="text-align: left;">
 function   </td><td style="text-align: left;"> package </td>
</tr>
<tr>
 <td style="text-align: left;">
 bar.err  </td><td style="text-align: left;"> (agricolae) </td>
</tr>
<tr>
 <td style="text-align: left;">
 plotCI </td><td style="text-align: left;"> (gplots) </td>
</tr>
<tr>
 <td style="text-align: left;">
 xYplot </td><td style="text-align: left;">(Hmisc) </td>
</tr>
<tr>
 <td style="text-align: left;">
 dispersion </td><td style="text-align: left;">(plotrix) </td>
</tr>
<tr>
 <td style="text-align: left;">
 plotCI </td><td style="text-align: left;">(plotrix) </td>
</tr>
<tr>
 <td style="text-align: left;">
 </td>
</tr>

</table>

<p>For advice why not to draw bar graphs  with error bars, see  the page at biostat.mc.vanderbilt.edu/wiki/Main/DynamitePlots.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
x &lt;- matrix(rnorm(1000),ncol=20)
boxplot(x,notch=TRUE,main="Notched boxplot with error bars")
error.bars(x,add=TRUE)
abline(h=0)

#show 50% confidence regions and color each variable separately
error.bars(attitude,alpha=.5,
   main="50 percent confidence limits",col=rainbow(ncol(attitude)) )
   
error.bars(attitude,bar=TRUE)  #show the use of bar graphs


#combine with a strip chart and boxplot
stripchart(attitude,vertical=TRUE,method="jitter",jitter=.1,pch=19,
           main="Stripchart with 95 percent confidence limits")
boxplot(attitude,add=TRUE)
error.bars(attitude,add=TRUE,arrow.len=.2)

#use statistics from somewhere else
#by specifying n, we are using the t distribution for confidences
#The first example allows the variables to be spaced along the x axis
my.stats &lt;- data.frame(values=c(1,2,8),mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
 error.bars(stats=my.stats,type="b",main="data with confidence intervals")
#don't connect the groups
my.stats &lt;- data.frame(values=c(1,2,8),mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
      error.bars(stats=my.stats,main="data with confidence intervals")
#by not specifying value, the groups are equally spaced
my.stats &lt;- data.frame(mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
rownames(my.stats) &lt;- c("First", "Second","Third")
error.bars(stats=my.stats,xlab="Condition",ylab="Score")


#Consider the case where we get stats from describe
temp &lt;- describe(attitude)
error.bars(stats=temp)

#show these do not differ from the other way by overlaying the two
error.bars(attitude,add=TRUE,col="red")

#n is omitted
#the error distribution is a normal distribution
my.stats &lt;- data.frame(mean=c(2,4,8),se=c(2,1,2))
rownames(my.stats) &lt;- c("First", "Second","Third")
error.bars(stats=my.stats,xlab="Condition",ylab="Score")
#n is specified
#compare this with small n which shows larger confidence regions
my.stats &lt;- data.frame(mean=c(2,4,8),se=c(2,1,2),n=c(10,10,3))
rownames(my.stats) &lt;- c("First", "Second","Third")
error.bars(stats=my.stats,xlab="Condition",ylab="Score")


#example of arrest rates (as percentage of condition)
arrest &lt;- data.frame(Control=c(14,21),Treated =c(3,23))
rownames(arrest) &lt;- c("Arrested","Not Arrested")
error.bars.tab(arrest,ylab="Probability of Arrest",xlab="Control vs Treatment",
main="Probability of Arrest varies by treatment")


#Show the raw  rates 
error.bars.tab(arrest,raw=TRUE,ylab="Number Arrested",xlab="Control vs Treatment",
main="Count of Arrest varies by treatment")

#If a grouping variable is specified, the function calls error.bars.by
#Use error.bars.by to have more control over the output.
#Show  how to use grouping variables
error.bars(SATV + SATQ ~ gender, data=sat.act) #one grouping variable, formula input
error.bars(SATV + SATQ ~ education + gender,data=sat.act)#two  grouping variables



</code></pre>

<hr>
<h2 id='error.bars.by'> Plot means and confidence intervals for multiple groups</h2><span id='topic+error.bars.by'></span>

<h3>Description</h3>

<p>One of the many functions in R to plot means and confidence intervals.  Meant mainly for demonstration purposes for showing the probabilty of replication from multiple samples.  Can also be combined with such functions as boxplot to summarize distributions.  Means and standard errors for each group are calculated using <code><a href="#topic+describeBy">describeBy</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error.bars.by(x,group,data=NULL, by.var=FALSE,x.cat=TRUE,ylab =NULL,xlab=NULL, main=NULL, 
	ylim= NULL, xlim=NULL, 
	eyes=TRUE, alpha=.05,sd=FALSE,labels=NULL,v.labels=NULL,v2.labels=NULL, 
	add.labels=NULL,pos=NULL, arrow.len=.05, min.size=1,add=FALSE, 
	bars=FALSE,within=FALSE,
	colors=c("black","blue","red"), lty,lines=TRUE, 
	legend=0,pch=16,density=-10,stats=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error.bars.by_+3A_x">x</code></td>
<td>
<p> A data frame or matrix </p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_group">group</code></td>
<td>
<p>A grouping variable</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_data">data</code></td>
<td>
<p>If using formula input, the data file must be specified</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_by.var">by.var</code></td>
<td>
<p>A different line for each group (default) or each variable</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_x.cat">x.cat</code></td>
<td>
<p>Is the grouping variable categorical (TRUE) or continuous (FALSE</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_ylab">ylab</code></td>
<td>
<p>y label</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_xlab">xlab</code></td>
<td>
<p>x label</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_main">main</code></td>
<td>
<p>title for figure</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_ylim">ylim</code></td>
<td>
<p>if specified, the y limits for the plot, otherwise based upon the data</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_xlim">xlim</code></td>
<td>
<p>if specified, the x limits for the plot, otherwise based upon the data</p>
</td></tr>  <tr><td><code id="error.bars.by_+3A_eyes">eyes</code></td>
<td>
<p>Should 'cats eyes' be drawn'</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of confidence interval.  Default is 1- alpha =95% confidence interval</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_sd">sd</code></td>
<td>
<p>sd=TRUE  will plot Standard Deviations instead of standard errors</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_labels">labels</code></td>
<td>
<p> X axis label </p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_v.labels">v.labels</code></td>
<td>
<p>For a bar plot legend, these are the variable labels, for a line plot,
the labels of the grouping variable.</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_v2.labels">v2.labels</code></td>
<td>
<p>the names for the 2nd grouping variable, if there is one</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_add.labels">add.labels</code></td>
<td>
<p>if !NULL, then add the v2.labels to the left/right of the lines (add.labels=&quot;right&quot;)</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_pos">pos</code></td>
<td>
<p>where to place text: below, left, above, right</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_arrow.len">arrow.len</code></td>
<td>
<p> How long should the top of the error bars be?</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_min.size">min.size</code></td>
<td>
<p>Draw error bars for groups &gt; min.size</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_add">add</code></td>
<td>
<p> add=FALSE, new plot, add=TRUE, just points and error bars</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_bars">bars</code></td>
<td>
<p>Draw a barplot with error bars rather than a simple plot of the means</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_within">within</code></td>
<td>
<p>Should the s.e. be corrected by the correlation with the other variables?</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_colors">colors</code></td>
<td>
<p>groups will be plotted in different colors (mod n.groups).  See the note for how to make them transparent.</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_lty">lty</code></td>
<td>
<p>line type may be specified in the case of not plotting by variables</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_lines">lines</code></td>
<td>
<p>By default, when plotting different groups, connect the groups with a line of type = lty.  If lines is FALSE, then do not connect the groups</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_legend">legend</code></td>
<td>
<p>Where should the legend be drawn: 0 (do not draw it), 1= lower right corner, 2 = bottom, 3 ... 8 continue  clockwise, 9 is the center</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_pch">pch</code></td>
<td>
<p>The first plot symbol to use.  Subsequent groups are pch + group</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_density">density</code></td>
<td>
<p>How many lines/inch should fill the cats eyes.  If missing, non-transparent colors are used.  If negative, transparent colors are used. May be a vector for different values.</p>
</td></tr>
<tr><td><code id="error.bars.by_+3A_stats">stats</code></td>
<td>
<p>if specified, the means, sd, n and perhaps of se of a data set to be plotted</p>
</td></tr> 
<tr><td><code id="error.bars.by_+3A_...">...</code></td>
<td>
<p>other parameters to pass to the plot function e.g., lty=&quot;dashed&quot; to draw dashed lines</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Drawing the mean +/- a confidence interval is a frequently used function when reporting experimental results. By default, the confidence interval is 1.96 standard errors (adjusted for the t-distribution). 
</p>
<p>Improved/modified in August, 2018 to allow formula input (see examples) as well as to more properly handle multiple groups. 
</p>
<p>Following a request for better labeling of the grouping variables, the v.lab option is implemented for line graphs as well as bar graphs.  Note that if using multiple grouping variables, the labels are for the variable with the most levels (which should be the first one.)
</p>
<p>This function was originally just a wrapper for <code><a href="#topic+error.bars">error.bars</a></code> but has been written to allow groups to be organized either as the x axis or as separate lines.
</p>
<p>If desired, a barplot with error bars can be shown. Many find this type of plot to be uninformative (e.g.,   https://biostat.mc.vanderbilt.edu/DynamitePlots ) and recommend the more standard dot plot. 
</p>
<p>Note in particular, if choosing to draw barplots, the starting value is 0.0 and setting the ylim parameter can lead to some awkward results if 0 is not included in the ylim range.  Did you really mean to draw a bar plot in this case?
</p>
<p>For up to three groups, the colors are by default &quot;black&quot;, &quot;blue&quot; and &quot;red&quot;. For more than 3 groups, they are by default rainbow colors with an alpha factor (transparency) of .5.
</p>
<p>To make colors semitransparent, set the density to a negative number.  See the last example.
</p>


<h3>Value</h3>

<p>Graphic output showing the means + x% confidence intervals for each group.  For ci=1.96, and normal data, this will be the 95% confidence region.  For ci=1, the 68% confidence region.
</p>
<p>These confidence regions are based upon normal theory and do not take into account any skew in the variables.  More accurate confidence intervals could be found by resampling.
</p>
<p>The results of describeBy are reported invisibly.
</p>


<h3>See Also</h3>

<p> See Also as <code><a href="#topic+error.crosses">error.crosses</a></code>,  <code><a href="#topic+histBy">histBy</a></code>,  <code><a href="#topic+scatterHist">scatterHist</a></code>, <code><a href="#topic+error.bars">error.bars</a></code> and <code><a href="#topic+error.dots">error.dots</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(sat.act)
#The generic plot of variables by group
error.bars.by( SATV + SATQ ~ gender,data=sat.act)  #formula input 
error.bars.by( SATV + SATQ ~ gender,data=sat.act,v.lab=cs(male,female)) #labels 
error.bars.by(SATV + SATQ ~ education + gender, data =sat.act) #see below
error.bars.by(sat.act[1:4],sat.act$gender,legend=7) #specification of variables
error.bars.by(sat.act[1:4],sat.act$gender,legend=7,labels=cs(male,female))

#a bar plot
error.bars.by(sat.act[5:6],sat.act$gender,bars=TRUE,labels=c("male","female"),
    main="SAT V and SAT Q by gender",ylim=c(0,800),colors=c("red","blue"),
    legend=5,v.labels=c("SATV","SATQ"))  #draw a barplot
#a bar plot of SAT by age -- not recommended, see the next plot
error.bars.by(SATV + SATQ ~ education,data=sat.act,bars=TRUE,xlab="Education",
   main="95 percent confidence limits of Sat V and Sat Q", ylim=c(0,800),
   v.labels=c("SATV","SATQ"),colors=c("red","blue") )
#a better graph uses points not bars
#use formulat input
  #plot SAT V and SAT Q by education
error.bars.by(SATV + SATQ ~ education,data=sat.act,TRUE, xlab="Education",
    legend=5,labels=colnames(sat.act[5:6]),ylim=c(525,700),
     main="self reported SAT scores by education",
     v.lab =c("HS","in coll", "&lt; 16", "BA/BS", "in Grad", "Grad/Prof"))
#make the cats eyes semi-transparent by specifying a negative density

error.bars.by(SATV + SATQ ~ education,data=sat.act, xlab="Education",
    legend=5,labels=c("SATV","SATQ"),ylim=c(525,700),
     main="self reported SAT scores by education",density=-10,
     v.lab =c("HS","in coll", "&lt; 16", "BA/BS", "in Grad", "Grad/Prof"))

#use labels to specify the 2nd grouping variable, v.lab to specify the first
error.bars.by(SATV  ~ education  + gender,data=sat.act, xlab="Education",
    legend=5,labels=cs(male,female),ylim=c(525,700),
     main="self reported SAT scores by education",density=-10,
     v.lab =c("HS","in coll", "&lt; 16", "BA/BS", "in Grad", "Grad/Prof"),
     colors=c("red","blue"))


#now for a more complicated examples using 25 big 5 items scored into 5 scales
#and showing age trends by decade 
#this shows how to convert many levels of a grouping variable (age) into more manageable levels.
data(bfi)   #The Big 5 data
#first create the keys 
 keys.list &lt;- list(Agree=c(-1,2:5),Conscientious=c(6:8,-9,-10),
        Extraversion=c(-11,-12,13:15),Neuroticism=c(16:20),Openness = c(21,-22,23,24,-25))
 keys &lt;- make.keys(psychTools::bfi,keys.list)
 #then create the scores for those older than 10 and less than 80
 bfis &lt;- subset(psychTools::bfi,((psychTools::bfi$age &gt; 10) &amp; (psychTools::bfi$age &lt; 80)))

 scores &lt;- scoreItems(keys,bfis,min=1,max=6) #set the right limits for item reversals
 #now draw the results by age
#specify the particular colors to use
 error.bars.by(scores$scores,round(bfis$age/10)*10,by.var=TRUE,
      main="BFI age trends",legend=3,labels=colnames(scores$scores),
        xlab="Age",ylab="Mean item score",
        colors=cs(green,yellow,black,red,blue),
        v.labels =cs(10-14,15-24,25-34,35-44,45-54,55-64,65-74))
#show transparency 
 error.bars.by(scores$scores,round(bfis$age/10)*10,by.var=TRUE,
      main="BFI age trends",legend=3,labels=colnames(scores$scores),
        xlab="Age",ylab="Mean item score", density=-10, 
        colors=cs(green,yellow,black,red,blue),
        v.labels =cs(10-14,15-24,25-34,35-44,45-54,55-64,65-74))
</code></pre>

<hr>
<h2 id='error.crosses'> Plot x and y error bars </h2><span id='topic+error.crosses'></span>

<h3>Description</h3>

<p>Given two vectors of data (X and Y), plot the means and show standard errors in both X and Y directions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error.crosses(x,y,labels=NULL,main=NULL,xlim=NULL,ylim= NULL, 
xlab=NULL,ylab=NULL,pos=NULL,offset=1,arrow.len=.2,alpha=.05,sd=FALSE,add=FALSE,
colors=NULL,col.arrows=NULL,col.text=NULL,...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error.crosses_+3A_x">x</code></td>
<td>
<p> A vector of data or summary statistics (from Describe) </p>
</td></tr>
<tr><td><code id="error.crosses_+3A_y">y</code></td>
<td>
<p> A second vector of data or summary statistics (also from Describe)</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_labels">labels</code></td>
<td>
<p>the names of each pair &ndash; defaults to rownames of x </p>
</td></tr>
<tr><td><code id="error.crosses_+3A_main">main</code></td>
<td>
<p>The title for the graph</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_xlim">xlim</code></td>
<td>
<p>xlim values if desired&ndash; defaults to min and max mean(x) +/- 2 se</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_ylim">ylim</code></td>
<td>
<p>ylim values if desired  &ndash; defaults to min and max mean(y) +/- 2 se</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_xlab">xlab</code></td>
<td>
<p>label for x axis &ndash; grouping variable 1</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_ylab">ylab</code></td>
<td>
<p>label for y axis &ndash; grouping variable 2</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_pos">pos</code></td>
<td>
<p>Labels are located where with respect to the mean?  </p>
</td></tr>
<tr><td><code id="error.crosses_+3A_offset">offset</code></td>
<td>
<p>Labels are then offset from this location</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_arrow.len">arrow.len</code></td>
<td>
<p> Arrow length </p>
</td></tr>
<tr><td><code id="error.crosses_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of error bars </p>
</td></tr>
<tr><td><code id="error.crosses_+3A_sd">sd</code></td>
<td>
<p>if sd is TRUE, then draw means +/- 1 sd)</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_add">add</code></td>
<td>
<p>if TRUE, overlay the values with a prior plot</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_colors">colors</code></td>
<td>
<p>What color(s) should be used for the plot character? Defaults to black</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_col.arrows">col.arrows</code></td>
<td>
<p>What color(s) should be used for the arrows &ndash; defaults to colors</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_col.text">col.text</code></td>
<td>
<p>What color(s) should be used for the text &ndash; defaults to colors</p>
</td></tr>
<tr><td><code id="error.crosses_+3A_...">...</code></td>
<td>
<p> Other parameters for plot  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an example of two way error bars describing the effects of mood manipulations upon positive and negative affect, see <a href="https://personality-project.org/revelle/publications/happy-sad-appendix/FIG.A-6.pdf">https://personality-project.org/revelle/publications/happy-sad-appendix/FIG.A-6.pdf</a>
</p>
<p>The second example shows how error crosses can be done for multiple variables where the grouping variable is found dynamically. The <code><a href="#topic+errorCircles">errorCircles</a></code> example shows how to do this in one step.
</p>


<h3>Author(s)</h3>

<p> William Revelle <br />     
<a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>See Also</h3>

<p>To draw error bars for single variables  <code><a href="#topic+error.bars">error.bars</a></code>, or by groups <code><a href="#topic+error.bars.by">error.bars.by</a></code>, or to find descriptive statistics <code><a href="#topic+describe">describe</a></code>  or descriptive statistics by a grouping variable <code><a href="#topic+describeBy">describeBy</a></code> and <code><a href="#topic+statsBy">statsBy</a></code>. 
</p>
<p>A much improved version is now called <code><a href="#topic+errorCircles">errorCircles</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>
#just draw one pair of variables
desc &lt;- describe(attitude)
x &lt;- desc[1,]
y &lt;- desc[2,]
error.crosses(x,y,xlab=rownames(x),ylab=rownames(y))   

#now for a bit more complicated plotting 
data(psychTools::bfi)
desc &lt;- describeBy(psychTools::bfi[1:25],psychTools::bfi$gender) #select a high and low group
error.crosses(desc$'1',desc$'2',ylab="female scores",
   xlab="male scores",main="BFI scores by gender")
 abline(a=0,b=1)
 
#do it from summary statistics  (using standard errors) 
g1.stats &lt;- data.frame(n=c(10,20,30),mean=c(10,12,18),se=c(2,3,5))
g2.stats &lt;- data.frame(n=c(15,20,25),mean=c(6,14,15),se =c(1,2,3))
error.crosses(g1.stats,g2.stats)

#Or, if you prefer to draw +/- 1 sd. instead of 95% confidence
g1.stats &lt;- data.frame(n=c(10,20,30),mean=c(10,12,18),sd=c(2,3,5))
g2.stats &lt;- data.frame(n=c(15,20,25),mean=c(6,14,15),sd =c(1,2,3))
error.crosses(g1.stats,g2.stats,sd=TRUE)

#and seem even fancy plotting: This is taken from a study of mood
#four films were given (sad, horror, neutral, happy)
#with a pre and post test
data(psychTools::affect)
colors &lt;- c("black","red","green","blue")
films &lt;- c("Sad","Horror","Neutral","Happy")
affect.mat &lt;- describeBy(psychTools::affect[10:17],psychTools::affect$Film,mat=TRUE)
 error.crosses(affect.mat[c(1:4,17:20),],affect.mat[c(5:8,21:24),],
    labels=films[affect.mat$group1],xlab="Energetic Arousal",
     ylab="Tense Arousal",colors = 
     colors[affect.mat$group1],pch=16,cex=2)

</code></pre>

<hr>
<h2 id='error.dots'>Show a  dot.chart with error bars for different groups or variables</h2><span id='topic+error.dots'></span>

<h3>Description</h3>

<p>Yet one more of the graphical ways of showing data with error bars for different groups.
A dot.chart with error bars for different groups or variables is found using from <code><a href="#topic+describe">describe</a></code>,
<code><a href="#topic+describeBy">describeBy</a></code>,  <code><a href="#topic+statsBy">statsBy</a></code>, <code><a href="#topic+corCi">corCi</a></code>, <code><a href="#topic+corr.test">corr.test</a></code> or data from <code><a href="#topic+bestScales">bestScales</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error.dots(x=NULL, var = NULL, se = NULL, group = NULL, sd = FALSE, effect=NULL,
stats=NULL, head = 12, tail = 12, sort = TRUE, decreasing = TRUE, main = NULL,
 alpha = 0.05, eyes = FALSE,items=FALSE, min.n = NULL, max.labels = 40, labels = NULL,
 label.width=NULL, select=NULL,
 groups = NULL, gdata = NULL, cex =  par("cex"),  pt.cex = cex, pch = 21, gpch = 21, 
 bg = par("bg"), fg=par("fg"), color = par("fg"), gcolor = par("fg"), 
 lcolor = "gray", xlab = NULL, ylab = NULL, xlim = NULL,add=FALSE,order=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error.dots_+3A_x">x</code></td>
<td>
<p>A data frame or matrix of raw data, or the resulting object from <code><a href="#topic+describe">describe</a></code>, 
<code><a href="#topic+describeBy">describeBy</a></code>,  <code><a href="#topic+statsBy">statsBy</a></code>,  <code><a href="#topic+bestScales">bestScales</a></code>,
<code><a href="#topic+corCi">corCi</a></code>, <code><a href="#topic+corr.test">corr.test</a></code>, or <code><a href="#topic+cohen.d">cohen.d</a></code>
</p>
</td></tr>
<tr><td><code id="error.dots_+3A_var">var</code></td>
<td>
<p>The variable to show (particularly if doing describeBy or StatsBy plots).</p>
</td></tr>
<tr><td><code id="error.dots_+3A_se">se</code></td>
<td>
<p>Source of a standard error</p>
</td></tr>
<tr><td><code id="error.dots_+3A_group">group</code></td>
<td>
<p>A grouping variable, if desired. Will group the data on group for one variable (var) </p>
</td></tr>
<tr><td><code id="error.dots_+3A_sd">sd</code></td>
<td>
<p>if FALSE, confidence intervals in terms of standard errors, otherwise draw one standard deviation</p>
</td></tr>
<tr><td><code id="error.dots_+3A_effect">effect</code></td>
<td>
<p>Should the data be compared to a specified group (with mean set to 0) in effect size units?</p>
</td></tr> 
<tr><td><code id="error.dots_+3A_stats">stats</code></td>
<td>
<p>A matrix of means and se to use instead of finding them from the data</p>
</td></tr>
<tr><td><code id="error.dots_+3A_head">head</code></td>
<td>
<p>The number of largest values to report</p>
</td></tr>
<tr><td><code id="error.dots_+3A_tail">tail</code></td>
<td>
<p>The number of smallest values to report</p>
</td></tr>     
<tr><td><code id="error.dots_+3A_sort">sort</code></td>
<td>
<p>Sort the groups/variables by value</p>
</td></tr>
<tr><td><code id="error.dots_+3A_decreasing">decreasing</code></td>
<td>
<p>Should they be sorted in increasing or decreasing order (from top to bottom)</p>
</td></tr>
<tr><td><code id="error.dots_+3A_main">main</code></td>
<td>
<p>The caption for the figure</p>
</td></tr>
<tr><td><code id="error.dots_+3A_alpha">alpha</code></td>
<td>
<p>p value for confidence intervals</p>
</td></tr>
<tr><td><code id="error.dots_+3A_eyes">eyes</code></td>
<td>
<p>Draw catseyes for error limits</p>
</td></tr>
<tr><td><code id="error.dots_+3A_items">items</code></td>
<td>
<p>If showing results from best scales, show the items for a specified dv</p>
</td></tr>
<tr><td><code id="error.dots_+3A_min.n">min.n</code></td>
<td>
<p>If using describeBy or statsBy, what should be the minimum sample size to draw</p>
</td></tr>
<tr><td><code id="error.dots_+3A_max.labels">max.labels</code></td>
<td>
<p>Length of labels  (truncate after this value)</p>
</td></tr>
<tr><td><code id="error.dots_+3A_labels">labels</code></td>
<td>
<p>Specify the labels versus find them from the row names</p>
</td></tr>
<tr><td><code id="error.dots_+3A_label.width">label.width</code></td>
<td>
<p>Truncate after labels.width</p>
</td></tr>
<tr><td><code id="error.dots_+3A_select">select</code></td>
<td>
<p>Scale the plot for all the variables, but just show the select variables</p>
</td></tr>
<tr><td><code id="error.dots_+3A_groups">groups</code></td>
<td>
<p>ignored: to be added eventually</p>
</td></tr>
<tr><td><code id="error.dots_+3A_gdata">gdata</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="error.dots_+3A_cex">cex</code></td>
<td>
<p>The standard meaning of cex for graphics</p>
</td></tr>
<tr><td><code id="error.dots_+3A_pt.cex">pt.cex</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="error.dots_+3A_pch">pch</code></td>
<td>
<p>Plot character of the mean</p>
</td></tr>
<tr><td><code id="error.dots_+3A_gpch">gpch</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="error.dots_+3A_bg">bg</code></td>
<td>
<p>background color (of the dots showing the means)</p>
</td></tr>
<tr><td><code id="error.dots_+3A_fg">fg</code></td>
<td>
<p>foreground color (of the line segments)</p>
</td></tr>
<tr><td><code id="error.dots_+3A_color">color</code></td>
<td>
<p>Color of the text labels</p>
</td></tr>
<tr><td><code id="error.dots_+3A_gcolor">gcolor</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="error.dots_+3A_lcolor">lcolor</code></td>
<td>
<p>ignored until groups are implemented </p>
</td></tr>
<tr><td><code id="error.dots_+3A_xlab">xlab</code></td>
<td>
<p>Label the x axis, if NULL, the variable name is used</p>
</td></tr>
<tr><td><code id="error.dots_+3A_ylab">ylab</code></td>
<td>
<p>If NULL, then the group rownames are used</p>
</td></tr>
<tr><td><code id="error.dots_+3A_xlim">xlim</code></td>
<td>
<p>If NULL, then calculated to show nice values</p>
</td></tr>
<tr><td><code id="error.dots_+3A_add">add</code></td>
<td>
<p>If TRUE, will add the plot to a previous plot (e.g., from dotchart)</p>
</td></tr>
<tr><td><code id="error.dots_+3A_order">order</code></td>
<td>
<p>if sort=TRUE, if order is NULL, sort on values, otherwise, if order is returned from a previous figure, use that order. </p>
</td></tr>
<tr><td><code id="error.dots_+3A_...">...</code></td>
<td>
<p>And any other graphic parameters we have forgotten</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adapted from the dot.chart function to include error bars and to use the output of <code><a href="#topic+describe">describe</a></code>,  <code><a href="#topic+describeBy">describeBy</a></code>,   <code><a href="#topic+statsBy">statsBy</a></code>, <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+bestScales">bestScales</a></code> or <code><a href="#topic+cohen.d">cohen.d</a></code>.   
To speed up multiple plots, the function can work from the output of a previous run.  Thus describeBy will be done and the results can be show for multiple variables.
</p>
<p>If using the add=TRUE option to add an error.dots plot to a dotplot, note that the order of variables in dot plots goes from last to first (highest y value is actually the last value in a vector.)  Also note that the xlim parameter should be set to make sure the plots line up correctly.</p>


<h3>Value</h3>

<p>Returns (invisibily) either a describeBy or describe object as well as the order if sorted
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Used in particular for showing https://sapa-project.org output.</p>


<h3>See Also</h3>

<p><code><a href="#topic+describe">describe</a></code>,  <code><a href="#topic+describeBy">describeBy</a></code>, or  <code><a href="#topic+statsBy">statsBy</a></code> as well as  <code><a href="#topic+error.bars">error.bars</a></code>,  <code><a href="#topic+error.bars.by">error.bars.by</a></code>,  <code><a href="#topic+statsBy">statsBy</a></code>, <code><a href="#topic+bestScales">bestScales</a></code> or <code><a href="#topic+cohen.d">cohen.d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>temp &lt;- error.dots(psychTools::bfi[1:25],sort=TRUE, 
xlab="Mean score for the item, sorted by difficulty")
error.dots(psychTools::bfi[1:25],sort=TRUE, order=temp$order,
 add=TRUE, eyes=TRUE) #over plot with eyes

error.dots(psychTools::ability,eyes=TRUE, xlab="Mean score for the item")

cd &lt;- cohen.d(psychTools::bfi[1:26],"gender")
temp &lt;- error.dots(cd, select=c(1:15,21:25),head=12,tail=13,
      main="Cohen d and confidence intervals of BFI by gender")
error.dots(cd,select=c(16:20),head=13,tail=12,col="blue",add=TRUE,fg="red" ,main="")
abline(v=0)
#now show cis for correlations
R &lt;- corCi(attitude,plot=FALSE)
error.dots(R, sort=FALSE)
#the asymmetric case
R &lt;- corr.test(attitude[,1:2],attitude[,3:7])
error.dots(R, sort=FALSE)
</code></pre>

<hr>
<h2 id='errorCircles'>Two way plots of means, error bars, and sample sizes</h2><span id='topic+errorCircles'></span>

<h3>Description</h3>

<p>Given a matrix or data frame, data, find statistics based upon a grouping variable and then plot x and y means with error bars for each value of the grouping variable.  If the data are paired (e.g. by gender), then plot means and error bars for the two groups on all variables. </p>


<h3>Usage</h3>

<pre><code class='language-R'>errorCircles(x, y, data, ydata = NULL, group=NULL, paired = FALSE, labels = NULL, 
main = NULL, xlim = NULL, ylim = NULL, xlab = NULL, ylab = NULL,add=FALSE, pos = NULL,
 offset = 1, arrow.len = 0.2, alpha = 0.05, sd = FALSE, bars = TRUE, circles = TRUE,
 colors=NULL,col.arrows=NULL,col.text=NULL,circle.size=1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errorCircles_+3A_x">x</code></td>
<td>
<p>The x variable (by name or number) to plot</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_y">y</code></td>
<td>
<p>The y variable (name or number) to plot</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_data">data</code></td>
<td>
<p>The matrix or data.frame to use for the x data</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_ydata">ydata</code></td>
<td>
<p>If plotting data from two data.frames, then the y variable of the ydata frame will be used.</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_group">group</code></td>
<td>
<p>If specified, then <code><a href="#topic+statsBy">statsBy</a></code> is called first to find the statistics by group</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_paired">paired</code></td>
<td>
<p>If TRUE, plot all x and y variables for the two values of the grouping variable.</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_labels">labels</code></td>
<td>
<p>Variable names</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_main">main</code></td>
<td>
<p>Main title for plot</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_xlim">xlim</code></td>
<td>
<p>xlim values if desired&ndash; defaults to min and max mean(x) +/- 2 se</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_ylim">ylim</code></td>
<td>
<p>ylim values if desired  &ndash; defaults to min and max mean(y) +/- 2 se</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_xlab">xlab</code></td>
<td>
<p>label for x axis &ndash; grouping variable 1</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_ylab">ylab</code></td>
<td>
<p>label for y axis &ndash; grouping variable 2</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_add">add</code></td>
<td>
<p>If TRUE, add to the prior plot</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_pos">pos</code></td>
<td>
<p>Labels are located where with respect to the mean?  </p>
</td></tr>
<tr><td><code id="errorCircles_+3A_offset">offset</code></td>
<td>
<p>Labels are then offset from this location</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_arrow.len">arrow.len</code></td>
<td>
<p> Arrow length </p>
</td></tr>
<tr><td><code id="errorCircles_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of error bars </p>
</td></tr>
<tr><td><code id="errorCircles_+3A_sd">sd</code></td>
<td>
<p>if sd is TRUE, then draw means +/- 1 sd)</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_bars">bars</code></td>
<td>
<p>Should error.bars be drawn for both x and y </p>
</td></tr>
<tr><td><code id="errorCircles_+3A_circles">circles</code></td>
<td>
<p>Should circles representing the relative sample sizes be drawn?</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_colors">colors</code></td>
<td>
<p>Plot the points using colors &ndash; default is black</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_col.text">col.text</code></td>
<td>
<p>What color for the text labels (defaults to colors)</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_col.arrows">col.arrows</code></td>
<td>
<p>What color should the arrows and circles be?  Defaults to colors</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_circle.size">circle.size</code></td>
<td>
<p>A scaling parameter for the error.circles.  Defaults to 1, but can be adjusted downwards to make them less instrusive.</p>
</td></tr>
<tr><td><code id="errorCircles_+3A_...">...</code></td>
<td>
<p> Other parameters for plot  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>When visualizing the effect of an experimental manipulation or the relationship of multiple groups, it is convenient to plot their means as well as their confidence regions in a two dimensional space.  
</p>
<p>The diameter of the enclosing circle (ellipse) scales as 1/sqrt(N) * the maximum standard error of all variables.  That is to say, the area of the ellipse reflects sample size.  
</p>


<h3>Value</h3>

<p>If the group variable is specified, then the statistics from <code><a href="#topic+statsBy">statsBy</a></code> are (invisibly) returned.  
</p>


<h3>Note</h3>

<p>Basically this is a combination (and improvement) of <code><a href="#topic+statsBy">statsBy</a></code> with <code><a href="#topic+error.crosses">error.crosses</a></code>.  Can also serve some of the functionality of <code><a href="#topic+error.bars.by">error.bars.by</a></code> (see the last example).
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+statsBy">statsBy</a></code>, <code><a href="#topic+describeBy">describeBy</a></code>, <code><a href="#topic+error.crosses">error.crosses</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#BFI scores for males and females
errorCircles(1:25,1:25,data=psychTools::bfi,group="gender",paired=TRUE,ylab="female scores",
      xlab="male scores",main="BFI scores by gender")
 abline(a=0,b=1)
#drop the circles since all samples are the same sizes
errorCircles(1:25,1:25,data=psychTools::bfi,group="gender",paired=TRUE,circles=FALSE,
     ylab="female scores",xlab="male scores",main="BFI scores by gender")
 abline(a=0,b=1)
 
 data(psychTools::affect)
colors &lt;- c("black","red","white","blue")
films &lt;- c("Sad","Horror","Neutral","Happy")
affect.stats &lt;- errorCircles("EA2","TA2",data=psychTools::affect[-c(1,20)],
group="Film",labels=films,
      xlab="Energetic Arousal",ylab="Tense Arousal",ylim=c(10,22),xlim=c(8,20), 
      pch=16,cex=2,colors=colors, main ="EA and TA pre and post affective movies")
#now, use the stats from the prior run 
errorCircles("EA1","TA1",data=affect.stats,labels=films,pch=16,cex=2,colors=colors,add=TRUE)

#show sample size with the size of the circles
errorCircles("SATV","SATQ",sat.act,group="education")

#Can also provide error.bars.by functionality
 errorCircles(2,5,group=2,data=sat.act,circles=FALSE,pch=16,colors="blue",
      ylim= c(200,800),main="SATV by education",labels="")
 #just do the breakdown and then show the points
# errorCircles(3,5,group=3,data=sat.act,circles=FALSE,pch=16,colors="blue",
#         ylim= c(200,800),main="SATV by age",labels="",bars=FALSE)


</code></pre>

<hr>
<h2 id='esem'>Perform and Exploratory Structural Equation Model (ESEM) by using factor extension techniques</h2><span id='topic+esem'></span><span id='topic+esemDiagram'></span><span id='topic+esem.diagram'></span><span id='topic+interbattery'></span><span id='topic+cancorDiagram'></span>

<h3>Description</h3>

<p>Structural Equation Modeling (SEM) is a powerful tool for confirming multivariate structures and is well done by the lavaan, sem, or OpenMx packages. Because they are confirmatory, SEM  models test specific models.  Exploratory Structural Equation Modeling (ESEM), on the other hand, takes a more exploratory approach.  By using factor extension, it is possible to extend the factors of one set of variables (X) into the variable space of another set (Y). Using this technique, it is then possible to estimate the correlations between the two sets of latent variables, much the way normal SEM would do.  Based upon exploratory factor analysis (EFA) this approach provides a quick and easy approach to do exploratory structural equation modeling.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>esem(r, varsX, varsY, nfX = 1, nfY = 1, n.obs = NULL, fm = "minres", 
      rotate = "oblimin", rotateY="oblimin", plot = TRUE, cor = "cor",
       use = "pairwise",weight=NULL, ...)
esemDiagram(esem=NULL,labels=NULL,cut=.3,errors=FALSE,simple=TRUE,
	regression=FALSE,lr=TRUE, digits=1,e.size=.1,adj=2,
     main="Exploratory Structural Model", ...)
esem.diagram(esem=NULL,labels=NULL,cut=.3,errors=FALSE,simple=TRUE,
	regression=FALSE,lr=TRUE, digits=1,e.size=.1,adj=2,
     main="Exploratory Structural Model", ...) #deprecated
cancorDiagram(fit, cut=.1,digits=2, nfX = NULL, nfY = NULL,simple=FALSE,e.size=.1,
     main="Canonical Correlation",...)    
interbattery(r, varsX, varsY, nfX = 1, nfY = 1, n.obs = NULL,cor = "cor", 
       use = "pairwise",weight=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="esem_+3A_r">r</code></td>
<td>
<p>A correlation matrix or a raw data matrix suitable for factor analysis</p>
</td></tr>
<tr><td><code id="esem_+3A_varsx">varsX</code></td>
<td>
<p>The variables defining set X</p>
</td></tr>
<tr><td><code id="esem_+3A_varsy">varsY</code></td>
<td>
<p>The variables defining set Y</p>
</td></tr>
<tr><td><code id="esem_+3A_nfx">nfX</code></td>
<td>
<p>The number of factors to extract for the X variables</p>
</td></tr>
<tr><td><code id="esem_+3A_nfy">nfY</code></td>
<td>
<p>The number of factors to extract for the Y variables</p>
</td></tr>
<tr><td><code id="esem_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations (needed for eBIC and chi square), can be ignored.</p>
</td></tr>
<tr><td><code id="esem_+3A_fm">fm</code></td>
<td>
<p>The factor  method to use, e.g., &quot;minres&quot;, &quot;mle&quot; etc.  (see fa for details)</p>
</td></tr>
<tr><td><code id="esem_+3A_rotate">rotate</code></td>
<td>
<p>Which rotation to use. (see fa for details)</p>
</td></tr>
<tr><td><code id="esem_+3A_rotatey">rotateY</code></td>
<td>
<p>Which rotatation to use for the Y variables.</p>
</td></tr>
<tr><td><code id="esem_+3A_plot">plot</code></td>
<td>
<p>If TRUE, draw the esemDiagram</p>
</td></tr>
<tr><td><code id="esem_+3A_cor">cor</code></td>
<td>
<p>What options for to use for correlations (see fa for details)</p>
</td></tr>
<tr><td><code id="esem_+3A_use">use</code></td>
<td>
<p>&quot;pairwise&quot; for pairwise complete data, for other options see cor</p>
</td></tr>
<tr><td><code id="esem_+3A_weight">weight</code></td>
<td>
<p>Weights to apply to cases when finding wt.cov</p>
</td></tr>
<tr><td><code id="esem_+3A_...">...</code></td>
<td>
<p>other parameters to pass to fa or to esemDiagram functions.</p>
</td></tr>
<tr><td><code id="esem_+3A_esem">esem</code></td>
<td>
<p>The object returned from esem and passed to esemDiagram</p>
</td></tr>
<tr><td><code id="esem_+3A_fit">fit</code></td>
<td>
<p>The object returned by lmCor with canonical variates</p>
</td></tr>
<tr><td><code id="esem_+3A_labels">labels</code></td>
<td>
<p> Variable labels </p>
</td></tr>
<tr><td><code id="esem_+3A_cut">cut</code></td>
<td>
<p> Loadings with abs(loading) &gt; cut will be shown </p>
</td></tr>
<tr><td><code id="esem_+3A_simple">simple</code></td>
<td>
<p>Only the biggest loading per item is shown</p>
</td></tr>
<tr><td><code id="esem_+3A_errors">errors</code></td>
<td>
<p>include error estimates (as arrows)</p>
</td></tr>
<tr><td><code id="esem_+3A_e.size">e.size</code></td>
<td>
<p>size of ellipses (adjusted by the number of variables)</p>
</td></tr>
<tr><td><code id="esem_+3A_digits">digits</code></td>
<td>
<p>Round coefficient to digits</p>
</td></tr>
<tr><td><code id="esem_+3A_adj">adj</code></td>
<td>
<p>loadings are adjusted by factor number mod adj to decrease likelihood of overlap</p>
</td></tr>
<tr><td><code id="esem_+3A_main">main</code></td>
<td>
<p> Graphic title, defaults to &quot;Exploratory Structural Model&quot; </p>
</td></tr>
<tr><td><code id="esem_+3A_lr">lr</code></td>
<td>
<p>draw the graphic left to right (TRUE) or top to bottom (FALSE)</p>
</td></tr>
<tr><td><code id="esem_+3A_regression">regression</code></td>
<td>
<p>Not yet implemented</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Factor analysis as implemented in <code><a href="#topic+fa">fa</a></code> attempts to  summarize the covariance (correlational) structure of a set of variables with a small set of latent variables or &ldquo;factors&quot;.  This solution may be &lsquo;extended&rsquo; into a larger space with more variables without changing the original solution (see <code><a href="#topic+fa.extension">fa.extension</a></code>.  Similarly,  the factors of a second set of variables  (the Y set) may be extended into the original (X ) set.  Doing so allows two independent measurement models, a measurement model for X and a measurement model for Y.  These two sets of latent variables may then be correlated  for an Exploratory Structural Equation Model.  (This is exploratory because it is based upon exploratory factor analysis (EFA) rather than a confirmatory factor model (CFA) using more traditional Structural Equation Modeling packages such as sem, lavaan, or Mx.)
</p>
<p>Although the output seems very similar to that of a normal EFA using  <code><a href="#topic+fa">fa</a></code>, it is actually two independent factor analyses (of the X and the Y sets) that are then mutually extended into each other.  That is, the loadings and structure matrices from sets X and Y are merely combined, and the correlations between the two sets of factors are found.
</p>
<p>Interbattery factor analysis was developed by Tucker (1958) as a way of comparing the factors in common to two batteries of tests.   (Currently under development and not yet complete). Using some straight forward linear algebra It is easy to find the factors of the intercorrelations between the two sets of variables.  This does not require estimating communalities and is highly related to the procedures of canonical correlation.  
</p>
<p>The difference between the esem and the interbattery approach is that the first factors the X set and then relates those factors to factors of the Y set.  Interbattery factor analysis, on the other hand, tries to find one set of factors that links both sets but is still distinct from factoring both sets together.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>communality</code></td>
<td>
<p>The amount of variance in each of the X and Y variables accounted for by the total model.</p>
</td></tr>
<tr><td><code>sumsq</code></td>
<td>
<p>The amount of variance accounted for by each factor &ndash; independent of the other factors.</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>Degrees of freedom of the model.  </p>
</td></tr>
<tr><td><code>esem.dof</code></td>
<td>
<p>Alternatively consider the df1 for the X model and df2 for the Y model.  esem.dof = df1 + df2</p>
</td></tr>
<tr><td><code>null.dof</code></td>
<td>
<p>Degrees of freedom of the null  model (the correlation matrix)</p>
</td></tr>
<tr><td><code>ENull</code></td>
<td>
<p>chi square of the null model</p>
</td></tr>
<tr><td><code>chi</code></td>
<td>
<p>chi square of the model.  This is found by examining the size of the residuals compared to their standard error.</p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>The root mean square of the residuals.</p>
</td></tr>
<tr><td><code>nh</code></td>
<td>
<p>Harmonic sample size if using min.chi for factor extraction.</p>
</td></tr>
<tr><td><code>EPVAL</code></td>
<td>
<p>Probability  of the Emprical Chi Square given the hypothesis of an identity matrix.</p>
</td></tr>
<tr><td><code>crms</code></td>
<td>
<p>Adjusted root mean square residual</p>
</td></tr>
<tr><td><code>EBIC</code></td>
<td>
<p>When normal theory fails (e.g., in the case of non-positive definite matrices), it useful to examine the empirically derived EBIC based upon the empirical <code class="reqn">\chi^2</code> - 2 df. </p>
</td></tr>
<tr><td><code>ESABIC</code></td>
<td>
<p>Sample size adjusted empirical BIC</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>sum of squared residuals versus sum of squared original values</p>
</td></tr>
<tr><td><code>fit.off</code></td>
<td>
<p>fit applied to the off diagonal elements</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of the residuals</p>
</td></tr>
<tr><td><code>factors</code></td>
<td>
<p>Number of factors extracted</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>Item complexity</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>Number of total observations</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>The factor pattern matrix for the combined X and Y factors</p>
</td></tr>
<tr><td><code>Structure</code></td>
<td>
<p>The factor structure matrix for the combined X and Y factors</p>
</td></tr> 
<tr><td><code>loadsX</code></td>
<td>
<p>Just the X set of loadings (pattern) without  the extension variables.</p>
</td></tr>
<tr><td><code>loadsY</code></td>
<td>
<p>Just the Y set of loadings (pattern) without the extension variables.</p>
</td></tr>      <tr><td><code>PhiX</code></td>
<td>
<p>The correlations of the X factors</p>
</td></tr>
<tr><td><code>PhiY</code></td>
<td>
<p>the correlations of the Y factors</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>the correlations of the X and Y factors within the selves and across sets.</p>
</td></tr>
<tr><td><code>fm</code></td>
<td>
<p>The factor method used</p>
</td></tr>
<tr><td><code>fx</code></td>
<td>
<p>The complete factor analysis output for the X set</p>
</td></tr>
<tr><td><code>fy</code></td>
<td>
<p>The complete factor analysis output for the Y set</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>The residual correlation matrix (R - model).  May be examined by a call to residual().</p>
</td></tr>    
<tr><td><code>Call</code></td>
<td>
<p>Echo back the original call to the function.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>model code for SEM and for lavaan to do subsequent confirmatory modeling</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Developed September, 2016, revised December, 2018 to produce code for lavaan and sem from the <code><a href="#topic+esem">esem</a></code> and <code><a href="#topic+esemDiagram">esemDiagram</a></code> functions. Suggestions or comments are most welcome. 
</p>
<p>This is clearly an exploratory approach and the degrees of freedom for the model are unclear.  
</p>
<p>In December 2023, I added the canCorDiagram function to help understand canonical correlations. 
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Tucker, Ledyard (1958) An inter-battery method of factor analysis, Psychometrika, 23, 111-136.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+principal">principal</a></code> for principal components analysis (PCA).  PCA will give very similar solutions to factor analysis when there are many variables.  The differences become more salient as the number variables decrease.  The PCA and FA models are actually very different and should not be confused.  One is a model of the observed variables, the other is a model of latent variables.  
</p>
<p><code><a href="#topic+lmCor">lmCor</a></code> which does a canonical correlation between the X and Y sets of variables.
</p>
<p><code><a href="#topic+irt.fa">irt.fa</a></code> for Item Response Theory analyses using factor analysis, using the two parameter IRT equivalent of loadings and difficulties.
</p>
<p><code><a href="#topic+VSS">VSS</a></code> will produce the Very Simple Structure (VSS) and MAP criteria for the number of factors, <code><a href="#topic+nfactors">nfactors</a></code> to compare many different factor criteria.
</p>
<p><code><a href="#topic+ICLUST">ICLUST</a></code> will do a hierarchical cluster analysis alternative to factor analysis or principal components analysis.
</p>
<p><code><a href="#topic+predict.psych">predict.psych</a></code> to find predicted scores based upon new data, <code><a href="#topic+fa.extension">fa.extension</a></code> to extend the factor solution to new variables, <code><a href="#topic+omega">omega</a></code> for hierarchical factor analysis with one general factor. 
<code><a href="#topic+fa.multi">fa.multi</a></code> for hierarchical factor analysis with an arbitrary number of higher order factors. 
</p>
<p><code><a href="#topic+faRegression">faRegression</a></code> to do multiple regression from factor analysis solutions.
</p>
<p><code><a href="#topic+fa.sort">fa.sort</a></code> will sort the factor loadings into echelon form. <code><a href="#topic+fa.organize">fa.organize</a></code> will reorganize the factor pattern matrix into any arbitrary order of factors and items.  
</p>
<p><code><a href="#topic+KMO">KMO</a></code> and <code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code> for various tests that some people like. 
</p>
<p><code><a href="#topic+factor2cluster">factor2cluster</a></code> will prepare unit weighted scoring keys of the factors that can be used with <code><a href="#topic+scoreItems">scoreItems</a></code>.
</p>
<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> will print the factor analysis loadings matrix along with the item &ldquo;content&quot; taken from a dictionary of items.  This is useful when examining the meaning of the factors.  
</p>
<p><code><a href="#topic+anova.psych">anova.psych</a></code> allows for testing the difference between two (presumably nested) factor models .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#make up a sem like problem using sim.structure
fx &lt;-matrix(c( .9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)  
fy &lt;- matrix(c(.6,.5,.4),ncol=1)
rownames(fx) &lt;- c("V","Q","A","nach","Anx")
rownames(fy)&lt;- c("gpa","Pre","MA")
Phi &lt;-matrix( c(1,0,.7,.0,1,.7,.7,.7,1),ncol=3)
gre.gpa &lt;- sim.structural(fx,Phi,fy)
print(gre.gpa)

#now esem it:
example &lt;- esem(gre.gpa$model,varsX=1:5,varsY=6:8,nfX=2,nfY=1,n.obs=1000,plot=FALSE)
example
esemDiagram(example,simple=FALSE)
#compare this to the canonical solution
mod &lt;- lmCor(y=6:8, x =1:5, data=gre.gpa$model, plot=FALSE)
cancorDiagram(mod)  #does not work because of imaginary roots
#compare two alternative solutions to the first 2 factors of the neo.
#solution 1 is the normal 2 factor solution.
#solution 2 is an esem with 1 factor for the first 6 variables, and 1 for the second 6.
f2 &lt;- fa(psychTools::neo[1:12,1:12],2)
es2 &lt;- esem(psychTools::neo,1:6,7:12,1,1)
summary(f2)
summary(es2)
fa.congruence(f2,es2)

interbattery(Thurstone.9,1:4,5:9,2,2)  #compare to the solution of Tucker.  We are not there yet.

</code></pre>

<hr>
<h2 id='fa'>Exploratory Factor analysis using MinRes (minimum residual) as well as EFA by Principal Axis, Weighted Least Squares or Maximum Likelihood </h2><span id='topic+fa'></span><span id='topic+fac'></span><span id='topic+fa.sapa'></span><span id='topic+fa.pooled'></span>

<h3>Description</h3>

<p>Among the many ways to do latent variable exploratory factor analysis (EFA), one of the better is to use Ordinary Least Squares (OLS) to find the minimum residual (minres) solution. This produces solutions very similar to maximum likelihood even for badly behaved matrices. A variation on minres is to do weighted least squares (WLS). Perhaps the most conventional technique is principal axes (PAF).  An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.   Yet another estimate procedure is maximum likelihood. For well behaved matrices, maximum likelihood factor analysis (either in the fa or in the factanal function) is probably preferred.  Bootstrapped confidence intervals of the loadings and interfactor correlations are found by fa  with n.iter &gt; 1. </p>


<h3>Usage</h3>

<pre><code class='language-R'>fa(r, nfactors=1, n.obs = NA, n.iter=1, rotate="oblimin", scores="regression", 
	residuals=FALSE, SMC=TRUE, covar=FALSE, missing=FALSE,	impute="none",
	min.err = 0.001,  max.iter = 50, symmetric=TRUE, warnings=TRUE, fm="minres",
 	alpha=.1, p=.05, oblique.scores=FALSE, np.obs=NULL, use="pairwise", cor="cor",
 	correct=.5, weight=NULL, n.rotations=1, hyper=.15, smooth=TRUE,...)

fac(r,nfactors=1,n.obs = NA, rotate="oblimin", scores="tenBerge", residuals=FALSE,
 	SMC=TRUE, covar=FALSE, missing=FALSE, impute="median", min.err = 0.001, 
	max.iter=50, symmetric=TRUE, warnings=TRUE, fm="minres", alpha=.1,
	oblique.scores=FALSE, np.obs=NULL, use="pairwise", cor="cor", correct=.5, 
	weight=NULL, n.rotations=1, hyper=.15, smooth=TRUE,...)

fa.pooled(datasets,nfactors=1,rotate="oblimin", scores="regression", 
	residuals=FALSE, SMC=TRUE, covar=FALSE, missing=FALSE,impute="median", min.err = 
	.001, max.iter=50,symmetric=TRUE, warnings=TRUE, fm="minres", alpha=.1, 
	p =.05, oblique.scores=FALSE,np.obs=NULL, use="pairwise", cor="cor",
	correct=.5,	weight=NULL,...) 

fa.sapa(r,nfactors=1, n.obs = NA,n.iter=1,rotate="oblimin",scores="regression", 
	residuals=FALSE, SMC=TRUE, covar=FALSE, missing=FALSE, impute="median", 
	min.err = .001, max.iter=50,symmetric=TRUE,warnings=TRUE, fm="minres", alpha=.1, 
	p =.05, oblique.scores=FALSE, np.obs=NULL, use="pairwise", cor="cor", correct=.5, 
	weight=NULL, frac=.1,...) 


</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa_+3A_r">r</code></td>
<td>
<p>A correlation or covariance matrix or a raw data matrix. If raw data, the correlation matrix will be found using pairwise deletion. If covariances are supplied, they will be converted to correlations unless the covar option is TRUE.</p>
</td></tr>
<tr><td><code id="fa_+3A_nfactors">nfactors</code></td>
<td>
<p> Number of factors to extract, default is 1 </p>
</td></tr>
<tr><td><code id="fa_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.</p>
</td></tr>
<tr><td><code id="fa_+3A_np.obs">np.obs</code></td>
<td>
<p>The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.</p>
</td></tr>
<tr><td><code id="fa_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;,  &quot;bentlerT&quot;, &quot;equamax&quot;, &quot;varimin&quot;, &quot;geominT&quot; and &quot;bifactor&quot; are orthogonal rotations.  &quot;Promax&quot;, &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, &quot;bentlerQ,  &quot;geominQ&quot; and &quot;biquartimin&quot; and &quot;cluster&quot; are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to &quot;promax&quot; which does the normalization before calling Promax in GPArotation.</p>
</td></tr>
<tr><td><code id="fa_+3A_n.rotations">n.rotations</code></td>
<td>
<p>Number of random starts for the rotations.</p>
</td></tr>
<tr><td><code id="fa_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of bootstrap interations to do in fa or fa.poly</p>
</td></tr>
<tr><td><code id="fa_+3A_residuals">residuals</code></td>
<td>
<p>Should the residual matrix be shown </p>
</td></tr>
<tr><td><code id="fa_+3A_scores">scores</code></td>
<td>
<p>the default=&quot;regression&quot; finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression (&quot;Thurstone&quot;), correlaton preserving (&quot;tenBerge&quot;) as well as &quot;Anderson&quot; and &quot;Bartlett&quot; using the appropriate algorithms ( <code><a href="#topic+factor.scores">factor.scores</a></code>). Although scores=&quot;tenBerge&quot; is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  </p>
</td></tr>
<tr><td><code id="fa_+3A_smc">SMC</code></td>
<td>
<p>Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. </p>
</td></tr>
<tr><td><code id="fa_+3A_covar">covar</code></td>
<td>
<p>if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix</p>
</td></tr>
<tr><td><code id="fa_+3A_missing">missing</code></td>
<td>
<p>if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean. Specifying impute=&quot;none&quot; will not impute data points and thus will have some missing data in the factor scores.</p>
</td></tr>
<tr><td><code id="fa_+3A_impute">impute</code></td>
<td>
<p>&quot;median&quot; or &quot;mean&quot; values are used to replace missing values</p>
</td></tr>
<tr><td><code id="fa_+3A_min.err">min.err</code></td>
<td>
<p>Iterate until the change in communalities is less than min.err</p>
</td></tr>
<tr><td><code id="fa_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for convergence </p>
</td></tr>
<tr><td><code id="fa_+3A_symmetric">symmetric</code></td>
<td>
<p>symmetric=TRUE forces symmetry by just looking at the lower off diagonal values</p>
</td></tr>
<tr><td><code id="fa_+3A_hyper">hyper</code></td>
<td>
<p>Count number of (absolute) loadings less than hyper. </p>
</td></tr>
<tr><td><code id="fa_+3A_warnings">warnings</code></td>
<td>
<p>warnings=TRUE =&gt; warn if number of factors is too many </p>
</td></tr>
<tr><td><code id="fa_+3A_fm">fm</code></td>
<td>
<p>Factoring method  fm=&quot;minres&quot; will do a minimum residual as will fm=&quot;uls&quot;.  Both of these use a first derivative.  fm=&quot;ols&quot; differs very slightly from &quot;minres&quot; in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm=&quot;wls&quot; will do a weighted least squares (WLS) solution, fm=&quot;gls&quot; does a generalized weighted least squares (GLS), fm=&quot;pa&quot; will do the principal factor solution, fm=&quot;ml&quot; will do a maximum likelihood factor analysis. fm=&quot;minchi&quot; will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm =&quot;minrank&quot; will do a minimum rank factor analysis. &quot;old.min&quot; will do minimal residual the way it was done prior to April, 2017 (see discussion below).  fm=&quot;alpha&quot; will do alpha factor analysis as described in Kaiser and Coffey (1965)</p>
</td></tr>
<tr><td><code id="fa_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence intervals for RMSEA</p>
</td></tr>
<tr><td><code id="fa_+3A_p">p</code></td>
<td>
<p>if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals</p>
</td></tr>
<tr><td><code id="fa_+3A_oblique.scores">oblique.scores</code></td>
<td>
<p>When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  Now it is always false.  If you want oblique factor scores, use tenBerge.  </p>
</td></tr> 
<tr><td><code id="fa_+3A_weight">weight</code></td>
<td>
<p>If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.</p>
</td></tr>
<tr><td><code id="fa_+3A_use">use</code></td>
<td>
<p>How to treat missing data, use=&quot;pairwise&quot; is the default&quot;.  See cor for other options.</p>
</td></tr>
<tr><td><code id="fa_+3A_cor">cor</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, 
&quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric, &quot;mixed&quot; uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate</p>
</td></tr>
<tr><td><code id="fa_+3A_correct">correct</code></td>
<td>
<p>When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)</p>
</td></tr> 
<tr><td><code id="fa_+3A_frac">frac</code></td>
<td>
<p>The fraction of data to sample n.iter times if showing stability across sample sizes</p>
</td></tr>
<tr><td><code id="fa_+3A_datasets">datasets</code></td>
<td>
<p>A list of replication data sets to analyse in fa.pooled. All need to have the same number of variables.</p>
</td></tr>
<tr><td><code id="fa_+3A_smooth">smooth</code></td>
<td>
<p>By default, improper correlations matrices are smoothed.  This is not necessary for factor extraction using fm=&quot;pa&quot; or fm=&quot;minres&quot;, but is needed to give some of the goodness of fit tests. (see notes)</p>
</td></tr>
<tr><td><code id="fa_+3A_...">...</code></td>
<td>
<p>additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Factor analysis is an attempt to approximate a correlation or covariance matrix with one of lesser rank.  The basic model is that <code class="reqn">_nR_n \approx _{n}F_{kk}F_n'+ U^2</code> where k is much less than n. There are many ways to do factor analysis, and maximum likelihood procedures are probably the most commonly preferred (see <code><a href="stats.html#topic+factanal">factanal</a></code> ).  The existence of uniquenesses is what distinguishes factor analysis from principal components analysis (e.g., <code><a href="#topic+principal">principal</a></code>). If variables are thought to represent a &ldquo;true&quot; or latent part then factor analysis provides an estimate of the correlations with the latent factor(s) representing the data.  If variables are thought to be measured without error, then principal components provides the most parsimonious description of the data.  
</p>
<p>Factor loadings will be smaller than component loadings for the later reflect unique error in each variable. The off diagonal residuals for a factor solution will be superior (smaller) than that of a component model.  Factor loadings can be thought of as the asymptotic component loadings as the number of variables loading on each factor increases.  
</p>
<p>The fa function will do factor analyses using one of six different algorithms: minimum residual (minres, aka ols, uls), principal axes,  alpha factoring, weighted least squares, minimum rank, or maximum likelihood.
</p>
<p>Principal axes factor analysis has a long history in exploratory analysis and is a straightforward procedure.  Successive eigen value decompositions are done on a correlation matrix with the diagonal replaced with  diag (FF') until <code class="reqn">\sum(diag(FF'))</code> does not change (very much).  The current limit of max.iter =50 seems to work for most problems, but the Holzinger-Harmon 24 variable problem needs about 203 iterations to converge for a 5 factor solution.  
</p>
<p>Not all factor programs that do principal axes do iterative solutions.  The example from the SAS manual (Chapter 33) is such a case. To achieve that solution, it is necessary to specify that the max.iter = 1.  Comparing that solution to an iterated one (the default) shows that iterations improve the solution. 
</p>
<p>In addition, fm=&quot;mle&quot; produces even better solutions for this example.  Both the RMSEA and the root mean square of the residuals are smaller than the fm=&quot;pa&quot; solution.
</p>
<p>However, simulations of multiple problem sets suggest that fm=&quot;pa&quot; tends to produce slightly smaller residuals while having slightly larger RMSEAs than does fm=&quot;minres&quot; or fm=&quot;mle&quot;.    That is, the sum of squared residuals for fm=&quot;pa&quot; seem to be slightly smaller than those found using fm=&quot;minres&quot; but the RMSEAs are slightly worse when using fm=&quot;pa&quot;.  That is to say, the &quot;true&quot; minimal residual is probably found by fm=&quot;pa&quot;. 
</p>
<p>Following extensive correspondence with Hao Wu and Mikko Ronkko, in April, 2017 the derivative of the minres  and uls) fitting was modified.  This leads to slightly smaller residuals (appropriately enough for a method claiming to minimize them) than the prior procedure.  For  consistency with prior analyses, &quot;old.min&quot; was added to give these slightly larger residuals.  The differences between old.min and the newer &quot;minres&quot; and &quot;ols&quot; solutions are at the third to fourth decimal, but none the less, are worth noting. For comparison purposes, the fm=&quot;ols&quot; uses empirical first derivatives, while uls and minres use equation based first derivatives.  The results seem to be identical, but the minres and uls solutions require fewer iterations for larger problems and are faster. Thanks to Hao Wu for some very thoughtful help.  
</p>
<p>Although usually these various algorithms produce equivalent results, there are several data sets included that show large differences between the methods. <code><a href="psychTools.html#topic+Schutz">Schutz</a></code> produces Heywood and super Heywood cases,   <code><a href="psychTools.html#topic+blant">blant</a></code> leads to very different solutions.  In particular, the minres solution produces smaller residuals than does the mle solution, and the factor.congruence coefficients show very different solutions.
</p>
<p>A very strong argument against using MLE is found in the chapter by MacCallum, Brown and Cai (2007) who show that OLS approaches produce equivalent solutions most of the time, and better solutions some of the time.  This particularly in the case of models with some unmodeled small factors.  (See <code><a href="#topic+sim.minor">sim.minor</a></code> to generate such data.)
</p>
<p>Principal axes may be used in cases when maximum likelihood solutions fail to converge, although fm=&quot;minres&quot; will also do that and tends to produce better (smaller RMSEA) solutions.
</p>
<p>The fm=&quot;minchi&quot; option is a variation on the &quot;minres&quot; (ols) solution and minimizes the sample size weighted residuals rather than just the residuals. This was developed to handle the problem of data that Massively Missing Completely at Random (MMCAR) which a condition that happens in the SAPA project.
</p>
<p>A traditional problem in factor analysis was to find the best estimate of the original communalities in order to speed up convergence.  Using the Squared Multiple Correlation (SMC) for each variable will underestimate the original communalities, using 1s will over estimate.  By default, the SMC estimate is used.  Note that in the case of non-invertible matrices, the pseudo-inverse is found so smcs are still estimated. In either case, iterative techniques will tend to converge on a stable solution. If, however, a solution fails to be achieved, it is useful to try again using ones (SMC =FALSE).  Alternatively, a vector of starting values for the communalities may be specified by the SMC option.
</p>
<p>The iterated principal axes algorithm does not attempt to find the best (as defined by a maximum likelihood criterion) solution, but rather one that converges rapidly using successive eigen value decompositions.  The maximum likelihood criterion of fit and the associated chi square value are reported, and will be (slightly) worse than that found using maximum likelihood procedures.
</p>
<p>The minimum residual (minres) solution is an unweighted least squares solution that takes a slightly different approach.  It uses the <code><a href="stats.html#topic+optim">optim</a></code> function and adjusts the diagonal elements of the correlation matrix to mimimize the squared residual when the factor model is the eigen value decomposition of the reduced matrix.  MINRES and PA will both work when ML will not, for they can be used when the matrix is singular. Although before the change in the derivative,  the MINRES solution was slightly more similar to the ML solution than is the PA solution. With the change in the derivative of the minres fit, the minres, pa and uls solutions are practically identical. To a great extent, the minres and wls solutions follow ideas in the <code><a href="stats.html#topic+factanal">factanal</a></code> function with the change in the derivative. 
</p>
<p>The weighted least squares (wls) solution weights the residual matrix by 1/ diagonal of the inverse of the correlation matrix.  This has the effect of weighting items with low communalities more than those with high communalities. 
</p>
<p>The generalized least squares (gls) solution weights the residual matrix by the inverse of the correlation matrix.  This has the effect of weighting those variables with low communalities even more than those with high communalities.
</p>
<p>The maximum likelihood solution takes yet another approach and finds those communality values that minimize the chi square goodness of fit test.  The fm=&quot;ml&quot; option provides a maximum likelihood solution following the procedures used in <code><a href="stats.html#topic+factanal">factanal</a></code> but does not provide all the extra features of that function.  It does, however, produce more expansive output.
</p>
<p>The minimum rank factor model (MRFA) roughly follows ideas by Shapiro and Ten Berge (2002) and Ten Berge and Kiers (1991).  It makes use of the glb.algebraic procedure contributed by Andreas Moltner.  MRFA attempts to extract factors such that the residual matrix is still positive semi-definite.  This version is still being tested and feedback is most welcome.
</p>
<p>Alpha factor analysis finds solutions based upon a correlation matrix corrected for communalities and then rescales these to the original correlation matrix.  This procedure is described by Kaiser and Coffey, 1965.  
</p>
<p>Test cases comparing the output to SPSS suggest that the PA algorithm matches what SPSS calls uls, and that the wls solutions are equivalent in their fits. The wls and gls solutions have slightly larger eigen values, but slightly worse fits of the off diagonal residuals than do the minres or maximum likelihood solutions.  Comparing the results to the examples in Harman (1976), the PA solution with no iterations matches what Harman calls Principal Axes (as does SAS), while the iterated PA solution matches his minres solution.  The minres solution found in psych tends to have slightly smaller off diagonal residuals (as it should) than does the iterated PA solution.   
</p>
<p>Although for items, it is typical to find factor scores by scoring the salient items (using, e.g., <code><a href="#topic+scoreItems">scoreItems</a></code>) factor scores can be estimated by regression as well as several other means. There are multiple approaches that are possible (see Grice, 2001) and  one taken here was developed by tenBerge et al.
(see <code><a href="#topic+factor.scores">factor.scores</a></code>). The alternative, which will match factanal is to find the scores using regression &ndash;  Thurstone's least squares regression where the weights are found by
<code class="reqn">W = R^{-1}S</code> where R is the correlation matrix of the variables ans S is the structure matrix.  Then, factor scores are just <code class="reqn">Fs = X W</code>.
</p>
<p>In the oblique case, the factor loadings are referred to as Pattern coefficients and are related to the Structure coefficients by <code class="reqn">S = P \Phi</code> and thus <code class="reqn">P = S \Phi^{-1}</code>.  When estimating factor scores, <code><a href="#topic+fa">fa</a></code> and  <code><a href="stats.html#topic+factanal">factanal</a></code> differ in that <code><a href="#topic+fa">fa</a></code> finds the factors from the Structure matrix while <code><a href="stats.html#topic+factanal">factanal</a></code> seems to do it from the Pattern matrix.  Thus, although in the orthogonal case, fa and factanal agree perfectly in their factor score estimates, they do not agree in the case of oblique factors.  Setting oblique.scores = TRUE  will produce factor score estimate that match those of <code><a href="stats.html#topic+factanal">factanal</a></code>.
</p>
<p>It is sometimes useful to extend the factor solution to variables that were not factored.  This may be done using <code><a href="#topic+fa.extension">fa.extension</a></code>. Factor extension is typically done in the case where some variables were not appropriate to factor, but factor loadings on the original factors are still desired.  Factor extension is a very powerful procedure in that it allows one to find the factor-criterion correlations without using factor scores. 
</p>
<p>For dichotomous items or polytomous items, it is recommended to analyze the <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> correlations rather than the Pearson correlations. This may be done by specifying cor=&quot;poly&quot; or cor=&quot;tet&quot; or cor=&quot;mixed&quot; if the data have a mixture of dichotomous, polytomous, and continous variables.  
</p>
<p>Analysis of dichotomous or polytomous data may also be done by using  <code><a href="#topic+irt.fa">irt.fa</a></code> or simply setting the cor=&quot;poly&quot; option.  In the first case,  the factor analysis results are reported in Item Response Theory (IRT) terms, although the original factor solution is returned in the results. In the later case, a typical factor loadings matrix is returned, but the tetrachoric/polychoric correlation matrix and item statistics are saved for reanalysis by <code><a href="#topic+irt.fa">irt.fa</a></code>. (See also the <code><a href="#topic+mixed.cor">mixed.cor</a></code> function to find correlations from a mixture of continuous, dichotomous, and polytomous items.)
</p>
<p>Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The &ldquo;cluster&rdquo; option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional &quot;keys&quot; parameter, the &quot;target&quot; option will rotate to a target supplied as a keys matrix. (See <code><a href="#topic+target.rot">target.rot</a></code>.)
</p>
<p>oblimin is implemented in GPArotation by a call to quartimin with delta=0.  This leads to confusion when people refer to quartimin solutions. 
</p>
<p>It is important to note a dirty little secret about factor rotation.  That is the problem of local minima.  Multiple restarts of the rotation algorithms are strongly encouraged (see Nguyen and Waller, N. G. 2021). 
</p>
<p>Two additional target rotation options are available through calls to GPArotation.  These are the targetQ (oblique) and targetT (orthogonal) target rotations of Michael Browne.  See <code><a href="#topic+target.rot">target.rot</a></code> for more documentation. 
</p>
<p>The &quot;bifactor&quot; rotation implements the Jennrich and Bentler (2011) bifactor rotation by calling the GPForth function in the GPArotation package and using two functions adapted from  the MatLab code of Jennrich and Bentler.  This seems to have a problem with local minima and multiple starting values should be used.
</p>
<p>There are two varimax rotation functions.  One, Varimax, in the GPArotation package does not by default apply Kaiser normalization.  The other, varimax, in the stats package, does.  It appears that the two rotation functions produce slightly different results even when normalization is set. For consistency with the other rotation functions, Varimax is probably preferred.
</p>
<p>The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by <code class="reqn">\Phi = \theta' \theta</code>
</p>
<p>There are two ways to handle dichotomous or polytomous responses: <code><a href="#topic+fa">fa</a></code> with the cor=&quot;poly&quot; option which will return the tetrachoric or polychoric correlation matrix, as well as the normal factor analysis output, and <code><a href="#topic+irt.fa">irt.fa</a></code> which returns a two parameter irt analysis as well as the normal fa output. 
</p>
<p>When factor analyzing items with dichotomous or polytomous responses, the <code><a href="#topic+irt.fa">irt.fa</a></code> function provides an Item Response Theory representation of the factor output. The factor analysis results are available, however, as an object in the irt.fa output. 
</p>
<p><code><a href="#topic+fa.poly">fa.poly</a></code> is deprecated, for its functioning is matched by setting cor=&quot;poly&quot;.  It will produce normal factor analysis output but also will save the polychoric matrix  (rho) and items difficulties (tau) for subsequent irt analyses.  <code><a href="#topic+fa.poly">fa.poly</a></code> will,  by default, find factor scores if the data are available.  The correlations are found using either <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> and then this matrix is factored.  Weights from the factors are then applied to the original data to estimate factor scores.
</p>
<p>The function <code><a href="#topic+fa">fa</a></code> will repeat the analysis n.iter times on a bootstrapped sample of the data (if they exist) or of a simulated data set based upon the observed correlation matrix.  The mean estimate and standard deviation of the estimate are returned and will print the original factor analysis as well as the alpha level confidence intervals for the estimated coefficients.  The bootstrapped solutions are rotated towards the original solution using target.rot. The factor loadings are z-transformed, averaged and then back transformed. This leads to an error in the case of Heywood cases.  The probably better alternative is to just find the mean bootstrapped value and find the confidence intervals based upon the observed range of values. The default is to have n.iter =1 and thus not do bootstrapping.
</p>
<p>If using polytomous or dichotomous items, it is perhaps more useful  to find the Item Response Theory parameters equivalent to the factor loadings reported in fa.poly by using the <code><a href="#topic+irt.fa">irt.fa</a></code> function.  
</p>
<p>For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin <code><a href="#topic+KMO">KMO</a></code> test may be found from the correlation matrix or data matrix using the <code><a href="#topic+KMO">KMO</a></code> function.  Similarly, the Bartlett's test of Sphericity may be found using the  <code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code> function.
</p>
<p>For those who want to have an object of the variances accounted for, this is returned invisibly by the print function.  (e.g., p &lt;- print(fa(ability))$Vaccounted ).  This is now returned by the fa function as well (e.g. p &lt;- fa(ability)$Vaccounted ).  Just as communalities may be found by the diagonal of Pattern %*% t(Structure) so can the variance accounted for be found by diagonal ( t(Structure) %*% Pattern.  Note that referred to as SS loadings. 
</p>
<p>The output from the print.psych.fa function displays the factor loadings (from the pattern matrix, the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the factor loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared factor loadings. But for an oblique solution, it is the row sum of the orthogonal factor loadings (remember, that rotations or transformations do not change the communality).  
</p>
<p>In response to a request from Asghar Minaei who wanted to combine several imputation data sets from mice, fa.pooled was added.  The separate data sets are combined into a list (datasets) which are then factored separately. Each solution is rotated towards the first set in the list.  The results are then reported in terms of pooled loadings and confidence intervals based upon the replications.  
</p>
<p><code><a href="#topic+fa.sapa">fa.sapa</a></code> simulates the process of doing SAPA (Synthetic Aperture Personality Assessment).  It will do iterative solutions for successive random samples of  fractions (frac) of the data set. This allows us to find the stability of solutions for various sample sizes and various sample rates. Need to specify the number of iterations (n.iter) as well as the percent of data sampled (frac).  
</p>


<h3>Value</h3>

<table>
<tr><td><code>values</code></td>
<td>
<p>Eigen values of the common factor solution</p>
</td></tr>
<tr><td><code>e.values</code></td>
<td>
<p>Eigen values of the original matrix</p>
</td></tr>
<tr><td><code>communality</code></td>
<td>
<p>Communality estimates for each item.  These are merely the sum of squared factor loadings for that item.</p>
</td></tr>
<tr><td><code>communalities</code></td>
<td>
<p>If using minrank factor analysis, these are the communalities reflecting the total amount of common variance.  They will exceed the communality (above) which is the model estimated common variance. </p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>which rotation was requested?</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>number of observations specified or found</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>An item by factor (pattern) loading matrix of class &ldquo;loadings&quot;  Suitable for use in other programs (e.g., GPA rotation or factor2cluster. To show these by sorted order, use <code><a href="#topic+print.psych">print.psych</a></code> with sort=TRUE</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>Hoffman's index of complexity for each item.  This is just <code class="reqn">\frac{(\Sigma a_i^2)^2}{\Sigma a_i^4}</code> where a_i is the factor loading on the ith factor. From Hofmann (1978), MBR. See also  Pettersson and Turkheimer (2010).</p>
</td></tr>
<tr><td><code>Structure</code></td>
<td>
<p>An item by factor structure matrix of class &ldquo;loadings&quot;. This is just the loadings (pattern) matrix times the factor intercorrelation matrix.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>How well does the factor model reproduce the correlation matrix. This is just <code class="reqn">\frac{\Sigma r_{ij}^2 - \Sigma r^{*2}_{ij} }{\Sigma r_{ij}^2}
</code>  (See <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, and <code><a href="#topic+principal">principal</a></code> for this fit statistic.</p>
</td></tr>
<tr><td><code>fit.off</code></td>
<td>
<p>how well are the off diagonal elements reproduced?</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>Degrees of Freedom for this model. This is the number of observed correlations minus the number of independent parameters.  Let n=Number of items, nf = number of factors then
<br />
<code class="reqn">dof = n * (n-1)/2 - n * nf + nf*(nf-1)/2</code></p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>Value of the function that is minimized by a maximum likelihood procedures.  This is reported for comparison purposes and as a way to estimate chi square goodness of fit.  The objective function is 
<br />
<code class="reqn">f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^{-1} R|) - n.items</code>. When using MLE, this function is minimized.  When using OLS (minres), although we are not minimizing this function directly, we can still calculate it in order to compare the solution to a MLE fit. </p>
</td></tr>
<tr><td><code>STATISTIC</code></td>
<td>
<p>If the number of observations is specified or found, this is a chi square based upon the objective function, f (see above). Using the formula from <code><a href="stats.html#topic+factanal">factanal</a></code>(which seems to be Bartlett's test) :
<br />
<code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> </p>
</td></tr>
<tr><td><code>PVAL</code></td>
<td>
<p>If n.obs &gt; 0, then what is the probability of observing a chisquare this large or larger?</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>If oblique rotations (e.g,m using oblimin from the GPArotation package or promax) are requested, what is the interfactor correlation?</p>
</td></tr>
<tr><td><code>communality.iterations</code></td>
<td>
<p>The history of the communality estimates (For principal axis only.) Probably only useful for teaching what happens in the process of iterative fitting.</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>The matrix of residual correlations after the factor model is applied. To display it conveniently, use the <code><a href="stats.html#topic+residuals">residuals</a></code> command. </p>
</td></tr>
<tr><td><code>chi</code></td>
<td>
<p>When normal theory fails (e.g., in the case of non-positive definite matrices), it useful to examine the empirically derived <code class="reqn">\chi^2</code> based upon the sum of the squared residuals * N.  This will differ slightly from the MLE estimate which is based upon the fitting function rather than the actual residuals.</p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>This is the sum of the squared (off diagonal residuals) divided by the degrees of freedom.  Comparable to an RMSEA which, because it is based upon  <code class="reqn">\chi^2</code>, requires the number of observations to be specified.  The rms is an empirical value while the RMSEA is based upon normal theory and the non-central <code class="reqn">\chi^2</code> distribution. That is to say, if the residuals are particularly non-normal, the rms value and the associated  <code class="reqn">\chi^2</code> and RMSEA can differ substantially. </p>
</td></tr> 
<tr><td><code>crms</code></td>
<td>
<p>rms adjusted for degrees of freedom</p>
</td></tr>
<tr><td><code>RMSEA</code></td>
<td>
<p>The Root Mean Square Error of Approximation is based upon the non-central 
<code class="reqn">\chi^2</code> distribution and the <code class="reqn">\chi^2</code> estimate found from the MLE fitting function.  With normal theory data, this is fine.  But when the residuals are not distributed according to a noncentral <code class="reqn">\chi^2</code>, this can give very strange values.  (And thus the confidence intervals can not be calculated.) The RMSEA is a conventional index of goodness (badness) of fit but it is also useful to examine the actual rms values.  </p>
</td></tr>   
<tr><td><code>TLI</code></td>
<td>
<p>The Tucker Lewis Index of factoring reliability which is also known as the non-normed fit index.  </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Based upon <code class="reqn">\chi^2</code> with the assumption of normal theory and using the <code class="reqn">\chi^2</code> found using the objective function defined above. This is just <code class="reqn">\chi^2 - 2 df</code></p>
</td></tr>
<tr><td><code>eBIC</code></td>
<td>
<p>When normal theory fails (e.g., in the case of non-positive definite matrices), it useful to examine the empirically derived eBIC based upon the empirical <code class="reqn">\chi^2</code> - 2 df. </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001).  Derived from R2 is is the minimum correlation between any two factor estimates = 2R2-1. </p>
</td></tr>
<tr><td><code>r.scores</code></td>
<td>
<p>The correlations of the factor score estimates using the specified model, if they were to be found.  Comparing these correlations with that of the scores themselves will show, if an alternative estimate of factor scores is used (e.g., the tenBerge method), the problem of factor indeterminacy.  For these correlations will not necessarily be the same.  </p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The beta weights to find the factor score estimates.  These are also used by the <code><a href="#topic+predict.psych">predict.psych</a></code> function to find predicted factor scores for new cases.  These weights will depend upon the scoring method requested.  </p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>The factor scores as requested.  Note that these scores reflect the choice of the way scores should be estimated (see scores in the input).  That is, simple regression (&quot;Thurstone&quot;), correlaton preserving (&quot;tenBerge&quot;) as well as &quot;Anderson&quot; and &quot;Bartlett&quot; using the appropriate algorithms (see <code><a href="#topic+factor.scores">factor.scores</a></code>).  The correlation between factor score estimates (r.scores) is based upon using the regression/Thurstone approach.  The actual correlation between scores will reflect the rotation algorithm chosen and may be found by correlating those scores. Although the scores are found by multiplying the standarized data by the weights matrix, this will not result in standard scores if using regression. </p>
</td></tr>
<tr><td><code>valid</code></td>
<td>
<p>The validity coffiecient of course coded (unit weighted) factor score estimates (From Grice, 2001)</p>
</td></tr>
<tr><td><code>score.cor</code></td>
<td>
<p>The correlation matrix of coarse coded (unit weighted) factor score estimates, if they were to be found, based upon the structure matrix rather than the weights or loadings matrix.  </p>
</td></tr>
<tr><td><code>rot.mat</code></td>
<td>
<p>The rotation matrix as returned from GPArotation.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Thanks to Erich Studerus for some very helpful suggestions about various rotation and factor scoring algorithms, and to  Gumundur Arnkelsson  for suggestions about factor scores for singular matrices.
</p>
<p>The fac function is the original fa function which is now called by fa repeatedly to get confidence intervals. 
</p>
<p>SPSS will sometimes use a Kaiser normalization before rotating.  This will lead to different solutions than reported here.  To get the Kaiser normalized loadings, use <code><a href="#topic+kaiser">kaiser</a></code>.
</p>
<p>The communality for a variable is the amount of variance accounted for by all of the factors.  That is to say, for orthogonal factors, it is the sum of the squared factor loadings (rowwise).  The communality is insensitive to rotation.  However, if an oblique solution is found, then the communality is not the sum of squared pattern coefficients. In both cases (oblique or orthogonal) the communality is the diagonal of the reproduced correlation matrix where <code class="reqn">_nR_n =  _{n}P_{k  k}\Phi_{k k}P_n'</code>  where P is the pattern matrix and <code class="reqn">\Phi</code> is the factor intercorrelation matrix.     This is the same, of course to multiplying the pattern by the structure: <code class="reqn">R = P S'</code> where the Structure matrix is <code class="reqn">S = \Phi P</code>. Similarly, the eigen values are the diagonal of the product <code class="reqn"> _k\Phi_{kk}P'_{nn}P_{k}
</code>. 
</p>
<p>A frequently asked question is why are the factor names of the rotated solution not in ascending order?  That is, for example, if factoring the 25 items of the bfi, the factor names are MR2   MR3   MR5   MR1   MR4, rather than the seemingly more logical  &quot;MR1&quot; &quot;MR2&quot; &quot;MR3&quot; &quot;MR4&quot; &quot;MR5&quot;.  This is for pedagogical reasons, in that factors as extracted are orthogonal and are in order of amount of variance accounted for.  But when rotated (orthogonally) or transformed (obliquely) the simple structure solution does not preserve that order. The factors are still ordered according to variance accounted for, but because rotation changes how much variance each factor explains, the order may not the be the same as the original order.  The factor names are, of course, arbitrary, and are kept with the original names to show the effect of rotation/transformation.  To give them names associated with their ordinal position, simply paste(&quot;F&quot;, 1:nf, sep=&quot;&quot;) where nf is the number of factors. See the last example.
</p>
<p>The print function for the fa output will return (invisibly) an object (Vaccounted) that matches the printed output for the variance accounted for by each factor, as well as the cumulative variance, and the percentage of variance accounted for by each factor. 
</p>
<p>Correction to documentation: as of September, 2014, the oblique.scores option is correctly explained. (It had been backwards.)  The default (oblique.scores=FALSE) finds scores based upon the Structure matrix, while oblique.scores=TRUE finds them based upon the pattern matrix.  The latter case matches factanal.  This error was detected by Mark Seeto. 
</p>
<p>If the raw data are factored, factors scores are estimated.  By default this will be done using 'regression' but alternatives are available. Although the scores are found by multiplying the standardized data by the weights, if using regression, the resulting factor scores will not necessarily have unit variance. 
</p>
<p>In the case of missing data for some items, the factor scores will return NA for any case where any item is missing.  Specifying missing=TRUE will return factor scores for all cases using the imputation option chosen.  
</p>
<p>The minimum residual solution is done by finding those communalities that will minimize the off diagonal residual. The uls solution finds those communalities that minimize the total residuals.   
</p>
<p>The minres solution has been modified (April, 2107) following suggestions by Hao Wu. Although the fitting function was the minimal residual, the first derivative of the fitting function was incorrect. This has now been modified so that the results match those of SPSS and CEFA.  The prior solutions are still available using fm=&quot;old.min&quot;. 
</p>
<p>Alpha factoring was added in August, 2017 to add to the numerous alternative models of factoring.    
</p>
<p>A few more lines of output were added in August 2017 to show the measures of factor adequacy for different rotations.  This had been available in the results from <code><a href="#topic+factor.scores">factor.scores</a></code> but now is added to the fa output. 
</p>
<p>For a comparison of the <code><a href="#topic+fa">fa</a></code> to factor analysis using SPSS, see Grieder and Steiner (2021). 
</p>
<p>Some correlation matrices that arise from using pairwise deletion or from tetrachoric or polychoric matrices will not be proper.  That is, they will not be positive semi-definite (all eigen values &gt;= 0).  The <code><a href="#topic+cor.smooth">cor.smooth</a></code> function will adjust correlation matrices (smooth them) by making all negative eigen values slightly greater than 0, rescaling the other eigen values to sum to the number of variables, and then recreating the correlation matrix.  See <code><a href="#topic+cor.smooth">cor.smooth</a></code> for an example of this problem using the <code><a href="psychTools.html#topic+burt">burt</a></code> data set.
</p>
<p>One reason for this problem when using tetrachorics or polychorics seems to be the adjustment for continuity.  Setting correct=0 turns this off and seems to produce more proper matrices.
</p>
<p>Although both  fm =&quot;pa&quot; or fm=&quot;minres&quot; will work with these &ldquo;improper&quot; matrices, the goodness of fit tests do not.  Thus, by default, calls to fa.stats will pass the smooth parameter as TRUE.  This may be prevented by forcing smooth=FALSE.  Npt smoothing does not affect the pa or minres solution, but will affect the goodness of fit statistics slightly.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Gorsuch, Richard, (1983) Factor Analysis. Lawrence Erlebaum Associates.
</p>
<p>Grice, James W.  (2001), Computing and evaluating factor scores. Psychological Methods, 6, 430-450
</p>
<p>Harman, Harry and Jones, Wayne (1966) Factor analysis by minimizing residuals (minres), Psychometrika, 31, 3, 351-368.
</p>
<p>Hofmann, R. J. ( 1978 ) . Complexity and simplicity as objective indices descriptive of factor solutions. Multivariate Behavioral Research, 13, 247-250.
</p>
<p>Grieder, Silvia and Steiner, Markus. D.  (2021) Algorithmic jingle jungle: A comparison of implementations of principal axis factoring and promax rotation in R and SPSS. Behavior Research Methods. Doi: 10.3758/s13428-021-01581
</p>
<p>Kaiser, Henry F. and Caffrey, John.  Alpha factor analysis, Psychometrika, (30) 1-14.
</p>
<p>Nguyen, H. V. &amp; Waller, N. G. (in press). Local minima and factor rotations in exploratory factor analysis. Psychological Methods. 
</p>
<p>Pettersson E, Turkheimer E. (2010) Item selection, evaluation, and simple structure in personality data. Journal of research in personality, 44(4), 407-420.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Shapiro, A. and ten Berge, Jos M. F, (2002) Statistical inference of minimum rank factor analysis.  Psychometika, (67) 79-84.
</p>
<p>ten Berge, Jos M. F. and Kiers, Henk A. L. (1991). A numerical approach to the approximate and the exact minimum rank of a covariance matrix. Psychometrika, (56) 309-315. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+principal">principal</a></code> for principal components analysis (PCA).  PCA will give very similar solutions to factor analysis when there are many variables.  The differences become more salient as the number variables decrease.  The PCA and FA models are actually very different and should not be confused.  One is a model of the observed variables, the other is a model of latent variables.    Although some commercial packages (e.g., SPSS and SAS) refer to both as factor models, they are not.  It is incorrect to report doing a factor analysis using principal components.  
</p>
<p><code><a href="#topic+irt.fa">irt.fa</a></code> for Item Response Theory analyses using factor analysis, using the two parameter IRT equivalent of loadings and difficulties.
</p>
<p><code><a href="#topic+fa.random">fa.random</a></code> applies a random intercepts model by removing the mean score for each subject prior to factoring.  
</p>
<p><code><a href="#topic+VSS">VSS</a></code> will produce the Very Simple Structure (VSS) and MAP criteria for the number of factors, <code><a href="#topic+nfactors">nfactors</a></code> to compare many different factor criteria.
</p>
<p><code><a href="#topic+ICLUST">ICLUST</a></code> will do a hierarchical cluster analysis alternative to factor analysis or principal components analysis.
</p>
<p><code><a href="#topic+factor.scores">factor.scores</a></code> to find factor scores with alternative options.
<code><a href="#topic+predict.psych">predict.psych</a></code> to find predicted scores based upon new data, <code><a href="#topic+fa.extension">fa.extension</a></code> to extend the factor solution to new variables, <code><a href="#topic+omega">omega</a></code> for hierarchical factor analysis with one general factor. 
<code><a href="#topic+fa.multi">fa.multi</a></code> for hierarchical factor analysis with an arbitrary number of 2nd  order factors. 
</p>
<p><code><a href="#topic+fa.sort">fa.sort</a></code> will sort the factor loadings into echelon form. <code><a href="#topic+fa.organize">fa.organize</a></code> will reorganize the factor pattern matrix into any arbitrary order of factors and items.  
</p>
<p><code><a href="#topic+KMO">KMO</a></code> and <code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code> for various tests that some people like. 
</p>
<p><code><a href="#topic+factor2cluster">factor2cluster</a></code> will prepare unit weighted scoring keys of the factors that can be used with <code><a href="#topic+scoreItems">scoreItems</a></code>.
</p>
<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> will print the factor analysis loadings matrix along with the item &ldquo;content&quot; taken from a dictionary of items.  This is useful when examining the meaning of the factors.  
</p>
<p><code><a href="#topic+anova.psych">anova.psych</a></code> allows for testing the difference between two (presumably nested) factor models .
</p>
<p><code><a href="#topic+bassAckward">bassAckward</a></code> will perform repeated factorings and organize them in a top-down structure suggested by Goldberg (2006) and Waller (2007).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using the Harman 24 mental tests, compare a principal factor with a principal components solution
pc &lt;- principal(Harman74.cor$cov,4,rotate="varimax")   #principal components
pa &lt;- fa(Harman74.cor$cov,4,fm="pa" ,rotate="varimax")  #principal axis 
uls &lt;- fa(Harman74.cor$cov,4,rotate="varimax")          #unweighted least squares is minres
wls &lt;- fa(Harman74.cor$cov,4,fm="wls")       #weighted least squares

#to show the loadings sorted by absolute value
print(uls,sort=TRUE)

#then compare with a maximum likelihood solution using factanal
mle &lt;- factanal(covmat=Harman74.cor$cov,factors=4)
factor.congruence(list(mle,pa,pc,uls,wls))

#note that the order of factors and the sign of some of factors may differ 

#finally, compare the unrotated factor, ml, uls, and  wls solutions
wls &lt;- fa(Harman74.cor$cov,4,rotate="none",fm="wls")
pa &lt;- fa(Harman74.cor$cov,4,rotate="none",fm="pa")
minres &lt;-  factanal(factors=4,covmat=Harman74.cor$cov,rotation="none")
mle &lt;- fa(Harman74.cor$cov,4,rotate="none",fm="mle")
uls &lt;- fa(Harman74.cor$cov,4,rotate="none",fm="uls")
factor.congruence(list(minres,mle,pa,wls,uls))
#in particular, note the similarity of the mle and min res solutions
#note that the order of factors and the sign of some of factors may differ 



#an example of where the ML and PA and MR models differ is found in Thurstone.33.
#compare the first two factors with the 3 factor solution 
Thurstone.33 &lt;- as.matrix(Thurstone.33)
mle2 &lt;- fa(Thurstone.33,2,rotate="none",fm="mle")
mle3 &lt;- fa(Thurstone.33,3 ,rotate="none",fm="mle")
pa2 &lt;- fa(Thurstone.33,2,rotate="none",fm="pa")
pa3 &lt;- fa(Thurstone.33,3,rotate="none",fm="pa")
mr2 &lt;- fa(Thurstone.33,2,rotate="none")
mr3 &lt;- fa(Thurstone.33,3,rotate="none")
factor.congruence(list(mle2,mr2,pa2,mle3,pa3,mr3))

#f5 &lt;- fa(psychTools::bfi[1:25],5)
#f5  #names are not in ascending numerical order (see note)
#colnames(f5$loadings) &lt;- paste("F",1:5,sep="")
#f5

#Get the variance accounted for object from the print function
p &lt;- print(mr3)
round(p$Vaccounted,2)

#pool data and fa the pooled result (not run)
#datasets.list &lt;- list(bfi[1:500,1:25],bfi[501:1000,1:25],
#   bfi[1001:1500,1:25],bfi[1501:2000,1:25],bfi[2001:2500,1:25])  #five different data sets
#temp &lt;- fa.pooled(datasets.list,5)    #do 5 factor analyses, pool the results
</code></pre>

<hr>
<h2 id='fa.diagram'> Graph factor loading matrices</h2><span id='topic+fa.graph'></span><span id='topic+fa.rgraph'></span><span id='topic+fa.diagram'></span><span id='topic+extension.diagram'></span><span id='topic+het.diagram'></span>

<h3>Description</h3>

<p>Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code><a href="#topic+fa.diagram">fa.diagram</a></code> uses the various <code><a href="#topic+diagram">diagram</a></code> functions to draw the diagram. <code><a href="#topic+fa.graph">fa.graph</a></code> generates dot code for external plotting.  <code><a href="#topic+fa.rgraph">fa.rgraph</a></code> uses the Rgraphviz package (if available) to draw the graph. <code><a href="#topic+het.diagram">het.diagram</a></code> will draw &quot;heterarchy&quot; diagrams of factor/scale solutions at different levels.  All of these as well as some additional functions may be called by <code><a href="#topic+diagram">diagram</a></code> which is a wrapper to the specific functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.diagram(fa.results,Phi=NULL,fe.results=NULL,sort=TRUE,labels=NULL,cut=.3,
     simple=TRUE, errors=FALSE,g=FALSE,digits=1,e.size=.05,rsize=.15,side=2,
    main,cex=NULL,l.cex=NULL, marg=c(.5,.5,1,.5),adj=1,ic=FALSE, ...) 
het.diagram(r,levels,cut=.3,digits=2,both=TRUE, 
        main="Heterarchy diagram",l.cex,gap.size,...) 
extension.diagram(fa.results,Phi=NULL,fe.results=NULL,sort=TRUE,labels=NULL,cut=.3,
    f.cut, e.cut=.1,simple=TRUE,e.simple=FALSE,errors=FALSE,g=FALSE,
    digits=1,e.size=.05,rsize=.15,side=2,main,cex=NULL, e.cex=NULL,
    marg=c(.5,.5,1,.5),adj=1,ic=FALSE, ...)

fa.graph(fa.results,out.file=NULL,labels=NULL,cut=.3,simple=TRUE,
   size=c(8,6), node.font=c("Helvetica", 14),
    edge.font=c("Helvetica", 10), rank.direction=c("RL","TB","LR","BT"),
     digits=1,main="Factor Analysis", ...)
fa.rgraph(fa.results,out.file=NULL,labels=NULL,cut=.3,simple=TRUE,
   size=c(8,6), node.font=c("Helvetica", 14),
    edge.font=c("Helvetica", 10), rank.direction=c("RL","TB","LR","BT"),
     digits=1,main="Factor Analysis",graphviz=TRUE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.diagram_+3A_fa.results">fa.results</code></td>
<td>
<p>The output of factor analysis, principal components analysis, or ICLUST analysis.  May also be a factor loading matrix from anywhere.</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_phi">Phi</code></td>
<td>
<p>Normally not specified (it is is found in the FA, pc, or ICLUST, solution), this may be given if the input is a loadings matrix.</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_fe.results">fe.results</code></td>
<td>
<p>the results of a factor extension analysis (if any)</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_out.file">out.file</code></td>
<td>
<p> If it exists, a dot representation of the graph will be stored here (fa.graph)</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_labels">labels</code></td>
<td>
<p> Variable labels </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_cut">cut</code></td>
<td>
<p> Loadings with abs(loading) &gt; cut will be shown </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_f.cut">f.cut</code></td>
<td>
<p>factor correlations &gt; f.cut will be shown</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_e.cut">e.cut</code></td>
<td>
<p>extension loadings with abs(loading) &gt; e.cut will be shown</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_simple">simple</code></td>
<td>
<p>Only the biggest loading per item is shown</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_e.simple">e.simple</code></td>
<td>
<p>Only the biggest loading per extension item is shown </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_g">g</code></td>
<td>
<p>Does the factor matrix reflect a g (first) factor.  If so, then draw this to the left of the variables, with the remaining factors to the right of the variables.  It is useful to turn off the simple parameter in this case.</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_ic">ic</code></td>
<td>
<p>If drawing a cluster analysis result, should we treat it as latent variable model (ic=FALSE) or as an observed variable model (ic=TRUE) </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_r">r</code></td>
<td>
<p>A correlation matrix for the het.diagram function</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_levels">levels</code></td>
<td>
<p>A list of the elements in each level</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_both">both</code></td>
<td>
<p>Should arrows have double heads (in het.diagram)</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_size">size</code></td>
<td>
<p>graph size </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_sort">sort</code></td>
<td>
<p>sort the factor loadings before showing the diagram</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_errors">errors</code></td>
<td>
<p>include error estimates (as arrows)</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_e.size">e.size</code></td>
<td>
<p>size of ellipses</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_rsize">rsize</code></td>
<td>
<p>size of rectangles</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_side">side</code></td>
<td>
<p>on which side should error arrows go?</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_cex">cex</code></td>
<td>
<p>modify font size</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_e.cex">e.cex</code></td>
<td>
<p>Modify the font size for the Dependent variables, defaults to cex</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_l.cex">l.cex</code></td>
<td>
<p>modify the font size in arrows, defaults to cex</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_gap.size">gap.size</code></td>
<td>
<p>The gap in the arrow for the label.  Can be adjusted to compensate for variations in cex or l.cex</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_marg">marg</code></td>
<td>
<p>sets the margins to be wider than normal, returns them to the normal size upon exit</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_adj">adj</code></td>
<td>
<p>how many different positions (1-3) should be used for the numeric labels. Useful if they overlap each other.</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_node.font">node.font</code></td>
<td>
<p>what font should be used for nodes in fa.graph  </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_edge.font">edge.font</code></td>
<td>
<p>what font should be used for edges in fa.graph  </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_rank.direction">rank.direction</code></td>
<td>
<p> parameter passed to Rgraphviz&ndash; which way to draw the graph </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_digits">digits</code></td>
<td>
<p> Number of digits to show as an edgelable </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_main">main</code></td>
<td>
<p> Graphic title, defaults to &quot;factor analyis&quot; or &quot;factor analysis and extension&quot; </p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_graphviz">graphviz</code></td>
<td>
<p>Should we try to use Rgraphviz for output?</p>
</td></tr>
<tr><td><code id="fa.diagram_+3A_...">...</code></td>
<td>
<p> other parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. 
</p>
<p>By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.
</p>
<p>fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. 
</p>
<p>Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., <code><a href="#topic+bifactor">bifactor</a></code> and <code><a href="#topic+biquartimin">biquartimin</a></code>.  See <code><a href="#topic+omega">omega</a></code> for an alternative way to find these structures.  
</p>
<p>The <code><a href="#topic+het.diagram">het.diagram</a></code> function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  
</p>
<p><code><a href="#topic+extension.diagram">extension.diagram</a></code> will draw a <code><a href="#topic+fa.extend">fa.extend</a></code> result with slightly more control than using <code><a href="#topic+fa.diagram">fa.diagram</a></code> or the more generic <code><a href="#topic+diagram">diagram</a></code> function.
</p>
<p>In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. 
</p>
<p>The normal input is taken from the output of either <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+ICLUST">ICLUST</a></code>. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.
</p>
<p>It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. 
</p>
<p>It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  
</p>
<p>To specify the model for a structural equation confirmatory analysis of the results, use <code><a href="#topic+structure.diagram">structure.diagram</a></code> instead. 
</p>


<h3>Value</h3>

<p>fa.diagram: A path diagram is drawn without using Rgraphviz.  This is probably the more useful function.
</p>
<p>fa.rgraph: A graph is drawn using rgraphviz.  If an output file is specified, the graph instructions are also saved in the dot language. 
</p>
<p>fa.graph: the graph instructions are saved in the dot language. </p>


<h3>Note</h3>

<p> fa.rgraph requires Rgraphviz. Because there are occasional difficulties installing Rgraphviz from Bioconductor in that some libraries are misplaced and need to be relinked, it is probably better to use fa.diagram or fa.graph.
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>See Also</h3>

 <p><code><a href="#topic+diagram">diagram</a></code>, <code><a href="#topic+omega.graph">omega.graph</a></code>, <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>, <code><a href="#topic+bassAckward.diagram">bassAckward.diagram</a></code>.  <code><a href="#topic+structure.diagram">structure.diagram</a></code> will convert the factor diagram to sem modeling code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
test.simple &lt;- fa(item.sim(16),2,rotate="oblimin")
#if(require(Rgraphviz)) {fa.graph(test.simple) } 
fa.diagram(test.simple)
f3 &lt;- fa(Thurstone,3,rotate="cluster")
fa.diagram(f3,cut=.4,digits=2)
f3l &lt;- f3$loadings
fa.diagram(f3l,main="input from a matrix")
Phi &lt;- f3$Phi
fa.diagram(f3l,Phi=Phi,main="Input from a matrix")
fa.diagram(ICLUST(Thurstone,2,title="Two cluster solution of Thurstone"),main="Input from ICLUST")
het.diagram(Thurstone,levels=list(1:4,5:8,3:7))

#show how caffiene increases arousal and tension
select &lt;- c("active" , "alert" ,  "aroused", "sleepy",  "tired" , 
          "drowsy" , "anxious", "jittery" ,"nervous",
 "calm" ,   "relaxed" ,"at.ease" ,"gender",  "drug")
 msq.small &lt;- psychTools::msqR[select]
fe &lt;- fa.extend(msq.small, 2,1:12,14)
extension.diagram(fe, ic=TRUE) #drug causes arousal
</code></pre>

<hr>
<h2 id='fa.extension'>Apply Dwyer's factor extension to find factor loadings for extended variables</h2><span id='topic+fa.extension'></span><span id='topic+fa.extend'></span><span id='topic+faRegression'></span><span id='topic+faReg'></span>

<h3>Description</h3>

<p>Dwyer (1937) introduced a method for finding factor loadings for variables not included in the original analysis.  This is basically finding the unattenuated correlation of the extension variables with the factor scores.  An alternative, which does not correct for factor reliability was proposed by Gorsuch (1997). Both options are an application of exploratory factor analysis with extensions to new variables. Also useful for finding the validities of variables in the factor space.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.extension(Roe,fo,correct=TRUE)
fa.extend(r,nfactors=1,ov=NULL,ev=NULL,n.obs = NA, np.obs=NULL,
  correct=TRUE,rotate="oblimin",SMC=TRUE,   warnings=TRUE, fm="minres",
  alpha=.1,omega=FALSE,cor="cor",use="pairwise",cor.correct=.5,weight=NULL,
    missing=FALSE, smooth=TRUE, ...) 
faRegression(r,nfactors=1,ov=NULL,dv=NULL, n.obs = NA
 , np.obs=NULL,correct=TRUE,rotate="oblimin",SMC=TRUE,warnings=TRUE, fm="minres",alpha=.1,
 omega=FALSE,cor="cor",use="pairwise",cor.correct=.5,weight=NULL,smooth=TRUE, ...)
#this is just an alias for 
faReg(r,nfactors=1,ov=NULL,dv=NULL, n.obs = NA
 , np.obs=NULL,correct=TRUE,rotate="oblimin",SMC=TRUE,warnings=TRUE, fm="minres",alpha=.1,
 omega=FALSE,cor="cor",use="pairwise",cor.correct=.5,weight=NULL,smooth=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.extension_+3A_roe">Roe</code></td>
<td>
<p>The correlations of the original variables with the extended variables</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_fo">fo</code></td>
<td>
<p>The output from the <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+omega">omega</a></code> functions applied to the original variables.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_correct">correct</code></td>
<td>
<p>correct=TRUE produces Dwyer's solution, correct=FALSE does not correct for factor reliability.  This is not quite the Gorsuch technique.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_r">r</code></td>
<td>
<p>A correlation or data matrix with all of the variables to be analyzed by fa.extend</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_ov">ov</code></td>
<td>
<p>The original variables to factor</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_ev">ev</code></td>
<td>
<p>The extension variables</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_dv">dv</code></td>
<td>
<p>The dependent variables if doing faRegression</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_nfactors">nfactors</code></td>
<td>
<p> Number of factors to extract, default is 1 </p>
</td></tr>
<tr><td><code id="fa.extension_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_np.obs">np.obs</code></td>
<td>
<p>Pairwise number of observations.  Required if using fm=&quot;minchi&quot;, suggested in other cases to estimate the empirical goodness of fit.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;,  &quot;bentlerT&quot;,  &quot;geominT&quot; and &quot;bifactor&quot; are orthogonal rotations.  &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, &quot;bentlerQ,  &quot;geominQ&quot; and &quot;biquartimin&quot; and &quot;cluster&quot; are possible rotations or transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_smc">SMC</code></td>
<td>
<p>Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. </p>
</td></tr>
<tr><td><code id="fa.extension_+3A_warnings">warnings</code></td>
<td>
<p>warnings=TRUE =&gt; warn if number of factors is too many </p>
</td></tr>
<tr><td><code id="fa.extension_+3A_fm">fm</code></td>
<td>
<p>factoring method  fm=&quot;minres&quot; will do a minimum residual (OLS), fm=&quot;wls&quot; will do a weighted least squares (WLS) solution, fm=&quot;gls&quot; does a generalized weighted least squares (GLS), fm=&quot;pa&quot; will do the principal factor solution, fm=&quot;ml&quot; will do a maximum likelihood factor analysis. fm=&quot;minchi&quot; will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence intervals for RMSEA</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_omega">omega</code></td>
<td>
<p>Do the extension analysis for an omega type analysis</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_cor">cor</code></td>
<td>
<p>Pass the kind of correlation to fa (defaults to Pearson, can use mixed)</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_use">use</code></td>
<td>
<p>Option for the cor function on how to handle missing data.</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_cor.correct">cor.correct</code></td>
<td>
<p>The correction to be passed to mixed, tet, or polycor (defaults to .5)</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_weight">weight</code></td>
<td>
<p>Should we weight the variables? (see <code><a href="#topic+fa">fa</a></code>)</p>
</td></tr> 
<tr><td><code id="fa.extension_+3A_missing">missing</code></td>
<td>
<p>When finding factor scores, are missing data allowed?</p>
</td></tr>
<tr><td><code id="fa.extension_+3A_smooth">smooth</code></td>
<td>
<p>Should we smooth the correlation matrix &ndash; smoothing produces bad output if there are NAs in the R matrix </p>
</td></tr>
<tr><td><code id="fa.extension_+3A_...">...</code></td>
<td>
<p>Additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is sometimes the case that factors are derived from a set of variables (the Fo factor loadings) and we want to see what the loadings of an extended set of variables (Fe) would be. Given the original correlation matrix Ro and the correlation of these original variables with the extension variables of Roe, it is a straight forward calculation to find the loadings Fe of the extended variables on the original factors.  This technique was developed by Dwyer (1937) for the case of adding new variables to a factor analysis without doing all the work over again. But, as discussed by Horn (1973) factor extension is also appropriate when one does not want to include the extension variables in the original factor analysis, but does want to see what the loadings would be anyway.
</p>
<p>This could be done by estimating the factor scores and then finding the covariances of the extension variables with the factor scores. If raw data are available, this is done by <code><a href="#topic+faReg">faReg</a></code>. But if the original data are not available, but just the covariance or correlation matrix is, then the use of <code><a href="#topic+fa.extension">fa.extension</a></code> is most appropriate to estimate those correlations before doing the regression.  
</p>
<p>The factor analysis results from either <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+omega">omega</a></code> functions applied to the original correlation matrix is extended to the extended variables given the correlations (Roe) of the extended variables with the original variables.
</p>
<p><code><a href="#topic+fa.extension">fa.extension</a></code> assumes that the original factor solution was found by the <code><a href="#topic+fa">fa</a></code> function.
</p>
<p>For a very nice discussion of the relationship between factor scores, correlation matrices, and the factor loadings in a factor extension, see Horn (1973).
</p>
<p>The <code><a href="#topic+fa.extend">fa.extend</a></code> function may be thought of as a &quot;seeded&quot; factor analysis.  That is, the variables in the original set are factored, this solution is then extended to the extension set, and the resulting output is presented as if both the original and extended variables were factored together.  This may also be done for an omega analysis. 
</p>
<p>The example of  <code><a href="#topic+fa.extend">fa.extend</a></code> compares the extended solution to a direct solution of all of the variables using <code><a href="#topic+factor.congruence">factor.congruence</a></code>. 
</p>
<p>Another important use of factor extension is when parts of a correlation matrix are missing.  Thus suppose you have a correlation matrix formed of variables 1 .. i, j..n  where the first i variables correlate with each other (R11) and with the  j... n (R12) variables, but the elements of of the R22 (j..n) are missing.  That is, X has a missing data structure but we can find the correlation matrix R[1..n,1..n]  =cor(X,na.rm=TRUE) 
</p>
<p>R[1..n,1..n] = </p>

<table>
<tr>
 <td style="text-align: right;">
   R[1..i,1..i] </td><td style="text-align: right;"> R[1..i,j..n]  </td>
</tr>
<tr>
 <td style="text-align: right;">
   R[j..n,1..i] </td><td style="text-align: right;"> NA  </td>
</tr>
<tr>
 <td style="text-align: right;"> 
</td>
</tr>

</table>

<p>Factors can be found for the R11 matrix and then extended to the variables in the entire matrix. This allows for irt approaches to be applied even with significantly missing data.
</p>
<p>f &lt;- fa.extend(R,nf=1, ov=1:i,ev=j:n)
</p>
<p>Then the loadings of n loadings of f may be found. 
</p>
<p>Combining this procedure with <code><a href="#topic+fa2irt">fa2irt</a></code> allows us to find the irt based parameters of the n variables, even though we have substantially incomplete data. 
</p>
<p>Using the idea behind factor extension it is straightforward to apply these techniques to multiple regression, because the extended loadings are functionally beta weights.  . <code><a href="#topic+faRegression">faRegression</a></code> just organizes the extension analysis in terms  of a regression analysis.  If the raw data are available, it first finds the factor scores and then correlates these with the dependent variable in the standard regression approach.  But, if just the correlation matrix is available, it estimates the factor by dependent variable correlations by the use of <code>link{fa.extend}</code> before doing the regression.
</p>


<h3>Value</h3>

<p>Factor Loadings of the exended variables on the original factors</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Paul S. Dwyer (1937) The determination of the factor loadings of a given test from the known factor loadings of other tests. Psychometrika, 3, 173-178
</p>
<p>Gorsuch, Richard L. (1997) New procedure for extension analysis in exploratory factor analysis,  Educational and Psychological Measurement, 57, 725-740
</p>
<p>Horn, John L. (1973) On extension analysis and its relation to correlations between variables and factor scores.  Multivariate Behavioral Research, 8, (4), 477-489.
</p>


<h3>See Also</h3>

<p> See Also as <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+Dwyer">Dwyer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> #The Dwyer Example
Ro &lt;- Dwyer[1:7,1:7]
Roe &lt;- Dwyer[1:7,8]
fo &lt;- fa(Ro,2,rotate="none")
fe &lt;- fa.extension(Roe,fo)

#an example from simulated data
set.seed(42) 
 d &lt;- sim.item(12)    #two orthogonal factors 
 R &lt;- cor(d)
 Ro &lt;- R[c(1,2,4,5,7,8,10,11),c(1,2,4,5,7,8,10,11)]
 Roe &lt;- R[c(1,2,4,5,7,8,10,11),c(3,6,9,12)]
 fo &lt;- fa(Ro,2)
 fe &lt;- fa.extension(Roe,fo)
 fa.diagram(fo,fe=fe)
 
 #alternatively just specify the original variables and the extension variables
 fe = fa.extend(R, 2, ov =c(1,2,4,5,7,8,10,11), ev=c(3,6,9,12))
 fa.diagram(fe$fo, fe = fe$fe)
 
 #create two correlated factors
 fx &lt;- matrix(c(.9,.8,.7,.85,.75,.65,rep(0,12),.9,.8,.7,.85,.75,.65),ncol=2)
 Phi &lt;- matrix(c(1,.6,.6,1),2)
 sim.data &lt;- sim.structure(fx,Phi,n=1000,raw=TRUE)
 R &lt;- cor(sim.data$observed)
 Ro &lt;- R[c(1,2,4,5,7,8,10,11),c(1,2,4,5,7,8,10,11)]
 Roe &lt;- R[c(1,2,4,5,7,8,10,11),c(3,6,9,12)]
 fo &lt;- fa(Ro,2)
 fe &lt;- fa.extension(Roe,fo)
 fa.diagram(fo,fe=fe)
 
 #now show how fa.extend works with the same data set
 #note that we have to make sure that the variables are in the order to do the factor congruence
 fe2 &lt;- fa.extend(sim.data$observed,2,ov=c(1,2,4,5,7,8,10,11),ev=c(3,6,9,12))
 fa.diagram(fe2,main="factor analysis with extension variables")
 fa2 &lt;- fa(sim.data$observed[,c(1,2,4,5,7,8,10,11,3,6,9,12)],2)
 factor.congruence(fe2,fa2)
 summary(fe2)
 
 #an example of extending an omega analysis
 
 
fload &lt;- matrix(c(c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),
        rep(0,20)),c(c(c(.9,.8,.7,.6),rep(0,20)),c(.9,.8,.7,.6))),ncol=5)
 gload &lt;- matrix(rep(.7,5))
 five.factor &lt;- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
 ss &lt;- c(1,2,3,5,6,7,9,10,11,13,14,15,17,18,19)
 Ro &lt;- cor(five.factor$observed[,ss])
 Re &lt;- cor(five.factor$observed[,ss],five.factor$observed[,-ss])
 om5 &lt;-omega(Ro,5)   #the omega analysis
 om.extend &lt;- fa.extension(Re,om5) #the extension analysis
 om.extend #show it
 #now, include it in an omega diagram
 combined.om &lt;- rbind(om5$schmid$sl[,1:ncol(om.extend$loadings)],om.extend$loadings)
 class(combined.om) &lt;-c("psych","extend")
 omega.diagram(combined.om,main="Extended Omega") 
 
 #show how to use fa.extend to do regression analyses with the raw data
 b5 &lt;- faReg (bfi, nfactors = 5, ov =1:25, dv =26:28)
 extension.diagram(b5)
 R &lt;-cor(bfi,use="pairwise")
 b5.r &lt;- faReg(R, nfactors = 5, ov =1:25, dv =26:28) # not identical to b5
 round(b5$regression$coefficients - b5.r$regression$coefficients,2)
</code></pre>

<hr>
<h2 id='fa.lookup'>A set of functions for factorial and empirical scale construction</h2><span id='topic+lookup'></span><span id='topic+lookupItems'></span><span id='topic+fa.lookup'></span><span id='topic+item.lookup'></span><span id='topic+itemSort'></span><span id='topic+keys.lookup'></span><span id='topic+lookupFromKeys'></span><span id='topic+lmCorLookup'></span>

<h3>Description</h3>

<p> When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code><a href="#topic+fa.lookup">fa.lookup</a></code> of a set of items), or to some specific set of criteria  (e.g., <code><a href="#topic+bestScales">bestScales</a></code>). Given a dictionary of item content, these routines will sort by factor loading, item means,  or criteria correlations and display the item content. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lookup(x,y,criteria=NULL)
lookupItems(content=NULL,dictionary=NULL,search=c("Item","Content","item"))
fa.lookup(f,dictionary=NULL,digits=2,cut=.0,n=NULL,sort=TRUE)
item.lookup(f,m, dictionary,cut=.3, digits = 2) 
itemSort(m, dictionary, ascending=TRUE, digits = 2)
keys.lookup(keys.list,dictionary)
lookupFromKeys(keys.list,dictionary,n=20,cors=NULL,sort=TRUE,suppress.names=FALSE,
      digits=2)
lmCorLookup(x,dictionary=NULL,cut=0,digits=2,p=.05) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.lookup_+3A_x">x</code></td>
<td>
<p>A data matrix or data frame depending upon the function.</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_y">y</code></td>
<td>
<p>A data matrix or data frame or a vector</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_criteria">criteria</code></td>
<td>
<p>Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. </p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_f">f</code></td>
<td>
<p>The object returned from either a factor analysis (fa) or a principal components analysis (principal) </p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_content">content</code></td>
<td>
<p>The word(s) to search for from a dictionary</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_keys.list">keys.list</code></td>
<td>
<p>A list of scoring keys suitable to use for make.keys</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_cut">cut</code></td>
<td>
<p>Return all values in abs(x[,c1]) &gt; cut.</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_n">n</code></td>
<td>
<p>Return the n best items per factor (as long as they have their highest loading on that factor)</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_cors">cors</code></td>
<td>
<p>If provided (e.g. from scoreItems) will be added to the lookupFromKeys output</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_dictionary">dictionary</code></td>
<td>
<p>a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content. See Notes for how to construct a dictionary.</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_search">search</code></td>
<td>
<p>Column names of dictionary to search, defaults to &quot;Item&quot; or &quot;Content&quot; (dictionaries have different labels for this column), can search any column specified by search.</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_m">m</code></td>
<td>
<p>A data frame of item means</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_digits">digits</code></td>
<td>
<p>round to digits</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_sort">sort</code></td>
<td>
<p>Should the factors be sorted first?</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_suppress.names">suppress.names</code></td>
<td>
<p>In lookupFromKeys, should we suppress the column labels</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_p">p</code></td>
<td>
<p>Show lmCor regressions with probability &lt; p</p>
</td></tr>
<tr><td><code id="fa.lookup_+3A_ascending">ascending</code></td>
<td>
<p>order to sort the means in itemSort &ndash; see dfOrder</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> and <code><a href="#topic+lookup">lookup</a></code> are simple helper functions to summarize correlation matrices or factor loading matrices.  <code><a href="#topic+bestItems">bestItems</a></code> will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values &gt; cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary <code><a href="psychTools.html#topic+bfi.dictionary">bfi.dictionary</a></code>).
</p>
<p><code><a href="#topic+lookup">lookup</a></code> is used by <code><a href="#topic+bestItems">bestItems</a></code> and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.
Suppose that you have a &quot;dictionary&quot; of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). 
</p>
<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the <code><a href="psychTools.html#topic+df2latex">df2latex</a></code> function with the char option set to TRUE.
</p>
<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> will work with output from <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+pca">pca</a></code> or <code><a href="#topic+omega">omega</a></code>.  For omega output, the items are sorted by the non-general factor loadings.
</p>
<p>Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)
</p>
<p><code><a href="#topic+item.lookup">item.lookup</a></code> combines the output from a factor analysis <code><a href="#topic+fa">fa</a></code> with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings &gt; cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements.  Note the means must be a one column matrix (with row names), not a vector (without rownames.) 
</p>
<p><code><a href="#topic+itemSort">itemSort</a></code> Combine item means and item content and then sort them by the item means.
</p>
<p><code><a href="#topic+lookupItems">lookupItems</a></code> searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
</p>


<h3>Value</h3>

 
<p><code><a href="#topic+bestItems">bestItems</a></code> returns a sorted list of factor loadings or correlations with the labels as provided in the dictionary.
</p>
<p><code><a href="#topic+lookup">lookup</a></code> is a very simple implementation of the match function. 
</p>
<p><code><a href="#topic+fa.lookup">fa.lookup</a></code> takes a factor/cluster analysis object (or just a keys like matrix), sorts it using <code><a href="#topic+fa.sort">fa.sort</a></code> and then matches by row.name to the corresponding dictionary entries.
</p>


<h3>Note</h3>

<p>Although empirical scale construction is appealing, it has the basic problem of capitalizing on chance.  Thus, be careful of over interpreting the results unless working with large samples.  Iteration and bootstrapping aggregation (bagging) gives information on the stability of the solutions. See <code><a href="#topic+bestScales">bestScales</a></code>
</p>
<p>To create a dictionary, create an object with row names as the item numbers, and the columns as the item content.  See the bfi.dictionary in the psychTools package as an example.  The bfi.dictionary was constructed from a spreadsheet with multiple columns, the first of which was the column names of the bfi. See the first (not run) example. 
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, W.  (in preparation) An introduction to psychometric theory with applications in R. Springer.  (Available online at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>). 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+iclust">iclust</a></code>,<code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+bestScales">bestScales</a></code> and <code><a href="#topic+bestItems">bestItems</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Tne following shows how to create a dictionary
#first, copy the spreadsheet to the clipboard
#the spreadsheet should have multiple columns
#col 1         col 2     col 3
#item	       content   label 
#A1           Am indifferent to the feelings of others.     (q_146)
#A2 	      Inquire about others' well-being. (q_1162
#...

# bfi.dictionary &lt;- read.clipboard.tab()  #read from the clipboard
# rownames(bfi.dictionary) &lt;- bfi.dictionary[1] #the first column had the names
# bfi.dictionary &lt;- bfi.dictionary[-1]  #these are redundant, drop them

f5 &lt;- fa(psychTools::bfi,5)
m &lt;- colMeans(psychTools::bfi,na.rm=TRUE)
item.lookup(f5,m,dictionary=psychTools::bfi.dictionary[2,drop=FALSE])
 #just show the item content, not the source of the items
fa.lookup(f5,dictionary=psychTools::bfi.dictionary[2]) 

#just show the means and the items
#use the m vector we found above
itemSort(as.matrix(m),dictionary=psychTools::bfi.dictionary[,2:3,drop=FALSE])


#show how to use lookupFromKeys 
bfi.keys &lt;- 
list(agree=c("-A1","A2","A3","A4","A5"),conscientiousness=c("C1","C2","C3","-C4","-C5"),
extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
openness = c("O1","-O2","O3","O4","-O5")) 
bfi.over &lt;- scoreOverlap(bfi.keys,bfi) #returns the corrected for overlap values
lookupFromKeys(bfi.keys,psychTools::bfi.dictionary,n=5, cors=bfi.over$item.cor)
 #show the keying information
lookupItems("life",psychTools::spi.dictionary) #find those items with "life" in the item


</code></pre>

<hr>
<h2 id='fa.multi'>Multi level (hierarchical) factor analysis
</h2><span id='topic+fa.multi'></span><span id='topic+fa.multi.diagram'></span>

<h3>Description</h3>

<p>Some factor analytic solutions produce correlated factors which may in turn be factored.  If the solution has one higher order, the omega function is most appropriate.  But, in the case of multi higher order factors, then the faMulti function will do a lower level factoring and then factor the resulting correlation matrix.  Multi level factor diagrams are also shown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.multi(r, nfactors = 3, nfact2 = 1, n.obs = NA, n.iter = 1, rotate = "oblimin", 
 scores = "regression", residuals = FALSE, SMC = TRUE, covar = FALSE, missing = 
 FALSE,impute = "median", min.err = 0.001, max.iter = 50, symmetric = TRUE, warnings 
 =TRUE, fm = "minres", alpha = 0.1, p = 0.05, oblique.scores = FALSE, np.obs = NULL, 
 use ="pairwise", cor = "cor", ...)

fa.multi.diagram(multi.results,sort=TRUE,labels=NULL,flabels=NULL,f2labels=NULL,cut=.2,
	gcut=.2, simple=TRUE,errors=FALSE,
    digits=1,e.size=.1,rsize=.15,side=3,main=NULL,cex=NULL,color.lines=TRUE
    ,marg=c(.5,.5,1.5,.5),adj=2, ...) 

</code></pre>


<h3>Arguments</h3>

<p>The arguments match those of the fa function.
</p>
<table>
<tr><td><code id="fa.multi_+3A_r">r</code></td>
<td>

<p>A correlation matrix or raw data matrix
</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_nfactors">nfactors</code></td>
<td>
<p>The desired number of factors for the lower level
</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_nfact2">nfact2</code></td>
<td>
<p>The desired number of factors for the higher level
</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_np.obs">np.obs</code></td>
<td>
<p>The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;,  &quot;bentlerT&quot;, &quot;equamax&quot;, &quot;varimin&quot;, &quot;geominT&quot; and &quot;bifactor&quot; are orthogonal rotations.  &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, &quot;bentlerQ,  &quot;geominQ&quot; and &quot;biquartimin&quot; and &quot;cluster&quot; are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of bootstrap interations to do in fa or fa.poly</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_residuals">residuals</code></td>
<td>
<p>Should the residual matrix be shown </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_scores">scores</code></td>
<td>
<p>the default=&quot;regression&quot; finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression (&quot;Thurstone&quot;), correlaton preserving (&quot;tenBerge&quot;) as well as &quot;Anderson&quot; and &quot;Bartlett&quot; using the appropriate algorithms (see factor.scores). Although scores=&quot;tenBerge&quot; is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_smc">SMC</code></td>
<td>
<p>Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_covar">covar</code></td>
<td>
<p>if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_missing">missing</code></td>
<td>
<p>if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_impute">impute</code></td>
<td>
<p>&quot;median&quot; or &quot;mean&quot; values are used to replace missing values</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_min.err">min.err</code></td>
<td>
<p>Iterate until the change in communalities is less than min.err</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for convergence </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_symmetric">symmetric</code></td>
<td>
<p>symmetric=TRUE forces symmetry by just looking at the lower off diagonal values</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_warnings">warnings</code></td>
<td>
<p>warnings=TRUE =&gt; warn if number of factors is too many </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_fm">fm</code></td>
<td>
<p>factoring method  fm=&quot;minres&quot; will do a minimum residual (OLS), fm=&quot;wls&quot; will do a weighted least squares (WLS) solution, fm=&quot;gls&quot; does a generalized weighted least squares (GLS), fm=&quot;pa&quot; will do the principal factor solution, fm=&quot;ml&quot; will do a maximum likelihood factor analysis. fm=&quot;minchi&quot; will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence intervals for RMSEA</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_p">p</code></td>
<td>
<p>if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_oblique.scores">oblique.scores</code></td>
<td>
<p>When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  </p>
</td></tr> 
<tr><td><code id="fa.multi_+3A_use">use</code></td>
<td>
<p>How to treat missing data, use=&quot;pairwise&quot; is the default&quot;.  See cor for other options.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_cor">cor</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, 
&quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric, &quot;mixed&quot; uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_multi.results">multi.results</code></td>
<td>
<p>The results from fa.multi</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_labels">labels</code></td>
<td>
<p> variable labels </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_flabels">flabels</code></td>
<td>
<p>Labels for the factors (not counting g)</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_f2labels">f2labels</code></td>
<td>
<p>The labels for the second order factors</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_size">size</code></td>
<td>
<p>size of graphics window </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_digits">digits</code></td>
<td>
<p> Precision of labels </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_cex">cex</code></td>
<td>
<p>control font size</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_color.lines">color.lines</code></td>
<td>
<p>Use black for positive, red for negative</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_marg">marg</code></td>
<td>
<p>The margins for the figure are set to be wider than normal by default</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_adj">adj</code></td>
<td>
<p>Adjust the location of the factor loadings to vary as factor mod 4 + 1</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_main">main</code></td>
<td>
<p> main figure caption </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_...">...</code></td>
<td>
<p>additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax.  In addition, for fa.multi.diagram, other options to pass into the graphics packages </p>
</td></tr>
<tr><td><code id="fa.multi_+3A_e.size">e.size</code></td>
<td>
<p>the size to draw the ellipses for the factors. This is scaled by the  number of variables.</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_cut">cut</code></td>
<td>
<p>Minimum path coefficient to draw</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_gcut">gcut</code></td>
<td>
<p>Minimum general factor path to draw</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_simple">simple</code></td>
<td>
<p>draw just one path per item</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_sort">sort</code></td>
<td>
<p>sort the solution before making the diagram</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_side">side</code></td>
<td>
<p>on which side should errors  be drawn?</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_errors">errors</code></td>
<td>
<p>show the error estimates</p>
</td></tr>
<tr><td><code id="fa.multi_+3A_rsize">rsize</code></td>
<td>
<p>size of the rectangles</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+fa">fa</a></code> and <code><a href="#topic+omega">omega</a></code> for a discussion of factor analysis and of the case of one higher order factor.
</p>


<h3>Value</h3>

<table>
<tr><td><code>f1</code></td>
<td>
<p>The standard output from a factor analysis from  <code><a href="#topic+fa">fa</a></code> for the raw variables</p>
</td></tr>
<tr><td><code>f2</code></td>
<td>
<p>The standard output from a factor analysis from  <code><a href="#topic+fa">fa</a></code> for the correlation matrix of the level 1 solution. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This is clearly an early implementation (Feb 14 2016) which might be improved.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+omega">omega</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f31 &lt;- fa.multi(Thurstone,3,1) #compare with \code{\link{omega}}
f31
fa.multi.diagram(f31)
</code></pre>

<hr>
<h2 id='fa.parallel'>Scree plots of data or correlation matrix compared to random &ldquo;parallel&quot; matrices </h2><span id='topic+fa.parallel'></span><span id='topic+paSelect'></span><span id='topic+fa.parallel.poly'></span><span id='topic+plot.poly.parallel'></span>

<h3>Description</h3>

<p>One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the &ldquo;scree&quot; plot of the successive eigenvalues.  Sharp breaks in the plot suggest the appropriate number of components or factors to extract.  &ldquo;Parallel&quot; analysis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original. This may be done for continuous , dichotomous, or polytomous data using Pearson, tetrachoric or polychoric correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.parallel(x,n.obs=NULL,fm="minres",fa="both",nfactors=1, 
	main="Parallel Analysis Scree Plots",
	n.iter=20,error.bars=FALSE,se.bars=FALSE,SMC=FALSE,ylabel=NULL,show.legend=TRUE,
	sim=TRUE,quant=.95,cor="cor",use="pairwise",plot=TRUE,correct=.5,sqrt=FALSE)
paSelect(keys,x,cor="cor", fm="minres",plot=FALSE)
	
fa.parallel.poly(x ,n.iter=10,SMC=TRUE,  fm = "minres",correct=TRUE,sim=FALSE,
       fa="both",global=TRUE)   #deprecated
## S3 method for class 'poly.parallel'
plot(x,show.legend=TRUE,fa="both",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.parallel_+3A_x">x</code></td>
<td>
<p> A data.frame or data matrix of scores.  If the matrix is square, it is assumed to be a correlation matrix.  Otherwise, correlations (with pairwise deletion) will be found. </p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_n.obs">n.obs</code></td>
<td>
<p>n.obs=0 implies a data matrix/data.frame.  Otherwise, how many cases were used to find the correlations. </p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_fm">fm</code></td>
<td>
<p>What factor method to use. (minres, ml, uls, wls, gls, pa) See  <code><a href="#topic+fa">fa</a></code> for details.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_fa">fa</code></td>
<td>
<p>show the eigen values for a principal components (fa=&quot;pc&quot;) or a principal axis factor analysis (fa=&quot;fa&quot;) or both principal components and principal factors (fa=&quot;both&quot;)</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_nfactors">nfactors</code></td>
<td>
<p>The number of factors to extract when estimating the eigen values. Defaults to 1, which was the prior value used.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_main">main</code></td>
<td>
<p> a title for the analysis </p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of simulated analyses to perform</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_use">use</code></td>
<td>
<p>How to treat missing data, use=&quot;pairwise&quot; is the default&quot;.  See cor for other options.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_cor">cor</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, 
&quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric, &quot;mixed&quot; uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate.  This matches the call to <code><a href="#topic+fa">fa</a></code>.  </p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_keys">keys</code></td>
<td>
<p>A list of scoring keys to allow fa.parallel on multiple subsets of the data </p>
</td></tr> 
<tr><td><code id="fa.parallel_+3A_correct">correct</code></td>
<td>
<p>For tetrachoric correlations, should a correction for continuity be applied. (See <code><a href="#topic+tetrachoric">tetrachoric</a></code>.)  If set to 0, then no correction is applied, otherwise, the default is to add .5 observations to the cell.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_sim">sim</code></td>
<td>
<p>For continuous data, the default is to resample as well as to generate random normal data.  If sim=FALSE, then just show the resampled results. These two results are very similar. This does not make sense in the case of  correlation matrix, in which case resampling is impossible. In the case of polychoric or tetrachoric data, in addition to randomizing the real data, should we compare the solution to random simulated data.  This will double the processing time, but will basically show the same result.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_error.bars">error.bars</code></td>
<td>
<p>Should error.bars be plotted (default = FALSE)</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_se.bars">se.bars</code></td>
<td>
<p>Should the error bars be standard errors (se.bars=TRUE) or 1 standard deviation (se.bars=FALSE, the default).  With many iterations, the standard errors are very small and some prefer to see the broader range.  The default has been changed in version 1.7.8 to be se.bars=FALSE to more properly show the range.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_smc">SMC</code></td>
<td>
<p>SMC=TRUE finds eigen values after estimating communalities by using SMCs.  smc = FALSE finds eigen values after estimating communalities with the first factor.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_ylabel">ylabel</code></td>
<td>
<p>Label for the y axis &ndash; defaults to &ldquo;eigen values of factors and components&quot;, can be made empty to show many graphs</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_show.legend">show.legend</code></td>
<td>
<p>the default is to have a legend.  For multiple panel graphs, it is better to not show the legend</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_quant">quant</code></td>
<td>
<p>if nothing is specified, the empirical eigen values are compared to the mean of the resampled or simulated eigen values.  If a value (e.g., quant=.95) is specified, then the eigen values are compared against the matching quantile of the simulated data.  Clearly the larger the value of quant, the few factors/components that will be identified.  The default is to use quant=.95.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_global">global</code></td>
<td>
<p>If doing polychoric analyses (fa.parallel.poly) and the number of alternatives differ across items, it is necessary to turn off the global option. fa.parallel.poly is deprecated but this choice is still relevant.</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_...">...</code></td>
<td>
<p>additional plotting parameters, for plot.poly.parallel</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_plot">plot</code></td>
<td>
<p>By default, fa.parallel draws the eigen value plots.  If FALSE, suppresses the graphic output</p>
</td></tr>
<tr><td><code id="fa.parallel_+3A_sqrt">sqrt</code></td>
<td>
<p>If TRUE, take the squareroot of the eigen values to more understandably show the scale. Note, although providing more useful graphics the results will be the same. See DelGuidice, 2022</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cattell's &ldquo;scree&quot; test is one of most simple tests for the number of factors problem.  Horn's (1965) &ldquo;parallel&quot; analysis is an equally compelling procedure.  Other procedures for determining the most optimal number of factors include finding the Very Simple Structure (VSS) criterion (<code><a href="#topic+VSS">VSS</a></code> ) and Velicer's <code><a href="#topic+MAP">MAP</a></code> procedure (included in <code><a href="#topic+VSS">VSS</a></code>). Both the VSS and the MAP criteria are included in the <code><a href="#topic+nfactors">nfactors</a></code> function which also reports the  mean item complexity and the BIC for each of multiple solutions.   fa.parallel plots the eigen values for a principal components and the factor solution (minres by default) and does the same for random matrices of the same size as the original data matrix.  For raw data, the random matrices are 1) a matrix of univariate normal data and 2) random samples (randomized across rows) of the original data.
</p>
<p><code><a href="#topic+fa.parallel">fa.parallel</a></code>
with the  cor=poly option will do what <code><a href="#topic+fa.parallel.poly">fa.parallel.poly</a></code>
explicitly does: parallel analysis for polychoric and tetrachoric factors. 
If the data are dichotomous, <code><a href="#topic+fa.parallel.poly">fa.parallel.poly</a></code>
will find tetrachoric correlations for the real and simulated data, otherwise, if the number of categories is less than 10, it will find polychoric correlations.  
Note that fa.parallel.poly is slower than fa.parallel because of the complexity of calculating the tetrachoric/polychoric correlations.  
The functionality of <code><a href="#topic+fa.parallel.poly">fa.parallel.poly</a></code> is included in <code><a href="#topic+fa.parallel">fa.parallel</a></code> with cor=poly option (etc.) option but the older <code><a href="#topic+fa.parallel.poly">fa.parallel.poly</a></code> is kept for those who call it directly.
</p>
<p>That is, <code><a href="#topic+fa.parallel">fa.parallel</a></code> now will do tetrachorics or polychorics directly if the cor option is set to &quot;tet&quot; or &quot;poly&quot;.  As with <code><a href="#topic+fa.parallel.poly">fa.parallel.poly</a></code> this will take longer.  
</p>
<p>The means of (ntrials) random solutions are shown.  Error bars are usually very small and are suppressed by default but can be shown if requested.  If the sim option is set to TRUE (default), then parallel analyses are done on resampled data as well as random normal data. In the interests of speed, the parallel analyses are done just on resampled data if sim=FALSE.    Both procedures tend to agree.  
</p>
<p>As of version 1.5.4, I added the ability to specify the quantile of the simulated/resampled data, and to plot standard deviations or standard errors.  By default, this is set to the 95th percentile.  
</p>
<p>Alternative ways to estimate the number of factors problem are discussed in the Very Simple Structure  (Revelle and Rocklin, 1979) documentation (<code><a href="#topic+VSS">VSS</a></code>) and include Wayne Velicer's <code><a href="#topic+MAP">MAP</a></code> algorithm (Veicer, 1976).  
</p>
<p>Parallel analysis for factors is actually harder than it seems, for the question is what are the appropriate communalities to use.  If communalities are estimated by the Squared Multiple Correlation (SMC) <code><a href="#topic+smc">smc</a></code>, then the eigen values of the original data will reflect major as well as minor factors (see <code><a href="#topic+sim.minor">sim.minor</a></code> to simulate such data).  Random data will not, of course, have any structure and thus the number of factors will tend to be biased upwards by the presence of the minor factors.  
</p>
<p>By default, fa.parallel estimates the communalities based upon a one factor minres solution.  Although this will underestimate the communalities, it does seem to lead to better solutions on simulated or real (e.g., the <code><a href="psychTools.html#topic+bfi">bfi</a></code> or Harman74) data sets.  
</p>
<p>For comparability with other algorithms (e.g, the paran function in the paran package), setting smc=TRUE will use smcs as estimates of communalities. This will tend towards identifying more factors than the default option.
</p>
<p>Yet another option (suggested by Florian Scharf) is to estimate the eigen values based upon a particular factor model (e.g., specify nfactors &gt; 1).   
</p>
<p>Printing the results will show the eigen values of the original data that are greater than simulated values.
</p>
<p>A sad observation about parallel analysis is that it is sensitive to sample size.  That is, for large data sets, the eigen values of random data are very close to 1.  This will lead to different estimates of the number of factors as a function of sample size.  Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model).  For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated.  This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.
</p>
<p>Although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted!
</p>
<p>When simulating dichotomous data in fa.parallel.poly, the simulated data have the same difficulties as the original data.  This functionally means that the simulated and the resampled results will be very similar.  Note that fa.parallel.poly has functionally been replaced with fa.parallel with the cor=&quot;poly&quot; option.
</p>
<p>As with many psych functions, fa.parallel has been changed to allow for multicore processing.  For running a large number of iterations, it is obviously faster to increase the number of cores to the maximum possible (using the options(&quot;mc.cores=n) command where n is determined from detectCores(). 
</p>


<h3>Value</h3>

<p>A plot of the eigen values for the original data, ntrials of resampling of the  original data, and of a equivalent size matrix of random normal deviates.  If the data are a correlation matrix, specify the number of observations.  
</p>
<p>Also returned (invisibly) are:
</p>
<table>
<tr><td><code>fa.values</code></td>
<td>
<p>The eigen values of the factor model for the real data.</p>
</td></tr>
<tr><td><code>fa.sim</code></td>
<td>
<p>The descriptive statistics of the simulated factor models.</p>
</td></tr>
<tr><td><code>pc.values</code></td>
<td>
<p>The eigen values of a principal components of the real data.</p>
</td></tr>
<tr><td><code>pc.sim</code></td>
<td>
<p>The descriptive statistics of the simulated principal components analysis.</p>
</td></tr>
<tr><td><code>nfact</code></td>
<td>
<p>Number of factors with eigen values &gt; eigen values of random data</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>Number of components with eigen values &gt; eigen values of random data</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>The simulated values for all simulated trials</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Although by default the test is applied to the 95th percentile eigen values, this can be modified by setting the quant parameter to any particular quantile. The actual simulated data are also returned (invisibly) in the value object.  Thus, it is possible to do descriptive statistics on those to choose a preferred comparison.  See the last example (not run)
The simulated and resampled data tend to be very similar, so for a slightly cleaner figure, set sim=FALSE.  
</p>
<p>For relatively small samples with dichotomous data and  cor=&quot;tet&quot; if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)    )
</p>


<h3>Note</h3>

<p>Gagan Atreya reports a problem with the multi-core implementation of fa.parallel when running Microsoft Open R.  This can be resolved by setMKLthreads(1) to set the number of threads to 1.  </p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Del Giudice, M. (2022, October 21). The Square-Root Scree Plot: A Simple Improvement to a Classic Display. doi: 10.31234/osf.io/axubd
</p>
<p>Floyd, Frank J.  and  Widaman, Keith. F (1995)  Factor analysis in the development and refinement of clinical assessment instruments. Psychological Assessment, 7(3):286-299, 1995.
</p>
<p>Horn, John (1965) A rationale and test for the number of factors in factor analysis. Psychometrika, 30, 179-185.
</p>
<p>Humphreys, Lloyd G. and Montanelli, Richard G. (1975), An investigation of the parallel analysis criterion for determining the number of common factors.
Multivariate Behavioral Research, 10, 193-205.
</p>
<p>Revelle, William and Rocklin, Tom (1979) Very simple structure - alternative procedure for estimating the optimal number of interpretable factors. Multivariate Behavioral Research, 14(4):403-414.
</p>
<p>Velicer, Wayne. (1976) Determining the number of components from the matrix of partial correlations. Psychometrika, 41(3):321-327, 1976.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+nfactors">nfactors</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+VSS.plot">VSS.plot</a></code>,   <code><a href="#topic+VSS.parallel">VSS.parallel</a></code>, <code><a href="#topic+sim.minor">sim.minor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
#test.data &lt;- Harman74.cor$cov   #The 24 variable Holzinger - Harman problem
#fa.parallel(test.data,n.obs=145)
fa.parallel(Thurstone,n.obs=213)   #the 9 variable Thurstone problem

#set.seed(123)
#minor &lt;- sim.minor(24,4,400) #4 large and 12 minor factors
#ffa.parallel(minor$observed) #shows 5 factors and 4 components -- compare with 
#fa.parallel(minor$observed,SMC=FALSE) #which shows 6  and 4 components factors
#a demonstration of parallel analysis of a dichotomous variable
#fp &lt;- fa.parallel(psychTools::ability)    #use the default Pearson correlation
#fpt &lt;- fa.parallel(psychTools::ability,cor="tet")  #do a tetrachoric correlation
#fpt &lt;- fa.parallel(psychTools::ability,cor="tet",quant=.95)  #do a tetrachoric correlation and 
#use the 95th percentile of the simulated results
#apply(fp$values,2,function(x) quantile(x,.95))  #look at the 95th percentile of values
#apply(fpt$values,2,function(x) quantile(x,.95))  #look at the 95th percentile of values
#describe(fpt$values)  #look at all the statistics of the simulated values
#paSelect(bfi.keys,bfi)#do 5 different runs, once for each subscale
#paSelect(ability.keys,ability,cor="poly",fm="minrank")

</code></pre>

<hr>
<h2 id='fa.poly'>Deprecated Exploratory Factor analysis functions.  Please use fa</h2><span id='topic+factor.pa'></span><span id='topic+factor.minres'></span><span id='topic+factor.wls'></span><span id='topic+fa.poly'></span>

<h3>Description</h3>

<p>After 6  years, it is time to stop using these deprecated functions!  Please see <code><a href="#topic+fa">fa</a></code> which includes all of the functionality of these older functions.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
fa.poly(x,nfactors=1,n.obs = NA, n.iter=1, rotate="oblimin", SMC=TRUE,  missing=FALSE,
 impute="median", min.err = .001, max.iter=50, symmetric=TRUE, warnings=TRUE,
 fm="minres",alpha=.1, p =.05,scores="regression", oblique.scores=TRUE,
        weight=NULL,global=TRUE,...)  #deprecated
        
factor.minres(r, nfactors=1, residuals = FALSE, rotate = "varimax",n.obs = NA,
scores = FALSE,SMC=TRUE, missing=FALSE,impute="median",min.err = 0.001, digits = 2,
 max.iter = 50,symmetric=TRUE,warnings=TRUE,fm="minres")    #deprecated

factor.wls(r,nfactors=1,residuals=FALSE,rotate="varimax",n.obs = NA,
scores=FALSE,SMC=TRUE,missing=FALSE,impute="median", min.err = .001,
 digits=2,max.iter=50,symmetric=TRUE,warnings=TRUE,fm="wls")  #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.poly_+3A_r">r</code></td>
<td>
<p>deprecated.</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_x">x</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_nfactors">nfactors</code></td>
<td>
<p> deprecated </p>
</td></tr>
<tr><td><code id="fa.poly_+3A_n.obs">n.obs</code></td>
<td>
<p>deprecated </p>
</td></tr>
<tr><td><code id="fa.poly_+3A_rotate">rotate</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_n.iter">n.iter</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_residuals">residuals</code></td>
<td>
<p>deprecated </p>
</td></tr>
<tr><td><code id="fa.poly_+3A_scores">scores</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_smc">SMC</code></td>
<td>
<p>deprecated </p>
</td></tr>
<tr><td><code id="fa.poly_+3A_missing">missing</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_impute">impute</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_max.iter">max.iter</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_symmetric">symmetric</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_warnings">warnings</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_fm">fm</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_alpha">alpha</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_p">p</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_oblique.scores">oblique.scores</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_weight">weight</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_global">global</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_digits">digits</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_min.err">min.err</code></td>
<td>
<p>deprecated</p>
</td></tr>
<tr><td><code id="fa.poly_+3A_...">...</code></td>
<td>
<p>deprecated</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please see the writeup for <code><a href="#topic+fa">fa</a></code> for all of the functionality in these older functions.
</p>


<h3>Value</h3>

<p>Pleases see the writeup for <code><a href="#topic+fa">fa</a></code>
</p>


<h3>Note</h3>

<p>These functions have been deprecated for 8 years.  Don't use them.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#none, you should see fa
#using the Harman 24 mental tests, compare a principal factor with a principal components solution

</code></pre>

<hr>
<h2 id='fa.random'>A first approximation to Random Effects Exploratory Factor Analysis</h2><span id='topic+fa.random'></span>

<h3>Description</h3>

<p>Inspired, in part, by the wprifm function in the profileR package, fa.random removes between subject differences in mean level and then does a normal exploratory factor analysis of the ipsatized data.  Functionally, this removes a general factor of the data before factoring. To prevent non-positive definiteness of the residual data matrix, a very small amount of random noise is added to each variable. This is just a call to fa after removing the between subjects effect. Read the help file for <code><a href="#topic+fa">fa</a></code> for a detailed explanation of all of the input parameters and the output objects. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.random(data, nfactors = 1, fix = TRUE, n.obs = NA, n.iter = 1, rotate = "oblimin",
 scores = "regression", residuals = FALSE, SMC = TRUE, covar = FALSE, missing = FALSE,  
 impute = "median", min.err = 0.001, max.iter = 50, symmetric = TRUE, warnings = TRUE,
  fm = "minres", alpha = 0.1, p = 0.05, oblique.scores = FALSE, np.obs = NULL, 
  use = "pairwise", cor = "cor", weight = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.random_+3A_data">data</code></td>
<td>
<p>A raw data matrix (or data.frame)</p>
</td></tr>
<tr><td><code id="fa.random_+3A_nfactors">nfactors</code></td>
<td>
<p> Number of factors to extract, default is 1 </p>
</td></tr>
<tr><td><code id="fa.random_+3A_fix">fix</code></td>
<td>
<p>If TRUE, then a small amount of random error is added to each observed variable to keep the matrix positive semi-definite.  If FALSE, then this is not done but because the matrix is non-positive semi-definite it will need to be smoothed when finding the scores and the various statistics.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals. Ignored.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_np.obs">np.obs</code></td>
<td>
<p>The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;,  &quot;bentlerT&quot;, &quot;equamax&quot;, &quot;varimin&quot;, &quot;geominT&quot; and &quot;bifactor&quot; are orthogonal rotations.  &quot;Promax&quot;, &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, &quot;bentlerQ,  &quot;geominQ&quot; and &quot;biquartimin&quot; and &quot;cluster&quot; are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to &quot;promax&quot; which does the normalization before calling Promax in GPArotation.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of bootstrap interations to do in fa or fa.poly</p>
</td></tr>
<tr><td><code id="fa.random_+3A_residuals">residuals</code></td>
<td>
<p>Should the residual matrix be shown </p>
</td></tr>
<tr><td><code id="fa.random_+3A_scores">scores</code></td>
<td>
<p>the default=&quot;regression&quot; finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression (&quot;Thurstone&quot;), correlaton preserving (&quot;tenBerge&quot;) as well as &quot;Anderson&quot; and &quot;Bartlett&quot; using the appropriate algorithms ( <code><a href="#topic+factor.scores">factor.scores</a></code>). Although scores=&quot;tenBerge&quot; is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  </p>
</td></tr>
<tr><td><code id="fa.random_+3A_smc">SMC</code></td>
<td>
<p>Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. </p>
</td></tr>
<tr><td><code id="fa.random_+3A_covar">covar</code></td>
<td>
<p>if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix</p>
</td></tr>
<tr><td><code id="fa.random_+3A_missing">missing</code></td>
<td>
<p>if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean</p>
</td></tr>
<tr><td><code id="fa.random_+3A_impute">impute</code></td>
<td>
<p>&quot;median&quot; or &quot;mean&quot; values are used to replace missing values</p>
</td></tr>
<tr><td><code id="fa.random_+3A_min.err">min.err</code></td>
<td>
<p>Iterate until the change in communalities is less than min.err</p>
</td></tr>
<tr><td><code id="fa.random_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for convergence </p>
</td></tr>
<tr><td><code id="fa.random_+3A_symmetric">symmetric</code></td>
<td>
<p>symmetric=TRUE forces symmetry by just looking at the lower off diagonal values</p>
</td></tr>
<tr><td><code id="fa.random_+3A_warnings">warnings</code></td>
<td>
<p>warnings=TRUE =&gt; warn if number of factors is too many </p>
</td></tr>
<tr><td><code id="fa.random_+3A_fm">fm</code></td>
<td>
<p>Factoring method  fm=&quot;minres&quot; will do a minimum residual as will fm=&quot;uls&quot;.  Both of these use a first derivative.  fm=&quot;ols&quot; differs very slightly from &quot;minres&quot; in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm=&quot;wls&quot; will do a weighted least squares (WLS) solution, fm=&quot;gls&quot; does a generalized weighted least squares (GLS), fm=&quot;pa&quot; will do the principal factor solution, fm=&quot;ml&quot; will do a maximum likelihood factor analysis. fm=&quot;minchi&quot; will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm =&quot;minrank&quot; will do a minimum rank factor analysis. &quot;old.min&quot; will do minimal residual the way it was done prior to April, 2017 (see discussion below).</p>
</td></tr>
<tr><td><code id="fa.random_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence intervals for RMSEA</p>
</td></tr>
<tr><td><code id="fa.random_+3A_p">p</code></td>
<td>
<p>if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals</p>
</td></tr>
<tr><td><code id="fa.random_+3A_oblique.scores">oblique.scores</code></td>
<td>
<p>When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  </p>
</td></tr> 
<tr><td><code id="fa.random_+3A_weight">weight</code></td>
<td>
<p>If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_use">use</code></td>
<td>
<p>How to treat missing data, use=&quot;pairwise&quot; is the default&quot;.  See cor for other options.</p>
</td></tr>
<tr><td><code id="fa.random_+3A_cor">cor</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, 
&quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric, &quot;mixed&quot; uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate</p>
</td></tr>
<tr><td><code id="fa.random_+3A_...">...</code></td>
<td>
<p>additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is inspired by the wprifm function in the profileR package and the citation there to a paper by Davison, Kim and Close (2009).  The basic logic is to extract a means vector from each subject and then to analyze the resulting ipsatized data matrix.  This can be seen as removing acquiecence in the case of personality items, or the general factor, in the case of ability items.  Factors composed of items that are all keyed the same way (e.g., Neuroticism in the <code><a href="psychTools.html#topic+bfi">bfi</a></code> data set) will be most affected by this technique. 
</p>
<p>The output is identical to the normal <code><a href="#topic+fa">fa</a></code> output with the addition of two objects:  subject and within.r.  The subject object is just the vector of the mean score for each subject on all the items. within.r is just the correlation of each item with those scores.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>subject</code></td>
<td>
<p>A vector of the  mean score on all items for each subject</p>
</td></tr>
<tr><td><code>within.r</code></td>
<td>
<p>The correlation of each item with the subject vector</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>Eigen values of the common factor solution</p>
</td></tr>
<tr><td><code>e.values</code></td>
<td>
<p>Eigen values of the original matrix</p>
</td></tr>
<tr><td><code>communality</code></td>
<td>
<p>Communality estimates for each item.  These are merely the sum of squared factor loadings for that item.</p>
</td></tr>
<tr><td><code>communalities</code></td>
<td>
<p>If using minrank factor analysis, these are the communalities reflecting the total amount of common variance.  They will exceed the communality (above) which is the model estimated common variance. </p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>which rotation was requested?</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>number of observations specified or found</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>An item by factor (pattern) loading matrix of class &ldquo;loadings&quot;  Suitable for use in other programs (e.g., GPA rotation or factor2cluster. To show these by sorted order, use <code><a href="#topic+print.psych">print.psych</a></code> with sort=TRUE</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>Hoffman's index of complexity for each item.  This is just <code class="reqn">\frac{(\Sigma a_i^2)^2}{\Sigma a_i^4}</code> where a_i is the factor loading on the ith factor. From Hofmann (1978), MBR. See also  Pettersson and Turkheimer (2010).</p>
</td></tr>
<tr><td><code>Structure</code></td>
<td>
<p>An item by factor structure matrix of class &ldquo;loadings&quot;. This is just the loadings (pattern) matrix times the factor intercorrelation matrix.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>How well does the factor model reproduce the correlation matrix. This is just <code class="reqn">\frac{\Sigma r_{ij}^2 - \Sigma r^{*2}_{ij} }{\Sigma r_{ij}^2}
</code>  (See <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, and <code><a href="#topic+principal">principal</a></code> for this fit statistic.</p>
</td></tr>
<tr><td><code>fit.off</code></td>
<td>
<p>how well are the off diagonal elements reproduced?</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>Degrees of Freedom for this model. This is the number of observed correlations minus the number of independent parameters.  Let n=Number of items, nf = number of factors then
<br />
<code class="reqn">dof = n * (n-1)/2 - n * nf + nf*(nf-1)/2</code></p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>Value of the function that is minimized by a maximum likelihood procedures.  This is reported for comparison purposes and as a way to estimate chi square goodness of fit.  The objective function is 
<br />
<code class="reqn">f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^{-1} R|) - n.items</code>. When using MLE, this function is minimized.  When using OLS (minres), although we are not minimizing this function directly, we can still calculate it in order to compare the solution to a MLE fit. </p>
</td></tr>
<tr><td><code>STATISTIC</code></td>
<td>
<p>If the number of observations is specified or found, this is a chi square based upon the objective function, f (see above). Using the formula from <code><a href="stats.html#topic+factanal">factanal</a></code>(which seems to be Bartlett's test) :
<br />
<code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> </p>
</td></tr>
<tr><td><code>PVAL</code></td>
<td>
<p>If n.obs &gt; 0, then what is the probability of observing a chisquare this large or larger?</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>If oblique rotations (e.g,m using oblimin from the GPArotation package or promax) are requested, what is the interfactor correlation?</p>
</td></tr>
<tr><td><code>communality.iterations</code></td>
<td>
<p>The history of the communality estimates (For principal axis only.) Probably only useful for teaching what happens in the process of iterative fitting.</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>The matrix of residual correlations after the factor model is applied. To display it conveniently, use the <code><a href="stats.html#topic+residuals">residuals</a></code> command. </p>
</td></tr>
<tr><td><code>chi</code></td>
<td>
<p>When normal theory fails (e.g., in the case of non-positive definite matrices), it useful to examine the empirically derived <code class="reqn">\chi^2</code> based upon the sum of the squared residuals * N.  This will differ slightly from the MLE estimate which is based upon the fitting function rather than the actual residuals.</p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>This is the sum of the squared (off diagonal residuals) divided by the degrees of freedom.  Comparable to an RMSEA which, because it is based upon  <code class="reqn">\chi^2</code>, requires the number of observations to be specified.  The rms is an empirical value while the RMSEA is based upon normal theory and the non-central <code class="reqn">\chi^2</code> distribution. That is to say, if the residuals are particularly non-normal, the rms value and the associated  <code class="reqn">\chi^2</code> and RMSEA can differ substantially. </p>
</td></tr> 
<tr><td><code>crms</code></td>
<td>
<p>rms adjusted for degrees of freedom</p>
</td></tr>
<tr><td><code>RMSEA</code></td>
<td>
<p>The Root Mean Square Error of Approximation is based upon the non-central 
<code class="reqn">\chi^2</code> distribution and the <code class="reqn">\chi^2</code> estimate found from the MLE fitting function.  With normal theory data, this is fine.  But when the residuals are not distributed according to a noncentral <code class="reqn">\chi^2</code>, this can give very strange values.  (And thus the confidence intervals can not be calculated.) The RMSEA is a conventional index of goodness (badness) of fit but it is also useful to examine the actual rms values.  </p>
</td></tr>   
<tr><td><code>TLI</code></td>
<td>
<p>The Tucker Lewis Index of factoring reliability which is also known as the non-normed fit index.  </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Based upon <code class="reqn">\chi^2</code> with the assumption of normal theory and using the <code class="reqn">\chi^2</code> found using the objective function defined above. This is just <code class="reqn">\chi^2 - 2 df</code></p>
</td></tr>
<tr><td><code>eBIC</code></td>
<td>
<p>When normal theory fails (e.g., in the case of non-positive definite matrices), it useful to examine the empirically derived eBIC based upon the empirical <code class="reqn">\chi^2</code> - 2 df. </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001).  Derived from R2 is is the minimum correlation between any two factor estimates = 2R2-1. </p>
</td></tr>
<tr><td><code>r.scores</code></td>
<td>
<p>The correlations of the factor score estimates using the specified model, if they were to be found.  Comparing these correlations with that of the scores themselves will show, if an alternative estimate of factor scores is used (e.g., the tenBerge method), the problem of factor indeterminacy.  For these correlations will not necessarily be the same.  </p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The beta weights to find the factor score estimates.  These are also used by the <code><a href="#topic+predict.psych">predict.psych</a></code> function to find predicted factor scores for new cases.  These weights will depend upon the scoring method requested.  </p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>The factor scores as requested.  Note that these scores reflect the choice of the way scores should be estimated (see scores in the input).  That is, simple regression (&quot;Thurstone&quot;), correlaton preserving (&quot;tenBerge&quot;) as well as &quot;Anderson&quot; and &quot;Bartlett&quot; using the appropriate algorithms (see <code><a href="#topic+factor.scores">factor.scores</a></code>).  The correlation between factor score estimates (r.scores) is based upon using the regression/Thurstone approach.  The actual correlation between scores will reflect the rotation algorithm chosen and may be found by correlating those scores. Although the scores are found by multiplying the standarized data by the weights matrix, this will not result in standard scores if using regression. </p>
</td></tr>
<tr><td><code>valid</code></td>
<td>
<p>The validity coffiecient of course coded (unit weighted) factor score estimates (From Grice, 2001)</p>
</td></tr>
<tr><td><code>score.cor</code></td>
<td>
<p>The correlation matrix of course coded (unit weighted) factor score estimates, if they were to be found, based upon the loadings matrix rather than the weights matrix.  </p>
</td></tr>
<tr><td><code>rot.mat</code></td>
<td>
<p>The rotation matrix as returned from GPArotation.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>An interesting, but not necessarily good, idea. 
To see what this does if there is a general factor, consider the unrotated solutions to the ability data set. In particular, compare the first factor loading with its congruence to the ipsatized solution means vector correlated with the items (the within.r object).  </p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Davison, Mark L. and  Kim, Se-Kang and  Close, Catherine (2009)  Factor Analytic Modeling of Within Person Variation in Score Profiles.  Multivariate Behavioral Research (44(5) 668-687.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fa.ab &lt;- fa(psychTools::ability,4,rotate="none")  #normal factor analysis
fa.ab.ip &lt;- fa.random(psychTools::ability,3,rotate="none") 
fa.congruence(list(fa.ab,fa.ab.ip,fa.ab.ip$within.r))


  </code></pre>

<hr>
<h2 id='fa.sort'>Sort factor analysis or principal components analysis loadings</h2><span id='topic+fa.sort'></span><span id='topic+fa.organize'></span>

<h3>Description</h3>

<p>Although the print.psych function will sort factor analysis loadings, sometimes it is useful to do this outside of the print function. fa.sort takes the output from the fa or principal functions and sorts the loadings for each factor.  Items are located in terms of their greatest loading.  The new order is returned as an element in the fa list. fa.organize allows for the columns or rows to be reorganized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.sort(fa.results,polar=FALSE)
fa.organize(fa.results,o=NULL,i=NULL,cn=NULL,echelon=TRUE,flip=TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fa.sort_+3A_fa.results">fa.results</code></td>
<td>
<p>The output from a factor analysis or principal components analysis using <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+principal">principal</a></code>.  Can also just be a matrix of loadings. </p>
</td></tr>
<tr><td><code id="fa.sort_+3A_polar">polar</code></td>
<td>
<p>Sort by polar coordinates of first two factors (FALSE)</p>
</td></tr>
<tr><td><code id="fa.sort_+3A_o">o</code></td>
<td>
<p>The order in which to order the factors</p>
</td></tr>
<tr><td><code id="fa.sort_+3A_i">i</code></td>
<td>
<p>The order in which to order the items</p>
</td></tr>
<tr><td><code id="fa.sort_+3A_cn">cn</code></td>
<td>
<p>new factor names</p>
</td></tr>
<tr><td><code id="fa.sort_+3A_echelon">echelon</code></td>
<td>
<p>Organize the factors so that they are in echelon form (variable 1 .. n on factor 1, n+1 ...n=k on factor 2, etc.) </p>
</td></tr>
<tr><td><code id="fa.sort_+3A_flip">flip</code></td>
<td>
<p>Flip factor loadings such that the colMean is positive.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The fa.results$loadings are replaced with sorted loadings.
</p>
<p>fa.organize takes a factor analysis or components output and reorganizes the factors in the o order. Items are organized in the i order.  This is useful when comparing alternative factor solutions.
</p>
<p>The flip option works only for the case of matrix input, not for full <code><a href="#topic+fa">fa</a></code> objects. Use the  <code><a href="#topic+reflect">reflect</a></code> function.
</p>


<h3>Value</h3>

<p> A sorted factor analysis, principal components analysis, or omega loadings matrix. 
</p>
<p>These sorted values are used internally by the various diagram functions. 
</p>
<p>The values returned are the same as  <code><a href="#topic+fa">fa</a></code>, except in sorted order.  In addition, the order is returned as an additional element in the fa list.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="#topic+fa">fa</a></code>,<code><a href="#topic+print.psych">print.psych</a></code>, <code><a href="#topic+fa.diagram">fa.diagram</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test.simple &lt;- fa(sim.item(16),2)
fa.sort(test.simple)
fa.organize(test.simple,c(2,1))  #the factors but not the items have been rearranged
</code></pre>

<hr>
<h2 id='faCor'>Correlations between two factor analysis solutions</h2><span id='topic+faCor'></span>

<h3>Description</h3>

<p>Given two factor analysis or pca solutions to a data matrix or correlation, what are the similarities between the two solutions. This may be found by factor correlations as well as factor congruences.  Factor correlations are found by the matrix product of the factor weights and the correlation matrix and are estimates of what the factor score correlations would be.  Factor congruence (aka Tucker or Burt coefficient) is the cosine of the vectors of factor loadings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>faCor(r, nfactors = c(1, 1), fm = c("minres", "minres"), rotate =
 c("oblimin", "oblimin"), scores = c("tenBerge", "tenBerge"), adjust=c(TRUE,TRUE),
     use = "pairwise", cor = "cor", weight = NULL, correct = 0.5,Target=list(NULL,NULL))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="faCor_+3A_r">r</code></td>
<td>
<p>A correlation matrix or a data matrix suitable for factoring</p>
</td></tr>
<tr><td><code id="faCor_+3A_nfactors">nfactors</code></td>
<td>
<p>Number of factors in each solution to extract</p>
</td></tr>
<tr><td><code id="faCor_+3A_fm">fm</code></td>
<td>
<p>Factor method.  The default is 'minres' factoring. To compare with pca solutions, can also be (fm =&quot;pca&quot;)
</p>
</td></tr>
<tr><td><code id="faCor_+3A_rotate">rotate</code></td>
<td>
<p>What type of rotations to apply.  The default for factors is oblimin, for pca is varimax.</p>
</td></tr>
<tr><td><code id="faCor_+3A_scores">scores</code></td>
<td>
<p>What factor scoring algorithm should be used. Defaults to tenBerge for both cases.</p>
</td></tr>
<tr><td><code id="faCor_+3A_adjust">adjust</code></td>
<td>
<p>Should the factor intercorrelations be corrected by the lack of factor deteriminancy (i.e. divide by the square root of the factor R2)</p>
</td></tr>
<tr><td><code id="faCor_+3A_use">use</code></td>
<td>
<p>How to treat missing data.  Use='pairwise&quot; finds pairwise complete correlations.
</p>
</td></tr>
<tr><td><code id="faCor_+3A_cor">cor</code></td>
<td>
<p>What kind of correlation to find.  The default is Pearson.
</p>
</td></tr>
<tr><td><code id="faCor_+3A_weight">weight</code></td>
<td>
<p>Should cases be weighted?  Default, no.</p>
</td></tr>
<tr><td><code id="faCor_+3A_correct">correct</code></td>
<td>
<p>If finding tetrachoric or polychoric correlations, what correction should be applied to empty cells (defaults to .5)</p>
</td></tr>
<tr><td><code id="faCor_+3A_target">Target</code></td>
<td>
<p>If doing target rotations (e.g., TargetQ or TargetT), then the Target must be specified.  If TargetT, this may be a matrix, if TargetQ, this must be a list.  (Strange property of GPARotation.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The factor correlations are found using the approach discussed by Gorsuch (1983) and uses the weights matrices found by <code class="reqn">w=S R^{-1}</code> and <code class="reqn">r = w' R w</code> where S is the structure matrix and is   <code class="reqn">s= F \Phi</code>.  The resulting correlations may be  adjusted for the factor score variances (the diagonal of r) (the default). 
</p>
<p>For factor loading vectors of F1 and F2 the measure of factor congruence, phi, is 
</p>
<p style="text-align: center;"><code class="reqn">
\phi = \frac{\sum F_1 F_2}{\sqrt{\sum(F_1^2)\sum(F_2^2)}} 
.</code>
</p>
<p> and is also found in <code><a href="#topic+factor.congruence">factor.congruence</a></code>.
</p>
<p>For comparisons of factor solutions from 1 to n, use <code><a href="#topic+bassAckward">bassAckward</a></code>. This function just compares two solutions from the same correlation/data matrix.  <code><a href="#topic+factor.congruence">factor.congruence</a></code> can be used to compare any two sets of factor loadings. 
</p>
<p>Note that alternative ways of finding weights (e.g., regression, Bartlett, tenBerge) will produce somewhat different results.  tenBerge produces weights that maintain the factor correlations in the factor scores.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Call</code></td>
<td>
<p>The function call</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The factor intercorrelations</p>
</td></tr>
<tr><td><code>congruence</code></td>
<td>
<p>The Burt/Tucker coefficient of congruence</p>
</td></tr>
<tr><td><code>f1</code></td>
<td>
<p>The first factor analysis</p>
</td></tr>
<tr><td><code>f2</code></td>
<td>
<p>The second factor analysis</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Useful for comparing factor solutions from the same data.  Will not work for different data sets
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Gorsuch, Richard, (1983) Factor Analysis. Lawrence Erlebaum Associates.
</p>
<p>Burt, Cyril (1948) The factorial study of temperamental traits. British Journal of Statistical Psychology, 1(3) 178-203.
</p>
<p>Horn, John L. (1973) On extension analysis and its relation to correlations between variables and factor scores.  Multivariate Behavioral Research, 8, (4), 477-489.
</p>
<p>Lorenzo-Seva, U. and ten Berge, J. M. F. (2006). Tucker's congruence coefficient as a meaningful index of factor similarity. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 2(2):57-64.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+pca">pca</a></code>,  <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+iclust">iclust</a></code>, and <code>{link{bassAckward}</code> for alternative hierarchical solutions.  <code><a href="#topic+fa.extend">fa.extend</a></code> and <code><a href="#topic+fa.extension">fa.extension</a></code> for other uses of factor - item correlations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>faCor(Thurstone,nfactors=c(2,3)) #compare two solutions to the Thurstone problem
faCor(psychTools::bfi[1:25],nfactors=c(5,5),fm=c("minres","pca")) #compare pca and fa solutions
#compare two levels of factor extraction, and then find the correlations of the scores
faCor(psychTools::bfi[1:25],nfactors=c(3,5)) #based upon linear algebra 
f3 &lt;- fa(psychTools::bfi[1:25],3,scores="tenBerge")
f5 &lt;- fa(psychTools::bfi[1:25],5 ,scores="tenBerge")
cor2(f3$scores,f5$scores) #the correlation between the factor score estimates
  
</code></pre>

<hr>
<h2 id='factor.congruence'>Coefficient of factor congruence </h2><span id='topic+factor.congruence'></span><span id='topic+fa.congruence'></span>

<h3>Description</h3>

<p>Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1937,1  1948), this is frequently known as the Tucker index of factor congruence. Cohen's Profile similarity may be found as well. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.congruence(x, y=NULL,digits=2,use=NULL,structure=FALSE)
fa.congruence(x, y=NULL,digits=2,use=NULL,structure=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.congruence_+3A_x">x</code></td>
<td>
<p> A matrix of factor loadings or a list of matrices of factor loadings</p>
</td></tr>
<tr><td><code id="factor.congruence_+3A_y">y</code></td>
<td>
<p> A second matrix of factor loadings (if x is a list, then y may be empty)</p>
</td></tr>
<tr><td><code id="factor.congruence_+3A_digits">digits</code></td>
<td>
<p>Round off to digits</p>
</td></tr>
<tr><td><code id="factor.congruence_+3A_use">use</code></td>
<td>
<p>If NULL, then no loading matrices may contain missing values.  If use=&quot;complete&quot; then variables with any missing loadings are dropped (with a warning)</p>
</td></tr>
<tr><td><code id="factor.congruence_+3A_structure">structure</code></td>
<td>
<p>If TRUE, find the factor congruences based upon the Structure matrix (if available), otherwise based upon the pattern matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Find the coefficient of factor congruence between two sets of factor loadings. 
</p>
<p>Factor congruences are the cosines of pairs of vectors defined by the loadings matrix and based at the origin.  Thus, for loadings that differ only by a scaler (e.g. the size of the eigen value), the factor congruences will be 1.
</p>
<p>For factor loading vectors of F1 and F2 the measure of factor congruence, phi, is 
</p>
<p style="text-align: center;"><code class="reqn">
\phi = \frac{\sum F_1 F_2}{\sqrt{\sum(F_1^2)\sum(F_2^2)}} 
.</code>
</p>

<p>It is an interesting exercise to compare factor congruences with the correlations of factor loadings.  Factor congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of factor loadings are cosines of the vectors based at the mean loading for each factor.   
</p>
<p style="text-align: center;"><code class="reqn">
\phi = \frac{\sum (F_1-a) (F_2 - b)}{\sqrt{\sum((F_1-a)^2)\sum((F_2-b)^2)}} 
.</code>
</p>
<p>.
</p>
<p>For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.
</p>
<p>Input may either be matrices or factor analysis or principal components analyis output (which includes a loadings object), or a mixture of the two.
</p>
<p>To compare more than two solutions, x may be a list of matrices, all of which will be compared.
</p>
<p>Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.
</p>
<p>If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.
</p>
<p><code><a href="#topic+cohen.profile">cohen.profile</a></code> applies the <code><a href="#topic+congruence">congruence</a></code> function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). 
</p>
<p><code><a href="#topic+distance">distance</a></code> finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r &gt;2).
</p>


<h3>Value</h3>

<p>A matrix of factor congruences or distances.
</p>


<h3>Author(s)</h3>

<p><a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
<a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a></p>


<h3>References</h3>

<p>Burt, Cyril (1948) Methods of factor-analysis with and without successive approximation.  British Journal of Educational Psychology, 7 (2) 172-195.
</p>
<p>Burt, Cyril (1948) The factorial study of temperamental traits. British Journal of Statistical Psychology, 1(3) 178-203.
</p>
<p>Cohen, Jacob (1969), rc: A profile similarity coefficient invariant over variable reflection. Psychological Bulletin, 71 (4) 281-284.
</p>
<p>Lorenzo-Seva, U. and ten Berge, J. M. F. (2006). Tucker's congruence coefficient as a meaningful index of factor similarity. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 2(2):57-64.
</p>
<p>Gorsuch, Richard, (1983) Factor Analysis. Lawrence Erlebaum Associates.
</p>
<p>Revelle, W. (In preparation) An Introduction to Psychometric Theory with applications in R (<a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>)
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+fa">fa</a></code>. <code><a href="#topic+faCor">faCor</a></code> will find factor correlations as well as congruences. </p>


<h3>Examples</h3>

<pre><code class='language-R'>#factor congruence of factors and components, both rotated
#fa &lt;- fa(Harman74.cor$cov,4)
#pc &lt;- principal(Harman74.cor$cov,4)
#factor.congruence(fa,pc)
 #    RC1  RC3  RC2  RC4
#MR1 0.98 0.41 0.28 0.32
#MR3 0.35 0.96 0.41 0.31
#MR2 0.23 0.16 0.95 0.28
#MR4 0.28 0.38 0.36 0.98



#factor congruence without rotation
#fa &lt;- fa(Harman74.cor$cov,4,rotate="none")
#pc &lt;- principal(Harman74.cor$cov,4,rotate="none")
#factor.congruence(fa,pc)   #just show the beween method congruences
#     PC1   PC2   PC3   PC4
#MR1 1.00 -0.04 -0.06 -0.01
#MR2 0.15  0.97 -0.01 -0.15
#MR3 0.31  0.05  0.94  0.11
#MR4 0.07  0.21 -0.12  0.96

#factor.congruence(list(fa,pc))  #this shows the within method congruence as well

 #     MR1   MR2  MR3   MR4  PC1   PC2   PC3   PC4
#MR1  1.00  0.11 0.25  0.06 1.00 -0.04 -0.06 -0.01
#MR2  0.11  1.00 0.06  0.07 0.15  0.97 -0.01 -0.15
#MR3  0.25  0.06 1.00  0.01 0.31  0.05  0.94  0.11
#MR4  0.06  0.07 0.01  1.00 0.07  0.21 -0.12  0.96
#PC1  1.00  0.15 0.31  0.07 1.00  0.00  0.00  0.00
#PC2 -0.04  0.97 0.05  0.21 0.00  1.00  0.00  0.00
#PC3 -0.06 -0.01 0.94 -0.12 0.00  0.00  1.00  0.00
#PC4 -0.01 -0.15 0.11  0.96 0.00  0.00  0.00  1.00

#pa &lt;- fa(Harman74.cor$cov,4,fm="pa")
# factor.congruence(fa,pa)
#         PA1  PA3  PA2  PA4
#Factor1 1.00 0.61 0.46 0.55
#Factor2 0.61 1.00 0.50 0.60
#Factor3 0.46 0.50 1.00 0.57
#Factor4 0.56 0.62 0.58 1.00


#compare with 
#round(cor(fa$loading,pc$loading),2)
#      RC1   RC3   RC2   RC4
#MR1  0.99 -0.18 -0.33 -0.34
#MR3 -0.33  0.96 -0.16 -0.43
#MR2 -0.29 -0.46  0.98 -0.21
#MR4 -0.44 -0.30 -0.22  0.98
</code></pre>

<hr>
<h2 id='factor.fit'>   How well does the factor model fit a correlation matrix. Part of the VSS package </h2><span id='topic+factor.fit'></span>

<h3>Description</h3>

<p>The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose: F'F or P'P.  One simple index of fit is the 1 - sum squared residuals/sum squared original correlations. This fit index is used by <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, etc. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.fit(r, f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.fit_+3A_r">r</code></td>
<td>
<p>a correlation matrix </p>
</td></tr>
<tr><td><code id="factor.fit_+3A_f">f</code></td>
<td>
<p>A factor matrix of loadings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are probably as many fit indices as there are psychometricians.  This fit is a plausible estimate of the amount of reduction in a correlation matrix given a factor model.  Note that it is sensitive to the size of the original correlations.  That is, if the residuals are small but the original correlations are small, that is a bad fit. 
</p>
<p>Let </p>
<p style="text-align: center;"><code class="reqn">R* = R - FF'</code>
</p>

<p style="text-align: center;"><code class="reqn">fit = 1 - \frac{ \sum(R*^2)}{\sum(R^2)}</code>
</p>
<p>. 
</p>
<p>The sums are taken for the off diagonal elements.</p>


<h3>Value</h3>

<p>fit
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#compare the fit of 4 to 3 factors for the Harman 24 variables
fa4 &lt;- factanal(x,4,covmat=Harman74.cor$cov)
round(factor.fit(Harman74.cor$cov,fa4$loading),2)
#[1] 0.9
fa3 &lt;- factanal(x,3,covmat=Harman74.cor$cov)
round(factor.fit(Harman74.cor$cov,fa3$loading),2)
#[1] 0.88


## End(Not run)

</code></pre>

<hr>
<h2 id='factor.model'> Find R =  F F' + U2 is the basic factor model </h2><span id='topic+factor.model'></span>

<h3>Description</h3>

<p>The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose.  Find this reproduced matrix.  Used by <code><a href="#topic+factor.fit">factor.fit</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.model(f,Phi=NULL,U2=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.model_+3A_f">f</code></td>
<td>
<p> A matrix of loadings. </p>
</td></tr>
<tr><td><code id="factor.model_+3A_phi">Phi</code></td>
<td>
<p>A matrix of factor correlations</p>
</td></tr>
<tr><td><code id="factor.model_+3A_u2">U2</code></td>
<td>
<p>Should the diagonal be model by ff' (U2 = TRUE) or replaced with 1's (U2 = FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A correlation or covariance matrix.
</p>


<h3>Author(s)</h3>

 
<p><a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
<a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
</p>


<h3>References</h3>

<p>Gorsuch, Richard, (1983) Factor Analysis. Lawrence Erlebaum Associates. 
<br />
Revelle, W. In preparation) An Introduction to Psychometric Theory with applications in R (<a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>,<code><a href="#topic+ICLUST.cluster">ICLUST.cluster</a></code>, <code><a href="#topic+cluster.fit">cluster.fit</a> </code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+omega">omega</a> </code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
f2 &lt;- matrix(c(.9,.8,.7,rep(0,6),.6,.7,.8),ncol=2)
mod &lt;- factor.model(f2)
round(mod,2)
</code></pre>

<hr>
<h2 id='factor.residuals'> R* =  R- F F' </h2><span id='topic+factor.residuals'></span>

<h3>Description</h3>

<p>The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose.  Find the residuals of the original minus the  reproduced matrix.  Used by <code><a href="#topic+factor.fit">factor.fit</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.residuals(r, f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.residuals_+3A_r">r</code></td>
<td>
<p> A correlation matrix </p>
</td></tr>
<tr><td><code id="factor.residuals_+3A_f">f</code></td>
<td>
<p> A factor model matrix or a list of class loadings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic factor equation is <code class="reqn">_nR_n \approx _{n}F_{kk}F_n'+ U^2</code>. Residuals are just  R* = R - F'F. The residuals should be (but in practice probably rarely are) examined to understand the adequacy of the factor analysis.  When doing Factor analysis or Principal Components analysis, one usually continues to extract factors/components until the residuals do not differ from those expected from a random matrix.
</p>


<h3>Value</h3>

<p> rstar is the residual correlation matrix.
</p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle &lt;revelle@northwestern.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>,  <code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>fa2 &lt;- fa(Harman74.cor$cov,2,rotate=TRUE)
 fa2resid &lt;- factor.residuals(Harman74.cor$cov,fa2)
 fa2resid[1:4,1:4] #residuals with two factors extracted
 fa4 &lt;- fa(Harman74.cor$cov,4,rotate=TRUE)
 fa4resid &lt;- factor.residuals(Harman74.cor$cov,fa4)
 fa4resid[1:4,1:4] #residuals with 4 factors extracted

</code></pre>

<hr>
<h2 id='factor.rotate'>&ldquo;Hand&quot; rotate a factor loading matrix </h2><span id='topic+factor.rotate'></span>

<h3>Description</h3>

<p>Given a factor or components matrix, it is sometimes useful to do arbitrary rotations of particular pairs of variables.  This supplements the much more powerful rotation package GPArotation and is meant for specific requirements to do unusual rotations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.rotate(f, angle, col1=1, col2=2,plot=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.rotate_+3A_f">f</code></td>
<td>
<p>original loading matrix or a data frame  (can be output from a factor analysis function</p>
</td></tr>
<tr><td><code id="factor.rotate_+3A_angle">angle</code></td>
<td>
<p> angle (in degrees!) to rotate </p>
</td></tr>
<tr><td><code id="factor.rotate_+3A_col1">col1</code></td>
<td>
<p> column in factor matrix defining the first variable</p>
</td></tr>
<tr><td><code id="factor.rotate_+3A_col2">col2</code></td>
<td>
<p> column in factor matrix defining the second variable </p>
</td></tr>
<tr><td><code id="factor.rotate_+3A_plot">plot</code></td>
<td>
<p>plot the original (unrotated) and rotated factors</p>
</td></tr>
<tr><td><code id="factor.rotate_+3A_...">...</code></td>
<td>
<p>parameters to pass to fa.plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partly meant as a demonstration of how rotation works, factor.rotate is useful for those cases that require specific rotations that are not available in more advanced packages such as GPArotation.  If the plot option is set to TRUE, then the original axes are shown as dashed lines.
</p>
<p>The rotation is in degrees counter clockwise.
</p>


<h3>Value</h3>

<p>the resulting rotated matrix of loadings.
</p>


<h3>Note</h3>

<p>For a complete rotation package, see GPArotation </p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> <br /> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#using the Harman 24 mental tests, rotate the 2nd and 3rd factors 45 degrees
f4&lt;- fa(Harman74.cor$cov,4,rotate="TRUE")
f4r45 &lt;- factor.rotate(f4,45,2,3)
f4r90 &lt;- factor.rotate(f4r45,45,2,3)
print(factor.congruence(f4,f4r45),digits=3) #poor congruence with original
print(factor.congruence(f4,f4r90),digits=3) #factor 2 and 3 have been exchanged and 3 flipped

#a graphic example
data(Harman23.cor)
f2 &lt;- fa(Harman23.cor$cov,2,rotate="none")
op &lt;- par(mfrow=c(1,2))
cluster.plot(f2,xlim=c(-1,1),ylim=c(-1,1),title="Unrotated ")
f2r &lt;- factor.rotate(f2,-33,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1),title="rotated -33 degrees")
op &lt;- par(mfrow=c(1,1))

</code></pre>

<hr>
<h2 id='factor.scores'>Various ways to estimate factor scores for the factor analysis model</h2><span id='topic+factor.scores'></span>

<h3>Description</h3>

<p>A fundamental problem with factor analysis is that although the model is defined at the structural level, it is indeterminate at the data level. This problem of factor indeterminancy leads to alternative ways of estimating factor scores, none of which is ideal.  Following Grice (2001) four different methods are available here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.scores(x, f, Phi = NULL, method = c("Thurstone", "tenBerge", "Anderson", 
       "Bartlett", "Harman","components"),rho=NULL,missing=FALSE,impute="none")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.scores_+3A_x">x</code></td>
<td>
<p>Either a matrix of data if scores are to be found, or a correlation matrix if just the factor weights are to be found.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_f">f</code></td>
<td>
<p>The output from the <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+irt.fa">irt.fa</a></code> functions, or a factor loading matrix.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_phi">Phi</code></td>
<td>
<p>If a pattern matrix is provided, then what were the factor intercorrelations.  Does not need to be specified if f is the output from the  <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+irt.fa">irt.fa</a></code> functions.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_method">method</code></td>
<td>
<p>Which of four factor score estimation procedures should be used. Defaults to &quot;Thurstone&quot; or regression based weights.  See details below for the other four methods.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_rho">rho</code></td>
<td>
<p>If x is a set of data and rho is specified, then find scores based upon the fa results and the correlations reported in rho.  Used when scoring fa.poly results.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_missing">missing</code></td>
<td>
<p>If missing is TRUE, missing items  are imputed using either the median or mean.  If missing is FALSE, the default, scores are found based upon the mean of the available items for each subject.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_impute">impute</code></td>
<td>
<p>By default, only complete cases are scored.  But, missing data can be imputed using &quot;median&quot; or &quot;mean&quot;.  The number of missing by subject is reported.  If impute = &quot;none&quot;, missing data are not scored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although the factor analysis model is defined at the structural level, it is undefined at the data level.  This is a well known but little discussed problem with factor analysis.  
</p>
<p>Factor scores represent estimates of common part of the variables and should not be thought of as identical to the factors themselves. If a factor is thought of as a chop stick stuck into the center of an ice cream cone and factor score estimates are represented by straws anywhere along the edge of the cone the problem of factor indeterminacy becomes clear, for depending on the shape of the cone, two straws can be negatively correlated with each other. (The imagery is taken from Niels Waller, adapted from Stanley Mulaik). In a very clear discussion of the problem of factor score indeterminacy, Grice (2001) reviews several alternative ways of estimating factor scores and considers weighting schemes that will produce uncorrelated factor score estimates as well as the effect of using coarse coded (unit weighted) factor weights.
</p>
<p><code><a href="#topic+factor.scores">factor.scores</a></code> uses four different ways of estimate factor scores.  In all cases, the factor score estimates are based upon the data matrix, X, times a weighting matrix, W, which weights the observed variables.
</p>
<p>For polytomous or dichotmous data, factor scores can be estimated using Item Response Theory techniques (e.g., using <code>link{irt.fa}</code> and then <code>link{scoreIrt}</code>.  Such scores are still just factor score estimates, for the IRT model is a latent variable model equivalent to factor analysis.  
</p>

<ul>
<li><p> method=&quot;Thurstone&quot; finds the regression based weights: <code class="reqn">W = R^{-1} F</code> where R is the correlation matrix and F is the factor loading matrix. 
</p>
</li>
<li><p> method=&quot;tenBerge&quot; finds weights such that the correlation between factors for an oblique solution is preserved. Note that  formula 8 in Grice has a typo in the formula for C and should be:
<code class="reqn">L = F \Phi^(1/2) </code>
<code class="reqn">C = R^(-1/2) L (L' R^(-1) L)^(-1/2) </code>
<code class="reqn">W = R ^(-1/2) C \Phi^(1/2) </code>
</p>
</li>
<li><p> method=&quot;Anderson&quot; finds weights such that the factor scores will be uncorrelated: <code class="reqn">W = U^{-2}F (F' U^{-2} R  U^{-2} F)^{-1/2}</code> where U is the diagonal matrix of uniquenesses. The Anderson method works for orthogonal factors only, while the tenBerge method works for orthogonal or oblique solutions.
</p>
</li>
<li><p> method = &quot;Bartlett&quot;  finds weights given <code class="reqn">W = U^{-2}F (F' U^{-2}F)^{-1}</code>
</p>
</li>
<li><p> method=&quot;Harman&quot; finds weights based upon socalled &quot;idealized&quot; variables: <code class="reqn">W =  F (t(F) F)^{-1}</code>.
</p>
</li>
<li><p> method=&quot;components&quot; uses weights that are just component loadings.  
</p>
</li></ul>



<h3>Value</h3>


<ul>
<li><p> scores (the factor scores if the raw data is given)
</p>
</li>
<li><p> weights (the factor weights) 
</p>
</li>
<li><p> r.scores  (The correlations of the factor score estimates.)
</p>
</li>
<li><p> missing   A vector of the number of missing observations per subject
</p>
</li></ul>



<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Grice, James W.,2001, Computing and evaluating factor scores,  Psychological Methods, 6,4, 430-450. (note the typo in equation 8)
</p>
<p>ten Berge, Jos M.F.,  Wim P. Krijnen, Tom Wansbeek and Alexander Shapiro (1999) Some new results on correlation-preserving factor scores prediction methods. Linear Algebra and its Applications, 289, 311-318.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+factor.stats">factor.stats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f3 &lt;- fa(Thurstone,3 )
f3$weights  #just the scoring weights
f5 &lt;- fa(psychTools::bfi,5) #this does the factor analyis
my.scores &lt;- factor.scores(psychTools::bfi,f5, method="tenBerge")
#compare the tenBerge factor score correlation to the factor correlations
cor(my.scores$scores,use="pairwise") - f5$Phi  #compare to the f5$Phi values
#compare the default (regression) score correlations to the factor correlations
cor(f5$scores,use="pairwise")  - f5$Phi
#compare to the f5 solution

</code></pre>

<hr>
<h2 id='factor.stats'>Find various goodness of fit statistics for factor analysis and principal components </h2><span id='topic+factor.stats'></span><span id='topic+fa.stats'></span>

<h3>Description</h3>

<p>Chi square and other goodness of fit statistics are found based upon the fit of a factor or components model to a correlation matrix.  Although these statistics are normally associated with a maximum likelihood solution, they can be found for minimal residual (OLS), principal axis, or principal component solutions as well.  Primarily called from within these functions, factor.stats can be used by itself. Measures of factorial adequacy and validity follow the paper by Grice, 2001.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fa.stats(r=NULL,f,phi=NULL,n.obs=NA,np.obs=NULL,alpha=.05,fm=NULL,
     smooth=TRUE, coarse=TRUE) 
factor.stats(r=NULL,f,phi=NULL,n.obs=NA,np.obs=NULL,alpha=.1,fm=NULL,
      smooth=TRUE, coarse=TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.stats_+3A_r">r</code></td>
<td>
<p>A correlation matrix or a data frame of raw data</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_f">f</code></td>
<td>
<p>A factor analysis loadings matrix or the output from a factor or principal components analysis.  In which case the r matrix need not be specified.</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_phi">phi</code></td>
<td>
<p>A factor intercorrelation matrix if the factor solution was oblique.</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_n.obs">n.obs</code></td>
<td>
<p>The number of observations for the correlation matrix.  If not specified, and a correlation matrix is used, chi square will not be reported. Not needed if the input is a data matrix.</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_np.obs">np.obs</code></td>
<td>
<p>The pairwise number of subjects for each pair in the correlation matrix.  This is used for finding observed chi square.</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_alpha">alpha</code></td>
<td>
<p>alpha level of confidence intervals for RMSEA (twice the confidence at each tail)</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_fm">fm</code></td>
<td>
<p>flag if components are being given statistics</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_smooth">smooth</code></td>
<td>
<p>Should the corelation matrix be smoothed before finding the stats</p>
</td></tr>
<tr><td><code id="factor.stats_+3A_coarse">coarse</code></td>
<td>
<p>By default, find the coarse coded statistics.</p>
</td></tr> </table>


<h3>Details</h3>

<p>Combines the goodness of fit tests used in <code><a href="#topic+fa">fa</a></code> and principal into one function.  If the matrix is singular, will smooth the correlation matrix before finding the fit functions. Now will find the RMSEA (root mean square error of approximation) and the alpha confidence intervals similar to a SEM function.  Also reports the root mean square residual.
</p>
<p>Chi square is found two ways.  The first (STATISTIC) applies the goodness of fit test from Maximum Likelihood objective function (see below).  This assumes multivariate normality.  The second is the empirical chi square based upon the observed residual correlation matrix and the observed sample size for each correlation.  This is found by summing the squared residual correlations time the sample size.  
</p>


<h3>Value</h3>

 
<table>
<tr><td><code>fit</code></td>
<td>
<p>How well does the factor model reproduce the correlation matrix. (See <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, and <code><a href="#topic+principal">principal</a></code> for this fit statistic.</p>
</td></tr>
<tr><td><code>fit.off</code></td>
<td>
<p>how well are the off diagonal elements reproduced?  This is just 1 - the  relative magnitude of the squared off diagonal residuals to the squared off diagonal original values.</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>Degrees of Freedom for this model. This is the number of observed correlations minus the number of independent parameters.  Let n=Number of items, nf = number of factors then
<br />
<code class="reqn">dof = n * (n-1)/2 - n * nf + nf*(nf-1)/2</code></p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>value of the function that is minimized by maximum likelihood procedures.  This is reported for comparison purposes and as a way to estimate chi square goodness of fit.  The objective function is 
<br />
<code class="reqn">f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^{-1} R|) - n.items</code>. </p>
</td></tr>
<tr><td><code>STATISTIC</code></td>
<td>
<p>If the number of observations is specified or found, this is a chi square based upon the objective function, f.  Using the formula from <code><a href="stats.html#topic+factanal">factanal</a></code>(which seems to be Bartlett's test) :
<br />
<code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> 
</p>
<p>Note that this is different from the chi square reported by the sem package which seems to use 
<code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> 
</p>
</td></tr>
<tr><td><code>PVAL</code></td>
<td>
<p>If n.obs &gt; 0, then what is the probability of observing a chisquare this large or larger?</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>If oblique rotations (using oblimin from the GPArotation package or promax) are requested, what is the interfactor correlation.</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001)</p>
</td></tr>
<tr><td><code>r.scores</code></td>
<td>
<p>The correlations of the factor score estimates, if they were to be found.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The beta weights to find the factor score estimates</p>
</td></tr>
<tr><td><code>valid</code></td>
<td>
<p>The validity coffiecient of coarse coded (unit weighted) factor score estimates (From Grice, 2001)</p>
</td></tr>
<tr><td><code>score.cor</code></td>
<td>
<p>The correlation matrix of coarse coded (unit weighted) factor score estimates, if they were to be found, based upon the loadings matrix.  Note that these are not the same as the correlation of the factor score estimates r.scores</p>
</td></tr>
<tr><td><code>RMSEA</code></td>
<td>
<p>The Root Mean Square Error of Approximation and the alpha confidence intervals. Based upon the chi square non-centrality parameter.
This is found as <code class="reqn">\sqrt{f/dof - 1(/-1)}</code> </p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>The empirically found square root of the squared residuals.  This does not require sample size to be specified nor does it make assumptions about normality.</p>
</td></tr>
<tr><td><code>crms</code></td>
<td>
<p>While the rms uses the number of correlations to find the average, the crms uses the number of degrees of freedom.  Thus, there is a penalty for having too complex a model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The problem of factor and factor score estimates leads to multiple different estimates of the correlations between the factors.  Phi is the factor intercorrelation matrix from the rotations, r.scores is the correlation of the factor score estimates (if they were to be found from the data), score.cor is the correlation of the coarse coded factor score estimates, (if they were to be found). and of course the correlation of the factor score estimates  themselves.  By default, the first three of these are found.  </p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Grice, James W.,2001, Computing and evaluating factor scores,  Psychological Methods, 6,4, 430-450.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code> with fm=&quot;pa&quot; for principal axis factor analysis, <code><a href="#topic+fa">fa</a></code> with fm=&quot;minres&quot; for minimum residual factor analysis (default).  <code><a href="#topic+factor.pa">factor.pa</a></code> also does principal axis factor analysis, but is deprecated, as is <code><a href="#topic+factor.minres">factor.minres</a></code> for minimum residual factor analysis. See <code><a href="#topic+principal">principal</a></code> for principal components.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>v9 &lt;- sim.hierarchical()
f3 &lt;- fa(v9,3)
factor.stats(v9,f3,n.obs=500)
f3o &lt;- fa(v9,3,fm="pa",rotate="Promax")
factor.stats(v9,f3o,n.obs=500)


</code></pre>

<hr>
<h2 id='factor2cluster'> Extract cluster definitions from factor loadings </h2><span id='topic+factor2cluster'></span>

<h3>Description</h3>

<p>Given a factor or principal components loading matrix, assign each item to a cluster corresponding to the largest (signed) factor loading for that item.  Essentially, this is a Very Simple Structure approach to cluster definition that corresponds to what most people actually do: highlight the largest loading for each item and ignore the rest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor2cluster(loads, cut = 0,aslist=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor2cluster_+3A_loads">loads</code></td>
<td>
<p>either a matrix of loadings, or the result of a factor analysis/principal components analyis with a loading component </p>
</td></tr>
<tr><td><code id="factor2cluster_+3A_cut">cut</code></td>
<td>
<p>Extract items with absolute loadings &gt; cut</p>
</td></tr>
<tr><td><code id="factor2cluster_+3A_aslist">aslist</code></td>
<td>
<p>if TRUE, Return a keys list, else return a keys matrix (old style) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A factor/principal components analysis loading matrix is converted to a cluster (-1,0,1) definition matrix where each item is assigned to one and only one cluster.  This is a fast way to extract items that will be unit weighted to form cluster composites.  Use this function in combination with cluster.cor to find the corrleations of these composite scores. 
</p>
<p>A typical use in the SAPA project is to form item composites by clustering or factoring (see <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+principal">principal</a></code>), extract the clusters from these results (<code><a href="#topic+factor2cluster">factor2cluster</a></code>), and then form the composite correlation matrix using <code><a href="#topic+cluster.cor">cluster.cor</a></code>.  The variables in this reduced matrix may then be used in multiple R procedures using mat.regress.
</p>
<p>The input may be a matrix of item loadings, or the output from a factor analysis which includes a loadings matrix.
</p>


<h3>Value</h3>

<p>a keys list (new style or a matrix of -1,0,1 cluster definitions for each item.
</p>


<h3>Author(s)</h3>

 
<p><a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
</p>
<p>Maintainer: William Revelle  <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.vss.html">https://personality-project.org/r/r.vss.html</a>  </p>


<h3>See Also</h3>

<p><code><a href="#topic+cluster.cor">cluster.cor</a></code>, <code><a href="#topic+factor2cluster">factor2cluster</a></code>, <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>
<code><a href="#topic+make.keys">make.keys</a></code>, <code><a href="#topic+keys2list">keys2list</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#matches the factanal example
f4  &lt;- fa(Harman74.cor$cov,4,rotate="varimax")
factor2cluster(f4) </code></pre>

<hr>
<h2 id='faRotations'>Multiple rotations of factor loadings to find local minima</h2><span id='topic+faRotations'></span>

<h3>Description</h3>

<p>A dirty little secret of factor rotation algorithms is the problem of local minima (Nguyen and Waller,2022).  Following ideas in that article, we allow for multiple random restarts and then return the global optimal solution.  Used as part of the <code><a href="#topic+fa">fa</a></code> function or available as a stand alone function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>faRotations(loadings, r = NULL, rotate = "oblimin", hyper = 0.15, n.rotations = 10,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="faRotations_+3A_loadings">loadings</code></td>
<td>
<p>Factor loadings matrix from <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+pca">pca</a></code> or any N x k loadings matrix</p>
</td></tr>
<tr><td><code id="faRotations_+3A_r">r</code></td>
<td>
<p>The correlation matrix used to find the factors.  (Used to find the factor indeterminancy of the solution)</p>
</td></tr>
<tr><td><code id="faRotations_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;,  &quot;bentlerT&quot;, &quot;equamax&quot;, &quot;varimin&quot;, &quot;geominT&quot; and &quot;bifactor&quot; are orthogonal rotations.  &quot;Promax&quot;, &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, &quot;bentlerQ,  &quot;geominQ&quot; and &quot;biquartimin&quot; and &quot;cluster&quot; are possible oblique transformations of the solution.  Defaults to oblimin. 
</p>
</td></tr>
<tr><td><code id="faRotations_+3A_hyper">hyper</code></td>
<td>
<p>The value defining when a loading is in the &ldquo;hyperplane&quot;.</p>
</td></tr>
<tr><td><code id="faRotations_+3A_n.rotations">n.rotations</code></td>
<td>
<p>The number of random restarts to use.</p>
</td></tr>
<tr><td><code id="faRotations_+3A_...">...</code></td>
<td>
<p>additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Nguyen and Waller review the problem of local minima in factor analysis.  This is a problem for all rotation algorithms, but is more so for some.  <code><a href="#topic+faRotate">faRotate</a></code> generates n.rotations different starting values and then applies the specified rotation to the original loadings using multiple start values.  Hyperplane counts and complexity indices are reported for each starting matrix, and the one with the highest hyoerplane count and the lowest complexity is returned.
</p>


<h3>Value</h3>

<table>
<tr><td><code>loadings</code></td>
<td>
<p>The best rotated solution</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>Factor correlations</p>
</td></tr>
<tr><td><code>rotation.stats</code></td>
<td>
<p>Hyperplane count, complexity.</p>
</td></tr>
<tr><td><code>rot.mat</code></td>
<td>
<p>The rotation matrix used.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Adapted from the fungible package by Waller
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Nguyen, H. V., &amp; Waller, N. G. (2022, January 6). Local Minima and Factor Rotations in Exploratory Factor Analysis. Psychological Methods. Advance online publication. doi 10.1037/met0000467
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f5 &lt;- fa(psychTools::bfi[,1:25],5,rotate="none")
faRotations(f5,n.rotations=10)   #note that the factor analysis needs to not do the rotation
faRotations(f5$loadings)  #matrix input
geo &lt;- faRotations(f5,rotate="geominQ",n.rotation=10)
 # a popular alternative, but more sensitive to local minima
describe(geo$rotation.stats[,1:3]) 
 </code></pre>

<hr>
<h2 id='fisherz'>Transformations of r, d, and t  including Fisher r to z and z to r and confidence intervals</h2><span id='topic+fisherz'></span><span id='topic+fisherz2r'></span><span id='topic+r.con'></span><span id='topic+r2c'></span><span id='topic+r2t'></span><span id='topic+t2r'></span><span id='topic+g2r'></span><span id='topic+chi2r'></span><span id='topic+r2chi'></span><span id='topic+cor2cov'></span>

<h3>Description</h3>

<p>Convert a correlation to a z or t, or d, or chi or covariance matrix
or z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code><a href="#topic+cohen.d">cohen.d</a></code> for other conversions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fisherz(rho)
fisherz2r(z)
r.con(rho,n,p=.95,twotailed=TRUE)
r2t(rho,n)
t2r(t,df)
g2r(g,df,n)
chi2r(chi2,n)
r2chi(rho,n)
r2c(rho,sigma)
cor2cov(rho,sigma)




</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fisherz_+3A_rho">rho</code></td>
<td>
<p> a Pearson r </p>
</td></tr>
<tr><td><code id="fisherz_+3A_z">z</code></td>
<td>
<p>A Fisher z</p>
</td></tr>
<tr><td><code id="fisherz_+3A_n">n</code></td>
<td>
<p>Sample size for confidence intervals</p>
</td></tr>
<tr><td><code id="fisherz_+3A_df">df</code></td>
<td>
<p>degrees of freedom for t, or g</p>
</td></tr>
<tr><td><code id="fisherz_+3A_p">p</code></td>
<td>
<p>Confidence interval</p>
</td></tr>
<tr><td><code id="fisherz_+3A_twotailed">twotailed</code></td>
<td>
<p>Treat p as twotailed p</p>
</td></tr>
<tr><td><code id="fisherz_+3A_g">g</code></td>
<td>
<p>An effect size (Hedge's g)</p>
</td></tr>
<tr><td><code id="fisherz_+3A_t">t</code></td>
<td>
<p>A student's t value</p>
</td></tr>
<tr><td><code id="fisherz_+3A_chi2">chi2</code></td>
<td>
<p>A chi square</p>
</td></tr>
<tr><td><code id="fisherz_+3A_sigma">sigma</code></td>
<td>
<p>a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix</p>
</td></tr>
</table>


<h3>Value</h3>

 
<table>
<tr><td><code>z</code></td>
<td>
<p> value corresponding to r  (fisherz)</p>
</td></tr> 
<tr><td><code>r</code></td>
<td>
<p>r corresponding to z (fisherz2r)</p>
</td></tr> 
<tr><td><code>r.con</code></td>
<td>
<p>lower and upper p confidence intervals (r.con)</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>t with n-2 df (r2t)</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>r corresponding to effect size d or d corresponding to r.</p>
</td></tr>
<tr><td><code>r2c</code></td>
<td>
<p>r2c  is the reverse of the cor2con function of base R.  It just converts a correlation matrix to the corresponding covariance matrix given a vector of standard deviations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle  <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 n &lt;- 30
 r &lt;- seq(0,.9,.1)
 d &lt;- r2d(r)
 rc &lt;- matrix(r.con(r,n),ncol=2)
 t &lt;- r*sqrt(n-2)/sqrt(1-r^2)
 p &lt;- (1-pt(t,n-2))*2
 r1 &lt;- t2r(t,(n-2))
 r2 &lt;- d2r(d)
 chi &lt;- r2chi(r,n)
 r3 &lt;- chi2r(chi,n)
 r.rc &lt;- data.frame(r=r,z=fisherz(r),lower=rc[,1],upper=rc[,2],t=t,p=p,d=d,
     chi2=chi,d2r=r2,t2r=r1,chi2r=r3)
 round(r.rc,2)


</code></pre>

<hr>
<h2 id='fparse'>
Parse and exten  formula input from a model and return the DV, IV, and associated terms.
</h2><span id='topic+fparse'></span>

<h3>Description</h3>

<p>Formula input from e.g., lm,  may be extended to include mediators,  quadratic and partial terms using a standard syntax. This is use by <code><a href="#topic+lmCor">lmCor</a></code> and <code><a href="#topic+mediate">mediate</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fparse(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fparse_+3A_expr">expr</code></td>
<td>
<p>A legitimate expression in the form y ~ x1 ,  etc. (see details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic formula input given as DV1 + DV2 ~ IV1 + IV2 + (IV3) + I(IV4^2) - IV5 will be parsed to return 2 DVs (1 and 2), two normal IVs (1 and 2), a mediator (IV3) a quadratic (IV4) and a variable to be partialed (IV5). See the various examples in <code><a href="#topic+lmCor">lmCor</a></code> and <code><a href="#topic+mediate">mediate</a></code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y</code></td>
<td>
<p>A list of elements from the left side of the formula</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>A list of elements from the right side of the formula</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>A list of those elements of the formula included in ()</p>
</td></tr>
<tr><td><code>prod</code></td>
<td>
<p>A list of elements separated by a * sign</p>
</td></tr>
<tr><td><code>ex</code></td>
<td>
<p>A list of elements marked by I()</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fparse(DV  ~ IV1 + IV2 * IV2*IV3 + (IV4) + I(IV5^2) )
#somewhat more complicated
fparse(DV1 + DV2 ~ IV1 + IV2 + IV3*IV4 + I(IV5^2) + I(Iv6^2) + (IV7) + (IV8) - IV9)

</code></pre>

<hr>
<h2 id='Garcia'>Data from the sexism (protest) study of Garcia, Schmitt, Branscome, and Ellemers (2010)
</h2><span id='topic+Garcia'></span><span id='topic+protest'></span><span id='topic+GSBE'></span>

<h3>Description</h3>

<p>Garcia, Schmitt, Branscombe, and Ellemers (2010) report data for 129 subjects on the effects of perceived sexism on anger and liking of women's reactions to ingroup members who protest discrimination. This data set is also used as the &lsquo;protest&rsquo; data set by Hayes (2013 and 2018).  It is a useful example of mediation and moderation in regression. It may also be used as an example of plotting interactions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GSBE")</code></pre>


<h3>Format</h3>

<p>A data frame with 129 observations on the following 6 variables.
</p>

<dl>
<dt><code>protest</code></dt><dd><p>0 = no protest, 1 = Individual Protest, 2 = Collective Protest</p>
</dd>
<dt><code>sexism</code></dt><dd><p>Means of an 8 item Modern Sexism Scale.</p>
</dd>
<dt><code>anger</code></dt><dd><p>Anger towards the target of discrimination. &ldquo;I feel angry towards Catherine&quot;.</p>
</dd>
<dt><code>liking</code></dt><dd><p>Mean rating of 6 liking ratings of the target.</p>
</dd>
<dt><code>respappr</code></dt><dd><p>Mean of four items of appropriateness of the target's response.</p>
</dd>
<dt><code>prot2</code></dt><dd><p>A recoding of protest into two levels (to match Hayes, 2013).</p>
</dd>
</dl>



<h3>Details</h3>

<p>The reaction of women to women who protest discriminatory treatment was examined in an experiment reported by Garcia et al. (2010). 129 women  were given a description of sex discrimination in the workplace (a male lawyer was promoted over a clearly more qualified female lawyer).  Subjects then read that the target lawyer felt that the decision was unfair.  Subjects were then randomly assigned to three conditions: Control (no protest), Individual Protest (&ldquo;They are treating me unfairly&quot;) , or Collective Protest (&ldquo;The firm is is treating women unfairly&quot;). 
</p>
<p>Participants were then asked how much they liked the target (liking), how angry they were to the target (anger) and to evaluate the appropriateness of the target's response (respappr).  
</p>
<p>Garcia et al (2010) report a number of interactions (moderation effects) as well as moderated-mediation effects.
</p>
<p>This data set is used as an example in Hayes (2013) for moderated mediation.  It is used here to show how to do moderation (interaction terms) in regression (see <code><a href="#topic+setCor">setCor</a></code>) , how to do moderated mediation (see <code><a href="#topic+mediate">mediate</a></code>) and how draw interaction graphs (see help).
</p>


<h3>Source</h3>

<p>The data were downloaded from the webpages of Andrew Hayes (https://www.afhayes.com/public/hayes2018data.zip)  supporting the first and second edition of his book.  The second edition includes 6 variables, the first, just four.  The protest variable in 2018 has three levels, but just two in the 2013 source. 
</p>
<p>The data are used by kind permission of Donna M. Garcia, Michael T. Schmitt, Nyla R. Branscombe, and Naomi Ellemers.  
</p>


<h3>References</h3>

<p>Garcia, Donna M. and Schmitt, Michael T. and Branscombe, Nyla R. and Ellemers, Naomi (2010). Women's reactions to ingroup members who protest discriminatory treatment: The importance of beliefs about inequality and response appropriateness. European Journal of Social Psychology, (40) 733-745.
</p>
<p>Hayes, Andrew F. (2013)  Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.  Guilford Press. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GSBE)  #alias to Garcia data set

## Just do regressions with interactions
lmCor(respappr ~ prot2 * sexism,std=FALSE,data=Garcia,main="Moderated (mean centered )")
lmCor(respappr ~ prot2 * sexism,std=FALSE,data=Garcia,main="Moderated (don't center)", zero=FALSE)
#demonstrate interaction plots
plot(respappr ~ sexism, pch = 23- protest, bg = c("black","red", "blue")[protest], 
data=Garcia, main = "Response to sexism varies as type of protest")
by(Garcia,Garcia$protest, function(x) abline(lm(respappr ~ sexism,
   data =x),lty=c("solid","dashed","dotted")[x$protest+1])) 
text(6.5,3.5,"No protest")
text(3,3.9,"Individual")
text(3,5.2,"Collective")

 
#compare two models  (bootstrapping n.iter set to 50 for speed
# 1) mean center the variables prior to taking product terms
mod1 &lt;- mediate(respappr ~ prot2 * sexism +(sexism),data=Garcia,n.iter=50
 ,main="Moderated mediation (mean centered)")
# 2) do not mean center
mod2 &lt;- mediate(respappr ~ prot2 * sexism +(sexism),data=Garcia,zero=FALSE, n.iter=50,   
    main="Moderated  mediation (not centered")

summary(mod1)
summary(mod2)


</code></pre>

<hr>
<h2 id='geometric.mean'> Find the geometric mean of a vector or columns of a data.frame. </h2><span id='topic+geometric.mean'></span>

<h3>Description</h3>

<p>The geometric mean is the nth root of n products or e to the mean log of x.
Useful for describing non-normal, i.e., geometric distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geometric.mean(x,na.rm=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geometric.mean_+3A_x">x</code></td>
<td>
<p> a vector or data.frame</p>
</td></tr>
<tr><td><code id="geometric.mean_+3A_na.rm">na.rm</code></td>
<td>
<p>remove NA values before processing</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful for teaching how to write functions, also useful for showing the different ways of estimating central tendency. 
</p>


<h3>Value</h3>

<p>geometric mean(s) of x or x.df.
</p>


<h3>Note</h3>

<p> Not particularly useful if there are elements that are &lt;= 0.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+harmonic.mean">harmonic.mean</a></code>, <code><a href="base.html#topic+mean">mean</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- seq(1,5)
x2 &lt;- x^2
x2[2] &lt;- NA
X &lt;- data.frame(x,x2)
geometric.mean(x)
geometric.mean(x2)
geometric.mean(X)
geometric.mean(X,na.rm=FALSE)

</code></pre>

<hr>
<h2 id='glb.algebraic'>Find the greatest lower bound to reliability.
</h2><span id='topic+glb.algebraic'></span>

<h3>Description</h3>

<p>The greatest lower bound solves the &ldquo;educational testing problem&quot;. That is, what is the reliability of a test? (See <code><a href="#topic+guttman">guttman</a></code> for a discussion of the problem). Although there are many estimates of a test reliability (Guttman, 1945) most underestimate the true reliability of a test.
</p>
<p>For a given covariance matrix of items, C, the function finds the greatest lower bound to reliability of the total score using the csdp function from the Rcsdp package.</p>


<h3>Usage</h3>

<pre><code class='language-R'>glb.algebraic(Cov, LoBounds = NULL, UpBounds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glb.algebraic_+3A_cov">Cov</code></td>
<td>
<p>A p * p covariance matrix. Positive definiteness is not checked.</p>
</td></tr>
<tr><td><code id="glb.algebraic_+3A_lobounds">LoBounds</code></td>
<td>
<p>A vector <code class="reqn">l =(l_1, \dots, l_p)</code> of length p with lower bounds to the diagonal elements <code class="reqn">x_i</code>.	The  default l=(0, . . . , 0) does not imply any constraint, because positive semidefiniteness of the matrix <code class="reqn">\tilde{ C} + Diag(x)</code> implies <code class="reqn">0 \leq x_i</code></p>
</td></tr>
<tr><td><code id="glb.algebraic_+3A_upbounds">UpBounds</code></td>
<td>
<p>A vector u =(u1, . . . , up) of length p with upper bounds to the diagonal elements xi. The default is u = v.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If C is a p * p-covariance matrix, v = diag(C) its diagonal (i. e. the vector of variances <code class="reqn">v_i = c_{ii}</code>), <code class="reqn">\tilde { C} = C - Diag(v)</code> is the covariance matrix with 0s substituted in the diagonal and x = the vector <code class="reqn">x_1, \dots ,x_n</code> the educational testing problem is (see e. g., Al-Homidan 2008)
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^p x_i \rightarrow \min</code>
</p>

<p>s.t. </p>
<p style="text-align: center;"><code class="reqn">\tilde{ C} + Diag(x) \geq 0</code>
</p>
<p>(i.e. positive semidefinite) and <code class="reqn">x_i \leq v_i, i=1,\dots,p</code>. This is the same as minimizing the trace of the symmetric matrix
</p>
<p style="text-align: center;"><code class="reqn">\tilde{ C}+diag(x)=\left(\begin{array}{llll}
      x_1    &amp; c_{12} &amp; \ldots &amp; c_{1p} \\
      c_{12} &amp; x_2    &amp; \ldots &amp; c_{2p} \\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
      c_{1p} &amp; c_{2p} &amp; \ldots &amp; x_p\\
      \end{array}\right)</code>
</p>

<p>s. t. <code class="reqn">\tilde{ C} + Diag(x)</code> is positive semidefinite and <code class="reqn">x_i \leq v_i</code>. 
</p>
<p>The greatest lower bound to reliability is
</p>
<p style="text-align: center;"><code class="reqn">\frac{\sum_{ij} \bar{c_{ij}} + \sum_i x_i}{\sum_{ij}c_{ij}}</code>
</p>

<p>Additionally, function glb.algebraic allows the user to  change the upper bounds <code class="reqn">x_i \leq v_i</code> to
<code class="reqn">x_i \leq u_i</code> and add lower bounds <code class="reqn">l_i \leq x_i</code>.
</p>
<p>The greatest lower bound to reliability is applicable for tests with non-homogeneous items. It gives a sharp lower bound to the reliability of the total test score.
</p>
<p>Caution: Though glb.algebraic gives exact lower bounds for exact covariance matrices, the estimates from empirical matrices may be strongly biased upwards for small and medium sample sizes.
</p>
<p>glb.algebraic is wrapper for a call to function csdp of package Rcsdp (see its documentation).
</p>
<p>If Cov is the covariance matrix of subtests/items with known lower bounds, rel, to their reliabilities (e. g. Cronbachs <code class="reqn">\alpha</code>), LoBounds can be used to improve the lower bound to reliability by setting LoBounds &lt;- rel*diag(Cov).
</p>
<p>Changing UpBounds can be used to relax constraints <code class="reqn">x_i \leq v_i</code> or to fix <code class="reqn">x_i</code>-values by setting LoBounds[i] &lt; -z; UpBounds[i] &lt;- z.
</p>


<h3>Value</h3>

<table>
<tr><td><code>glb</code></td>
<td>
<p>The algebraic greatest lower bound</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>The vector x of the solution of the semidefinite program. These are the elements on the diagonal of C.</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Status of the solution. See documentation of csdp in package Rcsdp. If status is 2 or greater or equal than 4, no glb and solution is returned. If status is not 0, a warning message is generated.</p>
</td></tr>
<tr><td><code>Call</code></td>
<td>
<p>The calling string</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andreas Moltner <br /> 
Center of Excellence for Assessment in Medicine/Baden-Wurttemberg<br />
University of Heidelberg<br /> 
</p>
<p>William Revelle<br /> 
Department of Psychology <br />
Northwestern University Evanston, Illiniois <br /> https://personality-project.org/revelle.html
</p>


<h3>References</h3>

<p>Al-Homidan S (2008). Semidefinite programming for the educational testing problem. Central European Journal of Operations Research, 16:239-249.
</p>
<p>Bentler PM (1972) A lower-bound method for the dimension-free measurement of internal consistency. Soc Sci Res 1:343-357.
</p>
<p>Fletcher R (1981) A nonlinear programming problem in statistics (educational testing). SIAM J Sci Stat Comput 2:257-267.
</p>
<p>Shapiro A, ten Berge JMF (2000). The asymptotic bias of minimum trace factor analysis, with applications to the greatest lower bound to reliability. Psychometrika, 65:413-425.
</p>
<p>ten Berge, Socan G (2004). The greatest bound to reliability of a test and the hypothesis of unidimensionality. Psychometrika, 69:613-625.
</p>


<h3>See Also</h3>

<p>For an alternative estimate of the greatest lower bound, see <code><a href="#topic+glb.fa">glb.fa</a></code>.  For multiple estimates of reliablity, see <code><a href="#topic+guttman">guttman</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Cv&lt;-matrix(c(215, 64, 33, 22,
              64, 97, 57, 25,
              33, 57,103, 36,
              22, 25, 36, 77),ncol=4)

Cv                    # covariance matrix of a test with 4 subtests
Cr&lt;-cov2cor(Cv)       # Correlation matrix of tests
if(!require(Rcsdp)) {print("Rcsdp must be installed to find the glb.algebraic")} else {
 glb.algebraic(Cv)     # glb of total score
glb.algebraic(Cr)      # glb of sum of standardized scores

 w&lt;-c(1,2,2,1)         # glb of weighted total score
 glb.algebraic(diag(w) %*% Cv %*% diag(w))  
alphas &lt;- c(0.8,0,0,0) # Internal consistency of first test is known

glb.algebraic(Cv,LoBounds=alphas*diag(Cv))

                      # Fix all diagonal elements to 1 but the first:

lb &lt;- glb.algebraic(Cr,LoBounds=c(0,1,1,1),UpBounds=c(1,1,1,1))
lb$solution[1]        # should be the same as the squared mult. corr.
smc(Cr)[1] 
}                        

</code></pre>

<hr>
<h2 id='Gleser'>
Example data from Gleser, Cronbach and Rajaratnam (1965) to show basic principles of generalizability theory. 
</h2><span id='topic+Gleser'></span>

<h3>Description</h3>

<p>Gleser, Cronbach and Rajaratnam (1965) discuss the estimation of variance components and their ratios as part of their introduction to generalizability theory.  This is a adaptation of their &quot;illustrative data for a completely matched G study&quot; (Table 3).  12 patients are rated on 6 symptoms by two judges.  Components of variance are derived from the ANOVA. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Gleser)</code></pre>


<h3>Format</h3>

<p>A data frame with 12 observations on the following 12 variables. J item by judge:
</p>

<dl>
<dt><code>J11</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J12</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J21</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J22</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J31</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J32</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J41</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J42</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J51</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J52</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J61</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>J62</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>Generalizability theory is the application of a components of variance approach to the analysis of reliability.  Given a G study (generalizability) the components are estimated and then may be used in a D study (Decision).  Different ratios are formed as appropriate for the particular D study.
</p>


<h3>Source</h3>

<p>Gleser, G., Cronbach, L., and Rajaratnam, N. (1965). Generalizability of scores influenced by multiple sources of variance. Psychometrika, 30(4):395-418. (Table 3, rearranged to show increasing patient severity and increasing item severity.
</p>


<h3>References</h3>

<p>Gleser, G., Cronbach, L., and Rajaratnam, N. (1965). Generalizability of scores influenced by multiple sources of variance. Psychometrika, 30(4):395-418.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Find the MS for each component:
#First, stack the data
data(Gleser)
stack.g &lt;- stack(Gleser)
st.gc.df &lt;- data.frame(stack.g,Persons=rep(letters[1:12],12),
Items=rep(letters[1:6],each=24),Judges=rep(letters[1:2],each=12))
#now do the ANOVA
anov &lt;- aov(values ~ (Persons*Judges*Items),data=st.gc.df)
summary(anov)
</code></pre>

<hr>
<h2 id='Gorsuch'>Example data set from Gorsuch (1997) for an example factor extension.
</h2><span id='topic+Gorsuch'></span>

<h3>Description</h3>

<p>Gorsuch (1997) suggests an alternative to the classic Dwyer (1937) factor extension technique.  This data set is taken from that article.  Useful for comparing <code>link{fa.extension}</code> with and without the correct=TRUE option.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Gorsuch)</code></pre>


<h3>Details</h3>

<p>Gorsuc (1997) suggested an alternative model for factor extension.  His method is appropriate for the case of repeated variables.  This is handled in <code>link{fa.extension}</code> with correct=FALSE
</p>


<h3>Source</h3>

<p>Richard L. Gorsuch (1997) New Procedure for Extension Analysis in Exploratory Factor Analysis. Educational and Psychological Measurement, 57, 725-740.
</p>


<h3>References</h3>

<p>Dwyer, Paul S. (1937), The determination of the factor loadings of a given test from the known factor loadings of other tests. Psychometrika, 3, 173-178
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Gorsuch)

Ro &lt;- Gorsuch[1:6,1:6]
Roe &lt;- Gorsuch[1:6,7:10]
fo &lt;- fa(Ro,2,rotate="none")
fa.extension(Roe,fo,correct=FALSE)
</code></pre>

<hr>
<h2 id='Harman'>Five data sets from Harman (1967). 9 cognitive variables from Holzinger and 8 emotional variables from Burt</h2><span id='topic+Harman'></span><span id='topic+Harman.Burt'></span><span id='topic+Harman.Holzinger'></span><span id='topic+Harman.political'></span><span id='topic+Harman.5'></span><span id='topic+Harman.8'></span>

<h3>Description</h3>

<p>Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Harman)  
data(Harman.5)
data(Harman.political)
data(Harman.8)
</code></pre>


<h3>Details</h3>


<ul>
<li><p> Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696.
</p>
</li>
<li><p> Harman.Burt: a 8 x 8 correlation matrix of &ldquo;emotional&quot; items. N = 172
</p>
</li>
<li><p> Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14)
</p>
</li>
<li><p> Harman.political:  p 166.
</p>
</li>
<li><p> Harman.8   8 physical measures
</p>
</li></ul>

<p>Harman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the <code><a href="#topic+omega">omega</a></code> function. Should not be confused with the <code><a href="#topic+Holzinger">Holzinger</a></code> or  <code><a href="#topic+Holzinger.9">Holzinger.9</a></code> data sets in <code><a href="#topic+bifactor">bifactor</a></code>.
</p>
<p>See also the Holzinger-Swineford data set of 301 subjects with 26 variables in <code><a href="psychTools.html#topic+holzinger.swineford">holzinger.swineford</a></code>. These data were provided by Keith Widaman. &quot;The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the &quot;24 Psychological Variables&quot; example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.
</p>
<p>Rosseel, who developed the &quot;lavaan&quot; package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a &quot;resident&quot; data set when one downloads the lavaan package. Several background variables are included in this &quot;resident&quot; data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the &quot;resident&quot; data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs.&quot;
</p>
<p>Harman.Burt. Eight &ldquo;emotional&quot; variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations &gt; 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm=&quot;minres&quot; or fm=&quot;pa&quot; but will fail for fm=&quot;ml&quot;.)
</p>
<p>The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of &quot;lankiness&quot; and then four measures of &quot;stockiness&quot;.  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). 
</p>
<p>Variable 6 (&quot;Bitrochanteric diamter&quot;) is the distance between the outer points of the hips.  
</p>
<p>The row names match the original Harman paper, the column names have been abbreviated.
</p>
<p>See also the <code><a href="psychTools.html#topic+usaf">usaf</a></code> data set for other physical measurements.
</p>
<p>The <code><a href="#topic+fa">fa</a></code> solution for principal axes (fm=&quot;pa&quot;) matches the reported minres solution, as does the fm=&quot;minres&quot;. 
</p>
<p>For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  
</p>
<p>The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. 
</p>
<p>As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See <code><a href="psychTools.html#topic+burt">burt</a></code>.
</p>
<p>Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in <code><a href="#topic+Bechtoldt">Bechtoldt</a></code> and the 24 ability measures in <code><a href="datasets.html#topic+Harman74.cor">Harman74.cor</a></code>
</p>
<p>There are several other Harman  examples in the psych package (i.e., <a href="#topic+Harman.8">Harman.8</a>) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at <a href="datasets.html#topic+Harman74.cor">Harman74.cor</a>. 
</p>
<p>Other Harman data sets are 5 socioeconomic variables for 12 census tracts <a href="#topic+Harman.5">Harman.5</a> used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is <a href="#topic+Harman.political">Harman.political</a>.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
</p>


<h3>Source</h3>

<p>Harman (1967 p 164 and p 244.)
</p>
<p>H. Harman and W.Jones. (1966)
Factor analysis by minimizing residuals (minres).  Psychometrika, 31(3):351-368.
</p>


<h3>References</h3>

<p>Harman, Harry Horace (1967), Modern factor analysis. Chicago, University of Chicago Press.
</p>
<p>P.Bentler. Covariance structure models for maximal reliability of unit-weighted
composites. In Handbook of latent variable and related models, pages 1&ndash;17.  North Holland, 2007.
</p>
<p>Burt, C.General and Specific Factors underlying the Primary Emotions. Reports of the British Association for the Advancement of Science, 85th meeting, held in Manchester,  September 7-11, 1915. London, John Murray, 1916, p. 694-696 (retrieved from the web at https://www.biodiversitylibrary.org/item/95822#790
</p>


<h3>See Also</h3>

<p> See also the original <code><a href="psychTools.html#topic+burt">burt</a></code> data set, the <a href="#topic+Harman.5">Harman.5</a> and <a href="#topic+Harman.political">Harman.political</a> data sets.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Harman)
cor.plot(Harman.Holzinger)
cor.plot(Harman.Burt)  
smc(Harman.Burt)  #note how this produces impossible results
f2 &lt;- fa(Harman.8,2, rotate="none")  #minres matches Harman and Jones
</code></pre>

<hr>
<h2 id='harmonic.mean'> Find the harmonic mean of a vector, matrix, or columns of a data.frame</h2><span id='topic+harmonic.mean'></span>

<h3>Description</h3>

<p>The harmonic mean is merely the reciprocal of the arithmetic mean of the reciprocals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>harmonic.mean(x,na.rm=TRUE,zero=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="harmonic.mean_+3A_x">x</code></td>
<td>
<p> a vector, matrix, or data.frame </p>
</td></tr>
<tr><td><code id="harmonic.mean_+3A_na.rm">na.rm</code></td>
<td>
<p>na.rm=TRUE remove NA values before processing</p>
</td></tr>
<tr><td><code id="harmonic.mean_+3A_zero">zero</code></td>
<td>
<p>If TRUE, then if there are any zeros, return 0, else, return the harmonic mean of the non-zero elements</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Included as an example for teaching about functions. As well as for a discussion of how to estimate central tendencies.
Also used in <code><a href="#topic+statsBy">statsBy</a></code> to weight by the harmonic mean.
</p>
<p>Values of 0 can be included (in which case the harmonic.mean = 0) or converted to NA according to the zero option.  
</p>
<p>Added the zero option, March, 2017.
</p>


<h3>Value</h3>

<p>The harmonic mean(s)
</p>


<h3>Note</h3>

<p>Included as a simple demonstration of how to write a function</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(1,5)
x2 &lt;- x^2
x2[2] &lt;- NA
y &lt;- x - 1
X &lt;- data.frame(x,x2,y)
harmonic.mean(x)
harmonic.mean(x2)
harmonic.mean(X)
harmonic.mean(X,na.rm=FALSE)
harmonic.mean(X,zero=FALSE)

</code></pre>

<hr>
<h2 id='headTail'>Combine calls to head and tail</h2><span id='topic+headtail'></span><span id='topic+headTail'></span><span id='topic+topBottom'></span><span id='topic+quickView'></span>

<h3>Description</h3>

<p>A quick way to show the first and last n lines of a data.frame, matrix, or a text object.  Just a pretty call to <code><a href="utils.html#topic+head">head</a></code> and <code><a href="utils.html#topic+tail">tail</a></code> or <code><a href="utils.html#topic+View">View</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>headTail(x, top=4,bottom=4,from=1,to=NULL, digits=2, hlength = 4, tlength =4, 
ellipsis=TRUE) 
headtail(x,hlength=4,tlength=4,digits=2,ellipsis=TRUE,from=1,to=NULL)
topBottom(x, top=4,bottom=4,from=1,to=NULL, digits=2, hlength = 4, tlength = 4) 
quickView(x,top=8,bottom=8,from=1,to=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="headTail_+3A_x">x</code></td>
<td>
<p> A matrix or data frame or free text</p>
</td></tr>
<tr><td><code id="headTail_+3A_top">top</code></td>
<td>
<p>The number of lines at the beginning to show</p>
</td></tr>
<tr><td><code id="headTail_+3A_bottom">bottom</code></td>
<td>
<p>The number of lines at the end to show</p>
</td></tr>
<tr><td><code id="headTail_+3A_digits">digits</code></td>
<td>
<p>Round off the data to digits</p>
</td></tr>
<tr><td><code id="headTail_+3A_ellipsis">ellipsis</code></td>
<td>
<p>Separate the head and tail with dots (ellipsis)</p>
</td></tr>
<tr><td><code id="headTail_+3A_from">from</code></td>
<td>
<p>The first column to show (defaults to 1)</p>
</td></tr>
<tr><td><code id="headTail_+3A_to">to</code></td>
<td>
<p>The last column to show (defaults to the number of columns)</p>
</td></tr>
<tr><td><code id="headTail_+3A_hlength">hlength</code></td>
<td>
<p>The number of lines at the beginning to show (an alias for top)</p>
</td></tr>
<tr><td><code id="headTail_+3A_tlength">tlength</code></td>
<td>
<p>The number of lines at the end to show (an alias for bottom)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The first top and last bottom lines of a matrix or data frame with an ellipsis in between. If the input is neither a matrix nor data frame, the output will be the first top and last bottom lines. 
</p>
<p>For each line, just columns starting at from and going to to will be displayed.  Bt default, from = 1 and to = the last column.
</p>
<p>topBottom is just a call to headTail with ellipsis = FALSE and returning a matrix output.
</p>
<p>quickView is a call to <code><a href="utils.html#topic+View">View</a></code> which opens a viewing window which is scrollable (if needed because the number of lines listed is more than a screen's worth).  View (and therefore quickView) is slower than <code><a href="#topic+headTail">headTail</a></code> or  <code><a href="#topic+topBottom">topBottom</a></code>.  
</p>


<h3>See Also</h3>

 <p><code><a href="utils.html#topic+head">head</a></code> and <code><a href="utils.html#topic+tail">tail</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
headTail(psychTools::iqitems,4,8,to=6) #the first 4 and last 6 items from 1 to 6
topBottom(psychTools::ability,from =2, to = 6) #the first and last 4 items from 2 to 6
headTail(psychTools::bfi,top=4, bottom=4,from =6,to=10) #the first and last 4 from 6 to 10
#not shown
#quickView(ability,hlength=10,tlength=10)  #this loads a spreadsheet like table  
</code></pre>

<hr>
<h2 id='ICC'> Intraclass Correlations (ICC1, ICC2, ICC3 from Shrout and Fleiss) </h2><span id='topic+ICC'></span>

<h3>Description</h3>

<p>The Intraclass correlation is used as a measure of association when studying the reliability of raters.  Shrout and Fleiss (1979) outline 6 different estimates, that depend upon the particular experimental design. All are implemented and given confidence limits.  Uses either aov or lmer depending upon options.  lmer allows for missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICC(x,missing=TRUE,alpha=.05,lmer=TRUE,check.keys=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICC_+3A_x">x</code></td>
<td>
<p>a matrix or dataframe of ratings</p>
</td></tr>
<tr><td><code id="ICC_+3A_missing">missing</code></td>
<td>
<p>if TRUE, remove missing data &ndash; work on complete cases only (aov only)</p>
</td></tr>
<tr><td><code id="ICC_+3A_alpha">alpha</code></td>
<td>
<p>The alpha level for significance for finding the confidence intervals</p>
</td></tr>
<tr><td><code id="ICC_+3A_lmer">lmer</code></td>
<td>
<p>Should we use the lmer function from lme4?  This handles missing data and gives variance components as well. TRUE by default.</p>
</td></tr>
<tr><td><code id="ICC_+3A_check.keys">check.keys</code></td>
<td>
<p>If TRUE reverse those items that do not correlate with total score. This is not done by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shrout and Fleiss (1979) consider six cases of reliability of ratings done by k raters on n targets. McGraw and Wong (1996) consider 10, 6 of which are identical to Shrout and Fleiss and  4 are conceptually different but use the same equations as the 6 in Shrout and Fleiss. 
</p>
<p>The intraclass correlation is used if raters are all of the same &ldquo;class&quot;.  That is, there is no logical way of distinguishing them.  Examples include correlations between  pairs of twins, correlations between raters.  If the variables are logically distinguishable (e.g., different items on a test), then the more typical coefficient is based upon the inter-class correlation (e.g., a Pearson r) and a statistic such as <code><a href="#topic+alpha">alpha</a></code> or <code><a href="#topic+omega">omega</a></code> might be used.  alpha and ICC3k are identical.
</p>
<p>Where the data are laid out in terms of Rows (subjects) and Columns (rater or tests), the various ICCs are found by the ratio of various estimates of variance components.  In all cases, subjects are taken as varying at random, and the residual variance is also random.  The distinction between models 2 and 3 is whether the judges (items/tests) are seen as random or fixed.  A further distinction is whether the emphasis is upon absolute agreement of the judges, or merely consistency.  
</p>
<p>As discussed by Liljequist et al. (2019), McGraw and Wong lay out 5 models which use just three forms of the ICC equations.
</p>
<p>Model 1  is a one way model with 
</p>
<p>ICC1: Each  target is rated by a different  judge and the judges are selected at random.  
</p>
<p style="text-align: center;"><code class="reqn">
ICC(1,1) = \rho_{1,1} = \frac{\sigma^2_r}{\sigma^2_r + \sigma^2_w} </code>
</p>

<p>(This is a one-way ANOVA fixed effects model and is found by  (MSB- MSW)/(MSB+ (nr-1)*MSW))   
</p>
<p>ICC2: A random sample of k judges rate each target.  The measure is one of absolute agreement in the ratings.  
</p>
<p style="text-align: center;"><code class="reqn">
ICC(2,1) = \rho_{2,1} = \frac{\sigma^2_r}{\sigma^2_r + \sigma^2_c +\sigma^2_{rc}  + \sigma^2_e} </code>
</p>

<p>Found as (MSB- MSE)/(MSB + (nr-1)*MSE + nr*(MSJ-MSE)/nc) 
</p>
<p>ICC3: A fixed set of k judges rate each target. There is no generalization to a larger population of judges. 
</p>
<p style="text-align: center;"><code class="reqn">
ICC(3,1) = \rho_{3,1} = \frac{\sigma^2_r}{\sigma^2_r + \sigma^2_c + \sigma^2_e} </code>
</p>

<p>(MSB - MSE)/(MSB+ (nr-1)*MSE)
</p>
<p>Then, for each of these cases, is reliability to be estimated for a single rating or for the average of k ratings?  (The 1 rating case is equivalent to the average intercorrelation, the k rating case to the Spearman Brown adjusted reliability.)
</p>
<p>ICC1 is sensitive to differences in means between raters and is a measure of absolute agreement.
</p>
<p>ICC2 and ICC3 remove mean differences between judges, but are sensitive to interactions of raters by judges.  The difference between ICC2 and ICC3 is whether raters are seen as fixed or random effects.
</p>
<p>ICC1k, ICC2k, ICC3K reflect the means of k raters.   
</p>
<p>If using the lmer option, then missing data are allowed.  In addition the lme object returns the variance decomposition.  (This is simliar to  <code><a href="#topic+testRetest">testRetest</a></code> which works on the items from two occasions.
</p>
<p>The check.keys option by default reverses items that are negatively correlated with total score.  A message is issued.
</p>


<h3>Value</h3>

<table>
<tr><td><code>results</code></td>
<td>
<p>A matrix of 6 rows and 8 columns, including the ICCs, F test, p values, and confidence limits</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>The anova summary table or the lmer summary table</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>The anova statistics (converted from lmer if using lmer)</p>
</td></tr>
<tr><td><code>MSW</code></td>
<td>
<p>Mean Square Within based upon the anova</p>
</td></tr>
<tr><td><code>lme</code></td>
<td>
<p>The variance decomposition if using the lmer option</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The results for the Lower and Upper Bounds for ICC(2,k) do not match those of SPSS 9 or 10, but do match the definitions of Shrout and Fleiss.  SPSS  seems to have been using the  formula in McGraw and Wong, but not the errata on p 390.  They seem to have fixed it in more recent releases (15). 
</p>
<p>Starting with psych  1.4.2, the confidence intervals are based upon (1-alpha)% at both tails of the confidence interval.  This is in agreement with Shrout and Fleiss.  Prior to 1.4.2 the confidence intervals were (1-alpha/2)%. 
However, at some point, this error slipped back again.  It has been fixed in version 1.9.5 (5/21/19).
</p>
<p>However, following some very useful comments from Mijke Rhumtulla, I should divide by 2 after all.  This has been fixed (again) 3/05/22 for version 2.2.2.
</p>
<p>An occasionally asked question: How do these 6 measures compare to the 10 ICCsreported by SPSS?  The answer is that the 6 Shrout and Fleiss are the same as the 10 by McGraw and Wong.
</p>
<p>A very helpful discussion of the formulae used in the calculations is by Liljequist et al, 2019. 
</p>
<p>Although M &amp; W distinguish between two way random effects and two way mixed effects for consistency of a single rater, examining their formula show that both of these are S&amp; F ICC(3,1).
</p>
<p>Similarly, the S&amp;F ICC(2,1) is called both Two way random effects, absolute agreement and Two way mixed effects, absolute agreement by M&amp;W.
</p>
<p>The same confusion occurs with multiple raters (3,k) are both two way random effects and mixed effects for consistency  and (2,k) are both two way random and two way mixed absolute agreement.
</p>
<p>As M&amp;W say &quot;The importance of the random-fixed effects distinction is in its effect on the interpretation, but not calculation, of an ICC. Namely, when levels of the column factor are randomly sampled, one can generalize beyond one's data, but not when they are fixed. In either case, however, the value of the ICC is the same,&quot; (p 37) 
</p>
<p>To quote from IBM support:
</p>
<p>&ldquo;Problem
</p>
<p>I'm using the SPSS RELIABILITY procedure to compute intraclass correlation coefficients (ICCs), and am trying to make sure I understand the correspondence between the measures available in SPSS and those discussed in Shrout and Fleiss. How do the two sets of models relate to each other?
</p>
<p>Resolving The Problem
Shrout and Fleiss (1979) discussed six ICC measures, which consist of pairs of measures for the reliability of a single rating and that of the average of k ratings (where k is the number of raters) for three different models: the one-way random model (called Case 1), the two-way random model (Case 2), and the two-way mixed model (Case 3). The measures implemented in SPSS were taken from McGraw and Wong (1996), who discussed these six measures, plus four others. The additional ones in McGraw and Wong are actually numerically identical to the four measures for the two-way cases discussed by Shrout and Fleiss, differing only in their interpretation.
</p>
<p>The correspondence between the measures available in SPSS and those discussed in Shrout and Fleiss is as follows:
</p>
<p>S&amp;F(1,1) = SPSS One-Way Random Model, Absolute Agreement, Single Measure
</p>
<p>S&amp;F(1,k) = SPSS One-Way Random Model, Absolute Agreement, Average Measure
</p>
<p>S&amp;F(2,1) = SPSS Two-Way Random Model, Absolute Agreement, Single Measure
</p>
<p>S&amp;F(2,k) = SPSS Two-Way Random Model, Absolute Agreement, Average Measure
</p>
<p>S&amp;F(3,1) = SPSS Two-Way Mixed Model, Consistency, Single Measure
</p>
<p>S&amp;F(3,k) = SPSS Two-Way Mixed Model, Consistency, Average Measure
</p>
<p>SPSS also offers consistency measures for the two-way random case (numerically equivalent to the consistency measures for the two-way mixed case, but differing in interpretation), and absolute agreement measures for the two-way mixed case (numerically equivalent to the absolute agreement measures for the two-way random case, again differing in interpretation)
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

 
<p>Shrout, Patrick E. and Fleiss, Joseph L. Intraclass correlations: uses in assessing rater reliability.  Psychological Bulletin, 1979, 86, 420-3428.
</p>
<p>McGraw, Kenneth O. and Wong, S. P. (1996), Forming inferences about some intraclass correlation coefficients.  Psychological Methods, 1, 30-46. + errata on page 390.
</p>
<p>Liljequist David, Elfving Britt and Skavberg Kirsti 
(2019) Intraclass correlation-A discussion and demonstration of basic features. PLoS ONE 14(7): e0219854. https://doi.org/10.1371/journal.
</p>
<p>Revelle, W. (in prep) An introduction to psychometric theory with applications in R. Springer. (working draft available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sf &lt;- matrix(c(
9,    2,   5,    8,
6,    1,   3,    2,
8,    4,   6,    8,
7,    1,   2,    6,
10,   5,   6,    9,
6,   2,   4,    7),ncol=4,byrow=TRUE)
colnames(sf) &lt;- paste("J",1:4,sep="")
rownames(sf) &lt;- paste("S",1:6,sep="")
sf  #example from Shrout and Fleiss (1979)
ICC(sf,lmer=FALSE)  #just use the aov procedure
   
#data(sai)
sai &lt;- psychTools::sai
sai.xray &lt;- subset(sai,(sai$study=="XRAY") &amp; (sai$time==1))
xray.icc &lt;- ICC(sai.xray[-c(1:3)],lmer=TRUE,check.keys=TRUE)
xray.icc
xray.icc$lme  #show the variance components as well
</code></pre>

<hr>
<h2 id='iclust'>iclust: Item Cluster Analysis   &ndash; Hierarchical cluster analysis using psychometric principles </h2><span id='topic+ICLUST'></span><span id='topic+iclust'></span>

<h3>Description</h3>

<p>A common data reduction technique is to cluster cases (subjects). Less common, but particularly useful in psychological research, is to cluster items (variables). This may be thought of as an alternative to factor analysis, based upon a much simpler model. The cluster model is that the correlations between variables reflect that each item loads on at most one cluster, and that items that load on those clusters correlate as a function of their respective loadings on that cluster and items that define different clusters correlate as a function of their respective cluster loadings and the intercluster correlations. 
Essentially, the cluster model is a Very Simple Structure factor model of complexity one (see <code><a href="#topic+VSS">VSS</a></code>).
</p>
<p>This function applies the iclust algorithm to hierarchically cluster items to form composite scales. Clusters are combined if coefficients alpha and beta will increase in the new cluster.
</p>
<p><code class="reqn">\alpha</code>, the mean split half correlation, and  <code class="reqn">\beta</code>, the worst split half correlation, are estimates of the reliability and general factor saturation of the test.  (See also the <code><a href="#topic+omega">omega</a></code> function to estimate McDonald's coeffients   <code class="reqn">\omega_h</code> and   <code class="reqn">\omega_t</code>). Other reliability estimates are found in the <code><a href="#topic+reliability">reliability</a></code> and <code><a href="#topic+splitHalf">splitHalf</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iclust(r.mat, nclusters=0, alpha=3, beta=1, beta.size=4, alpha.size=3,
correct=TRUE,correct.cluster=TRUE, reverse=TRUE, beta.min=.5, output=1,
digits=2,labels=NULL,cut=0, n.iterations =0, title="ICLUST", plot=TRUE, 
weighted=TRUE,cor.gen=TRUE,SMC=TRUE,purify=TRUE,diagonal=FALSE)

ICLUST(r.mat, nclusters=0, alpha=3, beta=1, beta.size=4, alpha.size=3,
correct=TRUE,correct.cluster=TRUE, reverse=TRUE, beta.min=.5, output=1, 
digits=2,labels=NULL,cut=0,n.iterations = 0,title="ICLUST",plot=TRUE,
weighted=TRUE,cor.gen=TRUE,SMC=TRUE,purify=TRUE,diagonal=FALSE)


#iclust(r.mat)    #use all defaults
#iclust(r.mat,nclusters =3)    #use all defaults and if possible stop at 3 clusters
#ICLUST(r.mat, output =3)     #long output shows clustering history
#ICLUST(r.mat, n.iterations =3)  #clean up solution by item reassignment
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iclust_+3A_r.mat">r.mat</code></td>
<td>
<p> A correlation matrix or data matrix/data.frame. (If r.mat is not square i.e, a correlation matrix, the data are correlated using pairwise deletion for Pearson correlations. </p>
</td></tr>
<tr><td><code id="iclust_+3A_nclusters">nclusters</code></td>
<td>
<p>Extract clusters until nclusters remain (default will extract until the other criteria are met or 1 cluster, whichever happens first). See the discussion below for alternative techniques for specifying the number of clusters. </p>
</td></tr>
<tr><td><code id="iclust_+3A_alpha">alpha</code></td>
<td>
<p> Apply the increase in alpha criterion  (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate alphas. (default = 3) </p>
</td></tr>
<tr><td><code id="iclust_+3A_beta">beta</code></td>
<td>
<p>  Apply the increase in beta criterion (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate betas. (default =1) </p>
</td></tr>
<tr><td><code id="iclust_+3A_beta.size">beta.size</code></td>
<td>
<p> Apply the beta criterion after clusters are of beta.size  (default = 4)</p>
</td></tr>
<tr><td><code id="iclust_+3A_alpha.size">alpha.size</code></td>
<td>
<p> Apply the alpha criterion after clusters are of size alpha.size (default =3) </p>
</td></tr>
<tr><td><code id="iclust_+3A_correct">correct</code></td>
<td>
<p> Correct correlations for reliability (default = TRUE) </p>
</td></tr>
<tr><td><code id="iclust_+3A_correct.cluster">correct.cluster</code></td>
<td>
<p>Correct cluster -sub cluster correlations for reliability of the sub cluster , default is TRUE))</p>
</td></tr> 
<tr><td><code id="iclust_+3A_reverse">reverse</code></td>
<td>
<p>Reverse negative keyed items (default = TRUE</p>
</td></tr>
<tr><td><code id="iclust_+3A_beta.min">beta.min</code></td>
<td>
<p> Stop clustering if the beta is not greater than beta.min (default = .5) </p>
</td></tr>
<tr><td><code id="iclust_+3A_output">output</code></td>
<td>
<p> 1) short, 2) medium, 3 ) long output (default =1)</p>
</td></tr>
<tr><td><code id="iclust_+3A_labels">labels</code></td>
<td>
<p>vector of item content or labels. If NULL, then the colnames are used.  If FALSE, then labels are V1 .. Vn</p>
</td></tr>
<tr><td><code id="iclust_+3A_cut">cut</code></td>
<td>
<p>sort cluster loadings &gt; absolute(cut) (default = 0) </p>
</td></tr>
<tr><td><code id="iclust_+3A_n.iterations">n.iterations</code></td>
<td>
<p>iterate the solution n.iterations times to &quot;purify&quot; the clusters (default = 0)</p>
</td></tr>
<tr><td><code id="iclust_+3A_digits">digits</code></td>
<td>
<p> Precision of digits of output (default = 2) </p>
</td></tr>
<tr><td><code id="iclust_+3A_title">title</code></td>
<td>
<p> Title for this run </p>
</td></tr>
<tr><td><code id="iclust_+3A_plot">plot</code></td>
<td>
<p>Should ICLUST.diagram be called automatically for plotting (does not require Rgraphviz default=TRUE)</p>
</td></tr>
<tr><td><code id="iclust_+3A_weighted">weighted</code></td>
<td>
<p>Weight the intercluster correlation by the size of the two clusters (TRUE) or do not weight them (FALSE)</p>
</td></tr>
<tr><td><code id="iclust_+3A_cor.gen">cor.gen</code></td>
<td>
<p>When correlating clusters with subclusters, base the correlations on the general factor (default) or general + group (cor.gen=FALSE)</p>
</td></tr>
<tr><td><code id="iclust_+3A_smc">SMC</code></td>
<td>
<p>When estimating cluster-item correlations, use the smcs as the estimate of an item communality (SMC=TRUE) or use the maximum correlation (SMC=FALSE).</p>
</td></tr>  
<tr><td><code id="iclust_+3A_purify">purify</code></td>
<td>
<p>Should clusters be defined as the original groupings (purify = FAlSE) or by the items with the highest loadings on those original clusters? (purify = TRUE)  </p>
</td></tr>
<tr><td><code id="iclust_+3A_diagonal">diagonal</code></td>
<td>
<p>Should the diagonal be included in the fit statistics.  The default is not to include it.  Prior to 1.2.8, the diagonal was included.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p> Extensive documentation and justification of the algorithm is available in the original MBR 1979 <a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a> paper.  Further discussion of the algorithm and sample output is available on the personality-project.org web page: <a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a> 
</p>
<p>A common problem in the social sciences is to construct scales or composites of items to measure constructs of theoretical interest and practical importance. This process frequently involves administering a battery of items from which those that meet certain criteria are selected. These criteria might be rational, empirical,or factorial. A similar problem is to analyze the adequacy of scales that already have been formed and to decide whether the putative constructs are measured properly. Both of these problems have been discussed in numerous texts,  as well as in myriad articles. Proponents of various methods have argued for the importance of face validity, discriminant validity, construct validity, factorial homogeneity, and theoretical importance. 
</p>
<p>Revelle (1979) proposed that hierachical cluster analysis could be used to estimate a new coefficient (beta) that was an estimate of the  general factor saturation of a test.  More recently, Zinbarg, Revelle, Yovel and Li (2005) compared McDonald's Omega to Cronbach's alpha and Revelle's beta. They conclude that   <code class="reqn">\omega_h</code> hierarchical is the best estimate.  An algorithm for estimating <code><a href="#topic+omega">omega</a> </code> is available as part of the psych package. 
</p>
<p>Revelle and Zinbarg (2009) discuss alpha, beta, and omega, as well as other estimates of reliability. 
</p>
<p>The original ICLUST program was written in FORTRAN to run on CDC and IBM mainframes and was then modified to run in PC-DOS.  The R version of iclust is a completely new version  written for the psych package.  
</p>
<p>A requested feature (not yet available) is to specify certain items as forming a cluster.  That is, to do confirmatory cluster analysis.  
</p>
<p>The program currently has three primary functions: cluster, loadings, and graphics.  
</p>
<p>In June, 2009, the option of weighted versus unweighted beta was introduced.  Unweighted beta calculates beta based upon the correlation between  two clusters, corrected for test length using the Spearman-Brown prophecy formala, while weighted beta finds the average interitem correlation between the items  within two clusters and then finds beta from this.  That is, for two clusters A and B of size N and M with between average correlation rb, weighted beta is (N+M)^2 rb/(Va +Vb + 2Cab).  Raw (unweighted) beta is 2rab/(1+rab) where rab = Cab/sqrt(VaVb).   Weighted beta seems a more appropriate estimate and is now the default.  Unweighted beta is still available for consistency with prior versions.
</p>
<p>Also modified in June, 2009 was the way of correcting for item overlap when calculating the cluster-subcluster correlations for the graphic output.  This does not affect the final cluster solution, but does produce slightly different path values.  In addition, there are two ways to solve for the cluster - subcluster correlation.
</p>
<p>Given the covariance between two clusters, Cab with average rab = Cab/(N*M), and cluster variances Va and Vb with Va = N  + N*(N-1)*ra  then the  correlation of cluster A with the combined cluster AB  is either
</p>
<p>a) ((N^2)ra + Cab)/sqrt(Vab*Va)   (option cor.gen=TRUE) or
b) (Va - N + Nra + Cab)/sqrt(Vab*Va)    (option cor.gen=FALSE)
</p>
<p>The default is to use cor.gen=TRUE.
</p>
<p>Although iclust will give what it thinks is the best solution in terms of the number of clusters to extract, the user will sometimes disagree.  To get more clusters than the default solution, just set the nclusters parameter to the number desired.  However, to get fewer than meet the alpha and beta criteria, it is sometimes necessary to set alpha=0 and beta=0 and then set the nclusters to the desired number.   
</p>
<p>Clustering 24 tests of mental ability
</p>
<p>A sample output using the 24 variable problem by Harman can be represented both graphically and in terms of the cluster order. The default is to produce graphics using the <code><a href="#topic+diagram">diagram</a></code> functions.  An alternative is to use the Rgraphviz package (from BioConductor).  Because this package is sometimes hard to install, there is an alternative option (<code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code> to write  dot language instructions for subsequent processing.  This will create a  graphic instructions suitable for any viewing program that uses the dot language.  <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code> produces the dot code for Graphviz.  Somewhat lower resolution graphs with fewer options are available in the <code><a href="#topic+ICLUST.diagram">ICLUST.diagram</a></code> function which does not require Rgraphviz.  Dot code can be viewed directly in Graphviz or can be tweaked using commercial software packages (e.g., OmniGraffle)
</p>
<p>Note that for the Harman 24 variable problem, with the default parameters, the data form one large cluster. (This is consistent with the Very Simple Structure (<code><a href="#topic+VSS">VSS</a></code>) output as well, which shows a clear one factor solution for complexity 1 data.)  
</p>
<p>An alternative solution is to ask for a somewhat more stringent set of criteria and require an increase in the size of beta for all clusters greater than 3 variables.  This produces a 4 cluster solution.
</p>
<p>It is also possible to use the original parameter settings, but ask for a 4 cluster solution.
</p>
<p>At least for the Harman 24 mental ability measures, it is interesting to compare the cluster pattern matrix with the oblique rotation solution from a factor analysis.  The factor congruence of a four factor oblique pattern solution with the four cluster solution is &gt; .99 for three of the four clusters and &gt; .97 for the fourth cluster.  The cluster pattern matrix (returned as an invisible object in the output)
</p>
<p>In September, 2012, the fit statistics (pattern fit and cluster fit) were slightly modified to (by default) not consider the diagonal (diagonal=FALSE).  Until then, the diagonal was included in the cluster fit statistics.  The pattern fit is analogous to factor analysis and is based upon the model = P x Structure where Structure is Pattern * Phi.  Then R* = R - model and fit is the ratio of sum(r*^2)/sum(r^2) for the off diagonal elements.  
</p>
<p>The results are best visualized using  <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>, the results of which can be saved as a dot file for the Graphviz program.  https://www.graphviz.org/. The <code><a href="#topic+iclust.diagram">iclust.diagram</a></code> is called automatically to produce cluster diagrams.  The resulting diagram is not quite as pretty as what can be achieved in dot code but is quite adequate if you don't want to use an external graphics program. With the installation of Rgraphviz, ICLUST can also provide cluster graphs.
</p>


<h3>Value</h3>

<table>
<tr><td><code>title</code></td>
<td>
<p>Name of this analysis</p>
</td></tr>
<tr><td><code>results</code></td>
<td>
<p>A list containing the step by step cluster history, including which pair was grouped, what were the alpha and betas of the two groups and of the combined group.  
</p>
<p>Note that the alpha values are &ldquo;standardized alphas&rdquo; based upon the correlation matrix, rather than the raw alphas that will come from <code><a href="#topic+scoreItems">scoreItems</a></code>
</p>
<p>The print.psych and summary.psych functions will print out just the must important results.</p>
</td></tr>
<tr><td><code>corrected</code></td>
<td>
<p>The raw and corrected for alpha reliability cluster intercorrelations.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>a matrix of -1, 0, and 1 values to define cluster membership.</p>
</td></tr>
<tr><td><code>purified</code></td>
<td>
<p>A list of the cluster definitions and cluster loadings of the purified solution.  These are sorted by importance (the eigenvalues of the clusters). The cluster membership from the original (O) and purified (P) clusters are indicated along with the cluster structure matrix.  These item loadings are the same as those found by the <code><a href="#topic+scoreItems">scoreItems</a></code> function and are found by correcting the item-cluster correlation for item overlap by summing the item-cluster covariances with all except that item and then adding in the smc for that item. These resulting correlations are then corrected for scale reliability.  
</p>
<p>To show just the most salient items, use the cutoff option in <code><a href="#topic+print.psych">print.psych</a></code> </p>
</td></tr>
<tr><td><code>cluster.fit</code>, <code>structure.fit</code>, <code>pattern.fit</code></td>
<td>
<p>There are a number of ways to evaluate how well any factor or cluster matrix reproduces the original matrix. Cluster fit considers how well the clusters fit if only correlations with clusters are considered.  Structure fit evaluates R = CC' while pattern fit evaluate R = C  inverse (phi) C'  where C is the cluster loading matrix, and phi is the intercluster correlation matrix.</p>
</td></tr>
<tr><td><code>pattern</code></td>
<td>
<p>The pattern matrix loadings.   Pattern is just C  inverse (Phi).  The pattern matrix is conceptually equivalent to that of a factor analysis, in that the pattern coefficients are b weights of the cluster to the variables, while the normal cluster loadings are correlations of the items with the cluster.  The four cluster and four factor pattern matrices for the Harman problem are very similar.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>This is a vector of the variable names in the order of the cluster diagram. This is useful as a tool for sorting correlations matrices.  See the last example.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>iclust draws graphical displays with or without using Rgraphiviz. Because of difficulties installing Rgraphviz on many systems, the default it not even try using it.  With the introduction of the <code><a href="#topic+diagram">diagram</a></code> functions, iclust now draws using iclust.diagram which is not as pretty as using Rgraphviz, but more stable. However, Rgraphviz can be used by using <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code> to  produces slightly better graphics. It is also possible to export dot code in the dot language for further massaging of the graphic. This may be done using <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>.  This last option is probably preferred for nice graphics which can be massaged in any dot code program (e.g., graphviz (https://graphviz.org)  or a commercial program such as OmniGraffle. 
</p>
<p>To view the cluster structure more closely, it is possible to save the graphic output as a pdf and then magnify this using a pdf viewer.  This is useful when clustering a large number of variables.
</p>
<p>In order to sort the clusters by cluster loadings, use <code><a href="#topic+iclust.sort">iclust.sort</a></code>.
</p>
<p>By default, the correlations used for the similarity matrix  are Pearson correlations. It is of coure possible to <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> to form the correlation matrix for later analysis. 
</p>
<p>iclust can also be used to organize complex correlation matrices.  Thus, by clustering the items in a correlation matrix, sorting that matrix by the cluster loadings using <code><a href="#topic+mat.sort">mat.sort</a></code>, and then plotting with  <code><a href="#topic+corPlot">corPlot</a></code>.  See the penultimate example.  An alterative way of ordering the variables is to use the order object which is just a vector of the variable names sorted by the way they appear in the tree diagram.  (See the final example.)
</p>


<h3>Author(s)</h3>

<p>William Revelle   </p>


<h3>References</h3>

<p>Revelle, W. Hierarchical Cluster Analysis and the Internal Structure of Tests. Multivariate Behavioral Research, 1979, 14, 57-74.
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma. Psychometrika, 2009. 
</p>
<p><a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a> <br />
See also  more extensive documentation at 
<a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a> and <br />
Revelle, W. (in prep) An introduction to psychometric theory with applications in R. To be published by Springer.  (working draft available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+iclust.sort">iclust.sort</a></code>, <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>, <code><a href="#topic+ICLUST.cluster">ICLUST.cluster</a></code>, <code><a href="#topic+cluster.fit">cluster.fit</a> </code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+omega">omega</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>test.data &lt;- Harman74.cor$cov
ic.out &lt;- iclust(test.data,title="ICLUST of the Harman data")
summary(ic.out)

#use all defaults and stop at 4 clusters
ic.out4 &lt;- iclust(test.data,nclusters =4,title="Force 4 clusters")  
summary(ic.out4)
ic.out1 &lt;- iclust(test.data,beta=3,beta.size=3)  #use more stringent criteria
ic.out  #more complete output 
plot(ic.out4)    #this shows the spatial representation
#use a dot graphics viewer on the out.file
#dot.graph &lt;- ICLUST.graph(ic.out,out.file="test.ICLUST.graph.dot")  
 #show the equivalent of a factor solution 
fa.diagram(ic.out4$pattern,Phi=ic.out4$Phi,main="Pattern taken from iclust") 

#demonstrating the use of labels using the bfi data set 
iclust(psychTools::bfi[,1:25], labels=psychTools::bfi.dictionary[1:25,2] ,
    title="ICLUST of bfi data set")
    
#organize a correlation matrix based upon cluster solution.
R &lt;- cor(psychTools::bfi, use="pairwise")
ic &lt;- iclust(R,plot=FALSE) #suppress the plot
R.s &lt;- mat.sort(R, ic)
corPlot(R.s, main ="bfi sorted by iclust loadings")
#compare with 
corPlot(R[ic$order,ic$order] ,main="bfi sorted by iclust order")
</code></pre>

<hr>
<h2 id='ICLUST.cluster'>Function to form hierarchical cluster analysis of items </h2><span id='topic+ICLUST.cluster'></span>

<h3>Description</h3>

<p>The guts of the  <code><a href="#topic+ICLUST">ICLUST</a></code> algorithm.  Called by <code><a href="#topic+ICLUST">ICLUST</a></code> See ICLUST for description.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLUST.cluster(r.mat, ICLUST.options,smc.items)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLUST.cluster_+3A_r.mat">r.mat</code></td>
<td>
<p> A correlation matrix</p>
</td></tr>
<tr><td><code id="ICLUST.cluster_+3A_iclust.options">ICLUST.options</code></td>
<td>
<p> A list of options (see <code><a href="#topic+ICLUST">ICLUST</a></code>) </p>
</td></tr>
<tr><td><code id="ICLUST.cluster_+3A_smc.items">smc.items</code></td>
<td>
<p>passed from the main program to speed up processing</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+ICLUST">ICLUST</a></code>
</p>


<h3>Value</h3>

<p>A list of cluster statistics, described more fully in <code><a href="#topic+ICLUST">ICLUST</a></code>
</p>
<table>
<tr><td><code>comp1</code></td>
<td>
<p>Description of 'comp1'</p>
</td></tr>
<tr><td><code>comp2</code></td>
<td>
<p>Description of 'comp2'</p>
</td></tr>
</table>
<p>...
</p>


<h3>Note</h3>

<p> Although the main code for ICLUST is here in ICLUST.cluster, the more extensive documentation is for <code><a href="#topic+ICLUST">ICLUST</a></code>.
</p>


<h3>Author(s)</h3>

<p>William Revelle  </p>


<h3>References</h3>

<p>Revelle, W. 1979, Hierarchical Cluster Analysis and the Internal Structure of Tests. Multivariate Behavioral Research, 14, 57-74. 
<a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a> <br />
See also  more extensive documentation at 
<a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>,<code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+cluster.fit">cluster.fit</a> </code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+omega">omega</a> </code></p>

<hr>
<h2 id='iclust.diagram'> Draw an ICLUST hierarchical cluster structure diagram </h2><span id='topic+iclust.diagram'></span><span id='topic+ICLUST.diagram'></span>

<h3>Description</h3>

<p>Given a cluster structure determined by <code><a href="#topic+ICLUST">ICLUST</a></code>, create a graphic structural diagram using graphic functions in the psych package To create dot code to describe the <code><a href="#topic+ICLUST">ICLUST</a></code> output with more precision, use <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>. If Rgraphviz has been successfully installed, the alternative is to use <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iclust.diagram(ic, labels = NULL, short = FALSE, digits = 2, cex = NULL, 
    min.size = NULL,
     e.size =1,
     colors=c("black","blue"), 
     main = "ICLUST diagram",
     cluster.names=NULL,marg=c(.5,.5,1.5,.5),plot=TRUE, bottomup=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iclust.diagram_+3A_ic">ic</code></td>
<td>
<p>Output from ICLUST</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_labels">labels</code></td>
<td>
<p>labels for variables (if not specified as rownames in the ICLUST output</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_short">short</code></td>
<td>
<p>if short=TRUE, variable names are replaced with Vn</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_digits">digits</code></td>
<td>
<p>Round the path coefficients to digits accuracy</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_cex">cex</code></td>
<td>
<p>The standard graphic control parameter for font size modifications.  This can be used to make the labels bigger or smaller than the default values.</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_min.size">min.size</code></td>
<td>
<p>Don't provide statistics for clusters less than min.size</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_e.size">e.size</code></td>
<td>
<p>size of the ellipses with the cluster statistics.</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_colors">colors</code></td>
<td>
<p>postive and negative </p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_main">main</code></td>
<td>
<p>The main graphic title</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_cluster.names">cluster.names</code></td>
<td>
<p>Normally, clusters are named sequentially C1 ... Cn.  If cluster.names are specified, then these values will be used instead.</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_marg">marg</code></td>
<td>
<p>Sets the margins to be narrower than the default values. Resets them upon return</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_plot">plot</code></td>
<td>
<p>If plot is TRUE, then draw the diagram,  if FALSE, then just return the veriable order from the plot</p>
</td></tr>
<tr><td><code id="iclust.diagram_+3A_bottomup">bottomup</code></td>
<td>
<p>Which way to draw the arrows.  TRUE means from the items to the clusters.  See note. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>iclust.diagram provides most of the power of <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code> without the difficulties involved in installing Rgraphviz.  It is called automatically from ICLUST. 
</p>
<p>Following a request by Michael Kubovy, cluster.names may be specified to replace the normal C1 ... Cn names.  
</p>
<p>If access to a dot language graphics program is available, it is probably better to use the iclust.graph function to get dot output for offline editing.
</p>
<p>Until 3/11/2 arrows went from clusters to items.  The default value for bottomup has been changed to draw from items to clusters.  To draw the old way, set bottomup=TRUE.</p>


<h3>Value</h3>

<p>Graphical output summarizing the hierarchical cluster structure.  The graph is drawn using the diagram functions (e.g., <code><a href="#topic+dia.curve">dia.curve</a></code>,  <code><a href="#topic+dia.arrow">dia.arrow</a></code>, <code><a href="#topic+dia.rect">dia.rect</a></code>, <code><a href="#topic+dia.ellipse">dia.ellipse</a></code> ) created as a work around to Rgraphviz.
</p>
<p>Also returned (invisibly) is a vector of variable names ordered by their location in the tree diagram. The plot option suppresses the plot for speed. 
</p>


<h3>Note</h3>

<p>Suggestions for improving the graphic output are welcome.   
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Revelle, W. Hierarchical Cluster Analysis and the Internal Structure of Tests. Multivariate Behavioral Research, 1979, 14, 57-74.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICLUST">ICLUST</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>v9 &lt;- sim.hierarchical()
v9c &lt;- ICLUST(v9)
test.data &lt;- Harman74.cor$cov
ic.out &lt;- ICLUST(test.data)
#now show how to relabel clusters
ic.bfi &lt;- iclust(psychTools::bfi[1:25],beta=3) #find the clusters
cluster.names &lt;- rownames(ic.bfi$results) #get the old names
#change the names to the desired ones
cluster.names[c(16,19,18,15,20)] &lt;- c("Neuroticism","Extra-Open","Agreeableness",
      "Conscientiousness","Open")
#now show the new names
iclust.diagram(ic.bfi,cluster.names=cluster.names,min.size=4,e.size=1.75)

</code></pre>

<hr>
<h2 id='ICLUST.graph'> create control code for ICLUST graphical output</h2><span id='topic+ICLUST.graph'></span><span id='topic+iclust.graph'></span>

<h3>Description</h3>

<p>Given a cluster structure determined by <code><a href="#topic+ICLUST">ICLUST</a></code>, create dot code to describe the <code><a href="#topic+ICLUST">ICLUST</a></code> output.  To use the dot code, use either https://www.graphviz.org/ Graphviz or a commercial viewer (e.g., OmniGraffle).  This function parallels <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code> which uses Rgraphviz.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLUST.graph(ic.results, out.file,min.size=1, short = FALSE,labels=NULL,
size = c(8, 6), node.font = c("Helvetica", 14), edge.font = c("Helvetica", 12), 
 rank.direction=c("RL","TB","LR","BT"), digits = 2, title = "ICLUST", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLUST.graph_+3A_ic.results">ic.results</code></td>
<td>
<p>output list from ICLUST </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_out.file">out.file</code></td>
<td>
<p> name of output file (defaults to console) </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_min.size">min.size</code></td>
<td>
<p>draw a smaller node (without all the information) for clusters &lt; min.size &ndash; useful for large problems</p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_short">short</code></td>
<td>
<p>if short==TRUE, don't use variable names</p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_labels">labels</code></td>
<td>
<p>vector of text labels (contents) for the variables</p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_size">size</code></td>
<td>
<p>size of output </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_node.font">node.font</code></td>
<td>
<p> Font to use for nodes in the graph </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_edge.font">edge.font</code></td>
<td>
<p> Font to use for the labels of the arrows (edges)</p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_rank.direction">rank.direction</code></td>
<td>
<p>LR or RL  </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_digits">digits</code></td>
<td>
<p> number of digits to show </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_title">title</code></td>
<td>
<p> any title </p>
</td></tr>
<tr><td><code id="ICLUST.graph_+3A_...">...</code></td>
<td>
<p> other options to pass </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Will create (or overwrite) an output file and print out the dot code to show a cluster structure. This dot file may be imported directly into a dot viewer (e.g.,  https://www.graphviz.org/).  The &quot;dot&quot; language is a powerful graphic description language that is particulary appropriate for viewing cluster output.  Commercial graphics programs (e.g., OmniGraffle) can also read (and clean up) dot files.  
</p>
<p>ICLUST.graph takes the output from <code><a href="#topic+ICLUST">ICLUST</a></code> results and processes it to provide a pretty picture of the results.  Original variables shown as rectangles and ordered on the left hand side (if rank direction is RL) of the graph.  Clusters are drawn as ellipses and include the alpha, beta, and size of the cluster.  Edges show the cluster intercorrelations.
</p>
<p>It is possible to trim the output to not show all cluster information. Clusters &lt; min.size are shown as small ovals without alpha, beta, and size information.
</p>
<p>Although it would be nice to process the dot code directly in R, the Rgraphviz package is difficult to use on all platforms and thus the dot code is written directly.
</p>


<h3>Value</h3>

<p> Output is a set of dot commands written either to console or to the output file.  These commands may then be used as input to any &quot;dot&quot; viewer, e.g., Graphviz.
</p>


<h3>Author(s)</h3>

<p><a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
<a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a></p>


<h3>References</h3>

<p> ICLUST: <a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a></p>


<h3>See Also</h3>

  <p><code><a href="#topic+VSS.plot">VSS.plot</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
test.data &lt;- Harman74.cor$cov
ic.out &lt;- ICLUST(test.data)
#out.file &lt;- file.choose(new=TRUE)   #create a new file to write the plot commands to 
#ICLUST.graph(ic.out,out.file)   
now go to graphviz (outside of R) and open the out.file you created
print(ic.out,digits=2)

## End(Not run)

 
#test.data &lt;- Harman74.cor$cov 
#my.iclust &lt;- ICLUST(test.data)
#ICLUST.graph(my.iclust)
#
#
#digraph ICLUST {
#  rankdir=RL;
#  size="8,8";
#  node [fontname="Helvetica" fontsize=14 shape=box, width=2];
#  edge [fontname="Helvetica" fontsize=12];
# label = "ICLUST";
#	fontsize=20;
#V1  [label = VisualPerception];
#V2  [label = Cubes];
#V3  [label = PaperFormBoard];
#V4  [label = Flags];
#V5  [label = GeneralInformation];
#V6  [label = PargraphComprehension];
#V7  [label = SentenceCompletion];
#V8  [label = WordClassification];
#V9  [label = WordMeaning];
#V10  [label = Addition];
#V11  [label = Code];
#V12  [label = CountingDots];
#V13  [label = StraightCurvedCapitals];
#V14  [label = WordRecognition];
#V15  [label = NumberRecognition];
#V16  [label = FigureRecognition];
#V17  [label = ObjectNumber];
#V18  [label = NumberFigure];
#V19  [label = FigureWord];
#V20  [label = Deduction];
#V21  [label = NumericalPuzzles];
#V22  [label = ProblemReasoning];
#V23  [label = SeriesCompletion];
#V24  [label = ArithmeticProblems];
#node [shape=ellipse, width ="1"];
#C1-&gt; V9 [ label = 0.78 ];
#C1-&gt; V5 [ label = 0.78 ];
#C2-&gt; V12 [ label = 0.66 ];
#C2-&gt; V10 [ label = 0.66 ];
#C3-&gt; V18 [ label = 0.53 ];
#C3-&gt; V17 [ label = 0.53 ];
#C4-&gt; V23 [ label = 0.59 ];
#C4-&gt; V20 [ label = 0.59 ];
#C5-&gt; V13 [ label = 0.61 ];
#C5-&gt; V11 [ label = 0.61 ];
#C6-&gt; V7 [ label = 0.78 ];
#C6-&gt; V6 [ label = 0.78 ];
#C7-&gt; V4 [ label = 0.55 ];
#C7-&gt; V1 [ label = 0.55 ];
#C8-&gt; V16 [ label = 0.5 ];
#C8-&gt; V14 [ label = 0.49 ];
#C9-&gt; C1 [ label = 0.86 ];
#C9-&gt; C6 [ label = 0.86 ];
#C10-&gt; C4 [ label = 0.71 ];
#C10-&gt; V22 [ label = 0.62 ];
#C11-&gt; V21 [ label = 0.56 ];
#C11-&gt; V24 [ label = 0.58 ];
#C12-&gt; C10 [ label = 0.76 ];
#C12-&gt; C11 [ label = 0.67 ];
#C13-&gt; C8 [ label = 0.61 ];
#C13-&gt; V15 [ label = 0.49 ];
#C14-&gt; C2 [ label = 0.74 ];
#C14-&gt; C5 [ label = 0.72 ];
#C15-&gt; V3 [ label = 0.48 ];
#C15-&gt; C7 [ label = 0.65 ];
#C16-&gt; V19 [ label = 0.48 ];
#C16-&gt; C3 [ label = 0.64 ];
#C17-&gt; V8 [ label = 0.62 ];
#C17-&gt; C12 [ label = 0.8 ];
#C18-&gt; C17 [ label = 0.82 ];
#C18-&gt; C15 [ label = 0.68 ];
#C19-&gt; C16 [ label = 0.66 ];
#C19-&gt; C13 [ label = 0.65 ];
#C20-&gt; C19 [ label = 0.72 ];
#C20-&gt; C18 [ label = 0.83 ];
#C21-&gt; C20 [ label = 0.87 ];
#C21-&gt; C9 [ label = 0.76 ];
#C22-&gt; 0 [ label = 0 ];
#C22-&gt; 0 [ label = 0 ];
#C23-&gt; 0 [ label = 0 ];
#C23-&gt; 0 [ label = 0 ];
#C1  [label =   "C1\n  alpha= 0.84\n beta=  0.84\nN= 2"] ;
#C2  [label =   "C2\n  alpha= 0.74\n beta=  0.74\nN= 2"] ;
#C3  [label =   "C3\n  alpha= 0.62\n beta=  0.62\nN= 2"] ;
#C4  [label =   "C4\n  alpha= 0.67\n beta=  0.67\nN= 2"] ;
#C5  [label =   "C5\n  alpha= 0.7\n beta=  0.7\nN= 2"] ;
#C6  [label =   "C6\n  alpha= 0.84\n beta=  0.84\nN= 2"] ;
#C7  [label =   "C7\n  alpha= 0.64\n beta=  0.64\nN= 2"] ;
#C8  [label =   "C8\n  alpha= 0.58\n beta=  0.58\nN= 2"] ;
#C9  [label =   "C9\n  alpha= 0.9\n beta=  0.87\nN= 4"] ;
#C10  [label =   "C10\n  alpha= 0.74\n beta=  0.71\nN= 3"] ;
#C11  [label =   "C11\n  alpha= 0.62\n beta=  0.62\nN= 2"] ;
#C12  [label =   "C12\n  alpha= 0.79\n beta=  0.74\nN= 5"] ;
#C13  [label =   "C13\n  alpha= 0.64\n beta=  0.59\nN= 3"] ;
#C14  [label =   "C14\n  alpha= 0.79\n beta=  0.74\nN= 4"] ;
#C15  [label =   "C15\n  alpha= 0.66\n beta=  0.58\nN= 3"] ;
#C16  [label =   "C16\n  alpha= 0.65\n beta=  0.57\nN= 3"] ;
#C17  [label =   "C17\n  alpha= 0.81\n beta=  0.71\nN= 6"] ;
#C18  [label =   "C18\n  alpha= 0.84\n beta=  0.75\nN= 9"] ;
#C19  [label =   "C19\n  alpha= 0.74\n beta=  0.65\nN= 6"] ;
#C20  [label =   "C20\n  alpha= 0.87\n beta=  0.74\nN= 15"] ;
#C21  [label =   "C21\n  alpha= 0.9\n beta=  0.77\nN= 19"] ;
#C22  [label =   "C22\n  alpha= 0\n beta=  0\nN= 0"] ;
#C23  [label =   "C23\n  alpha= 0\n beta=  0\nN= 0"] ;
#{ rank=same;
#V1;V2;V3;V4;V5;V6;V7;V8;V9;V10;V11;V12;V13;V14;V15;V16;V17;V18;V19;V20;V21;V22;V23;V24;}}
#
#copy the above output to Graphviz and draw it
#see \url{https://personality-project.org/r/r.ICLUST.html} for an example.

</code></pre>

<hr>
<h2 id='ICLUST.rgraph'> Draw an ICLUST graph using the Rgraphviz package </h2><span id='topic+ICLUST.rgraph'></span>

<h3>Description</h3>

<p>Given a cluster structure determined by <code><a href="#topic+ICLUST">ICLUST</a></code>, create a rgraphic directly using Rgraphviz.  To create dot code to describe the <code><a href="#topic+ICLUST">ICLUST</a></code> output with more precision, use <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>.  As an option, dot code is also generated and saved in a file. To  use the dot code, use either https://www.graphviz.org/ Graphviz or a commercial viewer (e.g., OmniGraffle).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLUST.rgraph(ic.results, out.file = NULL, min.size = 1, short = FALSE, 
    labels = NULL, size = c(8, 6), node.font = c("Helvetica", 14), 
   edge.font = c("Helvetica", 10), rank.direction=c("RL","TB","LR","BT"),
    digits = 2, title = "ICLUST",label.font=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLUST.rgraph_+3A_ic.results">ic.results</code></td>
<td>
<p>output list from ICLUST </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_out.file">out.file</code></td>
<td>
<p> File name to save optional dot code. </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_min.size">min.size</code></td>
<td>
<p>draw a smaller node (without all the information) for clusters &lt; min.size &ndash; useful for large problems</p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_short">short</code></td>
<td>
<p>if short==TRUE, don't use variable names</p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_labels">labels</code></td>
<td>
<p>vector of text labels (contents) for the variables</p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_size">size</code></td>
<td>
<p>size of output </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_node.font">node.font</code></td>
<td>
<p> Font to use for nodes in the graph </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_edge.font">edge.font</code></td>
<td>
<p> Font to use for the labels of the arrows (edges)</p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_rank.direction">rank.direction</code></td>
<td>
<p>LR or TB  or RL  </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_digits">digits</code></td>
<td>
<p> number of digits to show </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_title">title</code></td>
<td>
<p> any title </p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_label.font">label.font</code></td>
<td>
<p>The variable labels can be a different size than the other nodes.  This is particularly helpful if the number of variables is large or the labels are long.</p>
</td></tr>
<tr><td><code id="ICLUST.rgraph_+3A_...">...</code></td>
<td>
<p> other options to pass </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Will create (or overwrite) an output file and print out the dot code to show a cluster structure. This dot file may be imported directly into a dot viewer (e.g.,  https://www.graphviz.org/).  The &quot;dot&quot; language is a powerful graphic description language that is particulary appropriate for viewing cluster output.  Commercial graphics programs (e.g., OmniGraffle) can also read (and clean up) dot files.  
</p>
<p>ICLUST.rgraph takes the output from <code><a href="#topic+ICLUST">ICLUST</a></code> results and processes it to provide a pretty picture of the results.  Original variables shown as rectangles and ordered on the left hand side (if rank direction is RL) of the graph.  Clusters are drawn as ellipses and include the alpha, beta, and size of the cluster.  Edges show the cluster intercorrelations.
</p>
<p>It is possible to trim the output to not show all cluster information. Clusters &lt; min.size are shown as small ovals without alpha, beta, and size information.
</p>


<h3>Value</h3>

<p>Output is a set of dot commands written either to console or to the output file.  These commands may then be used as input to any &quot;dot&quot; viewer, e.g., Graphviz.
</p>
<p>ICLUST.rgraph is a version of <code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code> that uses Rgraphviz to draw on the screen as well.  
</p>
<p>Additional output is drawn to main graphics screen. </p>


<h3>Note</h3>

<p> Requires Rgraphviz</p>


<h3>Author(s)</h3>

<p><a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> <br />
<a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a></p>


<h3>References</h3>

<p> ICLUST: https://personality-project.org/r/r.ICLUST.html</p>


<h3>See Also</h3>

  <p><code><a href="#topic+VSS.plot">VSS.plot</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>test.data &lt;- Harman74.cor$cov
ic.out &lt;- ICLUST(test.data)   #uses iclust.diagram instead 
</code></pre>

<hr>
<h2 id='ICLUST.sort'>Sort items by absolute size of cluster loadings</h2><span id='topic+ICLUST.sort'></span><span id='topic+iclust.sort'></span>

<h3>Description</h3>

<p>Given a cluster analysis or factor analysis loadings matrix, sort the items by the (absolute) size of each column of loadings.  Used as part of ICLUST and SAPA analyses. The columns are rearranged by the 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLUST.sort(ic.load, cut = 0, labels = NULL,keys=FALSE,clustsort=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLUST.sort_+3A_ic.load">ic.load</code></td>
<td>
<p> The output from a factor or principal components analysis, or from ICLUST, or a matrix of loadings.</p>
</td></tr>
<tr><td><code id="ICLUST.sort_+3A_cut">cut</code></td>
<td>
<p>Do not include items in clusters with absolute loadings less than cut</p>
</td></tr>
<tr><td><code id="ICLUST.sort_+3A_labels">labels</code></td>
<td>
<p>labels for each item.</p>
</td></tr>
<tr><td><code id="ICLUST.sort_+3A_keys">keys</code></td>
<td>
<p>should cluster keys be returned? Useful if clusters scales are to be scored.</p>
</td></tr>
<tr><td><code id="ICLUST.sort_+3A_clustsort">clustsort</code></td>
<td>
<p>TRUE will will sort the clusters by their eigenvalues</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When interpreting cluster or factor analysis outputs, is is useful to group the items in terms of which items have their biggest loading on each factor/cluster and then to sort the items by size of the absolute factor loading.
</p>
<p>A stable cluster solution will be one in which the output of these cluster definitions does not vary when clusters are formed from the clusters so defined.
</p>
<p>With the keys=TRUE option, the resulting cluster keys may be used to score the original data or the correlation matrix to form clusters from the factors.
</p>


<h3>Value</h3>

<table>
<tr><td><code>sorted</code></td>
<td>
<p>A data.frame of item numbers, item contents, and item x factor loadings.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>A matrix of -1, 0, 1s defining each item by the factor/cluster with the row wise largest absolute loading. </p>
</td></tr>
</table>
<p>...
</p>


<h3>Note</h3>

<p> Although part of the ICLUST set of programs, this is also more useful for factor or principal components analysis.
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.ICLUST.html">https://personality-project.org/r/r.ICLUST.html</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST.graph">ICLUST.graph</a></code>,<code><a href="#topic+ICLUST.cluster">ICLUST.cluster</a></code>, <code><a href="#topic+cluster.fit">cluster.fit</a> </code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+factor2cluster">factor2cluster</a> </code></p>

<hr>
<h2 id='interp.median'>Find the interpolated sample median, quartiles, or specific quantiles for a vector, matrix, or data frame</h2><span id='topic+interp.median'></span><span id='topic+interp.quantiles'></span><span id='topic+interp.quartiles'></span><span id='topic+interp.boxplot'></span><span id='topic+interp.values'></span><span id='topic+interp.qplot.by'></span><span id='topic+interp.q'></span><span id='topic+interp.quart'></span>

<h3>Description</h3>

<p>For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interp.median(x, w = 1,na.rm=TRUE)
interp.quantiles(x, q = .5, w = 1,na.rm=TRUE)
interp.quartiles(x,w=1,na.rm=TRUE)
interp.boxplot(x,w=1,na.rm=TRUE)
interp.values(x,w=1,na.rm=TRUE)
interp.qplot.by(y,x,w=1,na.rm=TRUE,xlab="group",ylab="dependent",
               ylim=NULL,arrow.len=.05,typ="b",add=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interp.median_+3A_x">x</code></td>
<td>
<p>input vector </p>
</td></tr>
<tr><td><code id="interp.median_+3A_q">q</code></td>
<td>
<p>quantile to estimate ( 0 &lt; q &lt; 1</p>
</td></tr>
<tr><td><code id="interp.median_+3A_w">w</code></td>
<td>
<p>category width</p>
</td></tr>
<tr><td><code id="interp.median_+3A_y">y</code></td>
<td>
<p>input vector for interp.qplot.by</p>
</td></tr>
<tr><td><code id="interp.median_+3A_na.rm">na.rm</code></td>
<td>
<p>should missing values be removed</p>
</td></tr>
<tr><td><code id="interp.median_+3A_xlab">xlab</code></td>
<td>
<p>x label</p>
</td></tr>
<tr><td><code id="interp.median_+3A_ylab">ylab</code></td>
<td>
<p>Y label</p>
</td></tr>
<tr><td><code id="interp.median_+3A_ylim">ylim</code></td>
<td>
<p>limits for the y axis</p>
</td></tr>
<tr><td><code id="interp.median_+3A_arrow.len">arrow.len</code></td>
<td>
<p>length of arrow in interp.qplot.by</p>
</td></tr>
<tr><td><code id="interp.median_+3A_typ">typ</code></td>
<td>
<p>plot type in interp.qplot.by</p>
</td></tr>
<tr><td><code id="interp.median_+3A_add">add</code></td>
<td>
<p>add the plot or not</p>
</td></tr>
<tr><td><code id="interp.median_+3A_...">...</code></td>
<td>
<p>additional parameters to plotting function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the total number of responses is N, with median, M, and the number of responses at the median value, Nm &gt;1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. 
</p>
<p>The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.
</p>
<p>A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.
</p>
<p>If the input is a matrix or data frame, quantiles are reported for each variable.
</p>


<h3>Value</h3>

<table>
<tr><td><code>im</code></td>
<td>
<p>interpolated median(quantile)</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>interpolated values for all data points</p>
</td></tr>
</table>


<h3>See Also</h3>

  <p><code><a href="stats.html#topic+median">median</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>interp.median(c(1,2,3,3,3))  # compare with median = 3
interp.median(c(1,2,2,5))
interp.quantiles(c(1,2,2,5),.25)
x &lt;- sample(10,100,TRUE)
interp.quartiles(x)
#
x &lt;-  c(1,1,2,2,2,3,3,3,3,4,5,1,1,1,2,2,3,3,3,3,4,5,1,1,1,2,2,3,3,3,3,4,2)
y &lt;-  c(1,2,3,3,3,3,4,4,4,4,4,1,2,3,3,3,3,4,4,4,4,5,1,5,3,3,3,3,4,4,4,4,4)
x &lt;-  x[order(x)]   #sort the data by ascending order to make it clearer
y &lt;- y[order(y)]
xv &lt;- interp.values(x)
yv &lt;- interp.values(y)
barplot(x,space=0,xlab="ordinal position",ylab="value")
lines(1:length(x)-.5,xv)
points(c(length(x)/4,length(x)/2,3*length(x)/4),interp.quartiles(x))
barplot(y,space=0,xlab="ordinal position",ylab="value")
lines(1:length(y)-.5,yv)
points(c(length(y)/4,length(y)/2,3*length(y)/4),interp.quartiles(y))
data(psychTools::galton)
galton &lt;- psychTools::galton
interp.median(galton)
interp.qplot.by(galton$child,galton$parent,ylab="child height"
,xlab="Mid parent height") 


</code></pre>

<hr>
<h2 id='irt.1p'>Item Response Theory estimate of theta (ability) using a Rasch (like) model</h2><span id='topic+irt.0p'></span><span id='topic+irt.1p'></span><span id='topic+irt.2p'></span><span id='topic+irt.person.rasch'></span>

<h3>Description</h3>

<p>Item Response Theory models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  
This is an early and crude attempt to capture this modeling procedure. A better procedure is to use  <code><a href="#topic+irt.fa">irt.fa</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irt.person.rasch(diff, items)
irt.0p(items)
irt.1p(delta,items)
irt.2p(delta,beta,items) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irt.1p_+3A_diff">diff</code></td>
<td>
<p> A vector of item difficulties  &ndash;probably taken from irt.item.diff.rasch</p>
</td></tr>
<tr><td><code id="irt.1p_+3A_items">items</code></td>
<td>
<p>A matrix of 0,1 items  nrows = number of subjects, ncols = number of items</p>
</td></tr>
<tr><td><code id="irt.1p_+3A_delta">delta</code></td>
<td>
<p>delta is the same as diff and is the item difficulty parameter</p>
</td></tr>
<tr><td><code id="irt.1p_+3A_beta">beta</code></td>
<td>
<p>beta is the item discrimination parameter  found in <code><a href="#topic+irt.discrim">irt.discrim</a></code> </p>
</td></tr> 
</table>


<h3>Details</h3>

<p>A very preliminary IRT estimation procedure.
Given scores xij for ith individual on jth item <br />
Classical Test Theory ignores item difficulty and defines ability as expected score : abilityi = theta(i) = x(i.)
A zero parameter model rescales these mean scores from 0 to 1 to a quasi logistic scale ranging from - 4 to 4
This is merely a non-linear transform of the raw data to reflect a logistic mapping.
</p>
<p>Basic 1 parameter (Rasch) model considers item difficulties (delta j):  
p(correct on item j for the ith subject |theta i, deltaj) = 1/(1+exp(deltaj - thetai))
If we have estimates of item difficulty (delta), then we can find theta i by optimization 
</p>
<p>Two parameter model adds item sensitivity (beta j):
p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj- theta i)))
Estimate delta, beta, and theta  to maximize fit of model to data.
</p>
<p>The procedure used here is to first find the item difficulties assuming theta = 0
Then find theta given those deltas
Then find beta given delta and theta.
</p>
<p>This is not an &quot;official&quot; way to do IRT, but is useful for basic item development. See <code><a href="#topic+irt.fa">irt.fa</a></code> and <code><a href="#topic+score.irt">score.irt</a></code>  for far better options. 
</p>


<h3>Value</h3>

<p> a data.frame with estimated ability (theta) and quality of fit. (for irt.person.rasch)
<br />
a data.frame with the raw means, theta0, and the number of items completed</p>


<h3>Note</h3>

<p> Not recommended for serious use.  This code is under development. Much better functions are in the ltm and eRm packages. Similar analyses can be done using <code><a href="#topic+irt.fa">irt.fa</a></code> and <code><a href="#topic+score.irt">score.irt</a></code>. 
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+sim.irt">sim.irt</a></code>, <code><a href="#topic+sim.rasch">sim.rasch</a></code>,  <code><a href="#topic+logistic">logistic</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code>,  <code><a href="#topic+irt.item.diff.rasch">irt.item.diff.rasch</a></code>   </p>

<hr>
<h2 id='irt.fa'>Item Response Analysis by Exploratory Factor Analysis of tetrachoric/polychoric correlations</h2><span id='topic+irt.fa'></span><span id='topic+irt.select'></span><span id='topic+fa2irt'></span>

<h3>Description</h3>

<p>Although exploratory factor analysis and Item Response Theory seem to be very different models of binary data, they can provide equivalent parameter estimates of item difficulty and item discrimination.  Tetrachoric or polychoric correlations of a data set of dichotomous or polytomous items may be factor analysed using a minimum residual or maximum likelihood factor analysis and the result loadings transformed to item discrimination parameters.  The tau parameter from the tetrachoric/polychoric correlations combined with the item factor loading may be used to estimate item difficulties. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irt.fa(x,nfactors=1,correct=TRUE,plot=TRUE,n.obs=NULL,rotate="oblimin",fm="minres",
        sort=FALSE,...)
irt.select(x,y)
fa2irt(f,rho,plot=TRUE,n.obs=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irt.fa_+3A_x">x</code></td>
<td>
<p>A data matrix of dichotomous or discrete items, or the result of <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code>   </p>
</td></tr>
<tr><td><code id="irt.fa_+3A_nfactors">nfactors</code></td>
<td>
<p>Defaults to 1 factor</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_correct">correct</code></td>
<td>
<p>If true, then correct the tetrachoric correlations for continuity.  (See <code><a href="#topic+tetrachoric">tetrachoric</a></code>).  </p>
</td></tr>
<tr><td><code id="irt.fa_+3A_plot">plot</code></td>
<td>
<p>If TRUE, automatically call the <code><a href="#topic+plot.irt">plot.irt</a></code> or <code><a href="#topic+plot.poly">plot.poly</a></code> functions.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_y">y</code></td>
<td>
<p>the subset of variables to pick from the rho and tau output of a previous irt.fa analysis to allow for further analysis.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_n.obs">n.obs</code></td>
<td>
<p>The number of subjects used in the initial analysis if doing a second analysis of a correlation matrix.  In particular, if using the fm=&quot;minchi&quot; option, this should be the matrix returned by <code><a href="#topic+count.pairwise">count.pairwise</a></code>.</p>
</td></tr> 
<tr><td><code id="irt.fa_+3A_rotate">rotate</code></td>
<td>
<p>The default rotation is oblimin.  See <code><a href="#topic+fa">fa</a></code> for the other options.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_fm">fm</code></td>
<td>
<p>The default factor extraction is minres.  See <code><a href="#topic+fa">fa</a></code> for the other options.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_f">f</code></td>
<td>
<p>The object returned from <code><a href="#topic+fa">fa</a></code> </p>
</td></tr>
<tr><td><code id="irt.fa_+3A_rho">rho</code></td>
<td>
<p>The object returned from <code><a href="#topic+polychoric">polychoric</a></code> or <code><a href="#topic+tetrachoric">tetrachoric</a></code>.  This will include both a correlation matrix and the item difficulty levels.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_sort">sort</code></td>
<td>
<p>Should the factor loadings be sorted before preparing the item information tables.  Defaults to FALSE as this is more useful for scoring items. For tabular output it is better to have sort=TRUE.</p>
</td></tr>
<tr><td><code id="irt.fa_+3A_...">...</code></td>
<td>
<p>Additional parameters to pass to the factor analysis function</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+irt.fa">irt.fa</a></code> combines several functions into one to make the process of item response analysis easier.  Correlations are found using either <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code>.  Exploratory factor analyeses with all the normal options are then done using <code><a href="#topic+fa">fa</a></code>.  The results are then organized to be reported in terms of IRT parameters (difficulties and discriminations) as well as the more conventional factor analysis output. In addition, because the correlation step is somewhat slow, reanalyses may be done using the correlation matrix found in the first step.  In this case, if it is desired to use the fm=&quot;minchi&quot; factoring method, the number of observations needs to be specified as the matrix resulting from <code><a href="#topic+pairwiseCount">pairwiseCount</a></code>.
</p>
<p>The tetrachoric correlation matrix of dichotomous items may be factored using a (e.g.) minimum residual factor analysis function <code><a href="#topic+fa">fa</a></code> and the resulting loadings, <code class="reqn">\lambda_i</code> are transformed to discriminations by
<code class="reqn">\alpha = \frac{\lambda_i}{\sqrt{1-\lambda_i^2}} </code>.
</p>
<p>The difficulty parameter, <code class="reqn">\delta</code> is found from the <code class="reqn">\tau</code> parameter of the <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> function.
</p>
<p><code class="reqn">\delta_i = \frac{\tau_i}{\sqrt{1-\lambda_i^2}}</code>
</p>
<p>Similar analyses may be done with discrete item responses using polychoric correlations and distinct estimates of item difficulty (location)  for each item response.
</p>
<p>The results may be shown graphically using <code><a href="#topic+plot.irt">plot.irt</a></code> for dichotomous items or  <code><a href="#topic+plot.poly">plot.poly</a></code> for polytomous items.  These are called by plotting the <code><a href="#topic+irt.fa">irt.fa</a></code> output, see the examples).   For plotting there are three options: type = &quot;ICC&quot; will plot the item characteristic response function.  type = &quot;IIC&quot; will plot the item information function, and type= &quot;test&quot; will plot the test information function.  Invisible output from the plot function will return tables of item information as a function of several levels of the trait, as well as the standard error of measurement and the reliability at each of those levels.
</p>
<p>The normal input is just the raw data.  If, however, the correlation matrix has already been found using <code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+polychoric">polychoric</a></code>, or a previous analysis using <code><a href="#topic+irt.fa">irt.fa</a></code> then that result can be processed directly.  Because  <code><a href="#topic+irt.fa">irt.fa</a></code> saves the rho and tau matrices from the analysis, subsequent analyses of the same data set are much faster if the input is the object returned on the first run.  A similar feature is available in <code><a href="#topic+omega">omega</a></code>. 
</p>
<p>The output is best seen in terms of graphic displays.  Plot the output from irt.fa to see item and test information functions.  
</p>
<p>The print function will print the item location and discriminations.  The additional factor analysis output is available as an object in the output and may be printed directly by specifying the $fa object.
</p>
<p>The <code><a href="#topic+irt.select">irt.select</a></code> function is a helper function to allow for selecting a subset of a prior analysis for further analysis. First run irt.fa, then select a subset of variables to be analyzed in a subsequent irt.fa analysis.  Perhaps a better approach is to just plot and find the information for selected items.  
</p>
<p>The plot function for an irt.fa object will plot ICC (item characteristic curves), IIC (item information curves), or test information curves. In addition, by using the &quot;keys&quot; option,  these three kinds of plots can be done for selected items. This is particularly useful when trying to see the information characteristics of short forms of tests based upon the longer form factor analysis.
</p>
<p>The plot function will also return (invisibly) the informaton at multiple levels of the trait, the average information (area under the curve) as well as the location of the peak information for each item.  These may be then printed or printed in sorted order using the sort option in print.
</p>


<h3>Value</h3>

<table>
<tr><td><code>irt</code></td>
<td>
<p>A list of Item location (difficulty) and discrimination</p>
</td></tr>
<tr><td><code>fa</code></td>
<td>
<p>A list of statistics for the factor analyis</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>The tetrachoric/polychoric correlation matrix</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The tetrachoric/polychoric cut points</p>
</td></tr>
</table>


<h3>Note</h3>

<p><code><a href="#topic+irt.fa">irt.fa</a></code> makes use of the <code><a href="#topic+tetrachoric">tetrachoric</a></code> or 
<code><a href="#topic+tetrachoric">tetrachoric</a></code> functions. Both of these will use multiple cores if this is an option.  To set these use options(&quot;mc.cores&quot;=x) where x is the number of cores to use. (Macs default to 2, PCs seem to default to 1).
</p>
<p>In comparing irt.fa to the ltm function in the ltm package or to the analysis reported in Kamata and Bauer (2008) the discrimination parameters are not identical, because the irt.fa reports them in units of the normal curve while ltm and Kamata and Bauer report them in logistic units.  In addition, Kamata and Bauer do their factor analysis using a logistic error model.  Their results match the irt.fa results (to the 2nd or 3rd decimal) when examining their analyses using a normal model.  (With thanks to Akihito Kamata for sharing that analysis.) 
</p>
<p><code><a href="#topic+irt.fa">irt.fa</a></code> reports parameters in normal units.  To convert them to conventional IRT parameters, multiply by 1.702.  In addition, the location parameter is expressed in terms of difficulty (high positive scores imply lower frequency of response.)
</p>
<p>The results of <code><a href="#topic+irt.fa">irt.fa</a></code>  can be used by <code><a href="#topic+score.irt">score.irt</a></code> for irt based scoring.  First run <code><a href="#topic+irt.fa">irt.fa</a></code> and then score the results using a two parameter model using <code><a href="#topic+score.irt">score.irt</a></code>.  
</p>
<p>There is also confusion in the literature of how to interpret the tau parameter.  Here I treat it the item threshold for a response, which is to say that tau reflects item difficulty.  
</p>
<p>Further note that the rho parameter in the <code><a href="#topic+fa2irt">fa2irt</a></code> function is the result from a <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> analysis and reflects both the correlations and the item thresholds. 
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Kamata, Akihito and Bauer, Daniel J. (2008) A Note on the Relation Between Factor Analytic and Item Response Theory Models
Structural Equation Modeling, 15 (1) 136-153.
</p>
<p>McDonald, Roderick P. (1999) Test theory: A unified treatment. L. Erlbaum Associates.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+sim.irt">sim.irt</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+polychoric">polychoric</a></code> as well as <code><a href="#topic+plot.psych">plot.psych</a></code> for plotting the IRT item curves. The use of <code><a href="#topic+fa.extend">fa.extend</a></code> is helpful for the case of an incomplete block design to find the factor solution.  
</p>
<p>See also <code><a href="#topic+score.irt">score.irt</a></code> for scoring items based upon these parameter estimates. <code><a href="#topic+irt.responses">irt.responses</a></code>  will plot the empirical response curves for the alternative response choices for multiple choice items.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(17)
d9 &lt;- sim.irt(9,1000,-2.5,2.5,mod="normal") #dichotomous items
test &lt;- irt.fa(d9$items)
test 
op &lt;- par(mfrow=c(3,1))
plot(test,type="ICC")
plot(test,type="IIC")
plot(test,type="test")
par(op)

#compare this result to first finding the correlations, then factoring them, and 
# then applying fa2irt.
R  &lt;- tetrachoric(d9$items,correct=TRUE)
f &lt;- fa(R$rho)
test2 &lt;- fa2irt(f,R)
test2
test2$plot$sumInfo[[1]] - test$plot$sumInfo[[1]]  #identical results

set.seed(17)
items &lt;- sim.congeneric(N=500,short=FALSE,categorical=TRUE) #500 responses to 4 discrete items
d4 &lt;- irt.fa(items$observed)  #item response analysis of congeneric measures
d4    #show just the irt output
d4$fa  #show just the factor analysis output


op &lt;- par(mfrow=c(2,2))
plot(d4,type="ICC")
par(op)


#using the iq data set for an example of real items
#first need to convert the responses to tf
data(iqitems)
iq.keys &lt;- c(4,4,4, 6, 6,3,4,4,  5,2,2,4,  3,2,6,7)

iq.tf &lt;- score.multiple.choice(iq.keys,psychTools::iqitems,score=FALSE)  #just the responses
iq.irt &lt;- irt.fa(iq.tf)
print(iq.irt,short=FALSE) #show the IRT as well as factor analysis output
p.iq &lt;- plot(iq.irt)  #save the invisible summary table
p.iq  #show the summary table of information by ability level
#select a subset of these variables
small.iq.irt &lt;- irt.select(iq.irt,c(1,5,9,10,11,13))
small.irt &lt;- irt.fa(small.iq.irt)
plot(small.irt)
#find the information for three subset of iq items
keys &lt;- make.keys(16,list(all=1:16,some=c(1,5,9,10,11,13),others=c(1:5)))
plot(iq.irt,keys=keys)

## End(Not run)
#compare output to the ltm package or Kamata and Bauer   -- these are in logistic units 
ls &lt;- irt.fa(lsat6)
#library(ltm)
# lsat.ltm &lt;- ltm(lsat6~z1)
#  round(coefficients(lsat.ltm)/1.702,3)  #convert to normal (approximation)
#
#   Dffclt Dscrmn
#Q1 -1.974  0.485
#Q2 -0.805  0.425
#Q3 -0.164  0.523
#Q4 -1.096  0.405
#Q5 -1.835  0.386


#Normal results  ("Standardized and Marginal")(from Akihito Kamata )       
#Item       discrim             tau 
#  1       0.4169             -1.5520   
#  2       0.4333             -0.5999 
#  3       0.5373             -0.1512 
#  4       0.4044             -0.7723  
#  5       0.3587             -1.1966
#compare to ls 

  #Normal results  ("Standardized and conditional") (from Akihito Kamata )   
#item            discrim   tau
#  1           0.3848    -1.4325  
#  2           0.3976    -0.5505 
#  3           0.4733    -0.1332 
#  4           0.3749    -0.7159 
#  5           0.3377    -1.1264 
#compare to ls$fa and ls$tau 

#Kamata and Bauer (2008) logistic estimates
#1   0.826    2.773
#2   0.723    0.990
#3   0.891    0.249  
#4   0.688    1.285
#5   0.657    2.053
 
 

 


</code></pre>

<hr>
<h2 id='irt.item.diff.rasch'>Simple function to estimate item difficulties using IRT concepts</h2><span id='topic+irt.item.diff.rasch'></span><span id='topic+irt.discrim'></span>

<h3>Description</h3>

<p>Steps toward a  very crude and preliminary IRT program. These two functions  estimate item difficulty and discrimination parameters.  A better procedure is to use <code><a href="#topic+irt.fa">irt.fa</a></code> or the ltm package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irt.item.diff.rasch(items)
irt.discrim(item.diff,theta,items)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irt.item.diff.rasch_+3A_items">items</code></td>
<td>
<p> a matrix of items </p>
</td></tr>
<tr><td><code id="irt.item.diff.rasch_+3A_item.diff">item.diff</code></td>
<td>
<p>a vector of item difficulties (found by irt.item.diff)</p>
</td></tr>
<tr><td><code id="irt.item.diff.rasch_+3A_theta">theta</code></td>
<td>
<p>ability estimate from irt.person.theta</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Item Response Theory (aka &quot;The new psychometrics&quot;) models individual responses to items with a logistic function and an individual (theta) and item difficulty (diff) parameter.
</p>
<p>irt.item.diff.rasch finds item difficulties with the assumption of theta=0 for all subjects and that all items are equally discriminating.
</p>
<p>irt.discrim takes those  difficulties and theta estimates from <code><a href="#topic+irt.person.rasch">irt.person.rasch</a></code> to find item discrimination (beta) parameters.
</p>
<p>A far better package with these features is the ltm package.  The IRT functions in the psych-package are for pedagogical rather than production purposes.  They are believed to be accurate, but are not guaranteed. They do seem to be slightly more robust to missing data structures associated with SAPA data sets than the ltm package. 
</p>
<p>The <code><a href="#topic+irt.fa">irt.fa</a></code> function is also an alternative. This will find <code><a href="#topic+tetrachoric">tetrachoric</a></code> or <code><a href="#topic+polychoric">polychoric</a></code> correlations and then convert to IRT parameters using factor analysis (<code><a href="#topic+fa">fa</a></code>).
</p>


<h3>Value</h3>

<p>a vector of item difficulties or item discriminations.
</p>


<h3>Note</h3>

<p> Under development. Not recommended for public consumption.  See <code><a href="#topic+irt.fa">irt.fa</a></code> and <code><a href="#topic+score.irt">score.irt</a></code>  for far better options. 
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>See Also</h3>

 <p><code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+irt.person.rasch">irt.person.rasch</a></code> </p>

<hr>
<h2 id='irt.responses'>Plot probability of multiple choice responses as a function of a latent trait 
</h2><span id='topic+irt.responses'></span>

<h3>Description</h3>

<p>When analyzing ability tests, it is important to consider how the distractor alternatives vary as a function of the latent trait.  The simple graphical solution is to plot response endorsement frequencies against the values of the latent trait found from multiple items. A good item is one in which the probability of the distractors decrease and the keyed answer increases as the latent trait increases. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irt.responses(theta,items, breaks = 11,show.missing=FALSE, show.legend=TRUE, 
legend.location="topleft", colors=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irt.responses_+3A_theta">theta</code></td>
<td>
<p>The estimated latent trait (found, for example by using <code><a href="#topic+score.irt">score.irt</a></code>).
</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_items">items</code></td>
<td>

<p>A matrix or data frame of the multiple choice item responses.</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_breaks">breaks</code></td>
<td>
<p>The number of levels of the theta to use to form the probability estimates. May be increased if there are enough cases. </p>
</td></tr>
<tr><td><code id="irt.responses_+3A_show.legend">show.legend</code></td>
<td>
<p>Show the legend</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_show.missing">show.missing</code></td>
<td>
<p>For some SAPA data sets, there are a very large number of missing responses.  In general, we do not want to show their frequency.</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_legend.location">legend.location</code></td>
<td>
<p>Choose among c(&quot;bottomright&quot;, &quot;bottom&quot;, &quot;bottomleft&quot;, &quot;left&quot;, &quot;topleft&quot;, &quot;top&quot;, &quot;topright&quot;, &quot;right&quot;,  &quot;center&quot;,&quot;none&quot;).  The default is &quot;topleft&quot;.</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_colors">colors</code></td>
<td>
<p>if NULL, then use the default colors, otherwise, specify the color choices. The basic color palette is c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;, &quot;gold2&quot;, &quot;gray50&quot;, &quot;cornflowerblue&quot;, &quot;mediumorchid2&quot;).</p>
</td></tr>
<tr><td><code id="irt.responses_+3A_...">...</code></td>
<td>
<p>Other parameters for plots and points</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a convenient way to analyze the quality of item alternatives in a multiple choice ability test.  The typical use is to first score the test (using, e.g., <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code> according to some scoring key and to then find the <code><a href="#topic+score.irt">score.irt</a></code> based scores.  Response frequencies for each alternative are then plotted against total score.  An ideal item is one in which just one alternative (the correct one) has a monotonically increasing response probability.
</p>
<p>Because of the similar pattern of results for IRT based or simple sum based item scoring, the function can be run on scores calculated either by <code><a href="#topic+score.irt">score.irt</a></code> or by  <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code>. In the latter case, the number of breaks should not exceed the number of possible score alternatives.
</p>


<h3>Value</h3>

<p>Graphic output</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Revelle, W. An introduction to psychometric theory with applications in R (in prep) Springer. Draft chapters available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code>, <code><a href="#topic+score.irt">score.irt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(psychTools::iqitems)
iq.keys &lt;- c(4,4,4, 6,6,3,4,4,  5,2,2,4,  3,2,6,7)
scores &lt;- score.multiple.choice(iq.keys,psychTools::iqitems,score=TRUE,short=FALSE)
#note that for speed we can just do this on simple item counts rather
# than IRT based scores.
op &lt;- par(mfrow=c(2,2))  #set this to see the output for multiple items
irt.responses(scores$scores,psychTools::iqitems[1:4],breaks=11)
op &lt;-  par(op)
</code></pre>

<hr>
<h2 id='kaiser'>Apply the Kaiser normalization when rotating factors</h2><span id='topic+kaiser'></span>

<h3>Description</h3>

<p>Kaiser (1958) suggested normalizing factor loadings before rotating them, and then denormalizing them after rotation.  The GPArotation package does not (by default) normalize, nor does the <code><a href="#topic+fa">fa</a></code> function. Then, to make it more confusing, varimax in stats does,Varimax in GPArotation does not. <code><a href="#topic+kaiser">kaiser</a></code> will take the output of a non-normalized solution and report the normalized solution. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kaiser(f, rotate = "oblimin",m=4,pro.m=4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kaiser_+3A_f">f</code></td>
<td>
<p>A factor analysis output from <code><a href="#topic+fa">fa</a></code> or a factor loading matrix.
</p>
</td></tr>
<tr><td><code id="kaiser_+3A_rotate">rotate</code></td>
<td>
<p>Any of the standard rotations avaialable in the GPArotation package. </p>
</td></tr>
<tr><td><code id="kaiser_+3A_m">m</code></td>
<td>
<p>a parameter to pass to <code><a href="#topic+Promax">Promax</a></code>  </p>
</td></tr>
<tr><td><code id="kaiser_+3A_pro.m">pro.m</code></td>
<td>
<p>A redundant parameter, which is used to replace m in calls to Promax</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Best results if called from an unrotated solution.  Repeated calls using a rotated solution will produce incorrect estimates of the correlations between the factors.
</p>


<h3>Value</h3>

<p>See the values returned by GPArotation functions</p>


<h3>Note</h3>

<p>Prepared in response to a question about why <code><a href="#topic+fa">fa</a></code> oblimin results are different from SPSS.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Kaiser, H. F. (1958) The varimax criterion for analytic rotation in factor analysis. Psychometrika 23, 187-200.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+Promax">Promax</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f3 &lt;- fa(Thurstone,3)
f3n &lt;- kaiser(fa(Thurstone,3,rotate="none"))
f3p &lt;- kaiser(fa(Thurstone,3,rotate="none"),rotate="Promax",m=3)
factor.congruence(list(f3,f3n,f3p))
</code></pre>

<hr>
<h2 id='KMO'>Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy</h2><span id='topic+KMO'></span>

<h3>Description</h3>

<p>Henry Kaiser (1970) introduced an Measure of Sampling Adequacy (MSA) of factor analytic data matrices. Kaiser and Rice (1974) then modified it. This is just a function of the squared elements of the &lsquo;image&rsquo; matrix compared to the squares of the original correlations.  The overall MSA as well as estimates for each item are found. The index is known as the Kaiser-Meyer-Olkin (KMO) index.</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMO(r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMO_+3A_r">r</code></td>
<td>
<p>A correlation matrix or a data matrix (correlations will be found)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let  <code class="reqn">S^2 = diag(R^{-1})^{-1} </code> and <code class="reqn">Q = SR^{-1}S</code>.  Then Q is said to  be the anti-image intercorrelation matrix.  Let <code class="reqn">sumr2 = \sum{R^2}</code> and <code class="reqn">sumq2 = \sum{Q^2}</code> for all off diagonal elements of R and Q, then  <code class="reqn">SMA=sumr2/(sumr2 + sumq2)</code>.  Although originally MSA was 1 - sumq2/sumr2  (Kaiser, 1970), this was modified in Kaiser and Rice, (1974) to be   <code class="reqn">SMA=sumr2/(sumr2 + sumq2)</code>.  This is the formula used by Dziuban and Shirkey (1974) and by SPSS.
</p>
<p>In his delightfully flamboyant style, Kaiser (1975)
suggested that KMO &gt; .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and  less than .5, unacceptable.
</p>
<p>An alternative measure of whether the matrix is factorable is the Bartlett test  <code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code> which tests the degree that the matrix deviates from an identity matrix.
</p>


<h3>Value</h3>


<ul>
<li><p> MSA: The overall Measure of Sampling Adequacy
</p>
</li>
<li><p> MSAi:  The measure of sampling adequacy for each item
</p>
</li>
<li><p> Image: The Image correlation matrix (Q)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>H.~F. Kaiser. (1970) A second generation little jiffy.
Psychometrika, 35(4):401&ndash;415.
</p>
<p>H.~F. Kaiser and J.~Rice. (1974) Little jiffy, mark iv.
Educational and Psychological Measurement, 34(1):111&ndash;117.
</p>
<p>H.F. Kaiser. 1974) An index of factor simplicity.  Psychometrika,  39 (1) 31-36.
</p>
<p>Dziuban, Charles D. and Shirkey, Edwin C. (1974) When is a correlation matrix appropriate for factor analysis? Some decision rules. Psychological Bulletin, 81 (6) 358 - 361.
</p>


<h3>See Also</h3>

<p> See Also as <code><a href="#topic+fa">fa</a></code>,  <code><a href="#topic+cortest.bartlett">cortest.bartlett</a></code>, <code><a href="#topic+Harman.political">Harman.political</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>KMO(Thurstone)
KMO(Harman.political)   #compare to the results in Dziuban and Shirkey (1974)

</code></pre>

<hr>
<h2 id='lmCor'>Multiple Regression, Canonical and Set Correlation from matrix or raw input</h2><span id='topic+setCor'></span><span id='topic+lmCor'></span><span id='topic+lmCor.diagram'></span><span id='topic+lmDiagram'></span><span id='topic+set.cor'></span><span id='topic+mat.regress'></span><span id='topic+matReg'></span><span id='topic+crossValidation'></span><span id='topic+crossValidationBoot'></span><span id='topic+matPlot'></span>

<h3>Description</h3>

<p>A generalization of the lm function to correlation/covariance matrix input. Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.  The model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data will be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets. Associated functions allow for cross validation of these regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmCor(y,x,data,z=NULL,n.obs=NULL,use="pairwise",std=TRUE,square=FALSE,
       main="Regression Models",plot=TRUE,show=FALSE,zero=TRUE, alpha = .05,part=FALSE)
 #the prior name    
setCor(y,x,data,z=NULL,n.obs=NULL,use="pairwise",std=TRUE,square=FALSE,
       main="Regression Models",plot=TRUE,show=FALSE,zero=TRUE, alpha = .05,part=FALSE)
       
lmDiagram(sc,main="Regression model",digits=2,show=FALSE,cex=1,l.cex=1,...)    

lmCor.diagram(sc,main="Regression model",digits=2,show=FALSE,cex=1,l.cex=1,...)
#an alias to setCor or lmCor

set.cor(y,x,data,z=NULL,n.obs=NULL,use="pairwise",std=TRUE,square=FALSE,
 main="Regression Models",plot=TRUE,show=FALSE,zero=TRUE,part=FALSE)   
  
mat.regress(y, x,data, z=NULL,n.obs=NULL,use="pairwise",square=FALSE) #the old form

#does not handle formula input
matReg(x,y,C,m=NULL,z=NULL,n.obs=0,means=NULL,std=FALSE,raw=TRUE,part=FALSE) 

#useful helper functions
crossValidation(model,data,options=NULL,select=NULL)
crossValidationBoot(y,x,data,n.iter=100)

matPlot(x, type = "b", minlength=6, xlas=0,legend=NULL,
      lab=NULL,pch=16,col=1:6,lty=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmCor_+3A_y">y</code></td>
<td>
<p>Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c(&quot;Flags&quot;,&quot;Addition&quot;). See notes and examples for each.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_x">x</code></td>
<td>
<p> either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c(&quot;Cubes&quot;,&quot;PaperFormBoard&quot;).  x and y may also be set by use of the formula style of lm.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_data">data</code></td>
<td>
<p>A matrix or data.frame of correlations or, if not square, of raw data. Missing values are allowed (see use)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_c">C</code></td>
<td>
<p>A variance/covariance matrix, or a correlation matrix</p>
</td></tr>
<tr><td><code id="lmCor_+3A_m">m</code></td>
<td>
<p>The column name or numbers of the set of mediating variables (see <code><a href="#topic+mediate">mediate</a></code>).</p>
</td></tr>
<tr><td><code id="lmCor_+3A_z">z</code></td>
<td>
<p>the column names or numbers of the set of covariates. </p>
</td></tr>
<tr><td><code id="lmCor_+3A_n.obs">n.obs</code></td>
<td>
<p>If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of bootstrap resamples to take in crossValidationBoot</p>
</td></tr>
<tr><td><code id="lmCor_+3A_use">use</code></td>
<td>
<p>Find the correlations using &quot;pairwise&quot; (default) or just use &quot;complete&quot; cases (to match the lm function)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_std">std</code></td>
<td>
<p>Report standardized betas (based upon the correlations) or raw bs (based upon covariances)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_part">part</code></td>
<td>
<p>z is specified should part (TRUE) or partial (default) correlations be found.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_raw">raw</code></td>
<td>
<p>Are data from a correlation matrix or data matrix?</p>
</td></tr>
<tr><td><code id="lmCor_+3A_means">means</code></td>
<td>
<p>A vector of means for the data in matReg if giving matrix input</p>
</td></tr>
<tr><td><code id="lmCor_+3A_main">main</code></td>
<td>
<p>The title for lmCor.diagram</p>
</td></tr>
<tr><td><code id="lmCor_+3A_square">square</code></td>
<td>
<p>if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_sc">sc</code></td>
<td>
<p>The output of lmCor or setCor may be used for drawing diagrams</p>
</td></tr>
<tr><td><code id="lmCor_+3A_digits">digits</code></td>
<td>
<p>How many digits should be displayed in the lmCor.diagram?</p>
</td></tr>
<tr><td><code id="lmCor_+3A_show">show</code></td>
<td>
<p>Show the unweighted matrix correlation between the x and y sets?</p>
</td></tr>
<tr><td><code id="lmCor_+3A_zero">zero</code></td>
<td>
<p>zero center the x variables before finding the interaction terms.</p>
</td></tr>
<tr><td><code id="lmCor_+3A_alpha">alpha</code></td>
<td>
<p>p value of the confidence intervals for the beta coefficients</p>
</td></tr>
<tr><td><code id="lmCor_+3A_plot">plot</code></td>
<td>
<p>By default, lmCor makes a plot of the results, set to FALSE to suppress the plot</p>
</td></tr>
<tr><td><code id="lmCor_+3A_cex">cex</code></td>
<td>
<p>Text size of boxes displaying the variables in the diagram</p>
</td></tr>
<tr><td><code id="lmCor_+3A_l.cex">l.cex</code></td>
<td>
<p>Text size of numbers in arrows, defaults to cex</p>
</td></tr>
<tr><td><code id="lmCor_+3A_...">...</code></td>
<td>
<p>Additional graphical parameters for lmCor.diagram</p>
</td></tr>
<tr><td><code id="lmCor_+3A_model">model</code></td>
<td>
<p>The resulting object from lmCor or bestScales</p>
</td></tr>
<tr><td><code id="lmCor_+3A_options">options</code></td>
<td>
<p>If using a bestScales object, which set of keys to use (&quot;best.key&quot;,&quot;weights&quot;,&quot;optimal.key&quot;,&quot;optimal.weights&quot;)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_select">select</code></td>
<td>
<p>Not yet implemented option to crossValidation</p>
</td></tr>
<tr><td><code id="lmCor_+3A_type">type</code></td>
<td>
<p>type=&quot;b&quot; draws both lines and points</p>
</td></tr>
<tr><td><code id="lmCor_+3A_xlas">xlas</code></td>
<td>
<p>Orientation of the x labels (3 to make vertical)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_minlength">minlength</code></td>
<td>
<p>When abbreviating x labels how many characters as a minimum</p>
</td></tr>
<tr><td><code id="lmCor_+3A_legend">legend</code></td>
<td>
<p>If not NULL, the draw a legend at the appropriate location</p>
</td></tr>
<tr><td><code id="lmCor_+3A_pch">pch</code></td>
<td>
<p>What plot character(s) to use in matPlot (defaults to 18.  should be less than 26 -nvar)</p>
</td></tr>
<tr><td><code id="lmCor_+3A_col">col</code></td>
<td>
<p>colors to use in matPlot</p>
</td></tr>
<tr><td><code id="lmCor_+3A_lty">lty</code></td>
<td>
<p>line types to use in matPlot</p>
</td></tr>
<tr><td><code id="lmCor_+3A_lab">lab</code></td>
<td>
<p>What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.
</p>
<p>Input is either conventional formula mode, or the set of y variables and the set of x variables.  Formula mode  can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of <code><a href="stats.html#topic+lm">lm</a></code> or the results discussed in Hayes (2013).  
</p>
<p>Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the  examples.)  
</p>
<p>If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. 
</p>
<p>The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.  Since 2.3.12, the canonical loadings for the X and Y variables are returned (Xmat,YMat).
</p>
<p>An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.
</p>
<p>Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canonicial correlation (Hotelling, 1936) and <code class="reqn">1 - \prod(1-\rho_i^2)</code> where <code class="reqn">\rho_i^2</code> is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). 
</p>
<p>R2 between two sets is just </p>
<p style="text-align: center;"><code class="reqn">R^2 = 1- \frac{\left | R_{yx} \right |}{\left | R_y \right | \left |R_x\right |}  =  1 - \prod(1-\rho_i^2)   </code>
</p>
<p> where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.
</p>
<p>Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: &quot;In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association.&quot; ( p613). 
</p>
<p>Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, 
</p>
<p style="text-align: center;"><code class="reqn">R_{uw}  =\frac{ 1 R_{xy} 1' }{(1R_{yy}1')^{.5} (1R_{xx}1')^{.5}}    </code>
</p>
<p> where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. 
</p>
<p>When finding the unweighted correlations, as is done in <code><a href="#topic+alpha">alpha</a></code>, items are flipped so that they all are positively signed.  
</p>
<p>A typical use in the SAPA project is to form item composites by clustering or factoring (see  <code><a href="#topic+fa">fa</a></code>,<code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+principal">principal</a></code>), extract the clusters from these results (<code><a href="#topic+factor2cluster">factor2cluster</a></code>), and then form the composite correlation matrix using <code><a href="#topic+cluster.cor">cluster.cor</a></code>.  The variables in this reduced matrix may then be used in multiple R procedures using <code><a href="#topic+lmCor">lmCor</a></code>.
</p>
<p>Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.
</p>
<p>If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.
</p>
<p>If the input is rectangular (not square), correlations or covariances are found from the data. By default, these are pairwise complete correlations. This may be specified in the 'use' option.
</p>
<p>The print function reports t and p values for the beta weights, the summary function just reports the beta weights.
</p>
<p>The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF &gt; 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).
</p>
<p>The Guide and Ketokivi article is well worth reading for all who want to use various regression models. 
</p>
<p><code><a href="#topic+anova.psych">anova.psych</a></code> allows for comparisons between alternative models. 
</p>
<p><code><a href="#topic+crossValidation">crossValidation</a></code> can be used to take the results from <code><a href="#topic+lmCor">lmCor</a></code> or <code><a href="#topic+bestScales">bestScales</a></code> and apply the weights to a different data set.
</p>
<p><code><a href="#topic+crossValidationBoot">crossValidationBoot</a></code> will bootstrap resample data and perform a lmCor and then return the crossValidation R values. Although it will handle covariates (z), it will not handle product terms.  
</p>
<p><code><a href="#topic+matPlot">matPlot</a></code> can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.
</p>
<p><code><a href="#topic+lmCorLookup">lmCorLookup</a></code> will sort the beta weights and report them with item contents if given a dictionary. 
</p>
<p><code><a href="#topic+matReg">matReg</a></code> is primarily a helper function for <code><a href="#topic+mediate">mediate</a></code> but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.
<code><a href="#topic+matReg">matReg</a></code> does not work on data matrices, nor does it take formula input.  It is really just a helper function for  <code><a href="#topic+mediate">mediate</a></code>
</p>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>the beta weights for each variable in X for each variable in Y</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The multiple R for each equation (the amount of change a unit in the predictor set leads to in the criterion set). </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>The multiple R2 (% variance acounted for) for each equation</p>
</td></tr>
<tr><td><code>VIF</code></td>
<td>
<p>The Variance Inflation Factor which is just 1/(1-smc(x))</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>Standard errors of beta weights (if n.obs is specified)</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>t value of beta weights (if n.obs is specified)</p>
</td></tr>
<tr><td><code>Probability</code></td>
<td>
<p>Probability of beta = 0 (if n.obs is specified)</p>
</td></tr>
<tr><td><code>shrunkenR2</code></td>
<td>
<p>Estimated shrunken R2 (if n.obs is specified)</p>
</td></tr>
<tr><td><code>setR2</code></td>
<td>
<p>The multiple R2 of the set correlation between the x and y sets</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>The residual correlation matrix of Y with x and z removed</p>
</td></tr>
<tr><td><code>ruw</code></td>
<td>
<p>The unit weighted multiple correlation for each dependent variable</p>
</td></tr>
<tr><td><code>Ruw</code></td>
<td>
<p>The unit weighted set correlation</p>
</td></tr>
<tr><td><code>cancor</code></td>
<td>
<p>The canonical correlations between the X and Y set.</p>
</td></tr> 
<tr><td><code>Xmat</code></td>
<td>
<p>The path coefficients from the X variables to the canonical variables.</p>
</td></tr>
<tr><td><code>Ymat</code></td>
<td>
<p>The path coefficients from the Y variables to the canonical variables.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>As of April 30, 2011, the order of x and y was swapped in the call to be consistent with the general y ~ x  syntax of the lm and aov functions. In addition, the primary name of the function was switched to setCor from mat.regress to reflect the estimation of the set correlation.  This was changed to lmCor in the March, 2023 release to make the equivalence to lm more obvious.
</p>
<p>In October, 2017 I added the ability to specify the input in formula mode and allow for higher level and multiple interactions. 
</p>
<p>The denominator degrees of freedom for the set correlation does not match that reported by Cohen et al., 2003 in the example on page 621 but does match the formula on page 615, except for the typo in the estimation of F (see Cohen 1982).   The difference seems to be that they are adding in a correction factor of df 2 = df2 + df1. 
</p>
<p>Following a suggestion by Keith Widaman, when zero centering for product terms, the y variables are no longer zero centered. This puts the regression in the units of the DV. (2/22/22). 
</p>
<p>In December, 2023 I added the Xmat and Ymat coefficients for those who want to examine canonical correlations. This follows the example in Tabachnick and Fidell.  Also added <code><a href="#topic+cancorDiagram">cancorDiagram</a></code> to show the canonical path coefficients. 
</p>


<h3>Author(s)</h3>

<p>William Revelle <br />
</p>
<p>Maintainer: William Revelle &lt;revelle@northwestern.edu&gt; </p>


<h3>References</h3>

<p>J. Cohen (1982)  Set correlation as a general multivariate data-analytic method. Multivariate Behavioral Research, 17(3):301-341.
</p>
<p>J. Cohen, P. Cohen, S.G. West, and L.S. Aiken. (2003) Applied multiple regression/correlation analysis for the behavioral sciences. L. Erlbaum Associates, Mahwah, N.J., 3rd ed edition.
</p>
<p>H. Hotelling. (1936) Relations between two sets of variates. Biometrika 28(3/4):321-377.
</p>
<p>E.Cramer and W. A. Nicewander (1979) Some symmetric, invariant measures of multivariate association. Psychometrika, 44:43-54.
</p>
<p>V. Daniel R. Guide Jr. and M. Ketokivim (2015) Notes from the Editors: Redefining some methodological criteria for the journal.  Journal of Operations Management. 37. v-viii.
</p>
<p>B.G. Tabacnick and L.S. Fidell (2007) Using Multivariate Statistics. Allyn and Bacon.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mediate">mediate</a></code> for an alternative regression model with 'mediation'.  <code><a href="stats.html#topic+predict">predict</a></code> to find predicted values given regression weights. 
<code><a href="#topic+partial.r">partial.r</a></code> for partial and  part correlations. 
<code><a href="#topic+cluster.cor">cluster.cor</a></code>, <code><a href="#topic+factor2cluster">factor2cluster</a></code>,<code><a href="#topic+principal">principal</a></code>,<code><a href="#topic+ICLUST">ICLUST</a></code>, <code>link{cancor}</code> and cca in the yacca package. 
<code><a href="#topic+GSBE">GSBE</a></code> for further demonstrations of mediation and moderation. </p>


<h3>Examples</h3>

<pre><code class='language-R'>#First compare to lm using data input
summary(lm(rating ~ complaints + privileges, data = attitude))
lmCor(rating ~ complaints + privileges, data = attitude, std=FALSE) #do not standardize
z.attitude &lt;- data.frame(scale(attitude))  #standardize the data before doing lm
summary(lm(rating ~ complaints + privileges, data = z.attitude))  #regressions on z scores
lmCor(rating ~ complaints + privileges, data = attitude)  #by default we standardize and 
# the results are the same as the standardized lm


R &lt;- cor(attitude) #find the correlations
#Do the regression on the correlations  
#Note that these match the regressions on the standard scores of the data
lmCor(rating ~ complaints + privileges, data =R, n.obs=30)

#now, partial out learning and critical
lmCor(rating ~ complaints + privileges - learning - critical, data =R, n.obs=30)
#compare with the full regression:
lmCor(rating ~ complaints + privileges + learning + critical, data =R, n.obs=30)



#Canonical correlations:

#The first Kelley data set from Hotelling
kelley1 &lt;- structure(c(1, 0.6328, 0.2412, 0.0586, 0.6328, 1, -0.0553, 0.0655, 
0.2412, -0.0553, 1, 0.4248, 0.0586, 0.0655, 0.4248, 1), .Dim = c(4L, 
4L), .Dimnames = list(c("reading.speed", "reading.power", "math.speed", 
"math.power"), c("reading.speed", "reading.power", "math.speed", 
"math.power")))
lowerMat(kelley1)
mod1 &lt;- lmCor(y = math.speed + math.power ~ reading.speed + reading.power, 
    data = kelley1, n.obs=140)
mod1$cancor
#Hotelling reports .3945 and .0688  we get  0.39450592 0.06884787

#the second Kelley data from Hotelling
kelley &lt;- structure(list(speed = c(1, 0.4248, 0.042, 0.0215, 0.0573), power = c(0.4248, 
1, 0.1487, 0.2489, 0.2843), words = c(0.042, 0.1487, 1, 0.6693, 
0.4662), symbols = c(0.0215, 0.2489, 0.6693, 1, 0.6915), meaningless = c(0.0573, 
0.2843, 0.4662, 0.6915, 1)), .Names = c("speed", "power", "words", 
"symbols", "meaningless"), class = "data.frame", row.names = c("speed", 
"power", "words", "symbols", "meaningless"))

lowerMat(kelley)

mod2 &lt;- lmCor(power + speed ~ words + symbols + meaningless,data=kelley)  #formula mode
#lmCor(y= 1:2,x = 3:5,data = kelley) #order of variables input
cancorDiagram(mod2, cut=.05)
#Hotelling reports canonical correlations of .3073 and .0583  or squared correlations of
# 0.09443329 and 0.00339889 vs. our values of cancor = 0.3076 0.0593  with squared values
#of  0.0946 0.0035,

lmCor(y=c(7:9),x=c(1:6),data=Thurstone,n.obs=213)  #easier to just list variable  
                                              #locations if we have long names
#now try partialling out some variables
lmCor(y=c(7:9),x=c(1:3),z=c(4:6),data=Thurstone) #compare with the previous
#compare complete print out with summary printing 
sc &lt;- lmCor(SATV + SATQ ~ gender + education,data=sat.act) # regression from raw data
sc
summary(sc) 

lmCor(Pedigrees ~ Sentences + Vocabulary - First.Letters - Four.Letter.Words ,
data=Thurstone)  #showing formula input with two covariates

#Do some regressions with real data (rather than correlation matrices)
lmCor(reaction ~ cond + pmi + import, data = Tal.Or)

#partial out importance
lmCor(reaction ~ cond + pmi - import, data = Tal.Or, main="Partial out importance")

#compare with using lm by partialling
mod1 &lt;- lm(reaction ~ cond + pmi + import, data = Tal.Or)
reaction.import &lt;- lm(reaction~import,data=Tal.Or)$resid
cond.import &lt;-  lm(cond~import,data=Tal.Or)$resid
pmi.import &lt;-   lm(pmi~import,data=Tal.Or)$resid
mod.partial &lt;- lm(reaction.import ~ cond.import + pmi.import)
summary(mod.partial)
#lm uses raw scores, so set std = FALSE for lmCor
print(lmCor(y = reaction ~ cond + pmi - import, data = Tal.Or,std = FALSE,
 main = "Partial out importance"),digits=4)
#notice that the dfs of the partial approach using lm are 1 more than the lmCor dfs 
 
 #Show how to find quadratic terms
sc &lt;- lmCor(reaction ~ cond + pmi + I(import^2), data = Tal.Or)
sc
#pairs.panels(sc$data) #show the SPLOM of the data

#Consider an example of a derivation and cross validation sample
set.seed(42)
ss &lt;- sample(1:2800,1400)
model &lt;- lmCor(y=26:28,x=1:25,data=bfi[ss,],plot=FALSE)
original.fit &lt;- crossValidation(model,bfi[ss,]) #the derivation set
cross.fit &lt;- crossValidation(model,bfi[-ss,])  #the cross validation set
summary(original.fit)
summary(cross.fit)
predicted &lt;- predict(model,bfi[-ss,])
cor2(predicted,bfi[-ss,26:28])  #these are the correlations of the predicted with observed

#now do it for the correlations matrices - these should be the same
R1 &lt;- lowerCor(bfi[ss,], show=FALSE)
R2 &lt;- lowerCor(bfi[-ss,], show=FALSE)
mod1 &lt;- lmCor(y=26:28,x=1:25,data=R1,plot=FALSE)
original &lt;-  crossValidation(model,R1) #the derivation set capitalizes on chance
cross &lt;- crossValidation(model,R2)  #the cross validation set -- values are lower
summary(original)  #inflated by chance
summary(cross)    #cross validated values are better estimates
</code></pre>

<hr>
<h2 id='logistic'>Logistic transform from x to p and logit transform from p to x</h2><span id='topic+logistic'></span><span id='topic+logit'></span><span id='topic+logistic.grm'></span>

<h3>Description</h3>

<p>The logistic function (1/(1+exp(-x)) and logit function (log(p/(1-p)) are fundamental to Item Response Theory.  Although just one line functions, they are included here for ease of demonstrations and in drawing IRT models. Also included is the logistic.grm for a graded response model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic(x,d=0, a=1,c=0, z=1)
logit(p)
logistic.grm( x,d=0,a=1.5,c=0,z=1,r=2,s=c(-1.5,-.5,.5,1.5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logistic_+3A_x">x</code></td>
<td>
<p>Any integer or real value </p>
</td></tr>
<tr><td><code id="logistic_+3A_d">d</code></td>
<td>
<p>Item difficulty or delta parameter </p>
</td></tr>
<tr><td><code id="logistic_+3A_a">a</code></td>
<td>
<p>The slope of the curve at x=0  is equivalent to the discrimination parameter in 2PL models or alpha parameter. Is either 1  in 1PL or  1.702 in 1PN approximations.  </p>
</td></tr>
<tr><td><code id="logistic_+3A_c">c</code></td>
<td>
<p>Lower asymptote  =  guessing parameter in 3PL models or gamma </p>
</td></tr>
<tr><td><code id="logistic_+3A_z">z</code></td>
<td>
<p>The upper asymptote  &mdash; in 4PL models</p>
</td></tr> 
<tr><td><code id="logistic_+3A_p">p</code></td>
<td>
<p>Probability to be converted to logit value</p>
</td></tr>
<tr><td><code id="logistic_+3A_r">r</code></td>
<td>
<p>The response category for the graded response model</p>
</td></tr>
<tr><td><code id="logistic_+3A_s">s</code></td>
<td>
<p>The response thresholds</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>These three functions are provided as simple helper functions for demonstrations of Item Response Theory. The one parameter logistic (1PL) model is also known as the Rasch model.  It assumes items differ only in difficulty.
1PL, 2PL, 3PL and 4PL curves may be drawn by choosing the appropriate d (delta or item difficulty), a (discrimination or slope), c (gamma or  guessing) and z (zeta or upper asymptote).
</p>
<p>logit is just the inverse of logistic.
</p>
<p>logistic.grm will create the responses for a graded response model for the rth category where cutpoints are in s.
</p>


<h3>Value</h3>

<table>
<tr><td><code>p</code></td>
<td>
<p>logistic returns the probability associated with x</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>logit returns the real number associated with p</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>curve(logistic(x,a=1.702),-3,3,ylab="Probability of x",
 main="Logistic transform of x",xlab="z score units") 
 #logistic with a=1.702 is almost the same as pnorm 
 
curve(pnorm(x),add=TRUE,lty="dashed")  
curve(logistic(x),add=TRUE)
text(2,.8, expression(alpha ==1))
text(2,1.0,expression(alpha==1.7))
curve(logistic(x),-4,4,ylab="Probability of x",
            main = "Logistic transform of x in logit units",xlab="logits")
curve(logistic(x,d=-1),add=TRUE)
curve(logistic(x,d=1),add=TRUE)
curve(logistic(x,c=.2),add=TRUE,lty="dashed")
text(1.3,.5,"d=1")
text(.3,.5,"d=0")
text(-1.5,.5,"d=-1")
text(-3,.3,"c=.2")
#demo of graded response model
 curve(logistic.grm(x,r=1),-4,4,ylim=c(0,1),main="Five level response scale",
           ylab="Probability of endorsement",xlab="Latent attribute on logit scale")
 curve(logistic.grm(x,r=2),add=TRUE)
 curve(logistic.grm(x,r=3),add=TRUE)
 curve(logistic.grm(x,r=4),add=TRUE)
 curve(logistic.grm(x,r=5),add=TRUE)
 
 text(-2.,.5,1)
 text(-1.,.4,2)
 text(0,.4,3)
 text(1.,.4,4)
  text(2.,.4,5)
</code></pre>

<hr>
<h2 id='lowerUpper'>Combine two square matrices to have a lower off diagonal for one, upper off diagonal for the other</h2><span id='topic+lowerUpper'></span>

<h3>Description</h3>

<p>When reporting correlation matrices for two samples (e.g., males and females), it is convenient to show them as one matrix, with entries below the diagonal representing one matrix, and entries above the diagonal the other matrix.  It is also useful to compare a correlation matrix with the residuals from a fitted (e.g., factor) model.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>lowerUpper(lower, upper=NULL, diff=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lowerUpper_+3A_lower">lower</code></td>
<td>
<p>A square matrix</p>
</td></tr>
<tr><td><code id="lowerUpper_+3A_upper">upper</code></td>
<td>
<p>A square matrix of the same size as the first (if omitted, then the matrix is converted to two symmetric matrices).</p>
</td></tr>
<tr><td><code id="lowerUpper_+3A_diff">diff</code></td>
<td>
<p>Find the difference between the first and second matrix and put the results in the above the diagonal entries.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>If just one matrix is provided (i.e., upper is missing), it is decomposed into two square matrices, one equal to the lower off diagonal entries, the other to the upper off diagonal entries. In the normal case two symmetric matrices are provided and combined into one non-symmetric matrix with the lower off diagonals representing the lower matrix and the upper off diagonals representing the upper matrix.
</p>
<p>If diff is true, the upper off diagonal matrix reflects the differences between the two matrices.
</p>


<h3>Value</h3>

<p>Either one matrix or a list of two</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="psychTools.html#topic+read.clipboard.lower">read.clipboard.lower</a></code>, <code><a href="#topic+corPlot">corPlot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> b1 &lt;- Bechtoldt.1
 b2 &lt;- Bechtoldt.2
 b12 &lt;- lowerUpper(b1,b2)
 cor.plot(b12)
 diff12 &lt;- lowerUpper(b1,b2,diff=TRUE)

 corPlot(t(diff12),numbers=TRUE,main="Bechtoldt1 and the differences from Bechtoldt2")
 
 #Compare r and partial r
 lower &lt;- lowerCor(sat.act)
 upper &lt;- partial.r(sat.act)
 both = lowerUpper(lower,upper)
 corPlot(both,numbers=TRUE,main="r and partial r for the sat.act data set")
#now show the differences
 both = lowerUpper(lower,upper,diff=TRUE)
 corPlot(both,numbers=TRUE,main="Differences between r and partial r for the sat.act data set")
</code></pre>

<hr>
<h2 id='make.keys'> Create a keys matrix for use by score.items or cluster.cor</h2><span id='topic+make.keys'></span><span id='topic+keys2list'></span><span id='topic+selectFromKeys'></span><span id='topic+makePositiveKeys'></span>

<h3>Description</h3>

<p> When scoring items by forming composite scales either from the raw data using <code><a href="#topic+scoreItems">scoreItems</a></code> or from the correlation matrix using <code><a href="#topic+cluster.cor">cluster.cor</a></code>, it used to be  necessary to create a keys matrix. This is no longer necessary as most of the scoring functions will directly use a keys list. <code><a href="#topic+make.keys">make.keys</a></code>  is just a short cut for creating a keys matrix.  The keys matrix is a nvar x nscales matrix of -1,0, 1 and defines the membership for each scale. Items can be specified by location or by name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.keys(nvars, keys.list, item.labels = NULL, key.labels = NULL)
keys2list(keys,sign=TRUE)
selectFromKeys(keys.list)
makePositiveKeys(keys.list,sign=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.keys_+3A_nvars">nvars</code></td>
<td>
<p>Number of variables items to be scored, or the name of the data.frame/matrix to be scored</p>
</td></tr>
<tr><td><code id="make.keys_+3A_keys.list">keys.list</code></td>
<td>
<p> A list of the scoring keys, one element for each scale</p>
</td></tr>
<tr><td><code id="make.keys_+3A_item.labels">item.labels</code></td>
<td>
<p> Typically, just the colnames of the items data matrix. </p>
</td></tr>
<tr><td><code id="make.keys_+3A_key.labels">key.labels</code></td>
<td>
<p> Labels for the scales can be specified here, or in the key.list </p>
</td></tr>
<tr><td><code id="make.keys_+3A_keys">keys</code></td>
<td>
<p>A keys matrix returned from make.keys</p>
</td></tr>
<tr><td><code id="make.keys_+3A_sign">sign</code></td>
<td>
<p>if TRUE, prefix negatively keyed items with - (e.g., &ldquo;-E2&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The easiest way to prepare keys for <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>, <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>, or <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code>  is to specify a keys.list.  This is just a list specifying the name of the scales to be scores and the direction of the items to be used.
</p>
<p>In earlier versions (prior to 1.6.9) keys were formed as a matrix of -1, 0, and 1s for all the items using make.keys.  This is no longer necessary, but make.keys is kept for compatibility with earlier versions.
</p>
<p>There are three ways to create keys for the <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>, <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>, or <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> functions. One is to laboriously do it in a spreadsheet and then copy them into R.  The other is to just specify them by item number in a list. <code><a href="#topic+make.keys">make.keys</a></code> allows one to specify items by name or by location or a mixture of both.
</p>
<p><code><a href="#topic+keys2list">keys2list</a></code> reverses the <code><a href="#topic+make.keys">make.keys</a></code> process and returns a list of scoring keys with the item names for each  item to be keyed.  If sign=FALSE, this is just a list of the items to be scored. (Useful for <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code>
</p>
<p><code><a href="#topic+selectFromKeys">selectFromKeys</a></code> will strip the signs from a keys.list and create a vector of item names (deleting duplicates) associated with those keys.  This is useful if using a keys.list to define scales and then just selecting those items that are in subset of the keys.list.  This is now done in the scoring functions in the interest of speed. 
</p>
<p>Since these scoring functions <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>, <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>, or <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> can now (&gt; version 1.6.9)  just take a keys.list as input, make.keys is not as important, but is kept for documentation purposes.
</p>
<p>To address items by name it is necessary to specify item names, either by using the item.labels value, or by putting the name of the data file or the colnames of the data file to be scored into the first (nvars) position.
</p>
<p>If specifying by number (location), then nvars is the total number of items in the object to be scored, not just the number of items used.
</p>
<p>See the examples for the various options.
</p>
<p>Note that make.keys was revised in Sept, 2013 to allow for keying by name.
</p>
<p>It is also possible to do several make.keys operations and then combine them using <code><a href="#topic+superMatrix">superMatrix</a></code>.  The alternative, if using the keys.list features is just to concatenate them. 
</p>
<p>makePositiveKeys is useful for taking subsets of keys (e.g. from <code><a href="#topic+bestScales">bestScales</a></code> )and create separate keys for the positively and negatively keyed items.
</p>


<h3>Value</h3>

<table>
<tr><td><code>keys</code></td>
<td>
<p>a nvars x nkeys matrix of -1, 0, or 1s describing how to score each scale. nkeys is the length of the keys.list</p>
</td></tr>
</table>


<h3>See Also</h3>

 <p><code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code> <code><a href="#topic+superMatrix">superMatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(attitude)  #specify the items by location
 key.list &lt;- list(all=c(1,2,3,4,-5,6,7),
                  first=c(1,2,3),
                  last=c(4,5,6,7))
 keys &lt;- make.keys(7,key.list,item.labels = colnames(attitude))
 keys
 #now, undo this 
new.keys.list &lt;- keys2list(keys)  #note, these are now given the variable names

select &lt;- selectFromKeys(key.list)

 
 #scores &lt;- score.items(keys,attitude)
 #scores
 
# data(psychTools::bfi)
 #first create the keys by location (the conventional way)
 keys.list &lt;- list(agree=c(-1,2:5),conscientious=c(6:8,-9,-10),
 extraversion=c(-11,-12,13:15),neuroticism=c(16:20),openness = c(21,-22,23,24,-25))   
 keys &lt;- make.keys(25,keys.list,item.labels=colnames(psychTools::bfi)[1:25])
 new.keys.list &lt;- keys2list(keys)  #these will be in the form of variable names
 
 #alternatively, create by a mixture of names and locations 
 keys.list &lt;- list(agree=c("-A1","A2","A3","A4","A5"),
conscientious=c("C1","C2","C2","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"),
neuroticism=c(16:20),openness = c(21,-22,23,24,-25)) 
keys &lt;- make.keys(psychTools::bfi, keys.list) #specify the data file to be scored (bfi)
#or
keys &lt;- make.keys(colnames(psychTools::bfi),keys.list) #specify the names of the variables 
#to be used
#or
#specify the number of variables to be scored and their names in all cases
keys &lt;- make.keys(28,keys.list,colnames(psychTools::bfi)) 


 scores &lt;- scoreItems(keys,psychTools::bfi)
 summary(scores)

</code></pre>

<hr>
<h2 id='manhattan'>&quot;Manhattan&quot; plots of correlations with a set of criteria.</h2><span id='topic+manhattan'></span>

<h3>Description</h3>

<p>A useful way of showing the strength of many correlations with a particular criterion is the Manhattan plot.  This is just a plot of correlations ordered by some keying variable.  Useful to understand the basis of items used in <code><a href="#topic+bestScales">bestScales</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manhattan(x, criteria = NULL, keys = NULL,raw=TRUE,n.obs=NULL, abs = TRUE, 
  ylab = NULL, labels = NULL, log.p = FALSE,ci=.05, pch = 21,
 main = "Manhattan Plot of", adjust="holm",ylim = NULL,digits=2,dictionary=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manhattan_+3A_x">x</code></td>
<td>
<p>A matrix or data.frame of items or a correlation matrix.</p>
</td></tr>
<tr><td><code id="manhattan_+3A_criteria">criteria</code></td>
<td>
<p>What column names should be predicted.  If a separate file, what are the variables to predict.</p>
</td></tr>
<tr><td><code id="manhattan_+3A_keys">keys</code></td>
<td>
<p>a keys.list similar to that used in <code><a href="#topic+scoreItems">scoreItems</a></code>  </p>
</td></tr>
<tr><td><code id="manhattan_+3A_raw">raw</code></td>
<td>
<p>The default is raw data, the alternative is a correlation matrix</p>
</td></tr>
<tr><td><code id="manhattan_+3A_n.obs">n.obs</code></td>
<td>
<p>If given a correlation matrix, and showing log.p, we need the number of observations</p>
</td></tr>
<tr><td><code id="manhattan_+3A_abs">abs</code></td>
<td>
<p>Should we show the absolute value of the correlations.</p>
</td></tr>
<tr><td><code id="manhattan_+3A_ylab">ylab</code></td>
<td>
<p>If NULL, will label as either correlations or log (10) of correlations</p>
</td></tr>
<tr><td><code id="manhattan_+3A_labels">labels</code></td>
<td>
<p>if NULL, will use the names of the keys</p>
</td></tr>
<tr><td><code id="manhattan_+3A_log.p">log.p</code></td>
<td>
<p>Should we show the correlations (log.p = FALSE) or the log of the probabilities of the correlations (TRUE)</p>
</td></tr>
<tr><td><code id="manhattan_+3A_ci">ci</code></td>
<td>
<p>The probability for the upper and lower confidence intervals &ndash; bonferroni adjusted</p>
</td></tr>
<tr><td><code id="manhattan_+3A_pch">pch</code></td>
<td>
<p>The default plot chararcter is a filled circle</p>
</td></tr>
<tr><td><code id="manhattan_+3A_main">main</code></td>
<td>
<p>The title for each criterion</p>
</td></tr>
<tr><td><code id="manhattan_+3A_adjust">adjust</code></td>
<td>
<p>Which adjustment for multiple correlations should be applied (&quot;holm&quot;, &quot;bonferroni&quot;, &quot;none&quot;)</p>
</td></tr>
<tr><td><code id="manhattan_+3A_ylim">ylim</code></td>
<td>
<p>If NULL will be the min and max of the data</p>
</td></tr>
<tr><td><code id="manhattan_+3A_digits">digits</code></td>
<td>
<p>Round off the results to digits</p>
</td></tr>
<tr><td><code id="manhattan_+3A_dictionary">dictionary</code></td>
<td>
<p>A dictionary of items</p>
</td></tr>
<tr><td><code id="manhattan_+3A_...">...</code></td>
<td>
<p>Other graphic parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When exploring the correlations of many items with a few criteria, it is useful to form scales from the most correlated items (see <code><a href="#topic+bestScales">bestScales</a></code>.  To get a feeling of the distribution of items across various measures, we can display their correlations (or the log of the probabilities) grouped by some set of scale keys. May also be used to display and order correlations (rows) with a criteria (columns) if given a correlation as input (raw=FALSE).
</p>


<h3>Value</h3>

<p>The correlations or the log p values are returned (invisibily)
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bestScales">bestScales</a></code>, <code><a href="#topic+error.dots">error.dots</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(mfrow=(c(2,3))) #we want to compare two different sets of plots
manhattan(psychTools::bfi[1:25],psychTools::bfi[26:28]
,labels=colnames(psychTools::bfi)[1:25], dictionary=psychTools::bfi.dictionary)
manhattan(psychTools::bfi[1:25],psychTools::bfi[26:28],log.p=TRUE,
    dictionary=psychTools::bfi.dictionary)

#Do it again, but now show items by the keys.list
bfi.keys &lt;-
  list(agree=c("-A1","A2","A3","A4","A5"),conscientious=c("C1","C2","C3","-C4","-C5"),
 extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
 openness = c("O1","-O2","O3","O4","-O5"))
man &lt;-  manhattan(psychTools::bfi[1:25],psychTools::bfi[26:28],keys=bfi.keys,
     dictionary=psychTools::bfi.dictionary[1:2])
manhattan(psychTools::bfi[1:25],psychTools::bfi[26:28],keys=bfi.keys,log.p=TRUE,
dictionary=psychTools::bfi.dictionary[1:2]) 

#Alternatively, use a matrix as input
R &lt;-cor(psychTools::bfi[1:25],psychTools::bfi[26:28],use="pairwise")
manhattan(R,cs(gender,education,age),keys=bfi.keys,
      dictionary=psychTools::bfi.dictionary[1:2], raw=FALSE,abs=FALSE)
par &lt;- op



psychTools::dfOrder(man,1,ascending=FALSE)  #print out the items sorted on gender

</code></pre>

<hr>
<h2 id='mardia'> Calculate univariate or multivariate (Mardia's test) skew and kurtosis for a vector, matrix, or data.frame</h2><span id='topic+mardia'></span><span id='topic+skew'></span><span id='topic+kurtosi'></span>

<h3>Description</h3>

<p>Find the skew and kurtosis for each variable in a data.frame or matrix.  Unlike skew and kurtosis in e1071, this calculates a different skew for each variable or column of a data.frame/matrix. mardia applies Mardia's tests for multivariate skew and kurtosis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skew(x, na.rm = TRUE,type=3)
kurtosi(x, na.rm = TRUE,type=3)
mardia(x,na.rm = TRUE,plot=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mardia_+3A_x">x</code></td>
<td>
<p> A data.frame or matrix </p>
</td></tr>
<tr><td><code id="mardia_+3A_na.rm">na.rm</code></td>
<td>
<p> how to treat missing data </p>
</td></tr>
<tr><td><code id="mardia_+3A_type">type</code></td>
<td>
<p>See the discussion in describe</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="mardia_+3A_plot">plot</code></td>
<td>
<p>Plot the expected normal distribution values versus the Mahalanobis distance of the subjects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>given a matrix or data.frame x, find the skew or kurtosis for each column (for skew and kurtosis) or the multivariate skew and kurtosis in the case of mardia.
</p>
<p>As of version 1.2.3,when finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). 
</p>
<p>If we define <code class="reqn">m_r = [\sum(X- mx)^r]/n</code> then 
</p>
<p>Type 1 finds skewness and kurtosis by <code class="reqn">g_1 = m_3/(m_2)^{3/2} </code> and <code class="reqn">g_2 = m_4/(m_2)^2 -3</code>.  
</p>
<p>Type 2 is <code class="reqn">G1 = g1 * \sqrt{n *(n-1)}/(n-2)</code> and <code class="reqn">G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3))</code>.  
</p>
<p>Type 3 is <code class="reqn">b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2}</code> and <code class="reqn">b2 =  [(n-1)/n]^{3/2} m_4/m_2^2)</code>. 
</p>
<p>For consistency with e1071 and with the Joanes and Gill, the types are now defined as above.
</p>
<p>However, from revision 1.0.93 to 1.2.3, kurtosi by default gives an unbiased estimate of the kurtosis (DeCarlo, 1997). Prior versions used a different equation which produced a biased estimate.  (See the kurtosis function in the e1071 package for the distinction between these two formulae.  The default, type 1 gave what is called type 2 in e1071.  The other is their type 3.)  For comparison with previous releases, specifying type = 2 will give the old estimate.  These type numbers are now changed.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>skew</code></td>
<td>
<p>if input is a matrix or data.frame, skew is a vector of skews</p>
</td></tr>
<tr><td><code>kurtosi</code></td>
<td>
<p>if input is a matrix or data.frame, kurtosi is a vector of kurtosi</p>
</td></tr>
<tr><td><code>bp1</code></td>
<td>
<p>Mardia's bp1 estimate of multivariate skew</p>
</td></tr>
<tr><td><code>bp2</code></td>
<td>
<p>Mardia's bp2 estimate of multivariate kurtosis</p>
</td></tr>
<tr><td><code>skew</code></td>
<td>
<p>Mardia's skew statistic</p>
</td></tr>
<tr><td><code>small.skew</code></td>
<td>
<p>Mardia's small sample skew statistic</p>
</td></tr>
<tr><td><code>p.skew</code></td>
<td>
<p>Probability of skew</p>
</td></tr>
<tr><td><code>p.small</code></td>
<td>
<p>Probability of small.skew</p>
</td></tr>
<tr><td><code>kurtosis</code></td>
<td>
<p>Mardia's multivariate kurtosis statistic</p>
</td></tr>
<tr><td><code>p.kurtosis</code></td>
<td>
<p>Probability of kurtosis statistic</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Mahalanobis distance of cases from centroid</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The mean function supplies means for the columns of a data.frame, but the overall mean for a matrix.  Mean will throw a warning for non-numeric data, but colMeans stops with non-numeric data. Thus, the function uses either mean (for data frames) or colMeans (for matrices).  This is true for skew and kurtosi as well.
</p>


<h3>Note</h3>

<p> 	Probability values less than  10^-300 are set to 0.  </p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Joanes, D.N. and Gill, C.A (1998).  Comparing measures of sample skewness and kurtosis.  The Statistician, 47, 183-189.
</p>
<p>L.DeCarlo. 1997) On the meaning and use of kurtosis, Psychological Methods, 2(3):292-307,
</p>
<p>K.V. Mardia (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3):pp. 519-30, 1970.</p>


<h3>See Also</h3>

<p><code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+describe.by">describe.by</a></code>, mult.norm in QuantPsyc, Kurt in QuantPsyc</p>


<h3>Examples</h3>

<pre><code class='language-R'>round(skew(attitude),2)   #type 3 (default)
round(kurtosi(attitude),2)  #type 3 (default)
#for the differences between the three types of skew and kurtosis:
round(skew(attitude,type=1),2)  #type 1
round(skew(attitude,type=2),2)  #type 2 
mardia(attitude)
x &lt;- matrix(rnorm(1000),ncol=10)
describe(x)
mardia(x)
</code></pre>

<hr>
<h2 id='mat.sort'>Sort the elements of a correlation matrix to reflect factor loadings</h2><span id='topic+mat.sort'></span><span id='topic+matSort'></span>

<h3>Description</h3>

<p>To see the structure of a correlation matrix, it is helpful to organize the items so that the similar items are grouped together. One such grouping technique is factor analysis.  mat.sort will sort the items by a factor model (if specified), or any other order, or by the loadings on the first factor (if unspecified)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mat.sort(m, f = NULL)
matSort(m, f = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mat.sort_+3A_m">m</code></td>
<td>
<p>A correlation matrix
</p>
</td></tr>
<tr><td><code id="mat.sort_+3A_f">f</code></td>
<td>
<p>A factor analysis output (i.e., one with a loadings matrix) or a matrix of weights
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The factor analysis output is sorted by size of the largest  factor loading for each variable  and then the matrix items are organized by those loadings.  The default is to sort by the loadings on the first factor.  Alternatives allow for ordering based upon any vector or matrix. 
</p>


<h3>Value</h3>

<p>A sorted correlation matrix, suitable for showing with <code><a href="#topic+corPlot">corPlot</a></code>.
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+corPlot">corPlot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Bechtoldt.1)
sorted &lt;- mat.sort(Bechtoldt.1,fa(Bechtoldt.1,5))
corPlot(sorted,xlas=2) #vertical xaxis names
</code></pre>

<hr>
<h2 id='matrix.addition'>A function to add two vectors or matrices </h2><span id='topic+matrix.addition'></span><span id='topic++25+2B+25'></span>

<h3>Description</h3>

<p>It is sometimes convenient to add two vectors or matrices in an operation analogous to matrix multiplication. For matrices nXm and mYp, the matrix sum  of the i,jth element of nSp = sum(over m) of iXm + mYj. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>x %+% y </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix.addition_+3A_x">x</code></td>
<td>
<p> a n by m matrix (or vector if m= 1)</p>
</td></tr>
<tr><td><code id="matrix.addition_+3A_y">y</code></td>
<td>
<p> a m by p matrix (or vector if m = 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used in such problems as Thurstonian scaling.  Although not technically matrix addition, as pointed out by Krus, there are many applications where the sum or difference of two vectors or matrices is a useful operation.  An alternative operation for vectors is  outer(x ,y , FUN=&quot;+&quot;) but this does not work for matrices.  
</p>


<h3>Value</h3>

<p>a n by p matix of sums
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Krus, D. J. (2001) Matrix addition. Journal of Visual Statistics, 1, (February, 2001).</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- seq(1,4)
z &lt;- x %+% -t(x)
x
z
#compare with outer(x,-x,FUN="+")
x &lt;- matrix(seq(1,6),ncol=2)
y &lt;- matrix(seq(1,10),nrow=2)
z &lt;- x %+% y
x
y
z
#but compare this with outer(x ,y,FUN="+") 
</code></pre>

<hr>
<h2 id='mediate'>Estimate and display direct and indirect effects of mediators and  moderator in path models</h2><span id='topic+mediate'></span><span id='topic+mediate.diagram'></span><span id='topic+moderate.diagram'></span>

<h3>Description</h3>

<p>Find the direct and indirect effects of a predictor in path models of mediation and moderation. Bootstrap confidence intervals for the indirect effects.  Mediation models are just extended regression models making explicit the effect of particular covariates in the model. Moderation is done by multiplication of the predictor variables.   This function supplies basic mediation/moderation analyses for some of the classic problem types. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mediate(y, x, m=NULL, data, mod = NULL, z = NULL, n.obs = NULL, use = "pairwise",
 n.iter = 5000,  alpha = 0.05, std = FALSE,plot=TRUE,zero=TRUE,part=FALSE, 
  main="Mediation")
mediate.diagram(medi,digits=2,ylim=c(3,7),xlim=c(-1,10),show.c=TRUE,
     main="Mediation model",cex=1,l.cex=1,...)
moderate.diagram(medi,digits=2,ylim=c(2,8),main="Moderation model", cex=1,l.cex=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mediate_+3A_y">y</code></td>
<td>
<p>The dependent variable (or a formula suitable for a linear model),  If a formula, then this is of the form y ~ x +(m) -z (see details)</p>
</td></tr>
<tr><td><code id="mediate_+3A_x">x</code></td>
<td>
<p>One or more predictor variables</p>
</td></tr>
<tr><td><code id="mediate_+3A_m">m</code></td>
<td>
<p>One (or more) mediating variables</p>
</td></tr>
<tr><td><code id="mediate_+3A_data">data</code></td>
<td>
<p>A data frame holding the data or a correlation or covariance  matrix. </p>
</td></tr>
<tr><td><code id="mediate_+3A_mod">mod</code></td>
<td>
<p>A moderating variable, if desired</p>
</td></tr>
<tr><td><code id="mediate_+3A_z">z</code></td>
<td>
<p>Variables to partial out, if desired</p>
</td></tr>
<tr><td><code id="mediate_+3A_n.obs">n.obs</code></td>
<td>
<p>If the data are from a correlation or covariance matrix, 
how many observations were used. This will lead to simulated data for the bootstrap.</p>
</td></tr>
<tr><td><code id="mediate_+3A_use">use</code></td>
<td>
<p>use=&quot;pairwise&quot; is the default when finding correlations or covariances</p>
</td></tr>
<tr><td><code id="mediate_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of bootstrap resamplings to conduct</p>
</td></tr>
<tr><td><code id="mediate_+3A_alpha">alpha</code></td>
<td>
<p>Set the width of the confidence interval to be 1 - alpha</p>
</td></tr>
<tr><td><code id="mediate_+3A_std">std</code></td>
<td>
<p>standardize the covariances to find the standardized betas</p>
</td></tr>
<tr><td><code id="mediate_+3A_part">part</code></td>
<td>
<p>if part=TRUE, find part correlations otherwise find partial correlations when partialling</p>
</td></tr>
<tr><td><code id="mediate_+3A_plot">plot</code></td>
<td>
<p>Plot the resulting paths</p>
</td></tr>
<tr><td><code id="mediate_+3A_zero">zero</code></td>
<td>
<p>By default, will zero center the data before doing moderation</p>
</td></tr>
<tr><td><code id="mediate_+3A_digits">digits</code></td>
<td>
<p>The number of digits to report in the mediate.diagram.</p>
</td></tr> 
<tr><td><code id="mediate_+3A_medi">medi</code></td>
<td>
<p>The output from mediate may be imported into mediate.diagram</p>
</td></tr>
<tr><td><code id="mediate_+3A_ylim">ylim</code></td>
<td>
<p>The limits for the y axis in the mediate and moderate diagram functions</p>
</td></tr>
<tr><td><code id="mediate_+3A_xlim">xlim</code></td>
<td>
<p>The limits for the x axis.  Make the minimum more negative if the x by x correlations do not fit.</p>
</td></tr>
<tr><td><code id="mediate_+3A_show.c">show.c</code></td>
<td>
<p>If FALSE, do not draw the c lines, just the partialed (c')  lines</p>
</td></tr>
<tr><td><code id="mediate_+3A_main">main</code></td>
<td>
<p>The title for the mediate and moderate functions</p>
</td></tr>
<tr><td><code id="mediate_+3A_cex">cex</code></td>
<td>
<p>Adjust the text size (defaults to 1)</p>
</td></tr>
<tr><td><code id="mediate_+3A_l.cex">l.cex</code></td>
<td>
<p>Adjust the text size in arrows, defaults to cex which in turn defaults to 1</p>
</td></tr>
<tr><td><code id="mediate_+3A_...">...</code></td>
<td>
<p>Additional graphical parameters to pass to mediate.diagram</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When doing linear modeling, it is frequently convenient to estimate the direct effect of a predictor controlling for the indirect effect of a mediator.  See Preacher and Hayes (2004) for a very thorough discussion of mediation.  The mediate function will do some basic mediation and moderation models, with bootstrapped confidence intervals for the mediation/moderation effects. 
</p>
<p>Functionally, this is just regular linear regression and partial correlation with some different output.
</p>
<p>In the case of two predictor variables, X and M, and a criterion variable Y, then the direct effect of X on Y, labeled with the path c, is said to be mediated by the effect of x on M (path a) and the effect of M on Y (path b).  This partial effect (a b) is said to mediate the direct effect of X &ndash;c&ndash;&gt; Y:     X &ndash;a -&gt;  M  &ndash;b&ndash;&gt; Y with X &ndash;c'&ndash;&gt; Y where c' = c - ab.
</p>
<p>Testing the significance of the ab mediation effect is done through bootstrapping many random resamples (with replacement) of the data.  
</p>
<p>For moderation, the moderation effect of Z on the relationship between X -&gt; Y is found by taking the (centered) product of X and Z and then adding this XZ term into the regression. By default, the data are zero centered before doing moderation (product terms).  This is following the advice of Cohen, Cohen, West and Aiken (2003).  However, to agree with the analyses reported in Hayes (2013) we can set the zero=FALSE option to not zero center the data.   
</p>
<p>To partial out variables, either define them in the z term, or express as negative entries in the formula mode:
</p>
<p>y1 ~ x1 + x2 + (m1)+ (m2) -z    will look for the effect of x1 and x2 on y, mediated through m1 and m2 after z is partialled out.  
</p>
<p>Moderated mediation is done by specifying a product term.
</p>
<p>y1 ~ x1 + x2*x3 + (m1)+ (m2) -z    will look for the effect of x1, x2, x3 and the product of x2 and x3 on y, mediated through m1 and m2 after z is partialled out.  
</p>
<p>In the case of being provided just a correlation matrix, the bootstrapped values are based upon bootstrapping from data matching the original covariance/correlation matrix with the addition of normal errors.  This allows us to test the mediation/moderation effect even if not given raw data.  Moderation can not be done with just correlation matrix.
</p>
<p>The function has been tested against some of the basic cases and examples in Hayes (2013) and the associated data sets.
</p>
<p>Unless there is a temporal component that allows one to directly distinguish causal paths (time does not reverse direction), interpreting mediation models is problematic. Some people find it useful to compare the differences between mediation models where the causal paths (arrows) are reversed.  This is a mistake  and should not be done (Thoemmes, 2015). 
</p>
<p>For fine tuning the size of the graphic output, xlim and ylim can be specified in the mediate.diagram function. Otherwise, the graphics produced by mediate and moderate use the default xlim and ylim values.
</p>
<p>Interaction terms (moderation) or mediated moderation can be specified as product terms.
</p>


<h3>Value</h3>

<table>
<tr><td><code>total</code></td>
<td>
<p>The total direct effect of x on y  (c)</p>
</td></tr>
<tr><td><code>direct</code></td>
<td>
<p>The beta effects of x (c') and m  (b) on y   </p>
</td></tr>
<tr><td><code>indirect</code></td>
<td>
<p>The indirect effect of x through m on y (c-ab)</p>
</td></tr>
<tr><td><code>mean.boot</code></td>
<td>
<p>mean bootstrapped value of indirect effect</p>
</td></tr>
<tr><td><code>sd.boot</code></td>
<td>
<p>Standard deviation of bootstrapped values</p>
</td></tr>
<tr><td><code>ci.quant</code></td>
<td>
<p>The upper and lower confidence intervals based upon the quantiles of the bootstrapped distribution.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>The bootstrapped values themselves.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>The effect of x on m</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The effect of m on y</p>
</td></tr>
<tr><td><code>b.int</code></td>
<td>
<p>The interaction of x and mod (if specified)</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The original data plus the product term (if specified)</p>
</td></tr>
</table>


<h3>Note</h3>

<p> There are a number of other packages that do mediation analysis (e.g., sem and lavaan) and they are probably preferred for more complicated models.  This function is supplied for the more basic cases, with 1..k y variables, 1..n x variables,  1 ..j mediators and 1 ..z variables to partial. The number of moderated effects is not limited, but more than 3rd order interactions are not very meaningful.  It will not do two step mediation. 
</p>
<p>The current version will not correctly handle more than one DV.  
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>J. Cohen, P. Cohen, S.G. West, and L.S. Aiken. (2003) Applied multiple regression/correlation analysis for the behavioral sciences. L. Erlbaum Associates, Mahwah, N.J., 3rd ed edition.
</p>
<p>Hayes, Andrew F. (2013)  Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.  Guilford Press. 
</p>
<p>Preacher, Kristopher J and Hayes, Andrew F (2004) SPSS and SAS procedures for estimating indirect effects in simple mediation models. Behavior Research Methods, Instruments, &amp; Computers 36, (4) 717-731.
</p>
<p>Thoemmes, Felix (2015) Reversing arrows in mediation models does not distinguish plausible models.  Basic and applied social psychology, 27: 226-234. 
</p>
<p>Data from  Hayes (2013), Preacher and Hayes (2004), and from Kerchoff (1974).  
</p>
<p>The Tal_Or data set  is from Nurit Tal-Or and Jonathan Cohen and Yariv Tsfati and Albert C. Gunther, (2010) &ldquo;Testing Causal Direction in the Influence of Presumed Media Influence&quot;, Communication Research, 37, 801-824 and is used with their kind permission.  It is adapted from the webpage of A.F. Hayes.  (www.afhayes.com/public/hayes2013data.zip).
</p>
<p>The Garcia data set is from Garcia, Donna M. and Schmitt, Michael T. and Branscombe, Nyla R. and Ellemers, Naomi (2010). Women's reactions to ingroup members who protest discriminatory treatment: The importance of beliefs about inequality and response appropriateness. European Journal of Social Psychology, (40) 733-745 and is used with their kind permission.  It was downloaded from the Hayes (2013) website.
</p>
<p>For an example of how to display  the sexism by protest interaction, see the examples in the <code><a href="#topic+GSBE">GSBE</a></code> (Garcia) data set.
</p>
<p>See the &ldquo;how to do mediation and moderation&quot; at  personality-project.org/r/psych/HowTo/mediation.pdf as well as the introductory vignette.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lmCor">lmCor</a></code> and <code><a href="#topic+lmCor.diagram">lmCor.diagram</a></code> for regression and moderation, <code><a href="#topic+Garcia">Garcia</a></code> for further demonstrations of mediation and moderation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
# A simple mediation example is the Tal_Or data set (pmi for Hayes)
#The pmi data set from Hayes is available as the Tal_Or data set. 
mod4 &lt;- mediate(reaction ~ cond + (pmi), data =Tal_Or,n.iter=50) 
summary(mod4)
#Two mediators (from Hayes model 6 (chapter 5))
mod6 &lt;- mediate(reaction ~ cond + (pmi) + (import), data =Tal_Or,n.iter=50) 
summary(mod6)

#Moderated mediation is done for the Garcia (Garcia, 2010) data set.
# (see Hayes, 2013 for the protest data set
#n.iter set to 50 (instead of default of 5000) for speed of example
#no mediation, just an interaction
mod7 &lt;- mediate(liking ~  sexism * prot2 , data=Garcia, n.iter = 50)
summary(mod7)
data(GSBE)   #The Garcia et al data set (aka GSBE)
mod11.4 &lt;- mediate(liking ~  sexism * prot2 + (respappr), data=Garcia,
        n.iter = 50,zero=FALSE)   #to match Hayes
summary(mod11.4)
#to see this interaction graphically, run the examples in ?Garcia


#data from Preacher and Hayes (2004)
sobel &lt;- structure(list(SATIS = c(-0.59, 1.3, 0.02, 0.01, 0.79, -0.35, 
-0.03, 1.75, -0.8, -1.2, -1.27, 0.7, -1.59, 0.68, -0.39, 1.33, 
-1.59, 1.34, 0.1, 0.05, 0.66, 0.56, 0.85, 0.88, 0.14, -0.72, 
0.84, -1.13, -0.13, 0.2), THERAPY = structure(c(0, 1, 1, 0, 1, 
1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 
1, 1, 1, 0), value.labels = structure(c(1, 0), .Names = c("cognitive", 
"standard"))), ATTRIB = c(-1.17, 0.04, 0.58, -0.23, 0.62, -0.26, 
-0.28, 0.52, 0.34, -0.09, -1.09, 1.05, -1.84, -0.95, 0.15, 0.07, 
-0.1, 2.35, 0.75, 0.49, 0.67, 1.21, 0.31, 1.97, -0.94, 0.11, 
-0.54, -0.23, 0.05, -1.07)), .Names = c("SATIS", "THERAPY", "ATTRIB"
), row.names = c(NA, -30L), class = "data.frame", variable.labels = structure(c("Satisfaction", 
"Therapy", "Attributional Positivity"), .Names = c("SATIS", "THERAPY", 
"ATTRIB")))
 #n.iter set to 50 (instead of default of 5000) for speed of example

#There are several forms of input.  The original specified y, x , and the mediator 
#mediate(1,2,3,sobel,n.iter=50)  #The example in Preacher and Hayes
#As of October, 2017 we can specify this in a formula mode
mediate (SATIS ~ THERAPY + (ATTRIB),data=sobel, n.iter=50) #specify the mediator by 
# adding parentheses

#A.C. Kerchoff, (1974) Ambition and Attainment: A Study of Four Samples of American Boys.
#Data from sem package taken from Kerckhoff (and in turn, from Lisrel manual)
R.kerch &lt;- structure(list(Intelligence = c(1, -0.1, 0.277, 0.25, 0.572, 
0.489, 0.335), Siblings = c(-0.1, 1, -0.152, -0.108, -0.105, 
-0.213, -0.153), FatherEd = c(0.277, -0.152, 1, 0.611, 0.294, 
0.446, 0.303), FatherOcc = c(0.25, -0.108, 0.611, 1, 0.248, 0.41, 
0.331), Grades = c(0.572, -0.105, 0.294, 0.248, 1, 0.597, 0.478
), EducExp = c(0.489, -0.213, 0.446, 0.41, 0.597, 1, 0.651), 
    OccupAsp = c(0.335, -0.153, 0.303, 0.331, 0.478, 0.651, 1
    )), .Names = c("Intelligence", "Siblings", "FatherEd", "FatherOcc", 
"Grades", "EducExp", "OccupAsp"), class = "data.frame", row.names = c("Intelligence", 
"Siblings", "FatherEd", "FatherOcc", "Grades", "EducExp", "OccupAsp"
))

 #n.iter set to 50 (instead of default of 5000) for speed of demo
#mod.k &lt;- mediate("OccupAsp","Intelligence",m= c(2:5),data=R.kerch,n.obs=767,n.iter=50)
#new style 
mod.k &lt;- mediate(OccupAsp ~ Intelligence + (Siblings) + (FatherEd) + (FatherOcc) + 
(Grades), data = R.kerch, n.obs=767, n.iter=50)

mediate.diagram(mod.k) 
#print the path values 
mod.k

#Compare the following solution to the path coefficients found by the sem package

#mod.k2 &lt;- mediate(y="OccupAsp",x=c("Intelligence","Siblings","FatherEd","FatherOcc"),
#     m= c(5:6),data=R.kerch,n.obs=767,n.iter=50)
#new format 
mod.k2 &lt;- mediate(OccupAsp ~ Intelligence + Siblings + FatherEd + FatherOcc + (Grades) + 
(EducExp),data=R.kerch, n.obs=767, n.iter=50)
mediate.diagram(mod.k2,show.c=FALSE) #simpler output 
#print the path values
mod.k2

#Several interesting test cases are taken from analyses of the Spengler data set
#This is temporarily added to psych from psychTools to help build for CRAN
#Although the sample sizes are actually very large in the first wave,  I use the
#sample sizes from the last wave 
#We set the n.iter to be 50 instead of the default value of 5,000
if(require("psychTools")) {
 mod1 &lt;- mediate(Income.50 ~ IQ + Parental+ (Ed.11) ,data=Spengler,
    n.obs = 1952, n.iter=50)
 mod2 &lt;- mediate(Income.50 ~ IQ + Parental+ (Ed.11)  + (Income.11)
  ,data=Spengler,n.obs = 1952, n.iter=50)
  
  #Now, compare these models
anova(mod1,mod2)
 
 #Current version does not support two DVs
 #mod22 &lt;-  mediate(Income.50 + Educ.50 ~ IQ + Parental+ (Ed.11)  + (Income.11) 
 #    ,data=Spengler,n.obs = 1952, n.iter=50)

}

</code></pre>

<hr>
<h2 id='mixedCor'>Find correlations for mixtures of continuous, polytomous, and dichotomous variables</h2><span id='topic+mixedCor'></span><span id='topic+mixed.cor'></span>

<h3>Description</h3>

<p>For data sets with continuous, polytomous and dichotmous variables, the absolute Pearson correlation is downward biased from the underlying latent correlation.  mixedCor finds Pearson correlations for the continous variables, <code><a href="#topic+polychoric">polychoric</a></code>s for the polytomous items, <code><a href="#topic+tetrachoric">tetrachoric</a></code>s for the dichotomous items, and the <code><a href="#topic+polyserial">polyserial</a></code> or <code><a href="#topic+biserial">biserial</a></code> correlations for the various mixed variables. Results include the complete correlation matrix, as well as the separate correlation matrices and difficulties for the polychoric and tetrachoric correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixedCor(data=NULL,c=NULL,p=NULL,d=NULL,smooth=TRUE,correct=.5,global=TRUE,ncat=8,
             use="pairwise",method="pearson",weight=NULL)
             
mixed.cor(x = NULL, p = NULL, d=NULL,smooth=TRUE, correct=.5,global=TRUE, 
        ncat=8,use="pairwise",method="pearson",weight=NULL)  #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixedCor_+3A_data">data</code></td>
<td>
<p>The data set to be analyzed (either a matrix or dataframe)</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_c">c</code></td>
<td>
<p>The names (or locations) of the continuous variables) (may be missing)</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_x">x</code></td>
<td>
<p>A set of continuous variables (may be missing) or, if p and d are missing, the variables to be analyzed.</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_p">p</code></td>
<td>
<p>A set of polytomous items (may be missing)</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_d">d</code></td>
<td>
<p>A set of dichotomous items (may be missing)</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_smooth">smooth</code></td>
<td>
<p>If TRUE, then smooth the correlation matix if it is non-positive definite</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_correct">correct</code></td>
<td>
<p>When finding tetrachoric correlations, what value should be used to correct for continuity?</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_global">global</code></td>
<td>
<p>For polychorics, should the global values of the tau parameters be used, or should the pairwise values be used.  Set to local if errors are occurring.</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_ncat">ncat</code></td>
<td>
<p>The number of categories beyond which a variable is considered &quot;continuous&quot;.</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_use">use</code></td>
<td>
<p>The various options to the <code><a href="stats.html#topic+cor">cor</a></code> function include &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot;. The default here is &quot;pairwise&quot;</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_method">method</code></td>
<td>
<p>The correlation method to use for the continuous variables. &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;</p>
</td></tr>
<tr><td><code id="mixedCor_+3A_weight">weight</code></td>
<td>
<p>If specified, this is a vector of weights (one per participant) to differentially weight participants. The NULL case is equivalent of weights of 1 for all cases. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is particularly useful as part of the Synthetic Apeture Personality Assessment (SAPA) (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) data sets where continuous variables (age, SAT V, SAT Q, etc) and mixed with polytomous personality items taken from the International Personality Item Pool (IPIP) and the dichotomous experimental IQ items that have been developed as part of SAPA (see, e.g., Revelle, Wilt and Rosenthal, 2010 or Revelle, Dworak and Condon, 2020.).  
</p>
<p>This is a very computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package. (See the note for timing comparisons.) This adjusts the  number of cores to use when doing polychoric or tetrachoric. The greatest step in speed is going from 1 core to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the <code><a href="base.html#topic+options">options</a></code> command:  options(&quot;mc.cores&quot;=4) will set the number of cores to 4.
</p>
<p>Item response analyses using <code><a href="#topic+irt.fa">irt.fa</a></code> may be done separately on the polytomous and dichotomous items  in order to develop internally consistent scales. These scale may, in turn, be correlated with each other using the complete correlation matrix found by mixed.cor and using the <code><a href="#topic+score.items">score.items</a></code> function.
</p>
<p>This function is not quite as flexible as the hetcor function in John Fox's polychor package. 
</p>
<p>Note that the variables may be organized by type of data:  continuous, polytomous, and dichotomous. This is done by simply specifying c, p, and d. This is advantageous in the case of some continuous variables having a limited number of categories because of subsetting.  
</p>
<p><code><a href="#topic+mixedCor">mixedCor</a></code> is essentially a wrapper for <code><a href="stats.html#topic+cor">cor</a></code>, <code><a href="#topic+polychoric">polychoric</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+polydi">polydi</a></code> and <code><a href="#topic+polyserial">polyserial</a></code>. It first identifies the types of variables, organizes them by type (continuous, polytomous, dichotomous), calls the appropriate correlation function, and then binds the resulting matrices together.   
</p>


<h3>Value</h3>

<table>
<tr><td><code>rho</code></td>
<td>
<p>The complete matrix</p>
</td></tr>
<tr><td><code>rx</code></td>
<td>
<p>The Pearson correlation matrix for the continuous items</p>
</td></tr>
<tr><td><code>poly</code></td>
<td>
<p>the polychoric correlation (poly$rho) and the item difficulties (poly$tau)</p>
</td></tr>
<tr><td><code>tetra</code></td>
<td>
<p>the tetrachoric correlation (tetra$rho) and the item difficulties (tetra$tau)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>mixedCor was designed for the SAPA project (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) with large data sets with a mixture of continuous, dichotomous, and polytomous data.  For smaller data sets, it is sometimes the case that the global estimate of the tau parameter will lead to unstable solutions.  This may be corrected by setting the global parameter = FALSE.  
</p>
<p><code><a href="#topic+mixedCor">mixedCor</a></code> was added in April, 2017 to be slightly more user friendly. <code><a href="#topic+mixed.cor">mixed.cor</a></code> was deprecated in February, 2018.
</p>
<p>When finding correlations between dummy coded SAPA data (e.g., of occupations), the real correlations are all slightly less than zero because of the ipsatized nature of the data.  This leads to a non-positive definite correlation matrix because the matrix is no longer of full rank. Smoothing will correct this, even though this might not be desired.  Turn off smoothing in this case.
</p>
<p>Note that the variables no longer need to be organized by type of data: first continuous, then polytomous, then dichotomous.  However, this automatic detection will lead to problems if the variables such as age are limited to less than 8 categories but those category values differ from the polytomous items.  The fall back is to specify x, p, and d. 
</p>
<p>If the number of alternatives in the polychoric data differ and there are some dicthotomous data, it is advisable to set correct=0.
</p>
<p>Timing:  For large data sets, <code><a href="#topic+mixedCor">mixedCor</a></code> takes a while.  Progress messages <code><a href="#topic+progressBar">progressBar</a></code>  report what is happening but do not actually report the rate of progress. ( The steps are initializing, Pearson r, polychorics, tetrachoric, polydi).  It is recommended to use the multicore option although the benefit between 2, 4 and 8 cores seems fairly small: For large data sets (e.g., 255K subjects, 950 variables), with 4 cores running  in parallel  (options(&quot;mc.cores=4&quot;) on a MacBook Pro with 2.8 Ghz Intel Core I7, it took 2,873/2,887 seconds elapsed time, 8,152/7,718 secs of user time, and 1,762/1,489 of system time (with and without smoothing). This is noticeabably better than the 4,842 elapsed time (7,313 user, 1,459 system) for 2 cores but not much worse than running 8 virtual cores, with an elapsed time of 2,629, user time of 13,460, and system time of 2,679. On a Macbook Pro with 2 physical cores and a 3.3 GHz Intel Cor I7, running 4 multicores took 4,423 seconds elapsed time, 12,781 seconds of user time, and 2,605 system time.  Running with 2 multicores, took slightly longer: 6,193 seconds elapsed time, 10,099 user time and 2,413 system time.  
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>W.Revelle, J.Wilt, and A.Rosenthal. Personality and cognition: The personality-cognition link. In A.Gruszka, G. Matthews, and B. Szymura, editors,  Handbook of Individual Differences in Cognition: Attention, Memory and Executive
Control, chapter 2, pages 27-49. Springer, 2010.
</p>
<p>W Revelle, D. M. Condon,  J. Wilt,  J.A. French,  A. Brown, and L G. Elleman(2016) Web and phone based data collection using planned missing designs in Nigel G. Fielding and Raymond M. Lee and Grant Blank (eds) SAGE Handbook of Online Research Methods, Sage Publications, Inc.
</p>
<p>W. Revelle, E.M. Dworak and D.M. Condon (2020) Exploring the persome: The power of the item in understanding personality structure. Personality and Individual Differences, <code>/10.1016/j.paid.2020.109905</code>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+polychoric">polychoric</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+scoreItems">scoreItems</a></code>,  <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> <code><a href="#topic+scoreIrt">scoreIrt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bfi) 
r &lt;- mixedCor(data=psychTools::bfi[,c(1:5,26,28)])
r
#this is the same as
r &lt;- mixedCor(data=psychTools::bfi,p=1:5,c=28,d=26)
r #note how the variable order reflects the original order in the data
#compare to raw Pearson
#note that the biserials and polychorics are not attenuated
rp &lt;- cor(psychTools::bfi[c(1:5,26,28)],use="pairwise")
lowerMat(rp)
</code></pre>

<hr>
<h2 id='mssd'>Find von Neuman's Mean Square of Successive Differences</h2><span id='topic+mssd'></span><span id='topic+rmssd'></span><span id='topic+autoR'></span>

<h3>Description</h3>

<p>Von Neuman et al. (1941) discussed the Mean Square of Successive Differences as a measure of variability that takes into account gradual shifts in mean. This is appropriate when studying errors in ballistics or variability and stability in mood when studying affect. For random data, this will be twice the variance, but for data with a sequential order and a positive autocorrelation, this will be much smaller. Since the mssd is just twice the variance - the autocorrelation, it is thus possible to also find the autocorrelation for a particular lag. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mssd(x,group=NULL, lag = 1,na.rm=TRUE)
rmssd(x,group=NULL, lag=1, na.rm=TRUE)
autoR(x,group=NULL,lag=1,na.rm=TRUE,use="pairwise")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mssd_+3A_x">x</code></td>
<td>
<p>a vector, data.frame or matrix</p>
</td></tr>
<tr><td><code id="mssd_+3A_lag">lag</code></td>
<td>
<p>the lag to use when finding <code><a href="base.html#topic+diff">diff</a></code>     </p>
</td></tr>
<tr><td><code id="mssd_+3A_group">group</code></td>
<td>
<p>A column of the x data.frame to be used for grouping</p>
</td></tr>
<tr><td><code id="mssd_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing data be removed?</p>
</td></tr>
<tr><td><code id="mssd_+3A_use">use</code></td>
<td>
<p>How to handle missing data in autoR</p>
</td></tr></table>
<p>.
</p>


<h3>Details</h3>

<p>When examining multiple measures within subjects, it is sometimes useful to consider the variability of trial by trial observations in addition to the over all between trial variation.  The Mean Square of Successive Differences (mssd) and root mean square of successive differences (rmssd) is just 
</p>
<p><code class="reqn">\sigma^2 = \Sigma(x_i - x_{i+1})^2 /(n-lag) </code> 
</p>
<p>Where n-lag is used because there are only n-lag cases.
</p>
<p>In the case of multiple subjects  (groups) with multiple observations per subject (group), specify the grouping variable will produce output for each group.   
</p>
<p>Similar functions are available in the matrixStats package. However, the varDiff function in that package is variance of the difference rather than the MeanSquare. This is just the variance and standard deviation applied to the result from the <code><a href="base.html#topic+diff">diff</a></code> function.
</p>
<p>Perhaps useful when studying mood, the <code><a href="#topic+autoR">autoR</a></code> function finds the autocorrelation for each item for the specified lag.  It also returns the rmssd (root means square successive difference). This is done by finding the correlation of the lag data. 
</p>


<h3>Value</h3>

<p>The average squared successive difference (mssd) and the square root of the average squared successive difference (rmssd).  Note that this is not the same as the standard deviation of the lagged differences.
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Jahng, Seungmin and Wood, Phillip K and Trull, Timothy J. Analysis of affective instability in ecological momentary assessment: Indices using successive difference and group comparison via multilevel modeling. Psychological methods (2008) 13, 354-375. 
</p>
<p>Von Neumann, J., Kent, R., Bellinson, H., and Hart, B. (1941). The mean square successive difference. The Annals of Mathematical Statistics, pages 153-162.
</p>


<h3>See Also</h3>

<p>See Also <code><a href="#topic+rmssd">rmssd</a></code> for the standard deviation or  <code><a href="#topic+describe">describe</a></code> for more conventional statistics.  <code><a href="#topic+describeBy">describeBy</a></code> and <code><a href="#topic+statsBy">statsBy</a></code> give group level statistics.  See also <code>link{mlr}</code>, <code>link{mlreliability}</code>, <code>link{mlPlot}</code> for other ways of examining within person variability over time. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>t &lt;- seq(-pi, pi, .1)
trial &lt;- 1: length(t)
gr &lt;- trial %% 8 
c &lt;- cos(t)
ts &lt;- sample(t,length(t))
cs &lt;- cos(ts)
x.df &lt;- data.frame(trial,gr,t,c,ts,cs)
rmssd(x.df)
rmssd(x.df,gr)
autoR(x.df,gr)
describe(x.df)
#pairs.panels(x.df)
#mlPlot(x.df,grp="gr",Time="t",items=c(4:6))
</code></pre>

<hr>
<h2 id='multi.hist'>  Multiple histograms with density and normal fits on one page</h2><span id='topic+multi.hist'></span><span id='topic+histo.density'></span><span id='topic+histBy'></span>

<h3>Description</h3>

<p>Given a matrix or data.frame, produce histograms for each variable in a &quot;matrix&quot; form. Include normal fits and density distributions for each plot.
</p>
<p>The number of rows and columns may be specified, or calculated.
May be used for single variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi.hist(x,nrow=NULL,ncol=NULL,density=TRUE,freq=FALSE,bcol="white",
      dcol=c("black","black"),dlty=c("dashed","dotted"),
      main=NULL,mar=c(2,1,1,1),breaks=21,global=TRUE,...)
histBy(x,var,group,data=NULL,density=TRUE,alpha=.5,breaks=21,col,xlab,
            main="Histograms by group",freq=FALSE,...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi.hist_+3A_x">x</code></td>
<td>
<p> matrix or data.frame</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_var">var</code></td>
<td>
<p>The variable in x to plot in histBy</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_group">group</code></td>
<td>
<p>The name of the variable in x to use as the grouping variable</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_data">data</code></td>
<td>
<p>Needs to be specified if using formula input to histBy</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_nrow">nrow</code></td>
<td>
<p>number of rows in the plot</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_ncol">ncol</code></td>
<td>
<p>number of columns in the plot</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_density">density</code></td>
<td>
<p>density=TRUE, show the normal fits and density distributions</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_freq">freq</code></td>
<td>
<p>freq=FALSE shows probability densities and density distribution, freq=TRUE shows frequencies</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_bcol">bcol</code></td>
<td>
<p>Color for the bars</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_dcol">dcol</code></td>
<td>
<p>The color(s) for the normal and the density fits. Defaults to black. </p>
</td></tr>
<tr><td><code id="multi.hist_+3A_dlty">dlty</code></td>
<td>
<p>The line type (lty) of the normal and density fits.  (specify the optional graphic parameter lwd to change the line size)</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_main">main</code></td>
<td>
<p>title for each panel will be set to the column name unless specified</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_mar">mar</code></td>
<td>
<p>Specify the lower, left, upper and right hand side margin in lines &ndash; set to be tighter than normal default of c(5,4,4,2) + .1 </p>
</td></tr>
<tr><td><code id="multi.hist_+3A_xlab">xlab</code></td>
<td>
<p>Label for the x variable</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_breaks">breaks</code></td>
<td>
<p>The number of breaks in histBy (see hist)</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_global">global</code></td>
<td>
<p>If TRUE, use the same x-axis for all plots</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_alpha">alpha</code></td>
<td>
<p>The degree of transparency of the overlapping bars in histBy</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_col">col</code></td>
<td>
<p>A vector of colors in histBy  (defaults to the rainbow)</p>
</td></tr>
<tr><td><code id="multi.hist_+3A_...">...</code></td>
<td>
<p>additional graphic parameters (e.g., col)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This allows for quick summaries of multiple distributions.  Particularly useful when examining the results of multiple-split halves that come from the <code><a href="#topic+reliability">reliability</a></code> function.  
</p>
<p>By default, will try to make a square plot with equal number of rows and columns.  However, the number of columns and rows may be specified for a particular plot.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>See Also</h3>

 <p><code><a href="#topic+bi.bars">bi.bars</a></code> for drawing pairwise histograms and  <code><a href="#topic+scatterHist">scatterHist</a></code> for bivariate scatter and histograms. <code><a href="#topic+densityBy">densityBy</a></code>, <code><a href="#topic+violinBy">violinBy</a></code> and <code><a href="#topic+violin">violin</a></code> for density plots.</p>


<h3>Examples</h3>

<pre><code class='language-R'>multi.hist(sat.act) 
multi.hist(sat.act,bcol="red")
multi.hist(sat.act,dcol="blue")  #make both lines blue
multi.hist(sat.act,dcol= c("blue","red"),dlty=c("dotted", "solid")) 
multi.hist(sat.act,freq=TRUE)   #show the frequency plot
multi.hist(sat.act,nrow=2)
histBy(sat.act,"SATQ","gender") #input by variable names
histBy(SATQ~ gender, data=sat.act) #formula input
</code></pre>

<hr>
<h2 id='multilevel.reliability'>Find and plot various reliability/gneralizability coefficients for multilevel data
</h2><span id='topic+mlr'></span><span id='topic+multilevel.reliability'></span><span id='topic+mlArrange'></span><span id='topic+mlPlot'></span>

<h3>Description</h3>

<p>Various indicators of reliability of multilevel data (e.g., items over time nested within subjects) may be found using generalizability theory.  A basic three way anova is applied to the data from which variance components are extracted. Random effects for a nested design are found by lme.    These are, in turn, converted to several reliability/generalizability coefficients.  An optional call to lme4 to use lmer may be used for unbalanced designs with missing data. mlArrange is a  helper function to convert wide to long format.  Data can be rearranged from wide to long format, and multiple lattice plots of observations overtime for multiple variables and multiple subjects are created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlr(x, grp = "id", Time = "time", items = c(3:5),alpha=TRUE,icc=FALSE, aov=TRUE,
      lmer=FALSE,lme = TRUE,long=FALSE,values=NA,na.action="na.omit",plot=FALSE,
        main="Lattice Plot by subjects over time")
mlArrange(x, grp = "id", Time = "time", items = c(3:5),extra=NULL)
mlPlot(x, grp = "id", Time = "time", items = c(3:5),extra=NULL, 
   col=c("blue","red","black","grey"),type="b",
    main="Lattice Plot by subjects over time",...)
multilevel.reliability(x, grp = "id", Time = "time", items = c(3:5),alpha=TRUE,icc=FALSE,
 aov=TRUE,lmer=FALSE,lme = TRUE,long=FALSE,values=NA,na.action="na.omit",
   plot=FALSE,main="Lattice Plot by subjects over time") #alias for mlr
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multilevel.reliability_+3A_x">x</code></td>
<td>
<p>A data frame with persons, time, and items.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_grp">grp</code></td>
<td>
<p>Which variable specifies people (groups)</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_time">Time</code></td>
<td>
<p>Which variable specifies the temporal sequence?</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_items">items</code></td>
<td>
<p>Which items should be scored?  Note that if there are multiple scales, just specify the items on one scale at a time.  An item to be reversed scored can be specified by a minus sign. If long format, this is the column specifying item number. </p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_alpha">alpha</code></td>
<td>
<p>If TRUE, report alphas for every subject (default)</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_icc">icc</code></td>
<td>
<p>If TRUE, find ICCs for each person &ndash; can take a while</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_aov">aov</code></td>
<td>
<p>if FALSE, and if icc  is FALSE, then just draw the within subject plots</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_lmer">lmer</code></td>
<td>
<p>Should we use the lme4 package and lmer or just do the ANOVA?  Requires the
lme4 package to be installed.  Necessary to do crossed designs with missing data but takes a very long time.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_lme">lme</code></td>
<td>
<p>If TRUE, will find the nested components of variance.  Relatively fast.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_long">long</code></td>
<td>
<p>Are the data in wide (default) or long format.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_values">values</code></td>
<td>
<p>If the data are in long format, which column name (number) has the values to be analyzed?</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_na.action">na.action</code></td>
<td>
<p>How to handle missing data.  Passed to the lme function. </p>
</td></tr>   
<tr><td><code id="multilevel.reliability_+3A_plot">plot</code></td>
<td>
<p>If TRUE, show a lattice plot of the data by subject</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_extra">extra</code></td>
<td>
<p>Names or locations of extra columns to include in the long output.  These will be carried over from the wide form and duplicated for all items. See example.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_col">col</code></td>
<td>
<p>Color for the lines in mlPlot.  Note that items are categorical and thus drawn in alphabetical order. Order the colors appropriately.</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_type">type</code></td>
<td>
<p>The standard type for lines (p,l, b)</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_main">main</code></td>
<td>
<p>The main title for the plot (if drawn)</p>
</td></tr>
<tr><td><code id="multilevel.reliability_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to xyplot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Classical reliabiiity theory estimates the amount of variance in a set of observations due to a true score that varies over subjects.  Generalizability theory extends this model to include other sources of variance, specifically, time.  The classic studies using this approach are people measured over multiple time points with multiple items.  Then the question is, how stable are various individual differences. Intraclass correlations (ICC) are found for each subject over items, and for each subject over time. Alpha reliabilities  are found for each subject for the items across time.   
</p>
<p>More importantly, components of variance for people, items, time, and their interactions are found either by classical analysis of variance (aov) or by multilevel mixed effect modeling (lme).  These are then used to form several different estimates of generalizability.   Very thoughtful discussions of these procedure may be found in chapters by Shrout and Lane.  
</p>
<p>The variance components are the Between Person Variance <code class="reqn">\sigma^2_P</code>, the variance between items <code class="reqn">\sigma^2_I</code>, over time <code class="reqn">\sigma^2_T</code>,  and their interactions. 
</p>
<p>Then, <code class="reqn">RKF</code> is the  reliability of average of all ratings across all items and  times (Fixed time effects). (Shrout and Lane, Equation 6): 
</p>
<p style="text-align: center;"><code class="reqn">R_{kF} = \frac{\sigma^2_P + \sigma^2_{PI}/n.I}{\sigma^2_P + \sigma^2_{PI}/n.I + \sigma^2_e/(n.I n.P}</code>
</p>
 
<p>The generalizability of a single time point across all items (Random time effects) is just
</p>
<p style="text-align: center;"><code class="reqn">R_{1R} = \frac{\sigma^2_P + \sigma^2_{PI}/n.I}{\sigma^2_P + \sigma^2_{PI}/n.I + \sigma^2_T + \sigma^2_{PT}+ \sigma^2_e/(n.I)}</code>
</p>

<p>(Shrout and Lane equation 7 with a correction per Sean Lane.)
</p>
<p>Generalizability of average time points across all items (Random effects). (Shrout and Lane, equation 8)
</p>
<p style="text-align: center;"><code class="reqn">R_{kR} = \frac{\sigma^2_P + \sigma^2_{PI}/n.I}{\sigma^2_P + \sigma^2_{PI}/n.I + \sigma^2_T/n.T + \sigma^2_{PT}/n.T+ \sigma^2_e/n.I}</code>
</p>

<p>Generalizability of change scores (Shrout and Lane, equation  9)
</p>
<p style="text-align: center;"><code class="reqn">R_{C} = \frac{\sigma^2_{PT}}{\sigma^2_{PT} + \sigma^2_e/n.I}</code>
</p>
<p>.
</p>
<p>If the design may be thought of as fully crossed, then either aov or lmer can be used to estimate the components of variance.  With no missing data and a balanced design, these will give identical answers. However aov breaks down with missing data and seems to be very slow and very memory intensive for large problems ( 5,919  seconds for 209 cases with with 88 time points and three items on a Mac Powerbook with a 2.8 GHZ Intel Core I7). The slowdown probably is memory related, as the memory demands increased to 22.62 GB of compressed memory.   lmer will handle this design but is not nearly as slow  (242 seconds for the 209 cases with 88 time points and three items) as the aov approach.   
</p>
<p>If the design is thought of as nested, rather than crossed, the components of variance are found using the <code><a href="nlme.html#topic+lme">lme</a></code> function from nlme. This is very fast (114 cases with 88 time points and three items took 3.5 seconds). 
</p>
<p>The nested design leads to the generalizability of K random effects Nested (Shrout and Lane, equation 10):
</p>
<p style="text-align: center;"><code class="reqn">R_{KRN} = \frac{\sigma^2_P }{\sigma^2_P + \sigma^2_{T(P)}/n.I + \sigma^2_e/(n.I n.P}</code>
</p>
 
<p>And, finally, to the reliability of between person differences, averaged over items.  (Shrout and Lane, equation 11).
</p>
<p style="text-align: center;"><code class="reqn">R_{CN} = \frac{\sigma^2_{T(P)} }{\sigma^2_{T(P)} + \sigma^2_e/(n.I}</code>
</p>
 
<p>Unfortunately, when doing the nested analysis, <code><a href="nlme.html#topic+lme">lme</a></code> will sometimes issue an obnoxious error about failing to converge.  To fix this, turning off <code><a href="nlme.html#topic+lme">lme</a></code>  and just using lmer seems to solve the problem (i.e., set lme=FALSE and lmer=TRUE).  (<code><a href="nlme.html#topic+lme">lme</a></code> is part of core R and its namespace is automatically attached when loading <code><a href="#topic+psych">psych</a></code>). For many problems, lmer is not necessary and is thus not loaded.  However sometimes it is useful.  To use lmer it is necessary to have the lme4 package installed.  It will be automatically loaded if it is installed and requested. In the interests of making a 'thin' package, lmer is suggested,not required.
</p>
<p>The input can either be in 'wide' or 'long' form.  If in wide form, then specify the grouping variable, the 'time' variable, and the the column numbers or names of the items. (See the first example).  If in  long format, then what is the column (name or number) of the dependent variable.  (See the second example.)
</p>
<p><code><a href="#topic+mlArrange">mlArrange</a></code> takes a wide data.frame and organizes it into a &lsquo;long&rsquo; data.frame suitable for a lattice <code><a href="lattice.html#topic+xyplot">xyplot</a></code>.  This is a convenient alternative to <code><a href="utils.html#topic+stack">stack</a></code>, particularly for unbalanced designs.  The wide data frame is reorganized into a long data frame organized by grp (typically a subject id), by Time (typically a time varying variable, but can be anything, and then stacks the items within each person and time.  Extra variables are carried over and matched to the appropriate grp  and Time. 
</p>
<p>Thus, if we have N subjects over t time points for k items, in wide format for N * t rows where each row has k items and e extra pieces of information, we get a N x t * k row by 4 + e column dataframe.  The first four columns in the long output are id, time, values, and item names, the remaining columns are the extra values.  These  could be something such as a trait measure for each subject, or the situation in which the items are given.
</p>
<p><code><a href="#topic+mlArrange">mlArrange</a></code> plots k items over the  t time dimensions for each subject. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>n.obs</code></td>
<td>
<p>Number of individuals</p>
</td></tr>
<tr><td><code>n.time</code></td>
<td>
<p>Maximum number of time intervals</p>
</td></tr>
<tr><td><code>n.items</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>components</code></td>
<td>
<p>Components of variance associated with individuals, Time, Items, and their interactions.</p>
</td></tr>
<tr><td><code>RkF</code></td>
<td>
<p>Reliability of average of all ratings across all items and times (fixed effects).</p>
</td></tr>
<tr><td><code>R1R</code></td>
<td>
<p>Generalizability of a single time point across all items (Random effects)</p>
</td></tr>
<tr><td><code>RkR</code></td>
<td>
<p>Generalizability of average time points across all items (Random effects)</p>
</td></tr>
<tr><td><code>Rc</code></td>
<td>
<p>Generalizability of change scores over time.</p>
</td></tr>
<tr><td><code>RkRn</code></td>
<td>
<p> Generalizability of between person differences averaged over time and items</p>
</td></tr>  
<tr><td><code>Rcn</code></td>
<td>
<p>Generalizability of within person variations averaged over items (nested structure)</p>
</td></tr>
<tr><td><code>ANOVA</code></td>
<td>
<p>The summary anova table from which the components are found (if done),</p>
</td></tr>
<tr><td><code>s.lmer</code></td>
<td>
<p>The summary of the lmer analysis (if done),</p>
</td></tr>
<tr><td><code>s.lme</code></td>
<td>
<p>The summary of the lme analysis (if done),</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Within subject alpha over items and time.</p>
</td></tr>
<tr><td><code>summary.by.person</code></td>
<td>
<p>Summary table of ICCs organized by person,</p>
</td></tr>
<tr><td><code>summary.by.time</code></td>
<td>
<p>Summary table of ICCs organized by time.</p>
</td></tr>
<tr><td><code>ICC.by.person</code></td>
<td>
<p>A rather long list of ICCs by person.</p>
</td></tr>
<tr><td><code>ICC.by.time</code></td>
<td>
<p>Another long list of ICCs, this time for each time period,</p>
</td></tr>
<tr><td><code>long</code></td>
<td>
<p>The data (x) have been rearranged into long form for graphics or for further analyses using lme, lmer, or aov that require long form.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Bolger, Niall and Laurenceau, Jean-Phillippe, (2013) Intensive longitudinal models.  New York.  Guilford Press. 
</p>
<p>Cranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., &amp; Bolger, N. (2006). A procedure for evaluating sensitivity to within-person change: Can mood measures in diary studies detect change reliably? Personality and Social Psychology Bulletin, 32(7), 917-929.
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.  <a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Wilt, J. (2017) Analyzing dynamic data: a tutorial. Personality and Individual Differences. DOI: 10.1016/j.paid.2017.08.020
</p>
<p>Shrout, Patrick and Lane, Sean P (2012), Psychometrics.  In M.R. Mehl and T.S. Conner (eds)  Handbook of research methods for studying daily life, (p 302-320) New York. Guilford Press
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sim.multi">sim.multi</a></code> and <code><a href="#topic+sim.multilevel">sim.multilevel</a></code> to generate multilevel data, <code><a href="#topic+statsBy">statsBy</a></code> a for statistics for multi level analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data from Shrout and Lane, 2012.

shrout &lt;- structure(list(Person = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), Time = c(1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
4L, 4L), Item1 = c(2L, 3L, 6L, 3L, 7L, 3L, 5L, 6L, 3L, 8L, 4L, 
4L, 7L, 5L, 6L, 1L, 5L, 8L, 8L, 6L), Item2 = c(3L, 4L, 6L, 4L, 
8L, 3L, 7L, 7L, 5L, 8L, 2L, 6L, 8L, 6L, 7L, 3L, 9L, 9L, 7L, 8L
), Item3 = c(6L, 4L, 5L, 3L, 7L, 4L, 7L, 8L, 9L, 9L, 5L, 7L, 
9L, 7L, 8L, 4L, 7L, 9L, 9L, 6L)), .Names = c("Person", "Time", 
"Item1", "Item2", "Item3"), class = "data.frame", row.names = c(NA, 
-20L))

#make shrout super wide
#Xwide &lt;- reshape(shrout,v.names=c("Item1","Item2","Item3"),timevar="Time", 
#direction="wide",idvar="Person")
#add more helpful Names
#colnames(Xwide ) &lt;- c("Person",c(paste0("Item",1:3,".T",1),paste0("Item",1:3,".T",2), 
#paste0("Item",1:3,".T",3),paste0("Item",1:3,".T",4)))
#make superwide into normal form  (i.e., just return it to the original shrout data
#Xlong &lt;-Xlong &lt;- reshape(Xwide,idvar="Person",2:13)

#Now use these data for a multilevel repliability study, use the normal wide form output
mg &lt;- mlr(shrout,grp="Person",Time="Time",items=3:5) 
#which is the same as 
#mg &lt;- multilevel.reliability(shrout,grp="Person",Time="Time",items=
#         c("Item1","Item2","Item3"),plot=TRUE)
#to show the lattice plot by subjects, set plot = TRUE

#Alternatively for long input (returned in this case from the prior run)
mlr(mg$long,grp="id",Time ="time",items="items", values="values",long=TRUE)

#example of mlArrange
#First, add two new columns to shrout and 
#then convert to long output using mlArrange
total &lt;- rowSums(shrout[3:5])
caseid &lt;- rep(paste0("ID",1:5),4)
new.shrout &lt;- cbind(shrout,total=total,case=caseid)
#now convert to long
new.long &lt;- mlArrange(new.shrout,grp="Person",Time="Time",items =3:5,extra=6:7)
headTail(new.long,6,6)
</code></pre>

<hr>
<h2 id='omega'>  Calculate McDonald's  omega estimates of general and total factor saturation </h2><span id='topic+omega'></span><span id='topic+omegaSem'></span><span id='topic+omegaFromSem'></span><span id='topic+omegah'></span><span id='topic+omegaDirect'></span><span id='topic+directSl'></span>

<h3>Description</h3>

<p>McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>omega(m,nfactors=3,fm="minres",n.iter=1,p=.05,poly=FALSE,key=NULL,
    flip=TRUE,digits=2, title="Omega",sl=TRUE,labels=NULL,
    plot=TRUE,n.obs=NA,rotate="oblimin",Phi=NULL,option="equal",covar=FALSE, ...)
omegaSem(m,nfactors=3,fm="minres",key=NULL,flip=TRUE,digits=2,title="Omega",
  sl=TRUE,labels=NULL, plot=TRUE,n.obs=NA,rotate="oblimin",
  Phi = NULL, option="equal",lavaan=TRUE,...)
  
omegah(m,nfactors=3,fm="minres",key=NULL,flip=TRUE, 
digits=2,title="Omega",sl=TRUE,labels=NULL, plot=TRUE,
   n.obs=NA,rotate="oblimin",Phi = NULL,option="equal",covar=FALSE,two.ok=FALSE,...) 

omegaFromSem(fit,m=NULL,flip=TRUE,plot=TRUE)
omegaDirect(m,nfactors=3,fm="minres",rotate="oblimin",cut=.3,
   plot=TRUE,main="Direct Schmid Leiman")
directSl(m,nfactors=3,fm="minres",rotate="oblimin",cut=.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="omega_+3A_m">m</code></td>
<td>
<p>A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix </p>
</td></tr>
<tr><td><code id="omega_+3A_nfactors">nfactors</code></td>
<td>
<p>Number of factors believed to be group factors</p>
</td></tr>
<tr><td><code id="omega_+3A_n.iter">n.iter</code></td>
<td>
<p>How many replications to do in omega for bootstrapped estimates</p>
</td></tr>
<tr><td><code id="omega_+3A_fm">fm</code></td>
<td>
<p>factor method (the default is minres)  fm=&quot;pa&quot; for principal axes, fm=&quot;minres&quot; for a minimum residual (OLS) solution, fm=&quot;pc&quot; for principal components (see note), or  fm=&quot;ml&quot; for maximum likelihood.</p>
</td></tr>
<tr><td><code id="omega_+3A_poly">poly</code></td>
<td>
<p>should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations</p>
</td></tr>
<tr><td><code id="omega_+3A_key">key</code></td>
<td>
<p>a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.</p>
</td></tr>
<tr><td><code id="omega_+3A_flip">flip</code></td>
<td>
<p>If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.</p>
</td></tr>
<tr><td><code id="omega_+3A_p">p</code></td>
<td>
<p>probability of two tailed conference boundaries</p>
</td></tr>
<tr><td><code id="omega_+3A_digits">digits</code></td>
<td>
<p>if specified, round the output to digits</p>
</td></tr>
<tr><td><code id="omega_+3A_title">title</code></td>
<td>
<p>Title for this analysis</p>
</td></tr>
<tr><td><code id="omega_+3A_main">main</code></td>
<td>
<p>main for this analysis  (directSl)</p>
</td></tr>
<tr><td><code id="omega_+3A_cut">cut</code></td>
<td>
<p>Loadings greater than cut are used in directSl</p>
</td></tr>
<tr><td><code id="omega_+3A_sl">sl</code></td>
<td>
<p>If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)</p>
</td></tr>
<tr><td><code id="omega_+3A_labels">labels</code></td>
<td>
<p>If plotting, what labels should be applied to the variables? If not specified, will default to the column names.</p>
</td></tr>
<tr><td><code id="omega_+3A_plot">plot</code></td>
<td>
<p>plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then <code><a href="#topic+omega.graph">omega.graph</a></code> may be used separately.</p>
</td></tr>
<tr><td><code id="omega_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations - used for goodness of fit statistic</p>
</td></tr>
<tr><td><code id="omega_+3A_rotate">rotate</code></td>
<td>
<p>What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See <code><a href="#topic+target.rot">target.rot</a></code>)</p>
</td></tr>
<tr><td><code id="omega_+3A_phi">Phi</code></td>
<td>
<p>If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).</p>
</td></tr>
<tr><td><code id="omega_+3A_option">option</code></td>
<td>
<p>In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  <code><a href="#topic+schmid">schmid</a></code> for treating the case of two group factors.</p>
</td></tr>
<tr><td><code id="omega_+3A_covar">covar</code></td>
<td>
<p>defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.</p>
</td></tr>
<tr><td><code id="omega_+3A_two.ok">two.ok</code></td>
<td>
<p>If TRUE, do not give a warning about 3 factors being required. </p>
</td></tr>
<tr><td><code id="omega_+3A_lavaan">lavaan</code></td>
<td>
<p>if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. </p>
</td></tr>
<tr><td><code id="omega_+3A_fit">fit</code></td>
<td>
<p>The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.</p>
</td></tr>
<tr><td><code id="omega_+3A_...">...</code></td>
<td>
<p>Allows additional parameters to be passed through to the factor routines.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>&ldquo;Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an 
interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. 
</p>
<p>The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her 
observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale.&quot; (Zinbarg, Yovel, Revelle, and McDonald, 2006).
</p>
<p>McDonald has proposed coefficient omega_hierarchical (<code class="reqn">\omega_h</code>) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) 
<a href="https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf">https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf</a> compare McDonald's <code class="reqn">\omega_h</code> to Cronbach's <code class="reqn">\alpha</code> and Revelle's <code class="reqn">\beta</code>.  They conclude that <code class="reqn">\omega_h</code> is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   
</p>
<p>One way to find <code class="reqn">\omega_h</code> is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (<a href="#topic+schmid">schmid</a>) transformation to find general factor loadings, and then find <code class="reqn">\omega_h</code>.  Here we present code to do that.  
</p>
<p><code class="reqn">\omega_h</code> differs as a function of how the factors are estimated.  Four options are available, three use the <code><a href="#topic+fa">fa</a></code> function but with different factoring methods: the default does a minres factor solution, fm=&quot;pa&quot;  does a principle axes factor analysis  fm=&quot;mle&quot; does a maximum likelihood solution; fm=&quot;pc&quot; does a principal components analysis using (<code><a href="#topic+principal">principal</a></code>).  
</p>
<p>For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the <code><a href="#topic+scoreItems">scoreItems</a></code> function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).
</p>
<p><code class="reqn">\beta</code>, an alternative to <code class="reqn">\omega_h</code>, is defined as the worst split half 
reliability (Revelle, 1979).  It can be estimated by using <code><a href="#topic+ICLUST">ICLUST</a></code> (a 
hierarchical clustering algorithm originally developed for main frames and written in 
Fortran and that is now part of the psych package.  (For a very complimentary review of 
why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). 
</p>
<p>The <code><a href="#topic+omega">omega</a></code> function uses exploratory factor analysis to estimate the <code class="reqn">\omega_h</code> coefficient.  It is important to remember that  &ldquo;A recommendation that should be heeded, regardless of the method chosen to estimate <code class="reqn">\omega_h</code>, is to always examine the pattern of the estimated general factor loadings prior to estimating <code class="reqn">\omega_h</code>. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading <code class="reqn">\omega_h</code> estimates occasionally produced in the simulations reported here.&quot; (Zinbarg et al., 2006, p 137).
</p>
<p>A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  
</p>
<p>Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (<a href="#topic+describe">describe</a>) the $schmid$sl results.
</p>
<p>Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  
</p>
<p>There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be &quot;equal&quot; which will be the sqrt(oblique correlations between the factors) or to &quot;first&quot; or &quot;second&quot; in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option=&quot;first&quot; or option=&quot;second&quot; to the call.
</p>
<p>Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.
</p>
<p>In addition to <code class="reqn">\omega_h</code>, another of McDonald's coefficients is <code class="reqn">\omega_t</code>.  This is an estimate of the total reliability of a test. 
</p>
<p>McDonald's <code class="reqn">\omega_t</code>, which is similar to Guttman's <code class="reqn">\lambda_6</code>, <code><a href="#topic+guttman">guttman</a></code> but uses the estimates of uniqueness (<code class="reqn">u^2</code>) from factor analysis to find <code class="reqn">e_j^2</code>. This is based on a decomposition of the variance of a test score, <code class="reqn">V_x</code>  into four parts: that due to a general factor, <code class="reqn">\vec{g}</code>, that due to a set of group factors, <code class="reqn">\vec{f}</code>,  (factors common to some but not all of the items), specific factors, <code class="reqn">\vec{s}</code> unique to each item, and <code class="reqn">\vec{e}</code>, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). 
</p>
<p>Letting <code class="reqn">\vec{x} =  \vec{cg} + \vec{Af} + \vec {Ds} + \vec{e}</code>
then the communality of item<code class="reqn">_j</code>, based upon general as well as group factors,
<code class="reqn">h_j^2 = c_j^2 + \sum{f_{ij}^2}</code>
and the unique variance for the item
<code class="reqn">u_j^2 = \sigma_j^2 (1-h_j^2)</code>
may be used to estimate the test reliability.
That is, if <code class="reqn">h_j^2</code> is the communality of item<code class="reqn">_j</code>, based upon general as well as group factors,  then for standardized items,  <code class="reqn">e_j^2 = 1 - h_j^2</code> and
</p>
<p style="text-align: center;"><code class="reqn">
\omega_t = \frac{\vec{1}\vec{cc'}\vec{1} + \vec{1}\vec{AA'}\vec{1}'}{V_x} = 1 - \frac{\sum(1-h_j^2)}{V_x} = 1 - \frac{\sum u^2}{V_x}</code>
</p>

<p>Because <code class="reqn">h_j^2 \geq r_{smc}^2</code>, <code class="reqn">\omega_t \geq \lambda_6</code>.
</p>
<p>It is important to distinguish here between the two <code class="reqn">\omega</code> coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, <code class="reqn">\omega_t</code> and <code class="reqn">\omega_h</code>.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. 
</p>
<p style="text-align: center;"><code class="reqn">\omega_h = \frac{ \vec{1}\vec{cc'}\vec{1}}{V_x}</code>
</p>

<p>Another estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by 
</p>
<p style="text-align: center;"><code class="reqn">\omega_{limit} = \frac{\vec{1}\vec{cc'}\vec{1}}{\vec{1}\vec{cc'}\vec{1} + \vec{1}\vec{AA'}\vec{1}'}</code>
</p>
<p>. 
</p>
<p>Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.
</p>
<p>The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  
</p>
<p><code><a href="#topic+omega">omega</a></code> is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  <code><a href="#topic+omegaSem">omegaSem</a></code> first calls <code><a href="#topic+omega">omega</a></code> and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  <code class="reqn">\omega_h</code> is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   <code class="reqn">R^2</code> will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)
</p>
<p>In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. 
</p>
<p>It is also possible to give <code><a href="#topic+omega">omega</a></code> a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  
</p>
<p><code><a href="#topic+omegaFromSem">omegaFromSem</a></code> takes the output from a sem model and uses it to find 
<code class="reqn">\omega_h</code>.  The estimate of factor indeterminacy, found by the multiple <code class="reqn">R^2</code> of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   <code class="reqn">R^2</code> will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)
</p>
<p>The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.
</p>
<p>Finally, and still being tested, is <code><a href="#topic+omegaDirect">omegaDirect</a></code> adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (<code><a href="#topic+directSl">directSl</a></code>).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+omegaSem">omegaSem</a></code>.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.
</p>
<p>Moral: Finding omega_h is tricky and one should probably compare <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+omegaSem">omegaSem</a></code>,  <code><a href="#topic+omegaDirect">omegaDirect</a></code> and even <code><a href="#topic+iclust">iclust</a></code> solutions to understand the differences.
</p>
<p>The summary of the omega object is a reduced set of the most useful output. 
</p>
<p>The various objects returned from omega include:
</p>


<h3>Value</h3>

<table>
<tr><td><code>omega hierarchical</code></td>
<td>
<p>The <code class="reqn">\omega_h</code> coefficient</p>
</td></tr>
<tr><td><code>omega.lim</code></td>
<td>
<p>The limit of <code class="reqn">\omega_h</code> as the test becomes infinitly large</p>
</td></tr>
<tr><td><code>omega total</code></td>
<td>
<p>The <code class="reqn">omega_t</code> coefficient</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Cronbach's <code class="reqn">\alpha</code></p>
</td></tr>
<tr><td><code>schmid</code></td>
<td>
<p>The Schmid Leiman transformed factor matrix and associated matrices</p>
</td></tr>
<tr><td><code>schmid$sl</code></td>
<td>
<p>The g factor loadings as well as the residualized factors</p>
</td></tr>
<tr><td><code>schmid$orthog</code></td>
<td>
<p>Varimax rotated solution of the original factors</p>
</td></tr>
<tr><td><code>schmid$oblique</code></td>
<td>
<p>The oblimin or promax transformed factors</p>
</td></tr>
<tr><td><code>schmid$phi</code></td>
<td>
<p>the correlation matrix of the oblique factors</p>
</td></tr>
<tr><td><code>schmid$gloading</code></td>
<td>
<p>The loadings on the higher order, g, factor of the oblimin factors</p>
</td></tr>
<tr><td><code>key</code></td>
<td>
<p>A vector of -1 or 1 showing which direction the items were scored.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>a list of two elements, one suitable to give  to the sem function for structure equation models, the other, to give to the lavaan package. </p>
</td></tr>
<tr><td><code>sem</code></td>
<td>
<p>The output from a sem analysis</p>
</td></tr>
<tr><td><code>omega.group</code></td>
<td>
<p>The summary statistics for the omega total, omega hierarchical (general) and omega within each group.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>Factor score estimates are found for the Schmid-Leiman solution.  To get 
scores for the hierarchical model see the note.</p>
</td></tr> 
<tr><td><code>various fit statistics</code></td>
<td>
<p>various fit statistics, see output</p>
</td></tr>
<tr><td><code>OmegaSem</code></td>
<td>
<p> is an object that contains the fits for the OmegaSem output.</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>The direct SL rotated object (from omegaDirect)</p>
</td></tr>
<tr><td><code>orth.f</code></td>
<td>
<p>The original, unrotated solution from omegaDirect</p>
</td></tr>
<tr><td><code>Target</code></td>
<td>
<p>The cluster based target for rotation in directSl</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Requires the GPArotation package.
</p>
<p>The default rotation uses oblimin from the GPArotation package.  Alternatives include the simplimax function, as well as <code><a href="#topic+Promax">Promax</a></code> or the <code><a href="stats.html#topic+promax">promax</a></code> rotations.  promax will do a Kaiser normalization before applying Promax rotation.
</p>
<p>If the factor solution leads to an exactly orthogonal solution (probably only for demonstration data sets), then use the rotate=&quot;Promax&quot; option to get a solution.
</p>
<p><code><a href="#topic+omegaSem">omegaSem</a></code> requires the sem or lavaan packages.  <code><a href="#topic+omegaFromSem">omegaFromSem</a></code> uses the output from the sem or lavaan package.
</p>
<p><code><a href="#topic+omega">omega</a></code> may be run on raw data (finding either  Pearson or  tetrachoric/polychoric corrlations, depending upon the poly option) a correlation matrix, a polychoric correlation matrix (found by e.g., <code><a href="#topic+polychoric">polychoric</a></code>), or the output of a previous omega run.  This last case is particularly useful when working with categorical data using the poly=TRUE option.  For in this case, most of the time is spent in finding the correlation matrix.  The matrix is saved as part of the omega output and may be used as input for subsequent runs.  A similar feature is found in <code><a href="#topic+irt.fa">irt.fa</a></code> where the output of one analysis can be taken as the input to the subsequent analyses.  
</p>
<p>However, simulations based upon tetrachoric and polychoric correlations suggest that although the structure is better defined, that the estimates of omega are inflated over the true general factor saturation.
</p>
<p>Omega returns factor scores based upon the Schmid-Leiman transformation.  To get the hierarchical factor scores, it is necessary to do this outside of omega. See the example (not run).
</p>
<p>Consider the case of the raw data in an object data.  Then 
</p>
<p>f3 &lt;- fa(data,3,scores=&quot;tenBerge&quot;, oblique.rotation=TRUE
f1 &lt;- fa(f3$scores)
hier.scores &lt;- data.frame(f1$scores,f3$scores)
</p>
<p>When doing fm=&quot;pc&quot;, principal components are done for the original correlation matrix, but minres is used when examining the intercomponent correlations.   A warning is issued that the method was changed to minres for the higher order solution.  omega is a factor model, and finding loadings using principal components will overestimate the resulting solution.  This is particularly problematic for the amount of group saturation, and thus the omega.group statistics are overestimates.  
</p>
<p>The last three lines of omega report &quot;Total, General and Subset omega for each subset&quot;.  These are available as the omega.group object in the output.
</p>
<p>The last of these (omega group)  is effectively what Steve Reise calls omegaS for the subset omega.
</p>
<p>The omega general is the amount of variance in the group that is accounted for by the general factor, the omega total is the amount of variance in the group accounted for by general + group.
</p>
<p>This is based upon a cluster solution (that is to say, every item is assigned to one group) and this is why for  first column the omega general and group do not add up to omega total.  Some of the variance is found in the cross loadings between groups.
</p>
<p>Reise and others like to report the ratio of the second line to the first line (what portion of the reliable variance is general factor) and the third row to the first (what portion of the reliable variance is within group but not general.  This may be found by using the omega.group object that is returned by omega. (See the last example.)
</p>
<p>If using the lavaan=TRUE option in <code><a href="#topic+omegaSem">omegaSem</a></code> please note that variable names can not start with a digit (e.g. 4.Letter.Words in the  <code><a href="#topic+Thurstone">Thurstone</a></code> data set. The leading digit needs to be removed. 
</p>
<p><code><a href="#topic+omegaSem">omegaSem</a></code> will do an exploratory efa and omega, create (and return) the commands for doing either a sem or lavaan analysis.  The commands are returned as the model object.  This can be used for further sem/lavaan analyses.  
</p>
<p>Omega can also be found from  an analysis  done using lavaan or sem directly by calling  <code><a href="#topic+omegaFromSem">omegaFromSem</a></code> with the original correlation matrix and the fit of the sem/lavaan model.  See the last (not run) example)
</p>


<h3>Author(s)</h3>

 
<p><a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
Maintainer: William Revelle  <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> 
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.omega.html">https://personality-project.org/r/r.omega.html</a> <br />
</p>
<p>Jensen, Arthur R. and Li-Jen  Weng (1994) What is a good g?  Intelligence, 18, 3, 231-258
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Revelle, W. (1979).  Hierarchical cluster analysis and the internal structure of tests. Multivariate Behavioral Research, 14, 57-74. (<a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a>)
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.  <a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma.  Psychometrika, 74, 1, 145-154. (<a href="https://personality-project.org/revelle/publications/rz09.pdf">https://personality-project.org/revelle/publications/rz09.pdf</a>
</p>
<p>Waller, N. G. (2017) Direct Schmid-Leiman Transformations and Rank-Deficient Loadings Matrices.  Psychometrika.  DOI: 10.1007/s11336-017-9599-0
</p>
<p>Zinbarg, R.E., Revelle, W., Yovel, I., &amp; Li. W.  (2005). Cronbach's Alpha, Revelle's Beta, McDonald's Omega: Their relations with each and two alternative conceptualizations of reliability. Psychometrika. 70, 123-133.  <a href="https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf">https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf</a>
</p>
<p>Zinbarg, R., Yovel, I. &amp; Revelle, W.  (2007).  Estimating omega  for structures containing two group factors:  Perils and prospects.  Applied Psychological Measurement. 31 (2), 135-157.
</p>
<p>Zinbarg, R., Yovel, I., Revelle, W. &amp; McDonald, R. (2006).  Estimating generalizability to a universe of indicators that all have one attribute in common:  A comparison of estimators for omega.  Applied Psychological Measurement, 30, 121-144. DOI: 10.1177/0146621605278814
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+omega.graph">omega.graph</a></code> for a dot code graphic.   <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+ICLUST.diagram">ICLUST.diagram</a></code> for hierarchical cluster analysis, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+nfactors">nfactors</a></code>, <code><a href="#topic+fa.parallel">fa.parallel</a></code> for alternative ways of estimating the appropriate number of factors. <code><a href="#topic+schmid">schmid</a> </code> for the decomposition used in omega.
<code><a href="#topic+make.hierarchical">make.hierarchical</a></code> to simulate a hierarchical model.
</p>
<p><code><a href="#topic+fa.multi">fa.multi</a></code> for hierarchical factor analysis with an arbitrary number of 2nd  order factors. 
</p>
<p><code><a href="#topic+reliability">reliability</a></code> to do multiple omega analysis at the same time on different subsets of items.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
 test.data &lt;- Harman74.cor$cov
# if(!require(GPArotation)) {message("Omega requires GPA rotation" )} else {
      my.omega &lt;- omega(test.data)       
      print(my.omega,digits=2)
#}
 
#create 9 variables with a hierarchical structure
v9 &lt;- sim.hierarchical()  
#with correlations of
round(v9,2)
#find omega 
v9.omega &lt;- omega(v9,digits=2)
v9.omega

#create 8 items with a two factor solution, showing the use of the flip option
sim2 &lt;- item.sim(8)
omega(sim2)   #an example of misidentification-- remember to look at the loadings matrices.
omega(sim2,2)  #this shows that in fact there is no general factor
omega(sim2,2,option="first") #but, if we define one of the two group factors 
     #as a general factor, we get a falsely high omega 
#apply omega to analyze 6 mental ability tests 
data(ability.cov)   #has a covariance matrix
omega(ability.cov$cov)

#om &lt;- omega(Thurstone)
#round(om$omega.group,2)
#round(om$omega.group[2]/om$omega.group[1],2)  #fraction of reliable that is general variance
# round(om$omega.group[3]/om$omega.group[1],2)  #fraction of reliable that is group variance

#To find factor score estimates for the hierarchical model it is necessary to 
#do two extra steps.

#Consider the case of the raw data in an object data.  (An example from simulation)
# set.seed(42)
# gload &lt;- matrix(c(.9,.8,.7),nrow=3)
# fload &lt;- matrix(c(.8,.7,.6,rep(0,9),.7,.6,.5,rep(0,9),.7,.6,.4),   ncol=3)
# data &lt;- sim.hierarchical(gload=gload,fload=fload, n=100000, raw=TRUE)
# 
# f3 &lt;- fa(data$observed,3,scores="tenBerge", oblique.scores=TRUE)
# f1 &lt;- fa(f3$scores)

# om &lt;- omega(data$observed,sl=FALSE) #draw the hierarchical figure
# The scores from om are based upon the Schmid-Leiman factors and although the g factor 
# is identical, the group factors are not.
# This is seen in the following correlation matrix
# hier.scores &lt;- cbind(om$scores,f1$scores,f3$scores)
# lowerCor(hier.scores)
#
#this next set of examples require lavaan
#jensen &lt;- sim.hierarchical()   #create a hierarchical structure (same as v9 above)
#om.jen &lt;- omegaSem(jensen,lavaan=TRUE)  #do the exploratory omega with confirmatory as well
#lav.mod &lt;- om.jen$omegaSem$model$lavaan #get the lavaan code or create it yourself
# lav.mod &lt;- 'g =~ +V1+V2+V3+V4+V5+V6+V7+V8+V9
#              F1=~  + V1 + V2 + V3             
#              F2=~  + V4 + V5 + V6 
#              F3=~  + V7 + V8 + V9 '  
#lav.jen &lt;- cfa(lav.mod,sample.cov=jensen,sample.nobs=500,orthogonal=TRUE,std.lv=TRUE)
# omegaFromSem(lav.jen,jensen)
#the directSl solution
#direct.jen &lt;- directSl(jen)
#direct.jen 

#try a one factor solution -- this is not recommended, but sometimes done
#it will just give omega_total
# lav.mod.1 &lt;- 'g =~ +V1+V2+V3+V4+V5+V6+V7+V8+V9 '  
#lav.jen.1&lt;- cfa(lav.mod.1,sample.cov=jensen,sample.nobs=500,orthogonal=TRUE,std.lv=TRUE)
# omegaFromSem(lav.jen.1,jensen)




## End(Not run)
</code></pre>

<hr>
<h2 id='omega.graph'>Graph hierarchical factor structures </h2><span id='topic+omega.diagram'></span><span id='topic+omega.graph'></span>

<h3>Description</h3>

<p>Hierarchical factor structures represent the correlations between variables in terms of a smaller set of correlated factors which themselves can be represented by a higher order factor.
</p>
<p>Two alternative solutions to such structures are found by the <code><a href="#topic+omega">omega</a></code> function.  The correlated factors solutions represents the effect of the higher level, general factor, through its effect on the correlated factors.  The other representation makes use of the Schmid Leiman transformation to find the direct effect of the general factor upon the original variables as well as the effect of orthogonal residual group factors upon the items. 
</p>
<p>Graphic presentations of these two alternatives are helpful in understanding the structure.  omega.graph  and omega.diagram draw both such structures.  Graphs are drawn directly onto the graphics window or expressed in &ldquo;dot&quot; commands for conversion to graphics using implementations of Graphviz (if using omega.graph).
</p>
<p>Using Graphviz allows the user to clean up the Rgraphviz output. However, if Graphviz and Rgraphviz are not available, use omega.diagram.  
</p>
<p>See the other structural diagramming functions, <code><a href="#topic+fa.diagram">fa.diagram</a></code> and <code><a href="#topic+structure.diagram">structure.diagram</a></code>. 
</p>
<p>In addition 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>omega.diagram(om.results,sl=TRUE,sort=TRUE,labels=NULL,flabels=NULL,cut=.2,
gcut=.2,simple=TRUE,  errors=FALSE, digits=1,e.size=.1,rsize=.15,side=3,
    main=NULL,cex=NULL,color.lines=TRUE,marg=c(.5,.5,1.5,.5),adj=2, ...) 
omega.graph(om.results, out.file = NULL,  sl = TRUE, labels = NULL, size = c(8, 6), 
    node.font = c("Helvetica", 14), edge.font = c("Helvetica", 10),  
    rank.direction=c("RL","TB","LR","BT"), digits = 1, title = "Omega", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="omega.graph_+3A_om.results">om.results</code></td>
<td>
<p>The output from the omega function </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_out.file">out.file</code></td>
<td>
<p> Optional output file for off line analysis using Graphviz </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_sl">sl</code></td>
<td>
<p> Orthogonal clusters using the Schmid-Leiman transform (sl=TRUE) or oblique clusters </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_labels">labels</code></td>
<td>
<p> variable labels </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_flabels">flabels</code></td>
<td>
<p>Labels for the factors (not counting g)</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_size">size</code></td>
<td>
<p>size of graphics window </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_node.font">node.font</code></td>
<td>
<p> What font to use for the items</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_edge.font">edge.font</code></td>
<td>
<p>What font to use for the edge labels </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_rank.direction">rank.direction</code></td>
<td>
<p> Defaults to left to right </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_digits">digits</code></td>
<td>
<p> Precision of labels </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_cex">cex</code></td>
<td>
<p>control font size</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_color.lines">color.lines</code></td>
<td>
<p>Use black for positive, red for negative</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_marg">marg</code></td>
<td>
<p>The margins for the figure are set to be wider than normal by default</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_adj">adj</code></td>
<td>
<p>Adjust the location of the factor loadings to vary as factor mod 4 + 1</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_title">title</code></td>
<td>
<p> Figure title </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_main">main</code></td>
<td>
<p> main figure caption </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_...">...</code></td>
<td>
<p>Other options to pass into the graphics packages </p>
</td></tr>
<tr><td><code id="omega.graph_+3A_e.size">e.size</code></td>
<td>
<p>the size to draw the ellipses for the factors. This is scaled by the  number of variables.</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_cut">cut</code></td>
<td>
<p>Minimum path coefficient to draw</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_gcut">gcut</code></td>
<td>
<p>Minimum general factor path to draw</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_simple">simple</code></td>
<td>
<p>draw just one path per item</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_sort">sort</code></td>
<td>
<p>sort the solution before making the diagram</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_side">side</code></td>
<td>
<p>on which side should errors  be drawn?</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_errors">errors</code></td>
<td>
<p>show the error estimates</p>
</td></tr>
<tr><td><code id="omega.graph_+3A_rsize">rsize</code></td>
<td>
<p>size of the rectangles</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While omega.graph requires the Rgraphviz package, omega.diagram does not.  <code><a href="#topic+omega">omega</a></code> requires the GPArotation package.
</p>


<h3>Value</h3>

<table>
<tr><td><code>clust.graph</code></td>
<td>
<p>A graph object</p>
</td></tr>
<tr><td><code>sem</code></td>
<td>
<p>A matrix suitable to be run throughe the sem function in the sem package.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> omega.graph requires rgraphviz.   &ndash; omega requires GPArotation
</p>


<h3>Author(s)</h3>

 
<p><a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
Maintainer: William Revelle  <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> 
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.omega.html">https://personality-project.org/r/r.omega.html</a> <br />
</p>
<p>Revelle, W. (in preparation) An Introduction to  Psychometric Theory with applications in R.  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>
<p>Revelle, W. (1979).  Hierarchical cluster analysis and the internal structure of tests. Multivariate Behavioral Research, 14, 57-74. (<a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a>)
</p>
<p>Zinbarg, R.E., Revelle, W., Yovel, I., &amp; Li. W.  (2005). Cronbach's Alpha, Revelle's Beta, McDonald's Omega: Their relations with each and two alternative conceptualizations of reliability. Psychometrika. 70, 123-133.  <a href="https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf">https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf</a>
</p>
<p>Zinbarg, R., Yovel, I., Revelle, W. &amp; McDonald, R. (2006).  Estimating generalizability to a universe of indicators that all have one attribute in common:  A comparison of estimators for omega.  Applied Psychological Measurement, 30, 121-144. DOI: 10.1177/0146621605278814 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+omega">omega</a></code>,  <code><a href="#topic+make.hierarchical">make.hierarchical</a></code>, <code><a href="#topic+ICLUST.rgraph">ICLUST.rgraph</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#24 mental tests from Holzinger-Swineford-Harman
if(require(GPArotation) ) {om24 &lt;- omega(Harman74.cor$cov,4) } #run omega

#
#example hierarchical structure from Jensen and Weng
if(require(GPArotation) ) {jen.omega &lt;- omega(make.hierarchical())}


</code></pre>

<hr>
<h2 id='outlier'>Find and graph Mahalanobis squared distances to detect outliers</h2><span id='topic+outlier'></span>

<h3>Description</h3>

<p>The Mahalanobis distance is <code class="reqn">D^2 = (x-\mu)' \Sigma^-1 (x-\mu)</code> where <code class="reqn">\Sigma</code> is the covariance of the x matrix.  D2 may be used as a way of detecting outliers in distribution.  Large D2 values, compared to the expected Chi Square values indicate an unusual response pattern.  The mahalanobis function in stats does not handle missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outlier(x, plot = TRUE, bad = 5,na.rm = TRUE, xlab, ylab, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outlier_+3A_x">x</code></td>
<td>
<p>A data matrix or data.frame</p>
</td></tr>
<tr><td><code id="outlier_+3A_plot">plot</code></td>
<td>
<p>Plot the resulting QQ graph</p>
</td></tr>
<tr><td><code id="outlier_+3A_bad">bad</code></td>
<td>
<p>Label the bad worst values</p>
</td></tr>
<tr><td><code id="outlier_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing data be deleted</p>
</td></tr>
<tr><td><code id="outlier_+3A_xlab">xlab</code></td>
<td>
<p>Label for x axis</p>
</td></tr>
<tr><td><code id="outlier_+3A_ylab">ylab</code></td>
<td>
<p>Label for y axis</p>
</td></tr>
<tr><td><code id="outlier_+3A_...">...</code></td>
<td>
<p>More graphic parameters, e.g., cex=.8</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adapted from the mahalanobis function and help page from stats.
</p>


<h3>Value</h3>

<p>The D2 values for each case</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Yuan, Ke-Hai and Zhong, Xiaoling, (2008) Outliers, Leverage Observations, and Influential Cases in Factor Analysis: Using Robust Procedures to Minimize Their Effect, Sociological Methodology, 38, 329-368.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+mahalanobis">mahalanobis</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#first, just find and graph the outliers
d2 &lt;- outlier(sat.act)
#combine with the data frame and plot it with the outliers highlighted in blue
sat.d2 &lt;- data.frame(sat.act,d2)
pairs.panels(sat.d2,bg=c("yellow","blue")[(d2 &gt; 25)+1],pch=21)
</code></pre>

<hr>
<h2 id='p.rep'>Find the probability of replication for an F, t, or r and estimate effect size </h2><span id='topic+p.rep'></span><span id='topic+p.rep.f'></span><span id='topic+p.rep.t'></span><span id='topic+p.rep.r'></span>

<h3>Description</h3>

<p>The probability of replication of an experimental or correlational finding as discussed by Peter Killeen (2005) is the probability of finding an effect in the same direction upon an exact replication.  For articles submitted to Psychological Science, p.rep needs to be reported. 
</p>
<p>F, t, p and r are all estimates of the size of an effect.  But F, t, and p also are also a function of the sample size.  Effect size, d prime, may be expressed as differences between means compared to within cell standard deviations, or as a correlation coefficient.  These functions convert p, F, and t to d prime and the r equivalent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p.rep(p = 0.05, n=NULL,twotailed = FALSE)
p.rep.f(F,df2,twotailed=FALSE) 
p.rep.r(r,n,twotailed=TRUE) 
p.rep.t(t,df,df2=NULL,twotailed=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p.rep_+3A_p">p</code></td>
<td>
<p>conventional probability of statistic (e.g., of F, t, or r)</p>
</td></tr>
<tr><td><code id="p.rep_+3A_f">F</code></td>
<td>
<p>The F statistic</p>
</td></tr>
<tr><td><code id="p.rep_+3A_df">df</code></td>
<td>
<p>Degrees of freedom of the t-test, or of the first group if unequal sizes</p>
</td></tr>
<tr><td><code id="p.rep_+3A_df2">df2</code></td>
<td>
<p>Degrees of freedom of the denominator of F or the second group in an unequal sizes t test</p>
</td></tr>
<tr><td><code id="p.rep_+3A_r">r</code></td>
<td>
<p>Correlation coefficient</p>
</td></tr>
<tr><td><code id="p.rep_+3A_n">n</code></td>
<td>
<p>Total sample size if using r </p>
</td></tr>
<tr><td><code id="p.rep_+3A_t">t</code></td>
<td>
<p>t-statistic if doing a t-test or testing significance of a regression slope</p>
</td></tr>
<tr><td><code id="p.rep_+3A_twotailed">twotailed</code></td>
<td>
<p>Should a one or two tailed test be used? </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conventional Null Hypothesis Significance Test (NHST) is the likelihood of observing the data given the null hypothesis of no effect.  But this tells us nothing about the probability of the null hypothesis.  Peter Killeen (2005) introduced the probability of replication as a more useful measure.  The probability of replication is the probability that an exact replication study will find a result in the <em>same direction</em> as the original result.
</p>
<p>p.rep is based upon a 1 tailed probability value of the observed statistic.  
</p>
<p>Other frequently called for statistics are estimates of the effect size, expressed either as Cohen's d, Hedges g, or the equivalent value of the correlation, r. 
</p>
<p>For p.rep.t, if the cell sizes are unequal, the effect size estimates are adjusted by the ratio of the mean cell size to the harmonic mean cell size (see Rownow et al., 2000).   
</p>


<h3>Value</h3>

<table>
<tr><td><code>p.rep</code></td>
<td>
<p>Probability of replication</p>
</td></tr>
<tr><td><code>dprime</code></td>
<td>
<p>Effect size (Cohen's d) if more than just p is specified</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>Probability of F, t, or r.  Note that this can be either the one-tailed or two tailed probability value.</p>
</td></tr>
<tr><td><code>r.equivalent</code></td>
<td>
<p>For t-tests, the r equivalent to the t (see Rosenthal and Rubin(2003), Rosnow, Rosenthal, and Rubin, 2000))</p>
</td></tr></table>
<p>.
</p>


<h3>Note</h3>

<p> The p.rep value is the one tailed probability value of obtaining a result in the same direction.
</p>


<h3>References</h3>

<p>Cummings, Geoff (2005) Understanding the average probability of replication: comment on Killeen 2005). Psychological Science, 16, 12, 1002-1004). <br />
</p>
<p>Killeen, Peter H. (2005) An alternative to Null-Hypothesis Significance Tests.  Psychological Science, 16, 345-353 <br />
</p>
<p>Rosenthal, R. and Rubin, Donald B.(2003), r-sub(equivalent): A Simple Effect Size Indicator.  Psychological Methods, 8, 492-496.
</p>
<p>Rosnow, Ralph L., Rosenthal, Robert and Rubin, Donald B. (2000) Contrasts and correlations in effect-size estimation, Psychological Science, 11.  446-453.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p.rep(.05)  #probability of replicating a result if the original study had a  p = .05
p.rep.f(9.0,98)  #probability of replicating  a result with F = 9.0 with 98 df
p.rep.r(.4,50)    #probability of replicating a result if r =.4 with n = 50
p.rep.t(3,98)   #probability of replicating a result if t = 3 with df =98
p.rep.t(2.14,84,14) #effect of equal sample sizes (see Rosnow et al.)

</code></pre>

<hr>
<h2 id='paired.r'> Test the difference between (un)paired correlations </h2><span id='topic+paired.r'></span>

<h3>Description</h3>

<p>   Test the difference between two (paired or unpaired) correlations. Given 3 variables, x, y, z,  is the correlation between xy different than that between xz?  If y and z are independent, this is a simple t-test of the z transformed rs.  But, if they are dependent, it is a bit more complicated. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paired.r(xy, xz, yz=NULL, n, n2=NULL,twotailed=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="paired.r_+3A_xy">xy</code></td>
<td>
<p>r(xy) </p>
</td></tr>
<tr><td><code id="paired.r_+3A_xz">xz</code></td>
<td>
<p>r(xz) </p>
</td></tr>
<tr><td><code id="paired.r_+3A_yz">yz</code></td>
<td>
<p>r(yz) </p>
</td></tr>
<tr><td><code id="paired.r_+3A_n">n</code></td>
<td>
<p>Number of subjects for first group</p>
</td></tr>
<tr><td><code id="paired.r_+3A_n2">n2</code></td>
<td>
<p>Number of subjects in second group (if not equal to n)</p>
</td></tr>
<tr><td><code id="paired.r_+3A_twotailed">twotailed</code></td>
<td>
<p>Calculate two or one tailed probability values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To find the z of the difference between two independent correlations, first convert them to z scores using the Fisher r-z transform and then find the z of the difference between the two correlations.  The default assumption is that the group sizes are the same, but the test can be done for different size groups by specifying n2.
</p>
<p>If the correlations are not independent (i.e., they are from the same sample) then the correlation with the third variable r(yz) must be specified. Find a t statistic for the difference of thee two dependent correlations.
</p>


<h3>Value</h3>

<p>a list containing the calculated t or z values and the associated two (or one) tailed probability.
</p>
<table>
<tr><td><code>t</code></td>
<td>
<p>t test of the difference between two dependent correlations</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>probability of the t or of the z</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>z test of the difference between two independent correlations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+r.test">r.test</a></code> for more tests of independent as well as dependent (paired) tests. <code><a href="#topic+p.rep.r">p.rep.r</a></code> for the probability of replicating a particular correlation.
<code><a href="stats.html#topic+cor.test">cor.test</a></code> from stats for testing a single correlation and   <code><a href="#topic+corr.test">corr.test</a></code> for finding the values and probabilities of multiple correlations.  See also  <code><a href="#topic+set.cor">set.cor</a></code> to do multiple correlations from matrix input.</p>


<h3>Examples</h3>

<pre><code class='language-R'>paired.r(.5,.3, .4, 100) #dependent correlations
paired.r(.5,.3,NULL,100) #independent correlations same sample size
paired.r(.5,.3,NULL, 100, 64) # independent correlations, different sample sizes
</code></pre>

<hr>
<h2 id='pairs.panels'>SPLOM, histograms and correlations for a data matrix</h2><span id='topic+pairs.panels'></span><span id='topic+panel.cor'></span><span id='topic+panel.cor.scale'></span><span id='topic+panel.hist'></span><span id='topic+panel.lm'></span><span id='topic+panel.lm.ellipse'></span><span id='topic+panel.hist.density'></span><span id='topic+panel.ellipse'></span><span id='topic+panel.smoother'></span>

<h3>Description</h3>

<p>Adapted from the  help page for pairs, pairs.panels shows a scatter plot of matrices (SPLOM), with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal. Useful for descriptive statistics of small data sets.  If lm=TRUE, linear regression fits are shown for both y by x and x by y.  Correlation ellipses are also shown. Points may be given different colors depending upon some grouping variable.  Robust fitting is done using lowess or loess regression. Confidence intervals of either the lm or loess are drawn if requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'panels'
pairs(x, smooth = TRUE, scale = FALSE, density=TRUE,ellipses=TRUE,
     digits = 2,method="pearson", pch = 20, lm=FALSE,cor=TRUE,jiggle=FALSE,factor=2, 
     hist.col="cyan",show.points=TRUE,rug=TRUE, breaks = "Sturges",cex.cor=1,wt=NULL,
     smoother=FALSE,stars=FALSE,ci=FALSE,alpha=.05,hist.border="black" ,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairs.panels_+3A_x">x</code></td>
<td>
<p>a data.frame or matrix</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_smooth">smooth</code></td>
<td>
<p>TRUE draws loess smooths </p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_scale">scale</code></td>
<td>
<p> TRUE scales the correlation font by the size of the absolute correlation. </p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_density">density</code></td>
<td>
<p>TRUE shows the density plots as well as histograms</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_ellipses">ellipses</code></td>
<td>
<p>TRUE draws correlation ellipses</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_lm">lm</code></td>
<td>
<p>Plot the linear fit rather than the LOESS smoothed fits.</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_digits">digits</code></td>
<td>
<p> the number of digits to show</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_method">method</code></td>
<td>
<p>method parameter for the correlation (&quot;pearson&quot;,&quot;spearman&quot;,&quot;kendall&quot;)</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_pch">pch</code></td>
<td>
<p>The plot character (defaults to 20 which is a '.').</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_cor">cor</code></td>
<td>
<p>If plotting regressions, should correlations be reported?</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_jiggle">jiggle</code></td>
<td>
<p>Should the points be jittered before plotting?</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_factor">factor</code></td>
<td>
<p>factor for jittering (1-5)</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_hist.col">hist.col</code></td>
<td>
<p>What color should the histogram on the diagonal be?</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_show.points">show.points</code></td>
<td>
<p>If FALSE, do not show the data points, just the data ellipses and smoothed functions</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_rug">rug</code></td>
<td>
<p>if TRUE (default) draw a rug under the histogram, if FALSE, don't draw the rug</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_breaks">breaks</code></td>
<td>
<p>If specified, allows control for the number of breaks in the histogram (see the hist function)</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_cex.cor">cex.cor</code></td>
<td>
<p>If this is specified, this will change the size of the text in the correlations. this allows one to also change the size of the points in the plot by specifying the normal cex values. If just specifying cex, it will change the character size, if cex.cor is specified, then cex will function to change the point size.</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_wt">wt</code></td>
<td>
<p>If specified, then weight the correlations by a weights matrix (see note for some comments)</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_smoother">smoother</code></td>
<td>
<p>If TRUE, then smooth.scatter the data points  &ndash; slow but pretty with lots of subjects </p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_stars">stars</code></td>
<td>
<p>For those people who like to show the significance of correlations by using magic astricks, set stars=TRUE</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_ci">ci</code></td>
<td>
<p>Draw confidence intervals for the linear model or for the loess fit, defaults to ci=FALSE. If confidence intervals are not drawn, the fitting function is lowess.</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_alpha">alpha</code></td>
<td>
<p>The alpha level for the confidence regions, defaults to .05</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_hist.border">hist.border</code></td>
<td>
<p>Set to &quot;NA&quot; to not have lines between the histogram values</p>
</td></tr>
<tr><td><code id="pairs.panels_+3A_...">...</code></td>
<td>
<p>other options for pairs </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shamelessly adapted from the pairs help page.  Uses panel.cor, panel.cor.scale, and panel.hist, all taken from the help pages for pairs. Also adapts the ellipse function from John Fox's car package. 
</p>
<p><code><a href="#topic+pairs.panels">pairs.panels</a></code> is most useful when the number of variables to plot is less than about 6-10. It is particularly useful for an initial overview of the data.
</p>
<p>To show different groups with different colors, use a plot character (pch) between 21 and 25 and then set the background color to vary by group. (See the second example).
</p>
<p>When plotting more than about 10 variables, it is useful to set the gap parameter to something less than 1 (e.g., 0).  Alternatively, consider using <code><a href="#topic+cor.plot">cor.plot</a></code>
</p>
<p>In addition, when plotting more than about 100-200 cases, it is useful to set the plotting character to be a point. (pch=&quot;.&quot;)
</p>
<p>Sometimes it useful to draw the correlation ellipses and best fitting loess without the points. (points.false=TRUE).
</p>


<h3>Value</h3>

<p>A scatter plot matrix (SPLOM) is drawn in the graphic window. The lower off diagonal draws scatter plots, the diagonal histograms, the upper off diagonal reports the Pearson correlation (with pairwise deletion).
</p>
<p>If lm=TRUE, then the scatter plots are drawn above and below the diagonal, each with a linear regression fit.  Useful to show the difference between regression lines.
</p>


<h3>Note</h3>

<p>If the data are either categorical or character, this is flagged with an astrix for the variable name.  If character, they are changed to factors before plotting.
</p>
<p>The wt parameter allows for scatter plots of the raw data while showing the weighted correlation matrix (found by using <code><a href="#topic+cor.wt">cor.wt</a></code>). The current implementation uses the first two columns of the weights matrix for all analyses.  This is useful, but not perfect.  The use of this option would be to plot the means from a <code><a href="#topic+statsBy">statsBy</a></code> analysis and then display the weighted correlations by specifying the means and ns from the statsBy run.  See the final (not run) example.</p>


<h3>See Also</h3>

  <p><code><a href="graphics.html#topic+pairs">pairs</a></code> which is the base from which pairs.panels is derived, <code><a href="#topic+cor.plot">cor.plot</a></code> to do a heat map of correlations, and <code><a href="#topic+scatter.hist">scatter.hist</a></code> to draw a single correlation plot with histograms and best fitted lines. 
</p>
<p>To find the probability &quot;significance&quot; of the correlations using normal theory, use <code><a href="#topic+corr.test">corr.test</a></code>.  To find confidence intervals using boot strapping procedures, use <code><a href="#topic+cor.ci">cor.ci</a></code>.  To graphically show confidence intervals, see <code><a href="#topic+cor.plot.upperLowerCi">cor.plot.upperLowerCi</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pairs.panels(attitude)   #see the graphics window
data(iris)
pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
        pch=21,main="Fisher Iris data by Species") #to show color grouping

pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
  pch=21+as.numeric(iris$Species),main="Fisher Iris data by Species",hist.col="red") 
   #to show changing the diagonal
   
#to show 'significance'
   pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
  pch=21+as.numeric(iris$Species),main="Fisher Iris data by Species",hist.col="red",stars=TRUE) 
  


#demonstrate not showing the data points
data(sat.act)
pairs.panels(sat.act,show.points=FALSE)
#better yet is to show the points as a period
pairs.panels(sat.act,pch=".")
#show many variables with 0 gap between scatterplots
# data(bfi)
# pairs.panels(psychTools::bfi,show.points=FALSE,gap=0)

#plot raw data points and then the weighted correlations.
#output from statsBy
sb &lt;- statsBy(sat.act,"education")
pairs.panels(sb$mean,wt=sb$n)  #report the weighted correlations
#compare with 
pairs.panels(sb$mean) #unweighted correlations
</code></pre>

<hr>
<h2 id='pairwiseCount'>Count number of pairwise cases for a data set with missing (NA) data and impute values. </h2><span id='topic+pairwiseCount'></span><span id='topic+pairwiseCountBig'></span><span id='topic+count.pairwise'></span><span id='topic+pairwiseDescribe'></span><span id='topic+pairwiseZero'></span><span id='topic+pairwiseSample'></span><span id='topic+pairwiseReport'></span><span id='topic+pairwiseImpute'></span><span id='topic+pairwisePlot'></span>

<h3>Description</h3>

<p>When doing cor(x, use= &quot;pairwise&quot;), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwiseCount(x, y = NULL,diagonal=TRUE)
pairwiseDescribe(x,y,diagonal=FALSE,...) 
pairwiseZero(x,y=NULL, min=0, short=TRUE)
pairwiseImpute(keys,R,fix=FALSE)
pairwiseReport(x,y=NULL,cut=0,diagonal=FALSE,...) 
pairwiseSample(x,y=NULL,diagonal=FALSE,size=100,...)
pairwiseCountBig(x,size=NULL)
pairwisePlot(x,y=NULL,upper=TRUE,diagonal=TRUE,labels=TRUE,show.legend=TRUE,n.legend=10,
colors=FALSE,gr=NULL,minlength=6,xlas=1,ylas=2,
main="Relative Frequencies",count=TRUE,...)

count.pairwise(x, y = NULL,diagonal=TRUE) #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwiseCount_+3A_x">x</code></td>
<td>
<p> An input matrix, typically a data matrix ready to be correlated. </p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_y">y</code></td>
<td>
<p> An optional second input matrix </p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_diagonal">diagonal</code></td>
<td>
<p>if TRUE, then report the diagonal, else fill the diagonals with NA</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to describe</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_min">min</code></td>
<td>
<p>Count the number of item pairs with &lt;= min entries</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_short">short</code></td>
<td>
<p>Show the table of the item pairs that have entries &lt;= min</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_keys">keys</code></td>
<td>
<p>A keys.list specifying which items belong to which scale.</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_r">R</code></td>
<td>
<p>A correlation matrix to be described or imputed</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_cut">cut</code></td>
<td>
<p>Report the item pairs and numbers with cell sizes less than cut</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_fix">fix</code></td>
<td>
<p>If TRUE, then replace all NA correlations with the mean correlation for that  
within or between scale</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_upper">upper</code></td>
<td>
<p>Should the upper off diagonal matrix be drawn, or left blank?</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_labels">labels</code></td>
<td>
<p>if NULL, use column and row names, otherwise use labels</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_show.legend">show.legend</code></td>
<td>
<p>A legend (key) to the colors is shown on the right hand side</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_n.legend">n.legend</code></td>
<td>
<p>How many categories should be labelled in the legend?</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_colors">colors</code></td>
<td>
<p>Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \
from the colorRampPalette from red through white to blue</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_minlength">minlength</code></td>
<td>
<p>If not NULL, then the maximum number of characters to use in 
row/column labels</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_xlas">xlas</code></td>
<td>
<p>Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_ylas">ylas</code></td>
<td>
<p>Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_main">main</code></td>
<td>
<p>A title. Defaults to &quot;Relative Frequencies&quot;</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_gr">gr</code></td>
<td>
<p>A color gradient: e.g.,  gr &lt;- colorRampPalette(c(&quot;#B52127&quot;, &quot;white&quot;, &quot;#2171B5&quot;))  will produce slightly more pleasing (to some) colors. See next to last example of <code><a href="#topic+corPlot">corPlot</a></code>.  </p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_count">count</code></td>
<td>
<p>Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?</p>
</td></tr>
<tr><td><code id="pairwiseCount_+3A_size">size</code></td>
<td>
<p>Sample size of the number of variables to sample in pairwiseSample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (<code><a href="#topic+pairwiseCount">pairwiseCount</a></code>).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses <code><a href="#topic+fa">fa</a></code> or reliability analsyes <code><a href="#topic+omega">omega</a></code> or <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> impossible.
</p>
<p><code><a href="#topic+pairwiseCountBig">pairwiseCountBig</a></code> may be used to count the cells of large data sets.  It is analogous to <code><a href="#topic+bigCor">bigCor</a></code> and returns the cell sizes for each pair of correlations.
</p>
<p>In order to identify item pairs with counts less than a certain value <code><a href="#topic+pairwiseReport">pairwiseReport</a></code> reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs &lt; cut. Even more detail is available in the returned objects.
</p>
<p>The specific pairs that have values &lt;= n min in any particular table of the paiwise counts may be given by <code><a href="#topic+pairwiseZero">pairwiseZero</a></code>.  
</p>
<p>To remedy the problem of missing correlations, we impute the missing correlations using <code><a href="#topic+pairwiseImpute">pairwiseImpute</a></code>.
The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the <code><a href="psychTools.html#topic+ability">ability</a></code> items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by <code><a href="#topic+pairwiseImpute">pairwiseImpute</a></code> and if the fix paremeter is specified, the imputed correlation matrix is returned. 
</p>
<p>Alternative methods of imputing these correlations are not yet implemented.
</p>
<p>The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. <code><a href="#topic+pairwiseSample">pairwiseSample</a></code> will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   </p>


<h3>Value</h3>

<table>
<tr><td><code>result</code></td>
<td>
<p> = matrix of counts of pairwise observations (if pairwiseCount)</p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>The average correlation value of the observed correlations
within/between scales</p>
</td></tr> 
<tr><td><code>count</code></td>
<td>
<p>The numer of observed correlations within/between each scale</p>
</td></tr>
<tr><td><code>percent</code></td>
<td>
<p>The percentage of complete data by scale</p>
</td></tr>
<tr><td><code>imputed</code></td>
<td>
<p>The original correlation matrix with NA values replaced by the mean  
correlation for items within/between the appropriate scale.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix(rnorm(900),ncol=6)
y &lt;- matrix(rnorm(450),ncol=3)
x[x &lt; 0] &lt;- NA
y[y &gt; 1] &lt;- NA

pairwiseCount(x)
pairwiseCount(y)
pairwiseCount(x,y)
pairwiseCount(x,diagonal=FALSE)
pairwiseDescribe(x,quant=c(.1,.25,.5,.75,.9))

#examine the structure of the ability data set
keys &lt;- list(ICAR16=colnames(psychTools::ability),reasoning =  
  cs(reason.4,reason.16,reason.17,reason.19),
  letters=cs(letter.7, letter.33,letter.34,letter.58, letter.7), 
  matrix=cs(matrix.45,matrix.46,matrix.47,matrix.55), 
  rotate=cs(rotate.3,rotate.4,rotate.6,rotate.8))
 pairwiseImpute(keys,psychTools::ability)

    
</code></pre>

<hr>
<h2 id='parcels'>Find miniscales (parcels) of size 2 or 3 from a set of items</h2><span id='topic+parcels'></span><span id='topic+keysort'></span>

<h3>Description</h3>

<p>Given a set of n items, form n/2 or n/3 mini scales or parcels of the most similar pairs or triplets of items.  These may be used as the basis for subsequent scale construction or multivariate (e.g., factor) analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parcels(x, size = 3, max = TRUE, flip=TRUE,congruence = FALSE)
keysort(keys)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parcels_+3A_x">x</code></td>
<td>
<p>A matrix/dataframe of items or a correlation/covariance matrix of items</p>
</td></tr>
<tr><td><code id="parcels_+3A_size">size</code></td>
<td>
<p>Form parcels of size 2 or size 3</p>
</td></tr>
<tr><td><code id="parcels_+3A_flip">flip</code></td>
<td>
<p>if flip=TRUE, negative correlations lead to at least one item being negatively scored</p>
</td></tr>
<tr><td><code id="parcels_+3A_max">max</code></td>
<td>
<p>Should item correlation/covariance be adjusted for their maximum correlation</p>
</td></tr>
<tr><td><code id="parcels_+3A_congruence">congruence</code></td>
<td>
<p>Should the correlations be converted to congruence coefficients?</p>
</td></tr>
<tr><td><code id="parcels_+3A_keys">keys</code></td>
<td>
<p>Sort a matrix of keys to reflect item order as much as possible</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Items used in measuring ability or other aspects of personality are typically not very reliable.  One suggestion has been to form items into homogeneous item composites (HICs), Factorially Homogeneous Item Dimensions (FHIDs) or mini scales (parcels).  Parcelling may be done rationally, factorially, or empirically based upon the structure of the correlation/covariance matrix.  <code>link{parcels}</code> facilitates the finding of parcels by forming a keys matrix suitable for using in <code><a href="#topic+score.items">score.items</a></code>.  These keys represent the n/2 most similar pairs or the n/3 most similar triplets.
</p>
<p>The algorithm is straightforward:  For size = 2, the correlation matrix is searched for the highest correlation.  These two items form the first parcel and are dropped from the matrix.  The procedure is repeated until there are no more pairs to form.
</p>
<p>For size=3, the three items with the greatest sum of variances and covariances with each other is found.  This triplet is the first parcel.  All three items are removed and the procedure then identifies the next most similar triplet.  The procedure repeats until n/3 parcels are identified.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>keys</code></td>
<td>
<p>A matrix of scoring keys to be used to form mini scales (parcels) These will be in order of importance, that is, the first parcel (P1) will reflect the most similar pair or triplet.  The keys may also be sorted by average item order by using the keysort function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Cattell, R. B.	(1956).	Validation and intensification of the sixteen personality factor	questionnaire.	Journal	of	Clinical	Psychology ,	12 (3),	205	-214.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+score.items">score.items</a></code> to score the parcels or  <code><a href="#topic+iclust">iclust</a></code> for an alternative way of forming item clusters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>parcels(Thurstone)
keys &lt;- parcels(psychTools::bfi)
keys &lt;- keysort(keys)
score.items(keys,psychTools::bfi)
</code></pre>

<hr>
<h2 id='partial.r'> Find the partial correlations for a set (x) of variables with set (y) removed. </h2><span id='topic+partial.r'></span>

<h3>Description</h3>

<p>A straightforward application of matrix algebra to remove the effect of the variables in the y set from the x set. Input may be either a data matrix or a correlation matrix.  Variables in x and y are specified by location.  If x and y are not specified, then the effect of all variables are partialled from all the other correlations.  May also be done using formula input which is more convenient when comparing results to regression models.  Also has the option to find part (aka semi-partial) correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial.r(data, x, y, use="pairwise",method="pearson",part=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial.r_+3A_data">data</code></td>
<td>
<p>A data or correlation matrix</p>
</td></tr>
<tr><td><code id="partial.r_+3A_x">x</code></td>
<td>
<p>The variable names or locations associated with the X set (or formula input) </p>
</td></tr>
<tr><td><code id="partial.r_+3A_y">y</code></td>
<td>
<p>The variable  names or locations associated with the Y set to be partialled from the X set</p>
</td></tr>
<tr><td><code id="partial.r_+3A_use">use</code></td>
<td>
<p>How should we treat missing data? The default is pairwise complete.</p>
</td></tr>
<tr><td><code id="partial.r_+3A_method">method</code></td>
<td>
<p>Which method of correlation should we use, the default is pearson.</p>
</td></tr>
<tr><td><code id="partial.r_+3A_part">part</code></td>
<td>
<p>Find the part correlation (aka semi-partial) , defaults to finding partial correlations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two ways to use <code><a href="#topic+partial.r">partial.r</a></code>.  One is to find the complete partial correlation matrix (that is, partial all the other variables out of each variable).  This may be done by simply specifying the raw data or correlation matrix.  (In the case of raw data, correlations will be found according to use and method.)  In this case, just specify the data matrix. 
</p>
<p>Alternatively, if we think of the data as an X matrix and Y matrix, then (D = X + Y) with correlations R.  Then the partial correlations of the X predictors with the Y variables partialled out are just the last column of R^(-1). See the <code><a href="#topic+Tal.Or">Tal.Or</a></code> example below.
</p>
<p>The second usage is to partial a set of variables(y) out of another set (x). It is sometimes convenient to partial the effect of a number of variables (e.g., sex, age, education) out of the correlations of another set of variables.  This could be done laboriously by finding the residuals of various multiple correlations, and then correlating these residuals.  The matrix algebra alternative is to do it directly. 
To find the confidence intervals and &quot;significance&quot; of the correlations, use the <code><a href="#topic+corr.p">corr.p</a></code> function with n = n - s where s is the number of covariates. 
</p>
<p>A perhaps easier format is to use formula input compatible with that used in <code><a href="#topic+lmCor">lmCor</a></code>. If using formula input,we specify X and Y with the partialled variables specified by subtraction.    That is X ~ Y - z,
This is useful in the case of multiple regression using  which uses this notation.
</p>
<p>Following a thoughtful request from Fransisco Wilheim, I just find the correlations of the variables specified in the call (previously I  had found the entire correlation matrix, which is a waste of time and breaks if some variables are non-numeric).)
</p>
<p>In the case of non-positive definite matrices, find the Pinv (pseudo inverse) of the matrix.
</p>


<h3>Value</h3>

<p>The matrix of partial correlations.</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p> Revelle, W. (in prep) An introduction to psychometric theory with applications in R. To be published by Springer.  (working draft available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+lmCor">lmCor</a></code> for a similar application for regression. <code><a href="#topic+lowerMat">lowerMat</a></code> to neatly show a correlation matrix, and <code><a href="#topic+corr.p">corr.p</a></code> to find the confidence intervals of a correlation. </p>


<h3>Examples</h3>

<pre><code class='language-R'>jen &lt;- make.hierarchical()    #make up a correlation matrix 
lowerMat(jen[1:5,1:5])
par.r &lt;- partial.r(jen,c(1,3,5),c(2,4))
lowerMat(par.r)
#or
R &lt;- jen[1:5,1:5]
par.r &lt;- partial.r(R, y = cs(V2,V4))
lowerMat(par.r)
cp &lt;- corr.p(par.r,n=98)  #assumes the jen data based upon n =100.
print(cp,short=FALSE)  #show the confidence intervals as well
 #partial all from all correlations.
lowerMat(partial.r(jen)) 


#Consider the Tal.Or data set.
lowerCor(Tal.Or)
#partial gender and age from these relations (they hardly change)
partial.r(Tal.Or,1:4,cs(gender,age))
#find the partial correlations between the first three variables and the DV (reaction)
round(partial.r(Tal.Or[1:4])[4,1:3],2) #The partial correlations with the criterion

#Consider the eminence data set from Del Giudice.
if(require("psychTools")) {
data(eminence)
partial.r(reputation ~ works + citations - birth.year, data=eminence)
#now do a part correlation
partial.r(reputation ~ works + citations - birth.year, data=eminence, part=TRUE) 
}


</code></pre>

<hr>
<h2 id='phi'> Find the phi coefficient of correlation between two dichotomous variables </h2><span id='topic+phi'></span>

<h3>Description</h3>

<p>Given a 1 x 4 vector or a 2 x 2 matrix of frequencies, find the phi coefficient of correlation.  Typical use is in the case of predicting a dichotomous criterion from a dichotomous predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phi(t, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phi_+3A_t">t</code></td>
<td>
<p>a 1 x 4 vector or a 2 x 2 matrix </p>
</td></tr>
<tr><td><code id="phi_+3A_digits">digits</code></td>
<td>
<p> round the result to digits </p>
</td></tr>
</table>


<h3>Details</h3>

<p>In many prediction situations, a dichotomous predictor (accept/reject) is validated against a dichotomous criterion (success/failure).  Although a polychoric correlation estimates the underlying Pearson correlation as if the predictor and criteria were continuous and bivariate normal variables, and the tetrachoric correlation if both x and y are assumed to dichotomized normal distributions,  the phi coefficient is the Pearson applied to a matrix of 0's and 1s. 
</p>
<p>The phi coefficient was first reported by Yule (1912), but should not be confused with the <code><a href="#topic+Yule">Yule</a></code> Q coefficient. 
</p>
<p>For a very useful discussion of various measures of association given a 2 x 2 table, and why one should probably prefer the <code><a href="#topic+Yule">Yule</a></code> Q coefficient, see Warren (2008). 
</p>
<p>Given a two x two table of counts <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> a </td><td style="text-align: left;"> b </td><td style="text-align: left;"> a+b (R1)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> c </td><td style="text-align: left;"> d  </td><td style="text-align: left;"> c+d (R2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> a+c(C1) </td><td style="text-align: left;"> b+d (C2) </td><td style="text-align: left;"> a+b+c+d (N)
</td>
</tr>

</table>

<p>convert all counts to fractions of the total and then  <br />
Phi = [a- (a+b)*(a+c)]/sqrt((a+b)(c+d)(a+c)(b+d) ) =  <br />
(a - R1 * C1)/sqrt(R1 * R2 * C1 * C2)
</p>
<p>This is in contrast to the Yule coefficient, Q, where <br />
Q = (ad - bc)/(ad+bc) which is the same as <br />
[a- (a+b)*(a+c)]/(ad+bc)
</p>
<p>Since the phi coefficient is just a Pearson correlation applied to dichotomous data, to find a matrix of phis from a data set involves just finding the correlations using cor  or <code><a href="#topic+lowerCor">lowerCor</a></code> or <code><a href="#topic+corr.test">corr.test</a></code>.  
</p>


<h3>Value</h3>

<p>phi coefficient of correlation
</p>


<h3>Author(s)</h3>

<p>William Revelle with modifications by Leo Gurtler </p>


<h3>References</h3>

<p>Warrens, Matthijs (2008), On Association Coefficients for 2x2 Tables and Properties That Do Not Depend on the Marginal Distributions. Psychometrika, 73, 777-789.
</p>
<p>Yule, G.U. (1912). On the methods of measuring the association between two attributes. Journal of the Royal Statistical Society, 75, 579-652.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+phi2tetra">phi2tetra</a></code>, <code><a href="#topic+AUC">AUC</a></code>, <code><a href="#topic+Yule">Yule</a></code>, <code><a href="#topic+Yule.inv">Yule.inv</a></code> <code><a href="#topic+Yule2phi">Yule2phi</a></code>, <code><a href="#topic+comorbidity">comorbidity</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code> and <code><a href="#topic+polychoric">polychoric</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>phi(c(30,20,20,30))
phi(c(40,10,10,40))
x &lt;- matrix(c(40,5,20,20),ncol=2)
phi(x)


</code></pre>

<hr>
<h2 id='phi.demo'> A simple demonstration of the Pearson, phi, and polychoric corelation</h2><span id='topic+phi.demo'></span>

<h3>Description</h3>

<p>A not very interesting demo of what happens if bivariate continuous data are dichotomized.  Bascially a demo of r, phi, and polychor.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>phi.demo(n=1000,r=.6, cuts=c(-2,-1,0,1,2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phi.demo_+3A_n">n</code></td>
<td>
<p>number of cases to simulate</p>
</td></tr>
<tr><td><code id="phi.demo_+3A_r">r</code></td>
<td>
<p> correlation between latent and observed </p>
</td></tr>
<tr><td><code id="phi.demo_+3A_cuts">cuts</code></td>
<td>
<p>form dichotomized variables at the value of cuts</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A demonstration of the problem of different base rates on the phi correlation, and how these are partially solved by using the polychoric correlation. Not one of my more interesting demonstrations.  See 
<a href="https://personality-project.org/r/simulating-personality.html">https://personality-project.org/r/simulating-personality.html</a> and 
<a href="https://personality-project.org/r/r.datageneration.html">https://personality-project.org/r/r.datageneration.html</a> for better demonstrations of data generation.
</p>


<h3>Value</h3>

<p>a matrix of correlations and a graphic plot.   The items above the diagonal are the tetrachoric correlations, below the diagonal are raw correlations.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p><a href="https://personality-project.org/r/simulating-personality.html">https://personality-project.org/r/simulating-personality.html</a> and 
<a href="https://personality-project.org/r/r.datageneration.html">https://personality-project.org/r/r.datageneration.html</a> for better demonstrations of data generation. </p>


<h3>See Also</h3>

<p><code><a href="#topic+VSS.simulate">VSS.simulate</a></code>,<code><a href="#topic+item.sim">item.sim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#demo &lt;- phi.demo() #compare the phi (lower off diagonal and polychoric correlations
# (upper off diagonal)
#show the result from tetrachoric  which corrects for zero entries by default
#round(demo$tetrachoric$rho,2)
#show the result from phi2poly
#tetrachorics above the diagonal, phi below the diagonal 
#round(demo$phis,2) 
</code></pre>

<hr>
<h2 id='phi2tetra'> Convert a phi coefficient to a tetrachoric correlation </h2><span id='topic+phi2tetra'></span><span id='topic+phi2poly'></span>

<h3>Description</h3>

<p>Given a phi coefficient (a Pearson r calculated on two dichotomous variables), and the marginal frequencies (in percentages), what is the corresponding estimate of the tetrachoric correlation?
</p>
<p>Given a two x two table of counts <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> a </td><td style="text-align: left;"> b </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> c </td><td style="text-align: left;"> d  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The phi coefficient is (a - (a+b)*(a+c))/sqrt((a+b)(a+c)(b+d)(c+c)).
</p>
<p>This function reproduces the cell entries for specified marginals and then calls the tetrachoric function. (Which was originally based upon John Fox's polychor function.)
The phi2poly name will become deprecated in the future. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phi2tetra(ph,m,n=NULL,correct=TRUE)
phi2poly(ph,cp,cc,n=NULL,correct=TRUE) #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phi2tetra_+3A_ph">ph</code></td>
<td>
<p>phi </p>
</td></tr>
<tr><td><code id="phi2tetra_+3A_m">m</code></td>
<td>
<p>a vector of the selection ratio and probability of criterion.  In the case where ph is a matrix, m is a vector of the frequencies of the selected cases</p>
</td></tr>
<tr><td><code id="phi2tetra_+3A_correct">correct</code></td>
<td>
<p>When finding tetrachoric correlations, should we correct for continuity for small marginals.  See <code><a href="#topic+tetrachoric">tetrachoric</a></code> for a discussion.</p>
</td></tr>
<tr><td><code id="phi2tetra_+3A_n">n</code></td>
<td>
<p>If the marginals are given as frequencies, what was the total number of cases?</p>
</td></tr>
<tr><td><code id="phi2tetra_+3A_cp">cp</code></td>
<td>
<p> probability of the predictor &ndash; the so called selection ratio </p>
</td></tr>
<tr><td><code id="phi2tetra_+3A_cc">cc</code></td>
<td>
<p>probability of the criterion &ndash; the so called success rate. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>used to require the mvtnorm package but this has been replaced with mnormt
</p>


<h3>Value</h3>

<p>a tetrachoric correlation
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tetrachoric">tetrachoric</a></code>, <code><a href="#topic+Yule2phi.matrix">Yule2phi.matrix</a></code>, <code><a href="#topic+phi2poly.matrix">phi2poly.matrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'> phi2tetra(.3,c(.5,.5))
#phi2poly(.3,.3,.7)
</code></pre>

<hr>
<h2 id='Pinv'>Compute the  Moore-Penrose Pseudo Inverse of a matrix</h2><span id='topic+Pinv'></span>

<h3>Description</h3>

<p>Given a matrix of less than full rank, the conventional inverse function will fail.  The pseudoinverse or generalized inverse resolves this problem by using just the postive  values of the singular value decomposition d matrix. An adaptation of the ginv function from MASS and the pinv function from pracma. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pinv(X, tol = sqrt(.Machine$double.eps))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pinv_+3A_x">X</code></td>
<td>
<p>A correlation or covariance matrix to analyze</p>
</td></tr>
<tr><td><code id="Pinv_+3A_tol">tol</code></td>
<td>
<p>A very small number. Reject values with eigen values less than tolerance</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The singular value decomposition of a matrix X is UdV where for full rank matrices, d is the vector of eigen values and U and V are the matrices of eigen vectors. The inverse is just U/d.  If the matrix is less than full rank, many of the d values are effectively zero (at the limit of computational accuracy.) Thus, to solve matrix equations with matrices of less than full rank (e.g. the <code><a href="#topic+schmid">schmid</a></code> Schmid-Leiman solution), we need to find the generalized inverse. 
</p>


<h3>Value</h3>

<p>The generalized inverse</p>


<h3>Note</h3>

<p>Adapted from the ginv function in MASS and the pinv function in pracma. Installed here to avoid loading those packages.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (1999) Modern Applied Statistics with S-PLUS. Third Edition. Springer. p.100.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+schmid">schmid</a></code>, <code><a href="#topic+faCor">faCor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>round(Pinv(Thurstone) %*% Thurstone,2)  #an identity matrix
sl &lt;- schmid(Thurstone,3)  #The schmid-leiman solution is less than full rank
F &lt;- sl$sl[,1:4]    #the SL solution is general + 3 gropus
R &lt;- Thurstone      #
diag(R) &lt;- sl$sl[,5]  #the reproduced matrix (R - U2)
S &lt;- t(Pinv(t(F) %*% F) %*% t(F) %*% R)  #the structure matrix
Phi &lt;- t(S) %*%  F %*% Pinv(t(F) %*% F)  #the factor covariances

</code></pre>

<hr>
<h2 id='plot.psych'>Plotting functions for the psych package of class &ldquo;psych&quot;</h2><span id='topic+plot.psych'></span><span id='topic+plot.poly'></span><span id='topic+plot.irt'></span><span id='topic+plot.residuals'></span>

<h3>Description</h3>

<p>Combines several plotting functions into one for objects of class &ldquo;psych&quot;.  This can be used to plot the results of <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+factor.pa">factor.pa</a></code>, or <code><a href="#topic+principal">principal</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
plot(x,labels=NULL,...)
## S3 method for class 'irt'
plot(x,xlab,ylab,main,D,type=c("ICC","IIC","test"),cut=.3,labels=NULL,
 keys=NULL, xlim,ylim,y2lab,lncol="black",...)
## S3 method for class 'poly'
plot(x,D,xlab,ylab,xlim,ylim,main,type=c("ICC","IIC","test"),cut=.3,labels,
     keys=NULL,y2lab,lncol="black",...)
## S3 method for class 'residuals'
plot(x,main,type=c("qq","chi","hist","cor"),std, bad=4,
     numbers=TRUE, upper=FALSE,diag=FALSE,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.psych_+3A_x">x</code></td>
<td>
<p>The object to plot </p>
</td></tr>
<tr><td><code id="plot.psych_+3A_labels">labels</code></td>
<td>
<p>Variable labels</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_xlab">xlab</code></td>
<td>
<p>Label for the x axis  &ndash; defaults to Latent Trait</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_ylab">ylab</code></td>
<td>
<p>Label for the y axis</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_xlim">xlim</code></td>
<td>
<p>The limits for the x axis</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_ylim">ylim</code></td>
<td>
<p>Specify the limits for the y axis</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_main">main</code></td>
<td>
<p>Main title for graph</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_type">type</code></td>
<td>
<p>&quot;ICC&quot; plots items, &quot;IIC&quot; plots item information, &quot;test&quot; plots test information, defaults to IIC.,&quot;qq&quot; does a quantile plot,&quot;chi&quot; plots chi square distributions,&quot;hist&quot; shows the histogram,&quot;cor&quot; does a corPlot of the residuals. </p>
</td></tr>
<tr><td><code id="plot.psych_+3A_d">D</code></td>
<td>
<p>The discrimination parameter</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_cut">cut</code></td>
<td>
<p>Only plot item responses with discrimiantion greater than cut</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_keys">keys</code></td>
<td>
<p>Used in plotting irt results from irt.fa.</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_y2lab">y2lab</code></td>
<td>
<p>ylab for test reliability, defaults to &quot;reliability&quot;</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_bad">bad</code></td>
<td>
<p>label the most 1.. bad items in residuals</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_numbers">numbers</code></td>
<td>
<p>if using the cor option in plot residuals, show the numeric values</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_upper">upper</code></td>
<td>
<p>if using the cor option in plot residuals, show the upper off diagonal
values</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_diag">diag</code></td>
<td>
<p>if using the cor option in plot residuals, show the diagonal values</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_std">std</code></td>
<td>
<p>Standardize the resduals?</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_lncol">lncol</code></td>
<td>
<p>The color of the lines in the IRT plots.  Defaults to all being black, but it is possible to specify lncol as a vector of colors to be used.</p>
</td></tr>
<tr><td><code id="plot.psych_+3A_...">...</code></td>
<td>
<p>other calls to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Passes the appropriate values to plot.  For plotting the results of <code><a href="#topic+irt.fa">irt.fa</a></code>, there are three options: type = &quot;IIC&quot; (default) will plot the item characteristic respone function.  type = &quot;IIC&quot; will plot the item information function, and type= &quot;test&quot; will plot the test information function.
</p>
<p>Note that plotting an irt result will call either plot.irt or plot.poly depending upon the type of data that were used in the original <code><a href="#topic+irt.fa">irt.fa</a></code> call.  
</p>
<p>These are calls to the generic plot function that are intercepted for objects of type &quot;psych&quot;.  More precise plotting control is available in the separate plot functions.  plot may be used for psych objects returned from <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+principal">principal</a></code> as well as <code><a href="#topic+plot.reliability">plot.reliability</a></code> .
</p>
<p>A &quot;jiggle&quot; parameter is available in the fa.plot function (called from plot.psych when the type is a factor or cluster.  If jiggle=TRUE, then the points are jittered slightly (controlled by amount) before plotting.  This option is useful when plotting items with identical factor loadings (e.g., when comparing hypothetical models).
</p>
<p>Objects from <code><a href="#topic+irt.fa">irt.fa</a></code> are plotted according to &quot;type&quot; (Item informations, item characteristics, or test information).  In addition, plots for selected items may be done if using the keys matrix.  Plots of irt information return three invisible objects, a summary of information for each item at  levels of the trait, the average area under the curve (the average information) for each item as well as where the item is most informative.
</p>
<p>If plotting multiple factor solutions in plot.poly, then main can be a vector of names, one for each factor.  The default is to give main + the factor number.
</p>
<p>It is also possible to create irt like plots based upon just a scoring key and item difficulties, or from a factor analysis and item difficulties.  These are not true IRT type analyses, in that the parameters are not estimated from the data, but are rather indications of item location and discrimination for arbitrary sets of items.  To do this, find <code><a href="#topic+irt.stats.like">irt.stats.like</a></code> and then plot the results.
</p>
<p><code><a href="#topic+plot.residuals">plot.residuals</a></code> allows the user to graphically examine the residuals of models formed by  <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+omega">omega</a></code>, as well as  <code><a href="#topic+principal">principal</a></code> and display them in a number of ways.  &quot;qq&quot; will show quantiles of standardized or unstandardized residuals, &quot;chi&quot; will show quantiles of the squared standardized or unstandardized residuals plotted against the expected chi square values, &quot;hist&quot; will draw the histogram of the raw or standardized residuals, and &quot;cor&quot; will show a corPlot of the residual correlations. 
</p>


<h3>Value</h3>

<p>Graphic output for factor analysis, cluster analysis and item response analysis.  
</p>


<h3>Note</h3>

<p> More precise plotting control is available in the separate plot functions.
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>See Also</h3>

 <p><code><a href="#topic+VSS.plot">VSS.plot</a></code> and <code><a href="#topic+fa.plot">fa.plot</a></code>, <code><a href="#topic+cluster.plot">cluster.plot</a></code>, <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, <code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+principal">principal</a></code> or   <code><a href="#topic+plot.reliability">plot.reliability</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test.data &lt;- Harman74.cor$cov
f4 &lt;- fa(test.data,4)
plot(f4)
plot(resid(f4))
plot(resid(f4),main="Residuals from a 4 factor solution",qq=FALSE)
#not run
#data(bfi)
#e.irt &lt;- irt.fa(bfi[11:15])  #just the extraversion items
#plot(e.irt)   #the information curves
#
ic &lt;- iclust(test.data,3)   #shows hierarchical structure 
plot(ic)                    #plots loadings
#


</code></pre>

<hr>
<h2 id='polar'>Convert Cartesian factor loadings into polar coordinates </h2><span id='topic+polar'></span>

<h3>Description</h3>

<p>Factor and cluster analysis output typically presents item by factor correlations (loadings).  Tables of factor loadings are frequently sorted by the size of loadings.  This style of presentation tends to make it difficult to notice the pattern of loadings on other, secondary, dimensions.  By converting to polar coordinates, it is easier to see the pattern of the secondary loadings. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polar(f, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polar_+3A_f">f</code></td>
<td>
<p>A matrix of loadings or the output from a factor or cluster analysis program</p>
</td></tr>
<tr><td><code id="polar_+3A_sort">sort</code></td>
<td>
<p> sort=TRUE: sort items by the angle of the items on the first pair of factors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although many uses of factor analysis/cluster analysis assume a simple structure where items have one and only one large loading, some domains such as personality or affect items have a more complex structure and some items have high loadings on two factors.  (These items are said to have complexity 2, see <code><a href="#topic+VSS">VSS</a></code>).  By expressing the factor loadings in polar coordinates, this structure is more readily perceived.
</p>
<p>For each pair of factors, item loadings are converted to an angle with the first factor, and a vector length corresponding to the amount of variance in the item shared with the two factors.  
</p>
<p>For a two dimensional structure, this will lead to a column of angles and a column of vector lengths.  For n factors, this leads to n* (n-1)/2 columns of angles and an equivalent number of vector lengths.
</p>


<h3>Value</h3>

<table>
<tr><td><code>polar</code></td>
<td>
<p>A data frame of polar coordinates </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Rafaeli, E. &amp; Revelle, W. (2006). A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion. \
</p>
<p>Hofstee, W. K. B., de Raad, B., &amp; Goldberg, L. R. (1992). Integration of the big five and circumplex approaches to trait structure. Journal of Personality and Social Psychology, 63, 146-163.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+cluster.plot">cluster.plot</a></code>, <code><a href="#topic+circ.tests">circ.tests</a></code>,  <code><a href="#topic+fa">fa</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
circ.data &lt;- circ.sim(24,500)
circ.fa &lt;- fa(circ.data,2)
circ.polar &lt;- round(polar(circ.fa),2)
circ.polar
#compare to the graphic
cluster.plot(circ.fa)
</code></pre>

<hr>
<h2 id='polychor.matrix'>Phi or Yule coefficient matrix  to polychoric coefficient matrix</h2><span id='topic+polychor.matrix'></span><span id='topic+Yule2poly.matrix'></span><span id='topic+phi2poly.matrix'></span><span id='topic+Yule2phi.matrix'></span>

<h3>Description</h3>

<p>A set of deprecated functions that have replaced by <code><a href="#topic+Yule2tetra">Yule2tetra</a></code> and <code><a href="#topic+Yule2phi">Yule2phi</a></code>. 
</p>
<p>Some older correlation matrices were reported as matrices of Phi or of Yule correlations.  That is, correlations were found from the two by two table of counts:
<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> a </td><td style="text-align: left;"> b </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> c </td><td style="text-align: left;"> d  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Yule Q is (ad - bc)/(ad+bc). <br />
</p>
<p>With marginal frequencies of a+b, c+d, a+c, b+d.
</p>
<p>Given a square matrix of such correlations, and the proportions for each variable that are in the a + b cells, it is possible to reconvert each correlation into a two by two table and then estimate the corresponding polychoric correlation (using John Fox's polychor function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Yule2poly.matrix(x, v)  #deprectated
phi2poly.matrix(x, v)     #deprectated
Yule2phi.matrix(x, v)     #deprectated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polychor.matrix_+3A_x">x</code></td>
<td>
<p>a matrix of phi or Yule  coefficients </p>
</td></tr>
<tr><td><code id="polychor.matrix_+3A_v">v</code></td>
<td>
<p>A vector of marginal frequencies  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions call <code><a href="#topic+Yule2poly">Yule2poly</a></code>,  <code><a href="#topic+Yule2phi">Yule2phi</a></code> or <code><a href="#topic+phi2poly">phi2poly</a></code> for each cell of the matrix. See those functions for more details.  See <code><a href="#topic+phi.demo">phi.demo</a></code> for an example.
</p>


<h3>Value</h3>

<p>A matrix of correlations
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>#demo &lt;- phi.demo() 
#compare the phi (lower off diagonal and polychoric correlations (upper off diagonal)
#show the result from poly.mat
#round(demo$tetrachoric$rho,2)
#show the result from phi2poly
#tetrachorics above the diagonal, phi below the diagonal 
#round(demo$phis,2) 


</code></pre>

<hr>
<h2 id='predict.psych'>Prediction function for factor analysis, principal components (pca), bestScales
</h2><span id='topic+predict.psych'></span>

<h3>Description</h3>

<p>Finds predicted factor/component scores from a factor analysis or principal components analysis (pca) of data set A predicted to data set B.  Predicted factor scores use the weights matrix used to find estimated factor scores, predicted components use the loadings matrix. Scores are either standardized with respect to the prediction sample or based upon the original data. Predicted scores from a bestScales model are based upon the statistics from the original sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
predict(object, data,old.data,options=NULL,missing=FALSE,impute="none",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.psych_+3A_object">object</code></td>
<td>
<p>the result of a factor analysis, principal components analysis (pca) or bestScales of data set A</p>
</td></tr>
<tr><td><code id="predict.psych_+3A_data">data</code></td>
<td>
<p>Data set B, of the same number of variables as data set A.</p>
</td></tr>
<tr><td><code id="predict.psych_+3A_old.data">old.data</code></td>
<td>
<p>if specified, the data set B will be standardized in terms of values from the old data.  This is probably the preferred option. This is done automatically if object is from <code><a href="#topic+bestScales">bestScales</a></code> </p>
</td></tr>
<tr><td><code id="predict.psych_+3A_options">options</code></td>
<td>
<p>scoring options for bestScales objects 
(&quot;best.keys&quot;,&quot;weights&quot;,&quot;optimal.keys&quot;,&quot;optimal.weights&quot;)</p>
</td></tr>
<tr><td><code id="predict.psych_+3A_missing">missing</code></td>
<td>
<p>If missing=FALSE, cases with missing data are given NA scores, otherwise they are given the values based upon the wts x complete data </p>
</td></tr>
<tr><td><code id="predict.psych_+3A_impute">impute</code></td>
<td>
<p>Should missing cases be replaced by &quot;means&quot;, &quot;medians&quot; or treated as missing 
(&quot;none&quot; is the default</p>
</td></tr>
<tr><td><code id="predict.psych_+3A_...">...</code></td>
<td>
<p>More options to pass to predictions </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predicted factor/components/criteria scores.  If predicting from either <code><a href="#topic+fa">fa</a></code> or <code><a href="#topic+pca">pca</a></code>,the scores are based upon standardized items where the standardization is either that of the original data (old.data) or of the prediction set.  This latter case can lead to confusion if just a small number of predicted scores are found.   
</p>
<p>If the object is from <code><a href="#topic+bestScales">bestScales</a></code>, unit weighted scales are found (by default) using the best.keys and the predicted scores are then put into the metric of the means and standard deviations of the derivation sample. Other scoring key options may be specified using the &quot;options&quot; parameter.  Possible values are best.keys&quot;,&quot;weights&quot;,&quot;optimal.keys&quot;,&quot;optimal.weights&quot;.  See <code><a href="#topic+bestScales">bestScales</a></code> for details.
</p>
<p>By default, predicted scores are found by the matrix product of the standardized data with the factor or regression weights.  If missing is TRUE, then the predicted scores are the mean of the standardized data x weights for those data points that are not NA.
</p>


<h3>Note</h3>

<p>Thanks to Reinhold Hatzinger for the suggestion and request and to Sarah McDougald for the bestScales prediction.</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code>, <code><a href="#topic+bestScales">bestScales</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
x &lt;- sim.item(12,500)
f2 &lt;- fa(x[1:250,],2,scores="regression")  # a two factor solution
p2 &lt;- principal(x[1:250,],2,scores=TRUE)  # a two component solution
round(cor(f2$scores,p2$scores),2)  #correlate the components and factors from the A set
#find the predicted scores (The B set)
pf2 &lt;- predict(f2,x[251:500,],x[1:250,]) 

  #use the original data for standardization values 
pp2 &lt;- predict(p2,x[251:500,],x[1:250,]) 
 #standardized based upon the first set 
round(cor(pf2,pp2),2)   #find the correlations in the B set
#test how well these predicted scores match the factor scores from the second set
fp2 &lt;- fa(x[251:500,],2,scores=TRUE)
round(cor(fp2$scores,pf2),2)

pf2.n &lt;- predict(f2,x[251:500,])  #Standardized based upon the new data set
round(cor(fp2$scores,pf2.n))   
   #predict factors of set two from factors of set 1, factor order is arbitrary


#note that the signs of the factors in the second set are arbitrary

#predictions from bestScales
#the derivation sample
bs &lt;- bestScales(psychTools::bfi[1:1400,], cs(gender,education,age),folds=10,p.keyed=.5) 
pred &lt;- predict(bs,psychTools::bfi[1401:2800,]) #The prediction sample
cor2(pred,psychTools::bfi[1401:2800,26:28] ) #the validity of the prediction
summary(bs) #compare with bestScales cross validations

</code></pre>

<hr>
<h2 id='predicted.validity'>Find the predicted validities of a set of scales based on item statistics</h2><span id='topic+predicted.validity'></span><span id='topic+item.validity'></span><span id='topic+validityItem'></span>

<h3>Description</h3>

<p>The validity of a scale varies as a function of the number of items in the scale, their average intercorrelation, and their average validity. The asymptotic limit of a scales validity for any particular criterion is just the average validity divided by the square root of the average within scale item correlation.  <code><a href="#topic+predicted.validity">predicted.validity</a></code> will find the predicted validity for a set of scales (defined by a keys.list) and the average item validity for various criteria.  
</p>
<p>The function will find (and report) scale reliabilities (using <code><a href="#topic+reliability">reliability</a></code>) and average item validities (using <code><a href="#topic+item.validity">item.validity</a></code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predicted.validity(x, criteria, keys, scale.rel = NULL, item.val = NULL)
item.validity(x,criteria,keys) 
validityItem(x,criteria,keys)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predicted.validity_+3A_x">x</code></td>
<td>
<p>A data set</p>
</td></tr>
<tr><td><code id="predicted.validity_+3A_criteria">criteria</code></td>
<td>
<p>Variables to predict from the scales
</p>
</td></tr>
<tr><td><code id="predicted.validity_+3A_keys">keys</code></td>
<td>
<p>A  keys.list that defines the scales
</p>
</td></tr>
<tr><td><code id="predicted.validity_+3A_scale.rel">scale.rel</code></td>
<td>
<p>If not specified, these will be found.  Otherwise, this is the output from <code><a href="#topic+reliability">reliability</a></code>.
</p>
</td></tr>
<tr><td><code id="predicted.validity_+3A_item.val">item.val</code></td>
<td>
<p>If not specified, the average item validities for each scale will be found. Otherwise use the output from <code><a href="#topic+item.validity">item.validity</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When predicting criteria from a set of items formed into scales, the validity of the scale (that is, the correlations of the scale with each criteria) is a function of the average item validity (r_y), the average intercorrelation of the items in the scale (r_x), and the number of items in the scale (n).  The limit of validity is r_y/sqrt(r_x).  
</p>
<p>Criteria will differ in their predictability from a set of scales. These asymptotic values may be used to help the decision on which scales to develop further.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>predicted</code></td>
<td>
<p>The predicted validities given the scales specified</p>
</td></tr>
<tr><td><code>item.validities</code></td>
<td>
<p>The average item validities for each scale with each criterion</p>
</td></tr>
<tr><td><code>scale.reliabilities</code></td>
<td>
<p>The various statistics reported by the <code><a href="#topic+reliability">reliability</a></code> function</p>
</td></tr>
<tr><td><code>asymptotic</code></td>
<td>
<p>A matrix of the asymptotic validities</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.  <a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+reliability">reliability</a></code>, <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreFast">scoreFast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pred.bfi &lt;- predicted.validity(psychTools::bfi[,1:25], psychTools::bfi[,26:28],
     psychTools::bfi.keys)
pred.bfi
</code></pre>

<hr>
<h2 id='principal'> Principal components analysis (PCA)</h2><span id='topic+principal'></span><span id='topic+pca'></span>

<h3>Description</h3>

<p>Does an eigen value decomposition and returns eigen values, loadings, and degree of fit for a specified number of components.  Basically it is just  doing a principal components analysis (PCA) for n principal components of either a correlation or covariance matrix.  Can show the residual correlations as well. The quality of reduction in the squared correlations is reported by comparing residual correlations to original correlations. Unlike princomp, this returns a subset of just the best nfactors. The eigen vectors are rescaled by the sqrt of the eigen values to produce the component loadings more typical in factor analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>principal(r, nfactors = 1, residuals = FALSE,rotate="varimax",n.obs=NA, covar=FALSE,
 scores=TRUE,missing=FALSE,impute="median",oblique.scores=TRUE,method="regression",
 use ="pairwise",cor="cor",correct=.5,weight=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="principal_+3A_r">r</code></td>
<td>
<p>a correlation matrix.  If a raw data matrix is used, the correlations will be found using pairwise deletions for missing values.</p>
</td></tr>
<tr><td><code id="principal_+3A_nfactors">nfactors</code></td>
<td>
<p>Number of components to extract </p>
</td></tr>
<tr><td><code id="principal_+3A_residuals">residuals</code></td>
<td>
<p> FALSE, do not show residuals, TRUE, report residuals </p>
</td></tr>
<tr><td><code id="principal_+3A_rotate">rotate</code></td>
<td>
<p>&quot;none&quot;, &quot;varimax&quot;, &quot;quartimax&quot;, &quot;promax&quot;, &quot;oblimin&quot;, &quot;simplimax&quot;, and &quot;cluster&quot; are possible rotations/transformations of the solution. See <code><a href="#topic+fa">fa</a></code> for all rotations avaiable.</p>
</td></tr>
<tr><td><code id="principal_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics.</p>
</td></tr>
<tr><td><code id="principal_+3A_covar">covar</code></td>
<td>
<p>If false, find the correlation matrix from the raw data or convert to a correlation matrix if given a square matrix as input.</p>
</td></tr>
<tr><td><code id="principal_+3A_scores">scores</code></td>
<td>
<p>If TRUE, find component scores</p>
</td></tr>
<tr><td><code id="principal_+3A_missing">missing</code></td>
<td>
<p>if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean</p>
</td></tr>
<tr><td><code id="principal_+3A_impute">impute</code></td>
<td>
<p>&quot;median&quot; or &quot;mean&quot; values are used to replace missing values</p>
</td></tr> 
<tr><td><code id="principal_+3A_oblique.scores">oblique.scores</code></td>
<td>
<p>If TRUE (default), then the component scores are based upon the structure matrix.  If FALSE, upon the pattern matrix.</p>
</td></tr>
<tr><td><code id="principal_+3A_method">method</code></td>
<td>
<p>Which way of finding component scores should be used. The default is &quot;regression&quot;</p>
</td></tr>
<tr><td><code id="principal_+3A_weight">weight</code></td>
<td>
<p>If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.</p>
</td></tr>
<tr><td><code id="principal_+3A_use">use</code></td>
<td>
<p>How to treat missing data, use=&quot;pairwise&quot; is the default&quot;.  See cor for other options.</p>
</td></tr>
<tr><td><code id="principal_+3A_cor">cor</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, 
&quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric, &quot;mixed&quot; uses mixedCor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate</p>
</td></tr>
<tr><td><code id="principal_+3A_correct">correct</code></td>
<td>
<p>When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)</p>
</td></tr> 
<tr><td><code id="principal_+3A_...">...</code></td>
<td>
<p>other parameters to pass to functions such as factor.scores or the various rotation functions.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful for those cases where the correlation matrix is improper (perhaps because of SAPA techniques).
</p>
<p>There are a number of data reduction techniques including principal components analysis (PCA) and factor analysis (EFA).  Both PC and FA attempt to approximate a given correlation or covariance matrix of rank n with matrix of lower rank (p).  <code class="reqn">_nR_n \approx _{n}F_{kk}F_n'+ U^2</code> where k is much less than n.  For principal components, the item uniqueness is assumed to be zero and all elements of the correlation or covariance matrix are fitted. That is, <code class="reqn">_nR_n \approx _{n}F_{kk}F_n'</code>  The primary empirical difference between a components versus a factor model is the treatment of the variances for each item.  Philosophically, components are weighted composites of observed variables while in the factor model, variables are weighted composites of the factors. As the number of items increases, the difference between the two models gets smaller.  Factor loadings are the asymptotic component loadings as the number of items gets larger. 
</p>
<p>For a n x n correlation matrix, the n principal components completely reproduce the correlation matrix.  However, if just the first k principal components are extracted, this is the best k dimensional approximation of the matrix.
</p>
<p>It is important to recognize that rotated principal components are not principal components (the axes associated with the eigen value decomposition) but are merely components.  To point this out, unrotated principal components are labelled as PCi, while rotated PCs are now labeled as RCi (for rotated components) and obliquely transformed components as TCi (for transformed components). (Thanks to Ulrike Gromping for this suggestion.)
</p>
<p>Rotations and transformations are either part of psych (Promax and cluster), of base R (varimax), or of GPArotation (simplimax, quartimax, oblimin, etc.).  
</p>
<p>Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The &ldquo;cluster&rdquo; option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional &quot;keys&quot; parameter, the &quot;target&quot; option will rotate to a target supplied as a keys matrix. (See <code><a href="#topic+target.rot">target.rot</a></code>.)
</p>
<p>The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by <code class="reqn">\Phi = \theta' \theta</code>
</p>
<p>Some of the statistics reported are more appropriate for (maximum likelihood) factor analysis rather than principal components analysis, and are reported to allow comparisons with these other models. 
</p>
<p>Although for items, it is typical to find component scores by scoring the salient items (using, e.g., <code><a href="#topic+scoreItems">scoreItems</a></code>) component scores are found  by regression where the regression weights are <code class="reqn">R^{-1} \lambda</code> where <code class="reqn">\lambda</code> is the matrix of component loadings.   The regression approach is done  to be parallel with the factor analysis function <code><a href="#topic+fa">fa</a></code>.  The regression weights are found from the inverse of the correlation matrix times the component loadings.   This has the result that the component scores are standard scores (mean=0, sd = 1) of the standardized input.  A comparison to the scores from <code><a href="stats.html#topic+princomp">princomp</a></code> shows this difference.  princomp does not, by default, standardize the data matrix, nor are the components themselves standardized.  The regression weights are found from the Structure matrix, not the Pattern matrix. If the scores are found with the covar option = TRUE, then the scores are not standardized but are just mean centered.  
</p>
<p>Jolliffe (2002) discusses why the interpretation of rotated components is complicated.   Rencher (1992) discourages the use of rotated components. The approach used here is consistent with the factor analytic tradition.  The correlations of the items with the component scores closely matches (as it should) the component loadings (as reported in the structure matrix). 
</p>
<p>The output from the print.psych function displays the component loadings (from the pattern matrix), the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the component loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared component loadings. But for an oblique solution, it is the row sum of the (squared) orthogonal component loadings (remember, that rotations or transformations do not change the communality).  This information is returned (invisibly) from the print function as the object Vaccounted.
</p>


<h3>Value</h3>

<table>
<tr><td><code>values</code></td>
<td>
<p>Eigen Values of all components &ndash; useful for a scree plot</p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>which rotation was requested?</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>number of observations specified or found</p>
</td></tr>
<tr><td><code>communality</code></td>
<td>
<p>Communality estimates for each item.  These are merely the sum of squared factor loadings for that item.</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>Hoffman's index of complexity for each item.  This is just <code class="reqn">\frac{(\Sigma a_i^2)^2}{\Sigma a_i^4}</code> where a_i is the factor loading on the ith factor. From Hofmann (1978), MBR. See also  Pettersson and Turkheimer (2010).</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>A standard loading matrix of class &ldquo;loadings&quot;</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>Fit of the model to the correlation matrix </p>
</td></tr>
<tr><td><code>fit.off</code></td>
<td>
<p>how well are the off diagonal elements reproduced?</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>Residual matrix &ndash; if requested</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>Degrees of Freedom for this model. This is the number of observed correlations minus the number of independent parameters (number of items * number of factors - nf*(nf-1)/2.   That is, dof = niI * (ni-1)/2 - ni * nf + nf*(nf-1)/2.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>value of the function that is minimized by maximum likelihood procedures.  This is reported for comparison purposes and as a way to estimate chi square goodness of fit.  The objective function is 
<br />
<code class="reqn">f = (trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^{-1} R|) - n.items</code>.   Because components do not minimize the off diagonal, this fit will be not as good as for factor analysis. It is included merely for comparison purposes.</p>
</td></tr>
<tr><td><code>STATISTIC</code></td>
<td>
<p>If the number of observations is specified or found, this is a chi square based upon the objective function, f.  Using the formula from <code><a href="stats.html#topic+factanal">factanal</a></code>:
<br />
<code class="reqn">\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f </code> </p>
</td></tr>
<tr><td><code>PVAL</code></td>
<td>
<p>If n.obs &gt; 0, then what is the probability of observing a chisquare this large or larger?</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>If oblique rotations (using oblimin from the GPArotation package) are requested, what is the interfactor correlation.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>If scores=TRUE, then estimates of the factor scores are reported </p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The beta weights to find the principal components from the data</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001)  For components, these are of course 1.0.</p>
</td></tr>
<tr><td><code>valid</code></td>
<td>
<p>The correlations of the component score estimates with the components, if they were to be found and unit weights were used. (So called course coding).</p>
</td></tr>
<tr><td><code>r.scores</code></td>
<td>
<p>The correlation of the component scores.  (Since components are just weighted linear sums of the items, this is the same as Phi).</p>
</td></tr>
<tr><td><code>rot.mat</code></td>
<td>
<p>The rotation matrix used to produce the rotated component loadings. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>By default, the accuracy of the varimax rotation function seems to be less than the Varimax function. This can be enhanced by specifying eps=1e-14 in the call to principal if using varimax rotation. Furthermore, note that Varimax by default does not apply the Kaiser normalization, but varimax does. Gottfried Helms compared these two rotations with those produced by SPSS and found identical values if using the appropriate options. (See the last two examples.)
</p>
<p>The ability to use different kinds of correlations was added in version 1.9.12.31 to be compatible with the options in fa.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p>Grice, James W.  (2001), Computing and evaluating factor scores. Psychological Methods, 6, 430-450
</p>
<p>Jolliffe, I. (2002)  Principal Component Analysis (2nd ed). Springer.
</p>
<p>Rencher, A. C. (1992) Interpretation of Canonical Discriminant Functions, Canonical Variates, and Principal Components, the American Statistician, (46) 217-225.
</p>
<p>Revelle, W. An introduction to psychometric theory with applications in R (in prep) Springer. Draft chapters available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+VSS">VSS</a></code> (to test for the number of components or factors to extract), <code><a href="#topic+VSS.scree">VSS.scree</a></code> and <code><a href="#topic+fa.parallel">fa.parallel</a></code> to show a scree plot and compare it with random resamplings of the data), <code><a href="#topic+factor2cluster">factor2cluster</a></code> (for course coding keys), <code><a href="#topic+fa">fa</a></code> (for factor analysis), <code><a href="#topic+factor.congruence">factor.congruence</a></code> (to compare solutions), <code><a href="#topic+predict.psych">predict.psych</a></code> to find factor/component scores for a new data set based upon the weights from an original data set. </p>


<h3>Examples</h3>

<pre><code class='language-R'>#Four principal components of the Harman 24 variable problem
#compare to a four factor principal axes solution using factor.congruence
pc &lt;- principal(Harman74.cor$cov,4,rotate="varimax")
mr &lt;- fa(Harman74.cor$cov,4,rotate="varimax")  #minres factor analysis
pa &lt;- fa(Harman74.cor$cov,4,rotate="varimax",fm="pa")  # principal axis factor analysis
round(factor.congruence(list(pc,mr,pa)),2)

pc2 &lt;- principal(Harman.5,2,rotate="varimax")
pc2
round(cor(Harman.5,pc2$scores),2)  #compare these correlations to the loadings 
#now do it for unstandardized scores, and transform obliquely
pc2o &lt;- principal(Harman.5,2,rotate="promax",covar=TRUE)
pc2o
round(cov(Harman.5,pc2o$scores),2) 
pc2o$Structure    #this matches the covariances with the scores
biplot(pc2,main="Biplot of the Harman.5 socio-economic variables",labels=paste0(1:12))

#For comparison with SPSS  (contributed by Gottfried Helms)
pc2v &lt;- principal(iris[1:4],2,rotate="varimax",normalize=FALSE,eps=1e-14)
print(pc2v,digits=7)
pc2V &lt;- principal(iris[1:4],2,rotate="Varimax",eps=1e-7)
p &lt;- print(pc2V,digits=7)
round(p$Vaccounted,2)   # the amount of variance accounted for is returned as an object of print
</code></pre>

<hr>
<h2 id='print.psych'> Print and summary functions for the psych class </h2><span id='topic+print.psych'></span><span id='topic+summary.psych'></span>

<h3>Description</h3>

<p>Give limited output (print) or somewhat more detailed (summary) for most of the functions in psych. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
print(x,digits=2,all=FALSE,cut=NULL,sort=FALSE,short=TRUE,lower=TRUE,signif=NULL,...)

## S3 method for class 'psych'
summary(object,digits=2,items=FALSE,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.psych_+3A_x">x</code></td>
<td>
<p> Output from a psych function (e.g., factor.pa, omega,ICLUST, score.items, cluster.cor</p>
</td></tr>
<tr><td><code id="print.psych_+3A_object">object</code></td>
<td>
<p>Output from a psych function</p>
</td></tr>
<tr><td><code id="print.psych_+3A_items">items</code></td>
<td>
<p>items=TRUE (default) does not print the item whole correlations</p>
</td></tr>
<tr><td><code id="print.psych_+3A_digits">digits</code></td>
<td>
<p>Number of digits to use in printing</p>
</td></tr>
<tr><td><code id="print.psych_+3A_all">all</code></td>
<td>
<p>if all=TRUE, then the object is declassed and all output from the function is printed</p>
</td></tr>
<tr><td><code id="print.psych_+3A_cut">cut</code></td>
<td>
<p>Cluster loadings &lt; cut will not be printed.  For the factor analysis functions (fa and factor.pa etc.), cut defaults to 0, for ICLUST  to .3, for omega to .2.</p>
</td></tr>
<tr><td><code id="print.psych_+3A_sort">sort</code></td>
<td>
<p>Cluster loadings are in sorted order</p>
</td></tr>
<tr><td><code id="print.psych_+3A_short">short</code></td>
<td>
<p>Controls how much to print</p>
</td></tr>
<tr><td><code id="print.psych_+3A_lower">lower</code></td>
<td>
<p>For square matrices, just print the lower half of the matrix</p>
</td></tr> 
<tr><td><code id="print.psych_+3A_signif">signif</code></td>
<td>
<p>If not NULL,  a numeric value, show just signif number of leading digits for describe output</p>
</td></tr>
<tr><td><code id="print.psych_+3A_...">...</code></td>
<td>
<p>More options to pass to summary and print</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most of the psych functions produce too much output.  print.psych and summary.psych use generic methods for printing just the highlights.  To see what else is available,  ask for the structure of the particular object: (str(theobject) ).
</p>
<p>Alternatively, to get complete output, unclass(theobject) and then print it. This may be done by using the all=TRUE option.
</p>
<p>As an added feature, if the promax function is applied to a factanal loadings matrix, the normal output just provides the rotation matrix.  print.psych will provide the factor correlations. (Following a suggestion by John Fox and Uli Keller to the R-help list).  The alternative is to just use the Promax function directly on the factanal object.
</p>


<h3>Value</h3>

<p> Various psych functions produce copious output.  This is a way to summarize the most important parts of the output of the score.items, cluster.scores, and ICLUST functions.  See those ( <code><a href="#topic+score.items">score.items</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code>, <code><a href="#topic+cluster.loadings">cluster.loadings</a></code>, or <code><a href="#topic+ICLUST">ICLUST</a></code>) for details on what is produced.
</p>
<p>The signf option is available for the output from  <code><a href="#topic+describe">describe</a></code> to adjust the number of digits shown for all columns.  This is slightly different from what happens if you specify digits, which rounds all output to the number of digits.   print(x,signif=3) will print just the 3 largest digits of x, which will frequently result in scientific notation for any column where that would be appropriate for at least one row.
</p>


<h3>Note</h3>

<p> See <code><a href="#topic+score.items">score.items</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code>, <code><a href="#topic+cluster.loadings">cluster.loadings</a></code>, or <code><a href="#topic+ICLUST">ICLUST</a></code>for details on what is printed.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bfi)
 keys.list &lt;- list(agree=c(-1,2:5),conscientious=c(6:8,-9,-10),
 extraversion=c(-11,-12,13:15),neuroticism=c(16:20),openness = c(21,-22,23,24,-25))
 keys &lt;- make.keys(25,keys.list,item.labels=colnames(psychTools::bfi[1:25]))
 scores &lt;- score.items(keys,psychTools::bfi[1:25])
 scores
 summary(scores)
</code></pre>

<hr>
<h2 id='Promax'> Perform Procustes,bifactor, promax or targeted rotations and return the inter factor angles.</h2><span id='topic+Promax'></span><span id='topic+faRotate'></span><span id='topic+Procrustes'></span><span id='topic+TargetQ'></span><span id='topic+TargetT'></span><span id='topic+target.rot'></span><span id='topic+bifactor'></span><span id='topic+biquartimin'></span><span id='topic+varimin'></span><span id='topic+vgQ.bimin'></span><span id='topic+vgQ.targetQ'></span><span id='topic+vgQ.varimin'></span><span id='topic+equamax'></span>

<h3>Description</h3>

<p>The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>faRotate(loadings,rotate="oblimin",...)
bifactor(L, Tmat=diag(ncol(L)), normalize=FALSE, eps=1e-5, maxit=1000)
biquartimin(L, Tmat=diag(ncol(L)), normalize=FALSE, eps=1e-5, maxit=1000)
TargetQ(L, Tmat=diag(ncol(L)), normalize=FALSE, eps=1e-5, maxit=1000,Target=NULL)
TargetT(L, Tmat=diag(ncol(L)), normalize=FALSE, eps=1e-5, maxit=1000,Target=NULL)
Promax(x,m=4, normalize=FALSE, pro.m = 4) 
Procrustes(L,Target) #adapted from Niels Waler
target.rot(x,keys=NULL)
varimin(L, Tmat = diag(ncol(L)), normalize = FALSE, eps = 1e-05, maxit = 1000)
vgQ.bimin(L)   #called by bifactor
vgQ.targetQ(L,Target=NULL)  #called by TargetQ
vgQ.varimin(L)  #called by varimin
equamax(L, Tmat=diag(ncol(L)), eps=1e-5, maxit=1000) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Promax_+3A_x">x</code></td>
<td>
<p>A loadings matrix</p>
</td></tr>
<tr><td><code id="Promax_+3A_l">L</code></td>
<td>
<p>A loadings matrix</p>
</td></tr>
<tr><td><code id="Promax_+3A_loadings">loadings</code></td>
<td>
<p>A loadings matrix</p>
</td></tr>
<tr><td><code id="Promax_+3A_rotate">rotate</code></td>
<td>
<p>Which rotation should be used?</p>
</td></tr>
<tr><td><code id="Promax_+3A_m">m</code></td>
<td>
<p>the power to which to raise the varimax loadings (for Promax)</p>
</td></tr>
<tr><td><code id="Promax_+3A_pro.m">pro.m</code></td>
<td>
<p>the power to which to raise the various loadings in Promax.</p>
</td></tr>
<tr><td><code id="Promax_+3A_keys">keys</code></td>
<td>
<p>An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.</p>
</td></tr>
<tr><td><code id="Promax_+3A_target">Target</code></td>
<td>
<p>A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.</p>
</td></tr>  
<tr><td><code id="Promax_+3A_tmat">Tmat</code></td>
<td>
<p>An initial rotation matrix</p>
</td></tr>
<tr><td><code id="Promax_+3A_normalize">normalize</code></td>
<td>
<p>parameter passed to optimization routine (GPForth in the GPArotation package and Promax)</p>
</td></tr>
<tr><td><code id="Promax_+3A_eps">eps</code></td>
<td>
<p>parameter passed to optimization routine (GPForth in the GPArotation package) </p>
</td></tr>
<tr><td><code id="Promax_+3A_maxit">maxit</code></td>
<td>
<p>parameter passed to optimization routine (GPForth in the GPArotation package)</p>
</td></tr>
<tr><td><code id="Promax_+3A_...">...</code></td>
<td>
<p>Other parameters to pass (e.g. to faRotate) include a Target list or matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). 
</p>
<p>TargetT is an orthogonal target rotation function which allows for missing NA values in the target. 
</p>
<p>faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.
</p>
<p>The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.
</p>
<p>bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm=&quot;mle&quot; is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. 
</p>
<p>Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. 
</p>
<p>varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).
</p>
<p>In addition, these functions will take output from either the factanal, <code><a href="#topic+fa">fa</a></code> or earlier (<code><a href="#topic+factor.pa">factor.pa</a></code>, <code><a href="#topic+factor.minres">factor.minres</a></code> or <code><a href="#topic+principal">principal</a></code>)  functions and select just the loadings matrix for analysis.
</p>
<p>equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. 
</p>
<p>TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.
</p>
<p>The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. 
</p>
<p>The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) 
</p>
<p>target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
</p>


<h3>Value</h3>

<table>
<tr><td><code>loadings</code></td>
<td>
<p>Oblique factor loadings</p>
</td></tr>
<tr><td><code>rotmat</code></td>
<td>
<p>The rotation matrix applied to the original loadings to produce the promax solution or the targeted matrix</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>The interfactor correlation matrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Promax is direct adaptation of the stats:promax function following suggestions to the R-help list by Ulrich Keller and John Fox. Further  modified to do  targeted rotation similar to a function of Michael Browne.
</p>
<p>varimin is a direct application of the GPArotation GPForth function modified to do varimin. 
</p>


<h3>Note</h3>

<p>The Target for TargetT can be a matrix, but for TartetQ must be a list.  This seems to be a feature of GPArotation.
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Ertel, S. (2013). Factor analysis: healing an ailing model. Universitatsverlag Gottingen.
</p>
<p>Hendrickson, A. E. and  White, P. O, 1964, British Journal of Statistical Psychology, 17, 65-70.
</p>
<p>Jennrich, Robert and Bentler, Peter (2011) Exploratory Bi-Factor Analysis. Psychometrika,  1-13
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+promax">promax</a></code>, <code><a href="#topic+fa">fa</a></code>, or <code><a href="#topic+principal">principal</a></code> for examples of data analysis and   <code><a href="#topic+Holzinger">Holzinger</a></code> or <code><a href="#topic+Bechtoldt">Bechtoldt</a></code> for  examples of bifactor data. <code><a href="#topic+factor.rotate">factor.rotate</a></code> for 'hand rotation'.  </p>


<h3>Examples</h3>

<pre><code class='language-R'>jen &lt;- sim.hierarchical()
f3 &lt;- fa(jen,3,rotate="varimax")
f3   #not a very clean solution
Promax(f3)  #this obliquely rotates, but from the varimax target
target.rot(f3)  #this obliquely rotates to wards a simple structure target

#compare this rotation with the solution from a targeted rotation aimed for 
#an independent cluster solution
#now try a bifactor solution
fb &lt;-fa(jen,3,rotate="bifactor")
fq &lt;- fa(jen,3,rotate="biquartimin")
#Suitbert Ertel has suggested varimin
fm &lt;-  fa(jen,3,rotate="varimin") #the Ertel varimin
fn &lt;- fa(jen,3,rotate="none")  #just the unrotated factors
#compare them
factor.congruence(list(f3,fb,fq,fm,fn))
# compare an oblimin with a target rotation using the Browne algorithm
 #note that we are changing the factor #order (this is for demonstration only)
 Targ &lt;- make.keys(9,list(f1=1:3,f2=7:9,f3=4:6)) 
 Targ &lt;- scrub(Targ,isvalue=1)  #fix the 0s, allow the NAs to be estimated
 Targ &lt;- list(Targ)  #input must be a list
#show the target
 Targ
 fa(Thurstone,3,rotate="TargetQ",Target=Targ)  #targeted oblique rotation
#compare with oblimin
f3 &lt;- fa(Thurstone,3)
#now try a targeted orthogonal rotation
Targ &lt;- make.keys(9,list(f1=1:3,f2=7:9,f3=4:6)) 
faRotate(f3$loadings,rotate="TargetT",Target=list(Targ)) #orthogonal


</code></pre>

<hr>
<h2 id='psych.misc'>Miscellaneous helper functions for the psych package</h2><span id='topic+psych.misc'></span><span id='topic+misc'></span><span id='topic+tableF'></span><span id='topic+lowerCor'></span><span id='topic+lowerMat'></span><span id='topic+matMult'></span><span id='topic+progressBar'></span><span id='topic+reflect'></span><span id='topic+shannon'></span><span id='topic+test.all'></span><span id='topic+cor2'></span><span id='topic+levels2numeric'></span><span id='topic+char2numeric'></span><span id='topic+nchar2numeric'></span><span id='topic+isCorrelation'></span><span id='topic+isCovariance'></span><span id='topic+fromTo'></span><span id='topic+cs'></span><span id='topic+acs'></span><span id='topic+SAPAfy'></span>

<h3>Description</h3>

<p>This is a set of minor, if not trivial, helper functions.
lowerCor finds the correlation of x variables and then prints them using 
lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.
reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)
progressBar prints out ...  as a calling routine (e.g., <code><a href="#topic+tetrachoric">tetrachoric</a></code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code><a href="#topic+cor2">cor2</a></code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psych.misc()
lowerCor(x,digits=2,use="pairwise",method="pearson",minlength=5,cor="cor",show=TRUE)
cor2(x,y,digits=2,use="pairwise",method="pearson",cor="cor",show=TRUE)
lowerMat(R, digits = 2,minlength=5)
matMult(x,y) #multiply two matrices with missing data
tableF(x,y)
reflect(f,flip=NULL)
progressBar(value,max,label=NULL) 
shannon(x,correct=FALSE,base=2)
test.all(pl,package="psych",dependencies 
      = c("Depends", "Imports", "LinkingTo"),find=FALSE,skip=NULL) 
 levels2numeric(x) 
 char2numeric(x,flag=TRUE) 
 nchar2numeric(x,flag=TRUE)
 isCorrelation(x,na.rm=FALSE) #test if an object is a symmetric matrix 
    # with diagonals of 1 and  all values between -1 and 1
 isCovariance(x)  #test if an object is a symmetric matrix   
 fromTo(data,from,to=NULL) #convert character names to locations as specified in colnames
 #of data
 cs(...) #convert a list of text words to  character vector
 acs(...) #convert a list of text words to  a single string
 SAPAfy(x,y)  #sample y columns from x, replace other columns with NA

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psych.misc_+3A_r">R</code></td>
<td>
<p>A rectangular matrix or data frame (probably a correlation matrix)</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_x">x</code></td>
<td>
<p>A data matrix or data frame or a vector depending upon the function.</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_y">y</code></td>
<td>
<p>A data matrix or data frame or a vector</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_f">f</code></td>
<td>
<p>The object returned from either a factor analysis (fa) or a principal components analysis (principal) </p>
</td></tr>
<tr><td><code id="psych.misc_+3A_digits">digits</code></td>
<td>
<p>round to digits</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_minlength">minlength</code></td>
<td>
<p>Abbreviate to minlength in lowerCor</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_show">show</code></td>
<td>
<p>Display the correlations from lowerCor or cor2 (default is TRUE)</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_use">use</code></td>
<td>
<p>Should pairwise deletion be done, or one of the other options to cor</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_na.rm">na.rm</code></td>
<td>
<p>Should we check for NA on the diagonal of a correlation matices</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_method">method</code></td>
<td>
<p>&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;</p>
</td></tr> 
<tr><td><code id="psych.misc_+3A_cor">cor</code></td>
<td>
<p>defaults to the normal cor function, but can also do tetrachoric, polychoric or covariances (cov)</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_value">value</code></td>
<td>
<p>the current value of some looping variable</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_max">max</code></td>
<td>
<p>The maximum value the loop will achieve</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_label">label</code></td>
<td>
<p>what function is looping</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_flip">flip</code></td>
<td>
<p>The factor or components to be reversed keyed (by factor number)</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_flag">flag</code></td>
<td>
<p>flag=TRUE in char2numeric will flag (with *) those variables that had been numeric. This changes the variable name. flag = FALSE does not mark those variables.</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_correct">correct</code></td>
<td>
<p>Correct for the maximum possible information in this item</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_base">base</code></td>
<td>
<p>What is the base for the log function (default=2, e implies base = exp(1))</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_pl">pl</code></td>
<td>
<p>The name of a package (or list of packages) to be activated and then have all      
the examples tested.</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_package">package</code></td>
<td>
<p>Find the dependencies for this package, e.g., psych</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_dependencies">dependencies</code></td>
<td>
<p>Which type of dependency to examine?</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_find">find</code></td>
<td>
<p>Look up the dependencies, and then test all of their examples</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_skip">skip</code></td>
<td>
<p>Do not test these dependencies</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_data">data</code></td>
<td>
<p>A dataframe or matrix to choose from</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_from">from</code></td>
<td>
<p>select from column with name from to column with name to</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_to">to</code></td>
<td>
<p>select from column from to column to</p>
</td></tr>
<tr><td><code id="psych.misc_+3A_...">...</code></td>
<td>
<p>Any string of legitimate objects</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+lowerCor">lowerCor</a></code> prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn calls
</p>
<p><code><a href="#topic+lowerMat">lowerMat</a></code> which does the pretty printing.  
</p>
<p>It is important to remember to not call <code><a href="#topic+lowerCor">lowerCor</a></code> when all you need is <code><a href="#topic+lowerMat">lowerMat</a></code>!  
</p>
<p><code><a href="#topic+cs">cs</a></code> is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
</p>


<h3>Value</h3>

 
<p><code><a href="#topic+tableF">tableF</a></code> is fast alternative to the table function for creating two way tables of numeric variables.  It does not have any of the elegant checks of the table function and thus is much faster.  Used in the <code><a href="#topic+tetrachoric">tetrachoric</a></code> and <code><a href="#topic+polychoric">polychoric</a></code> functions to maximize speed.
</p>
<p><code><a href="#topic+lowerCor">lowerCor</a></code> Finds and prints (using <code><a href="#topic+lowerMat">lowerMat</a></code>) the lower diagonal correlation matrix but returns (invisibly) the full correlation matrix found with the use, method and  cor  parameters. The default values are for pairwise deletion of variables, &quot;pearson&quot; correlations.  Specify cor=&quot;tetrachoric&quot;, &quot;polychoric&quot;, or &quot;mixed&quot; for alternatives. By default, digits is set to print to 2 decimal places.  By default, it will change character variables to numeric and flag them. 
</p>
<p><code><a href="#topic+lowerMat">lowerMat</a></code>Shows the lower triangle of a matrix, rounded to digits with titles abbreviated to digits + 3
</p>
<p><code><a href="#topic+progressBar">progressBar</a></code> Display a series of dots as we progress through a slow loop (removed from anything  using multicores). 
</p>
<p><code><a href="#topic+tableF">tableF</a></code> (for tableFast) is a cut down version of table that does no error checking, nor returns pretty output, but is significantly faster than table.  It will just work on two integer vectors.  This is used in polychoric an tetrachoric for about a 50% speed improvement for large problems.  
</p>
<p><code><a href="#topic+shannon">shannon</a></code> finds Shannon's H index of information.  Used for estimating the complexity or diversity of the distribution of responses in a vector or matrix. </p>
<p style="text-align: center;"><code class="reqn">H = -\sum{p_i log(p_i) }</code>
</p>

<p><code><a href="#topic+test.all">test.all</a></code> allows one to test all the examples in specified package.  This allows us to make sure that those examples work when other packages (e.g., psych) are also loaded.  This is used when developing revisions to the psych package to make sure the the other packages work.  Some packages will not work and/or crash the system (e.g., DeducerPlugInScaling requires Java and even with Java, crashes when loaded, even if psych is not there!).  Alternatively, if testing a long list of dependencies, you can skip the first part by specifying them by name.
</p>
<p><code><a href="#topic+cor2">cor2</a></code> will find and display the correlations between two sets of variables, rounded to digits, using the other options. If x is a list of multiple sets (two or more), then all sets are correlated.  
</p>
<p><code><a href="#topic+levels2numeric">levels2numeric</a></code>converts character data with levels to numeric data.  Used in the SAPA analyses where we code some variables, (e.g., gender, education) with character codes to help in the documentation of files, but want to do analyses of correlations with other categorical variables. 
</p>
<p><code><a href="#topic+char2numeric">char2numeric</a></code>converts character data with levels to numeric data.  Used for cases when data from questionnaires include the response categories rathere than numeric data.  Unless the levels of the data are in meaningful order, the numeric results are not useful.  Most useful if doing polychoric analyses.  Note this is not suitable for recoding numeric data stored as characters, for it will force them to levels first.  See <code><a href="#topic+nchar2numeric">nchar2numeric</a></code>.  Note that for very small data sets, because the recoding done by char2numeric is column wise, it might result in different numerical results for the same characters in different columns.   
</p>
<p>Problems with <code><a href="#topic+char2numeric">char2numeric</a></code> can be solved by using the <code>psychTools::recode</code> function.
</p>
<p><code><a href="#topic+nchar2numeric">nchar2numeric</a></code>converts numbers coded as characters (quoted) to numeric without forcing them to factors first.  
</p>
<p><code><a href="#topic+fromTo">fromTo</a></code> selects the columns in data from to (see the examples)
</p>
<p><code><a href="#topic+cs">cs</a></code> concatenates strings without the need to identify variables by &quot; &quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corr.test">corr.test</a></code> to find correlations, count the pairwise occurrences, and to give significance tests for each correlation.  <code><a href="#topic+r.test">r.test</a></code> for a number of tests of correlations, including tests of the difference between correlations.  <code><a href="#topic+lowerUpper">lowerUpper</a></code> will display the differences between two matrices.</p>


<h3>Examples</h3>

<pre><code class='language-R'>lowerMat(Thurstone)
lb &lt;- lowerCor(psychTools::bfi[1:10])  #finds and prints the lower correlation matrix, 
  # returns the square matrix.
lowerCor(psychTools::ability[,1:5]) #the Pearson correlations, compare with
lowerCor(psychTools::ability[,1:5],cor="tetra")
#fiml &lt;- corFiml(psychTools::bfi[1:10])     #FIML correlations require lavaan package
#lowerMat(fiml)  #to get pretty output
f3 &lt;- fa(Thurstone,3)
f3r &lt;- reflect(f3,2)  #reflect the second factor
#find the complexity of the response patterns of the iqitems.
round(shannon(psychTools::iqitems),2) 
#test.all('BinNor')  #Does the BinNor package work when we are using other packages
bestItems(lb,"A3",cut=.1,dictionary=psychTools::bfi.dictionary[1:2])
#to make this a latex table 
#df2latex(bestItems(lb,2,cut=.2))
#
data(psychTools::bfi.dictionary)
f2 &lt;- fa(psychTools::bfi[1:10],2)
fa.lookup(f2,psychTools::bfi.dictionary)

sa1 &lt;-sat.act[1:2]
sa2 &lt;- sat.act[3:4]
sa3 &lt;- sat.act[5:6]
cor2(sa1,sa2)
cor2(list(sa1,sa2))  #show within set and between set cors
cor2(list(sa1,sa2,sa3))
lowerCor(fromTo(sat.act,"ACT","SATQ")) #show some correlations
vect &lt;- cs(ACT,SATQ)  #skip the quotes
vect   #they are in this vector
#to combine longer terms
vect &lt;- cs("Here is a longish",vector, that, we ,"want to combine", into, several)
vect
temp &lt;- acs("Here is a longish",vector, that, we ,"want to combine", into, one)
temp
lowerCor(fromTo(sat.act,cs(ACT,SATQ)))
lowerCor(fromTo(bfi,cs(A3,C4)),cor="poly") #compare with
lowerCor(fromTo(bfi,cs(A3,C4)))


set.seed(42)  
temp &lt;- SAPAfy(bfi[1:10],3)  #30 % sample from bfi
f2 &lt;- fa(bfi[1:10],2)
 f2s &lt;- fa(temp,2)
 fa.congruence(f2s,f2)  #the two factor structure are almost identical
 #although the scores are not identical
cor2(f2$scores, f2s$scores)

</code></pre>

<hr>
<h2 id='r.test'>Tests of significance for correlations</h2><span id='topic+r.test'></span>

<h3>Description</h3>

<p>Tests the significance of a single correlation, the difference between two independent correlations, the difference between two dependent correlations sharing one variable (Williams's Test), or the difference between two dependent correlations with different variables (Steiger Tests).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r.test(n, r12, r34 = NULL, r23 = NULL, r13 = NULL, r14 = NULL, r24 = NULL, 
          n2 = NULL,pooled=TRUE, twotailed = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r.test_+3A_n">n</code></td>
<td>
<p>Sample size of first group </p>
</td></tr>
<tr><td><code id="r.test_+3A_r12">r12</code></td>
<td>
<p>Correlation to be tested</p>
</td></tr>
<tr><td><code id="r.test_+3A_r34">r34</code></td>
<td>
<p>Test if this correlation is different from r12, if r23 is specified, but r13 is not, then r34 becomes r13  </p>
</td></tr>
<tr><td><code id="r.test_+3A_r23">r23</code></td>
<td>
<p>if ra = r(12) and rb = r(13)  then test for differences of dependent correlations given r23</p>
</td></tr>
<tr><td><code id="r.test_+3A_r13">r13</code></td>
<td>
<p>implies  ra =r(12) and rb =r(34)  test for difference of dependent correlations </p>
</td></tr>
<tr><td><code id="r.test_+3A_r14">r14</code></td>
<td>
<p>implies   ra =r(12) and rb =r(34) </p>
</td></tr>
<tr><td><code id="r.test_+3A_r24">r24</code></td>
<td>
<p> ra =r(12) and rb =r(34)</p>
</td></tr>
<tr><td><code id="r.test_+3A_n2">n2</code></td>
<td>
<p>n2 is specified in the case of two independent correlations. n2 defaults to n if if not specified </p>
</td></tr>
<tr><td><code id="r.test_+3A_pooled">pooled</code></td>
<td>
<p>use pooled estimates of correlations</p>
</td></tr>
<tr><td><code id="r.test_+3A_twotailed">twotailed</code></td>
<td>
<p> should a twotailed or one tailed test be used </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending upon the input, one of four different tests of correlations is done.
1) For a sample size n, find the t value for a single correlation where </p>
<p style="text-align: center;"><code class="reqn">t = \frac{r * \sqrt(n-2)}{\sqrt(1-r^2)}
</code>
</p>
<p> and 
</p>
<p style="text-align: center;"><code class="reqn">se = \sqrt{\frac{1-r^2}{n-2}}) </code>
</p>
<p>.
</p>
<p>2) For sample sizes of n and n2 (n2 = n if not specified) find the z of the difference between the z transformed correlations divided by the standard error of the difference of two z scores: 
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{z_1  - z_2}{\sqrt{\frac{1}{(n_1 - 3) + (n_2 - 3)}}}</code>
</p>
<p>.
</p>
<p>3) For sample size n, and correlations r12, r13 and r23 test for the difference of two dependent correlations (r12 vs r13).  
</p>
<p>4) For sample size n, test for the difference between two dependent correlations involving different variables.
</p>
<p>Consider the correlations from Steiger (1980), Table 1:  Because these all from the same subjects, any tests must be of dependent correlations.  For dependent correlations, it is necessary to specify at least 3 correlations (e.g., r12, r13, r23) 
</p>

<table>
<tr>
 <td style="text-align: left;">
Variable </td><td style="text-align: right;"> M1 </td><td style="text-align: right;"> F1 </td><td style="text-align: right;"> V1 </td><td style="text-align: right;"> M2 </td><td style="text-align: right;"> F2  </td><td style="text-align: right;"> V2 </td>
</tr>
<tr>
 <td style="text-align: left;">
M1  1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
F1  </td><td style="text-align: right;"> .10 </td><td style="text-align: right;"> 1.00</td>
</tr>
<tr>
 <td style="text-align: left;">
V1 </td><td style="text-align: right;"> .40 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
M2 </td><td style="text-align: right;"> .70 </td><td style="text-align: right;"> .05 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
F2 </td><td style="text-align: right;"> .05 </td><td style="text-align: right;"> .70 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> 1.00  </td>
</tr>
<tr>
 <td style="text-align: left;">
V2 </td><td style="text-align: right;"> .45 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> .80 </td><td style="text-align: right;"> .50 </td><td style="text-align: right;"> .60 </td><td style="text-align: right;"> 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>For clarity, correlations may be specified by value.  If specified by location and if doing the test of dependent correlations, if three correlations are specified, they are assumed to be in the order r12, r13, r23.
</p>
<p>Consider the examples from Steiger: 
</p>
<p>Case A: 
where Masculinity at time 1 (M1) correlates with Verbal Ability .5 (r12), femininity at time 1 (F1) correlates with Verbal ability  r13 =.4, and M1 correlates with F1 (r23= .1).  Then, given the correlations: r12 = .4, r13 = .5, and r23 = .1, t = -.89 for n =103, i.e.,
r.test(n=103, r12=.4, r13=.5,r23=.1)  
</p>
<p>Case B: Test whether correlation between two variables (e.g., F and V) is the same over time (e.g. F1V1 = F2V2) 
</p>
<p>r.test(n = 103, r12 = 0.5, r34 = 0.6, r23 = 0.5, r13 = 0.7, r14 = 0.5,  r24 = 0.8)
</p>


<h3>Value</h3>

<table>
<tr><td><code>test</code></td>
<td>
<p>Label of test done</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>z value for tests 2 or 4</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>t value for tests 1 and 3</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>probability value of z or t</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Steiger specifically rejects using the Hotelling T test to test the difference between correlated correlations. Instead, he recommends Williams' test. (See also Dunn and Clark, 1971). These tests follow Steiger's advice.
The test of two independent correlations is just a z test of the difference of the  Fisher's z transformed correlations divided by the standard error of the difference.  (See Cohen et al, p 49).
</p>
<p>One of the beautiful features of R is what works on single value works on vectors and matrices.  Thus, r.test can be used to test the pairwise diference of all the elements of a correlation matrix.  See the last example.
</p>
<p>By default, the probabilities are reported to 2 decimal places.  This will, of course, sometimes lead to statements such as p &lt; .1  when in fact p &lt; .1001 or even more precisely p &lt; .1000759.  To achieve the higher precision, use a print statement with the preferred number of digits.  See the next to last set of examples (courtesy of Julia Rohrer).
</p>


<h3>Author(s)</h3>

<p>William Revelle </p>


<h3>References</h3>

<p>Cohen, J. and Cohen, P. and West, S.G. and Aiken, L.S. (2003) Applied multiple regression/correlation analysis for the behavioral sciences, L.Erlbaum Associates, Mahwah, N.J.
</p>
<p>Olkin, I. and Finn, J. D. (1995). Correlations redux. Psychological Bulletin, 118(1):155-164. 
</p>
<p>Steiger, J.H. (1980), Tests for comparing elements of a correlation matrix, Psychological Bulletin, 87, 245-251. 
</p>
<p>Williams, E.J.  (1959) Regression analysis. Wiley, New York, 1959.
</p>


<h3>See Also</h3>

<p> See also  <code><a href="#topic+corTest">corTest</a></code> which tests all the elements of a  correlation matrix, and <code><a href="#topic+cortest.mat">cortest.mat</a></code> to compare two matrices of correlations.  r.test  extends the tests in <code><a href="#topic+paired.r">paired.r</a></code>,<code><a href="#topic+r.con">r.con</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 30 
r &lt;- seq(0,.9,.1) 
rc &lt;- matrix(r.con(r,n),ncol=2) 
test &lt;- r.test(n,r)
r.rc &lt;- data.frame(r=r,z=fisherz(r),lower=rc[,1],upper=rc[,2],t=test$t,p=test$p) 
round(r.rc,2) 

r.test(50,r)
r.test(30,.4,.6)       #test the difference between two independent correlations
r.test(103,.4,.5,.1)   #Steiger case A of dependent correlations 
r.test(n=103, r12=.4, r13=.5,r23=.1)  
#for complicated tests, it is probably better to specify correlations by name
r.test(n=103,r12=.5,r34=.6,r13=.7,r23=.5,r14=.5,r24=.8)   #steiger Case B 


##By default, the precision of p values is 2 decimals
#Consider three different precisions shown by varying the requested number of digits
r12 =  0.693458895410494 
r23 =  0.988475791500198 
r13 =  0.695966022434845 
print(r.test(n =  5105 ,  r12 =  r12 ,  r23 = r23 , r13 =  r13 )) #probability &lt; 0.1
print(r.test(n =  5105 ,  r12 =  r12, r23 = r23 , r13 =  r13 ),digits=4) #p &lt; 0.1001
print(r.test(n =  5105 , r12 =  r12, r23 = r23 , r13 =  r13 ),digits=8) #p&lt; &lt;0.1000759 



#an example of how to compare the elements of two matrices 
R1 &lt;- lowerCor(psychTools::bfi[1:200,1:5])  #find one set of Correlations
R2 &lt;- lowerCor(psychTools::bfi[201:400,1:5]) #and now another set sampled 
#from the same population
test &lt;- r.test(n=200, r12 = R1, r34 = R2)
round(lowerUpper(R1,R2,diff=TRUE),digits=2)  #show the differences between correlations
#lowerMat(test$p)  #show the p values of the difference between the two matrices
adjusted &lt;- p.adjust(test$p[upper.tri(test$p)])
both &lt;- test$p 
both[upper.tri(both)] &lt;- adjusted
round(both,digits=2)  #The lower off diagonal are the raw ps, the upper the adjusted ps


</code></pre>

<hr>
<h2 id='rangeCorrection'>Correct correlations for restriction of range. (Thorndike Case 2)
</h2><span id='topic+rangeCorrection'></span>

<h3>Description</h3>

<p>In applied settings, it is typical to find a correlation between a predictor and some criterion.  Unfortunately, if the predictor is used to choose the subjects, the range of the predictor is seriously reduced.  This restricts the observed correlation to be less than would be observed in the full range of the predictor.  A correction for this problem is well known as Thorndike Case 2:
</p>
<p>Let R the unrestricted correlaton, r the restricted correlation, S the unrestricted standard deviation, s the restricted standard deviation, then
</p>
<p>R = (rS/s)/  sqrt(1-r^2 + r^2(S^2/s^2)).   
</p>
<p>Several other cases of restriction were also considered by Thorndike and are implemented in <code><a href="#topic+rangeCorrection">rangeCorrection</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rangeCorrection(r,sdu,sdr,sdxu=NULL,sdxr=NULL,case=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rangeCorrection_+3A_r">r</code></td>
<td>
<p>The observed correlation</p>
</td></tr>
<tr><td><code id="rangeCorrection_+3A_sdu">sdu</code></td>
<td>
<p>The unrestricted standard deviation)</p>
</td></tr>
<tr><td><code id="rangeCorrection_+3A_sdr">sdr</code></td>
<td>
<p>The restricted standard deviation</p>
</td></tr>
<tr><td><code id="rangeCorrection_+3A_sdxu">sdxu</code></td>
<td>
<p>Unrestricted standard deviation for case 4</p>
</td></tr>
<tr><td><code id="rangeCorrection_+3A_sdxr">sdxr</code></td>
<td>
<p>Restricted standard deviation for case 4</p>
</td></tr>
<tr><td><code id="rangeCorrection_+3A_case">case</code></td>
<td>
<p>Which of the four Thurstone/Stauffer cases to use</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When participants in a study are selected on one variable, that will reduce the variance of that variable and the resulting correlation.  Thorndike (1949) considered four cases of range restriction.  Others have continued this discussion but have changed the case numbers.  
</p>
<p>Can be used to find correlations in a restricted sample as well as the unrestricted sample.  Not the same as the correction to reliability for restriction of range.
</p>


<h3>Value</h3>

<p>The corrected correlation.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Stauffer, Joseph and Mendoza, Jorge. (2001) The proper sequence for correcting correlation coefficients for range restriction and unreliability.
Psychometrika, 66, 63-68.
</p>


<h3>See Also</h3>

<p>cRRr in the psychometric package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rangeCorrection(.33,100.32,48.19) #example from Revelle (in prep) Chapter 4.
</code></pre>

<hr>
<h2 id='reliability'>Reports 7 different estimates of scale reliabity including alpha, omega, split half</h2><span id='topic+reliability'></span><span id='topic+plot.reliability'></span>

<h3>Description</h3>

<p>Revelle and Condon, (2019) reviewed the problem of reliability in a tutorial meant to useful to the theoretician as well as the practitioner.  Although there are a number of functions in psych for estimating reliability of single scales, (e.g. <code><a href="#topic+alpha">alpha</a></code> and <code><a href="#topic+omega">omega</a></code>), for split half reliability <code><a href="#topic+splitHalf">splitHalf</a></code> or for finding test-retest reliability <code><a href="#topic+testRetest">testRetest</a></code> or multilevel reliability <code><a href="#topic+mlr">mlr</a></code>, the <code><a href="#topic+reliability">reliability</a></code> function  combines several of these functions to report these recommended measures for multiple scales. 
</p>
<p>To quote from Revelle and Condon (2019) &ldquo;Reliability is a fundamental problem for measurement in all of science for &lsquo;(a)ll measurement is befuddled by error&rsquo; (p 294 McNemar, 1946). Perhaps because psychological measures are more befuddled than those of the other natural sciences, psychologists have long studied the problem of reliability.   
</p>
<p>&ldquo;Issues of reliability are fundamental to understanding how correlations between observed variables are (attenuated) underestimates of the relationships between the underlying constructs, how observed estimates of a person's score are biased estimates of their latent score, and how to estimate the confidence intervals around any particular measurement. Understanding the many ways to estimate reliability as well as the ways to use these estimates allows one to better assess individuals and to evaluate selection and prediction techniques. This is not just a problem for measurement specialists but for all who want to make theoretical inferences from observed data.
</p>
<p>&ldquo; It is no longer  acceptable to report one  coefficient that is only correct if all items are exactly equally good measures of a construct.  Researchers are encouraged to report at least two coefficients (e.g., omega_h and omega_t) and then discuss why each is appropriate for the inference that is being made.  They are discouraged from reporting just alpha unless they can justify the assumptions implicit in using it (i.e., tau equivalence and unidimensionality).&quot;  Here we make it easy to do so.
</p>
<p>Although the <code><a href="#topic+alpha">alpha</a></code> and <code><a href="#topic+omega">omega</a></code> functions will find reliability estimates for a single scale, and <code><a href="#topic+scoreItems">scoreItems</a></code> and <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> will find alpha for multiple scales, it sometimes is convenient to call <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+splitHalf">splitHalf</a></code> for multiple scales.  <code><a href="#topic+reliability">reliability</a></code> takes a keys list (suitable for <code><a href="#topic+scoreItems">scoreItems</a></code> ) and then finds hierarchical and total omega as well as split half reliabilities for each separate scale. 
</p>
<p><code><a href="#topic+plot.reliability">plot.reliability</a></code> takes the output of  <code><a href="#topic+reliability">reliability</a></code> and displays it as a dot chart showing the values of both omegas as well as alpha and the distributions of split half reliabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliability(keys=NULL, items, nfactors = 2,n.obs=NA, 
   split = TRUE, 
   raw=TRUE, 
    plot=FALSE,
    hist=FALSE,
    n.sample=10000,
    brute=FALSE, 
    check.keys=TRUE, 
    covar=FALSE, 
    fm="minres")
## S3 method for class 'reliability'
plot(x,omega=TRUE,alpha=TRUE,split=TRUE,uni=TRUE,add=FALSE,
xlim=NULL, main=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reliability_+3A_keys">keys</code></td>
<td>
<p>A list of items to be scored (may be taken from a keys.list for <code><a href="#topic+scoreItems">scoreItems</a></code>. This list may contain one or more keys. ) If keys are not specified, then all the items are used.
</p>
</td></tr>
<tr><td><code id="reliability_+3A_items">items</code></td>
<td>
<p>The matrix or data.frame of items to be scored.  Can be substantially greater than the items included in keys. For just those items in each key are scored.
</p>
</td></tr>
<tr><td><code id="reliability_+3A_nfactors">nfactors</code></td>
<td>
<p>Omega is not well defined for two factors, but for small sets of items, two is the better choice. For larger number of items per scale, 3 is probably preferable.
</p>
</td></tr>
<tr><td><code id="reliability_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations, if given a correlation matrix.  Needed for CFI</p>
</td></tr>
<tr><td><code id="reliability_+3A_split">split</code></td>
<td>
<p>By default, find splitHalf reliabilities as well as the omega statistics. When plotting, split implies that raw was called in reliability.</p>
</td></tr>
<tr><td><code id="reliability_+3A_plot">plot</code></td>
<td>
<p>By default, suppress the omega plots for each scale.</p>
</td></tr>
<tr><td><code id="reliability_+3A_raw">raw</code></td>
<td>
<p>If TRUE, return a list of all the possible splits (up to n.samples).  Useful for graphic display. </p>
</td></tr>
<tr><td><code id="reliability_+3A_hist">hist</code></td>
<td>
<p>If TRUE then  split and raw are forced to TRUE and the histograms of the split half values are drawn.  (Otherwise, just return the values for later plotting)</p>
</td></tr>
<tr><td><code id="reliability_+3A_n.sample">n.sample</code></td>
<td>
<p>Normally defaults to 10,000.  This means that for up to 16 item tests, all possible splits are found.  choose(n,n/2)/2 explodes above that, eg. for all splits of the epi E scale requires 1,352,078 splits or 23.4 seconds on a MacBook Pro with a 2.4GHZ 8 core Intel Core I9.  Can be done, but do you want to do so?  </p>
</td></tr>
<tr><td><code id="reliability_+3A_brute">brute</code></td>
<td>
<p>Do all possible splits rather than sampling. (see <code><a href="#topic+splitHalf">splitHalf</a></code> for details)</p>
</td></tr>
<tr><td><code id="reliability_+3A_check.keys">check.keys</code></td>
<td>
<p>By default, check that the keys are signe in the direction of the loadings on the first PCA.  </p>
</td></tr>
<tr><td><code id="reliability_+3A_covar">covar</code></td>
<td>
<p>Should we use covariances rather than correlations?</p>
</td></tr>
<tr><td><code id="reliability_+3A_fm">fm</code></td>
<td>
<p>The factor extraction method to use.  By default this is minres but could be &quot;minrank&quot; or &quot;mle&quot;.</p>
</td></tr>
<tr><td><code id="reliability_+3A_x">x</code></td>
<td>
<p>The object returned from reliability</p>
</td></tr>
<tr><td><code id="reliability_+3A_omega">omega</code></td>
<td>
<p>Add in the values of omega_h and omega_t</p>
</td></tr>
<tr><td><code id="reliability_+3A_uni">uni</code></td>
<td>
<p>Show the unidimensionality value from <code><a href="#topic+unidim">unidim</a></code>.  </p>
</td></tr>
<tr><td><code id="reliability_+3A_alpha">alpha</code></td>
<td>
<p>Add the value of alpha</p>
</td></tr>
<tr><td><code id="reliability_+3A_add">add</code></td>
<td>
<p>Allows us to merge this figure with other ones</p>
</td></tr>
<tr><td><code id="reliability_+3A_main">main</code></td>
<td>
<p>Defaults to &quot;Split half distributions + omega, alpha&quot;</p>
</td></tr>
<tr><td><code id="reliability_+3A_xlim">xlim</code></td>
<td>
<p>The xlim of the plot</p>
</td></tr>
<tr><td><code id="reliability_+3A_...">...</code></td>
<td>
<p>Other graphical parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+reliability">reliability</a></code> is basically just a wrapper for <code><a href="#topic+omegah">omegah</a></code>, <code><a href="#topic+unidim">unidim</a></code> and <code><a href="#topic+splitHalf">splitHalf</a></code>. Revelle and Condon (2019) recommended reporting at least three reliability statistics for any scale, here we make it easy to do. 
</p>
<p>If the hist option is set to true, histgrams and density plots of the split half values for each test are also shown. The output from <code><a href="#topic+reliability">reliability</a></code> can be passed to <code><a href="#topic+error.dots">error.dots</a></code> to show the reliability statistics for multiple scales graphically.  It is however more useful to just call the <code><a href="#topic+plot.reliability">plot.reliability</a></code> function to show the basic information.
</p>
<p>For detailed analysis of any one scale, it is recommended to do a complete <code><a href="#topic+omega">omega</a></code> analysis, perhaps combined with a <code><a href="#topic+splitHalf">splitHalf</a></code> analysis. The <code><a href="#topic+reliability">reliability</a></code> function is just meant for the case where the user has multiple scales (perhaps scored using <code><a href="#topic+scoreItems">scoreItems</a></code>) and then wants to get more complete reliability information for all of the scales.  
</p>
<p>Following a suggestion, the ability to not bother with keys and just do omega and split half and draw the results has been added.  Either specify that keys=NULL, or just specify the items to use. (See the first example.)
</p>
<p><code><a href="#topic+plot.reliability">plot.reliability</a></code> provides a dot chart summary of the distributions of the split half values, as well as the estimates of omega and alpha and unidimensionality.  It can also be called by just issuing a plot command.
</p>


<h3>Value</h3>

<table>
<tr><td><code>omega_h</code></td>
<td>
<p>Omega_h is the (model based) hierarchical estimate of the general factor saturation of a scale.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The conventional alpha statistic (which is not model based)</p>
</td></tr>
<tr><td><code>omega.tot</code></td>
<td>
<p>A model based estimate of the total reliability of a scale</p>
</td></tr>
<tr><td><code>Uni</code></td>
<td>
<p>An experimental estimate of unidimensionality (from unidim)</p>
</td></tr>
<tr><td><code>r.fit</code></td>
<td>
<p>How well does the average r of the correlations reproduce the matrix?</p>
</td></tr>
<tr><td><code>f.fit</code></td>
<td>
<p>How well does a single factor reproduce the correlation matrix</p>
</td></tr>
<tr><td><code>max.split</code></td>
<td>
<p>The greatest split half reliability of the scale.  Found by finding all possible splits (if this is &lt; 10,000) or sampled from 10,000 possible splits.</p>
</td></tr>
<tr><td><code>min.split</code></td>
<td>
<p>The lowest split half reliability of the scale.  An estimate of beta (see <code><a href="#topic+iclust">iclust</a></code>). </p>
</td></tr>
<tr><td><code>mean.r</code></td>
<td>
<p>The average correlation of the items in the scale</p>
</td></tr>
<tr><td><code>med.r</code></td>
<td>
<p>The median correlation of the items.  If this differs from the mean, that is a sign of poor scale.</p>
</td></tr>
<tr><td><code>splits</code></td>
<td>
<p>A list of the split half values for all possible splits.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For much more information on reliability, see the help pages for <code><a href="#topic+omega">omega</a></code>  as well as the Revelle and Condon (2019) tutorial or the Revelle (in prep) chapter on reliability. 
</p>
<p>For some rare cases in some simulations, check.keys=FALSE gets a better minimum split half.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. https://doi.org/10.1037/pas0000754.  <a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Condon, D.M (2023) Using undim rather than omega in estimating undimensionality.  Working draft available at  <a href="https://personality-project.org/revelle/publications/rc.23.pdf">https://personality-project.org/revelle/publications/rc.23.pdf</a> 
</p>


<h3>See Also</h3>

<p>See Also <code><a href="#topic+omega">omega</a></code> to find more complete output for the various omega analyses,<code><a href="#topic+splitHalf">splitHalf</a></code> to show more detail on split half estimates, <code><a href="#topic+scoreItems">scoreItems</a></code> to find scores on multiple scales using unit weights, <code><a href="#topic+testRetest">testRetest</a></code> to find test retest reliabilities, <code><a href="#topic+mlr">mlr</a></code> to find multilevel reliabilities.  
</p>
<p><code><a href="#topic+predicted.validity">predicted.validity</a></code> will call <code><a href="#topic+reliability">reliability</a></code> and <code><a href="#topic+item.validity">item.validity</a></code> to use the average r information to find the asymptotic validity of a set of scales for a set of criteria.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>reliability(psychTools::ability) #an example of finding reliability for all items
rel &lt;- reliability(psychTools::ability.keys,psychTools::ability)  #use keys to select scales
R &lt;- cor(psychTools::ability,use="pairwise") #find the correlations to test 
rel.R &lt;- reliability(psychTools::ability.keys,R) #this should be the same as rel
plot(rel.R) #versus all and subsets
all.equal(rel$result.df,rel.R$result.df ) #should be TRUE
reliability(psychTools::bfi.keys,psychTools::bfi)  #reliability when items are keyed negative

## Not run:   
#this takes a few seconds but shows nice graphic displays

spi.rel &lt;- reliability(psychTools::spi.keys,psychTools::spi,hist=TRUE) #graph them
spi.rel #show them

#plot them using plot.reliability
plot(spi.rel)  #draw the density distrbutions

plot(spi.rel,split=FALSE) #don't draw the split half density distribution
plot(spi.rel,omega=FALSE) # don't add omega values to the diagram
#or do this without the densities

#plot the first three values in a dot chart
error.dots(spi.rel$result.df[,1],sort=FALSE, xlim=c(.3,1),head=16,tail=16,
          main = expression(paste(omega[h], ~~~~ alpha,~~~~ omega[t])))
  #plot the omega_h values
error.dots(spi.rel$result.df[,2],sort=FALSE,pch=2,xlim=c(.3,1),head=16,tail=16,
       main="",labels="",add=TRUE)#add the alpha values
error.dots(spi.rel$result.df[,3],sort=FALSE, xlim=c(.3,1),head=16,tail=16, 
       pch=3,labels="", main="",add=TRUE) #and the omega_t values
       
#or, show the smallest and greatest split half, as well as alpha
error.dots(spi.rel$result.df[,4],sort=FALSE, xlim=c(.3,1),head=16,tail=16,
          main = expression(paste(beta, ~~~~ alpha,~~~~  glb)))
error.dots(spi.rel$result.df[,5],sort=FALSE,pch=5,xlim=c(.3,1),head=16,tail=16,
       main="",labels="",add=TRUE)#add the GLB values
error.dots(spi.rel$result.df[,2],sort=FALSE,pch=2,xlim=c(.3,1),head=16,tail=16,
       main="",labels="",add=TRUE)#add the alpha values
       


## End(Not run) 
</code></pre>

<hr>
<h2 id='rescale'>Function to convert scores to &ldquo;conventional
&quot; metrics</h2><span id='topic+rescale'></span>

<h3>Description</h3>

<p>Psychologists frequently report data in terms of transformed scales such as &ldquo;IQ&quot; (mean=100, sd=15, &ldquo;SAT/GRE&quot; (mean=500, sd=100), &ldquo;ACT&quot; (mean=18, sd=6), &ldquo;T-scores&quot; (mean=50, sd=10), or &ldquo;Stanines&quot; (mean=5, sd=2). The <code><a href="#topic+rescale">rescale</a></code> function converts the data to standard scores and then rescales to the specified mean(s) and standard deviation(s). </p>


<h3>Usage</h3>

<pre><code class='language-R'>rescale(x, mean = 100, sd = 15,df=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rescale_+3A_x">x</code></td>
<td>
<p>A matrix or data frame </p>
</td></tr>
<tr><td><code id="rescale_+3A_mean">mean</code></td>
<td>
<p>Desired mean of the rescaled scores- may be a vector</p>
</td></tr>
<tr><td><code id="rescale_+3A_sd">sd</code></td>
<td>
<p>Desired standard deviation of the rescaled scores</p>
</td></tr>
<tr><td><code id="rescale_+3A_df">df</code></td>
<td>
<p>if TRUE, returns a data frame, otherwise a matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame (default) or matrix of rescaled scores.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>See Also</h3>

<p>  See Also  <code><a href="base.html#topic+scale">scale</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>T &lt;- rescale(attitude,50,10) #all put on same scale
describe(T)
T1 &lt;- rescale(attitude,seq(0,300,50),seq(10,70,10)) #different means and sigmas
describe(T1)
</code></pre>

<hr>
<h2 id='residuals.psych'>Extract residuals from various psych objects</h2><span id='topic+residuals.psych'></span><span id='topic+resid.psych'></span>

<h3>Description</h3>

<p>Residuals in the various psych functions are extracted and then may be &quot;pretty&quot; printed.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psych'
residuals(object,diag=TRUE,...)
## S3 method for class 'psych'
resid(object,diag=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.psych_+3A_object">object</code></td>
<td>
<p>The object returned by a psych function.</p>
</td></tr>
<tr><td><code id="residuals.psych_+3A_diag">diag</code></td>
<td>
<p>if FALSE, then convert the diagonal of the residuals to NA</p>
</td></tr>
<tr><td><code id="residuals.psych_+3A_...">...</code></td>
<td>
<p>Other parameters to be passed to residual (ignored but required by the generic function)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently implemented for <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+principal">principal</a></code>,   <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+irt.fa">irt.fa</a></code>, and <code><a href="#topic+fa.extension">fa.extension</a></code>. 
</p>


<h3>Value</h3>

<p>residuals:  a matrix of residual estimates</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f3 &lt;- fa(Thurstone,3)
residuals(f3)
sum(residuals(f3)^2) #include diagonal
sum(residuals(f3,diag=FALSE)^2,na.rm=TRUE) #drop diagonal
</code></pre>

<hr>
<h2 id='reverse.code'>Reverse the coding of selected items prior to scale analysis</h2><span id='topic+reverse.code'></span>

<h3>Description</h3>

<p>Some IRT functions require all items to be coded in the same direction.  Some data sets have items that need to be reverse coded (e.g., 6 -&gt; 1, 1 -&gt; 6).  reverse.code will flip items based upon a keys vector of 1s and -1s.  Reversed items are subtracted from the item max + item min.  These may be specified or may be calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reverse.code(keys, items, mini = NULL, maxi = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reverse.code_+3A_keys">keys</code></td>
<td>
<p>A vector of 1s and -1s.  -1 implies reverse the item</p>
</td></tr>
<tr><td><code id="reverse.code_+3A_items">items</code></td>
<td>
<p>A data set of items</p>
</td></tr>
<tr><td><code id="reverse.code_+3A_mini">mini</code></td>
<td>
<p>if NULL, the empirical minimum for each item.  Otherwise, a vector of minima</p>
</td></tr>
<tr><td><code id="reverse.code_+3A_maxi">maxi</code></td>
<td>
<p>f NULL, the empirical maximum for each item.  Otherwise, a vector of maxima</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not a very complicated function, but useful in the case that items need to be reversed prior to using IRT functions from the ltm or eRM packages. Most psych functions do not require reversing prior to analysis, but will do so within the function.
</p>


<h3>Value</h3>

<p>The corrected items.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>original &lt;- matrix(sample(6,50,replace=TRUE),10,5)
keys &lt;- c(1,1,-1,-1,1)  #reverse the 3rd and 4th items
new &lt;- reverse.code(keys,original,mini=rep(1,5),maxi=rep(6,5))
original[1:3,]
new[1:3,]
</code></pre>

<hr>
<h2 id='RMSEA'>Root Mean Squared Error of Approximation from chisq, df, and n</h2><span id='topic+RMSEA'></span>

<h3>Description</h3>

<p>Find the RMSEA from model chi square, degrees of freedom and number of observations.
Show confidence intervals. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSEA(chisq, dof, n.obs, alpha = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMSEA_+3A_chisq">chisq</code></td>
<td>
<p>The Chi square statistic from an analysis</p>
</td></tr>
<tr><td><code id="RMSEA_+3A_dof">dof</code></td>
<td>
<p>Degrees of freedom of the model</p>
</td></tr>
<tr><td><code id="RMSEA_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code id="RMSEA_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for confidence intervals</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RMSEA is just a chisquare adjusted by its degrees of freedom and the sample size.
<code class="reqn">\sqrt{chisq/(dof * (nobs)) -1/(n.obs -1)}</code>.  It is given in most of the appropriate functions (e.g.,<code><a href="#topic+fa">fa</a></code>) and given here for completeness.</p>


<h3>Value</h3>

<table>
<tr><td><code>RMSEA</code></td>
<td>
<p>the estimated value</p>
</td></tr>
<tr><td><code>RMSEA-L</code></td>
<td>
<p>the lower bound</p>
</td></tr>
<tr><td><code>RMSEA-U</code></td>
<td>
<p>the upper bound</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p> Steiger, J. H.,and  Lind, J. C. (1980). Statistically based tests for the number of common factors. Paper presented at the Annual Meeting of the Psychometric Society, Iowa City, IA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+omega">omega</a></code> , <code><a href="#topic+esem">esem</a></code>
</p>

<hr>
<h2 id='sat.act'>3 Measures of ability: SATV, SATQ, ACT</h2><span id='topic+sat.act'></span>

<h3>Description</h3>

<p>Self reported scores on the SAT Verbal, SAT Quantitative and ACT   were collected as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  Age, gender, and education are also reported. The data from 700 subjects are included here as a demonstration set for correlation and analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sat.act)</code></pre>


<h3>Format</h3>

<p>A data frame with 700 observations on the following 6 variables.
</p>

<dl>
<dt><code>gender</code></dt><dd><p>males = 1, females = 2</p>
</dd>
<dt><code>education</code></dt><dd><p>self reported education 1 = high school ... 5 = graduate work</p>
</dd>
<dt><code>age</code></dt><dd><p>age</p>
</dd>
<dt><code>ACT</code></dt><dd><p>ACT composite scores may range from 1 - 36.  National norms have a mean of 20. </p>
</dd>
<dt><code>SATV</code></dt><dd><p>SAT Verbal scores may range from 200 - 800. </p>
</dd>
<dt><code>SATQ</code></dt><dd><p>SAT Quantitative scores may range from 200 - 800</p>
</dd>
</dl>



<h3>Details</h3>

<p>hese items were collected as part of the  SAPA  project  (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>)to develop online measures of ability (Revelle, Wilt and Rosenthal, 2009).  The score means are higher than national norms suggesting both self selection for people taking on line personality and ability tests and a self reporting bias in scores.
</p>
<p>See also the iq.items data set.  
</p>


<h3>Source</h3>

<p><a href="https://personality-project.org/">https://personality-project.org/</a> 
</p>


<h3>References</h3>

<p>Revelle, William, Wilt, Joshua,  and Rosenthal, Allen (2009)  Personality and Cognition: The Personality-Cognition Link. In Gruszka, Alexandra  and Matthews, Gerald   and Szymura, Blazej (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control,
Springer. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sat.act)
describe(sat.act)
pairs.panels(sat.act)
</code></pre>

<hr>
<h2 id='scaling.fits'> Test the adequacy of simple choice, logistic, or Thurstonian scaling. </h2><span id='topic+scaling.fits'></span>

<h3>Description</h3>

<p>Given a matrix of choices and a vector of scale values, how well do the scale values capture the choices?  That is, what is size of the squared residuals given the model versus the size of the squared choice values?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaling.fits(model, data, test = "logit", digits = 2, rowwise = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scaling.fits_+3A_model">model</code></td>
<td>
<p>A vector of scale values </p>
</td></tr>
<tr><td><code id="scaling.fits_+3A_data">data</code></td>
<td>
<p> A matrix or dataframe of choice frequencies </p>
</td></tr>
<tr><td><code id="scaling.fits_+3A_test">test</code></td>
<td>
<p> &quot;choice&quot;, &quot;logistic&quot;, &quot;normal&quot; </p>
</td></tr>
<tr><td><code id="scaling.fits_+3A_digits">digits</code></td>
<td>
<p> Precision of answer </p>
</td></tr>
<tr><td><code id="scaling.fits_+3A_rowwise">rowwise</code></td>
<td>
<p>Are the choices ordered by column over row (TRUE) or row over column False)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>How well does a model fit the data is the classic problem of all of statistics.  One fit statistic for scaling is the just the size of the residual matrix compared to the original estimates. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>GF</code></td>
<td>
<p>Goodness of fit of the model</p>
</td></tr>
<tr><td><code>original</code></td>
<td>
<p>Sum of squares for original data</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>Sum of squares for residuals given the data and the model</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>Residual matrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Mainly for demonstration purposes for a course on  psychometrics
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Revelle, W. (in preparation) Introduction to psychometric theory with applications in R, Springer.  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+thurstone">thurstone</a></code>, <code><a href="psychTools.html#topic+vegetables">vegetables</a></code></p>

<hr>
<h2 id='scatterHist'>Draw a scatter plot with associated X and Y histograms, densities and correlation</h2><span id='topic+scatter.hist'></span><span id='topic+scatterHist'></span>

<h3>Description</h3>

<p>Draw a X Y scatter plot with associated X and Y histograms with estimated densities.  Will also draw density plots by groups, as well as distribution ellipses by group. Partly a demonstration of the use of layout. Also includes lowess smooth or linear model slope, as well as correlation and Mahalanobis distances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scatterHist(x,y=NULL,smooth=TRUE,ab=FALSE, correl=TRUE,data=NULL, density=TRUE,means=TRUE, 
   ellipse=TRUE,digits=2,method="pearson",cex.cor=1,cex.point=1,
   title="Scatter plot + density",
   xlab=NULL,ylab=NULL,smoother=FALSE,nrpoints=0,xlab.hist=NULL,ylab.hist=NULL,grid=FALSE,
   xlim=NULL,ylim=NULL,x.breaks=11,y.breaks=11,
   x.space=0,y.space=0,freq=TRUE,x.axes=TRUE,y.axes=TRUE,size=c(1,2),
   col=c("blue","red","black"),legend=NULL,alpha=.5,pch=21, show.d=TRUE,
   x.arrow=NULL,y.arrow=NULL,d.arrow=FALSE,cex.arrow=1,...)
 
scatter.hist(x,y=NULL,smooth=TRUE,ab=FALSE, correl=TRUE,data=NULL,density=TRUE,
  means=TRUE, ellipse=TRUE,digits=2,method="pearson",cex.cor=1,cex.point=1,
  title="Scatter plot + density",
  xlab=NULL,ylab=NULL,smoother=FALSE,nrpoints=0,xlab.hist=NULL,ylab.hist=NULL,grid=FALSE,
  xlim=NULL,ylim=NULL,x.breaks=11,y.breaks=11,
  x.space=0,y.space=0,freq=TRUE,x.axes=TRUE,y.axes=TRUE,size=c(1,2),
  col=c("blue","red","black"),legend=NULL,alpha=.5,pch=21, show.d=TRUE,
   x.arrow=NULL,y.arrow=NULL,d.arrow=FALSE,cex.arrow=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scatterHist_+3A_x">x</code></td>
<td>
<p>The X vector, or the first column of a  data.frame or matrix. Can be specified using formula input.  </p>
</td></tr>
<tr><td><code id="scatterHist_+3A_y">y</code></td>
<td>
<p>The Y vector, of if X is a data.frame or matrix, the second column of X</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_smooth">smooth</code></td>
<td>
<p>if TRUE, then add a loess smooth to the plot</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_ab">ab</code></td>
<td>
<p>if TRUE, then show the best fitting linear fit</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_correl">correl</code></td>
<td>
<p>TRUE: Show the correlation</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_data">data</code></td>
<td>
<p>if using formula input, the data must be specified</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_density">density</code></td>
<td>
<p>TRUE: Show the estimated densities</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_means">means</code></td>
<td>
<p>TRUE: If TRUE, show the means for the distributions. </p>
</td></tr>
<tr><td><code id="scatterHist_+3A_ellipse">ellipse</code></td>
<td>
<p>TRUE: draw 1 and 2 sigma ellipses and smooth</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_digits">digits</code></td>
<td>
<p>How many digits to use if showing the correlation</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_method">method</code></td>
<td>
<p>Which method to use for correlation (&quot;pearson&quot;,&quot;spearman&quot;,&quot;kendall&quot;) 
defaults to &quot;pearson&quot;</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_smoother">smoother</code></td>
<td>
<p>if TRUE, use smoothScatter instead of plot. Nice for large N.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_nrpoints">nrpoints</code></td>
<td>
<p>If using smoothScatter, show nrpoints as dots. Defaults to 0</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_grid">grid</code></td>
<td>
<p>If TRUE, show a grid for the scatter plot.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_cex.cor">cex.cor</code></td>
<td>
<p>Adjustment for the size of the correlation</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_cex.point">cex.point</code></td>
<td>
<p>Adjustment for the size of the data points</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_xlab">xlab</code></td>
<td>
<p>Label for the x axis</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_ylab">ylab</code></td>
<td>
<p>Label for the y axis</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_xlim">xlim</code></td>
<td>
<p>Allow specification for limits of x axis, although this seems to just work for the scatter plots.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_ylim">ylim</code></td>
<td>
<p>Allow specification for limits of y axis</p>
</td></tr> 
<tr><td><code id="scatterHist_+3A_x.breaks">x.breaks</code></td>
<td>
<p>Number of breaks to suggest to the x axis histogram.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_y.breaks">y.breaks</code></td>
<td>
<p>Number of breaks to suggest to the y axis histogram.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_x.space">x.space</code></td>
<td>
<p>space between bars</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_y.space">y.space</code></td>
<td>
<p>Space between y bars</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_freq">freq</code></td>
<td>
<p>Show frequency counts, otherwise show density counts</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_x.axes">x.axes</code></td>
<td>
<p>Show the x axis for the x histogram</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_y.axes">y.axes</code></td>
<td>
<p>Show the y axis for the y histogram</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_size">size</code></td>
<td>
<p>The sizes of the ellipses (in sd units).  Defaults to 1,2</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_col">col</code></td>
<td>
<p>Colors to use when showing groups</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_alpha">alpha</code></td>
<td>
<p>Amount of transparency in the density plots</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_legend">legend</code></td>
<td>
<p>Where to put a legend  c(&quot;topleft&quot;,&quot;topright&quot;,&quot;top&quot;,&quot;left&quot;,&quot;right&quot;)</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_pch">pch</code></td>
<td>
<p>Base plot character (each group is one more)</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_xlab.hist">xlab.hist</code></td>
<td>
<p>Not currently available</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_ylab.hist">ylab.hist</code></td>
<td>
<p>Label for y axis histogram.  Not currently available.</p>
</td></tr> 
<tr><td><code id="scatterHist_+3A_title">title</code></td>
<td>
<p>An optional title</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_show.d">show.d</code></td>
<td>
<p>If TRUE, show the distances between the groups</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_d.arrow">d.arrow</code></td>
<td>
<p>If TRUE, draw an arrow between the two centroids</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_x.arrow">x.arrow</code></td>
<td>
<p>optional lable for the arrow connecting the two groups for the x axis</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_y.arrow">y.arrow</code></td>
<td>
<p>optional lable for the arrow connecting the two groups for the y axis</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_cex.arrow">cex.arrow</code></td>
<td>
<p>cex control for the label size of the arrows.</p>
</td></tr>
<tr><td><code id="scatterHist_+3A_...">...</code></td>
<td>
<p>Other parameters for graphics</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just a straightforward application of layout and barplot, with some tricks taken from <code><a href="#topic+pairs.panels">pairs.panels</a></code>.  The various options allow for correlation ellipses (1 and 2 sigma from the mean), lowess smooths, linear fits, density curves on the histograms, and the value of the correlation.  ellipse = TRUE implies smooth = TRUE.  The grid option provides a background grid to the scatterplot.
</p>
<p>If using grouping variables, will draw ellipses (defaults to 1 sd) around each centroid. This is useful when demonstrating Mahalanobis distances. 
</p>
<p>Formula input allows specification of grouping variables as well.  )
</p>
<p>For plotting data for two groups, Mahalobnis differences between the groups may be shown by drawing an arrow between the two centroids.  This is a bit messy and it is useful to use pch=&quot;.&quot; in this case.
</p>


<h3>Note</h3>

<p>Originally adapted from Addicted to R example 78.  Modified following some nice suggestions from Jared Smith. Substantial revisions in 2021 to allow for a clearer demonstration of group differences.    
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+pairs.panels">pairs.panels</a></code> for multiple plots, <code><a href="#topic+multi.hist">multi.hist</a></code> for multiple histograms and  <code><a href="#topic+histBy">histBy</a></code> for single variables with multiple groups. Perhaps the best example is found in the psychTools::GERAS data set. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sat.act)
with(sat.act,scatterHist(SATV,SATQ))
scatterHist(SATV ~ SATQ,data=sat.act)  #formula input

#or for something a bit more splashy
scatter.hist(sat.act[5:6],pch=(19+sat.act$gender),col=c("blue","red")[sat.act$gender],grid=TRUE)
#better yet
scatterHist(SATV ~ SATQ + gender,data=sat.act) #formula input with a grouping variable
</code></pre>

<hr>
<h2 id='schmid'>Apply the Schmid Leiman transformation to a correlation matrix</h2><span id='topic+schmid'></span>

<h3>Description</h3>

<p>One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. Here is the code for Schmid Leiman.  The S-L transform takes a factor or PC solution, transforms it to an oblique solution, factors the oblique solution to find a higher order (g ) factor, and then residualizes g out of the the group factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>schmid(model, nfactors = 3, fm = "minres",digits=2,rotate="oblimin",
            n.obs=NA,option="equal",Phi=NULL,covar=FALSE,two.ok=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="schmid_+3A_model">model</code></td>
<td>
<p> A correlation matrix </p>
</td></tr>
<tr><td><code id="schmid_+3A_nfactors">nfactors</code></td>
<td>
<p> Number of factors to extract </p>
</td></tr>
<tr><td><code id="schmid_+3A_fm">fm</code></td>
<td>
<p>the default is to do minres. fm=&quot;pa&quot; for principal axes, fm=&quot;pc&quot; for principal components, fm = &quot;minres&quot; for minimum residual (OLS), pc=&quot;ml&quot; for maximum likelihood </p>
</td></tr>
<tr><td><code id="schmid_+3A_digits">digits</code></td>
<td>
<p>if digits not equal NULL, rounds to digits</p>
</td></tr>
<tr><td><code id="schmid_+3A_rotate">rotate</code></td>
<td>
<p>The default, oblimin, produces somewhat more correlated factors than the alternative, simplimax. Other options include Promax (not Kaiser normalized) or promax (Promax with Kaiser normalization).  See <code><a href="#topic+fa">fa</a></code> for possible oblique rotations.</p>
</td></tr>
<tr><td><code id="schmid_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations, used to find fit statistics if specified.  Will be calculated if input is raw data</p>
</td></tr>
<tr><td><code id="schmid_+3A_option">option</code></td>
<td>
<p>When asking for just two group factors, option can be for &quot;equal&quot;, &quot;first&quot; or &quot;second&quot;</p>
</td></tr>
<tr><td><code id="schmid_+3A_phi">Phi</code></td>
<td>
<p>If Phi is specified, then the analysis is done on a pattern matrix with the associated factor intercorrelation (Phi) matrix. This allows for reanalysess of published results</p>
</td></tr>
<tr><td><code id="schmid_+3A_covar">covar</code></td>
<td>
<p>Defaults to FALSE and finds correlations.  If set to TRUE, then do the calculations on the unstandardized variables.</p>
</td></tr>
<tr><td><code id="schmid_+3A_two.ok">two.ok</code></td>
<td>
<p>If TRUE, do not give a warning about three factors being required.</p>
</td></tr>
<tr><td><code id="schmid_+3A_...">...</code></td>
<td>
<p>Allows additional parameters to be passed to the factoring routines</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Schmid Leiman orthogonalizations are typical in the ability domain, but are not seen as often in the non-cognitive personality domain.  S-L is one way of finding the loadings of items on the general factor for estimating omega. 
</p>
<p>A typical example would be in the study of anxiety and depression.  A general neuroticism factor (g) accounts for much of the variance, but smaller group factors of tense anxiety, panic disorder, depression, etc. also need to be considerd.
</p>
<p>An alternative model is to consider hierarchical cluster analysis techniques such as <code><a href="#topic+ICLUST">ICLUST</a></code>.
</p>
<p>Requires the GPArotation package.
</p>
<p>Although 3 factors are the minimum number necessary to define the solution uniquely, it is occasionally useful to allow for a two factor solution.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be &quot;equal&quot; which will be the sqrt(oblique correlations between the factors) or to &quot;first&quot; or &quot;second&quot; in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. 
</p>
<p>A diagnostic tool for testing the appropriateness of a hierarchical model is p2 which is the percent of the common variance for each variable that is general factor variance.  In general, p2 should not have much variance. 
</p>
<p>An alternative approach is to use the <code><a href="#topic+directSl">directSl</a></code> function to do a direct Schmid Leiman transformation (code adapted from Niels Waller).
</p>


<h3>Value</h3>

<table>
<tr><td><code>sl</code></td>
<td>
<p>loadings on g + nfactors group factors, communalities, uniqueness, percent of g2 of h2</p>
</td></tr>
<tr><td><code>orthog</code></td>
<td>
<p>original orthogonal factor loadings</p>
</td></tr>
<tr><td><code>oblique</code></td>
<td>
<p>oblique factor loadings</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>correlations among the transformed factors</p>
</td></tr>
<tr><td><code>gload</code></td>
<td>
<p>loadings of the lower order factors on g</p>
</td></tr>
</table>
<p>...
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p><a href="https://personality-project.org/r/r.omega.html">https://personality-project.org/r/r.omega.html</a> gives an example taken from Jensen and Weng, 1994 of a S-L transformation.</p>


<h3>See Also</h3>

  <p><code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+directSl">directSl</a></code>, <code><a href="#topic+omega.graph">omega.graph</a></code>, <code><a href="#topic+fa.graph">fa.graph</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>,<code><a href="#topic+VSS">VSS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>jen &lt;- sim.hierarchical()  #create a hierarchical demo
if(!require(GPArotation)) {
   message("I am sorry, you must have GPArotation installed to use schmid.")} else {
   p.jen &lt;- schmid(jen,digits=2)   #use the oblimin rotation
p.jen &lt;- schmid(jen,rotate="promax") #use the promax rotation
}
</code></pre>

<hr>
<h2 id='Schmid'>12 variables created by Schmid and Leiman to show the Schmid-Leiman Transformation</h2><span id='topic+Schmid'></span><span id='topic+schmid.leiman'></span><span id='topic+West'></span><span id='topic+Chen'></span>

<h3>Description</h3>

<p>John Schmid and John M. Leiman (1957) discuss how to transform a hierarchical factor structure to a bifactor structure. Schmid contains the example 12 x 12 correlation matrix. schmid.leiman is a 12 x 12 correlation matrix with communalities on the diagonal. This can be used to show the effect of correcting for attenuation. Two additional data sets are taken from Chen et al. (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Schmid)</code></pre>


<h3>Details</h3>

<p> Two artificial correlation matrices from Schmid and Leiman (1957). One real and one artificial covariance matrices from Chen et al. (2006). 
</p>

<ul>
<li><p> Schmid: a 12 x 12 artificial correlation matrix created to show the Schmid-Leiman transformation. 
</p>
</li>
<li><p> schmid.leiman: A 12 x 12 matrix with communalities on the diagonal.  Treating this as a covariance matrix shows the 6 x 6 factor solution
</p>
</li>
<li><p> Chen: An 18 x 18 covariance matrix of health related quality of life items from Chen et al. (2006). Number of observations = 403.  The first item is a measure of the quality of life.  The remaining 17 items form four subfactors: The items are (a) Cognition subscale: &ldquo;Have difficulty reasoning
and solving problems?&quot;  &ldquo;React slowly to things that were said or done?&quot;; &ldquo;Become confused and start several actions at a time?&quot;  &ldquo;Forget where you
put things or appointments?&quot;; &ldquo;Have difficulty concentrating?&quot;  (b) Vitality
subscale: &ldquo;Feel tired?&quot;  &ldquo;Have enough energy to do the things you want?&quot; (R) 
&ldquo;Feel worn out?&quot; ; &ldquo;Feel full of pep?&quot; (R). (c) Mental health subscale: &ldquo;Feel
calm and peaceful?&quot;(R)  &ldquo;Feel downhearted and blue?&quot;; &ldquo;Feel very
happy&quot;(R) ; &ldquo;Feel very nervous?&quot; ; &ldquo;Feel so down in the dumps nothing could
cheer you up?  (d) Disease worry subscale: &ldquo;Were you afraid because of your health?&quot;; &ldquo;Were you frustrated about your health?&quot;; &ldquo;Was your health a worry in your life?&quot; .
</p>
</li>
<li><p> West: A 16 x 16 artificial covariance matrix from Chen et al. (2006).
</p>
</li></ul>



<h3>Source</h3>

<p>John Schmid Jr. and John. M. Leiman (1957), The development of hierarchical factor solutions.Psychometrika, 22, 83-90.
</p>
<p>F.F. Chen, S.G. West, and K.H. Sousa.(2006) A comparison of bifactor and second-order models of quality of life. Multivariate Behavioral Research, 41(2):189-225, 2006.
</p>


<h3>References</h3>

<p>Y.-F. Yung, D.Thissen, and L.D. McLeod. (1999) On the relationship between the higher-order factor model and the hierarchical factor model. Psychometrika, 64(2):113-128, 1999.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Schmid)
cor.plot(Schmid,TRUE)
print(fa(Schmid,6,rotate="oblimin"),cut=0)  #shows an oblique solution
round(cov2cor(schmid.leiman),2)
cor.plot(cov2cor(West),TRUE)
</code></pre>

<hr>
<h2 id='score.alpha'> Score scales and find Cronbach's alpha as well as associated statistics</h2><span id='topic+score.alpha'></span>

<h3>Description</h3>

<p>Given a matrix or data.frame of k keys for m items (-1, 0, 1), and a matrix or data.frame of items scores for m items and n people, find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, the average r, the scale intercorrelations, and the item by scale correlations.  (Superseded by  <code><a href="#topic+score.items">score.items</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score.alpha(keys, items, labels = NULL, totals=TRUE,digits = 2) #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score.alpha_+3A_keys">keys</code></td>
<td>
<p> A matrix or dataframe of -1, 0, or 1 weights for each item on each scale </p>
</td></tr>
<tr><td><code id="score.alpha_+3A_items">items</code></td>
<td>
<p>Data frame or matrix of raw item scores </p>
</td></tr>
<tr><td><code id="score.alpha_+3A_labels">labels</code></td>
<td>
<p>column names for the resulting scales</p>
</td></tr>
<tr><td><code id="score.alpha_+3A_totals">totals</code></td>
<td>
<p>Find sum scores (default) or average score</p>
</td></tr>
<tr><td><code id="score.alpha_+3A_digits">digits</code></td>
<td>
<p>Number of digits for answer (default =2) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function has been replaced with <code><a href="#topic+scoreItems">scoreItems</a></code> (for multiple scales) and <code><a href="#topic+alpha">alpha</a></code> for single scales.
</p>
<p>The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find the sum or the average scale score.  
</p>
<p>Various estimates of scale reliability include &ldquo;Cronbach's alpha&quot;, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  
</p>
<p>Alpha is a poor estimate of the general factor saturation of a test (see Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a useful statistic to report. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>scores</code></td>
<td>
<p>Sum or average scores for each subject on the k scales</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Cronbach's coefficient alpha.  A simple (but non-optimal) measure of the internal consistency of a test. See also beta and omega.</p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>The average correlation within a scale, also known as alpha 1 is a useful index of the internal consistency of a domain.</p>
</td></tr>
<tr><td><code>n.items</code></td>
<td>
<p>Number of items on each scale</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>The intercorrelation of all the scales</p>
</td></tr>
<tr><td><code>item.cor</code></td>
<td>
<p>The correlation of each item with each scale.  Because this is not corrected for item overlap, it will overestimate the amount that an item correlates with the other items in a scale.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>An introduction to psychometric theory with applications in R (in preparation).  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+score.items">score.items</a></code>, <code><a href="#topic+alpha">alpha</a></code>, <code><a href="#topic+correct.cor">correct.cor</a></code>,  <code><a href="#topic+cluster.loadings">cluster.loadings</a></code>, <code><a href="#topic+omega">omega</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- attitude     #from the datasets package
keys &lt;- matrix(c(rep(1,7),rep(1,4),rep(0,7),rep(-1,3)),ncol=3)
labels &lt;- c("first","second","third")
x &lt;- score.alpha(keys,y,labels) #deprecated


</code></pre>

<hr>
<h2 id='score.multiple.choice'>Score multiple choice items and provide basic test statistics </h2><span id='topic+score.multiple.choice'></span>

<h3>Description</h3>

<p>Ability tests are typically multiple choice with one right answer.  score.multiple.choice takes a scoring key and a data matrix (or data.frame) and finds total or average number right for each participant.  Basic test statistics (alpha, average r, item means, item-whole correlations) are also reported. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score.multiple.choice(key, data, score = TRUE, totals = FALSE, ilabels = NULL, 
      missing = TRUE, impute = "median", digits = 2,short=TRUE,skew=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score.multiple.choice_+3A_key">key</code></td>
<td>
<p> A vector of the correct item alternatives</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_data">data</code></td>
<td>
<p>a matrix or data frame of items to be scored.</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_score">score</code></td>
<td>
<p>score=FALSE, just convert to right (1) or wrong (0).<br />
score=TRUE, find the totals or average scores and do item analysis</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_totals">totals</code></td>
<td>
<p>total=FALSE: find the average number correct <br />
total=TRUE: find the total number correct</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_ilabels">ilabels</code></td>
<td>
<p>item labels </p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_missing">missing</code></td>
<td>
<p>missing=TRUE: missing values are replaced with means or medians <br />   missing=FALSE  missing values are not scored </p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_impute">impute</code></td>
<td>
<p>impute=&quot;median&quot;, replace missing items with the median score
<br /> impute=&quot;mean&quot;: replace missing values with the item mean</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_digits">digits</code></td>
<td>
<p> How many digits of output </p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_short">short</code></td>
<td>
<p>short=TRUE, just report the item statistics, <br />
short=FALSE, report item statistics and subject scores as well</p>
</td></tr>
<tr><td><code id="score.multiple.choice_+3A_skew">skew</code></td>
<td>
<p>Should the skews and kurtosi of the raw data be reported? Defaults to FALSE because what is the meaning of skew for a multiple choice item?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically combines <code><a href="#topic+score.items">score.items</a></code> with a conversion from multiple choice to right/wrong.
</p>
<p>The item-whole correlation is inflated because of item overlap.
</p>
<p>The example data set is taken from the Synthetic Aperture Personality Assessment personality and ability test at <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>scores</code></td>
<td>
<p>Subject scores on one scale</p>
</td></tr>
<tr><td><code>missing</code></td>
<td>
<p>Number of missing items for each subject</p>
</td></tr>
<tr><td><code>item.stats</code></td>
<td>
<p>scoring key, response frequencies, item whole correlations, n subjects scored, mean, sd, skew, kurtosis and se for each item</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Cronbach's coefficient alpha</p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>Average interitem correlation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

  <p><code><a href="#topic+score.items">score.items</a></code>, <code><a href="#topic+omega">omega</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(psychTools::iqitems)
iq.keys &lt;- c(4,4,4, 6,6,3,4,4,  5,2,2,4,  3,2,6,7)
score.multiple.choice(iq.keys,psychTools::iqitems)
#just convert the items to true or false 
iq.tf &lt;- score.multiple.choice(iq.keys,psychTools::iqitems,score=FALSE)
describe(iq.tf)  #compare to previous results

</code></pre>

<hr>
<h2 id='scoreIrt'>Find Item Response Theory (IRT) based scores for dichotomous or polytomous items</h2><span id='topic+scoreIrt'></span><span id='topic+scoreIrt.1pl'></span><span id='topic+scoreIrt.2pl'></span><span id='topic+score.irt'></span><span id='topic+score.irt.poly'></span><span id='topic+score.irt.2'></span><span id='topic+irt.stats.like'></span><span id='topic+make.irt.stats'></span><span id='topic+irt.tau'></span><span id='topic+irt.se'></span>

<h3>Description</h3>

<p><code><a href="#topic+irt.fa">irt.fa</a></code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code><a href="#topic+scoreIrt">scoreIrt</a></code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> will score lists of scales.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scoreIrt(stats=NULL, items, keys=NULL,cut = 0.3,bounds=c(-4,4),mod="logistic") 
scoreIrt.1pl(keys.list,items,correct=.5,messages=FALSE,cut=.3,bounds=c(-4,4),
     mod="logistic")  #Rasch like scaling
scoreIrt.2pl(itemLists,items,correct=.5,messages=FALSE,cut=.3,bounds=c(-4,4),
   mod="logistic")  #2 pl scoring
#the next is an alias for scoreIrt both of which are wrappers for 
#     score.irt.2 and score.irt.poly
score.irt(stats=NULL, items, keys=NULL,cut = 0.3,bounds=c(-4,4),mod="logistic") 
 #the higher order call just calls one of the next two
  #for dichotomous items 
score.irt.2(stats, items,keys=NULL,cut = 0.3,bounds=c(-4,4),mod="logistic") 
  #for polytomous items
score.irt.poly(stats, items, keys=NULL, cut = 0.3,bounds=c(-4,4),mod="logistic")
    #to create irt like statistics for plotting
irt.stats.like(items,stats,keys=NULL,cut=.3)
make.irt.stats(difficulty,discrimination) 
   
irt.tau(x)    #find the tau values for the x object
irt.se(stats,scores=0,D=1.702)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scoreIrt_+3A_stats">stats</code></td>
<td>
<p>Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. </p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_items">items</code></td>
<td>
<p>The raw data, may be either dichotomous or polytomous.</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_itemlists">itemLists</code></td>
<td>
<p>a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_keys.list">keys.list</code></td>
<td>
<p>A list of items to be scored with keying direction  (see example)</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_keys">keys</code></td>
<td>
<p>A keys matrix of which items should be scored for each factor</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_cut">cut</code></td>
<td>
<p>Only items with discrimination values &gt; cut will be used for scoring.</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_x">x</code></td>
<td>
<p>The raw data to be used to find the tau parameter in irt.tau</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_bounds">bounds</code></td>
<td>
<p>The lower and upper estimates for the fitting function</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_mod">mod</code></td>
<td>
<p>Should a logistic or normal model be used in estimating the scores?</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_correct">correct</code></td>
<td>
<p>What value should be used for continuity correction when finding the
tetrachoric or polychoric correlations when using irt.fa</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_messages">messages</code></td>
<td>
<p>Should messages be suppressed when running multiple scales?</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_scores">scores</code></td>
<td>
<p>A single score or a vector of scores to find standard errors</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_d">D</code></td>
<td>
<p>The scaling function for the test information statistic used in irt.se</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_difficulty">difficulty</code></td>
<td>
<p>The difficulties for each item in a polytomous scoring</p>
</td></tr>
<tr><td><code id="scoreIrt_+3A_discrimination">discrimination</code></td>
<td>
<p>The item discrimin</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta <code class="reqn">\theta</code> that best fits the equation <code class="reqn">P(x|\theta) = 1/(1+exp(\beta(\delta - \theta) )</code> for a score vector X, and location <code class="reqn">\delta</code> and discrimination <code class="reqn">\beta</code> provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.
</p>
<p>The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. 
</p>
<p>As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.
</p>
<p>There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   
</p>
<p>Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  <code><a href="#topic+scoreIrt">scoreIrt</a></code> seems to do a good job of recovering the basic structure.
</p>
<p>If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.
</p>
<p>The two wrapper functions <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code> and <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> are very fast and are meant for scoring one or many scales at a time with a one factor model (<code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code>) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (<code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>) or just items to score  for a number of scales <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code>.  <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> will then apply <code><a href="#topic+irt.fa">irt.fa</a></code> to the items for each scale separately, and then find the 2pl scores. 
</p>
<p>The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>.  Alternatively, a keys matrix can be created using <code><a href="#topic+make.keys">make.keys</a></code>.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See <code><a href="#topic+scoreItems">scoreItems</a></code> or <code><a href="#topic+make.keys">make.keys</a></code> for details.  The default case is to score all items with absolute discriminations &gt; cut.
</p>
<p>If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using <code><a href="#topic+irt.tau">irt.tau</a></code> or combine this information with a scoring keys matrix (see <code><a href="#topic+scoreItems">scoreItems</a></code> and <code><a href="#topic+make.keys">make.keys</a></code> and create quasi-IRT statistics using <code><a href="#topic+irt.stats.like">irt.stats.like</a></code>.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using <code><a href="#topic+irt.tau">irt.tau</a></code> or just found before doing the scoring.  This is all done for you inside of <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code>. 
</p>
<p>Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. 
</p>
<p>David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  <code><a href="#topic+scoreIrt.2pl">scoreIrt.2pl</a></code> takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  
</p>
<p>There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.
</p>
<p><code><a href="#topic+irt.se">irt.se</a></code> finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by <code><a href="#topic+irt.fa">irt.fa</a></code> and are not based upon the particular score of a particular subject.
</p>


<h3>Value</h3>

<table>
<tr><td><code>scores</code></td>
<td>
<p>A data frame of theta estimates, total scores based upon raw sums, and estimates of fit.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>Returned by irt.tau: A data frame of the tau values for an object of dichotomous or polytomous items.  Found without bothering to find the correlations.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is very important to note that when using <code><a href="#topic+irt.fa">irt.fa</a></code> to find the discriminations, to set the sort option to be FALSE. This is now the default.  Otherwise, the discriminations will not match the item order.  
</p>
<p>Always under development.  Suggestions for improvement are most appreciated.
</p>
<p>scoreIrt is just a wrapper to score.irt.poly and score.irt.2.  The previous version had score.irt which is now deprecated as I try to move to camelCase.
</p>
<p>scoreIrt.2pl is a wrapper for irt.fa and scoreIrt.  It was originally developed by David Condon.
</p>


<h3>Author(s)</h3>

<p>William Revelle, David Condon
</p>


<h3>References</h3>

<p>Kamata, Akihito and Bauer, Daniel J. (2008) A Note on the Relation Between Factor Analytic and Item Response Theory Models
Structural Equation Modeling, 15 (1) 136-153.
</p>
<p>McDonald, Roderick P. (1999) Test theory: A unified treatment. L. Erlbaum Associates.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+irt.fa">irt.fa</a></code> for finding the parameters. For more conventional scoring algorithms see <code><a href="#topic+scoreItems">scoreItems</a></code>. <code><a href="#topic+irt.responses">irt.responses</a></code> will plot the empirical response patterns for the alternative response choices for  multiple choice items. For more conventional IRT estimations, see the ltm package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #not run in the interest of time, but worth doing
d9 &lt;- sim.irt(9,1000,-2.5,2.5,mod="normal") #dichotomous items
test &lt;- irt.fa(d9$items)
scores &lt;- scoreIrt(test,d9$items)
scores.df &lt;- data.frame(scores,true=d9$theta) #combine the estimates with the true thetas.
pairs.panels(scores.df,pch=".",
main="Comparing IRT and classical with complete data") 
#now show how to do this with a quasi-Rasch model
tau &lt;- irt.tau(d9$items)
scores.rasch &lt;- scoreIrt(tau,d9$items,key=rep(1,9))
scores.dfr&lt;- data.frame(scores.df,scores.rasch) #almost identical to 2PL model!
pairs.panels(scores.dfr)
#with all the data, why bother ?

#now delete some of the data
d9$items[1:333,1:3] &lt;- NA
d9$items[334:666,4:6] &lt;- NA
d9$items[667:1000,7:9] &lt;- NA
scores &lt;- scoreIrt(test,d9$items)
scores.df &lt;- data.frame(scores,true=d9$theta) #combine the estimates with the true thetas.
pairs.panels(scores.df, pch=".",
main="Comparing IRT and classical with random missing data")
 #with missing data, the theta estimates are noticably better.
#now show how to do this with a quasi-Rasch model
tau &lt;- irt.tau(d9$items)
scores.rasch &lt;- scoreIrt(tau,d9$items,key=rep(1,9))
scores.dfr &lt;- data.frame(scores.df,rasch = scores.rasch)
pairs.panels(scores.dfr)  #rasch is actually better!



v9 &lt;- sim.irt(9,1000,-2.,2.,mod="normal") #dichotomous items
items &lt;- v9$items
test &lt;- irt.fa(items)
total &lt;- rowSums(items)
ord &lt;- order(total)
items &lt;- items[ord,]


#now delete some of the data - note that they are ordered by score
items[1:333,5:9] &lt;- NA
items[334:666,3:7] &lt;- NA
items[667:1000,1:4] &lt;- NA
items[990:995,1:9] &lt;- NA   #the case of terrible data
items[996:998,] &lt;- 0   #all wrong
items[999:1000] &lt;- 1   #all right
scores &lt;- scoreIrt(test,items)
unitweighted &lt;- scoreIrt(items=items,keys=rep(1,9)) #each item has a discrimination of 1
#combine the estimates with the true thetas.
scores.df &lt;- data.frame(v9$theta[ord],scores,unitweighted) 
   
colnames(scores.df) &lt;- c("True theta","irt theta","total","fit","rasch","total","fit")
pairs.panels(scores.df,pch=".",main="Comparing IRT and classical with missing data") 
 #with missing data, the theta estimates are noticably better estimates 
 #of the generating theta than using the empirically derived factor loading weights

#now show the ability to score multiple scales using keys
ab.tau &lt;- irt.tau(psychTools::ability)  #first find the tau values
ab.keys &lt;- make.keys(psychTools::ability,list(g=1:16,reason=1:4,
letter=5:8,matrix=9:12,rotate=13:16))
#ab.scores &lt;- scoreIrt(stats=ab.tau, items = psychTools::ability, keys = ab.keys)

#and now do it for polytomous items using 2pl
bfi.scores &lt;- scoreIrt.2pl(bfi.keys,bfi[1:25])
#compare with classical unit weighting by using scoreItems
#not run in the interests of time
#bfi.unit &lt;- scoreItems(psychTools::bfi.keys,psychTools::bfi[1:25])
#bfi.df &lt;- data.frame(bfi.scores,bfi.unit$scores)
#pairs.panels(bfi.df,pch=".")


bfi.irt &lt;- scoreIrt(items=bfi[16:20])  #find irt based N scores

#Specify item difficulties and discriminations from a different data set.
stats &lt;- structure(list(MR1 = c(1.4, 1.3, 1.3, 0.8, 0.7), difficulty.1 = c(-1.2, 
-2, -1.5, -1.2, -0.9), difficulty.2 = c(-0.1, -0.8, -0.4, -0.3, 
-0.1), difficulty.3 = c(0.6, -0.2, 0.2, 0.2, 0.3), difficulty.4 = c(1.5, 
0.9, 1.1, 1, 1), difficulty.5 = c(2.5, 2.1, 2.2, 1.7, 1.6)), row.names = c("N1", 
"N2", "N3", "N4", "N5"), class = "data.frame")


stats #show them
bfi.new &lt;-scoreIrt(stats,bfi[16:20])
bfi.irt &lt;- scoreIrt(items=bfi[16:20]) 
cor2(bfi.new,bfi.irt)
newstats &lt;- stats
newstats[2:6] &lt;-stats[2:6 ] + 1  #change the difficulties
bfi.harder &lt;- scoreIrt(newstats,bfi[16:20])
pooled &lt;- cbind(bfi.irt,bfi.new,bfi.harder)
describe(pooled)  #note that the mean scores have changed
lowerCor(pooled) #and although the unit weighted scale are identical,
# the irt scales differ by changing the difficulties



</code></pre>

<hr>
<h2 id='scoreItems'> Score item composite scales and find Cronbach's alpha, Guttman lambda 6 and item whole correlations </h2><span id='topic+scoreItems'></span><span id='topic+scoreFast'></span><span id='topic+scoreVeryFast'></span><span id='topic+score.items'></span><span id='topic+response.frequencies'></span><span id='topic+responseFrequency'></span>

<h3>Description</h3>

<p>Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code><a href="#topic+make.keys">make.keys</a></code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code><a href="#topic+cluster.cor">cluster.cor</a></code>). <code><a href="#topic+response.frequencies">response.frequencies</a></code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code><a href="#topic+scoreFast">scoreFast</a></code> and   <code><a href="#topic+scoreVeryFast">scoreVeryFast</a></code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scoreItems(keys, items, totals = FALSE, ilabels = NULL,missing=TRUE, impute="median",
      delete=TRUE, min = NULL, max = NULL, digits = 2,n.obs=NULL,select=TRUE)
score.items(keys, items, totals = FALSE, ilabels = NULL,missing=TRUE, impute="median",
      delete=TRUE, min = NULL, max = NULL, digits = 2,select=TRUE) 
scoreFast(keys, items, totals = FALSE, ilabels = NULL,missing=TRUE, impute="none",
       delete=TRUE, min = NULL, max = NULL,count.responses=FALSE, digits = 2)
scoreVeryFast(keys,items,totals=FALSE, min=NULL,max=NULL,count.responses=FALSE)
response.frequencies(items,max=10,uniqueitems=NULL)
responseFrequency(items,max=10,uniqueitems=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scoreItems_+3A_keys">keys</code></td>
<td>
<p>A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using <code><a href="#topic+make.keys">make.keys</a></code>. Just using a list of scoring keys (see example) is probably more convenient.</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_items">items</code></td>
<td>
<p> Matrix or dataframe of raw item scores</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_totals">totals</code></td>
<td>
<p> if TRUE  find total scores, if FALSE (default), find average scores </p>
</td></tr>
<tr><td><code id="scoreItems_+3A_ilabels">ilabels</code></td>
<td>
<p> a vector of item labels. </p>
</td></tr>
<tr><td><code id="scoreItems_+3A_missing">missing</code></td>
<td>
<p>missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_impute">impute</code></td>
<td>
<p>impute=&quot;median&quot; replaces missing values with the item medians, impute = &quot;mean&quot; replaces values with the mean response. impute=&quot;none&quot; the subject's scores are based upon the average of the keyed, but non missing scores. impute = &quot;none&quot; is probably more appropriate for a large number of missing cases (e.g., SAPA data).  </p>
</td></tr>
<tr><td><code id="scoreItems_+3A_delete">delete</code></td>
<td>
<p>if delete=TRUE, automatically delete items with no variance (and issue a warning)</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_min">min</code></td>
<td>
<p>May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_max">max</code></td>
<td>
<p>May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. </p>
</td></tr>
<tr><td><code id="scoreItems_+3A_uniqueitems">uniqueitems</code></td>
<td>
<p>If specified, the set of possible unique response categories</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_digits">digits</code></td>
<td>
<p> Number of digits to report for mean scores </p>
</td></tr>
<tr><td><code id="scoreItems_+3A_n.obs">n.obs</code></td>
<td>
<p>If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_select">select</code></td>
<td>
<p>By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.</p>
</td></tr>
<tr><td><code id="scoreItems_+3A_count.responses">count.responses</code></td>
<td>
<p>If TRUE, report the number of items/scale answered for each subject.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  
</p>
<p>When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using <code><a href="#topic+make.keys">make.keys</a></code> or may be entered as a keys.list directly.  
</p>
<p>Various estimates of scale reliability include &ldquo;Cronbach's alpha&quot;, Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  
</p>
<p>Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's <code class="reqn">\alpha</code>  (1951) underestimates the reliability of a test and over estimates the first factor saturation.
</p>
<p><code class="reqn">\alpha</code> (Cronbach, 1951) is the same as Guttman's  <code class="reqn">\lambda_3</code> (Guttman, 1945) and may be found by
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_3 =  \frac{n}{n-1}\Bigl(1 - \frac{tr(\vec{V})_x}{V_x}\Bigr) = \frac{n}{n-1} \frac{V_x - tr(\vec{V}_x)}{V_x} = \alpha
</code>
</p>

<p>Perhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is &ldquo;lumpy&quot;) coefficients <code class="reqn">\beta</code> (Revelle, 1979; see <code><a href="#topic+ICLUST">ICLUST</a></code>) and <code class="reqn">\omega_h</code> (see <code><a href="#topic+omega">omega</a></code>)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  <code class="reqn">\omega_t</code> (see <code><a href="#topic+omega">omega</a></code>) is a better estimate of the reliability of the total test.  
</p>
<p>Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, <code class="reqn">e_j^2</code>,  and is
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_6 = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1-r_{smc}^2)}{V_x}
.</code>
</p>

<p>The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.
</p>
<p>G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha &gt; G6, but if the loadings are unequal or if there is a general factor, G6 &gt; alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.
</p>
<p>Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).
</p>
<p>A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just </p>
<p style="text-align: center;"><code class="reqn">s/n = \frac{n \bar{r}}{1-n \bar{r}}</code>
</p>
<p>  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).
</p>
<p>Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).
</p>
<p>More complete reliability analyses of a single scale can be done using the <code><a href="#topic+omega">omega</a></code> function which finds <code class="reqn">\omega_h</code> and <code class="reqn">\omega_t</code> based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the <code><a href="#topic+guttman">guttman</a></code> function. 
</p>
<p>Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (<code class="reqn">\omega_h</code> and <code class="reqn">\omega_t</code>) should be encouraged. 
</p>
<p>Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.
</p>
<p>There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute=&quot;mean&quot;), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute=&quot;none&quot;).  This is useful for findings means for scales for the SAPA project (see <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The &quot;alpha observed&quot; values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. 
</p>
<p>Using the impute=&quot;none&quot; option as well as asking for totals (totals=&quot;TRUE&quot;) will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.
</p>
<p>The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:
</p>
<p>scores$scores[scores$missing &gt;0] &lt;- NA 
</p>
<p>This is shown in the last example.
</p>
<p>Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.
</p>
<p>By default, <code><a href="#topic+scoreItems">scoreItems</a></code> will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?
</p>
<p><code><a href="#topic+scoreItems">scoreItems</a></code> can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  
</p>
<p><code><a href="#topic+scoreFast">scoreFast</a></code> just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!
<code><a href="#topic+scoreVeryFast">scoreVeryFast</a></code> is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
</p>


<h3>Value</h3>

 
<table>
<tr><td><code>scores</code></td>
<td>
<p>Sum or average scores for each subject on the k scales</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Cronbach's coefficient alpha.  A simple (but non-optimal) measure of the internal consistency of a test. See also beta and omega. Set to 1 for scales of length 1. </p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>The average correlation within a scale, also known as alpha 1, is a useful index of the internal consistency of a domain. Set to 1 for scales with 1 item.</p>
</td></tr>
<tr><td><code>G6</code></td>
<td>
<p>Guttman's Lambda 6 measure of reliability</p>
</td></tr>
<tr><td><code>G6*</code></td>
<td>
<p>A generalization of Guttman's Lambda 6 measure of reliability using all the items to find the smc.</p>
</td></tr>
<tr><td><code>n.items</code></td>
<td>
<p>Number of items on each scale</p>
</td></tr>
<tr><td><code>item.cor</code></td>
<td>
<p>The correlation of each item with each scale.  Because this is not corrected for item overlap, it will overestimate the amount that an item correlates with the other items in a scale.</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>The intercorrelation of all the scales based upon the interitem correlations (see note for why these differ from the correlations of the observed scales themselves).</p>
</td></tr>
<tr><td><code>corrected</code></td>
<td>
<p>The correlations of all scales (below the diagonal), alpha on the diagonal, and the unattenuated correlations (above the diagonal)</p>
</td></tr>
<tr><td><code>item.corrected</code></td>
<td>
<p>The item by scale correlations for each item, corrected for item overlap by replacing the item variance with the smc for that item</p>
</td></tr>
<tr><td><code>response.freq</code></td>
<td>
<p>The response frequency (based upon number of non-missing responses) for each alternative.</p>
</td></tr>
<tr><td><code>missing</code></td>
<td>
<p>How many items were not answered  for each scale </p>
</td></tr>
<tr><td><code>num.ob.item</code></td>
<td>
<p>The average number of items with responses on a scale.  Used in calculating the alpha.observed&ndash; relevant for SAPA type data structures.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is important to recognize in the case of massively missing data (e.g., data from a Synthetic Aperture Personality Assessment (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) or the International Cognitive Ability Resources (<a href="https://icar-project.org">https://icar-project.org</a>)) study where perhaps only 10-50% of the items per scale are given to any one subject)) that the number of items per scale, and hence standardized alpha, is not the nominal value and hence alpha of the observed scales will be overestimated.  For this case (impute=&quot;none&quot;), an additional alpha (alpha.ob) is reported. 
</p>
<p>More importantly in this case of massively missing data, there is a difference between the correlations of the composite scales based upon the correlations of the items and the correlations of the scored scales based upon the observed data.  That is, the cor object will have correlations as if all items had been given, while the correlation of the scores object will reflect the actual correlation of the scores.  For SAPA data, it is recommended to use the cor object.  Confidence of these correlations may be found using the <code><a href="#topic+cor.ci">cor.ci</a></code> function.
</p>
<p>Further note that the inter-scale correlations are based upon the correlations of scales formed from the covariance matrix of the items.  This will differ from the correlation  of scales based upon the correlation of the items.  Thus, although  <code><a href="#topic+scoreItems">scoreItems</a></code> will produce reliabilities and intercorrelations from either the raw data or from a correlation matrix, these values will differ slightly.  In addition, with a great deal of missing data, the scale intercorrelations will differ from the correlations of the scores produced, for the latter will be attenuated. 
</p>
<p>An alternative to classical test theory scoring is to use <code><a href="#topic+scoreIrt">scoreIrt</a></code> to find score estimates based upon Item Response Theory. This is particularly useful in the case of SAPA data which tend to be massively missing.  It is also useful  to find scores based upon polytomous items following a factor analysis of the polychoric correlation matrix (see <code><a href="#topic+irt.fa">irt.fa</a></code>).  However, remember that  this only makes sense if the items are unidimensional. That is to say, if forming item composites from (e.g., <code><a href="#topic+bestScales">bestScales</a></code>), that are empirically derived, they will necessarily have a clear factor structure and the IRT based scoring does not make sense.
</p>
<p>When reverse scoring items from a set where items differ in their possible minima or maxima, it is important to specify the min and max values.  Items are reversed by subtracting them from max + min.  Thus, if items range from 1 to 6, items are reversed by subtracting them from 7.  But, if the data set includes other variables, (say an id field) that far exceeds the item min or max, then the max id will incorrectly be used to reverse key.  min and max can either be single values, or vectors for all items. Compare two examples of scoring the <code><a href="psychTools.html#topic+bfi">bfi</a></code> data set.
</p>
<p>If scales are formed with overlapping items, then the correlations of the scales will be seriously inflated.  <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> will adjust the correlations for this overlap.
</p>
<p>Yet another possibility for scoring large data sets is to ignore all the reliability calculations and just find the scores.  This may be done using <code><a href="#topic+scoreFast">scoreFast</a></code> or 
<code><a href="#topic+scoreVeryFast">scoreVeryFast</a></code>.  These two functions just find mean scores (<code><a href="#topic+scoreVeryFast">scoreVeryFast</a></code> without imputation) or will do imputation if desired <code><a href="#topic+scoreFast">scoreFast</a></code>. For 200K cases on 1000 variables with 11 scales, <code><a href="#topic+scoreVeryFast">scoreVeryFast</a></code> took 4.7 seconds on a Mac PowerBook with a 2.8GHZ Intel I7 and <code><a href="#topic+scoreFast">scoreFast</a></code> took 23.2 seconds.  <code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code> for the same problem took xxx with options(&quot;mc.cores&quot;=1) (not parallel processing) and 1259 seconds with  options(&quot;mc.cores&quot;=NULL) (implying 2 cores) and with four cores was very slow (probably too much parallel processing). 
</p>
<p>Yet one more possibility is to find scores based upon a matrix of weights (e.g. zero order correlations, beta weights, or factor weights.) In this case, scores are simply the product of the weights times the (standardized) items.   If using coefficients from a regression analysis (lm), a column of 1's is added to the data and labeled &quot;(Intercept)&quot; and this is used in the calculation. This is done by <code><a href="#topic+scoreWtd">scoreWtd</a></code>.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Cronbach, L.J. and Gleser G.C.  (1964)The signal/noise ratio in the comparison of reliability coefficients. Educational and Psychological Measurement, 24 (3) 467-480. 
</p>
<p>Duhachek, A. and Iacobucci, D. (2004). Alpha's standard error (ase): An accurate and precise confidence interval estimate. Journal of Applied Psychology, 89(5):792-808.
</p>
<p>McDonald, R. P. (1999). Test theory: A unified treatment. L. Erlbaum Associates, Mahwah, N.J.
</p>
<p>Revelle, W. (in preparation) An introduction to psychometric theory with applications in R.  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>
<p>Revelle, W. and Condon, D.C. Reliability.  In Irwing, P., Booth, T. and Hughes, D. (Eds). the Wiley-Blackwell Handbook of Psychometric Testing (in press).
</p>
<p>Revelle W. and R.E. Zinbarg. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma. Psychometrika, 74(1):145-154.
</p>
<p>Zinbarg, R. E., Revelle, W., Yovel, I. and Li, W. (2005) Cronbach's alpha, Revelle's beta, and McDonald's omega h, Their relations with each other and two alternative conceptualizations of reliability, Psychometrika, 70, 123-133.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.keys">make.keys</a></code> for a convenient way to create the keys file,  <code><a href="#topic+score.multiple.choice">score.multiple.choice</a></code> for multiple choice items, <br /> <code><a href="#topic+alpha">alpha</a></code>, <code><a href="#topic+correct.cor">correct.cor</a></code>, <code><a href="#topic+cluster.cor">cluster.cor</a></code> , <code><a href="#topic+cluster.loadings">cluster.loadings</a></code>, <code><a href="#topic+omega">omega</a></code>, <code><a href="#topic+guttman">guttman</a></code> for item/scale analysis.
</p>
<p>If scales are formed from overlapping sets of items, their correlations will be inflated.  This is corrected for when using the <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> function which, although it will not produce scores, will report scale intercorrelations corrected for item overlap. 
</p>
<p>In addition, the <code><a href="#topic+irt.fa">irt.fa</a></code> function provides an alternative way of examining the structure of a test and emphasizes item response theory approaches to the information returned by each item and the total test. Associated with these IRT parameters is the <code><a href="#topic+scoreIrt">scoreIrt</a></code> function for finding IRT based scores as well as <code><a href="#topic+irt.responses">irt.responses</a></code> to show response curves for the alternatives in a multiple choice test.
</p>
<p><code><a href="#topic+scoreIrt">scoreIrt</a></code> will find both  IRT based estimates as well as average item response scores.  These latter correlate perfectly with those found by scoreItems.  If using a keys matrix, the score.irt results are based upon the item difficulties with the assumption that all items are equally discriminating (effectively a Rasch model).  These scores are probably most useful  in the case of massively missing data because they can take into account the item difficulties. 
</p>
<p><code><a href="#topic+scoreIrt.1pl">scoreIrt.1pl</a></code> finds the item difficulty parameters and then applies a 1 parameter (Rasch like) model.  It chooses items based upon a keys.list.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#see  the example including the bfi data set
data(psychTools::bfi)
keys.list &lt;- list(agree=c("-A1","A2","A3","A4","A5"),
  conscientious=c("C1","C2","C3","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"),
  neuroticism=c("N1","N2","N3","N4","N5"), openness = c("O1","-O2","O3","O4","-O5")) 
  keys &lt;- make.keys(psychTools::bfi,keys.list)  #no longer necessary
 scores &lt;- scoreItems(keys,psychTools::bfi,min=1,max=6)  #using a keys matrix 
 scores &lt;- scoreItems(keys.list,psychTools::bfi,min=1,max=6)  # or just use the keys.list
 summary(scores)
 #to get the response frequencies, we need to not use the age variable
 scores &lt;- scoreItems(keys[1:25,],psychTools::bfi[1:25]) #we do not need to specify min or 
 #max if  there are no values (such as age) outside the normal item range.
 scores
 #The scores themselves are available in the scores$scores object.  I.e.,
 describe(scores$scores)
 
 
 #compare this output to that for the impute="none" option for SAPA type data
 #first make many of the items missing in a missing pattern way
 missing.bfi &lt;- psychTools::bfi
 missing.bfi[1:1000,3:8] &lt;- NA
 missing.bfi[1001:2000,c(1:2,9:10)] &lt;- NA
 scores &lt;- scoreItems(keys.list,missing.bfi,impute="none",min=1,max=6)
 scores
 describe(scores$scores)  #the actual scores themselves
 
 #If we want to delete scales scores for people who did not answer some items for one 
 #(or more) scales, we can do the following:
 
  scores &lt;- scoreItems(keys.list,missing.bfi,totals=TRUE,min=1,max=6) #find total scores
  describe(scores$scores) #note that missing data were replaced with median for the item
  scores$scores[scores$missing &gt; 0] &lt;- NA  #get rid of cases with missing data
  describe(scores$scores)


</code></pre>

<hr>
<h2 id='scoreOverlap'>Find correlations of composite variables (corrected for overlap) from a larger matrix.</h2><span id='topic+cluster.cor'></span><span id='topic+scoreOverlap'></span><span id='topic+scoreBy'></span>

<h3>Description</h3>

<p>Given a  n x c cluster definition matrix of -1s, 0s, and 1s (the keys) , and a n x n correlation matrix, or an N x n data matrix, find the correlations of the composite clusters.  The keys matrix can be entered by hand, copied from the clipboard (<code><a href="psychTools.html#topic+read.clipboard">read.clipboard</a></code>), or taken as output from the <code><a href="#topic+factor2cluster">factor2cluster</a></code> or <code><a href="#topic+make.keys">make.keys</a></code> functions.  Similar functionality to <code><a href="#topic+scoreItems">scoreItems</a></code> which also gives item by cluster correlations. <code><a href="#topic+scoreBy">scoreBy</a></code> does this for individual subjects after a call to <code><a href="#topic+statsBy">statsBy</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scoreOverlap(keys, r, correct = TRUE, SMC = TRUE, av.r = TRUE, item.smc = NULL, 
     impute = TRUE,select=TRUE, scores=FALSE,  min=NULL,max=NULL)
scoreBy(keys,stats, correct = TRUE, SMC = TRUE, av.r = TRUE, item.smc = NULL, 
     impute = TRUE,select=TRUE,min.n=3,smooth=FALSE)
cluster.cor(keys, r.mat, correct = TRUE,SMC=TRUE,item.smc=NULL,impute=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scoreOverlap_+3A_keys">keys</code></td>
<td>
<p>A list of scale/cluster keys, or a  matrix of cluster keys </p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_r.mat">r.mat</code></td>
<td>
<p>A correlation matrix </p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_r">r</code></td>
<td>
<p>Either a correlation matrix or a raw data matrix</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_stats">stats</code></td>
<td>
<p>The output from statsBy.  Needed for scoreBy.</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_correct">correct</code></td>
<td>
<p> TRUE shows both raw and corrected for attenuation correlations</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_smc">SMC</code></td>
<td>
<p>Should squared multiple correlations be used as communality estimates for the correlation matrix? </p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_item.smc">item.smc</code></td>
<td>
<p>the smcs of the items may be passed into the function for speed, or calculated if SMC=TRUE </p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_impute">impute</code></td>
<td>
<p>if TRUE and scores=FALSE, impute missing scale correlations based upon the average interitem correlation, otherwise return NA. If scores=TRUE, impute missing data by means, medians, or none.</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_av.r">av.r</code></td>
<td>
<p>Should the average r be used in correcting for overlap? smcs otherwise.</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_select">select</code></td>
<td>
<p>By default, just find statistics for items included in the scoring keys. This allows for finding scores from matrices with bad items if they are not included in the set of scoring keys.</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_min.n">min.n</code></td>
<td>
<p>The minimum number of pairwise observations needed to define a correlation (in scoreBy)</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_smooth">smooth</code></td>
<td>
<p>If the matrices used in scoreBy are not positive semi-definite, should we smooth them?</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_scores">scores</code></td>
<td>
<p>Find the raw average scores for each subject.  Same functionality as <code><a href="#topic+scoreItems">scoreItems</a></code></p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_min">min</code></td>
<td>
<p>May be specified as minimum item score allowed, else will be calculated from data. min and max should be specified if items differ in their possible minima or maxima. See notes for details.</p>
</td></tr>
<tr><td><code id="scoreOverlap_+3A_max">max</code></td>
<td>
<p>May be specified as maximum item score allowed, else will be calculated from data. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>These  are three of the functions used in the SAPA (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) procedures to form synthetic correlation matrices.  Given any correlation matrix of items, it is easy to find the correlation matrix of scales made up of those items. This can also be done from the original data matrix or from the correlation matrix using <code><a href="#topic+scoreItems">scoreItems</a></code> which is probably preferred unless the keys are overlapping.  It is important to remember with SAPA data, that scale correlations should be found from the item correlations, not the raw data.
</p>
<p>In the case of overlapping keys, (items being scored on multiple scales), <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> will adjust for this overlap by replacing the overlapping covariances (which are variances when overlapping) with the corresponding best estimate of an item's &ldquo;true&quot; variance using either the average correlation or the smc estimate for that item.  This parallels the operation done when finding alpha reliability.  This is similar to ideas suggested by Cureton (1966) and Bashaw and Anderson (1966) but uses the smc or the average interitem correlation (default).
</p>
<p>A typical use in the SAPA project is to form item composites by clustering or factoring (see <code><a href="#topic+fa">fa</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+principal">principal</a></code>), extract the clusters from these results (<code><a href="#topic+factor2cluster">factor2cluster</a></code>), and then form the composite correlation matrix using <code><a href="#topic+cluster.cor">cluster.cor</a></code>.  The variables in this reduced matrix may then be used in multiple correlatin procedures using <code><a href="#topic+mat.regress">mat.regress</a></code>.
</p>
<p>The original correlation is pre and post multiplied by the (transpose) of the keys matrix. 
</p>
<p>If some correlations are missing from the original matrix this will lead to missing values (NA) for scale intercorrelations based upon those lower level correlations. If impute=TRUE (the default), a warning is issued and the correlations are imputed based upon the average correlations of the non-missing elements of each scale. 
</p>
<p>Because the alpha estimate of reliability is based upon the correlations of the items rather than upon the covariances, this estimate of alpha is sometimes called &ldquo;standardized alpha&quot;.  If the raw items are available, it is useful to compare standardized alpha with the raw alpha found using <code><a href="#topic+scoreItems">scoreItems</a></code>.  They will differ substantially only if the items differ a great deal in their variances.  
</p>
<p>The MIMS matrix reflects the average correlations of the Multiple Items within each of the Multiple Scales.  This has been suggested as a measure of scale validity.  These correlations are adjusted for item overlap. 
</p>
<p><code><a href="#topic+scoreOverlap">scoreOverlap</a></code> answers an important question when developing scales and related subscales, or when comparing alternative versions of scales.  For by removing the effect of item overlap, it gives a better estimate the relationship between the latent variables estimated by the observed sum (mean) scores.
</p>
<p><code><a href="#topic+scoreBy">scoreBy</a></code> finds the within subject correlations after preprocessing with <code><a href="#topic+statsBy">statsBy</a></code>.  This is useful if doing an ESM study with multiple occasions for each subject.   It also makes it possible to find the correlations for subsets of subjects. See the example.  Note that it likely that for ESM data with a high level of missingness that the correlation matrices will not be positive-semi-definite. This can lead to composite score correlations that exceed 1.  Smoothing will resolve this problem.
</p>
<p><code><a href="#topic+scoreBy">scoreBy</a></code>  is useful when examining multi-level models where we want to examine the correlations within subjects (e.g., for ESM data) or within groups of subjects (when examining the stability of correlational structures by subgroups).  For both cases the data must be processed first by <code><a href="#topic+statsBy">statsBy</a></code>.  To find the variances of the scales it is necessary to use the cor=&quot;cov&quot; option in <code><a href="#topic+statsBy">statsBy</a></code>. 
</p>
<p>Starting in version 2.4.1, the scores option has been added. This will score the data (as does <code><a href="#topic+scoreItems">scoreItems</a></code>) if it is not a correlation matrix but does not have all the options of  <code><a href="#topic+scoreItems">scoreItems</a></code>.  Obviously, while the correlations can be corrected for item ovelap, the raw scores can not be so corrected.   
</p>


<h3>Value</h3>

<table>
<tr><td><code>cor</code></td>
<td>
<p>the (raw) correlation matrix of the clusters</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of the cluster scores</p>
</td></tr>
<tr><td><code>corrected</code></td>
<td>
<p>raw correlations below the diagonal, alphas on diagonal, disattenuated above diagonal</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The (standardized) alpha reliability of each scale.</p>
</td></tr>
<tr><td><code>G6</code></td>
<td>
<p>Guttman's Lambda 6 reliability estimate is based upon the smcs for each item in a scale.  G6 uses the smc based upon the entire item domain.</p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>The average inter item correlation within a scale</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>How many items are in each cluster?</p>
</td></tr>
<tr><td><code>MIMS</code></td>
<td>
<p>A matrix of the average correlations, corrected for overlap, between the various scales.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> See SAPA  Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Personality and Cognition: The Personality-Cognition Link. In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer. 
</p>
<p>The second example uses the <code><a href="psychTools.html#topic+msq">msq</a></code> data set of 72 measures of motivational state to examine the overlap between four lower level scales and two higher level scales.
</p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

<p>Bashaw, W. and Anderson Jr, H. E. (1967). A correction for replicated error in correlation coefficients. Psychometrika, 32(4):435-441.
</p>
<p>Cureton, E. (1966). Corrected item-test correlations. Psychometrika, 31(1):93-96.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+factor2cluster">factor2cluster</a></code>, <code><a href="#topic+mat.regress">mat.regress</a></code>, <code><a href="#topic+alpha">alpha</a></code>, and most importantly, <code><a href="#topic+scoreItems">scoreItems</a></code>, which will do all of what cluster.cor does for most users.  cluster.cor is an important helper function for <code><a href="#topic+iclust">iclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#use the msq data set that shows the structure of energetic and tense arousal
small.msq &lt;- psychTools::msq[ c("active", "energetic", "vigorous", "wakeful", 
"wide.awake", "full.of.pep", "lively", "sleepy", "tired", "drowsy","intense", 
"jittery", "fearful", "tense", "clutched.up", "quiet", "still",    "placid",
 "calm", "at.rest") ]
small.R &lt;- cor(small.msq,use="pairwise")
keys.list &lt;- list(
EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
       "lively", "-sleepy", "-tired", "-drowsy"),
TA =c("intense", "jittery", "fearful", "tense", "clutched.up", "-quiet", "-still", 
       "-placid", "-calm", "-at.rest") ,

high.EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
       "lively"),
low.EA =c("sleepy", "tired", "drowsy"),
lowTA= c("quiet", "still", "placid", "calm", "at.rest"),
highTA = c("intense", "jittery", "fearful", "tense", "clutched.up")
   ) 
   
keys &lt;- make.keys(small.R,keys.list)
      
adjusted.scales &lt;- scoreOverlap(keys.list,small.R)
#compare with unadjusted
confounded.scales &lt;- cluster.cor(keys,small.R)
summary(adjusted.scales)
#note that the EA and high and low EA and TA and high and low TA 
# scale correlations are confounded
summary(confounded.scales) 

bfi.stats &lt;- statsBy(bfi,group="education",cors=TRUE ,cor="cov")
 #specify to find covariances
 bfi.plus.keys &lt;- c(bfi.keys,gender="gender",age ="age")
bfi.by &lt;- scoreBy(bfi.plus.keys,bfi.stats)
bfi.by$var   #to show the variances of each scale by groupl
round(bfi.by$cor.mat,2)  #the correlations by group
bfi.by$alpha 
</code></pre>

<hr>
<h2 id='scoreWtd'>Score items using regression or correlation based weights</h2><span id='topic+scoreWtd'></span>

<h3>Description</h3>

<p>Item weights from <code><a href="#topic+bestScales">bestScales</a></code> or <code><a href="#topic+lmCor">lmCor</a></code> are used to find weighted scale scores. In contrast to the unit weights used in <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+scoreWtd">scoreWtd</a></code> will multiply the data by a set of weights to find scale scores.  These weight may come from a regression (e.g., <code><a href="stats.html#topic+lm">lm</a></code> or <code><a href="#topic+lmCor">lmCor</a></code>) or may be the zero order correlation weights from <code><a href="#topic+bestScales">bestScales</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scoreWtd(weights, items, std = TRUE, sums = FALSE, impute = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scoreWtd_+3A_weights">weights</code></td>
<td>
<p>This is just a matrix of weights to use for each item for each scale.</p>
</td></tr>
<tr><td><code id="scoreWtd_+3A_items">items</code></td>
<td>
<p> Matrix or dataframe of raw item scores</p>
</td></tr>
<tr><td><code id="scoreWtd_+3A_std">std</code></td>
<td>
<p>if TRUE, then find weighted standard scores else just use raw data</p>
</td></tr>
<tr><td><code id="scoreWtd_+3A_sums">sums</code></td>
<td>
<p>By default, find the average item score. If sums = TRUE, then find the sum scores.  This is useful for regression with an intercept term</p>
</td></tr>
<tr><td><code id="scoreWtd_+3A_impute">impute</code></td>
<td>
<p>impute=&quot;median&quot; replaces missing values with the item medians, impute = &quot;mean&quot; replaces values with the mean response. impute=&quot;none&quot; the subject's scores are based upon the average of the keyed, but non missing scores. impute = &quot;none&quot; is probably more appropriate for a large number of missing cases (e.g., SAPA data).  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although meant for finding correlation weighted scores using the weights from <code><a href="#topic+bestScales">bestScales</a></code>, it also possible to use alternative weight matrices, such as those returned by the coefficients in <code><a href="stats.html#topic+lm">lm</a></code>.   
</p>


<h3>Value</h3>

<p>A data frame of scores.</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+bestScales">bestScales</a></code> and  <code><a href="#topic+lmCor">lmCor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
#find the weights from a regression model and then apply them to a new set
#derivation of weights from the first 20 cases 
model.lm &lt;- lm(rating ~ complaints + privileges + learning,data=attitude[1:20,])
#or use lmCor to find the coefficents
model &lt;- lmCor(rating ~ complaints + privileges +learning,data=attitude[1:20,],std=FALSE)
 
 #Apply these to a different set of data (the last 10 cases)
  #note that the regression coefficients need to be a matrix
  scores.lm &lt;- scoreWtd(as.matrix(model.lm$coefficients),attitude[21:30,],sums=TRUE,std=FALSE)
scores &lt;- scoreWtd(model$coefficients,attitude[21:30,],sums=TRUE,std=FALSE)
describe(scores)  

</code></pre>

<hr>
<h2 id='scrub'>A utility for basic data cleaning and recoding.  Changes values outside of minimum and maximum limits to NA. </h2><span id='topic+scrub'></span>

<h3>Description</h3>

<p>A tedious part of data analysis is addressing the problem of miscoded data that need to be converted to NA or some other value.  For a given data.frame or matrix, scrub will set all values of columns from=from to to=to that are less than a set (vector) of min values or more than a vector of max values to NA. Can also be used to do basic recoding of data for all values=isvalue to newvalue. Will also recode continuus variables into fewer categories.  Will convert Nan, -Inf and Inf to NA
</p>
<p>The length of the where, isvalue, and newvalues must either match, or be 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrub(x, where, min, max,isvalue,newvalue, cuts=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scrub_+3A_x">x</code></td>
<td>
<p>a data frame or matrix</p>
</td></tr>
<tr><td><code id="scrub_+3A_where">where</code></td>
<td>
<p>The variables to examine.  (Can be by name or by column number)</p>
</td></tr>
<tr><td><code id="scrub_+3A_min">min</code></td>
<td>
<p>a vector of minimum values that are acceptable</p>
</td></tr>
<tr><td><code id="scrub_+3A_max">max</code></td>
<td>
<p>a vector of maximum values that are acceptable</p>
</td></tr>
<tr><td><code id="scrub_+3A_isvalue">isvalue</code></td>
<td>
<p>a vector of values to be converted to newvalue (one per variable)</p>
</td></tr>
<tr><td><code id="scrub_+3A_newvalue">newvalue</code></td>
<td>
<p>a vector of values to replace those that match isvalue</p>
</td></tr>
<tr><td><code id="scrub_+3A_cuts">cuts</code></td>
<td>
<p>The lower,and upper boundaries for recoding </p>
</td></tr></table>


<h3>Details</h3>

<p>Solves a tedious problem that can be done directly but that is sometimes awkward.  Will either replace specified values with NA or will recode to values within a range.
</p>


<h3>Value</h3>

<p>The corrected data frame.</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+reverse.code">reverse.code</a></code>, <code><a href="#topic+rescale">rescale</a></code> for other simple utilities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(attitude)
x &lt;- scrub(attitude,isvalue=55) #make all occurrences of 55 NA
x1 &lt;- scrub(attitude, where=c(4,5,6), isvalue =c(30,40,50), 
     newvalue = c(930,940,950)) #will do this for the 4th, 5th, and 6th variables
x2 &lt;- scrub(attitude, where=c(4,4,4), isvalue =c(30,40,50), 
            newvalue = c(930,940,950)) #will just do it for the 4th column
new &lt;- scrub(attitude,1:3,cuts= c(10,40,50,60,100))  #change many values to fewer 
#get rid of a complicated set of cases and replace with missing values
y &lt;- scrub(attitude,where=2:4,min=c(20,30,40),max= c(120,110,100),isvalue= c(32,43,54))
y1 &lt;- scrub(attitude,where="learning",isvalue=55,newvalue=999) #change a column by name
y2 &lt;- scrub(attitude,where="learning",min=45,newvalue=999) #change a column by name

y3 &lt;- scrub(attitude,where="learning",isvalue=c(45,48),
    newvalue=999) #change a column by name look for multiple values in that column
y4 &lt;- scrub(attitude,where="learning",isvalue=c(45,48),
      newvalue= c(999,-999)) #change values in one column to one of two different things

</code></pre>

<hr>
<h2 id='SD'> Find the Standard deviation for a vector, matrix, or data.frame - do not return error if there are no cases </h2><span id='topic+SD'></span>

<h3>Description</h3>

<p>Find the standard deviation of a vector, matrix, or data.frame.  In the latter two cases, return the sd of each column.  Unlike the sd function, return NA if there are no observations rather than throw an error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SD(x, na.rm = TRUE)   #deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SD_+3A_x">x</code></td>
<td>
<p>a vector, data.frame, or matrix </p>
</td></tr>
<tr><td><code id="SD_+3A_na.rm">na.rm</code></td>
<td>
<p>na.rm is assumed to be TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds the standard deviation of a vector, matrix, or data.frame.  Returns NA if no cases.
</p>
<p>Just an adaptation of the stats:sd function to return the functionality found in R &lt; 2.7.0 or R &gt;= 2.8.0
Because this problem seems to have been fixed, SD will be removed eventually.</p>


<h3>Value</h3>

<p>The standard deviation
</p>


<h3>Note</h3>

<p> Until R 2.7.0, sd would return a NA rather than an error if no cases were observed.  SD brings back that functionality.  Although unusual, this condition will arise when analyzing data with high rates of missing values.
This function will probably be removed as 2.7.0 becomes outdated.</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>See Also</h3>

<p> These functions use SD rather than sd:  <code><a href="#topic+describe.by">describe.by</a></code>, <code><a href="#topic+skew">skew</a></code>, <code><a href="#topic+kurtosi">kurtosi</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(attitude)
apply(attitude,2,sd) #all complete
attitude[,1] &lt;- NA
SD(attitude) #missing a column
describe(attitude)
</code></pre>

<hr>
<h2 id='sim'>Functions to simulate psychological/psychometric data.</h2><span id='topic+sim'></span><span id='topic+sim.simplex'></span><span id='topic+sim.minor'></span>

<h3>Description</h3>

<p>A number of functions in the psych package will generate simulated data with particular structures. The three documented here are for basic factor analysis simulations.  These functions include
<code><a href="#topic+sim">sim</a></code> for a factor simplex, and <code><a href="#topic+sim.simplex">sim.simplex</a></code> for a data simplex, and <code><a href="#topic+sim.minor">sim.minor</a></code> to simulate major and minor factors.  
</p>
<p>Other simulations aimed at special item structures include  <code><a href="#topic+sim.circ">sim.circ</a></code> for a circumplex structure, <code><a href="#topic+sim.congeneric">sim.congeneric</a></code> for a one factor factor congeneric model, <code><a href="#topic+sim.dichot">sim.dichot</a></code> to simulate dichotomous items, <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> to create a hierarchical factor model, <code><a href="#topic+sim.item">sim.item</a></code> a more general item simulation are documented is separate help files.  They are listed here to help find them.
</p>
<p><code><a href="#topic+simCor">simCor</a></code> to generate sample correlation matrices from a target matrix.
</p>
<p><code><a href="#topic+sim.omega">sim.omega</a></code> to test various examples of omega,
<code><a href="#topic+sim.parallel">sim.parallel</a></code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: 
<code><a href="#topic+sim.rasch">sim.rasch</a></code> to create simulated rasch data, 
<code><a href="#topic+sim.irt">sim.irt</a></code> to create general 1 to 4 parameter IRT data by calling 
<code><a href="#topic+sim.npl">sim.npl</a></code> 1 to 4 parameter logistic IRT or 
<code><a href="#topic+sim.npn">sim.npn</a></code> 1 to 4 paramater normal IRT,
<code><a href="#topic+sim.poly">sim.poly</a></code> to create polytomous ideas by calling
<code><a href="#topic+sim.poly.npn">sim.poly.npn</a></code> 1-4 parameter polytomous normal theory items or
<code><a href="#topic+sim.poly.npl">sim.poly.npl</a></code> 1-4 parameter polytomous logistic items, and 
<code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> which creates data following an ideal point or unfolding model by calling 
<code><a href="#topic+sim.poly.ideal.npn">sim.poly.ideal.npn</a></code> 1-4 parameter polytomous normal theory ideal point model or 
<code><a href="#topic+sim.poly.ideal.npl">sim.poly.ideal.npl</a></code> 1-4 parameter polytomous logistic ideal point model.
</p>
<p><code><a href="#topic+sim.structural">sim.structural</a></code> a general simulation of structural models,  and <code><a href="#topic+sim.anova">sim.anova</a></code> for ANOVA and lm simulations, and <code><a href="#topic+sim.VSS">sim.VSS</a></code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim(fx=NULL,Phi=NULL,fy=NULL,alpha=.8,lambda = 0,n=0,mu=NULL,raw=TRUE,threshold=NULL,
       items = FALSE, low=-2,high=2,cat=5)
sim.simplex(nvar =12, alpha=.8,lambda=0,beta=1,mu=NULL, n=0,threshold=NULL)
sim.minor(nvar=12,nfact=3,n=0,g=NULL,fbig=NULL,fsmall = c(-.2,.2),n.small=NULL, 
    Phi=NULL, bipolar=TRUE, threshold=NULL, 
     items = FALSE, low=-2,high=2,cat=5) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_+3A_fx">fx</code></td>
<td>
<p>The measurement model for x. If NULL, a 4 factor model is generated</p>
</td></tr>
<tr><td><code id="sim_+3A_phi">Phi</code></td>
<td>
<p>The structure matrix of the latent variables</p>
</td></tr>
<tr><td><code id="sim_+3A_fy">fy</code></td>
<td>
<p>The measurement model for y</p>
</td></tr>
<tr><td><code id="sim_+3A_mu">mu</code></td>
<td>
<p>The means structure for the fx factors, defaults to 0,1,... 3. Set to 0 if using thresholds.</p>
</td></tr>
<tr><td><code id="sim_+3A_n">n</code></td>
<td>
<p> Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.</p>
</td></tr>
<tr><td><code id="sim_+3A_raw">raw</code></td>
<td>
<p>if raw=TRUE, raw data are returned as well.</p>
</td></tr>
<tr><td><code id="sim_+3A_threshold">threshold</code></td>
<td>
<p>If raw=TRUE (n &gt;0) and threshold is not NULL, convert the data to binary values, cut at threshold.</p>
</td></tr>
<tr><td><code id="sim_+3A_items">items</code></td>
<td>
<p>TRUE if simulating items, FALSE if simulating scales</p>
</td></tr>
<tr><td><code id="sim_+3A_low">low</code></td>
<td>
<p>Restrict the item difficulties to range from low to high</p>
</td></tr>
<tr><td><code id="sim_+3A_high">high</code></td>
<td>
<p>Restrict the item difficulties to range from low to high</p>
</td></tr>
<tr><td><code id="sim_+3A_cat">cat</code></td>
<td>
<p>Number of categories when creating binary (2) or polytomous items</p>
</td></tr>
<tr><td><code id="sim_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables for a simplex structure</p>
</td></tr>
<tr><td><code id="sim_+3A_nfact">nfact</code></td>
<td>
<p>Number of large factors to simulate in sim.minor,number of group factors in sim.general,sim.omega</p>
</td></tr>
<tr><td><code id="sim_+3A_g">g</code></td>
<td>
<p>General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor</p>
</td></tr>
<tr><td><code id="sim_+3A_alpha">alpha</code></td>
<td>
<p>the base correlation for an autoregressive simplex</p>
</td></tr>
<tr><td><code id="sim_+3A_lambda">lambda</code></td>
<td>
<p>the trait component of a State Trait Autoregressive Simplex</p>
</td></tr>
<tr><td><code id="sim_+3A_beta">beta</code></td>
<td>
<p>Test reliability of a STARS simplex</p>
</td></tr>
<tr><td><code id="sim_+3A_fbig">fbig</code></td>
<td>
<p>Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.</p>
</td></tr>
<tr><td><code id="sim_+3A_bipolar">bipolar</code></td>
<td>
<p>if TRUE, then positive and negative loadings are generated from fbig</p>
</td></tr>
<tr><td><code id="sim_+3A_fsmall">fsmall</code></td>
<td>
<p>nvar/2 small factors are generated with loadings sampled from e.g. (-.2,0,.2)</p>
</td></tr>
<tr><td><code id="sim_+3A_n.small">n.small</code></td>
<td>
<p>If NULL, then generate nvar/2 small factors, else generate n.small small factors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing &ldquo;truth&quot; it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  
</p>
<p>The simulations documented here are the core set of functions.  Others are documented in other help files.
</p>
<p><code><a href="#topic+sim">sim</a></code> simulates one (fx) or two (fx and fy) factor structures where both fx and fy respresent factor loadings of variables.  The use of fy is particularly appropriate for simulating sem models.  This is better documentated in the help for <code><a href="#topic+sim.structural">sim.structural</a></code>.
</p>
<p>The code for <code><a href="#topic+sim">sim</a></code> has been enhanced (10/21/23) to simulate items as well as continuous variables.  This matches the code in <code><a href="#topic+sim.structural">sim.structural</a></code>. 
</p>
<p>Perhaps the easist simulation to understand is just <code><a href="#topic+sim">sim</a></code>.  A factor model (fx) and perhaps fy with intercorrelations between the two factor sets of Phi.  This will produce a correlation matrix R = fx' phi fy.  Factors can differ in their mean values by specifying mu. 
</p>
<p>With the addition of the treshold parameter, <code><a href="#topic+sim">sim</a></code> will generate dichotomous items where values &lt; threshold become 0, greater become 1.  If threshold is a vector less than nvar, then values are sampled from threshold.  If the length of threshold = nvar, then each item is dichotomized at threshold. 
</p>
<p>With the adddition of the items=TRUE option, continuous observed variables are transformed into discrete categories. 
</p>
<p>The default values for <code><a href="#topic+sim.structure">sim.structure</a></code> is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (<code><a href="#topic+sim.simplex">sim.simplex</a></code>) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). 
</p>
<p>Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures <code><a href="#topic+sim.simplex">sim.simplex</a></code> will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  
</p>
<p>An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  <code><a href="#topic+sim.simplex">sim.simplex</a></code> by specifying a non-zero lambda value.
</p>
<p>Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.
For a nice discussion of this situation, see Maccallum, Browne and Cai (2007).  The challenge is thus to identify the major factors. <code><a href="#topic+sim.minor">sim.minor</a></code> generates such structures.  The structures generated can be thought of as having a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix. By default, the  nvar/2 minor factors are given loadings of +/- .2.  If the major factors are very big, this leads to impossible models.  a warning to try again is issued. 
</p>
<p>Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using <code><a href="#topic+sim.general">sim.general</a></code>.
</p>
<p>Although coefficient <code class="reqn">\omega</code> is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as &ldquo;general&rdquo; and  the omega estimate is too large.  This situation may be explored using the <code><a href="#topic+sim.omega">sim.omega</a></code> function with general left as NULL.  If there is a general factor, then results from <code><a href="#topic+sim.omega">sim.omega</a></code> suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. 
</p>
<p>The four irt simulations, <code><a href="#topic+sim.rasch">sim.rasch</a></code>,  <code><a href="#topic+sim.irt">sim.irt</a></code>,  <code><a href="#topic+sim.npl">sim.npl</a></code> and  <code><a href="#topic+sim.npn">sim.npn</a></code> simulate dichotomous items following the Item Response model.   <code><a href="#topic+sim.irt">sim.irt</a></code> just calls either  <code><a href="#topic+sim.npl">sim.npl</a></code> (for logistic models) or  <code><a href="#topic+sim.npn">sim.npn</a></code> (for normal models) depending upon the specification of the model. 
</p>
<p>The logistic model is </p>
<p style="text-align: center;"><code class="reqn">P(i,j) = \gamma + \frac{\zeta-\gamma}{1+ e^{\alpha(\delta-\theta)}}</code>
</p>
<p> where <code class="reqn">\gamma</code> is the lower asymptote or guesssing parameter, <code class="reqn">\zeta</code> is the upper asymptote (normally 1), <code class="reqn">\alpha</code> is item discrimination and <code class="reqn">\delta</code> is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.
</p>
<p>For the 2PL and 2PN models, a = <code class="reqn">\alpha</code> and  d = <code class="reqn">\delta</code> are specified. <br />
For the 3PL or 3PN models, items also differ in their guessing parameter c =<code class="reqn">\gamma</code>. <br />
For the 4PL and 4PN models, the upper asymptote, z= <code class="reqn">\zeta</code> is also specified.  <br />
(Graphics of these may be seen in the demonstrations for the <code><a href="#topic+logistic">logistic</a></code> function.)
</p>
<p>The normal model (irt.npn) calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = <code class="reqn">\alpha</code> parameter = 1.702 in the logistic model the two models are practically identical.
</p>
<p>In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the <code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.
</p>
<p>By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. 
</p>
<p>The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.
</p>
<p>Other simulation functions in psych that are documented separately includ: 
</p>
<p><code><a href="#topic+sim.structure">sim.structure</a></code>  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with <code><a href="#topic+structure.diagram">structure.diagram</a></code> to see the proposed structure.  
</p>
<p><code><a href="#topic+sim.congeneric">sim.congeneric</a></code>   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.
</p>
<p><code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code>  A function to create data with a hierarchical (bifactor) structure.  
</p>
<p><code><a href="#topic+sim.item">sim.item</a></code>      A function to create items that either have a simple structure or a circumplex structure.
</p>
<p><code><a href="#topic+sim.circ">sim.circ</a></code>    Create data with a circumplex structure.
</p>
<p><code><a href="#topic+sim.dichot">sim.dichot</a></code>    Create dichotomous item data with a simple or circumplex structure.
</p>
<p><code><a href="#topic+sim.minor">sim.minor</a></code>   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 &ldquo;minor&quot; factors for n observations.  
</p>
<p>Although the standard factor model assumes that K major factors (K &lt;&lt; nvar) will account for the correlations among the variables
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + U^2</code>
</p>
 
<p>where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that 
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + MM' + U^2</code>
</p>
 
<p>where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  
</p>
<p>Such a correlation matrix will have a poor <code class="reqn">\chi^2</code> value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  
</p>
<p><code><a href="#topic+sim.minor">sim.minor</a></code> will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.
</p>
<p><code><a href="#topic+sim.parallel">sim.parallel</a></code> Create a number of simulated data sets using  <code><a href="#topic+sim.minor">sim.minor</a></code> to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. 
</p>
<p><code><a href="#topic+sim.anova">sim.anova</a></code>    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. 
</p>
<p><code><a href="#topic+sim.multilevel">sim.multilevel</a></code>  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>model</code></td>
<td>
<p>The model based correlation matrix (FF' )</p>
</td></tr> 
<tr><td><code>reliability</code></td>
<td>
<p>Model based reliability</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>Observed (sample based) correlation matrix</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>True scores generating the data</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code>fload</code></td>
<td>
<p>The factor model (F + S) generating the model</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The theta values are &quot;true scores&quot; and may be compared to the factor analytic based factor score estimates of the data.  This is a useful way to understand factor indeterminancy.</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>MacCallum, Robert C. and Browne, Michael W. and Cai, Li (2007)  Factor analysis models as approximations. Factor analysis at 100: Historical developments and future directions. (Cudeck and MacCallum Eds). Lawrence Erlbaum Associates Publishers.
</p>
<p>Revelle, W. (in preparation) An Introduction to Psychometric Theory with applications in R. Springer. at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>  
</p>


<h3>See Also</h3>

<p> See above</p>


<h3>Examples</h3>

<pre><code class='language-R'>simplex &lt;- sim.simplex() #create the default simplex structure
lowerMat(simplex) #the correlation matrix
#create a congeneric matrix
congeneric &lt;- sim.congeneric()
lowerMat(congeneric)
R &lt;- sim.hierarchical()
lowerMat(R)
#now simulate categorical items with the hierarchical factor structure.  
#Let the items be dichotomous with varying item difficulties.
marginals = matrix(c(seq(.1,.9,.1),seq(.9,.1,-.1)),byrow=TRUE,nrow=2)
X &lt;- sim.poly.mat(R=R,m=marginals,n=1000)
lowerCor(X) #show the raw correlations
#lowerMat(tetrachoric(X)$rho) # show the tetrachoric correlations (not run)
#generate a structure 
fx &lt;- matrix(c(.9,.8,.7,rep(0,6),c(.8,.7,.6)),ncol=2)
fy &lt;- c(.6,.5,.4)
Phi &lt;- matrix(c(1,0,.5,0,1,.4,0,0,0),ncol=3)
R &lt;- sim.structure(fx,Phi,fy) 
cor.plot(R$model) #show it graphically

simp &lt;- sim.simplex()
#show the simplex structure using cor.plot
cor.plot(simp,colors=TRUE,main="A simplex structure")
#Show a STARS model 
simp &lt;- sim.simplex(alpha=.8,lambda=.4)
#show the simplex structure using cor.plot
cor.plot(simp,colors=TRUE,main="State Trait Auto Regressive Simplex" )

dichot.sim &lt;- sim.irt()  #simulate 5 dichotomous items
poly.sim &lt;- sim.poly(theta=dichot.sim$theta)  #simulate 5 polytomous items that correlate 
  #with the dichotomous items

</code></pre>

<hr>
<h2 id='sim.anova'>Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures.  </h2><span id='topic+sim.anova'></span>

<h3>Description</h3>

<p>For teaching basic statistics, it is useful to be able to generate examples suitable for analysis of variance or simple linear models.  sim.anova will generate the design matrix of three independent variables (IV1, IV2, IV3) with an arbitrary number of levels and effect sizes for each main effect and interaction.  IVs can be either continuous or categorical and can have linear or quadratic effects. Either a single dependent variable or multiple (within subject) dependent variables are generated according to the specified model. The repeated measures are assumed to be tau equivalent with a specified reliability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.anova(es1 = 0, es2 = 0, es3 = 0, es12 = 0, es13 = 0,
    es23 = 0, es123 = 0, es11=0,es22=0, es33=0,n = 2,n1 = 2, n2 = 2, n3 = 2, 
    within=NULL,r=.8,factors=TRUE,center = TRUE,std=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.anova_+3A_es1">es1</code></td>
<td>
<p>Effect size of IV1</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es2">es2</code></td>
<td>
<p>Effect size of IV2</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es3">es3</code></td>
<td>
<p>Effect size of IV3</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es12">es12</code></td>
<td>
<p>Effect size of the IV1 x IV2 interaction</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es13">es13</code></td>
<td>
<p>Effect size of the IV1 x IV3 interaction</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es23">es23</code></td>
<td>
<p>Effect size of the IV2 x IV3 interaction</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es123">es123</code></td>
<td>
<p>Effect size of the IV1 x IV2 * IV3  interaction</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es11">es11</code></td>
<td>
<p>Effect size of the quadratric term of IV1</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es22">es22</code></td>
<td>
<p>Effect size of the quadratric term of IV2</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_es33">es33</code></td>
<td>
<p>Effect size of the quadratric term of IV3</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_n">n</code></td>
<td>
<p>Sample size per cell (if all variables are categorical) or (if at least one variable is continuous), the total sample size</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_n1">n1</code></td>
<td>
<p>Number of levels of IV1 (0) if continuous</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_n2">n2</code></td>
<td>
<p>Number of levels of IV2</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_n3">n3</code></td>
<td>
<p>Number of levels of IV3</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_within">within</code></td>
<td>
<p>if not NULL, then within should be a vector of the means of  any repeated measures.</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_r">r</code></td>
<td>
<p>the correlation between the repeated measures (if they exist).  This can be thought of as the reliablility of the measures.</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_factors">factors</code></td>
<td>
<p>report the IVs as factors rather than numeric</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_center">center</code></td>
<td>
<p>center=TRUE provides orthogonal contrasts, center=FALSE adds the minimum value + 1 to all contrasts</p>
</td></tr>
<tr><td><code id="sim.anova_+3A_std">std</code></td>
<td>
<p>Standardize the effect sizes by standardizing the IVs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple simulation for teaching about ANOVA, regression and reliability.  A variety of demonstrations of the relation between anova and lm can be shown.
</p>
<p>The default is to produce categorical IVs (factors).  For more than two levels of an IV, this will show the difference between the linear model and anova in terms of the comparisons made.
</p>
<p>The within vector can be used to add congenerically equivalent dependent variables. These will have intercorrelations (reliabilities) of r and means as specified as values of within.
</p>
<p>To demonstrate the effect of centered versus non-centering, make factors = center=FALSE. The default is to center the IVs. By not centering them, the lower order effects will  be incorrect given the higher order interaction terms. 
</p>


<h3>Value</h3>

<p>y.df is a data.frame of the 3 IV values as well as the DV values. 
</p>
<table>
<tr><td><code>IV1 ... IV3</code></td>
<td>
<p>Independent variables 1 ... 3</p>
</td></tr>
<tr><td><code>DV</code></td>
<td>
<p>If there is a single dependent variable</p>
</td></tr>
<tr><td><code>DV.1 ... DV.n</code></td>
<td>
<p>If within is specified, then the n within subject dependent variables</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p> The general set of simulation functions in the psych package <code><a href="#topic+sim">sim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
data.df &lt;- sim.anova(es1=1,es2=-.5,es13=1)  # two main effect and one interaction
psych::describe(data.df)
pairs.panels(data.df)   #show how the design variables are orthogonal
#
data.df &lt;- char2numeric(data.df,flag=FALSE)

summary(lm(DV~IV1*IV2*IV3,data=data.df))

summary(aov(DV~IV1*IV2*IV3,data=data.df))
lmCor(DV~IV1*IV2*IV3,data=data.df, std=FALSE)
set.seed(42)
 #demonstrate the effect of not centering the data on the regression
data.df &lt;- sim.anova(es1=1,es2=.5,es13=1,center=FALSE)  #
psych::describe(data.df)
#
#this one is incorrect, because the IVs are not centered
data.df &lt;- char2numeric(data.df,flag=FALSE)
summary(lm(DV~IV1*IV2*IV3,data=data.df)) 
data.df &lt;- char2numeric(data.df,flag=FALSE)

summary(aov(DV~IV1*IV2*IV3,data=data.df)) #compare with the lm model
#but lmCor by default zero centers which works
lmCor(DV~IV1*IV2*IV3,data=data.df)
#now examine multiple levels and quadratic terms
set.seed(42)
data.df &lt;- sim.anova(es1=1,es13=1,n2=3,n3=4,es22=1)
summary(lm(DV~IV1*IV2*IV3,data=data.df))
summary(aov(DV~IV1*IV2*IV3,data=data.df))
pairs.panels(data.df)
#
data.df &lt;- sim.anova(es1=1,es2=-.5,within=c(-1,0,1),n=10)
pairs.panels(data.df)

</code></pre>

<hr>
<h2 id='sim.congeneric'> Simulate a congeneric data set with or without minor factors </h2><span id='topic+congeneric.sim'></span><span id='topic+sim.congeneric'></span><span id='topic+make.congeneric'></span>

<h3>Description</h3>

<p>Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error.  Parallel tests are the special case where (usually two) tests have equal factor loadings.  Tau equivalent tests have equal factor loadings but may have unequal errors.  Congeneric tests may differ in both factor loading and error variances.  Minor factors may be added as systematic but trivial disturbances 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.congeneric(loads = c(0.8, 0.7, 0.6, 0.5),N = NULL,  err=NULL, short = TRUE, 
              categorical=FALSE, low=-3,high=3,cuts=NULL,minor=FALSE,fsmall = c(-.2,.2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.congeneric_+3A_n">N</code></td>
<td>
<p>How many subjects to simulate. If NULL, return the population model </p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_loads">loads</code></td>
<td>
<p> A vector of factor loadings for the tests  </p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_err">err</code></td>
<td>
<p>A vector of error variances &ndash; if NULL then error = 1 - loading 2</p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_short">short</code></td>
<td>
<p>short=TRUE: Just give the test correlations, short=FALSE, report observed test scores as well as the implied pattern matrix</p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_categorical">categorical</code></td>
<td>
<p> continuous or categorical (discrete) variables.  </p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_low">low</code></td>
<td>
<p> values less than low are forced to low </p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_high">high</code></td>
<td>
<p> values greater than high are forced to high  </p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_cuts">cuts</code></td>
<td>
<p>If specified, and categorical = TRUE, will cut the resulting continuous output at the value of cuts</p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_minor">minor</code></td>
<td>
<p>Should n/2 minor factors be added (see Maccallum and Tucker, 1991)</p>
</td></tr>
<tr><td><code id="sim.congeneric_+3A_fsmall">fsmall</code></td>
<td>
<p>nvar/2 small factors are generated with loadings sampled from fsmall e.g. (-.2,0,.2)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When constructing examples for reliability analysis, it is convenient to simulate congeneric data structures.  These are the most simple of item structures, having just one factor. Mainly used for a discussion of reliability theory as well as factor score estimates. 
</p>
<p>Maccallum and Tucker (1991) suggest that factor models should include minor factors, that at not random error but unspecifed by the basic model.  This option has been added in November, 2022.
</p>
<p>The implied covariance matrix is just pattern %*% t(pattern). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>model</code></td>
<td>
<p>The implied population correlation matrix if N=NULL or short=FALSE, otherwise the sample correlation matrix</p>
</td></tr>
<tr><td><code>pattern</code></td>
<td>
<p>The pattern matrix implied by the loadings and error variances</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The sample correlation matrix for long output</p>
</td></tr>
<tr><td><code>observed</code></td>
<td>
<p>a matrix of test scores for n tests</p>
</td></tr>
<tr><td><code>latent</code></td>
<td>
<p>The latent trait and error scores </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Revelle, W. (in prep) An introduction to psychometric theory with applications in R. To be published by Springer.  (working draft available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>MacCallum, R. C., &amp; Tucker, L. R. (1991). Representing sources of error in the 
common-factormodel: Implications for theory and practice. Psychological Bulletin, 109(3), 502-511.</p>


<h3>See Also</h3>

  <p><code><a href="#topic+item.sim">item.sim</a></code> for other simulations, <code><a href="#topic+fa">fa</a></code> for an example of factor scores, <code><a href="#topic+irt.fa">irt.fa</a></code> and <code><a href="#topic+polychoric">polychoric</a></code> for the treatment of item data with discrete values.</p>


<h3>Examples</h3>

<pre><code class='language-R'>test &lt;- sim.congeneric(c(.9,.8,.7,.6))   #just the population matrix
test &lt;- sim.congeneric(c(.9,.8,.7,.6),N=100)   # a sample correlation matrix
test &lt;- sim.congeneric(short=FALSE, N=100)
round(cor(test$observed),2) # show  a congeneric correlation matrix
f1=fa(test$observed,scores=TRUE)
round(cor(f1$scores,test$latent),2)  
     #factor score estimates are correlated with but not equal to the factor scores
set.seed(42)
#500 responses to 4 discrete items
items &lt;- sim.congeneric(N=500,short=FALSE,low=-2,high=2,categorical=TRUE) 
d4 &lt;- irt.fa(items$observed)  #item response analysis of congeneric measures



</code></pre>

<hr>
<h2 id='sim.hierarchical'>Create a population or sample correlation matrix, perhaps with hierarchical structure. </h2><span id='topic+sim.hierarchical'></span><span id='topic+make.hierarchical'></span><span id='topic+sim.bonds'></span>

<h3>Description</h3>

<p>Create a population orthogonal or hierarchical correlation matrix from a set of factor loadings and factor intercorrelations. Samples of size n may be then be drawn from this population.  Return either the sample data, sample correlations, or population correlations.  This is used to create sample data sets for instruction and demonstration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.hierarchical(gload=NULL, fload=NULL, n = 0, raw = TRUE,mu = NULL,
    categorical=FALSE, low=-3,high=3,threshold=NULL)
sim.bonds(nvar=9,nf=NULL, loads=c(0,0,.5,.6),validity=.8)

make.hierarchical(gload=NULL, fload=NULL, n = 0, raw = FALSE)  #deprecated

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.hierarchical_+3A_gload">gload</code></td>
<td>
<p> Loadings of group factors on a general factor. Defaults to c(.9,.8,.7)  </p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_fload">fload</code></td>
<td>
<p> Loadings of items on the group factors. Defaults to matrix(c(.8,.7,.6,rep(0,9),.7,.6,.5,rep(0,9),.6,.5,.4),   ncol=3)</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_n">n</code></td>
<td>
<p> Number of subjects to generate: N=0 =&gt; population values </p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_raw">raw</code></td>
<td>
<p> raw=TRUE, report the raw data, raw=FALSE, report the sample  correlation matrix. </p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_mu">mu</code></td>
<td>
<p>means for the individual variables</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_low">low</code></td>
<td>
<p>lower cutoff for categorical data</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_categorical">categorical</code></td>
<td>
<p>If True, then create categorical data</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_threshold">threshold</code></td>
<td>
<p>If categorical is TRUE, and binary output is desired, what is the threshold to convert continuous scores into 0/1.  May be a vector.</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_high">high</code></td>
<td>
<p>Upper cuttoff for categorical data</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables to simulate</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_loads">loads</code></td>
<td>
<p>A vector of loadings that will be sampled (rowwise) to define the factors</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_validity">validity</code></td>
<td>
<p>The factor loadings of &lsquo;pure&rsquo; measures of the factor.</p>
</td></tr>
<tr><td><code id="sim.hierarchical_+3A_nf">nf</code></td>
<td>
<p>Number of factors to generate in sim.bonds</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many personality and cognitive tests have a hierarchical factor structure.  For demonstration purposes, it is useful to be able to create such matrices, either with population values, or sample values. 
</p>
<p>Given a matrix of item factor loadings (fload) and of loadings of these factors on a general factor (gload), we create a population correlation matrix by using the general factor law  <code class="reqn"> R \approx F' \theta F + U^2</code> where <code class="reqn">\theta = g'g</code>  
</p>
<p>The default is to return population correlation matrices. Sample correlation matrices are generated if n &gt; 0.  Raw data are returned if raw = TRUE.
</p>
<p>The default values for gload and fload create a data matrix discussed by Jensen and Weng, 1994.
</p>
<p>In order to properly simulate polytomous items, the categorical option will round continuous scores into integers.  To simulate dichotomous items, the threshold vector may be used to specify the value at which the continuous values are cut into 0/1.  If the length of threshold is less than the number of variables, the vector will be sampled (with replacement) to fill it out.
</p>
<p>Although written to create hierarchical structures, if the gload matrix is all 0, then a non-hierarchical structure will be generated.
</p>
<p>Yet another model is that of Godfrey H. Thomson (1916) who suggested that independent bonds could produce the same factor structure as a g factor model. This is simulated in <code><a href="#topic+sim.bonds">sim.bonds</a></code>.  Compare the <code><a href="#topic+omega">omega</a></code> solutions for a <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> with a <code><a href="#topic+sim.bonds">sim.bonds</a></code> model. Both produce reasonable values of omega, although the one was generated without a general factor.
</p>


<h3>Value</h3>

<table>
<tr><td><code>r</code></td>
<td>
<p>a matrix of correlations</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>The population correlation matrix</p>
</td></tr>
<tr><td><code>observed</code></td>
<td>
<p>The simulated data matrix with the defined structure</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>The latent factor scores used to generate the data. Compare how these correlate with the observed data  with the results from <code><a href="#topic+omega">omega</a></code>.</p>
</td></tr>
<tr><td><code>sl</code></td>
<td>
<p>The Schmid Leiman transformed factor loadings.  These may be used to test factor scoring problem. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.omega.html">https://personality-project.org/r/r.omega.html</a>
<br /> Jensen, A.R., Weng, L.J. (1994) What is a Good g? Intelligence, 18, 231-258.
</p>
<p>Godfrey H. Thomson (1916) A hierarchy without a general factor, British Journal of Psychology, 8, 271-281. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+omega">omega</a></code>,   <code><a href="#topic+schmid">schmid</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>,  <code><a href="#topic+VSS">VSS</a></code> for ways of analyzing these data.  Also see <code><a href="#topic+sim.structure">sim.structure</a></code> to simulate a variety of structural models (e.g., multiple correlated factor models).  </p>


<h3>Examples</h3>

<pre><code class='language-R'>
gload &lt;-  gload&lt;-matrix(c(.9,.8,.7),nrow=3)    # a higher order factor matrix
fload &lt;-matrix(c(                    #a lower order (oblique) factor matrix
           .8,0,0,
           .7,0,.0,
           .6,0,.0,
            0,.7,.0,
            0,.6,.0,
            0,.5,0,
            0,0,.6,
            0,0,.5,
            0,0,.4),   ncol=3,byrow=TRUE)
            
jensen &lt;- sim.hierarchical(gload,fload)    #the test set used by omega
round(jensen,2)
set.seed(42) #for reproducible results
jensen &lt;-  sim.hierarchical(n=10000,categorical =TRUE, threshold =c(-1,0,1)) 
           #use the same gload and fload values, but produce the data  
#items have three levels of difficulty
#Compare factor scores using the sl model with those that generated the data
lowerCor(jensen$theta) #the correlations of the factors
fs &lt;- factor.scores(jensen$observed, jensen$sl)  #find factor scores from the data
lowerCor(fs$scores) #these are now correlated
cor2(fs$scores,jensen$theta)  #correlation with the generating factors 


#compare this to a simulation of the bonds model
set.seed(42)
R &lt;- sim.bonds()
R$R    

#simulate a non-hierarchical structure
fload &lt;- matrix(c(c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),rep(0,20)),
    c(c(.9,.8,.7,.6),rep(0,20)),c(c(c(.9,.8,.7,.6),rep(0,20)),c(.9,.8,.7,.6))),ncol=5)
gload &lt;- matrix(rep(0,5))
five.factor &lt;- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
#do it again with a hierachical structure
gload &lt;- matrix(rep(.7,5)  )
five.factor.g &lt;- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
#compare these two with omega
#not run
#om.5 &lt;- omega(five.factor$observed,5)
#om.5g &lt;- omega(five.factor.g$observed,5)
</code></pre>

<hr>
<h2 id='sim.irt'>Functions to simulate psychological/psychometric data.</h2><span id='topic+sim.irt'></span><span id='topic+sim.rasch'></span><span id='topic+sim.npl'></span><span id='topic+sim.npn'></span><span id='topic+sim.poly'></span><span id='topic+sim.poly.npl'></span><span id='topic+sim.poly.npn'></span><span id='topic+sim.poly.ideal'></span><span id='topic+sim.poly.ideal.npl'></span><span id='topic+sim.poly.ideal.npn'></span><span id='topic+sim.poly.mat'></span>

<h3>Description</h3>

<p>A number of functions in the psych package will generate simulated data with particular structures.  These functions include
<code><a href="#topic+sim">sim</a></code> for a factor simplex, and <code><a href="#topic+sim.simplex">sim.simplex</a></code> for a data simplex, <code><a href="#topic+sim.circ">sim.circ</a></code> for a circumplex structure, <code><a href="#topic+sim.congeneric">sim.congeneric</a></code> for a one factor factor congeneric model, <code><a href="#topic+sim.dichot">sim.dichot</a></code> to simulate dichotomous items, <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> to create a hierarchical factor model, <code><a href="#topic+sim.item">sim.item</a></code> a more general item simulation,
<code><a href="#topic+sim.minor">sim.minor</a></code> to simulate major and minor factors,
<code><a href="#topic+sim.omega">sim.omega</a></code> to test various examples of omega,
<code><a href="#topic+sim.parallel">sim.parallel</a></code> to compare the efficiency of various ways of deterimining the number of factors,
<code><a href="#topic+sim.rasch">sim.rasch</a></code> to create simulated rasch data, 
<code><a href="#topic+sim.irt">sim.irt</a></code> to create general 1 to 4 parameter IRT data by calling 
<code><a href="#topic+sim.npl">sim.npl</a></code> 1 to 4 parameter logistic IRT or 
<code><a href="#topic+sim.npn">sim.npn</a></code> 1 to 4 paramater normal IRT,
<code><a href="#topic+sim.poly">sim.poly</a></code> to create polytomous ideas by calling
<code><a href="#topic+sim.poly.npn">sim.poly.npn</a></code> 1-4 parameter polytomous normal theory items or
<code><a href="#topic+sim.poly.npl">sim.poly.npl</a></code> 1-4 parameter polytomous logistic items, and 
<code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> which creates data following an ideal point or unfolding model by calling 
<code><a href="#topic+sim.poly.ideal.npn">sim.poly.ideal.npn</a></code> 1-4 parameter polytomous normal theory ideal point model or 
<code><a href="#topic+sim.poly.ideal.npl">sim.poly.ideal.npl</a></code> 1-4 parameter polytomous logistic ideal point model.
</p>
<p><code><a href="#topic+sim.structural">sim.structural</a></code> a general simulation of structural models,  and <code><a href="#topic+sim.anova">sim.anova</a></code> for ANOVA and lm simulations, and <code><a href="#topic+sim.VSS">sim.VSS</a></code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
sim.rasch(nvar = 5,n = 500, low=-3,high=3,d=NULL, a=1,mu=0,sd=1)
sim.irt(nvar = 5, n = 500, low=-3, high=3, a=NULL,c=0, z=1,d=NULL, mu=0,sd=1,  
  mod="logistic",theta=NULL)
sim.npl(nvar = 5, n = 500, low=-3,high=3,a=NULL,c=0,z=1,d=NULL,mu=0,sd=1,theta=NULL)
sim.npn(nvar = 5, n = 500, low=-3,high=3,a=NULL,c=0,z=1,d=NULL,mu=0,sd=1,theta=NULL)
sim.poly(nvar = 5 ,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL, 
    mu=0,sd=1,cat=5,mod="logistic",theta=NULL) 
sim.poly.npn(nvar = 5 ,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL,   mu=0, sd=1, 
     cat=5,theta=NULL) 
sim.poly.npl(nvar = 5 ,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL,  mu=0, sd=1, 
cat=5,theta=NULL) 
sim.poly.ideal(nvar = 5 ,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL, 
   mu=0,sd=1,cat=5,mod="logistic") 
sim.poly.ideal.npn(nvar = 5,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL, mu=0,sd=1,cat=5) 
sim.poly.ideal.npl(nvar = 5,n = 500,low=-2,high=2,a=NULL,c=0,z=1,d=NULL, 
      mu=0,sd=1,cat=5,theta=NULL)
sim.poly.mat(R,m,n) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.irt_+3A_n">n</code></td>
<td>
<p>Number of cases to simulate</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_mu">mu</code></td>
<td>
<p>The means for the items (if not 0)</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables for a simplex structure</p>
</td></tr> 
<tr><td><code id="sim.irt_+3A_low">low</code></td>
<td>
<p>lower difficulty for sim.rasch or sim.irt</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_high">high</code></td>
<td>
<p>higher difficulty for sim.rasch or sim.irt</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_a">a</code></td>
<td>
<p>if not specified as a vector, the descrimination parameter a = <code class="reqn">\alpha</code> will be set to 1.0 for all items</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_d">d</code></td>
<td>
<p> if not specified as a vector, item difficulties (d = <code class="reqn">\delta</code>) will range from low to high</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_c">c</code></td>
<td>
<p>the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_z">z</code></td>
<td>
<p>the zeta parameter: if not specified as a vector, set to 1</p>
</td></tr> 
<tr><td><code id="sim.irt_+3A_sd">sd</code></td>
<td>
<p>the standard deviation for the underlying latent variable in the irt simulations</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_mod">mod</code></td>
<td>
<p>which IRT model to use, mod=&quot;logistic&quot; simulates a logistic function, otherwise, a normal function</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_cat">cat</code></td>
<td>
<p>Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_theta">theta</code></td>
<td>
<p>The underlying latent trait value for each simulated subject</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_r">R</code></td>
<td>
<p>A correlation matrix to be simulated using the sim.poly.mat function</p>
</td></tr>
<tr><td><code id="sim.irt_+3A_m">m</code></td>
<td>
<p>The matrix of marginals for all the items</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing &ldquo;truth&quot; it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  
</p>
<p>The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.
</p>
<p>The default values for <code><a href="#topic+sim.structure">sim.structure</a></code> is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (<code><a href="#topic+sim.simplex">sim.simplex</a></code>) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). 
</p>
<p>Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures <code><a href="#topic+sim.simplex">sim.simplex</a></code> will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  
</p>
<p>An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  <code><a href="#topic+sim.simplex">sim.simplex</a></code> by specifying a non-zero lambda value.
</p>
<p>Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. <code><a href="#topic+sim.minor">sim.minor</a></code> generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.
</p>
<p>Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using <code><a href="#topic+sim.general">sim.general</a></code>.
</p>
<p>Although coefficient <code class="reqn">\omega</code> is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as &ldquo;general&rdquo; and  the omega estimate is too large.  This situation may be explored using the <code><a href="#topic+sim.omega">sim.omega</a></code> function with general left as NULL.  If there is a general factor, then results from <code><a href="#topic+sim.omega">sim.omega</a></code> suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. 
</p>
<p>The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. 
</p>
<p>The logistic model is </p>
<p style="text-align: center;"><code class="reqn">P(i,j) = \gamma + \frac{\zeta-\gamma}{1+ e^{\alpha(\delta-\theta)}}</code>
</p>
<p> where <code class="reqn">\gamma</code> is the lower asymptote or guesssing parameter, <code class="reqn">\zeta</code> is the upper asymptote (normally 1), <code class="reqn">\alpha</code> is item discrimination and <code class="reqn">\delta</code> is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.
</p>
<p>For the 2PL and 2PN models, a = <code class="reqn">\alpha</code> and  d = <code class="reqn">\delta</code> are specified. <br />
For the 3PL or 3PN models, items also differ in their guessing parameter c =<code class="reqn">\gamma</code>. <br />
For the 4PL and 4PN models, the upper asymptote, z= <code class="reqn">\zeta</code> is also specified.  <br />
(Graphics of these may be seen in the demonstrations for the <code><a href="#topic+logistic">logistic</a></code> function.)
</p>
<p>The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = <code class="reqn">\alpha</code> parameter = 1.702 in the logistic model the two models are practically identical.
</p>
<p>In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the <code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.
</p>
<p>By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. 
</p>
<p>The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.
</p>
<p>Other simulation functions in psych are:
</p>
<p><code><a href="#topic+sim.structure">sim.structure</a></code>  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with <code><a href="#topic+structure.diagram">structure.diagram</a></code> to see the proposed structure.  
</p>
<p><code><a href="#topic+sim.congeneric">sim.congeneric</a></code>   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.
</p>
<p><code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code>  A function to create data with a hierarchical (bifactor) structure.  
</p>
<p><code><a href="#topic+sim.item">sim.item</a></code>      A function to create items that either have a simple structure or a circumplex structure.
</p>
<p><code><a href="#topic+sim.circ">sim.circ</a></code>    Create data with a circumplex structure.
</p>
<p><code><a href="#topic+sim.dichot">sim.dichot</a></code>    Create dichotomous item data with a simple or circumplex structure.
</p>
<p><code><a href="#topic+sim.minor">sim.minor</a></code>   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 &ldquo;minor&quot; factors for n observations.  
</p>
<p>Although the standard factor model assumes that K major factors (K &lt;&lt; nvar) will account for the correlations among the variables
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + U^2</code>
</p>
 
<p>where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that 
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + MM' + U^2</code>
</p>
 
<p>where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  
</p>
<p>Such a correlation matrix will have a poor <code class="reqn">\chi^2</code> value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  
</p>
<p>sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.
</p>
<p><code><a href="#topic+sim.parallel">sim.parallel</a></code> Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. 
</p>
<p><code><a href="#topic+sim.anova">sim.anova</a></code>    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. 
</p>
<p><code><a href="#topic+sim.multilevel">sim.multilevel</a></code>  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, W. (in preparation) An Introduction to Psychometric Theory with applications in R. Springer. at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>  </p>


<h3>See Also</h3>

<p> See above</p>


<h3>Examples</h3>

<pre><code class='language-R'>simplex &lt;- sim.simplex() #create the default simplex structure
lowerMat(simplex) #the correlation matrix
#create a congeneric matrix
congeneric &lt;- sim.congeneric()
lowerMat(congeneric)
R &lt;- sim.hierarchical()
lowerMat(R)
#now simulate categorical items with the hierarchical factor structure.  
#Let the items be dichotomous with varying item difficulties.
marginals = matrix(c(seq(.1,.9,.1),seq(.9,.1,-.1)),byrow=TRUE,nrow=2)
X &lt;- sim.poly.mat(R=R,m=marginals,n=1000)
lowerCor(X) #show the raw correlations
#lowerMat(tetrachoric(X)$rho) # show the tetrachoric correlations (not run)
#generate a structure 
fx &lt;- matrix(c(.9,.8,.7,rep(0,6),c(.8,.7,.6)),ncol=2)
fy &lt;- c(.6,.5,.4)
Phi &lt;- matrix(c(1,0,.5,0,1,.4,0,0,0),ncol=3)
R &lt;- sim.structure(fx,Phi,fy) 
cor.plot(R$model) #show it graphically

simp &lt;- sim.simplex()
#show the simplex structure using cor.plot
cor.plot(simp,colors=TRUE,main="A simplex structure")
#Show a STARS model 
simp &lt;- sim.simplex(alpha=.8,lambda=.4)
#show the simplex structure using cor.plot
cor.plot(simp,colors=TRUE,main="State Trait Auto Regressive Simplex" )

dichot.sim &lt;- sim.irt()  #simulate 5 dichotomous items
poly.sim &lt;- sim.poly(theta=dichot.sim$theta)  #simulate 5 polytomous items that correlate 
  #with the dichotomous items

</code></pre>

<hr>
<h2 id='sim.item'>Generate simulated data structures for circumplex, spherical, or simple structure </h2><span id='topic+sim.spherical'></span><span id='topic+item.sim'></span><span id='topic+sim.item'></span><span id='topic+sim.dichot'></span><span id='topic+item.dichot'></span><span id='topic+sim.circ'></span><span id='topic+circ.sim'></span><span id='topic+con2cat'></span>

<h3>Description</h3>

<p>Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.item(nvar = 72, nsub = 500, circum = FALSE, xloading = 0.6, yloading = 0.6, 
 gloading = 0, xbias = 0, ybias = 0, categorical = FALSE, low = -3, high = 3, 
 truncate = FALSE, threshold=NULL)
sim.circ(nvar = 72, nsub = 500, circum = TRUE, xloading = 0.6, yloading = 0.6, 
  gloading = 0, xbias = 0, ybias = 0, categorical = FALSE, low = -3, high = 3, 
  truncate = FALSE, cutpoint = 0)
sim.dichot(nvar = 72, nsub = 500, circum = FALSE, xloading = 0.6, yloading = 0.6, 
    gloading = 0, xbias = 0, ybias = 0, low = 0, high = 0) 
item.dichot(nvar = 72, nsub = 500, circum = FALSE, xloading = 0.6, yloading = 0.6,
  gloading = 0, xbias = 0, ybias = 0, low = 0, high = 0) 
sim.spherical(simple=FALSE, nx=7,ny=12 ,nsub = 500,  xloading =.55, yloading = .55,
   zloading=.55, gloading=0, xbias=0,  ybias = 0, zbias=0,categorical=FALSE, 
   low=-3,high=3,truncate=FALSE, threshold=NULL) 
con2cat(old,cuts=c(0,1,2,3),where)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.item_+3A_nvar">nvar</code></td>
<td>
<p> Number of variables to simulate </p>
</td></tr>
<tr><td><code id="sim.item_+3A_nsub">nsub</code></td>
<td>
<p>Number of subjects to simulate </p>
</td></tr>
<tr><td><code id="sim.item_+3A_circum">circum</code></td>
<td>
<p> circum=TRUE is circumplex structure, FALSE is simple structure</p>
</td></tr>
<tr><td><code id="sim.item_+3A_simple">simple</code></td>
<td>
<p>simple structure or spherical structure in sim.spherical</p>
</td></tr>
<tr><td><code id="sim.item_+3A_xloading">xloading</code></td>
<td>
<p>the average loading on the first dimension </p>
</td></tr>
<tr><td><code id="sim.item_+3A_yloading">yloading</code></td>
<td>
<p>Average loading on the second dimension </p>
</td></tr>
<tr><td><code id="sim.item_+3A_zloading">zloading</code></td>
<td>
<p>the average loading on the third dimension in sim.spherical</p>
</td></tr>
<tr><td><code id="sim.item_+3A_gloading">gloading</code></td>
<td>
<p>Average loading on a general factor (default=0)</p>
</td></tr>
<tr><td><code id="sim.item_+3A_xbias">xbias</code></td>
<td>
<p>To introduce skew, how far off center is the first dimension </p>
</td></tr>
<tr><td><code id="sim.item_+3A_ybias">ybias</code></td>
<td>
<p>To introduce skew on the second dimension</p>
</td></tr>
<tr><td><code id="sim.item_+3A_zbias">zbias</code></td>
<td>
<p>To introduce skew on the third dimension &ndash; if using sim.spherical</p>
</td></tr>
<tr><td><code id="sim.item_+3A_categorical">categorical</code></td>
<td>
<p> continuous or categorical variables.  </p>
</td></tr>
<tr><td><code id="sim.item_+3A_low">low</code></td>
<td>
<p> values less than low are forced to low (or 0 in item.dichot)</p>
</td></tr>
<tr><td><code id="sim.item_+3A_high">high</code></td>
<td>
<p> values greater than high are forced to high (or 1 in item.dichot) </p>
</td></tr>
<tr><td><code id="sim.item_+3A_cutpoint">cutpoint</code></td>
<td>
<p>cut items at cutpoint (see threshold)</p>
</td></tr>
<tr><td><code id="sim.item_+3A_truncate">truncate</code></td>
<td>
<p>Change all values less than cutpoint to cutpoint. </p>
</td></tr>
<tr><td><code id="sim.item_+3A_threshold">threshold</code></td>
<td>
<p>A vector of cutpoints to conver continuous items to binary </p>
</td></tr>
<tr><td><code id="sim.item_+3A_nx">nx</code></td>
<td>
<p>number of variables for the first factor in sim.spherical</p>
</td></tr></table>
<p>y  </p>
<table>
<tr><td><code id="sim.item_+3A_ny">ny</code></td>
<td>
<p>number of variables for the second and third factors in sim.spherical</p>
</td></tr>
<tr><td><code id="sim.item_+3A_old">old</code></td>
<td>
<p>a matrix or data frame</p>
</td></tr>
<tr><td><code id="sim.item_+3A_cuts">cuts</code></td>
<td>
<p>Values of old to be used as cut points when converting continuous values to categorical values</p>
</td></tr>
<tr><td><code id="sim.item_+3A_where">where</code></td>
<td>
<p>Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1). 
</p>
<p>With the addition of a threshold parameter (replacing the previous cut parameter), each item will converted to a binary (0/1) value if the theta exceeds the threshold.  If threshold is a vector with length less than nvar, then it will be filled out to length nvar by sampling with replacement. 
</p>
<p>The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). 
</p>
<p>It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an <code><a href="#topic+omega">omega</a></code> analysis do not capture the underlying structure.  See the last example.
</p>
<p>Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. <code><a href="#topic+sim.spherical">sim.spherical</a></code> produces such data.
</p>


<h3>Value</h3>

<p>A data matrix of (nsub) subjects by (nvar) variables.  
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p> Variations of a routine used in Rafaeli and Revelle, 2006;
Rafaeli, E. &amp; Revelle, W. (2006). A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion. <a href="https://personality-project.org/revelle/publications/rafaeli.revelle.06.pdf">https://personality-project.org/revelle/publications/rafaeli.revelle.06.pdf</a>
</p>
<p>Acton, G. S. and Revelle, W. (2004) Evaluation of Ten Psychometric Criteria for Circumplex Structure. Methods of Psychological Research Online, Vol. 9, No. 1 (formerly (https://www.dgps.de/fachgruppen/methoden/mpr-online/issue22/mpr110_10.pdf)  also at  <a href="https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf">https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf</a> </p>


<h3>See Also</h3>

<p>  See also the implementation in this series of functions to generate numerous data structures.  <code><a href="#topic+simCor">simCor</a></code>, <code><a href="#topic+simulation.circ">simulation.circ</a></code>,  <code><a href="#topic+circ.tests">circ.tests</a></code> as well as other simulations ( <code><a href="#topic+sim.structural">sim.structural</a></code>  <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>
round(cor(circ.sim(nvar=8,nsub=200)),2)
plot(fa(circ.sim(16,500),2)$loadings,main="Circumplex Structure") #circumplex structure
#
#
plot(fa(item.sim(16,500),2)$loadings,main="Simple Structure") #simple structure
#
cluster.plot(fa(item.dichot(16,low=0,high=1),2))

 set.seed(42)
 
 data &lt;- mnormt::rmnorm(1000, c(0, 0), matrix(c(1, .5, .5, 1), 2, 2)) #continuous data
 new &lt;- con2cat(data,c(-1.5,-.5,.5,1.5))  #discreet data
 polychoric(new)
#not run
#x12 &lt;- sim.item(12,gloading=.6)
#f3 &lt;- fa(x12,3,rotate="none")
#f3  #observe the general factor
#oblimin(f3$loadings[,2:3])  #show the 2nd and 3 factors.
#f3 &lt;- fa(x12,3)   #now do it with oblimin rotation
#f3  # not what one naively expect.

</code></pre>

<hr>
<h2 id='sim.multilevel'>Simulate multilevel data with specified within group and between group correlations</h2><span id='topic+sim.multilevel'></span><span id='topic+sim.multi'></span>

<h3>Description</h3>

<p>Multilevel data occur when observations are nested within groups. This can produce correlational structures that are sometimes difficult to understand. These two simulations allow for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. The correlations of aggregated data is sometimes called an 'ecological correlation'. That group level and individual level correlations are independent makes such inferences problematic.  Within individual data are simulated in sim.multi with a variety of possible within person structures.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.multi(n.obs=4,nvar = 2, nfact=2, ntrials=96, days=16, mu=0,sigma=1, fact=NULL, 
loading=.9, phi=0,phi.i=NULL,beta.i=0,mu.i=0, sigma.i = 1,sin.i=0, cos.i=0, AR1=0, 
f.i=NULL, plot=TRUE)
sim.multilevel(nvar = 9, ngroups = 4, ncases = 16, rwg, rbg, eta)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.multilevel_+3A_n.obs">n.obs</code></td>
<td>
<p>How many subjects should be simulated.  Four allows for nice graphics, use more to examine structural properties.</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_nvar">nvar</code></td>
<td>
<p>How many  variables are to be simulated?</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_nfact">nfact</code></td>
<td>
<p>How many factors are simulated,defaults to 2</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_ntrials">ntrials</code></td>
<td>
<p>How many observations per subject (across time)</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_days">days</code></td>
<td>
<p>How many days do these observations reflect? This is relevant if we are adding 
sine and cosines to the model to model diurnal rhythms.</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_mu">mu</code></td>
<td>
<p>The grand mean for each variable across subjects</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_sigma">sigma</code></td>
<td>
<p>The between person standard deviation</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_fact">fact</code></td>
<td>
<p>if NULL, a two factor model is created with loadings of loading or zero in a simple structure form</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_loading">loading</code></td>
<td>
<p>If fact is NULL,  then we create a factor model with loading or zeros</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_phi">phi</code></td>
<td>
<p>The between person factor intercorrelation </p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_phi.i">phi.i</code></td>
<td>
<p>The within person factor intercorrelations</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_beta.i">beta.i</code></td>
<td>
<p>Within subject rate of change over trials</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_mu.i">mu.i</code></td>
<td>
<p>The within subject mean for each subject</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_sigma.i">sigma.i</code></td>
<td>
<p>the within subject standard deviation</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_sin.i">sin.i</code></td>
<td>
<p>To what extent should we diurnally vary by subject?</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_cos.i">cos.i</code></td>
<td>
<p>This will specify the within subject diurnal phase (lag)</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_ar1">AR1</code></td>
<td>
<p>Auto regressive value implies error at time t +1 is partly a function of error at time t.  </p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_f.i">f.i</code></td>
<td>
<p>Factor loadings for each subject</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_plot">plot</code></td>
<td>
<p>If TRUE, create a lattice plot for each subject</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_ngroups">ngroups</code></td>
<td>
<p>The number of groups to simulate</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_ncases">ncases</code></td>
<td>
<p>The number of simulated cases</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_rwg">rwg</code></td>
<td>
<p>The within group correlational structure</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_rbg">rbg</code></td>
<td>
<p>The between group correlational structure</p>
</td></tr>
<tr><td><code id="sim.multilevel_+3A_eta">eta</code></td>
<td>
<p>The correlation of the data with the within data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic concepts of the independence of within group and between group correlations is discussed very clearly by Pedhazur (1997) as well as by Bliese (2009).  <code><a href="#topic+sim.multi">sim.multi</a></code> generates within subject data to model the traditional two level structure of multilevel data.
</p>
<p>This is meant to show how within subject data measured over ntrials can vary independently within and between subjects.  Furthermore, several variables can correlate within subjects show a person by person factor structure.
</p>
<p>Factor scores for n.obs subjects are created for nfact factors with loadings on nvar  variables.  A simple structure model is assumed, so that the loadings on nvar/fact are set to loading for each factor, the others are set to 0. Factors are allowed to correlate phi between subjects and phi.i for each subject.  (Which can be different for each subject). 
Scores can change over time with a slope of beta.i and can vary diurnally as a function of sine and cosine of time (24 hours/day converted to radians).  Error is added to every trial and can be related across trials with a lag of 1.  Thus, if we set AR1=1, then the errors at time t = error + error at t -1. This will lead to auto correlations of about .5. (See <code><a href="#topic+autoR">autoR</a></code> ).  
</p>
<p><code><a href="#topic+sim.multilevel">sim.multilevel</a></code>  merely simulates pooled correlations (mixtures of between group and within group correlations) to allow for a better understanding of the problems inherent in multi-level modeling.  
</p>
<p>Data (wg) are created with a particular within group structure (rwg).  Independent data (bg) are also created with a between group structure (rbg). Note that although there are ncases rows to this data matrix, there are only ngroups independent cases.  That is, every ngroups case is a repeat. The resulting data frame (xy) is a weighted sum of the wg and bg.  This is the inverse procedure for estimating estimating rwg and rbg from an observed rxy which is done by the <code><a href="#topic+statsBy">statsBy</a></code> function.  
</p>
<p><code class="reqn">r_{xy} = \eta_{x_{within}} * \eta_{y_{within}} * r_{xy_{within}} + \eta_{x_{between}} * \eta_{y_{between}} * r_{xy_{between}} </code>
</p>


<h3>Value</h3>

<table>
<tr><td><code>x.df</code></td>
<td>
<p>A data frame for further analysis using <code><a href="#topic+statsBy">statsBy</a></code> including nvar variable values for each of n.obs subjects (id)  for ntrials.  </p>
</td></tr> 
<tr><td><code>wg</code></td>
<td>
<p>A matrix (ncases * nvar) of simulated within group scores</p>
</td></tr>
<tr><td><code>bg</code></td>
<td>
<p>A matrix (ncases * nvar) of simulated between group scores</p>
</td></tr>
<tr><td><code>xy</code></td>
<td>
<p>A matrix ncases * (nvar +1) of pooled data</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>P. D. Bliese. Multilevel modeling in R (2.3) a brief introduction to R, the multilevel package and the nlme package, 2009.
</p>
<p>Pedhazur, E.J. (1997) Multiple regression in behavioral research: explanation and prediction.  Harcourt Brace.
</p>
<p>Revelle, W. An introduction to psychometric theory with applications in R (in prep) Springer. Draft chapters available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+statsBy">statsBy</a></code>  for the decomposition of multi level data and <code><a href="#topic+withinBetween">withinBetween</a></code> for an example data set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#First, show a few results from sim.multi

x.df &lt;- sim.multi()  #the default is 4 subjects for two variables 
#                     over 16 days measured 6 times/day 

#sb &lt;- statsBy(x.df,group ="id",cors=TRUE)
#round(sb$within,2)  #show the within subject correlations

#get some parameters to simulate
data(withinBetween)
wb.stats &lt;- statsBy(withinBetween,"Group")
rwg &lt;- wb.stats$rwg
rbg &lt;- wb.stats$rbg
eta &lt;- rep(.5,9)

#simulate them.  Try this again to see how it changes
XY &lt;- sim.multilevel(ncases=100,ngroups=10,rwg=rwg,rbg=rbg,eta=eta)
lowerCor(XY$wg)  #based upon 89 df
lowerCor(XY$bg)  #based upon 9 df   -- 
</code></pre>

<hr>
<h2 id='sim.omega'>Further functions to simulate psychological/psychometric data.
</h2><span id='topic+sim.omega'></span><span id='topic+sim.general'></span><span id='topic+sim.parallel'></span>

<h3>Description</h3>

<p>A number of functions in the psych package will generate simulated data with particular structures.  These functions include
<code><a href="#topic+sim">sim</a></code> for a factor simplex, and <code><a href="#topic+sim.simplex">sim.simplex</a></code> for a data simplex, <code><a href="#topic+sim.circ">sim.circ</a></code> for a circumplex structure, <code><a href="#topic+sim.congeneric">sim.congeneric</a></code> for a one factor factor congeneric model, <code><a href="#topic+sim.dichot">sim.dichot</a></code> to simulate dichotomous items, <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code> to create a hierarchical factor model, <code><a href="#topic+sim.item">sim.item</a></code> a more general item simulation,
<code><a href="#topic+sim.minor">sim.minor</a></code> to simulate major and minor factors,
<code><a href="#topic+sim.omega">sim.omega</a></code> to test various examples of omega,
<code><a href="#topic+sim.parallel">sim.parallel</a></code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: 
<code><a href="#topic+sim.rasch">sim.rasch</a></code> to create simulated rasch data, 
<code><a href="#topic+sim.irt">sim.irt</a></code> to create general 1 to 4 parameter IRT data by calling 
<code><a href="#topic+sim.npl">sim.npl</a></code> 1 to 4 parameter logistic IRT or 
<code><a href="#topic+sim.npn">sim.npn</a></code> 1 to 4 paramater normal IRT,
<code><a href="#topic+sim.poly">sim.poly</a></code> to create polytomous ideas by calling
<code><a href="#topic+sim.poly.npn">sim.poly.npn</a></code> 1-4 parameter polytomous normal theory items or
<code><a href="#topic+sim.poly.npl">sim.poly.npl</a></code> 1-4 parameter polytomous logistic items, and 
<code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> which creates data following an ideal point or unfolding model by calling 
<code><a href="#topic+sim.poly.ideal.npn">sim.poly.ideal.npn</a></code> 1-4 parameter polytomous normal theory ideal point model or 
<code><a href="#topic+sim.poly.ideal.npl">sim.poly.ideal.npl</a></code> 1-4 parameter polytomous logistic ideal point model.
</p>
<p><code><a href="#topic+sim.structural">sim.structural</a></code> a general simulation of structural models,  and <code><a href="#topic+sim.anova">sim.anova</a></code> for ANOVA and lm simulations, and <code><a href="#topic+sim.VSS">sim.VSS</a></code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.omega(nvar = 12, nfact = 3, n = 500, g = NULL, sem = FALSE, fbig = NULL, 
     fsmall = c(-0.2, 0.2),bipolar = TRUE, om.fact = 3, flip = TRUE,
      option = "equal", ntrials = 10,threshold=NULL)
sim.parallel(ntrials=10,nvar = c(12,24,36,48),nfact = c(1,2,3,4,6),
            n = c(200,400)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.omega_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_nfact">nfact</code></td>
<td>
<p>Number of group factors in sim.general, sim.omega</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_n">n</code></td>
<td>
<p> Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_g">g</code></td>
<td>
<p>General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_sem">sem</code></td>
<td>
<p>Should the sim.omega function do both an EFA omega as well as a CFA omega using the sem package?</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_fbig">fbig</code></td>
<td>
<p>Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_fsmall">fsmall</code></td>
<td>
<p>nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_bipolar">bipolar</code></td>
<td>
<p>if TRUE, then positive and negative loadings are generated from fbig</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_om.fact">om.fact</code></td>
<td>
<p>Number of  factors to extract in omega</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_flip">flip</code></td>
<td>
<p>In omega, should item signs be flipped if negative</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_option">option</code></td>
<td>
<p>In omega, for the case of two factors, how to weight them?</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_threshold">threshold</code></td>
<td>
<p>If raw=TRUE (n &gt;0) and threshold is not NULL, convert the data to binary values, cut at threshold.</p>
</td></tr>
<tr><td><code id="sim.omega_+3A_ntrials">ntrials</code></td>
<td>
<p>Number of replications per level</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing &ldquo;truth&quot; it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  
</p>
<p>There are a number of simulation functions included in psych. sim.omega is just one of them.  
</p>
<p>The default values for <code><a href="#topic+sim.structure">sim.structure</a></code> is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (<code><a href="#topic+sim.simplex">sim.simplex</a></code>) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). 
</p>
<p>Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures <code><a href="#topic+sim.simplex">sim.simplex</a></code> will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  
</p>
<p>An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  <code><a href="#topic+sim.simplex">sim.simplex</a></code> by specifying a non-zero lambda value.
</p>
<p>Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. <code><a href="#topic+sim.minor">sim.minor</a></code> generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.
</p>
<p>Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using <code><a href="#topic+sim.general">sim.general</a></code>.
</p>
<p>Although coefficient <code class="reqn">\omega</code> is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as &ldquo;general&rdquo; and  the omega estimate is too large.  This situation may be explored using the <code><a href="#topic+sim.omega">sim.omega</a></code> function with general left as NULL.  If there is a general factor, then results from <code><a href="#topic+sim.omega">sim.omega</a></code> suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. 
</p>
<p>The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. 
</p>
<p>The logistic model is </p>
<p style="text-align: center;"><code class="reqn">P(i,j) = \gamma + \frac{\zeta-\gamma}{1+ e^{\alpha(\delta-\theta)}}</code>
</p>
<p> where <code class="reqn">\gamma</code> is the lower asymptote or guesssing parameter, <code class="reqn">\zeta</code> is the upper asymptote (normally 1), <code class="reqn">\alpha</code> is item discrimination and <code class="reqn">\delta</code> is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.
</p>
<p>For the 2PL and 2PN models, a = <code class="reqn">\alpha</code> and  d = <code class="reqn">\delta</code> are specified. <br />
For the 3PL or 3PN models, items also differ in their guessing parameter c =<code class="reqn">\gamma</code>. <br />
For the 4PL and 4PN models, the upper asymptote, z= <code class="reqn">\zeta</code> is also specified.  <br />
(Graphics of these may be seen in the demonstrations for the <code><a href="#topic+logistic">logistic</a></code> function.)
</p>
<p>The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = <code class="reqn">\alpha</code> parameter = 1.702 in the logistic model the two models are practically identical.
</p>
<p>In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the <code><a href="#topic+sim.poly.ideal">sim.poly.ideal</a></code> functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.
</p>
<p>By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. 
</p>
<p>The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.
</p>
<p>Other simulation functions in psych are:
</p>
<p><code><a href="#topic+sim.structure">sim.structure</a></code>  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with <code><a href="#topic+structure.diagram">structure.diagram</a></code> to see the proposed structure.  
</p>
<p><code><a href="#topic+sim.congeneric">sim.congeneric</a></code>   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.
</p>
<p><code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code>  A function to create data with a hierarchical (bifactor) structure.  
</p>
<p><code><a href="#topic+sim.item">sim.item</a></code>      A function to create items that either have a simple structure or a circumplex structure.
</p>
<p><code><a href="#topic+sim.circ">sim.circ</a></code>    Create data with a circumplex structure.
</p>
<p><code><a href="#topic+sim.dichot">sim.dichot</a></code>    Create dichotomous item data with a simple or circumplex structure.
</p>
<p><code><a href="#topic+sim.minor">sim.minor</a></code>   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 &ldquo;minor&quot; factors for n observations.  
</p>
<p>Although the standard factor model assumes that K major factors (K &lt;&lt; nvar) will account for the correlations among the variables
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + U^2</code>
</p>
 
<p>where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that 
</p>
<p style="text-align: center;"><code class="reqn">R = FF' + MM' + U^2</code>
</p>
 
<p>where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  
</p>
<p>Such a correlation matrix will have a poor <code class="reqn">\chi^2</code> value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  
</p>
<p>sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.
</p>
<p><code><a href="#topic+sim.parallel">sim.parallel</a></code> Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. 
</p>
<p><code><a href="#topic+sim.anova">sim.anova</a></code>    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. 
</p>
<p><code><a href="#topic+sim.multilevel">sim.multilevel</a></code>  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
</p>


<h3>Value</h3>

<p><code>link{sim.parallel}</code> returns the results of multiple simulations of fa.parallel for various combinations of the number of variables, numbers of factors, and sample size.
<code>link{sim.general}</code> returns either a correlation matrix (if n is not specified) or a data matrix with a general factor.
</p>


<h3>Note</h3>

<p>A few  of the various sim functions are included here.
</p>


<h3>Author(s)</h3>

 
<p><a href="https://personality-project.org/revelle.html">https://personality-project.org/revelle.html</a> <br />
Maintainer: William Revelle  <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a> 
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.omega.html">https://personality-project.org/r/r.omega.html</a> <br />
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>
<p>Revelle, W. (1979).  Hierarchical cluster analysis and the internal structure of tests. Multivariate Behavioral Research, 14, 57-74. (<a href="https://personality-project.org/revelle/publications/iclust.pdf">https://personality-project.org/revelle/publications/iclust.pdf</a>)
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma.  Psychometrika, 74, 1, 145-154. (<a href="https://personality-project.org/revelle/publications/rz09.pdf">https://personality-project.org/revelle/publications/rz09.pdf</a>
</p>
<p>Waller, N. G. (2017) Direct Schmid-Leiman Transformations and Rank-Deficient Loadings Matrices.  Psychometrika.  DOI: 10.1007/s11336-017-9599-0
</p>
<p>Zinbarg, R.E., Revelle, W., Yovel, I., &amp; Li. W.  (2005). Cronbach's Alpha, Revelle's Beta, McDonald's Omega: Their relations with each and two alternative conceptualizations of reliability. Psychometrika. 70, 123-133.  <a href="https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf">https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf</a>
</p>
<p>Zinbarg, R., Yovel, I. &amp; Revelle, W.  (2007).  Estimating omega  for structures containing two group factors:  Perils and prospects.  Applied Psychological Measurement. 31 (2), 135-157.
</p>
<p>Zinbarg, R., Yovel, I., Revelle, W. &amp; McDonald, R. (2006).  Estimating generalizability to a universe of indicators that all have one attribute in common:  A comparison of estimators for omega.  Applied Psychological Measurement, 30, 121-144. DOI: 10.1177/0146621605278814
</p>


<h3>See Also</h3>

<p><code><a href="#topic+omega">omega</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#test &lt;- sim.omega()
  </code></pre>

<hr>
<h2 id='sim.structure'>Create correlation matrices or data matrices with a particular measurement and structural model </h2><span id='topic+sim.structure'></span><span id='topic+sim.structural'></span><span id='topic+sim.correlation'></span><span id='topic+simCor'></span>

<h3>Description</h3>

<p>Structural Equation Models decompose correlation or correlation matrices into a measurement (factor) model and a structural (regression) model.  sim.structural creates data sets with known measurement and structural properties. Population or sample correlation matrices with known properties are generated. Optionally raw data are produced. 
</p>
<p>It is also possible to specify a measurement model for a set of x variables separately from a set of y variables.  They are then combined into one model with the correlation structure between the two sets.
</p>
<p>Finally, the general case is given a population correlation matrix, generate data that  reproduce (with sampling variability) that correlation matrix.  <code><a href="#topic+simCor">simCor</a></code> <code><a href="#topic+sim.correlation">sim.correlation</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.structure(fx=NULL,Phi=NULL, fy=NULL, f=NULL, n=0, uniq=NULL, raw=TRUE, 
  items = FALSE, low=-2,high=2,d=NULL,cat=5, mu=0)
sim.structural(fx=NULL, Phi=NULL, fy=NULL, f=NULL, n=0, uniq=NULL, raw=TRUE,
      items = FALSE, low=-2,high=2,d=NULL,cat=5, mu=0)  #deprecated
simCor(R,n=1000,data=FALSE,scale=TRUE, skew=c("none","log","lognormal",
   "sqrt","abs"),vars=NULL,latent=FALSE,quant=NULL)
sim.correlation(R,n=1000,data=FALSE,scale=TRUE, skew=c("none","log","lognormal",
    "sqrt","abs"),vars=NULL,latent=FALSE,quant=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.structure_+3A_fx">fx</code></td>
<td>
<p>The measurement model for x</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_phi">Phi</code></td>
<td>
<p>The structure matrix of the latent variables</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_fy">fy</code></td>
<td>
<p>The measurement model for y</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_f">f</code></td>
<td>
<p> The measurement model</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_n">n</code></td>
<td>
<p> Number of cases to simulate.  If n=0, the population matrix is returned.</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_uniq">uniq</code></td>
<td>
<p>The uniquenesses if creating a covariance matrix</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_raw">raw</code></td>
<td>
<p>if raw=TRUE, raw data are returned as well for n &gt; 0.</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_items">items</code></td>
<td>
<p>TRUE if simulating items, FALSE if simulating scales</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_low">low</code></td>
<td>
<p>Restrict the item difficulties to range from low to high</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_high">high</code></td>
<td>
<p>Restrict the item difficulties to range from low to high</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_d">d</code></td>
<td>
<p>A vector of item difficulties, if NULL will range uniformly from low to high</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_cat">cat</code></td>
<td>
<p>Number of categories when creating binary (2) or polytomous items</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_mu">mu</code></td>
<td>
<p>A vector of means, defaults to 0</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_r">R</code></td>
<td>
<p>The correlation matrix to reproduce</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_data">data</code></td>
<td>
<p>if TRUE, return the raw data, otherwise return the sample correlation matrix.</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_scale">scale</code></td>
<td>
<p>standardize the simulated data?</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_skew">skew</code></td>
<td>
<p>Defaults to none (the multivariate normal case.  Alternatives take the log, the squareroot, or the absolute value of latent or observed data )</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_vars">vars</code></td>
<td>
<p>Apply the skewing or cuts to just these variables.  If NULL, to all the variables/</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_latent">latent</code></td>
<td>
<p>Should the skewing transforms be applied to the latent variables, or the observed variables?</p>
</td></tr>
<tr><td><code id="sim.structure_+3A_quant">quant</code></td>
<td>
<p>Either a single number or a vector length nvar.  The data will be dichotomized at quant.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the measurement model, fx and the structure model Phi, the model is  f %*% Phi %*%  t(f).   Reliability is f %*% t(f). <code class="reqn">f \phi f'</code> and the reliability for each test is the items communality or just the diag of the model. 
</p>
<p>If creating a correlation matrix, (uniq=NULL) then the diagonal is set to 1, otherwise the diagonal is diag(model) + uniq and the resulting structure is a covariance matrix.
</p>
<p>A special case of a structural model are one factor models such as parallel tests, tau equivalent tests, and congeneric tests.  These may be created by letting the structure matrix = 1 and then defining a vector of factor loadings. Alternatively, <code><a href="#topic+sim.congeneric">sim.congeneric</a></code> will do the same. 
</p>
<p>The general case is to use <code><a href="#topic+simCor">simCor</a></code> aka <code><a href="#topic+sim.correlation">sim.correlation</a></code> which will create data sampled from a specified correlation matrix for a particular sample size. This follows a procedure described by Kaiser and Dickman (1962).  If desired, it will just return the sample correlation matrix.  With data=TRUE, it will return the sample data as well.  It uses an eigen value decomposition of the original matrix times a matrix of random normal deviates (code adapted from the mvnorm function of Brian Ripley's MASS package).  These resulting scores may be transformed using a number of transforms (see the skew option) or made into dichotomous variables (see quant option) for all or a select set (vars option) of the variables.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>model</code></td>
<td>
<p>The implied population correlation or covariance matrix</p>
</td></tr> 
<tr><td><code>reliability</code></td>
<td>
<p>The population reliability values</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The sample correlation or covariance matrix</p>
</td></tr> 
<tr><td><code>observed</code></td>
<td>
<p>If raw=TRUE, a sample data matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Kaiser, H.F. and Dickman, W. (1962) Sample and population score matrices and sample correlation matrices from an arbitrary population correlation matrix. Psychometrika, 27, 179-182.
</p>
<p>Revelle, W. (in preparation) An Introduction to Psychometric Theory with applications in R. Springer. at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>   
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+make.hierarchical">make.hierarchical</a></code> for another structural model and <code><a href="#topic+make.congeneric">make.congeneric</a></code> for the one factor case. <code><a href="#topic+structure.list">structure.list</a></code> and <code><a href="#topic+structure.list">structure.list</a></code> for making symbolic structures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#First, create a sem like model with a factor model of x and ys with correlation Phi
fx &lt;-matrix(c( .9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)  
fy &lt;- matrix(c(.6,.5,.4),ncol=1)
rownames(fx) &lt;- c("V","Q","A","nach","Anx")
rownames(fy)&lt;- c("gpa","Pre","MA")
Phi &lt;-matrix( c(1,0,.7,.0,1,.7,.7,.7,1),ncol=3)
#now create this structure
gre.gpa &lt;- sim.structural(fx,Phi,fy)
print(gre.gpa,2)  
#correct for attenuation to see structure
#the raw correlations are below the diagonal, the adjusted above
round(correct.cor(gre.gpa$model,gre.gpa$reliability),2) 

#These are the population values,
# we can also create a correlation matrix sampled from this population
GRE.GPA  &lt;- sim.structural(fx,Phi,fy,n=250,raw=FALSE)
lowerMat(GRE.GPA$r)

#or we can show data sampled from such a population
GRE.GPA  &lt;- sim.structural(fx,Phi,fy,n=250,raw=TRUE)
lowerCor(GRE.GPA$observed)


 
congeneric &lt;- sim.structure(f=c(.9,.8,.7,.6)) # a congeneric model 
congeneric 

#now take this correlation matrix as a population value and create samples from it
example.congeneric &lt;- sim.correlation(congeneric$model,n=200) #create a sample matrix
lowerMat(example.congeneric ) #show the correlation matrix
#or create another sample and show the data
example.congeneric.data &lt;- simCor(congeneric$model,n=200,data=TRUE) 
describe(example.congeneric.data)
lowerCor(example.congeneric.data )
example.skewed &lt;- simCor(congeneric$model,n=200,vars=c(1,2),data=TRUE,skew="log") 
describe(example.skewed)

</code></pre>

<hr>
<h2 id='sim.VSS'> create VSS like data</h2><span id='topic+sim.VSS'></span><span id='topic+VSS.simulate'></span><span id='topic+VSS.sim'></span>

<h3>Description</h3>

<p>Simulation is one of most useful techniques in statistics and psychometrics.  Here we simulate a correlation matrix with a simple structure composed of a specified number of factors.  Each item is assumed to have complexity one.  See <code><a href="#topic+circ.sim">circ.sim</a></code> and <code><a href="#topic+item.sim">item.sim</a></code> for alternative simulations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.VSS(ncases=1000, nvariables=16, nfactors=4, meanloading=.5,dichot=FALSE,cut=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.VSS_+3A_ncases">ncases</code></td>
<td>
<p> number of simulated subjects </p>
</td></tr>
<tr><td><code id="sim.VSS_+3A_nvariables">nvariables</code></td>
<td>
<p> Number of variables </p>
</td></tr>
<tr><td><code id="sim.VSS_+3A_nfactors">nfactors</code></td>
<td>
<p> Number of factors to generate </p>
</td></tr>
<tr><td><code id="sim.VSS_+3A_meanloading">meanloading</code></td>
<td>
<p>with a mean loading </p>
</td></tr>
<tr><td><code id="sim.VSS_+3A_dichot">dichot</code></td>
<td>
<p>dichot=FALSE give continuous variables, dichot=TRUE gives dichotomous variables</p>
</td></tr>
<tr><td><code id="sim.VSS_+3A_cut">cut</code></td>
<td>
<p>if dichotomous = TRUE, then items with values &gt; cut are assigned 1, otherwise 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ncases x nvariables  matrix 
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+VSS">VSS</a></code>, <code><a href="#topic+ICLUST">ICLUST</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
simulated &lt;- sim.VSS(1000,20,4,.6)
vss &lt;- VSS(simulated,rotate="varimax")
VSS.plot(vss)

## End(Not run)

</code></pre>

<hr>
<h2 id='simulation.circ'> Simulations of circumplex and simple structure</h2><span id='topic+simulation.circ'></span><span id='topic+circ.simulation'></span><span id='topic+circ.sim.plot'></span>

<h3>Description</h3>

<p>Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulation.circ(samplesize=c(100,200,400,800), numberofvariables=c(16,32,48,72))
circ.sim.plot(x.df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulation.circ_+3A_samplesize">samplesize</code></td>
<td>
<p>a vector of sample sizes to simulate </p>
</td></tr>
<tr><td><code id="simulation.circ_+3A_numberofvariables">numberofvariables</code></td>
<td>
<p>vector of the number of variables to simulate </p>
</td></tr>
<tr><td><code id="simulation.circ_+3A_x.df">x.df</code></td>
<td>
<p>A data frame resulting from <code><a href="#topic+simulation.circ">simulation.circ</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>&ldquo;A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle &amp; Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.
</p>
<p>&ldquo;A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, &amp; Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte &amp; Plutchik, 1981; Larsen &amp; Diener, 1992).&quot; (Acton and Revelle, 2004)
</p>
<p>Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:
</p>
<p>The Gap test of equal spacing
</p>
<p>Fisher's test of equality of axes
</p>
<p>A test of indifference to Rotation
</p>
<p>A test of equal Variance of squared factor loadings across arbitrary rotations.
</p>
<p>Included in this set of functions are simple procedure to generate circumplex structured or simple structured data, the four test statistics, and a simple simulation showing the effectiveness of the four procedures.
</p>
<p><code><a href="#topic+circ.sim.plot">circ.sim.plot</a></code> compares the four tests for circumplex, ellipsoid and simple structure data as function of the number of variables and the sample size.  What one can see from this plot is that although no one test is sufficient to discriminate these alternative structures, the set of four tests does a very good job of doing so.  When testing a particular data set for structure, comparing the results of all four tests to the simulated data will give a good indication of the structural properties of the data.
</p>


<h3>Value</h3>

<p>A data.frame with simulation results for circumplex, ellipsoid, and simple structure data sets for each of the four tests.</p>


<h3>Note</h3>

<p>The simulations default values are for sample sizes of 100, 200, 400, and 800 cases, with 16, 32, 48 and 72 items.  </p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p> Acton, G. S. and Revelle, W. (2004) Evaluation of Ten Psychometric Criteria for Circumplex Structure.  Methods of Psychological Research Online, Vol. 9, No. 1 (formerly at https://www.dgps.de/fachgruppen/methoden/mpr-online/issue22/mpr110_10.pdf  and now at  <a href="https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf">https://personality-project.org/revelle/publications/acton.revelle.mpr110_10.pdf</a>. </p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+circ.tests">circ.tests</a></code>, <code><a href="#topic+sim.circ">sim.circ</a></code>, <code><a href="#topic+sim.structural">sim.structural</a></code>, <code><a href="#topic+sim.hierarchical">sim.hierarchical</a></code>,  <code><a href="#topic+simCor">simCor</a></code>,  <code><a href="#topic+sim">sim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#not run
demo &lt;- simulation.circ()
boxplot(demo[3:14])
title("4 tests of Circumplex Structure",sub="Circumplex, Ellipsoid, Simple Structure")
circ.sim.plot(demo[3:14])  #compare these results to real data
</code></pre>

<hr>
<h2 id='smc'>Find the Squared Multiple Correlation (SMC) of each variable with the remaining variables in a matrix</h2><span id='topic+smc'></span>

<h3>Description</h3>

<p>The squared multiple correlation of a variable with the remaining variables in a matrix is sometimes used as initial estimates of the communality of a variable.
</p>
<p>SMCs are also used when estimating reliability using Guttman's lambda 6  <code><a href="#topic+guttman">guttman</a></code> coefficient. 
</p>
<p>The SMC is just  1 - 1/diag(R.inv) where R.inv is the inverse of R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smc(R,covar=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smc_+3A_r">R</code></td>
<td>
<p> A correlation matrix or a dataframe.  In the latter case, correlations are found.</p>
</td></tr>
<tr><td><code id="smc_+3A_covar">covar</code></td>
<td>
<p>if covar = TRUE and  R is either a covariance matrix or data frame, then return the smc * variance for each item</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of squared multiple correlations.  Or, if covar=TRUE, a vector of squared multiple correlations * the item variances
</p>
<p>If the matrix is not invertible, then a vector of 1s is returned.  Note, that I now take the left pseudo inverse so this is less likely to happen (if at all).
</p>
<p>In the case of correlation or covariance matrices with some NAs, those variables with NAs are dropped and the SMC for the remaining variables are found.  The missing SMCs are then estimated by finding the maximum correlation for that column (with a warning).  
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>See Also</h3>

  <p><code><a href="#topic+mat.regress">mat.regress</a></code>, <code><a href="#topic+fa">fa</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>R &lt;- make.hierarchical()
round(smc(R),2)
 </code></pre>

<hr>
<h2 id='spider'>Make &quot;radar&quot; or &quot;spider&quot; plots.</h2><span id='topic+spider'></span><span id='topic+radar'></span>

<h3>Description</h3>

<p>Radar plots and spider plots are just two of the many ways to show multivariate data.  <code><a href="#topic+radar">radar</a></code>  plots correlations as vectors ranging in length from 0 (corresponding to r=-1) to 1 (corresponding to an r=1).  The vectors are arranged radially around a circle. Spider plots connect the end points of each vector. The plots are most appropriate if the variables are organized in some meaningful manner. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spider(y,x,data,labels=NULL,rescale=FALSE,center=FALSE,connect=TRUE,overlay=FALSE,
    scale=1,ncolors=31,fill=FALSE,main=NULL,...)
    
radar(x,labels=NULL,keys=NULL,center=FALSE,connect=FALSE,scale=1,ncolors=31,fill=FALSE,
    add=FALSE,linetyp="solid", main="Radar Plot",angle=0,absolute=FALSE, 
    show=TRUE,digits=2,cut=.2,circles=TRUE, shape=FALSE, clockwise=FALSE,
     delta = NULL,label.pos=NULL,position=NULL,
     xlim=c(-1,1),ylim=c(-1, 1),...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spider_+3A_y">y</code></td>
<td>
<p>The y variables to plot.  Each y is plotted against all the x variables</p>
</td></tr>
<tr><td><code id="spider_+3A_x">x</code></td>
<td>
<p>The x variables defining each line.  Each y is plotted against all the x variables</p>
</td></tr>
<tr><td><code id="spider_+3A_data">data</code></td>
<td>
<p>A correlation matrix from which the x and y variables are selected</p>
</td></tr>
<tr><td><code id="spider_+3A_labels">labels</code></td>
<td>
<p>Labels (assumed to be colnames of the data matrix) for each x variable</p>
</td></tr>
<tr><td><code id="spider_+3A_rescale">rescale</code></td>
<td>
<p>If TRUE, then rescale the data to have mean 0 and sd = 1. This is used if plotting raw data rather than correlations.</p>
</td></tr>
<tr><td><code id="spider_+3A_center">center</code></td>
<td>
<p>if TRUE, then lines originate at the center of the plot, otherwise they start at the mid point.</p>
</td></tr>
<tr><td><code id="spider_+3A_connect">connect</code></td>
<td>
<p>if TRUE, a spider plot is drawn, if FALSE, just a radar plot</p>
</td></tr>
<tr><td><code id="spider_+3A_scale">scale</code></td>
<td>
<p>can be used to magnify the plot, to make small values appear larger.</p>
</td></tr>
<tr><td><code id="spider_+3A_ncolors">ncolors</code></td>
<td>
<p>if ncolors &gt; 2, then positive correlations are plotted with shades of blue and negative correlations shades of red.  This is particularly useful if fill is TRUE.  ncolors should be an odd number, so that neutral values are coded as white.  </p>
</td></tr>
<tr><td><code id="spider_+3A_fill">fill</code></td>
<td>
<p>if TRUE, fill the polygons with colors scaled to size of correlation</p>
</td></tr>
<tr><td><code id="spider_+3A_overlay">overlay</code></td>
<td>
<p>If TRUE, plot multiple spiders on one plot, otherwise plot them as separate plots</p>
</td></tr>
<tr><td><code id="spider_+3A_add">add</code></td>
<td>
<p>If TRUE, add a new spider diagram to the previous one.</p>
</td></tr>
<tr><td><code id="spider_+3A_linetyp">linetyp</code></td>
<td>
<p>see lty in the par options</p>
</td></tr>
<tr><td><code id="spider_+3A_main">main</code></td>
<td>
<p>A label or set of labels for the plots</p>
</td></tr>
<tr><td><code id="spider_+3A_keys">keys</code></td>
<td>
<p>If a keys list is provided, then variables are grouped by the keys, with labels drawn for the key names</p>
</td></tr>
<tr><td><code id="spider_+3A_angle">angle</code></td>
<td>
<p>Rotate the entire figure angle/nvar to the left.  Useful for drawing circumplex structures</p>
</td></tr>
<tr><td><code id="spider_+3A_absolute">absolute</code></td>
<td>
<p>If TRUE, then just use color to show correlation size</p>
</td></tr>
<tr><td><code id="spider_+3A_show">show</code></td>
<td>
<p>If TRUE, show the values at the end of the radar lines if they are &gt; cut</p>
</td></tr>
<tr><td><code id="spider_+3A_digits">digits</code></td>
<td>
<p>round the values to digits</p>
</td></tr>
<tr><td><code id="spider_+3A_cut">cut</code></td>
<td>
<p>Just show values &gt; cut</p>
</td></tr>
<tr><td><code id="spider_+3A_circles">circles</code></td>
<td>
<p>Draw circles at .25, .5 and .75</p>
</td></tr>
<tr><td><code id="spider_+3A_shape">shape</code></td>
<td>
<p>If TRUE, do not draw circles, but rather polygons with nvar sides</p>
</td></tr>
<tr><td><code id="spider_+3A_clockwise">clockwise</code></td>
<td>
<p>If TRUE, organize the variables clockwise</p>
</td></tr>
<tr><td><code id="spider_+3A_delta">delta</code></td>
<td>
<p>How far from the ends of the lines should the values be placed (defaults to 1.05 * length of line). May be vector.</p>
</td></tr>
<tr><td><code id="spider_+3A_label.pos">label.pos</code></td>
<td>
<p>How far out should the labels be placed?  (defaults to 1.05 which is just outside of the outer circle.)</p>
</td></tr>
<tr><td><code id="spider_+3A_position">position</code></td>
<td>
<p>A way of passing the pos parameter that includes NULL as a value.  (See pos in graphics help) </p>
</td></tr>
<tr><td><code id="spider_+3A_xlim">xlim</code></td>
<td>
<p>default values may be changed for more space for labels</p>
</td></tr>
<tr><td><code id="spider_+3A_ylim">ylim</code></td>
<td>
<p>default values by be changed for more space for labelssap</p>
</td></tr>
<tr><td><code id="spider_+3A_...">...</code></td>
<td>
<p>Additional parameters can be passed to the underlying graphics call</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Displaying multivariate profiles may be done by a series of lines (see, e.g., matplot), by colors (see, e.g., <code><a href="#topic+corPlot">corPlot</a></code>, or by radar or spider plots. Spiders are particularly suitable for showing data thought to have circumplex structure. 
</p>
<p>To show just one variable as a function of several others, use <code><a href="#topic+radar">radar</a></code>.  To make multiple plots, use <code><a href="#topic+spider">spider</a></code>.  An additional option when comparing just a few y values is to do overlay plots.  Alternatively, set the plotting options to do several on one page.
</p>


<h3>Value</h3>

<p>Either a spider or radar plot</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+corPlot">corPlot</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(mfrow=c(3,2))
spider(y=1,x=2:9,data=Thurstone,connect=FALSE) #a radar plot
spider(y=1,x=2:9,data=Thurstone) #same plot as a spider plot
 spider(y=1:3,x=4:9,data=Thurstone,overlay=TRUE)
 #make a somewhat oversized plot
spider(y=26:28,x=1:25,data=cor(psychTools::bfi,use="pairwise"),fill=TRUE,scale=2) 
par(op)

#another example taken from  Lippa (2001, page 193) 
lippa.df &lt;- 
structure(list(labels = c("Assured - Dominant", "Gregarious\nExtraverted", 
"Warm\nAgreeable", "Unassuming\nIngeneous", "Unassured - Submissive", 
"Aloof\nIntroverted", "Cold\nHearted", "Arrogant\nCalculating"
), pos = c(0.8, 0.85, 0.83, 0.8, 0.75, 0.83, 0.85, 0.85), values = c(0.41, 
-0.29, -0.53, -0.61, -0.38, 0.14, 0.59, 0.6), delta = c(1.1, 
1.2, 1.2, 1.1, 1.1, 1.5, 1.2, 1.1)), row.names = c(NA, -8L), class = "data.frame")

radar(lippa.df$values,abs=TRUE,labels=lippa.df$labels,angle=90,clockwise=TRUE,lwd=3,
 label.pos=lippa.df$pos,main="Data from Lippa (2001)",scale=.9,circles=FALSE,
  cut=0,delta=lippa.df$delta)
 segments(-1,0,1,0,lwd=.2)  # Add hairline axes
 segments(0,-1,0,1,lwd=.2)
text(0,1.05,expression(italic("Masculine Instrumentality")))
text(1.05,0,expression(italic("Feminine Communion")),srt=270)

#show how to draw a hexagon
RIASEC.df &lt;- structure(list(labels = c("Realistic", "Investigative", "Artistic", 
"Social", "Enterprising", "Conventional"), Su = c(0.84, 0.26, 
-0.35, -0.68, 0.04, -0.33), Morris = c(1.14, 0.32, -0.19, -0.38, 
0.22, 0.23)), row.names = c(NA, -6L), class = "data.frame")

 radar(RIASEC.df$Morris,RIASEC.df$labels,clockwise=TRUE,angle=0,absolute=TRUE,circl=FALSE,scale=.7,
 position=c(1,0,0,0,0,0), lwd=4,label.pos=rep(.80,6),main="",cut=0, shape=TRUE,
  delta =c(1.1,1.25,1.25, 1.25, 1.45,1.45) )
 text(-1.04,0,expression(italic("People")),srt=90)
 text(1.04,0,expression(italic("Things")),srt=270)
 text(0,.91,expression(italic("Data")))
 text(0,-.91 ,expression(italic("Ideas")))
 segments(-1,0,1,0,lwd=.2)     #add hairline axes
 segments(0,-.86,0,.86,lwd=.2)
 text(0,1.2, "Data from Su")



</code></pre>

<hr>
<h2 id='splitHalf'>Alternative estimates of test reliabiity </h2><span id='topic+splitHalf'></span><span id='topic+guttman'></span><span id='topic+tenberge'></span><span id='topic+glb'></span><span id='topic+glb.fa'></span>

<h3>Description</h3>

<p>Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<code class="reqn">\mu_0 \dots \mu_3)</code> as well as <code class="reqn">\beta</code> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <code class="reqn">\omega_h</code> and <code class="reqn">\omega_t</code> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitHalf(r,raw=FALSE,brute=FALSE,n.sample=15000,covar=FALSE,check.keys=TRUE,
           key=NULL,ci=.05,use="pairwise")
guttman(r,key=NULL) 
tenberge(r)
glb(r,key=NULL)
glb.fa(r,key=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitHalf_+3A_r">r</code></td>
<td>
<p>A correlation or covariance matrix or raw data matrix.</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_raw">raw</code></td>
<td>
<p>return a vector of split half reliabilities</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_brute">brute</code></td>
<td>
<p>Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_n.sample">n.sample</code></td>
<td>
<p>If brute is false, how many samples of split halves should be tried? (16 items takes 12,780)</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_covar">covar</code></td>
<td>
<p>Should the covariances or correlations be used for reliability calculations</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_check.keys">check.keys</code></td>
<td>
<p>If TRUE, any item with a negative loading on the first factor will be flipped in sign</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_key">key</code></td>
<td>
<p>a vector of -1, 0, 1 to select or reverse key items.  See <code><a href="#topic+scoreItems">scoreItems</a></code> for an example of keying. If the key vector is less than the number of variables, then item numbers to be reverse can be specified.</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_use">use</code></td>
<td>
<p>Should we find the correlations using &quot;pairwise&quot; or &quot;complete&quot; (see ?cor)</p>
</td></tr>
<tr><td><code id="splitHalf_+3A_ci">ci</code></td>
<td>
<p>The alpha level to use for the confidence intervals of the split half estimates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's <code class="reqn">\alpha</code>  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using <code><a href="#topic+splitHalf">splitHalf</a></code> for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  
</p>
<p>The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's <code class="reqn">\beta</code> (1979) using <code><a href="#topic+splitHalf">splitHalf</a></code>. The companion function, <code><a href="#topic+omega">omega</a></code> calculates omega hierarchical (<code class="reqn">\omega_h</code>)  and omega total (<code class="reqn">\omega_t</code>). 
</p>
<p>Guttman's first estimate <code class="reqn">\lambda_1</code> assumes that all the variance of an item is error:
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_1 = 1 - \frac{tr(\vec{V_x})}{V_x} = \frac{V_x - tr(\vec{V}_x)}{V_x}
</code>
</p>

<p>This is a clear underestimate.
</p>
<p>The second bound, <code class="reqn">\lambda_2</code>, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let <code class="reqn">C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' </code>, then 
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_2 = \lambda_1 + \frac{\sqrt{\frac{n}{n-1}C_2}}{V_x} = \frac{V_x - tr(\vec{V}_x) + \sqrt{\frac{n}{n-1}C_2} }{V_x}</code>
</p>

<p>Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  
</p>
<p>Guttman's 3rd lower bound, <code class="reqn">\lambda_3</code>, also  modifies <code class="reqn">\lambda_1</code> and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's <code class="reqn">\alpha</code>. 
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_3 = \lambda_1 + \frac{\frac{V_X - tr(\vec{V}_X)}{n (n-1)}}{V_X} = \frac{n \lambda_1}{n-1} = \frac{n}{n-1}\Bigl(1 - \frac{tr(\vec{V})_x}{V_x}\Bigr) = \frac{n}{n-1} \frac{V_x - tr(\vec{V}_x)}{V_x} = \alpha
</code>
</p>

<p>This is just replacing the diagonal elements with the average off diagonal elements.  <code class="reqn">\lambda_2 \geq \lambda_3</code> with  <code class="reqn">\lambda_2 &gt; \lambda_3</code> if the covariances are not identical.
</p>
<p><code class="reqn">\lambda_3</code> and <code class="reqn">\lambda_2</code> are both corrections to <code class="reqn">\lambda_1</code> and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) 
</p>
<p style="text-align: center;"><code class="reqn">
\mu_r = \frac{1}{V_x} \bigl( p_o + (p_1 + (p_2 + \dots (p_{r-1} +( p_r)^{1/2})^{1/2} \dots )^{1/2})^{1/2}    \bigr), r = 0, 1, 2, \dots
</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">
p_h = \sum_{i\ne j}\sigma_{ij}^{2h}, h = 0, 1, 2, \dots r-1
</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">
p_h = \frac{n}{n-1}\sigma_{ij}^{2h}, h = r 
</code>
</p>

<p>tenberge and Zegers (1978).  Clearly <code class="reqn">\mu_0 = \lambda_3 = \alpha</code> and <code class="reqn"> \mu_1 = \lambda_2</code>.  <code class="reqn">\mu_r \geq \mu_{r-1} \geq \dots \mu_1 \geq \mu_0</code>, although the series does not improve much after the first two steps.
</p>
<p>Guttman's fourth lower bound, <code class="reqn">\lambda_4</code> was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If <code class="reqn">\vec{X}</code> is split into  two parts, <code class="reqn">\vec{X}_a</code> and <code class="reqn">\vec{X}_b</code>, with correlation <code class="reqn">r_{ab}</code> then
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_4 = 2\Bigl(1 - \frac{V_{X_a} + V_{X_b}}{V_X} \Bigr) =  \frac{4 r_{ab}}{V_x} = \frac{4 r_{ab}}{V_{X_a} + V_{X_b}+ 2r_{ab}V_{X_a}  V_{X_b}}
</code>
</p>

<p>which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. 
</p>
<p><code class="reqn">\lambda_5</code>, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariances
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_5 =  \lambda_1 + \frac{2 \sqrt{\bar{C_2}}}{V_X}.
</code>
</p>

<p>Although superior to <code class="reqn">\lambda_1</code>, <code class="reqn">\lambda_5</code> underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in <code class="reqn">\lambda_3</code>:
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_{5+} =  \lambda_1 + \frac{n}{n-1}\frac{2 \sqrt{\bar{C_2}}}{V_X}.
</code>
</p>

<p><code class="reqn">\lambda_6</code>,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, <code class="reqn">e_j^2</code>,  and is
</p>
<p style="text-align: center;"><code class="reqn">\lambda_6 = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1-r_{smc}^2)}{V_x}
</code>
</p>
<p>.
</p>
<p>The smc is found from all the items.  A modification to Guttman <code class="reqn">\lambda_6</code>, <code class="reqn">\lambda_6*</code> reported by the <code><a href="#topic+score.items">score.items</a></code> function is to find the smc from the entire pool of items given, not just the items on the selected scale.  
</p>
<p>Guttman's <code class="reqn">\lambda_4</code> is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using <code><a href="#topic+splitHalf">splitHalf</a></code> to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.
</p>
<p>The algorithms that had been tried before included:
</p>
<p>a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.
</p>
<p>b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.
</p>
<p>c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)
</p>
<p>These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.
</p>
<p>The brute force and the sampling procedures seem to provide more stable and larger estimates. 
</p>
<p>Yet another procedure, implemented in <code><a href="#topic+splitHalf">splitHalf</a></code> is actually form all possible (for n items &lt;= 16) or sample 15,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples. For a 24 item problem with exactly 2 factors (by simulation), the worst split half is much lower than just random sampling would indicate.  s
</p>
<p>Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  
The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.
</p>
<p>There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, <code class="reqn">\lambda_4</code>. This considers the test as set of items and examines how best to partition the items into splits. The other two, <code><a href="#topic+glb.fa">glb.fa</a></code> and <code><a href="#topic+glb.algebraic">glb.algebraic</a></code>, are alternative ways of weighting the diagonal of the matrix. 
</p>
<p><code><a href="#topic+glb.fa">glb.fa</a></code> estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by 
</p>
<p style="text-align: center;"><code class="reqn">
glb = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1- h^2)}{V_x}
</code>
</p>

<p>This estimate will differ slightly from that found by  <code><a href="#topic+glb.algebraic">glb.algebraic</a></code>, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. 
</p>
<p>Compared to <code><a href="#topic+glb.algebraic">glb.algebraic</a></code>, <code><a href="#topic+glb.fa">glb.fa</a></code> seems to have less (positive) bias for smallish sample sizes (n &lt; 500) but larger for large (&gt; 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  <code><a href="#topic+glb.algebraic">glb.algebraic</a></code> seems to converge on the population value while glb.fa has a positive bias. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>The worst split half reliability. This is an estimate of the general factor saturation.</p>
</td></tr>
<tr><td><code>maxrb</code></td>
<td>
<p>The maximimum split half reliability.  This is Guttman's lambda 4</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Also known as Guttman's Lambda 3</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>The 2.5%, 50%, and 97.5%  values of the raw or sampled split half.  Note that it necessary to specify raw=TRUE to get these.</p>
</td></tr>
<tr><td><code>tenberge$mu1</code></td>
<td>
<p>tenBerge mu 1 is functionally alpha</p>
</td></tr>
<tr><td><code>tenberge$mu2</code></td>
<td>
<p>one of the sequence of estimates mu1 ... mu3</p>
</td></tr>
<tr><td><code>glb</code></td>
<td>
<p>glb found from factor analysis</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Cronbach, L.J. (1951) Coefficient alpha and the internal strucuture of tests.  Psychometrika, 16, 297-334.
</p>
<p>Guttman, L. (1945). A basis for analyzing test-retest reliability. Psychometrika, 10 (4), 255-282. 
</p>
<p>Revelle, W. (1979). Hierarchical cluster-analysis and the internal structure of tests. Multivariate Behavioral Research, 14 (1), 57-74. 
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. DOI: 10.1037/pas0000754.  
<a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma. Psychometrika, 2009. 
</p>
<p>Ten Berge, J. M. F., &amp; Zegers, F. E. (1978). A series of lower bounds to the reliability of a test. Psychometrika, 43 (4), 575-579. 
</p>
<p>Zinbarg, R. E., Revelle, W., Yovel, I., &amp; Li, W. (2005). Cronbach's <code class="reqn">\alpha</code> , Revelle's <code class="reqn">\beta</code> , and McDonald's <code class="reqn">\omega_h</code> ): Their relations with each other and two alternative conceptualizations of reliability. <a href="https://doi.org/10.1007/s11336-003-0974-7">doi:10.1007/s11336-003-0974-7</a> Psychometrika, 70 (1), 123-133.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+reliability">reliability</a></code>,  <code><a href="#topic+alpha">alpha</a></code>,  <code><a href="#topic+omega">omega</a></code>, 
<code><a href="#topic+ICLUST">ICLUST</a></code>,  <code><a href="#topic+unidim">unidim</a></code>,  <code><a href="#topic+glb.algebraic">glb.algebraic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(attitude)
splitHalf(attitude)
splitHalf(attitude,covar=TRUE) #do it on the covariances
temp &lt;- splitHalf(attitude,raw=TRUE)
temp$ci #to show the confidence intervals, you need to specify that raw=TRUE

glb(attitude)
glb.fa(attitude)
if(require(Rcsdp)) {glb.algebraic(cor(attitude)) }
guttman(attitude)

#to show the histogram of all possible splits for the ability test
#sp &lt;- splitHalf(psychTools::ability,raw=TRUE)  #this saves the results
#hist(sp$raw,breaks=101,ylab="SplitHalf reliability",main="SplitHalf 
#    reliabilities of a test with 16 ability items")
sp &lt;- splitHalf(psychTools::bfi[1:10],key=c(1,9,10))

</code></pre>

<hr>
<h2 id='statsBy'>Find statistics (including correlations) within and between groups for basic multilevel analyses</h2><span id='topic+statsBy'></span><span id='topic+statsBy.boot'></span><span id='topic+statsBy.boot.summary'></span><span id='topic+faBy'></span>

<h3>Description</h3>

<p>When examining data at two levels (e.g., the individual and by some set of grouping variables), it is useful to find basic descriptive statistics (means, sds, ns per group, within group correlations) as well as between group statistics (over all descriptive statistics, and overall between group correlations). Of particular use is the ability to decompose a matrix of correlations at the individual level into correlations within group and correlations between groups. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>statsBy(data, group, cors = FALSE, cor="cor", method="pearson", use="pairwise", 
	poly=FALSE, na.rm=TRUE,alpha=.05,minlength=5,
	weights=NULL,mean.weights=NULL, min.n=1)
statsBy.boot(data,group,ntrials=10,cors=FALSE,replace=TRUE,method="pearson")
statsBy.boot.summary(res.list,var="ICC2")
faBy(stats, nfactors = 1, rotate = "oblimin", fm = "minres", free = TRUE, all=FALSE,
   min.n = 12,quant=.1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="statsBy_+3A_data">data</code></td>
<td>
<p>A matrix or dataframe with rows for subjects, columns for variables.  
One of these columns should be the values of a grouping variable.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_group">group</code></td>
<td>
<p>The names or numbers of the variable in data to use as the grouping variables.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_cors">cors</code></td>
<td>
<p>Should the results include the correlation matrix within each group?  Default is FALSE.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_cor">cor</code></td>
<td>
<p>Type of correlation/covariance to find within groups and between groups.  
The default is Pearson correlation.  To find within and between covariances, set cor=&quot;cov&quot;. 
Although polychoric, tetrachoric, and mixed correlations can be found within groups, 
this does not make sense for the between groups or the pooled within groups.  
In this case, correlations for each group will be as specified, but the between groups 
and pooled within will be Pearson. See the discussion below.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_method">method</code></td>
<td>
<p>What kind of correlations should be found (default is Pearson product moment)</p>
</td></tr>
<tr><td><code id="statsBy_+3A_use">use</code></td>
<td>
<p>How to treat missing data.  use=&quot;pairwise&quot; is the default</p>
</td></tr>
<tr><td><code id="statsBy_+3A_poly">poly</code></td>
<td>
<p>Find polychoric.tetrachoric correlations within groups if requested.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing values be deleted (na.rm=TRUE) or should we assume the data are clean?</p>
</td></tr>
<tr><td><code id="statsBy_+3A_alpha">alpha</code></td>
<td>
<p>The alpha level for the confidence intervals for the ICC1 and ICC2, and rwg, rbg</p>
</td></tr>
<tr><td><code id="statsBy_+3A_minlength">minlength</code></td>
<td>
<p>The minimum length to use when abbreviating the labels for confidence intervals</p>
</td></tr>
<tr><td><code id="statsBy_+3A_weights">weights</code></td>
<td>
<p>If specified, weight the groups by weights when finding the pooled within group correlation.
Otherwise weight by sample size.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_mean.weights">mean.weights</code></td>
<td>
<p>If not NULL, what weights should be use we wait the withingroup variables by some weights vector?</p>
</td></tr>
<tr><td><code id="statsBy_+3A_ntrials">ntrials</code></td>
<td>
<p>The number of trials to run when bootstrapping statistics</p>
</td></tr>
<tr><td><code id="statsBy_+3A_replace">replace</code></td>
<td>
<p>Should the bootstrap be done by permuting the data (replace=FALSE) or sampling with replacement (replace=TRUE)</p>
</td></tr>
<tr><td><code id="statsBy_+3A_res.list">res.list</code></td>
<td>
<p>The results from statsBy.boot may be summarized using boot.stats</p>
</td></tr>
<tr><td><code id="statsBy_+3A_var">var</code></td>
<td>
<p>Name of the variable to be summarized from statsBy.boot</p>
</td></tr>
<tr><td><code id="statsBy_+3A_stats">stats</code></td>
<td>
<p>The output of statsBy</p>
</td></tr>
<tr><td><code id="statsBy_+3A_nfactors">nfactors</code></td>
<td>
<p>The number of factors to extract in each subgroup</p>
</td></tr>
<tr><td><code id="statsBy_+3A_rotate">rotate</code></td>
<td>
<p>The factor rotation/transformation</p>
</td></tr>
<tr><td><code id="statsBy_+3A_fm">fm</code></td>
<td>
<p>The factor method (see <code><a href="#topic+fa">fa</a></code> for details)</p>
</td></tr>
<tr><td><code id="statsBy_+3A_free">free</code></td>
<td>
<p>Allow the factor solution to be freely estimated for each individual (see note).</p>
</td></tr>
<tr><td><code id="statsBy_+3A_all">all</code></td>
<td>
<p>Report individual factor analyses for each group as well as the summary table</p>
</td></tr>
<tr><td><code id="statsBy_+3A_min.n">min.n</code></td>
<td>
<p>The minimum number of within subject cases before we factor analyze it or find group statistics. Only works for one grouping variable.</p>
</td></tr>
<tr><td><code id="statsBy_+3A_quant">quant</code></td>
<td>
<p>Show the upper and lower quant quantile of the factor loadings in faBy</p>
</td></tr>
<tr><td><code id="statsBy_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to the fa function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Multilevel data are endemic in psychological research. In multilevel data, observations are taken on subjects who are nested within some higher level grouping variable.  The data might be experimental (participants are nested within experimental conditions) or observational (students are nested within classrooms, students are nested within college majors.) To analyze this type of data, one uses random effects models or mixed effect models, or more generally, multilevel models.  There are at least two very powerful packages (nlme and multilevel) which allow for complex analysis of hierarchical (multilevel) data structures.  <code><a href="#topic+statsBy">statsBy</a></code> is a much simpler function to give some of the basic descriptive statistics for two level models.  It is meant to supplement true multilevel modeling.
</p>
<p>For a group variable (group) for a data.frame or matrix (data), basic descriptive statistics (mean, sd, n) as well as within group correlations (cors=TRUE) are found for each group.  
</p>
<p>The amount of variance associated with the grouping variable compared to the total variance is the type 1 IntraClass Correlation (ICC1):
<code class="reqn">ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))</code>
where npr is the average number of cases within each group. 
</p>
<p>The reliability of the group differences may be found by the ICC2 which reflects how different the means are with respect to the within group variability.  
<code class="reqn">ICC2 = (MSb-MSw)/MSb</code>.
Because the mean square between is sensitive to sample size, this estimate will also reflect sample size.
</p>
<p>Perhaps the most useful part of <code><a href="#topic+statsBy">statsBy</a></code> is that it decomposes the observed correlations between variables into two parts: the within group and the between group correlation. This follows the decomposition of an observed correlation into the pooled correlation within groups (rwg) and the weighted correlation of the means between groups  discussed by Pedazur (1997) and by Bliese in the multilevel package.  
</p>
<p><code class="reqn">r_{xy} = eta_{x_{wg}} * eta_{y_{wg}} * r_{xy_{wg}}  +  eta_{x_{bg}} * eta_{y_{bg}} * r_{xy_{bg}}  </code>
</p>
<p>where <code class="reqn">r_{xy}</code> is the normal correlation which may be decomposed into a within group and between group correlations <code class="reqn">r_{xy_{wg}}</code> and <code class="reqn">r_{xy_{bg}}</code> and eta is the correlation of the data with the within group values, or the group means.
</p>
<p>It is important to realize that the within group and between group correlations are independent of each other.  That is to say, inferring from the 'ecological correlation' (between groups) to the lower level (within group) correlation is inappropriate.  However, these between group correlations are still very meaningful, if inferences are made at the higher level.  
</p>
<p>There are actually two ways of finding the within group correlations pooled across groups.  We can find the correlations within every group, weight these by the sample size and then report this pooled value (pooled).  This is found if the cors option is set to TRUE.  It is logically  equivalent to doing a sample size weighted meta-analytic correlation.  The other way, rwg, considers the covariances, variances, and thus correlations when each subject's scores are given as deviation score from the group mean.  
</p>
<p>If finding tetrachoric, polychoric, or mixed correlations, these two estimates will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. 
</p>
<p>If the weights parameter is specified, the pooled correlations are found by weighting the groups by the specified weight, rather than sample size.
</p>
<p>Confidence values and significance  of  <code class="reqn">r_{xy_{wg}}</code>, pwg, reflect the pooled number of cases within groups, while  <code class="reqn">r_{xy_{bg}} </code>, pbg, the number of groups. These are not corrected for multiple comparisons.
</p>
<p><code><a href="#topic+withinBetween">withinBetween</a></code> is an example data set of the mixture of within and between group correlations. <code><a href="#topic+sim.multilevel">sim.multilevel</a></code> will generate simulated data with a multilevel structure.
</p>
<p>The <code><a href="#topic+statsBy.boot">statsBy.boot</a></code> function will randomize the grouping variable ntrials times and find the statsBy output.  This can take a long time and will produce a great deal of output.  This output can then be summarized for relevant variables using the <code><a href="#topic+statsBy.boot.summary">statsBy.boot.summary</a></code> function specifying the variable of interest.  These two functions are useful in order to find if the mere act of grouping leads to large between group correlations.
</p>
<p>Consider the case of the relationship between various tests of ability when the data are grouped by level of education (statsBy(sat.act,&quot;education&quot;)) or when affect data are analyzed within and between an affect manipulation (statsBy(flat,group=&quot;Film&quot;) ). Note in this latter example, that because subjects were randomly assigned to Film condition for the pretest, that the pretest ICC1s cluster around 0. 
</p>
<p><code><a href="#topic+faBy">faBy</a></code> uses the output of <code><a href="#topic+statsBy">statsBy</a></code> to perform a factor analysis on the correlation matrix within each group. If the free parameter is FALSE, then each solution is rotated towards the group solution (as much as possible).  The output is a list of each factor solution, as well as a summary matrix of loadings and interfactor correlations for  all groups.
</p>


<h3>Value</h3>

<table>
<tr><td><code>means</code></td>
<td>
<p>The means for each group for each variable.  </p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>The standard deviations for each group for each variable.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The number of cases for each group and for each variable.</p>
</td></tr>
<tr><td><code>ICC1</code></td>
<td>
<p>The intraclass correlation reflects the amount of total variance associated with the grouping variable.</p>
</td></tr>
<tr><td><code>ICC2</code></td>
<td>
<p>The intraclass correlation (2) reflecting how much the groups means differ.</p>
</td></tr>
<tr><td><code>ci1</code></td>
<td>
<p>The confidence intervals for the ICC1</p>
</td></tr>
<tr><td><code>ci2</code></td>
<td>
<p>The confidence intervals for the ICC2</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>The F from a one-way anova of group means.</p>
</td></tr>
<tr><td><code>rwg</code></td>
<td>
<p>The pooled within group correlations.</p>
</td></tr>
<tr><td><code>ci.wg</code></td>
<td>
<p>The confidence intervals of the pooled within group correlations.</p>
</td></tr>
<tr><td><code>rbg</code></td>
<td>
<p>The sample size weighted between group correlations. </p>
</td></tr>
<tr><td><code>c.bg</code></td>
<td>
<p>The confidence intervals of the rbg values</p>
</td></tr>
<tr><td><code>etawg</code></td>
<td>
<p>The correlation of the data with the within group values.</p>
</td></tr>
<tr><td><code>etabg</code></td>
<td>
<p>The correlation of the data with the group means.</p>
</td></tr>
<tr><td><code>pbg</code></td>
<td>
<p>The probability of the between group correlation</p>
</td></tr>
<tr><td><code>pwg</code></td>
<td>
<p>The probability of the within group correlation</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>In the case that we want the correlations in each group, r is a list of the within group correlations for every group. Set cors=TRUE</p>
</td></tr>
<tr><td><code>within</code></td>
<td>
<p>is just another way of displaying these correlations.  within is a matrix which reports the lower off diagonal correlations as one row for each group.</p>
</td></tr>
<tr><td><code>pooled</code></td>
<td>
<p>The sample size weighted correlations.  This is just within weighted by the sample sizes. The cors option must be set to TRUE to get this. See the note. </p>
</td></tr>
<tr><td><code>slope</code></td>
<td>
<p>The within group slopes &ndash;  if using cor=&quot;cov&quot; these will be the b weights</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If finding polychoric correlations, the two estimates of the pooled within group correlations will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. As of March, 2020 the average is based upon the weighted FisherZ correlation (back transformed) rather than the average correlation. 
</p>
<p>The value of rbg (the between group correlation) is the group size weighted correlation.  This is not the same as just finding the correlation of the group means (i.e. cor(means)).
</p>
<p>The statsBy.boot function will sometimes fail if sampling with replacement because if the group sizes differ drastically, some groups will be empty.  In this case, sample without replacement.  
</p>
<p>The statsBy.boot function can take a long time.  (As I am writing this, I am running 1000 replications of a problem with 64,000 cases and 84 groups.  It is taking about 3 seconds per replication on a MacBook Pro.)
</p>
<p>The <code><a href="#topic+faBy">faBy</a></code> function takes the output of statsBy (with the cors=TRUE option) and then factors each individual subject.  By default, the solutions are organized so that the factors &quot;match&quot; the group solution in terms of their order.  It is also possible to attempt to force the solutions to match by order and also by using the TargetQ rotation function.  (free=FALSE)
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p> Pedhazur, E.J. (1997) Multiple regression in behavioral research: explanation and prediction.  Harcourt Brace.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+describeBy">describeBy</a></code> and the functions within the multilevel package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Taken from Pedhazur, 1997
pedhazur &lt;- structure(list(Group = c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
2L), X = c(5L, 2L, 4L, 6L, 3L, 8L, 5L, 7L, 9L, 6L), Y = 1:10), .Names = c("Group", 
"X", "Y"), class = "data.frame", row.names = c(NA, -10L))
pedhazur
ped.stats &lt;- statsBy(pedhazur,"Group")
ped.stats


#Now do this for the sat.act data set
sat.stats &lt;- statsBy(sat.act,c("education","gender"),cors=TRUE)   #group by two grouping variables
print(sat.stats,short=FALSE)
lowerMat(sat.stats$pbg)  #get the probability values

#show means by groups
round(sat.stats$mean)

#Do separate factor analyses for each group
sb.bfi &lt;- statsBy(psychTools::bfi[1:10], group=psychTools::bfi$gender,cors=TRUE)
faBy(sb.bfi,1)  #one factor per group 
faBy(sb.bfi,2) #two factors per group



</code></pre>

<hr>
<h2 id='structure.diagram'>Draw a structural equation model specified by two measurement models and a structural model</h2><span id='topic+structure.diagram'></span><span id='topic+structure.graph'></span><span id='topic+structure.sem'></span><span id='topic+lavaan.diagram'></span><span id='topic+sem.diagram'></span><span id='topic+sem.graph'></span>

<h3>Description</h3>

<p>Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.
</p>
<p>Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 
</p>
<p>Input can be specified as matrices or the output from  <code><a href="#topic+fa">fa</a></code>, factanal,  or a rotation package such as <em>GPArotation</em>.
</p>
<p>For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.
</p>
<p>As an option, for those without Rgraphviz installed, <code><a href="#topic+structure.sem">structure.sem</a></code> will just create the sem model and skip the graph. (This functionality is now included in <code><a href="#topic+structure.diagram">structure.diagram</a></code>.)
</p>
<p>structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.
</p>
<p><code><a href="#topic+lavaan.diagram">lavaan.diagram</a></code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code><a href="#topic+structure.diagram">structure.diagram</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structure.diagram(fx, Phi=NULL,fy=NULL,labels=NULL,cut=.3,errors=FALSE,simple=TRUE,
   regression=FALSE,lr=TRUE,Rx=NULL,Ry=NULL,digits=1,e.size=.1,
    main="Structural model", ...)
structure.graph(fx,  Phi = NULL,fy = NULL, out.file = NULL, labels = NULL, cut = 0.3, 
   errors=TRUE, simple=TRUE,regression=FALSE, size = c(8, 6), 
    node.font = c("Helvetica", 14), edge.font = c("Helvetica", 10), 
    rank.direction = c("RL", "TB", "LR", "BT"), digits = 1, 
     title = "Structural model", ...)
structure.sem(fx,  Phi = NULL, fy = NULL,out.file = NULL, labels = NULL,
     cut = 0.3, errors=TRUE, simple=TRUE,regression=FALSE)
lavaan.diagram(fit,main,e.size=.1,...) 
sem.diagram(fit,main="A SEM from the sem package",...)
sem.graph(fit,out.file=NULL,main= "A SEM from the sem package",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="structure.diagram_+3A_fx">fx</code></td>
<td>
<p>a factor model on the x variables. </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_phi">Phi</code></td>
<td>
<p>A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_fy">fy</code></td>
<td>
<p>a factor model on the y variables (can be empty) </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_rx">Rx</code></td>
<td>
<p>The correlation matrix among the x variables</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_ry">Ry</code></td>
<td>
<p>The correlation matrix among the y variables</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_out.file">out.file</code></td>
<td>
<p>name a file to send dot language instructions. </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_labels">labels</code></td>
<td>
<p>variable labels if not specified as colnames for the matrices</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_cut">cut</code></td>
<td>
<p>Draw paths for values &gt; cut </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_fit">fit</code></td>
<td>
<p>The output from a lavaan cfa or sem</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_errors">errors</code></td>
<td>
<p>draw an error term for observerd variables </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_simple">simple</code></td>
<td>
<p>Just draw one path per x or y variable </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_regression">regression</code></td>
<td>
<p>Draw a regression diagram (observed variables cause Y)</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_lr">lr</code></td>
<td>
<p>Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_e.size">e.size</code></td>
<td>
<p>size of the ellipses in structure.diagram</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_main">main</code></td>
<td>
<p>main title of diagram</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_size">size</code></td>
<td>
<p>page size of graphic </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_node.font">node.font</code></td>
<td>
<p> font type for graph </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_edge.font">edge.font</code></td>
<td>
<p>font type for graph </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_rank.direction">rank.direction</code></td>
<td>
<p> Which direction should the graph be oriented </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_digits">digits</code></td>
<td>
<p>Number of digits to draw</p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_title">title</code></td>
<td>
<p> Title of graphic </p>
</td></tr>
<tr><td><code id="structure.diagram_+3A_...">...</code></td>
<td>
<p> other options to pass to Rgraphviz </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  
</p>
<p>All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)
</p>
<p>The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. 
</p>
<p>lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.
</p>
<p>sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.
lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.
The figure is organized to show the appropriate paths between:
</p>
<p>The correlations between the X variables (if Rx is specified) <br />
The X variables and their latent factors  (if fx is specified) <br />
The latent X and the latent Y (if Phi is specified)  <br />
The latent Y and the observed Y (if fy is specified) <br />
The correlations between the Y variables (if Ry is specified)<br />
</p>
<p>A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.
</p>
<p><code><a href="#topic+lavaan.diagram">lavaan.diagram</a></code> may be called from the <code><a href="#topic+diagram">diagram</a></code> function which also will call  <code><a href="#topic+fa.diagram">fa.diagram</a></code>, <code><a href="#topic+omega.diagram">omega.diagram</a></code> or  <code><a href="#topic+iclust.diagram">iclust.diagram</a></code>, depending upon the class of the fit.
</p>
<p>Other diagram functions include <code><a href="#topic+fa.diagram">fa.diagram</a></code>, <code><a href="#topic+omega.diagram">omega.diagram</a></code>.  All of these functions use the various dia functions such as <code><a href="#topic+dia.rect">dia.rect</a></code>, <code><a href="#topic+dia.ellipse">dia.ellipse</a></code>, <code><a href="#topic+dia.arrow">dia.arrow</a></code>, <code><a href="#topic+dia.curve">dia.curve</a></code>, <code><a href="#topic+dia.curved.arrow">dia.curved.arrow</a></code>, and <code><a href="#topic+dia.shape">dia.shape</a></code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>sem</code></td>
<td>
<p>(invisible) a model matrix (partially) ready for input to John Fox's sem package.  It is of class &ldquo;mod&quot; for prettier output.  </p>
</td></tr>
<tr><td><code>lavaan</code></td>
<td>
<p>(invisible) A model specification for the lavaan package.</p>
</td></tr>
<tr><td><code>dotfile</code></td>
<td>
<p>If out.file is specified, a dot language file suitable for using in a dot graphics program such as graphviz or Omnigraffle.</p>
</td></tr>
</table>
<p>A graphic structural diagram in the graphics window
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

  <p><code><a href="#topic+fa.graph">fa.graph</a></code>, <code><a href="#topic+omega.graph">omega.graph</a></code>, <code><a href="#topic+sim.structural">sim.structural</a></code> to create artificial data sets with particular structural properties.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#A set of measurement and structural models
#First set up the various matrices
fx &lt;-  matrix(c(.9,.8,.7,rep(0,9), .6,.7,-.8,rep(0,9),.5,.6,.4),ncol=3)
fy &lt;- matrix(c(.9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)
Phi &lt;- matrix(c(1,.35,0,0,0,
                .35,1,.5,0,0,
                0,.5, 1,0,0,
                .7,-.6, 0, 1,0,
                .0, 0, .4,0,1 ),ncol=5,byrow=TRUE)
#now draw a number of models 
f1 &lt;- structure.diagram(fx,main = "A measurement model for x")
f2 &lt;- structure.diagram(fx,Phi, main = "A measurement model for x")  
f3 &lt;- structure.diagram(fy=fy, main = "A measurement model for y") 
f4 &lt;- structure.diagram(fx,Phi,fy,main="A structural path diagram")            
f5 &lt;- structure.diagram(fx,Phi,fy,main="A structural path diagram",errors=TRUE)

#a mimic model
fy &lt;- matrix(c(.9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)
fx &lt;- matrix(c(.6,.5,0,.4),ncol=2)
mimic &lt;- structure.diagram(fx,fy=fy,simple=FALSE,errors=TRUE, main="A mimic diagram")

fy &lt;- matrix(c(rep(.9,8),rep(0,16),rep(.8,8)),ncol=2)
structure.diagram(fx,fy=fy, e.size=.05)

#symbolic input
X2 &lt;- matrix(c("a",0,0,"b","e1",0,0,"e2"),ncol=4)
colnames(X2) &lt;- c("X1","X2","E1","E2")
phi2 &lt;- diag(1,4,4)
phi2[2,1] &lt;- phi2[1,2] &lt;- "r"
f2 &lt;- structure.diagram(X2,Phi=phi2,errors=FALSE,main="A symbolic model") 

#symbolic input with error 
X2 &lt;- matrix(c("a",0,0,"b"),ncol=2)
colnames(X2) &lt;- c("X1","X2")
phi2 &lt;- diag(1,2,2)
phi2[2,1] &lt;- phi2[1,2] &lt;- "r"
f3 &lt;- structure.diagram(X2,Phi=phi2,main="an alternative representation",e.size=.4)

#and yet another one
X6 &lt;- matrix(c("a","b","c",rep(0,6),"d","e","f"),nrow=6)
colnames(X6) &lt;- c("L1","L2")
rownames(X6) &lt;- c("x1","x2","x3","x4","x5","x6")
Y3 &lt;- matrix(c("u","w","z"),ncol=1)
colnames(Y3) &lt;- "Y"
rownames(Y3) &lt;- c("y1","y2","y3")
phi21 &lt;- matrix(c(1,0,"r1",0,1,"r2",0,0,1),ncol=3)
colnames(phi21) &lt;- rownames(phi21) &lt;-  c("L1","L2","Y")
f4 &lt;- structure.diagram(X6,phi21,Y3)

###the following example is not run but is included to show how to work with lavaan

library(lavaan)
mod.1 &lt;- 'A =~ A1 + A2 + A3 + A4 + A5
         C =~ C1 + C2 + C3 + C4 + C5
         E =~ E1 +E2 + E3 + E4 +E5'
fit.1 &lt;- sem(mod.1,psychTools::bfi[complete.cases(psychTools::bfi),],std.lv=TRUE)
lavaan.diagram(fit.1)   #a normal cfa

#compare with
f3 &lt;- fa(psychTools::bfi[complete.cases(psychTools::bfi),1:15],3)
fa.diagram(f3)


#a sem model
mod.2 &lt;- 'A =~ A1 + A2 + A3 + A4 + A5
         C =~ C1 + C2 + C3 + C4 + C5
         E =~ E1 +E2 + E3 + E4 +E5
         E ~ A + C '        
  
fit.2 &lt;- sem(mod.2,psychTools::bfi[complete.cases(psychTools::bfi),],std.lv=TRUE)
lavaan.diagram(fit.2, cut=0,simple=FALSE,main="sem  model")  # A SEM Model 


#a mimic model         
mod.3 &lt;- 'A =~ A1 + A2 + A3 + A4 + A5
         C =~ C1 + C2 + C3 + C4 + C5
         E =~ E1 +E2 + E3 + E4 +E5
         A ~ age + gender
         C ~ age + gender
         E ~ age + gender'

fit.3 &lt;- sem(mod.3,psychTools::bfi[complete.cases(psychTools::bfi),],std.lv=TRUE)
lavaan.diagram(fit.3, cut=0,simple=FALSE,main="mimic model", e.size=.03)





# and finally, a regression model
X7 &lt;- matrix(c("a","b","c","d","e","f"),nrow=6)
f5 &lt;- structure.diagram(X7,regression=TRUE,main = "Regression model")

#and a really messy regession model
x8 &lt;- c("b1","b2","b3")
r8 &lt;- matrix(c(1,"r12","r13","r12",1,"r23","r13","r23",1),ncol=3)
f6&lt;- structure.diagram(x8,Phi=r8,regression=TRUE,main="Regression model")
</code></pre>

<hr>
<h2 id='structure.list'>Create factor model matrices from an input list</h2><span id='topic+structure.list'></span><span id='topic+phi.list'></span>

<h3>Description</h3>

<p>When creating a structural diagram or a structural model, it is convenient to not have to specify all of the zero loadings in a structural matrix.  structure.list converts list input into a design matrix.  phi.list does the same for a correlation matrix. Factors with NULL values are filled with 0s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structure.list(nvars, f.list,f=NULL, f.labels = NULL, item.labels = NULL)
phi.list(nf,f.list, f.labels = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="structure.list_+3A_nvars">nvars</code></td>
<td>
<p>Number of variables in the design matrix </p>
</td></tr>
<tr><td><code id="structure.list_+3A_f.list">f.list</code></td>
<td>
<p>A list of items included in each factor (for structure.list, or the factors that correlate with the specified factor for phi.list</p>
</td></tr>
<tr><td><code id="structure.list_+3A_f">f</code></td>
<td>
<p>prefix for parameters &ndash; needed in case of creating an X set and a Y set</p>
</td></tr>
<tr><td><code id="structure.list_+3A_f.labels">f.labels</code></td>
<td>
<p>Names for the factors </p>
</td></tr>
<tr><td><code id="structure.list_+3A_item.labels">item.labels</code></td>
<td>
<p>Item labels </p>
</td></tr>
<tr><td><code id="structure.list_+3A_nf">nf</code></td>
<td>
<p>Number of factors in the phi matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is almost self explanatory.  See the examples.
</p>


<h3>Value</h3>

<table>
<tr><td><code>factor.matrix</code></td>
<td>
<p>a matrix of factor loadings to model</p>
</td></tr>
</table>


<h3>See Also</h3>

  <p><code><a href="#topic+structure.graph">structure.graph</a></code> for drawing it, or  <code><a href="#topic+sim.structure">sim.structure</a></code> for creating this data structure. </p>


<h3>Examples</h3>

<pre><code class='language-R'>fx &lt;- structure.list(9,list(F1=c(1,2,3),F2=c(4,5,6),F3=c(7,8,9)))
fy &lt;- structure.list(3,list(Y=c(1,2,3)),"Y")
phi &lt;- phi.list(4,list(F1=c(4),F2=c(1,4),F3=c(2),F4=c(1,2,3)))
fx
phi
fy


</code></pre>

<hr>
<h2 id='superMatrix'>Form a super matrix from two sub matrices.  </h2><span id='topic+superMatrix'></span><span id='topic+superCor'></span><span id='topic+super.matrix'></span>

<h3>Description</h3>

<p>Given the matrices nXm, and jYk, form the super matrix of dimensions (n+j) and (m+k) with  with elements x and y along the super diagonal. 
Useful when considering structural equations.  The measurement models x and y can be combined into a larger measurement model of all of the variables.  If either x or y is a list of matrices, then recursively form a super matrix of all of those elements.  superCor will form a matrix from two matrices and the intercorrelation of the elements of the two.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>superMatrix(x,y)
superCor(x,y=NULL, xy=NULL)
super.matrix(x, y)  #Deprecated
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="superMatrix_+3A_x">x</code></td>
<td>
<p>A n x m matrix or a list of such matrices, or the output object from <code>link{scoreOverlap}</code> </p>
</td></tr>
<tr><td><code id="superMatrix_+3A_y">y</code></td>
<td>
<p>A j x k matrix or a list of such matrices</p>
</td></tr>
<tr><td><code id="superMatrix_+3A_xy">xy</code></td>
<td>
<p>A n x k matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several functions, e.g., <code><a href="#topic+sim.structural">sim.structural</a></code>,<code><a href="#topic+structure.graph">structure.graph</a></code>, <code><a href="#topic+make.keys">make.keys</a></code> use matrices that can be thought of as formed from a set of submatrices.  In particular, when using <code><a href="#topic+make.keys">make.keys</a></code> in order to score a set of items (<code><a href="#topic+scoreItems">scoreItems</a></code> or <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>) or to form specified clusters (<code><a href="#topic+cluster.cor">cluster.cor</a></code>), it is convenient to define different sets of scoring keys for different sets of items and to combine these scoring keys into one super key.
</p>
<p>When developing scales and examining items using <code><a href="#topic+bestScales">bestScales</a></code> it is sometimes helpful to combine the matrix output from <code><a href="#topic+scoreOverlap">scoreOverlap</a></code> with the original correlation matrix.  Thus, let x =  correlation of the scales from <code><a href="#topic+scoreOverlap">scoreOverlap</a></code>, y = the correlations of the items used to form the scales, and xy = the correlation of the scales with the items.   
</p>


<h3>Value</h3>

<p>A (n+j)  x (m +k) matrix with appropriate row and column names
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

 <p><code><a href="#topic+sim.structural">sim.structural</a></code>,<code><a href="#topic+structure.graph">structure.graph</a></code>, <code><a href="#topic+make.keys">make.keys</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mx &lt;- matrix(c(.9,.8,.7,rep(0,4),.8,.7,.6),ncol=2)
my &lt;- matrix(c(.6,.5,.4))

colnames(mx) &lt;- paste("X",1:dim(mx)[2],sep="")
rownames(mx) &lt;- paste("Xv",1:dim(mx)[1],sep="")
colnames(my) &lt;- "Y"
rownames(my) &lt;- paste("Yv",1:3,sep="")
mxy &lt;- superMatrix(mx,my)
#show the use of a list to do this as well
key1 &lt;- make.keys(6,list(first=c(1,-2,3),second=4:6,all=1:6))  #make a scoring key
key2 &lt;- make.keys(4,list(EA=c(1,2),TA=c(3,4)))
superMatrix(list(key1,key2))

r &lt;- cor(bfi[1:15],use="pairwise")
bfi.scores &lt;- scoreOverlap(bfi.keys[1:2], r,select=FALSE) #note the select = FALSE
R &lt;- superCor(bfi.scores,r)
lowerMat(R)
#or to just get the scale correlations with the items
R &lt;- superCor(bfi.scores)
round(R,2)
</code></pre>

<hr>
<h2 id='table2matrix'> Convert a table with counts to a matrix or data.frame representing those counts.</h2><span id='topic+table2matrix'></span><span id='topic+table2df'></span>

<h3>Description</h3>

<p>Some historical sets are reported as summary tables of counts in a limited number of bins.  Transforming these tables to data.frames representing the original values is useful for pedagogical purposes.  (E.g., transforming the original Galton table of height x cubits in order to demonstrate regression.) The column and row names must be able to be converted to numeric values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>table2matrix(x, labs = NULL)
table2df(x, count=NULL,labs = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="table2matrix_+3A_x">x</code></td>
<td>
<p>A two dimensional table of counts with row and column names that can be converted to numeric values. </p>
</td></tr>
<tr><td><code id="table2matrix_+3A_count">count</code></td>
<td>
<p>if present, then duplicate each row count times</p>
</td></tr>
<tr><td><code id="table2matrix_+3A_labs">labs</code></td>
<td>
<p>Labels for the rows and columns. These will be used for the names of the two columns of the resulting matrix </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original Galton (1888) of heights by cubits (arm length) is in tabular form. To show this as a correlation or as a scatter plot, it is useful to convert the table to a matrix or data frame of two columns.  
</p>
<p>This function may also be used to convert an item response pattern table into a data table.  e.g., the Bock data set <code><a href="#topic+bock">bock</a></code>.  
</p>


<h3>Value</h3>

<p>A matrix (or data.frame) of sum(x) rows and two columns.
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

  <p><code><a href="psychTools.html#topic+cubits">cubits</a></code> and  <code><a href="#topic+bock">bock</a></code> data sets</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cubits)
cubit &lt;- table2matrix(psychTools::cubits,labs=c("height","cubit"))
describe(cubit)
ellipses(cubit,n=1)
data(bock)
responses &lt;- table2df(bock.table[,2:6],count=bock.table[,7],labs= paste("lsat6.",1:5,sep=""))
describe(responses)
</code></pre>

<hr>
<h2 id='Tal_Or'>
Data set testing causal direction in presumed media influence</h2><span id='topic+Tal_Or'></span><span id='topic+Tal.Or'></span><span id='topic+pmi'></span><span id='topic+tctg'></span>

<h3>Description</h3>

<p>Nurit Tal-Or, Jonanathan Cohen, Yariv Tasfati, and Albert Gunther (2010) examined the presumed effect of media on other people and change in attitudes.  This data set is from Study 2, and examined the effect of presumed influence of the media upon subsequent actions.  It is used as an example of mediation by Hayes (2013) and for the mediate function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Tal.Or")</code></pre>


<h3>Format</h3>

<p>A data frame with 123 observations on the following 6 variables.
</p>

<dl>
<dt><code>cond</code></dt><dd><p>Experimental Condition: 0 low media importance, 1 high media importance </p>
</dd>
<dt><code>pmi</code></dt><dd><p>Presumed media influence (based upon the mean of two items</p>
</dd>
<dt><code>import</code></dt><dd><p>Importance of the issue </p>
</dd>
<dt><code>reaction</code></dt><dd><p>Subjects rated agreement about possible reactions to the story (mean of 4 items).</p>
</dd>
<dt><code>gender</code></dt><dd><p>1 = male, 2 = female </p>
</dd>
<dt><code>age</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>Tal-Or et al. (2010) examined the presumed effect of the media in two experimental studies.  These data are from study 2. '... perceptions regarding the influence of a news story about an expected shortage in sugar were manipulated indirectly, by manipulating the perceived exposure to the news story, and behavioral intentions resulting from the story were consequently measured.&quot; (p 801). 
</p>


<h3>Source</h3>

<p>The data were downloaded from the webpages of Andrew Hayes (https://www.afhayes.com/public/hayes2018data.zip)  supporting the first and second edition of his book.  The name of the original data set was pmi.  (Gender was recoded to reflect the number of X chromosomes).
</p>
<p>The original data are from Nurit Tal-Or, Jonathan Cohen, Yariv Tsfati, and Albert C. Gunther and are used with their kind permission.
</p>


<h3>References</h3>

<p>Nurit Tal-Or, Jonathan Cohen, Yariv Tsfati and Albert C. Gunther (2010), Testing Causal Direction in the Influence of Presumed Media Influence, Communication Research, 37, 801-824.
</p>
<p>Hayes, Andrew F. (2013)  Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.  Guilford Press. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tal.Or)
mediate(reaction ~ cond + (pmi), data =Tal.Or,n.iter=50) 

</code></pre>

<hr>
<h2 id='test.irt'>A simple demonstration (and test) of various IRT scoring algorthims.
</h2><span id='topic+test.irt'></span>

<h3>Description</h3>

<p> Item Response Theory provides a number of alternative ways of estimating latent scores.  Here we compare 6 different ways to estimate the latent variable associated with a pattern of responses. Originally developed as a test for scoreIrt, but perhaps useful for demonstration purposes.  Items are simulated using <code><a href="#topic+sim.irt">sim.irt</a></code> and then scored using factor scores from <code><a href="#topic+factor.scores">factor.scores</a></code> using statistics found using <code><a href="#topic+irt.fa">irt.fa</a></code>, simple weighted models for 1 and 2 PL and 2 PN. Results show almost perfect agreement with estimates from MIRT and ltm for the dichotomous case and with MIRT for the polytomous case.  (Results from ltm are unstable for the polytomous case, sometimes agreeing with <code><a href="#topic+scoreIrt">scoreIrt</a></code> and MIRT, sometimes being much worse.)  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.irt(nvar = 9, n.obs = 1000, mod = "logistic",type="tetra", low = -3, high = 3,
 seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.irt_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables to create (simulate) and score</p>
</td></tr>
<tr><td><code id="test.irt_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of simulated subjects</p>
</td></tr>
<tr><td><code id="test.irt_+3A_mod">mod</code></td>
<td>
<p>&quot;logistic&quot; or &quot;normal&quot; theory data are generated</p>
</td></tr>
<tr><td><code id="test.irt_+3A_type">type</code></td>
<td>
<p>&quot;tetra&quot; for dichotomous, &quot;poly&quot; for polytomous</p>
</td></tr>
<tr><td><code id="test.irt_+3A_low">low</code></td>
<td>
<p>items range from low to high</p>
</td></tr>
<tr><td><code id="test.irt_+3A_high">high</code></td>
<td>
<p>items range from low to high</p>
</td></tr>
<tr><td><code id="test.irt_+3A_seed">seed</code></td>
<td>
<p>Set the random number seed using some non-nul value.  Otherwise, use the existing sequence of random numbers</p>
</td></tr>
</table>


<h3>Details</h3>

<p>n.obs observations (0/1)  on nvar variables are simulated using either a logistic or normal theory model.  Then, a number of different scoring algorithms are applied and shown graphically.  Requires the ltm package to be installed to compare ltm scores.
</p>


<h3>Value</h3>

<p>A dataframe of scores as well as the generating theta true score. A graphic display of the correlations is also shown.</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+scoreIrt">scoreIrt</a></code>,<code><a href="#topic+irt.fa">irt.fa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#not run
#test.irt(9,1000)
</code></pre>

<hr>
<h2 id='test.psych'> Testing of functions in the psych package </h2><span id='topic+test.psych'></span>

<h3>Description</h3>

<p>Test to make sure the psych functions run on basic test data sets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.psych(first=1,last=5,short=TRUE,all=FALSE,fapc=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.psych_+3A_first">first</code></td>
<td>
<p>first=1: start with dataset first</p>
</td></tr>
<tr><td><code id="test.psych_+3A_last">last</code></td>
<td>
<p>last=5: test for datasets until last</p>
</td></tr>
<tr><td><code id="test.psych_+3A_short">short</code></td>
<td>
<p>short=TRUE - don't return any analyses</p>
</td></tr>
<tr><td><code id="test.psych_+3A_all">all</code></td>
<td>
<p>To get around a failure on certain Solaris 32 bit systems, all=FALSE is the default</p>
</td></tr>
<tr><td><code id="test.psych_+3A_fapc">fapc</code></td>
<td>
<p>if fapc=TRUE, then do a whole series of tests of factor and principal component extraction and rotations.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>When modifying the psych package, it is useful to make sure that adding some code does not break something else.  The test.psych function tests the major functions on various standard data sets.  It  also shows off a number of the capabilities of the psych package.
</p>
<p>Uses 5 standard data sets: <br />
USArrests               Violent Crime Rates by US State  (4 variables) <br />
attitude               The Chatterjee-Price Attitude Data    <br />
Harman23.cor$cov       Harman Example 2.3 8 physical measurements  <br />
Harman74.cor$cov        Harman Example 7.4  24 mental measurements <br />
ability.cov$cov        8 Ability and Intelligence Tests  <br />
</p>
<p>It also uses the bfi and ability data sets from psych
</p>


<h3>Value</h3>

<table>
<tr><td><code>out</code></td>
<td>
<p>if short=FALSE, then list of the output from all functions tested</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>Warning messages will be thrown by fa.parallel and sometimes by fa for random datasets.</p>


<h3>Note</h3>

<p> Although test.psych may be used as a quick demo of the various functions in the psych packge, in general, it is better to try the specific functions themselves.  The main purpose of test.psych is to make sure functions throw error messages or correct for weird conditions.
</p>
<p>The datasets tested are part of the standard R data sets and represent some of the basic problems encountered.
</p>
<p>When version 1.1.10 was released, it caused errors when compiling and testing on some Solaris 32 bit systems.  The all option was added to avoid this problem (since I can't replicate the problem on Macs or PCs).  all=TRUE adds one more test, for a non-positive definite matrix.</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>#test &lt;- test.psych()
#not run
#test.psych(all=TRUE)
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5)
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="Varimax")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="varimax")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="bifactor")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="varimin")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="bentlerT")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="geominT")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="equamax")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="Promax")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="cluster")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="biquartimin")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="equamax")
#    f3 &lt;- fa(bfi[1:15],3,n.iter=5,rotate="Promax")
#    
#     fpoly &lt;- fa(bfi[1:10],2,n.iter=5,cor="poly")
#     f1 &lt;- fa(psychTools::ability,n.iter=4)
#     f1p &lt;- fa(psychTools::ability,n.iter=4,cor="tet")


</code></pre>

<hr>
<h2 id='testRetest'>Find various test-retest statistics, including test, person and item reliability</h2><span id='topic+testRetest'></span><span id='topic+testReliability'></span>

<h3>Description</h3>

<p>Given two presentations of a test, it is straightforward to find the test-retest reliablity, as well as the item reliability and person stability across items.  Using the multi-level structure of the data, it is also possible to do a variance deomposition to find variance components for people, items, time, people x time, people x items, and items x time as well as the residual variance. This leads to various generalizability cofficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testRetest(t1,t2=NULL,keys=NULL,id="id", time=  "time", select=NULL, 
check.keys=TRUE, warnings=TRUE,lmer=TRUE,sort=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testRetest_+3A_t1">t1</code></td>
<td>
<p>a data.frame or matrix for the first time of measurement.</p>
</td></tr>
<tr><td><code id="testRetest_+3A_t2">t2</code></td>
<td>
<p>a data.frame or matrix for the second time of measurement. May be NULL if time is specifed in t1</p>
</td></tr>
<tr><td><code id="testRetest_+3A_keys">keys</code></td>
<td>
<p>item names (or locations) to analyze, preface by &quot;-&quot; to reverse score. </p>
</td></tr>
<tr><td><code id="testRetest_+3A_id">id</code></td>
<td>
<p>subject identification codes to match across time</p>
</td></tr>
<tr><td><code id="testRetest_+3A_time">time</code></td>
<td>
<p>The name of the time variable identifying time 1 or 2 if just one data set is supplied. </p>
</td></tr>
<tr><td><code id="testRetest_+3A_select">select</code></td>
<td>
<p>A subset of items to analyze</p>
</td></tr>
<tr><td><code id="testRetest_+3A_check.keys">check.keys</code></td>
<td>
<p>If TRUE will automatically reverse items based upon their correlation with the first principal component.  Will throw a warning when doing so, but some people seem to miss this kind of message.</p>
</td></tr>
<tr><td><code id="testRetest_+3A_warnings">warnings</code></td>
<td>
<p>If TRUE, then warn when items are reverse scored</p>
</td></tr>
<tr><td><code id="testRetest_+3A_lmer">lmer</code></td>
<td>
<p>If TRUE, include the lmer variance decomposition. By default, this is true, but this can lead to long times for large data sets. </p>
</td></tr>
<tr><td><code id="testRetest_+3A_sort">sort</code></td>
<td>
<p>If TRUE, the data are sorted by id and time.  This allows for random ordering of data, but will fail if ids are duplicated in different studies.  In that case, we need to add a constant to the ids for each study.  See the last example.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many ways of measuring reliability. Test - Retest is one way.  If the time interval is very short (or immediate), this is known as a dependability correlation, if the time interval is longer, a stability coefficient.  In all cases, this is a correlation between two measures at different time points.  Given the multi-level nature of these data, it is possible to find variance components associated with individuals, time, item, and time by item, etc.  This leads to several different estimates of reliability (see <code><a href="#topic+multilevel.reliability">multilevel.reliability</a></code> for a discussion and references).
</p>
<p>It is also possible to find the subject reliability across time (this is the correlation across the items at time 1 with time 2 for each subject).  This is a sign of subject reliability (Wood et al, 2017).  Items can show differing amounts of test-retest reliability over time.  Unfortunately, the within person correlation has problems if people do not differ very much across items.  If all items are in the same keyed direction, and measuring the same construct, then the response profile for an individual is essentially flat. This implies that the even with almost perfect reproducibility, that the correlation can actually be negative.  The within person distance (d2) across items is just the mean of the squared differences for each item.  Although highly negatively correlated with the rqq score, this does distinguish between random responders (high dqq and low rqq) from consistent responders with lower variance (low dqq and low rqq). 
</p>
<p>Several individual statistics are reported in the scores object.  These can be displayed by using <code><a href="#topic+pairs.panels">pairs.panels</a></code> for a graphic display of the relationship and ranges of the various measures.
</p>
<p>Although meant to decompose the variance for tests with items nested within tests, if just given two tests, the variance components for people and for time will also be shown.  The resulting variance ratio of people to total variance is the intraclass correlation between the two tests.  See also <code><a href="#topic+ICC">ICC</a></code> for the more general case.
</p>


<h3>Value</h3>

<table>
<tr><td><code>r12</code></td>
<td>
<p>The time 1 time 2 correlation of scaled scores across time</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Guttman's lambda 3 (aka alpha) and lambda 6* (item reliabilities based upon smcs) are found for the scales at times 1 and 2.</p>
</td></tr>
<tr><td><code>rqq</code></td>
<td>
<p>The within subject test retest reliability of response patterns over items</p>
</td></tr>
<tr><td><code>item.stats</code></td>
<td>
<p>Item reliabilities, item loadings at time 1 and 2, item means at time 1 and time 2</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>A data frame of principal component scores at time 1 and time 2, raw scores from time 1 and time 2, the within person standard deviation for time 1 and time 2, and the rqq  and dqq scores for each subject.  </p>
</td></tr>
<tr><td><code>xy.df</code></td>
<td>
<p>If given separate t1 and t2 data.frames, this is combination suitable for using <code><a href="#topic+multilevel.reliability">multilevel.reliability</a></code> </p>
</td></tr>
<tr><td><code>key</code></td>
<td>
<p>A key vector showing which items have been reversed</p>
</td></tr>
<tr><td><code>ml</code></td>
<td>
<p>The multilevel output</p>
</td></tr> 
</table>


<h3>Note</h3>

<p>lmer=TRUE is the default and will do the variance decomposition using lmer.  This will take some time.  For 3032 cases with 10 items from the msqR and sai data set, this takes 92 seconds, but just .63 seconds if lmer = FALSE.
For the 1895 subjects with repeated measures on the <code><a href="psychTools.html#topic+sai">sai</a></code>, it takes 85 seconds with lmer and .38 without out lmer.
</p>
<p>In the case of just two tests (no items specified), the item based statistics (alpha, rqq, item.stats, scores, xy.df) are not reported. 
</p>
<p>Several examples are given.  The first takes 200 cases from the <code><a href="psychTools.html#topic+sai">sai</a></code> data set.  Subjects were given the <code>link[psychTools]{sai}</code> twice with an intervening mood manipulation (four types of short film clips, with or without placebo/caffeine).  The test retest stability of the sai are based upon the 20 sai items.  The second example compares the scores of the 10 sai items that overlap with 10 items from the <code><a href="psychTools.html#topic+msqR">msqR</a></code> data set from the same study.  <code>link[psychTools]{sai}</code> and  <code><a href="psychTools.html#topic+msqR">msqR</a></code> were given immediately after each other and although the format differs slightly, can be seen as measures of dependability. 
</p>
<p>The third example considers the case of the Impulsivity scale from the Eysenck Personality Inventory.  This is a nice example of how different estimates of reliability will differ.  The alpha reliability is .51 but the test-retest correlation across several weeks is .71! That is to say, the items don't correlate very much with each other (alpha) but do with themselves across time (test-retest).  
</p>
<p>The fourth example takes the same data and jumble the subjects (who are identified by their ids).  This result should be the same as the prior result because the data are automatically sorted by id.  
</p>
<p>Although typically, there are as many subjects at time 1 as time 2, testRetest will handle the case of a different number of subjects.  The data are first sorted by time and id, and then those cases from time 1 that are matched at time 2 are analyzed. It is important to note that the subject id numbers must be unique. 
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Cattell, R. B. (1964). Validity and reliability: A proposed more basic set of concepts. Journal of Educational Psychology, 55(1), 1 - 22. doi: 10.1037/h0046462
</p>
<p>Cranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., &amp; Bolger, N. (2006). A procedure for evaluating sensitivity to within-person change: Can mood measures in diary studies detect change reliably? Personality and Social Psychology Bulletin, 32(7), 917-929.
</p>
<p>DeSimone, J. A. (2015). New techniques for evaluating temporal consistency. Organizational Research Methods, 18(1), 133-152. doi: 10.1177/1094428114553061
</p>
<p>Revelle, W. and Condon, D. Reliability from alpha to omega: A tutorial. Psychological Assessment, 31 (12) 1395-1411. 
</p>
<p>Revelle, W.  (in preparation) An introduction to psychometric theory with applications in R. Springer.  (Available online at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>). 
</p>
<p>Shrout, P. E., &amp; Lane, S. P. (2012). Psychometrics. In Handbook of research methods for studying daily life. Guilford Press.
</p>
<p>Wood, D., Harms, P. D., Lowman, G. H., &amp; DeSimone, J. A. (2017). Response speed and response consistency as mutually validating indicators of data quality in online samples. Social Psychological and Personality Science, 8(4), 454-464. doi: 10.1177/1948550617703168
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alpha">alpha</a></code>, <code><a href="#topic+omega">omega</a></code> <code><a href="#topic+scoreItems">scoreItems</a></code>, <code><a href="#topic+cor2">cor2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #for faster compiling, dont test 
#lmer set to FALSE for speed.
#set lmer to TRUE to get variance components
sai.xray &lt;- subset(psychTools::sai,psychTools::sai$study=="XRAY")
#The case where the two measures are identified by time
#automatically reverses items but throws a warning
stability &lt;- testRetest(sai.xray[-c(1,3)],lmer=FALSE) 
stability  #show the results
#get a second data set
sai.xray1 &lt;- subset(sai.xray,sai.xray$time==1)
msq.xray &lt;- subset(psychTools::msqR,
 (psychTools::msqR$study=="XRAY") &amp; (psychTools::msqR$time==1))
select &lt;- colnames(sai.xray1)[is.element(colnames(sai.xray1 ),colnames(psychTools::msqR))] 

select &lt;-select[-c(1:3)]  #get rid of the id information
#The case where the two times are in the form x, y

dependability &lt;-  testRetest(sai.xray1,msq.xray,keys=select,lmer=FALSE)
dependability  #show the results

#now examine the Impulsivity subscale of the EPI
#use the epiR data set which includes epi.keys
data("epiR",package="psychTools")
#Imp &lt;- selectFromKeys(epi.keys$Imp)   #fixed temporarily with 
Imp &lt;- c("V1", "V3", "V8", "V10","V13" ,"V22", "V39" , "V5" , "V41")
imp.analysis &lt;- testRetest(psychTools::epiR,select=Imp) #test-retest = .7, alpha=.51,.51 
imp.analysis

#demonstrate random ordering  -- the results should be the same
n.obs &lt;- NROW(psychTools::epiR)
set.seed(42)
ss &lt;- sample(n.obs,n.obs)
temp.epi &lt;- psychTools::epiR
temp.epi &lt;-char2numeric(temp.epi)  #make the study numeric
temp.epi$id &lt;- temp.epi$id + 300*temp.epi$study
random.epi &lt;- temp.epi[ss,]
random.imp.analysis &lt;- testRetest(random.epi,select=Imp)

</code></pre>

<hr>
<h2 id='tetrachoric'>Tetrachoric, polychoric, biserial and polyserial correlations from various types of input</h2><span id='topic+tetrachoric'></span><span id='topic+tetrachor'></span><span id='topic+polychoric'></span><span id='topic+biserial'></span><span id='topic+polydi'></span><span id='topic+polyserial'></span><span id='topic+poly.mat'></span>

<h3>Description</h3>

<p>The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).
The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tetrachoric(x,y=NULL,correct=.5,smooth=TRUE,global=TRUE,weight=NULL,na.rm=TRUE,
     delete=TRUE)
polychoric(x,y=NULL,smooth=TRUE,global=TRUE,polycor=FALSE,ML=FALSE, std.err=FALSE, 
     weight=NULL,correct=.5,progress=TRUE,na.rm=TRUE,  delete=TRUE,max.cat=8)
biserial(x,y)  
polyserial(x,y) 
polydi(p,d,taup,taud,global=TRUE,ML = FALSE, std.err = FALSE,
     weight=NULL,progress=TRUE,na.rm=TRUE,delete=TRUE,correct=.5) 
#deprecated  use polychoric instead
poly.mat(x, short = TRUE, std.err = FALSE, ML = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tetrachoric_+3A_x">x</code></td>
<td>
<p>The input may be in one of four forms:
</p>
<p>a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. 
</p>
<p>b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). 
</p>
<p>c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)
</p>
<p>d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_y">y</code></td>
<td>
<p>A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (&lt;10?) categories.</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_correct">correct</code></td>
<td>
<p>Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_smooth">smooth</code></td>
<td>
<p>if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_global">global</code></td>
<td>
<p>When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. </p>
</td></tr> 
<tr><td><code id="tetrachoric_+3A_polycor">polycor</code></td>
<td>
<p>A no longer used option, kept to stop other packages from breaking.</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_weight">weight</code></td>
<td>
<p>A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  </p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_short">short</code></td>
<td>
<p> short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_std.err">std.err</code></td>
<td>
<p>std.err=FALSE does not report the standard errors (faster)  deprecated</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_progress">progress</code></td>
<td>
<p>Show the progress bar (if  not doing multicores)</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_ml">ML</code></td>
<td>
<p> ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood &mdash; very slow! Deprecated</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing data be deleted</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_delete">delete</code></td>
<td>
<p>Cases with no variance are deleted with a warning before proceeding.</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_max.cat">max.cat</code></td>
<td>
<p>The maximum number of categories to bother with for polychoric.  </p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_p">p</code></td>
<td>
<p>The polytomous input to polydi</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_d">d</code></td>
<td>
<p>The dichotomous input to polydi</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_taup">taup</code></td>
<td>
<p>The tau values for the polytomous variables &ndash; if global=TRUE</p>
</td></tr>
<tr><td><code id="tetrachoric_+3A_taud">taud</code></td>
<td>
<p>The tau values for the dichotomous variables &ndash; if globabl = TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). 
</p>
<p>The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See <code><a href="#topic+draw.tetra">draw.tetra</a></code> and <code><a href="#topic+draw.cor">draw.cor</a></code> for  illustrations.)
</p>
<p>This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the <code><a href="base.html#topic+options">options</a></code> command:  options(&quot;mc.cores&quot;=4) will set the number of cores to 4.
</p>
<p><code><a href="#topic+tetrachoric">tetrachoric</a></code> and <code><a href="#topic+polychoric">polychoric</a></code> can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See <code><a href="#topic+fa.extension">fa.extension</a></code> for an application of this.
</p>
<p>The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:
</p>
<p>Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).
</p>
<p>Examples include the lsat6 and lsat7 data sets in the <code><a href="#topic+bock">bock</a></code> data.
</p>
<p>The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. 
</p>
<p>For finding one polychoric correlation from a table, see the Olsson example (below).
</p>
<p><code><a href="#topic+polychoric">polychoric</a></code> replaces <code><a href="#topic+poly.mat">poly.mat</a></code> and is recommended.   <code><a href="#topic+poly.mat">poly.mat</a></code> was an alternative wrapper to the polycor function. 
</p>
<p>biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).
</p>
<p>The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (<a href="https://www.icar-project.org/">https://www.icar-project.org/</a>) for similar data.
</p>
<p>Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.
</p>
<p>A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, <code class="reqn">rbi = r s* \sqrt(pq)/zp </code>.
</p>
<p>As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).
</p>
<p>The 'ad hoc' polyserial correlation, rps is just <code class="reqn">r = r * sqrt(n-1)/n) \sigma y /\sum(zpi) </code> where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) 
</p>
<p>All of these were inspired by (and adapted from) John Fox's polycor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. 
</p>
<p>Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.
</p>
<p>For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  
</p>
<p>John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  
</p>
<p>For combinations of continous, categorical, and dichotomous variables, see <code><a href="#topic+mixed.cor">mixed.cor</a></code>.
</p>
<p>If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).
</p>
<p>For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   
</p>
<p><code><a href="#topic+mixedCor">mixedCor</a></code> which calls <code><a href="#topic+polychoric">polychoric</a></code> for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. 
</p>
<p>(Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     
</p>
<p>As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).
</p>
<p>*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
</p>


<h3>Value</h3>

<table>
<tr><td><code>rho</code></td>
<td>
<p>The (matrix) of tetrachoric/polychoric/biserial correlations</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The normal equivalent of the cutpoints</p>
</td></tr>
<tr><td><code>fixed</code></td>
<td>
<p>If any correlations were adjusted for continuity, the total number of adjustments will be reported. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>For tetrachoric, in the degenerate case of a cell entry with zero observations, a correction for continuity is applied and .5 is added to the cell entry.  A warning is issued.  If correct=FALSE the correction is not applied.
This correction is, by default, on.  It can be adjusted by specifying a smaller value.  See the examples.
</p>
<p>For correct=FALSE, the results agree perfectly with John Fox's polycor function.  
</p>
<p>Switched to using sadmvn from the mnormt package to speed up by 50%. 
</p>
<p>Some small data sets with differing number of alternatives and missing values can lead to errors.  This seems to be solved by setting the correct=0 option.  
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>A. Gunther and M. Hofler. Different results on tetrachorical correlations in mplus and stata-stata announces modified procedure. Int J Methods Psychiatr Res, 15(3):157-66, 2006.
</p>
<p>David Kirk (1973) On the numerical approximation of the bivariate normal (tetrachoric) correlation coefficient. Psychometrika, 38, 259-268.
</p>
<p>Foldnes, N., &amp; Gronneberg, S. (2021, April 1). The Sensitivity of Structural Equation Modeling With Ordinal Data to Underlying Non-Normality and Observed Distributional Forms. Psychological Methods. Advance online publication. http://dx.doi.org/10.1037/met0000385
</p>
<p>MacCallum, Robert C. and Zhang, Shaobo and Preacher, Kristopher J. and Rucker, Derek D. (2002) On the practice of dichotomization of quantitative variables., Psychological Methods, 7, (1) 19-40. 
</p>
<p>Olsson, U. Maximum Likelihood Estimation of the Polychoric Correlation Coefficient, Psychometrika, 44:443-460.
</p>
<p>U.Olsson, F.Drasgow, and N.Dorans (1982). The polyserial correlation coefficient. Psychometrika, 47:337-347. 
</p>
<p>Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer.
</p>
<p>Revelle, W,  Condon, D.M.,  Wilt, J.,  French, J.A., Brown, A.,  and  Elleman, L.G. (2016) Web and phone based data collection using planned missing designs. In  Fielding, N.G.,  Lee, R.M. and  Blank, G. (Eds). SAGE Handbook of Online Research Methods (2nd Ed), Sage Publcations
</p>
<p>W. Revelle, E.M. Dworak and D.M. Condon (2020) Exploring the persome: The power of the item in understanding personality structure. Personality and Individual Differences, <a href="https://doi.org/10.1016/j.paid.2020.109905">doi:10.1016/j.paid.2020.109905</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mixed.cor">mixed.cor</a></code> to find the correlations between mixtures of continuous, polytomous, and dichtomous variables. See the <code><a href="#topic+bigCor">bigCor</a></code> function for very large correlation matrices (uses Pearson correlations).  See also the  polychor function in the polycor package. <code><a href="#topic+irt.fa">irt.fa</a></code> uses the tetrachoric function to do item analysis with the <code><a href="#topic+fa">fa</a></code> factor analysis function.
<code><a href="#topic+draw.tetra">draw.tetra</a></code> shows the logic behind a tetrachoric correlation (for teaching purpuses.)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#if(require(mnormt)) {
data(bock)
tetrachoric(lsat6)
polychoric(lsat6)  #values should be the same
tetrachoric(matrix(c(44268,193,14,0),2,2))  #MPLUS reports.24

#Do not apply continuity correction -- compare with previous analysis!
tetrachoric(matrix(c(44268,193,14,0),2,2),correct=0)  

#the default is to add correct=.5 to 0 cells 
tetrachoric(matrix(c(61661,1610,85,20),2,2)) #Mplus reports .35
tetrachoric(matrix(c(62503,105,768,0),2,2)) #Mplus reports -.10
tetrachoric(matrix(c(24875,265,47,0),2,2)) #Mplus reports  0

polychoric(matrix(c(61661,1610,85,20),2,2)) #Mplus reports .35
polychoric(matrix(c(62503,105,768,0),2,2)) #Mplus reports -.10
polychoric(matrix(c(24875,265,47,0),2,2)) #Mplus reports  0

#Do not apply continuity correction- compare with previous analysis
tetrachoric(matrix(c(24875,265,47,0),2,2), correct=0) 
polychoric(matrix(c(24875,265,47,0),2,2), correct=0)  #the same result


#examples from Kirk 1973  
#note that Kirk's tables have joint probability followed by marginals, but 
#tetrachoric needs marginals followed by joint probability

tetrachoric(c(.5,.5,.333333))   #should be .5
tetrachoric(c(.5,.5,.1150267))  #should be -.75
tetrachoric(c(.5,.5,.397584))   #should e .8
tetrachoric(c(.158655254,.158655254,.145003)) #should be .99


#the example from Olsson, 1979
 x &lt;- as.table(matrix(c(13,69,41,6,113,132,0,22,104),3,3))
 polychoric(x,correct=FALSE)
#Olsson reports rho = .49, tau row = -1.77, -.14 and tau col =  -.69, .67

#give a vector of two marginals and the comorbidity
tetrachoric(c(.2, .15, .1))
tetrachoric(c(.2, .1001, .1))
 #} else {
 #       message("Sorry, you must have mnormt installed")}

# 4 plots comparing biserial to point biserial and latent Pearson correlation
set.seed(42)
x.4 &lt;- sim.congeneric(loads =c(.9,.6,.3,0),N=1000,short=FALSE)
y  &lt;- x.4$latent[,1]
for(i in 1:4) {
x &lt;- x.4$observed[,i]
r &lt;- round(cor(x,y),1)
ylow &lt;- y[x&lt;= 0]
yhigh &lt;- y[x &gt; 0]
yc &lt;- c(ylow,yhigh)
rpb &lt;- round(cor((x&gt;=0),y),2)
rbis &lt;- round(biserial(y,(x&gt;=0)),2)
ellipses(x,y,ylim=c(-3,3),xlim=c(-4,3),pch=21 - (x&gt;0),
       main =paste("r = ",r,"rpb = ",rpb,"rbis =",rbis))

dlow &lt;- density(ylow)
dhigh &lt;- density(yhigh)
points(dlow$y*5-4,dlow$x,typ="l",lty="dashed")
lines(dhigh$y*5-4,dhigh$x,typ="l")
}

#show non-symmeteric results
test1 &lt;- tetrachoric(psychTools::ability[,1:4],psychTools::ability[,5:10])
test2 &lt;-  polychoric(psychTools::ability[,1:4],psychTools::ability[,5:10])
all &lt;- tetrachoric(psychTools::ability[,1:10])


</code></pre>

<hr>
<h2 id='thurstone'>Thurstone Case V scaling</h2><span id='topic+thurstone'></span>

<h3>Description</h3>

<p>Thurstone Case V scaling allows for a scaling of objects compared to other objects. As one of the cases considered by Thurstone, Case V makes the assumption of equal variances and uncorrelated distributions. </p>


<h3>Usage</h3>

<pre><code class='language-R'>thurstone(x, ranks = FALSE, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thurstone_+3A_x">x</code></td>
<td>
<p> A square matrix or data frame of preferences, or a rectangular data frame or matrix rank order choices. </p>
</td></tr>
<tr><td><code id="thurstone_+3A_ranks">ranks</code></td>
<td>
<p>TRUE if rank orders are presented</p>
</td></tr>
<tr><td><code id="thurstone_+3A_digits">digits</code></td>
<td>
<p>number of digits in the goodness of fit</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Louis L. Thurstone was a pioneer in psychometric theory and measurement of attitudes, interests, and abilities.  Among his many contributions was a systematic analysis of the process of comparative judgment (thurstone, 1927).  He considered the case of asking subjects to successively compare pairs of objects. If the same subject does this repeatedly, or if  subjects act as random replicates of each other, their judgments can be thought of as sampled from a normal distribution of underlying (latent) scale  scores for each object, Thurstone  proposed that the comparison between the value of two objects could be represented as representing the differences of the average value for each object compared to the standard deviation of the differences between objects.  The basic model is that each item has a normal distribution of response strength and that choice represents the stronger of the two response strengths.  A justification for the normality assumption is that each decision represents the sum of many independent  inputs and thus, through the central limit theorem, is normally distributed. 
</p>
<p>Thurstone considered five different sets of assumptions about the equality and independence of the variances for each item (Thurston, 1927). Torgerson expanded this analysis slightly by considering three classes of data collection (with individuals, between individuals and mixes of within and between) crossed with three sets of assumptions (equal covariance of decision process, equal correlations and small differences in variance, equal variances).  
</p>
<p>The data may be either a square matrix of dataframe of preferences (as proportions with the probability of the column variable being chosen over the row variable) or a matrix or dataframe of rank orders ( 1 being prefered to 2, etc.)
</p>
<p>The second example creates 100 random permutations of ranks 1-5.  These data are then converted to a matrix of choices and then scaled.  The goodness of fit is practically perfect, even though the data are meaningless.   
</p>
<p>This suggests a better goodness of fit test should be applied.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>GF</code></td>
<td>
<p>Goodness of fit 1 = 1 - sum(squared residuals/squared original) for lower off diagonal. <br />
</p>
<p>Goodness of fit 2 =  1 -  sum(squared residuals/squared original) for full matrix.</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>square matrix of residuals (of class dist)</p>
</td></tr>
<tr><td><code>choice</code></td>
<td>
<p>The original choice data</p>
</td></tr>
</table>
<p>...
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p> Thurstone, L. L. (1927) A law of comparative judgments.  Psychological Review, 34, 273-286.
</p>
<p>Revelle, W. An introduction to psychometric theory with applications in R. (in preparation), Springer. <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(psychTools::vegetables)
thurstone(psychTools::veg)
#But consider the case of 100 random orders
set.seed((42))
ranks &lt;- matrix(NA,nrow=100,ncol=5)
for(i in 1:100) ranks[i,] &lt;- sample(5,5)
 t &lt;- thurstone(ranks,TRUE)
 t   #show the fits
 t$hoice #show the choice matrix



</code></pre>

<hr>
<h2 id='tr'>Find the trace of a square matrix</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Hardly worth coding, if it didn't appear in so many formulae in psychometrics, the trace of a (square) matrix is just the sum of the diagonal elements. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tr_+3A_m">m</code></td>
<td>
<p>A square matrix </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tr function is used in various matrix operations and is the sum of the diagonal elements of a matrix.
</p>


<h3>Value</h3>

<p> The sum of the diagonal elements of a square matrix.  <br />
i.e. tr(m)  &lt;- sum(diag(m)). 
</p>


<h3>Examples</h3>

<pre><code class='language-R'> m &lt;- matrix(1:16,ncol=4)
 m
 tr(m)
 
</code></pre>

<hr>
<h2 id='Tucker'>
9 Cognitive variables discussed by Tucker and Lewis (1973)
</h2><span id='topic+Tucker'></span>

<h3>Description</h3>

<p>Tucker and Lewis (1973) introduced a reliability coefficient for ML factor analysis.  Their example data set was previously reported by Tucker (1958) and taken from Thurstone and Thurstone (1941).  The correlation matrix is a 9 x 9 for 710 subjects and has two correlated factors of ability: Word Fluency and Verbal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Tucker)</code></pre>


<h3>Format</h3>

<p>A data frame with 9 observations on the following 9 variables.
</p>

<dl>
<dt><code>t42</code></dt><dd><p>Prefixes</p>
</dd>
<dt><code>t54</code></dt><dd><p>Suffixes</p>
</dd>
<dt><code>t45</code></dt><dd><p>Chicago Reading Test: Vocabulary</p>
</dd>
<dt><code>t46</code></dt><dd><p>Chicago Reading Test: Sentences</p>
</dd>
<dt><code>t23</code></dt><dd><p>First and last letters</p>
</dd>
<dt><code>t24</code></dt><dd><p>First letters</p>
</dd>
<dt><code>t27</code></dt><dd><p>Four letter words</p>
</dd>
<dt><code>t10</code></dt><dd><p>Completion</p>
</dd>
<dt><code>t51</code></dt><dd><p>Same or Opposite</p>
</dd>
</dl>



<h3>Details</h3>

<p>The correlation matrix from Tucker (1958) was used in Tucker and Lewis (1973) for the Tucker-Lewis Index of factoring reliability.
</p>


<h3>Source</h3>

<p>Tucker, Ledyard (1958) An inter-battery method of factor analysis, Psychometrika, 23, 111-136.
</p>


<h3>References</h3>

<p>L.~Tucker and C.~Lewis. (1973) A reliability coefficient for maximum likelihood factor analysis.
Psychometrika, 38(1):1&ndash;10.
</p>
<p>F.~J. Floyd and K.~F. Widaman. (1995)  Factor analysis in the development and refinement of clinical
assessment instruments., Psychological Assessment, 7(3):286 &ndash; 299.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tucker)
fa(Tucker,2,n.obs=710)
omega(Tucker,2)
</code></pre>

<hr>
<h2 id='unidim'>Several indices of the unidimensionality of a set of variables.</h2><span id='topic+unidim'></span>

<h3>Description</h3>

<p>There are a variety of ways of assessing whether a set of items measures one latent trait.  <code><a href="#topic+unidim">unidim</a></code> is just one more way.  If a one factor model holds in the data, then the factor analytic decomposition F implies that FF' should reproduce the correlations with communalities along the diagonal. In this case, the fit FF' should be identical to the correlation matrix minus the uniquenesses.  unidim is just the ratio of these two estimates.  The higher it is, the more the evidence for unidimensionality. A number of alternative statistics are estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unidim(keys=NULL,x=NULL,cor="cor",use="pairwise", fm="minres", correct=.5, 
       check.keys=TRUE,n.obs=NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unidim_+3A_x">x</code></td>
<td>
<p>An input matrix or data frame.  If x is not a correlation matrix, then the correlations are found.</p>
</td></tr>
<tr><td><code id="unidim_+3A_keys">keys</code></td>
<td>
<p>If specified,  then a number of scales can be tested at once. (See <code><a href="#topic+scoreItems">scoreItems</a></code> for a similar procedure.)</p>
</td></tr>
<tr><td><code id="unidim_+3A_cor">cor</code></td>
<td>
<p>By default, find the Pearson correlation, other options are &quot;spearman&quot;,&quot;kendall&quot;,&quot;tet&quot;(for tetrachoric), &quot;poly&quot; (for polychoric), or  &quot;mixed&quot;</p>
</td></tr>
<tr><td><code id="unidim_+3A_use">use</code></td>
<td>
<p>pairwise complete cases is the default</p>
</td></tr>
<tr><td><code id="unidim_+3A_fm">fm</code></td>
<td>
<p>factor extraction method defaults to &quot;minres&quot; but could be &quot;mle&quot; or &quot;minrank&quot;</p>
</td></tr>
<tr><td><code id="unidim_+3A_correct">correct</code></td>
<td>
<p>If using &quot;tetrachoric&quot; or &quot;polychoric&quot; correlations, should we correct empty cells for continuity, and if so, by how much.  (See <code><a href="#topic+tetrachoric">tetrachoric</a></code> for a discussion of this correction)</p>
</td></tr>
<tr><td><code id="unidim_+3A_check.keys">check.keys</code></td>
<td>
<p>If TRUE, then items will be keyed based upon their loadings on the first factor.  Automatically done if key.list is NULL.</p>
</td></tr>
<tr><td><code id="unidim_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observartions.  If given a correlation matrix as input, n.obs is required for some of the goodness of fit estimates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is set of new indices to test the unidimensionality of scale.  A number of test cases suggest that u provides high values when the data are in fact unidimensional, low values when they are not.
</p>
<p>The logic is deceptively simple:  Unidimensionality implies that a one factor model of the data fits the covariances of the data.  If this is the case, then factor model implies R = FF' + U2 will have residuals of 0.  Similarly, this also implies that the observed correlations will equal the model.  Thus, the sum of the observed correlations (with the diagonal replaced by the communalities) should match the factor model.  Compare these two models:  R - U2  versus FF'.   This is the rho_c  estimate. It is basically a test of whether a congeneric model fits.  (That is, all the items have loadings on just one factor.)
</p>
<p>This works well, but when some of the loadings are very small, even though 1 factor is correct, it is probably not a good idea to think of the items as forming a unidimen- sional scale. Thus, an alternative model (the Tau statistic) considers the residuals found by subtracting the average correlation from the observed correlations. This will achieve a maximum if the item covariances are all identical (a tau equivalent model).
</p>
<p>The product of rho_c and Tau is the measure of unidimensionality, u That is, congeneric fit x tau equivalent fit as a measure of unidimensionality.
</p>
<p>The main unidim estimates of the results are reported in the uni object. Conventional factor goodness of fit are in the fa.stats object.
</p>


<h3>Value</h3>

 
<table>
<tr><td><code>u</code></td>
<td>
<p>The estimate of unidimensionality which is just the product of</p>
</td></tr>
<tr><td><code>Tau</code></td>
<td>
<p>The fit of the average r to the correlation matrix</p>
</td></tr>
<tr><td><code>rho_c</code></td>
<td>
<p>The off diagonal fit from  <code><a href="#topic+fa">fa</a></code>  </p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Standardized alpha of the keyed items (after appropriate reversals)</p>
</td></tr>
<tr><td><code>av.r</code></td>
<td>
<p>The average interitem correlation of the keyed items.</p>
</td></tr>
<tr><td><code>median.r</code></td>
<td>
<p>The median value of the iteritem correlations of the keyed items.</p>
</td></tr>
<tr><td><code>Unidim.A</code></td>
<td>
<p>The unidimensional criterion when items are keyed in positive direction.</p>
</td></tr>
<tr><td><code>Unidim</code></td>
<td>
<p>The raw value of the unidimensional criterion</p>
</td></tr>
<tr><td><code>raw.model</code></td>
<td>
<p>The ratio of the FF' model to the sum(R)</p>
</td></tr>
<tr><td><code>adj.model</code></td>
<td>
<p>The ratio of the FF' model to the sum(R) when items are flipped.</p>
</td></tr>
<tr><td><code>Total</code></td>
<td>
<p>The ratio of the sum(R - uniqueness)/sum(R)</p>
</td></tr>
<tr><td><code>Total.A</code></td>
<td>
<p>Same ratio with flipped items</p>
</td></tr>
<tr><td><code>CFI</code></td>
<td>
<p>Comparative Fit Index</p>
</td></tr>
<tr><td><code>ECV</code></td>
<td>
<p>Explained Common Variance</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A perhaps interesting idea but still underdevelopment. Treat with appropriate caution.  It is (perhaps) useful to compare the unidim statistics with those generated by <code><a href="#topic+omega">omega</a></code>.  A quick way to do this is to use the <code><a href="#topic+reliability">reliability</a></code> function which will find alpha, omega_h and omega_t as well as split half reliablities and the unidim measures.  When this done, it can be seen that u is not sensitive to the number of items and seems robust across various sample sizes.  
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

<p>Revelle, W. and Condon, D.M (2023) Using undim rather than omega in estimating undimensionality.  Working draft available at  <a href="https://personality-project.org/revelle/publications/rc.23.pdf">https://personality-project.org/revelle/publications/rc.23.pdf</a> 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fa">fa</a></code> for factor analysis, <code><a href="#topic+omega">omega</a></code> and <code><a href="#topic+reliability">reliability</a></code> for reliability.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#test the unidimensionality of the five factors of the bfi data set.


unidim(psychTools::bfi.keys,psychTools::bfi) 
unidim(psychTools::ability.keys,psychTools::ability)
#Try a known 3 factor structure
x &lt;- sim.minor(nfact=3,bipolar=FALSE)   #this makes all the items positive
unidim(x$model) 
keys.list &lt;- list(first =paste0("V",1:4) ,second = paste0("V",5:8),
   third=paste0("V",9:12),all= paste0("V",1:12))
unidim(keys.list, x$model)

x &lt;- sim.minor(nfact=3)
unidim(keys.list,x$model)   #we flip the negative items 

#what about a hierarchical model?
H &lt;- sim.hierarchical()  # by default, a nice hierarchical model
H.keys &lt;- list(First = paste0("V",1:3),Second=paste0("V",4:6),Third=paste0("V",7:9),
  All = paste0("V",1:9))
unidim(H.keys,H)



</code></pre>

<hr>
<h2 id='VSS'> Apply the Very Simple Structure, MAP, and other criteria to determine the appropriate number of factors.</h2><span id='topic+vss'></span><span id='topic+VSS'></span><span id='topic+MAP'></span><span id='topic+eigenCi'></span><span id='topic+nfactors'></span><span id='topic+vssSelect'></span>

<h3>Description</h3>

<p>There are multiple ways to determine the appropriate number of factors in exploratory factor analysis. Routines for the Very Simple Structure (VSS) criterion allow one to compare solutions of varying complexity and for different number of factors. Graphic output indicates the &quot;optimal&quot; number of factors for different levels of complexity.  The Velicer MAP criterion is another good choice. <code><a href="#topic+nfactors">nfactors</a></code> finds and plots several of these alternative estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vss(x, n = 8, rotate = "varimax", diagonal = FALSE, fm = "minres", 
	n.obs=NULL,plot=TRUE,title="Very Simple Structure",use="pairwise",cor="cor",...)
VSS(x, n = 8, rotate = "varimax", diagonal = FALSE, fm = "minres", 
	n.obs=NULL,plot=TRUE,title="Very Simple Structure",use="pairwise",cor="cor",...)
	nfactors(x,n=20,rotate="varimax",diagonal=FALSE,fm="minres",n.obs=NULL,
             title="Number of Factors",pch=16,use="pairwise", cor="cor",...)
vssSelect(keys,x,cor="cor", fm="minres",plot=FALSE)             
eigenCi(x,n.iter=1000, use="pairwise", alpha=.05,plot=FALSE,root=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VSS_+3A_x">x</code></td>
<td>
<p> a correlation matrix or a data matrix</p>
</td></tr>
<tr><td><code id="VSS_+3A_n">n</code></td>
<td>
<p>Number of factors to extract &ndash; should be more than hypothesized! </p>
</td></tr>
<tr><td><code id="VSS_+3A_rotate">rotate</code></td>
<td>
<p> what rotation to use c(&quot;none&quot;, &quot;varimax&quot;,  &quot;oblimin&quot;,&quot;promax&quot;)</p>
</td></tr>
<tr><td><code id="VSS_+3A_diagonal">diagonal</code></td>
<td>
<p>Should we fit the diagonal as well </p>
</td></tr>
<tr><td><code id="VSS_+3A_fm">fm</code></td>
<td>
<p>factoring method &ndash; fm=&quot;pa&quot;  Principal Axis Factor Analysis, fm = &quot;minres&quot; minimum residual (OLS) factoring fm=&quot;mle&quot;  Maximum Likelihood FA, fm=&quot;pc&quot; Principal Components&quot; </p>
</td></tr>
<tr><td><code id="VSS_+3A_n.obs">n.obs</code></td>
<td>
<p>Number of observations if doing a factor analysis of correlation matrix.  This value is ignored by VSS but is necessary for the ML factor analysis package.</p>
</td></tr>
<tr><td><code id="VSS_+3A_plot">plot</code></td>
<td>
<p>plot=TRUE  Automatically call VSS.plot with the VSS output, otherwise don't plot</p>
</td></tr>
<tr><td><code id="VSS_+3A_keys">keys</code></td>
<td>
<p>A keys list specifying the subsets of items to analyze</p>
</td></tr>
<tr><td><code id="VSS_+3A_title">title</code></td>
<td>
<p>a title to be passed on to VSS.plot</p>
</td></tr>
<tr><td><code id="VSS_+3A_pch">pch</code></td>
<td>
<p>the plot character for the nfactors plots</p>
</td></tr>
<tr><td><code id="VSS_+3A_use">use</code></td>
<td>
<p>If doing covariances or Pearson R, should we use &quot;pairwise&quot; or &quot;complete cases&quot;</p>
</td></tr>
<tr><td><code id="VSS_+3A_cor">cor</code></td>
<td>
<p>What kind of correlation to find, defaults to Pearson but see fa for the choices</p>
</td></tr>
<tr><td><code id="VSS_+3A_n.iter">n.iter</code></td>
<td>
<p>How many iterations of the bootstrap for eigenCi</p>
</td></tr>
<tr><td><code id="VSS_+3A_alpha">alpha</code></td>
<td>
<p>Width of confidence intervals = 1- alpha</p>
</td></tr>
<tr><td><code id="VSS_+3A_root">root</code></td>
<td>
<p>By default show the eigen values, if root=TRUE plot the squareroots of the eigen values.</p>
</td></tr>
<tr><td><code id="VSS_+3A_...">...</code></td>
<td>
<p>parameters to pass to the factor analysis program 
The most important of these is if using a correlation matrix is covmat= xx</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Determining the most interpretable number of factors from a factor analysis is perhaps one of the greatest challenges in factor analysis.  There are many solutions to this problem, none of which is uniformly the best.  &quot;Solving the number of factors problem is easy, I do it everyday before breakfast.&quot;  But knowing the right solution is harder. (Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  
</p>
<p>Techniques most commonly used include
</p>
<p>1)  Extracting factors until the chi square of the residual matrix is not significant.
</p>
<p>2) Extracting factors until the change in chi square from factor n to factor n+1 is not significant.
</p>
<p>3) Extracting factors until the eigen values of the real data are less than the corresponding eigen values of a random data set of the same size (parallel analysis) <code><a href="#topic+fa.parallel">fa.parallel</a></code>.
</p>
<p>4) Plotting the magnitude of the successive eigen values and applying the scree test (a sudden drop in eigen values analogous to the change in slope seen when scrambling up the talus slope of a mountain and approaching the rock face.
</p>
<p>5) Extracting principal components until the eigen value &lt; 1. 
</p>
<p>6) Extracting factors as long as they are interpetable.
</p>
<p>7) Using the Very Simple Structure Criterion (VSS).
</p>
<p>8) Using Wayne Velicer's Minimum Average Partial (MAP) criterion. 
</p>
<p>Each of the procedures has its advantages and disadvantages.  Using either the chi square test or the change in square test is, of course, sensitive to the number of subjects and leads to the nonsensical condition that if one wants to find many factors, one simply runs more subjects. Parallel analysis is partially sensitive to sample size in that for large samples the eigen values of random factors will be very small.  The scree test is quite appealling but can lead to differences of interpretation as to when the scree &quot;breaks&quot;. The eigen value of 1 rule, although the default for many programs, seems to be a rough way of dividing the number of variables by 3.  Extracting interpretable factors means that the number of factors reflects the investigators creativity more than the data.  VSS, while very simple to understand, will not work very well if the data are very factorially complex. (Simulations suggests it will work fine if the complexities of some of the items are no more than 2).
</p>
<p>Most users of factor analysis tend to interpret factor output by focusing their attention on the largest loadings for every variable and ignoring the smaller ones.  Very Simple Structure operationalizes this tendency by  comparing the original correlation matrix to that reproduced by a simplified version (S) of the original factor matrix (F).  R = SS' + U2.   S is composed of just the c greatest (in absolute value) loadings for each variable.  C (or complexity) is a parameter of the model and may vary from 1 to the number of factors.  
</p>
<p>The VSS criterion compares the fit of the simplified model to the original correlations: VSS = 1 -sumsquares(r*)/sumsquares(r)  where R* is the residual matrix R* = R - SS' and r* and r are the elements of R* and R respectively. 
</p>
<p>VSS for a given complexity will tend to peak at the optimal (most interpretable) number of factors (Revelle and Rocklin, 1979). 
</p>
<p>Although originally written in Fortran for main frame computers, VSS has been adapted to micro computers (e.g., Macintosh OS 6-9) using Pascal. We now release R code for calculating VSS. 
</p>
<p>Note that if using a correlation matrix (e.g., my.matrix) and doing a factor analysis, the parameters n.obs should be specified for the factor analysis:
e.g., the call is VSS(my.matrix,n.obs=500).  Otherwise it defaults to 1000. 
</p>
<p>Wayne Velicer's MAP criterion has been added as an additional test for the optimal number of components to extract.  Note that VSS and MAP will not always agree as to the optimal number.
</p>
<p>The nfactors function will do a VSS, find MAP, and report a number of other criteria (e.g., BIC, complexity, chi square, ...)
</p>
<p>A variety of rotation options are available. These include varimax, promax, and oblimin. Others can be added.  Suggestions are welcome.
</p>
<p>vssSelect will perform a VSS analysis on subsets of the data where the subsets are specified by a keys list.
</p>


<h3>Value</h3>

<p> A data.frame with entries:
map:  Velicer's MAP values (lower values are better) <br />
dof:  degrees of freedom    (if using FA) <br />
chisq: chi square (from the factor analysis output  (if using FA) <br />
prob: probability of residual matrix &gt; 0  (if using FA) <br />
sqresid: squared residual correlations<br />
RMSEA: the RMSEA for each number of factors <br />
BIC: the BIC for each number of factors <br />
eChiSq: the empirically found chi square <br />
eRMS: Empirically found mean residual <br />
eCRMS: Empirically found mean residual corrected for df <br />
eBIC: The empirically found BIC based upon the eChiSq <br />
fit: factor fit of the complete model<br />
cfit.1: VSS fit of complexity 1<br />
cfit.2: VSS fit of complexity 2 <br />
... <br />
cfit.8: VSS fit of complexity 8<br />
cresidiual.1: sum squared residual correlations for complexity 1<br />
...:  sum squared residual correlations for complexity 2 ..8<br />
</p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/vss.html">https://personality-project.org/r/vss.html</a>,
Revelle, W. An introduction to psychometric theory with applications in R (in prep) Springer. Draft chapters available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>
<p>Revelle, W. and Rocklin, T. 1979, Very Simple Structure: an Alternative Procedure for Estimating the Optimal Number of Interpretable Factors, Multivariate Behavioral Research, 14, 403-414. 
<a href="https://personality-project.org/revelle/publications/vss.pdf">https://personality-project.org/revelle/publications/vss.pdf</a>
</p>
<p>Velicer, W. (1976) Determining the number of components from the matrix of partial correlations. Psychometrika, 41, 321-327.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+VSS.plot">VSS.plot</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code>,
<code><a href="#topic+fa.parallel">fa.parallel</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
#test.data &lt;- Harman74.cor$cov
#my.vss &lt;- VSS(test.data,title="VSS of 24 mental tests")      
#print(my.vss[,1:12],digits =2) 
#VSS.plot(my.vss, title="VSS of 24 mental tests")

#now, some simulated data with two factors
#VSS(sim.circ(nvar=24),fm="minres" ,title="VSS of 24 circumplex variables")
VSS(sim.item(nvar=24),fm="minres" ,title="VSS of 24 simple structure variables")

#vssSelect(bfi.keys, bfi) 
</code></pre>

<hr>
<h2 id='VSS.parallel'>Compare real and random VSS solutions</h2><span id='topic+VSS.parallel'></span>

<h3>Description</h3>

<p>Another useful test for the number of factors is when the eigen values of a random matrix are greater than the eigen values of a a real matrix. Here we show VSS solutions to random data. A better test is probably <code><a href="#topic+fa.parallel">fa.parallel</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VSS.parallel(ncases, nvariables,scree=FALSE,rotate="none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VSS.parallel_+3A_ncases">ncases</code></td>
<td>
<p>Number of simulated cases </p>
</td></tr>
<tr><td><code id="VSS.parallel_+3A_nvariables">nvariables</code></td>
<td>
<p> number of simulated variables </p>
</td></tr>
<tr><td><code id="VSS.parallel_+3A_scree">scree</code></td>
<td>
<p>Show a scree plot for random data &ndash; see  <code><a href="#topic+omega">omega</a></code></p>
</td></tr>
<tr><td><code id="VSS.parallel_+3A_rotate">rotate</code></td>
<td>
<p>rotate=&quot;none&quot; or rotate=&quot;varimax&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>VSS like output to be plotted by VSS.plot
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>References</h3>

<p>Very Simple Structure (VSS)</p>


<h3>See Also</h3>

  <p><code><a href="#topic+fa.parallel">fa.parallel</a></code>, <code><a href="#topic+VSS.plot">VSS.plot</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#VSS.plot(VSS.parallel(200,24))
</code></pre>

<hr>
<h2 id='VSS.plot'>Plot VSS fits</h2><span id='topic+VSS.plot'></span>

<h3>Description</h3>

<p>The Very Simple Structure criterion ( <code><a href="#topic+VSS">VSS</a></code>) for estimating the optimal number of factors is plotted as a function of the increasing complexity and increasing number of factors.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VSS.plot(x, title = "Very Simple Structure", line = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VSS.plot_+3A_x">x</code></td>
<td>
<p>output from VSS </p>
</td></tr>
<tr><td><code id="VSS.plot_+3A_title">title</code></td>
<td>
<p>any title </p>
</td></tr>
<tr><td><code id="VSS.plot_+3A_line">line</code></td>
<td>
<p> connect different complexities </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Item-factor models differ in their &quot;complexity&quot;.  Complexity 1 means that all except the greatest (absolute) loading for an item are ignored. Basically a cluster model (e.g., <code><a href="#topic+ICLUST">ICLUST</a></code>). Complexity 2 implies all except the greatest two, etc.  
</p>
<p>Different complexities can suggest different number of optimal number of factors to extract.  For personality items, complexity 1 and 2 are probably the most meaningful.
</p>
<p>The Very Simple Structure criterion will tend to peak at the number of factors that are most interpretable for a given level of complexity.  Note that some problems, the most interpretable number of factors will differ as a function of complexity.  For instance, when doing the Harman 24 psychological variable problems, an unrotated solution of complexity one suggests one factor (g), while a complexity two solution suggests that a four factor solution is most appropriate.  This latter probably reflects a bi-factor structure.  
</p>
<p>For examples of VSS.plot output, see <a href="https://personality-project.org/r/r.vss.html">https://personality-project.org/r/r.vss.html</a>
</p>


<h3>Value</h3>

<p>A plot window showing the VSS criterion varying as the number of factors and the complexity of the items.
</p>


<h3>Author(s)</h3>

<p>Maintainer: William Revelle <a href="mailto:revelle@northwestern.edu">revelle@northwestern.edu</a>
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/r.vss.html">https://personality-project.org/r/r.vss.html</a></p>


<h3>See Also</h3>

  <p><code><a href="#topic+VSS">VSS</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>test.data &lt;- Harman74.cor$cov
my.vss &lt;- VSS(test.data)         #suggests that 4 factor complexity two solution is optimal
VSS.plot(my.vss,title="VSS of Holzinger-Harmon problem")                 #see the graphics window


</code></pre>

<hr>
<h2 id='VSS.scree'>Plot the successive eigen values for a scree test</h2><span id='topic+VSS.scree'></span><span id='topic+scree'></span>

<h3>Description</h3>

<p>Cattell's scree test is one of most simple ways of testing the number of components or factors in  a correlation matrix. Here we plot the  eigen values of a correlation matrix as well as the eigen values of a factor analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scree(rx,factors=TRUE,pc=TRUE,main="Scree plot",hline=NULL,add=FALSE,sqrt=FALSE) 
VSS.scree(rx, main = "scree plot",sqrt=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VSS.scree_+3A_rx">rx</code></td>
<td>
<p> a correlation matrix or a data matrix. If data, then correlations are found using pairwise deletions. </p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_factors">factors</code></td>
<td>
<p>If true, draw the scree for factors </p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_pc">pc</code></td>
<td>
<p>If true, draw the scree for components</p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_hline">hline</code></td>
<td>
<p>if null, draw a horizontal line at 1, otherwise draw it at hline (make negative to not draw it)</p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_main">main</code></td>
<td>
<p> Title </p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_add">add</code></td>
<td>
<p>Should multiple plots be drawn?</p>
</td></tr>
<tr><td><code id="VSS.scree_+3A_sqrt">sqrt</code></td>
<td>
<p>If TRUE, take the sqrt of the eigen value before plotting</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Among the many ways to choose the optimal number of factors is the scree test.  A better function to show the scree as well as compare it to randomly parallel solutions is found found in <code><a href="#topic+fa.parallel">fa.parallel</a></code>
</p>
<p>Following a suggestion from Marco Del Giudice, I added the sqrt option for version 2.2.12. - 
</p>


<h3>Author(s)</h3>

<p>William Revelle 
</p>


<h3>References</h3>

 <p><a href="https://personality-project.org/r/vss.html">https://personality-project.org/r/vss.html</a></p>


<h3>See Also</h3>

  <p><code><a href="#topic+fa.parallel">fa.parallel</a></code> <code><a href="#topic+VSS.plot">VSS.plot</a></code>,  <code><a href="#topic+ICLUST">ICLUST</a></code>, <code><a href="#topic+omega">omega</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>scree(attitude)
#VSS.scree(cor(attitude)


</code></pre>

<hr>
<h2 id='winsor'>Find the Winsorized scores, means, sds or variances  for a vector, matrix, or data.frame </h2><span id='topic+winsor'></span><span id='topic+winsor.mean'></span><span id='topic+winsor.means'></span><span id='topic+winsor.sd'></span><span id='topic+winsor.var'></span>

<h3>Description</h3>

<p>Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    </p>


<h3>Usage</h3>

<pre><code class='language-R'>winsor(x, trim = 0.2, na.rm = TRUE)
winsor.mean(x, trim = 0.2, na.rm = TRUE)
winsor.means(x, trim = 0.2, na.rm = TRUE)  
winsor.sd(x, trim = 0.2, na.rm = TRUE)  
winsor.var(x, trim = 0.2, na.rm = TRUE)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="winsor_+3A_x">x</code></td>
<td>
<p>A data vector, matrix or data frame</p>
</td></tr>
<tr><td><code id="winsor_+3A_trim">trim</code></td>
<td>
<p>Percentage of data to move  from the top and bottom of the distributions</p>
</td></tr>
<tr><td><code id="winsor_+3A_na.rm">na.rm</code></td>
<td>
<p>Missing data are removed </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
</p>


<h3>Value</h3>

<p>A scalar or vector of winsorized scores or winsorized means, sds, or variances (depending upon the call).
</p>


<h3>Author(s)</h3>

<p>William Revelle with modifications suggested by Joe Paxton and a further correction added  (January, 2009) to preserve the original order for the winsor case.</p>


<h3>References</h3>

<p>Wilcox, Rand R. (2005) Introduction to robust estimation and hypothesis testing. Elsevier/Academic Press. Amsterdam ; Boston.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+interp.median">interp.median</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sat.act)
winsor.means(sat.act) #compare with the means of the winsorized scores
y &lt;- winsor(sat.act)
describe(y)
xy &lt;- data.frame(sat.act,y)
#pairs.panels(xy) #to see the effect of winsorizing 
x &lt;- matrix(1:100,ncol=5)
winsor(x)
winsor.means(x)
y &lt;- 1:11
winsor(y,trim=.5)
</code></pre>

<hr>
<h2 id='withinBetween'>An example of the distinction between within group and between group correlations</h2><span id='topic+withinBetween'></span>

<h3>Description</h3>

<p>A demonstration that a correlation may be decomposed to a within group correlation  and a between group correlations and these two correlations are independent.  Between group correlations are sometimes called ecological correlations, the decomposition into  within and between group correlations is a basic concept in multilevel modeling.  This data set shows the composite correlations between 9 variables, representing 16 cases with four groups.    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(withinBetween)</code></pre>


<h3>Format</h3>

<p>A data frame with 16 observations on the following 10 variables.
</p>

<dl>
<dt><code>Group</code></dt><dd><p>An example grouping factor.</p>
</dd>
<dt><code>V1</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V2</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V3</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V4</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V5</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V6</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V7</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V8</code></dt><dd><p>A column of 16 observations </p>
</dd>
<dt><code>V9</code></dt><dd><p>A column of 16 observations </p>
</dd>
</dl>



<h3>Details</h3>

<p>Correlations between individuals who belong to different natural groups (based upon e.g., ethnicity, age, gender, college major,or country) reflect an unknown mixture of the pooled correlation within each group as well as the correlation of the means of these groups. These two correlations are independent and do not allow inferences from one level (the group) to the other level (the individual).  This data set shows this independence.  The within group correlations between 9 variables are set to be 1, 0, and -1 while those between groups are also set to be 1, 0, -1.  These two sets of correlations are crossed such that V1, V4, and V7 have within group correlations of 1, as do V2, V5 and V8, and V3, V6 and V9.  V1 has a within group correlation of 0 with V2, V5, and V8, and a -1 within group correlation with V3, V6 and V9.  V1, V2, and V3 share a between group correlation of 1, as do V4, V5 and V6, and V7, V8 and V9.  The first group has a 0 between group correlation with the second and a -1 with the third group.  
</p>
<p><code><a href="#topic+statsBy">statsBy</a></code> can decompose the observed correlation in the between and within correlations.  <code><a href="#topic+sim.multilevel">sim.multilevel</a></code> can produce similar data.
</p>


<h3>Source</h3>

<p>The data were created for this example</p>


<h3>References</h3>

<p>P. D. Bliese. Multilevel modeling in R (2.3) a brief introduction to R, the multilevel package and the nlme package, 2009.
</p>
<p>Pedhazur, E.J. (1997) Multiple regression in behavioral research: explanation and prediction.  Harcourt Brace.
</p>
<p>Revelle, W. An introduction to psychometric theory with applications in R (in prep) Springer. Draft chapters available at  <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+statsBy">statsBy</a></code>,   <code><a href="#topic+describeBy">describeBy</a></code>, and  <code><a href="#topic+sim.multilevel">sim.multilevel</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(withinBetween)
pairs.panels(withinBetween,bg=c("red","blue","white","black")[withinBetween[,1]],
     pch=21,ellipses=FALSE,lm=TRUE)
stats &lt;- statsBy(withinBetween,'Group')
print(stats,short=FALSE)
</code></pre>

<hr>
<h2 id='Yule'>From a two by two table, find the Yule coefficients of association, convert to phi, or tetrachoric, recreate table the table to create the Yule coefficient.</h2><span id='topic+Yule'></span><span id='topic+Yule.inv'></span><span id='topic+Yule2phi'></span><span id='topic+Yule2tetra'></span><span id='topic+Yule2poly'></span><span id='topic+YuleBonett'></span><span id='topic+YuleCor'></span>

<h3>Description</h3>

<p>One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> a </td><td style="text-align: left;"> b </td><td style="text-align: left;"> R1 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> c </td><td style="text-align: left;"> d </td><td style="text-align: left;"> R2 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> C1 </td><td style="text-align: left;"> C2 </td><td style="text-align: left;"> n </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Yule Q is (ad - bc)/(ad+bc). <br />
Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the &ldquo;coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions&quot; (p 787). 
<br />
ad/bc is the odds ratio and Q = (OR-1)/(OR+1) 
<br />
Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)
Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code><a href="#topic+phi">phi</a></code> or tetrachoric correlations <code><a href="#topic+tetrachoric">tetrachoric</a></code>
<br />
Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.
</p>
<p>Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>YuleBonett(x,c=1,bonett=FALSE,alpha=.05) #find the generalized Yule cofficients
YuleCor(x,c=1,bonett=FALSE,alpha=.05) #do this for a matrix 
Yule(x,Y=FALSE)  #find Yule given a two by two table of frequencies
 #find the frequencies that produce a Yule Q given the Q and marginals
Yule.inv(Q,m,n=NULL)   
#find the phi coefficient that matches the Yule Q given the marginals
Yule2phi(Q,m,n=NULL)    
Yule2tetra(Q,m,n=NULL,correct=TRUE) 


   #Find the tetrachoric correlation given the Yule Q and the marginals
#(deprecated) Find the tetrachoric correlation given the Yule Q and the marginals   
Yule2poly(Q,m,n=NULL,correct=TRUE)   
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Yule_+3A_x">x</code></td>
<td>
<p>A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix </p>
</td></tr>
<tr><td><code id="Yule_+3A_c">c</code></td>
<td>
<p>1 returns Yule Q, .5, Yule's Y, .75 Digby's H</p>
</td></tr>
<tr><td><code id="Yule_+3A_bonett">bonett</code></td>
<td>
<p>If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient</p>
</td></tr>
<tr><td><code id="Yule_+3A_alpha">alpha</code></td>
<td>
<p>The two tailed probability for confidence intervals</p>
</td></tr>
<tr><td><code id="Yule_+3A_y">Y</code></td>
<td>
<p>Y=TRUE return Yule's Y coefficient of colligation</p>
</td></tr>
<tr><td><code id="Yule_+3A_q">Q</code></td>
<td>
<p>Either a single Yule coefficient or a matrix of Yule coefficients</p>
</td></tr>
<tr><td><code id="Yule_+3A_m">m</code></td>
<td>
<p>The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)</p>
</td></tr>
<tr><td><code id="Yule_+3A_n">n</code></td>
<td>
<p>The number of subjects (if the marginals are given as frequencies</p>
</td></tr>
<tr><td><code id="Yule_+3A_correct">correct</code></td>
<td>
<p>When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See <code>{link{tetrachoric}</code> for a discussion.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
</p>


<h3>Value</h3>

<table>
<tr><td><code>Q</code></td>
<td>
<p>The Yule Q coefficient</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>A two by two matrix of counts</p>
</td></tr>
<tr><td><code>result</code></td>
<td>
<p>If given matrix input, then a matrix of phis or tetrachorics</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>From YuleBonett and YuleCor</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>The upper and lower confidence intervals in matrix form (From YuleBonett and YuleCor).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Yule.inv is currently done by using the optimize function, but presumably could be redone by solving a quadratic equation.
</p>


<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Yule, G. Uday (1912) On the methods of measuring association between two attributes. Journal of the Royal Statistical Society, LXXV, 579-652
</p>
<p>Bonett, D.G. and Price, R.M, (2007) Statistical Inference for Generalized Yule Coefficients in 2 x 2 Contingency Tables. Sociological Methods and Research, 35, 429-446.
</p>
<p>Warrens, Matthijs (2008), On Association Coefficients for 2x2 Tables and Properties That Do Not Depend on the Marginal Distributions. Psychometrika, 73, 777-789. 
</p>


<h3>See Also</h3>

<p> See Also as <code><a href="#topic+phi">phi</a></code>, <code><a href="#topic+tetrachoric">tetrachoric</a></code>,  <code><a href="#topic+Yule2poly.matrix">Yule2poly.matrix</a></code>, <code><a href="#topic+Yule2phi.matrix">Yule2phi.matrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>Nach &lt;- matrix(c(40,10,20,50),ncol=2,byrow=TRUE)
Yule(Nach)
Yule.inv(.81818,c(50,60),n=120)
Yule2phi(.81818,c(50,60),n=120)
Yule2tetra(.81818,c(50,60),n=120)
phi(Nach)  #much less
#or express as percents and do not specify n
Nach &lt;- matrix(c(40,10,20,50),ncol=2,byrow=TRUE)
Nach/120
Yule(Nach)
Yule.inv(.81818,c(.41667,.5))
Yule2phi(.81818,c(.41667,.5))
Yule2tetra(.81818,c(.41667,.5))
phi(Nach)  #much less
YuleCor(psychTools::ability[,1:4],,TRUE)
YuleBonett(Nach,1)  #Yule Q
YuleBonett(Nach,.5)  #Yule Y
YuleBonett(Nach,.75)  #Digby H
YuleBonett(Nach,,TRUE)  #Yule* is a generalized Yule

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
