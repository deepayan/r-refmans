<!DOCTYPE html><html><head><title>Help for package snn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {snn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cv.tune'>
<p>Tuning via 5 fold Cross-Validation.</p></a></li>
<li><a href='#mybnn'>
<p>Bagged Nearest Neighbor Classifier</p></a></li>
<li><a href='#mycis'>
<p>Classification Instability</p></a></li>
<li><a href='#mydata'>
<p>Data Generator</p></a></li>
<li><a href='#myerror'>
<p>Classification Error</p></a></li>
<li><a href='#myknn'>
<p>K Nearest Neighbor Classifier</p></a></li>
<li><a href='#myownn'>
<p>Optimal Weighted Nearest Neighbor Classifier</p></a></li>
<li><a href='#mysnn'>
<p>Stabilized Nearest Neighbor Classifier</p></a></li>
<li><a href='#mywnn'>
<p>Weighted Nearest Neighbor Classifier</p></a></li>
<li><a href='#snn-package'>
<p>Package for Stabilized Nearest Neighbor Classifier</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Stabilized Nearest Neighbor Classifier</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-08-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Wei Sun, Xingye Qiao, and Guang Cheng</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wei Sun &lt;sunweisurrey8@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implement K-nearest neighbor classifier, weighted nearest neighbor classifier, bagged nearest neighbor classifier, optimal weighted nearest neighbor classifier and stabilized nearest neighbor classifier, and perform model selection via 5 fold cross-validation for them. This package also provides functions for computing the classification error and classification instability of a classification procedure.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0), stats</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-08-22 22:19:27 UTC; wei</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-08-23 10:22:09</td>
</tr>
</table>
<hr>
<h2 id='cv.tune'>
Tuning via 5 fold Cross-Validation.
</h2><span id='topic+cv.tune'></span>

<h3>Description</h3>

<p>Implement the tuning procedure for K-nearest neighbor classifier, bagged nearest neighbor classifier, optimal weighted nearest neighbor classifier, and stabilized nearest neighbor classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.tune(train, numgrid = 20, classifier = "snn")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.tune_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="cv.tune_+3A_numgrid">numgrid</code></td>
<td>

<p>Number of grids for search
</p>
</td></tr>
<tr><td><code id="cv.tune_+3A_classifier">classifier</code></td>
<td>

<p>The classifier for tuning. Possible choices are knn, bnn, ownn, snn.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the K-nearest neighbor classifier (knn), the grids for search are equal spaced integers in [1, n/2].
</p>
<p>Given the best k for the K-nearest neighbor classifier, the best parameter for the bagged nearest neighbor classifier (bnn) is computed via (3.5) in Samworth (2012).
</p>
<p>Given the best k for the K-nearest neighbor classifier, the best parameter for Samworth's optimal weighted nearest neighbor classifier (ownn) is computed via (2.9) in Samworth (2012).
</p>
<p>For the stabilized nearest neighbor classifier (snn), we first identify a set of lambda's whose corresponding risks are among the lower 10th percentiles, and then choose from them an optimal one which has the minimal estimated classification instability. The grids of lambda's are chosen such that each one is corresponding to an evenly spaced grid of k in [1, n/2]. See Sun et al. (2015) for details.
</p>


<h3>Value</h3>

<p>The returned list contains:
</p>
<table>
<tr><td><code>parameter.opt</code></td>
<td>
<p>The best tuning parameter for the chosen classifier. For example, the best K for knn and ownn, the best ratio for bnn, and the best lambda for snn.</p>
</td></tr>
<tr><td><code>parameter.list</code></td>
<td>
<p>The list of parameters in the grid search for the chosen classifier.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>References</h3>

<p>R.J. Samworth (2012), &quot;Optimal Weighted Nearest Neighbor Classifiers,&quot; Annals of Statistics, 40:5, 2733-2763.
</p>
<p>W. Sun, X. Qiao, and G. Cheng (2015) Stabilized Nearest Neighbor Classifier and Its Statistical Properties. Available at arxiv.org/abs/1405.6642. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	## Tuning procedure
	out.tune = cv.tune(DATA, classifier = "knn") 
	out.tune

</code></pre>

<hr>
<h2 id='mybnn'>
Bagged Nearest Neighbor Classifier
</h2><span id='topic+mybnn'></span>

<h3>Description</h3>

<p>Implement the bagged nearest neighbor classification algorithm to predict the label of a new input using a training data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mybnn(train, test, ratio)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mybnn_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="mybnn_+3A_test">test</code></td>
<td>

<p>Vector of a test point. It also admits a matrix input with each row representing a new test point.
</p>
</td></tr>
<tr><td><code id="mybnn_+3A_ratio">ratio</code></td>
<td>

<p>Resampling ratio.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bagged nearest neighbor classifier is asymptotically equivalent to a weighted nearest neighbor classifier with the i-th weight a function of the resampling ratio,
the sample size n, and i. See Hall and Samworth (2005) for details. The tuning parameter ratio can be tuned via cross-validation, see cv.tune function for the tuning procedure.
</p>


<h3>Value</h3>

<p>It returns the predicted class label of the new test point. If input is a matrix, it returns a vector which contains the predicted class labels of all the new test points.  
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng 
</p>


<h3>References</h3>

<p>Hall, P. and Samworth, R. (2005). Properties of Bagged Nearest Neighbor Classifiers. Journal of the Royal Statistical Society, Series B, 67, 363-379.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]
	
	# bagged nearest neighbor classifier
	mybnn(DATA, TEST.x, ratio = 0.5)

</code></pre>

<hr>
<h2 id='mycis'>
Classification Instability
</h2><span id='topic+mycis'></span>

<h3>Description</h3>

<p>Compute the classification instability of a classification procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mycis(predict1, predict2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mycis_+3A_predict1">predict1</code></td>
<td>

<p>The list of predicted labels based on one training data set.
</p>
</td></tr>
<tr><td><code id="mycis_+3A_predict2">predict2</code></td>
<td>

<p>The list of predicted labels based on another training data set.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>CIS of a classification procedure is defined as the probability that the same object is classified to two different classes by this classification procedure trained from two i.i.d. data sets.
Therefore, the arguments predict1 and predict2 are generated on the same test data from the same classification procedure trained on two i.i.d. training data sets.
CIS is among [0,1] and a smaller CIS represents a more stable classification procedure. See Section 2 of Sun et al. (2015) for details.
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>References</h3>

<p>W. Sun, X. Qiao, and G. Cheng (2015) Stabilized Nearest Neighbor Classifier and Its Statistical Properties. Available at arxiv.org/abs/1405.6642. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]

	## Compute classification instability for knn, bnn, ownn, and snn with given parameters
	nn=floor(n/2)  
	permIndex = sample(n)
	predict1.knn = myknn(DATA[permIndex[1:nn],], TEST.x, K = 5)
	predict2.knn = myknn(DATA[permIndex[-(1:nn)],], TEST.x, K = 5)
	predict1.bnn = mybnn(DATA[permIndex[1:nn],], TEST.x, ratio = 0.5)
	predict2.bnn = mybnn(DATA[permIndex[-(1:nn)],], TEST.x, ratio = 0.5)
	predict1.ownn = myownn(DATA[permIndex[1:nn],], TEST.x, K = 5)
	predict2.ownn = myownn(DATA[permIndex[-(1:nn)],], TEST.x, K = 5)
	predict1.snn = mysnn(DATA[permIndex[1:nn],], TEST.x, lambda = 10)
	predict2.snn = mysnn(DATA[permIndex[-(1:nn)],], TEST.x, lambda = 10)

	mycis(predict1.knn, predict2.knn)
	mycis(predict1.bnn, predict2.bnn)
	mycis(predict1.ownn, predict2.ownn)
	mycis(predict1.snn, predict2.snn)

</code></pre>

<hr>
<h2 id='mydata'>
Data Generator
</h2><span id='topic+mydata'></span>

<h3>Description</h3>

<p>Generate random data from mixture Gaussian distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mydata(n, d, mu = 0.8, portion = 1/2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mydata_+3A_n">n</code></td>
<td>

<p>The number of observations (sample size).
</p>
</td></tr>
<tr><td><code id="mydata_+3A_d">d</code></td>
<td>

<p>The number of variables (dimension).
</p>
</td></tr>
<tr><td><code id="mydata_+3A_mu">mu</code></td>
<td>

<p>In the Gaussian mixture model, the first Gaussian is generated with zero mean and identity covariance matrix.
The second Gaussian is generated with mean a d-dimensional vector with all mu and identity covariance matrix.
</p>
</td></tr>
<tr><td><code id="mydata_+3A_portion">portion</code></td>
<td>

<p>The prior probability for the first Gaussian component.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return the data matrix with n rows and d + 1 columns. Each row represents a sample generated from the mixture Gaussian distribution. The first d columns are features and the last column is the class label of the corresponding sample.
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)
	
	DATA.x = DATA[,1:d]
	DATA.y = DATA[,d+1]
	
</code></pre>

<hr>
<h2 id='myerror'>
Classification Error
</h2><span id='topic+myerror'></span>

<h3>Description</h3>

<p>Compute the error of the predict list given the true list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myerror(predict, true)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="myerror_+3A_predict">predict</code></td>
<td>

<p>The list of predicted labels
</p>
</td></tr>
<tr><td><code id="myerror_+3A_true">true</code></td>
<td>

<p>The list of true labels
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns the errors of the predicted labels from a classification algorithm. 
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)
	
	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]
	TEST.y = TEST[,d+1]
	
	## Compute the errors for knn, bnn, ownn, and snn with given parameters.
	predict.knn = myknn(DATA, TEST.x, K = 5)
	predict.bnn = mybnn(DATA, TEST.x, ratio = 0.5)
	predict.ownn = myownn(DATA, TEST.x, K = 5)
	predict.snn = mysnn(DATA, TEST.x, lambda = 10)

	myerror(predict.knn, TEST.y)
	myerror(predict.bnn, TEST.y)
	myerror(predict.ownn, TEST.y)
	myerror(predict.snn, TEST.y)

</code></pre>

<hr>
<h2 id='myknn'>
K Nearest Neighbor Classifier
</h2><span id='topic+myknn'></span>

<h3>Description</h3>

<p>Implement the K nearest neighbor classification algorithm to predict the label of a new input using a training data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myknn(train, test, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="myknn_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="myknn_+3A_test">test</code></td>
<td>

<p>Vector of a test point. It also admits a matrix input with each row representing a new test point.
</p>
</td></tr>
<tr><td><code id="myknn_+3A_k">K</code></td>
<td>

<p>Number of nearest neighbors considered.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tuning parameter K can be tuned via cross-validation, see cv.tune function for the tuning procedure.
</p>


<h3>Value</h3>

<p>It returns the predicted class label of the new test point. If input is a matrix, it returns a vector which contains the predicted class labels of all the new test points.  
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng 
</p>


<h3>References</h3>

<p>Fix, E. and Hodges, J. L., Jr. (1951). Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties. Randolph Field, Texas, Project 21-49-004, Report No.4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]
	
	# K nearest neighbor classifier
	myknn(DATA, TEST.x, K = 5)

</code></pre>

<hr>
<h2 id='myownn'>
Optimal Weighted Nearest Neighbor Classifier
</h2><span id='topic+myownn'></span>

<h3>Description</h3>

<p>Implement Samworth's optimal weighted nearest neighbor classification algorithm to predict the label of a new input using a training data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myownn(train, test, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="myownn_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="myownn_+3A_test">test</code></td>
<td>

<p>Vector of a test point. It also admits a matrix input with each row representing a new test point.
</p>
</td></tr>
<tr><td><code id="myownn_+3A_k">K</code></td>
<td>

<p>Number of nearest neighbors considered.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tuning parameter K can be tuned via cross-validation, see cv.tune function for the tuning procedure.
</p>


<h3>Value</h3>

<p>It returns the predicted class label of the new test point. If input is a matrix, it returns a vector which contains the predicted class labels of all the new test points.  
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>References</h3>

<p>R.J. Samworth (2012), &quot;Optimal Weighted Nearest Neighbor Classifiers,&quot; Annals of Statistics, 40:5, 2733-2763.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]
	
	# optimal weighted nearest neighbor classifier
	myownn(DATA, TEST.x, K = 5)

</code></pre>

<hr>
<h2 id='mysnn'>
Stabilized Nearest Neighbor Classifier
</h2><span id='topic+mysnn'></span>

<h3>Description</h3>

<p>Implement the stabilized nearest neighbor classification algorithm to predict the label of a new input using a training data set. The stabilized nearest neighbor classifier contains the K-nearest neighbor classifier and the optimal weighted nearest neighbor classifier as two special cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mysnn(train, test, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mysnn_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="mysnn_+3A_test">test</code></td>
<td>

<p>Vector of a test point. It also admits a matrix input with each row representing a new test point.
</p>
</td></tr>
<tr><td><code id="mysnn_+3A_lambda">lambda</code></td>
<td>

<p>Tuning parameter controlling the degree of stabilization of the nearest neighbor classification procedure. The larger lambda, the more stable the procedure is.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tuning parameter lambda can be tuned via cross-validation, see cv.tune for the tuning procedure.
</p>


<h3>Value</h3>

<p>It returns the predicted class label of the new test point. If input is a matrix, it returns a vector which contains the predicted class labels of all the new test points.  
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>References</h3>

<p>W. Sun, X. Qiao, and G. Cheng (2015) Stabilized Nearest Neighbor Classifier and Its Statistical Properties. Available at arxiv.org/abs/1405.6642. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# Training data
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	# Testing data
	set.seed(2015)
	ntest = 100  
	TEST = mydata(ntest, d)
	TEST.x = TEST[,1:d]
	
	# stabilized nearest neighbor classifier
	mysnn(DATA, TEST.x, lambda = 10)

</code></pre>

<hr>
<h2 id='mywnn'>
Weighted Nearest Neighbor Classifier
</h2><span id='topic+mywnn'></span>

<h3>Description</h3>

<p>Implement the weighted nearest neighbor classification algorithm to predict the label of a new input using a training data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mywnn(train, test, weight)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mywnn_+3A_train">train</code></td>
<td>

<p>Matrix of training data sets. An n by (d+1) matrix, where n is the sample size and d is the dimension. The last column is the class label.
</p>
</td></tr>
<tr><td><code id="mywnn_+3A_test">test</code></td>
<td>

<p>Vector of a test point.
</p>
</td></tr>
<tr><td><code id="mywnn_+3A_weight">weight</code></td>
<td>

<p>The weight vector for all n nearest neighbors.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns the predicted class label of the new test point. 
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	set.seed(1)
	n = 100
	d = 10
	DATA = mydata(n, d)

	## weighted nearest neighbor classifier
	weight.vec = c(rep(0.02,50), rep(0,50))
	mywnn(DATA, rep(-5,d), weight = weight.vec)

</code></pre>

<hr>
<h2 id='snn-package'>
Package for Stabilized Nearest Neighbor Classifier
</h2><span id='topic+snn-package'></span><span id='topic+snn'></span>

<h3>Description</h3>

<p>A package for implementations of various nearest neighbor classifiers, including K-nearest neighbor classifier, weighted nearest neighbor classifier, bagged nearest neighbor classifier, optimal weighted nearest neighbor classifier, and a new stabilized nearest neighbor classifier. This package also provides functions for computing the classification error and classification instability of a classification procedure.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> snn</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-07-31</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The package &quot;snn&quot; provides 8 main functions:
(1) the classification error.
(2) the classification instability.
(3) the K-nearest neighbor classifier.
(4) the weighted neighbor classifier.
(5) the bagged nearest neighbor classifier.
(6) the optimal nearest neighbor classifier.
(7) the stabilized nearest neighbor classifier.
(8) the model selection via cross-validation for K-nearest neighbor classifier, bagged nearest neighbor classifier, optimal nearest neighbor classifier, and stabilized nearest neighbor classifier.
</p>


<h3>Author(s)</h3>

<p>Wei Sun, Xingye Qiao, and Guang Cheng
</p>
<p>Maintainer: Wei Sun  &lt;sunweisurrey8@gmail.com&gt;
</p>


<h3>References</h3>

<p>W. Sun, X. Qiao, and G. Cheng (2015) Stabilized Nearest Neighbor Classifier and Its Statistical Properties. Available at arxiv.org/abs/1405.6642. 
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
