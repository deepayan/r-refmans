<!DOCTYPE html><html><head><title>Help for package choosepc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {choosepc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#choosepc-package'>
<p>Choose the Number of Principal Components via Recistruction Error</p></a></li>
<li><a href='#Choose+20the+20number+20of+20principal+20components+20via+20reconstruction+20error'>
<p>Choose the number of principal components via reconstruction error</p></a></li>
<li><a href='#Confidence+20interval+20for+20the+20percentage+20of+20variance+20retained+20by+20the+20first+20k+20components'>
<p>Confidence interval for the percentage of variance retained by the first <code class="reqn">\kappa</code> components</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Choose the Number of Principal Components via Recistruction
Error</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-23</td>
</tr>
<tr>
<td>Author:</td>
<td>Michail Tsagris [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michail Tsagris &lt;mtsagris@uoc.gr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, Rfast2, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>One way to choose the number of principal components is via the reconstruction error. This package is designed mainly for this purpose. Graphical representation is also supported, plus some other principal component analysis related functions. References include: Jolliffe I.T. (2002). Principal Component Analysis. &lt;<a href="https://doi.org/10.1007%2Fb98835">doi:10.1007/b98835</a>&gt; and Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. ISBN: 978-0124712522. London: Academic Press.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-23 13:15:40 UTC; mtsag</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-24 18:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='choosepc-package'>
Choose the Number of Principal Components via Recistruction Error
</h2><span id='topic+choosepc-package'></span>

<h3>Description</h3>

<p>A new robust principal component analysis algorithm is implemented that relies upon the Cauchy Distribution. The algorithm is suitable for high dimensional data even if the sample size is less than the number of variables.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> choosepc</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-10-22</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers</h3>

<p>Michail Tsagris &lt;mtsagris@uoc.gr&gt;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Jolliffe I.T. (2002). Principal Component Analysis.
</p>

<hr>
<h2 id='Choose+20the+20number+20of+20principal+20components+20via+20reconstruction+20error'>
Choose the number of principal components via reconstruction error
</h2><span id='topic+pc.choose'></span>

<h3>Description</h3>

<p>Choose the number of principal components via reconstruction error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.choose(x, graph = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Choose+2B20the+2B20number+2B20of+2B20principal+2B20components+2B20via+2B20reconstruction+2B20error_+3A_x">x</code></td>
<td>

<p>A numerical matrix with more rows than columns.
</p>
</td></tr>
<tr><td><code id="Choose+2B20the+2B20number+2B20of+2B20principal+2B20components+2B20via+2B20reconstruction+2B20error_+3A_graph">graph</code></td>
<td>

<p>Should the plot of the PRESS values appear? Default value is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SVD stands for Singular Value Decomposition of a rectangular matrix. That is any matrix, not only a square one in contrast to the Spectral Decomposition with eigenvalues and eigenvectors, produced by principal component analysis (PCA). Suppose we have a <code class="reqn">n \times p</code> matrix <code class="reqn">\bf X</code>. Then using SVD we can write the matrix as </p>
<p style="text-align: center;"><code class="reqn">
{\bf X}={\bf UDV}^{T},
</code>
</p>

<p>where <code class="reqn">\bf U</code> is an orthonormal matrix containing the eigenvectors of <code class="reqn">{\bf XX}^T</code>, the <code class="reqn">\bf V</code> is an orthonormal matrix containing the eigenvectors of <code class="reqn">{\bf X}^T{\bf X}</code> and <code class="reqn">D</code> is a <code class="reqn">p \times p</code> diagonal matrix containing the <code class="reqn">r</code> non zero singular values <code class="reqn">d_1,\ldots,d_r</code> (square root of the eigenvalues) of <code class="reqn">{\bf XX}^T</code> (or <code class="reqn">{\bf X}^T{\bf X}</code>) and the remaining <code class="reqn">p-r</code> elements of the diagonal are zero. We remind that the maximum rank of an <code class="reqn">n \times p</code> matrix is equal to <code class="reqn">\min\{n,p\}</code>. Using the SVD decomposition equaiton above, each column of <code class="reqn">\bf X</code> can be written as
</p>
<p style="text-align: center;"><code class="reqn">
{\bf x}_j=\sum_{k=1}^r{\bf u}_kd_k{\bf v}_{jk}.
</code>
</p>

<p>This means that we can reconstruct the matrix <code class="reqn">\bf X</code> using less columns (if <code class="reqn">n&gt;p</code>) than it has.
</p>
<p style="text-align: center;"><code class="reqn">
\tilde{{\bf x}}^{m}_j=\sum_{k=1}^m{\bf u}_kd_k{\bf v}_{jk},
</code>
</p>

<p>where <code class="reqn">m&lt;r</code>.
</p>
<p>The reconstructed matrix will have some discrepancy of course, but it is the level of discrepancy we are interested in. If we center the matrix <code class="reqn">\bf X</code>, subtract the column means from every column, and perform the SVD again, we will see that the orthonormal matrix <code class="reqn">\bf V</code> contains the eigenvectors of the covariance matrix of the original, the un-centred, matrix <code class="reqn">\bf X</code>.
</p>
<p>Coming back to the a matrix of <code class="reqn">n</code> observations and <code class="reqn">p</code> variables, the question was how many principal components to retain. We will give an answer to this using SVD to reconstruct the matrix. We describe the steps of this algorithm below.
1. Center the matrix by subtracting from each variable its mean <code class="reqn">{\bf Y}={\bf X}-{\bf m}</code>
</p>
<p>2. Perform SVD on the centred matrix <code class="reqn">\bf Y</code>.
</p>
<p>3. Choose a number from <code class="reqn">1</code> to <code class="reqn">r</code> (the rank of the matrix) and reconstruct the matrix. Let us denote by <code class="reqn">\widetilde{{\bf Y}}^{m}</code> the reconstructed matrix.
</p>
<p>4. Calculate the sum of squared differences between the reconstructed and the original values
</p>
<p style="text-align: center;"><code class="reqn">
PRESS\left(m\right)=\sum_{i=1}^n\sum_{j=1}^p\left(\tilde{y}^{m}_{ij}-y_{ij}\right)^2, m=1,..,r.
</code>
</p>

<p>5. Plot <code class="reqn">PRESS\left(m\right)</code> for all the values of <code class="reqn">m</code> and choose graphically the number of principal components.
</p>
<p>The graphical way of choosing the number of principal components is not the best and there alternative ways of making a decision (see for example Jolliffe (2002)).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>values</code></td>
<td>

<p>The eigenvalues of the covariance matrix.
</p>
</td></tr>
<tr><td><code>cumprop</code></td>
<td>

<p>The cumulative proportion of the eigenvalues of the covariance matrix.
</p>
</td></tr>
<tr><td><code>per</code></td>
<td>

<p>The differences in the cumulative proportion of the eigenvalues of the covariance matrix.
</p>
</td></tr>
<tr><td><code>press</code></td>
<td>

<p>The reconstruction error <code class="reqn">\sqrt{\sum_{ij}{(x_{ij}-\hat{x}_{ij})^2}}</code> for each number of eigenvectors.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Jolliffe I.T. (2002). Principal Component Analysis.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eigci">eigci</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
a &lt;- pc.choose(x, graph = FALSE)
</code></pre>

<hr>
<h2 id='Confidence+20interval+20for+20the+20percentage+20of+20variance+20retained+20by+20the+20first+20k+20components'>
Confidence interval for the percentage of variance retained by the first <code class="reqn">\kappa</code> components
</h2><span id='topic+eigci'></span>

<h3>Description</h3>

<p>Confidence interval for the percentage of variance retained by the first <code class="reqn">\kappa</code> components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eigci(x, k, alpha = 0.05, B = 1000, graph = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Confidence+2B20interval+2B20for+2B20the+2B20percentage+2B20of+2B20variance+2B20retained+2B20by+2B20the+2B20first+2B20k+2B20components_+3A_x">x</code></td>
<td>

<p>A numerical matrix with more rows than columns.
</p>
</td></tr>
<tr><td><code id="Confidence+2B20interval+2B20for+2B20the+2B20percentage+2B20of+2B20variance+2B20retained+2B20by+2B20the+2B20first+2B20k+2B20components_+3A_k">k</code></td>
<td>

<p>The number of principal components to use.
</p>
</td></tr>
<tr><td><code id="Confidence+2B20interval+2B20for+2B20the+2B20percentage+2B20of+2B20variance+2B20retained+2B20by+2B20the+2B20first+2B20k+2B20components_+3A_alpha">alpha</code></td>
<td>
<p>This is the significance level. Based on this, an <code class="reqn">(1-\alpha)\%</code> confidence interval will be computed.
</p>
</td></tr>
<tr><td><code id="Confidence+2B20interval+2B20for+2B20the+2B20percentage+2B20of+2B20variance+2B20retained+2B20by+2B20the+2B20first+2B20k+2B20components_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to generate.
</p>
</td></tr>
<tr><td><code id="Confidence+2B20interval+2B20for+2B20the+2B20percentage+2B20of+2B20variance+2B20retained+2B20by+2B20the+2B20first+2B20k+2B20components_+3A_graph">graph</code></td>
<td>

<p>Should the plot of the bootstrap replicates appear? Default value is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is taken by Mardia Kent and Bibby (1979, pg. 233&ndash;234). The percentage retained by the fist <code class="reqn">\kappa</code> principal components denoted by <code class="reqn">\hat{\psi}</code> is equal to
</p>
<p style="text-align: center;"><code class="reqn">
\hat{\psi}=\frac{ \sum_{i=1}^{\kappa}\hat{\lambda}_i }{\sum_{j=1}^p\hat{\lambda}_j },
</code>
</p>

<p>where <code class="reqn">\hat{\psi}</code> is asymptotically normal with mean <code class="reqn">\psi</code> and variance
</p>
<p style="text-align: center;"><code class="reqn">
\tau^2 = \frac{2}{\left(n-1\right)\left(tr\pmb{\Sigma} \right)^2}\left[ \left(1-\psi\right)^2\left(\lambda_1^2+...+\lambda_k^2\right)+
\psi^2\left(\lambda_{\kappa+1}^2+...\lambda_p^2\right) \right],
</code>
</p>

<p>where
<code class="reqn">a=\left( \lambda_1^2+...+\lambda_k^2\right)/\left( \lambda_1^2+...+\lambda_p^2\right)</code>
and <code class="reqn">\text{tr}\pmb{\Sigma}^2=\lambda_1^2+...+\lambda_p^2</code>.
</p>
<p>The bootstrap version provides an estimate of the bias, defined as <code class="reqn">\hat{\psi}_{boot}-\hat{\psi}</code> and confidence intervals calculated via the percentile method and via the standard (or normal) method Efron and Tibshirani (1993). The funciton gives the option to perform bootstrap.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>If B=1 (no bootstrap) a vector with the esimated percentage of variance due to the first <code class="reqn">k</code> components, <code class="reqn">\hat{\psi}</code> and its associated asymptotic <code class="reqn">(1-\alpha)\%</code> confidence interval.
If B&gt;1 (bootstrap) a vector with: the esimated percentage of variance due to the first <code class="reqn">k</code> components, <code class="reqn">\hat{\psi}</code>, its bootstrap estimate and its bootstrap estimated bias.
</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>

<p>This appears if B&gt;1 (bootstrap). The standard bootstrap and the empirical bootstrap <code class="reqn">(1-\alpha)\%</code> confidence interval for <code class="reqn">\psi</code>.
</p>
</td></tr>
</table>
<p>Futher, if B&gt;1 and &quot;graph&quot; was set equal to TRUE, a histogram with the bootstrap <code class="reqn">\hat{\psi}</code> values, the observed <code class="reqn">\hat{\psi}</code> value and its bootstrap estimate.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K.V., Kent, J.T. and Bibby, J.M. (1979). Multivariate Analysis.
London: Academic
Press.
</p>
<p>Efron B. and Tibshirani R. J. (1993). An introduction to the bootstrap.
Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pc.choose">pc.choose</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
eigci(x, k = 2, B = 1)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
