<!DOCTYPE html><html><head><title>Help for package olsrr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {olsrr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#auto'><p>Test Data Set</p></a></li>
<li><a href='#cement'><p>Test Data Set</p></a></li>
<li><a href='#fitness'><p>Test Data Set</p></a></li>
<li><a href='#hsb'><p>Test Data Set</p></a></li>
<li><a href='#ols_aic'><p>Akaike information criterion</p></a></li>
<li><a href='#ols_apc'><p>Amemiya's prediction criterion</p></a></li>
<li><a href='#ols_coll_diag'><p>Collinearity diagnostics</p></a></li>
<li><a href='#ols_correlations'><p>Part and partial correlations</p></a></li>
<li><a href='#ols_fpe'><p>Final prediction error</p></a></li>
<li><a href='#ols_hadi'><p>Hadi's influence measure</p></a></li>
<li><a href='#ols_hsp'><p>Hocking's Sp</p></a></li>
<li><a href='#ols_launch_app'><p>Launch shiny app</p></a></li>
<li><a href='#ols_leverage'><p>Leverage</p></a></li>
<li><a href='#ols_mallows_cp'><p>Mallow's Cp</p></a></li>
<li><a href='#ols_msep'><p>MSEP</p></a></li>
<li><a href='#ols_plot_added_variable'><p>Added variable plots</p></a></li>
<li><a href='#ols_plot_comp_plus_resid'><p>Residual plus component plot</p></a></li>
<li><a href='#ols_plot_cooksd_bar'><p>Cooks' D bar plot</p></a></li>
<li><a href='#ols_plot_cooksd_chart'><p>Cooks' D chart</p></a></li>
<li><a href='#ols_plot_dfbetas'><p>DFBETAs panel</p></a></li>
<li><a href='#ols_plot_dffits'><p>DFFITS plot</p></a></li>
<li><a href='#ols_plot_diagnostics'><p>Diagnostics panel</p></a></li>
<li><a href='#ols_plot_hadi'><p>Hadi plot</p></a></li>
<li><a href='#ols_plot_obs_fit'><p>Observed vs fitted values plot</p></a></li>
<li><a href='#ols_plot_reg_line'><p>Simple linear regression line</p></a></li>
<li><a href='#ols_plot_resid_box'><p>Residual box plot</p></a></li>
<li><a href='#ols_plot_resid_fit'><p>Residual vs fitted plot</p></a></li>
<li><a href='#ols_plot_resid_fit_spread'><p>Residual fit spread plot</p></a></li>
<li><a href='#ols_plot_resid_hist'><p>Residual histogram</p></a></li>
<li><a href='#ols_plot_resid_lev'><p>Studentized residuals vs leverage plot</p></a></li>
<li><a href='#ols_plot_resid_pot'><p>Potential residual plot</p></a></li>
<li><a href='#ols_plot_resid_qq'><p>Residual QQ plot</p></a></li>
<li><a href='#ols_plot_resid_regressor'><p>Residual vs regressor plot</p></a></li>
<li><a href='#ols_plot_resid_stand'><p>Standardized residual chart</p></a></li>
<li><a href='#ols_plot_resid_stud'><p>Studentized residual plot</p></a></li>
<li><a href='#ols_plot_resid_stud_fit'><p>Deleted studentized residual vs fitted values plot</p></a></li>
<li><a href='#ols_plot_response'><p>Response variable profile</p></a></li>
<li><a href='#ols_pred_rsq'><p>Predicted rsquare</p></a></li>
<li><a href='#ols_prep_avplot_data'><p>Added variable plot data</p></a></li>
<li><a href='#ols_prep_cdplot_data'><p>Cooks' D plot data</p></a></li>
<li><a href='#ols_prep_cdplot_outliers'><p>Cooks' d outlier data</p></a></li>
<li><a href='#ols_prep_dfbeta_data'><p>DFBETAs plot data</p></a></li>
<li><a href='#ols_prep_dfbeta_outliers'><p>DFBETAs plot outliers</p></a></li>
<li><a href='#ols_prep_dsrvf_data'><p>Deleted studentized residual plot data</p></a></li>
<li><a href='#ols_prep_outlier_obs'><p>Cooks' D outlier observations</p></a></li>
<li><a href='#ols_prep_regress_x'><p>Regress predictor on other predictors</p></a></li>
<li><a href='#ols_prep_regress_y'><p>Regress y on other predictors</p></a></li>
<li><a href='#ols_prep_rfsplot_fmdata'><p>Residual fit spread plot data</p></a></li>
<li><a href='#ols_prep_rstudlev_data'><p>Studentized residual vs leverage plot data</p></a></li>
<li><a href='#ols_prep_rvsrplot_data'><p>Residual vs regressor plot data</p></a></li>
<li><a href='#ols_prep_srchart_data'><p>Standardized residual chart data</p></a></li>
<li><a href='#ols_prep_srplot_data'><p>Studentized residual plot data</p></a></li>
<li><a href='#ols_press'><p>PRESS</p></a></li>
<li><a href='#ols_pure_error_anova'><p>Lack of fit F test</p></a></li>
<li><a href='#ols_regress'><p>Ordinary least squares regression</p></a></li>
<li><a href='#ols_sbc'><p>Bayesian information criterion</p></a></li>
<li><a href='#ols_sbic'><p>Sawa's bayesian information criterion</p></a></li>
<li><a href='#ols_step_all_possible'><p>All possible regression</p></a></li>
<li><a href='#ols_step_all_possible_betas'><p>All possible regression variable coefficients</p></a></li>
<li><a href='#ols_step_backward_adj_r2'><p>Stepwise Adjusted R-Squared backward regression</p></a></li>
<li><a href='#ols_step_backward_aic'><p>Stepwise AIC backward regression</p></a></li>
<li><a href='#ols_step_backward_p'><p>Stepwise backward regression</p></a></li>
<li><a href='#ols_step_backward_r2'><p>Stepwise R-Squared backward regression</p></a></li>
<li><a href='#ols_step_backward_sbc'><p>Stepwise SBC backward regression</p></a></li>
<li><a href='#ols_step_backward_sbic'><p>Stepwise SBIC backward regression</p></a></li>
<li><a href='#ols_step_best_subset'><p>Best subsets regression</p></a></li>
<li><a href='#ols_step_both_adj_r2'><p>Stepwise Adjusted R-Squared regression</p></a></li>
<li><a href='#ols_step_both_aic'><p>Stepwise AIC regression</p></a></li>
<li><a href='#ols_step_both_p'><p>Stepwise regression</p></a></li>
<li><a href='#ols_step_both_r2'><p>Stepwise R-Squared regression</p></a></li>
<li><a href='#ols_step_both_sbc'><p>Stepwise SBC regression</p></a></li>
<li><a href='#ols_step_both_sbic'><p>Stepwise SBIC regression</p></a></li>
<li><a href='#ols_step_forward_adj_r2'><p>Stepwise Adjusted R-Squared forward regression</p></a></li>
<li><a href='#ols_step_forward_aic'><p>Stepwise AIC forward regression</p></a></li>
<li><a href='#ols_step_forward_p'><p>Stepwise forward regression</p></a></li>
<li><a href='#ols_step_forward_r2'><p>Stepwise R-Squared forward regression</p></a></li>
<li><a href='#ols_step_forward_sbc'><p>Stepwise SBC forward regression</p></a></li>
<li><a href='#ols_step_forward_sbic'><p>Stepwise SBIC forward regression</p></a></li>
<li><a href='#ols_test_bartlett'><p>Bartlett test</p></a></li>
<li><a href='#ols_test_breusch_pagan'><p>Breusch pagan test</p></a></li>
<li><a href='#ols_test_correlation'><p>Correlation test for normality</p></a></li>
<li><a href='#ols_test_f'><p>F test</p></a></li>
<li><a href='#ols_test_normality'><p>Test for normality</p></a></li>
<li><a href='#ols_test_outlier'><p>Bonferroni Outlier Test</p></a></li>
<li><a href='#ols_test_score'><p>Score test</p></a></li>
<li><a href='#olsrr'><p><code>olsrr</code> package</p></a></li>
<li><a href='#rivers'><p>Test Data Set</p></a></li>
<li><a href='#rvsr_plot_shiny'><p>Residual vs regressors plot for shiny app</p></a></li>
<li><a href='#stepdata'><p>Test Data Set</p></a></li>
<li><a href='#surgical'><p>Surgical Unit Data Set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Building OLS Regression Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools designed to make it easier for users, particularly beginner/intermediate R users 
    to build ordinary least squares regression models. Includes comprehensive regression output, 
    heteroskedasticity tests, collinearity diagnostics, residual diagnostics, measures of influence, 
    model fit assessment and variable selection procedures.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>car, ggplot2, goftest, graphics, gridExtra, nortest, stats,
utils, xplorerr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, descriptr, knitr, rmarkdown, testthat, vdiffr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://olsrr.rsquaredacademy.com/">https://olsrr.rsquaredacademy.com/</a>,
<a href="https://github.com/rsquaredacademy/olsrr">https://github.com/rsquaredacademy/olsrr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/rsquaredacademy/olsrr/issues">https://github.com/rsquaredacademy/olsrr/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-12 09:50:32 UTC; HP</td>
</tr>
<tr>
<td>Author:</td>
<td>Aravind Hebbali [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aravind Hebbali &lt;hebbali.aravind@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-12 11:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='auto'>Test Data Set</h2><span id='topic+auto'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto
</code></pre>


<h3>Format</h3>

<p>An object of class <code>tbl_df</code> (inherits from <code>tbl</code>, <code>data.frame</code>) with 74 rows and 11 columns.
</p>

<hr>
<h2 id='cement'>Test Data Set</h2><span id='topic+cement'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cement
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 13 rows and 6 columns.
</p>

<hr>
<h2 id='fitness'>Test Data Set</h2><span id='topic+fitness'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitness
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 31 rows and 7 columns.
</p>

<hr>
<h2 id='hsb'>Test Data Set</h2><span id='topic+hsb'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hsb
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 200 rows and 15 columns.
</p>

<hr>
<h2 id='ols_aic'>Akaike information criterion</h2><span id='topic+ols_aic'></span>

<h3>Description</h3>

<p>Akaike information criterion for model selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_aic(model, method = c("R", "STATA", "SAS"), corrected = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_aic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_aic_+3A_method">method</code></td>
<td>
<p>A character vector; specify the method to compute AIC. Valid
options include R, STATA and SAS.</p>
</td></tr>
<tr><td><code id="ols_aic_+3A_corrected">corrected</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns corrected akaike information criterion for SAS method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AIC provides a means for model selection. Given a collection of models for
the data, AIC estimates the quality of each model, relative to each of the
other models. R and STATA use loglikelihood to compute AIC. SAS uses residual
sum of squares. Below is the formula in each case:
</p>
<p><em>R &amp; STATA</em>
</p>
<p style="text-align: center;"><code class="reqn">AIC = -2(loglikelihood) + 2p</code>
</p>

<p><em>SAS</em>
</p>
<p style="text-align: center;"><code class="reqn">AIC = n * ln(SSE / n) + 2p</code>
</p>

<p><em>corrected</em>
</p>
<p style="text-align: center;"><code class="reqn">AIC = n * ln(SSE / n) + ((n * (n + p)) / (n - p - 2))</code>
</p>

<p>where <em>n</em> is the sample size and <em>p</em> is the number of model parameters including intercept.
</p>


<h3>Value</h3>

<p>Akaike information criterion of the model.
</p>


<h3>References</h3>

<p>Akaike, H. (1969). “Fitting Autoregressive Models for Prediction.” Annals of the Institute of Statistical
Mathematics 21:243–247.
</p>
<p>Judge, G. G., Griffiths, W. E., Hill, R. C., and Lee, T.-C. (1980). The Theory and Practice of Econometrics.
New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># using R computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_aic(model)

# using STATA computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_aic(model, method = 'STATA')

# using SAS computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_aic(model, method = 'SAS')

# corrected akaike information criterion
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_aic(model, method = 'SAS', corrected = TRUE)

</code></pre>

<hr>
<h2 id='ols_apc'>Amemiya's prediction criterion</h2><span id='topic+ols_apc'></span>

<h3>Description</h3>

<p>Amemiya's prediction error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_apc(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_apc_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Amemiya's Prediction Criterion penalizes R-squared more heavily than does
adjusted R-squared for each addition degree of freedom used on the
right-hand-side of the equation.  The lower the better for this criterion.
</p>
<p style="text-align: center;"><code class="reqn">((n + p) / (n - p))(1 - (R^2))</code>
</p>

<p>where <em>n</em> is the sample size, <em>p</em> is the number of predictors including the intercept and
<em>R^2</em> is the coefficient of determination.
</p>


<h3>Value</h3>

<p>Amemiya's prediction error of the model.
</p>


<h3>References</h3>

<p>Amemiya, T. (1976). Selection of Regressors. Technical Report 225, Stanford University, Stanford, CA.
</p>
<p>Judge, G. G., Griffiths, W. E., Hill, R. C., and Lee, T.-C. (1980). The Theory and Practice of Econometrics.
New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_apc(model)

</code></pre>

<hr>
<h2 id='ols_coll_diag'>Collinearity diagnostics</h2><span id='topic+ols_coll_diag'></span><span id='topic+ols_vif_tol'></span><span id='topic+ols_eigen_cindex'></span>

<h3>Description</h3>

<p>Variance inflation factor, tolerance, eigenvalues and condition indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_coll_diag(model)

ols_vif_tol(model)

ols_eigen_cindex(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_coll_diag_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Collinearity implies two variables are near perfect linear combinations of
one another. Multicollinearity involves more than two variables. In the
presence of multicollinearity, regression estimates are unstable and have
high standard errors.
</p>
<p><em>Tolerance</em>
</p>
<p>Percent of variance in the predictor that cannot be accounted for by other predictors.
</p>
<p>Steps to calculate tolerance:
</p>

<ul>
<li><p> Regress the kth predictor on rest of the predictors in the model.
</p>
</li>
<li><p> Compute <code class="reqn">R^2</code> - the coefficient of determination from the regression in the above step.
</p>
</li>
<li> <p><code class="reqn">Tolerance = 1 - R^2</code>
</p>
</li></ul>

<p><em>Variance Inflation Factor</em>
</p>
<p>Variance inflation factors measure the inflation in the variances of the parameter estimates due to
collinearities that exist among the predictors. It is a measure of how much the variance of the estimated
regression coefficient <code class="reqn">\beta_k</code>  is inflated by the existence of correlation among the predictor variables
in the model. A VIF of 1 means that there is no correlation among the kth predictor and the remaining predictor
variables, and hence the variance of <code class="reqn">\beta_k</code> is not inflated at all. The general rule of thumb is that VIFs
exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity
requiring correction.
</p>
<p>Steps to calculate VIF:
</p>

<ul>
<li><p> Regress the kth predictor on rest of the predictors in the model.
</p>
</li>
<li><p> Compute <code class="reqn">R^2</code> - the coefficient of determination from the regression in the above step.
</p>
</li>
<li> <p><code class="reqn">Tolerance = 1 / 1 - R^2 = 1 / Tolerance</code>
</p>
</li></ul>

<p><em>Condition Index</em>
</p>
<p>Most multivariate statistical approaches involve decomposing a correlation matrix into linear
combinations of variables. The linear combinations are chosen so that the first combination has
the largest possible variance (subject to some restrictions), the second combination
has the next largest variance, subject to being uncorrelated with the first, the third has the largest
possible variance, subject to being uncorrelated with the first and second, and so forth. The variance
of each of these linear combinations is called an eigenvalue. Collinearity is spotted by finding 2 or
more variables that have large proportions of variance (.50 or more) that correspond to large condition
indices. A rule of thumb is to label as large those condition indices in the range of 30 or larger.
</p>


<h3>Value</h3>

<p><code>ols_coll_diag</code> returns an object of class <code>"ols_coll_diag"</code>.
An object of class <code>"ols_coll_diag"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>vif_t</code></td>
<td>
<p>tolerance and variance inflation factors</p>
</td></tr>
<tr><td><code>eig_cindex</code></td>
<td>
<p>eigen values and condition index</p>
</td></tr>
</table>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). Regression Diagnostics: Identifying Influential Data and
Sources of Collinearity. New York: John Wiley &amp; Sons.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(mpg ~ disp + hp + wt + drat, data = mtcars)

# vif and tolerance
ols_vif_tol(model)

# eigenvalues and condition indices
ols_eigen_cindex(model)

# collinearity diagnostics
ols_coll_diag(model)

</code></pre>

<hr>
<h2 id='ols_correlations'>Part and partial correlations</h2><span id='topic+ols_correlations'></span>

<h3>Description</h3>

<p>Zero-order, part and partial correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_correlations(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_correlations_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ols_correlations()</code> returns the relative importance of independent
variables in determining response variable. How much each variable uniquely
contributes to rsquare over and above that which can be accounted for by the
other predictors? Zero order correlation is the Pearson correlation
coefficient between the dependent variable and the independent variables.
Part correlations indicates how much rsquare will decrease if that variable
is removed from the model and partial correlations indicates amount of
variance in response variable, which is not estimated by the other
independent variables in the model, but is estimated by the specific
variable.
</p>


<h3>Value</h3>

<p><code>ols_correlations</code> returns an object of class <code>"ols_correlations"</code>.
An object of class <code>"ols_correlations"</code> is a data frame containing the
following components:
</p>
<table>
<tr><td><code>Zero-order</code></td>
<td>
<p>zero order correlations</p>
</td></tr>
<tr><td><code>Partial</code></td>
<td>
<p>partial correlations</p>
</td></tr>
<tr><td><code>Part</code></td>
<td>
<p>part correlations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Morrison, D. F. 1976. Multivariate statistical methods. New York: McGraw-Hill.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_correlations(model)

</code></pre>

<hr>
<h2 id='ols_fpe'>Final prediction error</h2><span id='topic+ols_fpe'></span>

<h3>Description</h3>

<p>Estimated mean square error of prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_fpe(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_fpe_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the estimated mean square error of prediction for each model
selected assuming that the values of the regressors are fixed and that the
model is correct.
</p>
<p style="text-align: center;"><code class="reqn">MSE((n + p) / n)</code>
</p>

<p>where <code class="reqn">MSE = SSE / (n - p)</code>, n is the sample size and p is the number of predictors including the intercept
</p>


<h3>Value</h3>

<p>Final prediction error of the model.
</p>


<h3>References</h3>

<p>Akaike, H. (1969). “Fitting Autoregressive Models for Prediction.” Annals of the Institute of Statistical
Mathematics 21:243–247.
</p>
<p>Judge, G. G., Griffiths, W. E., Hill, R. C., and Lee, T.-C. (1980). The Theory and Practice of Econometrics.
New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_fpe(model)

</code></pre>

<hr>
<h2 id='ols_hadi'>Hadi's influence measure</h2><span id='topic+ols_hadi'></span>

<h3>Description</h3>

<p>Measure of influence based on the fact that influential observations in
either the response variable or in the predictors or both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_hadi(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_hadi_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hadi's measure of the model.
</p>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>


<h3>See Also</h3>

<p>Other influence measures: 
<code><a href="#topic+ols_leverage">ols_leverage</a>()</code>,
<code><a href="#topic+ols_pred_rsq">ols_pred_rsq</a>()</code>,
<code><a href="#topic+ols_press">ols_press</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_hadi(model)

</code></pre>

<hr>
<h2 id='ols_hsp'>Hocking's Sp</h2><span id='topic+ols_hsp'></span>

<h3>Description</h3>

<p>Average prediction mean squared error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_hsp(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_hsp_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hocking's Sp criterion is an adjustment of the residual sum of
Squares. Minimize this criterion.
</p>
<p style="text-align: center;"><code class="reqn">MSE / (n - p - 1)</code>
</p>

<p>where <code class="reqn">MSE = SSE / (n - p)</code>, n is the sample size and p is the number of predictors including the intercept
</p>


<h3>Value</h3>

<p>Hocking's Sp of the model.
</p>


<h3>References</h3>

<p>Hocking, R. R. (1976). “The Analysis and Selection of Variables in a Linear Regression.” Biometrics
32:1–50.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_hsp(model)

</code></pre>

<hr>
<h2 id='ols_launch_app'>Launch shiny app</h2><span id='topic+ols_launch_app'></span>

<h3>Description</h3>

<p>Launches shiny app for interactive model building.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_launch_app()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ols_launch_app()

## End(Not run)
</code></pre>

<hr>
<h2 id='ols_leverage'>Leverage</h2><span id='topic+ols_leverage'></span>

<h3>Description</h3>

<p>The leverage of an observation is based on how much the observation's value
on the predictor variable differs from the mean of the predictor variable.
The greater an observation's leverage, the more potential it has to be an
influential observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_leverage(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_leverage_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Leverage of the model.
</p>


<h3>References</h3>

<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>See Also</h3>

<p>Other influence measures: 
<code><a href="#topic+ols_hadi">ols_hadi</a>()</code>,
<code><a href="#topic+ols_pred_rsq">ols_pred_rsq</a>()</code>,
<code><a href="#topic+ols_press">ols_press</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_leverage(model)

</code></pre>

<hr>
<h2 id='ols_mallows_cp'>Mallow's Cp</h2><span id='topic+ols_mallows_cp'></span>

<h3>Description</h3>

<p>Mallow's Cp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_mallows_cp(model, fullmodel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_mallows_cp_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_mallows_cp_+3A_fullmodel">fullmodel</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mallows' Cp statistic estimates the size of the bias that is introduced into
the predicted responses by having an underspecified model. Use Mallows' Cp
to choose between multiple regression models. Look for models where
Mallows' Cp is small and close to the number of predictors in the model plus
the constant (p).
</p>


<h3>Value</h3>

<p>Mallow's Cp of the model.
</p>


<h3>References</h3>

<p>Hocking, R. R. (1976). “The Analysis and Selection of Variables in a Linear Regression.” Biometrics
32:1–50.
</p>
<p>Mallows, C. L. (1973). “Some Comments on Cp.” Technometrics 15:661–675.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>full_model &lt;- lm(mpg ~ ., data = mtcars)
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_mallows_cp(model, full_model)

</code></pre>

<hr>
<h2 id='ols_msep'>MSEP</h2><span id='topic+ols_msep'></span>

<h3>Description</h3>

<p>Estimated error of prediction, assuming multivariate normality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_msep(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_msep_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the estimated mean square error of prediction assuming that both
independent and dependent variables are multivariate normal.
</p>
<p style="text-align: center;"><code class="reqn">MSE(n + 1)(n - 2) / n(n - p - 1)</code>
</p>

<p>where <code class="reqn">MSE = SSE / (n - p)</code>, n is the sample size and p is the number of
predictors including the intercept
</p>


<h3>Value</h3>

<p>Estimated error of prediction of the model.
</p>


<h3>References</h3>

<p>Stein, C. (1960). “Multiple Regression.” In Contributions to Probability and Statistics: Essays in Honor
of Harold Hotelling, edited by I. Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, and H. B. Mann,
264–305. Stanford, CA: Stanford University Press.
</p>
<p>Darlington, R. B. (1968). “Multiple Regression in Psychological Research and Practice.” Psychological
Bulletin 69:161–182.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_msep(model)

</code></pre>

<hr>
<h2 id='ols_plot_added_variable'>Added variable plots</h2><span id='topic+ols_plot_added_variable'></span>

<h3>Description</h3>

<p>Added variable plot provides information about the marginal importance of a
predictor variable, given the other predictor variables already in
the model. It shows the marginal importance of the variable in reducing the
residual variability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_added_variable(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_added_variable_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_added_variable_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The added variable plot was introduced by Mosteller and Tukey
(1977). It enables us to visualize the regression coefficient of a new
variable being considered to be included in a model. The plot can be
constructed for each predictor variable.
</p>
<p>Let us assume we want to test the effect of adding/removing variable <em>X</em> from a
model. Let the response variable of the model be <em>Y</em>
</p>
<p>Steps to construct an added variable plot:
</p>

<ul>
<li><p> Regress <em>Y</em> on all variables other than <em>X</em> and store the residuals (<em>Y</em> residuals).
</p>
</li>
<li><p> Regress <em>X</em> on all the other variables included in the model (<em>X</em> residuals).
</p>
</li>
<li><p> Construct a scatter plot of <em>Y</em> residuals and <em>X</em> residuals.
</p>
</li></ul>

<p>What do the <em>Y</em> and <em>X</em> residuals represent? The <em>Y</em> residuals represent the part
of <strong>Y</strong> not explained by all the variables other than X. The <em>X</em> residuals
represent the part of <strong>X</strong> not explained by other variables. The slope of the line
fitted to the points in the added variable plot is equal to the regression
coefficient when <strong>Y</strong> is regressed on all variables including <strong>X</strong>.
</p>
<p>A strong linear relationship in the added variable plot indicates the increased
importance of the contribution of <strong>X</strong> to the model already containing the
other predictors.
</p>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>
<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_regressor">ols_plot_resid_regressor()</a></code>, <code><a href="#topic+ols_plot_comp_plus_resid">ols_plot_comp_plus_resid()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_added_variable(model)

</code></pre>

<hr>
<h2 id='ols_plot_comp_plus_resid'>Residual plus component plot</h2><span id='topic+ols_plot_comp_plus_resid'></span>

<h3>Description</h3>

<p>The residual plus component plot indicates whether any non-linearity is
present in the relationship between response and predictor variables and can
suggest possible transformations for linearizing the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_comp_plus_resid(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_comp_plus_resid_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_comp_plus_resid_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>
<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_added_variable">ols_plot_added_variable()</a></code>, <code><a href="#topic+ols_plot_resid_regressor">ols_plot_resid_regressor()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_comp_plus_resid(model)

</code></pre>

<hr>
<h2 id='ols_plot_cooksd_bar'>Cooks' D bar plot</h2><span id='topic+ols_plot_cooksd_bar'></span>

<h3>Description</h3>

<p>Bar Plot of cook's distance to detect observations that strongly influence
fitted values of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_cooksd_bar(model, type = 1, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_cooksd_bar_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_bar_+3A_type">type</code></td>
<td>
<p>An integer between 1 and 5 selecting one of the 5 methods for
computing the threshold.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_bar_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_bar_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a
plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cook's distance was introduced by American statistician R Dennis Cook in
1977. It is used to identify influential data points. It depends on both the
residual and leverage i.e it takes it account both the <em>x</em> value and
<em>y</em> value of the observation.
</p>
<p>Steps to compute Cook's distance:
</p>

<ul>
<li><p> Delete observations one at a time.
</p>
</li>
<li><p> Refit the regression model on remaining <code class="reqn">n - 1</code> observations
</p>
</li>
<li><p> examine how much all of the fitted values change when the ith observation is deleted.
</p>
</li></ul>

<p>A data point having a large cook's d indicates that the data point strongly
influences the fitted values. There are several methods/formulas to compute
the threshold used for detecting or classifying observations as outliers and
we list them below.
</p>

<ul>
<li> <p><strong>Type 1</strong> : 4 / n
</p>
</li>
<li> <p><strong>Type 2</strong> : 4 / (n - k - 1)
</p>
</li>
<li> <p><strong>Type 3</strong> : ~1
</p>
</li>
<li> <p><strong>Type 4</strong> : 1 / (n - k - 1)
</p>
</li>
<li> <p><strong>Type 5</strong> : 3 * mean(Vector of cook's distance values)
</p>
</li></ul>

<p>where <strong>n</strong> and <strong>k</strong> stand for
</p>

<ul>
<li> <p><strong>n</strong>: Number of observations
</p>
</li>
<li> <p><strong>k</strong>: Number of predictors
</p>
</li></ul>



<h3>Value</h3>

<p><code>ols_plot_cooksd_bar</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number and <code>cooks distance</code> that exceed <code>threshold</code></p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_cooksd_chart">ols_plot_cooksd_chart()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_cooksd_bar(model)
ols_plot_cooksd_bar(model, type = 4)
ols_plot_cooksd_bar(model, threshold = 0.2)

</code></pre>

<hr>
<h2 id='ols_plot_cooksd_chart'>Cooks' D chart</h2><span id='topic+ols_plot_cooksd_chart'></span>

<h3>Description</h3>

<p>Chart of cook's distance to detect observations that strongly influence
fitted values of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_cooksd_chart(model, type = 1, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_cooksd_chart_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_chart_+3A_type">type</code></td>
<td>
<p>An integer between 1 and 5 selecting one of the 6 methods for computing the threshold.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_chart_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers.</p>
</td></tr>
<tr><td><code id="ols_plot_cooksd_chart_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cook's distance was introduced by American statistician R Dennis Cook in
1977. It is used to identify influential data points. It depends on both the
residual and leverage i.e it takes it account both the <em>x</em> value and
<em>y</em> value of the observation.
</p>
<p>Steps to compute Cook's distance:
</p>

<ul>
<li><p> Delete observations one at a time.
</p>
</li>
<li><p> Refit the regression model on remaining <code class="reqn">n - 1</code> observations
</p>
</li>
<li><p> exmine how much all of the fitted values change when the ith observation is deleted.
</p>
</li></ul>

<p>A data point having a large cook's d indicates that the data point strongly
influences the fitted values. There are several methods/formulas to compute
the threshold used for detecting or classifying observations as outliers
and we list them below.
</p>

<ul>
<li> <p><strong>Type 1</strong> : 4 / n
</p>
</li>
<li> <p><strong>Type 2</strong> : 4 / (n - k - 1)
</p>
</li>
<li> <p><strong>Type 3</strong> : ~1
</p>
</li>
<li> <p><strong>Type 4</strong> : 1 / (n - k - 1)
</p>
</li>
<li> <p><strong>Type 5</strong> : 3 * mean(Vector of cook's distance values)
</p>
</li></ul>

<p>where <strong>n</strong> and <strong>k</strong> stand for
</p>

<ul>
<li> <p><strong>n</strong>: Number of observations
</p>
</li>
<li> <p><strong>k</strong>: Number of predictors
</p>
</li></ul>



<h3>Value</h3>

<p><code>ols_plot_cooksd_chart</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number and <code>cooks distance</code> that exceed <code>threshold</code></p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_cooksd_bar">ols_plot_cooksd_bar()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_cooksd_chart(model)
ols_plot_cooksd_chart(model, type = 4)
ols_plot_cooksd_chart(model, threshold = 0.2)

</code></pre>

<hr>
<h2 id='ols_plot_dfbetas'>DFBETAs panel</h2><span id='topic+ols_plot_dfbetas'></span>

<h3>Description</h3>

<p>Panel of plots to detect influential observations using DFBETAs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_dfbetas(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_dfbetas_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_dfbetas_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DFBETA measures the difference in each parameter estimate with and without
the influential point. There is a DFBETA for each data point i.e if there are
n observations and k variables, there will be <code class="reqn">n * k</code> DFBETAs. In
general, large values of DFBETAS indicate observations that are influential
in estimating a given parameter. Belsley, Kuh, and Welsch recommend 2 as a
general cutoff value to indicate influential observations and
<code class="reqn">2/\sqrt(n)</code> as a size-adjusted cutoff.
</p>


<h3>Value</h3>

<p>list; <code>ols_plot_dfbetas</code> returns a list of <code>data.frame</code> (for intercept and each predictor)
with the observation number and DFBETA of observations that exceed the threshold for classifying
an observation as an outlier/influential observation.
</p>


<h3>References</h3>

<p>Belsley, David A.; Kuh, Edwin; Welsh, Roy E. (1980). Regression
Diagnostics: Identifying Influential Data and Sources of Collinearity.
</p>
<p>Wiley Series in Probability and Mathematical Statistics.
New York: John Wiley &amp; Sons. pp. ISBN 0-471-05856-4.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_dffits">ols_plot_dffits()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_dfbetas(model)

</code></pre>

<hr>
<h2 id='ols_plot_dffits'>DFFITS plot</h2><span id='topic+ols_plot_dffits'></span>

<h3>Description</h3>

<p>Plot for detecting influential observations using DFFITs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_dffits(model, size_adj_threshold = TRUE, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_dffits_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_dffits_+3A_size_adj_threshold">size_adj_threshold</code></td>
<td>
<p>logical; if <code>TRUE</code> (the default), size
adjusted threshold is used to determine influential observations.</p>
</td></tr>
<tr><td><code id="ols_plot_dffits_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a
plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DFFIT - difference in fits, is used to identify influential data points. It
quantifies the number of standard deviations that the fitted value changes
when the ith data point is omitted.
</p>
<p>Steps to compute DFFITs:
</p>

<ul>
<li><p> Delete observations one at a time.
</p>
</li>
<li><p> Refit the regression model on remaining <code class="reqn">n - 1</code> observations
</p>
</li>
<li><p> examine how much all of the fitted values change when the ith observation is deleted.
</p>
</li></ul>

<p>An observation is deemed influential if the absolute value of its DFFITS value is greater than:
</p>
<p style="text-align: center;"><code class="reqn">2\sqrt((p + 1) / (n - p -1))</code>
</p>

<p>A size-adjusted cutoff recommended by Belsley, Kuh, and Welsch is
</p>
<p style="text-align: center;"><code class="reqn">2\sqrt(p / n)</code>
</p>
<p> and is used by default in <strong>olsrr</strong>.
</p>
<p>where <code>n</code> is the number of observations and <code>p</code> is the number of predictors including intercept.
</p>


<h3>Value</h3>

<p><code>ols_plot_dffits</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number and <code>DFFITs</code> that exceed <code>threshold</code></p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier</p>
</td></tr>
</table>


<h3>References</h3>

<p>Belsley, David A.; Kuh, Edwin; Welsh, Roy E. (1980). Regression
Diagnostics: Identifying Influential Data and Sources of Collinearity.
</p>
<p>Wiley Series in Probability and Mathematical Statistics.
New York: John Wiley &amp; Sons. ISBN 0-471-05856-4.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_dfbetas">ols_plot_dfbetas()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_dffits(model)
ols_plot_dffits(model, size_adj_threshold = FALSE)

</code></pre>

<hr>
<h2 id='ols_plot_diagnostics'>Diagnostics panel</h2><span id='topic+ols_plot_diagnostics'></span>

<h3>Description</h3>

<p>Panel of plots for regression diagnostics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_diagnostics(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_diagnostics_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_diagnostics_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_diagnostics(model)

</code></pre>

<hr>
<h2 id='ols_plot_hadi'>Hadi plot</h2><span id='topic+ols_plot_hadi'></span>

<h3>Description</h3>

<p>Hadi's measure of influence based on the fact that influential observations
can be present in either the response variable or in the predictors or both.
The plot is used to detect influential observations based on Hadi's measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_hadi(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_hadi_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_hadi_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_pot">ols_plot_resid_pot()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_hadi(model)

</code></pre>

<hr>
<h2 id='ols_plot_obs_fit'>Observed vs fitted values plot</h2><span id='topic+ols_plot_obs_fit'></span>

<h3>Description</h3>

<p>Plot of observed vs fitted values to assess the fit of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_obs_fit(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_obs_fit_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_obs_fit_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ideally, all your points should be close to a regressed diagonal line. Draw
such a diagonal line within your graph and check out where the points lie. If
your model had a high R Square, all the points would be close to this
diagonal line. The lower the R Square, the weaker the Goodness of fit of your
model, the more foggy or dispersed your points are from this diagonal line.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_obs_fit(model)

</code></pre>

<hr>
<h2 id='ols_plot_reg_line'>Simple linear regression line</h2><span id='topic+ols_plot_reg_line'></span>

<h3>Description</h3>

<p>Plot to demonstrate that the regression line always passes  through mean of
the response and predictor variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_reg_line(response, predictor, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_reg_line_+3A_response">response</code></td>
<td>
<p>Response variable.</p>
</td></tr>
<tr><td><code id="ols_plot_reg_line_+3A_predictor">predictor</code></td>
<td>
<p>Predictor variable.</p>
</td></tr>
<tr><td><code id="ols_plot_reg_line_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>ols_plot_reg_line(mtcars$mpg, mtcars$disp)

</code></pre>

<hr>
<h2 id='ols_plot_resid_box'>Residual box plot</h2><span id='topic+ols_plot_resid_box'></span>

<h3>Description</h3>

<p>Box plot of residuals to examine if residuals are normally distributed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_box(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_box_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_box_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_fit">ols_plot_resid_fit</a>()</code>,
<code><a href="#topic+ols_plot_resid_hist">ols_plot_resid_hist</a>()</code>,
<code><a href="#topic+ols_plot_resid_qq">ols_plot_resid_qq</a>()</code>,
<code><a href="#topic+ols_test_correlation">ols_test_correlation</a>()</code>,
<code><a href="#topic+ols_test_normality">ols_test_normality</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_box(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_fit'>Residual vs fitted plot</h2><span id='topic+ols_plot_resid_fit'></span>

<h3>Description</h3>

<p>Scatter plot of residuals on the y axis and fitted values on the
x axis to detect non-linearity, unequal error variances, and outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_fit(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_fit_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_fit_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Characteristics of a well behaved residual vs fitted plot:
</p>

<ul>
<li><p> The residuals spread randomly around the 0 line indicating that the relationship is linear.
</p>
</li>
<li><p> The residuals form an approximate horizontal band around the 0 line indicating homogeneity of error variance.
</p>
</li>
<li><p> No one residual is visibly away from the random pattern of the residuals indicating that there are no outliers.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_box">ols_plot_resid_box</a>()</code>,
<code><a href="#topic+ols_plot_resid_hist">ols_plot_resid_hist</a>()</code>,
<code><a href="#topic+ols_plot_resid_qq">ols_plot_resid_qq</a>()</code>,
<code><a href="#topic+ols_test_correlation">ols_test_correlation</a>()</code>,
<code><a href="#topic+ols_test_normality">ols_test_normality</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_fit(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_fit_spread'>Residual fit spread plot</h2><span id='topic+ols_plot_resid_fit_spread'></span><span id='topic+ols_plot_fm'></span><span id='topic+ols_plot_resid_spread'></span>

<h3>Description</h3>

<p>Plot to detect non-linearity, influential observations and outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_fit_spread(model, print_plot = TRUE)

ols_plot_fm(model, print_plot = TRUE)

ols_plot_resid_spread(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_fit_spread_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_fit_spread_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consists of side-by-side quantile plots of the centered fit and the
residuals. It shows how much variation in the data is explained by the fit
and how much remains in the residuals. For inappropriate models, the spread
of the residuals in such a plot is often greater than the spread of the
centered fit.
</p>


<h3>References</h3>

<p>Cleveland, W. S. (1993). Visualizing Data. Summit, NJ: Hobart Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)

# residual fit spread plot
ols_plot_resid_fit_spread(model)

# fit mean plot
ols_plot_fm(model)

# residual spread plot
ols_plot_resid_spread(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_hist'>Residual histogram</h2><span id='topic+ols_plot_resid_hist'></span>

<h3>Description</h3>

<p>Histogram of residuals for detecting violation of normality assumption.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_hist(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_hist_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_hist_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_box">ols_plot_resid_box</a>()</code>,
<code><a href="#topic+ols_plot_resid_fit">ols_plot_resid_fit</a>()</code>,
<code><a href="#topic+ols_plot_resid_qq">ols_plot_resid_qq</a>()</code>,
<code><a href="#topic+ols_test_correlation">ols_test_correlation</a>()</code>,
<code><a href="#topic+ols_test_normality">ols_test_normality</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_hist(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_lev'>Studentized residuals vs leverage plot</h2><span id='topic+ols_plot_resid_lev'></span>

<h3>Description</h3>

<p>Graph for detecting outliers and/or observations with high leverage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_lev(model, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_lev_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_lev_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_lev_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_stud_fit">ols_plot_resid_stud_fit()</a></code>, <code><a href="#topic+ols_plot_resid_lev">ols_plot_resid_lev()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(read ~ write + math + science, data = hsb)
ols_plot_resid_lev(model)
ols_plot_resid_lev(model, threshold = 3)

</code></pre>

<hr>
<h2 id='ols_plot_resid_pot'>Potential residual plot</h2><span id='topic+ols_plot_resid_pot'></span>

<h3>Description</h3>

<p>Plot to aid in classifying unusual observations as high-leverage points,
outliers, or a combination of both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_pot(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_pot_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_pot_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_hadi">ols_plot_hadi()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_pot(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_qq'>Residual QQ plot</h2><span id='topic+ols_plot_resid_qq'></span>

<h3>Description</h3>

<p>Graph for detecting violation of normality assumption.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_qq(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_qq_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_qq_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_box">ols_plot_resid_box</a>()</code>,
<code><a href="#topic+ols_plot_resid_fit">ols_plot_resid_fit</a>()</code>,
<code><a href="#topic+ols_plot_resid_hist">ols_plot_resid_hist</a>()</code>,
<code><a href="#topic+ols_test_correlation">ols_test_correlation</a>()</code>,
<code><a href="#topic+ols_test_normality">ols_test_normality</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_qq(model)

</code></pre>

<hr>
<h2 id='ols_plot_resid_regressor'>Residual vs regressor plot</h2><span id='topic+ols_plot_resid_regressor'></span>

<h3>Description</h3>

<p>Graph to determine whether we should add a new predictor to the model already
containing other predictors. The residuals from the model is regressed on the
new predictor and if the plot shows non random pattern, you should consider
adding the new predictor to the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_regressor(model, variable, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_regressor_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_regressor_+3A_variable">variable</code></td>
<td>
<p>New predictor to be added to the <code>model</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_regressor_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_added_variable">ols_plot_added_variable()</a></code>, <code><a href="#topic+ols_plot_comp_plus_resid">ols_plot_comp_plus_resid()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_regressor(model, 'drat')

</code></pre>

<hr>
<h2 id='ols_plot_resid_stand'>Standardized residual chart</h2><span id='topic+ols_plot_resid_stand'></span>

<h3>Description</h3>

<p>Chart for identifying outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_stand(model, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_stand_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stand_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stand_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standardized residual (internally studentized) is the residual divided by
estimated standard deviation.
</p>


<h3>Value</h3>

<p><code>ols_plot_resid_stand</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number and <code>standardized resiudals</code> that
exceed <code>threshold</code></p>
</td></tr></table>
<p> for classifying an observation as an outlier
</p>
<table>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_stud">ols_plot_resid_stud()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_stand(model)
ols_plot_resid_stand(model, threshold = 3)

</code></pre>

<hr>
<h2 id='ols_plot_resid_stud'>Studentized residual plot</h2><span id='topic+ols_plot_resid_stud'></span>

<h3>Description</h3>

<p>Graph for identifying outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_stud(model, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_stud_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stud_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 3.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stud_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Studentized deleted residuals (or externally studentized residuals) is the
deleted residual divided by its estimated standard deviation. Studentized
residuals are going to be more effective for detecting outlying Y
observations than standardized residuals. If an observation has an externally
studentized residual that is larger than 3 (in absolute value) we can call
it an outlier.
</p>


<h3>Value</h3>

<p><code>ols_plot_resid_stud</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number and <code>studentized residuals</code> that
exceed <code>threshold</code></p>
</td></tr></table>
<p> for classifying an observation as an outlier
</p>
<table>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_stand">ols_plot_resid_stand()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_resid_stud(model)
ols_plot_resid_stud(model, threshold = 2)

</code></pre>

<hr>
<h2 id='ols_plot_resid_stud_fit'>Deleted studentized residual vs fitted values plot</h2><span id='topic+ols_plot_resid_stud_fit'></span>

<h3>Description</h3>

<p>Plot for detecting violation of assumptions about residuals such as
non-linearity, constant variances and outliers. It can also be used to
examine model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_resid_stud_fit(model, threshold = NULL, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_resid_stud_fit_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stud_fit_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
<tr><td><code id="ols_plot_resid_stud_fit_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Studentized deleted residuals (or externally studentized residuals) is the
deleted residual divided by its estimated standard deviation. Studentized
residuals are going to be more effective for detecting outlying Y
observations than standardized residuals. If an observation has an externally
studentized residual that is larger than 2 (in absolute value) we can call
it an outlier.
</p>


<h3>Value</h3>

<p><code>ols_plot_resid_stud_fit</code> returns  a list containing the
following components:
</p>
<table>
<tr><td><code>outliers</code></td>
<td>
<p>a <code>data.frame</code> with observation number, fitted values and deleted studentized
residuals that exceed the <code>threshold</code> for classifying observations as
outliers/influential observations</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p><code>threshold</code> for classifying an observation as an outlier/influential observation</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols_plot_resid_lev">ols_plot_resid_lev()</a></code>, <code><a href="#topic+ols_plot_resid_stand">ols_plot_resid_stand()</a></code>,
<code><a href="#topic+ols_plot_resid_stud">ols_plot_resid_stud()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_resid_stud_fit(model)
ols_plot_resid_stud_fit(model, threshold = 3)

</code></pre>

<hr>
<h2 id='ols_plot_response'>Response variable profile</h2><span id='topic+ols_plot_response'></span>

<h3>Description</h3>

<p>Panel of plots to explore and visualize the response variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_plot_response(model, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_plot_response_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_plot_response_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_response(model)

</code></pre>

<hr>
<h2 id='ols_pred_rsq'>Predicted rsquare</h2><span id='topic+ols_pred_rsq'></span>

<h3>Description</h3>

<p>Use predicted rsquared to determine how well the model predicts responses for
new observations. Larger values of predicted R2 indicate models of greater
predictive ability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_pred_rsq(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_pred_rsq_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predicted rsquare of the model.
</p>


<h3>See Also</h3>

<p>Other influence measures: 
<code><a href="#topic+ols_hadi">ols_hadi</a>()</code>,
<code><a href="#topic+ols_leverage">ols_leverage</a>()</code>,
<code><a href="#topic+ols_press">ols_press</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_pred_rsq(model)

</code></pre>

<hr>
<h2 id='ols_prep_avplot_data'>Added variable plot data</h2><span id='topic+ols_prep_avplot_data'></span>

<h3>Description</h3>

<p>Data for generating the added variable plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_avplot_data(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_avplot_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_prep_avplot_data(model)

</code></pre>

<hr>
<h2 id='ols_prep_cdplot_data'>Cooks' D plot data</h2><span id='topic+ols_prep_cdplot_data'></span>

<h3>Description</h3>

<p>Prepare data for cook's d bar plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_cdplot_data(model, type = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_cdplot_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_cdplot_data_+3A_type">type</code></td>
<td>
<p>An integer between 1 and 5 selecting one of the 6 methods for computing the threshold.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_prep_cdplot_data(model)

</code></pre>

<hr>
<h2 id='ols_prep_cdplot_outliers'>Cooks' d outlier data</h2><span id='topic+ols_prep_cdplot_outliers'></span>

<h3>Description</h3>

<p>Outlier data for cook's d bar plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_cdplot_outliers(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_cdplot_outliers_+3A_k">k</code></td>
<td>
<p>Cooks' d bar plot data.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
k &lt;- ols_prep_cdplot_data(model)
ols_prep_cdplot_outliers(k)

</code></pre>

<hr>
<h2 id='ols_prep_dfbeta_data'>DFBETAs plot data</h2><span id='topic+ols_prep_dfbeta_data'></span>

<h3>Description</h3>

<p>Prepares the data for dfbetas plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_dfbeta_data(d, threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_dfbeta_data_+3A_d">d</code></td>
<td>
<p>A <code>tibble</code> or <code>data.frame</code> with dfbetas.</p>
</td></tr>
<tr><td><code id="ols_prep_dfbeta_data_+3A_threshold">threshold</code></td>
<td>
<p>The threshold for outliers.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
dfb &lt;- dfbetas(model)
n &lt;- nrow(dfb)
threshold &lt;- 2 / sqrt(n)
dbetas  &lt;- dfb[, 1]
df_data &lt;- data.frame(obs = seq_len(n), dbetas = dbetas)
ols_prep_dfbeta_data(df_data, threshold)

</code></pre>

<hr>
<h2 id='ols_prep_dfbeta_outliers'>DFBETAs plot outliers</h2><span id='topic+ols_prep_dfbeta_outliers'></span>

<h3>Description</h3>

<p>Data for identifying outliers in dfbetas plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_dfbeta_outliers(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_dfbeta_outliers_+3A_d">d</code></td>
<td>
<p>A <code>tibble</code> or <code>data.frame</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
dfb &lt;- dfbetas(model)
n &lt;- nrow(dfb)
threshold &lt;- 2 / sqrt(n)
dbetas  &lt;- dfb[, 1]
df_data &lt;- data.frame(obs = seq_len(n), dbetas = dbetas)
d &lt;- ols_prep_dfbeta_data(df_data, threshold)
ols_prep_dfbeta_outliers(d)

</code></pre>

<hr>
<h2 id='ols_prep_dsrvf_data'>Deleted studentized residual plot data</h2><span id='topic+ols_prep_dsrvf_data'></span>

<h3>Description</h3>

<p>Generates data for deleted studentized residual vs fitted plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_dsrvf_data(model, threshold = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_dsrvf_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_dsrvf_data_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_prep_dsrvf_data(model)
ols_prep_dsrvf_data(model, threshold = 3)

</code></pre>

<hr>
<h2 id='ols_prep_outlier_obs'>Cooks' D outlier observations</h2><span id='topic+ols_prep_outlier_obs'></span>

<h3>Description</h3>

<p>Identify outliers in cook's d plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_outlier_obs(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_outlier_obs_+3A_k">k</code></td>
<td>
<p>Cooks' d bar plot data.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
k &lt;- ols_prep_cdplot_data(model)
ols_prep_outlier_obs(k)

</code></pre>

<hr>
<h2 id='ols_prep_regress_x'>Regress predictor on other predictors</h2><span id='topic+ols_prep_regress_x'></span>

<h3>Description</h3>

<p>Regress a predictor in the model on all the other predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_regress_x(data, i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_regress_x_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_regress_x_+3A_i">i</code></td>
<td>
<p>A numeric vector (indicates the predictor in the model).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
data &lt;- ols_prep_avplot_data(model)
ols_prep_regress_x(data, 1)

</code></pre>

<hr>
<h2 id='ols_prep_regress_y'>Regress y on other predictors</h2><span id='topic+ols_prep_regress_y'></span>

<h3>Description</h3>

<p>Regress y on all the predictors except the ith predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_regress_y(data, i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_regress_y_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_regress_y_+3A_i">i</code></td>
<td>
<p>A numeric vector (indicates the predictor in the model).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
data &lt;- ols_prep_avplot_data(model)
ols_prep_regress_y(data, 1)

</code></pre>

<hr>
<h2 id='ols_prep_rfsplot_fmdata'>Residual fit spread plot data</h2><span id='topic+ols_prep_rfsplot_fmdata'></span><span id='topic+ols_prep_rfsplot_rsdata'></span>

<h3>Description</h3>

<p>Data for generating residual fit spread plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_rfsplot_fmdata(model)

ols_prep_rfsplot_rsdata(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_rfsplot_fmdata_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_prep_rfsplot_fmdata(model)
ols_prep_rfsplot_rsdata(model)

</code></pre>

<hr>
<h2 id='ols_prep_rstudlev_data'>Studentized residual vs leverage plot data</h2><span id='topic+ols_prep_rstudlev_data'></span>

<h3>Description</h3>

<p>Generates data for studentized resiudual vs leverage plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_rstudlev_data(model, threshold = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_rstudlev_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_rstudlev_data_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(read ~ write + math + science, data = hsb)
ols_prep_rstudlev_data(model)
ols_prep_rstudlev_data(model, threshold = 3)


</code></pre>

<hr>
<h2 id='ols_prep_rvsrplot_data'>Residual vs regressor plot data</h2><span id='topic+ols_prep_rvsrplot_data'></span>

<h3>Description</h3>

<p>Data for generating residual vs regressor plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_rvsrplot_data(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_rvsrplot_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_prep_rvsrplot_data(model)

</code></pre>

<hr>
<h2 id='ols_prep_srchart_data'>Standardized residual chart data</h2><span id='topic+ols_prep_srchart_data'></span>

<h3>Description</h3>

<p>Generates data for standardized residual chart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_srchart_data(model, threshold = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_srchart_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_srchart_data_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 2.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(read ~ write + math + science, data = hsb)
ols_prep_srchart_data(model)
ols_prep_srchart_data(model, threshold = 3)

</code></pre>

<hr>
<h2 id='ols_prep_srplot_data'>Studentized residual plot data</h2><span id='topic+ols_prep_srplot_data'></span>

<h3>Description</h3>

<p>Generates data for studentized residual plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_prep_srplot_data(model, threshold = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_prep_srplot_data_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_prep_srplot_data_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for detecting outliers. Default is 3.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(read ~ write + math + science, data = hsb)
ols_prep_srplot_data(model)

</code></pre>

<hr>
<h2 id='ols_press'>PRESS</h2><span id='topic+ols_press'></span>

<h3>Description</h3>

<p>PRESS (prediction sum of squares) tells you how well the model will predict
new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_press(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_press_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction sum of squares (PRESS) is the sum of squares of the prediction
error. Each fitted to obtain the predicted value for the ith observation. Use
PRESS to assess your model's predictive ability. Usually, the smaller the
PRESS value, the better the model's predictive ability.
</p>


<h3>Value</h3>

<p>Predicted sum of squares of the model.
</p>


<h3>References</h3>

<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>See Also</h3>

<p>Other influence measures: 
<code><a href="#topic+ols_hadi">ols_hadi</a>()</code>,
<code><a href="#topic+ols_leverage">ols_leverage</a>()</code>,
<code><a href="#topic+ols_pred_rsq">ols_pred_rsq</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_press(model)

</code></pre>

<hr>
<h2 id='ols_pure_error_anova'>Lack of fit F test</h2><span id='topic+ols_pure_error_anova'></span>

<h3>Description</h3>

<p>Assess how much of the error in prediction is due to lack of model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_pure_error_anova(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_pure_error_anova_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_pure_error_anova_+3A_...">...</code></td>
<td>
<p>Other parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The residual sum of squares resulting from a regression can be decomposed
into 2 components:
</p>

<ul>
<li><p> Due to lack of fit
</p>
</li>
<li><p> Due to random variation
</p>
</li></ul>

<p>If most of the error is due to lack of fit and not just random error, the
model should be discarded and a new model must be built.
</p>


<h3>Value</h3>

<p><code>ols_pure_error_anova</code> returns an object of class
<code>"ols_pure_error_anova"</code>. An object of class <code>"ols_pure_error_anova"</code> is a
list containing the following components:
</p>
<table>
<tr><td><code>lackoffit</code></td>
<td>
<p>lack of fit sum of squares</p>
</td></tr>
<tr><td><code>pure_error</code></td>
<td>
<p>pure error sum of squares</p>
</td></tr>
<tr><td><code>rss</code></td>
<td>
<p>regression sum of squares</p>
</td></tr>
<tr><td><code>ess</code></td>
<td>
<p>error sum of squares</p>
</td></tr>
<tr><td><code>total</code></td>
<td>
<p>total sum of squares</p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>regression mean square</p>
</td></tr>
<tr><td><code>ems</code></td>
<td>
<p>error mean square</p>
</td></tr>
<tr><td><code>lms</code></td>
<td>
<p>lack of fit mean square</p>
</td></tr>
<tr><td><code>pms</code></td>
<td>
<p>pure error mean square</p>
</td></tr>
<tr><td><code>rf</code></td>
<td>
<p>f statistic</p>
</td></tr>
<tr><td><code>lf</code></td>
<td>
<p>lack of fit f statistic</p>
</td></tr>
<tr><td><code>pr</code></td>
<td>
<p>p-value of f statistic</p>
</td></tr>
<tr><td><code>pl</code></td>
<td>
<p>p-value pf lack of fit f statistic</p>
</td></tr>
<tr><td><code>mpred</code></td>
<td>
<p><code>data.frame</code> containing data for the response and predictor of the <code>model</code></p>
</td></tr>
<tr><td><code>df_rss</code></td>
<td>
<p>regression sum of squares degrees of freedom</p>
</td></tr>
<tr><td><code>df_ess</code></td>
<td>
<p>error sum of squares degrees of freedom</p>
</td></tr>
<tr><td><code>df_lof</code></td>
<td>
<p>lack of fit degrees of freedom</p>
</td></tr>
<tr><td><code>df_error</code></td>
<td>
<p>pure error degrees of freedom</p>
</td></tr>
<tr><td><code>final</code></td>
<td>
<p>data.frame; contains computed values used for the lack of fit f test</p>
</td></tr>
<tr><td><code>resp</code></td>
<td>
<p>character vector; name of <code>response variable</code></p>
</td></tr>
<tr><td><code>preds</code></td>
<td>
<p>character vector; name of <code>predictor variable</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>The lack of fit F test works only with simple linear regression.
Moreover, it is important that the data contains repeat observations i.e.
replicates for at least one of the values of the predictor x. This
test generally only applies to datasets with plenty of replicates.
</p>


<h3>References</h3>

<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp, data = mtcars)
ols_pure_error_anova(model)

</code></pre>

<hr>
<h2 id='ols_regress'>Ordinary least squares regression</h2><span id='topic+ols_regress'></span><span id='topic+ols_regress.lm'></span>

<h3>Description</h3>

<p>Ordinary least squares regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_regress(object, ...)

## S3 method for class 'lm'
ols_regress(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_regress_+3A_object">object</code></td>
<td>
<p>An object of class &quot;formula&quot; (or one that can be coerced to
that class): a symbolic description of the model to be fitted or class
<code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_regress_+3A_...">...</code></td>
<td>
<p>Other inputs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_regress</code> returns an object of class <code>"ols_regress"</code>.
An object of class <code>"ols_regress"</code> is a list containing the following
components:
</p>
<table>
<tr><td><code>r</code></td>
<td>
<p>square root of rsquare, correlation between observed and predicted values of dependent variable</p>
</td></tr>
<tr><td><code>rsq</code></td>
<td>
<p>coefficient of determination or r-square</p>
</td></tr>
<tr><td><code>adjr</code></td>
<td>
<p>adjusted rsquare</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>root mean squared error</p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>coefficient of variation</p>
</td></tr>
<tr><td><code>mse</code></td>
<td>
<p>mean squared error</p>
</td></tr>
<tr><td><code>mae</code></td>
<td>
<p>mean absolute error</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>akaike information criteria</p>
</td></tr>
<tr><td><code>sbc</code></td>
<td>
<p>bayesian information criteria</p>
</td></tr>
<tr><td><code>sbic</code></td>
<td>
<p>sawa bayesian information criteria</p>
</td></tr>
<tr><td><code>prsq</code></td>
<td>
<p>predicted rsquare</p>
</td></tr>
<tr><td><code>error_df</code></td>
<td>
<p>residual degrees of freedom</p>
</td></tr>
<tr><td><code>model_df</code></td>
<td>
<p>regression degrees of freedom</p>
</td></tr>
<tr><td><code>total_df</code></td>
<td>
<p>total degrees of freedom</p>
</td></tr>
<tr><td><code>ess</code></td>
<td>
<p>error sum of squares</p>
</td></tr>
<tr><td><code>rss</code></td>
<td>
<p>regression sum of squares</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>total sum of squares</p>
</td></tr>
<tr><td><code>rms</code></td>
<td>
<p>regression mean square</p>
</td></tr>
<tr><td><code>ems</code></td>
<td>
<p>error mean square</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>f statistis</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p-value for <code>f</code></p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of predictors including intercept</p>
</td></tr>
<tr><td><code>betas</code></td>
<td>
<p>betas; estimated coefficients</p>
</td></tr>
<tr><td><code>sbetas</code></td>
<td>
<p>standardized betas</p>
</td></tr>
<tr><td><code>std_errors</code></td>
<td>
<p>standard errors</p>
</td></tr>
<tr><td><code>tvalues</code></td>
<td>
<p>t values</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>
<p>p-value of <code>tvalues</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom of <code>betas</code></p>
</td></tr>
<tr><td><code>conf_lm</code></td>
<td>
<p>confidence intervals for coefficients</p>
</td></tr>
<tr><td><code>title</code></td>
<td>
<p>title for the model</p>
</td></tr>
<tr><td><code>dependent</code></td>
<td>
<p>character vector; name of the dependent variable</p>
</td></tr>
<tr><td><code>predictors</code></td>
<td>
<p>character vector; name of the predictor variables</p>
</td></tr>
<tr><td><code>mvars</code></td>
<td>
<p>character vector; name of the predictor variables including intercept</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>input model for <code>ols_regress</code></p>
</td></tr>
</table>


<h3>Interaction Terms</h3>

<p>If the model includes interaction terms, the standardized betas
are computed after scaling and centering the predictors.
</p>


<h3>References</h3>

<p>https://www.ssc.wisc.edu/~hemken/Stataworkshops/stdBeta/Getting%20Standardized%20Coefficients%20Right.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ols_regress(mpg ~ disp + hp + wt, data = mtcars)

# if model includes interaction terms set iterm to TRUE
ols_regress(mpg ~ disp * wt, data = mtcars, iterm = TRUE)

</code></pre>

<hr>
<h2 id='ols_sbc'>Bayesian information criterion</h2><span id='topic+ols_sbc'></span>

<h3>Description</h3>

<p>Bayesian information criterion for model selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_sbc(model, method = c("R", "STATA", "SAS"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_sbc_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_sbc_+3A_method">method</code></td>
<td>
<p>A character vector; specify the method to compute BIC. Valid
options include R, STATA and SAS.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SBC provides a means for model selection. Given a collection of models for
the data, SBC estimates the quality of each model, relative to each of the
other models. R and STATA use loglikelihood to compute SBC. SAS uses residual
sum of squares. Below is the formula in each case:
</p>
<p><em>R &amp; STATA</em>
</p>
<p style="text-align: center;"><code class="reqn">AIC = -2(loglikelihood) + ln(n) * 2p</code>
</p>

<p><em>SAS</em>
</p>
<p style="text-align: center;"><code class="reqn">AIC = n * ln(SSE / n) + p * ln(n)</code>
</p>

<p>where <em>n</em> is the sample size and <em>p</em> is the number of model parameters including intercept.
</p>


<h3>Value</h3>

<p>The bayesian information criterion of the model.
</p>


<h3>References</h3>

<p>Schwarz, G. (1978). “Estimating the Dimension of a Model.” Annals of Statistics 6:461–464.
</p>
<p>Judge, G. G., Griffiths, W. E., Hill, R. C., and Lee, T.-C. (1980). The Theory and Practice of Econometrics.
New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbic">ols_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># using R computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_sbc(model)

# using STATA computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_sbc(model, method = 'STATA')

# using SAS computation method
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_sbc(model, method = 'SAS')

</code></pre>

<hr>
<h2 id='ols_sbic'>Sawa's bayesian information criterion</h2><span id='topic+ols_sbic'></span>

<h3>Description</h3>

<p>Sawa's bayesian information criterion for model selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_sbic(model, full_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_sbic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_sbic_+3A_full_model">full_model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sawa (1978) developed a model selection criterion that was derived from a
Bayesian modification of the AIC criterion. Sawa's Bayesian Information
Criterion (BIC) is a function of the number of observations n, the SSE, the
pure error variance fitting the full model, and the number of independent
variables including the intercept.
</p>
<p style="text-align: center;"><code class="reqn">SBIC = n * ln(SSE / n) + 2(p + 2)q - 2(q^2)</code>
</p>

<p>where <code class="reqn">q = n(\sigma^2)/SSE</code>, <em>n</em> is the sample size, <em>p</em> is the number of model parameters including intercept
<em>SSE</em> is the residual sum of squares.
</p>


<h3>Value</h3>

<p>Sawa's Bayesian Information Criterion
</p>


<h3>References</h3>

<p>Sawa, T. (1978). “Information Criteria for Discriminating among Alternative Regression Models.” Econometrica
46:1273–1282.
</p>
<p>Judge, G. G., Griffiths, W. E., Hill, R. C., and Lee, T.-C. (1980). The Theory and Practice of Econometrics.
New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p>Other model selection criteria: 
<code><a href="#topic+ols_aic">ols_aic</a>()</code>,
<code><a href="#topic+ols_apc">ols_apc</a>()</code>,
<code><a href="#topic+ols_fpe">ols_fpe</a>()</code>,
<code><a href="#topic+ols_hsp">ols_hsp</a>()</code>,
<code><a href="#topic+ols_mallows_cp">ols_mallows_cp</a>()</code>,
<code><a href="#topic+ols_msep">ols_msep</a>()</code>,
<code><a href="#topic+ols_sbc">ols_sbc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>full_model &lt;- lm(mpg ~ ., data = mtcars)
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_sbic(model, full_model)

</code></pre>

<hr>
<h2 id='ols_step_all_possible'>All possible regression</h2><span id='topic+ols_step_all_possible'></span><span id='topic+ols_step_all_possible.default'></span><span id='topic+plot.ols_step_all_possible'></span>

<h3>Description</h3>

<p>Fits all regressions involving one regressor, two regressors, three
regressors, and so on. It tests all possible subsets of the set of potential
independent variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_all_possible(model, ...)

## Default S3 method:
ols_step_all_possible(model, max_order = NULL, ...)

## S3 method for class 'ols_step_all_possible'
plot(x, model = NA, print_plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_all_possible_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_all_possible_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_all_possible_+3A_max_order">max_order</code></td>
<td>
<p>Maximum subset order.</p>
</td></tr>
<tr><td><code id="ols_step_all_possible_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_all_possible</code>.</p>
</td></tr>
<tr><td><code id="ols_step_all_possible_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_all_possible</code> returns an object of class <code>"ols_step_all_possible"</code>.
An object of class <code>"ols_step_all_possible"</code> is a data frame containing the
following components:
</p>
<table>
<tr><td><code>mindex</code></td>
<td>
<p>model index</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of predictors</p>
</td></tr>
<tr><td><code>predictors</code></td>
<td>
<p>predictors in the model</p>
</td></tr>
<tr><td><code>rsquare</code></td>
<td>
<p>rsquare of the model</p>
</td></tr>
<tr><td><code>adjr</code></td>
<td>
<p>adjusted rsquare of the model</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>root mean squared error of the model</p>
</td></tr>
<tr><td><code>predrsq</code></td>
<td>
<p>predicted rsquare of the model</p>
</td></tr>
<tr><td><code>cp</code></td>
<td>
<p>mallow's Cp</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>akaike information criteria</p>
</td></tr>
<tr><td><code>sbic</code></td>
<td>
<p>sawa bayesian information criteria</p>
</td></tr>
<tr><td><code>sbc</code></td>
<td>
<p>schwarz bayes information criteria</p>
</td></tr>
<tr><td><code>msep</code></td>
<td>
<p>estimated MSE of prediction, assuming multivariate normality</p>
</td></tr>
<tr><td><code>fpe</code></td>
<td>
<p>final prediction error</p>
</td></tr>
<tr><td><code>apc</code></td>
<td>
<p>amemiya prediction criteria</p>
</td></tr>
<tr><td><code>hsp</code></td>
<td>
<p>hocking's Sp</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mendenhall William and  Sinsich Terry, 2012, A Second Course in Statistics Regression Analysis (7th edition).
Prentice Hall
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp, data = mtcars)
k &lt;- ols_step_all_possible(model)
k

# plot
plot(k)

# maximum subset
model &lt;- lm(mpg ~ disp + hp + drat + wt + qsec, data = mtcars)
ols_step_all_possible(model, max_order = 3)

</code></pre>

<hr>
<h2 id='ols_step_all_possible_betas'>All possible regression variable coefficients</h2><span id='topic+ols_step_all_possible_betas'></span>

<h3>Description</h3>

<p>Returns the coefficients for each variable from each model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_all_possible_betas(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_all_possible_betas_+3A_object">object</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_all_possible_betas_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_all_possible_betas</code> returns a <code>data.frame</code> containing:
</p>
<table>
<tr><td><code>model_index</code></td>
<td>
<p>model number</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>predictor</p>
</td></tr>
<tr><td><code>beta_coef</code></td>
<td>
<p>coefficient for the predictor</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_step_all_possible_betas(model)

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_backward_adj_r2'>Stepwise Adjusted R-Squared backward regression</h2><span id='topic+ols_step_backward_adj_r2'></span><span id='topic+ols_step_backward_adj_r2.default'></span><span id='topic+plot.ols_step_backward_adj_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on adjusted r-squared, in a stepwise
manner until there is no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_adj_r2(model, ...)

## Default S3 method:
ols_step_backward_adj_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_adj_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_adj_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_backward_adj_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_aic">ols_step_backward_aic</a>()</code>,
<code><a href="#topic+ols_step_backward_p">ols_step_backward_p</a>()</code>,
<code><a href="#topic+ols_step_backward_r2">ols_step_backward_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_sbc">ols_step_backward_sbc</a>()</code>,
<code><a href="#topic+ols_step_backward_sbic">ols_step_backward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_adj_r2(model)

# final model and selection metrics
k &lt;- ols_step_backward_aic(model)
k$metrics
k$model

# include or exclude variable
# force variables to be included in the selection process
ols_step_backward_adj_r2(model, include = c("alc_mod", "gender"))

# use index of variable instead of name
ols_step_backward_adj_r2(model, include = c(7, 6))

# force variable to be excluded from selection process
ols_step_backward_adj_r2(model, exclude = c("alc_heavy", "bcs"))

# use index of variable instead of name
ols_step_backward_adj_r2(model, exclude = c(8, 1))

</code></pre>

<hr>
<h2 id='ols_step_backward_aic'>Stepwise AIC backward regression</h2><span id='topic+ols_step_backward_aic'></span><span id='topic+ols_step_backward_aic.default'></span><span id='topic+plot.ols_step_backward_aic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on akaike information criterion, in a stepwise
manner until there is no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_aic(model, ...)

## Default S3 method:
ols_step_backward_aic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_aic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_aic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_backward_aic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_adj_r2">ols_step_backward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_p">ols_step_backward_p</a>()</code>,
<code><a href="#topic+ols_step_backward_r2">ols_step_backward_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_sbc">ols_step_backward_sbc</a>()</code>,
<code><a href="#topic+ols_step_backward_sbic">ols_step_backward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_aic(model)

# stepwise backward regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_backward_aic(model)
plot(k)

# selection metrics
k$metrics
 
# final model
k$model

# include or exclude variable
# force variables to be included in the selection process
ols_step_backward_aic(model, include = c("alc_mod", "gender"))

# use index of variable instead of name
ols_step_backward_aic(model, include = c(7, 6))

# force variable to be excluded from selection process
ols_step_backward_aic(model, exclude = c("alc_heavy", "bcs"))

# use index of variable instead of name
ols_step_backward_aic(model, exclude = c(8, 1))

</code></pre>

<hr>
<h2 id='ols_step_backward_p'>Stepwise backward regression</h2><span id='topic+ols_step_backward_p'></span><span id='topic+ols_step_backward_p.default'></span><span id='topic+plot.ols_step_backward_p'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on p values, in a stepwise manner until there is
no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_p(model, ...)

## Default S3 method:
ols_step_backward_p(
  model,
  p_val = 0.3,
  include = NULL,
  exclude = NULL,
  hierarchical = FALSE,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_p'
plot(x, model = NA, print_plot = TRUE, details = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_p_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_...">...</code></td>
<td>
<p>Other inputs.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_p_val">p_val</code></td>
<td>
<p>p value; variables with p more than <code>p_val</code> will be removed
from the model.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_hierarchical">hierarchical</code></td>
<td>
<p>Logical; if <code>TRUE</code>, performs hierarchical selection.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_p</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_p_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_backward_p</code> returns an object of class <code>"ols_step_backward_p"</code>.
An object of class <code>"ols_step_backward_p"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_adj_r2">ols_step_backward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_aic">ols_step_backward_aic</a>()</code>,
<code><a href="#topic+ols_step_backward_r2">ols_step_backward_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_sbc">ols_step_backward_sbc</a>()</code>,
<code><a href="#topic+ols_step_backward_sbic">ols_step_backward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_p(model)

# stepwise backward regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_backward_p(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_backward_p(model, include = c("age", "alc_mod"))

# use index of variable instead of name
ols_step_backward_p(model, include = c(5, 7))

# force variable to be excluded from selection process
ols_step_backward_p(model, exclude = c("pindex"))

# use index of variable instead of name
ols_step_backward_p(model, exclude = c(2))

# hierarchical selection
model &lt;- lm(y ~ bcs + alc_heavy + pindex + age + alc_mod, data = surgical)
ols_step_backward_p(model, 0.1, hierarchical = TRUE)

# plot
k &lt;- ols_step_backward_p(model, 0.1, hierarchical = TRUE)
plot(k)

</code></pre>

<hr>
<h2 id='ols_step_backward_r2'>Stepwise R-Squared backward regression</h2><span id='topic+ols_step_backward_r2'></span><span id='topic+ols_step_backward_r2.default'></span><span id='topic+plot.ols_step_backward_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on r-squared, in a stepwise manner until there is
no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_r2(model, ...)

## Default S3 method:
ols_step_backward_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_backward_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_adj_r2">ols_step_backward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_aic">ols_step_backward_aic</a>()</code>,
<code><a href="#topic+ols_step_backward_p">ols_step_backward_p</a>()</code>,
<code><a href="#topic+ols_step_backward_sbc">ols_step_backward_sbc</a>()</code>,
<code><a href="#topic+ols_step_backward_sbic">ols_step_backward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_r2(model)

# final model and selection metrics
k &lt;- ols_step_backward_aic(model)
k$metrics
k$model

# include or exclude variable
# force variables to be included in the selection process
ols_step_backward_r2(model, include = c("alc_mod", "gender"))

# use index of variable instead of name
ols_step_backward_r2(model, include = c(7, 6))

# force variable to be excluded from selection process
ols_step_backward_r2(model, exclude = c("alc_heavy", "bcs"))

# use index of variable instead of name
ols_step_backward_r2(model, exclude = c(8, 1))

</code></pre>

<hr>
<h2 id='ols_step_backward_sbc'>Stepwise SBC backward regression</h2><span id='topic+ols_step_backward_sbc'></span><span id='topic+ols_step_backward_sbc.default'></span><span id='topic+plot.ols_step_backward_sbc'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on schwarz bayesian criterion, in a stepwise
manner until there is no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_sbc(model, ...)

## Default S3 method:
ols_step_backward_sbc(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_sbc'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_sbc_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbc_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_adj_r2">ols_step_backward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_aic">ols_step_backward_aic</a>()</code>,
<code><a href="#topic+ols_step_backward_p">ols_step_backward_p</a>()</code>,
<code><a href="#topic+ols_step_backward_r2">ols_step_backward_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_sbic">ols_step_backward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_sbc(model)

# stepwise backward regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_backward_sbc(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variable
# force variables to be included in the selection process
ols_step_backward_sbc(model, include = c("alc_mod", "gender"))

# use index of variable instead of name
ols_step_backward_sbc(model, include = c(7, 6))

# force variable to be excluded from selection process
ols_step_backward_sbc(model, exclude = c("alc_heavy", "bcs"))

# use index of variable instead of name
ols_step_backward_sbc(model, exclude = c(8, 1))

</code></pre>

<hr>
<h2 id='ols_step_backward_sbic'>Stepwise SBIC backward regression</h2><span id='topic+ols_step_backward_sbic'></span><span id='topic+ols_step_backward_sbic.default'></span><span id='topic+plot.ols_step_backward_sbic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
removing predictors based on sawa bayesian criterion, in a stepwise
manner until there is no variable left to remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_backward_sbic(model, ...)

## Default S3 method:
ols_step_backward_sbic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_backward_sbic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_backward_sbic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_backward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_backward_sbic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other backward selection procedures: 
<code><a href="#topic+ols_step_backward_adj_r2">ols_step_backward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_aic">ols_step_backward_aic</a>()</code>,
<code><a href="#topic+ols_step_backward_p">ols_step_backward_p</a>()</code>,
<code><a href="#topic+ols_step_backward_r2">ols_step_backward_r2</a>()</code>,
<code><a href="#topic+ols_step_backward_sbc">ols_step_backward_sbc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise backward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_backward_sbic(model)

# stepwise backward regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_backward_sbic(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variable
# force variables to be included in the selection process
ols_step_backward_sbic(model, include = c("alc_mod", "gender"))

# use index of variable instead of name
ols_step_backward_sbic(model, include = c(7, 6))

# force variable to be excluded from selection process
ols_step_backward_sbic(model, exclude = c("alc_heavy", "bcs"))

# use index of variable instead of name
ols_step_backward_sbic(model, exclude = c(8, 1))

</code></pre>

<hr>
<h2 id='ols_step_best_subset'>Best subsets regression</h2><span id='topic+ols_step_best_subset'></span><span id='topic+ols_step_best_subset.default'></span><span id='topic+plot.ols_step_best_subset'></span>

<h3>Description</h3>

<p>Select the subset of predictors that do the best at meeting some
well-defined objective criterion, such as having the largest R2 value or the
smallest MSE, Mallow's Cp or AIC. The default metric used for selecting the
model is R2 but the user can choose any of the other available metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_best_subset(model, ...)

## Default S3 method:
ols_step_best_subset(
  model,
  max_order = NULL,
  include = NULL,
  exclude = NULL,
  metric = c("rsquare", "adjr", "predrsq", "cp", "aic", "sbic", "sbc", "msep", "fpe",
    "apc", "hsp"),
  ...
)

## S3 method for class 'ols_step_best_subset'
plot(x, model = NA, print_plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_best_subset_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_...">...</code></td>
<td>
<p>Other inputs.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_max_order">max_order</code></td>
<td>
<p>Maximum subset order.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_metric">metric</code></td>
<td>
<p>Metric to select model.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_best_subset</code>.</p>
</td></tr>
<tr><td><code id="ols_step_best_subset_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_best_subset</code> returns an object of class <code>"ols_step_best_subset"</code>.
An object of class <code>"ols_step_best_subset"</code> is a list containing the following:
</p>
<table>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_step_best_subset(model)
ols_step_best_subset(model, metric = "adjr")
ols_step_best_subset(model, metric = "cp")

# maximum subset
model &lt;- lm(mpg ~ disp + hp + drat + wt + qsec, data = mtcars)
ols_step_best_subset(model, max_order = 3)

# plot
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
k &lt;- ols_step_best_subset(model)
plot(k)

# return only models including `qsec`
ols_step_best_subset(model, include = c("qsec"))

# exclude `hp` from selection process
ols_step_best_subset(model, exclude = c("hp"))

</code></pre>

<hr>
<h2 id='ols_step_both_adj_r2'>Stepwise Adjusted R-Squared regression</h2><span id='topic+ols_step_both_adj_r2'></span><span id='topic+ols_step_both_adj_r2.default'></span><span id='topic+plot.ols_step_both_adj_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on adjusted r-squared, in a stepwise
manner until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_adj_r2(model, ...)

## Default S3 method:
ols_step_both_adj_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_adj_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_adj_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, details of variable selection will
be printed on screen.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_both_adj_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other both direction selection procedures: 
<code><a href="#topic+ols_step_both_aic">ols_step_both_aic</a>()</code>,
<code><a href="#topic+ols_step_both_r2">ols_step_both_r2</a>()</code>,
<code><a href="#topic+ols_step_both_sbc">ols_step_both_sbc</a>()</code>,
<code><a href="#topic+ols_step_both_sbic">ols_step_both_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = stepdata)
ols_step_both_adj_r2(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = stepdata)
k &lt;- ols_step_both_adj_r2(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
model &lt;- lm(y ~ ., data = stepdata)

ols_step_both_adj_r2(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_adj_r2(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_adj_r2(model, exclude = c("x2"))

# use index of variable instead of name
ols_step_both_adj_r2(model, exclude = c(2))

# include &amp; exclude variables in the selection process
ols_step_both_adj_r2(model, include = c("x6"), exclude = c("x2"))

# use index of variable instead of name
ols_step_both_adj_r2(model, include = c(6), exclude = c(2))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_both_aic'>Stepwise AIC regression</h2><span id='topic+ols_step_both_aic'></span><span id='topic+ols_step_both_aic.default'></span><span id='topic+plot.ols_step_both_aic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on akaike information criteria, in a
stepwise manner until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_aic(model, ...)

## Default S3 method:
ols_step_both_aic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_aic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_aic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, details of variable selection will
be printed on screen.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_both_aic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other both direction selection procedures: 
<code><a href="#topic+ols_step_both_adj_r2">ols_step_both_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_both_r2">ols_step_both_r2</a>()</code>,
<code><a href="#topic+ols_step_both_sbc">ols_step_both_sbc</a>()</code>,
<code><a href="#topic+ols_step_both_sbic">ols_step_both_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = stepdata)
ols_step_both_aic(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = stepdata)
k &lt;- ols_step_both_aic(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
model &lt;- lm(y ~ ., data = stepdata)

ols_step_both_aic(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_aic(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_aic(model, exclude = c("x2"))

# use index of variable instead of name
ols_step_both_aic(model, exclude = c(2))

# include &amp; exclude variables in the selection process
ols_step_both_aic(model, include = c("x6"), exclude = c("x2"))

# use index of variable instead of name
ols_step_both_aic(model, include = c(6), exclude = c(2))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_both_p'>Stepwise regression</h2><span id='topic+ols_step_both_p'></span><span id='topic+ols_step_both_p.default'></span><span id='topic+plot.ols_step_both_p'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on p values, in a stepwise manner
until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_p(model, ...)

## Default S3 method:
ols_step_both_p(
  model,
  p_enter = 0.1,
  p_remove = 0.3,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_p'
plot(x, model = NA, print_plot = TRUE, details = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_p_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_p_enter">p_enter</code></td>
<td>
<p>p value; variables with p value less than <code>p_enter</code> will enter
into the model.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_p_remove">p_remove</code></td>
<td>
<p>p value; variables with p more than <code>p_remove</code> will be removed
from the model.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_p</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_p_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_both_p</code> returns an object of class <code>"ols_step_both_p"</code>.
An object of class <code>"ols_step_both_p"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>beta_pval</code></td>
<td>
<p>beta and p values of models in each selection step</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_both_p(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_both_p(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
model &lt;- lm(y ~ ., data = stepdata)

# force variable to be included in selection process
ols_step_both_p(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_p(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_p(model, exclude = c("x1"))

# use index of variable instead of name
ols_step_both_p(model, exclude = c(1))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_both_r2'>Stepwise R-Squared regression</h2><span id='topic+ols_step_both_r2'></span><span id='topic+ols_step_both_r2.default'></span><span id='topic+plot.ols_step_both_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on r-squared, in a stepwise manner
until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_r2(model, ...)

## Default S3 method:
ols_step_both_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, details of variable selection will
be printed on screen.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_both_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other both direction selection procedures: 
<code><a href="#topic+ols_step_both_adj_r2">ols_step_both_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_both_aic">ols_step_both_aic</a>()</code>,
<code><a href="#topic+ols_step_both_sbc">ols_step_both_sbc</a>()</code>,
<code><a href="#topic+ols_step_both_sbic">ols_step_both_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = stepdata)
ols_step_both_r2(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = stepdata)
k &lt;- ols_step_both_r2(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
model &lt;- lm(y ~ ., data = stepdata)

ols_step_both_r2(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_r2(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_r2(model, exclude = c("x2"))

# use index of variable instead of name
ols_step_both_r2(model, exclude = c(2))

# include &amp; exclude variables in the selection process
ols_step_both_r2(model, include = c("x6"), exclude = c("x2"))

# use index of variable instead of name
ols_step_both_r2(model, include = c(6), exclude = c(2))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_both_sbc'>Stepwise SBC regression</h2><span id='topic+ols_step_both_sbc'></span><span id='topic+ols_step_both_sbc.default'></span><span id='topic+plot.ols_step_both_sbc'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on schwarz bayesian criterion, in a
stepwise manner until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_sbc(model, ...)

## Default S3 method:
ols_step_both_sbc(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_sbc'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_sbc_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, details of variable selection will
be printed on screen.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbc_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other both direction selection procedures: 
<code><a href="#topic+ols_step_both_adj_r2">ols_step_both_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_both_aic">ols_step_both_aic</a>()</code>,
<code><a href="#topic+ols_step_both_r2">ols_step_both_r2</a>()</code>,
<code><a href="#topic+ols_step_both_sbic">ols_step_both_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = stepdata)
ols_step_both_sbc(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = stepdata)
k &lt;- ols_step_both_sbc(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
model &lt;- lm(y ~ ., data = stepdata)

ols_step_both_sbc(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_sbc(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_sbc(model, exclude = c("x2"))

# use index of variable instead of name
ols_step_both_sbc(model, exclude = c(2))

# include &amp; exclude variables in the selection process
ols_step_both_sbc(model, include = c("x6"), exclude = c("x2"))

# use index of variable instead of name
ols_step_both_sbc(model, include = c(6), exclude = c(2))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_both_sbic'>Stepwise SBIC regression</h2><span id='topic+ols_step_both_sbic'></span><span id='topic+ols_step_both_sbic.default'></span><span id='topic+plot.ols_step_both_sbic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering and removing predictors based on sawa bayesian criterion, in a
stepwise manner until there is no variable left to enter or remove any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_both_sbic(model, ...)

## Default S3 method:
ols_step_both_sbic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_both_sbic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_both_sbic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, details of variable selection will
be printed on screen.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_both_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_both_sbic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other both direction selection procedures: 
<code><a href="#topic+ols_step_both_adj_r2">ols_step_both_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_both_aic">ols_step_both_aic</a>()</code>,
<code><a href="#topic+ols_step_both_r2">ols_step_both_r2</a>()</code>,
<code><a href="#topic+ols_step_both_sbc">ols_step_both_sbc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# stepwise regression
model &lt;- lm(y ~ ., data = stepdata)
ols_step_both_sbic(model)

# stepwise regression plot
model &lt;- lm(y ~ ., data = stepdata)
k &lt;- ols_step_both_sbic(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
model &lt;- lm(y ~ ., data = stepdata)

ols_step_both_sbic(model, include = c("x6"))

# use index of variable instead of name
ols_step_both_sbic(model, include = c(6))

# force variable to be excluded from selection process
ols_step_both_sbic(model, exclude = c("x2"))

# use index of variable instead of name
ols_step_both_sbic(model, exclude = c(2))

# include &amp; exclude variables in the selection process
ols_step_both_sbic(model, include = c("x6"), exclude = c("x2"))

# use index of variable instead of name
ols_step_both_sbic(model, include = c(6), exclude = c(2))

## End(Not run)

</code></pre>

<hr>
<h2 id='ols_step_forward_adj_r2'>Stepwise Adjusted R-Squared forward regression</h2><span id='topic+ols_step_forward_adj_r2'></span><span id='topic+ols_step_forward_adj_r2.default'></span><span id='topic+plot.ols_step_forward_adj_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on adjusted r-squared, in a stepwise
manner until there is no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_adj_r2(model, ...)

## Default S3 method:
ols_step_forward_adj_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_adj_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_adj_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_forward_adj_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_aic">ols_step_forward_aic</a>()</code>,
<code><a href="#topic+ols_step_forward_p">ols_step_forward_p</a>()</code>,
<code><a href="#topic+ols_step_forward_r2">ols_step_forward_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_sbc">ols_step_forward_sbc</a>()</code>,
<code><a href="#topic+ols_step_forward_sbic">ols_step_forward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_adj_r2(model)

# stepwise forward regression plot
k &lt;- ols_step_forward_adj_r2(model)
plot(k)

# selection metrics
k$metrics

# extract final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_adj_r2(model, include = c("age"))

# use index of variable instead of name
ols_step_forward_adj_r2(model, include = c(5))

# force variable to be excluded from selection process
ols_step_forward_adj_r2(model, exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_adj_r2(model, exclude = c(4))

# include &amp; exclude variables in the selection process
ols_step_forward_adj_r2(model, include = c("age"), exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_adj_r2(model, include = c(5), exclude = c(4))

</code></pre>

<hr>
<h2 id='ols_step_forward_aic'>Stepwise AIC forward regression</h2><span id='topic+ols_step_forward_aic'></span><span id='topic+ols_step_forward_aic.default'></span><span id='topic+plot.ols_step_forward_aic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on akaike information criterion, in a stepwise
manner until there is no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_aic(model, ...)

## Default S3 method:
ols_step_forward_aic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_aic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_aic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_forward_aic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_adj_r2">ols_step_forward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_p">ols_step_forward_p</a>()</code>,
<code><a href="#topic+ols_step_forward_r2">ols_step_forward_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_sbc">ols_step_forward_sbc</a>()</code>,
<code><a href="#topic+ols_step_forward_sbic">ols_step_forward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_aic(model)

# stepwise forward regression plot
k &lt;- ols_step_forward_aic(model)
plot(k)

# selection metrics
k$metrics

# extract final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_aic(model, include = c("age"))

# use index of variable instead of name
ols_step_forward_aic(model, include = c(5))

# force variable to be excluded from selection process
ols_step_forward_aic(model, exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_aic(model, exclude = c(4))

# include &amp; exclude variables in the selection process
ols_step_forward_aic(model, include = c("age"), exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_aic(model, include = c(5), exclude = c(4))

</code></pre>

<hr>
<h2 id='ols_step_forward_p'>Stepwise forward regression</h2><span id='topic+ols_step_forward_p'></span><span id='topic+ols_step_forward_p.default'></span><span id='topic+plot.ols_step_forward_p'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on p values, in a stepwise manner until there is
no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_p(model, ...)

## Default S3 method:
ols_step_forward_p(
  model,
  p_val = 0.3,
  include = NULL,
  exclude = NULL,
  hierarchical = FALSE,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_p'
plot(x, model = NA, print_plot = TRUE, details = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_p_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>; the model should include all
candidate predictor variables.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_p_val">p_val</code></td>
<td>
<p>p value; variables with p value less than <code>p_val</code> will
enter into the model</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_hierarchical">hierarchical</code></td>
<td>
<p>Logical; if <code>TRUE</code>, performs hierarchical selection.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_p</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_p_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_step_forward_p</code> returns an object of class <code>"ols_step_forward_p"</code>.
An object of class <code>"ols_step_forward_p"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. 5th ed. N.p.: John Wiley &amp; Sons, 2012. Print.
</p>
<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_adj_r2">ols_step_forward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_aic">ols_step_forward_aic</a>()</code>,
<code><a href="#topic+ols_step_forward_r2">ols_step_forward_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_sbc">ols_step_forward_sbc</a>()</code>,
<code><a href="#topic+ols_step_forward_sbic">ols_step_forward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_p(model)

# stepwise forward regression plot
model &lt;- lm(y ~ ., data = surgical)
k &lt;- ols_step_forward_p(model)
plot(k)

# selection metrics
k$metrics

# final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_p(model, include = c("age", "alc_mod"))

# use index of variable instead of name
ols_step_forward_p(model, include = c(5, 7))

# force variable to be excluded from selection process
ols_step_forward_p(model, exclude = c("pindex"))

# use index of variable instead of name
ols_step_forward_p(model, exclude = c(2))

# hierarchical selection
model &lt;- lm(y ~ bcs + alc_heavy + pindex + enzyme_test, data = surgical)
ols_step_forward_p(model, 0.1, hierarchical = TRUE)

# plot
k &lt;- ols_step_forward_p(model, 0.1, hierarchical = TRUE)
plot(k)

</code></pre>

<hr>
<h2 id='ols_step_forward_r2'>Stepwise R-Squared forward regression</h2><span id='topic+ols_step_forward_r2'></span><span id='topic+ols_step_forward_r2.default'></span><span id='topic+plot.ols_step_forward_r2'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on r-squared, in a stepwise manner until there
is no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_r2(model, ...)

## Default S3 method:
ols_step_forward_r2(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_r2'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_r2_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_forward_r2_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_adj_r2">ols_step_forward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_aic">ols_step_forward_aic</a>()</code>,
<code><a href="#topic+ols_step_forward_p">ols_step_forward_p</a>()</code>,
<code><a href="#topic+ols_step_forward_sbc">ols_step_forward_sbc</a>()</code>,
<code><a href="#topic+ols_step_forward_sbic">ols_step_forward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_r2(model)

# stepwise forward regression plot
k &lt;- ols_step_forward_r2(model)
plot(k)

# selection metrics
k$metrics

# extract final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_r2(model, include = c("age"))

# use index of variable instead of name
ols_step_forward_r2(model, include = c(5))

# force variable to be excluded from selection process
ols_step_forward_r2(model, exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_r2(model, exclude = c(4))

# include &amp; exclude variables in the selection process
ols_step_forward_r2(model, include = c("age"), exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_r2(model, include = c(5), exclude = c(4))

</code></pre>

<hr>
<h2 id='ols_step_forward_sbc'>Stepwise SBC forward regression</h2><span id='topic+ols_step_forward_sbc'></span><span id='topic+ols_step_forward_sbc.default'></span><span id='topic+plot.ols_step_forward_sbc'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on schwarz bayesian criterion, in a stepwise
manner until there is no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_sbc(model, ...)

## Default S3 method:
ols_step_forward_sbc(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_sbc'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_sbc_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbc_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_adj_r2">ols_step_forward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_aic">ols_step_forward_aic</a>()</code>,
<code><a href="#topic+ols_step_forward_p">ols_step_forward_p</a>()</code>,
<code><a href="#topic+ols_step_forward_r2">ols_step_forward_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_sbic">ols_step_forward_sbic</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_sbc(model)

# stepwise forward regression plot
k &lt;- ols_step_forward_sbc(model)
plot(k)

# selection metrics
k$metrics

# extract final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_sbc(model, include = c("age"))

# use index of variable instead of name
ols_step_forward_sbc(model, include = c(5))

# force variable to be excluded from selection process
ols_step_forward_sbc(model, exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_sbc(model, exclude = c(4))

# include &amp; exclude variables in the selection process
ols_step_forward_sbc(model, include = c("age"), exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_sbc(model, include = c(5), exclude = c(4))

</code></pre>

<hr>
<h2 id='ols_step_forward_sbic'>Stepwise SBIC forward regression</h2><span id='topic+ols_step_forward_sbic'></span><span id='topic+ols_step_forward_sbic.default'></span><span id='topic+plot.ols_step_forward_sbic'></span>

<h3>Description</h3>

<p>Build regression model from a set of candidate predictor variables by
entering predictors based on sawa bayesian criterion, in a stepwise
manner until there is no variable left to enter any more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_step_forward_sbic(model, ...)

## Default S3 method:
ols_step_forward_sbic(
  model,
  include = NULL,
  exclude = NULL,
  progress = FALSE,
  details = FALSE,
  ...
)

## S3 method for class 'ols_step_forward_sbic'
plot(x, print_plot = TRUE, details = TRUE, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_step_forward_sbic_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_include">include</code></td>
<td>
<p>Character or numeric vector; variables to be included in selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_exclude">exclude</code></td>
<td>
<p>Character or numeric vector; variables to be excluded from selection process.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_progress">progress</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will display variable selection progress.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_details">details</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will print the regression result at
each step.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_x">x</code></td>
<td>
<p>An object of class <code>ols_step_forward_*</code>.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
<tr><td><code id="ols_step_forward_sbic_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places to display.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>final model; an object of class <code>lm</code></p>
</td></tr>
<tr><td><code>metrics</code></td>
<td>
<p>selection metrics</p>
</td></tr>
<tr><td><code>others</code></td>
<td>
<p>list; info used for plotting and printing</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>See Also</h3>

<p>Other forward selection procedures: 
<code><a href="#topic+ols_step_forward_adj_r2">ols_step_forward_adj_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_aic">ols_step_forward_aic</a>()</code>,
<code><a href="#topic+ols_step_forward_p">ols_step_forward_p</a>()</code>,
<code><a href="#topic+ols_step_forward_r2">ols_step_forward_r2</a>()</code>,
<code><a href="#topic+ols_step_forward_sbc">ols_step_forward_sbc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># stepwise forward regression
model &lt;- lm(y ~ ., data = surgical)
ols_step_forward_sbic(model)

# stepwise forward regression plot
k &lt;- ols_step_forward_sbic(model)
plot(k)

# selection metrics
k$metrics

# extract final model
k$model

# include or exclude variables
# force variable to be included in selection process
ols_step_forward_sbic(model, include = c("age"))

# use index of variable instead of name
ols_step_forward_sbic(model, include = c(5))

# force variable to be excluded from selection process
ols_step_forward_sbic(model, exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_sbic(model, exclude = c(4))

# include &amp; exclude variables in the selection process
ols_step_forward_sbic(model, include = c("age"), exclude = c("liver_test"))

# use index of variable instead of name
ols_step_forward_sbic(model, include = c(5), exclude = c(4))

</code></pre>

<hr>
<h2 id='ols_test_bartlett'>Bartlett test</h2><span id='topic+ols_test_bartlett'></span><span id='topic+ols_test_bartlett.default'></span>

<h3>Description</h3>

<p>Test if k samples are from populations with equal variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_bartlett(data, ...)

## Default S3 method:
ols_test_bartlett(data, ..., group_var = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_bartlett_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tibble</code>.</p>
</td></tr>
<tr><td><code id="ols_test_bartlett_+3A_...">...</code></td>
<td>
<p>Columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ols_test_bartlett_+3A_group_var">group_var</code></td>
<td>
<p>Grouping variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bartlett's test is used to test if variances across samples is equal.
It is sensitive to departures from normality. The Levene test
is an alternative test that is less sensitive to departures from normality.
</p>


<h3>Value</h3>

<p><code>ols_test_bartlett</code> returns an object of class <code>"ols_test_bartlett"</code>.
An object of class <code>"ols_test_bartlett"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>fstat</code></td>
<td>
<p>f statistic</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>p-value of <code>fstat</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
</table>


<h3>References</h3>

<p>Snedecor, George W. and Cochran, William G. (1989), Statistical Methods,
Eighth Edition, Iowa State University Press.
</p>


<h3>See Also</h3>

<p>Other heteroskedasticity tests: 
<code><a href="#topic+ols_test_breusch_pagan">ols_test_breusch_pagan</a>()</code>,
<code><a href="#topic+ols_test_f">ols_test_f</a>()</code>,
<code><a href="#topic+ols_test_score">ols_test_score</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># using grouping variable
if (require("descriptr")) {
  library(descriptr)
  ols_test_bartlett(mtcarz, 'mpg', group_var = 'cyl')
}

# using variables
ols_test_bartlett(hsb, 'read', 'write')

</code></pre>

<hr>
<h2 id='ols_test_breusch_pagan'>Breusch pagan test</h2><span id='topic+ols_test_breusch_pagan'></span>

<h3>Description</h3>

<p>Test for constant variance. It assumes that the error terms are normally
distributed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_breusch_pagan(
  model,
  fitted.values = TRUE,
  rhs = FALSE,
  multiple = FALSE,
  p.adj = c("none", "bonferroni", "sidak", "holm"),
  vars = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_breusch_pagan_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_test_breusch_pagan_+3A_fitted.values">fitted.values</code></td>
<td>
<p>Logical; if TRUE, use fitted values of regression model.</p>
</td></tr>
<tr><td><code id="ols_test_breusch_pagan_+3A_rhs">rhs</code></td>
<td>
<p>Logical; if TRUE, specifies that tests for heteroskedasticity be
performed for the right-hand-side (explanatory) variables of the fitted
regression model.</p>
</td></tr>
<tr><td><code id="ols_test_breusch_pagan_+3A_multiple">multiple</code></td>
<td>
<p>Logical; if TRUE, specifies that multiple testing be performed.</p>
</td></tr>
<tr><td><code id="ols_test_breusch_pagan_+3A_p.adj">p.adj</code></td>
<td>
<p>Adjustment for p value, the following options are available:
bonferroni, holm, sidak and none.</p>
</td></tr>
<tr><td><code id="ols_test_breusch_pagan_+3A_vars">vars</code></td>
<td>
<p>Variables to be used for heteroskedasticity test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Breusch Pagan Test was introduced by Trevor Breusch and Adrian Pagan in 1979.
It is used to test for heteroskedasticity in a linear regression model.
It test whether variance of errors from a regression is dependent on the
values of a independent variable.
</p>

<ul>
<li><p> Null Hypothesis: Equal/constant variances
</p>
</li>
<li><p> Alternative Hypothesis: Unequal/non-constant variances
</p>
</li></ul>

<p>Computation
</p>

<ul>
<li><p> Fit a regression model
</p>
</li>
<li><p> Regress the squared residuals from the above model on the independent variables
</p>
</li>
<li><p> Compute <code class="reqn">nR^2</code>. It follows a chi square distribution with p -1 degrees of
freedom, where p is the number of independent variables, n is the sample size and
<code class="reqn">R^2</code> is the coefficient of determination from the regression in step 2.
</p>
</li></ul>



<h3>Value</h3>

<p><code>ols_test_breusch_pagan</code> returns an object of class <code>"ols_test_breusch_pagan"</code>.
An object of class <code>"ols_test_breusch_pagan"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>bp</code></td>
<td>
<p>breusch pagan statistic</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p-value of <code>bp</code></p>
</td></tr>
<tr><td><code>fv</code></td>
<td>
<p>fitted values of the regression model</p>
</td></tr>
<tr><td><code>rhs</code></td>
<td>
<p>names of explanatory variables of fitted regression model</p>
</td></tr>
<tr><td><code>multiple</code></td>
<td>
<p>logical value indicating if multiple tests should be performed</p>
</td></tr>
<tr><td><code>padj</code></td>
<td>
<p>adjusted p values</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>variables to be used for heteroskedasticity test</p>
</td></tr>
<tr><td><code>resp</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code>preds</code></td>
<td>
<p>predictors</p>
</td></tr>
</table>


<h3>References</h3>

<p>T.S. Breusch &amp; A.R. Pagan (1979), A Simple Test for Heteroscedasticity and
Random Coefficient Variation. Econometrica 47, 1287–1294
</p>
<p>Cook, R. D.; Weisberg, S. (1983). &quot;Diagnostics for Heteroskedasticity in Regression&quot;. Biometrika. 70 (1): 1–10.
</p>


<h3>See Also</h3>

<p>Other heteroskedasticity tests: 
<code><a href="#topic+ols_test_bartlett">ols_test_bartlett</a>()</code>,
<code><a href="#topic+ols_test_f">ols_test_f</a>()</code>,
<code><a href="#topic+ols_test_score">ols_test_score</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(mpg ~ disp + hp + wt + drat, data = mtcars)

# use fitted values of the model
ols_test_breusch_pagan(model)

# use independent variables of the model
ols_test_breusch_pagan(model, rhs = TRUE)

# use independent variables of the model and perform multiple tests
ols_test_breusch_pagan(model, rhs = TRUE, multiple = TRUE)

# bonferroni p value adjustment
ols_test_breusch_pagan(model, rhs = TRUE, multiple = TRUE, p.adj = 'bonferroni')

# sidak p value adjustment
ols_test_breusch_pagan(model, rhs = TRUE, multiple = TRUE, p.adj = 'sidak')

# holm's p value adjustment
ols_test_breusch_pagan(model, rhs = TRUE, multiple = TRUE, p.adj = 'holm')

</code></pre>

<hr>
<h2 id='ols_test_correlation'>Correlation test for normality</h2><span id='topic+ols_test_correlation'></span>

<h3>Description</h3>

<p>Correlation between observed residuals and expected residuals under normality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_correlation(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_correlation_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Correlation between fitted regression model residuals and expected
values of residuals.
</p>


<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_box">ols_plot_resid_box</a>()</code>,
<code><a href="#topic+ols_plot_resid_fit">ols_plot_resid_fit</a>()</code>,
<code><a href="#topic+ols_plot_resid_hist">ols_plot_resid_hist</a>()</code>,
<code><a href="#topic+ols_plot_resid_qq">ols_plot_resid_qq</a>()</code>,
<code><a href="#topic+ols_test_normality">ols_test_normality</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_test_correlation(model)

</code></pre>

<hr>
<h2 id='ols_test_f'>F test</h2><span id='topic+ols_test_f'></span>

<h3>Description</h3>

<p>Test for heteroskedasticity under the assumption that the errors are
independent and identically distributed (i.i.d.).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_f(model, fitted_values = TRUE, rhs = FALSE, vars = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_f_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_test_f_+3A_fitted_values">fitted_values</code></td>
<td>
<p>Logical; if TRUE, use fitted values of regression model.</p>
</td></tr>
<tr><td><code id="ols_test_f_+3A_rhs">rhs</code></td>
<td>
<p>Logical; if TRUE, specifies that tests for heteroskedasticity be
performed for the right-hand-side (explanatory) variables of the fitted
regression model.</p>
</td></tr>
<tr><td><code id="ols_test_f_+3A_vars">vars</code></td>
<td>
<p>Variables to be used for for heteroskedasticity test.</p>
</td></tr>
<tr><td><code id="ols_test_f_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_test_f</code> returns an object of class <code>"ols_test_f"</code>.
An object of class <code>"ols_test_f"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>f</code></td>
<td>
<p>f statistic</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p-value of <code>f</code></p>
</td></tr>
<tr><td><code>fv</code></td>
<td>
<p>fitted values of the regression model</p>
</td></tr>
<tr><td><code>rhs</code></td>
<td>
<p>names of explanatory variables of fitted regression model</p>
</td></tr>
<tr><td><code>numdf</code></td>
<td>
<p>numerator degrees of freedom</p>
</td></tr>
<tr><td><code>dendf</code></td>
<td>
<p>denominator degrees of freedom</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>variables to be used for heteroskedasticity test</p>
</td></tr>
<tr><td><code>resp</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code>preds</code></td>
<td>
<p>predictors</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wooldridge, J. M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western.
</p>


<h3>See Also</h3>

<p>Other heteroskedasticity tests: 
<code><a href="#topic+ols_test_bartlett">ols_test_bartlett</a>()</code>,
<code><a href="#topic+ols_test_breusch_pagan">ols_test_breusch_pagan</a>()</code>,
<code><a href="#topic+ols_test_score">ols_test_score</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)

# using fitted values
ols_test_f(model)

# using all predictors of the model
ols_test_f(model, rhs = TRUE)

# using fitted values
ols_test_f(model, vars = c('disp', 'hp'))

</code></pre>

<hr>
<h2 id='ols_test_normality'>Test for normality</h2><span id='topic+ols_test_normality'></span><span id='topic+ols_test_normality.lm'></span>

<h3>Description</h3>

<p>Test for detecting violation of normality assumption.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_normality(y, ...)

## S3 method for class 'lm'
ols_test_normality(y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_normality_+3A_y">y</code></td>
<td>
<p>A numeric vector or an object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_test_normality_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_test_normality</code> returns an object of class <code>"ols_test_normality"</code>.
An object of class <code>"ols_test_normality"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>kolmogorv</code></td>
<td>
<p>kolmogorv smirnov statistic</p>
</td></tr>
<tr><td><code>shapiro</code></td>
<td>
<p>shapiro wilk statistic</p>
</td></tr>
<tr><td><code>cramer</code></td>
<td>
<p>cramer von mises statistic</p>
</td></tr>
<tr><td><code>anderson</code></td>
<td>
<p>anderson darling statistic</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other residual diagnostics: 
<code><a href="#topic+ols_plot_resid_box">ols_plot_resid_box</a>()</code>,
<code><a href="#topic+ols_plot_resid_fit">ols_plot_resid_fit</a>()</code>,
<code><a href="#topic+ols_plot_resid_hist">ols_plot_resid_hist</a>()</code>,
<code><a href="#topic+ols_plot_resid_qq">ols_plot_resid_qq</a>()</code>,
<code><a href="#topic+ols_test_correlation">ols_test_correlation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_test_normality(model)

</code></pre>

<hr>
<h2 id='ols_test_outlier'>Bonferroni Outlier Test</h2><span id='topic+ols_test_outlier'></span>

<h3>Description</h3>

<p>Detect outliers using Bonferroni p values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_outlier(model, cut_off = 0.05, n_max = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_outlier_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_test_outlier_+3A_cut_off">cut_off</code></td>
<td>
<p>Bonferroni p-values cut off for reporting observations.</p>
</td></tr>
<tr><td><code id="ols_test_outlier_+3A_n_max">n_max</code></td>
<td>
<p>Maximum number of observations to report, default is 10.</p>
</td></tr>
<tr><td><code id="ols_test_outlier_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(y ~ ., data = surgical)
ols_test_outlier(model)

</code></pre>

<hr>
<h2 id='ols_test_score'>Score test</h2><span id='topic+ols_test_score'></span>

<h3>Description</h3>

<p>Test for heteroskedasticity under the assumption that the errors are
independent and identically distributed (i.i.d.).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_test_score(model, fitted_values = TRUE, rhs = FALSE, vars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_test_score_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="ols_test_score_+3A_fitted_values">fitted_values</code></td>
<td>
<p>Logical; if TRUE, use fitted values of regression model.</p>
</td></tr>
<tr><td><code id="ols_test_score_+3A_rhs">rhs</code></td>
<td>
<p>Logical; if TRUE, specifies that tests for heteroskedasticity be
performed for the right-hand-side (explanatory) variables of the fitted
regression model.</p>
</td></tr>
<tr><td><code id="ols_test_score_+3A_vars">vars</code></td>
<td>
<p>Variables to be used for for heteroskedasticity test.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols_test_score</code> returns an object of class <code>"ols_test_score"</code>.
An object of class <code>"ols_test_score"</code> is a list containing the
following components:
</p>
<table>
<tr><td><code>score</code></td>
<td>
<p>f statistic</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p value of <code>score</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
<tr><td><code>fv</code></td>
<td>
<p>fitted values of the regression model</p>
</td></tr>
<tr><td><code>rhs</code></td>
<td>
<p>names of explanatory variables of fitted regression model</p>
</td></tr>
<tr><td><code>resp</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code>preds</code></td>
<td>
<p>predictors</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breusch, T. S. and Pagan, A. R. (1979) A simple test for heteroscedasticity and random coefficient variation. Econometrica 47, 1287–1294.
</p>
<p>Cook, R. D. and Weisberg, S. (1983) Diagnostics for heteroscedasticity in regression. Biometrika 70, 1–10.
</p>
<p>Koenker, R. 1981. A note on studentizing a test for heteroskedasticity. Journal of Econometrics 17: 107–112.
</p>


<h3>See Also</h3>

<p>Other heteroskedasticity tests: 
<code><a href="#topic+ols_test_bartlett">ols_test_bartlett</a>()</code>,
<code><a href="#topic+ols_test_breusch_pagan">ols_test_breusch_pagan</a>()</code>,
<code><a href="#topic+ols_test_f">ols_test_f</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># model
model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)

# using fitted values of the model
ols_test_score(model)

# using predictors from the model
ols_test_score(model, rhs = TRUE)

# specify predictors from the model
ols_test_score(model, vars = c('disp', 'wt'))

</code></pre>

<hr>
<h2 id='olsrr'><code>olsrr</code> package</h2><span id='topic+olsrr'></span><span id='topic+_PACKAGE'></span><span id='topic+olsrr-package'></span>

<h3>Description</h3>

<p>Tools for teaching and learning OLS regression
</p>


<h3>Details</h3>

<p>See the README on
<a href="https://github.com/rsquaredacademy/olsrr">GitHub</a>
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Aravind Hebbali <a href="mailto:hebbali.aravind@gmail.com">hebbali.aravind@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://olsrr.rsquaredacademy.com/">https://olsrr.rsquaredacademy.com/</a>
</p>
</li>
<li> <p><a href="https://github.com/rsquaredacademy/olsrr">https://github.com/rsquaredacademy/olsrr</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/rsquaredacademy/olsrr/issues">https://github.com/rsquaredacademy/olsrr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='rivers'>Test Data Set</h2><span id='topic+rivers'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rivers
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 20 rows and 6 columns.
</p>

<hr>
<h2 id='rvsr_plot_shiny'>Residual vs regressors plot for shiny app</h2><span id='topic+rvsr_plot_shiny'></span>

<h3>Description</h3>

<p>Graph to determine whether we should add a new predictor to the
model already containing other predictors. The residuals from the model is
regressed on the new predictor and if the plot shows non random pattern,
you should consider adding the new predictor to the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rvsr_plot_shiny(model, data, variable, print_plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rvsr_plot_shiny_+3A_model">model</code></td>
<td>
<p>An object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="rvsr_plot_shiny_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tibble</code>.</p>
</td></tr>
<tr><td><code id="rvsr_plot_shiny_+3A_variable">variable</code></td>
<td>
<p>Character; new predictor to be added to the <code>model</code>.</p>
</td></tr>
<tr><td><code id="rvsr_plot_shiny_+3A_print_plot">print_plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, prints the plot else returns a plot object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ disp + hp + wt, data = mtcars)
rvsr_plot_shiny(model, mtcars, 'drat')

</code></pre>

<hr>
<h2 id='stepdata'>Test Data Set</h2><span id='topic+stepdata'></span>

<h3>Description</h3>

<p>Test Data Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepdata
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 20000 rows and 7 columns.
</p>

<hr>
<h2 id='surgical'>Surgical Unit Data Set</h2><span id='topic+surgical'></span>

<h3>Description</h3>

<p>A dataset containing data about survival of patients undergoing liver operation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>surgical
</code></pre>


<h3>Format</h3>

<p>A data frame with 54 rows and 9 variables:
</p>

<dl>
<dt>bcs</dt><dd><p>blood clotting score</p>
</dd>
<dt>pindex</dt><dd><p>prognostic index</p>
</dd>
<dt>enzyme_test</dt><dd><p>enzyme function test score</p>
</dd>
<dt>liver_test</dt><dd><p>liver function test score</p>
</dd>
<dt>age</dt><dd><p>age, in years</p>
</dd>
<dt>gender</dt><dd><p>indicator variable for gender (0 = male, 1 = female)</p>
</dd>
<dt>alc_mod</dt><dd><p>indicator variable for history of alcohol use (0 = None, 1 = Moderate)</p>
</dd>
<dt>alc_heavy</dt><dd><p>indicator variable for history of alcohol use (0 = None, 1 = Heavy)</p>
</dd>
<dt>y</dt><dd><p>Survival Time</p>
</dd>
</dl>



<h3>Source</h3>

<p>Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004, Applied Linear Statistical Models (5th edition).
Chicago, IL., McGraw Hill/Irwin.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
