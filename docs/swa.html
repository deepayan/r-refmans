<!DOCTYPE html><html lang="en"><head><title>Help for package swa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {swa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#swa'><p>Subsampling Winner Algorithm for Variable Selection in the Classification Setting</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Subsampling Winner Algorithm for Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.1</td>
</tr>
<tr>
<td>Description:</td>
<td>This algorithm conducts variable selection in the classification setting. It repeatedly subsamples variables and runs linear discriminant analysis (LDA) on the subsampled variables. Variables are scored based on the AUC and the t-statistics. Variables then enter a competition and the semi-finalist variables will be evaluated in a final round of LDA classification. The algorithm then outputs a list of variable selected. Qiao, Sun and Fan (2017) <a href="http://people.math.binghamton.edu/qiao/swa.html">http://people.math.binghamton.edu/qiao/swa.html</a>.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.1), ROCR, reshape, ggplot2</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-10-18 13:52:10 UTC; Xingye_Dell_5480</td>
</tr>
<tr>
<td>Author:</td>
<td>Xingye Qiao [aut, cre],
  Jiayang Sun [aut],
  Yiying Fan [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xingye Qiao &lt;qiao@math.binghamton.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-10-18 17:10:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='swa'>Subsampling Winner Algorithm for Variable Selection in the Classification Setting</h2><span id='topic+swa'></span>

<h3>Description</h3>

<p>This function conducts variable selection in the classification setting. The algorithm repeatedly subsamples variables and runs linear discriminant analysis (LDA) via linear regression based on the subsampled variables. Variables are scored based on the AUC and the t-statistics in linear regression. Variables then enter a competition and the semi-finalists will be used in a final round of LDA (or linear regression.) The semi-finalists and the final linear model are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swa(X, y, S = c(3, 5, 7, 10, 15, 20, 30, 40), q = 20, m = 500,
  MPplot = FALSE, screening = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="swa_+3A_x">X</code></td>
<td>
<p>The p by n design matrix where p is the dimension and n is the sample size.</p>
</td></tr>
<tr><td><code id="swa_+3A_y">y</code></td>
<td>
<p>The binary class labels (0 or 1.)</p>
</td></tr>
<tr><td><code id="swa_+3A_s">S</code></td>
<td>
<p>Size(s) of subsamplings. This could be a set of candidate sizes, or a single size. In the former case, all sizes will be tried. A vector of default values is c(3,5,7,10,15,20,30,40).</p>
</td></tr>
<tr><td><code id="swa_+3A_q">q</code></td>
<td>
<p>Number of semi-finalists. The default is 20.</p>
</td></tr>
<tr><td><code id="swa_+3A_m">m</code></td>
<td>
<p>Number of subsamplings. The default is 500.  One may increase the value of m to see if the outcomes have been stabilized.</p>
</td></tr>
<tr><td><code id="swa_+3A_mpplot">MPplot</code></td>
<td>
<p>Boolean input showing whether the multi-panel diagnostic plot is drawn. The default is FALSE.</p>
</td></tr>
<tr><td><code id="swa_+3A_screening">screening</code></td>
<td>
<p>Boolean input showing whether the variables are pre-screened based on marginal variance. The default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the indices of the semi-finalists, summary of the final regression model, and summary of the stepwise selection of the final model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- matrix(rnorm(20*100), ncol=20)
b = c(0.5,1,1.5,2,3); 
y &lt;- x[,1:5]%*%b + rnorm(100)
y &lt;- as.numeric(y&lt;0)
X &lt;- t(x)
swa(X,y,c(3,5,10,15,20),5,500,MPplot = TRUE) ;
## The MP plots show that the upper arm sets, i.e., the points above the elbow point of each 
## panel plot, started to be stabilized at s = 10. Thus, we fix s = 10 below.
sum_swa &lt;- swa(X,y,10,5,500)
sum_swa 

## The final model perfectly recovers the five true variables. The stepwise selection removes
## X1, which has the weakest signal (hence is not expected to be recovered.) We next check the
## results with a larger m = 1000.
sum_swa &lt;- swa(X,y,10,5,1000)   
sum_swa
# It now captured X2,X3,X4,X5, and also a noise variable X6 that is not as significant as the 
## former 4 covariates X2-X4. We next look at the estimates of all 20 coefficients including 
## those not selected.
coef_swa = numeric(20)
column_index = as.numeric(substring(rownames(sum_swa$step_sum$coef[-1,]),2,5))
coef_swa[column_index] &lt;- sum_swa$step_sum$coef[-1,1]

#### Compare the coefficients from the SWA on all covariates with the coefficients from the 
## oracle LDA. We first calculate the coefficients from the oracle LDA.
n = 100
n0 = sum(y==0)
n1 = sum(y==1)
yy=y
yy[y==0] &lt;- -n/n0 ;
yy[y==1] &lt;- n/n1 ;
X = X[1:5,]
X = X - apply(X,1,median);
X = X/apply(X,1,mad) ;
designX = t(X) ;
colnames(designX) &lt;- paste('X',1:5,sep='') ;
dataXY = data.frame(cbind(yy,designX)) ; dim(dataXY) ;
LDA &lt;- lm(yy ~ ., data = dataXY)
sum_LDA &lt;- summary(LDA)
coef_LDA = numeric(20)
coef_LDA[as.numeric(substring(rownames(sum_LDA$coef[-1,]),2,5))] &lt;- sum_LDA$coef[-1,1]
####  Compare the coefficients using correlation.
cor(coef_swa,coef_LDA); rbind(coef_swa, coef_LDA); 
####  There is an excellent correlation between the coefficients recovered from the SWA on 
## the full data and the coefficients from the benchmark, ie., the oracle LDA.

####  Next we try on a set of smaller coefficients that should be harder to recover from 
## the noisy data
x &lt;- matrix(rnorm(20*100), ncol=20)
b = c(0.3,0.6,0.9,1.2,1.5);
y &lt;- x[,1:5]%*%b + rnorm(100)
y &lt;- as.numeric(y&lt;0)
X &lt;- t(x)
swa(X,y,10,5,500) ;
####  SWA does a perfect job in recovering the true variables.
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
