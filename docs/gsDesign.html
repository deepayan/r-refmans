<!DOCTYPE html><html><head><title>Help for package gsDesign</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gsDesign}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gsDesign-package'><p>gsDesign: Group Sequential Design</p></a></li>
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#as_gt'><p>Convert a summary table object to a gt object</p></a></li>
<li><a href='#as_rtf'><p>Save a summary table object as an RTF file</p></a></li>
<li><a href='#as_table'><p>Create a summary table</p></a></li>
<li><a href='#checkLengths'><p>Utility functions to verify variable properties</p></a></li>
<li><a href='#ciBinomial'><p>Testing, Confidence Intervals, Sample Size and Power for Comparing Two</p>
Binomial Rates</a></li>
<li><a href='#condPower'><p>Sample size re-estimation based on conditional power</p></a></li>
<li><a href='#eEvents'><p>Expected number of events for a time-to-event study</p></a></li>
<li><a href='#gsBinomialExact'><p>One-Sample Binomial Routines</p></a></li>
<li><a href='#gsBound'><p>Boundary derivation - low level</p></a></li>
<li><a href='#gsBoundCP'><p>Conditional Power at Interim Boundaries</p></a></li>
<li><a href='#gsCP'><p>Conditional and Predictive Power, Overall and Conditional Probability of Success</p></a></li>
<li><a href='#gsDensity'><p>Group sequential design interim density function</p></a></li>
<li><a href='#gsDesign'><p>Design Derivation</p></a></li>
<li><a href='#gsProbability'><p>Boundary Crossing Probabilities</p></a></li>
<li><a href='#gsSurvCalendar'><p>Time-to-event endpoint design with calendar timing of analyses</p></a></li>
<li><a href='#hGraph'><p>Create multiplicity graphs using ggplot2</p></a></li>
<li><a href='#nNormal'><p>Normal distribution sample size (2-sample)</p></a></li>
<li><a href='#normalGrid'><p>Normal Density Grid</p></a></li>
<li><a href='#plot.gsDesign'><p>Plots for group sequential designs</p></a></li>
<li><a href='#print.nSurv'><p>Advanced time-to-event sample size calculation</p></a></li>
<li><a href='#print.nSurvival'><p>Time-to-event sample size calculation (Lachin-Foulkes)</p></a></li>
<li><a href='#sequentialPValue'><p>Sequential p-value computation</p></a></li>
<li><a href='#sfExponential'><p>Exponential Spending Function</p></a></li>
<li><a href='#sfHSD'><p>Hwang-Shih-DeCani Spending Function</p></a></li>
<li><a href='#sfLDOF'><p>Lan-DeMets Spending function overview</p></a></li>
<li><a href='#sfLinear'><p>Piecewise Linear and Step Function Spending Functions</p></a></li>
<li><a href='#sfLogistic'><p>Two-parameter Spending Function Families</p></a></li>
<li><a href='#sfPoints'><p>Pointwise Spending Function</p></a></li>
<li><a href='#sfPower'><p>Kim-DeMets (power) Spending Function</p></a></li>
<li><a href='#sfTDist'><p>t-distribution Spending Function</p></a></li>
<li><a href='#sfTruncated'><p>Truncated, trimmed and gapped spending functions</p></a></li>
<li><a href='#sfXG1'><p>Xi and Gallo conditional error spending functions</p></a></li>
<li><a href='#summary.gsDesign'><p>Bound Summary and Z-transformations</p></a></li>
<li><a href='#summary.spendfn'><p>Spending Function</p></a></li>
<li><a href='#toBinomialExact'><p>Translate survival design bounds to exact binomial bounds</p></a></li>
<li><a href='#toInteger'><p>Translate group sequential design to integer events (survival designs)</p>
or sample size (other designs)</a></li>
<li><a href='#xtable'><p>xtable</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>3.6.3</td>
</tr>
<tr>
<td>Title:</td>
<td>Group Sequential Design</td>
</tr>
<tr>
<td>Description:</td>
<td>Derives group sequential clinical trial designs and describes
    their properties. Particular focus on time-to-event, binary, and
    continuous outcomes. Largely based on methods described in
    Jennison, Christopher and Turnbull, Bruce W., 2000,
    "Group Sequential Methods with Applications to Clinical Trials"
    ISBN: 0-8493-0316-8.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>Copyright 2010, Merck Research Laboratories</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://keaven.github.io/gsDesign/">https://keaven.github.io/gsDesign/</a>,
<a href="https://github.com/keaven/gsDesign">https://github.com/keaven/gsDesign</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/keaven/gsDesign/issues">https://github.com/keaven/gsDesign/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr (&ge; 1.1.0), ggplot2 (&ge; 3.1.1), graphics, gt, magrittr,
methods, r2rtf, rlang, stats, tibble, tidyr, tools, xtable</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, data.table, gridExtra, kableExtra, knitr, mvtnorm,
ragg, rmarkdown, scales, testthat, utils</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-09 13:43:47 UTC; xiaonan4</td>
</tr>
<tr>
<td>Author:</td>
<td>Keaven Anderson [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Keaven Anderson &lt;keaven_anderson@merck.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-09 15:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gsDesign-package'>gsDesign: Group Sequential Design</h2><span id='topic+gsDesign-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Derives group sequential clinical trial designs and describes their properties. Particular focus on time-to-event, binary, and continuous outcomes. Largely based on methods described in Jennison, Christopher and Turnbull, Bruce W., 2000, &quot;Group Sequential Methods with Applications to Clinical Trials&quot; ISBN: 0-8493-0316-8.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://keaven.github.io/gsDesign/">https://keaven.github.io/gsDesign/</a>
</p>
</li>
<li> <p><a href="https://github.com/keaven/gsDesign">https://github.com/keaven/gsDesign</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/keaven/gsDesign/issues">https://github.com/keaven/gsDesign/issues</a>
</p>
</li></ul>


<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling 'rhs(lhs)'.
</p>

<hr>
<h2 id='as_gt'>Convert a summary table object to a gt object</h2><span id='topic+as_gt'></span><span id='topic+as_gt.gsBinomialExactTable'></span>

<h3>Description</h3>

<p>Convert a summary table object created with <code><a href="#topic+as_table">as_table</a></code>
to a <code>gt_tbl</code> object; currently only implemented for
<code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_gt(x, ...)

## S3 method for class 'gsBinomialExactTable'
as_gt(
  x,
  ...,
  title = "Operating Characteristics for the Truncated SPRT Design",
  subtitle = "Assumes trial evaluated sequentially after each response",
  theta_label = html("Underlying&lt;br&gt;response rate"),
  bound_label = c("Futility bound", "Efficacy bound"),
  prob_decimals = 2,
  en_decimals = 1,
  rr_decimals = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_gt_+3A_x">x</code></td>
<td>
<p>Object to be converted.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_...">...</code></td>
<td>
<p>Other parameters that may be specific to the object.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_title">title</code></td>
<td>
<p>Table title.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_subtitle">subtitle</code></td>
<td>
<p>Table subtitle.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_theta_label">theta_label</code></td>
<td>
<p>Label for theta.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_bound_label">bound_label</code></td>
<td>
<p>Label for bounds.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_prob_decimals">prob_decimals</code></td>
<td>
<p>Number of decimal places for probability of crossing.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_en_decimals">en_decimals</code></td>
<td>
<p>Number of decimal places for expected number of
observations when bound is crossed or when trial ends without crossing.</p>
</td></tr>
<tr><td><code id="as_gt_+3A_rr_decimals">rr_decimals</code></td>
<td>
<p>Number of decimal places for response rates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently only implemented for <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code> objects.
Creates a table to summarize an object.
For <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>, this summarized operating characteristics
across a range of effect sizes.
</p>


<h3>Value</h3>

<p>A <code>gt_tbl</code> object that may be extended by overloaded versions of
<code><a href="#topic+as_gt">as_gt</a></code>.
</p>


<h3>See Also</h3>

<p><code>vignette("binomialSPRTExample")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>safety_design &lt;- binomialSPRT(
  p0 = .04, p1 = .1, alpha = .04, beta = .2, minn = 4, maxn = 75
)
safety_power &lt;- gsBinomialExact(
  k = length(safety_design$n.I),
  theta = seq(.02, .16, .02),
  n.I = safety_design$n.I,
  a = safety_design$lower$bound,
  b = safety_design$upper$bound
)
safety_power %&gt;%
  as_table() %&gt;%
  as_gt(
    theta_label = gt::html("Underlying&lt;br&gt;AE rate"),
    prob_decimals = 3,
    bound_label = c("low rate", "high rate")
  )
</code></pre>

<hr>
<h2 id='as_rtf'>Save a summary table object as an RTF file</h2><span id='topic+as_rtf'></span><span id='topic+as_rtf.gsBinomialExactTable'></span><span id='topic+as_rtf.gsBoundSummary'></span>

<h3>Description</h3>

<p>Convert and save the summary table object created with <code><a href="#topic+as_table">as_table</a></code>
to an RTF file using r2rtf; currently only implemented for
<code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_rtf(x, ...)

## S3 method for class 'gsBinomialExactTable'
as_rtf(
  x,
  file,
  ...,
  title = paste("Operating Characteristics by Underlying Response Rate for",
    "Exact Binomial Group Sequential Design"),
  theta_label = "Underlying Response Rate",
  response_outcome = TRUE,
  bound_label = if (response_outcome) c("Futility Bound", "Efficacy Bound") else
    c("Efficacy Bound", "Futility Bound"),
  en_label = "Expected Sample Sizes",
  prob_decimals = 2,
  en_decimals = 1,
  rr_decimals = 0
)

## S3 method for class 'gsBoundSummary'
as_rtf(
  x,
  file,
  ...,
  title = "Boundary Characteristics for Group Sequential Design",
  footnote_p_onesided = "one-side p-value for experimental better than control",
  footnote_appx_effect_at_bound = NULL,
  footnote_p_cross_h0 = "Cumulative type I error assuming binding futility bound",
  footnote_p_cross_h1 = "Cumulative power under the alternate hypothesis",
  footnote_specify = NULL,
  footnote_text = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_rtf_+3A_x">x</code></td>
<td>
<p>Object to be saved as RTF file.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_...">...</code></td>
<td>
<p>Other parameters that may be specific to the object.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_file">file</code></td>
<td>
<p>File path for the output.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_title">title</code></td>
<td>
<p>Title of the report.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_theta_label">theta_label</code></td>
<td>
<p>Label for theta.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_response_outcome">response_outcome</code></td>
<td>
<p>Logical values indicating if the outcome is
response rate (<code>TRUE</code>) or failure rate (<code>FALSE</code>).
The default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_bound_label">bound_label</code></td>
<td>
<p>Label for bounds.
If the outcome is response rate, then the label is &quot;Futility bound&quot;
and &quot;Efficacy bound&quot;.
If the outcome is failure rate, then the label is &quot;Efficacy bound&quot;
and &quot;Futility bound&quot;.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_en_label">en_label</code></td>
<td>
<p>Label for expected number.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_prob_decimals">prob_decimals</code></td>
<td>
<p>Number of decimal places for probability of crossing.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_en_decimals">en_decimals</code></td>
<td>
<p>Number of decimal places for expected number of
observations when bound is crossed or when trial ends without crossing.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_rr_decimals">rr_decimals</code></td>
<td>
<p>Number of decimal places for response rates.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_p_onesided">footnote_p_onesided</code></td>
<td>
<p>Footnote for one-side p-value.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_appx_effect_at_bound">footnote_appx_effect_at_bound</code></td>
<td>
<p>Footnote for approximate effect treatment at bound.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_p_cross_h0">footnote_p_cross_h0</code></td>
<td>
<p>Footnote for cumulative type I error.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_p_cross_h1">footnote_p_cross_h1</code></td>
<td>
<p>Footnote for cumulative power under the alternate hypothesis.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_specify">footnote_specify</code></td>
<td>
<p>Vector of string to put footnote super script.</p>
</td></tr>
<tr><td><code id="as_rtf_+3A_footnote_text">footnote_text</code></td>
<td>
<p>Vector of string of footnote text.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently only implemented for <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code> objects.
Creates a table to summarize an object.
For <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>, this summarized operating characteristics
across a range of effect sizes.
</p>


<h3>Value</h3>

<p><code>as_rtf()</code> returns the input <code>x</code> invisibly.
</p>


<h3>See Also</h3>

<p><code>vignette("binomialSPRTExample")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># as_rtf for gsBinomialExact
zz &lt;- gsBinomialExact(
  k = 3, theta = seq(0.1, 0.9, 0.1), n.I = c(12, 24, 36),
  a = c(-1, 0, 11), b = c(5, 9, 12)
)
zz %&gt;%
  as_table() %&gt;%
  as_rtf(
    file = tempfile(fileext = ".rtf"),
    title = "Power/Type I Error and Expected Sample Size for a Group Sequential Design"
  )

safety_design &lt;- binomialSPRT(
  p0 = .04, p1 = .1, alpha = .04, beta = .2, minn = 4, maxn = 75
)
safety_power &lt;- gsBinomialExact(
  k = length(safety_design$n.I),
  theta = seq(.02, .16, .02),
  n.I = safety_design$n.I,
  a = safety_design$lower$bound,
  b = safety_design$upper$bound
)
safety_power %&gt;%
  as_table() %&gt;%
  as_rtf(
    file = tempfile(fileext = ".rtf"),
    theta_label = "Underlying\nAE Rate",
    prob_decimals = 3,
    bound_label = c("Low Rate", "High Rate")
  )
# as_rtf for gsBoundSummary
xgs &lt;- gsSurv(lambdaC = .2, hr = .5, eta = .1, T = 2, minfup = 1.5)
gsBoundSummary(xgs, timename = "Year", tdigits = 1) %&gt;% as_rtf(file = tempfile(fileext = ".rtf"))

ss &lt;- nSurvival(
  lambda1 = .2, lambda2 = .1, eta = .1, Ts = 2, Tr = .5,
  sided = 1, alpha = .025, ratio = 2
)
xs &lt;- gsDesign(nFixSurv = ss$n, n.fix = ss$nEvents, delta1 = log(ss$lambda2 / ss$lambda1))
gsBoundSummary(xs, logdelta = TRUE, ratio = ss$ratio) %&gt;% as_rtf(file = tempfile(fileext = ".rtf"))

xs &lt;- gsDesign(nFixSurv = ss$n, n.fix = ss$nEvents, delta1 = log(ss$lambda2 / ss$lambda1))
gsBoundSummary(xs, logdelta = TRUE, ratio = ss$ratio) %&gt;% 
  as_rtf(file = tempfile(fileext = ".rtf"),
  footnote_specify = "Z",
  footnote_text = "Z-Score")
</code></pre>

<hr>
<h2 id='as_table'>Create a summary table</h2><span id='topic+as_table'></span><span id='topic+as_table.gsBinomialExact'></span>

<h3>Description</h3>

<p>Create a tibble to summarize an object; currently only implemented for
<code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_table(x, ...)

## S3 method for class 'gsBinomialExact'
as_table(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_table_+3A_x">x</code></td>
<td>
<p>Object to be summarized.</p>
</td></tr>
<tr><td><code id="as_table_+3A_...">...</code></td>
<td>
<p>Other parameters that may be specific to the object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently only implemented for <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code> objects.
Creates a table to summarize an object.
For <code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>, this summarized operating characteristics
across a range of effect sizes.
</p>


<h3>Value</h3>

<p>A tibble which may have an extended class to enable further output processing.
</p>


<h3>See Also</h3>

<p><code>vignette("binomialSPRTExample")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>b &lt;- binomialSPRT(p0 = .1, p1 = .35, alpha = .08, beta = .2, minn = 10, maxn = 25)
b_power &lt;- gsBinomialExact(
  k = length(b$n.I), theta = seq(.1, .45, .05), n.I = b$n.I,
  a = b$lower$bound, b = b$upper$bound
)
b_power %&gt;%
  as_table() %&gt;%
  as_gt()
</code></pre>

<hr>
<h2 id='checkLengths'>Utility functions to verify variable properties</h2><span id='topic+checkLengths'></span><span id='topic+checkRange'></span><span id='topic+checkScalar'></span><span id='topic+isInteger'></span><span id='topic+checkVector'></span>

<h3>Description</h3>

<p>Utility functions to verify an objects's properties including whether it is
a scalar or vector, the class, the length, and (if numeric) whether the
range of values is on a specified interval. Additionally, the
<code>checkLengths</code> function can be used to ensure that all the supplied
inputs have equal lengths.
</p>
<p><code>isInteger</code> is similar to <code><a href="base.html#topic+is.integer">is.integer</a></code> except that
<code>isInteger(1)</code> returns <code>TRUE</code> whereas <code>is.integer(1)</code> returns
<code>FALSE</code>.
</p>
<p><code>checkScalar</code> is used to verify that the input object is a scalar as
well as the other properties specified above.
</p>
<p><code>checkVector</code> is used to verify that the input object is an atomic
vector as well as the other properties as defined above.
</p>
<p><code>checkRange</code> is used to check whether the numeric input object's values
reside on the specified interval.  If any of the values are outside the
specified interval, a <code>FALSE</code> is returned.
</p>
<p><code>checkLength</code> is used to check whether all of the supplied inputs have
equal lengths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkLengths(..., allowSingle = FALSE)

checkRange(
  x,
  interval = 0:1,
  inclusion = c(TRUE, TRUE),
  varname = deparse(substitute(x)),
  tol = 0
)

checkScalar(x, isType = "numeric", ...)

checkVector(x, isType = "numeric", ..., length = NULL)

isInteger(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkLengths_+3A_...">...</code></td>
<td>
<p>For the <code><a href="#topic+checkScalar">checkScalar</a></code> and <code><a href="#topic+checkVector">checkVector</a></code>
functions, this input represents additional arguments sent directly to the
<code><a href="#topic+checkRange">checkRange</a></code> function. For the
</p>
<p><code><a href="#topic+checkLengths">checkLengths</a></code> function, this input represents the arguments to
check for equal lengths.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_allowsingle">allowSingle</code></td>
<td>
<p>logical flag. If <code>TRUE</code>, arguments that are vectors
comprised of a single element are not included in the comparative length
test in the <code><a href="#topic+checkLengths">checkLengths</a></code> function. Partial matching on the
name of this argument is not performed so you must specify 'allowSingle' in
its entirety in the call.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_x">x</code></td>
<td>
<p>any object.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_interval">interval</code></td>
<td>
<p>two-element numeric vector defining the interval over which
the input object is expected to be contained.  Use the <code>inclusion</code>
argument to define the boundary behavior.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_inclusion">inclusion</code></td>
<td>
<p>two-element logical vector defining the boundary behavior
of the specified interval. A <code>TRUE</code> value denotes inclusion of the
corresponding boundary. For example, if <code>interval=c(3,6)</code> and
<code>inclusion=c(FALSE,TRUE)</code>, then all the values of the input object are
verified to be on the interval (3,6].</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_varname">varname</code></td>
<td>
<p>character string defining the name of the input variable as
sent into the function by the caller.  This is used primarily as a mechanism
to specify the name of the variable being tested when <code>checkRange</code> is
being called within a function.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_tol">tol</code></td>
<td>
<p>numeric scalar defining the tolerance to use in testing the
intervals of the
</p>
<p><code><a href="#topic+checkRange">checkRange</a></code> function.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_istype">isType</code></td>
<td>
<p>character string defining the class that the input object is
expected to be.</p>
</td></tr>
<tr><td><code id="checkLengths_+3A_length">length</code></td>
<td>
<p>integer specifying the expected length of the object in the
case it is a vector. If <code>length=NULL</code>, the default, then no length
check is performed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>isInteger</code>: Boolean value as checking result
Other functions have no return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# check whether input is an integer
isInteger(1)
isInteger(1:5)
try(isInteger("abc")) # expect error

# check whether input is an integer scalar
checkScalar(3, "integer")

# check whether input is an integer scalar that resides
# on the interval on [3, 6]. Then test for interval (3, 6].
checkScalar(3, "integer", c(3, 6))
try(checkScalar(3, "integer", c(3, 6), c(FALSE, TRUE))) # expect error

# check whether the input is an atomic vector of class numeric,
# of length 3, and whose value all reside on the interval [1, 10)
x &lt;- c(3, pi, exp(1))
checkVector(x, "numeric", c(1, 10), c(TRUE, FALSE), length = 3)

# do the same but change the expected length; expect error
try(checkVector(x, "numeric", c(1, 10), c(TRUE, FALSE), length = 2))

# create faux function to check input variable
foo &lt;- function(moo) checkVector(moo, "character")
foo(letters)
try(foo(1:5)) # expect error with function and argument name in message

# check for equal lengths of various inputs
checkLengths(1:2, 2:3, 3:4)
try(checkLengths(1, 2, 3, 4:5)) # expect error

# check for equal length inputs but ignore single element vectors
checkLengths(1, 2, 3, 4:5, 7:8, allowSingle = TRUE)


</code></pre>

<hr>
<h2 id='ciBinomial'>Testing, Confidence Intervals, Sample Size and Power for Comparing Two
Binomial Rates</h2><span id='topic+ciBinomial'></span><span id='topic+nBinomial'></span><span id='topic+simBinomial'></span><span id='topic+testBinomial'></span><span id='topic+varBinomial'></span>

<h3>Description</h3>

<p>Support is provided for sample size estimation, power, testing, confidence
intervals and simulation for fixed sample size trials (that is, not group
sequential or adaptive) with two arms and binary outcomes.  Both superiority
and non-inferiority trials are considered. While all routines default to
comparisons of risk-difference, options to base computations on risk-ratio
and odds-ratio are also included.
</p>
<p><code>nBinomial()</code> computes sample size or power using the method of
Farrington and Manning (1990) for a trial to test the difference between two
binomial event rates.  The routine can be used for a test of superiority or
non-inferiority. For a design that tests for superiority <code>nBinomial()</code>
is consistent with the method of Fleiss, Tytun, and Ury (but without the
continuity correction) to test for differences between event rates. This
routine is consistent with the Hmisc package routines <code>bsamsize</code> and
<code>bpower</code> for superiority designs. Vector arguments allow computing
sample sizes for multiple scenarios for comparative purposes.
</p>
<p><code>testBinomial()</code> computes a Z- or Chi-square-statistic that compares
two binomial event rates using the method of Miettinen and Nurminen (1980).
This can be used for superiority or non-inferiority testing. Vector
arguments allow easy incorporation into simulation routines for fixed, group
sequential and adaptive designs.
</p>
<p><code>ciBinomial()</code> computes confidence intervals for 1) the difference
between two rates, 2) the risk-ratio for two rates or 3) the odds-ratio for
two rates. This procedure provides inference that is consistent with
<code>testBinomial()</code> in that the confidence intervals are produced by
inverting the testing procedures in <code>testBinomial()</code>. The Type I error
<code>alpha</code> input to <code>ciBinomial</code> is always interpreted as 2-sided.
</p>
<p><code>simBinomial()</code> performs simulations to estimate the power for a
Miettinen and Nurminen (1985) test comparing two binomial rates for
superiority or non-inferiority.  As noted in documentation for
<code>bpower.sim()</code> in the HMisc package, by using <code>testBinomial()</code> you
can see that the formulas without any continuity correction are quite
accurate.  In fact, Type I error for a continuity-corrected test is
significantly lower (Gordon and Watson, 1996) than the nominal rate.  Thus,
as a default no continuity corrections are performed.
</p>
<p><code>varBinomial</code> computes blinded estimates of the variance of the
estimate of 1) event rate differences, 2) logarithm of the risk ratio, or 3)
logarithm of the odds ratio. This is intended for blinded sample size
re-estimation for comparative trials with a binary outcome.
</p>
<p>Testing is 2-sided when a Chi-square statistic is used and 1-sided when a
Z-statistic is used. Thus, these 2 options will produce substantially
different results, in general. For non-inferiority, 1-sided testing is
appropriate.
</p>
<p>You may wish to round sample sizes up using <code>ceiling()</code>.
</p>
<p>Farrington and Manning (1990) begin with event rates <code>p1</code> and <code>p2</code>
under the alternative hypothesis and a difference between these rates under
the null hypothesis, <code>delta0</code>. From these values, actual rates under
the null hypothesis are computed, which are labeled <code>p10</code> and
<code>p20</code> when <code>outtype=3</code>. The rates <code>p1</code> and <code>p2</code> are used
to compute a variance for a Z-test comparing rates under the alternative
hypothesis, while <code>p10</code> and <code>p20</code> are used under the null
hypothesis. This computational method is also used to estimate variances in
<code>varBinomial()</code> based on the overall event rate observed and the input
treatment difference specified in <code>delta0</code>.
</p>
<p>Sample size with <code>scale="Difference"</code> produces an error if
<code>p1-p2=delta0</code>.  Normally, the alternative hypothesis under
consideration would be <code>p1-p2-delta0</code>$&gt;0$. However, the alternative can
have <code>p1-p2-delta0</code>$&lt;0$.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ciBinomial(x1, x2, n1, n2, alpha = 0.05, adj = 0, scale = "Difference")

nBinomial(
  p1,
  p2,
  alpha = 0.025,
  beta = 0.1,
  delta0 = 0,
  ratio = 1,
  sided = 1,
  outtype = 1,
  scale = "Difference",
  n = NULL
)

simBinomial(
  p1,
  p2,
  n1,
  n2,
  delta0 = 0,
  nsim = 10000,
  chisq = 0,
  adj = 0,
  scale = "Difference"
)

testBinomial(
  x1,
  x2,
  n1,
  n2,
  delta0 = 0,
  chisq = 0,
  adj = 0,
  scale = "Difference",
  tol = 1e-11
)

varBinomial(x, n, delta0 = 0, ratio = 1, scale = "Difference")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ciBinomial_+3A_x1">x1</code></td>
<td>
<p>Number of &ldquo;successes&rdquo; in the control group</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_x2">x2</code></td>
<td>
<p>Number of &ldquo;successes&rdquo; in the experimental group</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_n1">n1</code></td>
<td>
<p>Number of observations in the control group</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_n2">n2</code></td>
<td>
<p>Number of observations in the experimental group</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_alpha">alpha</code></td>
<td>
<p>type I error; see <code>sided</code> below to distinguish between 1-
and 2-sided tests</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_adj">adj</code></td>
<td>
<p>With <code>adj=1</code>, the standard variance with a continuity
correction is used for a Miettinen and Nurminen test statistic This includes
a factor of <code class="reqn">n / (n - 1)</code> where <code class="reqn">n</code> is the total sample size. If
<code>adj</code> is not 1, this factor is not applied. The default is <code>adj=0</code>
since nominal Type I error is generally conservative with <code>adj=1</code>
(Gordon and Watson, 1996).</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_scale">scale</code></td>
<td>
<p>&ldquo;Difference&rdquo;, &ldquo;RR&rdquo;, &ldquo;OR&rdquo;; see the
<code>scale</code> parameter documentation above and Details.  This is a scalar
argument.</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_p1">p1</code></td>
<td>
<p>event rate in group 1 under the alternative hypothesis</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_p2">p2</code></td>
<td>
<p>event rate in group 2 under the alternative hypothesis</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_beta">beta</code></td>
<td>
<p>type II error</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_delta0">delta0</code></td>
<td>
<p>A value of 0 (the default) always represents no difference
between treatment groups under the null hypothesis. <code>delta0</code> is
interpreted differently depending on the value of the parameter
<code>scale</code>.  If <code>scale="Difference"</code> (the default), <code>delta0</code> is
the difference in event rates under the null hypothesis (p10 - p20). If
<code>scale="RR"</code>, <code>delta0</code> is the logarithm of the relative risk of
event rates (p10 / p20) under the null hypothesis. If <code>scale="LNOR"</code>,
<code>delta0</code> is the difference in natural logarithm of the odds-ratio under
the null hypothesis <code>log(p10 / (1 - p10)) - log(p20 / (1 - p20))</code>.</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_ratio">ratio</code></td>
<td>
<p>sample size ratio for group 2 divided by group 1</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_sided">sided</code></td>
<td>
<p>2 for 2-sided test, 1 for 1-sided test</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_outtype">outtype</code></td>
<td>
<p><code>nBinomial</code> only; 1 (default) returns total sample size;
2 returns a data frame with sample size for each group (<code>n1, n2</code>; if
<code>n</code> is not input as <code>NULL</code>, power is returned in <code>Power</code>; 3
returns a data frame with total sample size (<code>n</code>), sample size in each
group (<code>n1, n2</code>), Type I error (<code>alpha</code>), 1 or 2 (<code>sided</code>, as
input), Type II error (<code>beta</code>), power (<code>Power</code>), null and
alternate hypothesis standard deviations (<code>sigma0, sigma1</code>), input
event rates (<code>p1, p2</code>), null hypothesis difference in treatment group
means (<code>delta0</code>) and null hypothesis event rates (<code>p10, p20</code>).</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_n">n</code></td>
<td>
<p>If power is to be computed in <code>nBinomial()</code>, input total trial
sample size in <code>n</code>; this may be a vector. This is also the sample size
in <code>varBinomial</code>, in which case the argument must be a scalar.</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_nsim">nsim</code></td>
<td>
<p>The number of simulations to be performed in
<code>simBinomial()</code></p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_chisq">chisq</code></td>
<td>
<p>An indicator of whether or not a chi-square (as opposed to Z)
statistic is to be computed. If <code>delta0=0</code> (default), the difference in
event rates divided by its standard error under the null hypothesis is used.
Otherwise, a Miettinen and Nurminen chi-square statistic for a 2 x 2 table
is used.</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_tol">tol</code></td>
<td>
<p>Default should probably be used; this is used to deal with a
rounding issue in interim calculations</p>
</td></tr>
<tr><td><code id="ciBinomial_+3A_x">x</code></td>
<td>
<p>Number of &ldquo;successes&rdquo; in the combined control and
experimental groups.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>testBinomial()</code> and <code>simBinomial()</code> each return a vector
of either Chi-square or Z test statistics.  These may be compared to an
appropriate cutoff point (e.g., <code>qnorm(.975)</code> for normal or
<code>qchisq(.95,1)</code> for chi-square).
</p>
<p><code>ciBinomial()</code> returns a data frame with 1 row with a confidence
interval; variable names are <code>lower</code> and <code>upper</code>.
</p>
<p><code>varBinomial()</code> returns a vector of (blinded) variance estimates of the
difference of event rates (<code>scale="Difference"</code>), logarithm of the
odds-ratio (<code>scale="OR"</code>) or logarithm of the risk-ratio
(<code>scale="RR"</code>).
</p>
<p>With the default <code>outtype=1</code>, <code>nBinomial()</code> returns a vector of
total sample sizes is returned.  With <code>outtype=2</code>, <code>nBinomial()</code>
returns a data frame containing two vectors <code>n1</code> and <code>n2</code>
containing sample sizes for groups 1 and 2, respectively; if <code>n</code> is
input, this option also returns the power in a third vector, <code>Power</code>.
With <code>outtype=3</code>, <code>nBinomial()</code> returns a data frame with the
following columns: </p>
<table>
<tr><td><code>n</code></td>
<td>
<p>A vector with total samples size required for
each event rate comparison specified</p>
</td></tr> <tr><td><code>n1</code></td>
<td>
<p>A vector of sample sizes for
group 1 for each event rate comparison specified</p>
</td></tr> <tr><td><code>n2</code></td>
<td>
<p>A vector of
sample sizes for group 2 for each event rate comparison specified</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>sided</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As input; if
<code>n</code> is input, this is computed</p>
</td></tr> <tr><td><code>Power</code></td>
<td>
<p>If <code>n=NULL</code> on input,
this is <code>1-beta</code>; otherwise, the power is computed for each sample size
input</p>
</td></tr> <tr><td><code>sigma0</code></td>
<td>
<p>A vector containing the standard deviation of the
treatment effect difference under the null hypothesis times <code>sqrt(n)</code>
when <code>scale="Difference"</code> or <code>scale="OR"</code>; when <code>scale="RR"</code>,
this is the standard deviation time <code>sqrt(n)</code> for the numerator of the
Farrington-Manning test statistic <code>x1-exp(delta0)*x2</code>.</p>
</td></tr> <tr><td><code>sigma1</code></td>
<td>
<p>A
vector containing the values as <code>sigma0</code>, in this case estimated under
the alternative hypothesis.</p>
</td></tr> <tr><td><code>p1</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>p2</code></td>
<td>
<p>As input</p>
</td></tr>
<tr><td><code>p10</code></td>
<td>
<p>group 1 event rate used for null hypothesis</p>
</td></tr> <tr><td><code>p20</code></td>
<td>
<p>group 2
event rate used for null hypothesis</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Farrington, CP and Manning, G (1990), Test statistics and sample
size formulae for comparative binomial trials with null hypothesis of
non-zero risk difference or non-unity relative risk. <em>Statistics in
Medicine</em>; 9: 1447-1454.
</p>
<p>Fleiss, JL, Tytun, A and Ury (1980), A simple approximation for calculating
sample sizes for comparing independent proportions.
<em>Biometrics</em>;36:343-346.
</p>
<p>Gordon, I and Watson R (1985), The myth of continuity-corrected sample size
formulae. <em>Biometrics</em>; 52: 71-76.
</p>
<p>Miettinen, O and Nurminen, M (1985), Comparative analysis of two rates.
<em>Statistics in Medicine</em>; 4 : 213-226.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+Normal">Normal</a></code>,<code><a href="stats.html#topic+uniroot">uniroot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Compute z-test test statistic comparing 39/500 to 13/500
# use continuity correction in variance
x &lt;- testBinomial(x1 = 39, x2 = 13, n1 = 500, n2 = 500, adj = 1)
x
pnorm(x, lower.tail = FALSE)

# Compute with unadjusted variance
x0 &lt;- testBinomial(x1 = 39, x2 = 23, n1 = 500, n2 = 500)
x0
pnorm(x0, lower.tail = FALSE)

# Perform 50k simulations to test validity of the above
# asymptotic p-values
# (you may want to perform more to reduce standard error of estimate)
sum(as.double(x0) &lt;=
  simBinomial(p1 = .078, p2 = .078, n1 = 500, n2 = 500, nsim = 10000)) / 10000
sum(as.double(x0) &lt;=
  simBinomial(p1 = .052, p2 = .052, n1 = 500, n2 = 500, nsim = 10000)) / 10000

# Perform a non-inferiority test to see if p2=400 / 500 is within 5% of
# p1=410 / 500 use a z-statistic with unadjusted variance
x &lt;- testBinomial(x1 = 410, x2 = 400, n1 = 500, n2 = 500, delta0 = -.05)
x
pnorm(x, lower.tail = FALSE)

# since chi-square tests equivalence (a 2-sided test) rather than
# non-inferiority (a 1-sided test),
# the result is quite different
pchisq(testBinomial(
  x1 = 410, x2 = 400, n1 = 500, n2 = 500, delta0 = -.05,
  chisq = 1, adj = 1
), 1, lower.tail = FALSE)

# now simulate the z-statistic witthout continuity corrected variance
sum(qnorm(.975) &lt;=
  simBinomial(p1 = .8, p2 = .8, n1 = 500, n2 = 500, nsim = 100000)) / 100000

# compute a sample size to show non-inferiority
# with 5% margin, 90% power
nBinomial(p1 = .2, p2 = .2, delta0 = .05, alpha = .025, sided = 1, beta = .1)

# assuming a slight advantage in the experimental group lowers
# sample size requirement
nBinomial(p1 = .2, p2 = .19, delta0 = .05, alpha = .025, sided = 1, beta = .1)

# compute a sample size for comparing 15% vs 10% event rates
# with 1 to 2 randomization
nBinomial(p1 = .15, p2 = .1, beta = .2, ratio = 2, alpha = .05)

# now look at total sample size using 1-1 randomization
n &lt;- nBinomial(p1 = .15, p2 = .1, beta = .2, alpha = .05)
n
# check if inputing sample size returns the desired power
nBinomial(p1 = .15, p2 = .1, beta = .2, alpha = .05, n = n)

# re-do with alternate output types
nBinomial(p1 = .15, p2 = .1, beta = .2, alpha = .05, outtype = 2)
nBinomial(p1 = .15, p2 = .1, beta = .2, alpha = .05, outtype = 3)

# look at power plot under different control event rate and
# relative risk reductions
library(dplyr)
library(ggplot2)
p1 &lt;- seq(.075, .2, .000625)
len &lt;- length(p1)
p2 &lt;- c(p1 * .75, p1 * 2/3, p1 * .6, p1 * .5)
Reduction &lt;- c(rep("25 percent", len), rep("33 percent", len), 
               rep("40 percent", len), rep("50 percent", len))
df &lt;- tibble(p1 = rep(p1, 4), p2, Reduction) %&gt;% 
  mutate(`Sample size` = nBinomial(p1, p2, beta = .2, alpha = .025, sided = 1))
ggplot(df, aes(x = p1, y = `Sample size`, col = Reduction)) + 
  geom_line() + 
  xlab("Control group event rate") +
  ylim(0,6000) +
  ggtitle("Binomial sample size computation for 80 pct power")

# compute blinded estimate of treatment effect difference
x1 &lt;- rbinom(n = 1, size = 100, p = .2)
x2 &lt;- rbinom(n = 1, size = 200, p = .1)
# blinded estimate of risk difference variance
varBinomial(x = x1 + x2, n = 300, ratio = 2, delta0 = 0)
# blinded estimate of log-risk-ratio variance
varBinomial(x = x1 + x2, n = 300, ratio = 2, delta0 = 0, scale = "RR")
# blinded estimate of log-odds-ratio variance
varBinomial(x = x1 + x2, n = 300, ratio = 2, delta0 = 0, scale = "OR")
</code></pre>

<hr>
<h2 id='condPower'>Sample size re-estimation based on conditional power</h2><span id='topic+condPower'></span><span id='topic+ssrCP'></span><span id='topic+plot.ssrCP'></span><span id='topic+z2NC'></span><span id='topic+z2Z'></span><span id='topic+z2Fisher'></span><span id='topic+Power.ssrCP'></span>

<h3>Description</h3>

<p><code>ssrCP()</code> adapts 2-stage group sequential designs to 2-stage sample
size re-estimation designs based on interim analysis conditional power. This
is a simple case of designs developed by Lehmacher and Wassmer, Biometrics
(1999). The conditional power designs of Bauer and Kohne (1994), 
Proschan and Hunsberger (1995), Cui, Hung and Wang (1999) and Liu and Chi (2001), 
Gao, Ware and Mehta (2008), and Mehta and Pocock (2011). Either the estimated
treatment effect at the interim analysis or any chosen effect size can be
used for sample size re-estimation. If not done carefully, these designs can
be very inefficient. It is probably a good idea to compare any design to a
simpler group sequential design; see, for example, Jennison and Turnbull
(2003). The a assumes a small Type I error is included with the interim
analysis and that the design is an adaptation from a 2-stage group
sequential design Related functions include 3 pre-defined combination test
functions (<code>z2NC</code>, <code>z2Z</code>, <code>z2Fisher</code>) that represent the
inverse normal combination test (Lehmacher and Wassmer, 1999), the
sufficient statistic for the complete data, and Fisher's combination test.
<code>Power.ssrCP</code> computes unconditional power for a conditional power
design derived by <code>ssrCP</code>.
</p>
<p><code>condPower</code> is a supportive routine that also is interesting in its own
right; it computes conditional power of a combination test given an interim
test statistic, stage 2 sample size and combination test statistic. While
the returned data frame should make general plotting easy, the function
<code>plot.ssrCP()</code> prints a plot of study sample size by stage 1 outcome
with multiple x-axis labels for stage 1 z-value, conditional power, and
stage 1 effect size relative to the effect size for which the underlying
group sequential design was powered.
</p>
<p>Sample size re-estimation using conditional power and an interim estimate of
treatment effect was proposed by several authors in the 1990's (see
references below). Statistical testing for these original methods was based
on combination tests since Type I error could be inflated by using a
sufficient statistic for testing at the end of the trial. Since 2000, more
efficient variations of these conditional power designs have been developed.
Fully optimized designs have also been derived (Posch et all, 2003,
Lokhnygina and Tsiatis, 2008). Some of the later conditional power methods
allow use of sufficient statistics for testing (Chen, DeMets and Lan, 2004,
Gao, Ware and Mehta, 2008, and Mehta and Pocock, 2011).
</p>
<p>The methods considered here are extensions of 2-stage group sequential
designs that include both an efficacy and a futility bound at the planned
interim analysis. A maximum fold-increase in sample size (<code>maxinc</code>)from
the supplied group sequential design (<code>x</code>) is specified by the user, as
well as a range of conditional power (<code>cpadj</code>) where sample size should
be re-estimated at the interim analysis, 1-the targeted conditional power to
be used for sample size re-estimation (<code>beta</code>) and a combination test
statistic (<code>z2</code>) to be used for testing at the end of the trial. The
input value <code>overrun</code> represents incremental enrollment not included in
the interim analysis that is not included in the analysis; this is used for
calculating the required number of patients enrolled to complete the trial.
</p>
<p>Whereas most of the methods proposed have been based on using the interim
estimated treatment effect size (default for <code>ssrCP</code>), the variable
<code>theta</code> allows the user to specify an alternative; Liu and Chi (2001)
suggest that using the parameter value for which the trial was originally
powered is a good choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condPower(
  z1,
  n2,
  z2 = z2NC,
  theta = NULL,
  x = gsDesign(k = 2, timing = 0.5, beta = beta),
  ...
)

ssrCP(
  z1,
  theta = NULL,
  maxinc = 2,
  overrun = 0,
  beta = x$beta,
  cpadj = c(0.5, 1 - beta),
  x = gsDesign(k = 2, timing = 0.5),
  z2 = z2NC,
  ...
)

## S3 method for class 'ssrCP'
plot(
  x,
  z1ticks = NULL,
  mar = c(7, 4, 4, 4) + 0.1,
  ylab = "Adapted sample size",
  xlaboffset = -0.2,
  lty = 1,
  col = 1,
  ...
)

z2NC(z1, x, ...)

z2Z(z1, x, n2 = x$n.I[2] - x$n.I[1], ...)

z2Fisher(z1, x, ...)

Power.ssrCP(x, theta = NULL, delta = NULL, r = 18)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condPower_+3A_z1">z1</code></td>
<td>
<p>Scalar or vector with interim standardized Z-value(s). Input of
multiple values makes it easy to plot the revised sample size as a function
of the interim test statistic.</p>
</td></tr>
<tr><td><code id="condPower_+3A_n2">n2</code></td>
<td>
<p>stage 2 sample size to be used in computing sufficient statistic
when combining stage 1 and 2 test statistics.</p>
</td></tr>
<tr><td><code id="condPower_+3A_z2">z2</code></td>
<td>
<p>a combination function that returns a test statistic cutoff for
the stage 2 test based in the interim test statistic supplied in <code>z1</code>,
the design <code>x</code> and the stage 2 sample size.</p>
</td></tr>
<tr><td><code id="condPower_+3A_theta">theta</code></td>
<td>
<p>If <code>NULL</code> (default), conditional power calculation will be
based on estimated interim treatment effect. Otherwise, <code>theta</code> is the
standardized effect size used for conditional power calculation. Using the
alternate hypothesis treatment effect can be more efficient than the
estimated effect size; see Liu and Chi, Biometrics (2001).</p>
</td></tr>
<tr><td><code id="condPower_+3A_x">x</code></td>
<td>
<p>A group sequential design with 2 stages (<code>k=2</code>) generated by
<code><a href="#topic+gsDesign">gsDesign</a></code>. For <code>plot.ssrCP</code>, <code>x</code> is a design returned
by <code>ssrCP()</code>.</p>
</td></tr>
<tr><td><code id="condPower_+3A_...">...</code></td>
<td>
<p>Allows passing of arguments that may be needed by the
user-supplied function, codez2. In the case of <code>plot.ssrCP()</code>, allows
passing more graphical parameters.</p>
</td></tr>
<tr><td><code id="condPower_+3A_maxinc">maxinc</code></td>
<td>
<p>Maximum fold-increase from planned maximum sample size in
underlying group sequential design provided in <code>x</code>.</p>
</td></tr>
<tr><td><code id="condPower_+3A_overrun">overrun</code></td>
<td>
<p>The number of patients enrolled before the interim analysis
is completed who are not included in the interim analysis.</p>
</td></tr>
<tr><td><code id="condPower_+3A_beta">beta</code></td>
<td>
<p>Targeted Type II error (1 - targeted conditional power); used
for conditional power in sample size reestimation.</p>
</td></tr>
<tr><td><code id="condPower_+3A_cpadj">cpadj</code></td>
<td>
<p>Range of values strictly between 0 and 1 specifying the range
of interim conditional power for which sample size re-estimation is to be
performed. Outside of this range, the sample size supplied in <code>x</code> is
used.</p>
</td></tr>
<tr><td><code id="condPower_+3A_z1ticks">z1ticks</code></td>
<td>
<p>Test statistic values at which tick marks are to be made on
x-axis; automatically calculated under default of <code>NULL</code></p>
</td></tr>
<tr><td><code id="condPower_+3A_mar">mar</code></td>
<td>
<p>Plot margins; see help for <code>par</code></p>
</td></tr>
<tr><td><code id="condPower_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="condPower_+3A_xlaboffset">xlaboffset</code></td>
<td>
<p>offset on x-axis for printing x-axis labels</p>
</td></tr>
<tr><td><code id="condPower_+3A_lty">lty</code></td>
<td>
<p>line type for stage 2 sample size</p>
</td></tr>
<tr><td><code id="condPower_+3A_col">col</code></td>
<td>
<p>line color for stage 2 sample size</p>
</td></tr>
<tr><td><code id="condPower_+3A_delta">delta</code></td>
<td>
<p>Natural parameter values for power calculation; see
<code><a href="#topic+gsDesign">gsDesign</a></code> for a description of how this is related to
<code>theta</code>.</p>
</td></tr>
<tr><td><code id="condPower_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ssrCP</code> returns a list with the following items: </p>
<table>
<tr><td><code>x</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>z2fn</code></td>
<td>
<p>As input in <code>z2</code>.</p>
</td></tr> <tr><td><code>theta</code></td>
<td>
<p>standardize effect
size used for conditional power; if <code>NULL</code> is input, this is computed
as <code>z1/sqrt(n1)</code> where <code>n1</code> is the stage 1 sample size.</p>
</td></tr>
<tr><td><code>maxinc</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>overrun</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>cpadj</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>dat</code></td>
<td>
<p>A data frame containing the input
<code>z1</code> values, computed cutoffs for the standard normal test statistic
based solely on stage 2 data (<code>z2</code>), stage 2 sample sizes (<code>n2</code>),
stage 2 conditional power (<code>CP</code>), standardize effect size used for
conditional power calculation (<code>theta</code>), and the natural parameter
value corresponding to <code>theta</code> (<code>delta</code>). The relation between
<code>theta</code> and <code>delta</code> is determined by the <code>delta0</code> and
<code>delta1</code> values from <code>x</code>: <code>delta = delta0 +
theta(delta1-delta0)</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Bauer, Peter and Kohne, F., Evaluation of experiments with
adaptive interim analyses, Biometrics, 50:1029-1041, 1994.
</p>
<p>Chen, YHJ, DeMets, DL and Lan, KKG. Increasing the sample size when the
unblinded interim result is promising, Statistics in Medicine, 23:1023-1038,
2004.
</p>
<p>Gao, P, Ware, JH and Mehta, C, Sample size re-estimation for adaptive
sequential design in clinical trials, Journal of Biopharmaceutical
Statistics&quot;, 18:1184-1196, 2008.
</p>
<p>Jennison, C and Turnbull, BW.  Mid-course sample size modification in
clinical trials based on the observed treatment effect. Statistics in
Medicine, 22:971-993&quot;, 2003.
</p>
<p>Lehmacher, W and Wassmer, G. Adaptive sample size calculations in group
sequential trials, Biometrics, 55:1286-1290, 1999.
</p>
<p>Liu, Q and Chi, GY., On sample size and inference for two-stage adaptive
designs, Biometrics, 57:172-177, 2001.
</p>
<p>Mehta, C and Pocock, S. Adaptive increase in sample size when interim
results are promising: A practical guide with examples, Statistics in
Medicine, 30:3267-3284, 2011.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# quick trick for simple conditional power based on interim z-value, stage 1 and 2 sample size
# assumed treatment effect and final alpha level
# and observed treatment effect
alpha &lt;- .01 # set final nominal significance level
timing &lt;- .6 # set proportion of sample size, events or statistical information at IA
n2 &lt;- 40 # set stage 2 sample size events or statistical information
hr &lt;- .6 # for this example we will derive conditional power based on hazard ratios
n.fix &lt;- nEvents(hr=hr,alpha=alpha) # you could otherwise make n.fix an arbitrary positive value
# this just derives a group sequential design that should not change sample size from n.fix
# due to stringent IA bound
x &lt;- gsDesign(k=2,n.fix=n.fix,alpha=alpha,test.type=1,sfu=sfHSD,
sfupar=-20,timing=timing,delta1=log(hr))
# derive effect sizes for which you wish to compute conditional power
hrpostIA = seq(.4,1,.05)
# in the following, we convert HR into standardized effect size based on the design in x
powr &lt;- condPower(x=x,z1=1,n2=x$n.I[2]-x$n.I[1],theta=log(hrpostIA)/x$delta1*x$theta[2])
ggplot(
  data.frame(
    x = hrpostIA,
    y = condPower(
      x = x,
      z1 = 1,
      n2 = x$n.I[2] - x$n.I[1],
      theta = log(hrpostIA) / x$delta1 * x$theta[2]
    )
  ),
  aes(x = x, y = y)
) +
  geom_line() +
  labs(
    x = "HR post IA",
    y = "Conditional power",
    title = "Conditional power as a function of assumed HR"
  )

# Following is a template for entering parameters for ssrCP
# Natural parameter value null and alternate hypothesis values
delta0 &lt;- 0
delta1 &lt;- 1
# timing of interim analysis for underlying group sequential design
timing &lt;- .5
# upper spending function
sfu &lt;- sfHSD
# upper spending function paramater
sfupar &lt;- -12
# maximum sample size inflation
maxinflation &lt;- 2
# assumed enrollment overrrun at IA
overrun &lt;- 25
# interim z-values for plotting
z &lt;- seq(0, 4, .025)
# Type I error (1-sided)
alpha &lt;- .025
# Type II error for design
beta &lt;- .1
# Fixed design sample size
n.fix &lt;- 100
# conditional power interval where sample
# size is to be adjusted
cpadj &lt;- c(.3, .9)
# targeted Type II error when adapting sample size
betastar &lt;- beta
# combination test (built-in options are: z2Z, z2NC, z2Fisher)
z2 &lt;- z2NC

# use the above parameters to generate design
# generate a 2-stage group sequential design with
x &lt;- gsDesign(
  k = 2, n.fix = n.fix, timing = timing, sfu = sfu, sfupar = sfupar,
  alpha = alpha, beta = beta, delta0 = delta0, delta1 = delta1
)
# extend this to a conditional power design
xx &lt;- ssrCP(
  x = x, z1 = z, overrun = overrun, beta = betastar, cpadj = cpadj,
  maxinc = maxinflation, z2 = z2
)
# plot the stage 2 sample size
plot(xx)
# demonstrate overlays on this plot
# overlay with densities for z1 under different hypotheses
lines(z, 80 + 240 * dnorm(z, mean = 0), col = 2)
lines(z, 80 + 240 * dnorm(z, mean = sqrt(x$n.I[1]) * x$theta[2]), col = 3)
lines(z, 80 + 240 * dnorm(z, mean = sqrt(x$n.I[1]) * x$theta[2] / 2), col = 4)
lines(z, 80 + 240 * dnorm(z, mean = sqrt(x$n.I[1]) * x$theta[2] * .75), col = 5)
axis(side = 4, at = 80 + 240 * seq(0, .4, .1), labels = as.character(seq(0, .4, .1)))
mtext(side = 4, expression(paste("Density for ", z[1])), line = 2)
text(x = 1.5, y = 90, col = 2, labels = expression(paste("Density for ", theta, "=0")))
text(x = 3.00, y = 180, col = 3, labels = expression(paste("Density for ", theta, "=",
 theta[1])))
text(x = 1.00, y = 180, col = 4, labels = expression(paste("Density for ", theta, "=",
 theta[1], "/2")))
text(x = 2.5, y = 140, col = 5, labels = expression(paste("Density for ", theta, "=",
 theta[1], "*.75")))
# overall line for max sample size
nalt &lt;- xx$maxinc * x$n.I[2]
lines(x = par("usr")[1:2], y = c(nalt, nalt), lty = 2)

# compare above design with different combination tests
# use sufficient statistic for final testing
xxZ &lt;- ssrCP(
  x = x, z1 = z, overrun = overrun, beta = betastar, cpadj = cpadj,
  maxinc = maxinflation, z2 = z2Z
)
# use Fisher combination test for final testing
xxFisher &lt;- ssrCP(
  x = x, z1 = z, overrun = overrun, beta = betastar, cpadj = cpadj,
  maxinc = maxinflation, z2 = z2Fisher
)
# combine data frames from these designs
y &lt;- rbind(
  data.frame(cbind(xx$dat, Test = "Normal combination")),
  data.frame(cbind(xxZ$dat, Test = "Sufficient statistic")),
  data.frame(cbind(xxFisher$dat, Test = "Fisher combination"))
)
# plot stage 2 statistic required for positive combination test
ggplot2::ggplot(data = y, ggplot2::aes(x = z1, y = z2, col = Test)) + 
ggplot2::geom_line()
# plot total sample size versus stage 1 test statistic
ggplot2::ggplot(data = y, ggplot2::aes(x = z1, y = n2, col = Test)) + 
ggplot2::geom_line()
# check achieved Type I error for sufficient statistic design
Power.ssrCP(x = xxZ, theta = 0)

# compare designs using observed vs planned theta for conditional power
xxtheta1 &lt;- ssrCP(
  x = x, z1 = z, overrun = overrun, beta = betastar, cpadj = cpadj,
  maxinc = maxinflation, z2 = z2, theta = x$delta
)
# combine data frames for the 2 designs
y &lt;- rbind(
  data.frame(cbind(xx$dat, "CP effect size" = "Obs. at IA")),
  data.frame(cbind(xxtheta1$dat, "CP effect size" = "Alt. hypothesis"))
)
# plot stage 2 sample size by design
ggplot2::ggplot(data = y, ggplot2::aes(x = z1, y = n2, col = CP.effect.size)) + 
ggplot2::geom_line()
# compare power and expected sample size
y1 &lt;- Power.ssrCP(x = xx)
y2 &lt;- Power.ssrCP(x = xxtheta1)
# combine data frames for the 2 designs
y3 &lt;- rbind(
  data.frame(cbind(y1, "CP effect size" = "Obs. at IA")),
  data.frame(cbind(y2, "CP effect size" = "Alt. hypothesis"))
)
# plot expected sample size by design and effect size
ggplot2::ggplot(data = y3, ggplot2::aes(x = delta, y = en, col = CP.effect.size)) + 
ggplot2::geom_line() +
ggplot2::xlab(expression(delta)) + 
ggplot2::ylab("Expected sample size")
# plot power by design and effect size
ggplot2::ggplot(data = y3, ggplot2::aes(x = delta, y = Power, col = CP.effect.size)) + 
ggplot2::geom_line() + 
ggplot2::xlab(expression(delta))
</code></pre>

<hr>
<h2 id='eEvents'>Expected number of events for a time-to-event study</h2><span id='topic+eEvents'></span><span id='topic+print.eEvents'></span>

<h3>Description</h3>

<p><code>eEvents()</code> is used to calculate the expected number of events for a
population with a time-to-event endpoint.  It is based on calculations
demonstrated in Lachin and Foulkes (1986) and is fundamental in computations
for the sample size method they propose. Piecewise exponential survival and
dropout rates are supported as well as piecewise uniform enrollment. A
stratified population is allowed. Output is the expected number of events
observed given a trial duration and the above rate parameters.
</p>
<p><code>eEvents()</code> produces an object of class <code>eEvents</code> with the number
of subjects and events for a set of pre-specified trial parameters, such as
accrual duration and follow-up period. The underlying power calculation is
based on Lachin and Foulkes (1986) method for proportional hazards assuming
a fixed underlying hazard ratio between 2 treatment groups. The method has
been extended here to enable designs to test non-inferiority. Piecewise
constant enrollment and failure rates are assumed and a stratified
population is allowed. See also <code><a href="#topic+nSurvival">nSurvival</a></code> for other Lachin and
Foulkes (1986) methods assuming a constant hazard difference or exponential
enrollment rate.
</p>
<p><code>print.eEvents()</code> formats the output for an object of class
<code>eEvents</code> and returns the input value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eEvents(
  lambda = 1,
  eta = 0,
  gamma = 1,
  R = 1,
  S = NULL,
  T = 2,
  Tfinal = NULL,
  minfup = 0,
  digits = 4
)

## S3 method for class 'eEvents'
print(x, digits = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eEvents_+3A_lambda">lambda</code></td>
<td>
<p>scalar, vector or matrix of event hazard rates; rows represent
time periods while columns represent strata; a vector implies a single
stratum.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_eta">eta</code></td>
<td>
<p>scalar, vector or matrix of dropout hazard rates; rows represent
time periods while columns represent strata; if entered as a scalar, rate is
constant across strata and time periods; if entered as a vector, rates are
constant across strata.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_gamma">gamma</code></td>
<td>
<p>a scalar, vector or matrix of rates of entry by time period
(rows) and strata (columns); if entered as a scalar, rate is constant
across strata and time periods; if entered as a vector, rates are constant
across strata.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_r">R</code></td>
<td>
<p>a scalar or vector of durations of time periods for recruitment
rates specified in rows of <code>gamma</code>. Length is the same as number of
rows in <code>gamma</code>. Note that the final enrollment period is extended as
long as needed.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_s">S</code></td>
<td>
<p>a scalar or vector of durations of piecewise constant event rates
specified in rows of <code>lambda</code>, <code>eta</code> and <code>etaE</code>; this is NULL
if there is a single event rate per stratum (exponential failure) or length
of the number of rows in <code>lambda</code> minus 1, otherwise.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_t">T</code></td>
<td>
<p>time of analysis; if <code>Tfinal=NULL</code>, this is also the study
duration.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_tfinal">Tfinal</code></td>
<td>
<p>Study duration; if <code>NULL</code>, this will be replaced with
<code>T</code> on output.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_minfup">minfup</code></td>
<td>
<p>time from end of planned enrollment (<code>sum(R)</code> from output
value of <code>R</code>) until <code>Tfinal</code>.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_digits">digits</code></td>
<td>
<p>which controls number of digits for printing.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_x">x</code></td>
<td>
<p>an object of class <code>eEvents</code> returned from <code>eEvents()</code>.</p>
</td></tr>
<tr><td><code id="eEvents_+3A_...">...</code></td>
<td>
<p>Other arguments that may be passed to the generic print function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>eEvents()</code> and <code>print.eEvents()</code> return an object of
class <code>eEvents</code> which contains the following items: </p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>as
input; converted to a matrix on output.</p>
</td></tr> <tr><td><code>eta</code></td>
<td>
<p>as input; converted to a
matrix on output.</p>
</td></tr> <tr><td><code>gamma</code></td>
<td>
<p>as input.</p>
</td></tr> <tr><td><code>R</code></td>
<td>
<p>as input.</p>
</td></tr> <tr><td><code>S</code></td>
<td>
<p>as
input.</p>
</td></tr> <tr><td><code>T</code></td>
<td>
<p>as input.</p>
</td></tr> <tr><td><code>Tfinal</code></td>
<td>
<p>planned duration of study.</p>
</td></tr>
<tr><td><code>minfup</code></td>
<td>
<p>as input.</p>
</td></tr> <tr><td><code>d</code></td>
<td>
<p>expected number of events.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>expected sample size.</p>
</td></tr> <tr><td><code>digits</code></td>
<td>
<p>as input.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Lachin JM and Foulkes MA (1986), Evaluation of Sample Size and
Power for Analyses of Survival with Allowance for Nonuniform Patient Entry,
Losses to Follow-Up, Noncompliance, and Stratification. <em>Biometrics</em>,
42, 507-519.
</p>
<p>Bernstein D and Lagakos S (1978), Sample size and power determination for
stratified clinical trials. <em>Journal of Statistical Computation and
Simulation</em>, 8:65-73.
</p>


<h3>See Also</h3>

<p><code>vignette("gsDesignPackageOverview")</code>, <a href="#topic+plot.gsDesign">plot.gsDesign</a>,
<code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+gsHR">gsHR</a></code>,
<code><a href="#topic+nSurvival">nSurvival</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 3 enrollment periods, 3 piecewise exponential failure rates
str(eEvents(
  lambda = c(.05, .02, .01), eta = .01, gamma = c(5, 10, 20),
  R = c(2, 1, 2), S = c(1, 1), T = 20
))

# control group for example from Bernstein and Lagakos (1978)
lamC &lt;- c(1, .8, .5)
n &lt;- eEvents(
  lambda = matrix(c(lamC, lamC * 2 / 3), ncol = 6), eta = 0,
  gamma = matrix(.5, ncol = 6), R = 2, T = 4
)

</code></pre>

<hr>
<h2 id='gsBinomialExact'>One-Sample Binomial Routines</h2><span id='topic+gsBinomialExact'></span><span id='topic+print.gsBinomialExact'></span><span id='topic+binomialSPRT'></span><span id='topic+plot.gsBinomialExact'></span><span id='topic+plot.binomialSPRT'></span><span id='topic+nBinomial1Sample'></span>

<h3>Description</h3>

<p><code>gsBinomialExact</code> computes power/Type I error and expected sample size
for a group sequential design in a single-arm trial with a binary outcome.
This can also be used to compare event rates in two-arm studies. The print
function has been extended using <code>print.gsBinomialExact</code> to print
<code>gsBinomialExact</code> objects. Similarly, a plot function has
been extended using <code>plot.gsBinomialExact</code> to plot
<code>gsBinomialExact</code> objects.
</p>
<p><code>binomialSPRT</code> computes a truncated binomial sequential probability
ratio test (SPRT) which is a specific instance of an exact binomial group
sequential design for a single arm trial with a binary outcome.
</p>
<p><code>gsBinomialPP</code> computes a truncated binomial (group) sequential design
based on predictive probability.
</p>
<p><code>nBinomial1Sample</code> uses exact binomial calculations to compute power
and sample size for single arm binomial experiments.
</p>
<p><code>gsBinomialExact</code> is based on the book &quot;Group Sequential Methods with
Applications to Clinical Trials,&quot; Christopher Jennison and Bruce W.
Turnbull, Chapter 12, Section 12.1.2 Exact Calculations for Binary Data.
This computation is often used as an approximation for the distribution of
the number of events in one treatment group out of all events when the
probability of an event is small and sample size is large.
</p>
<p>An object of class <code>gsBinomialExact</code> is returned. On output, the values
of <code>theta</code> input to <code>gsBinomialExact</code> will be the parameter values
for which the boundary crossing probabilities and expected sample sizes are
computed.
</p>
<p>Note that a[1] equal to -1 lower bound at n.I[1] means 0 successes continues
at interim 1; a[2]==0 at interim 2 means 0 successes stops trial for
futility at 2nd analysis.  For final analysis, set a[k] equal to b[k]-1 to
incorporate all possibilities into non-positive trial; see example.
</p>
<p>The sequential probability ratio test (SPRT) is a sequential testing scheme
allowing testing after each observation. This likelihood ratio is used to
determine upper and lower cutoffs which are linear and parallel in the
number of responses as a function of sample size.  <code>binomialSPRT</code>
produces a variation the the SPRT that tests only within a range of sample
sizes. While the linear SPRT bounds are continuous, actual bounds are the
integer number of response at or beyond each linear bound for each sample
size where testing is performed. Because of the truncation and
discretization of the bounds, power and Type I error achieve will be lower
than the nominal levels specified by <code>alpha</code> and <code>beta</code> which can
be altered to produce desired values that are achieved by the planned sample
size. See also example that shows computation of Type I error when futility
bound is considered non-binding.
</p>
<p>Note that if the objective of a design is to demonstrate that a rate (e.g.,
failure rate) is lower than a certain level, two approaches can be taken.
First, 1 minus the failure rate is the success rate and this can be used for
planning. Second, the role of <code>beta</code> becomes to express Type I error
and <code>alpha</code> is used to express Type II error.
</p>
<p>Plots produced include boundary plots, expected sample size, response rate
at the boundary and power.
</p>
<p><code>gsBinomial1Sample</code> uses exact binomial computations based on the base
R functions <code>qbinom()</code> and <code>pbinom()</code>. The tabular output may be
convenient for plotting. Note that input variables are largely not checked,
so the user is largely responsible for results; it is a good idea to do a
run with <code>outtype=3</code> to check that you have done things appropriately.
If <code>n</code> is not ordered (a bad idea) or not sequential (maybe OK), be
aware of possible consequences.
</p>
<p><code>nBinomial1Sample</code> is based on code from Marc Schwartz <a href="mailto:marc_schwartz@me.com">marc_schwartz@me.com</a>. 
The possible sample size vector <code>n</code> needs to be selected in such a fashion
that it covers the possible range of values that include the true minimum. 
NOTE: the one-sided evaluation of significance is more conservative than using the 2-sided exact test in <code>binom.test</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsBinomialExact(
  k = 2,
  theta = c(0.1, 0.2),
  n.I = c(50, 100),
  a = c(3, 7),
  b = c(20, 30)
)

binomialSPRT(
  p0 = 0.05,
  p1 = 0.25,
  alpha = 0.1,
  beta = 0.15,
  minn = 10,
  maxn = 35
)

## S3 method for class 'gsBinomialExact'
plot(x, plottype = 1, ...)

## S3 method for class 'binomialSPRT'
plot(x, plottype = 1, ...)

nBinomial1Sample(
  p0 = 0.9,
  p1 = 0.95,
  alpha = 0.025,
  beta = NULL,
  n = 200:250,
  outtype = 1,
  conservative = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsBinomialExact_+3A_k">k</code></td>
<td>
<p>Number of analyses planned, including interim and final.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_theta">theta</code></td>
<td>
<p>Vector of possible underling binomial probabilities for a
single binomial sample.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_n.i">n.I</code></td>
<td>
<p>Sample size at analyses (increasing positive integers); vector of
length k.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_a">a</code></td>
<td>
<p>Number of &quot;successes&quot; required to cross lower bound cutoffs to
reject <code>p1</code> in favor of <code>p0</code> at each analysis; vector of length k;
-1 (minimum allowed) means no lower bound.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_b">b</code></td>
<td>
<p>Number of &quot;successes&quot; required to cross upper bound cutoffs for
rejecting <code>p0</code> in favor of <code>p1</code> at each analysis; vector of length
k; value &gt; n.I means no upper bound.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_p0">p0</code></td>
<td>
<p>Lower of the two response (event) rates hypothesized.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_p1">p1</code></td>
<td>
<p>Higher of the two response (event) rates hypothesized.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_alpha">alpha</code></td>
<td>
<p>Nominal probability of rejecting response (event) rate
<code>p0</code> when it is true.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_beta">beta</code></td>
<td>
<p>Nominal probability of rejecting response (event) rate <code>p1</code>
when it is true. If NULL, Type II error will be computed for all input values 
of <code>n</code> and output will be in a data frame.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_minn">minn</code></td>
<td>
<p>Minimum sample size at which sequential testing begins.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_maxn">maxn</code></td>
<td>
<p>Maximum sample size.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_x">x</code></td>
<td>
<p>Item of class <code>gsBinomialExact</code> or <code>binomialSPRT</code> for
<code>print.gsBinomialExact</code>. Item of class <code>gsBinomialExact</code> for
<code>plot.gsBinomialExact</code>. Item of class <code>binomialSPRT</code> for item of
class <code>plot.binomialSPRT</code>.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_plottype">plottype</code></td>
<td>
<p>1 produces a plot with counts of response at bounds (for
<code>binomialSPRT</code>, also produces linear SPRT bounds); 2 produces a plot
with power to reject null and alternate response rates as well as the
probability of not crossing a bound by the maximum sample size; 3 produces a
plot with the response rate at the boundary as a function of sample size
when the boundary is crossed; 6 produces a plot of the expected sample size
by the underlying event rate (this assumes there is no enrollment beyond the
sample size where the boundary is crossed).</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_...">...</code></td>
<td>
<p>arguments passed through to <code>ggplot</code>.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_n">n</code></td>
<td>
<p>sample sizes to be considered for <code>nBinomial1Sample</code>. These
should be ordered from smallest to largest and be &gt; 0.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_outtype">outtype</code></td>
<td>
<p>Operative when <code>beta != NULL</code>. <code>1</code> means routine
will return a single integer sample size while for <code>output=2</code>a data frame 
is returned (see Value); note that this not operative is <code>beta</code> is <code>NULL</code> 
in which case a data table is returned with Type II error and power for each input 
value of <code>n</code>.</p>
</td></tr>
<tr><td><code id="gsBinomialExact_+3A_conservative">conservative</code></td>
<td>
<p>operative when <code>outtype=1</code> or <code>2</code> and
<code>beta != NULL</code>. Default <code>FALSE</code> selects minimum sample size for
which power is at least <code>1-beta</code>. When <code>conservative=TRUE</code>, the
minimum sample sample size for which power is at least <code>1-beta</code> and
there is no larger sample size in the input <code>n</code> where power is less
than <code>1-beta</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gsBinomialExact()</code> returns a list of class
<code>gsBinomialExact</code> and <code>gsProbability</code> (see example); when
displaying one of these objects, the default function to print is
<code>print.gsProbability()</code>.  The object returned from
<code>gsBinomialExact()</code> contains the following elements: </p>
<table>
<tr><td><code>k</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>theta</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>n.I</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>lower</code></td>
<td>
<p>A list
containing two elements: <code>bound</code> is as input in <code>a</code> and
<code>prob</code> is a matrix of boundary crossing probabilities. Element
<code>i,j</code> contains the boundary crossing probability at analysis <code>i</code>
for the <code>j</code>-th element of <code>theta</code> input. All boundary crossing is
assumed to be binding for this computation; that is, the trial must stop if
a boundary is crossed.</p>
</td></tr> <tr><td><code>upper</code></td>
<td>
<p>A list of the same form as <code>lower</code>
containing the upper bound and upper boundary crossing probabilities.</p>
</td></tr>
<tr><td><code>en</code></td>
<td>
<p>A vector of the same length as <code>theta</code> containing expected
sample sizes for the trial design corresponding to each value in the vector
<code>theta</code>.</p>
</td></tr>
</table>
<p><code>binomialSPRT</code> produces an object of class <code>binomialSPRT</code> that is
an extension of the <code>gsBinomialExact</code> class. The values returned in
addition to those returned by <code>gsBinomialExact</code> are: </p>
<table>
<tr><td><code>intercept</code></td>
<td>
<p>A
vector of length 2 with the intercepts for the two SPRT bounds.</p>
</td></tr>
<tr><td><code>slope</code></td>
<td>
<p>A scalar with the common slope of the SPRT bounds.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>As input. Note that this will exceed the actual Type I error
achieved by the design returned.</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As input. Note that this will
exceed the actual Type II error achieved by the design returned.</p>
</td></tr>
<tr><td><code>p0</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>p1</code></td>
<td>
<p>As input.</p>
</td></tr>
</table>
<p><code>nBinomial1Sample</code> produces a data frame with power for each input value in <code>n</code> 
if <code>beta=NULL</code>. Otherwise, a sample size achieving the desired power is returned unless 
the minimum power for the values input in <code>n</code> is greater than or equal to the target or 
the maximum yields power less than the target, in which case an error message is shown. 
The input variable <code>outtype</code> has no effect if <code>beta=NULL</code>. 
Otherwise, <code>outtype=1</code> results in return of an integer sample size and <code>outtype=2</code> 
results in a data frame with one record which includes the desired sample size.
When a data frame is returned, the variables include: </p>
<table>
<tr><td><code>p0</code></td>
<td>
<p>Input null
hypothesis event (or response) rate.</p>
</td></tr> <tr><td><code>p1</code></td>
<td>
<p>Input alternative hypothesis
(or response) rate; must be <code>&gt; p0</code>.</p>
</td></tr> <tr><td><code>alpha</code></td>
<td>
<p>Input Type I error.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Input Type II error except when input is <code>NULL</code> in which
case realized Type II error is computed.</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>sample size.</p>
</td></tr> <tr><td><code>b</code></td>
<td>
<p>cutoff given <code>n</code> to control
Type I error; value is <code>NULL</code> if no such value exists.</p>
</td></tr> <tr><td><code>alphaR</code></td>
<td>
<p>Type I error achieved for each 
output value of <code>n</code>; less than or equal to the input value <code>alpha</code>.</p>
</td></tr> <tr><td><code>Power</code></td>
<td>
<p>Power achieved 
for each output value of <code>n</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Jon Hartzel, Yevgen Tymofyeyev and Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Code for nBinomial1Sample was based on code developed by
<a href="mailto:marc_schwartz@me.com">marc_schwartz@me.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsProbability">gsProbability</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)

zz &lt;- gsBinomialExact(
  k = 3, theta = seq(0.1, 0.9, 0.1), n.I = c(12, 24, 36),
  a = c(-1, 0, 11), b = c(5, 9, 12)
)

# let's see what class this is
class(zz)

# because of "gsProbability" class above, following is equivalent to
# \code{print.gsProbability(zz)}
zz

# also plot (see also plots below for \code{binomialSPRT})
# add lines using geom_line()
plot(zz) + 
ggplot2::geom_line()

# now for SPRT examples
x &lt;- binomialSPRT(p0 = .05, p1 = .25, alpha = .1, beta = .2)
# boundary plot
plot(x)
# power plot
plot(x, plottype = 2)
# Response (event) rate at boundary
plot(x, plottype = 3)
# Expected sample size at boundary crossing or end of trial
plot(x, plottype = 6)

# sample size for single arm exact binomial

# plot of table of power by sample size
# note that outtype need not be specified if beta is NULL
nb1 &lt;- nBinomial1Sample(p0 = 0.05, p1=0.2,alpha = 0.025, beta=NULL, n = 25:40)
nb1
library(scales)
ggplot2::ggplot(nb1, ggplot2::aes(x = n, y = Power)) + 
ggplot2::geom_line() + 
ggplot2::geom_point() + 
ggplot2::scale_y_continuous(labels = percent)

# simple call with same parameters to get minimum sample size yielding desired power
nBinomial1Sample(p0 = 0.05, p1 = 0.2, alpha = 0.025, beta = .2, n = 25:40)

# change to 'conservative' if you want all larger sample
# sizes to also provide adequate power
nBinomial1Sample(p0 = 0.05, p1 = 0.2, alpha = 0.025, beta = .2, n = 25:40, conservative = TRUE)

# print out more information for the selected derived sample size
nBinomial1Sample(p0 = 0.05, p1 = 0.2, alpha = 0.025, beta = .2, n = 25:40, conservative = TRUE,
 outtype = 2)
# Reproduce realized Type I error alphaR
stats::pbinom(q = 5, size = 39, prob = .05, lower.tail = FALSE)
# Reproduce realized power
stats::pbinom(q = 5, size = 39, prob = 0.2, lower.tail = FALSE)
# Reproduce critical value for positive finding
stats::qbinom(p = 1 - .025, size = 39, prob = .05) + 1
# Compute p-value for 7 successes
stats::pbinom(q = 6, size = 39, prob = .05, lower.tail = FALSE)
# what happens if input sample sizes not sufficient?
## Not run:  
  nBinomial1Sample(p0 = 0.05, p1 = 0.2, alpha = 0.025, beta = .2, n = 25:30)

## End(Not run)
</code></pre>

<hr>
<h2 id='gsBound'>Boundary derivation - low level</h2><span id='topic+gsBound'></span><span id='topic+gsBound1'></span>

<h3>Description</h3>

<p><code>gsBound()</code> and <code>gsBound1()</code> are lower-level functions used to
find boundaries for a group sequential design. They are not recommended
(especially <code>gsBound1()</code>) for casual users. These functions do not
adjust sample size as <code>gsDesign()</code> does to ensure appropriate power for
a design.
</p>
<p><code>gsBound()</code> computes upper and lower bounds given boundary crossing
probabilities assuming a mean of 0, the usual null hypothesis.
<code>gsBound1()</code> computes the upper bound given a lower boundary, upper
boundary crossing probabilities and an arbitrary mean (<code>theta</code>).
</p>
<p>The function <code>gsBound1()</code> requires special attention to detail and
knowledge of behavior when a design corresponding to the input parameters
does not exist.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsBound(I, trueneg, falsepos, tol = 1e-06, r = 18, printerr = 0)

gsBound1(theta, I, a, probhi, tol = 1e-06, r = 18, printerr = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsBound_+3A_i">I</code></td>
<td>
<p>Vector containing statistical information planned at each analysis.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_trueneg">trueneg</code></td>
<td>
<p>Vector of desired probabilities for crossing upper bound
assuming mean of 0.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_falsepos">falsepos</code></td>
<td>
<p>Vector of desired probabilities for crossing lower bound
assuming mean of 0.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_tol">tol</code></td>
<td>
<p>Tolerance for error (scalar; default is 0.000001). Normally this
will not be changed by the user.  This does not translate directly to number
of digits of accuracy, so use extra decimal places.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_r">r</code></td>
<td>
<p>Single integer value controlling grid for numerical integration as
in Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_printerr">printerr</code></td>
<td>
<p>If this scalar argument set to 1, this will print messages
from underlying C program.  Mainly intended to notify user when an output
solution does not match input specifications.  This is not intended to stop
execution as this often occurs when deriving a design in <code>gsDesign</code>
that uses beta-spending.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_theta">theta</code></td>
<td>
<p>Scalar containing mean (drift) per unit of statistical
information.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_a">a</code></td>
<td>
<p>Vector containing lower bound that is fixed for use in
<code>gsBound1</code>.</p>
</td></tr>
<tr><td><code id="gsBound_+3A_probhi">probhi</code></td>
<td>
<p>Vector of desired probabilities for crossing upper bound
assuming mean of theta.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Both routines return a list. Common items returned by the two
routines are: </p>
<table>
<tr><td><code>k</code></td>
<td>
<p>The length of vectors input; a scalar.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>As input in <code>gsBound1()</code>; 0 for <code>gsBound()</code>.</p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>a</code></td>
<td>
<p>For <code>gsbound1</code>, this is as input. For
<code>gsbound</code> this is the derived lower boundary required to yield the
input boundary crossing probabilities under the null hypothesis.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The derived upper boundary required to yield the input boundary
crossing probabilities under the null hypothesis.</p>
</td></tr> <tr><td><code>tol</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>error</code></td>
<td>
<p>Error code. 0 if no error; greater than 0
otherwise.</p>
</td></tr>
</table>
<p><code>gsBound()</code> also returns the following items: </p>
<table>
<tr><td><code>rates</code></td>
<td>
<p>a list
containing two items:</p>
</td></tr> <tr><td><code>falsepos</code></td>
<td>
<p>vector of upper boundary crossing
probabilities as input.</p>
</td></tr> <tr><td><code>trueneg</code></td>
<td>
<p>vector of lower boundary crossing
probabilities as input.</p>
</td></tr>
</table>
<p><code>gsBound1()</code> also returns the following items: </p>
<table>
<tr><td><code>problo</code></td>
<td>
<p>vector of
lower boundary crossing probabilities; computed using input lower bound and
derived upper bound.</p>
</td></tr> <tr><td><code>probhi</code></td>
<td>
<p>vector of upper boundary crossing
probabilities as input.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("gsDesignPackageOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code><a href="#topic+gsProbability">gsProbability</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# set boundaries so that probability is .01 of first crossing
# each upper boundary and .02 of crossing each lower boundary
# under the null hypothesis
x &lt;- gsBound(
  I = c(1, 2, 3) / 3, trueneg = rep(.02, 3),
  falsepos = rep(.01, 3)
)
x

#  use gsBound1 to set up boundary for a 1-sided test
x &lt;- gsBound1(
  theta = 0, I = c(1, 2, 3) / 3, a = rep(-20, 3),
  probhi = c(.001, .009, .015)
)
x$b

# check boundary crossing probabilities with gsProbability
y &lt;- gsProbability(k = 3, theta = 0, n.I = x$I, a = x$a, b = x$b)$upper$prob

#  Note that gsBound1 only computes upper bound
#  To get a lower bound under a parameter value theta:
#      use minus the upper bound as a lower bound
#      replace theta with -theta
#      set probhi as desired lower boundary crossing probabilities
#  Here we let set lower boundary crossing at 0.05 at each analysis
#  assuming theta=2.2
y &lt;- gsBound1(
  theta = -2.2, I = c(1, 2, 3) / 3, a = -x$b,
  probhi = rep(.05, 3)
)
y$b

#  Now use gsProbability to look at design
#  Note that lower boundary crossing probabilities are as
#  specified for theta=2.2, but for theta=0 the upper boundary
#  crossing probabilities are smaller than originally specified
#  above after first interim analysis
gsProbability(k = length(x$b), theta = c(0, 2.2), n.I = x$I, b = x$b, a = -y$b)
</code></pre>

<hr>
<h2 id='gsBoundCP'>Conditional Power at Interim Boundaries</h2><span id='topic+gsBoundCP'></span>

<h3>Description</h3>

<p><code>gsBoundCP()</code> computes the total probability of crossing future upper
bounds given an interim test statistic at an interim bound. For each interim
boundary, assumes an interim test statistic at the boundary and computes the
probability of crossing any of the later upper boundaries.
</p>
<p>See Conditional power section of manual for further clarification. See also
Muller and Schaffer (2001) for background theory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsBoundCP(x, theta = "thetahat", r = 18)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsBoundCP_+3A_x">x</code></td>
<td>
<p>An object of type <code>gsDesign</code> or <code>gsProbability</code></p>
</td></tr>
<tr><td><code id="gsBoundCP_+3A_theta">theta</code></td>
<td>
<p>if <code>"thetahat"</code> and <code>class(x)!="gsDesign"</code>,
conditional power computations for each boundary value are computed using
estimated treatment effect assuming a test statistic at that boundary
(<code>zi/sqrt(x$n.I[i])</code> at analysis <code>i</code>, interim test statistic
<code>zi</code> and interim sample size/statistical information of
<code>x$n.I[i]</code>). Otherwise, conditional power is computed assuming the
input scalar value <code>theta</code>.</p>
</td></tr>
<tr><td><code id="gsBoundCP_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing two vectors, <code>CPlo</code> and <code>CPhi</code>.
</p>
<table>
<tr><td><code>CPlo</code></td>
<td>
<p>A vector of length <code>x$k-1</code> with conditional powers of
crossing upper bounds given interim test statistics at each lower bound</p>
</td></tr>
<tr><td><code>CPhi</code></td>
<td>
<p>A vector of length <code>x$k-1</code> with conditional powers of
crossing upper bounds given interim test statistics at each upper bound.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Muller, Hans-Helge and Schaffer, Helmut (2001), Adaptive group sequential
designs for clinical trials: combining the advantages of adaptive and
classical group sequential approaches. <em>Biometrics</em>;57:886-891.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+gsProbability">gsProbability</a></code>,
<code><a href="#topic+gsCP">gsCP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# set up a group sequential design
x &lt;- gsDesign(k = 5)
x

# compute conditional power based on interim treatment effects
gsBoundCP(x)

# compute conditional power based on original x$delta
gsBoundCP(x, theta = x$delta)
</code></pre>

<hr>
<h2 id='gsCP'>Conditional and Predictive Power, Overall and Conditional Probability of Success</h2><span id='topic+gsCP'></span><span id='topic+gsPP'></span><span id='topic+gsPI'></span><span id='topic+gsPosterior'></span><span id='topic+gsPOS'></span><span id='topic+gsCPOS'></span>

<h3>Description</h3>

<p><code>gsCP()</code> computes conditional boundary crossing probabilities at future
planned analyses for a given group sequential design assuming an interim
z-statistic at a specified interim analysis. While <code>gsCP()</code> is designed
toward computing conditional power for a variety of underlying parameter
values, <code><a href="#topic+condPower">condPower</a></code> is built to compute conditional power for a
variety of interim test statistic values which is useful for sample size
adaptation (see <code><a href="#topic+ssrCP">ssrCP</a></code>). <code>gsPP()</code> averages conditional
power across a posterior distribution to compute predictive power.
<code>gsPI()</code> computes Bayesian prediction intervals for future analyses
corresponding to results produced by <code>gsPP()</code>.  <code>gsPosterior()</code>
computes the posterior density for the group sequential design parameter of
interest given a prior density and an interim outcome that is exact or in an
interval. <code>gsPOS()</code> computes the probability of success for a trial
using a prior distribution to average power over a set of <code>theta</code>
values of interest. <code>gsCPOS()</code> assumes no boundary has been crossed
before and including an interim analysis of interest, and computes the
probability of success based on this event. Note that <code>gsCP()</code> and
<code>gsPP()</code> take only the interim test statistic into account in computing
conditional probabilities, while <code>gsCPOS()</code> conditions on not crossing
any bound through a specified interim analysis.
</p>
<p>See Conditional power section of manual for further clarification. See also
Muller and Schaffer (2001) for background theory.
</p>
<p>For <code>gsPP()</code>, <code>gsPI()</code>, <code>gsPOS()</code> and <code>gsCPOS()</code>, the
prior distribution for the standardized parameter <code>theta</code> () for a
group sequential design specified through a gsDesign object is specified
through the arguments <code>theta</code> and <code>wgts</code>. This can be a discrete
or a continuous probability density function. For a discrete function,
generally all weights would be 1. For a continuous density, the <code>wgts</code>
would contain integration grid weights, such as those provided by
<a href="#topic+normalGrid">normalGrid</a>.
</p>
<p>For <code>gsPosterior</code>, a prior distribution in <code>prior</code> must be
composed of the vectors <code>z</code> <code>density</code>.  The vector <code>z</code>
contains points where the prior is evaluated and <code>density</code> the
corresponding density or, for a discrete distribution, the probabilities of
each point in <code>z</code>. Densities may be supplied as from
<code>normalGrid()</code> where grid weights for numerical integration are
supplied in <code>gridwgts</code>. If <code>gridwgts</code> are not supplied, they are
defaulted to 1 (equal weighting). To ensure a proper prior distribution, you
must have <code>sum(gridwgts * density)</code> equal to 1; this is NOT checked,
however.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsCP(x, theta = NULL, i = 1, zi = 0, r = 18)

gsPP(
  x,
  i = 1,
  zi = 0,
  theta = c(0, 3),
  wgts = c(0.5, 0.5),
  r = 18,
  total = TRUE
)

gsPI(
  x,
  i = 1,
  zi = 0,
  j = 2,
  level = 0.95,
  theta = c(0, 3),
  wgts = c(0.5, 0.5)
)

gsPosterior(x = gsDesign(), i = 1, zi = NULL, prior = normalGrid(), r = 18)

gsPOS(x, theta, wgts)

gsCPOS(i, x, theta, wgts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsCP_+3A_x">x</code></td>
<td>
<p>An object of type <code>gsDesign</code> or <code>gsProbability</code></p>
</td></tr>
<tr><td><code id="gsCP_+3A_theta">theta</code></td>
<td>
<p>a vector with <code class="reqn">\theta</code> value(s) at which conditional
power is to be computed; for <code>gsCP()</code> if <code>NULL</code>, an estimated
value of <code class="reqn">\theta</code> based on the interim test statistic
(<code>zi/sqrt(x$n.I[i])</code>) as well as at <code>x$theta</code> is computed. For
<code>gsPosterior</code>, this may be a scalar or an interval; for <code>gsPP</code> and
<code>gsCP</code>, this must be a scalar.</p>
</td></tr>
<tr><td><code id="gsCP_+3A_i">i</code></td>
<td>
<p>analysis at which interim z-value is given; must be from 1 to
<code>x$k-1</code></p>
</td></tr>
<tr><td><code id="gsCP_+3A_zi">zi</code></td>
<td>
<p>interim z-value at analysis i (scalar)</p>
</td></tr>
<tr><td><code id="gsCP_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
<tr><td><code id="gsCP_+3A_wgts">wgts</code></td>
<td>
<p>Weights to be used with grid points in <code>theta</code>. Length can
be one if weights are equal, otherwise should be the same length as
<code>theta</code>. Values should be positive, but do not need to sum to 1.</p>
</td></tr>
<tr><td><code id="gsCP_+3A_total">total</code></td>
<td>
<p>The default of <code>total=TRUE</code> produces the combined
probability for all planned analyses after the interim analysis specified in
<code>i</code>. Otherwise, information on each analysis is provided separately.</p>
</td></tr>
<tr><td><code id="gsCP_+3A_j">j</code></td>
<td>
<p>specific analysis for which prediction is being made; must be
<code>&gt;i</code> and no more than <code>x$k</code></p>
</td></tr>
<tr><td><code id="gsCP_+3A_level">level</code></td>
<td>
<p>The level to be used for Bayes credible intervals (which
approach confidence intervals for vague priors). The default
<code>level=.95</code> corresponds to a 95% credible interval. <code>level=0</code>
provides a point estimate rather than an interval.</p>
</td></tr>
<tr><td><code id="gsCP_+3A_prior">prior</code></td>
<td>
<p>provides a prior distribution in the form produced by
<code><a href="#topic+normalGrid">normalGrid</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gsCP()</code> returns an object of the class <code>gsProbability</code>.
Based on the input design and the interim test statistic, the output
gsDesign object has bounds for test statistics computed based on solely on
observations after interim <code>i</code>.  Boundary crossing probabilities are
computed for the input <code class="reqn">\theta</code> values. See manual and examples.
</p>
<p><code>gsPP()</code> if total==TRUE, returns a real value indicating the predictive
power of the trial conditional on the interim test statistic <code>zi</code> at
analysis <code>i</code>; otherwise returns vector with predictive power for each
future planned analysis.
</p>
<p><code>gsPI()</code> returns an interval (or point estimate if <code>level=0</code>)
indicating 100<code>level</code>% credible interval for the z-statistic at
analysis <code>j</code> conditional on the z-statistic at analysis <code>i&lt;j</code>.
The interval does not consider intervending interim analyses. The
probability estimate is based on the predictive distribution used for
<code>gsPP()</code> and requires a prior distribution for the group sequential
parameter <code>theta</code> specified in <code>theta</code> and <code>wgts</code>.
</p>
<p><code>gsPosterior()</code> returns a posterior distribution containing the the
vector <code>z</code> input in <code>prior$z</code>, the posterior density in
<code>density</code>, grid weights for integrating the posterior density as input
in <code>prior$gridwgts</code> or defaulted to a vector of ones, and the product
of the output values in <code>density</code> and <code>gridwgts</code> in <code>wgts</code>.
</p>
<p><code>gsPOS()</code> returns a real value indicating the probability of a positive
study weighted by the prior distribution input for <code>theta</code>.
</p>
<p><code>gsCPOS()</code> returns a real value indicating the probability of a
positive study weighted by the posterior distribution derived from the
interim test statistic and the prior distribution input for <code>theta</code>
conditional on an interim test statistic.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Proschan, Michael A., Lan, KK Gordon and Wittes, Janet Turk (2006),
<em>Statistical Monitoring of Clinical Trials</em>. NY: Springer.
</p>
<p>Muller, Hans-Helge and Schaffer, Helmut (2001), Adaptive group sequential
designs for clinical trials: combining the advantages of adaptive and
classical group sequential approaches. <em>Biometrics</em>;57:886-891.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalGrid">normalGrid</a></code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code><a href="#topic+gsProbability">gsProbability</a></code>, <code><a href="#topic+gsBoundCP">gsBoundCP</a></code>, <code><a href="#topic+ssrCP">ssrCP</a></code>,
<code><a href="#topic+condPower">condPower</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# set up a group sequential design
x &lt;- gsDesign(k = 5)
x

# set up a prior distribution for the treatment effect
# that is normal with mean .75*x$delta and standard deviation x$delta/2
mu0 &lt;- .75 * x$delta
sigma0 &lt;- x$delta / 2
prior &lt;- normalGrid(mu = mu0, sigma = sigma0)

# compute POS for the design given the above prior distribution for theta
gsPOS(x = x, theta = prior$z, wgts = prior$wgts)

# assume POS should only count cases in prior where theta &gt;= x$delta/2
gsPOS(x = x, theta = prior$z, wgts = prior$wgts * (prior$z &gt;= x$delta / 2))

# assuming a z-value at lower bound at analysis 2, what are conditional
# boundary crossing probabilities for future analyses
# assuming theta values from x as well as a value based on the interim
# observed z
CP &lt;- gsCP(x, i = 2, zi = x$lower$bound[2])
CP

# summing values for crossing future upper bounds gives overall
# conditional power for each theta value
CP$theta
t(CP$upper$prob) %*% c(1, 1, 1)

# compute predictive probability based on above assumptions
gsPP(x, i = 2, zi = x$lower$bound[2], theta = prior$z, wgts = prior$wgts)

# if it is known that boundary not crossed at interim 2, use
# gsCPOS to compute conditional POS based on this
gsCPOS(x = x, i = 2, theta = prior$z, wgts = prior$wgts)

# 2-stage example to compare results to direct computation
x &lt;- gsDesign(k = 2)
z1 &lt;- 0.5
n1 &lt;- x$n.I[1]
n2 &lt;- x$n.I[2] - x$n.I[1]
thetahat &lt;- z1 / sqrt(n1)
theta &lt;- c(thetahat, 0, x$delta)

# conditional power direct computation - comparison w gsCP
pnorm((n2 * theta + z1 * sqrt(n1) - x$upper$bound[2] * sqrt(n1 + n2)) / sqrt(n2))

gsCP(x = x, zi = z1, i = 1)$upper$prob

# predictive power direct computation - comparison w gsPP
# use same prior as above
mu0 &lt;- .75 * x$delta * sqrt(x$n.I[2])
sigma2 &lt;- (.5 * x$delta)^2 * x$n.I[2]
prior &lt;- normalGrid(mu = .75 * x$delta, sigma = x$delta / 2)
gsPP(x = x, zi = z1, i = 1, theta = prior$z, wgts = prior$wgts)
t &lt;- .5
z1 &lt;- .5
b &lt;- z1 * sqrt(t)
# direct from Proschan, Lan and Wittes eqn 3.10
# adjusted drift at n.I[2]
pnorm(((b - x$upper$bound[2]) * (1 + t * sigma2) +
  (1 - t) * (mu0 + b * sigma2)) /
  sqrt((1 - t) * (1 + sigma2) * (1 + t * sigma2)))

# plot prior then posterior distribution for unblinded analysis with i=1, zi=1
xp &lt;- gsPosterior(x = x, i = 1, zi = 1, prior = prior)
plot(x = xp$z, y = xp$density, type = "l", col = 2, xlab = expression(theta), ylab = "Density")
points(x = x$z, y = x$density, col = 1)

# add posterior plot assuming only knowlede that interim bound has
# not been crossed at interim 1
xpb &lt;- gsPosterior(x = x, i = 1, zi = 1, prior = prior)
lines(x = xpb$z, y = xpb$density, col = 4)

# prediction interval based in interim 1 results
# start with point estimate, followed by 90% prediction interval
gsPI(x = x, i = 1, zi = z1, j = 2, theta = prior$z, wgts = prior$wgts, level = 0)
gsPI(x = x, i = 1, zi = z1, j = 2, theta = prior$z, wgts = prior$wgts, level = .9)
</code></pre>

<hr>
<h2 id='gsDensity'>Group sequential design interim density function</h2><span id='topic+gsDensity'></span>

<h3>Description</h3>

<p>Given an interim analysis <code>i</code> of a group sequential design and a vector
of real values <code>zi</code>, <code>gsDensity()</code> computes an interim density
function at analysis <code>i</code> at the values in <code>zi</code>.  For each value in
<code>zi</code>, this interim density is the derivative of the probability that
the group sequential trial does not cross a boundary prior to the
<code>i</code>-th analysis and at the <code>i</code>-th analysis the interim Z-statistic
is less than that value. When integrated over the real line, this density
computes the probability of not crossing a bound at a previous analysis. It
corresponds to the subdistribution function at analysis <code>i</code> that
excludes the probability of crossing a bound at an earlier analysis.
</p>
<p>The initial purpose of this routine was as a component needed to compute the
predictive power for a trial given an interim result; see
<code><a href="#topic+gsPP">gsPP</a></code>.
</p>
<p>See Jennison and Turnbull (2000) for details on how these computations are
performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsDensity(x, theta = 0, i = 1, zi = 0, r = 18)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsDensity_+3A_x">x</code></td>
<td>
<p>An object of type <code>gsDesign</code> or <code>gsProbability</code></p>
</td></tr>
<tr><td><code id="gsDensity_+3A_theta">theta</code></td>
<td>
<p>a vector with <code class="reqn">\theta</code> value(s) at which the interim
density function is to be computed.</p>
</td></tr>
<tr><td><code id="gsDensity_+3A_i">i</code></td>
<td>
<p>analysis at which interim z-values are given; must be from 1 to
<code>x$k</code></p>
</td></tr>
<tr><td><code id="gsDensity_+3A_zi">zi</code></td>
<td>
<p>interim z-value at analysis <code>i</code> (scalar)</p>
</td></tr>
<tr><td><code id="gsDensity_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>zi</code></td>
<td>
<p>The input vector <code>zi</code>.</p>
</td></tr> <tr><td><code>theta</code></td>
<td>
<p>The input vector
<code>theta</code>.</p>
</td></tr> <tr><td><code>density</code></td>
<td>
<p>A matrix with <code>length(zi)</code> rows and
<code>length(theta)</code> columns.  The subdensity function for <code>z[j]</code>,
<code>theta[m]</code> at analysis <code>i</code> is returned in <code>density[j,m]</code>. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+gsProbability">gsProbability</a></code>,
<code><a href="#topic+gsBoundCP">gsBoundCP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# set up a group sequential design
x &lt;- gsDesign()

# set theta values where density is to be evaluated
theta &lt;- x$theta[2] * c(0, .5, 1, 1.5)

# set zi values from -1 to 7 where density is to be evaluated
zi &lt;- seq(-3, 7, .05)

# compute subdensity values at analysis 2
y &lt;- gsDensity(x, theta = theta, i = 2, zi = zi)

# plot sub-density function for each theta value
plot(y$zi, y$density[, 3],
  type = "l", xlab = "Z",
  ylab = "Interim 2 density", lty = 3, lwd = 2
)
lines(y$zi, y$density[, 2], lty = 2, lwd = 2)
lines(y$zi, y$density[, 1], lwd = 2)
lines(y$zi, y$density[, 4], lty = 4, lwd = 2)
title("Sub-density functions at interim analysis 2")
legend(
  x = c(3.85, 7.2), y = c(.27, .385), lty = 1:5, lwd = 2, cex = 1.5,
  legend = c(
    expression(paste(theta, "=0.0")),
    expression(paste(theta, "=0.5", delta)),
    expression(paste(theta, "=1.0", delta)),
    expression(paste(theta, "=1.5", delta))
  )
)

# add vertical lines with lower and upper bounds at analysis 2
# to demonstrate how likely it is to continue, stop for futility
# or stop for efficacy at analysis 2 by treatment effect
lines(rep(x$upper$bound[2], 2), c(0, .4), col = 2)
lines(rep(x$lower$bound[2], 2), c(0, .4), lty = 2, col = 2)

# Replicate part of figures 8.1 and 8.2 of Jennison and Turnbull text book
# O'Brien-Fleming design with four analyses

x &lt;- gsDesign(k = 4, test.type = 2, sfu = "OF", alpha = .1, beta = .2)

z &lt;- seq(-4.2, 4.2, .05)
d &lt;- gsDensity(x = x, theta = x$theta, i = 4, zi = z)

plot(z, d$density[, 1], type = "l", lwd = 2, ylab = expression(paste(p[4], "(z,", theta, ")")))
lines(z, d$density[, 2], lty = 2, lwd = 2)
u &lt;- x$upper$bound[4]
text(expression(paste(theta, "=", delta)), x = 2.2, y = .2, cex = 1.5)
text(expression(paste(theta, "=0")), x = .55, y = .4, cex = 1.5)
</code></pre>

<hr>
<h2 id='gsDesign'>Design Derivation</h2><span id='topic+gsDesign'></span><span id='topic+xtable.gsDesign'></span>

<h3>Description</h3>

<p><code>gsDesign()</code> is used to find boundaries and trial size required for a
group sequential design.
</p>
<p>Many parameters normally take on default values and thus do not require
explicit specification. One- and two-sided designs are supported. Two-sided
designs may be symmetric or asymmetric. Wang-Tsiatis designs, including
O'Brien-Fleming and Pocock designs can be generated. Designs with common
spending functions as well as other built-in and user-specified functions
for Type I error and futility are supported. Type I error computations for
asymmetric designs may assume binding or non-binding lower bounds. The print
function has been extended using <code><a href="#topic+print.gsDesign">print.gsDesign</a>()</code> to print
<code>gsDesign</code> objects; see examples.
</p>
<p>The user may ignore the structure of the value returned by <code>gsDesign()</code>
if the standard printing and plotting suffice; see examples.
</p>
<p><code>delta</code> and <code>n.fix</code> are used together to determine what sample
size output options the user seeks. The default, <code>delta=0</code> and
<code>n.fix=1</code>, results in a &lsquo;generic&rsquo; design that may be used with
any sampling situation. Sample size ratios are provided and the user
multiplies these times the sample size for a fixed design to obtain the
corresponding group sequential analysis times. If <code>delta&gt;0</code>,
<code>n.fix</code> is ignored, and <code>delta</code> is taken as the standardized
effect size - the signal to noise ratio for a single observation; for
example, the mean divided by the standard deviation for a one-sample normal
problem.  In this case, the sample size at each analysis is computed.  When
<code>delta=0</code> and <code>n.fix&gt;1</code>, <code>n.fix</code> is assumed to be the sample
size for a fixed design with no interim analyses. See examples below.
</p>
<p>Following are further comments on the input argument <code>test.type</code> which
is used to control what type of error measurements are used in trial design.
The manual may also be worth some review in order to see actual formulas for
boundary crossing probabilities for the various options.  Options 3 and 5
assume the trial stops if the lower bound is crossed for Type I and Type II
error computation (binding lower bound).  For the purpose of computing Type
I error, options 4 and 6 assume the trial continues if the lower bound is
crossed (non-binding lower bound); that is a Type I error can be made by
crossing an upper bound after crossing a previous lower bound.
Beta-spending refers to error spending for the lower bound crossing
probabilities under the alternative hypothesis (options 3 and 4). In this
case, the final analysis lower and upper boundaries are assumed to be the
same. The appropriate total beta spending (power) is determined by adjusting
the maximum sample size through an iterative process for all options. Since
options 3 and 4 must compute boundary crossing probabilities under both the
null and alternative hypotheses, deriving these designs can take longer than
other options. Options 5 and 6 compute lower bound spending under the null
hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsDesign(
  k = 3,
  test.type = 4,
  alpha = 0.025,
  beta = 0.1,
  astar = 0,
  delta = 0,
  n.fix = 1,
  timing = 1,
  sfu = sfHSD,
  sfupar = -4,
  sfl = sfHSD,
  sflpar = -2,
  tol = 1e-06,
  r = 18,
  n.I = 0,
  maxn.IPlan = 0,
  nFixSurv = 0,
  endpoint = NULL,
  delta1 = 1,
  delta0 = 0,
  overrun = 0,
  usTime = NULL,
  lsTime = NULL
)

## S3 method for class 'gsDesign'
xtable(
  x,
  caption = NULL,
  label = NULL,
  align = NULL,
  digits = NULL,
  display = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsDesign_+3A_k">k</code></td>
<td>
<p>Number of analyses planned, including interim and final.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_test.type">test.type</code></td>
<td>
<p><code>1=</code>one-sided <br /> <code>2=</code>two-sided symmetric <br />
<code>3=</code>two-sided, asymmetric, beta-spending with binding lower bound <br />
<code>4=</code>two-sided, asymmetric, beta-spending with non-binding lower bound
<br /> <code>5=</code>two-sided, asymmetric, lower bound spending under the null
hypothesis with binding lower bound <br /> <code>6=</code>two-sided, asymmetric,
lower bound spending under the null hypothesis with non-binding lower bound.
<br /> See details, examples and manual.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_alpha">alpha</code></td>
<td>
<p>Type I error, always one-sided. Default value is 0.025.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_beta">beta</code></td>
<td>
<p>Type II error, default value is 0.1 (90% power).</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_astar">astar</code></td>
<td>
<p>Normally not specified. If <code>test.type=5</code> or <code>6</code>,
<code>astar</code> specifies the total probability of crossing a lower bound at
all analyses combined.  This will be changed to <code class="reqn">1 - </code><code>alpha</code> when
default value of 0 is used.  Since this is the expected usage, normally
<code>astar</code> is not specified by the user.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_delta">delta</code></td>
<td>
<p>Effect size for theta under alternative hypothesis. This can be
set to the standardized effect size to generate a sample size if
<code>n.fix=NULL</code>. See details and examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_n.fix">n.fix</code></td>
<td>
<p>Sample size for fixed design with no interim; used to find
maximum group sequential sample size. For a time-to-event outcome, input
number of events required for a fixed design rather than sample size and
enter fixed design sample size (optional) in <code>nFixSurv</code>.  See details
and examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_timing">timing</code></td>
<td>
<p>Sets relative timing of interim analyses. Default of 1
produces equally spaced analyses.  Otherwise, this is a vector of length
<code>k</code> or <code>k-1</code>.  The values should satisfy <code>0 &lt; timing[1] &lt;
timing[2] &lt; ... &lt; timing[k-1] &lt; timing[k]=1</code>.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_sfu">sfu</code></td>
<td>
<p>A spending function or a character string indicating a boundary
type (that is, &ldquo;WT&rdquo; for Wang-Tsiatis bounds, &ldquo;OF&rdquo; for
O'Brien-Fleming bounds and &ldquo;Pocock&rdquo; for Pocock bounds).  For
one-sided and symmetric two-sided testing is used to completely specify
spending (<code>test.type=1, 2</code>), <code>sfu</code>.  The default value is
<code>sfHSD</code> which is a Hwang-Shih-DeCani spending function.  See details,
<code>vignette("SpendingFunctionOverview")</code>, manual and examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_sfupar">sfupar</code></td>
<td>
<p>Real value, default is <code class="reqn">-4</code> which is an
O'Brien-Fleming-like conservative bound when used with the default
Hwang-Shih-DeCani spending function. This is a real-vector for many spending
functions.  The parameter <code>sfupar</code> specifies any parameters needed for
the spending function specified by <code>sfu</code>; this will be ignored for
spending functions (<code>sfLDOF</code>, <code>sfLDPocock</code>) or bound types
(&ldquo;OF&rdquo;, &ldquo;Pocock&rdquo;) that do not require parameters.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_sfl">sfl</code></td>
<td>
<p>Specifies the spending function for lower boundary crossing
probabilities when asymmetric, two-sided testing is performed
(<code>test.type = 3</code>, <code>4</code>, <code>5</code>, or <code>6</code>).  Unlike the upper
bound, only spending functions are used to specify the lower bound.  The
default value is <code>sfHSD</code> which is a Hwang-Shih-DeCani spending
function.  The parameter <code>sfl</code> is ignored for one-sided testing
(<code>test.type=1</code>) or symmetric 2-sided testing (<code>test.type=2</code>).  See
details, spending functions, manual and examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_sflpar">sflpar</code></td>
<td>
<p>Real value, default is <code class="reqn">-2</code>, which, with the default
Hwang-Shih-DeCani spending function, specifies a less conservative spending
rate than the default for the upper bound.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_tol">tol</code></td>
<td>
<p>Tolerance for error (default is 0.000001). Normally this will not
be changed by the user.  This does not translate directly to number of
digits of accuracy, so use extra decimal places.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80.  Larger
values provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_n.i">n.I</code></td>
<td>
<p>Used for re-setting bounds when timing of analyses changes from
initial design; see examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_maxn.iplan">maxn.IPlan</code></td>
<td>
<p>Used for re-setting bounds when timing of analyses changes
from initial design; see examples.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_nfixsurv">nFixSurv</code></td>
<td>
<p>If a time-to-event variable is used, <code>nFixSurv</code>
computed as the sample size from <code>nSurvival</code> may be entered to have
<code>gsDesign</code> compute the total sample size required as well as the number
of events at each analysis that will be returned in <code>n.fix</code>; this is
rounded up to an even number.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_endpoint">endpoint</code></td>
<td>
<p>An optional character string that should represent the type
of endpoint used for the study. This may be used by output functions. Types
most likely to be recognized initially are &quot;TTE&quot; for time-to-event outcomes
with fixed design sample size generated by <code>nSurvival()</code> and &quot;Binomial&quot;
for 2-sample binomial outcomes with fixed design sample size generated by
<code>nBinomial()</code>.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_delta1">delta1</code></td>
<td>
<p><code>delta1</code> and <code>delta0</code> may be used to store
information about the natural parameter scale compared to <code>delta</code> that
is a standardized effect size. <code>delta1</code> is the alternative hypothesis
parameter value on the natural parameter scale (e.g., the difference in two
binomial rates).</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_delta0">delta0</code></td>
<td>
<p><code>delta0</code> is the null hypothesis parameter value on the
natural parameter scale.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_overrun">overrun</code></td>
<td>
<p>Scalar or vector of length <code>k-1</code> with patients enrolled
that are not included in each interim analysis.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_ustime">usTime</code></td>
<td>
<p>Default is NULL in which case upper bound spending time is 
determined by <code>timing</code>. Otherwise, this should be a vector of length 
<code>k</code> with the spending time at each analysis (see Details).</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_lstime">lsTime</code></td>
<td>
<p>Default is NULL in which case lower bound spending time is 
determined by <code>timing</code>. Otherwise, this should be a vector of length 
<code>k</code> with the spending time at each analysis (see Details).</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object of class found among <code>methods(xtable)</code>.  See
below on how to write additional method functions for <code>xtable</code>.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_caption">caption</code></td>
<td>
<p>Character vector of length 1 or 2 containing the
table's caption or title.  If length is 2, the second item is the
&quot;short caption&quot; used when LaTeX generates a &quot;List of Tables&quot;. Set to
<code>NULL</code> to suppress the caption.  Default value is <code>NULL</code>. </p>
</td></tr>
<tr><td><code id="gsDesign_+3A_label">label</code></td>
<td>
<p>Character vector of length 1 containing the LaTeX label
or HTML anchor. Set to <code>NULL</code> to suppress the label.  Default
value is <code>NULL</code>. </p>
</td></tr>
<tr><td><code id="gsDesign_+3A_align">align</code></td>
<td>
<p>Character vector of length equal to the number of columns
of the resulting table, indicating the alignment of the corresponding
columns.  Also, <code>"|"</code> may be used to produce vertical lines
between columns in LaTeX tables, but these are effectively ignored
when considering the required length of the supplied vector.  If a
character vector of length one is supplied, it is split as
<code>strsplit(align, "")[[1]]</code> before processing. Since the row
names are printed in the first column, the length of <code>align</code> is
one greater than <code>ncol(x)</code> if <code>x</code> is a
<code>data.frame</code>. Use <code>"l"</code>, <code>"r"</code>, and <code>"c"</code> to
denote left, right, and center alignment, respectively.  Use
<code>"p{3cm}"</code> etc. for a LaTeX column of the specified width. For
HTML output the <code>"p"</code> alignment is interpreted as <code>"l"</code>,
ignoring the width request. Default depends on the class of
<code>x</code>. </p>
</td></tr>
<tr><td><code id="gsDesign_+3A_digits">digits</code></td>
<td>

<p>Numeric vector of length equal to one (in which case it will be
replicated as necessary) or to the number of columns of the
resulting table <b>or</b> matrix of the same size as the resulting
table, indicating the number of digits to display in the
corresponding columns. Since the row names are printed in the first
column, the length of the vector <code>digits</code> or the number of
columns of the matrix <code>digits</code> is one greater than
<code>ncol(x)</code> if <code>x</code> is a <code>data.frame</code>. Default depends
on the class of <code>x</code>. If values of <code>digits</code> are negative, the
corresponding values of <code>x</code> are displayed in scientific format
with <code>abs(digits)</code> digits.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_display">display</code></td>
<td>

<p>Character vector of length equal to the number of columns of the
resulting table, indicating the format for the corresponding columns.
Since the row names are printed in the first column, the length of
<code>display</code> is one greater than <code>ncol(x)</code> if <code>x</code> is a
<code>data.frame</code>.  These values are passed to the <code>formatC</code>
function.  Use <code>"d"</code> (for integers), <code>"f"</code>, <code>"e"</code>,
<code>"E"</code>, <code>"g"</code>, <code>"G"</code>, <code>"fg"</code> (for reals), or
<code>"s"</code> (for strings).  <code>"f"</code> gives numbers in the usual
<code>xxx.xxx</code> format; <code>"e"</code> and <code>"E"</code> give
<code>n.ddde+nn</code> or <code>n.dddE+nn</code> (scientific format); <code>"g"</code>
and <code>"G"</code> put <code>x[i]</code> into scientific format only if it
saves space to do so.  <code>"fg"</code> uses fixed format as <code>"f"</code>,
but <code>digits</code> as number of <em>significant</em> digits.  Note that
this can lead to quite long result strings.  Default depends on the
class of <code>x</code>.</p>
</td></tr>
<tr><td><code id="gsDesign_+3A_...">...</code></td>
<td>
<p>Additional arguments.  (Currently ignored.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the class <code>gsDesign</code>. This class has the following
elements and upon return from <code>gsDesign()</code> contains: </p>
<table>
<tr><td><code>k</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>test.type</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>alpha</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>astar</code></td>
<td>
<p>As input, except when <code>test.type=5</code> or <code>6</code>
and <code>astar</code> is input as 0; in this case <code>astar</code> is changed to
<code>1-alpha</code>.</p>
</td></tr> <tr><td><code>delta</code></td>
<td>
<p>The standardized effect size for which the
design is powered. Will be as input to <code>gsDesign()</code> unless it was input
as 0; in that case, value will be computed to give desired power for fixed
design with input sample size <code>n.fix</code>.</p>
</td></tr> <tr><td><code>n.fix</code></td>
<td>
<p>Sample size
required to obtain desired power when effect size is <code>delta</code>.</p>
</td></tr>
<tr><td><code>timing</code></td>
<td>
<p>A vector of length <code>k</code> containing the portion of the
total planned information or sample size at each analysis.</p>
</td></tr> <tr><td><code>tol</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>r</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>n.I</code></td>
<td>
<p>Vector of length <code>k</code>. If values
are input, same values are output. Otherwise, <code>n.I</code> will contain the
sample size required at each analysis to achieve desired <code>timing</code> and
<code>beta</code> for the output value of <code>delta</code>.  If <code>delta=0</code> was
input, then this is the sample size required for the specified group
sequential design when a fixed design requires a sample size of
<code>n.fix</code>. If <code>delta=0</code> and <code>n.fix=1</code> then this is the relative
sample size compared to a fixed design; see details and examples.</p>
</td></tr>
<tr><td><code>maxn.IPlan</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>nFixSurv</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>nSurv</code></td>
<td>
<p>Sample
size for Lachin and Foulkes method when <code>nSurvival</code> is used for fixed
design input. If <code>nSurvival</code> is used to compute <code>n.fix</code>, then
<code>nFixSurv</code> is inflated by the same amount as <code>n.fix</code> and stored in
<code>nSurv</code>. Note that if you use <code>gsSurv</code> for time-to-event sample
size, this is not needed and a more complete output summary is given.</p>
</td></tr>
<tr><td><code>endpoint</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>delta1</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>delta0</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>overrun</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>usTime</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>lsTime</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>upper</code></td>
<td>
<p>Upper bound spending function,
boundary and boundary crossing probabilities under the NULL and alternate
hypotheses. See <code>vignette("SpendingFunctionOverview")</code> and manual for further
details.</p>
</td></tr> <tr><td><code>lower</code></td>
<td>
<p>Lower bound spending function, boundary and boundary
crossing probabilities at each analysis. Lower spending is under alternative
hypothesis (beta spending) for <code>test.type=3</code> or <code>4</code>.  For
<code>test.type=2</code>, <code>5</code> or <code>6</code>, lower spending is under the null
hypothesis. For <code>test.type=1</code>, output value is <code>NULL</code>. See
<code>vignette("SpendingFunctionOverview")</code> and manual.</p>
</td></tr> <tr><td><code>theta</code></td>
<td>
<p>Standarized
effect size under null (0) and alternate hypothesis. If <code>delta</code> is
input, <code>theta[1]=delta</code>. If <code>n.fix</code> is input, <code>theta[1]</code> is
computed using a standard sample size formula (pseudocode):
<code>((Zalpha+Zbeta)/theta[1])^2=n.fix</code>.</p>
</td></tr> <tr><td><code>falseprobnb</code></td>
<td>
<p>For
<code>test.type=4</code> or <code>6</code>, this contains false positive probabilities
under the null hypothesis assuming that crossing a futility bound does not
stop the trial.</p>
</td></tr> <tr><td><code>en</code></td>
<td>
<p>Expected sample size accounting for early
stopping. For time-to-event outcomes, this would be the expected number of
events (although <code>gsSurv</code> will give expected sample size). For
information-based-design, this would give the expected information when the
trial stops. If <code>overrun</code> is specified, the expected sample size
includes the overrun at each interim.</p>
</td></tr>
</table>
<p>An object of class &quot;xtable&quot; with attributes specifying formatting options for a table
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
Lan KK, DeMets DL (1989). Group sequential procedures: calendar versus information 
time. <em>Statistics in medicine</em> 8(10):1191-8.
Liu, Q, Lim, P, Nuamah, I, and Li, Y (2012), On adaptive error spending approach for 
group sequential trials with random information levels. <em>Journal of biopharmaceutical statistics</em>; 22(4), 687-699.
</p>


<h3>See Also</h3>

<p><code>vignette("gsDesignPackageOverview")</code>, <a href="#topic+gsBoundSummary">gsBoundSummary</a>,
<a href="#topic+plot.gsDesign">plot.gsDesign</a>,
<code><a href="#topic+gsProbability">gsProbability</a></code>, <code>vignette("SpendingFunctionOverview")</code>,
</p>
<p><code><a href="stats.html#topic+Normal">Normal</a></code>
<code><a href="xtable.html#topic+xtable">xtable</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
#  symmetric, 2-sided design with O'Brien-Fleming-like boundaries
#  lower bound is non-binding (ignored in Type I error computation)
#  sample size is computed based on a fixed design requiring n=800
x &lt;- gsDesign(k = 5, test.type = 2, n.fix = 800)

# note that "x" below is equivalent to print(x) and print.gsDesign(x)
x
plot(x)
plot(x, plottype = 2)

# Assuming after trial was designed actual analyses occurred after
# 300, 600, and 860 patients, reset bounds
y &lt;- gsDesign(
  k = 3, test.type = 2, n.fix = 800, n.I = c(300, 600, 860),
  maxn.IPlan = x$n.I[x$k]
)
y

#  asymmetric design with user-specified spending that is non-binding
#  sample size is computed relative to a fixed design with n=1000
sfup &lt;- c(.033333, .063367, .1)
sflp &lt;- c(.25, .5, .75)
timing &lt;- c(.1, .4, .7)
x &lt;- gsDesign(
  k = 4, timing = timing, sfu = sfPoints, sfupar = sfup, sfl = sfPoints,
  sflpar = sflp, n.fix = 1000
)
x
plot(x)
plot(x, plottype = 2)

# same design, but with relative sample sizes
gsDesign(
  k = 4, timing = timing, sfu = sfPoints, sfupar = sfup, sfl = sfPoints,
  sflpar = sflp
)
</code></pre>

<hr>
<h2 id='gsProbability'>Boundary Crossing Probabilities</h2><span id='topic+gsProbability'></span><span id='topic+print.gsProbability'></span>

<h3>Description</h3>

<p>Computes power/Type I error and expected sample size for a group sequential
design across a selected set of parameter values for a given set of analyses
and boundaries. The print function has been extended using
<code>print.gsProbability</code> to print <code>gsProbability</code> objects; see
examples.
</p>
<p>Depending on the calling sequence, an object of class <code>gsProbability</code>
or class <code>gsDesign</code> is returned. If it is of class <code>gsDesign</code> then
the members of the object will be the same as described in
<code><a href="#topic+gsDesign">gsDesign</a></code>. If <code>d</code> is input as <code>NULL</code> (the default),
all other arguments (other than <code>r</code>) must be specified and an object of
class <code>gsProbability</code> is returned. If <code>d</code> is passed as an object
of class <code>gsProbability</code> or <code>gsDesign</code> the only other argument
required is <code>theta</code>; the object returned has the same class as the
input <code>d</code>. On output, the values of <code>theta</code> input to
<code>gsProbability</code> will be the parameter values for which the design is
characterized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsProbability(k = 0, theta, n.I, a, b, r = 18, d = NULL, overrun = 0)

## S3 method for class 'gsProbability'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsProbability_+3A_k">k</code></td>
<td>
<p>Number of analyses planned, including interim and final.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_theta">theta</code></td>
<td>
<p>Vector of standardized effect sizes for which boundary crossing
probabilities are to be computed.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_n.i">n.I</code></td>
<td>
<p>Sample size or relative sample size at analyses; vector of length
k. See <code><a href="#topic+gsDesign">gsDesign</a></code> and manual.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_a">a</code></td>
<td>
<p>Lower bound cutoffs (z-values) for futility or harm at each
analysis, vector of length k.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_b">b</code></td>
<td>
<p>Upper bound cutoffs (z-values) for futility at each analysis;
vector of length k.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_r">r</code></td>
<td>
<p>Control for grid as in Jennison and Turnbull (2000); default is 18,
range is 1 to 80.  Normally this will not be changed by the user.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_d">d</code></td>
<td>
<p>If not <code>NULL</code>, this should be an object of type
<code>gsDesign</code> returned by a call to <code>gsDesign()</code>.  When this is
specified, the values of <code>k</code>, <code>n.I</code>, <code>a</code>, <code>b</code>, and
<code>r</code> will be obtained from <code>d</code> and only <code>theta</code> needs to be
specified by the user.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_overrun">overrun</code></td>
<td>
<p>Scalar or vector of length <code>k-1</code> with patients enrolled
that are not included in each interim analysis.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_x">x</code></td>
<td>
<p>An item of class <code>gsProbability</code>.</p>
</td></tr>
<tr><td><code id="gsProbability_+3A_...">...</code></td>
<td>
<p>Not implemented (here for compatibility with generic print
input).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>k</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>theta</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>n.I</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>A list containing two elements: <code>bound</code> is as input in
<code>a</code> and <code>prob</code> is a matrix of boundary crossing probabilities.
Element <code>i,j</code> contains the boundary crossing probability at analysis
<code>i</code> for the <code>j</code>-th element of <code>theta</code> input. All boundary
crossing is assumed to be binding for this computation; that is, the trial
must stop if a boundary is crossed.</p>
</td></tr> <tr><td><code>upper</code></td>
<td>
<p>A list of the same form as
<code>lower</code> containing the upper bound and upper boundary crossing
probabilities.</p>
</td></tr> <tr><td><code>en</code></td>
<td>
<p>A vector of the same length as <code>theta</code>
containing expected sample sizes for the trial design corresponding to each
value in the vector <code>theta</code>.</p>
</td></tr> <tr><td><code>r</code></td>
<td>
<p>As input.</p>
</td></tr></table>
<p> Note:
<code>print.gsProbability()</code> returns the input <code>x</code>.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><a href="#topic+plot.gsDesign">plot.gsDesign</a>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# making a gsDesign object first may be easiest...
x &lt;- gsDesign()

# take a look at it
x

# default plot for gsDesign object shows boundaries
plot(x)

# \code{plottype=2} shows boundary crossing probabilities
plot(x, plottype = 2)

# now add boundary crossing probabilities and
# expected sample size for more theta values
y &lt;- gsProbability(d = x, theta = x$delta * seq(0, 2, .25))
class(y)

# note that "y" below is equivalent to \code{print(y)} and
# \code{print.gsProbability(y)}
y

# the plot does not change from before since this is a
# gsDesign object; note that theta/delta is on x axis
plot(y, plottype = 2)

# now let's see what happens with a gsProbability object
z &lt;- gsProbability(
  k = 3, a = x$lower$bound, b = x$upper$bound,
  n.I = x$n.I, theta = x$delta * seq(0, 2, .25)
)

# with the above form,  the results is a gsProbability object
class(z)
z

# default plottype is now 2
# this is the same range for theta, but plot now has theta on x axis
plot(z)
</code></pre>

<hr>
<h2 id='gsSurvCalendar'>Time-to-event endpoint design with calendar timing of analyses</h2><span id='topic+gsSurvCalendar'></span>

<h3>Description</h3>

<p>Time-to-event endpoint design with calendar timing of analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsSurvCalendar(
  test.type = 4,
  alpha = 0.025,
  sided = 1,
  beta = 0.1,
  astar = 0,
  sfu = gsDesign::sfHSD,
  sfupar = -4,
  sfl = gsDesign::sfHSD,
  sflpar = -2,
  calendarTime = c(12, 24, 36),
  spending = c("information", "calendar"),
  lambdaC = log(2)/6,
  hr = 0.6,
  hr0 = 1,
  eta = 0,
  etaE = NULL,
  gamma = 1,
  R = 12,
  S = NULL,
  minfup = 18,
  ratio = 1,
  r = 18,
  tol = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsSurvCalendar_+3A_test.type">test.type</code></td>
<td>
<p>Test type. See <code><a href="#topic+gsSurv">gsSurv</a></code>.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_alpha">alpha</code></td>
<td>
<p>Type I error rate. Default is 0.025 since 1-sided
testing is default.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_sided">sided</code></td>
<td>
<p><code>1</code> for 1-sided testing, <code>2</code> for 2-sided testing.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_beta">beta</code></td>
<td>
<p>Type II error rate. Default is 0.10
(90% power); <code>NULL</code> if power is to be computed based on
other input values.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_astar">astar</code></td>
<td>
<p>Normally not specified. If <code>test.type = 5</code>
or <code>6</code>, <code>astar</code> specifies the total probability
of crossing a lower bound at all analyses combined. This
will be changed to <code>1 - alpha</code> when default value of
<code>0</code> is used. Since this is the expected usage,
normally <code>astar</code> is not specified by the user.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_sfu">sfu</code></td>
<td>
<p>A spending function or a character string
indicating a boundary type (that is, <code>"WT"</code> for
Wang-Tsiatis bounds, <code>"OF"</code> for O'Brien-Fleming bounds and
<code>"Pocock"</code> for Pocock bounds). For one-sided and symmetric
two-sided testing is used to completely specify spending
(<code>test.type = 1</code>, <code>2</code>), <code>sfu</code>. The default value is
<code>sfHSD</code> which is a Hwang-Shih-DeCani spending function.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_sfupar">sfupar</code></td>
<td>
<p>Real value, default is <code>-4</code> which is an
O'Brien-Fleming-like conservative bound when used with the
default Hwang-Shih-DeCani spending function. This is a
real-vector for many spending functions. The parameter
<code>sfupar</code> specifies any parameters needed for the spending
function specified by <code>sfu</code>; this will be ignored for
spending functions (<code>sfLDOF</code>, <code>sfLDPocock</code>)
or bound types (<code>"OF"</code>, <code>"Pocock"</code>)
that do not require parameters.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_sfl">sfl</code></td>
<td>
<p>Specifies the spending function for lower
boundary crossing probabilities when asymmetric,
two-sided testing is performed
(<code>test.type = 3</code>, <code>4</code>, <code>5</code>, or <code>6</code>).
Unlike the upper bound,
only spending functions are used to specify the lower bound.
The default value is <code>sfHSD</code> which is a
Hwang-Shih-DeCani spending function. The parameter
<code>sfl</code> is ignored for one-sided testing
(<code>test.type = 1</code>) or symmetric 2-sided testing
(<code>test.type = 2</code>).</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_sflpar">sflpar</code></td>
<td>
<p>Real value, default is <code>-2</code>, which, with the
default Hwang-Shih-DeCani spending function, specifies a
less conservative spending rate than the default for the
upper bound.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_calendartime">calendarTime</code></td>
<td>
<p>Vector of increasing positive numbers
with calendar times of analyses. Time 0 is start of
randomization.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_spending">spending</code></td>
<td>
<p>Select between calendar-based spending and
information-based spending.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_lambdac">lambdaC</code></td>
<td>
<p>Scalar, vector or matrix of event hazard
rates for the control group; rows represent time periods while
columns represent strata; a vector implies a single stratum.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_hr">hr</code></td>
<td>
<p>Hazard ratio (experimental/control) under the
alternate hypothesis (scalar).</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_hr0">hr0</code></td>
<td>
<p>Hazard ratio (experimental/control) under the null
hypothesis (scalar).</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_eta">eta</code></td>
<td>
<p>Scalar, vector or matrix of dropout hazard rates
for the control group; rows represent time periods while
columns represent strata; if entered as a scalar, rate is
constant across strata and time periods; if entered as a
vector, rates are constant across strata.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_etae">etaE</code></td>
<td>
<p>Matrix dropout hazard rates for the experimental
group specified in like form as <code>eta</code>; if <code>NULL</code>,
this is set equal to <code>eta</code>.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_gamma">gamma</code></td>
<td>
<p>A scalar, vector or matrix of rates of entry by
time period (rows) and strata (columns); if entered as a
scalar, rate is constant across strata and time periods;
if entered as a vector, rates are constant across strata.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_r">R</code></td>
<td>
<p>A scalar or vector of durations of time periods for
recruitment rates specified in rows of <code>gamma</code>. Length is the
same as number of rows in <code>gamma</code>. Note that when variable
enrollment duration is specified (input <code>T = NULL</code>), the final
enrollment period is extended as long as needed.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_s">S</code></td>
<td>
<p>A scalar or vector of durations of piecewise constant
event rates specified in rows of <code>lambda</code>, <code>eta</code> and <code>etaE</code>;
this is <code>NULL</code> if there is a single event rate per stratum
(exponential failure) or length of the number of rows in <code>lambda</code>
minus 1, otherwise.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_minfup">minfup</code></td>
<td>
<p>A non-negative scalar less than the maximum value
in <code>calendarTime</code>. Enrollment will be cut off at the
difference between the maximum value in <code>calendarTime</code>
and <code>minfup</code>.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_ratio">ratio</code></td>
<td>
<p>Randomization ratio of experimental treatment
divided by control; normally a scalar, but may be a vector with
length equal to number of strata.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical
integration as in Jennison and Turnbull (2000); default is 18,
range is 1 to 80. Larger values provide larger number of grid
points and greater accuracy. Normally <code>r</code> will not be changed by
the user.</p>
</td></tr>
<tr><td><code id="gsSurvCalendar_+3A_tol">tol</code></td>
<td>
<p>Tolerance for error passed to the <code><a href="#topic+gsDesign">gsDesign</a></code> function.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># First example: while timing is calendar-based, spending is event-based
x &lt;- gsSurvCalendar() %&gt;% toInteger()
gsBoundSummary(x)

# Second example: both timing and spending are calendar-based
# This results in less spending at interims and leaves more for final analysis
y &lt;- gsSurvCalendar(spending = "calendar") %&gt;% toInteger()
gsBoundSummary(y)

# Note that calendar timing for spending relates to planned timing for y
# rather than timing in y after toInteger() conversion

# Values plugged into spending function for calendar time
y$usTime
# Actual calendar fraction from design after toInteger() conversion
y$T / max(y$T)
</code></pre>

<hr>
<h2 id='hGraph'>Create multiplicity graphs using ggplot2</h2><span id='topic+hGraph'></span>

<h3>Description</h3>

<p><code>hGraph()</code> plots a multiplicity graph defined by user inputs.
The graph can also be used with the **gMCPLite** package to evaluate a set of nominal p-values for the tests of the hypotheses in the graph
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hGraph(
  nHypotheses = 4,
  nameHypotheses = paste("H", (1:nHypotheses), sep = ""),
  alphaHypotheses = 0.025/nHypotheses,
  m = matrix(array(1/(nHypotheses - 1), nHypotheses^2), nrow = nHypotheses) -
    diag(1/(nHypotheses - 1), nHypotheses),
  fill = 1,
  palette = grDevices::gray.colors(length(unique(fill)), start = 0.5, end = 0.8),
  labels = LETTERS[1:length(unique(fill))],
  legend.name = " ",
  legend.position = "none",
  halfWid = 0.5,
  halfHgt = 0.5,
  trhw = 0.1,
  trhh = 0.075,
  trprop = 1/3,
  digits = 5,
  trdigits = 2,
  size = 6,
  boxtextsize = 4,
  arrowsize = 0.02,
  radianStart = if ((nHypotheses)%%2 != 0) {
     pi * (1/2 + 1/nHypotheses)
 } else {

        pi * (1 + 2/nHypotheses)/2
 },
  offset = pi/4/nHypotheses,
  xradius = 2,
  yradius = xradius,
  x = NULL,
  y = NULL,
  wchar = if (as.character(Sys.info()[1]) == "Windows") {
     "w"
 } else {
     "w"
 }
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hGraph_+3A_nhypotheses">nHypotheses</code></td>
<td>
<p>number of hypotheses in graph</p>
</td></tr>
<tr><td><code id="hGraph_+3A_namehypotheses">nameHypotheses</code></td>
<td>
<p>hypothesis names</p>
</td></tr>
<tr><td><code id="hGraph_+3A_alphahypotheses">alphaHypotheses</code></td>
<td>
<p>alpha-levels or weights for ellipses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_m">m</code></td>
<td>
<p>square transition matrix of dimension 'nHypotheses'</p>
</td></tr>
<tr><td><code id="hGraph_+3A_fill">fill</code></td>
<td>
<p>grouping variable for hypotheses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_palette">palette</code></td>
<td>
<p>colors for groups</p>
</td></tr>
<tr><td><code id="hGraph_+3A_labels">labels</code></td>
<td>
<p>text labels for groups</p>
</td></tr>
<tr><td><code id="hGraph_+3A_legend.name">legend.name</code></td>
<td>
<p>text for legend header</p>
</td></tr>
<tr><td><code id="hGraph_+3A_legend.position">legend.position</code></td>
<td>
<p>text string or x,y coordinates for legend</p>
</td></tr>
<tr><td><code id="hGraph_+3A_halfwid">halfWid</code></td>
<td>
<p>half width of ellipses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_halfhgt">halfHgt</code></td>
<td>
<p>half height of ellipses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_trhw">trhw</code></td>
<td>
<p>transition box width</p>
</td></tr>
<tr><td><code id="hGraph_+3A_trhh">trhh</code></td>
<td>
<p>transition box height</p>
</td></tr>
<tr><td><code id="hGraph_+3A_trprop">trprop</code></td>
<td>
<p>proportion of transition arrow length where transition box is placed</p>
</td></tr>
<tr><td><code id="hGraph_+3A_digits">digits</code></td>
<td>
<p>number of digits to show for alphaHypotheses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_trdigits">trdigits</code></td>
<td>
<p>digits displayed for transition weights</p>
</td></tr>
<tr><td><code id="hGraph_+3A_size">size</code></td>
<td>
<p>text size in ellipses</p>
</td></tr>
<tr><td><code id="hGraph_+3A_boxtextsize">boxtextsize</code></td>
<td>
<p>transition text size</p>
</td></tr>
<tr><td><code id="hGraph_+3A_arrowsize">arrowsize</code></td>
<td>
<p>size of arrowhead for transition arrows</p>
</td></tr>
<tr><td><code id="hGraph_+3A_radianstart">radianStart</code></td>
<td>
<p>radians from origin for first ellipse; nodes spaced equally in clockwise order with centers on an ellipse by default</p>
</td></tr>
<tr><td><code id="hGraph_+3A_offset">offset</code></td>
<td>
<p>rotational offset in radians for transition weight arrows</p>
</td></tr>
<tr><td><code id="hGraph_+3A_xradius">xradius</code></td>
<td>
<p>horizontal ellipse diameter on which ellipses are drawn</p>
</td></tr>
<tr><td><code id="hGraph_+3A_yradius">yradius</code></td>
<td>
<p>vertical ellipse diameter on which ellipses are drawn</p>
</td></tr>
<tr><td><code id="hGraph_+3A_x">x</code></td>
<td>
<p>x coordinates for hypothesis ellipses if elliptical arrangement is not wanted</p>
</td></tr>
<tr><td><code id="hGraph_+3A_y">y</code></td>
<td>
<p>y coordinates for hypothesis ellipses if elliptical arrangement is not wanted</p>
</td></tr>
<tr><td><code id="hGraph_+3A_wchar">wchar</code></td>
<td>
<p>character for alphaHypotheses in ellipses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See vignette **Multiplicity graphs formatting using ggplot2** for explanation of formatting.
</p>


<h3>Value</h3>

<p>A 'ggplot' object with a multi-layer multiplicity graph
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 'gsDesign::hGraph' is deprecated.
# See the examples in 'gMCPLite::hGraph' instead.
</code></pre>

<hr>
<h2 id='nNormal'>Normal distribution sample size (2-sample)</h2><span id='topic+nNormal'></span>

<h3>Description</h3>

<p><code>nNormal()</code> computes a fixed design sample size for comparing 2 means
where variance is known. T The function allows computation of sample size
for a non-inferiority hypothesis. Note that you may wish to investigate 
other R packages such as the <code>pwr</code> package which uses the t-distribution.
In the examples below we show how to set up a 2-arm group sequential design with a normal outcome. 
</p>
<p><code>nNormal()</code> computes sample size for comparing two normal means when
the variance for observations in
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nNormal(
  delta1 = 1,
  sd = 1.7,
  sd2 = NULL,
  alpha = 0.025,
  beta = 0.1,
  ratio = 1,
  sided = 1,
  n = NULL,
  delta0 = 0,
  outtype = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nNormal_+3A_delta1">delta1</code></td>
<td>
<p>difference between sample means under the alternate
hypothesis.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_sd">sd</code></td>
<td>
<p>Standard deviation for the control arm.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_sd2">sd2</code></td>
<td>
<p>Standard deviation of experimental arm; this will be set to be
the same as the control arm with the default of <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_alpha">alpha</code></td>
<td>
<p>type I error rate. Default is 0.025 since 1-sided testing is
default.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_beta">beta</code></td>
<td>
<p>type II error rate. Default is 0.10 (90% power). Not needed if
<code>n</code> is provided.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_ratio">ratio</code></td>
<td>
<p>randomization ratio of experimental group compared to control.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_sided">sided</code></td>
<td>
<p>1 for 1-sided test (default), 2 for 2-sided test.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_n">n</code></td>
<td>
<p>Sample size; may be input to compute power rather than sample size.
If <code>NULL</code> (default) then sample size is computed.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_delta0">delta0</code></td>
<td>
<p>difference between sample means under the null hypothesis;
normally this will be left as the default of 0.</p>
</td></tr>
<tr><td><code id="nNormal_+3A_outtype">outtype</code></td>
<td>
<p>controls output; see value section below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is more of a convenience routine than one recommended for broad use without careful considerations
such as those outlined in Jennison and Turnbull (2000).
For larger studies where a conservative estimate of within group standard deviations is available, it
can be useful.
A more detailed formulation is available in the vignette on two-sample normal sample size.
</p>


<h3>Value</h3>

<p>If <code>n</code> is <code>NULL</code> (default), total sample size (2 arms
combined) is computed. Otherwise, power is computed. If <code>outtype=1</code>
(default), the computed value (sample size or power) is returned in a scalar
or vector. If <code>outtype=2</code>, a data frame with sample sizes for each arm
(<code>n1</code>, <code>n2</code>)is returned; if <code>n</code> is not input as <code>NULL</code>,
a third variable, <code>Power</code>, is added to the output data frame. If
<code>outtype=3</code>, a data frame with is returned with the following columns:
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>A vector with total samples size required for each event rate
comparison specified</p>
</td></tr> <tr><td><code>n1</code></td>
<td>
<p>A vector of sample sizes for group 1 for
each event rate comparison specified</p>
</td></tr> <tr><td><code>n2</code></td>
<td>
<p>A vector of sample sizes for
group 2 for each event rate comparison specified</p>
</td></tr> <tr><td><code>alpha</code></td>
<td>
<p>As input</p>
</td></tr>
<tr><td><code>sided</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As input; if <code>n</code> is input, this is
computed</p>
</td></tr> <tr><td><code>Power</code></td>
<td>
<p>If <code>n=NULL</code> on input, this is <code>1-beta</code>;
otherwise, the power is computed for each sample size input</p>
</td></tr> <tr><td><code>sd</code></td>
<td>
<p>As
input</p>
</td></tr> <tr><td><code>sd2</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>delta1</code></td>
<td>
<p>As input</p>
</td></tr> <tr><td><code>delta0</code></td>
<td>
<p>As input</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard error for estimate of difference in treatment group
means</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Lachin JM (1981), Introduction to sample size determination and
power analysis for clinical trials. <em>Controlled Clinical Trials</em>
2:93-113.
</p>
<p>Snedecor GW and Cochran WG (1989), Statistical Methods. 8th ed. Ames, IA:
Iowa State University Press.
</p>


<h3>See Also</h3>

<p><code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# EXAMPLES
# equal variances
n=nNormal(delta1=.5,sd=1.1,alpha=.025,beta=.2)
n
x &lt;- gsDesign(k = 3, n.fix = n, test.type = 4, alpha = 0.025, beta = 0.1, timing = c(.5,.75),
sfu = sfLDOF, sfl = sfHSD, sflpar = -1, delta1 = 0.5, endpoint = 'normal') 
gsBoundSummary(x)
summary(x)
# unequal variances, fixed design
nNormal(delta1 = .5, sd = 1.1, sd2 = 2, alpha = .025, beta = .2)
# unequal sample sizes
nNormal(delta1 = .5, sd = 1.1, alpha = .025, beta = .2, ratio = 2)
# non-inferiority assuming a better effect than null
nNormal(delta1 = .5, delta0 = -.1, sd = 1.2)
</code></pre>

<hr>
<h2 id='normalGrid'>Normal Density Grid</h2><span id='topic+normalGrid'></span>

<h3>Description</h3>

<p>normalGrid() is intended to be used for computation of the expected value of
a function of a normal random variable.  The function produces grid points
and weights to be used for numerical integration.
</p>
<p>This is a utility function to provide a normal density function and a grid
to integrate over as described by Jennison and Turnbull (2000), Chapter 19.
While integration can be performed over the real line or over any portion of
it, the numerical integration does not extend beyond 6 standard deviations
from the mean. The grid used for integration uses equally spaced points over
the middle of the distribution function, and spreads points further apart in
the tails. The values returned in <code>gridwgts</code> may be used to integrate
any function over the given grid, although the user should take care that
the function integrated is not large in the tails of the grid where points
are spread further apart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalGrid(r = 18, bounds = c(0, 0), mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalGrid_+3A_r">r</code></td>
<td>
<p>Control for grid points as in Jennison and Turnbull (2000), Chapter
19; default is 18. Range: 1 to 80.  This might be changed by the user (e.g.,
<code>r=6</code> which produces 65 gridpoints compare to 185 points when
<code>r=18</code>) when speed is more important than precision.</p>
</td></tr>
<tr><td><code id="normalGrid_+3A_bounds">bounds</code></td>
<td>
<p>Range of integration. Real-valued vector of length 2. Default
value of 0, 0 produces a range of + or - 6 standard deviations (6*sigma)
from the mean (=mu).</p>
</td></tr>
<tr><td><code id="normalGrid_+3A_mu">mu</code></td>
<td>
<p>Mean of the desired normal distribution.</p>
</td></tr>
<tr><td><code id="normalGrid_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of the desired normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>z</code></td>
<td>
<p>Grid points for numerical integration.</p>
</td></tr> <tr><td><code>density</code></td>
<td>
<p>The
standard normal density function evaluated at the values in <code>z</code>; see
examples.</p>
</td></tr> <tr><td><code>gridwgts</code></td>
<td>
<p>Simpson's rule weights for numerical integration
on the grid in <code>z</code>; see examples.</p>
</td></tr> <tr><td><code>wgts</code></td>
<td>
<p>Weights to be used with
the grid in <code>z</code> for integrating the normal density function; see
examples. This is equal to <code>density * gridwgts</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
#  standard normal distribution
x &lt;- normalGrid(r = 3)
plot(x$z, x$wgts)

#  verify that numerical integration replicates sigma
#  get grid points and weights
x &lt;- normalGrid(mu = 2, sigma = 3)

# compute squared deviation from mean for grid points
dev &lt;- (x$z - 2)^2

# multiply squared deviations by integration weights and sum
sigma2 &lt;- sum(dev * x$wgts)

# square root of sigma2 should be sigma (3)
sqrt(sigma2)

# do it again with larger r to increase accuracy
x &lt;- normalGrid(r = 22, mu = 2, sigma = 3)
sqrt(sum((x$z - 2)^2 * x$wgts))

# this can also be done by combining gridwgts and density
sqrt(sum((x$z - 2)^2 * x$gridwgts * x$density))

# integrate normal density and compare to built-in function
# to compute probability of being within 1 standard deviation
# of the mean
pnorm(1) - pnorm(-1)
x &lt;- normalGrid(bounds = c(-1, 1))
sum(x$wgts)
sum(x$gridwgts * x$density)

# find expected sample size for default design with
# n.fix=1000
x &lt;- gsDesign(n.fix = 1000)
x

# set a prior distribution for theta
y &lt;- normalGrid(r = 3, mu = x$theta[2], sigma = x$theta[2] / 1.5)
z &lt;- gsProbability(
  k = 3, theta = y$z, n.I = x$n.I, a = x$lower$bound,
  b = x$upper$bound
)
z &lt;- gsProbability(d = x, theta = y$z)
cat(
  "Expected sample size averaged over normal\n prior distribution for theta with \n mu=",
  x$theta[2], "sigma=", x$theta[2] / 1.5, ":",
  round(sum(z$en * y$wgt), 1), "\n"
)
plot(y$z, z$en,
  xlab = "theta", ylab = "E{N}",
  main = "Expected sample size for different theta values"
)
lines(y$z, z$en)
</code></pre>

<hr>
<h2 id='plot.gsDesign'>Plots for group sequential designs</h2><span id='topic+plot.gsDesign'></span><span id='topic+plot.gsProbability'></span>

<h3>Description</h3>

<p>The <code>plot()</code> function has been extended to work with objects returned
by <code>gsDesign()</code> and <code>gsProbability()</code>.  For objects of type
<code>gsDesign</code>, seven types of plots are provided: z-values at boundaries
(default), power, approximate treatment effects at boundaries, conditional
power at boundaries, spending functions, expected sample size, and B-values
at boundaries. For objects of type <code>gsProbability</code> plots are available
for z-values at boundaries, power (default), approximate treatment effects at
boundaries, conditional power, expected sample size and B-values at
boundaries.
</p>
<p>The intent is that many standard <code>plot()</code> parameters will function as
expected; exceptions to this rule exist. In particular, <code>main, xlab,
ylab, lty, col, lwd, type, pch, cex</code> have been tested and work for most
values of <code>plottype</code>; one exception is that <code>type="l"</code> cannot be
overridden when <code>plottype=2</code>. Default values for labels depend on
<code>plottype</code> and the class of <code>x</code>.
</p>
<p>Note that there is some special behavior for values plotted and returned for
power and expected sample size (ASN) plots for a <code>gsDesign</code> object. A
call to <code>x&lt;-gsDesign()</code> produces power and expected sample size for
only two <code>theta</code> values: 0 and <code>x$delta</code>.  The call <code>plot(x,
plottype="Power")</code> (or <code>plot(x,plottype="ASN"</code>) for a <code>gsDesign</code>
object produces power (expected sample size) curves and returns a
<code>gsDesign</code> object with <code>theta</code> values determined as follows.  If
<code>theta</code> is non-null on input, the input value(s) are used. Otherwise,
for a <code>gsProbability</code> object, the <code>theta</code> values from that object
are used. For a <code>gsDesign</code> object where <code>theta</code> is input as
<code>NULL</code> (the default), <code>theta=seq(0,2,.05)*x$delta</code>) is used.  For
a <code>gsDesign</code> object, the x-axis values are rescaled to
<code>theta/x$delta</code> and the label for the x-axis <code class="reqn">\theta / \delta</code>. For a
<code>gsProbability</code> object, the values of <code>theta</code> are plotted and are
labeled as <code class="reqn">\theta</code>. See examples below.
</p>
<p>Approximate treatment effects at boundaries are computed dividing the Z-values
at the boundaries by the square root of <code>n.I</code> at that analysis.
</p>
<p>Spending functions are plotted for a continuous set of values from 0 to 1.
This option should not be used if a boundary is used or a pointwise spending
function is used (<code>sfu</code> or <code>sfl="WT", "OF", "Pocock"</code> or
<code>sfPoints</code>).
</p>
<p>Conditional power is computed using the function <code>gsBoundCP()</code>.  The
default input for this routine is <code>theta="thetahat"</code> which will compute
the conditional power at each bound using the approximate treatment effect at
that bound.  Otherwise, if the input is <code>gsDesign</code> object conditional
power is computed assuming <code>theta=x$delta</code>, the original effect size
for which the trial was planned.
</p>
<p>Average sample number/expected sample size is computed using <code>n.I</code> at
each analysis times the probability of crossing a boundary at that analysis.
If no boundary is crossed at any analysis, this is counted as stopping at
the final analysis.
</p>
<p>B-values are Z-values multiplied by <code>sqrt(t)=sqrt(x$n.I/x$n.I[x$k])</code>.
Thus, the expected value of a B-value at an analysis is the true value of
<code class="reqn">\theta</code> multiplied by the proportion of total planned observations at
that time. See Proschan, Lan and Wittes (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gsDesign'
plot(x, plottype = 1, base = FALSE, ...)

## S3 method for class 'gsProbability'
plot(x, plottype = 2, base = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gsDesign_+3A_x">x</code></td>
<td>
<p>Object of class <code>gsDesign</code> for <code>plot.gsDesign()</code> or
<code>gsProbability</code> for
</p>
<p><code>plot.gsProbability()</code>.</p>
</td></tr>
<tr><td><code id="plot.gsDesign_+3A_plottype">plottype</code></td>
<td>
<p>1=boundary plot (default for <code>gsDesign</code>),
</p>
<p>2=power plot (default for <code>gsProbability</code>),
</p>
<p>3=approximate treatment effect at boundaries,
</p>
<p>4=conditional power at boundaries,
</p>
<p>5=spending function plot (only available if <code>class(x)=="gsDesign"</code>),
</p>
<p>6=expected sample size plot, and
</p>
<p>7=B-values at boundaries.
</p>
<p>Character values for <code>plottype</code> may also be entered: <code>"Z"</code> for
plot type 1, <code>"power"</code> for plot type 2, <code>"thetahat"</code> for plot type
3, <code>"CP"</code> for plot type 4, <code>"sf"</code> for plot type 5, <code>"ASN"</code>,
<code>"N"</code> or <code>"n"</code> for plot type 6, and <code>"B"</code>, <code>"B-val"</code> or
<code>"B-value"</code> for plot type 7.</p>
</td></tr>
<tr><td><code id="plot.gsDesign_+3A_base">base</code></td>
<td>
<p>Default is FALSE, which means ggplot2 graphics are used. If
true, base graphics are used for plotting.</p>
</td></tr>
<tr><td><code id="plot.gsDesign_+3A_...">...</code></td>
<td>
<p>This allows many optional arguments that are standard when
calling <code>plot</code>.
</p>
<p>Other arguments include:
</p>
<p><code>theta</code> which is used for <code>plottype=2</code>, <code>4</code>, <code>6</code>;
normally defaults will be adequate; see details.
</p>
<p><code>ses=TRUE</code> which applies only when <code>plottype=3</code> and
</p>
<p><code>class(x)=="gsDesign"</code>; indicates that approximate standardized effect
size at the boundary is to be plotted rather than the approximate natural parameter.
</p>
<p><code>xval="Default"</code> which is only effective when <code>plottype=2</code> or
<code>6</code>. Appropriately scaled (reparameterized) values for x-axis for power
and expected sample size graphs; see details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of <code>class(x)</code>; in many cases this is the input value
of <code>x</code>, while in others <code>x$theta</code> is replaced and corresponding
characteristics computed; see details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Proschan, MA, Lan, KKG, Wittes, JT (2006), <em>Statistical Monitoring of
Clinical Trials. A Unified Approach</em>.  New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+gsProbability">gsProbability</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
#  symmetric, 2-sided design with O'Brien-Fleming-like boundaries
#  lower bound is non-binding (ignored in Type I error computation)
#  sample size is computed based on a fixed design requiring n=100
x &lt;- gsDesign(k = 5, test.type = 2, n.fix = 100)
x

# the following translate to calls to plot.gsDesign since x was
# returned by gsDesign; run these commands one at a time
plot(x)
plot(x, plottype = 2)
plot(x, plottype = 3)
plot(x, plottype = 4)
plot(x, plottype = 5)
plot(x, plottype = 6)
plot(x, plottype = 7)

#  choose different parameter values for power plot
#  start with design in x from above
y &lt;- gsProbability(
  k = 5, theta = seq(0, .5, .025), x$n.I,
  x$lower$bound, x$upper$bound
)

# the following translates to a call to plot.gsProbability since
# y has that type
plot(y)
</code></pre>

<hr>
<h2 id='print.nSurv'>Advanced time-to-event sample size calculation</h2><span id='topic+print.nSurv'></span><span id='topic+nSurv'></span><span id='topic+print.gsSurv'></span><span id='topic+xtable.gsSurv'></span><span id='topic+tEventsIA'></span><span id='topic+nEventsIA'></span><span id='topic+gsSurv'></span>

<h3>Description</h3>

<p><code>nSurv()</code> is used to calculate the sample size for a clinical trial
with a time-to-event endpoint and an assumption of proportional hazards.
This set of routines is new with version 2.7 and will continue to be
modified and refined to improve input error checking and output format with
subsequent versions. It allows both the Lachin and Foulkes (1986) method
(fixed trial duration) as well as the Kim and Tsiatis(1990) method (fixed
enrollment rates and either fixed enrollment duration or fixed minimum
follow-up). Piecewise exponential survival is supported as well as piecewise
constant enrollment and dropout rates. The methods are for a 2-arm trial
with treatment groups referred to as experimental and control. A stratified
population is allowed as in Lachin and Foulkes (1986); this method has been
extended to derive non-inferiority as well as superiority trials.
Stratification also allows power calculation for meta-analyses.
<code>gsSurv()</code> combines <code>nSurv()</code> with <code>gsDesign()</code> to derive a
group sequential design for a study with a time-to-event endpoint.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nSurv'
print(x, digits = 4, ...)

nSurv(
  lambdaC = log(2)/6,
  hr = 0.6,
  hr0 = 1,
  eta = 0,
  etaE = NULL,
  gamma = 1,
  R = 12,
  S = NULL,
  T = 18,
  minfup = 6,
  ratio = 1,
  alpha = 0.025,
  beta = 0.1,
  sided = 1,
  tol = .Machine$double.eps^0.25
)

tEventsIA(x, timing = 0.25, tol = .Machine$double.eps^0.25)

nEventsIA(tIA = 5, x = NULL, target = 0, simple = TRUE)

gsSurv(
  k = 3,
  test.type = 4,
  alpha = 0.025,
  sided = 1,
  beta = 0.1,
  astar = 0,
  timing = 1,
  sfu = sfHSD,
  sfupar = -4,
  sfl = sfHSD,
  sflpar = -2,
  r = 18,
  lambdaC = log(2)/6,
  hr = 0.6,
  hr0 = 1,
  eta = 0,
  etaE = NULL,
  gamma = 1,
  R = 12,
  S = NULL,
  T = 18,
  minfup = 6,
  ratio = 1,
  tol = .Machine$double.eps^0.25,
  usTime = NULL,
  lsTime = NULL
)

## S3 method for class 'gsSurv'
print(x, digits = 2, ...)

## S3 method for class 'gsSurv'
xtable(
  x,
  caption = NULL,
  label = NULL,
  align = NULL,
  digits = NULL,
  display = NULL,
  auto = FALSE,
  footnote = NULL,
  fnwid = "9cm",
  timename = "months",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nSurv_+3A_x">x</code></td>
<td>
<p>An object of class <code>nSurv</code> or <code>gsSurv</code>.
<code>print.nSurv()</code> is used for an object of class <code>nSurv</code> which will
generally be output from <code>nSurv()</code>. For <code>print.gsSurv()</code> is used
for an object of class <code>gsSurv</code> which will generally be output from
<code>gsSurv()</code>. <code>nEventsIA</code> and <code>tEventsIA</code> operate on both the
<code>nSurv</code> and <code>gsSurv</code> class.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_digits">digits</code></td>
<td>
<p>Number of digits past the decimal place to print
(<code>print.gsSurv.</code>); also a pass through to generic <code>xtable()</code> from
<code>xtable.gsSurv()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_...">...</code></td>
<td>
<p>other arguments that may be passed to generic functions
underlying the methods here.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_lambdac">lambdaC</code></td>
<td>
<p>scalar, vector or matrix of event hazard rates for the
control group; rows represent time periods while columns represent strata; a
vector implies a single stratum.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_hr">hr</code></td>
<td>
<p>hazard ratio (experimental/control) under the alternate hypothesis
(scalar).</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_hr0">hr0</code></td>
<td>
<p>hazard ratio (experimental/control) under the null hypothesis
(scalar).</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_eta">eta</code></td>
<td>
<p>scalar, vector or matrix of dropout hazard rates for the control
group; rows represent time periods while columns represent strata; if
entered as a scalar, rate is constant across strata and time periods; if
entered as a vector, rates are constant across strata.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_etae">etaE</code></td>
<td>
<p>matrix dropout hazard rates for the experimental group specified
in like form as <code>eta</code>; if NULL, this is set equal to <code>eta</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_gamma">gamma</code></td>
<td>
<p>a scalar, vector or matrix of rates of entry by time period
(rows) and strata (columns); if entered as a scalar, rate is constant
across strata and time periods; if entered as a vector, rates are constant
across strata.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_r">R</code></td>
<td>
<p>a scalar or vector of durations of time periods for recruitment
rates specified in rows of <code>gamma</code>. Length is the same as number of
rows in <code>gamma</code>. Note that when variable enrollment duration is
specified (input <code>T=NULL</code>), the final enrollment period is extended as
long as needed.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_s">S</code></td>
<td>
<p>a scalar or vector of durations of piecewise constant event rates
specified in rows of <code>lambda</code>, <code>eta</code> and <code>etaE</code>; this is NULL
if there is a single event rate per stratum (exponential failure) or length
of the number of rows in <code>lambda</code> minus 1, otherwise.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_t">T</code></td>
<td>
<p>study duration; if <code>T</code> is input as <code>NULL</code>, this will be
computed on output; see details.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_minfup">minfup</code></td>
<td>
<p>follow-up of last patient enrolled; if <code>minfup</code> is input
as <code>NULL</code>, this will be computed on output; see details.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_ratio">ratio</code></td>
<td>
<p>randomization ratio of experimental treatment divided by
control; normally a scalar, but may be a vector with length equal to number
of strata.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_alpha">alpha</code></td>
<td>
<p>type I error rate. Default is 0.025 since 1-sided testing is
default.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_beta">beta</code></td>
<td>
<p>type II error rate. Default is 0.10 (90% power); NULL if power
is to be computed based on other input values.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_sided">sided</code></td>
<td>
<p>1 for 1-sided testing, 2 for 2-sided testing.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_tol">tol</code></td>
<td>
<p>for cases when <code>T</code> or <code>minfup</code> values are derived
through root finding (<code>T</code> or <code>minfup</code> input as <code>NULL</code>),
<code>tol</code> provides the level of error input to the <code>uniroot()</code>
root-finding function. The default is the same as for <code><a href="stats.html#topic+uniroot">uniroot</a></code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_timing">timing</code></td>
<td>
<p>Sets relative timing of interim analyses in <code>gsSurv</code>.
Default of 1 produces equally spaced analyses.  Otherwise, this is a vector
of length <code>k</code> or <code>k-1</code>.  The values should satisfy <code>0 &lt;
timing[1] &lt; timing[2] &lt; ... &lt; timing[k-1] &lt; timing[k]=1</code>. For
<code>tEventsIA</code>, this is a scalar strictly between 0 and 1 that indicates
the targeted proportion of final planned events available at an interim
analysis.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_tia">tIA</code></td>
<td>
<p>Timing of an interim analysis; should be between 0 and
<code>y$T</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_target">target</code></td>
<td>
<p>The targeted proportion of events at an interim analysis. This
is used for root-finding will be 0 for normal use.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_simple">simple</code></td>
<td>
<p>See output specification for <code>nEventsIA()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_k">k</code></td>
<td>
<p>Number of analyses planned, including interim and final.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_test.type">test.type</code></td>
<td>
<p><code>1=</code>one-sided <br /> <code>2=</code>two-sided symmetric <br />
<code>3=</code>two-sided, asymmetric, beta-spending with binding lower bound <br />
<code>4=</code>two-sided, asymmetric, beta-spending with non-binding lower bound
<br /> <code>5=</code>two-sided, asymmetric, lower bound spending under the null
hypothesis with binding lower bound <br /> <code>6=</code>two-sided, asymmetric,
lower bound spending under the null hypothesis with non-binding lower bound.
<br /> See details, examples and manual.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_astar">astar</code></td>
<td>
<p>Normally not specified. If <code>test.type=5</code> or <code>6</code>,
<code>astar</code> specifies the total probability of crossing a lower bound at
all analyses combined.  This will be changed to <code class="reqn">1 - </code><code>alpha</code> when
default value of 0 is used.  Since this is the expected usage, normally
<code>astar</code> is not specified by the user.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_sfu">sfu</code></td>
<td>
<p>A spending function or a character string indicating a boundary
type (that is, &ldquo;WT&rdquo; for Wang-Tsiatis bounds, &ldquo;OF&rdquo; for
O'Brien-Fleming bounds and &ldquo;Pocock&rdquo; for Pocock bounds).  For
one-sided and symmetric two-sided testing is used to completely specify
spending (<code>test.type=1, 2</code>), <code>sfu</code>.  The default value is
<code>sfHSD</code> which is a Hwang-Shih-DeCani spending function.  See details,
<code>vignette("SpendingFunctionOverview")</code>, manual and examples.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_sfupar">sfupar</code></td>
<td>
<p>Real value, default is <code class="reqn">-4</code> which is an
O'Brien-Fleming-like conservative bound when used with the default
Hwang-Shih-DeCani spending function. This is a real-vector for many spending
functions.  The parameter <code>sfupar</code> specifies any parameters needed for
the spending function specified by <code>sfu</code>; this will be ignored for
spending functions (<code>sfLDOF</code>, <code>sfLDPocock</code>) or bound types
(&ldquo;OF&rdquo;, &ldquo;Pocock&rdquo;) that do not require parameters.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_sfl">sfl</code></td>
<td>
<p>Specifies the spending function for lower boundary crossing
probabilities when asymmetric, two-sided testing is performed
(<code>test.type = 3</code>, <code>4</code>, <code>5</code>, or <code>6</code>).  Unlike the upper
bound, only spending functions are used to specify the lower bound.  The
default value is <code>sfHSD</code> which is a Hwang-Shih-DeCani spending
function.  The parameter <code>sfl</code> is ignored for one-sided testing
(<code>test.type=1</code>) or symmetric 2-sided testing (<code>test.type=2</code>).  See
details, spending functions, manual and examples.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_sflpar">sflpar</code></td>
<td>
<p>Real value, default is <code class="reqn">-2</code>, which, with the default
Hwang-Shih-DeCani spending function, specifies a less conservative spending
rate than the default for the upper bound.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_r">r</code></td>
<td>
<p>Integer value controlling grid for numerical integration as in
Jennison and Turnbull (2000); default is 18, range is 1 to 80. Larger values
provide larger number of grid points and greater accuracy.  Normally
<code>r</code> will not be changed by the user.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_ustime">usTime</code></td>
<td>
<p>Default is NULL in which case upper bound spending time is
determined by <code>timing</code>. Otherwise, this should be a vector of length
<code>k</code> with the spending time at each analysis (see Details in help for <code>gsDesign</code>).</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_lstime">lsTime</code></td>
<td>
<p>Default is NULL in which case lower bound spending time is
determined by <code>timing</code>. Otherwise, this should be a vector of length
<code>k</code> with the spending time at each analysis (see Details in help for <code>gsDesign</code>).</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_caption">caption</code></td>
<td>
<p>passed through to generic <code>xtable()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_label">label</code></td>
<td>
<p>passed through to generic <code>xtable()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_align">align</code></td>
<td>
<p>passed through to generic <code>xtable()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_display">display</code></td>
<td>
<p>passed through to generic <code>xtable()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_auto">auto</code></td>
<td>
<p>passed through to generic <code>xtable()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_footnote">footnote</code></td>
<td>
<p>footnote for xtable output; may be useful for describing
some of the design parameters.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_fnwid">fnwid</code></td>
<td>
<p>a text string controlling the width of footnote text at the
bottom of the xtable output.</p>
</td></tr>
<tr><td><code id="print.nSurv_+3A_timename">timename</code></td>
<td>
<p>character string with plural of time units (e.g., &quot;months&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print()</code>, <code>xtable()</code> and <code>summary()</code> methods are provided to
operate on the returned value from <code>gsSurv()</code>, an object of class
<code>gsSurv</code>. <code>print()</code> is also extended to <code>nSurv</code> objects. The
functions <code><a href="#topic+gsBoundSummary">gsBoundSummary</a></code> (data frame for tabular output),
<code><a href="#topic+xprint">xprint</a></code> (application of <code>xtable</code> for tabular output) and
<code>summary.gsSurv</code> (textual summary of <code>gsDesign</code> or <code>gsSurv</code>
object) may be preferred summary functions; see example in vignettes. See
also <a href="#topic+gsBoundSummary">gsBoundSummary</a> for output
of tabular summaries of bounds for designs produced by <code>gsSurv()</code>.
</p>
<p>Both <code>nEventsIA</code> and <code>tEventsIA</code> require a group sequential design
for a time-to-event endpoint of class <code>gsSurv</code> as input.
<code>nEventsIA</code> calculates the expected number of events under the
alternate hypothesis at a given interim time. <code>tEventsIA</code> calculates
the time that the expected number of events under the alternate hypothesis
is a given proportion of the total events planned for the final analysis.
</p>
<p><code>nSurv()</code> produces an object of class <code>nSurv</code> with the number of
subjects and events for a set of pre-specified trial parameters, such as
accrual duration and follow-up period. The underlying power calculation is
based on Lachin and Foulkes (1986) method for proportional hazards assuming
a fixed underlying hazard ratio between 2 treatment groups. The method has
been extended here to enable designs to test non-inferiority. Piecewise
constant enrollment and failure rates are assumed and a stratified
population is allowed. See also <code><a href="#topic+nSurvival">nSurvival</a></code> for other Lachin and
Foulkes (1986) methods assuming a constant hazard difference or exponential
enrollment rate.
</p>
<p>When study duration (<code>T</code>) and follow-up duration (<code>minfup</code>) are
fixed, <code>nSurv</code> applies exactly the Lachin and Foulkes (1986) method of
computing sample size under the proportional hazards assumption when For
this computation, enrollment rates are altered proportionately to those
input in <code>gamma</code> to achieve the power of interest.
</p>
<p>Given the specified enrollment rate(s) input in <code>gamma</code>, <code>nSurv</code>
may also be used to derive enrollment duration required for a trial to have
defined power if <code>T</code> is input as <code>NULL</code>; in this case, both
<code>R</code> (enrollment duration for each specified enrollment rate) and
<code>T</code> (study duration) will be computed on output.
</p>
<p>Alternatively and also using the fixed enrollment rate(s) in <code>gamma</code>,
if minimum follow-up <code>minfup</code> is specified as <code>NULL</code>, then the
enrollment duration(s) specified in <code>R</code> are considered fixed and
<code>minfup</code> and <code>T</code> are computed to derive the desired power. This
method will fail if the specified enrollment rates and durations either
over-powers the trial with no additional follow-up or underpowers the trial
with infinite follow-up. This method produces a corresponding error message
in such cases.
</p>
<p>The input to <code>gsSurv</code> is a combination of the input to <code>nSurv()</code>
and <code>gsDesign()</code>.
</p>
<p><code>nEventsIA()</code> is provided to compute the expected number of events at a
given point in time given enrollment, event and censoring rates. The routine
is used with a root finding routine to approximate the approximate timing of
an interim analysis. It is also used to extend enrollment or follow-up of a
fixed design to obtain a sufficient number of events to power a group
sequential design.
</p>


<h3>Value</h3>

<p><code>nSurv()</code> returns an object of type <code>nSurv</code> with the
following components: </p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>sided</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Type II error; if missing, this is computed.</p>
</td></tr> <tr><td><code>power</code></td>
<td>
<p>Power
corresponding to input <code>beta</code> or computed if output <code>beta</code> is
computed.</p>
</td></tr> <tr><td><code>lambdaC</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>etaC</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>etaE</code></td>
<td>
<p>As
input.</p>
</td></tr> <tr><td><code>gamma</code></td>
<td>
<p>As input unless none of the following are <code>NULL</code>:
<code>T</code>, <code>minfup</code>, <code>beta</code>; otherwise, this is a constant times
the input value required to power the trial given the other input
variables.</p>
</td></tr> <tr><td><code>ratio</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>R</code></td>
<td>
<p>As input unless <code>T</code> was
<code>NULL</code> on input.</p>
</td></tr> <tr><td><code>S</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>T</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>minfup</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>hr</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>hr0</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Total expected sample size corresponding to output accrual rates
and durations.</p>
</td></tr> <tr><td><code>d</code></td>
<td>
<p>Total expected number of events under the alternate
hypothesis.</p>
</td></tr> <tr><td><code>tol</code></td>
<td>
<p>As input, except when not used in computations in
which case this is returned as <code>NULL</code>.  This and the remaining output
below are not printed by the <code>print()</code> extension for the <code>nSurv</code>
class.</p>
</td></tr> <tr><td><code>eDC</code></td>
<td>
<p>A vector of expected number of events by stratum in the
control group under the alternate hypothesis.</p>
</td></tr> <tr><td><code>eDE</code></td>
<td>
<p>A vector of
expected number of events by stratum in the experimental group under the
alternate hypothesis.</p>
</td></tr> <tr><td><code>eDC0</code></td>
<td>
<p>A vector of expected number of events by
stratum in the control group under the null hypothesis.</p>
</td></tr> <tr><td><code>eDE0</code></td>
<td>
<p>A
vector of expected number of events by stratum in the experimental group
under the null hypothesis.</p>
</td></tr> <tr><td><code>eNC</code></td>
<td>
<p>A vector of the expected accrual in
each stratum in the control group.</p>
</td></tr> <tr><td><code>eNE</code></td>
<td>
<p>A vector of the expected
accrual in each stratum in the experimental group.</p>
</td></tr> <tr><td><code>variable</code></td>
<td>
<p>A text
string equal to &quot;Accrual rate&quot; if a design was derived by varying the
accrual rate, &quot;Accrual duration&quot; if a design was derived by varying the
accrual duration, &quot;Follow-up duration&quot; if a design was derived by varying
follow-up duration, or &quot;Power&quot; if accrual rates and duration as well as
follow-up duration was specified and <code>beta=NULL</code> was input.</p>
</td></tr>
</table>
<p><code>gsSurv()</code> returns much of the above plus variables in the class
<code>gsDesign</code>; see <code><a href="#topic+gsDesign">gsDesign</a></code>
for general documentation on what is returned in <code>gs</code>.  The value of
<code>gs$n.I</code> represents the number of endpoints required at each analysis
to adequately power the trial. Other items returned by <code>gsSurv()</code> are:
</p>
<table>
<tr><td><code>lambdaC</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>etaC</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>etaE</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>As input unless none of the following are <code>NULL</code>:
<code>T</code>, <code>minfup</code>, <code>beta</code>; otherwise, this is a constant times
the input value required to power the trial given the other input
variables.</p>
</td></tr> <tr><td><code>ratio</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>R</code></td>
<td>
<p>As input unless <code>T</code> was
<code>NULL</code> on input.</p>
</td></tr> <tr><td><code>S</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>T</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>minfup</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>hr</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>hr0</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>eNC</code></td>
<td>
<p>Total expected sample size corresponding to output accrual rates
and durations.</p>
</td></tr> <tr><td><code>eNE</code></td>
<td>
<p>Total expected sample size corresponding to
output accrual rates and durations.</p>
</td></tr> <tr><td><code>eDC</code></td>
<td>
<p>Total expected number of
events under the alternate hypothesis.</p>
</td></tr> <tr><td><code>eDE</code></td>
<td>
<p>Total expected number of
events under the alternate hypothesis.</p>
</td></tr> <tr><td><code>tol</code></td>
<td>
<p>As input, except when not
used in computations in which case this is returned as <code>NULL</code>.  This
and the remaining output below are not printed by the <code>print()</code>
extension for the <code>nSurv</code> class.</p>
</td></tr> <tr><td><code>eDC</code></td>
<td>
<p>A vector of expected
number of events by stratum in the control group under the alternate
hypothesis.</p>
</td></tr> <tr><td><code>eDE</code></td>
<td>
<p>A vector of expected number of events by stratum in
the experimental group under the alternate hypothesis.</p>
</td></tr> <tr><td><code>eNC</code></td>
<td>
<p>A vector of
the expected accrual in each stratum in the control group.</p>
</td></tr> <tr><td><code>eNE</code></td>
<td>
<p>A
vector of the expected accrual in each stratum in the experimental group.</p>
</td></tr>
<tr><td><code>variable</code></td>
<td>
<p>A text string equal to &quot;Accrual rate&quot; if a design was
derived by varying the accrual rate, &quot;Accrual duration&quot; if a design was
derived by varying the accrual duration, &quot;Follow-up duration&quot; if a design
was derived by varying follow-up duration, or &quot;Power&quot; if accrual rates and
duration as well as follow-up duration was specified and <code>beta=NULL</code>
was input.</p>
</td></tr>
</table>
<p><code>nEventsIA()</code> returns the expected proportion of the final planned
events observed at the input analysis time minus <code>target</code> when
<code>simple=TRUE</code>. When <code>simple=FALSE</code>, <code>nEventsIA</code> returns a
list with following components: </p>
<table>
<tr><td><code>T</code></td>
<td>
<p>The input value <code>tIA</code>.</p>
</td></tr>
<tr><td><code>eDC</code></td>
<td>
<p>The expected number of events in the control group at time the
output time <code>T</code>.</p>
</td></tr> <tr><td><code>eDE</code></td>
<td>
<p>The expected number of events in the
experimental group at the output time <code>T</code>.</p>
</td></tr> <tr><td><code>eNC</code></td>
<td>
<p>The expected
enrollment in the control group at the output time <code>T</code>.</p>
</td></tr> <tr><td><code>eNE</code></td>
<td>
<p>The
expected enrollment in the experimental group at the output time <code>T</code>.</p>
</td></tr>
</table>
<p><code>tEventsIA()</code> returns the same structure as <code>nEventsIA(..., simple=TRUE)</code> when
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Kim KM and Tsiatis AA (1990), Study duration for clinical trials
with survival response and early stopping rule. <em>Biometrics</em>, 46, 81-92
</p>
<p>Lachin JM and Foulkes MA (1986), Evaluation of Sample Size and Power for
Analyses of Survival with Allowance for Nonuniform Patient Entry, Losses to
Follow-Up, Noncompliance, and Stratification. <em>Biometrics</em>, 42,
507-519.
</p>
<p>Schoenfeld D (1981), The Asymptotic Properties of Nonparametric Tests for
Comparing Survival Distributions. <em>Biometrika</em>, 68, 316-319.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsBoundSummary">gsBoundSummary</a></code>, <code><a href="#topic+xprint">xprint</a></code>,
<code>vignette("gsDesignPackageOverview")</code>, <a href="#topic+plot.gsDesign">plot.gsDesign</a>,
<code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+gsHR">gsHR</a></code>, <code><a href="#topic+nSurvival">nSurvival</a></code>
</p>
<p><code><a href="stats.html#topic+uniroot">uniroot</a></code>
</p>
<p><code><a href="stats.html#topic+Normal">Normal</a></code>
<code><a href="xtable.html#topic+xtable">xtable</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# vary accrual rate to obtain power
nSurv(lambdaC = log(2) / 6, hr = .5, eta = log(2) / 40, gamma = 1, T = 36, minfup = 12)

# vary accrual duration to obtain power
nSurv(lambdaC = log(2) / 6, hr = .5, eta = log(2) / 40, gamma = 6, minfup = 12)

# vary follow-up duration to obtain power
nSurv(lambdaC = log(2) / 6, hr = .5, eta = log(2) / 40, gamma = 6, R = 25)

# piecewise constant enrollment rates (vary accrual duration)
nSurv(
  lambdaC = log(2) / 6, hr = .5, eta = log(2) / 40, gamma = c(1, 3, 6),
  R = c(3, 6, 9), T = NULL, minfup = 12
)

# stratified population (vary accrual duration)
nSurv(
  lambdaC = matrix(log(2) / c(6, 12), ncol = 2), hr = .5, eta = log(2) / 40,
  gamma = matrix(c(2, 4), ncol = 2), minfup = 12
)

# piecewise exponential failure rates (vary accrual duration)
nSurv(lambdaC = log(2) / c(6, 12), hr = .5, eta = log(2) / 40, S = 3, gamma = 6, minfup = 12)

# combine it all: 2 strata, 2 failure rate periods
nSurv(
  lambdaC = matrix(log(2) / c(6, 12, 18, 24), ncol = 2), hr = .5,
  eta = matrix(log(2) / c(40, 50, 45, 55), ncol = 2), S = 3,
  gamma = matrix(c(3, 6, 5, 7), ncol = 2), R = c(5, 10), minfup = 12
)

# example where only 1 month of follow-up is desired
# set failure rate to 0 after 1 month using lambdaC and S
nSurv(lambdaC = c(.4, 0), hr = 2 / 3, S = 1, minfup = 1)

# group sequential design (vary accrual rate to obtain power)
x &lt;- gsSurv(
  k = 4, sfl = sfPower, sflpar = .5, lambdaC = log(2) / 6, hr = .5,
  eta = log(2) / 40, gamma = 1, T = 36, minfup = 12
)
x
print(xtable::xtable(x,
  footnote = "This is a footnote; note that it can be wide.",
  caption = "Caption example."
))
# find expected number of events at time 12 in the above trial
nEventsIA(x = x, tIA = 10)

# find time at which 1/4 of events are expected
tEventsIA(x = x, timing = .25)
</code></pre>

<hr>
<h2 id='print.nSurvival'>Time-to-event sample size calculation (Lachin-Foulkes)</h2><span id='topic+print.nSurvival'></span><span id='topic+nSurvival'></span><span id='topic+nEvents'></span><span id='topic+zn2hr'></span><span id='topic+hrn2z'></span><span id='topic+hrz2n'></span>

<h3>Description</h3>

<p><code>nSurvival()</code> is used to calculate the sample size for a clinical trial
with a time-to-event endpoint. The Lachin and Foulkes (1986) method is used.
<code>nEvents</code> uses the Schoenfeld (1981) approximation to provide sample
size and power in terms of the underlying hazard ratio and the number of
events observed in a survival analysis. The functions <code>hrz2n()</code>,
<code>hrn2z()</code> and <code>zn2hr()</code> also use the Schoenfeld approximation to
provide simple translations between hazard ratios, z-values and the number
of events in an analysis; input variables can be given as vectors.
</p>
<p><code>nSurvival()</code> produces an object of class &quot;nSurvival&quot; with the number
of subjects and events for a set of pre-specified trial parameters, such as
accrual duration and follow-up period. The calculation is based on Lachin
and Foulkes (1986) method and can be used for risk ratio or risk difference.
The function also consider non-uniform (exponential) entry as well as
uniform entry.
</p>
<p>If the logical <code>approx</code> is <code>TRUE</code>, the variance under alternative
hypothesis is used to replace the variance under null hypothesis.  For
non-uniform entry, a non-zero value of <code>gamma</code> for exponential entry
must be supplied. For positive <code>gamma</code>, the entry distribution is
convex, whereas for negative <code>gamma</code>, the entry distribution is
concave.
</p>
<p><code>nEvents()</code> uses the Schoenfeld (1981) method to approximate the number
of events <code>n</code> (given <code>beta</code>) or the power (given <code>n</code>).
Arguments may be vectors or scalars, but any vectors must have the same
length.
</p>
<p>The functions <code>hrz2n</code>, <code>hrn2z</code> and <code>zn2hr</code> also all apply the
Schoenfeld approximation for proportional hazards modeling.  This
approximation is based on the asymptotic normal distribtuion of the logrank
statistic as well as related statistics are asymptotically normal.  Let
<code class="reqn">\lambda</code> denote the underlying hazard ratio (<code>lambda1/lambda2</code> in
terms of the arguments to <code>nSurvival</code>). Further, let <code class="reqn">n</code> denote the
number of events observed when computing the statistic of interest and
<code class="reqn">r</code> the ratio of the sample size in an experimental group relative to a
control. The estimated natural logarithm of the hazard ratio from a
proportional hazards ratio is approximately normal with a mean of
<code class="reqn">log{\lambda}</code> and variance <code class="reqn">(1+r)^2/nr</code>. Let <code class="reqn">z</code> denote a
logrank statistic (or a Wald statistic or score statistic from a
proportional hazards regression model). The same asymptotic theory implies
<code class="reqn">z</code> is asymptotically equivalent to a normalized estimate of the hazard
ratio <code class="reqn">\lambda</code> and thus <code class="reqn">z</code> is asymptotically normal with variance
1 and mean </p>
<p style="text-align: center;"><code class="reqn">\frac{log{\lambda}r}{(1+r)^2}.</code>
</p>
<p> Plugging the estimated
hazard ratio into the above equation allows approximating any one of the
following based on the other two: the estimate hazard ratio, the number of
events and the z-statistic. That is, </p>
<p style="text-align: center;"><code class="reqn">\hat{\lambda}=
\exp(z(1+r)/\sqrt{rn})</code>
</p>
 <p style="text-align: center;"><code class="reqn">z=\log(\hat{\lambda})\sqrt{nr}/(1+r)</code>
</p>
 <p style="text-align: center;"><code class="reqn">n=
(z(1+r)/\log(\hat{\lambda}))^2/r.</code>
</p>

<p><code>hrz2n()</code> translates an observed interim hazard ratio and interim
z-value into the number of events required for the Z-value and hazard ratio
to correspond to each other. <code>hrn2z()</code> translates a hazard ratio and
number of events into an approximate corresponding Z-value. <code>zn2hr()</code>
translates a Z-value and number of events into an approximate corresponding
hazard ratio. Each of these functions has a default assumption of an
underlying hazard ratio of 1 which can be changed using the argument
<code>hr0</code>. <code>hrn2z()</code> and <code>zn2hr()</code> also have an argument
<code>hr1</code> which is only used to compute the sign of the computed Z-value in
the case of <code>hrn2z()</code> and whether or not a z-value &gt; 0 corresponds to a
hazard ratio &gt; or &lt; the null hazard ratio <code>hr0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nSurvival'
print(x, ...)

nSurvival(
  lambda1 = 1/12,
  lambda2 = 1/24,
  Ts = 24,
  Tr = 12,
  eta = 0,
  ratio = 1,
  alpha = 0.025,
  beta = 0.1,
  sided = 1,
  approx = FALSE,
  type = c("rr", "rd"),
  entry = c("unif", "expo"),
  gamma = NA
)

nEvents(
  hr = 0.6,
  alpha = 0.025,
  beta = 0.1,
  ratio = 1,
  sided = 1,
  hr0 = 1,
  n = 0,
  tbl = FALSE
)

zn2hr(z, n, ratio = 1, hr0 = 1, hr1 = 0.7)

hrn2z(hr, n, ratio = 1, hr0 = 1, hr1 = 0.7)

hrz2n(hr, z, ratio = 1, hr0 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nSurvival_+3A_x">x</code></td>
<td>
<p>An object of class &quot;nSurvival&quot; returned by <code>nSurvival()</code>
(optional: used for output; &quot;months&quot; or &quot;years&quot; would be the 'usual'
choices).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_...">...</code></td>
<td>
<p>Allows additional arguments for <code>print.nSurvival()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_lambda1">lambda1</code>, <code id="print.nSurvival_+3A_lambda2">lambda2</code></td>
<td>
<p>event hazard rate for placebo and treatment group
respectively.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_ts">Ts</code></td>
<td>
<p>maximum study duration.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_tr">Tr</code></td>
<td>
<p>accrual (recruitment) duration.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_eta">eta</code></td>
<td>
<p>equal dropout hazard rate for both groups.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_ratio">ratio</code></td>
<td>
<p>randomization ratio between placebo and treatment group.
Default is balanced design, i.e., randomization ratio is 1.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_alpha">alpha</code></td>
<td>
<p>type I error rate. Default is 0.025 since 1-sided testing is
default.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_beta">beta</code></td>
<td>
<p>type II error rate. Default is 0.10 (90% power). Not needed for
<code>nEvents()</code> if n is provided.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_sided">sided</code></td>
<td>
<p>one or two-sided test? Default is one-sided test.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_approx">approx</code></td>
<td>
<p>logical. If <code>TRUE</code>, the approximation sample size formula
for risk difference is used.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_type">type</code></td>
<td>
<p>type of sample size calculation: risk ratio (&ldquo;rr&rdquo;) or
risk difference (&ldquo;rd&rdquo;).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_entry">entry</code></td>
<td>
<p>patient entry type: uniform entry (<code>"unif"</code>) or
exponential entry (<code>"expo"</code>).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_gamma">gamma</code></td>
<td>
<p>rate parameter for exponential entry. <code>NA</code> if entry type
is <code>"unif"</code> (uniform). A non-zero value is supplied if entry type is
<code>"expo"</code> (exponential).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_hr">hr</code></td>
<td>
<p>Hazard ratio. For <code>nEvents</code>, this is the hazard ratio under
the alternative hypothesis (&gt;0).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_hr0">hr0</code></td>
<td>
<p>Hazard ratio under the null hypothesis (&gt;0, for <code>nEvents</code>,
<code>!= hr</code>).</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_n">n</code></td>
<td>
<p>Number of events. For <code>nEvents</code> may be input to compute power
rather than sample size.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_tbl">tbl</code></td>
<td>
<p>Indicator of whether or not scalar (vector) or tabular output is
desired for <code>nEvents()</code>.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_z">z</code></td>
<td>
<p>A z-statistic.</p>
</td></tr>
<tr><td><code id="print.nSurvival_+3A_hr1">hr1</code></td>
<td>
<p>Hazard ratio under the alternate hypothesis for <code>hrn2z,
zn2hr</code> (&gt;0, <code>!= hr0</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>nSurvival</code> produces a list with the following component
returned: </p>
<table>
<tr><td><code>type</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>entry</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>Sample
size required (computed).</p>
</td></tr> <tr><td><code>nEvents</code></td>
<td>
<p>Number of events required
(computed).</p>
</td></tr> <tr><td><code>lambda1</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>lambda2</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>ratio</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>gamma</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>sided</code></td>
<td>
<p>As input.</p>
</td></tr>
<tr><td><code>Ts</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>Tr</code></td>
<td>
<p>As input.</p>
</td></tr>
</table>
<p><code>nEvents</code> produces a scalar or vector of sample sizes (or powers) when
<code>tbl=FALSE</code> or, when <code>tbl=TRUE</code> a data frame of values with the
following columns: </p>
<table>
<tr><td><code>hr</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>If <code>n[1]=0</code> on input
(default), output contains the number of events need to obtain the input
Type I and II error. If <code>n[1]&gt;0</code> on input, the input value is
returned.</p>
</td></tr> <tr><td><code>alpha</code></td>
<td>
<p>As input.</p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>If <code>n[1]=0</code> on input
(default), <code>beta</code> is output as input. Otherwise, this is the computed
Type II error based on the input <code>n</code>.</p>
</td></tr> <tr><td><code>Power</code></td>
<td>
<p>One minus the
output <code>beta</code>. When <code>tbl=FALSE, n[1]&gt;0</code>, this is the value or
vector of values returned.</p>
</td></tr> <tr><td><code>delta</code></td>
<td>
<p>Standardized effect size
represented by input difference between null and alternative hypothesis
hazard ratios.</p>
</td></tr> <tr><td><code>ratio</code></td>
<td>
<p>Ratio of experimental to control sample size
where 'experimental' is the same as the group with hazard represented in the
numerator of the hazard ratio.</p>
</td></tr> <tr><td><code>se</code></td>
<td>
<p>Estimated standard error for the
observed log(hazard ratio) with the given sample size.</p>
</td></tr>
</table>
<p><code>hrz2n</code> outputs a number of events required to approximately have the
input hazard ratio, z-statistic and sample size correspond. <code>hrn2z</code>
outputs an approximate z-statistic corresponding to an input hazard ratio
and number of events. <code>zn2hr</code> outputs an approximate hazard ratio
corresponding to an input z-statistic and number of events.
</p>


<h3>Author(s)</h3>

<p>Shanhong Guan <a href="mailto:shanhong.guan@gmail.com">shanhong.guan@gmail.com</a>, Keaven Anderson
<a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Lachin JM and Foulkes MA (1986), Evaluation of Sample Size and
Power for Analyses of Survival with Allowance for Nonuniform Patient Entry,
Losses to Follow-Up, Noncompliance, and Stratification. <em>Biometrics</em>,
42, 507-519.
</p>
<p>Schoenfeld D (1981), The Asymptotic Properties of Nonparametric Tests for
Comparing Survival Distributions. <em>Biometrika</em>, 68, 316-319.
</p>


<h3>See Also</h3>

<p><code>vignette("gsDesignPackageOverview")</code>, <a href="#topic+plot.gsDesign">plot.gsDesign</a>,
<a href="#topic+gsDesign">gsDesign</a>, <a href="#topic+gsHR">gsHR</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)

# consider a trial with
# 2 year maximum follow-up
# 6 month uniform enrollment
# Treatment/placebo hazards = 0.1/0.2 per 1 person-year
# drop out hazard 0.1 per 1 person-year
# alpha = 0.025 (1-sided)
# power = 0.9 (default beta=.1)

ss &lt;- nSurvival(
  lambda1 = .2, lambda2 = .1, eta = .1, Ts = 2, Tr = .5,
  sided = 1, alpha = .025
)

#  group sequential translation with default bounds
#  note that delta1 is log hazard ratio; used later in gsBoundSummary summary
x &lt;- gsDesign(
  k = 5, test.type = 2, n.fix = ss$nEvents, nFixSurv = ss$n,
  delta1 = log(ss$lambda2 / ss$lambda1)
)
# boundary plot
plot(x)
# effect size plot
plot(x, plottype = "hr")
# total sample size
x$nSurv
# number of events at analyses
x$n.I
# print the design
x
# overall design summary
cat(summary(x))
# tabular summary of bounds
gsBoundSummary(x, deltaname = "HR", Nname = "Events", logdelta = TRUE)



# approximate number of events required using Schoenfeld's method
# for 2 different hazard ratios
nEvents(hr = c(.5, .6), tbl = TRUE)
# vector output
nEvents(hr = c(.5, .6))

# approximate power using Schoenfeld's method
# given 2 sample sizes and hr=.6
nEvents(hr = .6, n = c(50, 100), tbl = TRUE)
# vector output
nEvents(hr = .6, n = c(50, 100))

# approximate hazard ratio corresponding to 100 events and z-statistic of 2
zn2hr(n = 100, z = 2)
# same when hr0 is 1.1
zn2hr(n = 100, z = 2, hr0 = 1.1)
# same when hr0 is .9 and hr1 is greater than hr0
zn2hr(n = 100, z = 2, hr0 = .9, hr1 = 1)

# approximate number of events corresponding to z-statistic of 2 and
# estimated hazard ratio of .5 (or 2)
hrz2n(hr = .5, z = 2)
hrz2n(hr = 2, z = 2)

# approximate z statistic corresponding to 75 events
# and estimated hazard ratio of .6 (or 1/.6)
# assuming 2-to-1 randomization of experimental to control
hrn2z(hr = .6, n = 75, ratio = 2)
hrn2z(hr = 1 / .6, n = 75, ratio = 2)
</code></pre>

<hr>
<h2 id='sequentialPValue'>Sequential p-value computation</h2><span id='topic+sequentialPValue'></span>

<h3>Description</h3>

<p><code>sequentialPValue</code> computes a sequential p-value for a group sequential design using a spending function as described in 
Maurer and Bretz (2013) and previously defined by Liu and Anderson (2008).
It is the minimum of repeated p-values computed at each analysis (Jennison and Turnbull, 2000).
This is particularly useful for multiplicity methods such as the graphical method for group sequential designs 
where sequential p-values for multiple hypotheses can be used as nominal p-values to plug into a multiplicity graph. 
A sequential p-value is described as the minimum alpha level at which a one-sided group sequential bound would 
be rejected given interim and final observed results.
It is meaningful for both one-sided designs and designs with non-binding futility bounds (<code>test.type</code> 1, 4, 6), 
but not for 2-sided designs with binding futility bounds (<code>test.type</code> 2, 3 or 5).
Mild restrictions are required on spending functions used, but these are satisfied for commonly used spending functions
such as the Lan-DeMets spending function approximating an O'Brien-Fleming bound or a Hwang-Shih-DeCani spending function; see Maurer and Bretz (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialPValue(
  gsD = gsDesign(),
  n.I = NULL,
  Z = NULL,
  usTime = NULL,
  interval = c(1e-05, 0.9999)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequentialPValue_+3A_gsd">gsD</code></td>
<td>
<p>Group sequential design generated by <code>gsDesign</code> or <code>gsSurv</code>.</p>
</td></tr>
<tr><td><code id="sequentialPValue_+3A_n.i">n.I</code></td>
<td>
<p>Event counts (for time-to-event outcomes) or sample size (for most other designs); numeric vector with increasing, positive values with at most one
value greater than or equal to largest value in <code>gsD$n.I</code>; NOTE: if NULL, planned n.I will be used (<code>gsD$n.I</code>).</p>
</td></tr>
<tr><td><code id="sequentialPValue_+3A_z">Z</code></td>
<td>
<p>Z-value tests corresponding to analyses in <code>n.I</code>; positive values indicate a positive finding; must have the same length as <code>n.I</code>.</p>
</td></tr>
<tr><td><code id="sequentialPValue_+3A_ustime">usTime</code></td>
<td>
<p>Spending time for upper bound at specified analyses; specify default: <code>NULL</code> if this is to be based on information fraction; 
if not <code>NULL</code>, must have the same length as <code>n.I</code>; increasing positive values with at most 1 greater than or equal to 1.</p>
</td></tr>
<tr><td><code id="sequentialPValue_+3A_interval">interval</code></td>
<td>
<p>Interval for search to derive p-value; Default: <code>c(1e-05, 0.9999)</code>. Lower end of interval must be &gt;0 and upper end must be &lt; 1. 
The primary reason to not use the defaults would likely be if a test were vs a Type I error &lt;0.0001.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solution is found with a search using <code>uniroot</code>.
This finds the maximum alpha-level for which an efficacy bound is crossed, 
completely ignoring any futility bound.
</p>


<h3>Value</h3>

<p>Sequential p-value (single numeric one-sided p-value between 0 and 1). 
Note that if the sequential p-value is less than the lower end of the input interval, 
the lower of interval will be returned.
Similarly, if the sequential p-value is greater than the upper end of the input interval, 
then the upper end of interval is returned.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Liu, Qing, and Keaven M. Anderson. &quot;On adaptive extensions of group sequential trials for clinical investigations.&quot; <em>Journal of the American Statistical Association</em> 103.484 (2008): 1621-1630.
</p>
<p>Maurer, Willi, and Frank Bretz. &quot;Multiple testing in group sequential trials using graphical approaches.&quot; <em>Statistics in Biopharmaceutical Research</em> 5.4 (2013): 311-320.;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Derive Group Sequential Design 
x &lt;- gsSurv(k = 4, alpha = 0.025, beta = 0.1, timing = c(.5,.65,.8), sfu = sfLDOF,
            sfl = sfHSD, sflpar = 2, lambdaC = log(2)/6, hr = 0.6,
            eta = 0.01 , gamma = c(2.5,5,7.5,10), R = c( 2,2,2,6 ),
            T = 30 , minfup = 18)
x$n.I
# Analysis at IA2
sequentialPValue(gsD=x,n.I=c(100,160),Z=c(1.5,2))
# Use planned spending instead of information fraction; do final analysis
sequentialPValue(gsD=x,n.I=c(100,160,190,230),Z=c(1.5,2,2.5,3),usTime=x$timing)
# Check bounds for updated design to verify at least one was crossed
xupdate &lt;- gsDesign(maxn.IPlan=max(x$n.I),n.I=c(100,160,190,230),usTime=x$timing,
                    delta=x$delta,delta1=x$delta1,k=4,alpha=x$alpha,test.type=1,
                    sfu=x$upper$sf,sfupar=x$upper$param)
</code></pre>

<hr>
<h2 id='sfExponential'>Exponential Spending Function</h2><span id='topic+sfExponential'></span>

<h3>Description</h3>

<p>The function <code>sfExponential</code> implements the exponential spending
function (Anderson and Clark, 2009). Normally <code>sfExponential</code> will be
passed to <code>gsDesign</code> in the parameter <code>sfu</code> for the upper bound or
<code>sfl</code> for the lower bound to specify a spending function family for a
design. In this case, the user does not need to know the calling sequence.
The calling sequence is useful, however, when the user wishes to plot a
spending function as demonstrated below in examples.
</p>
<p>An exponential spending function is defined for any positive <code>nu</code> and
<code class="reqn">0\le t\le 1</code> as
</p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,\nu)=\alpha(t)=\alpha^{t^{-\nu}}.</code>
</p>

<p>A value of <code>nu=0.8</code> approximates an O'Brien-Fleming spending function
well.
</p>
<p>The general class of spending functions this family is derived from requires
a continuously increasing cumulative distribution function defined for
<code class="reqn">x&gt;0</code> and is defined as </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,
\nu)=1-F\left(F^{-1}(1-\alpha)/ t^\nu\right).</code>
</p>
<p> The exponential spending function can be
derived by letting <code class="reqn">F(x)=1-\exp(-x)</code>, the exponential cumulative
distribution function. This function was derived as a generalization of the
Lan-DeMets (1983) spending function used to approximate an O'Brien-Fleming
spending function (<code>sfLDOF()</code>), </p>
<p style="text-align: center;"><code class="reqn">f(t; \alpha)=2-2\Phi \left(
\Phi^{-1}(1-\alpha/2)/ t^{1/2} \right).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sfExponential(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfExponential_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfExponential_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfExponential_+3A_param">param</code></td>
<td>
<p>A single positive value specifying the nu parameter for which
the exponential spending is to be computed; allowable range is (0, 1.5].</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual shows how to use <code>sfExponential()</code>
to closely approximate an O'Brien-Fleming design. An example is given below.
The manual is available at &lt;https://keaven.github.io/gsd-tech-manual/&gt;.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Anderson KM and Clark JB (2009), Fitting spending functions.
<em>Statistics in Medicine</em>; 29:321-327.
</p>
<p>Jennison C and Turnbull BW (2000), <em>Group Sequential Methods with
Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Lan, KKG and DeMets, DL (1983), Discrete sequential boundaries for clinical
trials. <em>Biometrika</em>; 70:659-663.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# use 'best' exponential approximation for k=6 to O'Brien-Fleming design
# (see manual for details)
gsDesign(
  k = 6, sfu = sfExponential, sfupar = 0.7849295,
  test.type = 2
)$upper$bound

# show actual O'Brien-Fleming bound
gsDesign(k = 6, sfu = "OF", test.type = 2)$upper$bound

# show Lan-DeMets approximation
# (not as close as sfExponential approximation)
gsDesign(k = 6, sfu = sfLDOF, test.type = 2)$upper$bound

# plot exponential spending function across a range of values of interest
t &lt;- 0:100 / 100
plot(t, sfExponential(0.025, t, 0.8)$spend,
  xlab = "Proportion of final sample size",
  ylab = "Cumulative Type I error spending",
  main = "Exponential Spending Function Example", type = "l"
)
lines(t, sfExponential(0.025, t, 0.5)$spend, lty = 2)
lines(t, sfExponential(0.025, t, 0.3)$spend, lty = 3)
lines(t, sfExponential(0.025, t, 0.2)$spend, lty = 4)
lines(t, sfExponential(0.025, t, 0.15)$spend, lty = 5)
legend(
  x = c(.0, .3), y = .025 * c(.7, 1), lty = 1:5,
  legend = c(
    "nu = 0.8", "nu = 0.5", "nu = 0.3", "nu = 0.2",
    "nu = 0.15"
  )
)
text(x = .59, y = .95 * .025, labels = "&lt;--approximates O'Brien-Fleming")
</code></pre>

<hr>
<h2 id='sfHSD'>Hwang-Shih-DeCani Spending Function</h2><span id='topic+sfHSD'></span>

<h3>Description</h3>

<p>The function <code>sfHSD</code> implements a Hwang-Shih-DeCani spending function.
This is the default spending function for <code>gsDesign()</code>. Normally it
will be passed to <code>gsDesign</code> in the parameter <code>sfu</code> for the upper
bound or <code>sfl</code> for the lower bound to specify a spending function
family for a design. In this case, the user does not need to know the
calling sequence. The calling sequence is useful, however, when the user
wishes to plot a spending function as demonstrated below in examples.
</p>
<p>A Hwang-Shih-DeCani spending function takes the form </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,
\gamma)=\alpha(1-e^{-\gamma t})/(1-e^{-\gamma})</code>
</p>
<p> where <code class="reqn">\gamma</code> is the
value passed in <code>param</code>. A value of <code class="reqn">\gamma=-4</code> is used
to approximate an O'Brien-Fleming design (see <code><a href="#topic+sfExponential">sfExponential</a></code>
for a better fit), while a value of <code class="reqn">\gamma=1</code> approximates a
Pocock design well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfHSD(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfHSD_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfHSD_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfHSD_+3A_param">param</code></td>
<td>
<p>A single real value specifying the gamma parameter for which
Hwang-Shih-DeCani spending is to be computed; allowable range is [-40, 40]</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See <code>vignette("SpendingFunctionOverview")</code> for further details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# design a 4-analysis trial using a Hwang-Shih-DeCani spending function
# for both lower and upper bounds
x &lt;- gsDesign(k = 4, sfu = sfHSD, sfupar = -2, sfl = sfHSD, sflpar = 1)

# print the design
x

# since sfHSD is the default for both sfu and sfl,
# this could have been written as
x &lt;- gsDesign(k = 4, sfupar = -2, sflpar = 1)

# print again
x

# plot the spending function using many points to obtain a smooth curve
# show default values of gamma to see how the spending function changes
# also show gamma=1 which is supposed to approximate a Pocock design
t &lt;- 0:100 / 100
plot(t, sfHSD(0.025, t, -4)$spend,
  xlab = "Proportion of final sample size",
  ylab = "Cumulative Type I error spending",
  main = "Hwang-Shih-DeCani Spending Function Example", type = "l"
)
lines(t, sfHSD(0.025, t, -2)$spend, lty = 2)
lines(t, sfHSD(0.025, t, 1)$spend, lty = 3)
legend(
  x = c(.0, .375), y = .025 * c(.8, 1), lty = 1:3,
  legend = c("gamma= -4", "gamma= -2", "gamma= 1")
)
</code></pre>

<hr>
<h2 id='sfLDOF'>Lan-DeMets Spending function overview</h2><span id='topic+sfLDOF'></span><span id='topic+sfLDPocock'></span>

<h3>Description</h3>

<p>Lan and DeMets (1983) first published the method of using spending functions 
to set boundaries for group sequential trials. In this publication they
proposed two specific spending functions: one to approximate an
O'Brien-Fleming design and the other to approximate a Pocock design.
The spending function to approximate O'Brien-Fleming has been generalized as proposed by Liu, et al (2012)
</p>
<p>With <code>param=1=rho</code>, the Lan-DeMets (1983) spending function to approximate an O'Brien-Fleming
bound is implemented in the function (<code>sfLDOF()</code>): </p>
<p style="text-align: center;"><code class="reqn">f(t; 
\alpha)=2-2\Phi\left(\Phi^{-1}(1-\alpha/2)/ t^{\rho/2}\right).</code>
</p>

<p>For <code>rho</code> otherwise in <code>[.005,2]</code>, this is the generalized version of Liu et al (2012).
For <code>param</code> outside of <code>[.005,2]</code>, <code>rho</code> is set to 1. The Lan-DeMets (1983)
spending function to approximate a Pocock design is implemented in the
function <code>sfLDPocock()</code>:
</p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha)=\alpha ln(1+(e-1)t).</code>
</p>
<p> As shown in
examples below, other spending functions can be used to ge t as good or
better approximations to Pocock and O'Brien-Fleming bounds. In particular,
O'Brien-Fleming bounds can be closely approximated using
<code><a href="#topic+sfExponential">sfExponential</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfLDOF(alpha, t, param = NULL)

sfLDPocock(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfLDOF_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfLDOF_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfLDOF_+3A_param">param</code></td>
<td>
<p>This parameter is not used for <code>sfLDPocock</code>, not required 
for <code>sfLDOF</code> and need not be specified. For <code>sfLDPocock</code> it is here 
so that the calling sequence conforms to the standard for spending functions used 
with <code>gsDesign()</code>. For <code>sfLDOF</code> it will default to 1 (Lan-DeMets function 
to approximate O'Brien-Fleming) if <code>NULL</code> or if outside of the range <code>[.005,2]</code>.
otherwise, it will be use to set rho from Liu et al (2012).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See spending functions for further
details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Lan, KKG and DeMets, DL (1983), Discrete sequential boundaries for clinical
trials. <em>Biometrika</em>;70: 659-663.
</p>
<p>Liu, Q, Lim, P, Nuamah, I, and Li, Y (2012), On adaptive error spending approach 
for group sequential trials with random information levels. 
<em>Journal of biopharmaceutical statistics</em>; 22(4), 687-699.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# 2-sided,  symmetric 6-analysis trial Pocock
# spending function approximation
gsDesign(k = 6, sfu = sfLDPocock, test.type = 2)$upper$bound

# show actual Pocock design
gsDesign(k = 6, sfu = "Pocock", test.type = 2)$upper$bound

# approximate Pocock again using a standard
# Hwang-Shih-DeCani approximation
gsDesign(k = 6, sfu = sfHSD, sfupar = 1, test.type = 2)$upper$bound

# use 'best' Hwang-Shih-DeCani approximation for Pocock,  k=6;
# see manual for details
gsDesign(k = 6, sfu = sfHSD, sfupar = 1.3354376, test.type = 2)$upper$bound

# 2-sided, symmetric 6-analysis trial
# O'Brien-Fleming spending function approximation
gsDesign(k = 6, sfu = sfLDOF, test.type = 2)$upper$bound

# show actual O'Brien-Fleming bound
gsDesign(k = 6, sfu = "OF", test.type = 2)$upper$bound

# approximate again using a standard Hwang-Shih-DeCani
# approximation to O'Brien-Fleming
x &lt;- gsDesign(k = 6, test.type = 2)
x$upper$bound
x$upper$param

# use 'best' exponential approximation for k=6; see manual for details
gsDesign(
  k = 6, sfu = sfExponential, sfupar = 0.7849295,
  test.type = 2
)$upper$bound

# plot spending functions for generalized Lan-DeMets approximation of
ti &lt;-(0:100)/100
rho &lt;- c(.05,.5,1,1.5,2,2.5,3:6,8,10,12.5,15,20,30,200)/10
df &lt;- NULL
for(r in rho){
  df &lt;- rbind(df,data.frame(t=ti,rho=r,alpha=.025,spend=sfLDOF(alpha=.025,t=ti,param=r)$spend))
}
ggplot(df,aes(x=t,y=spend,col=as.factor(rho)))+
  geom_line()+
  guides(col=guide_legend(expression(rho)))+
  ggtitle("Generalized Lan-DeMets O'Brien-Fleming Spending Function")
</code></pre>

<hr>
<h2 id='sfLinear'>Piecewise Linear and Step Function Spending Functions</h2><span id='topic+sfLinear'></span><span id='topic+sfStep'></span>

<h3>Description</h3>

<p>The function <code>sfLinear()</code> allows specification of a piecewise linear
spending function. The function <code>sfStep()</code> specifies a step function
spending function. Both functions provide complete flexibility in setting
spending at desired timepoints in a group sequential design. Normally these
function will be passed to <code>gsDesign()</code> in the parameter <code>sfu</code> for
the upper bound or <code>sfl</code> for the lower bound to specify a spending
function family for a design. When passed to <code>gsDesign()</code>, the value of
<code>param</code> would be passed to <code>sfLinear()</code> or <code>sfStep()</code> through
the <code>gsDesign()</code> arguments <code>sfupar</code> for the upper bound and
<code>sflpar</code> for the lower bound.
</p>
<p>Note that <code>sfStep()</code> allows setting a particular level of spending when
the timing is not strictly known; an example shows how this can inflate Type
I error when timing of analyses are changed based on knowing the treatment
effect at an interim.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfLinear(alpha, t, param)

sfStep(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfLinear_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size or information.</p>
</td></tr>
<tr><td><code id="sfLinear_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size or information for which the
spending function will be computed.</p>
</td></tr>
<tr><td><code id="sfLinear_+3A_param">param</code></td>
<td>
<p>A vector with a positive, even length. Values must range from 0
to 1, inclusive. Letting <code>m &lt;- length(param/2)</code>, the first <code>m</code>
points in param specify increasing values strictly between 0 and 1
corresponding to interim timing (proportion of final total statistical
information). The last <code>m</code> points in <code>param</code> specify
non-decreasing values from 0 to 1, inclusive, with the cumulative proportion
of spending at the specified timepoints.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>.  The cumulative spending returned
in <code>sfLinear$spend</code> is 0 for <code>t &lt;= 0</code> and <code>alpha</code> for
<code>t&gt;=1</code>.  For <code>t</code> between specified points, linear interpolation is
used to determine <code>sfLinear$spend</code>.
</p>
<p>The cumulative spending returned in <code>sfStep$spend</code> is 0 for
<code>t&lt;param[1]</code> and <code>alpha</code> for <code>t&gt;=1</code>.  Letting <code>m &lt;-
length(param/2)</code>, for <code>i=1,2,...m-1</code> and <code> param[i]&lt;= t &lt;
param[i+1]</code>, the cumulative spending is set at <code>alpha * param[i+m]</code>
(also for <code>param[m]&lt;=t&lt;1</code>).
</p>
<p>Note that if <code>param[2m]</code> is 1, then the first time an analysis is
performed after the last proportion of final planned information
(<code>param[m]</code>) will be the final analysis, using any remaining error that
was not previously spent.
</p>
<p>See <code>vignette("SpendingFunctionOverview")</code> for further details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# set up alpha spending and beta spending to be piecewise linear
sfupar &lt;- c(.2, .4, .05, .2)
sflpar &lt;- c(.3, .5, .65, .5, .75, .9)
x &lt;- gsDesign(sfu = sfLinear, sfl = sfLinear, sfupar = sfupar, sflpar = sflpar)
plot(x, plottype = "sf")
x

# now do an example where there is no lower-spending at interim 1
# and no upper spending at interim 2
sflpar &lt;- c(1 / 3, 2 / 3, 0, .25)
sfupar &lt;- c(1 / 3, 2 / 3, .1, .1)
x &lt;- gsDesign(sfu = sfLinear, sfl = sfLinear, sfupar = sfupar, sflpar = sflpar)
plot(x, plottype = "sf")
x

# now do an example where timing of interims changes slightly, but error spending does not
# also, spend all alpha when at least &gt;=90 percent of final information is in the analysis
sfupar &lt;- c(.2, .4, .9, ((1:3) / 3)^3)
x &lt;- gsDesign(k = 3, n.fix = 100, sfu = sfStep, sfupar = sfupar, test.type = 1)
plot(x, pl = "sf")
# original planned sample sizes
ceiling(x$n.I)
# cumulative spending planned at original interims
cumsum(x$upper$spend)
# change timing of analyses;
# note that cumulative spending "P(Cross) if delta=0" does not change from cumsum(x$upper$spend)
# while full alpha is spent, power is reduced by reduced sample size
y &lt;- gsDesign(
  k = 3, sfu = sfStep, sfupar = sfupar, test.type = 1,
  maxn.IPlan = x$n.I[x$k], n.I = c(30, 70, 95),
  n.fix = x$n.fix
)
# note that full alpha is used, but power is reduced due to lowered sample size
gsBoundSummary(y)

# now show how step function can be abused by 'adapting' stage 2 sample size based on interim result
x &lt;- gsDesign(k = 2, delta = .05, sfu = sfStep, sfupar = c(.02, .001), timing = .02, test.type = 1)
# spending jumps from miniscule to full alpha at first analysis after interim 1
plot(x, pl = "sf")
# sample sizes at analyses:
ceiling(x$n.I)
# simulate 1 million stage 1 sum of 178 Normal(0,1) random variables
# Normal(0,Variance=178) under null hypothesis
s1 &lt;- rnorm(1000000, 0, sqrt(178))
# compute corresponding z-values
z1 &lt;- s1 / sqrt(178)
# set stage 2 sample size to 1 if z1 is over final bound, otherwise full sample size
n2 &lt;- rep(1, 1000000)
n2[z1 &lt; 1.96] &lt;- ceiling(x$n.I[2]) - ceiling(178)
# now sample n2 observations for second stage
s2 &lt;- rnorm(1000000, 0, sqrt(n2))
# add sum and divide by standard deviation
z2 &lt;- (s1 + s2) / (sqrt(178 + n2))
# By allowing full spending when final analysis is either
# early or late depending on observed interim z1,
# Type I error is now almost twice the planned .025
sum(z1 &gt;= x$upper$bound[1] | z2 &gt;= x$upper$bound[2]) / 1000000
# if stage 2 sample size is random and independent of z1 with same frequency,
# this is not a problem
s1alt &lt;- rnorm(1000000, 0, sqrt(178))
z1alt &lt;- s1alt / sqrt(178)
z2alt &lt;- (s1alt + s2) / sqrt(178 + n2)
sum(z1alt &gt;= x$upper$bound[1] | z2alt &gt;= x$upper$bound[2]) / 1000000

</code></pre>

<hr>
<h2 id='sfLogistic'>Two-parameter Spending Function Families</h2><span id='topic+sfLogistic'></span><span id='topic+sfBetaDist'></span><span id='topic+sfCauchy'></span><span id='topic+sfExtremeValue'></span><span id='topic+sfExtremeValue2'></span><span id='topic+sfNormal'></span>

<h3>Description</h3>

<p>The functions <code>sfLogistic()</code>, <code>sfNormal()</code>,
<code>sfExtremeValue()</code>, <code>sfExtremeValue2()</code>, <code>sfCauchy()</code>, and
<code>sfBetaDist()</code> are all 2-parameter spending function families. These
provide increased flexibility in some situations where the flexibility of a
one-parameter spending function family is not sufficient. These functions
all allow fitting of two points on a cumulative spending function curve; in
this case, four parameters are specified indicating an x and a y coordinate
for each of 2 points. Normally each of these functions will be passed to
<code>gsDesign()</code> in the parameter <code>sfu</code> for the upper bound or
<code>sfl</code> for the lower bound to specify a spending function family for a
design. In this case, the user does not need to know the calling sequence.
The calling sequence is useful, however, when the user wishes to plot a
spending function as demonstrated in the examples; note, however, that an
automatic <code class="reqn">\alpha</code>- and <code class="reqn">\beta</code>-spending function plot
is also available.
</p>
<p><code>sfBetaDist(alpha,t,param)</code> is simply <code>alpha</code> times the incomplete
beta cumulative distribution function with parameters <code class="reqn">a</code> and <code class="reqn">b</code>
passed in <code>param</code> evaluated at values passed in <code>t</code>.
</p>
<p>The other spending functions take the form </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,a,b)=\alpha
F(a+bF^{-1}(t))</code>
</p>
<p> where <code class="reqn">F()</code> is a
cumulative distribution function with values <code class="reqn">&gt; 0</code> on the real line
(logistic for <code>sfLogistic()</code>, normal for <code>sfNormal()</code>, extreme
value for <code>sfExtremeValue()</code> and Cauchy for <code>sfCauchy()</code>) and
<code class="reqn">F^{-1}()</code> is its inverse.
</p>
<p>For the logistic spending function this simplifies to
</p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,a,b)=\alpha (1-(1+e^a(t/(1-t))^b)^{-1}).</code>
</p>

<p>For the extreme value distribution with </p>
<p style="text-align: center;"><code class="reqn">F(x)=\exp(-\exp(-x))</code>
</p>
<p> this
simplifies to </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,a,b)=\alpha \exp(-e^a (-\ln t)^b).</code>
</p>
<p> Since the
extreme value distribution is not symmetric, there is also a version where
the standard distribution is flipped about 0. This is reflected in
<code>sfExtremeValue2()</code> where </p>
<p style="text-align: center;"><code class="reqn">F(x)=1-\exp(-\exp(x)).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sfLogistic(alpha, t, param)

sfBetaDist(alpha, t, param)

sfCauchy(alpha, t, param)

sfExtremeValue(alpha, t, param)

sfExtremeValue2(alpha, t, param)

sfNormal(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfLogistic_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size or information.</p>
</td></tr>
<tr><td><code id="sfLogistic_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size or information for which the
spending function will be computed.</p>
</td></tr>
<tr><td><code id="sfLogistic_+3A_param">param</code></td>
<td>
<p>In the two-parameter specification, <code>sfBetaDist()</code>
requires 2 positive values, while <code>sfLogistic()</code>, <code>sfNormal()</code>,
<code>sfExtremeValue()</code>,
</p>
<p><code>sfExtremeValue2()</code> and <code>sfCauchy()</code> require the first parameter
to be any real value and the second to be a positive value.  The four
parameter specification is <code>c(t1,t2,u1,u2)</code> where the objective is that
<code>sf(t1)=alpha*u1</code> and <code>sf(t2)=alpha*u2</code>.  In this
parameterization, all four values must be between 0 and 1 and <code>t1 &lt;
t2</code>, <code>u1 &lt; u2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. 
See <code>vignette("SpendingFunctionOverview")</code> for further details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# design a 4-analysis trial using a Kim-DeMets spending function
# for both lower and upper bounds
x &lt;- gsDesign(k = 4, sfu = sfPower, sfupar = 3, sfl = sfPower, sflpar = 1.5)

# print the design
x

# plot the alpha- and beta-spending functions
plot(x, plottype = 5)

# start by showing how to fit two points with sfLogistic
# plot the spending function using many points to obtain a smooth curve
# note that curve fits the points x=.1,  y=.01 and x=.4,  y=.1
# specified in the 3rd parameter of sfLogistic
t &lt;- 0:100 / 100
plot(t, sfLogistic(1, t, c(.1, .4, .01, .1))$spend,
  xlab = "Proportion of final sample size",
  ylab = "Cumulative Type I error spending",
  main = "Logistic Spending Function Examples",
  type = "l", cex.main = .9
)
lines(t, sfLogistic(1, t, c(.01, .1, .1, .4))$spend, lty = 2)

# now just give a=0 and b=1 as 3rd parameters for sfLogistic
lines(t, sfLogistic(1, t, c(0, 1))$spend, lty = 3)

# try a couple with unconventional shapes again using
# the xy form in the 3rd parameter
lines(t, sfLogistic(1, t, c(.4, .6, .1, .7))$spend, lty = 4)
lines(t, sfLogistic(1, t, c(.1, .7, .4, .6))$spend, lty = 5)
legend(
  x = c(.0, .475), y = c(.76, 1.03), lty = 1:5,
  legend = c(
    "Fit (.1, 01) and (.4, .1)", "Fit (.01, .1) and (.1, .4)",
    "a=0,  b=1", "Fit (.4, .1) and (.6, .7)",
    "Fit (.1, .4) and (.7, .6)"
  )
)

# set up a function to plot comparsons of all
# 2-parameter spending functions
plotsf &lt;- function(alpha, t, param) {
  plot(t, sfCauchy(alpha, t, param)$spend,
    xlab = "Proportion of enrollment",
    ylab = "Cumulative spending", type = "l", lty = 2
  )
  lines(t, sfExtremeValue(alpha, t, param)$spend, lty = 5)
  lines(t, sfLogistic(alpha, t, param)$spend, lty = 1)
  lines(t, sfNormal(alpha, t, param)$spend, lty = 3)
  lines(t, sfExtremeValue2(alpha, t, param)$spend, lty = 6, col = 2)
  lines(t, sfBetaDist(alpha, t, param)$spend, lty = 7, col = 3)
  legend(
    x = c(.05, .475), y = .025 * c(.55, .9),
    lty = c(1, 2, 3, 5, 6, 7),
    col = c(1, 1, 1, 1, 2, 3),
    legend = c(
      "Logistic", "Cauchy", "Normal", "Extreme value",
      "Extreme value 2", "Beta distribution"
    )
  )
}
# do comparison for a design with conservative early spending
# note that Cauchy spending function is quite different
# from the others
param &lt;- c(.25, .5, .05, .1)
plotsf(.025, t, param)
</code></pre>

<hr>
<h2 id='sfPoints'>Pointwise Spending Function</h2><span id='topic+sfPoints'></span>

<h3>Description</h3>

<p>The function <code>sfPoints</code> implements a spending function with values
specified for an arbitrary set of specified points. It is now recommended to
use sfLinear rather than sfPoints. Normally <code>sfPoints</code> will be passed
to <code>gsDesign</code> in the parameter <code>sfu</code> for the upper bound or
<code>sfl</code> for the lower bound to specify a spending function family for a
design. In this case, the user does not need to know the calling sequence,
just the points they wish to specify. If using <code>sfPoints()</code> in a
design, it is recommended to specify how to interpolate between the
specified points (e.g,, linear interpolation); also consider fitting smooth
spending functions; see <code>vignette("SpendingFunctionOverview")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfPoints(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfPoints_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfPoints_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from &gt;0 and &lt;=1.  Values
of the proportion of sample size/information for which the spending function
will be computed.</p>
</td></tr>
<tr><td><code id="sfPoints_+3A_param">param</code></td>
<td>
<p>A vector of the same length as <code>t</code> specifying the
cumulative proportion of spending to corresponding to each point in
<code>t</code>; must be &gt;=0 and &lt;=1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See spending functions for further
details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>, <a href="#topic+sfLogistic">sfLogistic</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# example to specify spending on a pointwise basis
x &lt;- gsDesign(
  k = 6, sfu = sfPoints, sfupar = c(.01, .05, .1, .25, .5, 1),
  test.type = 2
)
x

# get proportion of upper spending under null hypothesis
# at each analysis
y &lt;- x$upper$prob[, 1] / .025

# change to cumulative proportion of spending
for (i in 2:length(y))
  y[i] &lt;- y[i - 1] + y[i]

# this should correspond to input sfupar
round(y, 6)

# plot these cumulative spending points
plot(1:6 / 6, y,
  main = "Pointwise spending function example",
  xlab = "Proportion of final sample size",
  ylab = "Cumulative proportion of spending",
  type = "p"
)

# approximate this with a t-distribution spending function
# by fitting 3 points
tx &lt;- 0:100 / 100
lines(tx, sfTDist(1, tx, c(c(1, 3, 5) / 6, .01, .1, .5))$spend)
text(x = .6, y = .9, labels = "Pointwise Spending Approximated by")
text(x = .6, y = .83, "t-Distribution Spending with 3-point interpolation")

# example without lower spending at initial interim or
# upper spending at last interim
x &lt;- gsDesign(
  k = 3, sfu = sfPoints, sfupar = c(.25, .25),
  sfl = sfPoints, sflpar = c(0, .25)
)
x

</code></pre>

<hr>
<h2 id='sfPower'>Kim-DeMets (power) Spending Function</h2><span id='topic+sfPower'></span>

<h3>Description</h3>

<p>The function <code>sfPower()</code> implements a Kim-DeMets (power) spending
function. This is a flexible, one-parameter spending function recommended by
Jennison and Turnbull (2000). Normally it will be passed to
<code>gsDesign()</code> in the parameter <code>sfu</code> for the upper bound or
<code>sfl</code> for the lower bound to specify a spending function family for a
design. In this case, the user does not need to know the calling sequence.
The calling sequence is useful, however, when the user wishes to plot a
spending function as demonstrated below in examples.
</p>
<p>A Kim-DeMets spending function takes the form </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha,\rho)=\alpha
t^\rho</code>
</p>
<p> where <code class="reqn">\rho</code> is the value
passed in <code>param</code>. See examples below for a range of values of
<code class="reqn">\rho</code> that may be of interest (<code>param=0.75</code> to <code>3</code> are
documented there).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfPower(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfPower_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfPower_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfPower_+3A_param">param</code></td>
<td>
<p>A single, positive value specifying the <code class="reqn">\rho</code>
parameter for which Kim-DeMets spending is to be computed; allowable range
is (0,50]</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See <code>vignette("SpendingFunctionOverview")</code> for further details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# design a 4-analysis trial using a Kim-DeMets spending function
# for both lower and upper bounds
x &lt;- gsDesign(k = 4, sfu = sfPower, sfupar = 3, sfl = sfPower, sflpar = 1.5)

# print the design
x

# plot the spending function using many points to obtain a smooth curve
# show rho=3 for approximation to O'Brien-Fleming and rho=.75 for
# approximation to Pocock design.
# Also show rho=2 for an intermediate spending.
# Compare these to Hwang-Shih-DeCani spending with gamma=-4,  -2,  1
t &lt;- 0:100 / 100
plot(t, sfPower(0.025, t, 3)$spend,
  xlab = "Proportion of sample size",
  ylab = "Cumulative Type I error spending",
  main = "Kim-DeMets (rho) versus Hwang-Shih-DeCani (gamma) Spending",
  type = "l", cex.main = .9
)
lines(t, sfPower(0.025, t, 2)$spend, lty = 2)
lines(t, sfPower(0.025, t, 0.75)$spend, lty = 3)
lines(t, sfHSD(0.025, t, 1)$spend, lty = 3, col = 2)
lines(t, sfHSD(0.025, t, -2)$spend, lty = 2, col = 2)
lines(t, sfHSD(0.025, t, -4)$spend, lty = 1, col = 2)
legend(
  x = c(.0, .375), y = .025 * c(.65, 1), lty = 1:3,
  legend = c("rho= 3", "rho= 2", "rho= 0.75")
)
legend(
  x = c(.0, .357), y = .025 * c(.65, .85), lty = 1:3, bty = "n", col = 2,
  legend = c("gamma= -4", "gamma= -2", "gamma=1")
)
</code></pre>

<hr>
<h2 id='sfTDist'>t-distribution Spending Function</h2><span id='topic+sfTDist'></span>

<h3>Description</h3>

<p>The function <code>sfTDist()</code> provides perhaps the maximum flexibility among
spending functions provided in the <code>gsDesign</code> package. This function
allows fitting of three points on a cumulative spending function curve; in
this case, six parameters are specified indicating an x and a y coordinate
for each of 3 points. Normally this function will be passed to
<code>gsDesign()</code> in the parameter <code>sfu</code> for the upper bound or
<code>sfl</code> for the lower bound to specify a spending function family for a
design. In this case, the user does not need to know the calling sequence.
The calling sequence is useful, however, when the user wishes to plot a
spending function as demonstrated below in examples.
</p>
<p>The t-distribution spending function takes the form </p>
<p style="text-align: center;"><code class="reqn">f(t;\alpha)=\alpha
F(a+bF^{-1}(t))</code>
</p>
<p> where <code class="reqn">F()</code> is a cumulative t-distribution function
with <code>df</code> degrees of freedom and <code class="reqn">F^{-1}()</code> is its inverse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfTDist(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfTDist_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size/information.</p>
</td></tr>
<tr><td><code id="sfTDist_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfTDist_+3A_param">param</code></td>
<td>
<p>In the three-parameter specification, the first paramater (a)
may be any real value, the second (b) any positive value, and the third
parameter (df=degrees of freedom) any real value 1 or greater. When
<code>gsDesign()</code> is called with a t-distribution spending function, this is
the parameterization printed.  The five parameter specification is
<code>c(t1,t2,u1,u2,df)</code> where the objective is that the resulting
cumulative proportion of spending at <code>t</code> represented by <code>sf(t)</code>
satisfies <code>sf(t1)=alpha*u1</code>, <code>sf(t2)=alpha*u2</code>. The t-distribution
used has <code>df</code> degrees of freedom.  In this parameterization, all the
first four values must be between 0 and 1 and <code>t1 &lt; t2</code>, <code>u1 &lt;
u2</code>.  The final parameter is any real value of 1 or more. This
parameterization can fit any two points satisfying these requirements.  The
six parameter specification attempts to fit 3 points, but does not have
flexibility to fit any three points.  In this case, the specification for
<code>param</code> is c(t1,t2,t3,u1,u2,u3) where the objective is that
<code>sf(t1)=alpha*u1</code>, <code>sf(t2)=alpha*u2</code>, and <code>sf(t3)=alpha*u3</code>.
See examples to see what happens when points are specified that cannot be
fit.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See spending functions for further
details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# 3-parameter specification: a,  b,  df
sfTDist(1, 1:5 / 6, c(-1, 1.5, 4))$spend

# 5-parameter specification fits 2 points,  in this case
# the 1st 2 interims are at 25% and 50% of observations with
# cumulative error spending of 10% and 20%, respectively
# final parameter is df
sfTDist(1, 1:3 / 4, c(.25, .5, .1, .2, 4))$spend

# 6-parameter specification fits 3 points
# Interims are at 25%. 50% and 75% of observations
# with cumulative spending of 10%, 20% and 50%, respectively
# Note: not all 3 point combinations can be fit
sfTDist(1, 1:3 / 4, c(.25, .5, .75, .1, .2, .5))$spend

# Example of error message when the 3-points specified
# in the 6-parameter version cannot be fit
try(sfTDist(1, 1:3 / 4, c(.25, .5, .75, .1, .2, .3))$errmsg)

# sfCauchy (sfTDist with 1 df) and sfNormal (sfTDist with infinite df)
# show the limits of what sfTdist can fit
# for the third point are u3 from 0.344 to 0.6 when t3=0.75
sfNormal(1, 1:3 / 4, c(.25, .5, .1, .2))$spend[3]
sfCauchy(1, 1:3 / 4, c(.25, .5, .1, .2))$spend[3]

# plot a few t-distribution spending functions fitting
# t=0.25, .5 and u=0.1, 0.2
# to demonstrate the range of flexibility
t &lt;- 0:100 / 100
plot(t, sfTDist(0.025, t, c(.25, .5, .1, .2, 1))$spend,
  xlab = "Proportion of final sample size",
  ylab = "Cumulative Type I error spending",
  main = "t-Distribution Spending Function Examples", type = "l"
)
lines(t, sfTDist(0.025, t, c(.25, .5, .1, .2, 1.5))$spend, lty = 2)
lines(t, sfTDist(0.025, t, c(.25, .5, .1, .2, 3))$spend, lty = 3)
lines(t, sfTDist(0.025, t, c(.25, .5, .1, .2, 10))$spend, lty = 4)
lines(t, sfTDist(0.025, t, c(.25, .5, .1, .2, 100))$spend, lty = 5)
legend(
  x = c(.0, .3), y = .025 * c(.7, 1), lty = 1:5,
  legend = c("df = 1", "df = 1.5", "df = 3", "df = 10", "df = 100")
)
</code></pre>

<hr>
<h2 id='sfTruncated'>Truncated, trimmed and gapped spending functions</h2><span id='topic+sfTruncated'></span><span id='topic+sfTrimmed'></span><span id='topic+sfGapped'></span>

<h3>Description</h3>

<p>The functions <code>sfTruncated()</code> and <code>sfTrimmed</code> apply any other
spending function over a restricted range. This allows eliminating spending
for early interim analyses when you desire not to stop for the bound being
specified; this is usually applied to eliminate early tests for a positive
efficacy finding. The truncation can come late in the trial if you desire to
stop a trial any time after, say, 90 percent of information is available and
an analysis is performed. This allows full Type I error spending if the
final analysis occurs early. Both functions set cumulative spending to 0
below a 'spending interval' in the interval [0,1], and set cumulative
spending to 1 above this range. <code>sfTrimmed()</code> otherwise does not change
an input spending function that is specified; probably the preferred and
more intuitive method in most cases. <code>sfTruncated()</code> resets the time
scale on which the input spending function is computed to the 'spending
interval.'
</p>
<p><code>sfGapped()</code> allows elimination of analyses after some time point in
the trial; see details and examples.
</p>
<p><code>sfTrimmed</code> simply computes the value of the input spending function
and parameters in the sub-range of [0,1], sets spending to 0 below this
range and sets spending to 1 above this range.
</p>
<p><code>sfGapped</code> spends outside of the range provided in trange. Below
trange, the input spending function is used. Above trange, full spending is
used; i.e., the first analysis performed above the interval in trange is the
final analysis. As long as the input spending function is strictly
increasing, this means that the first interim in the interval trange is the
final interim analysis for the bound being specified.
</p>
<p><code>sfTruncated</code> compresses spending into a sub-range of [0,1]. The
parameter <code>param$trange</code> specifies the range over which spending is to
occur. Within this range, spending is spent according to the spending
function specified in <code>param$sf</code> along with the corresponding spending
function parameter(s) in <code>param$param</code>. See example using
<code>sfLinear</code> that spends uniformly over specified range.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfTruncated(alpha, t, param)

sfTrimmed(alpha, t, param)

sfGapped(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfTruncated_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha=0.025</code> for one-sided Type I error specification or
<code>alpha=0.1</code> for Type II error specification. However, this could be set
to 1 if for descriptive purposes you wish to see the proportion of spending
as a function of the proportion of sample size or information.</p>
</td></tr>
<tr><td><code id="sfTruncated_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size or information for which the
spending function will be computed.</p>
</td></tr>
<tr><td><code id="sfTruncated_+3A_param">param</code></td>
<td>
<p>a list containing the elements sf (a spendfn object such as
sfHSD), trange (the range over which the spending function increases from 0
to 1; 0 &lt;= trange[1]&lt;trange[2] &lt;=1; for sfGapped, trange[1] must be &gt; 0),
and param (null for a spending function with no parameters or a scalar or
vector of parameters needed to fully specify the spending function in sf).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See <code>vignette("SpendingFunctionOverview")</code>
for further details.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Eliminate efficacy spending forany interim at or before 20 percent of information.
# Complete spending at first interim at or after 80 percent of information.
tx &lt;- (0:100) / 100
s &lt;- sfHSD(alpha = .05, t = tx, param = 1)$spend
x &lt;- data.frame(t = tx, Spending = s, sf = "Original spending")
param &lt;- list(trange = c(.2, .8), sf = sfHSD, param = 1)
s &lt;- sfTruncated(alpha = .05, t = tx, param = param)$spend
x &lt;- rbind(x, data.frame(t = tx, Spending = s, sf = "Truncated"))
s &lt;- sfTrimmed(alpha = .05, t = tx, param = param)$spend
x &lt;- rbind(x, data.frame(t = tx, Spending = s, sf = "Trimmed"))
s &lt;- sfGapped(alpha = .05, t = tx, param = param)$spend
x &lt;- rbind(x, data.frame(t = tx, Spending = s, sf = "Gapped"))
ggplot2::ggplot(x, ggplot2::aes(x = t, y = Spending, col = sf)) + 
ggplot2::geom_line()


# now apply the sfTrimmed version in gsDesign
# initially, eliminate the early efficacy analysis
# note: final spend must occur at &gt; next to last interim
x &lt;- gsDesign(
  k = 4, n.fix = 100, sfu = sfTrimmed,
  sfupar = list(sf = sfHSD, param = 1, trange = c(.3, .9))
)

# first upper bound=20 means no testing there
gsBoundSummary(x)

# now, do not eliminate early efficacy analysis
param &lt;- list(sf = sfHSD, param = 1, trange = c(0, .9))
x &lt;- gsDesign(k = 4, n.fix = 100, sfu = sfTrimmed, sfupar = param)

# The above means if final analysis is done a little early, all spending can occur
# Suppose we set calendar date for final analysis based on
# estimated full information, but come up with only 97 pct of plan
xA &lt;- gsDesign(
  k = x$k, n.fix = 100, n.I = c(x$n.I[1:3], .97 * x$n.I[4]),
  test.type = x$test.type,
  maxn.IPlan = x$n.I[x$k],
  sfu = sfTrimmed, sfupar = param
)
# now accelerate without the trimmed spending function
xNT &lt;- gsDesign(
  k = x$k, n.fix = 100, n.I = c(x$n.I[1:3], .97 * x$n.I[4]),
  test.type = x$test.type,
  maxn.IPlan = x$n.I[x$k],
  sfu = sfHSD, sfupar = 1
)
# Check last bound if analysis done at early time
x$upper$bound[4]
# Now look at last bound if done at early time with trimmed spending function
# that allows capture of full alpha
xA$upper$bound[4]
# With original spending function, we don't get full alpha and therefore have
# unnecessarily stringent bound at final analysis
xNT$upper$bound[4]

# note that if the last analysis is LATE, all 3 approaches should give the same
# final bound that has a little larger z-value
xlate &lt;- gsDesign(
  k = x$k, n.fix = 100, n.I = c(x$n.I[1:3], 1.25 * x$n.I[4]),
  test.type = x$test.type,
  maxn.IPlan = x$n.I[x$k],
  sfu = sfHSD, sfupar = 1
)
xlate$upper$bound[4]

# eliminate futility after the first interim analysis
# note that by setting trange[1] to .2, the spend at t=.2 is used for the first
# interim at or after 20 percent of information
x &lt;- gsDesign(n.fix = 100, sfl = sfGapped, sflpar = list(trange = c(.2, .9), sf = sfHSD, param = 1))
</code></pre>

<hr>
<h2 id='sfXG1'>Xi and Gallo conditional error spending functions</h2><span id='topic+sfXG1'></span><span id='topic+sfXG'></span><span id='topic+sfXG2'></span><span id='topic+sfXG3'></span>

<h3>Description</h3>

<p>Error spending functions based on Xi and Gallo (2019).
The intention of these spending functions is to provide bounds where the
conditional error at an efficacy bound is approximately equal to the
conditional error rate for crossing the final analysis bound.
This is explained in greater detail in
<code>vignette("ConditionalErrorSpending")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfXG1(alpha, t, param)

sfXG2(alpha, t, param)

sfXG3(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfXG1_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Normally,
<code>alpha = 0.025</code> for one-sided Type I error specification or
<code>alpha = 0.1</code> for Type II error specification.
However, this could be set to 1 if for descriptive purposes you wish
to see the proportion of spending as a function of the proportion of
sample size/information.</p>
</td></tr>
<tr><td><code id="sfXG1_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="sfXG1_+3A_param">param</code></td>
<td>
<p>This is the gamma parameter in the Xi and Gallo
spending function paper, distinct for each function.
See the details section for functional forms and range of param
acceptable for each spending function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Xi and Gallo use an additive boundary for group sequential designs with
connection to conditional error.
Three spending functions are defined: <code>sfXG1()</code>, <code>sfXG2()</code>,
and <code>sfXG3()</code>.
</p>
<p>Method 1 is defined for <code class="reqn">\gamma \in [0.5, 1)</code> as
</p>
<p style="text-align: center;"><code class="reqn">f(Z_K \ge u_K | Z_k = u_k) = 2 - 2\times \Phi\left(\frac{z_{\alpha/2} - z_\gamma\sqrt{1-t}}{\sqrt{t}} \right).</code>
</p>

<p>Method 2 is defined for <code class="reqn">\gamma \in [1 - \Phi(z_{\alpha/2}/2), 1)</code> as
</p>
<p style="text-align: center;"><code class="reqn">f_\gamma(t; \alpha)=2-2\Phi \left(
\Phi^{-1}(1-\alpha/2)/ t^{1/2} \right).</code>
</p>

<p>Method 3 is defined as for <code class="reqn">\gamma \in (\alpha/2, 1)</code> as
</p>
<p style="text-align: center;"><code class="reqn">f(t; \alpha)= 2 - 2\times \Phi\left(\frac{z_{\alpha/2} - z_\gamma(1-\sqrt t)}{\sqrt t} \right).</code>
</p>



<h3>Value</h3>

<p>An object of type <code>spendfn</code>. See spending functions for
further details.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>
<p><code>vignette("SpendingFunctionOverview")</code>, <code><a href="#topic+gsDesign">gsDesign</a></code>,
<code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>
<p>Xi D and Gallo P (2019), An additive boundary for group sequential designs
with connection to conditional error. <em>Statistics in Medicine</em>; 38 (23),
4656&ndash;4669.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot conditional error spending spending functions across
# a range of values of interest
pts &lt;- seq(0, 1.2, 0.01)
pal &lt;- palette()

plot(
  pts,
  sfXG1(0.025, pts, 0.5)$spend,
  type = "l", col = pal[1],
  xlab = "t", ylab = "Spending", main = "Xi-Gallo, Method 1"
)
lines(pts, sfXG1(0.025, pts, 0.6)$spend, col = pal[2])
lines(pts, sfXG1(0.025, pts, 0.75)$spend, col = pal[3])
lines(pts, sfXG1(0.025, pts, 0.9)$spend, col = pal[4])
legend(
  "topleft",
  legend = c("gamma=0.5", "gamma=0.6", "gamma=0.75", "gamma=0.9"),
  col = pal[1:4],
  lty = 1
)

plot(
  pts,
  sfXG2(0.025, pts, 0.14)$spend,
  type = "l", col = pal[1],
  xlab = "t", ylab = "Spending", main = "Xi-Gallo, Method 2"
)
lines(pts, sfXG2(0.025, pts, 0.25)$spend, col = pal[2])
lines(pts, sfXG2(0.025, pts, 0.5)$spend, col = pal[3])
lines(pts, sfXG2(0.025, pts, 0.75)$spend, col = pal[4])
lines(pts, sfXG2(0.025, pts, 0.9)$spend, col = pal[5])
legend(
  "topleft",
  legend = c("gamma=0.14", "gamma=0.25", "gamma=0.5", "gamma=0.75", "gamma=0.9"),
  col = pal[1:5],
  lty = 1
)

plot(
  pts,
  sfXG3(0.025, pts, 0.013)$spend,
  type = "l", col = pal[1],
  xlab = "t", ylab = "Spending", main = "Xi-Gallo, Method 3"
)
lines(pts, sfXG3(0.025, pts, 0.02)$spend, col = pal[2])
lines(pts, sfXG3(0.025, pts, 0.05)$spend, col = pal[3])
lines(pts, sfXG3(0.025, pts, 0.1)$spend, col = pal[4])
lines(pts, sfXG3(0.025, pts, 0.25)$spend, col = pal[5])
lines(pts, sfXG3(0.025, pts, 0.5)$spend, col = pal[6])
lines(pts, sfXG3(0.025, pts, 0.75)$spend, col = pal[7])
lines(pts, sfXG3(0.025, pts, 0.9)$spend, col = pal[8])
legend(
  "bottomright",
  legend = c(
    "gamma=0.013", "gamma=0.02", "gamma=0.05", "gamma=0.1",
    "gamma=0.25", "gamma=0.5", "gamma=0.75", "gamma=0.9"
  ),
  col = pal[1:8],
  lty = 1
)
</code></pre>

<hr>
<h2 id='summary.gsDesign'>Bound Summary and Z-transformations</h2><span id='topic+summary.gsDesign'></span><span id='topic+print.gsDesign'></span><span id='topic+gsBoundSummary'></span><span id='topic+xprint'></span><span id='topic+print.gsBoundSummary'></span><span id='topic+gsBValue'></span><span id='topic+gsDelta'></span><span id='topic+gsRR'></span><span id='topic+gsHR'></span><span id='topic+gsCPz'></span>

<h3>Description</h3>

<p>A tabular summary of a group sequential design's bounds and their properties
are often useful. The 'vintage' <code>print.gsDesign()</code> function provides a
complete but minimally formatted summary of a group sequential design
derived by <code>gsDesign()</code>. A brief description of the overall design can
also be useful (<code>summary.gsDesign()</code>.  A tabular summary of boundary
characteristics oriented only towards LaTeX output is produced by
<code><a href="#topic+xtable.gsSurv">xtable.gsSurv</a></code>. More flexibility is provided by
<code>gsBoundSummary()</code> which produces a tabular summary of a
user-specifiable set of package-provided boundary properties in a data
frame.  This can also be used to along with functions such as
<code><a href="base.html#topic+print.data.frame">print.data.frame</a>()</code>, <code><a href="utils.html#topic+write.table">write.table</a>()</code>,
<code><a href="utils.html#topic+write.csv">write.csv</a>()</code>, <code><a href="utils.html#topic+write.csv2">write.csv2</a>()</code> or, from the RTF
package, <code>addTable.RTF()</code> (from the rtf package) to produce console or
R Markdown output or output to a variety of file types. <code>xprint()</code> is
provided for LaTeX output by setting default options for
<code><a href="xtable.html#topic+print.xtable">print.xtable</a></code> when producing tables summarizing design
bounds.
</p>
<p>Individual transformation of z-value test statistics for interim and final
analyses are obtained from <code>gsBValue()</code>, <code>gsDelta()</code>,
<code>gsHR()</code> and <code>gsCPz()</code> for B-values, approximate treatment effect
(see details), approximate hazard ratio and conditional power, respectively.
</p>
<p>The <code>print.gsDesign</code> function is intended to provide an easier output
to review than is available from a simple list of all the output components.
The <code>gsBoundSummary</code> function is intended to provide a summary of
boundary characteristics that is often useful for evaluating boundary
selection; this outputs an extension of the <code>data.frame</code> class that
sets up default printing without row names using
<code>print.gsBoundSummary</code>. <code>summary.gsDesign</code>, on the other hand,
provides a summary of the overall design at a higher level; this provides
characteristics not included in the <code>gsBoundSummary</code> summary and no
detail concerning interim analysis bounds.
</p>
<p>In brief, the computed descriptions of group sequential design bounds are as
follows: <code>Z:</code> Standardized normal test statistic at design bound.
</p>
<p><code>p (1-sided):</code> 1-sided p-value for <code>Z</code>. This will be computed as
the probability of a greater EXCEPT for lower bound when a 2-sided design is
being summarized.
</p>
<p><code>delta at bound:</code> Approximate value of the natural parameter at the
bound. The approximate standardized effect size at the bound is generally
computed as <code>Z/sqrt(n)</code>. Calling this <code>theta</code>, this is translated
to the <code>delta</code> using the values <code>delta0</code> and <code>delta1</code> from
the input <code>x</code> by the formula <code>delta0 +
(delta1-delta0)/theta1*theta</code> where <code>theta1</code> is the alternate
hypothesis value of the standardized parameter. Note that this value will be
exponentiated in the case of relative risks, hazard ratios or when the user
specifies <code>logdelta=TRUE</code>. In the case of hazard ratios, the value is
computed instead by <code>gsHR()</code> to be consistent with
<code>plot.gsDesign()</code>. Similarly, the value is computed by <code>gsRR()</code>
when the relative risk is the natural parameter.
</p>
<p><code>Spending: </code>Incremental error spending at each given analysis. For
asymmetric designs, futility bound will have beta-spending summarized.
Efficacy bound always has alpha-spending summarized.
</p>
<p><code>B-value: </code><code>sqrt(t)*Z</code> where <code>t</code> is the proportion of
information at the analysis divided by the final analysis planned
information. The expected value for B-values is directly proportional to
<code>t</code>.
</p>
<p><code>CP: </code>Conditional power under the estimated treatment difference
assuming the interim Z-statistic is at the study bound
</p>
<p><code>CP H1: </code>Conditional power under the alternate hypothesis treatment
effect assuming the interim test statistic is at the study bound.
</p>
<p><code>PP: </code>Predictive power assuming the interim test statistic is at the
study bound and the input prior distribution for the standardized effect
size. This is the conditional power averaged across the posterior
distribution for the treatment effect given the interim test statistic
value. <code>P{Cross if delta=xx}: </code>For each of the parameter values in
<code>x</code>, the probability of crossing either bound given that treatment
effect is computed. This value is cumulative for each bound. For example,
the probability of crossing the efficacy bound at or before the analysis of
interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gsDesign'
summary(object, information = FALSE, timeunit = "months", ...)

## S3 method for class 'gsDesign'
print(x, ...)

gsBoundSummary(
  x,
  deltaname = NULL,
  logdelta = FALSE,
  Nname = NULL,
  digits = 4,
  ddigits = 2,
  tdigits = 0,
  timename = "Month",
  prior = normalGrid(mu = x$delta/2, sigma = 10/sqrt(x$n.fix)),
  POS = FALSE,
  ratio = NULL,
  exclude = c("B-value", "Spending", "CP", "CP H1", "PP"),
  r = 18,
  ...
)

xprint(
  x,
  include.rownames = FALSE,
  hline.after = c(-1, which(x$Value == x[1, ]$Value) - 1, nrow(x)),
  ...
)

## S3 method for class 'gsBoundSummary'
print(x, row.names = FALSE, digits = 4, ...)

gsBValue(z, i, x, ylab = "B-value", ...)

gsDelta(z, i, x, ylab = NULL, ...)

gsRR(z, i, x, ratio = 1, ylab = "Approximate risk ratio", ...)

gsHR(z, i, x, ratio = 1, ylab = "Approximate hazard ratio", ...)

gsCPz(z, i, x, theta = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.gsDesign_+3A_object">object</code></td>
<td>
<p>An item of class <code>gsDesign</code> or <code>gsSurv</code></p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_information">information</code></td>
<td>
<p>indicator of whether <code>n.I</code> in <code>object</code>
represents statistical information rather than sample size or event counts.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_timeunit">timeunit</code></td>
<td>
<p>Text string with time units used for time-to-event designs
created with <code>gsSurv()</code></p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_...">...</code></td>
<td>
<p>This allows many optional arguments that are standard when
calling <code>plot</code> for <code>gsBValue</code>, <code>gsDelta</code>, <code>gsHR</code>,
<code>gsRR</code> and <code>gsCPz</code></p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_x">x</code></td>
<td>
<p>An item of class <code>gsDesign</code> or <code>gsSurv</code>, except for
<code>print.gsBoundSummary()</code> where <code>x</code> is an object created by
<code>gsBoundSummary()</code> and <code>xprint()</code> which is used with <code>xtable</code>
(see examples)</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_deltaname">deltaname</code></td>
<td>
<p>Natural parameter name. If default <code>NULL</code> is used,
routine will default to <code>"HR"</code> when class is <code>gsSurv</code> or if
<code>nFixSurv</code> was input when creating <code>x</code> with <code>gsDesign()</code>.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_logdelta">logdelta</code></td>
<td>
<p>Indicates whether natural parameter is the natural logarithm
of the actual parameter. For example, the relative risk or odds-ratio would
be put on the logarithmic scale since the asymptotic behavior is 'more
normal' than a non-transformed value. As with <code>deltaname</code>, the default
will be changed to true if <code>x</code> has class <code>gsDesign</code> or if
<code>nFixSurv&gt;0</code> was input when <code>x</code> was created by <code>gsDesign()</code>;
that is, the natural parameter for a time-to-event endpoint will be on the
logarithmic scale.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_nname">Nname</code></td>
<td>
<p>This will normally be changed to <code>"N"</code> or, if a
time-to-event endpoint is used, <code>"Events"</code>. Other immediate possibility
are <code>"Deaths"</code> or <code>"Information"</code>.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_digits">digits</code></td>
<td>
<p>Number of digits past the decimal to be printed in the body of
the table.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_ddigits">ddigits</code></td>
<td>
<p>Number of digits past the decimal to be printed for the
natural parameter delta.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_tdigits">tdigits</code></td>
<td>
<p>Number of digits past the decimal point to be shown for
estimated timing of each analysis.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_timename">timename</code></td>
<td>
<p>Text string indicating time unit.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_prior">prior</code></td>
<td>
<p>A prior distribution for the standardized effect size. Must be
of the format produced by <code>normalGrid()</code>, but can reflect an arbitrary
prior distribution. The default reflects a normal prior centered half-way
between the null and alternate hypothesis with the variance being equivalent
to the treatment effect estimate if 1 percent of the sample size for a fixed
design were sampled. The prior is intended to be relatively uninformative.
This input will only be applied if <code>POS=TRUE</code> is input.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_pos">POS</code></td>
<td>
<p>This is an indicator of whether or not probability of success
(POS) should be estimated at baseline or at each interim based on the prior
distribution input in <code>prior</code>. The prior probability of success before
the trial starts is the power of the study averaged over the prior
distribution for the standardized effect size. The POS after an interim
analysis assumes the interim test statistic is an unknown value between the
futility and efficacy bounds. Based on this, a posterior distribution for
the standardized parameter is computed and the conditional power of the
trial is averaged over this posterior distribution.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_ratio">ratio</code></td>
<td>
<p>Sample size ratio assumed for experimental to control treatment
group sample sizes. This only matters when <code>x</code> for a binomial or
time-to-event endpoint where <code>gsRR</code> or <code>gsHR</code> are used for
approximating the treatment effect if a test statistic falls on a study
bound.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_exclude">exclude</code></td>
<td>
<p>A list of test statistics to be excluded from design boundary
summary produced; see details or examples for a list of all possible output
values. A value of <code>NULL</code> produces all available summaries.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_r">r</code></td>
<td>
<p>See <code><a href="#topic+gsDesign">gsDesign</a></code>. This is an integer used to control the
degree of accuracy of group sequential calculations which will normally not
be changed.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_include.rownames">include.rownames</code></td>
<td>
<p>indicator of whether or not to include row names in
output.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_hline.after">hline.after</code></td>
<td>
<p>table lines after which horizontal separation lines
should be set; default is to put lines between each analysis as well as at
the top and bottom of the table.</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_row.names">row.names</code></td>
<td>
<p>indicator of whether or not to print row names</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_z">z</code></td>
<td>
<p>A vector of z-statistics</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_i">i</code></td>
<td>
<p>A vector containing the analysis for each element in <code>z</code>; each
element must be in 1 to <code>x$k</code>, inclusive</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_ylab">ylab</code></td>
<td>
<p>Used when functions are passed to <code>plot.gsDesign</code> to
establish default y-axis labels</p>
</td></tr>
<tr><td><code id="summary.gsDesign_+3A_theta">theta</code></td>
<td>
<p>A scalar value representing the standardized effect size used
for conditional power calculations; see <code>gsDesign</code>; if NULL,
conditional power is computed at the estimated interim treatment effect
based on <code>z</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gsBValue()</code>, <code>gsDelta()</code>, <code>gsHR()</code> and
<code>gsCPz()</code> each returns a vector containing the B-values, approximate
treatment effect (see details), approximate hazard ratio and conditional
power, respectively, for each value specified by the interim test statistics
in <code>z</code> at interim analyses specified in <code>i</code>.
</p>
<p><code>summary</code> returns a text string summarizing the design at a high level.
This may be used with <code>gsBoundSummary</code> for a nicely formatted, concise
group sequential design description.
</p>
<p><code>gsBoundSummary</code> returns a table in a data frame providing a variety of
boundary characteristics. The tabular format makes formatting particularly
amenable to place in documents either through direct creation of readable by
Word (see the <code>rtf</code> package) or to a csv format readable by spreadsheet
software using <code>write.csv</code>.
</p>
<p><code>print.gsDesign</code> prints an overall summary a group sequential design.
While the design description is complete, the format is not as 'document
friendly' as <code>gsBoundSummary</code>.
</p>
<p><code>print.gsBoundSummary</code> is a simple extension of <code>print.data.frame</code>
intended for objects created with <code>gsBoundSummary</code>. The only extension
is to make the default to not print row names. This is probably 'not good R
style' but may be helpful for many lazy R programmers like the author.
</p>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><a href="#topic+gsDesign">gsDesign</a>, <a href="#topic+plot.gsDesign">plot.gsDesign</a>,
<code><a href="#topic+gsProbability">gsProbability</a></code>, <code><a href="#topic+xtable.gsSurv">xtable.gsSurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
# survival endpoint using gsSurv
# generally preferred over nSurv since time computations are shown
xgs &lt;- gsSurv(lambdaC = .2, hr = .5, eta = .1, T = 2, minfup = 1.5)
gsBoundSummary(xgs, timename = "Year", tdigits = 1)
summary(xgs)

# survival endpoint using nSurvival
# NOTE: generally recommend gsSurv above for this!
ss &lt;- nSurvival(
  lambda1 = .2, lambda2 = .1, eta = .1, Ts = 2, Tr = .5,
  sided = 1, alpha = .025, ratio = 2
)
xs &lt;- gsDesign(nFixSurv = ss$n, n.fix = ss$nEvents, delta1 = log(ss$lambda2 / ss$lambda1))
gsBoundSummary(xs, logdelta = TRUE, ratio = ss$ratio)
# generate some of the above summary statistics for the upper bound
z &lt;- xs$upper$bound
# B-values
gsBValue(z = z, i = 1:3, x = xs)
# hazard ratio
gsHR(z = z, i = 1:3, x = xs)
# conditional power at observed treatment effect
gsCPz(z = z[1:2], i = 1:2, x = xs)
# conditional power at H1 treatment effect
gsCPz(z = z[1:2], i = 1:2, x = xs, theta = xs$delta)

# information-based design
xinfo &lt;- gsDesign(delta = .3, delta1 = .3)
gsBoundSummary(xinfo, Nname = "Information")

# show all available boundary descriptions
gsBoundSummary(xinfo, Nname = "Information", exclude = NULL)

# add intermediate parameter value
xinfo &lt;- gsProbability(d = xinfo, theta = c(0, .15, .3))
class(xinfo) # note this is still as gsDesign class object
gsBoundSummary(xinfo, Nname = "Information")

# now look at a binomial endpoint; specify H0 treatment difference as p1-p2=.05
# now treatment effect at bound (say, thetahat) is transformed to
# xp$delta0 + xp$delta1*(thetahat-xp$delta0)/xp$delta
np &lt;- nBinomial(p1 = .15, p2 = .10)
xp &lt;- gsDesign(n.fix = np, endpoint = "Binomial", delta1 = .05)
summary(xp)
gsBoundSummary(xp, deltaname = "p[C]-p[E]")
# estimate treatment effect at lower bound
# by setting delta0=0 (default) and delta1 above in gsDesign
# treatment effect at bounds is scaled to these differences
# in this case, this is the difference in event rates
gsDelta(z = xp$lower$bound, i = 1:3, xp)

# binomial endpoint with risk ratio estimates
n.fix &lt;- nBinomial(p1 = .3, p2 = .15, scale = "RR")
xrr &lt;- gsDesign(k = 2, n.fix = n.fix, delta1 = log(.15 / .3), endpoint = "Binomial")
gsBoundSummary(xrr, deltaname = "RR", logdelta = TRUE)
gsRR(z = xp$lower$bound, i = 1:3, xrr)
plot(xrr, plottype = "RR")

# delta is odds-ratio: sample size slightly smaller than for relative risk or risk difference
n.fix &lt;- nBinomial(p1 = .3, p2 = .15, scale = "OR")
xOR &lt;- gsDesign(k = 2, n.fix = n.fix, delta1 = log(.15 / .3 / .85 * .7), endpoint = "Binomial")
gsBoundSummary(xOR, deltaname = "OR", logdelta = TRUE)

# for nice LaTeX table output, use xprint
xprint(xtable::xtable(gsBoundSummary(xOR, deltaname = "OR", logdelta = TRUE), 
                                          caption = "Table caption."))
</code></pre>

<hr>
<h2 id='summary.spendfn'>Spending Function</h2><span id='topic+summary.spendfn'></span><span id='topic+spendingFunction'></span>

<h3>Description</h3>

<p>Spending Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spendfn'
summary(object, ...)

spendingFunction(alpha, t, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.spendfn_+3A_object">object</code></td>
<td>
<p>A spendfn object to be summarized.</p>
</td></tr>
<tr><td><code id="summary.spendfn_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="summary.spendfn_+3A_alpha">alpha</code></td>
<td>
<p>Real value <code class="reqn">&gt; 0</code> and no more than 1. Defaults in calls to
<code>gsDesign()</code> are <code>alpha=0.025</code> for one-sided Type I error
specification and <code>alpha=0.1</code> for Type II error specification.
However, this could be set to 1 if, for descriptive purposes, you wish to
see the proportion of spending as a function of the proportion of sample
size/information.</p>
</td></tr>
<tr><td><code id="summary.spendfn_+3A_t">t</code></td>
<td>
<p>A vector of points with increasing values from 0 to 1, inclusive.
Values of the proportion of sample size/information for which the spending
function will be computed.</p>
</td></tr>
<tr><td><code id="summary.spendfn_+3A_param">param</code></td>
<td>
<p>A single real value or a vector of real values specifying the
spending function parameter(s); this must be appropriately matched to the
spending function specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>spendingFunction</code> and spending functions in general produce an
object of type <code>spendfn</code>.
</p>
<table>
<tr><td><code>name</code></td>
<td>
<p>A character string with the name of the spending function.</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p>any parameters used for the spending function.</p>
</td></tr>
<tr><td><code>parname</code></td>
<td>
<p>a character string or strings with the name(s) of
the parameter(s) in <code>param</code>.</p>
</td></tr>
<tr><td><code>sf</code></td>
<td>
<p>the spending function specified.</p>
</td></tr>
<tr><td><code>spend</code></td>
<td>
<p>a vector of cumulative spending values corresponding to
the input values in <code>t</code>.</p>
</td></tr>
<tr><td><code>bound</code></td>
<td>
<p>this is null when returned from the spending function,
but is set in <code>gsDesign()</code> if the spending function is called
from there.  Contains z-values for bounds of a design.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>this is null when returned from the spending function,
but is set in <code>gsDesign()</code> if the spending function is called
from there.  Contains probabilities of boundary crossing at <code>i</code>-th
analysis for <code>j</code>-th theta value input to <code>gsDesign()</code> in
<code>prob[i,j]</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The gsDesign technical manual is available at
<a href="https://keaven.github.io/gsd-tech-manual/">https://keaven.github.io/gsd-tech-manual/</a>.
</p>


<h3>Author(s)</h3>

<p>Keaven Anderson <a href="mailto:keaven_anderson@merck.com">keaven_anderson@merck.com</a>
</p>


<h3>References</h3>

<p>Jennison C and Turnbull BW (2000), <em>Group Sequential
Methods with Applications to Clinical Trials</em>. Boca Raton: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsDesign">gsDesign</a></code>, <code><a href="#topic+sfHSD">sfHSD</a></code>, <code><a href="#topic+sfPower">sfPower</a></code>,
<code><a href="#topic+sfLogistic">sfLogistic</a></code>, <code><a href="#topic+sfExponential">sfExponential</a></code>,
<code><a href="#topic+sfTruncated">sfTruncated</a></code>, <code>vignette("gsDesignPackageOverview")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: simple example showing what most users need to know

# Design a 4-analysis trial using a Hwang-Shih-DeCani spending function
# for both lower and upper bounds
x &lt;- gsDesign(k = 4, sfu = sfHSD, sfupar = -2, sfl = sfHSD, sflpar = 1)

# Print the design
x
# Summarize the spending functions
summary(x$upper)
summary(x$lower)

# Plot the alpha- and beta-spending functions
plot(x, plottype = 5)

# What happens to summary if we used a boundary function design
x &lt;- gsDesign(test.type = 2, sfu = "OF")
y &lt;- gsDesign(test.type = 1, sfu = "WT", sfupar = .25)
summary(x$upper)
summary(y$upper)

# Example 2: advanced example: writing a new spending function
# Most users may ignore this!

# Implementation of 2-parameter version of
# beta distribution spending function
# assumes t and alpha are appropriately specified (does not check!)
sfbdist &lt;- function(alpha, t, param) {
  # Check inputs
  checkVector(param, "numeric", c(0, Inf), c(FALSE, TRUE))
  if (length(param) != 2) {
    stop(
      "b-dist example spending function parameter must be of length 2"
    )
  }

  # Set spending using cumulative beta distribution and return
  x &lt;- list(
    name = "B-dist example", param = param, parname = c("a", "b"),
    sf = sfbdist, spend = alpha *
      pbeta(t, param[1], param[2]), bound = NULL, prob = NULL
  )

  class(x) &lt;- "spendfn"

  x
}

# Now try it out!
# Plot some example beta (lower bound) spending functions using
# the beta distribution spending function
t &lt;- 0:100 / 100
plot(
  t, sfbdist(1, t, c(2, 1))$spend,
  type = "l",
  xlab = "Proportion of information",
  ylab = "Cumulative proportion of total spending",
  main = "Beta distribution Spending Function Example"
)
lines(t, sfbdist(1, t, c(6, 4))$spend, lty = 2)
lines(t, sfbdist(1, t, c(.5, .5))$spend, lty = 3)
lines(t, sfbdist(1, t, c(.6, 2))$spend, lty = 4)
legend(
  x = c(.65, 1), y = 1 * c(0, .25), lty = 1:4,
  legend = c("a=2, b=1", "a=6, b=4", "a=0.5, b=0.5", "a=0.6, b=2")
)
</code></pre>

<hr>
<h2 id='toBinomialExact'>Translate survival design bounds to exact binomial bounds</h2><span id='topic+toBinomialExact'></span>

<h3>Description</h3>

<p>Translate survival design bounds to exact binomial bounds
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toBinomialExact(x, observedEvents = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toBinomialExact_+3A_x">x</code></td>
<td>
<p>An object of class <code>gsSurv</code>; i.e., an object generated by
the <code>gsSurv()</code> function.</p>
</td></tr>
<tr><td><code id="toBinomialExact_+3A_observedevents">observedEvents</code></td>
<td>
<p>If NULL (default), targeted timing of analyses will come from <code>x$n.I</code>. 
Otherwise, this should be vector of increasing positive integers with at most 1 value <code>&gt;= x$n.IPlan</code> and of length at least 2.
Only one value can be greater than or equal to <code>x$maxn.IPlan</code>. 
This determines the case count at each analysis performed. 
Primarily, this is used for updating a design at the time of analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The exact binomial routine <code>gsBinomialExact</code> has requirements that may not be satisfied
by the initial asymptotic approximation. 
Thus, the approximations are updated to satisfy the following requirements of <code>gsBinomialExact</code>:
<code>a</code> (the efficacy bound) must be positive, non-decreasing, and strictly less than n.I
<code>b</code> (the futility bound) must be positive, non-decreasing, strictly greater than a
<code>n.I - b</code> must be non-decreasing and &gt;= 0
</p>


<h3>Value</h3>

<p>An object of class <code>gsBinomialExact</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsBinomialExact">gsBinomialExact</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following code derives the group sequential design using the method
# of Lachin and Foulkes

x &lt;- gsSurv(
  k = 3,                 # 3 analyses
  test.type = 4,         # Non-binding futility bound 1 (no futility bound) and 4 are allowable
  alpha = .025,          # 1-sided Type I error
  beta = .1,             # Type II error (1 - power)
  timing = c(0.45, 0.7), # Proportion of final planned events at interims
  sfu = sfHSD,           # Efficacy spending function
  sfupar = -4,           # Parameter for efficacy spending function
  sfl = sfLDOF,          # Futility spending function; not needed for test.type = 1
  sflpar = 0,            # Parameter for futility spending function
  lambdaC = .001,        # Exponential failure rate
  hr = 0.3,              # Assumed proportional hazard ratio (1 - vaccine efficacy = 1 - VE)
  hr0 = 0.7,             # Null hypothesis VE
  eta = 5e-04,           # Exponential dropout rate
  gamma = 10,            # Piecewise exponential enrollment rates
  R = 16,                # Time period durations for enrollment rates in gamma
  T = 24,                # Planned trial duration
  minfup = 8,            # Planned minimum follow-up
  ratio = 3              # Randomization ratio (experimental:control)
)
# Convert bounds to exact binomial bounds
toBinomialExact(x)
# Update bounds at time of analysis
toBinomialExact(x, observedEvents = c(20,55,80))
</code></pre>

<hr>
<h2 id='toInteger'>Translate group sequential design to integer events (survival designs)
or sample size (other designs)</h2><span id='topic+toInteger'></span>

<h3>Description</h3>

<p>Translate group sequential design to integer events (survival designs)
or sample size (other designs)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toInteger(x, ratio = 0, roundUpFinal = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toInteger_+3A_x">x</code></td>
<td>
<p>An object of class <code>gsDesign</code> or <code>gsSurv</code>.</p>
</td></tr>
<tr><td><code id="toInteger_+3A_ratio">ratio</code></td>
<td>
<p>A non-negative integer, usually corresponding to experimental:control sample size ratio. 
Rounding is done to a multiple of <code>ratio + 1</code>. If input <code>x</code> has class <code>gsSurv</code> (design for time-to-event outcome),
and <code>x$ratio</code> is a whole number, <code>ratio</code> is replaced by <code>x$ratio</code>.
See details.</p>
</td></tr>
<tr><td><code id="toInteger_+3A_roundupfinal">roundUpFinal</code></td>
<td>
<p>Final value in returned <code>n.I</code> is rounded up
if <code>TRUE</code>; otherwise, just rounded. For <code>gsSurv</code> input, final total sample size is also controlled by this. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>ratio = 3</code>, rounding for final sample size is done to a multiple of 3 + 1 = 4. 
For a <code>gsSurv</code> object input in <code>x</code>, event counts output in <code>n.I</code> are rounded to nearest integer and 
final total sample size is rounded to a multiple of <code>ratio + 1</code>.
For other input values of <code>x</code> (<code>gsDesign</code> class), <code>n.I</code> is interpreted as sample size; 
final value is rounded to a multiple of <code>ratio + 1</code>, with <code>roundUpFinal</code> controlling rounding of last value.
</p>


<h3>Value</h3>

<p>Output is an object of the same class as input <code>x</code>; i.e., <code>gsDesign</code> with integer vector for <code>n.I</code>
or <code>gsSurv</code> with integer vector <code>n.I</code> and integer total sample size. See details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following code derives the group sequential design using the method
# of Lachin and Foulkes

x &lt;- gsSurv(
  k = 3,                 # 3 analyses
  test.type = 4,         # Non-binding futility bound 1 (no futility bound) and 4 are allowable
  alpha = .025,          # 1-sided Type I error
  beta = .1,             # Type II error (1 - power)
  timing = c(0.45, 0.7), # Proportion of final planned events at interims
  sfu = sfHSD,           # Efficacy spending function
  sfupar = -4,           # Parameter for efficacy spending function
  sfl = sfLDOF,          # Futility spending function; not needed for test.type = 1
  sflpar = 0,            # Parameter for futility spending function
  lambdaC = .001,        # Exponential failure rate
  hr = 0.3,              # Assumed proportional hazard ratio (1 - vaccine efficacy = 1 - VE)
  hr0 = 0.7,             # Null hypothesis VE
  eta = 5e-04,           # Exponential dropout rate
  gamma = 10,            # Piecewise exponential enrollment rates
  R = 16,                # Time period durations for enrollment rates in gamma
  T = 24,                # Planned trial duration
  minfup = 8,            # Planned minimum follow-up
  ratio = 3              # Randomization ratio (experimental:control)
)
# Convert bounds to exact binomial bounds
toInteger(x, ratio = 3)
</code></pre>

<hr>
<h2 id='xtable'>xtable</h2><span id='topic+xtable'></span>

<h3>Description</h3>

<p>xtable
</p>


<h3>Value</h3>

<p>an object of class &quot;xtable&quot;
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
