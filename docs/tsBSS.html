<!DOCTYPE html><html><head><title>Help for package tsBSS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tsBSS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AMUSEasymp'>
<p>Second-order Separation Sub-White-Noise Asymptotic Testing with AMUSE</p></a></li>
<li><a href='#AMUSEboot'>
<p>Second-order Separation Sub-White-Noise Bootstrap Testing with AMUSE</p></a></li>
<li><a href='#AMUSEladle'>
<p>Ladle Estimator to Estimate the Number of White Noise Components in SOS with AMUSE</p></a></li>
<li><a href='#bssvol'>
<p>Class: bssvol</p></a></li>
<li><a href='#FixNA'>
<p>The FixNA Method for Blind Source Separation</p></a></li>
<li><a href='#gFOBI'>
<p>Generalized FOBI</p></a></li>
<li><a href='#gJADE'>
<p>Generalized JADE</p></a></li>
<li><a href='#gSOBI'>
<p>Generalized SOBI</p></a></li>
<li><a href='#lbtest'>
<p>Modified Ljung-Box Test and Volatility Clustering Test for Time Series.</p></a></li>
<li><a href='#PVC'>
<p>A Modified Algorithm for Principal Volatility Component Estimator</p></a></li>
<li><a href='#SOBIasymp'>
<p>Second-order Separation Sub-White-Noise Asymptotic Testing with SOBI</p></a></li>
<li><a href='#SOBIboot'>
<p>Second-order Separation Sub-White-Noise Bootstrap Testing with SOBI</p></a></li>
<li><a href='#SOBIladle'>
<p>Ladle Estimator to Estimate the Number of White Noise Components in SOS with SOBI</p></a></li>
<li><a href='#summary.tssdr'>
<p>Summary of an Object of Class tssdr</p></a></li>
<li><a href='#tsBSS-package'>
<p>Blind Source Separation and Supervised Dimension Reduction for Time Series</p></a></li>
<li><a href='#tssdr'>
<p>Supervised Dimension Reduction for Multivariate Time Series</p></a></li>
<li><a href='#vSOBI'>
<p>A Variant of SOBI for Blind Source Separation</p></a></li>
<li><a href='#WeeklyReturnsData'>
<p>Logarithmic Returns of Exchange Rates of 7 Currencies Against US Dollar</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Blind Source Separation and Supervised Dimension Reduction for
Time Series</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-07-09</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Markus Matilainen &lt;markus.matilainen@outlook.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>ICtest (&ge; 0.3-2), JADE (&ge; 2.0-2), BSSprep</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0), forecast, boot, parallel, xts, zoo</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>stochvol, MTS, tsbox, dr</td>
</tr>
<tr>
<td>Description:</td>
<td>Different estimators are provided to solve the blind source separation problem for multivariate time series with stochastic volatility and supervised dimension reduction problem for multivariate time series. Different functions based on AMUSE and SOBI are also provided for estimating the dimension of the white noise subspace. The package is fully described in Nordhausen, Matilainen, Miettinen, Virta and Taskinen (2021) &lt;<a href="https://doi.org/10.18637%2Fjss.v098.i15">doi:10.18637/jss.v098.i15</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-09 19:21:54 UTC; knordhau</td>
</tr>
<tr>
<td>Author:</td>
<td>Markus Matilainen <a href="https://orcid.org/0000-0002-5597-2670"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut],
  Christophe Croux [aut],
  Jari Miettinen <a href="https://orcid.org/0000-0002-3270-7014"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Klaus Nordhausen <a href="https://orcid.org/0000-0002-3758-8501"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Hannu Oja [aut],
  Sara Taskinen <a href="https://orcid.org/0000-0001-9470-7258"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Joni Virta <a href="https://orcid.org/0000-0002-2150-2769"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-10 08:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AMUSEasymp'>
Second-order Separation Sub-White-Noise Asymptotic Testing with AMUSE
</h2><span id='topic+AMUSEasymp'></span><span id='topic+AMUSEasymp.default'></span><span id='topic+AMUSEasymp.ts'></span><span id='topic+AMUSEasymp.xts'></span><span id='topic+AMUSEasymp.zoo'></span>

<h3>Description</h3>

<p>The function uses AMUSE (Algorithm for Multiple Unknown Signals Extraction) to test whether the last <code>p-k</code> latent series are pure white noise, assuming a p-variate second-order stationary blind source separation (BSS) model. The test is asymptotic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AMUSEasymp(X, ...)

## Default S3 method:
AMUSEasymp(X, k, tau = 1, ...)
## S3 method for class 'ts'
AMUSEasymp(X, ...)
## S3 method for class 'xts'
AMUSEasymp(X, ...)
## S3 method for class 'zoo'
AMUSEasymp(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AMUSEasymp_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="AMUSEasymp_+3A_k">k</code></td>
<td>
<p>The number of latent series that are not white noise. Can be between <code class="reqn">0</code> and <code class="reqn">p-1</code>.</p>
</td></tr>
<tr><td><code id="AMUSEasymp_+3A_tau">tau</code></td>
<td>
<p>The lag for the AMUSE autocovariance matrix.</p>
</td></tr>
<tr><td><code id="AMUSEasymp_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AMUSE standardizes <code>X</code> with <code class="reqn">n</code> samples and computes the eigenedcomposition of the autocovariance matrix of the standardized data for a chosen lag <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the squares of the corresponding eigenvalues of the autocovariance matrix. Under the null hypothesis the final <code class="reqn">p - k</code> eigenvalues equal zero, <code class="reqn">\lambda_{p-k} = \cdots = \lambda_{p} = 0</code>, and their mean square <code class="reqn">m</code> can be used as a test statistic in inference on the true number of latent white noise series. 
</p>
<p>This function conducts the hypothesis test using the asymptotic null distribution of <code class="reqn">m</code>, a chi-squared distribution with <code class="reqn">(p - k)(p - k + 1)/2</code> degrees of freedom.
</p>


<h3>Value</h3>

<p>A list of class ictest, inheriting from class htest, containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>The degrees of freedom of the asymptotic null distribution.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string indicating which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>Character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The number of latent series that are not white noise used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the latent series.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>The underlying eigenvalues of the autocovariance matrix.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating AMUSE.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The used lag.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Virta, J. and Nordhausen, K. (2021), <em>Determining the Signal Dimension in Second Order Source Separation</em>. Statistica Sinica, 31, 135&ndash;156.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+SOBIasymp">SOBIasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000

  A &lt;- matrix(rnorm(16), 4, 4)
  s1 &lt;- arima.sim(list(ar = c(0.3, 0.6)), n)
  s2 &lt;- arima.sim(list(ma = c(-0.3, 0.3)), n)
  s3 &lt;- rnorm(n)
  s4 &lt;- rnorm(n)

  S &lt;- cbind(s1, s2, s3, s4)
  X &lt;- S %*% t(A)

  asymp_res_1 &lt;- AMUSEasymp(X, k = 1)
  asymp_res_1

  asymp_res_2 &lt;- AMUSEasymp(X, k = 2)
  asymp_res_2

  # Plots of the estimated sources, the last two are white noise
  plot(asymp_res_2)
</code></pre>

<hr>
<h2 id='AMUSEboot'>
Second-order Separation Sub-White-Noise Bootstrap Testing with AMUSE
</h2><span id='topic+AMUSEboot'></span><span id='topic+AMUSEboot.default'></span><span id='topic+AMUSEboot.ts'></span><span id='topic+AMUSEboot.xts'></span><span id='topic+AMUSEboot.zoo'></span>

<h3>Description</h3>

<p>The function uses AMUSE (Algorithm for Multiple Unknown Signals Extraction) to test whether the last <code>p-k</code> latent series are pure white noise, assuming a p-variate second-order stationary blind source separation (BSS) model.
Four different bootstrapping strategies are available and the function can be run in parallel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AMUSEboot(X, ...)

## Default S3 method:
AMUSEboot(X, k, tau = 1, n.boot = 200, s.boot = c("p", "np1", "np2", "np3"),
          ncores = NULL, iseed = NULL, ...)
## S3 method for class 'ts'
AMUSEboot(X, ...)
## S3 method for class 'xts'
AMUSEboot(X, ...)
## S3 method for class 'zoo'
AMUSEboot(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AMUSEboot_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_k">k</code></td>
<td>
<p>The number of latent series that are not white noise. Can be between <code class="reqn">0</code> and <code class="reqn">p-1</code>.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_tau">tau</code></td>
<td>
<p>The lag for the AMUSE autocovariance matrix.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_n.boot">n.boot</code></td>
<td>
<p>The number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_s.boot">s.boot</code></td>
<td>
<p>Bootstrapping strategy to be used. Possible values are <code>"p"</code> (default), <code>"np1"</code>, <code>"np2"</code>, <code>"np3"</code>. See details for further information.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to be used. If <code>NULL</code> or 1, no parallel computing is used. Otherwise <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code> with <code>type = "PSOCK"</code> is used. It is the users repsonsibilty to choose a reasonable value for <code>ncores</code>. The function <code><a href="parallel.html#topic+detectCores">detectCores</a></code> might be useful in this context.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_iseed">iseed</code></td>
<td>
<p>If parallel computation is used, the seed passed on to <code><a href="parallel.html#topic+clusterSetRNGStream">clusterSetRNGStream</a></code>. Default is <code>NULL</code> which means no fixed seed is used.</p>
</td></tr>
<tr><td><code id="AMUSEboot_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AMUSE standardizes <code>X</code> with <code class="reqn">n</code> samples and computes the eigendecomposition of the autocovariance matrix of the standardized data for a chosen lag <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the squares of the corresponding eigenvalues of the autocovariance matrix. Under the null hypothesis the final <code class="reqn">p - k</code> eigenvalues equal zero, <code class="reqn">\lambda_{p-k} = \cdots = \lambda_{p} = 0</code>, and their mean square <code class="reqn">m</code> can be used as a test statistic in bootstrap-based inference on the true number of latent white noise series.
</p>
<p>The function offers four different bootstrapping strategies for generating samples for which the null hypothesis approximately holds, and they are all based on the following general formula:
</p>

<ol>
<li><p> Decompose the AMUSE-estimated latent series <code class="reqn">\bf S</code> into the postulated signal <code class="reqn">{\bf S}_1</code> and white noise <code class="reqn">{\bf S}_2</code>.
</p>
</li>
<li><p> Take <code class="reqn">n</code> bootstrap samples <code class="reqn">{\bf S}_2^*</code> of <code class="reqn">{\bf S}_2</code>, see the different strategies below.
</p>
</li>
<li><p> Recombine <code class="reqn">\bf S^* = ({\bf S}_1, {\bf S}_2^*)</code> and back-transform <code class="reqn">{\bf X}^*= {\bf S}^* {\bf W}^{-1}</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">{\bf X}^*</code>.
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>The four different bootstrapping strategies are:
</p>

<ol>
<li> <p><code>s.boot = "p"</code>: 
The first strategy is parametric and simply generates all boostrap samples independently and identically from the standard normal distribution.
</p>
</li>
<li> <p><code>s.boot = "np1"</code>: 
The second strategy is non-parametric and pools all observed <code class="reqn">n(p - k)</code> white noise observations together and draws the bootstrap samples from amongst them.
</p>
</li>
<li> <p><code>s.boot = "np2"</code>: 
The third strategy is non-parametric and proceeds otherwise as the second strategy but acts component-wise. That is, for each of the <code class="reqn">p - k</code> white noise series it pools the observed <code class="reqn">n</code> white noise observations together and draws the bootstrap samples of that particular latent series from amongst them.
</p>
</li>
<li> <p><code>s.boot = "np3"</code>: 
The third strategy is non-parametric and instead of drawing the samples univariately as in the second and third strategies, it proceeds by resampling <code class="reqn">n</code> vectors of size <code class="reqn">p - k</code> from amongst all the observed <code class="reqn">n</code> white noise vectors.
</p>
</li></ol>

<p>The function can be run in parallel by setting <code>ncores</code> to the desired number of cores (should be less than the number of cores available - 1). When running code in parallel the standard random seed of R is overridden and if a random seed needs to be set it should be passed via the argument <code>iseed</code>. The argument <code>iseed</code> has no effect in case <code>ncores</code> equals 1 (the default value).
</p>


<h3>Value</h3>

<p>A list of class ictest, inheriting from class htest, containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>The number of bootstrap samples.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>Character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The number of latent series that are not white noise used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the latent series.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>The underlying eigenvalues of the autocovariance matrix.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating AMUSE.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The used lag.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string indicating which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>s.boot</code></td>
<td>
<p>Character string denoting which bootstrapping test version was used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen, Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Matilainen, M., Nordhausen, K. and Virta, J. (2018), <em>On the Number of Signals in Multivariate Time Series</em>. In Deville, Y., Gannot, S., Mason, R., Plumbley, M.D. and  Ward, D. (editors) &quot;International Conference on Latent Variable Analysis and Signal Separation&quot;, LNCS 10891, 248&ndash;258. Springer, Cham., &lt;doi:10.1007/978-3-319-93764-9_24&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+SOBIboot">SOBIboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000

  A &lt;- matrix(rnorm(16), 4, 4)
  s1 &lt;- arima.sim(list(ar = c(0.3, 0.6)), n)
  s2 &lt;- arima.sim(list(ma = c(-0.3, 0.3)), n)
  s3 &lt;- rnorm(n)
  s4 &lt;- rnorm(n)

  S &lt;- cbind(s1, s2, s3, s4)
  X &lt;- S %*% t(A)

  boot_res_1 &lt;- AMUSEboot(X, k = 1)
  boot_res_1

  boot_res_2 &lt;- AMUSEboot(X, k = 2)
  boot_res_2

  # Plots of the estimated sources, the last two are white noise
  plot(boot_res_2)
</code></pre>

<hr>
<h2 id='AMUSEladle'>
Ladle Estimator to Estimate the Number of White Noise Components in SOS with AMUSE
</h2><span id='topic+AMUSEladle'></span><span id='topic+AMUSEladle.default'></span><span id='topic+AMUSEladle.ts'></span><span id='topic+AMUSEladle.xts'></span><span id='topic+AMUSEladle.zoo'></span>

<h3>Description</h3>

<p>The ladle estimator uses the eigenvalues and eigenvectors of an autocovariance matrix with the chosen lag to estimate the number of white noise components in second-order source separation (SOS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AMUSEladle(X, ...)

## Default S3 method:
AMUSEladle(X, tau = 1, l = 20, sim = c("geom", "fixed"), n.boot = 200, ncomp = 
           ifelse(ncol(X) &gt; 10, floor(ncol(X)/log(ncol(X))), ncol(X) - 1), ...)
## S3 method for class 'ts'
AMUSEladle(X, ...)
## S3 method for class 'xts'
AMUSEladle(X, ...)
## S3 method for class 'zoo'
AMUSEladle(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AMUSEladle_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_tau">tau</code></td>
<td>
<p>The lag for the AMUSE autocovariance matrix.</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_l">l</code></td>
<td>
<p>If <code>sim = "geom"</code> (default) then <code>l</code> is the success probability of the geometric distribution from where the bootstrap block lengths for the stationary bootstrap are drawn. If <code>sim = "fixed"</code> then <code>l</code> is the fixed block length for the fixed block bootstrap.</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_sim">sim</code></td>
<td>
<p>If <code>"geom"</code> (default) then the stationary bootstrap is used. If <code>"fixed"</code> then the fixed block bootstrap is used.</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_n.boot">n.boot</code></td>
<td>
<p>The number of bootstrapping samples. See <code><a href="boot.html#topic+tsboot">tsboot</a></code> for details.</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_ncomp">ncomp</code></td>
<td>
<p>The number of components among which the ladle estimator is to be searched. Must be between <code>0</code> and <code>ncol(X)-1</code>. The default follows the recommendation of Luo and Li (2016).</p>
</td></tr>
<tr><td><code id="AMUSEladle_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AMUSE standardizes <code>X</code> with <code class="reqn">n</code> samples and computes the eigenedcomposition of the autocovariance matrix of the standardized data for a chosen lag <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the squares of the corresponding eigenvalues of the autocovariance matrix. Under the assumption that we have <code class="reqn">k</code> non-white-noise components, the final <code class="reqn">p - k</code> eigenvalues equal zero, <code class="reqn">\lambda_{p-k} = \cdots = \lambda_{p} = 0</code>.
</p>
<p>The change point from non-zero eigenvalues to zero eigenvalues is visible in the eigenvectors of the autocovariance matrix as an increase in their boostrap variablity. Similarly, before the change point, the squared eigenvalues decrease in magnitude and afterwards they stay constant. The ladle estimate combines the scaled eigenvector bootstrap variability with the scaled eigenvalues to estimate the number of non-white-noise components. The estimate is the value of <code class="reqn">k = 0, \ldots ,</code> <code>ncomp</code> where the combined measure achieves its minimum value.
</p>


<h3>Value</h3>

<p>A list of class <code>ladle</code> containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>The string AMUSE.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The estimated number of non-white-noise components.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>The vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>Normalized eigenvalues of the AMUSE matrix.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>The name of the data for which the ladle estimate was computed.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>The main criterion for the ladle estimate - the sum of <code>fn</code> and <code>phin</code>. <code>k</code> is the value where <code>gn</code> takes its minimum.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The eigenvalues of the AMUSE matrix.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the source components. Also known as the unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating the source components.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>The used boostrapping technique, either <code>"geom"</code> or <code>"fixed"</code>.</p>
</td></tr>
<tr><td><code>lag</code></td>
<td>
<p>The used lag.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Nordhausen, K. and Virta, J.(2018), <em>Ladle Estimator for Time Series Signal Dimension</em>. In 2018 IEEE Statistical Signal Processing Workshop (SSP), pp. 428&ndash;432, &lt;doi:10.1109/SSP.2018.8450695&gt;.
</p>
<p>Luo, W. and Li, B. (2016), <em>Combining Eigenvalues and Variation of Eigenvectors for Order Determination</em>, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+SOBIladle">SOBIladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000
  
  s1 &lt;- arima.sim(n = n, list(ar = 0.6, ma = c(0, -0.4)))
  s2 &lt;- arima.sim(n = n, list(ar = c(0.4,0.1,0.3), ma = c(0.2, 0.4)))
  s3 &lt;- arima.sim(n = n, list(ar = c(0.7, 0.1)))
  Snoise &lt;- matrix(rnorm(5*n), ncol = 5)
  S &lt;- cbind(s1, s2, s3, Snoise)

  A &lt;- matrix(rnorm(64), 8, 8)
  X &lt;- S %*% t(A)
  
  ladle_AMUSE &lt;- AMUSEladle(X, l = 20, sim = "geom")

  # The estimated number of non-white-noise components
  summary(ladle_AMUSE)
  
  # The ladle plot
  ladleplot(ladle_AMUSE)
  # Using ggplot
  ggladleplot(ladle_AMUSE)
  
  # Time series plots of the estimated components
  plot(ladle_AMUSE)
</code></pre>

<hr>
<h2 id='bssvol'>
Class: bssvol
</h2><span id='topic+bssvol'></span><span id='topic+print.bssvol'></span><span id='topic+plot.bssvol'></span>

<h3>Description</h3>

<p>Class bssvol (blind source separation in stochastic volatility processes) with methods print.bssvol (prints an object of class bssvol) and plot.bss (plots an object of class bssvol).
</p>
<p>Class also inherits methods from the class bss in package <code><a href="JADE.html#topic+JADE">JADE</a></code>: for extracting the components of an object of class bssvol (<code><a href="JADE.html#topic+bss.components">bss.components</a></code>) and the coefficients of an object of class bssvol (<code><a href="JADE.html#topic+coef.bss">coef.bss</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bssvol'
print(x, ...)

## S3 method for class 'bssvol'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bssvol_+3A_x">x</code></td>
<td>
<p>An object of class bssvol.</p>
</td></tr>
<tr><td><code id="bssvol_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+JADE">JADE</a></code>, <code><a href="JADE.html#topic+bss.components">bss.components</a></code>, <code><a href="JADE.html#topic+coef.bss">coef.bss</a></code>
</p>

<hr>
<h2 id='FixNA'>
The FixNA Method for Blind Source Separation
</h2><span id='topic+FixNA'></span><span id='topic+FixNA.default'></span><span id='topic+FixNA.ts'></span><span id='topic+FixNA.xts'></span><span id='topic+FixNA.zoo'></span>

<h3>Description</h3>

<p>The FixNA (Fixed-point algorithm for maximizing the Nonlinear Autocorrelation; Shi et al., 2009) and FixNA2 (Matilainen et al., 2017) methods for blind source separation of time series with stochastic volatility. These methods are alternatives to vSOBI method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FixNA(X, ...)

## Default S3 method:
FixNA(X, k = 1:12, eps = 1e-06, maxiter = 1000, G = c("pow", "lcosh"),
      method = c("FixNA", "FixNA2"),
      ordered = FALSE, acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
FixNA(X, ...)
## S3 method for class 'xts'
FixNA(X, ...)
## S3 method for class 'zoo'
FixNA(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FixNA_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:12</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_g">G</code></td>
<td>
<p>Function <code class="reqn">G(x)</code>. The choices are <code>"pow"</code> (default) and <code>"lcosh"</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_method">method</code></td>
<td>
<p>The method to be used. The choices are <code>"FixNA"</code> (default) and <code>"FixNA2"</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="FixNA_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is the sample covariance matrix of <code class="reqn">{\bf X}</code>. The algorithm for method FixNA finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}_1({\bf U}) = \sum_{k = 1}^K {\bf D}_{1k}({\bf U})= \sum_{k = 1}^K \sum_{i = 1}^p  \frac{1}{T - k}\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_t) G({\bf u}_i' {\bf Y}_{t + k})]</code>
</p>

<p>and the algorithm for method FixNA2 finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}_2({\bf U}) = \sum_{k = 1}^K {\bf D}_{2k}({\bf U}) </code>
</p>

<p style="text-align: center;"><code class="reqn">= \sum_{k = 1}^K \sum_{i = 1}^p\left|\frac{1}{T - k}\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_t) G({\bf u}_i' {\bf Y}_{t + k})] - \left(\frac{1}{T - k}\right)^2\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_t)]\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_{t + k})]\right|.</code>
</p>

<p>where <code class="reqn">i = 1, \ldots, p</code>, <code class="reqn">k = 1, \ldots, K</code> and <code class="reqn">t = 1, \ldots, T</code>. For function <code class="reqn">G(x)</code> the choices are <code class="reqn">x^2</code> and log(cosh(<code class="reqn">x</code>)).
</p>
<p>The algorithm works iteratively starting with <code>diag(p)</code> as an initial value for an orthogonal matrix <code class="reqn">{\bf U} = ({\bf u}_1, {\bf u}_2, \ldots, {\bf u}_p)'</code>.
</p>
<p>Matrix <code class="reqn">{\bf T}_{mik}</code> is a partial derivative of <code class="reqn">{\bf D}_{mk}({\bf U})</code>, for <code class="reqn">m = 1, 2</code>, with respect to <code class="reqn">{\bf u}_i</code>.
Then <code class="reqn">{\bf T}_{mk} = ({\bf T}_{m1k}, \ldots, {\bf T}_{mpk})'</code>, where <code class="reqn">p</code> is the number of columns in <code class="reqn">{\bf Y}</code>, and <code class="reqn">{\bf T}_m = \sum_{k = 1}^K {\bf T}_{mk}</code>.
The update for the orthogonal matrix <code class="reqn">{\bf U}_{new} = ({\bf T}_m{\bf T}_m')^{-1/2}{\bf T}_m</code> is calculated at each iteration step. The algorithm stops when
</p>
<p style="text-align: center;"><code class="reqn">||{\bf U}_{new} - {\bf U}_{old}||</code>
</p>

<p>is less than <code>eps</code>.
The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen
</p>


<h3>References</h3>

<p>Hyvärinen, A. (2001), <em>Blind Source Separation by Nonstationarity of Variance: A Cumulant-Based Approach</em>, IEEE Transactions on Neural Networks, 12(6): 1471&ndash;1474.
</p>
<p>Matilainen, M., Miettinen, J., Nordhausen, K., Oja, H. and Taskinen, S. (2017), <em>On Independent Component Analysis with Stochastic Volatility Models</em>, Austrian Journal of Statistics, 46(3&ndash;4), 57&ndash;66.
</p>
<p>Shi, Z., Jiang, Z. and Zhou, F. (2009), <em> Blind Source Separation with Nonlinear Autocorrelation and Non-Gaussianity</em>, Journal of Computational and Applied Mathematics, 223(1): 908&ndash;915.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vSOBI">vSOBI</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# Simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

# Create a daily time series
X &lt;- ts(cbind(s1, s2, s3) %*% t(A), end = c(2015, 338), frequency = 365.25)

res &lt;- FixNA(X)
res
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero
}
</code></pre>

<hr>
<h2 id='gFOBI'>
Generalized FOBI
</h2><span id='topic+gFOBI'></span><span id='topic+gFOBI.default'></span><span id='topic+gFOBI.ts'></span><span id='topic+gFOBI.xts'></span><span id='topic+gFOBI.zoo'></span>

<h3>Description</h3>

<p>The gFOBI (generalized Fourth Order Blind Identification) method for blind source separation of time series with stochastic volatility. The method is a generalization of FOBI, which is a method designed for iid data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gFOBI(X, ...)

## Default S3 method:
gFOBI(X, k = 0:12, eps = 1e-06, maxiter = 100, method = c("frjd", "rjd"),
      na.action = na.fail, weight = NULL, ordered = FALSE,
      acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
gFOBI(X, ...)
## S3 method for class 'xts'
gFOBI(X, ...)
## S3 method for class 'zoo'
gFOBI(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gFOBI_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-negative integer, or a vector consisting of them. Default is <code>0:12</code>. If <code class="reqn">k = 0</code>, this method reduces to <code><a href="JADE.html#topic+FOBI">FOBI</a></code>.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_method">method</code></td>
<td>
<p>The method to use for the joint diagonalization. The options are <code><a href="JADE.html#topic+rjd">&quot;rjd&quot;</a></code> and <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>. Default is <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_na.action">na.action</code></td>
<td>
<p>A function which indicates what should happen when the data contain 'NA's. Default is to fail.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_weight">weight</code></td>
<td>
<p>A vector of length k to give weight to the different matrices in joint diagonalization. If NULL, all matrices have equal weight.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="gFOBI_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is the sample covariance matrix of <code class="reqn">{\bf X}</code>. Algorithm first calculates
</p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{B}}^{ij}_k({\bf Y}) =  \frac{1}{T - k} \sum_{t = 1}^T [{\bf Y}_{t + k} {\bf Y}_t' {\bf E}^{ij} {\bf Y}_t {\bf Y}_{t + k}']</code>
</p>

<p>where <code class="reqn">t = 1, \ldots, T</code>, and then 
</p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{B}}_k({\bf Y}) = \sum_{i = 1}^p {\bf \widehat{B}}^{ii}_k({\bf Y}).</code>
</p>

<p>for <code class="reqn">i = 1, \ldots, p</code>.
</p>
<p>The algorithm finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{k = 0}^K ||\textrm{diag}({\bf U \widehat{B}}_k({\bf Y}) {\bf U}')||^2.</code>
</p>

<p>The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen, Klaus Nordhausen
</p>


<h3>References</h3>

<p>Cardoso, J.-F. (1989), <em>Source Separation Using Higher Order Moments</em>, in: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2109&ndash;2112.
</p>
<p>Matilainen, M., Nordhausen, K. and Oja, H. (2015), <em>New Independent Component Analysis Tools for Time Series</em>, Statistics &amp; Probability Letters, 105, 80&ndash;87.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+FOBI">FOBI</a></code>, <code><a href="JADE.html#topic+rjd">frjd</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

X &lt;- cbind(s1, s2, s3) %*% t(A)

res &lt;- gFOBI(X)
res
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero
}
</code></pre>

<hr>
<h2 id='gJADE'>
Generalized JADE
</h2><span id='topic+gJADE'></span><span id='topic+gJADE.default'></span><span id='topic+gJADE.ts'></span><span id='topic+gJADE.xts'></span><span id='topic+gJADE.zoo'></span>

<h3>Description</h3>

<p>The gJADE (generalized Joint Approximate Diagonalization of Eigenmatrices) method for blind source separation of time series with stochastic volatility.
The method is a generalization of JADE, which is a method for blind source separation problem using only marginal information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gJADE(X, ...)

## Default S3 method:
gJADE(X, k = 0:12, eps = 1e-06, maxiter = 100, method = c("frjd", "rjd"),
      na.action = na.fail, weight = NULL, ordered = FALSE,
      acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
gJADE(X, ...)
## S3 method for class 'xts'
gJADE(X, ...)
## S3 method for class 'zoo'
gJADE(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gJADE_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-negative integer, or a vector consisting of them. Default is <code>0:12</code>. If <code class="reqn">k = 0</code>, this method reduces to <code><a href="JADE.html#topic+JADE">JADE</a></code>.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_method">method</code></td>
<td>
<p>The method to use for the joint diagonalization. The options are <code><a href="JADE.html#topic+rjd">&quot;rjd&quot;</a></code> and <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>. Default is <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_na.action">na.action</code></td>
<td>
<p>A function which indicates what should happen when the data contain 'NA's. Default is to fail.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_weight">weight</code></td>
<td>
<p>A vector of length k to give weight to the different matrices in joint diagonalization. If NULL, all matrices have equal weight.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="gJADE_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is the sample covariance matrix of <code class="reqn">{\bf X}</code>. The matrix <code class="reqn">{\bf \widehat{C}}^{ij}_k({\bf Y})</code> is of the form
</p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{C}}^{ij}_k({\bf Y}) = {\bf \widehat{B}}^{ij}_k({\bf Y}) - {\bf S}_k({\bf Y}) ({\bf E}^{ij} + {\bf E}^{ji}) {\bf S}_k({\bf Y})' - \textrm{trace}({\bf E}^{ij}) {\bf I}_p,</code>
</p>

<p>for <code class="reqn">i, j = 1, \ldots, p</code>, where <code class="reqn">{\bf S}_k({\bf Y})</code> is the lagged sample covariance matrix of <code class="reqn">{\bf Y}</code> for lag <code class="reqn">k = 1, \ldots, K</code>, <code class="reqn">{\bf E}^{ij}</code> is a matrix where element <code class="reqn">(i,j)</code> equals to 1 and all other elements are 0, <code class="reqn">{\bf I}_p</code> is an identity matrix of order <code class="reqn">p</code> and <code class="reqn">{\bf \widehat{B}}^{ij}_k({\bf Y})</code> is as in <code><a href="#topic+gFOBI">gFOBI</a></code>.
</p>
<p>The algorithm finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i = 1}^p \sum_{j = 1}^p \sum_{k = 0}^K ||diag({\bf U \widehat{C}}^{ij}_k({\bf Y}) {\bf U}')||^2.</code>
</p>

<p>where <code class="reqn">k = 1, \ldots, K</code>.
The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Markus Matilainen
</p>


<h3>References</h3>

<p>Cardoso, J.-F., Souloumiac, A. (1993), <em>Blind Beamforming for Non-Gaussian Signals</em>, in: IEE-Proceedings-F, volume 140, pp. 362&ndash;370.
</p>
<p>Matilainen, M., Nordhausen, K. and Oja, H. (2015), <em>New Independent Component Analysis Tools for Time Series</em>, Statistics &amp; Probability Letters, 105, 80&ndash;87.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+rjd">frjd</a></code>, <code><a href="JADE.html#topic+JADE">JADE</a></code>, <code><a href="#topic+gFOBI">gFOBI</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

X &lt;- cbind(s1, s2, s3) %*% t(A)

res &lt;- gJADE(X)
res
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero
}
</code></pre>

<hr>
<h2 id='gSOBI'>
Generalized SOBI
</h2><span id='topic+gSOBI'></span><span id='topic+gSOBI.default'></span><span id='topic+gSOBI.ts'></span><span id='topic+gSOBI.xts'></span><span id='topic+gSOBI.zoo'></span>

<h3>Description</h3>

<p>The gSOBI (generalized Second Order Blind Identification) method for the blind source separation (BSS) problem. The method is designed for separating multivariate time series with or without stochastic volatility. The method is a combination of SOBI and vSOBI with <code class="reqn">G(x) = x^2</code> as a nonlinearity function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gSOBI(X, ...)

## Default S3 method:
gSOBI(X, k1 = 1:12, k2 = 1:3, b = 0.9, eps = 1e-06, maxiter = 1000, ordered = FALSE,
      acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
gSOBI(X, ...)
## S3 method for class 'xts'
gSOBI(X, ...)
## S3 method for class 'zoo'
gSOBI(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gSOBI_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_k1">k1</code></td>
<td>
<p>A vector of lags for SOBI part. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:12</code>.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_k2">k2</code></td>
<td>
<p>A vector of lags for vSOBI part. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:3</code>.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_b">b</code></td>
<td>
<p>The weight for the SOBI part, <code class="reqn">1-b</code> for the vSOBI part. Default is 0.9.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="gSOBI_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is the sample covariance matrix of <code class="reqn">{\bf X}</code>. The algorithm finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}({\bf U}) = b\sum_{k_1 = 1}^{K_1} {\bf D}_{k_1}({\bf U}) + (1 - b)\sum_{k_2 = 1}^{K_2} {\bf D}_{k_2}({\bf U}),</code>
</p>

<p>where SOBI part
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}_{k_1} = \sum_{i = 1}^p  \left(\frac{1}{T - k_1}\sum_{t=1}^{T - k_1}[({\bf u}_i' {\bf Y}_t) ({\bf u}_i' {\bf Y}_{t + k_1})]\right)^2.</code>
</p>

<p>and vSOBI part
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}_{k_2} = \sum_{i = 1}^p  \left(\frac{1}{T - k_2}\sum_{t=1}^{T - k_2}[({\bf u}_i' {\bf Y}_t)^2 ({\bf u}_i' {\bf Y}_{t + k_2})^2] - \left(\frac{1}{T - k_2}\right)^2\sum_{t=1}^{T - k_2}[({\bf u}_i' {\bf Y}_t)^2]\sum_{t=1}^{T - k_2}[({\bf u}_i' {\bf Y}_{t + k_2})^2]\right)^2</code>
</p>

<p>where <code class="reqn">b \in [0, 1].</code>
is a value between 0 and 1, and <code class="reqn">i = 1, \ldots, p</code>, <code class="reqn">k_1 = 1, \ldots, K_1</code>, <code class="reqn">k_2 = 1, \ldots, K_2</code> and <code class="reqn">t = 1, \ldots, T</code>
</p>
<p>The algorithm works iteratively starting with <code>diag(p)</code> as an initial value for an orthogonal matrix <code class="reqn">{\bf U} = ({\bf u}_1, {\bf u}_2, \ldots, {\bf u}_p)'</code>.
</p>
<p>Matrix <code class="reqn">{\bf T}_{ikj}</code> is a partial derivative of <code class="reqn">{\bf D}_{kj}({\bf U})</code>, where <code class="reqn">j = 1, 2</code>, with respect to <code class="reqn">{\bf u}_i</code>.
Then <code class="reqn">{\bf T}_{kj} = ({\bf T}_{1kj}, \ldots, {\bf T}_{pkj})'</code>, where <code class="reqn">p</code> is the number of columns in <code class="reqn">\bf Y</code>, and <code class="reqn">{\bf T}_j = \sum_{k_j = 1}^{K_j} {\bf T}_{kj}</code>, for <code class="reqn">j = 1, 2</code>. Finally <code class="reqn">{\bf T} = b{\bf T}_1 + (1-b){\bf T}_2</code>.
</p>
<p>The update for the orthogonal matrix <code class="reqn">{\bf U}_{new} = ({\bf TT}')^{-1/2}{\bf T}</code> is calculated at each iteration step. The algorithm stops when
</p>
<p style="text-align: center;"><code class="reqn">||{\bf U}_{new} - {\bf U}_{old}||</code>
</p>

<p>is less than <code>eps</code>.
The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k1</code></td>
<td>
<p>The vector of the used lags for the SOBI part.</p>
</td></tr>
<tr><td><code>k2</code></td>
<td>
<p>The vector of the used lags for the vSOBI part.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen, Jari Miettinen
</p>


<h3>References</h3>

<p>Belouchrani, A., Abed-Meriam, K., Cardoso, J.F. and Moulines, R. (1997), <em>A Blind Source Separation Technique Using Second-Order Statistics</em>, IEEE Transactions on Signal Processing, 434&ndash;444.
</p>
<p>Matilainen, M., Miettinen, J., Nordhausen, K., Oja, H. and Taskinen, S. (2017), <em>On Independent Component Analysis with Stochastic Volatility Models</em>, Austrian Journal of Statistics, 46(3&ndash;4), 57&ndash;66.
</p>
<p>Miettinen, M., Matilainen, M., Nordhausen, K. and Taskinen, S. (2020), <em>Extracting Conditionally Heteroskedastic Components Using Independent Component Analysis</em>, Journal of Time Series Analysis, 41, 293&ndash;311.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+vSOBI">vSOBI</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

# create a daily time series
X &lt;- ts(cbind(s1, s2, s3) %*% t(A), end = c(2015, 338), frequency = 365.25)

res &lt;- gSOBI(X, 1:4, 1:2, 0.99)
res$W
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero

# xts series as input
library("xts")
data(sample_matrix)
X2 &lt;- as.xts(sample_matrix)
res2 &lt;- gSOBI(X2, 1:4, 1:2, 0.99)
plot(res2, multi.panel = TRUE)

# zoo series as input
X3 &lt;- as.zoo(X)
res3 &lt;- gSOBI(X3, 1:4, 1:2, 0.99)
plot(res3)
}
</code></pre>

<hr>
<h2 id='lbtest'>
Modified Ljung-Box Test and Volatility Clustering Test for Time Series.
</h2><span id='topic+lbtest'></span><span id='topic+print.lbtest'></span>

<h3>Description</h3>

<p>Modified Ljung-Box test and volatility clustering test for time series. Time series can be univariate or multivariate. The modified Ljung-Box test checks whether there is linear autocorrelation in the time series. The volatility clustering test checks whether the time series has squared autocorrelation, which would indicate a presence of volatility clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lbtest(X, k, type = c("squared", "linear"))

## S3 method for class 'lbtest'
print(x, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lbtest_+3A_x">X</code></td>
<td>
<p>A numeric vector/matrix or a univariate/multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="lbtest_+3A_k">k</code></td>
<td>
<p>A vector of lags.</p>
</td></tr>
<tr><td><code id="lbtest_+3A_type">type</code></td>
<td>
<p>The type of the autocorrelation test. Options are Modified Ljung-Box test (<code>"linear"</code>) or volatility clustering test (<code>"squared"</code>) autocorrelation. Default is <code>"squared"</code>.</p>
</td></tr>
</table>
<p>In methods for class 'lbtest' only:
</p>
<table>
<tr><td><code id="lbtest_+3A_x">x</code></td>
<td>
<p>An object of class lbtest</p>
</td></tr>
<tr><td><code id="lbtest_+3A_digits">digits</code></td>
<td>
<p>The number of digits when printing an object of class lbtest. Default is 3</p>
</td></tr>
<tr><td><code id="lbtest_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume all the individual time series <code class="reqn">X_i</code> in <code class="reqn">\bf X</code> with <code class="reqn">T</code> observations are scaled to have variance 1.
</p>
<p>Then the modified Ljung-Box test statistic for testing the existence of linear autocorrelation in <code class="reqn">X_i</code> (<code>option = "linear"</code>) is
</p>
<p style="text-align: center;"><code class="reqn">T \sum_{j \in k} \left(\sum_{t=1}^T (X_{it} X_{i, t + j})/(T - j)\right)^2/V_{j}.</code>
</p>

<p>Here  
</p>
<p style="text-align: center;"><code class="reqn">V_{j} = \sum_{t=1}^{n-j}\frac{x_t^2 x_{t+j}^2}{n-j} + 2 \sum_{k=1}^{n-j-1} \frac{n-k}{n} \sum_{s=1}^{n-k-j}\frac{x_s x_{s+j }x_{s+k} x_{s+k+j}}{n-k-j}.</code>
</p>

<p>where <code class="reqn">t = 1, \ldots, n - j</code>, <code class="reqn">k = 1, \ldots, n - j - 1</code> and <code class="reqn">s = 1, \ldots, n - k - j</code>.
</p>
<p>The volatility clustering test statistic (<code>option = "squared"</code>) is
</p>
<p style="text-align: center;"><code class="reqn">T \sum_{j \in k} \left(\sum_{t=1}^T (X_{it}^2 X_{i, t + j}^2)/(T - j) - 1\right)^2</code>
</p>

<p>Test statistic related to each time series <code class="reqn">X_i</code> is then compared to <code class="reqn">\chi^2</code>-distribution with <code>length(k)</code> degrees of freedom, and the corresponding p-values are produced. Small p-value indicates the existence of autocorrelation.
</p>


<h3>Value</h3>

<p>A list of class 'lbtest' containing the following components:
</p>
<table>
<tr><td><code>TS</code></td>
<td>
<p>The values of the test statistic for each component of X as a vector.</p>
</td></tr>
<tr><td><code>p_val</code></td>
<td>
<p>The p-values based on the test statistic for each component of X as a vector.</p>
</td></tr>
<tr><td><code>Xname</code></td>
<td>
<p>The name of the data used as a character string.</p>
</td></tr>
<tr><td><code>varnames</code></td>
<td>
<p>The names of the variables used as a character string vector.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The lags used for testing the serial autocorrelation as a vector.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>The total number of lags used for testing the serial autocorrelation.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>The type of the autocorrelation test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen, Jari Miettinen
</p>


<h3>References</h3>

<p>Miettinen, M., Matilainen, M., Nordhausen, K. and Taskinen, S. (2020), <em>Extracting Conditionally Heteroskedastic Components Using Independent Component Analysis</em>, Journal of Time Series Analysis, 41, 293&ndash;311.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FixNA">FixNA</a></code>, <code><a href="#topic+gFOBI">gFOBI</a></code>, <code><a href="#topic+gJADE">gJADE</a></code>, <code><a href="#topic+vSOBI">vSOBI</a></code>, <code><a href="#topic+gSOBI">gSOBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
s1 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.1)$y
s2 &lt;- rnorm(n)
S &lt;- cbind(s1, s2)

lbtest(S, 1:3, type = "squared")
# First p-value should be very close to zero, as there exists stochastic volatility
}
</code></pre>

<hr>
<h2 id='PVC'>
A Modified Algorithm for Principal Volatility Component Estimator
</h2><span id='topic+PVC'></span><span id='topic+PVC.default'></span><span id='topic+PVC.ts'></span><span id='topic+PVC.xts'></span><span id='topic+PVC.zoo'></span>

<h3>Description</h3>

<p>PVC (Principal Volatility Component) estimator for the blind source separation (BSS) problem. This method is a modified version of PVC by Hu and Tsay (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVC(X, ...)

## Default S3 method:
PVC(X, k = 1:12, ordered = FALSE, acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
PVC(X, ...)
## S3 method for class 'xts'
PVC(X, ...)
## S3 method for class 'zoo'
PVC(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVC_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="PVC_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:12</code>.</p>
</td></tr>
<tr><td><code id="PVC_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="PVC_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="PVC_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="PVC_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="PVC_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">\bf S</code> is the sample covariance matrix of <code class="reqn">\bf X</code>.
Then for each lag <code class="reqn">k</code> we calculate
</p>
<p style="text-align: center;"><code class="reqn">\widehat{Cov}({\bf Y}_t {\bf Y}_t', Y_{ij, t-k}) = \frac{1}{T}\sum_{t = k + 1}^T \left({\bf Y}_t {\bf Y}_t' - \frac{1}{T-k}\sum_{t = k+1}^T {\bf Y}_t {\bf Y}_t' \right)\left(Y_{ij, t-k} - \frac{1}{T-k}\sum_{t = k+1}^T {Y}_{ij, t-k}\right),</code>
</p>

<p>where <code class="reqn">t = k + 1, \ldots, T</code> and <code class="reqn">Y_{ij, t-k} = Y_{i, t-k} Y_{j, t-k}, i, j = 1, \ldots, p</code>.
Then 
</p>
<p style="text-align: center;"><code class="reqn">{\bf g}_k({\bf Y}) = \sum_{i = 1}^p \sum_{j=1}^p (\widehat{Cov}({\bf Y}_t {\bf Y}_t', Y_{ij, t-k}))^2.</code>
</p>

<p>where <code class="reqn">i,j = 1, \ldots, p.</code>
Thus the generalized kurtosis matrix is 
</p>
<p style="text-align: center;"><code class="reqn">{\bf G}_K({\bf Y}) = \sum_{k = 1}^K {\bf g}_k({\bf Y}),</code>
</p>

<p>where <code class="reqn">k = 1, \ldots, K</code> is the set of chosen lags.
Then <code class="reqn">\bf U</code> is the matrix with eigenvectors of <code class="reqn">{\bf G}_K({\bf Y})</code> as its rows.
The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>, where the average value of each row is set to be positive.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jari Miettinen, Markus Matilainen
</p>


<h3>References</h3>

<p>Miettinen, M., Matilainen, M., Nordhausen, K. and Taskinen, S. (2020), <em>Extracting Conditionally Heteroskedastic Components Using Independent Component Analysis</em>, Journal of Time Series Analysis,41, 293&ndash;311.
</p>
<p>Hu, Y.-P. and Tsay, R. S. (2014), <em>Principal Volatility Component Analysis</em>, Journal of Business &amp; Economic Statistics, 32(2), 153&ndash;164.
</p>


<h3>See Also</h3>

<p><code><a href="MTS.html#topic+comVol">comVol</a></code>, <code><a href="#topic+gSOBI">gSOBI</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# Simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

# Create a daily time series
X &lt;- ts(cbind(s1, s2, s3) %*% t(A), end = c(2015, 338), frequency = 365.25)

res &lt;- PVC(X)
res
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero
}
</code></pre>

<hr>
<h2 id='SOBIasymp'>
Second-order Separation Sub-White-Noise Asymptotic Testing with SOBI
</h2><span id='topic+SOBIasymp'></span><span id='topic+SOBIasymp.default'></span><span id='topic+SOBIasymp.ts'></span><span id='topic+SOBIasymp.xts'></span><span id='topic+SOBIasymp.zoo'></span>

<h3>Description</h3>

<p>The function uses SOBI (Second Order Blind Identification) to test whether the last <code>p-k</code> latent series are pure white noise, assuming a p-variate second-order stationary blind source separation (BSS) model. The test is asymptotic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOBIasymp(X, ...)

## Default S3 method:
SOBIasymp(X, k, tau = 1:12, eps = 1e-06, maxiter = 200, ...)
## S3 method for class 'ts'
SOBIasymp(X, ...)
## S3 method for class 'xts'
SOBIasymp(X, ...)
## S3 method for class 'zoo'
SOBIasymp(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOBIasymp_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="SOBIasymp_+3A_k">k</code></td>
<td>
<p>The number of latent series that are not white noise. Can be between <code class="reqn">0</code> and <code class="reqn">p-1</code>.</p>
</td></tr>
<tr><td><code id="SOBIasymp_+3A_tau">tau</code></td>
<td>
<p>The lags for the SOBI autocovariance matrices.</p>
</td></tr>
<tr><td><code id="SOBIasymp_+3A_eps">eps</code></td>
<td>
<p>The convergence tolerance for the joint diagonalization.</p>
</td></tr>
<tr><td><code id="SOBIasymp_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations for the joint diagonalization.</p>
</td></tr>
<tr><td><code id="SOBIasymp_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SOBI standardizes <code>X</code> with <code class="reqn">n</code> samples and jointly diagonalizes the autocovariance matrices of the standardized data for a chosen set of lags <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the sums of squares of the corresponding &quot;eigenvalues&quot; produced by the joint diagonalization. Under the null hypothesis the lower right corner <code class="reqn">(p - k) \times (p - k)</code> blocks of the autocovariance matrices of the sources are zero matrices and the sum <code class="reqn">m</code> of their squared norms over all lags can be used as a test statistic in inference on the true number of latent white noise series.
</p>
<p>This function conducts the hypothesis test using the asymptotic null distribution of <code class="reqn">m</code>, a chi-squared distribution with <code class="reqn">T(p - k)(p - k + 1)/2</code> degrees of freedom where <code class="reqn">T</code> is the number of autocovariance matrices used by SOBI.
</p>


<h3>Value</h3>

<p>A list of class ictest, inheriting from class htest, containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>The degrees of freedom of the asymptotic null distribution.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string indicating which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>Character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The number of latent series that are not white noise used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the latent series.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>The underlying eigenvalues of the autocovariance matrix.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating SOBI.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The used set of lags for the SOBI autocovariance matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Virta, J. and Nordhausen, K. (2021), <em>Determining the Signal Dimension in Second Order Source Separation</em>. Statistica Sinica, 31, 135&ndash;156.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+AMUSEasymp">AMUSEasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000

  A &lt;- matrix(rnorm(16), 4, 4)
  s1 &lt;- arima.sim(list(ar = c(0, 0.6)), n)
  s2 &lt;- arima.sim(list(ma = c(0, -0.5)), n)
  s3 &lt;- rnorm(n)
  s4 &lt;- rnorm(n)

  S &lt;- cbind(s1, s2, s3, s4)
  X &lt;- S %*% t(A)

  asymp_res_1 &lt;- SOBIasymp(X, k = 1)
  asymp_res_1

  asymp_res_2 &lt;- SOBIasymp(X, k = 2)
  asymp_res_2

  # Plots of the estimated sources, the last two are white noise
  plot(asymp_res_2)
  
  # Note that AMUSEasymp with lag 1 does not work due to the lack of short range dependencies
  AMUSEasymp(X, k = 1)
</code></pre>

<hr>
<h2 id='SOBIboot'>
Second-order Separation Sub-White-Noise Bootstrap Testing with SOBI
</h2><span id='topic+SOBIboot'></span><span id='topic+SOBIboot.default'></span><span id='topic+SOBIboot.ts'></span><span id='topic+SOBIboot.xts'></span><span id='topic+SOBIboot.zoo'></span>

<h3>Description</h3>

<p>The function uses SOBI (Second Order Blind Identification) to test whether the last <code>p-k</code> latent series are pure white noise, assuming a p-variate second-order stationary blind source separation (BSS) model. Four different bootstrapping strategies are available and the function can be run in parallel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOBIboot(X, ...)

## Default S3 method:
SOBIboot(X, k, tau = 1:12, n.boot = 200, s.boot = c("p", "np1", "np2", "np3"),
         ncores = NULL, iseed = NULL, eps = 1e-06, maxiter = 200, ...)
## S3 method for class 'ts'
SOBIboot(X, ...)
## S3 method for class 'xts'
SOBIboot(X, ...)
## S3 method for class 'zoo'
SOBIboot(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOBIboot_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_k">k</code></td>
<td>
<p>The number of latent series that are not white noise. Can be between <code class="reqn">0</code> and <code class="reqn">p-1</code>.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_tau">tau</code></td>
<td>
<p>The vector of lags for the SOBI autocovariance matrices.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_n.boot">n.boot</code></td>
<td>
<p>The number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_s.boot">s.boot</code></td>
<td>
<p>Bootstrapping strategy to be used. Possible values are <code>"p"</code> (default), <code>"np1"</code>, <code>"np2"</code>, <code>"np3"</code>. See details for further information.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to be used. If <code>NULL</code> or 1, no parallel computing is used. Otherwise <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code> with <code>type = "PSOCK"</code> is used. It is the users repsonsibilty to choose a reasonable value for <code>ncores</code>. The function <code><a href="parallel.html#topic+detectCores">detectCores</a></code> might be useful in this context.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_iseed">iseed</code></td>
<td>
<p>If parallel computation is used, the seed passed on to <code><a href="parallel.html#topic+clusterSetRNGStream">clusterSetRNGStream</a></code>. Default is <code>NULL</code> which means no fixed seed is used.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_eps">eps</code></td>
<td>
<p>The convergence tolerance for the joint diagonalization.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations for the joint diagonalization.</p>
</td></tr>
<tr><td><code id="SOBIboot_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SOBI standardizes <code>X</code> with <code class="reqn">n</code> samples and jointly diagonalizes the autocovariance matrices of the standardized data for a chosen set of lags <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the sums of squares of the corresponding &quot;eigenvalues&quot; produced by the joint diagonalization. Under the null hypothesis the final <code class="reqn">p - k</code> &quot;eigenvalues&quot; of each of the autocovariance matrices equal zero, <code class="reqn">\lambda^\tau_{p-k} = \cdots = \lambda^\tau_{p} = 0</code>, and their mean square <code class="reqn">m</code> over all lags can be used as a test statistic in bootstrap-based inference on the true number of latent white noise series.
</p>
<p>The function offers four different bootstrapping strategies for generating samples for which the null hypothesis approximately holds, and they are all based on the following general formula:
</p>

<ol>
<li><p> Decompose the SOBI-estimated latent series <code class="reqn">\bf S</code> into the postulated signal <code class="reqn">{\bf S}_1</code> and white noise <code class="reqn">{\bf S}_2</code>.
</p>
</li>
<li><p> Take <code class="reqn">n</code> bootstrap samples <code class="reqn">{\bf S}_2^*</code> of <code class="reqn">{\bf S}_2</code>, see the different strategies below.
</p>
</li>
<li><p> Recombine <code class="reqn">\bf S^* = ({\bf S}_1, {\bf S}_2^*)</code> and back-transform <code class="reqn">{\bf X}^*= {\bf S}^* {\bf W}^{-1}</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">{\bf X}^*</code>.
</p>
</li></ol>

<p>The four different bootstrapping strategies are:
</p>

<ol>
<li> <p><code>s.boot = "p"</code>: 
The first strategy is parametric and simply generates all boostrap samples independently and identically from the standard normal distribution.
</p>
</li>
<li> <p><code>s.boot = "np1"</code>: 
The second strategy is non-parametric and pools all observed <code class="reqn">n(p - k)</code> white noise observations together and draws the bootstrap samples from amongst them.
</p>
</li>
<li> <p><code>s.boot = "np2"</code>: 
The third strategy is non-parametric and proceeds otherwise as the second strategy but acts component-wise. That is, separately for each of the <code class="reqn">p - k</code> white noise series it pools the observed <code class="reqn">n</code> white noise observations together and draws the bootstrap samples of that particular latent series from amongst them.
</p>
</li>
<li> <p><code>s.boot = "np3"</code>: 
The third strategy is non-parametric and instead of drawing the samples univariately as in the second and third strategies, proceeds by resampling <code class="reqn">n</code> vectors of size <code class="reqn">p - k</code> from amongst all the observed <code class="reqn">n</code> white noise vectors.
</p>
</li></ol>

<p>The function can be run in parallel by setting <code>ncores</code> to the desired number of cores (should be less than the number of cores available - 1). When running code in parallel the standard random seed of R is overridden and if a random seed needs to be set it should be passed via the argument <code>iseed</code>. The argument <code>iseed</code> has no effect in case <code>ncores</code> equals 1 (the default value).
</p>
<p>This function uses for the joint diagonalization the function <code><a href="JADE.html#topic+rjd">frjd.int</a></code>, which does not fail in case of failed convergence but returns the estimate from the final step.
</p>


<h3>Value</h3>

<p>A list of class ictest, inheriting from class htest, containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>The number of bootstrap samples.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>Character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The number of latent series that are not white noise used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the latent series.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>The underlying eigenvalues of the autocovariance matrix.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating SOBI.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The used set of lags.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string indicating which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>s.boot</code></td>
<td>
<p>Character string denoting which bootstrapping test version was used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen, Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Matilainen, M., Nordhausen, K. and Virta, J. (2018), <em>On the Number of Signals in Multivariate Time Series</em>. In Deville, Y., Gannot, S., Mason, R., Plumbley, M.D. and  Ward, D. (editors) &quot;International Conference on Latent Variable Analysis and Signal Separation&quot;, LNCS 10891, 248&ndash;258. Springer, Cham., &lt;doi:10.1007/978-3-319-93764-9_24&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="#topic+AMUSEboot">AMUSEboot</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="boot.html#topic+tsboot">tsboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000
  
  A &lt;- matrix(rnorm(16), 4, 4)
  s1 &lt;- arima.sim(list(ar = c(0, 0, 0.3, 0.6)), n)
  s2 &lt;- arima.sim(list(ma = c(0, 0, -0.3, 0.3)), n)
  s3 &lt;- rnorm(n)
  s4 &lt;- rnorm(n)
  
  S &lt;- cbind(s1, s2, s3, s4)
  X &lt;- S %*% t(A)
  
  boot_res_1 &lt;- SOBIboot(X, k = 1)
  boot_res_1
  
  boot_res_2 &lt;- SOBIboot(X, k = 2)
  boot_res_2

  # Plots of the estimated sources, the last two are white noise
  plot(boot_res_2)

  # Note that AMUSEboot with lag 1 does not work due to the lack of short range dependencies
  AMUSEboot(X, k = 1)
  
  # xts series as input
  library("xts")
  data(sample_matrix)
  X2 &lt;- as.xts(sample_matrix)
  boot_res_xts &lt;- SOBIboot(X2, k = 2)
  plot(boot_res_xts, multi.panel = TRUE)

  # zoo series as input
  X3 &lt;- as.zoo(X)
  boot_res_zoo &lt;- SOBIboot(X3, k = 2)
  plot(boot_res_zoo)

</code></pre>

<hr>
<h2 id='SOBIladle'>
Ladle Estimator to Estimate the Number of White Noise Components in SOS with SOBI
</h2><span id='topic+SOBIladle'></span><span id='topic+SOBIladle.default'></span><span id='topic+SOBIladle.ts'></span><span id='topic+SOBIladle.xts'></span><span id='topic+SOBIladle.zoo'></span>

<h3>Description</h3>

<p>The ladle estimator uses the joint diagonalization &quot;eigenvalues&quot; and &quot;eigenvectors&quot; of several autocovariance matrices to estimate the number of white noise components in second-order source separation (SOS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOBIladle(X, ...)

## Default S3 method:
SOBIladle(X, tau = 1:12, l = 20, sim = c("geom", "fixed"), n.boot = 200, ncomp =
          ifelse(ncol(X) &gt; 10, floor(ncol(X)/log(ncol(X))), ncol(X) - 1),
          maxiter = 1000, eps = 1e-06, ...)
## S3 method for class 'ts'
SOBIladle(X, ...)
## S3 method for class 'xts'
SOBIladle(X, ...)
## S3 method for class 'zoo'
SOBIladle(X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOBIladle_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_tau">tau</code></td>
<td>
<p>The lags for the SOBI autocovariance matrices.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_l">l</code></td>
<td>
<p>If <code>sim = "geom"</code> then <code>l</code> is the success probability of the geometric distribution from where the bootstrap block lengths for the stationary bootstrap are drawn. If <code>sim = "fixed"</code> then <code>l</code> is the fixed block length for the fixed block bootstrap.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_sim">sim</code></td>
<td>
<p>If <code>"geom"</code> (default) then the stationary bootstrap is used. If <code>"fixed"</code> then the fixed block bootstrap is used.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_n.boot">n.boot</code></td>
<td>
<p>The number of bootstrapping samples. See <code><a href="boot.html#topic+tsboot">tsboot</a></code> for details.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_ncomp">ncomp</code></td>
<td>
<p>The number of components among which the ladle estimator is to be searched. Must be between <code>0</code> and <code>ncol(X)-1</code>. The default follows the recommendation of Luo and Li (2016).</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="SOBIladle_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SOBI standardizes <code>X</code> with <code class="reqn">n</code> samples and jointly diagonalizes the autocovariance matrices of the standardized data for a chosen set of lags <code>tau</code>, yielding a transformation <code class="reqn">\bf W</code> giving the latent variables as <code class="reqn">{\bf S} = {\bf X} {\bf W}</code>. Assume, without loss of generality, that the latent components are ordered in decreasing order with respect to the sums of squares of the corresponding &quot;eigenvalues&quot; produced by the joint diagonalization.
Under the assumption that we have <code class="reqn">k</code> non-white-noise components, the final <code class="reqn">p - k</code> &quot;eigenvalues&quot; of each of the autocovariance matrices equal zero, <code class="reqn">\lambda^\tau_{p-k} = \cdots = \lambda^\tau_{p} = 0</code>.
</p>
<p>The change point from non-zero eigenvalues to zero eigenvalues is visible in the joint diagonalization &quot;eigenvectors&quot; of the autocovariance matrices as an increase in their boostrap variablity. Similarly, before the change point, the squared eigenvalues decrease in magnitude and afterwards they stay constant. The ladle estimate combines the scaled eigenvector bootstrap variability with the scaled eigenvalues to estimate the number of non-white-noise components. The estimate is the value of <code class="reqn">k = 0, \ldots ,</code> <code>ncomp</code> where the combined measure achieves its minimum value. 
</p>
<p>This function uses for the joint diagonalization the function <code><a href="JADE.html#topic+rjd">frjd.int</a></code>, which does not fail in case of failed convergence but returns the estimate from the final step.
</p>


<h3>Value</h3>

<p>A list of class <code>ladle</code> containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>The string SOBI.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The estimated number of non-white-noise components.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>The vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>Normalized sums of squared eigenvalues of the SOBI matrices.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>The name of the data for which the ladle estimate was computed.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>The main criterion for the ladle estimate - the sum of <code>fn</code> and <code>phin</code>. <code>k</code> is the value where <code>gn</code> takes its minimum.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The sums of squared eigenvalues of the SOBI matrices.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The transformation matrix to the source components. Also known as the unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Multivariate time series with the centered source components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The location of the data which was subtracted before calculating the source components.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>The used boostrapping technique, either <code>"geom"</code> or <code>"fixed"</code>.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The used set of lags for the SOBI autocovariance matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p>Nordhausen, K. and Virta, J.(2018), <em>Ladle Estimator for Time Series Signal Dimension</em>. In 2018 IEEE Statistical Signal Processing Workshop (SSP), pp. 428&ndash;432, &lt;doi:10.1109/SSP.2018.8450695&gt;.
</p>
<p>Luo, W. and Li, B. (2016), <em>Combining Eigenvalues and Variation of Eigenvectors for Order Determination</em>, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+AMUSEladle">AMUSEladle</a></code>, <code><a href="JADE.html#topic+rjd">frjd.int</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000
  
  s1 &lt;- arima.sim(n = n, list(ar = 0.6, ma = c(0, -0.4)))
  s2 &lt;- arima.sim(n = n, list(ar = c(0, 0.1,0.3), ma = c(0.2, 0.4)))
  s3 &lt;- arima.sim(n = n, list(ar = c(0, 0.8)))
  Snoise &lt;- matrix(rnorm(5*n), ncol = 5)
  S &lt;- cbind(s1, s2, s3, Snoise)

  A &lt;- matrix(rnorm(64), 8, 8)
  X &lt;- S %*% t(A)
  
  ladle_SOBI &lt;- SOBIladle(X, l = 20, sim = "geom")

  # The estimated number of non-white-noise components
  summary(ladle_SOBI)
  
  # The ladle plot
  ladleplot(ladle_SOBI)
  
  # Time series plots of the estimated components
  plot(ladle_SOBI)
  
  # Note that AMUSEladle with lag 1 does not work due to the lack of short range dependencies
  ladle_AMUSE &lt;- AMUSEladle(X)

  summary(ladle_AMUSE)
  ladleplot(ladle_AMUSE)
  
  # xts series as input
  library("xts")
  data(sample_matrix)
  X2 &lt;- as.xts(sample_matrix)
  ladle_SOBI_xts &lt;- SOBIladle(X2, l = 20, sim = "geom")
  plot(ladle_SOBI_xts, multi.panel = TRUE)

  # zoo series as input
  X3 &lt;- as.zoo(X)
  ladle_SOBI_zoo &lt;- SOBIladle(X3, l = 20, sim = "geom")
  plot(ladle_SOBI_zoo)
</code></pre>

<hr>
<h2 id='summary.tssdr'>
Summary of an Object of Class tssdr
</h2><span id='topic+summary.tssdr'></span><span id='topic+print.summary.tssdr'></span><span id='topic+components.summary.tssdr'></span><span id='topic+coef.summary.tssdr'></span><span id='topic+plot.summary.tssdr'></span>

<h3>Description</h3>

<p>Gives a summary of an object of class tssdr. It includes different types of methods to select the number of directions (sources) and lags.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tssdr'
summary(object, type = c("rectangle", "alllag", "alldir", "big"), thres = 0.8, ...)

## S3 method for class 'summary.tssdr'
print(x, digits = 3, ...)
## S3 method for class 'summary.tssdr'
components(x, ...)
## S3 method for class 'summary.tssdr'
coef(object, ...)
## S3 method for class 'summary.tssdr'
plot(x, main = "The response and the chosen directions", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.tssdr_+3A_object">object</code></td>
<td>
<p>An object of class tssdr.</p>
</td></tr>
<tr><td><code id="summary.tssdr_+3A_type">type</code></td>
<td>
<p>Method for choosing the important lags and directions. The choices are <code>"rectangle"</code>, <code>"alllag"</code>, <code>"alldir"</code> and <code>"big"</code>. Default is <code>"rectangle"</code>.</p>
</td></tr>
<tr><td><code id="summary.tssdr_+3A_thres">thres</code></td>
<td>
<p>The threshold value for choosing the lags and directions. Default is <code>0.8</code>.</p>
</td></tr>
<tr><td><code id="summary.tssdr_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>
<p><strong>In methods for class 'summary.tssdr' only:</strong>
</p>
<table>
<tr><td><code id="summary.tssdr_+3A_x">x</code></td>
<td>
<p>An object of class summary.tssdr</p>
</td></tr>
<tr><td><code id="summary.tssdr_+3A_digits">digits</code></td>
<td>
<p>The number of digits when printing an object of class summary.tssdr. Default is 3</p>
</td></tr>
<tr><td><code id="summary.tssdr_+3A_main">main</code></td>
<td>
<p>A title for a plot when printing an object of class summary.tssdr.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sum of values of <code class="reqn">k_0 \times p_0</code> matrix <code class="reqn">\bf L</code> of <code>object</code> is 1. The values of the matrix are summed together in ways detailed below, until the value is at least <code class="reqn">\pi</code> (<code>thres</code>). Let <code class="reqn">\lambda_{ij}</code> be the element <code class="reqn">(i, j)</code> of the matrix <code class="reqn">\bf L</code>.
</p>
<p>For <code>alllag</code>: <code class="reqn">k = k_0</code> and <code class="reqn">p</code> is the smallest value for which <code class="reqn">\sum_{i = 1}^p \lambda_{ij} \ge \pi.</code> where <code class="reqn">i = 1, \ldots, p</code>. The chosen number of lags and directions are returned.
</p>
<p>For <code>alldir</code>: <code class="reqn">p = p_0</code> and <code class="reqn">k</code> is the smallest value for which <code class="reqn">\sum_{j = 1}^k \lambda_{ij} \ge \pi</code> where <code class="reqn">j = 1, \ldots, k</code>. The chosen number of lags and directions are returned.
</p>
<p>For <code>rectangle</code>: <code class="reqn">k</code> and <code class="reqn">p</code> are values such that their product <code class="reqn">k p</code> is the smallest for which <code class="reqn">\sum_{i = 1}^p \sum_{j = 1}^k \lambda_{ij} \ge \pi</code> where <code class="reqn">i = 1, \ldots, p</code> and <code class="reqn">j = 1, \ldots, k</code>. The chosen number of lags and directions are returned.
</p>
<p>For <code>big</code>: <code class="reqn">r</code> is the smallest value of elements <code class="reqn">(i_1, j_1), \ldots, (i_r, j_r)</code> for which <code class="reqn">\sum_{k = 1}^r \lambda_{i_k,j_k} \ge \pi</code> where <code class="reqn">k = 1, \ldots, r</code>. Thi indices of the matrix corresponding to the chosen values are returned.
</p>
<p>Note that when printing a summary.tssdr object, all elements except the component S, which is the matrix of the chosen directions or a vector if there is only one direction, are printed.
</p>


<h3>Value</h3>

<p>A list of class 'summary.tssdr' containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated signal separation matrix</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>The Lambda matrix for choosing lags and directions.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated directions as time series object standardized to have mean 0 and unit variances.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>The method for choosing the important lags and directions.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>The used algorithm as a character string.</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>The name for the response time series <code class="reqn">y</code>.</p>
</td></tr>
<tr><td><code>Xname</code></td>
<td>
<p>The name for the predictor time series <code class="reqn">\bf X</code>.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The chosen number of lags (not for <code>type = "big"</code> ).</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The chosen number of directions (not for <code>type = "big"</code>).</p>
</td></tr>
<tr><td><code>pk</code></td>
<td>
<p>The chosen lag-direction combinations (for <code>type = "big"</code> only).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen</p>


<h3>References</h3>

<p>Matilainen, M., Croux, C., Nordhausen, K. and Oja, H. (2017), <em>Supervised Dimension Reduction for Multivariate Time Series</em>, Econometrics and Statistics, 4, 57&ndash;69.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tssdr">tssdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

x1 &lt;- arima.sim(n = n, list(ar = 0.2))
x2 &lt;- arima.sim(n = n, list(ar = 0.8))
x3 &lt;- arima.sim(n = n, list(ar = 0.3, ma = -0.4))
eps2 &lt;- rnorm(n - 1)
y &lt;- 2*x1[1:(n - 1)] + 3*x2[1:(n - 1)] + eps2
X &lt;- ((cbind(x1, x2, x3))[2:n, ]) %*% t(A)

res2 &lt;- tssdr(y, X, algorithm = "TSIR")
res2
summ2 &lt;- summary(res2, thres = 0.5)
summ2
summary(res2) # Chooses more lags with larger threshold
summary(res2, type = "alllag") # Chooses all lags
summary(res2, type = "alldir", thres = 0.5) # Chooses all directions
summary(res2, type = "big", thres = 0.5) # Same choices than in summ2
</code></pre>

<hr>
<h2 id='tsBSS-package'>
Blind Source Separation and Supervised Dimension Reduction for Time Series
</h2><span id='topic+tsBSS-package'></span>

<h3>Description</h3>

<p>Different estimators are provided to solve the blind source separation problem for multivariate time series with stochastic volatility and supervised dimension reduction problem for multivariate time series. Different functions based on AMUSE and SOBI are also provided for estimating the dimension of the white noise subspace. The package is fully described in Nordhausen, Matilainen, Miettinen, Virta and Taskinen (2021) &lt;doi:10.18637/jss.v098.i15&gt;.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> tsBSS</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-07-09</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)
</td>
</tr>

</table>

<p>This package contains functions for the blind source separation (BSS) problem for multivariate time series. The methods are designed for time series with stochastic volatility, such as GARCH and SV models.
The main functions of the package for the BSS problem are
</p>

<ul>
<li><p><code><a href="#topic+FixNA">FixNA</a></code>  Function to solve the BSS problem. Algorithm is an alternative to <code><a href="#topic+vSOBI">vSOBI</a></code> algorithm to acommodate stochastic volatility.
</p>
</li>
<li><p><code><a href="#topic+gFOBI">gFOBI</a></code>  Function to solve the BSS problem. Algorithm is a generalization of <code><a href="JADE.html#topic+FOBI">FOBI</a></code> designed for time series with stochastic volatility.
</p>
</li>
<li><p><code><a href="#topic+gJADE">gJADE</a></code>  Function to solve the BSS problem. Algorithm is a generalization of <code><a href="JADE.html#topic+JADE">JADE</a></code> designed for time series with stochastic volatility.
</p>
</li>
<li><p><code><a href="#topic+vSOBI">vSOBI</a></code>  Function to solve the BSS problem. Algorithm is a variant of <code><a href="JADE.html#topic+SOBI">SOBI</a></code> algorithm and an alternative to <code><a href="#topic+FixNA">FixNA</a></code> to acommodate stochastic volatility.
</p>
</li>
<li><p><code><a href="#topic+gSOBI">gSOBI</a></code>  Function to solve the BSS problem. Algorithm is a combination of <code><a href="JADE.html#topic+SOBI">SOBI</a></code> and <code><a href="#topic+vSOBI">vSOBI</a></code> algorithms.
</p>
</li>
<li><p><code><a href="#topic+PVC">PVC</a></code>  Function to solve the BSS problem. Algorithm is a modified version of Principal Component Volatility Analysis by Hu and Tsay (2011).
</p>
</li></ul>

<p>The input data can be a numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. For other classes, the <code><a href="tsbox.html#topic+tsbox-package">tsbox</a></code> package provides appropriate conversions to and from these classes.
</p>
<p>The main function of the package for the supervised dimension reduction is
</p>

<ul>
<li><p><code><a href="#topic+tssdr">tssdr</a></code>  Function for supervised dimension reduction for multivariate time series. Includes methods TSIR, TSAVE and TSSH.
</p>
</li></ul>

<p>Methods for ARMA models, such as AMUSE and SOBI, and some non-stationary BSS methods for time series are implemented in the <code><a href="JADE.html#topic+JADE-package">JADE</a></code> package. See function <code><a href="dr.html#topic+dr">dr</a></code> for methods for supervised dimension reduction for iid observations.
</p>
<p>Several functions in this package utilize <code><a href="JADE.html#topic+rjd">&quot;rjd&quot;</a></code> (real joint diagonalization) and <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code> (fast rjd) from the <code><a href="JADE.html#topic+JADE-package">JADE</a></code> package for joint diagonalization of k real-valued square matrices. For whitening the time series this package uses function <code><a href="BSSprep.html#topic+BSSprep">&quot;BSSprep&quot;</a></code> from package <code><a href="BSSprep.html#topic+BSSprep-package">BSSprep</a></code>.
</p>
<p>There are several functions for estimating the number of white noise latent series in second-order source separation (SOS) models. The functions are 
</p>
<ul>
<li><p><code><a href="#topic+AMUSEboot">AMUSEboot</a></code>, <code><a href="#topic+AMUSEladle">AMUSEladle</a></code> and  <code><a href="#topic+AMUSEasymp">AMUSEasymp</a></code> which are based on <code><a href="JADE.html#topic+AMUSE">AMUSE</a></code>.
</p>
</li>
<li><p><code><a href="#topic+SOBIboot">SOBIboot</a></code>, <code><a href="#topic+SOBIladle">SOBIladle</a></code> and  <code><a href="#topic+SOBIasymp">SOBIasymp</a></code> which are based on <code><a href="JADE.html#topic+SOBI">SOBI</a></code>.
</p>
</li></ul>

<p>Additionally, there is function <code><a href="#topic+lbtest">lbtest</a></code> for a modified Ljung-Box test and a volatility clustering test for univariate and multivariate time series.
</p>
<p>The package also contains a dataset <code><a href="#topic+WeeklyReturnsData">WeeklyReturnsData</a></code>, which has logarithmic returns of exchange rates of 7 currencies against US Dollar.
</p>


<h3>Author(s)</h3>

<p>Markus Matilainen, Christophe Croux, Jari Miettinen, Klaus Nordhausen, Hannu Oja, Sara Taskinen, Joni Virta
</p>
<p>Maintainer: Markus Matilainen &lt;markus.matilainen@outlook.com&gt; 
</p>


<h3>References</h3>

<p>Nordhausen, K., Matilainen, M., Miettinen, J., Virta, J. and Taskinen, S. (2021)  <em>Dimension Reduction for Time Series in a Blind Source Separation Context Using R</em>, Journal of Statistical Software, 98(15), 1&ndash;30. &lt;doi:10.18637/jss.v098.i15&gt;
</p>
<p>Matilainen, M., Nordhausen, K. and Oja, H. (2015), <em>New Independent Component Analysis Tools for Time Series</em>, Statistics &amp; Probability Letters, 105, 80&ndash;87.
</p>
<p>Matilainen, M., Miettinen, J., Nordhausen, K., Oja, H. and Taskinen, S. (2017), <em>On Independent Component Analysis with Stochastic Volatility Models</em>, Austrian Journal of Statistics, 46(3&ndash;4), 57&ndash;66.
</p>
<p>Matilainen, M., Croux, C., Nordhausen, K. and Oja, H. (2017), <em>Supervised Dimension Reduction for Multivariate Time Series</em>, Econometrics and Statistics, 4, 57&ndash;69.
</p>
<p>Matilainen, M., Croux, C., Nordhausen, K. and Oja, H. (2019), <em>Sliced Average Variance Estimation for Multivariate Time Series</em>. Statistics: A Journal of Theoretical and Applied Statistics, 53, 630&ndash;655.
</p>
<p>Shi, Z., Jiang, Z. and Zhou, F. (2009), <em>Blind Source Separation with Nonlinear Autocorrelation and Non-Gaussianity</em>, Journal of Computational and Applied Mathematics, 223(1): 908&ndash;915.
</p>
<p>Matilainen, M., Nordhausen, K. and Virta, J. (2018), <em>On the Number of Signals in Multivariate Time Series</em>. In Deville, Y., Gannot, S., Mason, R., Plumbley, M.D. and  Ward, D. (editors) &quot;International Conference on Latent Variable Analysis and Signal Separation&quot;, LNCS 10891, 248&ndash;258. Springer, Cham., &lt;doi:10.1007/978-3-319-93764-9_24&gt;.
</p>
<p>Nordhausen, K. and Virta, J.(2018), <em>Ladle Estimator for Time Series Signal Dimension</em>. In 2018 IEEE Statistical Signal Processing Workshop (SSP), pp. 428&ndash;432, &lt;doi:10.1109/SSP.2018.8450695&gt;.
</p>
<p>Virta, J. and Nordhausen, K. (2021), <em>Determining the Signal Dimension in Second Order Source Separation</em>. Statistica Sinica, 31, 135&ndash;156.
</p>
<p>Miettinen, M., Matilainen, M., Nordhausen, K. and Taskinen, S. (2020), <em>Extracting Conditionally Heteroskedastic Components Using Independent Component Analysis</em>, Journal of Time Series Analysis, 41, 293&ndash;311.
</p>
<p>Hu, Y.-P. and Tsay, R. S. (2014), <em>Principal Volatility Component Analysis</em>, Journal of Business &amp; Economic Statistics, 32(2), 153&ndash;164.
</p>

<hr>
<h2 id='tssdr'>
Supervised Dimension Reduction for Multivariate Time Series
</h2><span id='topic+tssdr'></span><span id='topic+tssdr.default'></span><span id='topic+tssdr.ts'></span><span id='topic+tssdr.xts'></span><span id='topic+tssdr.zoo'></span><span id='topic+print.tssdr'></span><span id='topic+components.tssdr'></span><span id='topic+plot.tssdr'></span>

<h3>Description</h3>

<p>Supervised dimension reduction for multivariate time series data. There are three different algorithms to choose from. TSIR is a time series version of Sliced Inverse Regression (SIR), TSAVE is a time series version of Sliced Average Variance Estimate (TSAVE) and a hybrid of TSIR and TSAVE is TSSH (Time series SIR SAVE Hybrid). For summary of an object of class tssdr, see <code><a href="#topic+summary.tssdr">summary.tssdr</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tssdr(y, X, ...)

## Default S3 method:
tssdr(y, X, algorithm = c("TSIR", "TSAVE", "TSSH"), k = 1:12, H = 10, weight = 0.5,
      method = c("frjd", "rjd"), eps = 1e-06, maxiter = 1000, ...)
## S3 method for class 'ts'
tssdr(y, X, ...)
## S3 method for class 'xts'
tssdr(y, X, ...)
## S3 method for class 'zoo'
tssdr(y, X, ...)

## S3 method for class 'tssdr'
print(x, digits = 3, ...)
## S3 method for class 'tssdr'
components(x, ...)
## S3 method for class 'tssdr'
plot(x, main = "The response and the directions", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tssdr_+3A_y">y</code></td>
<td>
<p>A numeric vector or a time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code> (same type as X). Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code> (same type as y). Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_algorithm">algorithm</code></td>
<td>
<p>Algorithm to be used. The options are <code>"TSIR"</code>, <code>"TSAVE"</code> and <code>"TSSH"</code>. Default is <code>"TSIR"</code>.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:12</code>.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_h">H</code></td>
<td>
<p>The number of slices. If <code>"TSSH"</code> is used, <code class="reqn">H</code> is a 2-vector; the first element is used for TSIR part and the second for TSAVE part. Default is <code class="reqn">H = 10</code>.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_weight">weight</code></td>
<td>
<p>Weight <code class="reqn">0 \le a \le 1</code> for the hybrid method <code>TSSH</code> only. With <code class="reqn">a = 1</code> it reduces to TSAVE and with <code class="reqn">a = 0</code> to TSIR. Default is <code class="reqn">a = 0.5</code>.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_method">method</code></td>
<td>
<p>The method to use for the joint diagonalization. The options are <code><a href="JADE.html#topic+rjd">&quot;rjd&quot;</a></code> and <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>. Default is <code><a href="JADE.html#topic+rjd">&quot;frjd&quot;</a></code>.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="tssdr_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>
<p><strong>In methods for class 'tssdr' only:</strong>
</p>
<table>
<tr><td><code id="tssdr_+3A_x">x</code></td>
<td>
<p>An object of class tssdr</p>
</td></tr>
<tr><td><code id="tssdr_+3A_digits">digits</code></td>
<td>
<p>The number of digits when printing an object of class tssdr. Default is 3</p>
</td></tr>
<tr><td><code id="tssdr_+3A_main">main</code></td>
<td>
<p>A title for a plot when printing an object of class tssdr.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the <code class="reqn">p</code>-variate time series <code class="reqn">{\bf Z}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Z}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is a sample covariance matrix of <code class="reqn">{\bf X}</code>.
Divide <code class="reqn">y</code> into <code class="reqn">H</code> disjoint intervals (slices) by its empirical quantiles.
</p>
<p>For each lag <code class="reqn">j</code>, denote <code class="reqn">y_{j}</code> for a vector of the last <code class="reqn">n - j</code> values of the sliced <code class="reqn">y</code>. Also denote <code class="reqn">{\bf Z}_j</code> for the first <code class="reqn">n - j</code> observations of <code class="reqn">{\bf Z}</code>. Then <code class="reqn">{\bf Z}_{jh}</code> are the disjoint slices of <code class="reqn">{\bf Z}_j</code> according to the values of <code class="reqn">y_{j}</code>.
</p>
<p>Let <code class="reqn">T_{jh}</code> be the number of observations in <code class="reqn">{\bf Z}_{jh}</code>.
Write <code class="reqn">\bf \widehat{A}_{jh} = \frac{1}{T_{jh}}\sum_{t = 1}^{T_{jh}}({\bf Z}_{jh})_{t}</code> for <code class="reqn">t = 1, \ldots, T_jh</code>,
and <code class="reqn">{\bf \widehat A}_j = ({\bf \widehat{A}}_{j1}, \ldots, {\bf \widehat{A}}_{jH})'</code>.
Then for algorithm <code>TSIR</code> matrix </p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{M}}_{0j} = {\bf \widehat{Cov}}_{A_j}.</code>
</p>

<p>Denote <code class="reqn">\bf \widehat{Cov}_{jh}</code> for a sample covariance matrix of <code class="reqn">{\bf Z}_{jh}</code>. Then for algorithm <code>TSAVE</code> matrix </p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{M}}_{0j} = \frac{1}{H}\sum_{h = 1}^H({\bf I}_p - {\bf \widehat{Cov}_{jh}})^2.</code>
</p>

<p><code class="reqn">h = 1, \ldots, H</code>.
</p>
<p>For <code>TSSH</code> then matrix </p>
<p style="text-align: center;"><code class="reqn">{\bf \widehat{M}}_{2j} = a{\bf \widehat{M}_{1j}} + (1-a){\bf \widehat{M}_{0j}},</code>
</p>
<p> for a chosen <code class="reqn">0 \le a \le 1</code>. Note that the value of <code class="reqn">H</code> can be different for TSIR and TSAVE parts.
</p>
<p>The algorithms find an orthogonal matrix <code class="reqn">{\bf U} = (\bf u_1, \ldots, \bf u_p)'</code> by maximizing, for <code class="reqn">b = 0, 1</code> or <code class="reqn">2</code>, 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i \in k} ||diag({\bf U} {\bf \widehat{M}}_{bj} {\bf U}')||^2 = \sum_{i \in 1}^p \sum_{j \in k} ({\bf u}_i' {\bf \widehat{M}}_{bj} {\bf u}_i)^2.</code>
</p>

<p>for <code class="reqn">i = 1, \ldots, p</code> and all lags <code class="reqn">j</code>.
The final signal separation matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>Write <code class="reqn">\lambda_{ij} = c({\bf u}_i' {\bf \widehat{M}}_{bj} {\bf u}_i)^2</code>, where <code class="reqn">c</code> is chosen in such way that <code class="reqn">\sum_{i = 1}^p \sum_{j \in k} \lambda_{ij}= 1.</code> for <code class="reqn">i = 1, \ldots, p</code> and all lags <code class="reqn">j</code>.
Then the <code class="reqn">(i, j)</code>:th element of the matrix <code class="reqn">\bf L</code> is <code class="reqn">\lambda_{ij}</code>.
</p>
<p>To make a choice on which lags and directions to keep, see <code><a href="#topic+summary.tssdr">summary.tssdr</a></code>. Note that when printing a tssdr object, all elements are printed, except the directions S.
</p>


<h3>Value</h3>

<p>A list of class 'tssdr' containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated signal separation matrix.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated directions as time series object standardized to have mean 0 and unit variances.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>The Lambda matrix for choosing lags and directions.</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>The used number of slices.</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>The name for the response time series <code class="reqn">y</code>.</p>
</td></tr>
<tr><td><code>Xname</code></td>
<td>
<p>The name for the predictor time series <code class="reqn">\bf X</code>.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>The used algorithm as a character string.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen
</p>


<h3>References</h3>

<p>Matilainen, M., Croux, C., Nordhausen, K. and Oja, H. (2017), <em>Supervised Dimension Reduction for Multivariate Time Series</em>, Econometrics and Statistics, 4, 57&ndash;69.
</p>
<p>Matilainen, M., Croux, C., Nordhausen, K. and Oja, H. (2019), <em>Sliced Average Variance Estimation for Multivariate Time Series</em>. Statistics: A Journal of Theoretical and Applied Statistics, 53, 630&ndash;655.
</p>
<p>Li, K.C. (1991), <em>Sliced Inverse Regression for Dimension Reduction</em>, Journal of the American Statistical Association, 86, 316&ndash;327.
</p>
<p>Cook, R. and Weisberg, S. (1991), <em>Sliced Inverse Regression for Dimension Reduction</em>, Comment. Journal of the American Statistical Association, 86, 328&ndash;332.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.tssdr">summary.tssdr</a></code>, <code><a href="dr.html#topic+dr">dr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

x1 &lt;- arima.sim(n = n, list(ar = 0.2))
x2 &lt;- arima.sim(n = n, list(ar = 0.8))
x3 &lt;- arima.sim(n = n, list(ar = 0.3, ma = -0.4))
eps2 &lt;- rnorm(n - 1)
y &lt;- 2*x1[1:(n - 1)] + eps2
X &lt;- ((cbind(x1, x2, x3))[2:n, ]) %*% t(A)

res1 &lt;- tssdr(y, X, algorithm = "TSAVE")
res1
summ1 &lt;- summary(res1, type = "alllag", thres = 0.8)
summ1
plot(summ1)
head(components(summ1))
coef(summ1)

# Hybrid of TSIR and TSAVE. For TSIR part H = 10 and for TSAVE part H = 2.
tssdr(y, X, algorithm = "TSSH", weight = 0.6, H = c(10, 2))
</code></pre>

<hr>
<h2 id='vSOBI'>
A Variant of SOBI for Blind Source Separation
</h2><span id='topic+vSOBI'></span><span id='topic+vSOBI.default'></span><span id='topic+vSOBI.ts'></span><span id='topic+vSOBI.xts'></span><span id='topic+vSOBI.zoo'></span>

<h3>Description</h3>

<p>The vSOBI (variant of Second Order Blind Identification) method for the blind source separation of time series with stochastic volatility.
The method is a variant of SOBI, which is a method designed to separate ARMA sources, and an alternative to FixNA and FixNA2 methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vSOBI(X, ...)

## Default S3 method:
vSOBI(X, k = 1:12, eps = 1e-06, maxiter = 1000, G = c("pow", "lcosh"), 
      ordered = FALSE, acfk = NULL, original = TRUE, alpha = 0.05, ...)
## S3 method for class 'ts'
vSOBI(X, ...)
## S3 method for class 'xts'
vSOBI(X, ...)
## S3 method for class 'zoo'
vSOBI(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vSOBI_+3A_x">X</code></td>
<td>
<p>A numeric matrix or a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>, <code><a href="xts.html#topic+xts">xts</a></code> or <code><a href="zoo.html#topic+zoo">zoo</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_k">k</code></td>
<td>
<p>A vector of lags. It can be any non-zero positive integer, or a vector consisting of them. Default is <code>1:12</code>.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_g">G</code></td>
<td>
<p>Function <code class="reqn">G(x)</code>. The choices are <code>"pow"</code> (default) and <code>"lcosh"</code>.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_ordered">ordered</code></td>
<td>
<p>Whether to order components according to their volatility. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_acfk">acfk</code></td>
<td>
<p>A vector of lags to be used in testing the presence of serial autocorrelation. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_original">original</code></td>
<td>
<p>Whether to return the original components or their residuals based on ARMA fit. Default is <code>TRUE</code>, i.e. the original components are returned. Applicable only if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for linear correlation detection. Default is 0.05.</p>
</td></tr>
<tr><td><code id="vSOBI_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that a <code class="reqn">p</code>-variate <code class="reqn">{\bf Y}</code> with <code class="reqn">T</code> observations is whitened, i.e. <code class="reqn">{\bf Y}={\bf S}^{-1/2}({\bf X}_t - \frac{1}{T}\sum_{t=1}^T {\bf X}_{t})</code>, for <code class="reqn">t = 1, \ldots, T</code>,
where <code class="reqn">{\bf S}</code> is the sample covariance matrix of <code class="reqn">{\bf X}</code>. The algorithm finds an orthogonal matrix <code class="reqn">{\bf U}</code> by maximizing
</p>
<p style="text-align: center;"><code class="reqn">{\bf D}({\bf U}) = \sum_{k = 1}^K {\bf D}_k({\bf U})</code>
</p>

<p style="text-align: center;"><code class="reqn">= \sum_{k = 1}^K \sum_{i = 1}^p  \left(\frac{1}{T - k}\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_t) G({\bf u}_i' {\bf Y}_{t + k})] - \left(\frac{1}{T - k}\right)^2\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_t)]\sum_{t=1}^{T - k}[G({\bf u}_i' {\bf Y}_{t + k})]\right)^2.</code>
</p>

<p>where <code class="reqn">i = 1, \ldots, p</code>, <code class="reqn">k = 1, \ldots, K</code> and <code class="reqn">t = 1, \ldots, T</code>. For function <code class="reqn">G(x)</code> the choices are <code class="reqn">x^2</code> and log(cosh(<code class="reqn">x</code>)).
</p>
<p>The algorithm works iteratively starting with <code>diag(p)</code> as an initial value for an orthogonal matrix <code class="reqn">{\bf U} = ({\bf u}_1, {\bf u}_2, \ldots, {\bf u}_p)'</code>.
</p>
<p>Matrix <code class="reqn">{\bf T}_{ik}</code> is a partial derivative of <code class="reqn">{\bf D}_k({\bf U})</code> with respect to <code class="reqn">{\bf u}_i</code>.
Then <code class="reqn">{\bf T}_k = ({\bf T}_{1k}, \ldots, {\bf T}_{pk})'</code>, where <code class="reqn">p</code> is the number of columns in <code class="reqn">{\bf Y}</code>, and <code class="reqn">{\bf T} = \sum_{k = 1}^K {\bf T}_k</code>.
The update for the orthogonal matrix <code class="reqn">{\bf U}_{new} = ({\bf TT}')^{-1/2}{\bf T}</code> is calculated at each iteration step. The algorithm stops when
</p>
<p style="text-align: center;"><code class="reqn">||{\bf U}_{new} - {\bf U}_{old}||</code>
</p>

<p>is less than <code>eps</code>.
The final unmixing matrix is then <code class="reqn">{\bf W} = {\bf US}^{-1/2}</code>.
</p>
<p>For <code>ordered = TRUE</code> the function orders the sources according to their volatility. First a possible linear autocorrelation is removed using <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>. Then a squared autocorrelation test is performed for the sources (or for their residuals, when linear correlation is present). The sources are then put in a decreasing order according to the value of the test statistic of the squared autocorrelation test. For more information, see <code><a href="#topic+lbtest">lbtest</a></code>.
</p>


<h3>Value</h3>

<p>A list of class 'bssvol', inheriting from class 'bss', containing the following components:
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>The estimated unmixing matrix. If <code>ordered = TRUE</code>, the rows are ordered according to the order of the components.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The vector of the used lags.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated sources as time series object standardized to have mean 0 and unit variances. If <code>ordered = TRUE</code>, then components are ordered according to their volatility. If <code>original = FALSE</code>, the sources with linear autocorrelation are replaced by their ARMA residuals.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>The mean vector of <code>X</code>.</p>
</td></tr>
</table>
<p>If <code>ordered = TRUE</code>, then also the following components included in the list:
</p>
<table>
<tr><td><code>Sraw</code></td>
<td>
<p>The ordered original estimated sources as time series object standardized to have mean 0 and unit variances. Returned only if <code>original = FALSE</code>.</p>
</td></tr>
<tr><td><code>fits</code></td>
<td>
<p>The ARMA fits for the components with linear autocorrelation.</p>
</td></tr>
<tr><td><code>armaeff</code></td>
<td>
<p>A logical vector. Is TRUE if ARMA fit was done to the corresponding component.</p>
</td></tr>
<tr><td><code>linTS</code></td>
<td>
<p>The value of the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>linP</code></td>
<td>
<p>p-value based on the modified Ljung-Box test statistic for each component.</p>
</td></tr>
<tr><td><code>volTS</code></td>
<td>
<p>The value of the volatility clustering test statistic.</p>
</td></tr>
<tr><td><code>volP</code></td>
<td>
<p>p-value based on the volatility clustering test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Matilainen
</p>


<h3>References</h3>

<p>Belouchrani, A., Abed-Meriam, K., Cardoso, J.F. and Moulines, R. (1997), <em>A Blind Source Separation Technique Using Second-Order Statistics</em>, IEEE Transactions on Signal Processing, 434&ndash;444.
</p>
<p>Matilainen, M., Miettinen, J., Nordhausen, K., Oja, H. and Taskinen, S. (2017), <em>On Independent Component Analysis with Stochastic Volatility Models</em>, Austrian Journal of Statistics, 46(3&ndash;4), 57&ndash;66.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FixNA">FixNA</a></code>, <code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="#topic+lbtest">lbtest</a></code>, <code><a href="forecast.html#topic+auto.arima">auto.arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")) {
n &lt;- 10000
A &lt;- matrix(rnorm(9), 3, 3)

# simulate SV models
s1 &lt;- svsim(n, mu = -10, phi = 0.8, sigma = 0.1)$y
s2 &lt;- svsim(n, mu = -10, phi = 0.9, sigma = 0.2)$y
s3 &lt;- svsim(n, mu = -10, phi = 0.95, sigma = 0.4)$y

# create a daily time series
X &lt;- ts(cbind(s1, s2, s3) %*% t(A), end = c(2015, 338), frequency = 365.25)

res &lt;- vSOBI(X)
res
coef(res)
plot(res)
head(bss.components(res))

MD(res$W, A) # Minimum Distance Index, should be close to zero
}
</code></pre>

<hr>
<h2 id='WeeklyReturnsData'>
Logarithmic Returns of Exchange Rates of 7 Currencies Against US Dollar
</h2><span id='topic+WeeklyReturnsData'></span>

<h3>Description</h3>

<p>This data set has logarithmic returns of exchange rates of 7 currencies against US dollar extracted from the International Monetary Fund's (IMF) database. These currencies are Australian Dollar (AUD), Canadian Dollar (CAD), Norwegian Kroner (NOK), Singapore Dollar (SGD), Swedish Kroner (SEK), Swiss Franc (CHF) and British Pound (GBP).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("WeeklyReturnsData")
</code></pre>


<h3>Format</h3>

<p>An object of class ts with 605 observations on the following 7 variables.
</p>

<dl>
<dt><code>AUD</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{AUD, t}</code> of the exchange rates of AUD against US Dollar.</p>
</dd>
<dt><code>CAD</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{CAD, t}</code> of the exchange rates of CAD against US Dollar.</p>
</dd>
<dt><code>NOK</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{NOK, t}</code> of the exchange rates of NOK against US Dollar.</p>
</dd>
<dt><code>SGD</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{SGD, t}</code> of the exchange rates of SGD against US Dollar.</p>
</dd>
<dt><code>SEK</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{SEK, t}</code> of the exchange rates of SEK against US Dollar.</p>
</dd>
<dt><code>CHF</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{CHF, t}</code> of the exchange rates of CHF against US Dollar.</p>
</dd>
<dt><code>GBP</code></dt><dd><p>The weekly logarithmic returns <code class="reqn">\mathbf r_{GBP, t}</code> of the exchange rates of GBP against US Dollar.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The daily exhange rates of the currencies against US Dollar from March 22, 2000 to October
26, 2011 are extracted from the International Monetary Fund's (IMF) Exchange Rates database from <a href="https://www.imf.org/external/np/fin/ert/GUI/Pages/CountryDataBase.aspx">https://www.imf.org/external/np/fin/ert/GUI/Pages/CountryDataBase.aspx</a>. These rates are representative rates (currency units per US Dollar), which are reported daily to the IMF by the issuing central bank.
</p>
<p>The weekly averages of these exchange rates are then calculated. The logarithmic returns of the average weekly exchange rates are calculated for the currency <code class="reqn">j</code> as follows.
</p>
<p>Let <code class="reqn">\mathbf x_{j, t}</code> be the exchange rates of <code class="reqn">j</code> against US Dollar. Then
</p>
<p style="text-align: center;"><code class="reqn">\mathbf r_{j, t} = \textrm{ log }(\mathbf x_{j, t}) - \textrm{ log } (\mathbf x_{j, t-1}),</code>
</p>

<p>where <code class="reqn">t = 1, \ldots, 605</code> and <code class="reqn">j = AUD, CAD, NOK, SGD, SEK, CHF, GBP</code>.
The six missing values in <code class="reqn">\mathbf r_{SEK, t}</code> are changed to 0. The assumption used here is that there has not been any change from the previous week.
</p>
<p>The weekly returns data is then changed to a multivariate time series object of class <code><a href="stats.html#topic+ts">ts</a></code>. The resulting <code><a href="stats.html#topic+ts">ts</a></code>
object is then dataset <code>WeeklyReturnsData</code>.
</p>
<p>An example analysis of the data is given in Miettinen et al. (2018). Same data has also been used in Hu and Tsay (2014).
</p>


<h3>Source</h3>

<p>International Monetary Fund (2017), <em>IMF Exchange Rates</em>, <a href="https://www.imf.org/external/np/fin/ert/GUI/Pages/CountryDataBase.aspx">https://www.imf.org/external/np/fin/ert/GUI/Pages/CountryDataBase.aspx</a>
</p>
<p>For IMF Copyrights and Usage, and special terms and conditions pertaining to the use of IMF data, see <a href="https://www.imf.org/external/terms.htm">https://www.imf.org/external/terms.htm</a>
</p>


<h3>References</h3>

<p>Miettinen, M., Matilainen, M., Nordhausen, K. and Taskinen, S. (2020), <em>Extracting Conditionally Heteroskedastic Components Using Independent Component Analysis</em>, Journal of Time Series Analysis,41, 293&ndash;311.
</p>
<p>Hu and Tsay (2014), <em>Principal Volatility Component Analysis</em>, Journal of Business &amp; Economic Statistics, 32(2), 153&ndash;164.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(WeeklyReturnsData)

res &lt;- gSOBI(WeeklyReturnsData)
res

coef(res)
plot(res)
head(bss.components(res))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
