<!DOCTYPE html><html><head><title>Help for package GMKMcharlie</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {GMKMcharlie}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#d2s'>
<p>Dense to sparse conversion</p></a></li>
<li><a href='#GM'>
<p>Multithreaded Gaussian mixture trainer</p></a></li>
<li><a href='#GMcw'>
<p>Multithreaded component-wise Gaussian mixture trainer</p></a></li>
<li><a href='#GMfj'>
<p>Multithreaded minimum message length Gaussian mixture trainer</p></a></li>
<li><a href='#KM'>
<p>K-means over dense representation of data</p></a></li>
<li><a href='#KMconstrained'>
<p>K-means over dense data input with constraints on cluster weights</p></a></li>
<li><a href='#KMconstrainedSparse'>
<p>K-means over sparse data input with constraints on cluster weights</p></a></li>
<li><a href='#KMppIni'>
<p>Minkowski and spherical, deterministic and stochastic, multithreaded K-means++ initialization over dense representation of data</p></a></li>
<li><a href='#KMppIniSparse'>
<p>Minkowski and spherical, deterministic and stochastic, multithreaded K-means++ initialization over sparse representation of data</p></a></li>
<li><a href='#KMsparse'>
<p>K-means over sparse representation of data</p></a></li>
<li><a href='#s2d'>
<p>Sparse to dense conversion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Unsupervised Gaussian Mixture and Minkowski and Spherical
K-Means with Constraints</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.5</td>
</tr>
<tr>
<td>Author:</td>
<td>Charlie Wusuo Liu</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Charlie Wusuo Liu &lt;liuwusuo@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>High performance trainers for parameterizing and clustering weighted data. The Gaussian mixture (GM) module includes the conventional EM (expectation maximization) trainer, the component-wise EM trainer, the minimum-message-length EM trainer by Figueiredo and Jain (2002) &lt;<a href="https://doi.org/10.1109%2F34.990138">doi:10.1109/34.990138</a>&gt;. These trainers accept additional constraints on mixture weights, covariance eigen ratios and on which mixture components are subject to update. The K-means (KM) module offers clustering with the options of (i) deterministic and stochastic K-means++ initializations, (ii) upper bounds on cluster weights (sizes), (iii) Minkowski distances, (iv) cosine dissimilarity, (v) dense and sparse representation of data input. The package improved the typical implementations of GM and KM algorithms in various aspects. It is carefully crafted in multithreaded C++ for modeling large data for industry use.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.0), RcppParallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS (&ge; 7.3.0), plot3D (&ge; 1.1.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppParallel, RcppArmadillo</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-29 05:19:13 UTC; i56087</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-29 06:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='d2s'>
Dense to sparse conversion
</h2><span id='topic+d2s'></span>

<h3>Description</h3>

<p>Convert data from dense representation (matrix) to sparse representation (list of data frames).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>d2s(
  X,
  zero = 0,
  threshold = 1e-16,
  verbose= TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="d2s_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of data points &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="d2s_+3A_zero">zero</code></td>
<td>

<p>A numeric value. Elements in <code>X</code> satisfying <code>abs(X[i]</code> <code>-</code> <code>zero)</code> <code>&lt;= threshold</code> are treated as zeros. Default 0.
</p>
</td></tr>
<tr><td><code id="d2s_+3A_threshold">threshold</code></td>
<td>

<p>A numeric value, explained above.
</p>
</td></tr>
<tr><td><code id="d2s_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of size <code>N</code>. <code>Value[[i]]</code> is a 2-column data frame. The 1st column is a sorted integer vector of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a numeric vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 2000L
d = 3000L
X = matrix(rnorm(N * d) + 2, nrow = d)
# Fill many zeros in X:
X = apply(X, 2, function(x) {
  x[sort(sample(d, d * runif(1, 0.95, 0.99)))] = 0; x})
# Get the sparse version of X.
sparseX = GMKMcharlie::d2s(X)
str(sparseX[1:5])
</code></pre>

<hr>
<h2 id='GM'>
Multithreaded Gaussian mixture trainer
</h2><span id='topic+GM'></span>

<h3>Description</h3>

<p>The traditional training algorithm via maximum likelihood for parameterizing weighted data with a mixture of Gaussian PDFs. Bounds on covariance matrix eigen ratios and mixture weights are optional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GM(
  X,
  Xw = rep(1, ncol(X)),
  alpha = numeric(0),
  mu = matrix(ncol = 0, nrow = 0),
  sigma = matrix(ncol = 0, nrow = 0),
  G = 5L,
  convergenceEPS = 1e-05,
  convergenceTail = 10,
  alphaEPS = 0,
  eigenRatioLim = Inf,
  embedNoise = 1e-6,
  maxIter = 1000L,
  maxCore = 7L,
  tlimit = 3600,
  verbose = TRUE,
  updateAlpha = TRUE,
  updateMean = TRUE,
  updateSigma = TRUE,
  checkInitialization = FALSE,
  KmeansFirst = TRUE,
  KmeansPPfirst = FALSE,
  KmeansRandomSeed = NULL,
  friendlyOutput = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GM_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of observations &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="GM_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[, i]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="GM_+3A_alpha">alpha</code></td>
<td>

<p>A numeric vector of size <code>K</code>, the number of Gaussian kernels in the mixture model. <code>alpha</code> are the initial mixture weights and should sum up to 1. Default empty.
</p>
</td></tr>
<tr><td><code id="GM_+3A_mu">mu</code></td>
<td>

<p>A <code>d x K</code> numeric matrix. <code>mu[, i]</code> is the initial mean for the <code>i</code>th Gaussian kernel. Default empty.
</p>
</td></tr>
<tr><td><code id="GM_+3A_sigma">sigma</code></td>
<td>

<p>Either a list of <code>d x d</code> matrices, or a <code>d^2 x K</code> numeric matrix. For the latter, each column represents a flattened <code>d x d</code> initial covariance matrix of the <code>i</code>th Gaussian kernel. In R, <code>as.numeric(aMatrix)</code> gives the flattened version of <code>aMatrix</code>. Covariance matrix of each Gaussian kernel MUST be positive-definite. Default empty.
</p>
</td></tr>
<tr><td><code id="GM_+3A_g">G</code></td>
<td>

<p>An integer. If at least one of the parameters <code>alpha</code>, <code>mu</code>, <code>sigma</code> are empty, the program will initialize <code>G</code> Gaussian kernels via K-means++ deterministic initialization. See <code>KMppIni()</code>. Otherwise <code>G</code> is ignored. Default 5.
</p>
</td></tr>
<tr><td><code id="GM_+3A_convergenceeps">convergenceEPS</code></td>
<td>

<p>A numeric value. If the average change of all parameters in the mixture model is below <code>convergenceEPS</code> relative to those in the pervious iteration, the program ends. Checking convergence this way is faster than recomputing the log-likelihood every iteration. Default 1e-5.
</p>
</td></tr>
<tr><td><code id="GM_+3A_convergencetail">convergenceTail</code></td>
<td>

<p>If every one of the last <code>convergenceTail</code> iteration produces less than a relative increase of <code>convergenceEPS</code> in log-likelihood, stop.
</p>
</td></tr>
<tr><td><code id="GM_+3A_alphaeps">alphaEPS</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's weight is no greater than <code>alphaEPS</code>, the kernel is deleted. Default 0.
</p>
</td></tr>
<tr><td><code id="GM_+3A_eigenratiolim">eigenRatioLim</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's max:min eigen value ratio exceeds <code>eigenRatioLim</code>, the kernel is treated as degenerate and deleted. Thresholding eigen ratios is in the interest of minimizing the effect of degenerate kernels in an early stage. Default <code>Inf</code>.
</p>
</td></tr>
<tr><td><code id="GM_+3A_embednoise">embedNoise</code></td>
<td>

<p>A small constant added to the diagonal entries of all covariance matrices. This may prevent covariance matrices collapsing prematurely. A suggested value is 1e-6. Covariance degeneration is detected during Cholesky decomposition, and will lead the trainer to remove the corresponding mixture component. For high-dimensional problem, setting <code>embedNoise</code> to nonzero may pose the illusion of massive log-likelihood, all because one or more mixture components are so close to singular, which makes the densities around them extremely high.
</p>
</td></tr>
<tr><td><code id="GM_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer, the maximal number of iterations.
</p>
</td></tr>
<tr><td><code id="GM_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. Should be no more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="GM_+3A_tlimit">tlimit</code></td>
<td>

<p>A numeric value. The program exits with the current model in <code>tlimit</code> seconds.
</p>
</td></tr>
<tr><td><code id="GM_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
<tr><td><code id="GM_+3A_updatealpha">updateAlpha</code></td>
<td>

<p>A boolean value or boolean vector. If a boolean value, <code>TRUE</code> implies weights on all mixture components are subject to update, otherwise they should stay unchanged during training. If a boolean vector, its size should equal the number of mixture components. <code>updateAlpha[i] == TRUE</code> implies the weight on the <code>i</code>th component is subject to update. Regardless of <code>updateAlpha</code>, the output will have normalized mixture weights.
</p>
</td></tr>
<tr><td><code id="GM_+3A_updatemean">updateMean</code></td>
<td>

<p>A boolean value or a boolean vector. If a boolean value, <code>TRUE</code> implies means of all mixture components are subject to update, otherwise they should stay unchanged during training. If a boolean vector, its size should equal the number of mixture components. <code>updateMean[i] == TRUE</code> implies the mean of the <code>i</code>th component is subject to update.
</p>
</td></tr>
<tr><td><code id="GM_+3A_updatesigma">updateSigma</code></td>
<td>

<p>A boolean value or a boolean vector. If a boolean value, <code>TRUE</code> implies covariances of all mixture components are subject to update, otherwise they should stay unchanged during training. If a boolean vector, its size should equal the number of mixture components. <code>updateSigma[i] == TRUE</code> implies the covariance of the <code>i</code>th component is subject to update.
</p>
</td></tr>
<tr><td><code id="GM_+3A_checkinitialization">checkInitialization</code></td>
<td>

<p>Check if any of the input covariance matrices are singular.
</p>
</td></tr>
<tr><td><code id="GM_+3A_kmeansfirst">KmeansFirst</code></td>
<td>

<p>A boolean value. Run K-means clustering for finding means.
</p>
</td></tr>
<tr><td><code id="GM_+3A_kmeansppfirst">KmeansPPfirst</code></td>
<td>

<p>A boolean value. Run stochastic K-means++ for K-means initialization.
</p>
</td></tr>
<tr><td><code id="GM_+3A_kmeansrandomseed">KmeansRandomSeed</code></td>
<td>

<p>An integer or <code>NULL</code>, the random seed for K-means++.
</p>
</td></tr>
<tr><td><code id="GM_+3A_friendlyoutput">friendlyOutput</code></td>
<td>

<p><code>TRUE</code> returns covariance matrices in a list rather than a single matrix of columns of flattened covariance matrices.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An in-place Cholesky decomposition of covariance matrix is implemented for space and speed efficiency. Only the upper triangle of the Cholesky decomposition is memorized for each Gaussian kernel, and it is then applied to a forward substitution routine for fast Mahalanobis distances computation. Each of the three main steps in an iteration &mdash; Gaussian density computation, parameter inference, parameter update &mdash; is multithreaded and hand-scheduled using Intel TBB. Extensive efforts have been made over cache-friendliness and extra multithreading overheads such as memory allocation.
</p>
<p>If <code>eigenRatioLim</code> is finite, eigen decomposition employs routines in <code>RcppArmadillo</code>.
</p>


<h3>Value</h3>

<p>A list of size 5:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>a numeric vector of size <code>K</code>. The mixture weights.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>a <code>d x K</code> numeric matrix. Each column is the mean of a Gaussian kernel.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>a <code>d^2 x K</code> numeric matrix. Each column is the flattened covariance matrix of a Gaussian kernel. Do <code>matrix(sigma[, i], nrow = d)</code> to recover the covariance matrix of the <code>i</code>th kernel.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>a numeric vector of size <code>N</code>. <code>fitted[i]</code> is the probability density of the <code>i</code>th observation given by the mixture model.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>a list of <code>K</code> integer vectors, the hard clustering inferred from the mixture model. Each integer vector contains the indexes of observations in <code>X</code>.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>For one-dimensional data, <code>X</code> should still follow the data structure requirements: a matrix where each column is an observation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># =============================================================================
# Examples below use 1 thread to pass CRAN check. Speed advantage of multiple
# threads will be more pronounced for larger data.
# =============================================================================


# =============================================================================
# Parameterize the iris data. Let the function initialize Gaussian kernels.
# =============================================================================
X = t(iris[1:4])
# CRAN check only allows 2 threads at most. Increase `maxCore` for
# acceleration.
gmmRst = GMKMcharlie::GM(X, G = 4L, maxCore = 1L, friendlyOutput = FALSE)
str(gmmRst)




# =============================================================================
# Parameterize the iris data given Gaussian kernels.
# =============================================================================
G = 3L
d = nrow(X) # Dimensionality.
alpha = rep(1, G) / G
mu = X[, sample(ncol(X), G)] # Sample observations as initial means.
# Take the average variance and create initial covariance matrices.
meanVarOfEachDim = sum(diag(var(t(X)))) / d
covar = diag(meanVarOfEachDim / G, d)
covars = matrix(rep(as.numeric(covar), G), nrow = d * d)


# Models are sensitive to initialization.
gmmRst2 = GMKMcharlie::GM(
  X, alpha = alpha, mu = mu, sigma = covars, maxCore = 1L,
  friendlyOutput = FALSE)
str(gmmRst2)




# =============================================================================
# For fun, fit Rosenbrock function with a Gaussian mixture.
# =============================================================================
set.seed(123)
rosenbrock &lt;- function(x, y) {(1 - x) ^ 2 + 100 * (y - x ^ 2) ^ 2}
N = 2000L
x = runif(N, -2, 2)
y = runif(N, -1, 3)
z = rosenbrock(x, y)


X = rbind(x, y)
Xw = z * (N / sum(z)) # Weights on observations should sum up to N.
gmmFit = GMKMcharlie::GM(X, Xw = Xw, G = 5L, maxCore = 1L, verbose = FALSE,
  friendlyOutput = FALSE)


oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x, y, gmmFit$fitted, pch = 20)
par(mfrow = oldpar)




# =============================================================================
# For fun, fit a 3D spiral distribution.
# =============================================================================
N = 2000
t = runif(N) ^ 2 * 15
x = cos(t) + rnorm(N) * 0.1
y = sin(t) + rnorm(N) * 0.1
z = t + rnorm(N) * 0.1


X = rbind(x, y, z)
d = 3L
G = 10L
gmmFit = GMKMcharlie::GM(X, G = G, maxCore = 1L, verbose = FALSE,
  KmeansFirst = TRUE, KmeansPPfirst = TRUE, KmeansRandomSeed = 42,
  friendlyOutput = TRUE)
# Sample N points from the Gaussian mixture.
ns = as.integer(round(N * gmmFit$alpha))
sampledPoints = list()
for(i in 1:G)
{
  sampledPoints[[i]] = MASS::mvrnorm(
    ns[i], mu = gmmFit$mu[, i], Sigma = matrix(gmmFit$sigma[[i]], nrow = d))
}
sampledPoints =
  matrix(unlist(lapply(sampledPoints, function(x) t(x))), nrow = d)


# Plot the original data and the samples from the mixture model.
oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x = sampledPoints[1, ],
                 y = sampledPoints[2, ],
                 z = sampledPoints[3, ], pch = 20)
par(mfrow = oldpar)




# =============================================================================
# For fun, fit a 3D spiral distribution. Fix parameters at random.
# =============================================================================
N = 2000
t = runif(N) ^ 2 * 15
x = cos(t) + rnorm(N) * 0.1
y = sin(t) + rnorm(N) * 0.1
z = t + rnorm(N) * 0.1


X = rbind(x, y, z); dimnames(X) = NULL
d = 3L
G = 10L
mu = X[, sample(ncol(X), G)]
s = matrix(rep(as.numeric(cov(t(X))), G), ncol = G)
alpha = rep(1 / G, G)
updateAlpha = sample(c(TRUE, FALSE), G, replace = TRUE)
updateMean = sample(c(TRUE, FALSE), G, replace = TRUE)
updateSigma = sample(c(TRUE, FALSE), G, replace = TRUE)
gmmFit = GMKMcharlie::GM(X, alpha = alpha, mu = mu, sigma = s, G = G,
                         maxCore = 2, verbose = FALSE,
                         updateAlpha = updateAlpha,
                         updateMean = updateMean,
                         updateSigma = updateSigma,
                         convergenceEPS = 1e-5, alphaEPS = 1e-8,
                         friendlyOutput = TRUE)
</code></pre>

<hr>
<h2 id='GMcw'>
Multithreaded component-wise Gaussian mixture trainer
</h2><span id='topic+GMcw'></span>

<h3>Description</h3>

<p>The component-wise variant of <code>GM()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMcw(
  X,
  Xw = rep(1, ncol(X)),
  alpha = numeric(0),
  mu = matrix(ncol = 0, nrow = 0),
  sigma = matrix(ncol = 0, nrow = 0),
  G = 5L,
  convergenceEPS = 1e-05,
  alphaEPS = 0,
  eigenRatioLim = Inf,
  maxIter = 1000L,
  maxCore = 7L,
  tlimit = 3600,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GMcw_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of observations &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[, i]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_alpha">alpha</code></td>
<td>

<p>A numeric vector of size <code>K</code>, the number of Gaussian kernels in the mixture model. <code>alpha</code> are the initial mixture weights and should sum up to 1. Default empty.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_mu">mu</code></td>
<td>

<p>A <code>d x K</code> numeric matrix. <code>mu[, i]</code> is the initial mean for the <code>i</code>th Gaussian kernel. Default empty matrix.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_sigma">sigma</code></td>
<td>

<p>A <code>d^2 x K</code> numeric matrix. Each column represents a flattened <code>d x d</code> initial covariance matrix of the <code>i</code>th Gaussian kernel. In R, <code>as.numeric(aMatrix)</code> gives the flattened version of <code>aMatrix</code>. Covariance matrix of each Gaussian kernel MUST be positive-definite. Default empty.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_g">G</code></td>
<td>

<p>An integer. If at least one of the parameters <code>alpha</code>, <code>mu</code>, <code>sigma</code> are empty, the program will initialize <code>G</code> Gaussian kernels via K-means++ deterministic initialization. See <code>KMppIni()</code>. Otherwise <code>G</code> is ignored. Default 5.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_convergenceeps">convergenceEPS</code></td>
<td>

<p>A numeric value. If the average change of all parameters in the mixture model is below <code>convergenceEPS</code> relative to those in the pervious iteration, the program ends. Checking convergence this way is faster than recomputing the log-likelihood every iteration. Default 1e-5.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_alphaeps">alphaEPS</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's weight is no greater than <code>alphaEPS</code>, the kernel is deleted. Default 0.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_eigenratiolim">eigenRatioLim</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's max:min eigen value ratio exceeds <code>eigenRatioLim</code>, the kernel is treated as degenerate and deleted. Thresholding eigen ratios is in the interest of minimizing the effect of degenerate kernels in an early stage. Default <code>Inf</code>.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer, the maximal number of iterations.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. Should be no more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_tlimit">tlimit</code></td>
<td>

<p>A numeric value. The program exits with the current model in <code>tlimit</code> seconds.
</p>
</td></tr>
<tr><td><code id="GMcw_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Relevant details can be found in <code>GM()</code>. In <code>GMcw()</code>, an update of any Gaussian kernel triggers the update of the underlying weighing matrix that directs the update of all Gaussian kernels. Only after that the next Gaussian kernel is updated. See references.
</p>
<p>In the actual implementation, the <code>N x K</code> weighing matrix <code>WEI</code> does not exist in memory. An <code>N x K</code> density matrix <code>DEN</code> instead stores each Gaussian kernel's probability density at every observation in <code>X</code>. Mathematically, the <code>i</code>th column of <code>WEI</code> equals <code>DEN</code>'s <code>i</code>th column divided by the row sum <code>RS</code>. <code>RS</code> is a vector of size <code>N</code> and is memorized and updated responding to the update of each Gaussian kernel: before updating the <code>i</code>th kernel, the algorithm subtracts the <code>i</code>th column of <code>DEN</code> from <code>RS</code>; after the kernel is updated and the probability densities are recomputed, the algorithm adds back the <code>i</code>th column of <code>DEN</code> to <code>RS</code>. Now, to update the <code>i+1</code>th Gaussian kernel, we can divide the <code>i+1</code>th column of <code>DEN</code> by <code>RS</code> to get the weighing coefficients.
</p>
<p>The above implementation makes the component-wise trainer comparable to the classic one in terms of speed. The component-wise trainer is a key component in Figuredo &amp; jain's MML (minimum message length) mixture model trainer to avoid premature deaths of the Gaussian kernels.
</p>


<h3>Value</h3>

<p>A list of size 5:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>a numeric vector of size <code>K</code>. The mixture weights.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>a <code>d x K</code> numeric matrix. Each column is the mean of a Gaussian kernel.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>a <code>d^2 x K</code> numeric matrix. Each column is the flattened covariance matrix of a Gaussian kernel. Do <code>matrix(sigma[, i], nrow = d)</code> to recover the covariance matrix of the <code>i</code>th kernel.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>a numeric vector of size <code>N</code>. <code>fitted[i]</code> is the probability density of the <code>i</code>th observation given by the mixture model.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>a list of <code>K</code> integer vectors, the hard clustering inferred from the mixture model. Each integer vector contains the indexes of observations in <code>X</code>.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>For one-dimensional data, <code>X</code> should still follow the data structure requirements: a matrix where each column is an observation.
</p>


<h3>References</h3>

<p>Celeux, Gilles, et al. &quot;A Component-Wise EM Algorithm for Mixtures.&quot; Journal of Computational and Graphical Statistics, vol. 10, no. 4, 2001, pp. 697-712. JSTOR, www.jstor.org/stable/1390967.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># =============================================================================
# Examples below use 1 thread to pass CRAN check. Speed advantage of multiple
# threads will be more pronounced for larger data.
# =============================================================================


# =============================================================================
# Parameterize the iris data. Let the function initialize Gaussian kernels.
# =============================================================================
X = t(iris[1:4])
# CRAN check only allows 2 threads at most. Increase `maxCore` for
# acceleration.
gmmRst = GMKMcharlie::GMcw(X, G = 3L, maxCore = 1L)
str(gmmRst)




# =============================================================================
# Parameterize the iris data given Gaussian kernels.
# =============================================================================
G = 3L
d = nrow(X) # Dimensionality.
alpha = rep(1, G) / G
mu = X[, sample(ncol(X), G)] # Sample observations as initial means.
# Take the average variance and create initial covariance matrices.
meanVarOfEachDim = sum(diag(var(t(X)))) / d
covar = diag(meanVarOfEachDim / G, d)
covars = matrix(rep(as.numeric(covar), G), nrow = d * d)


# Models could be different given a different initialization.
gmmRst2 = GMKMcharlie::GMcw(
  X, alpha = alpha, mu = mu, sigma = covars, maxCore = 1L)
str(gmmRst2)




# =============================================================================
# For fun, fit Rosenbrock function with a Gaussian mixture.
# =============================================================================
set.seed(123)
rosenbrock &lt;- function(x, y) {(1 - x) ^ 2 + 100 * (y - x ^ 2) ^ 2}
N = 2000L
x = runif(N, -2, 2)
y = runif(N, -1, 3)
z = rosenbrock(x, y)


X = rbind(x, y)
Xw = z * (N / sum(z)) # Weights on observations should sum up to N.
gmmFit = GMKMcharlie::GMcw(X, Xw = Xw, G = 5L, maxCore = 1L, verbose = FALSE)


oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x, y, gmmFit$fitted, pch = 20)
par(mfrow = oldpar)




# =============================================================================
# For fun, fit a 3D spiral distribution.
# =============================================================================
N = 2000
t = runif(N) ^ 2 * 15
x = cos(t) + rnorm(N) * 0.1
y = sin(t) + rnorm(N) * 0.1
z = t + rnorm(N) * 0.1


X = rbind(x, y, z)
d = 3L
G = 10L
gmmFit = GMKMcharlie::GMcw(X, G = G, maxCore = 1L, verbose = FALSE)
# Sample N points from the Gaussian mixture.
ns = as.integer(round(N * gmmFit$alpha))
sampledPoints = list()
for(i in 1L : G)
{
  sampledPoints[[i]] = MASS::mvrnorm(
    ns[i], mu = gmmFit$mu[, i], Sigma = matrix(gmmFit$sigma[, i], nrow = d))
}
sampledPoints =
  matrix(unlist(lapply(sampledPoints, function(x) t(x))), nrow = d)


# Plot the original data and the samples from the mixture model.
oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x = sampledPoints[1, ],
                 y = sampledPoints[2, ],
                 z = sampledPoints[3, ],
                 pch = 20)
par(mfrow = oldpar)
</code></pre>

<hr>
<h2 id='GMfj'>
Multithreaded minimum message length Gaussian mixture trainer
</h2><span id='topic+GMfj'></span>

<h3>Description</h3>

<p>Figueiredo and Jain's Gaussian mixture trainer with all options in <code>GM()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMfj(
  X,
  Xw = rep(1, ncol(X)),
  alpha = numeric(0),
  mu = matrix(ncol = 0, nrow = 0),
  sigma = matrix(ncol = 0, nrow = 0),
  G = 5L,
  Gmin = 2L,
  convergenceEPS = 1e-05,
  alphaEPS = 0,
  eigenRatioLim = Inf,
  maxIter = 1000L,
  maxCore = 7L,
  tlimit = 3600,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GMfj_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of observations &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[, i]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_alpha">alpha</code></td>
<td>

<p>A numeric vector of size <code>K</code>, the number of Gaussian kernels in the mixture model. <code>alpha</code> are the initial mixture weights and should sum up to 1. Default empty.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_mu">mu</code></td>
<td>

<p>A <code>d x K</code> numeric matrix. <code>mu[, i]</code> is the initial mean for the <code>i</code>th Gaussian kernel. Default empty.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_sigma">sigma</code></td>
<td>

<p>A <code>d^2 x K</code> numeric matrix. Each column represents a flattened <code>d x d</code> initial covariance matrix of the <code>i</code>th Gaussian kernel. In R, <code>as.numeric(aMatrix)</code> gives the flattened version of <code>aMatrix</code>. Covariance matrix of each Gaussian kernel MUST be positive-definite. Default empty.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_g">G</code></td>
<td>

<p>An integer. If at least one of the parameters <code>alpha</code>, <code>mu</code>, <code>sigma</code> are empty, the program will initialize <code>G</code> Gaussian kernels via K-means++ deterministic initialization. See <code>KMppIni()</code>. Otherwise <code>G</code> is ignored. Default 5.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_gmin">Gmin</code></td>
<td>

<p>An integer. The final model should have at least <code>Gmin</code> kernels.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_convergenceeps">convergenceEPS</code></td>
<td>

<p>A numeric value. If the average change of all parameters in the mixture model is below <code>convergenceEPS</code> relative to those in the pervious iteration, the program ends. Checking convergence this way is faster than recomputing the log-likelihood every iteration. Default 1e-5.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_alphaeps">alphaEPS</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's weight is no greater than <code>alphaEPS</code>, the kernel is deleted. Default 0.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_eigenratiolim">eigenRatioLim</code></td>
<td>

<p>A numeric value. During training, if any Gaussian kernel's max:min eigen value ratio exceeds <code>eigenRatioLim</code>, the kernel is treated as degenerate and deleted. Thresholding eigen ratios is in the interest of minimizing the effect of degenerate kernels in an early stage. Default <code>Inf</code>.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer, the maximal number of iterations.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. Should be no more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_tlimit">tlimit</code></td>
<td>

<p>A numeric value. The program exits with the current model in <code>tlimit</code> seconds.
</p>
</td></tr>
<tr><td><code id="GMfj_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although heavily cited, the paper has some misleading information and the algorithm's performance does not live up to its reputation. See &lt;https://stats.stackexchange.com/questions/423935/figueiredo-and-jains-gaussian-mixture-em-convergence-criterion&gt;. Nevertheless, it is a worthwhile algorithm to try in practice.
</p>


<h3>Value</h3>

<p>A list of size 5:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>a numeric vector of size <code>K</code>. The mixture weights.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>a <code>d x K</code> numeric matrix. Each column is the mean of a Gaussian kernel.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>a <code>d^2 x K</code> numeric matrix. Each column is the flattened covariance matrix of a Gaussian kernel. Do <code>matrix(sigma[, i], nrow = d)</code> to recover the covariance matrix of the <code>i</code>th kernel.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>a numeric vector of size <code>N</code>. <code>fitted[i]</code> is the probability density of the <code>i</code>th observation given by the mixture model.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>a list of <code>K</code> integer vectors, the hard clustering inferred from the mixture model. Each integer vector contains the indexes of observations in <code>X</code>.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>For one-dimensional data, <code>X</code> should still follow the data structure requirements: a matrix where each column is an observation.
</p>


<h3>References</h3>

<p>Mario A.T. Figueiredo &amp; Anil K. Jain (2002): &quot;Unsupervised learning of finite mixture models.&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence 24(3): 381-396.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># =============================================================================
# Examples below use 1 thread to pass CRAN check. Speed advantage of multiple
# threads will be more pronounced for larger data.
# =============================================================================


# =============================================================================
# Parameterize the iris data. Let the function initialize Gaussian kernels.
# =============================================================================
X = t(iris[1:4])
# CRAN check only allows 2 threads at most. Increase `maxCore` for
# acceleration.
system.time({gmmRst = GMKMcharlie::GMfj(
  X, G = 25L, Gmin = 2L, maxCore = 1L, verbose = FALSE)})
str(gmmRst)




# =============================================================================
# Parameterize the iris data given Gaussian kernels.
# =============================================================================
G = 25L
d = nrow(X) # Dimensionality.
alpha = rep(1, G) / G
mu = X[, sample(ncol(X), G)] # Sample observations as initial means.
# Take the average variance and create initial covariance matrices.
meanVarOfEachDim = sum(diag(var(t(X)))) / d
covar = diag(meanVarOfEachDim / G, d)
covars = matrix(rep(as.numeric(covar), G), nrow = d * d)


# Models are sensitive to initialization.
system.time({gmmRst2 = GMKMcharlie::GMfj(
  X, alpha = alpha, mu = mu, sigma = covars, maxCore = 1L, verbose = FALSE)})
str(gmmRst2)




# =============================================================================
# For fun, fit Rosenbrock function with a Gaussian mixture.
# =============================================================================
set.seed(123)
rosenbrock &lt;- function(x, y) {(1 - x) ^ 2 + 100 * (y - x ^ 2) ^ 2}
N = 2000L
x = runif(N, -2, 2)
y = runif(N, -1, 3)
z = rosenbrock(x, y)


X = rbind(x, y)
Xw = z * (N / sum(z)) # Weights on observations should sum up to N.
system.time({gmmFit = GMKMcharlie::GMfj(
  X, Xw = Xw, G = 5L, maxCore = 1L, verbose = FALSE)})


oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x, y, gmmFit$fitted, pch = 20)
par(mfrow = oldpar)




# =============================================================================
# For fun, fit a 3D spiral distribution.
# =============================================================================
N = 2000
t = runif(N) ^ 2 * 15
x = cos(t) + rnorm(N) * 0.1
y = sin(t) + rnorm(N) * 0.1
z = t + rnorm(N) * 0.1


X = rbind(x, y, z)
d = 3L
G = 10L
system.time({gmmFit = GMKMcharlie::GMfj(
  X, G = G, maxCore = 1L, verbose = FALSE)})
# Sample N points from the Gaussian mixture.
ns = as.integer(round(N * gmmFit$alpha))
sampledPoints = list()
for(i in 1L : G)
{
  sampledPoints[[i]] = MASS::mvrnorm(
    ns[i], mu = gmmFit$mu[, i], Sigma = matrix(gmmFit$sigma[, i], nrow = d))
}
sampledPoints =
  matrix(unlist(lapply(sampledPoints, function(x) t(x))), nrow = d)


# Plot the original data and the samples from the mixture model.
oldpar = par()$mfrow
par(mfrow = c(1, 2))
plot3D::points3D(x, y, z, pch = 20)
plot3D::points3D(x = sampledPoints[1, ],
                 y = sampledPoints[2, ],
                 z = sampledPoints[3, ], pch = 20)
par(mfrow = oldpar)
</code></pre>

<hr>
<h2 id='KM'>
K-means over dense representation of data
</h2><span id='topic+KM'></span>

<h3>Description</h3>

<p>Multithreaded weighted Minkowski and spherical K-means via Lloyd's algorithm over dense representation of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KM(
  X,
  centroid,
  Xw = rep(1, ncol(X)),
  minkP = 2,
  maxIter = 100L,
  maxCore = 7L,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KM_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of data points &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="KM_+3A_centroid">centroid</code></td>
<td>

<p>A <code>d x K</code> numeric matrix where <code>K</code> is the number of clusters. Each column represents a cluster center.
</p>
</td></tr>
<tr><td><code id="KM_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[, i]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="KM_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KM_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer. The maximal number of iterations. Default 100.
</p>
</td></tr>
<tr><td><code id="KM_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KM_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Implementation highlights include:
</p>
<p>(i) In Minkowski distance calculation, integer power no greater than 30 uses multiplications. Fractional powers or powers above 30 call <code>std::pow()</code>.
</p>
<p>(ii) Multithreaded observation-centroid distance calculations. Distances are memorized to avoid unnecessary recomputations if centroids did not change in the last iteration.
</p>
<p>(iii) A lookup table is built for storing observation - centroid ID pairs during the assignment step. Observation IDs are then grouped by centroid IDs which allows parallel computing cluster means.
</p>
<p>(iv) Function allows non-uniform weights on observations.
</p>
<p>(v) Meta-template programming trims branches over different distance functions and other computing methods during compile time.
</p>


<h3>Value</h3>

<p>A list of size <code>K</code>, the number of clusters. Each element is a list of 3 vectors:
</p>
<table>
<tr><td><code>centroid</code></td>
<td>
<p>a numeric vector of size <code>d</code>.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>an integer vector of indexes of elements grouped to <code>centroid</code>.</p>
</td></tr>
<tr><td><code>member2centroidDistance</code></td>
<td>
<p>a numeric vector of the same size of <code>clusterMember</code>. The <code>i</code>th element is the Minkowski distance or cosine dissimilarity from <code>centroid</code> to the <code>clusterMember[i]</code>th observation in <code>X</code>.</p>
</td></tr>
</table>
<p>Empty <code>clusterMember</code> implies empty cluster.
</p>


<h3>Note</h3>

<p>Although rarely happens, divergence of K-means with non-Euclidean distance <code>minkP != 2</code> measure is still a theoretical possibility.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ===========================================================================
# Play random numbers. See speed.
# ===========================================================================
N = 5000L # Number of points.
d = 500L # Dimensionality.
K = 50L # Number of clusters.
dat = matrix(rnorm(N * d) + runif(N * d), nrow = d)


# Use kmeans++ initialization.
centroidInd = GMKMcharlie::KMppIni(
  X = dat, K, firstSelection = 1L, minkP = 2, stochastic = FALSE,
  seed = sample(1e9L, 1), maxCore = 2L, verbose = TRUE)


centroid = dat[, centroidInd]


# Euclidean.
system.time({rst = GMKMcharlie::KM(
  X = dat, centroid = centroid, maxIter = 100,
  minkP = 2, maxCore = 2, verbose = TRUE)})


# Cosine dissimilarity.
dat = apply(dat, 2, function(x) x / sum(x ^ 2) ^ 0.5)
centroid = dat[, centroidInd]
system.time({rst2 = GMKMcharlie::KM(
  X = dat, centroid = centroid, maxIter = 100,
  minkP = "cosine", maxCore = 2, verbose = TRUE)})


# ===========================================================================
# Test against R's inbuilt km()
# ===========================================================================
dat = t(iris[1:4])
dimnames(dat) = NULL


# Use kmeans++ initialization.
centroidInd = GMKMcharlie::KMppIni(
  X = dat, K = 3, firstSelection = 1L, minkP = 2, stochastic = FALSE,
  seed = sample(1e9L, 1), maxCore = 2L, verbose = TRUE)
centroid = dat[, centroidInd]


rst = GMKMcharlie::KM(X = dat, centroid = centroid, maxIter = 100,
                      minkP = 2, maxCore = 2, verbose = TRUE)
rst = lapply(rst, function(x) sort(x$clusterMember))


rst2 = kmeans(x = t(dat), centers = t(centroid), algorithm = "Lloyd")
rst2 = aggregate(list(1L : length(rst2$cluster)),
                 list(rst2$cluster), function(x) sort(x))[[2]]


setdiff(rst, rst2)
</code></pre>

<hr>
<h2 id='KMconstrained'>
K-means over dense data input with constraints on cluster weights
</h2><span id='topic+KMconstrained'></span>

<h3>Description</h3>

<p>Multithreaded weighted Minkowski and spherical K-means via Lloyd's algorithm over dense representation of data given cluster size (weight) constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMconstrained(
  X,
  centroid,
  Xw = rep(1, ncol(X)),
  clusterWeightUB = rep(ncol(X) + 1, ncol(centroid)),
  minkP = 2,
  convergenceTail = 5L,
  tailConvergedRelaErr = 1e-04,
  maxIter = 100L,
  maxCore = 7L,
  paraSortInplaceMerge = FALSE,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMconstrained_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of data points &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_centroid">centroid</code></td>
<td>

<p>A <code>d x K</code> numeric matrix where <code>K</code> is the number of clusters. Each column represents a cluster center.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[, i]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_clusterweightub">clusterWeightUB</code></td>
<td>

<p>An integer vector of size <code>K</code>. The upper bound of weight for each cluster. If <code>Xw</code> are all 1, <code>clusterWeightUB</code> upper-bound cluster sizes.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_convergencetail">convergenceTail</code></td>
<td>

<p>An integer. The algorithm may end up with &quot;cyclical convergence&quot; due to the size / weight constraints, that is, every few iterations produce the same clustering. If the cost (total in-cluster distance) of each of the last <code>convergenceTail</code> iterations has a relative difference less than <code>tailConvergedRelaErr</code> against the cost from the prior iteration, the program stops.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_tailconvergedrelaerr">tailConvergedRelaErr</code></td>
<td>

<p>A numeric value, explained in <code>convergenceTail</code>.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer. The maximal number of iterations. Default 100.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_parasortinplacemerge">paraSortInplaceMerge</code></td>
<td>

<p>A boolean value. <code>TRUE</code> let the algorithm call <code>std::inplace_merge()</code> (<code>std</code> refers to C++ STL namespace) instead of <code>std::merge()</code> for parallel-sorting the observation-centroid distances. In-place merge is slower but requires no extra memory.
</p>
</td></tr>
<tr><td><code id="KMconstrained_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See details in <code>KM()</code> for common implementation highlights. Weight upper bounds are implemented as follows:
</p>
<p>In each iteration, all the (observation ID, centroid ID, distance) tuples are sorted by distance. From the first to the last tuple, the algorithm puts observation in the cluster labeled by the centroid ID, if (i) the observation has not already been assigned and (ii) the cluster size has not exceeded its upper bound. The actual implementation is slightly different. A parallel merge sort is crafted for computing speed.
</p>


<h3>Value</h3>

<p>A list of size <code>K</code>, the number of clusters. Each element is a list of 3 vectors:
</p>
<table>
<tr><td><code>centroid</code></td>
<td>
<p>a numeric vector of size <code>d</code>.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>an integer vector of indexes of elements grouped to <code>centroid</code>.</p>
</td></tr>
<tr><td><code>member2centroidDistance</code></td>
<td>
<p>a numeric vector of the same size of <code>clusterMember</code>. The <code>i</code>th element is the Minkowski distance or cosine dissimilarity from <code>centroid</code> to the <code>clusterMember[i]</code>th observation in <code>X</code>.</p>
</td></tr>
</table>
<p>Empty <code>clusterMember</code> implies empty cluster.
</p>


<h3>Note</h3>

<p>Although rarely happens, divergence of K-means with non-Euclidean distance <code>minkP != 2</code> measure is still a theoretical possibility. Bounding the cluster weights / sizes increases the chance of divergence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 3000L # Number of points.
d = 500L # Dimensionality.
K = 50L # Number of clusters.
dat = matrix(rnorm(N * d) + runif(N * d), nrow = d)


# Use kmeans++ initialization.
centroidInd = GMKMcharlie::KMppIni(
  X = dat, K, firstSelection = 1L, minkP = 2, stochastic = FALSE,
  seed = sample(1e9L, 1), maxCore = 2L, verbose = TRUE)


centroid = dat[, centroidInd]


# Each cluster size should not be greater than N / K * 2.
sizeConstraints = as.integer(rep(N / K * 2, K))
system.time({rst = GMKMcharlie::KMconstrained(
  X = dat, centroid = centroid, clusterWeightUB = sizeConstraints,
  maxCore = 2L, tailConvergedRelaErr = 1e-6, verbose = TRUE)})


# Size upper bounds vary in [N / K * 1.5, N / K * 2]
sizeConstraints = as.integer(round(runif(K, N / K * 1.5, N / K * 2)))
system.time({rst = GMKMcharlie::KMconstrained(
  X = dat, centroid = centroid, clusterWeightUB = sizeConstraints,
  maxCore = 2L, tailConvergedRelaErr = 1e-6, verbose = TRUE)})
</code></pre>

<hr>
<h2 id='KMconstrainedSparse'>
K-means over sparse data input with constraints on cluster weights
</h2><span id='topic+KMconstrainedSparse'></span>

<h3>Description</h3>

<p>Multithreaded weighted Minkowski and spherical K-means via Lloyd's algorithm over sparse representation of data given cluster size (weight) constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMconstrainedSparse(
  X,
  d,
  centroid,
  Xw = rep(1, length(X)),
  clusterWeightUB = rep(length(X) + 1, length(centroid)),
  minkP = 2,
  convergenceTail = 5L,
  tailConvergedRelaErr = 1e-04,
  maxIter = 100L,
  maxCore = 7L,
  paraSortInplaceMerge = FALSE,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMconstrainedSparse_+3A_x">X</code></td>
<td>

<p>A list of size <code>N</code>, the number of observations. <code>X[[i]]</code> is a 2-column data frame. The 1st column is a sorted <strong>integer vector</strong> of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a <strong>numeric vector</strong>. Internally the algorithm sets a 32-bit <em>int</em> pointer to the beginning of the 1st column and a 64-bit <em>double</em> pointer to the beginning of the 2nd column, so it is critical that the input has the correct type.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_d">d</code></td>
<td>

<p>An integer. The dimensionality of <code>X</code>. <code>d</code> MUST be no less than the maximum of all index vectors in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_centroid">centroid</code></td>
<td>

<p>A list of size <code>K</code>, the number of clusters. <code>centroid[[i]]</code> can be in dense or sparse representation. If dense, a numeric vector of size <code>d</code>. If sparse, a 2-column data frame in the same sense as <code>X[[i]]</code>.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[[i]]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_clusterweightub">clusterWeightUB</code></td>
<td>

<p>An integer vector of size <code>K</code>. The upper bound of weight for each cluster. If <code>Xw</code> are all 1s, <code>clusterWeightUB</code> upper-bound cluster sizes.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_convergencetail">convergenceTail</code></td>
<td>

<p>An integer. The algorithm may end up with &quot;cyclical convergence&quot; due to the size / weight constraints, that is, every few iterations produce the same clustering. If the cost (total in-cluster distance) of each of the last <code>convergenceTail</code> iterations has a relative difference less than <code>tailConvergedRelaErr</code> against the cost from the prior iteration, the program stops.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_tailconvergedrelaerr">tailConvergedRelaErr</code></td>
<td>

<p>A numeric value, explained in <code>convergenceTail</code>.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer. The maximal number of iterations. Default 100.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_parasortinplacemerge">paraSortInplaceMerge</code></td>
<td>

<p>A boolean value. <code>TRUE</code> let the algorithm call <code>std::inplace_merge()</code> (<code>std</code> refers to C++ STL namespace) instead of <code>std::merge()</code> for parallel-sorting the observation-centroid distances. In-place merge is slower but requires no extra memory.
</p>
</td></tr>
<tr><td><code id="KMconstrainedSparse_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See details for <code>KMconstrained()</code> and <code>KM()</code>
</p>


<h3>Value</h3>

<p>A list of size <code>K</code>, the number of clusters. Each element is a list of 3 vectors:
</p>
<table>
<tr><td><code>centroid</code></td>
<td>
<p>a numeric vector of size <code>d</code>.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>an integer vector of indexes of elements grouped to <code>centroid</code>.</p>
</td></tr>
<tr><td><code>member2centroidDistance</code></td>
<td>
<p>a numeric vector of the same size of <code>clusterMember</code>. The <code>i</code>th element is the Minkowski distance or cosine dissimilarity from <code>centroid</code> to the <code>clusterMember[i]</code>th observation in <code>X</code>.</p>
</td></tr>
</table>
<p>Empty <code>clusterMember</code> implies empty cluster.
</p>


<h3>Note</h3>

<p>Although rarely happens, divergence of K-means with non-Euclidean distance <code>minkP != 2</code> measure is still a theoretical possibility. Bounding the cluster weights / sizes increases the chance of divergence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 5000L # Number of points.
d = 500L # Dimensionality.
K = 50L # Number of clusters.


# Create a data matrix, about 95% of which are zeros.
dat = matrix(unlist(lapply(1L : N, function(x)
{
  tmp = numeric(d)
  # Nonzero entries.
  Nnz = as.integer(max(1, d * runif(1, 0, 0.05)))
  tmp[sample(d, Nnz)] = runif(Nnz) + rnorm(Nnz)
  tmp
})), nrow = d); gc()


# Convert to sparse representation.
# GMKMcharlie::d2s() is equivalent.
sparsedat = apply(dat, 2, function(x)
{
  nonz = which(x != 0)
  list(nonz, x[nonz])
}); gc()


centroidInd = sample(length(sparsedat), K)


# Test speed using sparse representation.
sparseCentroid = sparsedat[centroidInd]
# Size upper bounds vary in [N / K * 1.5, N / K * 2]
sizeConstraints = as.integer(round(runif(K, N / K * 1.5, N / K * 2)))
system.time({sparseRst = GMKMcharlie::KMconstrainedSparse(
  X = sparsedat, d = d, centroid = sparseCentroid,
  clusterWeightUB = sizeConstraints,
  tailConvergedRelaErr = 1e-6,
  maxIter = 100, minkP = 2, maxCore = 2, verbose = TRUE)})
</code></pre>

<hr>
<h2 id='KMppIni'>
Minkowski and spherical, deterministic and stochastic, multithreaded K-means++ initialization over dense representation of data
</h2><span id='topic+KMppIni'></span>

<h3>Description</h3>

<p>Find suitable observations as initial centroids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMppIni(
  X,
  K,
  firstSelection = 1L,
  minkP = 2,
  stochastic = FALSE,
  seed = 123,
  maxCore = 7L,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMppIni_+3A_x">X</code></td>
<td>

<p>A <code>d x N</code> numeric matrix where <code>N</code> is the number of data points &mdash; each column is an observation, and <code>d</code> is the dimensionality. Column-observation representation promotes cache locality.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_k">K</code></td>
<td>

<p>An integer, the number of centroids.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_firstselection">firstSelection</code></td>
<td>

<p>An integer, index of the observation selected as the first initial centroid in <code>X</code>. Should be no greater than <code>N</code>.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_stochastic">stochastic</code></td>
<td>

<p>A boolean value. <code>TRUE</code> runs the stochastic K-means++ initialization by Arthur and Vassilvitskii (2007). Roughly speaking, the algorithm is stochastic in the sense that each of the remaining observations has a probability of being selected as the next centroid, and the probability is an increasing function of the minimal distance between this observation and the existing centroids. In the same context, the deterministic version selects as the next centroid with probability 1 the observation that has the longest minimal distance to the existing centroids.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_seed">seed</code></td>
<td>

<p>Random seed if <code>stochastic</code>.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KMppIni_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In each iteration, the distances between the newly selected centroid and all the other observations are computed with multiple threads. Scheduling is homemade for minimizing the overhead of thread communication.
</p>


<h3>Value</h3>

<p>An integer vector of size <code>K</code>. The vector contains the indexes of observations selected as the initial centroids.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 30000L
d = 300L
K = 30L
X = matrix(rnorm(N * d) + 2, nrow = d)
# CRAN check allows examples invoking 2 threads at most. Change `maxCore`
# for acceleration.
kmppSt = KMppIni(X, K, firstSelection = 1L, minkP = 2,
                 stochastic = TRUE, seed = sample(1e9L, 1), maxCore = 2L)
kmppDt = KMppIni(X, K, firstSelection = 1L, minkP = 2,
                 stochastic = FALSE, maxCore = 2L)
str(kmppSt)
str(kmppDt)
</code></pre>

<hr>
<h2 id='KMppIniSparse'>
Minkowski and spherical, deterministic and stochastic, multithreaded K-means++ initialization over sparse representation of data
</h2><span id='topic+KMppIniSparse'></span>

<h3>Description</h3>

<p>Find suitable observations as initial centroids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMppIniSparse(
  X,
  d,
  K,
  firstSelection = 1L,
  minkP = 2,
  stochastic = FALSE,
  seed = 123,
  maxCore = 7L,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMppIniSparse_+3A_x">X</code></td>
<td>

<p>A list of size <code>N</code>, the number of observations. <code>X[[i]]</code> is a 2-column data frame. The 1st column is a sorted <strong>integer vector</strong> of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a <strong>numeric vector</strong>. Internally the algorithm sets a 32-bit <em>int</em> pointer to the beginning of the 1st column and a 64-bit <em>double</em> pointer to the beginning of the 2nd column, so it is critical that the input has the correct type.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_d">d</code></td>
<td>

<p>An integer. The dimensionality of <code>X</code>. <code>d</code> MUST be no less than the maximum of all index vectors in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_k">K</code></td>
<td>

<p>An integer, the number of centroids.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_firstselection">firstSelection</code></td>
<td>

<p>An integer, index of the observation selected as the first initial centroid in <code>X</code>. Should be no greater than <code>N</code>.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_stochastic">stochastic</code></td>
<td>

<p>A boolean value. <code>TRUE</code> runs the stochastic K-means++ initialization by Arthur and Vassilvitskii (2007). Roughly speaking, the algorithm is stochastic in the sense that each of the remaining observations has a probability of being selected as the next centroid, and the probability is an increasing function of the minimal distance between this observation and the existing centroids. In the same context, the deterministic version selects as the next centroid with probability 1 the observation that has the longest minimal distance to the existing centroids.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_seed">seed</code></td>
<td>

<p>Random seed if <code>stochastic</code>.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KMppIniSparse_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In each iteration, the distances between the newly selected centroid and all the other observations are computed with multiple threads. Scheduling is homemade for minimizing the overhead of thread communication.
</p>


<h3>Value</h3>

<p>An integer vector of size <code>K</code>. The vector contains the indexes of observations selected as the initial centroids.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 2000L
d = 3000L
X = matrix(rnorm(N * d) + 2, nrow = d)
# Fill many zeros in X:
X = apply(X, 2, function(x) {
  x[sort(sample(d, d * runif(1, 0.95, 0.99)))] = 0; x})
# Get the sparse version of X.
sparseX = GMKMcharlie::d2s(X)


K = 30L
seed = 123L
# Time cost of finding the centroids via dense representation.
# CRAN check allows only 2 threads. Increase `maxCore` for more speed.
system.time({kmppViaDense = GMKMcharlie::KMppIni(
  X, K, firstSelection = 1L, minkP = 2, stochastic = TRUE, seed = seed,
  maxCore = 2L)})


# Time cost of finding the initial centroids via sparse representation.
system.time({kmppViaSparse = GMKMcharlie::KMppIniSparse(
  sparseX, d, K, firstSelection = 1L, minkP = 2, stochastic = TRUE,
  seed = seed, maxCore = 2L)})


# Results should be identical.
sum(kmppViaSparse - kmppViaDense)
</code></pre>

<hr>
<h2 id='KMsparse'>
K-means over sparse representation of data
</h2><span id='topic+KMsparse'></span>

<h3>Description</h3>

<p>Multithreaded weighted Minkowski and spherical K-means via Lloyd's algorithm over sparse representation of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMsparse(
  X,
  d,
  centroid,
  Xw = rep(1, length(X)),
  minkP = 2,
  maxIter = 100L,
  maxCore = 7L,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMsparse_+3A_x">X</code></td>
<td>

<p>A list of size <code>N</code>, the number of observations. <code>X[[i]]</code> is a 2-column data frame. The 1st column is a sorted <strong>integer vector</strong> of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a <strong>numeric vector</strong>. Internally the algorithm sets a 32-bit <em>int</em> pointer to the beginning of the 1st column and a 64-bit <em>double</em> pointer to the beginning of the 2nd column, so it is critical that the input has the correct type.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_d">d</code></td>
<td>

<p>An integer. The dimensionality of <code>X</code>. <code>d</code> MUST be no less than the maximum of all index vectors in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_centroid">centroid</code></td>
<td>

<p>A list of size <code>K</code>, the number of clusters. <code>centroid[[i]]</code> can be in dense or sparse representation. If dense, a numeric vector of size <code>d</code>. If sparse, a 2-column data frame in the same sense as <code>X[[i]]</code>.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_xw">Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[[i]]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_minkp">minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_maxiter">maxIter</code></td>
<td>

<p>An integer. The maximal number of iterations. Default 100.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_maxcore">maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td></tr>
<tr><td><code id="KMsparse_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See details in <code>KM()</code> for implementation highlights. There are some other optimizations such as, except for the maximum norm, cost of computing the distance between a dense centroid vector and a sparse observation is linear to the size of the sparse observation, which should be largely less than the size of the dense vector. This is done by letting every centroid memorize its before-root Minkowski norm. The full distance can then be inferred from adding the residual norm to the partial distance.
</p>


<h3>Value</h3>

<p>A list of size <code>K</code>, the number of clusters. Each element is a list of 3 vectors:
</p>
<table>
<tr><td><code>centroid</code></td>
<td>
<p>a numeric vector of size <code>d</code>.</p>
</td></tr>
<tr><td><code>clusterMember</code></td>
<td>
<p>an integer vector of indexes of elements grouped to <code>centroid</code>.</p>
</td></tr>
<tr><td><code>member2centroidDistance</code></td>
<td>
<p>a numeric vector of the same size of <code>clusterMember</code>. The <code>i</code>th element is the Minkowski distance or cosine dissimilarity from <code>centroid</code> to the <code>clusterMember[i]</code>th observation in <code>X</code>.</p>
</td></tr>
</table>
<p>Empty <code>clusterMember</code> implies empty cluster.
</p>


<h3>Note</h3>

<p>Although rarely happens, divergence of K-means with non-Euclidean distance <code>minkP != 2</code> measure is still a theoretical possibility.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ===========================================================================
# Play random numbers. See speed.
# ===========================================================================
N = 10000L # Number of points.
d = 500L # Dimensionality.
K = 100L # Number of clusters.


# Create a data matrix, about 95% of which are zeros.
dat = matrix(unlist(lapply(1L : N, function(x)
{
  tmp = numeric(d)
  # Nonzero entries.
  Nnz = as.integer(max(1, d * runif(1, 0, 0.05)))
  tmp[sample(d, Nnz)] = runif(Nnz) + rnorm(Nnz)
  tmp
})), nrow = d); gc()


# Convert to sparse representation.
# GMKMcharlie::d2s() acheives the same.
sparsedat = apply(dat, 2, function(x)
{
  nonz = which(x != 0)
  list(nonz, x[nonz])
}); gc()


centroidInd = sample(length(sparsedat), K)


# Test speed using dense representation.
centroid = dat[, centroidInd]
system.time({rst = GMKMcharlie::KM(
  X = dat, centroid = centroid, maxIter = 100,
  minkP = 2, maxCore = 2, verbose = TRUE)})


# Test speed using sparse representation.
sparseCentroid = sparsedat[centroidInd]
system.time({sparseRst = GMKMcharlie::KMsparse(
  X = sparsedat, d = d, centroid = sparseCentroid,
  maxIter = 100, minkP = 2, maxCore = 2, verbose = TRUE)})
</code></pre>

<hr>
<h2 id='s2d'>
Sparse to dense conversion
</h2><span id='topic+s2d'></span>

<h3>Description</h3>

<p>Convert data from sparse representation (list of data frames) to dese representation (matrix).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s2d(
  X,
  d,
  zero = 0,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s2d_+3A_x">X</code></td>
<td>

<p>A list of size <code>N</code>, the number of observations. <code>X[[i]]</code> is a 2-column data frame. The 1st column is a sorted integer vector of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a numeric vector.
</p>
</td></tr>
<tr><td><code id="s2d_+3A_d">d</code></td>
<td>

<p>An integer. The dimensionality of <code>X</code>. <code>d</code> MUST be no less than the maximum of all index vectors in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="s2d_+3A_zero">zero</code></td>
<td>

<p>A numeric value. In the result matrix, entries not registered in <code>X</code> will be filled with <code>zero</code>.
</p>
</td></tr>
<tr><td><code id="s2d_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>d x N</code> numeric matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 2000L
d = 3000L
X = matrix(rnorm(N * d) + 2, nrow = d)
# Fill many zeros in X:
X = apply(X, 2, function(x) {
  x[sort(sample(d, d * runif(1, 0.95, 0.99)))] = 0; x})
# Get the sparse version of X.
sparseX = GMKMcharlie::d2s(X)
# Convert it back to dense.
X2 = GMKMcharlie::s2d(sparseX, d)
range(X - X2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
