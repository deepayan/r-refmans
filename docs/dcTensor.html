<!DOCTYPE html><html><head><title>Help for package dcTensor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dcTensor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dcTensor-package'>
<p>Discrete Matrix/Tensor Decomposition</p></a></li>
<li><a href='#djNMF'>
<p>Discretized Joint Non-negative Matrix Factorization Algorithms (djNMF)</p></a></li>
<li><a href='#dNMF'>
<p>Discretized Non-negative Matrix Factorization Algorithms (dNMF)</p></a></li>
<li><a href='#dNMTF'>
<p>Discretized Non-negative Matrix Tri-Factorization Algorithms (dNMTF)</p></a></li>
<li><a href='#dNTD'>
<p>Discretized Non-negative Tucker Decomposition Algorithms (dNTD)</p></a></li>
<li><a href='#dNTF'>
<p>Discretized Non-negative CP Decomposition Algorithms (dNTF)</p></a></li>
<li><a href='#dPLS'>
<p>Discretized Partial Least Squares (dPLS)</p></a></li>
<li><a href='#dsiNMF'>
<p>Discretized Simultaneous Non-negative Matrix Factorization Algorithms (dsiNMF)</p></a></li>
<li><a href='#dSVD'>
<p>Discretized Singular Value Decomposition (dSVD)</p></a></li>
<li><a href='#toyModel'>
<p>Toy model data for using dNMF, dSVD, dsiNMF, djNMF, dPLS, dNTF, and dNTD</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Discrete Matrix/Tensor Decomposition</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, MASS, fields, rTensor, nnTensor</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>Description:</td>
<td>Semi-Binary and Semi-Ternary Matrix Decomposition are performed based on Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). For the details of the methods, see the reference section of GitHub README.md <a href="https://github.com/rikenbit/dcTensor">https://github.com/rikenbit/dcTensor</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rikenbit/dcTensor">https://github.com/rikenbit/dcTensor</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-11 13:29:56 UTC; root</td>
</tr>
<tr>
<td>Author:</td>
<td>Koki Tsuyuzaki [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-11 13:53:33 UTC</td>
</tr>
</table>
<hr>
<h2 id='dcTensor-package'>
Discrete Matrix/Tensor Decomposition
</h2><span id='topic+dcTensor-package'></span><span id='topic+dcTensor'></span>

<h3>Description</h3>

<p>Semi-Binary and Semi-Ternary Matrix Decomposition are performed based on Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). For the details of the methods, see the reference section of GitHub README.md &lt;https://github.com/rikenbit/dcTensor&gt;.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> dcTensor</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Discrete Matrix/Tensor Decomposition</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.3.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person("Koki", "Tsuyuzaki", role = c("aut", "cre"),
                      email = "k.t.the-answer@hotmail.co.jp"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.4.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> methods,
MASS,
fields,
rTensor,
nnTensor</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> knitr,
rmarkdown,
testthat</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Semi-Binary and Semi-Ternary Matrix Decomposition are performed based on Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). For the details of the methods, see the reference section of GitHub README.md &lt;https://github.com/rikenbit/dcTensor&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> MIT + file LICENSE</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://github.com/rikenbit/dcTensor</td>
</tr>
<tr>
 <td style="text-align: left;">
VignetteBuilder: </td><td style="text-align: left;"> knitr</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Koki Tsuyuzaki [aut, cre]</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
dNMF                    Discretized Non-negative Matrix Factorization
                        Algorithms (dNMF)
dNMTF                   Discretized Non-negative Matrix
                        Tri-Factorization Algorithms (dNMTF)
dNTD                    Discretized Non-negative Tucker Decomposition
                        Algorithms (dNTD)
dNTF                    Discretized Non-negative CP Decomposition
                        Algorithms (dNTF)
dPLS                    Discretized Partial Least Squares (dPLS)
dSVD                    Discretized Singular Value Decomposition (dSVD)
dcTensor-package        Discrete Matrix/Tensor Decomposition
djNMF                   Discretized Joint Non-negative Matrix
                        Factorization Algorithms (djNMF)
dsiNMF                  Discretized Simultaneous Non-negative Matrix
                        Factorization Algorithms (dsiNMF)
toyModel                Toy model data for using dNMF, dSVD, dsiNMF,
                        djNMF, dPLS, dNTF, and dNTD
</pre>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>


<h3>References</h3>

<p>Z. Zhang, T. Li, C. Ding and X. Zhang, (2007). Binary Matrix Factorization with Applications, <em>Seventh IEEE International Conference on Data Mining (ICDM 2007)</em>, 391-400
</p>


<h3>See Also</h3>

<p><code><a href="#topic+toyModel">toyModel</a></code>,<code><a href="#topic+dNMF">dNMF</a></code>,<code><a href="#topic+dSVD">dSVD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>ls("package:dcTensor")
</code></pre>

<hr>
<h2 id='djNMF'>
Discretized Joint Non-negative Matrix Factorization Algorithms (djNMF)
</h2><span id='topic+djNMF'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::jNMF.
The input data objects are assumed to be a list containing multiple non-negative matrices (X_1, X_2, ..., X_K),
and decomposed to multiple matrix products
((W + V_1) H_1', (W + V_2) H_2', ..., (W + V_K) H_K'),
where W is common across all the data matrices
but each V_k or H_k (k=1..K) is specific in each X_k.
Unlike regular jNMF, in djNMF,
W, V_k, and H_k are estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, W, V_k, and H_k are estimated by adding ternary regularization
so that the values are 0, 1, or 2 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>djNMF(X, M=NULL, pseudocount=.Machine$double.eps,
    initW=NULL, initV=NULL, initH=NULL,
    fixW=FALSE, fixV=FALSE, fixH=FALSE,
    Bin_W=1e-10, Bin_V=rep(1e-10, length=length(X)), Bin_H=rep(1e-10, length=length(X)),
    Ter_W=1e-10, Ter_V=rep(1e-10, length=length(X)), Ter_H=rep(1e-10, length=length(X)),
    L1_W=1e-10, L1_V=rep(1e-10, length=length(X)), L1_H=rep(1e-10, length=length(X)),
    L2_W=1e-10, L2_V=rep(1e-10, length=length(X)), L2_H=rep(1e-10, length=length(X)),
    J = 3, w=NULL, algorithm = c("Frobenius", "KL", "IS", "PLTF"), p=1,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="djNMF_+3A_x">X</code></td>
<td>

<p>A list containing input matrices (X_k, &lt;N*Mk&gt;, k=1..K).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_m">M</code></td>
<td>

<p>A list containing the mask matrices (X_k, &lt;N*Mk&gt;, k=1..K). If the input matrix
has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_initw">initW</code></td>
<td>

<p>The initial values of factor matrix W, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_initv">initV</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(V_k, &lt;N*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_inith">initH</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(H_k, &lt;Mk*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_fixw">fixW</code></td>
<td>

<p>Whether the factor matrix W is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrices Vk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_fixh">fixH</code></td>
<td>

<p>Whether the factor matrices Hk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_bin_w">Bin_W</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_bin_v">Bin_V</code></td>
<td>

<p>A K-length vector containing the paramters for binary (0,1) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_bin_h">Bin_H</code></td>
<td>

<p>A K-length vector containing the paramters for binary (0,1) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_ter_w">Ter_W</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_ter_v">Ter_V</code></td>
<td>

<p>A K-length vector containing the paramters for terary (0,1,2) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_ter_h">Ter_H</code></td>
<td>

<p>A K-length vector containing the paramters for terary (0,1,2) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l1_w">L1_W</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l1_v">L1_V</code></td>
<td>

<p>A K-length vector containing the paramters for L1 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l1_h">L1_H</code></td>
<td>

<p>A K-length vector containing the paramters for L1 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l2_w">L2_W</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l2_v">L2_V</code></td>
<td>

<p>A K-length vector containing the paramters for L2 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_l2_h">L2_H</code></td>
<td>

<p>A K-length vector containing the paramters for L2 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_j">J</code></td>
<td>

<p>Number of low-dimension (J &lt; N, Mk).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_w">w</code></td>
<td>

<p>Weight vector (Default: NULL)
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_algorithm">algorithm</code></td>
<td>

<p>Divergence between X and X_bar. &quot;Frobenius&quot;, &quot;KL&quot;, and &quot;IS&quot; are available
(Default: &quot;KL&quot;).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_p">p</code></td>
<td>

<p>The parameter of Probabilistic Latent Tensor Factorization
(p=0: Frobenius, p=1: KL, p=2: IS)
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="djNMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>W : A matrix which has N-rows and J-columns (J &lt; N, Mk).
V : A list which has multiple elements containing N-rows and J-columns
(J &lt; N, Mk).
H : A list which has multiple elements containing Mk-rows and J-columns matrix
(J &lt; N, Mk).
RecError : The reconstruction error between data matrix and reconstructed matrix
from W and H.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Liviu Badea, (2008) Extracting Gene Expression Profiles Common to Colon and
Pancreatic Adenocarcinoma using Simultaneous nonnegative matrix factorization.
<em>Pacific Symposium on Biocomputing</em> 13:279-290
</p>
<p>Shihua Zhang, et al. (2012) Discovery of multi-dimensional modules by
integrative analysis of cancer genomic data. <em>Nucleic Acids Research</em>
40(19), 9379-9391
</p>
<p>Zi Yang, et al. (2016) A non-negative matrix factorization method for
detecting modules in heterogeneous omics multi-modal data,
<em>Bioinformatics</em> 32(1), 1-8
</p>
<p>Y. Kenan Yilmaz et al., (2010) Probabilistic Latent Tensor Factorization,
<em>International Conference on Latent Variable Analysis and Signal Separation</em>
346-353
</p>
<p>N. Fujita et al., (2018) Biomarker discovery by integrated joint non-negative matrix factorization and pathway signature analyses, <em>Scientific Report</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "dsiNMF_Hard")
out &lt;- djNMF(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='dNMF'>
Discretized Non-negative Matrix Factorization Algorithms (dNMF)
</h2><span id='topic+dNMF'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::NMF.
The input data X is assumed to be a non-negative matrix
and decomposed to a matrix product U V'.
Unlike regular NMF, in dNMF,
U and V are estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, U and V are estimated by adding ternary regularization
so that the values are 0, 1, or 2 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dNMF(X, M=NULL, pseudocount=.Machine$double.eps,
    initU=NULL, initV=NULL, fixU=FALSE, fixV=FALSE,
    Bin_U=1e-10, Bin_V=1e-10, Ter_U=1e-10, Ter_V=1e-10,
    L1_U=1e-10, L1_V=1e-10, L2_U=1e-10, L2_V=1e-10, J = 3,
    algorithm = c("Frobenius", "KL", "IS", "Beta"), Beta = 2,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dNMF_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_initu">initU</code></td>
<td>

<p>The initial values of factor matrix U, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_fixu">fixU</code></td>
<td>
<p> Whether the factor matrix U is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_bin_u">Bin_U</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_bin_v">Bin_V</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_ter_u">Ter_U</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_ter_v">Ter_V</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_l1_u">L1_U</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_l2_u">L2_U</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_j">J</code></td>
<td>

<p>The number of low-dimension (J &lt; {N, M}, Default: 3)
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_algorithm">algorithm</code></td>
<td>

<p>dNMF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, and &quot;Beta&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="dNMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J-columns (J &lt; {N, M}).
V : A matrix which has M-rows and J-columns (J &lt; {N, M}).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Z. Zhang, T. Li, C. Ding and X. Zhang, (2007). Binary Matrix Factorization with Applications, <em>Seventh IEEE International Conference on Data Mining (ICDM 2007)</em>, 391-400
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
matdata &lt;- toyModel(model = "dNMF")

# Simple usage
out &lt;- dNMF(matdata, J=5)
</code></pre>

<hr>
<h2 id='dNMTF'>
Discretized Non-negative Matrix Tri-Factorization Algorithms (dNMTF)
</h2><span id='topic+dNMTF'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::NMTF.
The input data is assumed to be non-negative matrix.
dNMTF decompose the matrix to three low-dimensional factor matices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dNMTF(X, M=NULL, pseudocount=.Machine$double.eps,
    initU=NULL, initS=NULL, initV=NULL,
    fixU=FALSE, fixS=FALSE, fixV=FALSE,
    Bin_U=1e-10, Bin_S=1e-10, Bin_V=1e-10,
    Ter_U=1e-10, Ter_S=1e-10, Ter_V=1e-10,
    L1_U=1e-10, L1_S=1e-10, L1_V=1e-10,
    L2_U=1e-10, L2_S=1e-10, L2_V=1e-10,
    rank = c(3, 4),
    algorithm = c("Frobenius", "KL", "IS", "Beta"),
    Beta = 2, root = FALSE, thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dNMTF_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the elements as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_initu">initU</code></td>
<td>

<p>The initial values of factor matrix U, which has N-rows and J1-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_inits">initS</code></td>
<td>

<p>The initial values of factor matrix S, which has J1-rows and J2-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J2-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_fixu">fixU</code></td>
<td>
<p> Whether the factor matrix U is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_fixs">fixS</code></td>
<td>
<p> Whether the factor matrix S is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_bin_u">Bin_U</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_bin_s">Bin_S</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_bin_v">Bin_V</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_ter_u">Ter_U</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_ter_s">Ter_S</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_ter_v">Ter_V</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l1_u">L1_U</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l1_s">L1_S</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l2_u">L2_U</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l2_s">L2_S</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension (J1 (&lt; N) and J2 (&lt; M)) (Default: c(3,4)).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_algorithm">algorithm</code></td>
<td>

<p>dNMTF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, and &quot;Beta&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence (Default: 2, which means &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_root">root</code></td>
<td>

<p>Whether square root is calculed in each iteration (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated (Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="dNMTF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J1-columns (J1 &lt; N).
S : A matrix which has J1-rows and J2-columns.
V : A matrix which has M-rows and J2-columns (J2 &lt; M).
rank : The number of low-dimension (J1 (&lt; N) and J2 (&lt; M)).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
algorithm: algorithm specified.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Fast Optimization of Non-Negative Matrix Tri-Factorization: Supporting Information, Andrej Copar, et. al., <em>PLOS ONE</em>, 14(6), e0217994, 2019
</p>
<p>Co-clustering by Block Value Decomposition, Bo Long et al., <em>SIGKDD'05</em>, 2005
</p>
<p>Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering, Chris Ding et. al., <em>12th ACM SIGKDD</em>, 2006
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(interactive()){
    # Test data
    matdata &lt;- toyModel(model = "dNMF")

    # Simple usage
    out &lt;- dNMTF(matdata, rank=c(4,4))
  }
</code></pre>

<hr>
<h2 id='dNTD'>
Discretized Non-negative Tucker Decomposition Algorithms (dNTD)
</h2><span id='topic+dNTD'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::NTD.
The input data X is assumed to be a non-negative tensor
and decomposed to a product of a dense core tensor (S) and
low-dimensional factor matrices (A_k, k=1..K).
Unlike regular NTD, in dNTD,
each A_k is estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, each A_k are estimated by adding ternary regularization
so that the values are 0, 1, or 2 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dNTD(X, M=NULL, pseudocount=.Machine$double.eps,
    initS=NULL, initA=NULL, fixS=FALSE, fixA=FALSE,
    Bin_A=rep(1e-10, length=length(dim(X))),
    Ter_A=rep(1e-10, length=length(dim(X))),
    L1_A=rep(1e-10, length=length(dim(X))),
    L2_A=rep(1e-10, length=length(dim(X))),
    rank = rep(3, length=length(dim(X))),
    modes = seq_along(dim(X)),
    algorithm = c("Frobenius", "KL", "IS", "Beta"),
    init = c("dNMF", "Random"),
    Beta = 2, thr = 1e-10, num.iter = 100,
    viz = FALSE,
    figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dNTD_+3A_x">X</code></td>
<td>

<p>K-order input tensor which has I_1, I_2, ..., and I_K dimensions.
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_m">M</code></td>
<td>

<p>K-order mask tensor which has I_1, I_2, ..., and I_K dimensions.
If the mask tensor has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero
(Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_inits">initS</code></td>
<td>

<p>The initial values of core tensor which has I_1, I_2, ..., and I_K dimensions
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_inita">initA</code></td>
<td>

<p>A list containing the initial values of K factor matrices
(A_k, &lt;Ik*Jk&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_fixs">fixS</code></td>
<td>

<p>Whether the core tensor S is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_fixa">fixA</code></td>
<td>

<p>Whether the factor matrices Ak are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_bin_a">Bin_A</code></td>
<td>

<p>A K-length vector containing the paramters for binary (0,1) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_ter_a">Ter_A</code></td>
<td>

<p>A K-length vector containing the paramters for terary (0,1,2) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_l1_a">L1_A</code></td>
<td>

<p>A K-length vector containing the paramters for L1 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_l2_a">L2_A</code></td>
<td>

<p>A K-length vector containing the paramters for L2 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension in each mode (Default: 3 for each mode).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_modes">modes</code></td>
<td>

<p>The vector of the modes on which to perform the decomposition
(Default: 1:K &lt;all modes&gt;).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_algorithm">algorithm</code></td>
<td>

<p>dNTD algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, and &quot;Beta&quot; are available
(Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_init">init</code></td>
<td>

<p>The initialization algorithms. &quot;NMF&quot;, &quot;ALS&quot;, and &quot;Random&quot; are available
(Default: &quot;NMF&quot;).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr1, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed tensor can be visualized.
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNTD_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S : K-order tensor object, which is defined as S4 class of rTensor package.
A : A list containing K factor matrices.
RecError : The reconstruction error between data tensor and reconstructed
tensor from S and A.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Yong-Deok Kim et. al., (2007). Nonnegative Tucker Decomposition.
<em>IEEE Conference on Computer Vision and Pattern Recognition</em>
</p>
<p>Yong-Deok Kim et. al., (2008). Nonneegative Tucker Decomposition With
Alpha-Divergence. <em>IEEE International Conference on Acoustics,
Speech and Signal Processing</em>
</p>
<p>Anh Huy Phan, (2008). Fast and efficient algorithms for nonnegative
Tucker decomposition. <em>Advances in Neural Networks - ISNN2008</em>
</p>
<p>Anh Hyu Phan et. al. (2011). Extended HALS algorithm for nonnegative
Tucker decomposition and its applications for multiway analysis and classification.
<em>Neurocomputing</em>
</p>


<h3>See Also</h3>

<p><code><a href="nnTensor.html#topic+plotTensor3D">plotTensor3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "dNTD")
out &lt;- dNTD(tensordata, rank=c(2,2,2), algorithm="Frobenius",
  init="Random", num.iter=2)
</code></pre>

<hr>
<h2 id='dNTF'>
Discretized Non-negative CP Decomposition Algorithms (dNTF)
</h2><span id='topic+dNTF'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::NTF.
The input data X is assumed to be a non-negative tensor
and decomposed to a product of a diagonal core tensor (S) and
low-dimensional factor matrices (A_k, k=1..K).
Unlike regular NTF, in dNTF,
each A_k is estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, each A_k are estimated by adding ternary regularization
so that the values are 0, 1, or 2 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dNTF(X, M=NULL, pseudocount=.Machine$double.eps,
    initA=NULL, fixA=FALSE,
    Bin_A=rep(1e-10, length=length(dim(X))),
    Ter_A=rep(1e-10, length=length(dim(X))),
    L1_A=rep(1e-10, length=length(dim(X))),
    L2_A=rep(1e-10, length=length(dim(X))),
    rank = 3,
    algorithm = c("Frobenius", "KL", "IS", "Beta"),
    init = c("dNMF", "Random"),
    Beta = 2, thr = 1e-10, num.iter = 100, viz = FALSE, figdir = NULL,
    verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dNTF_+3A_x">X</code></td>
<td>

<p>K-order input tensor which has I_1, I_2, ..., and I_K dimensions.
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_m">M</code></td>
<td>

<p>K-order mask tensor which has I_1, I_2, ..., and I_K dimensions.
If the mask tensor has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero
(Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_inita">initA</code></td>
<td>

<p>A list containing the initial values of K factor matrices
(A_k, &lt;Ik*Jk&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_fixa">fixA</code></td>
<td>

<p>Whether the factor matrices Ak are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_bin_a">Bin_A</code></td>
<td>

<p>A K-length vector containing the paramters for binary (0,1) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_ter_a">Ter_A</code></td>
<td>

<p>A K-length vector containing the paramters for terary (0,1,2) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_l1_a">L1_A</code></td>
<td>

<p>A K-length vector containing the paramters for L1 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_l2_a">L2_A</code></td>
<td>

<p>A K-length vector containing the paramters for L2 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension in each mode (Default: 3).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_algorithm">algorithm</code></td>
<td>

<p>dNTF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, and &quot;Beta&quot; are available
(Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_init">init</code></td>
<td>

<p>The initialization algorithms. &quot;dNMF&quot;, and &quot;Random&quot; are available
(Default: &quot;dNMF&quot;).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr1, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed tensor can be visualized.
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dNTF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S : K-order tensor object, which is defined as S4 class of rTensor package.
A : A list containing K factor matrices.
RecError : The reconstruction error between data tensor and reconstructed
tensor from S and A.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Andrzej CICHOCKI et. al., (2007). Non-negative Tensor Factorization using
Alpha and Beta Divergence. <em>IEEE ICASSP 2007</em>
</p>
<p>Anh Huy PHAN et. al., (2008). Multi-way Nonnegative Tensor Factorization
Using Fast Hierarchical Alternating Least Squares Algorithm (HALS). <em>NOLTA2008</em>
</p>
<p>Andrzej CICHOCKI et. al., (2008). Fast Local Algorithms for Large Scale
Nonnegative Matrix and Tensor Factorizations.
<em>IEICE Transactions on Fundamentals of Electronics, Communications
and Computer Sciences</em>
</p>


<h3>See Also</h3>

<p><code><a href="nnTensor.html#topic+plotTensor3D">plotTensor3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "dNTF")
out &lt;- dNTF(tensordata, rank=3, num.iter=2)
</code></pre>

<hr>
<h2 id='dPLS'>
Discretized Partial Least Squares (dPLS)
</h2><span id='topic+dPLS'></span>

<h3>Description</h3>

<p>This function is the discretized version of PLS.
The input data objects are assumed to be a list containing multiple matrices
(X_1, X_2, ..., X_K),
and decomposed to multiple matrix products (U_1 V_1', U_2 V_2', ..., U_K V_K'),
where each U_k and V_k (k=1..K) is specific in each X_k.
Unlike regular PLS, in dPLS,
U_k and V_k are estimated by adding ternary regularization
so that the values are -1, 0, or 1 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dPLS(X, M=NULL, pseudocount=.Machine$double.eps,
    initV=NULL, fixV=FALSE, Ter_V=1e-10,
    L1_V=1e-10, L2_V=1e-10, eta=1e+10, J = 3,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dPLS_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_ter_v">Ter_V</code></td>
<td>

<p>Paramter for terary (-1,0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_eta">eta</code></td>
<td>

<p>Stepsize of gradient descent algorithm (Default: 1e+10).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_j">J</code></td>
<td>

<p>The number of low-dimension (J &lt; {N, M}, Default: 3)
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="dPLS_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J-columns (J &lt; {N, M}).
V : A matrix which has M-rows and J-columns (J &lt; {N, M}).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
matdata &lt;- toyModel(model = "dPLS_Easy")

# Simple usage
out &lt;- dPLS(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='dsiNMF'>
Discretized Simultaneous Non-negative Matrix Factorization Algorithms (dsiNMF)
</h2><span id='topic+dsiNMF'></span>

<h3>Description</h3>

<p>This function is the discretized version of nnTensor::siNMF.
The input data objects are assumed to be a list containing multiple non-negative matrices (X_1, X_2, ..., X_K),
and decomposed to multiple matrix products (W H_1', W H_2', ..., W H_K'),
where W is common across all the data matrices
but each H_k (k=1..K) is specific in each X_k.
Unlike regular siNMF, in dsiNMF,
W and H_k are estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, W and H_k are estimated by adding ternary regularization
so that the values are 0, 1, or 2 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsiNMF(X, M=NULL, pseudocount=.Machine$double.eps,
    initW=NULL, initH=NULL,
    fixW=FALSE, fixH=FALSE,
    Bin_W=1e-10, Bin_H=rep(1e-10, length=length(X)),
    Ter_W=1e-10, Ter_H=rep(1e-10, length=length(X)),
    L1_W=1e-10, L1_H=rep(1e-10, length=length(X)),
    L2_W=1e-10, L2_H=rep(1e-10, length=length(X)),
    J = 3, w=NULL, algorithm = c("Frobenius", "KL", "IS", "PLTF"), p=1,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dsiNMF_+3A_x">X</code></td>
<td>

<p>A list containing the input matrices (X_k, &lt;N*Mk&gt;, k=1..K).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_m">M</code></td>
<td>

<p>A list containing the mask matrices (X_k, &lt;N*Mk&gt;, k=1..K). If the input matrix
has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_initw">initW</code></td>
<td>

<p>The initial values of factor matrix W, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_inith">initH</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(H_k, &lt;Mk*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_fixw">fixW</code></td>
<td>

<p>Whether the factor matrix W is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_fixh">fixH</code></td>
<td>

<p>Whether the factor matrices Hk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_bin_w">Bin_W</code></td>
<td>

<p>Paramter for binary (0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_bin_h">Bin_H</code></td>
<td>

<p>A K-length vector containing the paramters for binary (0,1) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_ter_w">Ter_W</code></td>
<td>

<p>Paramter for terary (0,1,2) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_ter_h">Ter_H</code></td>
<td>

<p>A K-length vector containing the paramters for terary (0,1,2) regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_l1_w">L1_W</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_l1_h">L1_H</code></td>
<td>

<p>A K-length vector containing the paramters for L1 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_l2_w">L2_W</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_l2_h">L2_H</code></td>
<td>

<p>A K-length vector containing the paramters for L2 regularitation
(Default: rep(1e-10, length=length(dim(X)))).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_j">J</code></td>
<td>

<p>Number of low-dimension (J &lt; N, Mk).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_w">w</code></td>
<td>

<p>Weight vector (Default: NULL)
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_algorithm">algorithm</code></td>
<td>

<p>Divergence between X and X_bar. &quot;Frobenius&quot;, &quot;KL&quot;, and &quot;IS&quot; are available
(Default: &quot;KL&quot;).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_p">p</code></td>
<td>

<p>The parameter of Probabilistic Latent Tensor Factorization
(p=0: Frobenius, p=1: KL, p=2: IS)
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="dsiNMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>W : A matrix which has N-rows and J-columns (J &lt; N, Mk).
H : A list which has multiple elements containing Mk-rows and
J-columns matrix (J &lt; N, Mk).
RecError : The reconstruction error between data matrix and
reconstructed matrix from W and H.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Liviu Badea, (2008) Extracting Gene Expression Profiles Common to Colon
and Pancreatic Adenocarcinoma using Simultaneous nonnegative matrix
factorization. <em>Pacific Symposium on Biocomputing</em> 13:279-290
</p>
<p>Shihua Zhang, et al. (2012) Discovery of multi-dimensional modules by
integrative analysis of cancer genomic data. <em>Nucleic Acids Research</em>
40(19), 9379-9391
</p>
<p>Zi Yang, et al. (2016) A non-negative matrix factorization method for
detecting modules in heterogeneous omics multi-modal data,
<em>Bioinformatics</em> 32(1), 1-8
</p>
<p>Y. Kenan Yilmaz et al., (2010) Probabilistic Latent Tensor Factorization,
<em>International Conference on Latent Variable Analysis and Signal Separation</em>
346-353
</p>
<p>N. Fujita et al., (2018) Biomarker discovery by integrated joint non-negative matrix factorization and pathway signature analyses, <em>Scientific Report</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "dsiNMF_Easy")
out &lt;- dsiNMF(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='dSVD'>
Discretized Singular Value Decomposition (dSVD)
</h2><span id='topic+dSVD'></span>

<h3>Description</h3>

<p>This function is the discretized version of SVD.
The input data X is decomposed to a matrix product U V'.
Unlike regular SVD, in dSVD,
U and V are estimated by adding binary regularization
so that the values are 0 or 1 as much as possible.
Likewise, U and V are estimated by adding ternary regularization
so that the values are -1, 0, or 1 as much as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dSVD(X, M=NULL, pseudocount=.Machine$double.eps,
    initU=NULL, initV=NULL, fixU=FALSE, fixV=FALSE,
    Ter_U=1e-10, L1_U=1e-10, L2_U=1e-10, eta=1e+10, J = 3,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dSVD_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_initu">initU</code></td>
<td>

<p>The initial values of factor matrix U, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_fixu">fixU</code></td>
<td>
<p> Whether the factor matrix U is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_ter_u">Ter_U</code></td>
<td>

<p>Paramter for terary (-1,0,1) regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_l1_u">L1_U</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_l2_u">L2_U</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_eta">eta</code></td>
<td>

<p>Stepsize of gradient descent algorithm (Default: 1e+10).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_j">J</code></td>
<td>

<p>The number of low-dimension (J &lt; {N, M}, Default: 3)
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="dSVD_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J-columns (J &lt; {N, M}).
V : A matrix which has M-rows and J-columns (J &lt; {N, M}).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
matdata &lt;- toyModel(model = "dSVD")

# Simple usage
out &lt;- dSVD(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='toyModel'>
Toy model data for using dNMF, dSVD, dsiNMF, djNMF, dPLS, dNTF, and dNTD
</h2><span id='topic+toyModel'></span>

<h3>Description</h3>

<p>The data is used to confirm that the algorithm are properly working.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toyModel(model = "dNMF", seeds=123)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toyModel_+3A_model">model</code></td>
<td>

<p>Single character string is specified.
&quot;dNMF&quot;, &quot;dSVD&quot;, &quot;dsiNMF_Easy&quot;, &quot;dsiNMF_Hard&quot;, &quot;dPLS_Easy&quot;, &quot;dPLS_Hard&quot;, &quot;dNTF&quot;, and &quot;dNTD&quot; are available (Default: &quot;dNMF&quot;).
</p>
</td></tr>
<tr><td><code id="toyModel_+3A_seeds">seeds</code></td>
<td>

<p>Random number for setting set.seeds in the function (Default: 123).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If model is specified as &quot;dNMF&quot; or &quot;dSVD&quot; a matrix is generated.
If model is specified as &quot;dsiNMF_Easy&quot;, &quot;dsiNMF_Hard&quot;,
&quot;dPLS_Easy&quot;, or &quot;dPLS_Hard&quot; three matrices are generated.
Otherwise, a tensor is generated.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>See Also</h3>

<p><code><a href="#topic+dNMF">dNMF</a></code>,<code><a href="#topic+dSVD">dSVD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "dNMF", seeds=123)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
