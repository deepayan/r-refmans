<!DOCTYPE html><html><head><title>Help for package gpboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gpboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agaricus.test'><p>Test part from Mushroom Data Set</p></a></li>
<li><a href='#agaricus.train'><p>Training part from Mushroom Data Set</p></a></li>
<li><a href='#bank'><p>Bank Marketing Data Set</p></a></li>
<li><a href='#coords'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#coords_test'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#dim.gpb.Dataset'><p>Dimensions of an <code>gpb.Dataset</code></p></a></li>
<li><a href='#dimnames.gpb.Dataset'><p>Handling of column names of <code>gpb.Dataset</code></p></a></li>
<li><a href='#fit'><p>Generic 'fit' method for a <code>GPModel</code></p></a></li>
<li><a href='#fit.GPModel'><p>Fits a <code>GPModel</code></p></a></li>
<li><a href='#fitGPModel'><p>Fits a <code>GPModel</code></p></a></li>
<li><a href='#get_nested_categories'><p>Auxiliary function to create categorical variables for nested grouped random effects</p></a></li>
<li><a href='#getinfo'><p>Get information of an <code>gpb.Dataset</code> object</p></a></li>
<li><a href='#gpb_shared_params'><p>Shared parameter docs</p></a></li>
<li><a href='#gpb.convert_with_rules'><p>Data preparator for GPBoost datasets with rules (integer)</p></a></li>
<li><a href='#gpb.cv'><p>CV function for number of boosting iterations</p></a></li>
<li><a href='#gpb.Dataset'><p>Construct <code>gpb.Dataset</code> object</p></a></li>
<li><a href='#gpb.Dataset.construct'><p>Construct Dataset explicitly</p></a></li>
<li><a href='#gpb.Dataset.create.valid'><p>Construct validation data</p></a></li>
<li><a href='#gpb.Dataset.save'><p>Save <code>gpb.Dataset</code> to a binary file</p></a></li>
<li><a href='#gpb.Dataset.set.categorical'><p>Set categorical feature of <code>gpb.Dataset</code></p></a></li>
<li><a href='#gpb.Dataset.set.reference'><p>Set reference of <code>gpb.Dataset</code></p></a></li>
<li><a href='#gpb.dump'><p>Dump GPBoost model to json</p></a></li>
<li><a href='#gpb.get.eval.result'><p>Get record evaluation result from booster</p></a></li>
<li><a href='#gpb.grid.search.tune.parameters'><p>Function for choosing tuning parameters</p></a></li>
<li><a href='#gpb.importance'><p>Compute feature importance in a model</p></a></li>
<li><a href='#gpb.interprete'><p>Compute feature contribution of prediction</p></a></li>
<li><a href='#gpb.load'><p>Load GPBoost model</p></a></li>
<li><a href='#gpb.model.dt.tree'><p>Parse a GPBoost model json dump</p></a></li>
<li><a href='#gpb.plot.importance'><p>Plot feature importance as a bar graph</p></a></li>
<li><a href='#gpb.plot.interpretation'><p>Plot feature contribution as a bar graph</p></a></li>
<li><a href='#gpb.plot.part.dep.interact'><p>Plot interaction partial dependence plots</p></a></li>
<li><a href='#gpb.plot.partial.dependence'><p>Plot partial dependence plots</p></a></li>
<li><a href='#gpb.save'><p>Save GPBoost model</p></a></li>
<li><a href='#gpb.train'><p>Main training logic for GBPoost</p></a></li>
<li><a href='#gpboost'><p>Train a GPBoost model</p></a></li>
<li><a href='#GPBoost_data'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#GPModel'><p>Create a <code>GPModel</code> object</p></a></li>
<li><a href='#GPModel_shared_params'><p>Documentation for parameters shared by <code>GPModel</code>, <code>gpb.cv</code>, and <code>gpboost</code></p></a></li>
<li><a href='#group_data'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#group_data_test'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#loadGPModel'><p>Load a <code>GPModel</code> from a file</p></a></li>
<li><a href='#neg_log_likelihood'><p>Evaluate the negative log-likelihood</p></a></li>
<li><a href='#neg_log_likelihood.GPModel'><p>Evaluate the negative log-likelihood</p></a></li>
<li><a href='#predict_training_data_random_effects'><p>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code></p></a></li>
<li><a href='#predict_training_data_random_effects.GPModel'><p>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code></p></a></li>
<li><a href='#predict.gpb.Booster'><p>Prediction function for <code>gpb.Booster</code> objects</p></a></li>
<li><a href='#predict.GPModel'><p>Make predictions for a <code>GPModel</code></p></a></li>
<li><a href='#readRDS.gpb.Booster'><p>readRDS for <code>gpb.Booster</code> models</p></a></li>
<li><a href='#saveGPModel'><p>Save a <code>GPModel</code></p></a></li>
<li><a href='#saveRDS.gpb.Booster'><p>saveRDS for <code>gpb.Booster</code> models</p></a></li>
<li><a href='#set_optim_params'><p>Set parameters for estimation of the covariance parameters</p></a></li>
<li><a href='#set_optim_params.GPModel'><p>Set parameters for estimation of the covariance parameters</p></a></li>
<li><a href='#set_prediction_data'><p>Set prediction data for a <code>GPModel</code></p></a></li>
<li><a href='#set_prediction_data.GPModel'><p>Set prediction data for a <code>GPModel</code></p></a></li>
<li><a href='#setinfo'><p>Set information of an <code>gpb.Dataset</code> object</p></a></li>
<li><a href='#slice'><p>Slice a dataset</p></a></li>
<li><a href='#summary.GPModel'><p>Summary for a <code>GPModel</code></p></a></li>
<li><a href='#X'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#X_test'><p>Example data for the GPBoost package</p></a></li>
<li><a href='#y'><p>Example data for the GPBoost package</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Combining Tree-Boosting with Gaussian Process and Mixed Effects
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-27</td>
</tr>
<tr>
<td>Description:</td>
<td>An R package that allows for combining tree-boosting with Gaussian process and mixed effects models. It also allows for independently doing tree-boosting as well as inference and prediction for Gaussian process and mixed effects models. See <a href="https://github.com/fabsig/GPBoost">https://github.com/fabsig/GPBoost</a> for more information on the software and Sigrist (2022, JMLR) <a href="https://www.jmlr.org/papers/v23/20-322.html">https://www.jmlr.org/papers/v23/20-322.html</a> and Sigrist (2023, TPAMI) &lt;<a href="https://doi.org/10.1109%2FTPAMI.2022.3168152">doi:10.1109/TPAMI.2022.3168152</a>&gt; for more information on the methodology.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (== 2.0)</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fabsig/GPBoost">https://github.com/fabsig/GPBoost</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fabsig/GPBoost/issues">https://github.com/fabsig/GPBoost/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5), R6 (&ge; 2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.9.6), graphics, RJSONIO, Matrix (&ge; 1.1-0),
methods, utils</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-27 12:15:09 UTC; whsigris</td>
</tr>
<tr>
<td>Author:</td>
<td>Fabio Sigrist [aut, cre],
  Tim Gyger [aut],
  Pascal Kuendig [aut],
  Benoit Jacob [cph],
  Gael Guennebaud [cph],
  Nicolas Carre [cph],
  Pierre Zoppitelli [cph],
  Gauthier Brun [cph],
  Jean Ceccato [cph],
  Jitse Niesen [cph],
  Other authors of Eigen for the included version of Eigen [ctb, cph],
  Timothy A. Davis [cph],
  Guolin Ke [ctb],
  Damien Soukhavong [ctb],
  James Lamb [ctb],
  Other authors of LightGBM for the included version of LightGBM [ctb],
  Microsoft Corporation [cph],
  Dropbox, Inc. [cph],
  Jay Loden [cph],
  Dave Daeschler [cph],
  Giampaolo Rodola [cph],
  Alberto Ferreira [ctb],
  Daniel Lemire [ctb],
  Victor Zverovich [cph],
  IBM Corporation [ctb],
  Keith O'Hara [cph],
  Stephen L. Moshier [cph],
  Jorge Nocedal [cph],
  Naoaki Okazaki [cph],
  Yixuan Qiu [cph],
  Dirk Toewe [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fabio Sigrist &lt;fabiosigrist@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-28 08:40:13 UTC</td>
</tr>
</table>
<hr>
<h2 id='agaricus.test'>Test part from Mushroom Data Set</h2><span id='topic+agaricus.test'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
This data set includes the following fields:
</p>

<ul>
<li><p><code>label</code>: the label for each record
</p>
</li>
<li><p><code>data</code>: a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.test)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 1611
rows and 126 variables</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='agaricus.train'>Training part from Mushroom Data Set</h2><span id='topic+agaricus.train'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
This data set includes the following fields:
</p>

<ul>
<li><p><code>label</code>: the label for each record
</p>
</li>
<li><p><code>data</code>: a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.train)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 6513
rows and 127 variables</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='bank'>Bank Marketing Data Set</h2><span id='topic+bank'></span>

<h3>Description</h3>

<p>This data set is originally from the Bank Marketing data set,
UCI Machine Learning Repository.
</p>
<p>It contains only the following: bank.csv with 10
randomly selected from 3 (older version of this dataset with less inputs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bank)
</code></pre>


<h3>Format</h3>

<p>A data.table with 4521 rows and 17 variables</p>


<h3>References</h3>

<p>http://archive.ics.uci.edu/ml/datasets/Bank+Marketing
</p>
<p>S. Moro, P. Cortez and P. Rita. (2014)
A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems
</p>

<hr>
<h2 id='coords'>Example data for the GPBoost package</h2><span id='topic+coords'></span>

<h3>Description</h3>

<p>A matrix with spatial coordinates for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='coords_test'>Example data for the GPBoost package</h2><span id='topic+coords_test'></span>

<h3>Description</h3>

<p>A matrix with spatial coordinates for predictions for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='dim.gpb.Dataset'>Dimensions of an <code>gpb.Dataset</code></h2><span id='topic+dim.gpb.Dataset'></span>

<h3>Description</h3>

<p>Returns a vector of numbers of rows and of columns in an <code>gpb.Dataset</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpb.Dataset'
dim(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dim.gpb.Dataset_+3A_x">x</code></td>
<td>
<p>Object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="dim.gpb.Dataset_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: since <code>nrow</code> and <code>ncol</code> internally use <code>dim</code>, they can also
be directly used with an <code>gpb.Dataset</code> object.
</p>


<h3>Value</h3>

<p>a vector of numbers of rows and of columns
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)

stopifnot(nrow(dtrain) == nrow(train$data))
stopifnot(ncol(dtrain) == ncol(train$data))
stopifnot(all(dim(dtrain) == dim(train$data)))

</code></pre>

<hr>
<h2 id='dimnames.gpb.Dataset'>Handling of column names of <code>gpb.Dataset</code></h2><span id='topic+dimnames.gpb.Dataset'></span><span id='topic+dimnames+3C-.gpb.Dataset'></span>

<h3>Description</h3>

<p>Only column names are supported for <code>gpb.Dataset</code>, thus setting of
row names would have no effect and returned row names would be NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpb.Dataset'
dimnames(x)

## S3 replacement method for class 'gpb.Dataset'
dimnames(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimnames.gpb.Dataset_+3A_x">x</code></td>
<td>
<p>object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="dimnames.gpb.Dataset_+3A_value">value</code></td>
<td>
<p>a list of two elements: the first one is ignored
and the second one is column names</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generic <code>dimnames</code> methods are used by <code>colnames</code>.
Since row names are irrelevant, it is recommended to use <code>colnames</code> directly.
</p>


<h3>Value</h3>

<p>A list with the dimension names of the dataset
</p>
<p>A list with the dimension names of the dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
gpb.Dataset.construct(dtrain)
dimnames(dtrain)
colnames(dtrain)
colnames(dtrain) &lt;- make.names(seq_len(ncol(train$data)))
print(dtrain, verbose = TRUE)

</code></pre>

<hr>
<h2 id='fit'>Generic 'fit' method for a <code>GPModel</code></h2><span id='topic+fit'></span>

<h3>Description</h3>

<p>Generic 'fit' method for a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit(gp_model, y, X, params, offset = NULL, fixed_effects = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_+3A_gp_model">gp_model</code></td>
<td>
<p>a <code>GPModel</code></p>
</td></tr>
<tr><td><code id="fit_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="fit_+3A_x">X</code></td>
<td>
<p>A <code>matrix</code> with numeric covariate data for the 
fixed effects linear regression term (if there is one)</p>
</td></tr>
<tr><td><code id="fit_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="fit_+3A_offset">offset</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor (= offset). 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="fit_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset</code> instead</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>

<hr>
<h2 id='fit.GPModel'>Fits a <code>GPModel</code></h2><span id='topic+fit.GPModel'></span>

<h3>Description</h3>

<p>Estimates the parameters of a <code>GPModel</code> using maximum likelihood estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
fit(gp_model, y, X = NULL, params = list(),
  offset = NULL, fixed_effects = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.GPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>a <code>GPModel</code></p>
</td></tr>
<tr><td><code id="fit.GPModel_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="fit.GPModel_+3A_x">X</code></td>
<td>
<p>A <code>matrix</code> with numeric covariate data for the 
fixed effects linear regression term (if there is one)</p>
</td></tr>
<tr><td><code id="fit.GPModel_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="fit.GPModel_+3A_offset">offset</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor (= offset). 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="fit.GPModel_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset</code> instead</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

#--------------------Grouped random effects model: single-level random effect----------------
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood="gaussian")
fit(gp_model, y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_var = TRUE)
pred$mu # Predicted mean
pred$var # Predicted variances
# Also predict covariance matrix
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted mean
pred$cov # Predicted covariance
 
#--------------------Gaussian process model----------------
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood="gaussian")
fit(gp_model, y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, gp_coords_pred = coords_test, 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted (posterior) mean of GP
pred$cov # Predicted (posterior) covariance matrix of GP


</code></pre>

<hr>
<h2 id='fitGPModel'>Fits a <code>GPModel</code></h2><span id='topic+fitGPModel'></span>

<h3>Description</h3>

<p>Estimates the parameters of a <code>GPModel</code> using maximum likelihood estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitGPModel(likelihood = "gaussian", group_data = NULL,
  group_rand_coef_data = NULL, ind_effect_group_rand_coef = NULL,
  drop_intercept_group_rand_effect = NULL, gp_coords = NULL,
  gp_rand_coef_data = NULL, cov_function = "exponential",
  cov_fct_shape = 0.5, gp_approx = "none", cov_fct_taper_range = 1,
  cov_fct_taper_shape = 0, num_neighbors = 20L,
  vecchia_ordering = "random", ind_points_selection = "kmeans++",
  num_ind_points = 500L, cover_tree_radius = 1,
  matrix_inversion_method = "cholesky", seed = 0L, cluster_ids = NULL,
  free_raw_data = FALSE, y, X = NULL, params = list(),
  vecchia_approx = NULL, vecchia_pred_type = NULL,
  num_neighbors_pred = NULL, offset = NULL, fixed_effects = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitGPModel_+3A_likelihood">likelihood</code></td>
<td>
<p>A <code>string</code> specifying the likelihood function (distribution) of the response variable. 
Available options:
</p>

<ul>
<li><p> &quot;gaussian&quot; 
</p>
</li>
<li><p> &quot;bernoulli_probit&quot;: binary data with Bernoulli likelihood and a probit link function 
</p>
</li>
<li><p> &quot;bernoulli_logit&quot;: binary data with Bernoulli likelihood and a logit link function 
</p>
</li>
<li><p> &quot;gamma&quot;: gamma distribution with a with log link function 
</p>
</li>
<li><p> &quot;poisson&quot;: Poisson distribution with a with log link function 
</p>
</li>
<li><p> &quot;negative_binomial&quot;: negative binomial distribution with a with log link function 
</p>
</li>
<li><p> Note: other likelihoods could be implemented upon request 
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_group_data">group_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> whose columns are categorical grouping variables. 
The elements being group levels defining grouped random effects.
The elements of 'group_data' can be integer, double, or character.
The number of columns corresponds to the number of grouped (intercept) random effects</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_group_rand_coef_data">group_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data 
for grouped random coefficients</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_ind_effect_group_rand_coef">ind_effect_group_rand_coef</code></td>
<td>
<p>A <code>vector</code> with integer indices that 
indicate the corresponding categorical grouping variable (=columns) in 'group_data' for 
every covariate in 'group_rand_coef_data'. Counting starts at 1.
The length of this index vector must equal the number of covariates in 'group_rand_coef_data'.
For instance, c(1,1,2) means that the first two covariates (=first two columns) in 'group_rand_coef_data'
have random coefficients corresponding to the first categorical grouping variable (=first column) in 'group_data',
and the third covariate (=third column) in 'group_rand_coef_data' has a random coefficient
corresponding to the second grouping variable (=second column) in 'group_data'</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_drop_intercept_group_rand_effect">drop_intercept_group_rand_effect</code></td>
<td>
<p>A <code>vector</code> of type <code>logical</code> (boolean). 
Indicates whether intercept random effects are dropped (only for random coefficients). 
If drop_intercept_group_rand_effect[k] is TRUE, the intercept random effect number k is dropped / not included. 
Only random effects with random slopes can be dropped.</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_gp_coords">gp_coords</code></td>
<td>
<p>A <code>matrix</code> with numeric coordinates (= inputs / features) for defining Gaussian processes</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_gp_rand_coef_data">gp_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data for
Gaussian process random coefficients</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cov_function">cov_function</code></td>
<td>
<p>A <code>string</code> specifying the covariance function for the Gaussian process. 
Available options:
</p>

<ul>
<li><p>&quot;exponential&quot;: Exponential covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p>&quot;gaussian&quot;: Gaussian, aka squared exponential, covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;matern&quot;: Matern covariance function with the smoothness specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Rasmussen and Williams, 2006) 
</p>
</li>
<li><p>&quot;powered_exponential&quot;: powered exponential covariance function with the exponent specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;wendland&quot;: Compactly supported Wendland covariance function (using the parametrization of Bevilacqua et al., 2019, AOS) 
</p>
</li>
<li><p> &quot;matern_space_time&quot;: Spatio-temporal Matern covariance function with different range parameters for space and time. 
Note that the first column in <code>gp_coords</code> must correspond to the time dimension 
</p>
</li>
<li><p> &quot;matern_ard&quot;: anisotropic Matern covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li>
<li><p> &quot;gaussian_ard&quot;: anisotropic Gaussian, aka squared exponential, covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cov_fct_shape">cov_fct_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape parameter of the covariance function 
(=smoothness parameter for Matern covariance)  
This parameter is irrelevant for some covariance functions such as the exponential or Gaussian</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_gp_approx">gp_approx</code></td>
<td>
<p>A <code>string</code> specifying the large data approximation
for Gaussian processes. Available options: 
</p>

<ul>
<li><p>&quot;none&quot;: No approximation 
</p>
</li>
<li><p>&quot;vecchia&quot;: A Vecchia approximation; see Sigrist (2022, JMLR) for more details 
</p>
</li>
<li><p>&quot;tapering&quot;: The covariance function is multiplied by 
a compactly supported Wendland correlation function 
</p>
</li>
<li><p>&quot;fitc&quot;: Fully Independent Training Conditional approximation aka 
modified predictive process approximation; see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li>
<li><p>&quot;full_scale_tapering&quot;: A full scale approximation combining an 
inducing point / predictive process approximation with tapering on the residual process; 
see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cov_fct_taper_range">cov_fct_taper_range</code></td>
<td>
<p>A <code>numeric</code> specifying the range parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cov_fct_taper_shape">cov_fct_taper_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape (=smoothness) parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_num_neighbors">num_neighbors</code></td>
<td>
<p>An <code>integer</code> specifying the number of neighbors for 
the Vecchia approximation. Note: for prediction, the number of neighbors can 
be set through the 'num_neighbors_pred' parameter in the 'set_prediction_data'
function. By default, num_neighbors_pred = 2 * num_neighbors. Further, 
the type of Vecchia approximation used for making predictions is set through  
the 'vecchia_pred_type' parameter in the 'set_prediction_data' function</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_vecchia_ordering">vecchia_ordering</code></td>
<td>
<p>A <code>string</code> specifying the ordering used in 
the Vecchia approximation. Available options:
</p>

<ul>
<li><p>&quot;none&quot;: the default ordering in the data is used 
</p>
</li>
<li><p>&quot;random&quot;: a random ordering 
</p>
</li>
<li><p>&quot;time&quot;: ordering accorrding to time (only for space-time models) 
</p>
</li>
<li><p>&quot;time_random_space&quot;: ordering according to time and randomly for all 
spatial points with the same time points (only for space-time models) 
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_ind_points_selection">ind_points_selection</code></td>
<td>
<p>A <code>string</code> specifying the method for choosing inducing points
Available options:
</p>

<ul>
<li><p>&quot;kmeans++: the k-means++ algorithm 
</p>
</li>
<li><p>&quot;cover_tree&quot;: the cover tree algorithm 
</p>
</li>
<li><p>&quot;random&quot;: random selection from data points 
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_num_ind_points">num_ind_points</code></td>
<td>
<p>An <code>integer</code> specifying the number of inducing 
points / knots for, e.g., a predictive process approximation</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cover_tree_radius">cover_tree_radius</code></td>
<td>
<p>A <code>numeric</code> specifying the radius (= &quot;spatial resolution&quot;) 
for the cover tree algorithm</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_matrix_inversion_method">matrix_inversion_method</code></td>
<td>
<p>A <code>string</code> specifying the method used for inverting covariance matrices. 
Available options:
</p>

<ul>
<li><p>&quot;cholesky&quot;: Cholesky factorization 
</p>
</li>
<li><p>&quot;iterative&quot;: iterative methods. A combination of conjugate gradient, Lanczos algorithm, and other methods. 
</p>
<p>This is currently only supported for the following cases: 
</p>

<ul>
<li><p>likelihood != &quot;gaussian&quot; and gp_approx == &quot;vecchia&quot; (non-Gaussian likelihoods with a Vecchia-Laplace approximation) 
</p>
</li>
<li><p>likelihood == &quot;gaussian&quot; and gp_approx == &quot;full_scale_tapering&quot; (Gaussian likelihood with a full-scale tapering approximation) 
</p>
</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_seed">seed</code></td>
<td>
<p>An <code>integer</code> specifying the seed used for model creation 
(e.g., random ordering in Vecchia approximation)</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_cluster_ids">cluster_ids</code></td>
<td>
<p>A <code>vector</code> with elements indicating independent realizations of 
random effects / Gaussian processes (same values = same process realization).
The elements of 'cluster_ids' can be integer, double, or character.</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_free_raw_data">free_raw_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the data (groups, coordinates, covariate data for random coefficients) 
is freed in R after initialization</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_x">X</code></td>
<td>
<p>A <code>matrix</code> with numeric covariate data for the 
fixed effects linear regression term (if there is one)</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="fitGPModel_+3A_vecchia_approx">vecchia_approx</code></td>
<td>
<p>Discontinued. Use the argument <code>gp_approx</code> instead</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_offset">offset</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor (= offset). 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="fitGPModel_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset</code> instead</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

#--------------------Grouped random effects model: single-level random effect----------------
gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1,
                       likelihood="gaussian", params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_var = TRUE)
pred$mu # Predicted mean
pred$var # Predicted variances
# Also predict covariance matrix
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted mean
pred$cov # Predicted covariance

#--------------------Two crossed random effects and a random slope----------------
gp_model &lt;- fitGPModel(group_data = group_data, likelihood="gaussian",
                       group_rand_coef_data = X[,2],
                       ind_effect_group_rand_coef = 1,
                       y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)

#--------------------Gaussian process model----------------
gp_model &lt;- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       likelihood="gaussian", y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, gp_coords_pred = coords_test, 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted (posterior) mean of GP
pred$cov # Predicted (posterior) covariance matrix of GP

#--------------------Gaussian process model with Vecchia approximation----------------
gp_model &lt;- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       gp_approx = "vecchia", num_neighbors = 20,
                       likelihood="gaussian", y = y)
summary(gp_model)

#--------------------Gaussian process model with random coefficients----------------
gp_model &lt;- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       gp_rand_coef_data = X[,2], y=y,
                       likelihood = "gaussian", params = list(std_dev = TRUE))
summary(gp_model)

#--------------------Combine Gaussian process with grouped random effects----------------
gp_model &lt;- fitGPModel(group_data = group_data,
                       gp_coords = coords, cov_function = "exponential",
                       likelihood = "gaussian", y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)


</code></pre>

<hr>
<h2 id='get_nested_categories'>Auxiliary function to create categorical variables for nested grouped random effects</h2><span id='topic+get_nested_categories'></span>

<h3>Description</h3>

<p>Auxiliary function to create categorical variables for nested grouped random effects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_nested_categories(outer_var, inner_var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_nested_categories_+3A_outer_var">outer_var</code></td>
<td>
<p>A <code>vector</code> containing the outer categorical grouping variable
within which the <code>inner_var is</code> nested in. Can be of type integer, double, or character.</p>
</td></tr>
<tr><td><code id="get_nested_categories_+3A_inner_var">inner_var</code></td>
<td>
<p>A <code>vector</code> containing the inner nested categorical grouping variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>vector</code> containing a categorical variable such that inner_var is nested in outer_var
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fit a model with Time as categorical fixed effects variables and Diet and Chick
#   as random effects, where Chick is nested in Diet using lme4
chick_nested_diet &lt;- get_nested_categories(ChickWeight$Diet, ChickWeight$Chick)
fixed_effects_matrix &lt;- model.matrix(weight ~ as.factor(Time), data = ChickWeight)
mod_gpb &lt;- fitGPModel(X = fixed_effects_matrix, 
                      group_data = cbind(diet=ChickWeight$Diet, chick_nested_diet), 
                      y = ChickWeight$weight, params = list(std_dev = TRUE))
summary(mod_gpb)
# This does (almost) the same thing as the following code using lme4:
# mod_lme4 &lt;-  lmer(weight ~ as.factor(Time) + (1 | Diet/Chick), data = ChickWeight, REML = FALSE)
# summary(mod_lme4)

</code></pre>

<hr>
<h2 id='getinfo'>Get information of an <code>gpb.Dataset</code> object</h2><span id='topic+getinfo'></span><span id='topic+getinfo.gpb.Dataset'></span>

<h3>Description</h3>

<p>Get one attribute of a <code>gpb.Dataset</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getinfo(dataset, ...)

## S3 method for class 'gpb.Dataset'
getinfo(dataset, name, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getinfo_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="getinfo_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="getinfo_+3A_name">name</code></td>
<td>
<p>the name of the information field to get (see details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>name</code> field can be one of the following:
</p>

<ul>
<li> <p><code>label</code>: label gpboost learn from ;
</p>
</li>
<li> <p><code>weight</code>: to do a weight rescale ;
</p>
</li>
<li><p><code>group</code>: used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results to be ranked.
For example, if you have a 100-document dataset with <code>group = c(10, 20, 40, 10, 10, 10)</code>,
that means that you have 6 groups, where the first 10 records are in the first group,
records 11-30 are in the second group, etc.
</p>
</li>
<li> <p><code>init_score</code>: initial score is the base prediction gpboost will boost from.
</p>
</li></ul>



<h3>Value</h3>

<p>info data
</p>
<p>info data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
gpb.Dataset.construct(dtrain)

labels &lt;- gpboost::getinfo(dtrain, "label")
gpboost::setinfo(dtrain, "label", 1 - labels)

labels2 &lt;- gpboost::getinfo(dtrain, "label")
stopifnot(all(labels2 == 1 - labels))

</code></pre>

<hr>
<h2 id='gpb_shared_params'>Shared parameter docs</h2><span id='topic+gpb_shared_params'></span>

<h3>Description</h3>

<p>Parameter docs shared by <code>gpb.train</code>, <code>gpb.cv</code>, and <code>gpboost</code>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb_shared_params_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_data">data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+gpb.cv">gpb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_eval">eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: &quot;test_neg_log_likelihood&quot;, &quot;mse&quot;, &quot;rmse&quot;, &quot;mae&quot;, 
&quot;auc&quot;, &quot;average_precision&quot;, &quot;binary_logloss&quot;, &quot;binary_error&quot;
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the &quot;metric&quot; section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effect when verbose &gt; 0</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_valids">valids</code></td>
<td>
<p>a list of <code>gpb.Dataset</code> objects, used for validation</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_colnames">colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_init_model">init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_nrounds">nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_obj">obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_params">params</code></td>
<td>
<p>list of &quot;tuning&quot; parameters. 
See <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information. 
A few key parameters:
</p>

<ul>
<li><p><code>learning_rate</code>: The learning rate, also called shrinkage or damping parameter 
(default = 0.1). An important tuning parameter for boosting. Lower values usually 
lead to higher predictive accuracy but more boosting iterations are needed 
</p>
</li>
<li><p><code>num_leaves</code>: Number of leaves in a tree. Tuning parameter for 
tree-boosting (default = 31)
</p>
</li>
<li><p><code>max_depth</code>: Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)
</p>
</li>
<li><p><code>min_data_in_leaf</code>: Minimal number of samples per leaf. Tuning parameter for 
tree-boosting (default = 20)
</p>
</li>
<li><p><code>lambda_l2</code>: L2 regularization (default = 0)
</p>
</li>
<li><p><code>lambda_l1</code>: L1 regularization (default = 0)
</p>
</li>
<li><p><code>max_bin</code>: Maximal number of bins that feature values will be bucketed in (default = 255)
</p>
</li>
<li><p><code>line_search_step_length</code> (default = FALSE): If TRUE, a line search is done to find the optimal 
step length for every boosting update (see, e.g., Friedman 2001). This is then multiplied by the learning rate 
</p>
</li>
<li><p><code>train_gp_model_cov_pars</code> (default = TRUE): If TRUE, the covariance parameters of the Gaussian process 
are estimated in every boosting iterations,  otherwise the gp_model parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide values via 
the 'init_cov_pars' parameter when creating the gp_model 
</p>
</li>
<li><p><code>use_gp_model_for_validation</code> (default = TRUE): If TRUE, the Gaussian process is also used 
(in addition to the tree model) for calculating predictions on the validation data 
</p>
</li>
<li><p><code>leaves_newton_update</code> (default = FALSE): Set this to TRUE to do a Newton update step for the tree leaves 
after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm) 
</p>
</li>
<li><p>num_threads: Number of threads. For the best speed, set this to
the number of real CPU cores(<code>parallel::detectCores(logical = FALSE)</code>),
not the number of threads (most CPU using hyper-threading to generate 2 threads
per CPU core).
</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0, also will disable the print of evaluation during training</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_line_search_step_length">line_search_step_length</code></td>
<td>
<p>Boolean. If TRUE, a line search is done to find the optimal step length for every boosting update 
(see, e.g., Friedman 2001). This is then multiplied by the <code>learning_rate</code>. 
Applies only to the GPBoost algorithm</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_use_gp_model_for_validation">use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td></tr>
<tr><td><code id="gpb_shared_params_+3A_train_gp_model_cov_pars">train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td></tr>
</table>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>

<hr>
<h2 id='gpb.convert_with_rules'>Data preparator for GPBoost datasets with rules (integer)</h2><span id='topic+gpb.convert_with_rules'></span>

<h3>Description</h3>

<p>Attempts to prepare a clean dataset to prepare to put in a <code>gpb.Dataset</code>.
Factor, character, and logical columns are converted to integer. Missing values
in factors and characters will be filled with 0L. Missing values in logicals
will be filled with -1L.
</p>
<p>This function returns and optionally takes in &quot;rules&quot; the describe exactly
how to convert values in columns.
</p>
<p>Columns that contain only NA values will be converted by this function but will
not show up in the returned <code>rules</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.convert_with_rules(data, rules = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.convert_with_rules_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table to prepare.</p>
</td></tr>
<tr><td><code id="gpb.convert_with_rules_+3A_rules">rules</code></td>
<td>
<p>A set of rules from the data preparator, if already used. This should be an R list,
where names are column names in <code>data</code> and values are named character
vectors whose names are column values and whose values are new values to
replace them with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the cleaned dataset (<code>data</code>) and the rules (<code>rules</code>).
Note that the data must be converted to a matrix format (<code>as.matrix</code>) for input in
<code>gpb.Dataset</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

str(iris)

new_iris &lt;- gpb.convert_with_rules(data = iris)
str(new_iris$data)

data(iris) # Erase iris dataset
iris$Species[1L] &lt;- "NEW FACTOR" # Introduce junk factor (NA)

# Use conversion using known rules
# Unknown factors become 0, excellent for sparse datasets
newer_iris &lt;- gpb.convert_with_rules(data = iris, rules = new_iris$rules)

# Unknown factor is now zero, perfect for sparse datasets
newer_iris$data[1L, ] # Species became 0 as it is an unknown factor

newer_iris$data[1L, 5L] &lt;- 1.0 # Put back real initial value

# Is the newly created dataset equal? YES!
all.equal(new_iris$data, newer_iris$data)

# Can we test our own rules?
data(iris) # Erase iris dataset

# We remapped values differently
personal_rules &lt;- list(
  Species = c(
    "setosa" = 3L
    , "versicolor" = 2L
    , "virginica" = 1L
  )
)
newest_iris &lt;- gpb.convert_with_rules(data = iris, rules = personal_rules)
str(newest_iris$data) # SUCCESS!

</code></pre>

<hr>
<h2 id='gpb.cv'>CV function for number of boosting iterations</h2><span id='topic+gpb.cv'></span>

<h3>Description</h3>

<p>Cross validation function for determining number of boosting iterations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.cv(params = list(), data, nrounds = 100L, gp_model = NULL,
  line_search_step_length = FALSE, use_gp_model_for_validation = TRUE,
  fit_GP_cov_pars_OOS = FALSE, train_gp_model_cov_pars = TRUE,
  folds = NULL, nfold = 4L, label = NULL, weight = NULL, obj = NULL,
  eval = NULL, verbose = 1L, record = TRUE, eval_freq = 1L,
  showsd = FALSE, stratified = TRUE, init_model = NULL, colnames = NULL,
  categorical_feature = NULL, early_stopping_rounds = NULL,
  callbacks = list(), reset_data = FALSE, delete_boosters_folds = FALSE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.cv_+3A_params">params</code></td>
<td>
<p>list of &quot;tuning&quot; parameters. 
See <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information. 
A few key parameters:
</p>

<ul>
<li><p><code>learning_rate</code>: The learning rate, also called shrinkage or damping parameter 
(default = 0.1). An important tuning parameter for boosting. Lower values usually 
lead to higher predictive accuracy but more boosting iterations are needed 
</p>
</li>
<li><p><code>num_leaves</code>: Number of leaves in a tree. Tuning parameter for 
tree-boosting (default = 31)
</p>
</li>
<li><p><code>max_depth</code>: Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)
</p>
</li>
<li><p><code>min_data_in_leaf</code>: Minimal number of samples per leaf. Tuning parameter for 
tree-boosting (default = 20)
</p>
</li>
<li><p><code>lambda_l2</code>: L2 regularization (default = 0)
</p>
</li>
<li><p><code>lambda_l1</code>: L1 regularization (default = 0)
</p>
</li>
<li><p><code>max_bin</code>: Maximal number of bins that feature values will be bucketed in (default = 255)
</p>
</li>
<li><p><code>line_search_step_length</code> (default = FALSE): If TRUE, a line search is done to find the optimal 
step length for every boosting update (see, e.g., Friedman 2001). This is then multiplied by the learning rate 
</p>
</li>
<li><p><code>train_gp_model_cov_pars</code> (default = TRUE): If TRUE, the covariance parameters of the Gaussian process 
are estimated in every boosting iterations,  otherwise the gp_model parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide values via 
the 'init_cov_pars' parameter when creating the gp_model 
</p>
</li>
<li><p><code>use_gp_model_for_validation</code> (default = TRUE): If TRUE, the Gaussian process is also used 
(in addition to the tree model) for calculating predictions on the validation data 
</p>
</li>
<li><p><code>leaves_newton_update</code> (default = FALSE): Set this to TRUE to do a Newton update step for the tree leaves 
after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm) 
</p>
</li>
<li><p>num_threads: Number of threads. For the best speed, set this to
the number of real CPU cores(<code>parallel::detectCores(logical = FALSE)</code>),
not the number of threads (most CPU using hyper-threading to generate 2 threads
per CPU core).
</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb.cv_+3A_data">data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+gpb.cv">gpb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_nrounds">nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_line_search_step_length">line_search_step_length</code></td>
<td>
<p>Boolean. If TRUE, a line search is done to find the optimal step length for every boosting update 
(see, e.g., Friedman 2001). This is then multiplied by the <code>learning_rate</code>. 
Applies only to the GPBoost algorithm</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_use_gp_model_for_validation">use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_fit_gp_cov_pars_oos">fit_GP_cov_pars_OOS</code></td>
<td>
<p>Boolean (default = FALSE). If TRUE, the covariance parameters of the 
<code>gp_model</code> model are estimated using the out-of-sample (OOS) predictions 
on the validation data using the optimal number of iterations (after performing the CV). 
This corresponds to the GPBoostOOS algorithm.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_train_gp_model_cov_pars">train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_folds">folds</code></td>
<td>
<p><code>list</code> provides a possibility to use a list of pre-defined CV folds
(each element must be a vector of test fold's indices). When folds are supplied,
the <code>nfold</code> and <code>stratified</code> parameters are ignored.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_nfold">nfold</code></td>
<td>
<p>the original dataset is randomly partitioned into <code>nfold</code> equal size subsamples.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_label">label</code></td>
<td>
<p>Vector of labels, used if <code>data</code> is not an <code><a href="#topic+gpb.Dataset">gpb.Dataset</a></code></p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_weight">weight</code></td>
<td>
<p>vector of response values. If not NULL, will set to dataset</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_obj">obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_eval">eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: &quot;test_neg_log_likelihood&quot;, &quot;mse&quot;, &quot;rmse&quot;, &quot;mae&quot;, 
&quot;auc&quot;, &quot;average_precision&quot;, &quot;binary_logloss&quot;, &quot;binary_error&quot;
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the &quot;metric&quot; section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb.cv_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0, also will disable the print of evaluation during training</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effect when verbose &gt; 0</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_showsd">showsd</code></td>
<td>
<p><code>boolean</code>, whether to show standard deviation of cross validation.
This parameter defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_stratified">stratified</code></td>
<td>
<p>a <code>boolean</code> indicating whether sampling of folds should be stratified
by the values of outcome labels.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_init_model">init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_colnames">colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_reset_data">reset_data</code></td>
<td>
<p>Boolean, setting it to TRUE (not the default value) will transform the booster model
into a predictor model which frees up memory and the original datasets</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_delete_boosters_folds">delete_boosters_folds</code></td>
<td>
<p>Boolean, setting it to TRUE (not the default value) will delete the boosters of the individual folds</p>
</td></tr>
<tr><td><code id="gpb.cv_+3A_...">...</code></td>
<td>
<p>other parameters, see Parameters.rst for more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained model <code>gpb.CVBooster</code>.
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Author(s)</h3>

<p>Authors of the LightGBM R package, Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples

library(gpboost)
data(GPBoost_data, package = "gpboost")

# Create random effects model and dataset
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood="gaussian")
dtrain &lt;- gpb.Dataset(X, label = y)
params &lt;- list(learning_rate = 0.05,
               max_depth = 6,
               min_data_in_leaf = 5)
# Run CV
cvbst &lt;- gpb.cv(params = params,
                data = dtrain,
                gp_model = gp_model,
                nrounds = 100,
                nfold = 4,
                eval = "l2",
                early_stopping_rounds = 5,
                use_gp_model_for_validation = TRUE)
print(paste0("Optimal number of iterations: ", cvbst$best_iter,
             ", best test error: ", cvbst$best_score))

</code></pre>

<hr>
<h2 id='gpb.Dataset'>Construct <code>gpb.Dataset</code> object</h2><span id='topic+gpb.Dataset'></span>

<h3>Description</h3>

<p>Construct <code>gpb.Dataset</code> object from dense matrix, sparse matrix
or local file (that was created previously by saving an <code>gpb.Dataset</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset(data, params = list(), reference = NULL, colnames = NULL,
  categorical_feature = NULL, free_raw_data = FALSE, info = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code> object or a character representing a filename</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_params">params</code></td>
<td>
<p>a list of parameters. See
<a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst#dataset-parameters">
the &quot;Dataset Parameters&quot; section of the parameter documentation</a> for a list of parameters
and valid values.</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_reference">reference</code></td>
<td>
<p>reference dataset. When GPBoost creates a Dataset, it does some preprocessing like binning
continuous features into histograms. If you want to apply the same bin boundaries from an existing
dataset to new <code>data</code>, pass that existing Dataset to this argument.</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_colnames">colnames</code></td>
<td>
<p>names of columns</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_free_raw_data">free_raw_data</code></td>
<td>
<p>GPBoost constructs its data format, called a &quot;Dataset&quot;, from tabular data.
By default, this Dataset object on the R side does keep a copy of the raw data.
If you set <code>free_raw_data = TRUE</code>, no copy of the raw data is kept (this reduces memory usage)</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_info">info</code></td>
<td>
<p>a list of information of the <code>gpb.Dataset</code> object</p>
</td></tr>
<tr><td><code id="gpb.Dataset_+3A_...">...</code></td>
<td>
<p>other information to pass to <code>info</code> or parameters pass to <code>params</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data_file &lt;- tempfile(fileext = ".data")
gpb.Dataset.save(dtrain, data_file)
dtrain &lt;- gpb.Dataset(data_file)
gpb.Dataset.construct(dtrain)

</code></pre>

<hr>
<h2 id='gpb.Dataset.construct'>Construct Dataset explicitly</h2><span id='topic+gpb.Dataset.construct'></span>

<h3>Description</h3>

<p>Construct Dataset explicitly
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset.construct(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset.construct_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>gpb.Dataset</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
gpb.Dataset.construct(dtrain)

</code></pre>

<hr>
<h2 id='gpb.Dataset.create.valid'>Construct validation data</h2><span id='topic+gpb.Dataset.create.valid'></span>

<h3>Description</h3>

<p>Construct validation data according to training data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset.create.valid(dataset, data, info = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset.create.valid_+3A_dataset">dataset</code></td>
<td>
<p><code>gpb.Dataset</code> object, training data</p>
</td></tr>
<tr><td><code id="gpb.Dataset.create.valid_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code> object or a character representing a filename</p>
</td></tr>
<tr><td><code id="gpb.Dataset.create.valid_+3A_info">info</code></td>
<td>
<p>a list of information of the <code>gpb.Dataset</code> object</p>
</td></tr>
<tr><td><code id="gpb.Dataset.create.valid_+3A_...">...</code></td>
<td>
<p>other information to pass to <code>info</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset.create.valid(dtrain, test$data, label = test$label)

</code></pre>

<hr>
<h2 id='gpb.Dataset.save'>Save <code>gpb.Dataset</code> to a binary file</h2><span id='topic+gpb.Dataset.save'></span>

<h3>Description</h3>

<p>Please note that <code>init_score</code> is not saved in binary file.
If you need it, please set it again after loading Dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset.save(dataset, fname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset.save_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="gpb.Dataset.save_+3A_fname">fname</code></td>
<td>
<p>object filename of output file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
gpb.Dataset.save(dtrain, tempfile(fileext = ".bin"))

</code></pre>

<hr>
<h2 id='gpb.Dataset.set.categorical'>Set categorical feature of <code>gpb.Dataset</code></h2><span id='topic+gpb.Dataset.set.categorical'></span>

<h3>Description</h3>

<p>Set the categorical features of an <code>gpb.Dataset</code> object. Use this function
to tell GPBoost which features should be treated as categorical.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset.set.categorical(dataset, categorical_feature)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset.set.categorical_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="gpb.Dataset.set.categorical_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data_file &lt;- tempfile(fileext = ".data")
gpb.Dataset.save(dtrain, data_file)
dtrain &lt;- gpb.Dataset(data_file)
gpb.Dataset.set.categorical(dtrain, 1L:2L)

</code></pre>

<hr>
<h2 id='gpb.Dataset.set.reference'>Set reference of <code>gpb.Dataset</code></h2><span id='topic+gpb.Dataset.set.reference'></span>

<h3>Description</h3>

<p>If you want to use validation data, you should set reference to training data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.Dataset.set.reference(dataset, reference)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.Dataset.set.reference_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="gpb.Dataset.set.reference_+3A_reference">reference</code></td>
<td>
<p>object of class <code>gpb.Dataset</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package ="gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset(test$data, test = train$label)
gpb.Dataset.set.reference(dtest, dtrain)

</code></pre>

<hr>
<h2 id='gpb.dump'>Dump GPBoost model to json</h2><span id='topic+gpb.dump'></span>

<h3>Description</h3>

<p>Dump GPBoost model to json
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.dump(booster, num_iteration = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.dump_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>gpb.Booster</code></p>
</td></tr>
<tr><td><code id="gpb.dump_+3A_num_iteration">num_iteration</code></td>
<td>
<p>number of iteration want to predict with, NULL or &lt;= 0 means use best iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>json format of model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(objective = "regression", metric = "l2")
valids &lt;- list(test = dtest)
model &lt;- gpb.train(
  params = params
  , data = dtrain
  , nrounds = 10L
  , valids = valids
  , min_data = 1L
  , learning_rate = 1.0
  , early_stopping_rounds = 5L
)
json_model &lt;- gpb.dump(model)

</code></pre>

<hr>
<h2 id='gpb.get.eval.result'>Get record evaluation result from booster</h2><span id='topic+gpb.get.eval.result'></span>

<h3>Description</h3>

<p>Given a <code>gpb.Booster</code>, return evaluation results for a
particular metric on a particular dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.get.eval.result(booster, data_name, eval_name, iters = NULL,
  is_err = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.get.eval.result_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>gpb.Booster</code></p>
</td></tr>
<tr><td><code id="gpb.get.eval.result_+3A_data_name">data_name</code></td>
<td>
<p>Name of the dataset to return evaluation results for.</p>
</td></tr>
<tr><td><code id="gpb.get.eval.result_+3A_eval_name">eval_name</code></td>
<td>
<p>Name of the evaluation metric to return results for.</p>
</td></tr>
<tr><td><code id="gpb.get.eval.result_+3A_iters">iters</code></td>
<td>
<p>An integer vector of iterations you want to get evaluation results for. If NULL
(the default), evaluation results for all iterations will be returned.</p>
</td></tr>
<tr><td><code id="gpb.get.eval.result_+3A_is_err">is_err</code></td>
<td>
<p>TRUE will return evaluation error instead</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of evaluation result
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train a regression model
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(objective = "regression", metric = "l2")
valids &lt;- list(test = dtest)
model &lt;- gpb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
  , valids = valids
  , min_data = 1L
  , learning_rate = 1.0
)

# Examine valid data_name values
print(setdiff(names(model$record_evals), "start_iter"))

# Examine valid eval_name values for dataset "test"
print(names(model$record_evals[["test"]]))

# Get L2 values for "test" dataset
gpb.get.eval.result(model, "test", "l2")

</code></pre>

<hr>
<h2 id='gpb.grid.search.tune.parameters'>Function for choosing tuning parameters</h2><span id='topic+gpb.grid.search.tune.parameters'></span>

<h3>Description</h3>

<p>Function that allows for choosing tuning parameters from a grid in a determinstic or random way using cross validation or validation data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.grid.search.tune.parameters(param_grid, data, params = list(),
  num_try_random = NULL, nrounds = 100L, gp_model = NULL,
  line_search_step_length = FALSE, use_gp_model_for_validation = TRUE,
  train_gp_model_cov_pars = TRUE, folds = NULL, nfold = 4L,
  label = NULL, weight = NULL, obj = NULL, eval = NULL,
  verbose_eval = 1L, stratified = TRUE, init_model = NULL,
  colnames = NULL, categorical_feature = NULL,
  early_stopping_rounds = NULL, callbacks = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_param_grid">param_grid</code></td>
<td>
<p><code>list</code> with candidate parameters defining the grid over which a search is done</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_data">data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+gpb.cv">gpb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_params">params</code></td>
<td>
<p><code>list</code> with other parameters not included in <code>param_grid</code></p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_num_try_random">num_try_random</code></td>
<td>
<p><code>integer</code> with number of random trial on parameter grid. If NULL, a deterministic search is done</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_nrounds">nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_line_search_step_length">line_search_step_length</code></td>
<td>
<p>Boolean. If TRUE, a line search is done to find the optimal step length for every boosting update 
(see, e.g., Friedman 2001). This is then multiplied by the <code>learning_rate</code>. 
Applies only to the GPBoost algorithm</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_use_gp_model_for_validation">use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_train_gp_model_cov_pars">train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_folds">folds</code></td>
<td>
<p><code>list</code> provides a possibility to use a list of pre-defined CV folds
(each element must be a vector of test fold's indices). When folds are supplied,
the <code>nfold</code> and <code>stratified</code> parameters are ignored.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_nfold">nfold</code></td>
<td>
<p>the original dataset is randomly partitioned into <code>nfold</code> equal size subsamples.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_label">label</code></td>
<td>
<p>Vector of labels, used if <code>data</code> is not an <code><a href="#topic+gpb.Dataset">gpb.Dataset</a></code></p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_weight">weight</code></td>
<td>
<p>vector of response values. If not NULL, will set to dataset</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_obj">obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_eval">eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: &quot;test_neg_log_likelihood&quot;, &quot;mse&quot;, &quot;rmse&quot;, &quot;mae&quot;, 
&quot;auc&quot;, &quot;average_precision&quot;, &quot;binary_logloss&quot;, &quot;binary_error&quot;
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the &quot;metric&quot; section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_verbose_eval">verbose_eval</code></td>
<td>
<p><code>integer</code>. Whether to display information on the progress of tuning parameter choice. 
If None or 0, verbose is of.
If = 1, summary progress information is displayed for every parameter combination.
If &gt;= 2, detailed progress is displayed at every boosting stage for every parameter combination.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_stratified">stratified</code></td>
<td>
<p>a <code>boolean</code> indicating whether sampling of folds should be stratified
by the values of outcome labels.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_init_model">init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_colnames">colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="gpb.grid.search.tune.parameters_+3A_...">...</code></td>
<td>
<p>other parameters, see Parameters.rst for more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the best parameter combination and score
The list has the following format:
list(&quot;best_params&quot; = best_params, &quot;best_iter&quot; = best_iter, &quot;best_score&quot; = best_score)
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples

library(gpboost)
data(GPBoost_data, package = "gpboost")

# Create random effects model, dataset, and define parameter grid
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood="gaussian")
dataset &lt;- gpb.Dataset(X, label = y)
param_grid = list("learning_rate" = c(1,0.1,0.01), 
                  "min_data_in_leaf" = c(10,100,1000),
                  "max_depth" = c(1,2,3,5,10),
                  "lambda_l2" = c(0,1,10))
other_params &lt;- list(num_leaves = 2^10)
# Note: here we try different values for 'max_depth' and thus set 'num_leaves' to a large value.
#       An alternative strategy is to impose no limit on 'max_depth', 
#       and try different values for 'num_leaves' as follows:
# param_grid = list("learning_rate" = c(1,0.1,0.01), 
#                   "min_data_in_leaf" = c(10,100,1000),
#                   "num_leaves" = 2^(1:10),
#                   "lambda_l2" = c(0,1,10))
# other_params &lt;- list(max_depth = -1)
set.seed(1)
opt_params &lt;- gpb.grid.search.tune.parameters(param_grid = param_grid, params = other_params,
                                              num_try_random = NULL, nfold = 4,
                                              data = dataset, gp_model = gp_model,
                                              use_gp_model_for_validation=TRUE, verbose_eval = 1,
                                              nrounds = 1000, early_stopping_rounds = 10)
print(paste0("Best parameters: ",
             paste0(unlist(lapply(seq_along(opt_params$best_params), 
                                  function(y, n, i) { paste0(n[[i]],": ", y[[i]]) }, 
                                  y=opt_params$best_params, 
                                  n=names(opt_params$best_params))), collapse=", ")))
print(paste0("Best number of iterations: ", opt_params$best_iter))
print(paste0("Best score: ", round(opt_params$best_score, digits=3)))
# Note: other scoring / evaluation metrics can be chosen using the 
#       'metric' argument, e.g., metric = "l1"

# Using manually defined validation data instead of cross-validation
valid_tune_idx &lt;- sample.int(length(y), as.integer(0.2*length(y)))
folds = list(valid_tune_idx)
opt_params &lt;- gpb.grid.search.tune.parameters(param_grid = param_grid, params = other_params,
                                              num_try_random = NULL, folds = folds,
                                              data = dataset, gp_model = gp_model,
                                              use_gp_model_for_validation=TRUE, verbose_eval = 1,
                                              nrounds = 1000, early_stopping_rounds = 10)


</code></pre>

<hr>
<h2 id='gpb.importance'>Compute feature importance in a model</h2><span id='topic+gpb.importance'></span>

<h3>Description</h3>

<p>Creates a <code>data.table</code> of feature importances in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.importance(model, percentage = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.importance_+3A_model">model</code></td>
<td>
<p>object of class <code>gpb.Booster</code>.</p>
</td></tr>
<tr><td><code id="gpb.importance_+3A_percentage">percentage</code></td>
<td>
<p>whether to show importance in relative percentage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For a tree model, a <code>data.table</code> with the following columns:
</p>

<ul>
<li><p><code>Feature</code>: Feature names in the model.
</p>
</li>
<li><p><code>Gain</code>: The total gain of this feature's splits.
</p>
</li>
<li><p><code>Cover</code>: The number of observation related to this feature.
</p>
</li>
<li><p><code>Frequency</code>: The number of times a feature splited in trees.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.1
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
)
model &lt;- gpb.train(
    params = params
    , data = dtrain
    , nrounds = 5L
)

tree_imp1 &lt;- gpb.importance(model, percentage = TRUE)
tree_imp2 &lt;- gpb.importance(model, percentage = FALSE)

</code></pre>

<hr>
<h2 id='gpb.interprete'>Compute feature contribution of prediction</h2><span id='topic+gpb.interprete'></span>

<h3>Description</h3>

<p>Computes feature contribution components of rawscore prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.interprete(model, data, idxset, num_iteration = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.interprete_+3A_model">model</code></td>
<td>
<p>object of class <code>gpb.Booster</code>.</p>
</td></tr>
<tr><td><code id="gpb.interprete_+3A_data">data</code></td>
<td>
<p>a matrix object or a dgCMatrix object.</p>
</td></tr>
<tr><td><code id="gpb.interprete_+3A_idxset">idxset</code></td>
<td>
<p>an integer vector of indices of rows needed.</p>
</td></tr>
<tr><td><code id="gpb.interprete_+3A_num_iteration">num_iteration</code></td>
<td>
<p>number of iteration want to predict with, NULL or &lt;= 0 means use best iteration.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For regression, binary classification and lambdarank model, a <code>list</code> of <code>data.table</code>
with the following columns:
</p>

<ul>
<li><p><code>Feature</code>: Feature names in the model.
</p>
</li>
<li><p><code>Contribution</code>: The total contribution of this feature's splits.
</p>
</li></ul>

<p>For multiclass classification, a <code>list</code> of <code>data.table</code> with the Feature column and
Contribution columns to each class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Logit &lt;- function(x) log(x / (1.0 - x))
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
setinfo(dtrain, "init_score", rep(Logit(mean(train$label)), length(train$label)))
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test

params &lt;- list(
    objective = "binary"
    , learning_rate = 0.1
    , max_depth = -1L
    , min_data_in_leaf = 1L
    , min_sum_hessian_in_leaf = 1.0
)
model &lt;- gpb.train(
    params = params
    , data = dtrain
    , nrounds = 3L
)

tree_interpretation &lt;- gpb.interprete(model, test$data, 1L:5L)

</code></pre>

<hr>
<h2 id='gpb.load'>Load GPBoost model</h2><span id='topic+gpb.load'></span>

<h3>Description</h3>

<p>Load GPBoost takes in either a file path or model string.
If both are provided, Load will default to loading from file
Boosters with gp_models can only be loaded from file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.load(filename = NULL, model_str = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.load_+3A_filename">filename</code></td>
<td>
<p>path of model file</p>
</td></tr>
<tr><td><code id="gpb.load_+3A_model_str">model_str</code></td>
<td>
<p>a str containing the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gpb.Booster
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(GPBoost_data, package = "gpboost")

# Train model and make prediction
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 16,
               learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var= TRUE, pred_latent = TRUE)
# Save model to file
filename &lt;- tempfile(fileext = ".json")
gpb.save(bst,filename = filename)
# Load from file and make predictions again
bst_loaded &lt;- gpb.load(filename = filename)
pred_loaded &lt;- predict(bst_loaded, data = X_test, group_data_pred = group_data_test[,1],
                       predict_var= TRUE, pred_latent = TRUE)
# Check equality
pred$fixed_effect - pred_loaded$fixed_effect
pred$random_effect_mean - pred_loaded$random_effect_mean
pred$random_effect_cov - pred_loaded$random_effect_cov

</code></pre>

<hr>
<h2 id='gpb.model.dt.tree'>Parse a GPBoost model json dump</h2><span id='topic+gpb.model.dt.tree'></span>

<h3>Description</h3>

<p>Parse a GPBoost model json dump into a <code>data.table</code> structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.model.dt.tree(model, num_iteration = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.model.dt.tree_+3A_model">model</code></td>
<td>
<p>object of class <code>gpb.Booster</code></p>
</td></tr>
<tr><td><code id="gpb.model.dt.tree_+3A_num_iteration">num_iteration</code></td>
<td>
<p>number of iterations you want to predict with. NULL or
&lt;= 0 means use best iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with detailed information about model trees' nodes and leafs.
</p>
<p>The columns of the <code>data.table</code> are:
</p>

<ul>
<li><p><code>tree_index</code>: ID of a tree in a model (integer)
</p>
</li>
<li><p><code>split_index</code>: ID of a node in a tree (integer)
</p>
</li>
<li><p><code>split_feature</code>: for a node, it's a feature name (character);
for a leaf, it simply labels it as <code>"NA"</code>
</p>
</li>
<li><p><code>node_parent</code>: ID of the parent node for current node (integer)
</p>
</li>
<li><p><code>leaf_index</code>: ID of a leaf in a tree (integer)
</p>
</li>
<li><p><code>leaf_parent</code>: ID of the parent node for current leaf (integer)
</p>
</li>
<li><p><code>split_gain</code>: Split gain of a node
</p>
</li>
<li><p><code>threshold</code>: Splitting threshold value of a node
</p>
</li>
<li><p><code>decision_type</code>: Decision type of a node
</p>
</li>
<li><p><code>default_left</code>: Determine how to handle NA value, TRUE -&gt; Left, FALSE -&gt; Right
</p>
</li>
<li><p><code>internal_value</code>: Node value
</p>
</li>
<li><p><code>internal_count</code>: The number of observation collected by a node
</p>
</li>
<li><p><code>leaf_value</code>: Leaf value
</p>
</li>
<li><p><code>leaf_count</code>: The number of observation collected by a leaf
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.01
  , num_leaves = 63L
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
)
model &lt;- gpb.train(params, dtrain, 10L)

tree_dt &lt;- gpb.model.dt.tree(model)

</code></pre>

<hr>
<h2 id='gpb.plot.importance'>Plot feature importance as a bar graph</h2><span id='topic+gpb.plot.importance'></span>

<h3>Description</h3>

<p>Plot previously calculated feature importance: Gain, Cover and Frequency, as a bar graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.plot.importance(tree_imp, top_n = 10L, measure = "Gain",
  left_margin = 10L, cex = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.plot.importance_+3A_tree_imp">tree_imp</code></td>
<td>
<p>a <code>data.table</code> returned by <code><a href="#topic+gpb.importance">gpb.importance</a></code>.</p>
</td></tr>
<tr><td><code id="gpb.plot.importance_+3A_top_n">top_n</code></td>
<td>
<p>maximal number of top features to include into the plot.</p>
</td></tr>
<tr><td><code id="gpb.plot.importance_+3A_measure">measure</code></td>
<td>
<p>the name of importance measure to plot, can be &quot;Gain&quot;, &quot;Cover&quot; or &quot;Frequency&quot;.</p>
</td></tr>
<tr><td><code id="gpb.plot.importance_+3A_left_margin">left_margin</code></td>
<td>
<p>(base R barplot) allows to adjust the left margin size to fit feature names.</p>
</td></tr>
<tr><td><code id="gpb.plot.importance_+3A_cex">cex</code></td>
<td>
<p>(base R barplot) passed as <code>cex.names</code> parameter to <code><a href="graphics.html#topic+barplot">barplot</a></code>.
Set a number smaller than 1.0 to make the bar labels smaller than R's default and values
greater than 1.0 to make them larger.</p>
</td></tr>
<tr><td><code id="gpb.plot.importance_+3A_...">...</code></td>
<td>
<p>other parameters passed to graphics::barplot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph represents each feature as a horizontal bar of length proportional to the defined importance of a feature.
Features are shown ranked in a decreasing importance order.
</p>


<h3>Value</h3>

<p>The <code>gpb.plot.importance</code> function creates a <code>barplot</code>
and silently returns a processed data.table with <code>top_n</code> features sorted by defined importance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)

params &lt;- list(
    objective = "binary"
    , learning_rate = 0.1
    , min_data_in_leaf = 1L
    , min_sum_hessian_in_leaf = 1.0
)

model &lt;- gpb.train(
    params = params
    , data = dtrain
    , nrounds = 5L
)

tree_imp &lt;- gpb.importance(model, percentage = TRUE)
gpb.plot.importance(tree_imp, top_n = 5L, measure = "Gain")

</code></pre>

<hr>
<h2 id='gpb.plot.interpretation'>Plot feature contribution as a bar graph</h2><span id='topic+gpb.plot.interpretation'></span>

<h3>Description</h3>

<p>Plot previously calculated feature contribution as a bar graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.plot.interpretation(tree_interpretation_dt, top_n = 10L, cols = 1L,
  left_margin = 10L, cex = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.plot.interpretation_+3A_tree_interpretation_dt">tree_interpretation_dt</code></td>
<td>
<p>a <code>data.table</code> returned by <code><a href="#topic+gpb.interprete">gpb.interprete</a></code>.</p>
</td></tr>
<tr><td><code id="gpb.plot.interpretation_+3A_top_n">top_n</code></td>
<td>
<p>maximal number of top features to include into the plot.</p>
</td></tr>
<tr><td><code id="gpb.plot.interpretation_+3A_cols">cols</code></td>
<td>
<p>the column numbers of layout, will be used only for multiclass classification feature contribution.</p>
</td></tr>
<tr><td><code id="gpb.plot.interpretation_+3A_left_margin">left_margin</code></td>
<td>
<p>(base R barplot) allows to adjust the left margin size to fit feature names.</p>
</td></tr>
<tr><td><code id="gpb.plot.interpretation_+3A_cex">cex</code></td>
<td>
<p>(base R barplot) passed as <code>cex.names</code> parameter to <code>barplot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph represents each feature as a horizontal bar of length proportional to the defined
contribution of a feature. Features are shown ranked in a decreasing contribution order.
</p>


<h3>Value</h3>

<p>The <code>gpb.plot.interpretation</code> function creates a <code>barplot</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Logit &lt;- function(x) {
  log(x / (1.0 - x))
}
data(agaricus.train, package = "gpboost")
labels &lt;- agaricus.train$label
dtrain &lt;- gpb.Dataset(
  agaricus.train$data
  , label = labels
)
setinfo(dtrain, "init_score", rep(Logit(mean(labels)), length(labels)))

data(agaricus.test, package = "gpboost")

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.1
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
)
model &lt;- gpb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
)

tree_interpretation &lt;- gpb.interprete(
  model = model
  , data = agaricus.test$data
  , idxset = 1L:5L
)
gpb.plot.interpretation(
  tree_interpretation_dt = tree_interpretation[[1L]]
  , top_n = 3L
)

</code></pre>

<hr>
<h2 id='gpb.plot.part.dep.interact'>Plot interaction partial dependence plots</h2><span id='topic+gpb.plot.part.dep.interact'></span>

<h3>Description</h3>

<p>Plot interaction partial dependence plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.plot.part.dep.interact(model, data, variables, n.pt.per.var = 20,
  subsample = pmin(1, n.pt.per.var^2 * 100/nrow(data)),
  discrete.variables = c(FALSE, FALSE), which.class = NULL,
  type = "filled.contour", nlevels = 20, xlab = variables[1],
  ylab = variables[2], zlab = "", main = "", return_plot_data = FALSE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_model">model</code></td>
<td>
<p>A <code>gpb.Booster</code> model object</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_data">data</code></td>
<td>
<p>A <code>matrix</code> with data for creating partial dependence plots</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_variables">variables</code></td>
<td>
<p>A <code>vector</code> of length two of type <code>string</code> with 
names of the columns or <code>integer</code> with indices of the columns in 
<code>data</code> for which an interaction dependence plot is created</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_n.pt.per.var">n.pt.per.var</code></td>
<td>
<p>Number of grid points per variable (used only if a variable is not discrete)
For continuous variables, the two-dimensional grid for the interaction plot 
has dimension c(n.pt.per.var, n.pt.per.var)</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_subsample">subsample</code></td>
<td>
<p>Fraction of random samples in <code>data</code> to be used for calculating the partial dependence plot</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_discrete.variables">discrete.variables</code></td>
<td>
<p>A <code>vector</code> of length two of type <code>boolean</code>. 
If an entry is TRUE, the evaluation grid of the corresponding variable is set to the unique values of the variable</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_which.class">which.class</code></td>
<td>
<p>An <code>integer</code> indicating the class in multi-class 
classification (value from 0 to num_class - 1)</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_type">type</code></td>
<td>
<p>A <code>character</code> string indicating the type of the plot. 
Supported values: &quot;filled.contour&quot; and &quot;contour&quot;</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_nlevels">nlevels</code></td>
<td>
<p>Parameter passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_xlab">xlab</code></td>
<td>
<p>Parameter passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_ylab">ylab</code></td>
<td>
<p>Parameter passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_zlab">zlab</code></td>
<td>
<p>Parameter passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_main">main</code></td>
<td>
<p>Parameter passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_return_plot_data">return_plot_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the data for creating the partial dependence  plot is returned</p>
</td></tr>
<tr><td><code id="gpb.plot.part.dep.interact_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to the <code>filled.contour</code> or <code>contour</code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with three entries for creating the partial dependence plot: 
the first two entries are <code>vector</code>s with x and y coordinates. 
The third is a two-dimensional <code>matrix</code> of dimension c(length(x), length(y)) 
with z-coordinates. This is only returned if <code>return_plot_data==TRUE</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(GPBoost_data, package = "gpboost")
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
gpboost_model &lt;- gpboost(data = X,
                        label = y,
                        gp_model = gp_model,
                        nrounds = 16,
                        learning_rate = 0.05,
                        max_depth = 6,
                        min_data_in_leaf = 5,
                        verbose = 0)
gpb.plot.part.dep.interact(gpboost_model, X, variables = c(1,2))

</code></pre>

<hr>
<h2 id='gpb.plot.partial.dependence'>Plot partial dependence plots</h2><span id='topic+gpb.plot.partial.dependence'></span>

<h3>Description</h3>

<p>Plot partial dependence plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.plot.partial.dependence(model, data, variable, n.pt = 100,
  subsample = pmin(1, n.pt * 100/nrow(data)), discrete.x = FALSE,
  which.class = NULL, xlab = deparse(substitute(variable)), ylab = "",
  type = if (discrete.x) "p" else "b", main = "",
  return_plot_data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.plot.partial.dependence_+3A_model">model</code></td>
<td>
<p>A <code>gpb.Booster</code> model object</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_data">data</code></td>
<td>
<p>A <code>matrix</code> with data for creating partial dependence plots</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_variable">variable</code></td>
<td>
<p>A <code>string</code> with a name of the column or an <code>integer</code> 
with an index of the column in <code>data</code> for which a dependence plot is created</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_n.pt">n.pt</code></td>
<td>
<p>Evaluation grid size (used only if x is not discrete)</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_subsample">subsample</code></td>
<td>
<p>Fraction of random samples in <code>data</code> to be used for calculating the partial dependence plot</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_discrete.x">discrete.x</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the evaluation grid is set to the unique values of x</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_which.class">which.class</code></td>
<td>
<p>An <code>integer</code> indicating the class in multi-class classification (value from 0 to num_class - 1)</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_xlab">xlab</code></td>
<td>
<p>Parameter passed to <code>plot</code></p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_ylab">ylab</code></td>
<td>
<p>Parameter passed to <code>plot</code></p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_type">type</code></td>
<td>
<p>Parameter passed to <code>plot</code></p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_main">main</code></td>
<td>
<p>Parameter passed to <code>plot</code></p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_return_plot_data">return_plot_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the data for creating the partial dependence  plot is returned</p>
</td></tr>
<tr><td><code id="gpb.plot.partial.dependence_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>plot</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A two-dimensional <code>matrix</code> with data for creating the partial dependence plot.
This is only returned if <code>return_plot_data==TRUE</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist (adapted from a version by Michael Mayer)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(GPBoost_data, package = "gpboost")

gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
gpboost_model &lt;- gpboost(data = X,
                         label = y,
                         gp_model = gp_model,
                         nrounds = 16,
                         learning_rate = 0.05,
                         max_depth = 6,
                         min_data_in_leaf = 5,
                         verbose = 0)
gpb.plot.partial.dependence(gpboost_model, X, variable = 1)

</code></pre>

<hr>
<h2 id='gpb.save'>Save GPBoost model</h2><span id='topic+gpb.save'></span>

<h3>Description</h3>

<p>Save GPBoost model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.save(booster, filename, start_iteration = NULL, num_iteration = NULL,
  save_raw_data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.save_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>gpb.Booster</code></p>
</td></tr>
<tr><td><code id="gpb.save_+3A_filename">filename</code></td>
<td>
<p>saved filename</p>
</td></tr>
<tr><td><code id="gpb.save_+3A_start_iteration">start_iteration</code></td>
<td>
<p>int or NULL, optional (default=NULL)
Start index of the iteration to predict.
If NULL or &lt;= 0, starts from the first iteration.</p>
</td></tr>
<tr><td><code id="gpb.save_+3A_num_iteration">num_iteration</code></td>
<td>
<p>int or NULL, optional (default=NULL)
Limit number of iterations in the prediction.
If NULL, if the best iteration exists and start_iteration is NULL or &lt;= 0, the
best iteration is used; otherwise, all iterations from start_iteration are used.
If &lt;= 0, all iterations from start_iteration are used (no limits).</p>
</td></tr>
<tr><td><code id="gpb.save_+3A_save_raw_data">save_raw_data</code></td>
<td>
<p>If TRUE, the raw data (predictor / covariate data) for the Booster is also saved.
Enable this option if you want to change <code>start_iteration</code> or <code>num_iteration</code> at prediction time after loading.</p>
</td></tr>
<tr><td><code id="gpb.save_+3A_...">...</code></td>
<td>
<p>Additional named arguments passed to the <code>predict()</code> method of
the <code>gpb.Booster</code> object passed to <code>object</code>. 
This is only used when there is a gp_model and when save_raw_data=FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gpb.Booster
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(GPBoost_data, package = "gpboost")

# Train model and make prediction
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 16,
               learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var= TRUE, pred_latent = TRUE)
# Save model to file
filename &lt;- tempfile(fileext = ".json")
gpb.save(bst,filename = filename)
# Load from file and make predictions again
bst_loaded &lt;- gpb.load(filename = filename)
pred_loaded &lt;- predict(bst_loaded, data = X_test, group_data_pred = group_data_test[,1],
                       predict_var= TRUE, pred_latent = TRUE)
# Check equality
pred$fixed_effect - pred_loaded$fixed_effect
pred$random_effect_mean - pred_loaded$random_effect_mean
pred$random_effect_cov - pred_loaded$random_effect_cov

</code></pre>

<hr>
<h2 id='gpb.train'>Main training logic for GBPoost</h2><span id='topic+gpb.train'></span>

<h3>Description</h3>

<p>Logic to train with GBPoost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpb.train(params = list(), data, nrounds = 100L, gp_model = NULL,
  use_gp_model_for_validation = TRUE, train_gp_model_cov_pars = TRUE,
  valids = list(), obj = NULL, eval = NULL, verbose = 1L,
  record = TRUE, eval_freq = 1L, init_model = NULL, colnames = NULL,
  categorical_feature = NULL, early_stopping_rounds = NULL,
  callbacks = list(), reset_data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpb.train_+3A_params">params</code></td>
<td>
<p>list of &quot;tuning&quot; parameters. 
See <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information. 
A few key parameters:
</p>

<ul>
<li><p><code>learning_rate</code>: The learning rate, also called shrinkage or damping parameter 
(default = 0.1). An important tuning parameter for boosting. Lower values usually 
lead to higher predictive accuracy but more boosting iterations are needed 
</p>
</li>
<li><p><code>num_leaves</code>: Number of leaves in a tree. Tuning parameter for 
tree-boosting (default = 31)
</p>
</li>
<li><p><code>max_depth</code>: Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)
</p>
</li>
<li><p><code>min_data_in_leaf</code>: Minimal number of samples per leaf. Tuning parameter for 
tree-boosting (default = 20)
</p>
</li>
<li><p><code>lambda_l2</code>: L2 regularization (default = 0)
</p>
</li>
<li><p><code>lambda_l1</code>: L1 regularization (default = 0)
</p>
</li>
<li><p><code>max_bin</code>: Maximal number of bins that feature values will be bucketed in (default = 255)
</p>
</li>
<li><p><code>line_search_step_length</code> (default = FALSE): If TRUE, a line search is done to find the optimal 
step length for every boosting update (see, e.g., Friedman 2001). This is then multiplied by the learning rate 
</p>
</li>
<li><p><code>train_gp_model_cov_pars</code> (default = TRUE): If TRUE, the covariance parameters of the Gaussian process 
are estimated in every boosting iterations,  otherwise the gp_model parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide values via 
the 'init_cov_pars' parameter when creating the gp_model 
</p>
</li>
<li><p><code>use_gp_model_for_validation</code> (default = TRUE): If TRUE, the Gaussian process is also used 
(in addition to the tree model) for calculating predictions on the validation data 
</p>
</li>
<li><p><code>leaves_newton_update</code> (default = FALSE): Set this to TRUE to do a Newton update step for the tree leaves 
after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm) 
</p>
</li>
<li><p>num_threads: Number of threads. For the best speed, set this to
the number of real CPU cores(<code>parallel::detectCores(logical = FALSE)</code>),
not the number of threads (most CPU using hyper-threading to generate 2 threads
per CPU core).
</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb.train_+3A_data">data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+gpb.cv">gpb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_nrounds">nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_use_gp_model_for_validation">use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_train_gp_model_cov_pars">train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td></tr>
<tr><td><code id="gpb.train_+3A_valids">valids</code></td>
<td>
<p>a list of <code>gpb.Dataset</code> objects, used for validation</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_obj">obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_eval">eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: &quot;test_neg_log_likelihood&quot;, &quot;mse&quot;, &quot;rmse&quot;, &quot;mae&quot;, 
&quot;auc&quot;, &quot;average_precision&quot;, &quot;binary_logloss&quot;, &quot;binary_error&quot;
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the &quot;metric&quot; section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="gpb.train_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0, also will disable the print of evaluation during training</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="gpb.train_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effect when verbose &gt; 0</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_init_model">init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_colnames">colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_reset_data">reset_data</code></td>
<td>
<p>Boolean, setting it to TRUE (not the default value) will transform the
booster model into a predictor model which frees up memory and the
original datasets</p>
</td></tr>
<tr><td><code id="gpb.train_+3A_...">...</code></td>
<td>
<p>other parameters, see <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained booster model <code>gpb.Booster</code>.
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


library(gpboost)
data(GPBoost_data, package = "gpboost")

#--------------------Combine tree-boosting and grouped random effects model----------------
# Create random effects model
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
# The default optimizer for covariance parameters (hyperparameters) is 
# Nesterov-accelerated gradient descent.
# This can be changed to, e.g., Nelder-Mead as follows:
# re_params &lt;- list(optimizer_cov = "nelder_mead")
# gp_model$set_optim_params(params=re_params)
# Use trace = TRUE to monitor convergence:
# re_params &lt;- list(trace = TRUE)
# gp_model$set_optim_params(params=re_params)
dtrain &lt;- gpb.Dataset(data = X, label = y)
# Train model
bst &lt;- gpb.train(data = dtrain, gp_model = gp_model, nrounds = 16,
                 learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
                 verbose = 0)
# Estimated random effects model
summary(gp_model)
# Make predictions
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var= TRUE)
pred$random_effect_mean # Predicted mean
pred$random_effect_cov # Predicted variances
pred$fixed_effect # Predicted fixed effect from tree ensemble
# Sum them up to otbain a single prediction
pred$random_effect_mean + pred$fixed_effect

#--------------------Combine tree-boosting and Gaussian process model----------------
# Create Gaussian process model
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood = "gaussian")
# Train model
dtrain &lt;- gpb.Dataset(data = X, label = y)
bst &lt;- gpb.train(data = dtrain, gp_model = gp_model, nrounds = 16,
                 learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
                 verbose = 0)
# Estimated random effects model
summary(gp_model)
# Make predictions
pred &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                predict_cov_mat =TRUE)
pred$random_effect_mean # Predicted (posterior) mean of GP
pred$random_effect_cov # Predicted (posterior) covariance matrix of GP
pred$fixed_effect # Predicted fixed effect from tree ensemble
# Sum them up to otbain a single prediction
pred$random_effect_mean + pred$fixed_effect


#--------------------Using validation data-------------------------
set.seed(1)
train_ind &lt;- sample.int(length(y),size=250)
dtrain &lt;- gpb.Dataset(data = X[train_ind,], label = y[train_ind])
dtest &lt;- gpb.Dataset.create.valid(dtrain, data = X[-train_ind,], label = y[-train_ind])
valids &lt;- list(test = dtest)
gp_model &lt;- GPModel(group_data = group_data[train_ind,1], likelihood="gaussian")
# Need to set prediction data for gp_model
gp_model$set_prediction_data(group_data_pred = group_data[-train_ind,1])
# Training with validation data and use_gp_model_for_validation = TRUE
bst &lt;- gpb.train(data = dtrain, gp_model = gp_model, nrounds = 100,
                 learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
                 verbose = 1, valids = valids,
                 early_stopping_rounds = 10, use_gp_model_for_validation = TRUE)
print(paste0("Optimal number of iterations: ", bst$best_iter,
             ", best test error: ", bst$best_score))
# Plot validation error
val_error &lt;- unlist(bst$record_evals$test$l2$eval)
plot(1:length(val_error), val_error, type="l", lwd=2, col="blue",
     xlab="iteration", ylab="Validation error", main="Validation error vs. boosting iteration")


#--------------------Do Newton updates for tree leaves---------------
# Note: run the above examples first
bst &lt;- gpb.train(data = dtrain, gp_model = gp_model, nrounds = 100,
                 learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
                 verbose = 1, valids = valids,
                 early_stopping_rounds = 5, use_gp_model_for_validation = FALSE,
                 leaves_newton_update = TRUE)
print(paste0("Optimal number of iterations: ", bst$best_iter,
             ", best test error: ", bst$best_score))
# Plot validation error
val_error &lt;- unlist(bst$record_evals$test$l2$eval)
plot(1:length(val_error), val_error, type="l", lwd=2, col="blue",
     xlab="iteration", ylab="Validation error", main="Validation error vs. boosting iteration")


#--------------------GPBoostOOS algorithm: GP parameters estimated out-of-sample----------------
# Create random effects model and dataset
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood="gaussian")
dtrain &lt;- gpb.Dataset(X, label = y)
params &lt;- list(learning_rate = 0.05,
               max_depth = 6,
               min_data_in_leaf = 5)
# Stage 1: run cross-validation to (i) determine to optimal number of iterations
#           and (ii) to estimate the GPModel on the out-of-sample data
cvbst &lt;- gpb.cv(params = params,
                data = dtrain,
                gp_model = gp_model,
                nrounds = 100,
                nfold = 4,
                eval = "l2",
                early_stopping_rounds = 5,
                use_gp_model_for_validation = TRUE,
                fit_GP_cov_pars_OOS = TRUE)
print(paste0("Optimal number of iterations: ", cvbst$best_iter))
# Estimated random effects model
# Note: ideally, one would have to find the optimal combination of
#               other tuning parameters such as the learning rate, tree depth, etc.)
summary(gp_model)
# Stage 2: Train tree-boosting model while holding the GPModel fix
bst &lt;- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = cvbst$best_iter,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 verbose = 0,
                 train_gp_model_cov_pars = FALSE)
# The GPModel has not changed:
summary(gp_model)

</code></pre>

<hr>
<h2 id='gpboost'>Train a GPBoost model</h2><span id='topic+gpboost'></span>

<h3>Description</h3>

<p>Simple interface for training a GPBoost model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpboost(data, label = NULL, weight = NULL, params = list(),
  nrounds = 100L, gp_model = NULL, line_search_step_length = FALSE,
  use_gp_model_for_validation = TRUE, train_gp_model_cov_pars = TRUE,
  valids = list(), obj = NULL, eval = NULL, verbose = 1L,
  record = TRUE, eval_freq = 1L, early_stopping_rounds = NULL,
  init_model = NULL, colnames = NULL, categorical_feature = NULL,
  callbacks = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpboost_+3A_data">data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+gpb.cv">gpb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="gpboost_+3A_label">label</code></td>
<td>
<p>Vector of response values / labels, used if <code>data</code> is not an <code><a href="#topic+gpb.Dataset">gpb.Dataset</a></code></p>
</td></tr>
<tr><td><code id="gpboost_+3A_weight">weight</code></td>
<td>
<p>Vector of weights. The GPBoost algorithm currently does not support weights</p>
</td></tr>
<tr><td><code id="gpboost_+3A_params">params</code></td>
<td>
<p>list of &quot;tuning&quot; parameters. 
See <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information. 
A few key parameters:
</p>

<ul>
<li><p><code>learning_rate</code>: The learning rate, also called shrinkage or damping parameter 
(default = 0.1). An important tuning parameter for boosting. Lower values usually 
lead to higher predictive accuracy but more boosting iterations are needed 
</p>
</li>
<li><p><code>num_leaves</code>: Number of leaves in a tree. Tuning parameter for 
tree-boosting (default = 31)
</p>
</li>
<li><p><code>max_depth</code>: Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)
</p>
</li>
<li><p><code>min_data_in_leaf</code>: Minimal number of samples per leaf. Tuning parameter for 
tree-boosting (default = 20)
</p>
</li>
<li><p><code>lambda_l2</code>: L2 regularization (default = 0)
</p>
</li>
<li><p><code>lambda_l1</code>: L1 regularization (default = 0)
</p>
</li>
<li><p><code>max_bin</code>: Maximal number of bins that feature values will be bucketed in (default = 255)
</p>
</li>
<li><p><code>line_search_step_length</code> (default = FALSE): If TRUE, a line search is done to find the optimal 
step length for every boosting update (see, e.g., Friedman 2001). This is then multiplied by the learning rate 
</p>
</li>
<li><p><code>train_gp_model_cov_pars</code> (default = TRUE): If TRUE, the covariance parameters of the Gaussian process 
are estimated in every boosting iterations,  otherwise the gp_model parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide values via 
the 'init_cov_pars' parameter when creating the gp_model 
</p>
</li>
<li><p><code>use_gp_model_for_validation</code> (default = TRUE): If TRUE, the Gaussian process is also used 
(in addition to the tree model) for calculating predictions on the validation data 
</p>
</li>
<li><p><code>leaves_newton_update</code> (default = FALSE): Set this to TRUE to do a Newton update step for the tree leaves 
after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm) 
</p>
</li>
<li><p>num_threads: Number of threads. For the best speed, set this to
the number of real CPU cores(<code>parallel::detectCores(logical = FALSE)</code>),
not the number of threads (most CPU using hyper-threading to generate 2 threads
per CPU core).
</p>
</li></ul>
</td></tr>
<tr><td><code id="gpboost_+3A_nrounds">nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td></tr>
<tr><td><code id="gpboost_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td></tr>
<tr><td><code id="gpboost_+3A_line_search_step_length">line_search_step_length</code></td>
<td>
<p>Boolean. If TRUE, a line search is done to find the optimal step length for every boosting update 
(see, e.g., Friedman 2001). This is then multiplied by the <code>learning_rate</code>. 
Applies only to the GPBoost algorithm</p>
</td></tr>
<tr><td><code id="gpboost_+3A_use_gp_model_for_validation">use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td></tr>
<tr><td><code id="gpboost_+3A_train_gp_model_cov_pars">train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td></tr>
<tr><td><code id="gpboost_+3A_valids">valids</code></td>
<td>
<p>a list of <code>gpb.Dataset</code> objects, used for validation</p>
</td></tr>
<tr><td><code id="gpboost_+3A_obj">obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td></tr>
<tr><td><code id="gpboost_+3A_eval">eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: &quot;test_neg_log_likelihood&quot;, &quot;mse&quot;, &quot;rmse&quot;, &quot;mae&quot;, 
&quot;auc&quot;, &quot;average_precision&quot;, &quot;binary_logloss&quot;, &quot;binary_error&quot;
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the &quot;metric&quot; section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="gpboost_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0, also will disable the print of evaluation during training</p>
</td></tr>
<tr><td><code id="gpboost_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="gpboost_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effect when verbose &gt; 0</p>
</td></tr>
<tr><td><code id="gpboost_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="gpboost_+3A_init_model">init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="gpboost_+3A_colnames">colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td></tr>
<tr><td><code id="gpboost_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="gpboost_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="gpboost_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+gpb.train">gpb.train</a></code>. For example
</p>

<ul>
<li><p><code>valids</code>: a list of <code>gpb.Dataset</code> objects, used for validation
</p>
</li>
<li><p><code>eval</code>: evaluation function, can be (a list of) character or custom eval function
</p>
</li>
<li><p><code>record</code>: Boolean, TRUE will record iteration message to <code>booster$record_evals</code>
</p>
</li>
<li><p><code>colnames</code>: feature names, if not null, will use this to overwrite the names in dataset
</p>
</li>
<li><p><code>categorical_feature</code>: categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g. <code>c(1L, 10L)</code> to
say &quot;the first and tenth columns&quot;).
</p>
</li>
<li><p><code>reset_data</code>: Boolean, setting it to TRUE (not the default value) will transform the booster model
into a predictor model which frees up memory and the original datasets
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained <code>gpb.Booster</code>
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


library(gpboost)
data(GPBoost_data, package = "gpboost")

#--------------------Combine tree-boosting and grouped random effects model----------------
# Create random effects model
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
# The default optimizer for covariance parameters (hyperparameters) is 
# Nesterov-accelerated gradient descent.
# This can be changed to, e.g., Nelder-Mead as follows:
# re_params &lt;- list(optimizer_cov = "nelder_mead")
# gp_model$set_optim_params(params=re_params)
# Use trace = TRUE to monitor convergence:
# re_params &lt;- list(trace = TRUE)
# gp_model$set_optim_params(params=re_params)

# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 16,
               learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)

# Make predictions
# Predict latent variables
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean
# For Gaussian data: pred$random_effect_mean + pred$fixed_effect = pred_resp$response_mean
pred$random_effect_mean + pred$fixed_effect - pred_resp$response_mean

#--------------------Combine tree-boosting and Gaussian process model----------------
# Create Gaussian process model
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood = "gaussian")
# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 8,
               learning_rate = 0.1, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)
# Make predictions
pred &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean

</code></pre>

<hr>
<h2 id='GPBoost_data'>Example data for the GPBoost package</h2><span id='topic+GPBoost_data'></span>

<h3>Description</h3>

<p>Simulated example data for the GPBoost package
This data set includes the following fields:
</p>

<ul>
<li><p><code>y</code>: response variable
</p>
</li>
<li><p><code>X</code>: a matrix with covariate information
</p>
</li>
<li><p><code>group_data</code>: a matrix with categorical grouping variables
</p>
</li>
<li><p><code>coords</code>: a matrix with spatial coordinates
</p>
</li>
<li><p><code>X_test</code>: a matrix with covariate information for predictions
</p>
</li>
<li><p><code>group_data_test</code>: a matrix with categorical grouping variables for predictions
</p>
</li>
<li><p><code>coords_test</code>: a matrix with spatial coordinates for predictions
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='GPModel'>Create a <code>GPModel</code> object</h2><span id='topic+GPModel'></span>

<h3>Description</h3>

<p>Create a <code>GPModel</code> which contains a Gaussian process and / or mixed effects model with grouped random effects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GPModel(likelihood = "gaussian", group_data = NULL,
  group_rand_coef_data = NULL, ind_effect_group_rand_coef = NULL,
  drop_intercept_group_rand_effect = NULL, gp_coords = NULL,
  gp_rand_coef_data = NULL, cov_function = "exponential",
  cov_fct_shape = 0.5, gp_approx = "none", cov_fct_taper_range = 1,
  cov_fct_taper_shape = 0, num_neighbors = 20L,
  vecchia_ordering = "random", ind_points_selection = "kmeans++",
  num_ind_points = 500L, cover_tree_radius = 1,
  matrix_inversion_method = "cholesky", seed = 0L, cluster_ids = NULL,
  free_raw_data = FALSE, vecchia_approx = NULL, vecchia_pred_type = NULL,
  num_neighbors_pred = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPModel_+3A_likelihood">likelihood</code></td>
<td>
<p>A <code>string</code> specifying the likelihood function (distribution) of the response variable. 
Available options:
</p>

<ul>
<li><p> &quot;gaussian&quot; 
</p>
</li>
<li><p> &quot;bernoulli_probit&quot;: binary data with Bernoulli likelihood and a probit link function 
</p>
</li>
<li><p> &quot;bernoulli_logit&quot;: binary data with Bernoulli likelihood and a logit link function 
</p>
</li>
<li><p> &quot;gamma&quot;: gamma distribution with a with log link function 
</p>
</li>
<li><p> &quot;poisson&quot;: Poisson distribution with a with log link function 
</p>
</li>
<li><p> &quot;negative_binomial&quot;: negative binomial distribution with a with log link function 
</p>
</li>
<li><p> Note: other likelihoods could be implemented upon request 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_group_data">group_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> whose columns are categorical grouping variables. 
The elements being group levels defining grouped random effects.
The elements of 'group_data' can be integer, double, or character.
The number of columns corresponds to the number of grouped (intercept) random effects</p>
</td></tr>
<tr><td><code id="GPModel_+3A_group_rand_coef_data">group_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data 
for grouped random coefficients</p>
</td></tr>
<tr><td><code id="GPModel_+3A_ind_effect_group_rand_coef">ind_effect_group_rand_coef</code></td>
<td>
<p>A <code>vector</code> with integer indices that 
indicate the corresponding categorical grouping variable (=columns) in 'group_data' for 
every covariate in 'group_rand_coef_data'. Counting starts at 1.
The length of this index vector must equal the number of covariates in 'group_rand_coef_data'.
For instance, c(1,1,2) means that the first two covariates (=first two columns) in 'group_rand_coef_data'
have random coefficients corresponding to the first categorical grouping variable (=first column) in 'group_data',
and the third covariate (=third column) in 'group_rand_coef_data' has a random coefficient
corresponding to the second grouping variable (=second column) in 'group_data'</p>
</td></tr>
<tr><td><code id="GPModel_+3A_drop_intercept_group_rand_effect">drop_intercept_group_rand_effect</code></td>
<td>
<p>A <code>vector</code> of type <code>logical</code> (boolean). 
Indicates whether intercept random effects are dropped (only for random coefficients). 
If drop_intercept_group_rand_effect[k] is TRUE, the intercept random effect number k is dropped / not included. 
Only random effects with random slopes can be dropped.</p>
</td></tr>
<tr><td><code id="GPModel_+3A_gp_coords">gp_coords</code></td>
<td>
<p>A <code>matrix</code> with numeric coordinates (= inputs / features) for defining Gaussian processes</p>
</td></tr>
<tr><td><code id="GPModel_+3A_gp_rand_coef_data">gp_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data for
Gaussian process random coefficients</p>
</td></tr>
<tr><td><code id="GPModel_+3A_cov_function">cov_function</code></td>
<td>
<p>A <code>string</code> specifying the covariance function for the Gaussian process. 
Available options:
</p>

<ul>
<li><p>&quot;exponential&quot;: Exponential covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p>&quot;gaussian&quot;: Gaussian, aka squared exponential, covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;matern&quot;: Matern covariance function with the smoothness specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Rasmussen and Williams, 2006) 
</p>
</li>
<li><p>&quot;powered_exponential&quot;: powered exponential covariance function with the exponent specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;wendland&quot;: Compactly supported Wendland covariance function (using the parametrization of Bevilacqua et al., 2019, AOS) 
</p>
</li>
<li><p> &quot;matern_space_time&quot;: Spatio-temporal Matern covariance function with different range parameters for space and time. 
Note that the first column in <code>gp_coords</code> must correspond to the time dimension 
</p>
</li>
<li><p> &quot;matern_ard&quot;: anisotropic Matern covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li>
<li><p> &quot;gaussian_ard&quot;: anisotropic Gaussian, aka squared exponential, covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_cov_fct_shape">cov_fct_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape parameter of the covariance function 
(=smoothness parameter for Matern covariance)  
This parameter is irrelevant for some covariance functions such as the exponential or Gaussian</p>
</td></tr>
<tr><td><code id="GPModel_+3A_gp_approx">gp_approx</code></td>
<td>
<p>A <code>string</code> specifying the large data approximation
for Gaussian processes. Available options: 
</p>

<ul>
<li><p>&quot;none&quot;: No approximation 
</p>
</li>
<li><p>&quot;vecchia&quot;: A Vecchia approximation; see Sigrist (2022, JMLR) for more details 
</p>
</li>
<li><p>&quot;tapering&quot;: The covariance function is multiplied by 
a compactly supported Wendland correlation function 
</p>
</li>
<li><p>&quot;fitc&quot;: Fully Independent Training Conditional approximation aka 
modified predictive process approximation; see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li>
<li><p>&quot;full_scale_tapering&quot;: A full scale approximation combining an 
inducing point / predictive process approximation with tapering on the residual process; 
see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_cov_fct_taper_range">cov_fct_taper_range</code></td>
<td>
<p>A <code>numeric</code> specifying the range parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="GPModel_+3A_cov_fct_taper_shape">cov_fct_taper_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape (=smoothness) parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="GPModel_+3A_num_neighbors">num_neighbors</code></td>
<td>
<p>An <code>integer</code> specifying the number of neighbors for 
the Vecchia approximation. Note: for prediction, the number of neighbors can 
be set through the 'num_neighbors_pred' parameter in the 'set_prediction_data'
function. By default, num_neighbors_pred = 2 * num_neighbors. Further, 
the type of Vecchia approximation used for making predictions is set through  
the 'vecchia_pred_type' parameter in the 'set_prediction_data' function</p>
</td></tr>
<tr><td><code id="GPModel_+3A_vecchia_ordering">vecchia_ordering</code></td>
<td>
<p>A <code>string</code> specifying the ordering used in 
the Vecchia approximation. Available options:
</p>

<ul>
<li><p>&quot;none&quot;: the default ordering in the data is used 
</p>
</li>
<li><p>&quot;random&quot;: a random ordering 
</p>
</li>
<li><p>&quot;time&quot;: ordering accorrding to time (only for space-time models) 
</p>
</li>
<li><p>&quot;time_random_space&quot;: ordering according to time and randomly for all 
spatial points with the same time points (only for space-time models) 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_ind_points_selection">ind_points_selection</code></td>
<td>
<p>A <code>string</code> specifying the method for choosing inducing points
Available options:
</p>

<ul>
<li><p>&quot;kmeans++: the k-means++ algorithm 
</p>
</li>
<li><p>&quot;cover_tree&quot;: the cover tree algorithm 
</p>
</li>
<li><p>&quot;random&quot;: random selection from data points 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_num_ind_points">num_ind_points</code></td>
<td>
<p>An <code>integer</code> specifying the number of inducing 
points / knots for, e.g., a predictive process approximation</p>
</td></tr>
<tr><td><code id="GPModel_+3A_cover_tree_radius">cover_tree_radius</code></td>
<td>
<p>A <code>numeric</code> specifying the radius (= &quot;spatial resolution&quot;) 
for the cover tree algorithm</p>
</td></tr>
<tr><td><code id="GPModel_+3A_matrix_inversion_method">matrix_inversion_method</code></td>
<td>
<p>A <code>string</code> specifying the method used for inverting covariance matrices. 
Available options:
</p>

<ul>
<li><p>&quot;cholesky&quot;: Cholesky factorization 
</p>
</li>
<li><p>&quot;iterative&quot;: iterative methods. A combination of conjugate gradient, Lanczos algorithm, and other methods. 
</p>
<p>This is currently only supported for the following cases: 
</p>

<ul>
<li><p>likelihood != &quot;gaussian&quot; and gp_approx == &quot;vecchia&quot; (non-Gaussian likelihoods with a Vecchia-Laplace approximation) 
</p>
</li>
<li><p>likelihood == &quot;gaussian&quot; and gp_approx == &quot;full_scale_tapering&quot; (Gaussian likelihood with a full-scale tapering approximation) 
</p>
</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="GPModel_+3A_seed">seed</code></td>
<td>
<p>An <code>integer</code> specifying the seed used for model creation 
(e.g., random ordering in Vecchia approximation)</p>
</td></tr>
<tr><td><code id="GPModel_+3A_cluster_ids">cluster_ids</code></td>
<td>
<p>A <code>vector</code> with elements indicating independent realizations of 
random effects / Gaussian processes (same values = same process realization).
The elements of 'cluster_ids' can be integer, double, or character.</p>
</td></tr>
<tr><td><code id="GPModel_+3A_free_raw_data">free_raw_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the data (groups, coordinates, covariate data for random coefficients) 
is freed in R after initialization</p>
</td></tr>
<tr><td><code id="GPModel_+3A_vecchia_approx">vecchia_approx</code></td>
<td>
<p>Discontinued. Use the argument <code>gp_approx</code> instead</p>
</td></tr>
<tr><td><code id="GPModel_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="GPModel_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code> containing ontains a Gaussian process and / or mixed effects model with grouped random effects
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples

data(GPBoost_data, package = "gpboost")

#--------------------Grouped random effects model: single-level random effect----------------
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood="gaussian")

#--------------------Gaussian process model----------------
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood="gaussian")

#--------------------Combine Gaussian process with grouped random effects----------------
gp_model &lt;- GPModel(group_data = group_data,
                    gp_coords = coords, cov_function = "exponential",
                    likelihood="gaussian")
</code></pre>

<hr>
<h2 id='GPModel_shared_params'>Documentation for parameters shared by <code>GPModel</code>, <code>gpb.cv</code>, and <code>gpboost</code></h2><span id='topic+GPModel_shared_params'></span>

<h3>Description</h3>

<p>Documentation for parameters shared by <code>GPModel</code>, <code>gpb.cv</code>, and <code>gpboost</code>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPModel_shared_params_+3A_likelihood">likelihood</code></td>
<td>
<p>A <code>string</code> specifying the likelihood function (distribution) of the response variable. 
Available options:
</p>

<ul>
<li><p> &quot;gaussian&quot; 
</p>
</li>
<li><p> &quot;bernoulli_probit&quot;: binary data with Bernoulli likelihood and a probit link function 
</p>
</li>
<li><p> &quot;bernoulli_logit&quot;: binary data with Bernoulli likelihood and a logit link function 
</p>
</li>
<li><p> &quot;gamma&quot;: gamma distribution with a with log link function 
</p>
</li>
<li><p> &quot;poisson&quot;: Poisson distribution with a with log link function 
</p>
</li>
<li><p> &quot;negative_binomial&quot;: negative binomial distribution with a with log link function 
</p>
</li>
<li><p> Note: other likelihoods could be implemented upon request 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_group_data">group_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> whose columns are categorical grouping variables. 
The elements being group levels defining grouped random effects.
The elements of 'group_data' can be integer, double, or character.
The number of columns corresponds to the number of grouped (intercept) random effects</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_group_rand_coef_data">group_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data 
for grouped random coefficients</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_ind_effect_group_rand_coef">ind_effect_group_rand_coef</code></td>
<td>
<p>A <code>vector</code> with integer indices that 
indicate the corresponding categorical grouping variable (=columns) in 'group_data' for 
every covariate in 'group_rand_coef_data'. Counting starts at 1.
The length of this index vector must equal the number of covariates in 'group_rand_coef_data'.
For instance, c(1,1,2) means that the first two covariates (=first two columns) in 'group_rand_coef_data'
have random coefficients corresponding to the first categorical grouping variable (=first column) in 'group_data',
and the third covariate (=third column) in 'group_rand_coef_data' has a random coefficient
corresponding to the second grouping variable (=second column) in 'group_data'</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_drop_intercept_group_rand_effect">drop_intercept_group_rand_effect</code></td>
<td>
<p>A <code>vector</code> of type <code>logical</code> (boolean). 
Indicates whether intercept random effects are dropped (only for random coefficients). 
If drop_intercept_group_rand_effect[k] is TRUE, the intercept random effect number k is dropped / not included. 
Only random effects with random slopes can be dropped.</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_gp_coords">gp_coords</code></td>
<td>
<p>A <code>matrix</code> with numeric coordinates (= inputs / features) for defining Gaussian processes</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_gp_rand_coef_data">gp_rand_coef_data</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with numeric covariate data for
Gaussian process random coefficients</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cov_function">cov_function</code></td>
<td>
<p>A <code>string</code> specifying the covariance function for the Gaussian process. 
Available options:
</p>

<ul>
<li><p>&quot;exponential&quot;: Exponential covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p>&quot;gaussian&quot;: Gaussian, aka squared exponential, covariance function (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;matern&quot;: Matern covariance function with the smoothness specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Rasmussen and Williams, 2006) 
</p>
</li>
<li><p>&quot;powered_exponential&quot;: powered exponential covariance function with the exponent specified by 
the <code>cov_fct_shape</code> parameter (using the parametrization of Diggle and Ribeiro, 2007) 
</p>
</li>
<li><p> &quot;wendland&quot;: Compactly supported Wendland covariance function (using the parametrization of Bevilacqua et al., 2019, AOS) 
</p>
</li>
<li><p> &quot;matern_space_time&quot;: Spatio-temporal Matern covariance function with different range parameters for space and time. 
Note that the first column in <code>gp_coords</code> must correspond to the time dimension 
</p>
</li>
<li><p> &quot;matern_ard&quot;: anisotropic Matern covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li>
<li><p> &quot;gaussian_ard&quot;: anisotropic Gaussian, aka squared exponential, covariance function with Automatic Relevance Determination (ARD), 
i.e., with a different range parameter for every coordinate dimension / column of <code>gp_coords</code> 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cov_fct_shape">cov_fct_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape parameter of the covariance function 
(=smoothness parameter for Matern covariance)  
This parameter is irrelevant for some covariance functions such as the exponential or Gaussian</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_gp_approx">gp_approx</code></td>
<td>
<p>A <code>string</code> specifying the large data approximation
for Gaussian processes. Available options: 
</p>

<ul>
<li><p>&quot;none&quot;: No approximation 
</p>
</li>
<li><p>&quot;vecchia&quot;: A Vecchia approximation; see Sigrist (2022, JMLR) for more details 
</p>
</li>
<li><p>&quot;tapering&quot;: The covariance function is multiplied by 
a compactly supported Wendland correlation function 
</p>
</li>
<li><p>&quot;fitc&quot;: Fully Independent Training Conditional approximation aka 
modified predictive process approximation; see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li>
<li><p>&quot;full_scale_tapering&quot;: A full scale approximation combining an 
inducing point / predictive process approximation with tapering on the residual process; 
see Gyger, Furrer, and Sigrist (2024) for more details 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cov_fct_taper_range">cov_fct_taper_range</code></td>
<td>
<p>A <code>numeric</code> specifying the range parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cov_fct_taper_shape">cov_fct_taper_shape</code></td>
<td>
<p>A <code>numeric</code> specifying the shape (=smoothness) parameter 
of the Wendland covariance function and Wendland correlation taper function. 
We follow the notation of Bevilacqua et al. (2019, AOS)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_num_neighbors">num_neighbors</code></td>
<td>
<p>An <code>integer</code> specifying the number of neighbors for 
the Vecchia approximation. Note: for prediction, the number of neighbors can 
be set through the 'num_neighbors_pred' parameter in the 'set_prediction_data'
function. By default, num_neighbors_pred = 2 * num_neighbors. Further, 
the type of Vecchia approximation used for making predictions is set through  
the 'vecchia_pred_type' parameter in the 'set_prediction_data' function</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_vecchia_ordering">vecchia_ordering</code></td>
<td>
<p>A <code>string</code> specifying the ordering used in 
the Vecchia approximation. Available options:
</p>

<ul>
<li><p>&quot;none&quot;: the default ordering in the data is used 
</p>
</li>
<li><p>&quot;random&quot;: a random ordering 
</p>
</li>
<li><p>&quot;time&quot;: ordering accorrding to time (only for space-time models) 
</p>
</li>
<li><p>&quot;time_random_space&quot;: ordering according to time and randomly for all 
spatial points with the same time points (only for space-time models) 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_ind_points_selection">ind_points_selection</code></td>
<td>
<p>A <code>string</code> specifying the method for choosing inducing points
Available options:
</p>

<ul>
<li><p>&quot;kmeans++: the k-means++ algorithm 
</p>
</li>
<li><p>&quot;cover_tree&quot;: the cover tree algorithm 
</p>
</li>
<li><p>&quot;random&quot;: random selection from data points 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_num_ind_points">num_ind_points</code></td>
<td>
<p>An <code>integer</code> specifying the number of inducing 
points / knots for, e.g., a predictive process approximation</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cover_tree_radius">cover_tree_radius</code></td>
<td>
<p>A <code>numeric</code> specifying the radius (= &quot;spatial resolution&quot;) 
for the cover tree algorithm</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_matrix_inversion_method">matrix_inversion_method</code></td>
<td>
<p>A <code>string</code> specifying the method used for inverting covariance matrices. 
Available options:
</p>

<ul>
<li><p>&quot;cholesky&quot;: Cholesky factorization 
</p>
</li>
<li><p>&quot;iterative&quot;: iterative methods. A combination of conjugate gradient, Lanczos algorithm, and other methods. 
</p>
<p>This is currently only supported for the following cases: 
</p>

<ul>
<li><p>likelihood != &quot;gaussian&quot; and gp_approx == &quot;vecchia&quot; (non-Gaussian likelihoods with a Vecchia-Laplace approximation) 
</p>
</li>
<li><p>likelihood == &quot;gaussian&quot; and gp_approx == &quot;full_scale_tapering&quot; (Gaussian likelihood with a full-scale tapering approximation) 
</p>
</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_seed">seed</code></td>
<td>
<p>An <code>integer</code> specifying the seed used for model creation 
(e.g., random ordering in Vecchia approximation)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
Default value if vecchia_pred_type = NULL: &quot;order_obs_first_cond_obs_only&quot;. 
Available options:
</p>

<ul>
<li><p>&quot;order_obs_first_cond_obs_only&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are only observed training data points 
</p>
</li>
<li><p>&quot;order_obs_first_cond_all&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are selected among all points (training + prediction) 
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_obs_only&quot;: Vecchia approximation for the latent process and observed data is 
ordered first and neighbors are only observed points
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_all&quot;: Vecchia approximation 
for the latent process and observed data is ordered first and neighbors are selected among all points 
</p>
</li>
<li><p>&quot;order_pred_first&quot;: Vecchia approximation for the observable process and prediction data is 
ordered first for making predictions. This option is only available for Gaussian likelihoods 
</p>
</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for the Vecchia approximation 
for making predictions. Default value if NULL: num_neighbors_pred = 2 * num_neighbors</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cg_delta_conv_pred">cg_delta_conv_pred</code></td>
<td>
<p>a <code>numeric</code> specifying the tolerance level for L2 norm of residuals for 
checking convergence in conjugate gradient algorithms when being used for prediction
Default value if NULL: 1e-3</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_nsim_var_pred">nsim_var_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of samples when simulation 
is used for calculating predictive variances
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_rank_pred_approx_matrix_lanczos">rank_pred_approx_matrix_lanczos</code></td>
<td>
<p>an <code>integer</code> specifying the rank 
of the matrix for approximating predictive covariances obtained using the Lanczos algorithm
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cluster_ids">cluster_ids</code></td>
<td>
<p>A <code>vector</code> with elements indicating independent realizations of 
random effects / Gaussian processes (same values = same process realization).
The elements of 'cluster_ids' can be integer, double, or character.</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_free_raw_data">free_raw_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the data (groups, coordinates, covariate data for random coefficients) 
is freed in R after initialization</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_x">X</code></td>
<td>
<p>A <code>matrix</code> with numeric covariate data for the 
fixed effects linear regression term (if there is one)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_offset">offset</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor (= offset). 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset</code> instead</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_group_data_pred">group_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with elements being group levels 
for which predictions are made (if there are grouped random effects in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_group_rand_coef_data_pred">group_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data 
for grouped random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_gp_coords_pred">gp_coords_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction coordinates (=features) for 
Gaussian process (if there is a GP in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_gp_rand_coef_data_pred">gp_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data for 
Gaussian process random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_cluster_ids_pred">cluster_ids_pred</code></td>
<td>
<p>A <code>vector</code> with elements indicating the realizations of 
random effects / Gaussian processes for which predictions are made 
(set to NULL if you have not specified this when creating the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_x_pred">X_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction covariate data for the 
fixed effects linear regression term (if there is one in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_predict_cov_mat">predict_cov_mat</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive covariance is calculated in addition to the (posterior) predictive mean</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_predict_var">predict_var</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive variances are calculated</p>
</td></tr>
<tr><td><code id="GPModel_shared_params_+3A_vecchia_approx">vecchia_approx</code></td>
<td>
<p>Discontinued. Use the argument <code>gp_approx</code> instead</p>
</td></tr>
</table>

<hr>
<h2 id='group_data'>Example data for the GPBoost package</h2><span id='topic+group_data'></span>

<h3>Description</h3>

<p>A matrix with categorical grouping variables for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='group_data_test'>Example data for the GPBoost package</h2><span id='topic+group_data_test'></span>

<h3>Description</h3>

<p>A matrix with categorical grouping variables for predictions for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='loadGPModel'>Load a <code>GPModel</code> from a file</h2><span id='topic+loadGPModel'></span>

<h3>Description</h3>

<p>Load a <code>GPModel</code> from a file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadGPModel(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadGPModel_+3A_filename">filename</code></td>
<td>
<p>filename for loading</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1, likelihood="gaussian")
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_var = TRUE)
# Save model to file
filename &lt;- tempfile(fileext = ".json")
saveGPModel(gp_model,filename = filename)
# Load from file and make predictions again
gp_model_loaded &lt;- loadGPModel(filename = filename)
pred_loaded &lt;- predict(gp_model_loaded, group_data_pred = group_data_test[,1], 
                       X_pred = X_test1, predict_var = TRUE)
# Check equality
pred$mu - pred_loaded$mu
pred$var - pred_loaded$var

</code></pre>

<hr>
<h2 id='neg_log_likelihood'>Evaluate the negative log-likelihood</h2><span id='topic+neg_log_likelihood'></span>

<h3>Description</h3>

<p>Evaluate the negative log-likelihood. If there is a linear fixed effects
predictor term, this needs to be calculated &quot;manually&quot; prior to calling this 
function (see example below)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_likelihood(gp_model, cov_pars, y, fixed_effects = NULL,
  aux_pars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_likelihood_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="neg_log_likelihood_+3A_cov_pars">cov_pars</code></td>
<td>
<p>A <code>vector</code> with <code>numeric</code> elements. 
Covariance parameters of Gaussian process and  random effects</p>
</td></tr>
<tr><td><code id="neg_log_likelihood_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="neg_log_likelihood_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with fixed effects, e.g., containing a linear predictor. 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="neg_log_likelihood_+3A_aux_pars">aux_pars</code></td>
<td>
<p>A <code>vector</code> with <code>numeric</code> elements. 
Additional parameters for non-Gaussian likelihoods (e.g., shape parameter of a gamma or negative_binomial likelihood)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
gp_model &lt;- GPModel(group_data = group_data, likelihood="gaussian")
X1 &lt;- cbind(rep(1,dim(X)[1]), X)
coef &lt;- c(0.1, 0.1, 0.1)
fixed_effects &lt;- as.numeric(X1 %*% coef)
neg_log_likelihood(gp_model, y = y, cov_pars = c(0.1,1,1), 
                   fixed_effects = fixed_effects)

</code></pre>

<hr>
<h2 id='neg_log_likelihood.GPModel'>Evaluate the negative log-likelihood</h2><span id='topic+neg_log_likelihood.GPModel'></span>

<h3>Description</h3>

<p>Evaluate the negative log-likelihood. If there is a linear fixed effects
predictor term, this needs to be calculated &quot;manually&quot; prior to calling this 
function (see example below)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
neg_log_likelihood(gp_model, cov_pars, y,
  fixed_effects = NULL, aux_pars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_likelihood.GPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="neg_log_likelihood.GPModel_+3A_cov_pars">cov_pars</code></td>
<td>
<p>A <code>vector</code> with <code>numeric</code> elements. 
Covariance parameters of Gaussian process and  random effects</p>
</td></tr>
<tr><td><code id="neg_log_likelihood.GPModel_+3A_y">y</code></td>
<td>
<p>A <code>vector</code> with response variable data</p>
</td></tr>
<tr><td><code id="neg_log_likelihood.GPModel_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with fixed effects, e.g., containing a linear predictor. 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="neg_log_likelihood.GPModel_+3A_aux_pars">aux_pars</code></td>
<td>
<p>A <code>vector</code> with <code>numeric</code> elements. 
Additional parameters for non-Gaussian likelihoods (e.g., shape parameter of a gamma or negative_binomial likelihood)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
gp_model &lt;- GPModel(group_data = group_data, likelihood="gaussian")
X1 &lt;- cbind(rep(1,dim(X)[1]), X)
coef &lt;- c(0.1, 0.1, 0.1)
fixed_effects &lt;- as.numeric(X1 %*% coef)
neg_log_likelihood(gp_model, y = y, cov_pars = c(0.1,1,1), 
                   fixed_effects = fixed_effects)

</code></pre>

<hr>
<h2 id='predict_training_data_random_effects'>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code></h2><span id='topic+predict_training_data_random_effects'></span>

<h3>Description</h3>

<p>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_training_data_random_effects(gp_model, predict_var = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_training_data_random_effects_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="predict_training_data_random_effects_+3A_predict_var">predict_var</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive variances are calculated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1, likelihood="gaussian")
all_training_data_random_effects &lt;- predict_training_data_random_effects(gp_model)
first_occurences &lt;- match(unique(group_data[,1]), group_data[,1])
unique_training_data_random_effects &lt;- all_training_data_random_effects[first_occurences]
head(unique_training_data_random_effects)

</code></pre>

<hr>
<h2 id='predict_training_data_random_effects.GPModel'>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code></h2><span id='topic+predict_training_data_random_effects.GPModel'></span>

<h3>Description</h3>

<p>Predict (&quot;estimate&quot;) training data random effects for a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
predict_training_data_random_effects(gp_model,
  predict_var = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_training_data_random_effects.GPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="predict_training_data_random_effects.GPModel_+3A_predict_var">predict_var</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive variances are calculated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1, likelihood="gaussian")
all_training_data_random_effects &lt;- predict_training_data_random_effects(gp_model)
first_occurences &lt;- match(unique(group_data[,1]), group_data[,1])
unique_training_data_random_effects &lt;- all_training_data_random_effects[first_occurences]
head(unique_training_data_random_effects)

</code></pre>

<hr>
<h2 id='predict.gpb.Booster'>Prediction function for <code>gpb.Booster</code> objects</h2><span id='topic+predict.gpb.Booster'></span>

<h3>Description</h3>

<p>Prediction function for <code>gpb.Booster</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpb.Booster'
predict(object, data, start_iteration = NULL,
  num_iteration = NULL, pred_latent = FALSE, predleaf = FALSE,
  predcontrib = FALSE, header = FALSE, reshape = FALSE,
  group_data_pred = NULL, group_rand_coef_data_pred = NULL,
  gp_coords_pred = NULL, gp_rand_coef_data_pred = NULL,
  cluster_ids_pred = NULL, predict_cov_mat = FALSE, predict_var = FALSE,
  cov_pars = NULL, ignore_gp_model = FALSE, rawscore = NULL,
  vecchia_pred_type = NULL, num_neighbors_pred = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.gpb.Booster_+3A_object">object</code></td>
<td>
<p>Object of class <code>gpb.Booster</code></p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code> object or a character representing a filename</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_start_iteration">start_iteration</code></td>
<td>
<p>int or NULL, optional (default=NULL)
Start index of the iteration to predict.
If NULL or &lt;= 0, starts from the first iteration.</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_num_iteration">num_iteration</code></td>
<td>
<p>int or NULL, optional (default=NULL)
Limit number of iterations in the prediction.
If NULL, if the best iteration exists and start_iteration is NULL or &lt;= 0, the
best iteration is used; otherwise, all iterations from start_iteration are used.
If &lt;= 0, all iterations from start_iteration are used (no limits).</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_pred_latent">pred_latent</code></td>
<td>
<p>If TRUE latent variables, both fixed effects (tree-ensemble) 
and random effects (<code>gp_model</code>) are predicted. Otherwise, the response variable 
(label) is predicted. Depending on how the argument 'pred_latent' is set,
different values are returned from this function; see the 'Value' section for more details. 
If there is no <code>gp_model</code>, this argument corresponds to 'raw_score' in LightGBM.</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_predleaf">predleaf</code></td>
<td>
<p>whether predict leaf index instead.</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_predcontrib">predcontrib</code></td>
<td>
<p>return per-feature contributions for each record.</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_header">header</code></td>
<td>
<p>only used for prediction for text file. True if text file has header</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_reshape">reshape</code></td>
<td>
<p>whether to reshape the vector of predictions to a matrix form when there are several
prediction outputs per case.</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_group_data_pred">group_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with elements being group levels 
for which predictions are made (if there are grouped random effects in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_group_rand_coef_data_pred">group_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data 
for grouped random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_gp_coords_pred">gp_coords_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction coordinates (=features) for 
Gaussian process (if there is a GP in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_gp_rand_coef_data_pred">gp_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data for 
Gaussian process random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_cluster_ids_pred">cluster_ids_pred</code></td>
<td>
<p>A <code>vector</code> with elements indicating the realizations of 
random effects / Gaussian processes for which predictions are made 
(set to NULL if you have not specified this when creating the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_predict_cov_mat">predict_cov_mat</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive covariance is calculated in addition to the (posterior) predictive mean</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_predict_var">predict_var</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive variances are calculated</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_cov_pars">cov_pars</code></td>
<td>
<p>A <code>vector</code> containing covariance parameters which are used if the 
<code>gp_model</code> has not been trained or if predictions should be made for other 
parameters than the trained ones</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_ignore_gp_model">ignore_gp_model</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, predictions are only made for the tree ensemble part
and the <code>gp_model</code> is ignored</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_rawscore">rawscore</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument 
<code>pred_latent</code> instead</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="predict.gpb.Booster_+3A_...">...</code></td>
<td>
<p>Additional named arguments passed to the <code>predict()</code> method of
the <code>gpb.Booster</code> object passed to <code>object</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either a list with vectors or a single vector / matrix depending on 
whether there is a <code>gp_model</code> or not
If there is a <code>gp_model</code>, the result dict contains the following entries.
1. If <code>pred_latent</code> is TRUE, the dict contains the following 3 entries:
- result[&quot;fixed_effect&quot;] are the predictions from the tree-ensemble.
- result[&quot;random_effect_mean&quot;] are the predicted means of the <code>gp_model</code>.
- result[&quot;random_effect_cov&quot;] are the predicted covariances or variances of the <code>gp_model</code>
(only if 'predict_var' or 'predict_cov' is TRUE).
2. If <code>pred_latent</code> is FALSE, the dict contains the following 2 entries:
- result[&quot;response_mean&quot;] are the predicted means of the response variable (Label) taking into account
both the fixed effects (tree-ensemble) and the random effects (<code>gp_model</code>)
- result[&quot;response_var&quot;] are the predicted  covariances or variances of the response variable
(only if 'predict_var' or 'predict_cov' is TRUE)
If there is no <code>gp_model</code> or <code>predcontrib</code> or <code>ignore_gp_model</code> 
are TRUE, the result contains predictions from the tree-booster only.
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


library(gpboost)
data(GPBoost_data, package = "gpboost")

#--------------------Combine tree-boosting and grouped random effects model----------------
# Create random effects model
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
# The default optimizer for covariance parameters (hyperparameters) is 
# Nesterov-accelerated gradient descent.
# This can be changed to, e.g., Nelder-Mead as follows:
# re_params &lt;- list(optimizer_cov = "nelder_mead")
# gp_model$set_optim_params(params=re_params)
# Use trace = TRUE to monitor convergence:
# re_params &lt;- list(trace = TRUE)
# gp_model$set_optim_params(params=re_params)

# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 16,
               learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)

# Make predictions
# Predict latent variables
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean
# For Gaussian data: pred$random_effect_mean + pred$fixed_effect = pred_resp$response_mean
pred$random_effect_mean + pred$fixed_effect - pred_resp$response_mean

#--------------------Combine tree-boosting and Gaussian process model----------------
# Create Gaussian process model
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood = "gaussian")
# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 8,
               learning_rate = 0.1, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)
# Make predictions
pred &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean

</code></pre>

<hr>
<h2 id='predict.GPModel'>Make predictions for a <code>GPModel</code></h2><span id='topic+predict.GPModel'></span>

<h3>Description</h3>

<p>Make predictions for a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
predict(object, y = NULL, group_data_pred = NULL,
  group_rand_coef_data_pred = NULL, gp_coords_pred = NULL,
  gp_rand_coef_data_pred = NULL, cluster_ids_pred = NULL,
  predict_cov_mat = FALSE, predict_var = FALSE, cov_pars = NULL,
  X_pred = NULL, use_saved_data = FALSE, predict_response = TRUE,
  offset = NULL, offset_pred = NULL, fixed_effects = NULL,
  fixed_effects_pred = NULL, vecchia_pred_type = NULL,
  num_neighbors_pred = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.GPModel_+3A_object">object</code></td>
<td>
<p>a <code>GPModel</code></p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_y">y</code></td>
<td>
<p>Observed data (can be NULL, e.g. when the model has been estimated 
already and the same data is used for making predictions)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_group_data_pred">group_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with elements being group levels 
for which predictions are made (if there are grouped random effects in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_group_rand_coef_data_pred">group_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data 
for grouped random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_gp_coords_pred">gp_coords_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction coordinates (=features) for 
Gaussian process (if there is a GP in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_gp_rand_coef_data_pred">gp_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data for 
Gaussian process random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_cluster_ids_pred">cluster_ids_pred</code></td>
<td>
<p>A <code>vector</code> with elements indicating the realizations of 
random effects / Gaussian processes for which predictions are made 
(set to NULL if you have not specified this when creating the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_predict_cov_mat">predict_cov_mat</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive covariance is calculated in addition to the (posterior) predictive mean</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_predict_var">predict_var</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the (posterior) 
predictive variances are calculated</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_cov_pars">cov_pars</code></td>
<td>
<p>A <code>vector</code> containing covariance parameters which are used if the 
<code>GPModel</code> has not been trained or if predictions should be made for other 
parameters than the trained ones</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_x_pred">X_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction covariate data for the 
fixed effects linear regression term (if there is one in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_use_saved_data">use_saved_data</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, predictions are done using 
a priory set data via the function '$set_prediction_data' (this option is not used by users directly)</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_predict_response">predict_response</code></td>
<td>
<p>A <code>boolean</code>. If TRUE, the response variable (label) 
is predicted, otherwise the latent random effects</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_offset">offset</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor (= offset). 
The length of this vector needs to equal the number of training data points.</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_offset_pred">offset_pred</code></td>
<td>
<p>A <code>numeric</code> <code>vector</code> with 
additional fixed effects contributions that are added to the linear predictor for the prediction points (= offset). 
The length of this vector needs to equal the number of prediction points.</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset</code> instead</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_fixed_effects_pred">fixed_effects_pred</code></td>
<td>
<p>This is discontinued. Use the renamed equivalent argument <code>offset_pred</code> instead</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for making predictions.
This is discontinued here. Use the function 'set_prediction_data' to specify this</p>
</td></tr>
<tr><td><code id="predict.GPModel_+3A_...">...</code></td>
<td>
<p>(not used, ignore this, simply here that there is no CRAN warning)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions from a <code>GPModel</code>. A list with three entries is returned:
</p>

<ul>
<li><p> &quot;mu&quot; (first entry): predictive (=posterior) mean. For (generalized) linear mixed
effects models, i.e., models with a linear regression term, this consists of the sum of 
fixed effects and random effects predictions 
</p>
</li>
<li><p> &quot;cov&quot; (second entry): predictive (=posterior) covariance matrix. 
This is NULL if 'predict_cov_mat=FALSE'  
</p>
</li>
<li><p> &quot;var&quot; (third entry) : predictive (=posterior) variances. 
This is NULL if 'predict_var=FALSE'  
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

#--------------------Grouped random effects model: single-level random effect----------------
gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1,
                       likelihood="gaussian", params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_var = TRUE)
pred$mu # Predicted mean
pred$var # Predicted variances
# Also predict covariance matrix
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted mean
pred$cov # Predicted covariance


#--------------------Gaussian process model----------------
gp_model &lt;- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       likelihood="gaussian", y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)
# Make predictions
pred &lt;- predict(gp_model, gp_coords_pred = coords_test, 
                X_pred = X_test1, predict_cov_mat = TRUE)
pred$mu # Predicted (posterior) mean of GP
pred$cov # Predicted (posterior) covariance matrix of GP


</code></pre>

<hr>
<h2 id='readRDS.gpb.Booster'>readRDS for <code>gpb.Booster</code> models</h2><span id='topic+readRDS.gpb.Booster'></span>

<h3>Description</h3>

<p>Attempts to load a model stored in a <code>.rds</code> file, using <code><a href="base.html#topic+readRDS">readRDS</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readRDS.gpb.Booster(file, refhook = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readRDS.gpb.Booster_+3A_file">file</code></td>
<td>
<p>a connection or the name of the file where the R object is saved to or read from.</p>
</td></tr>
<tr><td><code id="readRDS.gpb.Booster_+3A_refhook">refhook</code></td>
<td>
<p>a hook function for handling reference objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gpb.Booster</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(objective = "regression", metric = "l2")
valids &lt;- list(test = dtest)
model &lt;- gpb.train(
  params = params
  , data = dtrain
  , nrounds = 10L
  , valids = valids
  , min_data = 1L
  , learning_rate = 1.0
  , early_stopping_rounds = 5L
)
model_file &lt;- tempfile(fileext = ".rds")
saveRDS.gpb.Booster(model, model_file)
new_model &lt;- readRDS.gpb.Booster(model_file)

</code></pre>

<hr>
<h2 id='saveGPModel'>Save a <code>GPModel</code></h2><span id='topic+saveGPModel'></span>

<h3>Description</h3>

<p>Save a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saveGPModel(gp_model, filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saveGPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>a <code>GPModel</code></p>
</td></tr>
<tr><td><code id="saveGPModel_+3A_filename">filename</code></td>
<td>
<p>filename for saving</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1, likelihood="gaussian")
pred &lt;- predict(gp_model, group_data_pred = group_data_test[,1], 
                X_pred = X_test1, predict_var = TRUE)
# Save model to file
filename &lt;- tempfile(fileext = ".json")
saveGPModel(gp_model,filename = filename)
# Load from file and make predictions again
gp_model_loaded &lt;- loadGPModel(filename = filename)
pred_loaded &lt;- predict(gp_model_loaded, group_data_pred = group_data_test[,1], 
                       X_pred = X_test1, predict_var = TRUE)
# Check equality
pred$mu - pred_loaded$mu
pred$var - pred_loaded$var

</code></pre>

<hr>
<h2 id='saveRDS.gpb.Booster'>saveRDS for <code>gpb.Booster</code> models</h2><span id='topic+saveRDS.gpb.Booster'></span>

<h3>Description</h3>

<p>Attempts to save a model using RDS. Has an additional parameter (<code>raw</code>)
which decides whether to save the raw model or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saveRDS.gpb.Booster(object, file, ascii = FALSE, version = NULL,
  compress = TRUE, refhook = NULL, raw = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saveRDS.gpb.Booster_+3A_object">object</code></td>
<td>
<p>R object to serialize.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_file">file</code></td>
<td>
<p>a connection or the name of the file where the R object is saved to or read from.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_ascii">ascii</code></td>
<td>
<p>a logical. If TRUE or NA, an ASCII representation is written; otherwise (default),
a binary one is used. See the comments in the help for save.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_version">version</code></td>
<td>
<p>the workspace format version to use. <code>NULL</code> specifies the current default
version (2). Versions prior to 2 are not supported, so this will only be relevant
when there are later versions.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_compress">compress</code></td>
<td>
<p>a logical specifying whether saving to a named file is to use &quot;gzip&quot; compression,
or one of <code>"gzip"</code>, <code>"bzip2"</code> or <code>"xz"</code> to indicate the type of
compression to be used. Ignored if file is a connection.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_refhook">refhook</code></td>
<td>
<p>a hook function for handling reference objects.</p>
</td></tr>
<tr><td><code id="saveRDS.gpb.Booster_+3A_raw">raw</code></td>
<td>
<p>whether to save the model in a raw variable or not, recommended to leave it to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL invisibly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gpboost)
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "gpboost")
test &lt;- agaricus.test
dtest &lt;- gpb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(objective = "regression", metric = "l2")
valids &lt;- list(test = dtest)
model &lt;- gpb.train(
    params = params
    , data = dtrain
    , nrounds = 10L
    , valids = valids
    , min_data = 1L
    , learning_rate = 1.0
    , early_stopping_rounds = 5L
)
model_file &lt;- tempfile(fileext = ".rds")
saveRDS.gpb.Booster(model, model_file)

</code></pre>

<hr>
<h2 id='set_optim_params'>Set parameters for estimation of the covariance parameters</h2><span id='topic+set_optim_params'></span>

<h3>Description</h3>

<p>Set parameters for optimization of the covariance parameters of a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_optim_params(gp_model, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_optim_params_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="set_optim_params_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
gp_model &lt;- GPModel(group_data = group_data, likelihood="gaussian")
set_optim_params(gp_model, params=list(optimizer_cov="nelder_mead"))


</code></pre>

<hr>
<h2 id='set_optim_params.GPModel'>Set parameters for estimation of the covariance parameters</h2><span id='topic+set_optim_params.GPModel'></span>

<h3>Description</h3>

<p>Set parameters for optimization of the covariance parameters of a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
set_optim_params(gp_model, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_optim_params.GPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="set_optim_params.GPModel_+3A_params">params</code></td>
<td>
<p>A <code>list</code> with parameters for the estimation / optimization
</p>

<ul>
<li><p>optimizer_cov: <code>string</code> (default = &quot;lbfgs&quot; for linear mixed effects models and &quot;gradient_descent&quot; for the GPBoost algorithm). 
Optimizer used for estimating covariance parameters. 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;fisher_scoring&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'optimizer_cov' is also used for those 
</p>
</li>
<li><p>optimizer_coef: <code>string</code> (default = &quot;wls&quot; for Gaussian likelihoods and &quot;gradient_descent&quot; for other likelihoods). 
Optimizer used for estimating linear regression coefficients, if there are any 
(for the GPBoost algorithm there are usually none). 
Options: &quot;gradient_descent&quot;, &quot;lbfgs&quot;, &quot;wls&quot;, &quot;nelder_mead&quot;, &quot;adam&quot;. Gradient descent steps are done simultaneously 
with gradient descent steps for the covariance parameters. 
&quot;wls&quot; refers to doing coordinate descent for the regression coefficients using weighted least squares.
If 'optimizer_cov' is set to &quot;nelder_mead&quot;, &quot;lbfgs&quot;, or &quot;adam&quot;, 
'optimizer_coef' is automatically also set to the same value.
</p>
</li>
<li><p>maxit: <code>integer</code> (default = 1000). 
Maximal number of iterations for optimization algorithm 
</p>
</li>
<li><p>delta_rel_conv: <code>numeric</code> (default = 1E-6 except for &quot;nelder_mead&quot; for which the default is 1E-8). 
Convergence tolerance. The algorithm stops if the relative change 
in either the (approximate) log-likelihood or the parameters is below this value. 
For &quot;adam&quot;, the L2 norm of the gradient is used instead of the relative change in the log-likelihood. 
If &lt; 0, internal default values are used 
</p>
</li>
<li><p>convergence_criterion: <code>string</code> (default = &quot;relative_change_in_log_likelihood&quot;). 
The convergence criterion used for terminating the optimization algorithm.
Options: &quot;relative_change_in_log_likelihood&quot; or &quot;relative_change_in_parameters&quot; 
</p>
</li>
<li><p>init_coef: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for the regression coefficients (if there are any, can be NULL) 
</p>
</li>
<li><p>init_cov_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for covariance parameters of Gaussian process and 
random effects (can be NULL). The order it the same as the order 
of the parameters in the summary function: first is the error variance 
(only for &quot;gaussian&quot; likelihood), next follow the variances of the 
grouped random effects (if there are any, in the order provided in 'group_data'), 
and then follow the marginal variance and the range of the Gaussian process. 
If there are multiple Gaussian processes, then the variances and ranges follow alternatingly.
If 'init_cov_pars = NULL', an internal choice is used that depends on the 
likelihood and the random effects type and covariance function. 
If you select the option 'trace = TRUE' in the 'params' argument, 
you will see the first initial covariance parameters in iteration 0. 
</p>
</li>
<li><p>lr_coef: <code>numeric</code> (default = 0.1). 
Learning rate for fixed effect regression coefficients if gradient descent is used 
</p>
</li>
<li><p>lr_cov: <code>numeric</code> (default = 0.1 for &quot;gradient_descent&quot; and 1. for &quot;fisher_scoring&quot;). 
Initial learning rate for covariance parameters if &quot;gradient_descent&quot; or &quot;fisher_scoring&quot; is used. 
If lr_cov &lt; 0, internal default values are used.
If there are additional auxiliary parameters for non-Gaussian likelihoods, 
'lr_cov' is also used for those
</p>
</li>
<li><p>use_nesterov_acc: <code>boolean</code> (default = TRUE). 
If TRUE Nesterov acceleration is used.
This is used only for gradient descent 
</p>
</li>
<li><p>acc_rate_coef: <code>numeric</code> (default = 0.5). 
Acceleration rate for regression coefficients (if there are any) 
for Nesterov acceleration 
</p>
</li>
<li><p>acc_rate_cov: <code>numeric</code> (default = 0.5). 
Acceleration rate for covariance parameters for Nesterov acceleration 
</p>
</li>
<li><p>momentum_offset: <code>integer</code> (Default = 2). 
Number of iterations for which no momentum is applied in the beginning.
</p>
</li>
<li><p>trace: <code>boolean</code> (default = FALSE). 
If TRUE, information on the progress of the parameter
optimization is printed
</p>
</li>
<li><p>std_dev: <code>boolean</code> (default = TRUE). 
If TRUE, approximate standard deviations are calculated for the covariance and linear regression parameters 
(= square root of diagonal of the inverse Fisher information for Gaussian likelihoods and 
square root of diagonal of a numerically approximated inverse Hessian for non-Gaussian likelihoods) 
</p>
</li>
<li><p>init_aux_pars: <code>vector</code> with <code>numeric</code> elements (default = NULL). 
Initial values for additional parameters for non-Gaussian likelihoods 
(e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>estimate_aux_pars: <code>boolean</code> (default = TRUE). 
If TRUE, additional parameters for non-Gaussian likelihoods 
are also estimated (e.g., shape parameter of a gamma or negative_binomial likelihood) 
</p>
</li>
<li><p>cg_max_num_it: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithms 
</p>
</li>
<li><p>cg_max_num_it_tridiag: <code>integer</code> (default = 1000). 
Maximal number of iterations for conjugate gradient algorithm 
when being run as Lanczos algorithm for tridiagonalization 
</p>
</li>
<li><p>cg_delta_conv: <code>numeric</code> (default = 1E-2).
Tolerance level for L2 norm of residuals for checking convergence 
in conjugate gradient algorithm when being used for parameter estimation 
</p>
</li>
<li><p>num_rand_vec_trace: <code>integer</code> (default = 50). 
Number of random vectors (e.g., Rademacher) for stochastic approximation of the trace of a matrix 
</p>
</li>
<li><p>reuse_rand_vec_trace: <code>boolean</code> (default = TRUE). 
If true, random vectors (e.g., Rademacher) for stochastic approximations 
of the trace of a matrix are sampled only once at the beginning of 
the parameter estimation and reused in later trace approximations.
Otherwise they are sampled every time a trace is calculated 
</p>
</li>
<li><p>seed_rand_vec_trace: <code>integer</code> (default = 1). 
Seed number to generate random vectors (e.g., Rademacher) 
</p>
</li>
<li><p>piv_chol_rank: <code>integer</code> (default = 50). 
Rank of the pivoted Cholesky decomposition used as 
preconditioner in conjugate gradient algorithms 
</p>
</li>
<li><p>cg_preconditioner_type: <code>string</code>.
Type of preconditioner used for conjugate gradient algorithms.
</p>

<ul>
<li><p> Options for non-Gaussian likelihoods and gp_approx = &quot;vecchia&quot;: 
</p>

<ul>
<li><p>&quot;Sigma_inv_plus_BtWB&quot; (= default): (B^T * (D^-1 + W) * B) as preconditioner for inverting (B^T * D^-1 * B + W), 
where B^T * D^-1 * B approx= Sigma^-1 
</p>
</li></ul>

</li>
<li><p>&quot;piv_chol_on_Sigma&quot;: (Lk * Lk^T + W^-1) as preconditioner for inverting (B^-1 * D * B^-T + W^-1), 
where Lk is a low-rank pivoted Cholesky approximation for Sigma and B^-1 * D * B^-T approx= Sigma 
</p>
</li>
<li><p> Options for likelihood = &quot;gaussian&quot; and gp_approx = &quot;full_scale_tapering&quot;: 
</p>

<ul>
<li><p>&quot;predictive_process_plus_diagonal&quot; (= default): predictive process preconditiioner 
</p>
</li>
<li><p>&quot;none&quot;: no preconditioner 
</p>
</li></ul>

</li></ul>


</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
gp_model &lt;- GPModel(group_data = group_data, likelihood="gaussian")
set_optim_params(gp_model, params=list(optimizer_cov="nelder_mead"))

</code></pre>

<hr>
<h2 id='set_prediction_data'>Set prediction data for a <code>GPModel</code></h2><span id='topic+set_prediction_data'></span>

<h3>Description</h3>

<p>Set the data required for making predictions with a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_prediction_data(gp_model, vecchia_pred_type = NULL,
  num_neighbors_pred = NULL, cg_delta_conv_pred = NULL,
  nsim_var_pred = NULL, rank_pred_approx_matrix_lanczos = NULL,
  group_data_pred = NULL, group_rand_coef_data_pred = NULL,
  gp_coords_pred = NULL, gp_rand_coef_data_pred = NULL,
  cluster_ids_pred = NULL, X_pred = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_prediction_data_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
Default value if vecchia_pred_type = NULL: &quot;order_obs_first_cond_obs_only&quot;. 
Available options:
</p>

<ul>
<li><p>&quot;order_obs_first_cond_obs_only&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are only observed training data points 
</p>
</li>
<li><p>&quot;order_obs_first_cond_all&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are selected among all points (training + prediction) 
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_obs_only&quot;: Vecchia approximation for the latent process and observed data is 
ordered first and neighbors are only observed points
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_all&quot;: Vecchia approximation 
for the latent process and observed data is ordered first and neighbors are selected among all points 
</p>
</li>
<li><p>&quot;order_pred_first&quot;: Vecchia approximation for the observable process and prediction data is 
ordered first for making predictions. This option is only available for Gaussian likelihoods 
</p>
</li></ul>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for the Vecchia approximation 
for making predictions. Default value if NULL: num_neighbors_pred = 2 * num_neighbors</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_cg_delta_conv_pred">cg_delta_conv_pred</code></td>
<td>
<p>a <code>numeric</code> specifying the tolerance level for L2 norm of residuals for 
checking convergence in conjugate gradient algorithms when being used for prediction
Default value if NULL: 1e-3</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_nsim_var_pred">nsim_var_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of samples when simulation 
is used for calculating predictive variances
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_rank_pred_approx_matrix_lanczos">rank_pred_approx_matrix_lanczos</code></td>
<td>
<p>an <code>integer</code> specifying the rank 
of the matrix for approximating predictive covariances obtained using the Lanczos algorithm
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_group_data_pred">group_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with elements being group levels 
for which predictions are made (if there are grouped random effects in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_group_rand_coef_data_pred">group_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data 
for grouped random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_gp_coords_pred">gp_coords_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction coordinates (=features) for 
Gaussian process (if there is a GP in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_gp_rand_coef_data_pred">gp_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data for 
Gaussian process random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_cluster_ids_pred">cluster_ids_pred</code></td>
<td>
<p>A <code>vector</code> with elements indicating the realizations of 
random effects / Gaussian processes for which predictions are made 
(set to NULL if you have not specified this when creating the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data_+3A_x_pred">X_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction covariate data for the 
fixed effects linear regression term (if there is one in the <code>GPModel</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
set.seed(1)
train_ind &lt;- sample.int(length(y),size=250)
gp_model &lt;- GPModel(group_data = group_data[train_ind,1], likelihood="gaussian")
set_prediction_data(gp_model, group_data_pred = group_data[-train_ind,1])


</code></pre>

<hr>
<h2 id='set_prediction_data.GPModel'>Set prediction data for a <code>GPModel</code></h2><span id='topic+set_prediction_data.GPModel'></span>

<h3>Description</h3>

<p>Set the data required for making predictions with a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
set_prediction_data(gp_model, vecchia_pred_type = NULL,
  num_neighbors_pred = NULL, cg_delta_conv_pred = NULL,
  nsim_var_pred = NULL, rank_pred_approx_matrix_lanczos = NULL,
  group_data_pred = NULL, group_rand_coef_data_pred = NULL,
  gp_coords_pred = NULL, gp_rand_coef_data_pred = NULL,
  cluster_ids_pred = NULL, X_pred = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_prediction_data.GPModel_+3A_gp_model">gp_model</code></td>
<td>
<p>A <code>GPModel</code></p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_vecchia_pred_type">vecchia_pred_type</code></td>
<td>
<p>A <code>string</code> specifying the type of Vecchia approximation used for making predictions.
Default value if vecchia_pred_type = NULL: &quot;order_obs_first_cond_obs_only&quot;. 
Available options:
</p>

<ul>
<li><p>&quot;order_obs_first_cond_obs_only&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are only observed training data points 
</p>
</li>
<li><p>&quot;order_obs_first_cond_all&quot;: Vecchia approximation for the observable process and observed training data is 
ordered first and the neighbors are selected among all points (training + prediction) 
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_obs_only&quot;: Vecchia approximation for the latent process and observed data is 
ordered first and neighbors are only observed points
</p>
</li>
<li><p>&quot;latent_order_obs_first_cond_all&quot;: Vecchia approximation 
for the latent process and observed data is ordered first and neighbors are selected among all points 
</p>
</li>
<li><p>&quot;order_pred_first&quot;: Vecchia approximation for the observable process and prediction data is 
ordered first for making predictions. This option is only available for Gaussian likelihoods 
</p>
</li></ul>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_num_neighbors_pred">num_neighbors_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of neighbors for the Vecchia approximation 
for making predictions. Default value if NULL: num_neighbors_pred = 2 * num_neighbors</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_cg_delta_conv_pred">cg_delta_conv_pred</code></td>
<td>
<p>a <code>numeric</code> specifying the tolerance level for L2 norm of residuals for 
checking convergence in conjugate gradient algorithms when being used for prediction
Default value if NULL: 1e-3</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_nsim_var_pred">nsim_var_pred</code></td>
<td>
<p>an <code>integer</code> specifying the number of samples when simulation 
is used for calculating predictive variances
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_rank_pred_approx_matrix_lanczos">rank_pred_approx_matrix_lanczos</code></td>
<td>
<p>an <code>integer</code> specifying the rank 
of the matrix for approximating predictive covariances obtained using the Lanczos algorithm
Default value if NULL: 1000</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_group_data_pred">group_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with elements being group levels 
for which predictions are made (if there are grouped random effects in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_group_rand_coef_data_pred">group_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data 
for grouped random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_gp_coords_pred">gp_coords_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction coordinates (=features) for 
Gaussian process (if there is a GP in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_gp_rand_coef_data_pred">gp_rand_coef_data_pred</code></td>
<td>
<p>A <code>vector</code> or <code>matrix</code> with covariate data for 
Gaussian process random coefficients (if there are some in the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_cluster_ids_pred">cluster_ids_pred</code></td>
<td>
<p>A <code>vector</code> with elements indicating the realizations of 
random effects / Gaussian processes for which predictions are made 
(set to NULL if you have not specified this when creating the <code>GPModel</code>)</p>
</td></tr>
<tr><td><code id="set_prediction_data.GPModel_+3A_x_pred">X_pred</code></td>
<td>
<p>A <code>matrix</code> with prediction covariate data for the 
fixed effects linear regression term (if there is one in the <code>GPModel</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GPBoost_data, package = "gpboost")
set.seed(1)
train_ind &lt;- sample.int(length(y),size=250)
gp_model &lt;- GPModel(group_data = group_data[train_ind,1], likelihood="gaussian")
set_prediction_data(gp_model, group_data_pred = group_data[-train_ind,1])

</code></pre>

<hr>
<h2 id='setinfo'>Set information of an <code>gpb.Dataset</code> object</h2><span id='topic+setinfo'></span><span id='topic+setinfo.gpb.Dataset'></span>

<h3>Description</h3>

<p>Set one attribute of a <code>gpb.Dataset</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setinfo(dataset, ...)

## S3 method for class 'gpb.Dataset'
setinfo(dataset, name, info, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setinfo_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="setinfo_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="setinfo_+3A_name">name</code></td>
<td>
<p>the name of the field to get</p>
</td></tr>
<tr><td><code id="setinfo_+3A_info">info</code></td>
<td>
<p>the specific field of information to set</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>name</code> field can be one of the following:
</p>

<ul>
<li><p><code>label</code>: vector of labels to use as the target variable
</p>
</li>
<li><p><code>weight</code>: to do a weight rescale
</p>
</li>
<li><p><code>init_score</code>: initial score is the base prediction gpboost will boost from
</p>
</li>
<li><p><code>group</code>: used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results to be ranked.
For example, if you have a 100-document dataset with <code>group = c(10, 20, 40, 10, 10, 10)</code>,
that means that you have 6 groups, where the first 10 records are in the first group,
records 11-30 are in the second group, etc.
</p>
</li></ul>



<h3>Value</h3>

<p>the dataset you passed in
</p>
<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)
gpb.Dataset.construct(dtrain)

labels &lt;- gpboost::getinfo(dtrain, "label")
gpboost::setinfo(dtrain, "label", 1 - labels)

labels2 &lt;- gpboost::getinfo(dtrain, "label")
stopifnot(all.equal(labels2, 1 - labels))

</code></pre>

<hr>
<h2 id='slice'>Slice a dataset</h2><span id='topic+slice'></span><span id='topic+slice.gpb.Dataset'></span>

<h3>Description</h3>

<p>Get a new <code>gpb.Dataset</code> containing the specified rows of
original <code>gpb.Dataset</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slice(dataset, ...)

## S3 method for class 'gpb.Dataset'
slice(dataset, idxset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slice_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>gpb.Dataset</code></p>
</td></tr>
<tr><td><code id="slice_+3A_...">...</code></td>
<td>
<p>other parameters (currently not used)</p>
</td></tr>
<tr><td><code id="slice_+3A_idxset">idxset</code></td>
<td>
<p>an integer vector of indices of rows needed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed sub dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package = "gpboost")
train &lt;- agaricus.train
dtrain &lt;- gpb.Dataset(train$data, label = train$label)

dsub &lt;- gpboost::slice(dtrain, seq_len(42L))
gpb.Dataset.construct(dsub)
labels &lt;- gpboost::getinfo(dsub, "label")

</code></pre>

<hr>
<h2 id='summary.GPModel'>Summary for a <code>GPModel</code></h2><span id='topic+summary.GPModel'></span>

<h3>Description</h3>

<p>Summary for a <code>GPModel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GPModel'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.GPModel_+3A_object">object</code></td>
<td>
<p>a <code>GPModel</code></p>
</td></tr>
<tr><td><code id="summary.GPModel_+3A_...">...</code></td>
<td>
<p>(not used, ignore this, simply here that there is no CRAN warning)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary of a (fitted) <code>GPModel</code>
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples

data(GPBoost_data, package = "gpboost")
# Add intercept column
X1 &lt;- cbind(rep(1,dim(X)[1]),X)
X_test1 &lt;- cbind(rep(1,dim(X_test)[1]),X_test)

#--------------------Grouped random effects model: single-level random effect----------------
gp_model &lt;- fitGPModel(group_data = group_data[,1], y = y, X = X1,
                       likelihood="gaussian", params = list(std_dev = TRUE))
summary(gp_model)



#--------------------Gaussian process model----------------
gp_model &lt;- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       likelihood="gaussian", y = y, X = X1, params = list(std_dev = TRUE))
summary(gp_model)


</code></pre>

<hr>
<h2 id='X'>Example data for the GPBoost package</h2><span id='topic+X'></span>

<h3>Description</h3>

<p>A matrix with covariate data for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='X_test'>Example data for the GPBoost package</h2><span id='topic+X_test'></span>

<h3>Description</h3>

<p>A matrix with covariate information for the predictions for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

<hr>
<h2 id='y'>Example data for the GPBoost package</h2><span id='topic+y'></span>

<h3>Description</h3>

<p>Response variable for the example data of the GPBoost package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GPBoost_data)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
