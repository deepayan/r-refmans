<!DOCTYPE html><html lang="en"><head><title>Help for package LLMR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LLMR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Agent'><p>Agent Class for LLM Interactions</p></a></li>
<li><a href='#AgentAction'><p>AgentAction S3 Class</p></a></li>
<li><a href='#cache_llm_call'><p>Cache LLM API Calls</p></a></li>
<li><a href='#call_llm'><p>Call LLM API</p></a></li>
<li><a href='#call_llm_robust'><p>Robustly Call LLM API (Simple Retry)</p></a></li>
<li><a href='#llm_config'><p>Create LLM Configuration</p></a></li>
<li><a href='#LLMConversation'><p>LLMConversation Class for Coordinating Agents</p></a></li>
<li><a href='#log_llm_error'><p>Log LLMR Errors</p></a></li>
<li><a href='#parse_embeddings'><p>Parse Embedding Response into a Numeric Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Interface for Large Language Model APIs in R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.5</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Description:</td>
<td>A unified interface to interact with various Large Language Model (LLM) APIs such as 'OpenAI' (see <a href="https://platform.openai.com/docs/overview">https://platform.openai.com/docs/overview</a> for details), 'Anthropic' (see <a href="https://docs.anthropic.com/en/api/getting-started">https://docs.anthropic.com/en/api/getting-started</a> for details), 'Groq' (see <a href="https://console.groq.com/docs/api-reference">https://console.groq.com/docs/api-reference</a> for details), 'Together AI' (see <a href="https://docs.together.ai/docs/quickstart">https://docs.together.ai/docs/quickstart</a> for details), 'DeepSeek' (see <a href="https://api-docs.deepseek.com">https://api-docs.deepseek.com</a> for details), 'Gemini' (see <a href="https://aistudio.google.com">https://aistudio.google.com</a> for details), and 'Voyage AI' (see <a href="https://docs.voyageai.com/docs/introduction">https://docs.voyageai.com/docs/introduction</a> for details). Allows users to use and switch between various APIs seamlessly within R, and define LLM agents.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>httr2, purrr, rlang, memoise</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), roxygen2 (&ge; 7.1.2), httptest2</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/asanaei/LLMR">https://github.com/asanaei/LLMR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/asanaei/LLMR/issues">https://github.com/asanaei/LLMR/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-16 05:49:33 UTC; ali</td>
</tr>
<tr>
<td>Author:</td>
<td>Ali Sanaei [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ali Sanaei &lt;sanaei@uchicago.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-18 13:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Agent'>Agent Class for LLM Interactions</h2><span id='topic+Agent'></span>

<h3>Description</h3>

<p>An R6 class representing an agent that interacts with language models.
</p>
<p>*At agent-level we do not automate summarization.* The 'maybe_summarize_memory()'
function can be called manually if the user wishes to compress the agent's memory.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>id</code></dt><dd><p>Unique ID for this Agent.</p>
</dd>
<dt><code>context_length</code></dt><dd><p>Maximum number of conversation turns stored in memory.</p>
</dd>
<dt><code>model_config</code></dt><dd><p>The <code>llm_config</code> specifying which LLM to call.</p>
</dd>
<dt><code>memory</code></dt><dd><p>A list of speaker/text pairs that the agent has memorized.</p>
</dd>
<dt><code>persona</code></dt><dd><p>Named list for additional agent-specific details (e.g., role, style).</p>
</dd>
<dt><code>enable_summarization</code></dt><dd><p>Logical. If TRUE, user *may* call 'maybe_summarize_memory()'.</p>
</dd>
<dt><code>token_threshold</code></dt><dd><p>Numeric. If manually triggered, we can compare total_tokens.</p>
</dd>
<dt><code>total_tokens</code></dt><dd><p>Numeric. Estimated total tokens in memory.</p>
</dd>
<dt><code>summarization_density</code></dt><dd><p>Character. &quot;low&quot;, &quot;medium&quot;, or &quot;high&quot;.</p>
</dd>
<dt><code>summarization_prompt</code></dt><dd><p>Character. Optional custom prompt for summarization.</p>
</dd>
<dt><code>summarizer_config</code></dt><dd><p>Optional <code>llm_config</code> for summarizing the agent's memory.</p>
</dd>
<dt><code>auto_inject_conversation</code></dt><dd><p>Logical. If TRUE, automatically prepend conversation memory if missing.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Agent-new"><code>Agent$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-add_memory"><code>Agent$add_memory()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-maybe_summarize_memory"><code>Agent$maybe_summarize_memory()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-generate_prompt"><code>Agent$generate_prompt()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-call_llm_agent"><code>Agent$call_llm_agent()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-generate"><code>Agent$generate()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-think"><code>Agent$think()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-respond"><code>Agent$respond()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-reset_memory"><code>Agent$reset_memory()</code></a>
</p>
</li>
<li> <p><a href="#method-Agent-clone"><code>Agent$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Agent-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new Agent instance.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$new(
  id,
  context_length = 5,
  persona = NULL,
  model_config,
  enable_summarization = TRUE,
  token_threshold = 1000,
  summarization_density = "medium",
  summarization_prompt = NULL,
  summarizer_config = NULL,
  auto_inject_conversation = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt><dd><p>Character. The agent's unique identifier.</p>
</dd>
<dt><code>context_length</code></dt><dd><p>Numeric. The maximum number of messages stored (default = 5).</p>
</dd>
<dt><code>persona</code></dt><dd><p>A named list of persona details.</p>
</dd>
<dt><code>model_config</code></dt><dd><p>An <code>llm_config</code> object specifying LLM settings.</p>
</dd>
<dt><code>enable_summarization</code></dt><dd><p>Logical. If TRUE, you can manually call summarization.</p>
</dd>
<dt><code>token_threshold</code></dt><dd><p>Numeric. If you're calling summarization, use this threshold if desired.</p>
</dd>
<dt><code>summarization_density</code></dt><dd><p>Character. &quot;low&quot;, &quot;medium&quot;, &quot;high&quot; for summary detail.</p>
</dd>
<dt><code>summarization_prompt</code></dt><dd><p>Character. Optional custom prompt for summarization.</p>
</dd>
<dt><code>summarizer_config</code></dt><dd><p>Optional <code>llm_config</code> for summarization calls.</p>
</dd>
<dt><code>auto_inject_conversation</code></dt><dd><p>Logical. If TRUE, auto-append conversation memory to prompt if missing.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new <code>Agent</code> object.
</p>


<hr>
<a id="method-Agent-add_memory"></a>



<h4>Method <code>add_memory()</code></h4>

<p>Add a new message to the agent's memory.
We do NOT automatically call summarization here.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$add_memory(speaker, text)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>speaker</code></dt><dd><p>Character. The speaker name or ID.</p>
</dd>
<dt><code>text</code></dt><dd><p>Character. The message content.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Agent-maybe_summarize_memory"></a>



<h4>Method <code>maybe_summarize_memory()</code></h4>

<p>Manually compress the agent's memory if desired.
Summarizes all memory into a single &quot;summary&quot; message.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$maybe_summarize_memory()</pre></div>


<hr>
<a id="method-Agent-generate_prompt"></a>



<h4>Method <code>generate_prompt()</code></h4>

<p>Internal helper to prepare final prompt by substituting placeholders.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$generate_prompt(template, replacements = list())</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>template</code></dt><dd><p>Character. The prompt template.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>A named list of placeholder values.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Character. The prompt with placeholders replaced.
</p>


<hr>
<a id="method-Agent-call_llm_agent"></a>



<h4>Method <code>call_llm_agent()</code></h4>

<p>Low-level call to the LLM (via robust call_llm_robust) with a final prompt.
If persona is defined, a system message is prepended to help set the role.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$call_llm_agent(prompt, verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>prompt</code></dt><dd><p>Character. The final prompt text.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints debug info. Default FALSE.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A list with:
* text
* tokens_sent
* tokens_received
* full_response (raw list)
</p>


<hr>
<a id="method-Agent-generate"></a>



<h4>Method <code>generate()</code></h4>

<p>Generate a response from the LLM using a prompt template and optional replacements.
Substitutes placeholders, calls the LLM, saves output to memory, returns the response.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$generate(prompt_template, replacements = list(), verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>prompt_template</code></dt><dd><p>Character. The prompt template.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>A named list of placeholder values.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints extra info. Default FALSE.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A list with fields <code>text</code>, <code>tokens_sent</code>, <code>tokens_received</code>, <code>full_response</code>.
</p>


<hr>
<a id="method-Agent-think"></a>



<h4>Method <code>think()</code></h4>

<p>The agent &quot;thinks&quot; about a topic, possibly using the entire memory in the prompt.
If auto_inject_conversation is TRUE and the template lacks {{conversation}}, we prepend the memory.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$think(topic, prompt_template, replacements = list(), verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>topic</code></dt><dd><p>Character. Label for the thought.</p>
</dd>
<dt><code>prompt_template</code></dt><dd><p>Character. The prompt template.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>Named list for additional placeholders.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints info.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Agent-respond"></a>



<h4>Method <code>respond()</code></h4>

<p>The agent produces a public &quot;response&quot; about a topic.
If auto_inject_conversation is TRUE and the template lacks {{conversation}}, we prepend the memory.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$respond(topic, prompt_template, replacements = list(), verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>topic</code></dt><dd><p>Character. A short label for the question/issue.</p>
</dd>
<dt><code>prompt_template</code></dt><dd><p>Character. The prompt template.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>Named list of placeholder substitutions.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints extra info.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A list with <code>text</code>, <code>tokens_sent</code>, <code>tokens_received</code>, <code>full_response</code>.
</p>


<hr>
<a id="method-Agent-reset_memory"></a>



<h4>Method <code>reset_memory()</code></h4>

<p>Reset the agent's memory.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$reset_memory()</pre></div>


<hr>
<a id="method-Agent-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Agent$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='AgentAction'>AgentAction S3 Class</h2><span id='topic+AgentAction'></span>

<h3>Description</h3>

<p>An object that bundles an Agent together with a prompt and replacements so that
it can be chained onto a conversation with the '+' operator.
</p>
<p>When 'conversation + AgentAction' is called:
</p>

<ol>
<li><p> If the agent is not yet in the conversation, it is added.
</p>
</li>
<li><p> The agent is prompted with the provided prompt template (and replacements).
</p>
</li>
<li><p> The conversation is updated with the agent's response.
</p>
</li></ol>



<h3>Usage</h3>

<pre><code class='language-R'>AgentAction(agent, prompt_template, replacements = list(), verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AgentAction_+3A_agent">agent</code></td>
<td>
<p>An <code>Agent</code> object.</p>
</td></tr>
<tr><td><code id="AgentAction_+3A_prompt_template">prompt_template</code></td>
<td>
<p>A character string (the prompt).</p>
</td></tr>
<tr><td><code id="AgentAction_+3A_replacements">replacements</code></td>
<td>
<p>A named list for placeholder substitution (optional).</p>
</td></tr>
<tr><td><code id="AgentAction_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code>, prints verbose LLM response info. Default <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>AgentAction</code>, used in conversation chaining.
</p>

<hr>
<h2 id='cache_llm_call'>Cache LLM API Calls</h2><span id='topic+cache_llm_call'></span>

<h3>Description</h3>

<p>A memoised version of <code><a href="#topic+call_llm">call_llm</a></code> to avoid repeated identical requests.
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cache_llm_call_+3A_config">config</code></td>
<td>
<p>An <code>llm_config</code> object from <code><a href="#topic+llm_config">llm_config</a></code>.</p>
</td></tr>
<tr><td><code id="cache_llm_call_+3A_messages">messages</code></td>
<td>
<p>A list of message objects or character vector for embeddings.</p>
</td></tr>
<tr><td><code id="cache_llm_call_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If TRUE, prints the full API response (passed to <code><a href="#topic+call_llm">call_llm</a></code>).</p>
</td></tr>
<tr><td><code id="cache_llm_call_+3A_json">json</code></td>
<td>
<p>Logical. If TRUE, returns raw JSON (passed to <code><a href="#topic+call_llm">call_llm</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>- Requires the <code>memoise</code> package. Add <code>memoise</code> to your
package's DESCRIPTION.
- Clearing the cache can be done via <code>memoise::forget(cache_llm_call)</code>
or by restarting your R session.
</p>


<h3>Value</h3>

<p>The (memoised) response object from <code><a href="#topic+call_llm">call_llm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Using cache_llm_call:
  response1 &lt;- cache_llm_call(my_config, list(list(role="user", content="Hello!")))
  # Subsequent identical calls won't hit the API unless we clear the cache.
  response2 &lt;- cache_llm_call(my_config, list(list(role="user", content="Hello!")))

## End(Not run)
</code></pre>

<hr>
<h2 id='call_llm'>Call LLM API</h2><span id='topic+call_llm'></span>

<h3>Description</h3>

<p>Sends a message to the specified LLM API and retrieves the response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>call_llm(config, messages, verbose = FALSE, json = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="call_llm_+3A_config">config</code></td>
<td>
<p>An 'llm_config' object created by 'llm_config()'.</p>
</td></tr>
<tr><td><code id="call_llm_+3A_messages">messages</code></td>
<td>
<p>A list of message objects (or a character vector for embeddings) to send to the API.</p>
</td></tr>
<tr><td><code id="call_llm_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If 'TRUE', prints the full API response.</p>
</td></tr>
<tr><td><code id="call_llm_+3A_json">json</code></td>
<td>
<p>Logical. If 'TRUE', returns the raw JSON response as an attribute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The generated text response or embedding results with additional attributes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Make sure to set your needed API keys in environment variables
  # OpenAI Embedding Example (overwriting api_url):
  openai_embed_config &lt;- llm_config(
    provider = "openai",
    model = "text-embedding-3-small",
    api_key = Sys.getenv("OPENAI_KEY"),
    temperature = 0.3,
    api_url = "https://api.openai.com/v1/embeddings"
  )

  text_input &lt;- c("Political science is a useful subject",
                  "We love sociology",
                  "German elections are different",
                  "A student was always curious.")

  embed_response &lt;- call_llm(openai_embed_config, text_input)

  # Voyage AI Example:
  voyage_config &lt;- llm_config(
    provider = "voyage",
    model = "voyage-large-2",
    api_key = Sys.getenv("VOYAGE_API_KEY")
  )

  embedding_response &lt;- call_llm(voyage_config, text_input)
  embeddings &lt;- parse_embeddings(embedding_response)
  embeddings |&gt; cor() |&gt; print()


  # Gemini Example
  gemini_config &lt;- llm_config(
    provider = "gemini",
    model = "gemini-pro",          # Or another Gemini model
    api_key = Sys.getenv("GEMINI_API_KEY"),
    temperature = 0.9,               # Controls randomness
    max_tokens = 800,              # Maximum tokens to generate
    top_p = 0.9,                     # Nucleus sampling parameter
    top_k = 10                      # Top K sampling parameter
  )

  gemini_message &lt;- list(
    list(role = "user", content = "Explain the theory of relativity to a curious 3-year-old!")
  )

  gemini_response &lt;- call_llm(
    config = gemini_config,
    messages = gemini_message,
    json = TRUE # Get raw JSON for inspection if needed
  )

  # Display the generated text response
  cat("Gemini Response:", gemini_response, "\n")

  # Access and print the raw JSON response
  raw_json_gemini_response &lt;- attr(gemini_response, "raw_json")
  print(raw_json_gemini_response)

## End(Not run)
</code></pre>

<hr>
<h2 id='call_llm_robust'>Robustly Call LLM API (Simple Retry)</h2><span id='topic+call_llm_robust'></span>

<h3>Description</h3>

<p>Wraps <code><a href="#topic+call_llm">call_llm</a></code> to handle rate-limit errors (HTTP 429 or related
&quot;Too Many Requests&quot; messages). It retries the call a specified number of times,
now using exponential backoff. You can also choose to cache responses if you do
not need fresh results each time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>call_llm_robust(
  config,
  messages,
  tries = 3,
  wait_seconds = 10,
  backoff_factor = 2,
  verbose = FALSE,
  json = FALSE,
  memoize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="call_llm_robust_+3A_config">config</code></td>
<td>
<p>An <code>llm_config</code> object from <code><a href="#topic+llm_config">llm_config</a></code>.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_messages">messages</code></td>
<td>
<p>A list of message objects (or character vector for embeddings).</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_tries">tries</code></td>
<td>
<p>Integer. Number of retries before giving up. Default is 3.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_wait_seconds">wait_seconds</code></td>
<td>
<p>Numeric. Initial wait time (seconds) before the first retry. Default is 10.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Numeric. Multiplier for wait time after each failure. Default is 2.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If TRUE, prints the full API response.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_json">json</code></td>
<td>
<p>Logical. If TRUE, returns the raw JSON as an attribute.</p>
</td></tr>
<tr><td><code id="call_llm_robust_+3A_memoize">memoize</code></td>
<td>
<p>Logical. If TRUE, calls are cached to avoid repeated identical requests. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The successful result from <code><a href="#topic+call_llm">call_llm</a></code>, or an error if all retries fail.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Basic usage:
  robust_resp &lt;- call_llm_robust(
    config = my_llm_config,
    messages = list(list(role = "user", content = "Hello, LLM!")),
    tries = 3,
    wait_seconds = 10,
    memoize = FALSE
  )
  cat("Response:", robust_resp, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_config'>Create LLM Configuration</h2><span id='topic+llm_config'></span>

<h3>Description</h3>

<p>Creates a configuration object for interacting with a specified LLM API provider.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_config(provider, model, api_key, troubleshooting = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_config_+3A_provider">provider</code></td>
<td>
<p>A string specifying the API provider. Supported providers include:
&quot;openai&quot; for OpenAI,
&quot;anthropic&quot; for Anthropic,
&quot;groq&quot; for Groq,
&quot;together&quot; for Together AI,
&quot;deepseek&quot; for DeepSeek,
&quot;voyage&quot; for Voyage AI.
&quot;gemini&quot; for Google Gemini.</p>
</td></tr>
<tr><td><code id="llm_config_+3A_model">model</code></td>
<td>
<p>The model name to use. This depends on the provider.</p>
</td></tr>
<tr><td><code id="llm_config_+3A_api_key">api_key</code></td>
<td>
<p>Your API key for the provider.</p>
</td></tr>
<tr><td><code id="llm_config_+3A_troubleshooting">troubleshooting</code></td>
<td>
<p>Prints out all api calls. USE WITH EXTREME CAUTION as it prints your API key.</p>
</td></tr>
<tr><td><code id="llm_config_+3A_...">...</code></td>
<td>
<p>Additional model-specific parameters (e.g., 'temperature', 'max_tokens', etc.).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 'llm_config' containing API and model parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # OpenAI Example (chat)
  openai_config &lt;- llm_config(
    provider = "openai",
    model = "gpt-4o-mini",
    api_key = Sys.getenv("OPENAI_KEY"),
    temperature = 0.7,
    max_tokens = 500
  )

  # OpenAI Embedding Example (overwriting api_url):
  openai_embed_config &lt;- llm_config(
    provider = "openai",
    model = "text-embedding-3-small",
    api_key = Sys.getenv("OPENAI_KEY"),
    temperature = 0.3,
    api_url = "https://api.openai.com/v1/embeddings"
  )

  text_input &lt;- c("Political science is a useful subject",
                  "We love sociology",
                  "German elections are different",
                  "A student was always curious.")

  embed_response &lt;- call_llm(openai_embed_config, text_input)
  # parse_embeddings() can then be used to convert the embedding results.

  # Voyage AI Example:
  voyage_config &lt;- llm_config(
    provider = "voyage",
    model = "voyage-large-2",
    api_key = Sys.getenv("VOYAGE_API_KEY")
  )

  embedding_response &lt;- call_llm(voyage_config, text_input)
  embeddings &lt;- parse_embeddings(embedding_response)
  # Additional processing:
  embeddings |&gt; cor() |&gt; print()

## End(Not run)
</code></pre>

<hr>
<h2 id='LLMConversation'>LLMConversation Class for Coordinating Agents</h2><span id='topic+LLMConversation'></span>

<h3>Description</h3>

<p>An R6 class for managing a conversation among multiple <code>Agent</code> objects.
Includes optional conversation-level summarization if 'summarizer_config' is provided:
</p>

<ol>
<li> <p><strong>summarizer_config:</strong> A list that can contain:
</p>

<ul>
<li> <p><code>llm_config</code>: The <code>llm_config</code> used for the summarizer call (default a basic OpenAI).
</p>
</li>
<li> <p><code>prompt</code>: A custom summarizer prompt (default provided).
</p>
</li>
<li> <p><code>threshold</code>: Word-count threshold (default 3000 words).
</p>
</li>
<li> <p><code>summary_length</code>: Target length in words for the summary (default 400).
</p>
</li></ul>

</li>
<li><p> Once the total conversation word count exceeds 'threshold', a summarization is triggered.
</p>
</li>
<li><p> The conversation is replaced with a single condensed message that keeps track of who said what.
</p>
</li></ol>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>agents</code></dt><dd><p>A named list of <code>Agent</code> objects.</p>
</dd>
<dt><code>conversation_history</code></dt><dd><p>A list of speaker/text pairs for the entire conversation.</p>
</dd>
<dt><code>conversation_history_full</code></dt><dd><p>A list of speaker/text pairs for the entire conversation that is never modified and never used directly.</p>
</dd>
<dt><code>topic</code></dt><dd><p>A short string describing the conversation's theme.</p>
</dd>
<dt><code>prompts</code></dt><dd><p>An optional list of prompt templates (may be ignored).</p>
</dd>
<dt><code>shared_memory</code></dt><dd><p>Global store that is also fed into each agent's memory.</p>
</dd>
<dt><code>last_response</code></dt><dd><p>last response received</p>
</dd>
<dt><code>total_tokens_sent</code></dt><dd><p>total tokens sent in conversation</p>
</dd>
<dt><code>total_tokens_received</code></dt><dd><p>total tokens received in conversation</p>
</dd>
<dt><code>summarizer_config</code></dt><dd><p>Config list controlling optional conversation-level summarization.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LLMConversation-new"><code>LLMConversation$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-add_agent"><code>LLMConversation$add_agent()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-add_message"><code>LLMConversation$add_message()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-converse"><code>LLMConversation$converse()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-run"><code>LLMConversation$run()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-print_history"><code>LLMConversation$print_history()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-reset_conversation"><code>LLMConversation$reset_conversation()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-%7C%3E"><code>LLMConversation$|&gt;()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-maybe_summarize_conversation"><code>LLMConversation$maybe_summarize_conversation()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-summarize_conversation"><code>LLMConversation$summarize_conversation()</code></a>
</p>
</li>
<li> <p><a href="#method-LLMConversation-clone"><code>LLMConversation$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-LLMConversation-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new conversation.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$new(topic, prompts = NULL, summarizer_config = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>topic</code></dt><dd><p>Character. The conversation topic.</p>
</dd>
<dt><code>prompts</code></dt><dd><p>Optional named list of prompt templates.</p>
</dd>
<dt><code>summarizer_config</code></dt><dd><p>Optional list controlling conversation-level summarization.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LLMConversation-add_agent"></a>



<h4>Method <code>add_agent()</code></h4>

<p>Add an <code>Agent</code> to this conversation. The agent is stored by <code>agent$id</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$add_agent(agent)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>agent</code></dt><dd><p>An Agent object.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LLMConversation-add_message"></a>



<h4>Method <code>add_message()</code></h4>

<p>Add a message to the global conversation log. Also appended to shared memory.
Then possibly trigger summarization if configured.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$add_message(speaker, text)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>speaker</code></dt><dd><p>Character. Who is speaking?</p>
</dd>
<dt><code>text</code></dt><dd><p>Character. What they said.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LLMConversation-converse"></a>



<h4>Method <code>converse()</code></h4>

<p>Have a specific agent produce a response. The entire global conversation plus
shared memory is temporarily loaded into that agent. Then the new message is
recorded in the conversation. The agent's memory is then reset except for its new line.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$converse(
  agent_id,
  prompt_template,
  replacements = list(),
  verbose = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>agent_id</code></dt><dd><p>Character. The ID of the agent to converse.</p>
</dd>
<dt><code>prompt_template</code></dt><dd><p>Character. The prompt template for the agent.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>A named list of placeholders to fill in the prompt.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints extra info.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LLMConversation-run"></a>



<h4>Method <code>run()</code></h4>

<p>Run a multi-step conversation among a sequence of agents.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$run(
  agent_sequence,
  prompt_template,
  replacements = list(),
  verbose = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>agent_sequence</code></dt><dd><p>Character vector of agent IDs in the order they speak.</p>
</dd>
<dt><code>prompt_template</code></dt><dd><p>Single string or named list of strings keyed by agent ID.</p>
</dd>
<dt><code>replacements</code></dt><dd><p>Single list or list-of-lists with per-agent placeholders.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Logical. If TRUE, prints extra info.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LLMConversation-print_history"></a>



<h4>Method <code>print_history()</code></h4>

<p>Print the conversation so far to the console.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$print_history()</pre></div>


<hr>
<a id="method-LLMConversation-reset_conversation"></a>



<h4>Method <code>reset_conversation()</code></h4>

<p>Clear the global conversation and reset all agents' memories.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$reset_conversation()</pre></div>


<hr>
<a id="method-LLMConversation-|>"></a>



<h4>Method <code>|&gt;()</code></h4>

<p>Pipe-like operator to chain conversation steps. E.g., conv |&gt; &quot;Solver&quot;(...)
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$|&gt;(agent_id)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>agent_id</code></dt><dd><p>Character. The ID of the agent to call next.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A function that expects (prompt_template, replacements, verbose).
</p>


<hr>
<a id="method-LLMConversation-maybe_summarize_conversation"></a>



<h4>Method <code>maybe_summarize_conversation()</code></h4>

<p>Possibly summarize the conversation if summarizer_config is non-null and
the word count of conversation_history exceeds summarizer_config$threshold.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$maybe_summarize_conversation()</pre></div>


<hr>
<a id="method-LLMConversation-summarize_conversation"></a>



<h4>Method <code>summarize_conversation()</code></h4>

<p>Summarize the conversation so far into one condensed message.
The new conversation history becomes a single message with speaker = &quot;summary&quot;.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$summarize_conversation()</pre></div>


<hr>
<a id="method-LLMConversation-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LLMConversation$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='log_llm_error'>Log LLMR Errors</h2><span id='topic+log_llm_error'></span>

<h3>Description</h3>

<p>Logs an error with a timestamp for troubleshooting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_llm_error(err)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_llm_error_+3A_err">err</code></td>
<td>
<p>An error object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns <code>NULL</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Example of logging an error by catching a failure:
  # Use a deliberately fake API key to force an error
  config_test &lt;- llm_config(
    provider = "openai",
    model = "gpt-3.5-turbo",
    api_key = "FAKE_KEY",
    temperature = 0.5,
    top_p = 1,
    max_tokens = 30
  )

  tryCatch(
    call_llm(config_test, list(list(role = "user", content = "Hello world!"))),
    error = function(e) log_llm_error(e)
  )

## End(Not run)
</code></pre>

<hr>
<h2 id='parse_embeddings'>Parse Embedding Response into a Numeric Matrix</h2><span id='topic+parse_embeddings'></span>

<h3>Description</h3>

<p>Converts the embedding response data to a numeric matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_embeddings(embedding_response)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parse_embeddings_+3A_embedding_response">embedding_response</code></td>
<td>
<p>The response returned from an embedding API call.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix of embeddings with column names as sequence numbers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  text_input &lt;- c("Political science is a useful subject",
                  "We love sociology",
                  "German elections are different",
                  "A student was always curious.")

  # Configure the embedding API provider (example with Voyage API)
  voyage_config &lt;- llm_config(
    provider = "voyage",
    model = "voyage-large-2",
    api_key = Sys.getenv("VOYAGE_API_KEY")
  )

  embedding_response &lt;- call_llm(voyage_config, text_input)
  embeddings &lt;- parse_embeddings(embedding_response)
  # Additional processing:
  embeddings |&gt; cor() |&gt; print()

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
