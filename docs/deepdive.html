<!DOCTYPE html><html><head><title>Help for package deepdive</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepdive}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#deepforest'><p>Build or train bagged deeptree or deepnet of multiple architecture</p></a></li>
<li><a href='#deepnet'><p>Build and train an Artificial Neural Network of any size</p></a></li>
<li><a href='#deeptree'><p>Descision Tree augmented by Artificial Neural Network</p></a></li>
<li><a href='#predict.deepforest'><p>Predict Function for DeepForest</p></a></li>
<li><a href='#predict.deepnet'><p>Predict Function for Deepnet</p></a></li>
<li><a href='#predict.deeptree'><p>Predict Function for Deeptree</p></a></li>
<li><a href='#variableImportance'><p>Variable importance for models in this library</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Learning for General Purpose</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Author:</td>
<td>Rajesh Balakrishnan</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rajesh Balakirshnan &lt;rajeshbalakrishnan24@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Aims to provide simple intuitive functions to create quick prototypes of artificial neural network or deep learning models. In addition novel ensemble models like 'deeptree' and 'deepforest' has been included which combines decision trees and neural network.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>fastDummies,plyr,rpart,treeClust,data.table,stringr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://rajeshb24.github.io/deepdive/">https://rajeshb24.github.io/deepdive/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-09 22:02:42 UTC; brajesh</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-10 16:30:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='deepforest'>Build or train bagged deeptree or deepnet of multiple architecture</h2><span id='topic+deepforest'></span>

<h3>Description</h3>

<p>Build or train bagged deeptree or deepnet of multiple architecture.Based on error choice either select best model or average multiple model with random variable cut,data cut and architechture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepforest(
  x,
  y,
  networkCount = 3,
  layerChoice = c(2:3),
  unitsChoice = c(4:10),
  cutVarSizePercent = 0.6,
  cutDataSizePercent = 0.6,
  activation = c("sigmoid", "sigmoid"),
  reluLeak = 0,
  modelType = "regress",
  iterations = 500,
  eta = 10^-2,
  seed = 2,
  gradientClip = 0.8,
  regularisePar = 0,
  optimiser = "adam",
  parMomentum = 0.9,
  inputSizeImpact = 1,
  parRmsPropZeroAdjust = 10^-8,
  parRmsProp = 0.9999,
  treeLeaves = NA,
  treeMinSplitPercent = 0.3,
  treeMinSplitCount = 100,
  treeCp = 0.01,
  errorCover = 0.2,
  treeAugment = TRUE,
  printItrSize = 100,
  showProgress = TRUE,
  stopError = 0.01,
  miniBatchSize = NA,
  useBatchProgress = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepforest_+3A_x">x</code></td>
<td>
<p>a data frame with input variables</p>
</td></tr>
<tr><td><code id="deepforest_+3A_y">y</code></td>
<td>
<p>a data frame with ouptut variable</p>
</td></tr>
<tr><td><code id="deepforest_+3A_networkcount">networkCount</code></td>
<td>
<p>Integer, Number of deepnet or deeptree to build</p>
</td></tr>
<tr><td><code id="deepforest_+3A_layerchoice">layerChoice</code></td>
<td>
<p>vector, different layer choices</p>
</td></tr>
<tr><td><code id="deepforest_+3A_unitschoice">unitsChoice</code></td>
<td>
<p>vector , number of units choice</p>
</td></tr>
<tr><td><code id="deepforest_+3A_cutvarsizepercent">cutVarSizePercent</code></td>
<td>
<p>ratio, percentage of variable to for each network</p>
</td></tr>
<tr><td><code id="deepforest_+3A_cutdatasizepercent">cutDataSizePercent</code></td>
<td>
<p>ratio, percentage of data to for each network</p>
</td></tr>
<tr><td><code id="deepforest_+3A_activation">activation</code></td>
<td>
<p>choose from &quot;sigmoid&quot;,&quot;relu&quot;,&quot;sin&quot;,&quot;cos&quot;,&quot;none&quot;.Activations will be randomly chosen from chosen. Default is relu and sin</p>
</td></tr>
<tr><td><code id="deepforest_+3A_reluleak">reluLeak</code></td>
<td>
<p>numeric. Applicable when activation is &quot;relu&quot;. Specify value between 0 any number close to zero below 1. Eg: 0.01,0.001 etc</p>
</td></tr>
<tr><td><code id="deepforest_+3A_modeltype">modelType</code></td>
<td>
<p>one of &quot;regress&quot;,&quot;binary&quot;,&quot;multiClass&quot;. &quot;regress&quot; for regression will create a linear single unit output layer. &quot;binary&quot; will create a single unit sigmoid activated layer. &quot;multiClass&quot; will create layer with units corresponding to number of output classes with softmax activation.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_iterations">iterations</code></td>
<td>
<p>integer. This indicates number of iteratios or epochs in backpropagtion .The default value is 500.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_eta">eta</code></td>
<td>
<p>numeric.Hyperparameter,sets the Learning rate for backpropagation. Eta determines the convergence ability and speed of convergence.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_seed">seed</code></td>
<td>
<p>numeric. Set seed with this parameter. Incase of sin activation sometimes changing seed can yeild better results. Default is 2</p>
</td></tr>
<tr><td><code id="deepforest_+3A_gradientclip">gradientClip</code></td>
<td>
<p>numeric. Hyperparameter numeric value which limits gradient size for weight update operation in backpropagation. Default is 0.8 . It can take any postive value.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_regularisepar">regularisePar</code></td>
<td>
<p>numeric. L2 Regularisation Parameter .</p>
</td></tr>
<tr><td><code id="deepforest_+3A_optimiser">optimiser</code></td>
<td>
<p>one of &quot;gradientDescent&quot;,&quot;momentum&quot;,&quot;rmsProp&quot;,&quot;adam&quot;. Default value &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepforest_+3A_parmomentum">parMomentum</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;mometum&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepforest_+3A_inputsizeimpact">inputSizeImpact</code></td>
<td>
<p>numeric. Adjusts the gradient size by factor of percentage of rows in input. For very small data set setting this to 0 could yeild faster result. Default is 1.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_parrmspropzeroadjust">parRmsPropZeroAdjust</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepforest_+3A_parrmsprop">parRmsProp</code></td>
<td>
<p>numeric.Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepforest_+3A_treeleaves">treeLeaves</code></td>
<td>
<p>vector.Optional , leaves numbers from externally trained tree model can be supplied here. If supplied then model will not build a explicit tree and just fit a neural network to mentioned leaves.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_treeminsplitpercent">treeMinSplitPercent</code></td>
<td>
<p>numeric. This parameter controls depth of tree setting min split count for leaf subdivision as percentage of observations. Final minimum split will be chosen as max of count calculted with treeMinSplitPercent and treeMinSplitCount. Default 0.3. Range 0 to 1.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_treeminsplitcount">treeMinSplitCount</code></td>
<td>
<p>numeric. This parameter controls depth of tree setting min split count.Final minimum split will be chosen as max of count calculted with treeMinSplitPercent and treeMinSplitCount. Default 30</p>
</td></tr>
<tr><td><code id="deepforest_+3A_treecp">treeCp</code></td>
<td>
<p>complexity parameter. <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code></p>
</td></tr>
<tr><td><code id="deepforest_+3A_errorcover">errorCover</code></td>
<td>
<p>Ratio. Deault is 0.2 i.e all models within 20 percent error of best model will be selected.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_treeaugment">treeAugment</code></td>
<td>
<p>logical. If True fits deeptree and if False fits deepnet. Default is T</p>
</td></tr>
<tr><td><code id="deepforest_+3A_printitrsize">printItrSize</code></td>
<td>
<p>numeric. Number of iterations after which progress message should be shown. Default value 100 and for iterations below 100 atleast 5 messages will be seen</p>
</td></tr>
<tr><td><code id="deepforest_+3A_showprogress">showProgress</code></td>
<td>
<p>logical. True will show progress and F will not show progress</p>
</td></tr>
<tr><td><code id="deepforest_+3A_stoperror">stopError</code></td>
<td>
<p>Numeric. Rmse at which iterations can be stopped. Default is 0.01, can be set as NA in case all iterations needs to run.</p>
</td></tr>
<tr><td><code id="deepforest_+3A_minibatchsize">miniBatchSize</code></td>
<td>
<p>integer. Set the mini batch size for mini batch gradient</p>
</td></tr>
<tr><td><code id="deepforest_+3A_usebatchprogress">useBatchProgress</code></td>
<td>
<p>logical. Applicable for miniBatch , setting T will use show rmse in Batch and F will show error on full dataset. For large dataset set T</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns model object which can be passed into <code><a href="#topic+predict.deepforest">predict.deepforest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(deepdive)

x&lt;-data.frame(x1=runif(10),x2=runif(10))
y&lt;-data.frame(y=10*x$x1+20*x$x2+20)

mdeepf&lt;-deepforest(x,y,
                  networkCount=2,
                  layerChoice=c(2:3),
                  unitsChoice=c(4:10),
                  cutVarSizePercent=0.6,
                  cutDataSizePercent=0.6,
                  activation = c('relu',"sin"),
                  reluLeak=0.01,
                  modelType ='regress',
                  iterations = 10,
                  eta = 10 ^-2,
                  seed=2,
                  gradientClip=0.8,
                  regularisePar=0,
                  optimiser="adam",
                  parMomentum=0.9,
                  inputSizeImpact=1,
                  parRmsPropZeroAdjust=10^-8,
                  parRmsProp=0.9999,
                  treeLeaves=NA,
                  treeMinSplitPercent=0.3,
                  treeMinSplitCount=100,
                  treeCp=0.01 ,
                  errorCover=0.2,
                  treeAugment=TRUE,
                  printItrSize=100,
                  showProgress=TRUE,
                  stopError=0.01,
                  miniBatchSize=64,
                  useBatchProgress=TRUE)
</code></pre>

<hr>
<h2 id='deepnet'>Build and train an Artificial Neural Network of any size</h2><span id='topic+deepnet'></span>

<h3>Description</h3>

<p>Build and train Artifical Neural Network of any depth in a single line code. Choose the hyperparameters to improve the accuracy or generalisation of model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepnet(
  x,
  y,
  hiddenLayerUnits = c(2, 2),
  activation = c("sigmoid", "relu"),
  reluLeak = 0,
  modelType = c("regress"),
  iterations = 500,
  eta = 10^-2,
  seed = 2,
  gradientClip = 0.8,
  regularisePar = 0,
  optimiser = "adam",
  parMomentum = 0.9,
  inputSizeImpact = 1,
  parRmsPropZeroAdjust = 10^-8,
  parRmsProp = 0.9999,
  printItrSize = 100,
  showProgress = TRUE,
  stopError = 0.01,
  miniBatchSize = NA,
  useBatchProgress = FALSE,
  ignoreNAerror = FALSE,
  normalise = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepnet_+3A_x">x</code></td>
<td>
<p>a data frame with input variables</p>
</td></tr>
<tr><td><code id="deepnet_+3A_y">y</code></td>
<td>
<p>a data frame with ouptut variable</p>
</td></tr>
<tr><td><code id="deepnet_+3A_hiddenlayerunits">hiddenLayerUnits</code></td>
<td>
<p>a numeric vector, length of vector indicates number of hidden layers and each element in vector indicates corresponding hidden units Eg: c(6,4) for two layers, one with 6 hiiden units and other with 4 hidden units. Note: Output layer is automatically created.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_activation">activation</code></td>
<td>
<p>one of &quot;sigmoid&quot;,&quot;relu&quot;,&quot;sin&quot;,&quot;cos&quot;,&quot;none&quot;. The default is &quot;sigmoid&quot;. Choose a activation per hidden layer</p>
</td></tr>
<tr><td><code id="deepnet_+3A_reluleak">reluLeak</code></td>
<td>
<p>numeric. Applicable when activation is &quot;relu&quot;. Specify value between 0 any number close to zero below 1. Eg: 0.01,0.001 etc</p>
</td></tr>
<tr><td><code id="deepnet_+3A_modeltype">modelType</code></td>
<td>
<p>one of &quot;regress&quot;,&quot;binary&quot;,&quot;multiClass&quot;. &quot;regress&quot; for regression will create a linear single unit output layer. &quot;binary&quot; will create a single unit sigmoid activated layer. &quot;multiClass&quot; will create layer with units corresponding to number of output classes with softmax activation.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_iterations">iterations</code></td>
<td>
<p>integer. This indicates number of iteratios or epochs in backpropagtion .The default value is 500.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_eta">eta</code></td>
<td>
<p>numeric.Hyperparameter,sets the Learning rate for backpropagation. Eta determines the convergence ability and speed of convergence.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_seed">seed</code></td>
<td>
<p>numeric. Set seed with this parameter. Incase of sin activation sometimes changing seed can yeild better results. Default is 2</p>
</td></tr>
<tr><td><code id="deepnet_+3A_gradientclip">gradientClip</code></td>
<td>
<p>numeric. Hyperparameter numeric value which limits gradient size for weight update operation in backpropagation. Default is 0.8 . It can take any postive value.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_regularisepar">regularisePar</code></td>
<td>
<p>numeric. L2 Regularisation Parameter .</p>
</td></tr>
<tr><td><code id="deepnet_+3A_optimiser">optimiser</code></td>
<td>
<p>one of &quot;gradientDescent&quot;,&quot;momentum&quot;,&quot;rmsProp&quot;,&quot;adam&quot;. Default value &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepnet_+3A_parmomentum">parMomentum</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;mometum&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepnet_+3A_inputsizeimpact">inputSizeImpact</code></td>
<td>
<p>numeric. Adjusts the gradient size by factor of percentage of rows in input. For very small data set setting this to 0 could yeild faster result. Default is 1.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_parrmspropzeroadjust">parRmsPropZeroAdjust</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepnet_+3A_parrmsprop">parRmsProp</code></td>
<td>
<p>numeric.Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deepnet_+3A_printitrsize">printItrSize</code></td>
<td>
<p>numeric. Number of iterations after which progress message should be shown. Default value 100 and for iterations below 100 atleast 5 messages will be seen</p>
</td></tr>
<tr><td><code id="deepnet_+3A_showprogress">showProgress</code></td>
<td>
<p>logical. True will show progress and F will not show progress</p>
</td></tr>
<tr><td><code id="deepnet_+3A_stoperror">stopError</code></td>
<td>
<p>Numeric. Rmse at which iterations can be stopped. Default is 0.01, can be set as NA in case all iterations needs to run.</p>
</td></tr>
<tr><td><code id="deepnet_+3A_minibatchsize">miniBatchSize</code></td>
<td>
<p>integer. Set the mini batch size for mini batch gradient</p>
</td></tr>
<tr><td><code id="deepnet_+3A_usebatchprogress">useBatchProgress</code></td>
<td>
<p>logical. Applicable for miniBatch , setting T will use show rmse in Batch and F will show error on full dataset. For large dataset set T</p>
</td></tr>
<tr><td><code id="deepnet_+3A_ignorenaerror">ignoreNAerror</code></td>
<td>
<p>logical. Set T if iteration needs to be stopped when predictions become NA</p>
</td></tr>
<tr><td><code id="deepnet_+3A_normalise">normalise</code></td>
<td>
<p>logical. Set F if normalisation not required.Default T</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns model object which can be passed into <code><a href="#topic+predict.deepnet">predict.deepnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(deepdive)

x &lt;- data.frame(x1 = runif(10),x2 = runif(10))
y&lt;- data.frame(y=20*x$x1 +30*x$x2+10)

#train
modelnet&lt;-deepnet(x,y,c(2,2),
activation = c('relu',"sigmoid"),
reluLeak = 0.01,
modelType = "regress",
iterations =5,
eta=0.8,
optimiser="adam")

#predict
predDeepNet&lt;-predict.deepnet(modelnet,newData=x)

#evaluate
sqrt(mean((predDeepNet$ypred-y$y)^2))


</code></pre>

<hr>
<h2 id='deeptree'>Descision Tree augmented by Artificial Neural Network</h2><span id='topic+deeptree'></span>

<h3>Description</h3>

<p>This models divides the input space by fitting a tree followed by artificial neural network to each of leaf. Decision tree model is built using rpart package and neural network using deepdive.Feature of stacking predictions from other models is also made available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deeptree(
  x,
  y,
  hiddenLayerUnits = c(2, 2),
  activation = c("sigmoid", "sigmoid"),
  reluLeak = 0,
  modelType = "regress",
  iterations = 500,
  eta = 10^-2,
  seed = 2,
  gradientClip = 0.8,
  regularisePar = 0,
  optimiser = "adam",
  parMomentum = 0.9,
  inputSizeImpact = 1,
  parRmsPropZeroAdjust = 10^-8,
  parRmsProp = 0.9999,
  treeLeaves = NA,
  treeMinSplitPercent = 0.3,
  treeMinSplitCount = 30,
  treeCp = 0.01,
  stackPred = NA,
  printItrSize = 100,
  showProgress = TRUE,
  stopError = 0.01,
  miniBatchSize = NA,
  useBatchProgress = TRUE,
  ignoreNAerror = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deeptree_+3A_x">x</code></td>
<td>
<p>a data frame with input variables</p>
</td></tr>
<tr><td><code id="deeptree_+3A_y">y</code></td>
<td>
<p>a data frame with ouptut variable</p>
</td></tr>
<tr><td><code id="deeptree_+3A_hiddenlayerunits">hiddenLayerUnits</code></td>
<td>
<p>a numeric vector, length of vector indicates number of hidden layers and each element in vector indicates corresponding hidden units Eg: c(6,4) for two layers, one with 6 hiiden units and other with 4 hidden units. Note: Output layer is automatically created.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_activation">activation</code></td>
<td>
<p>one of &quot;sigmoid&quot;,&quot;relu&quot;,&quot;sin&quot;,&quot;cos&quot;,&quot;none&quot;. The default is &quot;sigmoid&quot;. Choose a activation per hidden layer</p>
</td></tr>
<tr><td><code id="deeptree_+3A_reluleak">reluLeak</code></td>
<td>
<p>numeric. Applicable when activation is &quot;relu&quot;. Specify value between 0 any number close to zero below 1. Eg: 0.01,0.001 etc</p>
</td></tr>
<tr><td><code id="deeptree_+3A_modeltype">modelType</code></td>
<td>
<p>one of &quot;regress&quot;,&quot;binary&quot;,&quot;multiClass&quot;. &quot;regress&quot; for regression will create a linear single unit output layer. &quot;binary&quot; will create a single unit sigmoid activated layer. &quot;multiClass&quot; will create layer with units corresponding to number of output classes with softmax activation.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_iterations">iterations</code></td>
<td>
<p>integer. This indicates number of iteratios or epochs in backpropagtion .The default value is 500.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_eta">eta</code></td>
<td>
<p>numeric.Hyperparameter,sets the Learning rate for backpropagation. Eta determines the convergence ability and speed of convergence.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_seed">seed</code></td>
<td>
<p>numeric. Set seed with this parameter. Incase of sin activation sometimes changing seed can yeild better results. Default is 2</p>
</td></tr>
<tr><td><code id="deeptree_+3A_gradientclip">gradientClip</code></td>
<td>
<p>numeric. Hyperparameter numeric value which limits gradient size for weight update operation in backpropagation. Default is 0.8 . It can take any postive value.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_regularisepar">regularisePar</code></td>
<td>
<p>numeric. L2 Regularisation Parameter .</p>
</td></tr>
<tr><td><code id="deeptree_+3A_optimiser">optimiser</code></td>
<td>
<p>one of &quot;gradientDescent&quot;,&quot;momentum&quot;,&quot;rmsProp&quot;,&quot;adam&quot;. Default value &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deeptree_+3A_parmomentum">parMomentum</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;mometum&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deeptree_+3A_inputsizeimpact">inputSizeImpact</code></td>
<td>
<p>numeric. Adjusts the gradient size by factor of percentage of rows in input. For very small data set setting this to 0 could yeild faster result. Default is 1.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_parrmspropzeroadjust">parRmsPropZeroAdjust</code></td>
<td>
<p>numeric. Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deeptree_+3A_parrmsprop">parRmsProp</code></td>
<td>
<p>numeric.Applicable for optimiser &quot;rmsProp&quot; and &quot;adam&quot;</p>
</td></tr>
<tr><td><code id="deeptree_+3A_treeleaves">treeLeaves</code></td>
<td>
<p>vector.Optional , leaves numbers from externally trained tree model can be supplied here. If supplied then model will not build a explicit tree and just fit a neural network to mentioned leaves.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_treeminsplitpercent">treeMinSplitPercent</code></td>
<td>
<p>numeric. This parameter controls depth of tree setting min split count for leaf subdivision as percentage of observations. Final minimum split will be chosen as max of count calculted with treeMinSplitPercent and treeMinSplitCount. Default 0.3. Range 0 to 1.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_treeminsplitcount">treeMinSplitCount</code></td>
<td>
<p>numeric. This parameter controls depth of tree setting min split count.Final minimum split will be chosen as max of count calculted with treeMinSplitPercent and treeMinSplitCount. Default 30</p>
</td></tr>
<tr><td><code id="deeptree_+3A_treecp">treeCp</code></td>
<td>
<p>complexity parameter. <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code></p>
</td></tr>
<tr><td><code id="deeptree_+3A_stackpred">stackPred</code></td>
<td>
<p>vector.Predictions from buildnet or other models can be supplied here. If for certain leaf stackPrep accuracy is better then stackpred predictions will be chosen.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_printitrsize">printItrSize</code></td>
<td>
<p>numeric. Number of iterations after which progress message should be shown. Default value 100 and for iterations below 100 atleast 5 messages will be seen</p>
</td></tr>
<tr><td><code id="deeptree_+3A_showprogress">showProgress</code></td>
<td>
<p>logical. True will show progress and F will not show progress</p>
</td></tr>
<tr><td><code id="deeptree_+3A_stoperror">stopError</code></td>
<td>
<p>Numeric. Rmse at which iterations can be stopped. Default is 0.01, can be set as NA in case all iterations needs to run.</p>
</td></tr>
<tr><td><code id="deeptree_+3A_minibatchsize">miniBatchSize</code></td>
<td>
<p>integer. Set the mini batch size for mini batch gradient</p>
</td></tr>
<tr><td><code id="deeptree_+3A_usebatchprogress">useBatchProgress</code></td>
<td>
<p>logical. Applicable for miniBatch , setting T will use show rmse in Batch and F will show error on full dataset. For large dataset set T</p>
</td></tr>
<tr><td><code id="deeptree_+3A_ignorenaerror">ignoreNAerror</code></td>
<td>
<p>logical. Set T if iteration needs to be stopped when predictions become NA</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns model object which can be passed into <code><a href="#topic+predict.deeptree">predict.deeptree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(deepdive)

x &lt;- data.frame(x1 = runif(10),x2 = runif(10))

y&lt;- data.frame(y=20*x$x1 +30* x$x2 +10)

deepTreeMod&lt;-deeptree(x,
y,
hiddenLayerUnits=c(4,4),
activation = c('relu',"sin"),
reluLeak=0.01,
modelType ='regress',
iterations = 1000,
eta = 0.4,
seed=2,
gradientClip=0.8,
regularisePar=0,
optimiser="adam",
parMomentum=0.9,
inputSizeImpact=1,
parRmsPropZeroAdjust=10^-8,
parRmsProp=0.9999,
treeLeaves=NA,
treeMinSplitPercent=0.4,
treeMinSplitCount=100,
stackPred =NA,
stopError=4,
miniBatchSize=64,
useBatchProgress=TRUE,
ignoreNAerror=FALSE)

</code></pre>

<hr>
<h2 id='predict.deepforest'>Predict Function for DeepForest</h2><span id='topic+predict.deepforest'></span>

<h3>Description</h3>

<p>Predict Function for DeepForest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepforest'
predict(object, newData, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.deepforest_+3A_object">object</code></td>
<td>
<p>deepforest model object</p>
</td></tr>
<tr><td><code id="predict.deepforest_+3A_newdata">newData</code></td>
<td>
<p>pass dataframe for prediction</p>
</td></tr>
<tr><td><code id="predict.deepforest_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns predictions vector or dataframe
</p>

<hr>
<h2 id='predict.deepnet'>Predict Function for Deepnet</h2><span id='topic+predict.deepnet'></span>

<h3>Description</h3>

<p>Predict Function for Deepnet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepnet'
predict(object, newData, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.deepnet_+3A_object">object</code></td>
<td>
<p>deepnet model object</p>
</td></tr>
<tr><td><code id="predict.deepnet_+3A_newdata">newData</code></td>
<td>
<p>pass dataframe for prediction</p>
</td></tr>
<tr><td><code id="predict.deepnet_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns predictions vector or dataframe
</p>

<hr>
<h2 id='predict.deeptree'>Predict Function for Deeptree</h2><span id='topic+predict.deeptree'></span>

<h3>Description</h3>

<p>Predict Function for Deeptree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deeptree'
predict(object, newData, treeLeaves = NA, stackPred = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.deeptree_+3A_object">object</code></td>
<td>
<p>deeptree model object</p>
</td></tr>
<tr><td><code id="predict.deeptree_+3A_newdata">newData</code></td>
<td>
<p>pass dataframe for prediction</p>
</td></tr>
<tr><td><code id="predict.deeptree_+3A_treeleaves">treeLeaves</code></td>
<td>
<p>Pass vector with tree leaves if fit outside deeptree. default NA.</p>
</td></tr>
<tr><td><code id="predict.deeptree_+3A_stackpred">stackPred</code></td>
<td>
<p>Pass stackPred of prediction data if it was passed in deeptree</p>
</td></tr>
<tr><td><code id="predict.deeptree_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns predictions vector or dataframe
</p>

<hr>
<h2 id='variableImportance'>Variable importance for models in this library</h2><span id='topic+variableImportance'></span>

<h3>Description</h3>

<p>Variable importance for models in this library
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variableImportance(model, x, y, showPlot = T, seed = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variableImportance_+3A_model">model</code></td>
<td>
<p>Model object</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_x">x</code></td>
<td>
<p>a data frame with input variables</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_y">y</code></td>
<td>
<p>a data frame with ouptut variable</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_showplot">showPlot</code></td>
<td>
<p>logical. True will show importance plot. Default True</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_seed">seed</code></td>
<td>
<p>Set seed with this parameter. Incase of sin activation sometimes changing seed can yeild better results. Default is 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns variable importance data frame
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
