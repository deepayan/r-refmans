<!DOCTYPE html><html><head><title>Help for package vip</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {vip}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gen_friedman'><p>Friedman benchmark data</p></a></li>
<li><a href='#get_feature_names'><p>Extract feature names</p></a></li>
<li><a href='#list_metrics'><p>List metrics</p></a></li>
<li><a href='#titanic'><p>Survival of Titanic passengers</p></a></li>
<li><a href='#titanic_mice'><p>Survival of Titanic passengers</p></a></li>
<li><a href='#vi'><p>Variable importance</p></a></li>
<li><a href='#vi_firm'><p>Variance-based variable importance</p></a></li>
<li><a href='#vi_model'><p>Model-specific variable importance</p></a></li>
<li><a href='#vi_permute'><p>Permutation-based variable importance</p></a></li>
<li><a href='#vi_shap'><p>SHAP-based variable importance</p></a></li>
<li><a href='#vip'><p>Variable importance plots</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Variable Importance Plots</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Description:</td>
<td>A general framework for constructing variable importance plots from 
  various types of machine learning models in R. Aside from some standard model-
  specific variable importance measures, this package also provides model-
  agnostic approaches that can be applied to any supervised learning algorithm.
  These include 1) an efficient permutation-based variable importance measure, 
  2) variable importance based on Shapley values (Strumbelj and Kononenko, 
  2014) &lt;<a href="https://doi.org/10.1007%2Fs10115-013-0679-x">doi:10.1007/s10115-013-0679-x</a>&gt;, and 3) the variance-based 
  approach described in Greenwell et al. (2018) &lt;<a href="https://arxiv.org/abs/1805.04755">arXiv:1805.04755</a>&gt;. A 
  variance-based method for quantifying the relative strength of interaction 
  effects is also included (see the previous reference for details).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/koalaverse/vip/">https://github.com/koalaverse/vip/</a>,
<a href="https://koalaverse.github.io/vip/">https://koalaverse.github.io/vip/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/koalaverse/vip/issues">https://github.com/koalaverse/vip/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>foreach, ggplot2 (&ge; 0.9.0), stats, tibble, utils, yardstick</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bookdown, DT, covr, doParallel, dplyr, fastshap (&ge; 0.1.0),
knitr, lattice, mlbench, modeldata, NeuralNetTools, pdp,
rmarkdown, tinytest (&ge; 1.4.1), varImp</td>
</tr>
<tr>
<td>Enhances:</td>
<td>C50, caret, Cubist, earth, gbm, glmnet, h2o, lightgbm,
mixOmics, mlr, mlr3, neuralnet, nnet, parsnip (&ge; 0.1.7),
party, partykit, pls, randomForest, ranger, rpart, RSNNS,
sparklyr (&ge; 0.8.0), tidymodels, workflows (&ge; 0.2.3), xgboost</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-18 21:16:45 UTC; bgreenwell</td>
</tr>
<tr>
<td>Author:</td>
<td>Brandon M. Greenwell
    <a href="https://orcid.org/0000-0002-8120-0084"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Brad Boehmke <a href="https://orcid.org/0000-0002-3611-8516"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brandon M. Greenwell &lt;greenwell.brandon@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-21 09:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gen_friedman'>Friedman benchmark data</h2><span id='topic+gen_friedman'></span>

<h3>Description</h3>

<p>Simulate data from the Friedman 1 benchmark problem. These data were
originally described in Friedman (1991) and Breiman (1996). For details, see
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html">sklearn.datasets.make_friedman1</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_friedman(
  n_samples = 100,
  n_features = 10,
  n_bins = NULL,
  sigma = 0.1,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_friedman_+3A_n_samples">n_samples</code></td>
<td>
<p>Integer specifying the number of samples (i.e., rows) to
generate. Default is 100.</p>
</td></tr>
<tr><td><code id="gen_friedman_+3A_n_features">n_features</code></td>
<td>
<p>Integer specifying the number of features to generate.
Default is 10.</p>
</td></tr>
<tr><td><code id="gen_friedman_+3A_n_bins">n_bins</code></td>
<td>
<p>Integer specifying the number of (roughly) equal sized bins to
split the response into. Default is <code>NULL</code> for no binning. Setting to
a positive integer &gt; 1 effectively turns this into a classification problem
where <code>n_bins</code> gives the number of classes.</p>
</td></tr>
<tr><td><code id="gen_friedman_+3A_sigma">sigma</code></td>
<td>
<p>Numeric specifying the standard deviation of the noise.</p>
</td></tr>
<tr><td><code id="gen_friedman_+3A_seed">seed</code></td>
<td>
<p>Integer specifying the random seed. If <code>NULL</code> (the default)
the results will be different each time the function is run.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24, pages 123-140.
</p>
<p>Friedman, Jerome H. (1991) Multivariate adaptive regression splines. The
Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gen_friedman()
</code></pre>

<hr>
<h2 id='get_feature_names'>Extract feature names</h2><span id='topic+get_feature_names'></span>

<h3>Description</h3>

<p>Extract predictor names from a fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_feature_names(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_feature_names_+3A_object">object</code></td>
<td>
<p>An appropriate fitted model object.</p>
</td></tr>
<tr><td><code id="get_feature_names_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.</p>
</td></tr>
</table>

<hr>
<h2 id='list_metrics'>List metrics</h2><span id='topic+list_metrics'></span>

<h3>Description</h3>

<p>List all available performance metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_metrics()
</code></pre>


<h3>Value</h3>

<p>A data frame with the following columns:
</p>

<ul>
<li> <p><code>metric</code> - the optimization or tuning metric;
</p>
</li>
<li> <p><code>description</code> - a brief description about the metric;
</p>
</li>
<li> <p><code>task</code> - whether the metric is suitable for regression or classification;
</p>
</li>
<li> <p><code>smaller_is_better</code> - logical indicating whether or not a smaller value of
the metric is considered better.
</p>
</li>
<li> <p><code>yardstick_function</code> - the name of the corresponding function from the
<a href="yardstick.html#topic+yardstick-package">yardstick</a> package.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>(metrics &lt;- list_metrics())
metrics[metrics$task == "Multiclass classification", ]
</code></pre>

<hr>
<h2 id='titanic'>Survival of Titanic passengers</h2><span id='topic+titanic'></span>

<h3>Description</h3>

<p>A data set containing the survival outcome, passenger class, age, sex, and
the number of family members for a large number of passengers aboard the
ill-fated Titanic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>titanic
</code></pre>


<h3>Format</h3>

<p>A data frame with 1309 observations on the following 6 variables:
</p>

<ul>
<li> <p><code>survived</code> - binary with levels <code>"yes"</code> for survived and <code>"no"</code>
otherwise;
</p>
</li>
<li> <p><code>pclass</code> - integer giving the corresponding passenger (i.e., ticket)
class with values 1&ndash;3;
</p>
</li>
<li> <p><code>age</code> - the age in years of the corresponding passenger (with 263
missing values);
</p>
</li>
<li> <p><code>age</code> - factor giving the sex of each passenger with levels
<code>"male"</code> and <code>"female"</code>;
</p>
</li>
<li> <p><code>sibsp</code> - integer giving the number of siblings/spouses aboard for each
passenger (ranges from 0&ndash;8);
</p>
</li>
<li> <p><code>parch</code> - integer giving the number of parents/children aboard for each
passenger (ranges from 0&ndash;9).
</p>
</li></ul>



<h3>Note</h3>

<p>As mentioned in the column description, <code>age</code> contains 263 <code>NA</code>s (or
missing values). For a complete version (or versions) of the data set, see
<a href="#topic+titanic_mice">titanic_mice</a>.
</p>


<h3>Source</h3>

<p><a href="https://hbiostat.org/data/">https://hbiostat.org/data/</a>.
</p>

<hr>
<h2 id='titanic_mice'>Survival of Titanic passengers</h2><span id='topic+titanic_mice'></span>

<h3>Description</h3>

<p>The <a href="#topic+titanic">titanic</a> data set contains 263 missing values (i.e., <code>NA</code>'s) in the
<code>age</code> column. This version of the data contains imputed values for the
<code>age</code> column using <em>multivariate imputation by chained equations</em> via
the <a href="https://cran.r-project.org/package=mice">mice</a> package. Consequently,
this is a list containing 11 imputed versions of the observations containd
in the <a href="#topic+titanic">titanic</a> data frame; each completed data sets has the same dimension
and column structure as <a href="#topic+titanic">titanic</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>titanic_mice
</code></pre>


<h3>Format</h3>

<p>An object of class <code>mild</code> (inherits from <code>list</code>) of length 21.
</p>


<h3>Source</h3>

<p>Greenwell, Brandon M. (2022). Tree-Based Methods for Statistical Learning in
R. CRC Press.
</p>

<hr>
<h2 id='vi'>Variable importance</h2><span id='topic+vi'></span><span id='topic+vi.default'></span>

<h3>Description</h3>

<p>Compute variable importance scores for the predictors in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi(object, ...)

## Default S3 method:
vi(
  object,
  method = c("model", "firm", "permute", "shap"),
  feature_names = NULL,
  abbreviate_feature_names = NULL,
  sort = TRUE,
  decreasing = TRUE,
  scale = FALSE,
  rank = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_+3A_object">object</code></td>
<td>
<p>A fitted model object (e.g., a
<a href="randomForest.html#topic+randomForest">randomForest</a> object) or an object that inherits
from class <code>"vi"</code>.</p>
</td></tr>
<tr><td><code id="vi_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed on to
<a href="#topic+vi_model">vi_model</a>, <a href="#topic+vi_firm">vi_firm</a>,
<a href="#topic+vi_permute">vi_permute</a>, or <a href="#topic+vi_shap">vi_shap</a>; see their
respective help pages for details.</p>
</td></tr>
<tr><td><code id="vi_+3A_method">method</code></td>
<td>
<p>Character string specifying the type of variable importance
(VI) to compute. Current options are:
</p>

<ul>
<li> <p><code>"model"</code> (the default), for model-specific VI scores (see
<a href="#topic+vi_model">vi_model</a> for details).
</p>
</li>
<li> <p><code>"firm"</code>, for variance-based VI scores (see <a href="#topic+vi_firm">vi_firm</a> for
details).
</p>
</li>
<li> <p><code>"permute"</code>, for permutation-based VI scores (see
<a href="#topic+vi_permute">vi_permute</a> for details).
</p>
</li>
<li> <p><code>"shap"</code>, for Shapley-based VI scores (see <a href="#topic+vi_shap">vi_shap</a> for
details).
</p>
</li></ul>
</td></tr>
<tr><td><code id="vi_+3A_feature_names">feature_names</code></td>
<td>
<p>Character string giving the names of the predictor
variables (i.e., features) of interest.</p>
</td></tr>
<tr><td><code id="vi_+3A_abbreviate_feature_names">abbreviate_feature_names</code></td>
<td>
<p>Integer specifying the length at which to
abbreviate feature names. Default is <code>NULL</code> which results in no
abbreviation (i.e., the full name of each feature will be printed).</p>
</td></tr>
<tr><td><code id="vi_+3A_sort">sort</code></td>
<td>
<p>Logical indicating whether or not to order the sort the variable
importance scores. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="vi_+3A_decreasing">decreasing</code></td>
<td>
<p>Logical indicating whether or not the variable importance
scores should be sorted in descending (<code>TRUE</code>) or ascending
(<code>FALSE</code>) order of importance. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="vi_+3A_scale">scale</code></td>
<td>
<p>Logical indicating whether or not to scale the variable
importance scores so that the largest is 100. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="vi_+3A_rank">rank</code></td>
<td>
<p>Logical indicating whether or not to rank the variable
importance scores (i.e., convert to integer ranks). Default is <code>FALSE</code>.
Potentially useful when comparing variable importance scores across different
models using different methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tidy data frame (i.e., a <a href="tibble.html#topic+tibble">tibble</a> object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as the average change in
performance after a random permutation (or permutations, if <code>nsim &gt; 1</code>) of
the feature in question.
</p>
</li></ul>

<p>For <a href="stats.html#topic+lm">lm</a>/<a href="stats.html#topic+glm">glm</a>-like objects, whenever
<code>method = "model"</code>, the sign (i.e., POS/NEG) of the original coefficient is
also included in a column called <code>Sign</code>.
</p>
<p>If <code>method = "permute"</code> and <code>nsim &gt; 1</code>, then an additional column (<code>StDev</code>)
containing the standard deviation of the individual permutation scores for
each feature is also returned; this helps assess the stability/variation of
the individual permutation importance for each feature.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#
# A projection pursuit regression example
#

# Load the sample data
data(mtcars)

# Fit a projection pursuit regression model
mtcars.ppr &lt;- ppr(mpg ~ ., data = mtcars, nterms = 1)

# Prediction wrapper that tells vi() how to obtain new predictions from your
# fitted model
pfun &lt;- function(object, newdata) predict(object, newdata = newdata)

# Compute permutation-based variable importance scores
set.seed(1434)  # for reproducibility
(vis &lt;- vi(mtcars.ppr, method = "permute", target = "mpg", nsim = 10,
           metric = "rmse", pred_wrapper = pfun, train = mtcars))

# Plot variable importance scores
vip(vis, include_type = TRUE, all_permutations = TRUE,
    geom = "point", aesthetics = list(color = "forestgreen", size = 3))

#
# A binary classification example
#
## Not run: 
library(rpart)  # for classification and regression trees

# Load Wisconsin breast cancer data; see ?mlbench::BreastCancer for details
data(BreastCancer, package = "mlbench")
bc &lt;- subset(BreastCancer, select = -Id)  # for brevity

# Fit a standard classification tree
set.seed(1032)  # for reproducibility
tree &lt;- rpart(Class ~ ., data = bc, cp = 0)

# Prune using 1-SE rule (e.g., use `plotcp(tree)` for guidance)
cp &lt;- tree$cptable
cp &lt;- cp[cp[, "nsplit"] == 2L, "CP"]
tree2 &lt;- prune(tree, cp = cp)  # tree with three splits

# Default tree-based VIP
vip(tree2)

# Computing permutation importance requires a prediction wrapper. For
# classification, the return value depends on the chosen metric; see
# `?vip::vi_permute` for details.
pfun &lt;- function(object, newdata) {
  # Need vector of predicted class probabilities when using  log-loss metric
  predict(object, newdata = newdata, type = "prob")[, "malignant"]
}

# Permutation-based importance (note that only the predictors that show up
# in the final tree have non-zero importance)
set.seed(1046)  # for reproducibility
vi(tree2, method = "permute", nsim = 10, target = "Class", train = bc,
   metric = "logloss", pred_wrapper = pfun, reference_class = "malignant")

# Equivalent (but not sorted)
set.seed(1046)  # for reproducibility
vi_permute(tree2, nsim = 10, target = "Class", metric = "logloss",
           pred_wrapper = pfun, reference_class = "malignant")

## End(Not run)
</code></pre>

<hr>
<h2 id='vi_firm'>Variance-based variable importance</h2><span id='topic+vi_firm'></span><span id='topic+vi_firm.default'></span>

<h3>Description</h3>

<p>Compute variance-based variable importance (VI) scores using a simple
<em>feature importance ranking measure</em> (FIRM) approach; for details, see
<a href="https://arxiv.org/abs/1805.04755">Greenwell et al. (2018)</a> and
<a href="https://arxiv.org/abs/1904.03959">Scholbeck et al. (2019)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_firm(object, ...)

## Default S3 method:
vi_firm(
  object,
  feature_names = NULL,
  train = NULL,
  var_fun = NULL,
  var_continuous = stats::sd,
  var_categorical = function(x) diff(range(x))/4,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_firm_+3A_object">object</code></td>
<td>
<p>A fitted model object (e.g., a
<a href="randomForest.html#topic+randomForest">randomForest</a> object).</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed on to the <code><a href="pdp.html#topic+partial">pdp::partial()</a></code>
function (e.g., <code>ice = TRUE</code>, <code>prob = TRUE</code>, or a prediction wrapper via the
<code>pred.fun</code> argument); see <code>?pdp::partial</code> for details on these and other
useful arguments.</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_feature_names">feature_names</code></td>
<td>
<p>Character string giving the names of the predictor
variables (i.e., features) of interest. If <code>NULL</code> (the default) then the
internal <code>get_feature_names()</code> function will be called to try and extract
them automatically. It is good practice to always specify this argument.</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_train">train</code></td>
<td>
<p>A matrix-like R object (e.g., a data frame or matrix)
containing the training data. If <code>NULL</code> (the default) then the
internal <code>get_training_data()</code> function will be called to try and extract it
automatically. It is good practice to always specify this argument.</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_var_fun">var_fun</code></td>
<td>
<p>Deprecated; use <code>var_continuous</code> and <code>var_categorical</code>
instead.</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_var_continuous">var_continuous</code></td>
<td>
<p>Function used to quantify the variability of effects
for continuous features. Defaults to using the sample standard deviation
(i.e., <code><a href="stats.html#topic+sd">stats::sd()</a></code>).</p>
</td></tr>
<tr><td><code id="vi_firm_+3A_var_categorical">var_categorical</code></td>
<td>
<p>Function used to quantify the variability of effects
for categorical features. Defaults to using the range divided by four; that
is, <code>function(x) diff(range(x)) / 4</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This approach is based on quantifying the relative &quot;flatness&quot; of the
effect of each feature and assumes the user has some familiarity with the
<code><a href="pdp.html#topic+partial">pdp::partial()</a></code> function. The  Feature effects can be assessed
using <em>partial dependence</em> (PD) plots (Friedman, 2001) or
<em>individual conditional expectation</em> (ICE) plots (Goldstein et al., 2014).
These methods are model-agnostic and can be applied to any supervised
learning algorithm. By default, relative &quot;flatness&quot; is defined by computing
the standard deviation of the y-axis values for each feature effect plot for
numeric features; for categorical features, the default is to use range
divided by 4. This can be changed via the <code>var_continuous</code> and
<code>var_categorical</code> arguments. See
<a href="https://arxiv.org/abs/1805.04755">Greenwell et al. (2018)</a> for details and
additional examples.
</p>


<h3>Value</h3>

<p>A tidy data frame (i.e., a <a href="tibble.html#topic+tibble">tibble</a> object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as described in
<a href="https://arxiv.org/abs/1805.04755">Greenwell et al. (2018)</a>.
</p>
</li></ul>



<h3>Note</h3>

<p>This approach can provide misleading results in the presence of
interaction effects (akin to interpreting main effect coefficients in a
linear with higher level interaction effects).
</p>


<h3>References</h3>

<p>J. H. Friedman. Greedy function approximation: A gradient boosting machine.
<em>Annals of Statistics</em>, <strong>29</strong>: 1189-1232, 2001.
</p>
<p>Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E., Peeking Inside the
Black Box: Visualizing Statistical Learning With Plots of Individual
Conditional Expectation. (2014) <em>Journal of Computational and Graphical
Statistics</em>, <strong>24</strong>(1): 44-65, 2015.
</p>
<p>Greenwell, B. M., Boehmke, B. C., and McCarthy, A. J. A Simple
and Effective Model-Based Variable Importance Measure. arXiv preprint
arXiv:1805.04755 (2018).
</p>
<p>Scholbeck, C. A. Scholbeck, and Molnar, C.,  and Heumann C., and Bischl, B.,
and Casalicchio, G. Sampling, Intervention, Prediction, Aggregation: A
Generalized Framework for Model-Agnostic Interpretations. arXiv preprint
arXiv:1904.03959 (2019).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#
# A projection pursuit regression example
#

# Load the sample data
data(mtcars)

# Fit a projection pursuit regression model
mtcars.ppr &lt;- ppr(mpg ~ ., data = mtcars, nterms = 1)

# Compute variable importance scores using the FIRM method; note that the pdp
# package knows how to work with a "ppr" object, so there's no need to pass
# the training data or a prediction wrapper, but it's good practice.
vi_firm(mtcars.ppr, train = mtcars)

# For unsopported models, need to define a prediction wrapper; this approach
# will work for ANY model (supported or unsupported, so better to just always
# define it pass it)
pfun &lt;- function(object, newdata) {
  # To use partial dependence, this function needs to return the AVERAGE
  # prediction (for ICE, simply omit the averaging step)
  mean(predict(object, newdata = newdata))
}

# Equivalent to the previous results (but would work if this type of model
# was not explicitly supported)
vi_firm(mtcars.ppr, pred.fun = pfun, train = mtcars)

# Equivalent VI scores, but the output is sorted by default
vi(mtcars.ppr, method = "firm")

# Use MAD to estimate variability of the partial dependence values
vi_firm(mtcars.ppr, var_continuous = stats::mad)

# Plot VI scores
vip(mtcars.ppr, method = "firm", train = mtcars, pred.fun = pfun)

## End(Not run)
</code></pre>

<hr>
<h2 id='vi_model'>Model-specific variable importance</h2><span id='topic+vi_model'></span><span id='topic+vi_model.default'></span><span id='topic+vi_model.C5.0'></span><span id='topic+vi_model.train'></span><span id='topic+vi_model.cubist'></span><span id='topic+vi_model.earth'></span><span id='topic+vi_model.gbm'></span><span id='topic+vi_model.glmnet'></span><span id='topic+vi_model.cv.glmnet'></span><span id='topic+vi_model.H2OBinomialModel'></span><span id='topic+vi_model.H2OMultinomialModel'></span><span id='topic+vi_model.H2ORegressionModel'></span><span id='topic+vi_model.lgb.Booster'></span><span id='topic+vi_model.mixo_pls'></span><span id='topic+vi_model.mixo_spls'></span><span id='topic+vi_model.WrappedModel'></span><span id='topic+vi_model.Learner'></span><span id='topic+vi_model.nn'></span><span id='topic+vi_model.nnet'></span><span id='topic+vi_model.RandomForest'></span><span id='topic+vi_model.constparty'></span><span id='topic+vi_model.cforest'></span><span id='topic+vi_model.mvr'></span><span id='topic+vi_model.randomForest'></span><span id='topic+vi_model.ranger'></span><span id='topic+vi_model.rpart'></span><span id='topic+vi_model.mlp'></span><span id='topic+vi_model.ml_model_decision_tree_regression'></span><span id='topic+vi_model.ml_model_decision_tree_classification'></span><span id='topic+vi_model.ml_model_gbt_regression'></span><span id='topic+vi_model.ml_model_gbt_classification'></span><span id='topic+vi_model.ml_model_generalized_linear_regression'></span><span id='topic+vi_model.ml_model_linear_regression'></span><span id='topic+vi_model.ml_model_random_forest_regression'></span><span id='topic+vi_model.ml_model_random_forest_classification'></span><span id='topic+vi_model.lm'></span><span id='topic+vi_model.model_fit'></span><span id='topic+vi_model.workflow'></span><span id='topic+vi_model.xgb.Booster'></span>

<h3>Description</h3>

<p>Compute model-specific variable importance scores for the predictors in a
fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_model(object, ...)

## Default S3 method:
vi_model(object, ...)

## S3 method for class 'C5.0'
vi_model(object, type = c("usage", "splits"), ...)

## S3 method for class 'train'
vi_model(object, ...)

## S3 method for class 'cubist'
vi_model(object, ...)

## S3 method for class 'earth'
vi_model(object, type = c("nsubsets", "rss", "gcv"), ...)

## S3 method for class 'gbm'
vi_model(object, type = c("relative.influence", "permutation"), ...)

## S3 method for class 'glmnet'
vi_model(object, lambda = NULL, ...)

## S3 method for class 'cv.glmnet'
vi_model(object, lambda = NULL, ...)

## S3 method for class 'H2OBinomialModel'
vi_model(object, ...)

## S3 method for class 'H2OMultinomialModel'
vi_model(object, ...)

## S3 method for class 'H2ORegressionModel'
vi_model(object, ...)

## S3 method for class 'lgb.Booster'
vi_model(object, type = c("gain", "cover", "frequency"), ...)

## S3 method for class 'mixo_pls'
vi_model(object, ncomp = NULL, ...)

## S3 method for class 'mixo_spls'
vi_model(object, ncomp = NULL, ...)

## S3 method for class 'WrappedModel'
vi_model(object, ...)

## S3 method for class 'Learner'
vi_model(object, ...)

## S3 method for class 'nn'
vi_model(object, type = c("olden", "garson"), ...)

## S3 method for class 'nnet'
vi_model(object, type = c("olden", "garson"), ...)

## S3 method for class 'RandomForest'
vi_model(object, type = c("accuracy", "auc"), ...)

## S3 method for class 'constparty'
vi_model(object, ...)

## S3 method for class 'cforest'
vi_model(object, ...)

## S3 method for class 'mvr'
vi_model(object, ...)

## S3 method for class 'mixo_pls'
vi_model(object, ncomp = NULL, ...)

## S3 method for class 'mixo_spls'
vi_model(object, ncomp = NULL, ...)

## S3 method for class 'WrappedModel'
vi_model(object, ...)

## S3 method for class 'Learner'
vi_model(object, ...)

## S3 method for class 'randomForest'
vi_model(object, ...)

## S3 method for class 'ranger'
vi_model(object, ...)

## S3 method for class 'rpart'
vi_model(object, ...)

## S3 method for class 'mlp'
vi_model(object, type = c("olden", "garson"), ...)

## S3 method for class 'ml_model_decision_tree_regression'
vi_model(object, ...)

## S3 method for class 'ml_model_decision_tree_classification'
vi_model(object, ...)

## S3 method for class 'ml_model_gbt_regression'
vi_model(object, ...)

## S3 method for class 'ml_model_gbt_classification'
vi_model(object, ...)

## S3 method for class 'ml_model_generalized_linear_regression'
vi_model(object, ...)

## S3 method for class 'ml_model_linear_regression'
vi_model(object, ...)

## S3 method for class 'ml_model_random_forest_regression'
vi_model(object, ...)

## S3 method for class 'ml_model_random_forest_classification'
vi_model(object, ...)

## S3 method for class 'lm'
vi_model(object, type = c("stat", "raw"), ...)

## S3 method for class 'model_fit'
vi_model(object, ...)

## S3 method for class 'workflow'
vi_model(object, ...)

## S3 method for class 'xgb.Booster'
vi_model(object, type = c("gain", "cover", "frequency"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_model_+3A_object">object</code></td>
<td>
<p>A fitted model object (e.g., a
<a href="randomForest.html#topic+randomForest">randomForest</a> object). See the details section
below to see how variable importance is computed for supported model types.</p>
</td></tr>
<tr><td><code id="vi_model_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed on to other methods.
See the details section below for arguments that can be passed to specific
object types.</p>
</td></tr>
<tr><td><code id="vi_model_+3A_type">type</code></td>
<td>
<p>Character string specifying the type of variable importance to
return (only used for some models). See the details section below for which
methods this argument applies to.</p>
</td></tr>
<tr><td><code id="vi_model_+3A_lambda">lambda</code></td>
<td>
<p>Numeric value for the penalty parameter of a
<a href="glmnet.html#topic+glmnet">glmnet</a> model (this is equivalent to the <code>s</code>
argument in <a href="glmnet.html#topic+predict.glmnet">coef.glmnet</a>). See the section on
<a href="glmnet.html#topic+glmnet">glmnet</a> in the details below.</p>
</td></tr>
<tr><td><code id="vi_model_+3A_ncomp">ncomp</code></td>
<td>
<p>An integer for the number of partial least squares components
to be used in the importance calculations. If more components are requested
than were used in the model, all of the model's components are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes model-specific variable importance scores depending on the class of
<code>object</code>:
</p>

<ul>
<li> <p><a href="C50.html#topic+C5.0">C5.0</a> - Variable importance is measured by determining
the percentage of training set samples that fall into all the terminal nodes
after the split. For example, the predictor in the first split automatically
has an importance measurement of 100 percent since all samples are affected
by this split. Other predictors may be used frequently in splits, but if the
terminal nodes cover only a handful of training set samples, the importance
scores may be close to zero. The same strategy is applied to rule-based
models and boosted versions of the model. The underlying function can also
return the number of times each predictor was involved in a split by using
the option <code>metric = "usage"</code>. See <a href="C50.html#topic+C5imp">C5imp</a> for
details.
</p>
</li>
<li> <p><a href="Cubist.html#topic+cubist.default">cubist</a> - The Cubist output contains variable usage
statistics. It gives the percentage of times where each variable was used in
a condition and/or a linear model. Note that this output will probably be
inconsistent with the rules shown in the output from summary.cubist. At each
split of the tree, Cubist saves a linear model (after feature selection) that
is allowed to have terms for each variable used in the current split or any
split above it. Quinlan (1992) discusses a smoothing algorithm where each
model prediction is a linear combination of the parent and child model along
the tree. As such, the final prediction is a function of all the linear
models from the initial node to the terminal node. The percentages shown in
the Cubist output reflects all the models involved in prediction (as opposed
to the terminal models shown in the output). The variable importance used
here is a linear combination of the usage in the rule conditions and the
model. See <a href="Cubist.html#topic+summary.cubist">summary.cubist</a> and
<a href="caret.html#topic+varImp">varImp</a> for details.
</p>
</li>
<li> <p><a href="glmnet.html#topic+glmnet">glmnet</a> - Similar to (generalized) linear models,
the absolute value of the coefficients are returned for a specific model.
It is important that the features  (and hence, the estimated coefficients) be
standardized prior to fitting the model. You can specify which coefficients
to return by passing the specific value of the penalty parameter via the
<code>lambda</code> argument (this is equivalent to the <code>s</code> argument in
<a href="glmnet.html#topic+predict.glmnet">coef.glmnet</a>). By default, <code>lambda = NULL</code> and the coefficients
corresponding to the final penalty value in the sequence are returned; in
other words, you should ALWAYS SPECIFY <code>lambda</code>! For <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a>
objects, the largest value of lambda such that the error is within one standard
error of the minimum is used by default. For a multinomial response, the
coefficients corresponding to the first class are used; that is, the first
component of <a href="glmnet.html#topic+predict.glmnet">coef.glmnet</a>.
</p>
</li>
<li> <p><a href="partykit.html#topic+cforest">cforest</a> - Variable importance is measured in a
way similar to those computed by <a href="randomForest.html#topic+importance">importance</a>.
Besides the standard version, a conditional version is available that
adjusts for correlations between predictor variables. If
<code>conditional = TRUE</code>, the importance of each variable is computed by
permuting within a grid defined by the predictors that are associated (with
1 - <em>p</em>-value greater than threshold) to the variable of interest. The
resulting variable importance score is conditional in the sense of beta
coefficients in regression models, but represents the effect of a variable in
both main effects and interactions. See Strobl et al. (2008) for details.
Note, however, that all random forest results are subject to random
variation. Thus, before interpreting the importance ranking, check whether
the same ranking is achieved with a different random seed - or otherwise
increase the number of trees ntree in <a href="partykit.html#topic+ctree_control">ctree_control</a>.
Note that in the presence of missings in the predictor variables the
procedure described in Hapfelmeier et al. (2012) is performed. See
<a href="partykit.html#topic+varimp">varimp</a> for details.
</p>
</li>
<li> <p><a href="earth.html#topic+earth">earth</a> - The <a href="earth.html#topic+earth">earth</a> package uses
three criteria for estimating the variable importance in a MARS model (see
<a href="earth.html#topic+evimp">evimp</a> for details):
</p>

<ul>
<li><p> The <code>nsubsets</code> criterion (<code>type = "nsubsets"</code>) counts the
number of model subsets that include each feature. Variables that are
included in more subsets are considered more important. This is the
criterion used by <a href="earth.html#topic+summary.earth">summary.earth</a> to print variable
importance. By &quot;subsets&quot; we mean the subsets of terms generated by
<code>earth()</code>'s backward pass. There is one subset for each model size
(from one to the size of the selected model) and the subset is the best set
of terms for that model size. (These subsets are specified in the
<code style="white-space: pre;">&#8288;$prune.terms&#8288;</code> component of <code>earth()</code>'s return value.) Only
subsets that are smaller than or equal in size to the final model are used
for estimating variable importance. This is the default method used by
<a href="#topic+vi_model">vi_model</a>.
</p>
</li>
<li><p> The <code>rss</code> criterion (<code>type = "rss"</code>) first calculates the
decrease in the RSS for each subset relative to the previous subset during
<code>earth()</code>’s backward pass. (For multiple response models, RSS's are
calculated over all responses.) Then for each variable it sums these
decreases over all subsets that include the variable. Finally, for ease of
interpretation the summed decreases are scaled so the largest summed
decrease is 100. Variables which cause larger net decreases in the RSS are
considered more important.
</p>
</li>
<li><p> The <code>gcv</code> criterion (<code>type = "gcv"</code>) is similar to the
<code>rss</code> approach, but uses the GCV statistic instead of the RSS. Note
that adding a variable can sometimes increase the GCV. (Adding the variable
has a deleterious effect on the model, as measured in terms of its
estimated predictive power on unseen data.) If that happens often enough,
the variable can have a negative total importance, and thus appear less
important than unused variables.
</p>
</li></ul>

</li>
<li> <p><a href="gbm.html#topic+gbm">gbm</a> - Variable importance is computed using one of
two approaches (See <a href="gbm.html#topic+summary.gbm">summary.gbm</a> for details):
</p>

<ul>
<li><p> The standard approach (<code>type = "relative.influence"</code>) described
in Friedman (2001). When <code>distribution = "gaussian"</code> this returns the
reduction of squared error attributable to each variable. For other loss
functions this returns the reduction attributable to each variable in sum
of squared error in predicting the gradient on each iteration. It describes
the <em>relative influence</em> of each variable in reducing the loss
function. This is the default method used by <a href="#topic+vi_model">vi_model</a>.
</p>
</li>
<li><p> An experimental permutation-based approach
(<code>type = "permutation"</code>). This method randomly permutes each predictor
variable at a time and computes the associated reduction in predictive
performance. This is similar to the variable importance measures Leo
Breiman uses for random forests, but <a href="gbm.html#topic+gbm">gbm</a> currently computes using
the entire training dataset (not the out-of-bag observations).
</p>
</li></ul>

</li>
<li> <p><a href="h2o.html#topic+H2OModel-class">H2OModel</a> - See <a href="h2o.html#topic+h2o.varimp">h2o.varimp</a> or visit
<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html</a>
for details.
</p>
</li>
<li> <p><a href="nnet.html#topic+nnet">nnet</a> - Two popular methods for constructing variable
importance scores with neural networks are the Garson algorithm
(Garson 1991), later modified by Goh (1995), and the Olden algorithm
(Olden et al. 2004). For both algorithms, the basis of these importance
scores is the network’s connection weights. The Garson algorithm determines
variable importance by identifying all weighted connections between the nodes
of interest. Olden’s algorithm, on the other hand, uses the product of the
raw connection weights between each input and output neuron and sums the
product across all hidden neurons. This has been shown to outperform the
Garson method in various simulations. For DNNs, a similar method due to
Gedeon (1997) considers the weights connecting the input features to the
first two hidden layers (for simplicity and speed); but this method can be
slow for large networks.. To implement the Olden and Garson algorithms, use
<code>type = "olden"</code> and <code>type = "garson"</code>, respectively. See
<a href="NeuralNetTools.html#topic+garson">garson</a> and <a href="NeuralNetTools.html#topic+olden">olden</a>
for details.
</p>
</li>
<li> <p><a href="stats.html#topic+lm">lm</a>/<a href="stats.html#topic+glm">glm</a> - In (generalized) linear models,
variable importance is typically based on the absolute value of the
corresponding <em>t</em>-statistics (Bring, 1994). For such models, the sign of the
original coefficient is also returned. By default, <code>type = "stat"</code> is used;
however, if the inputs have been appropriately standardized then the raw
coefficients can be used with <code>type = "raw"</code>. Note that Bring (1994)
provides motivation for using the absolute value of the associated
<em>t</em>-statistics.
</p>
</li>
<li> <p><a href="sparklyr.html#topic+ml_feature_importances">sparklyr</a> - The Spark ML
library provides standard variable importance measures for tree-based methods
(e.g., random forests). See
<a href="sparklyr.html#topic+ml_feature_importances">ml_feature_importances</a> for details.
</p>
</li>
<li> <p><a href="randomForest.html#topic+randomForest">randomForest</a> Random forests typically
provide two measures of variable importance.
</p>

<ul>
<li><p> The first measure is computed from permuting out-of-bag (OOB) data: for
each tree, the prediction error on the OOB portion of the data is recorded
(error rate for classification and MSE for regression). Then the same is
done after permuting each predictor variable. The difference between the
two are then averaged over all trees in the forest, and normalized by the
standard deviation of the differences. If the standard deviation of the
differences is equal to 0 for a variable, the division is not done (but the
average is almost always equal to 0 in that case).
</p>
</li>
<li><p> The second measure is the total decrease in node impurities from
splitting on the variable, averaged over all trees. For classification, the
node impurity is measured by the Gini index. For regression, it is measured
by residual sum of squares.
</p>
</li></ul>

<p>See <a href="randomForest.html#topic+importance">importance</a> for details, including
additional arguments that can be passed via the <code>...</code> argument in
<a href="#topic+vi_model">vi_model</a>.
</p>
</li>
<li> <p><a href="party.html#topic+cforest">cforest</a> - Same approach described in
<a href="partykit.html#topic+cforest">cforest</a> (from package <strong>partykit</strong>) above. See
<a href="party.html#topic+varimp">varimp</a> and <a href="party.html#topic+varimp">varimpAUC</a> (if <code>type = "auc"</code>)
for details.
</p>
</li>
<li> <p><a href="ranger.html#topic+ranger">ranger</a> - Variable importance for
<a href="ranger.html#topic+ranger">ranger</a> objects is computed in the usual way for random
forests. The approach used depends on the <code>importance</code> argument provided
in the initial call to <a href="ranger.html#topic+ranger">ranger</a>. See
<a href="ranger.html#topic+importance.ranger">importance</a> for details.
</p>
</li>
<li> <p><a href="rpart.html#topic+rpart">rpart</a> - As stated in one of the <a href="rpart.html#topic+rpart">rpart</a>
vignettes. A variable may appear in the tree many times, either as a primary
or a surrogate variable. An overall measure of variable importance is the sum
of the goodness of split measures for each split for which it was the primary
variable, plus &quot;goodness&quot; * (adjusted agreement) for all splits in which it
was a surrogate. Imagine two variables which were essentially duplicates of
each other; if we did not count surrogates, they would split the importance
with neither showing up as strongly as it should. See
<a href="rpart.html#topic+rpart">rpart</a> for details.
</p>
</li>
<li> <p><a href="caret.html#topic+train">caret</a> - Various model-specific and model-agnostic
approaches that depend on the learning algorithm employed in the original
call to <a href="caret.html#topic+train">caret</a>. See <a href="caret.html#topic+varImp">varImp</a> for details.
</p>
</li>
<li> <p><a href="xgboost.html#topic+xgb.train">xgboost</a> - For linear models, the variable
importance is the absolute magnitude of the estimated coefficients. For that
reason, in order to obtain a meaningful ranking by importance for a linear
model, the features need to be on the same scale (which you also would want
to do when using either L1 or L2 regularization). Otherwise, the approach
described in Friedman (2001) for <a href="gbm.html#topic+gbm">gbm</a>s is used. See
<a href="xgboost.html#topic+xgb.importance">xgb.importance</a> for details. For tree models, you
can obtain three different types of variable importance:
</p>

<ul>
<li><p> Using <code>type = "gain"</code> (the default) gives the fractional contribution of
each feature to the model based on the total gain of the corresponding
feature's splits.
</p>
</li>
<li><p> Using <code>type = "cover"</code> gives the number of observations related to each
feature.
</p>
</li>
<li><p> Using <code>type = "frequency"</code> gives the percentages representing
the relative number of times each feature has been used throughout each
tree in the ensemble.
</p>
</li></ul>

</li>
<li> <p><a href="lightgbm.html#topic+lightgbm">lightgbm</a> - Same as for <a href="xgboost.html#topic+xgb.train">xgboost</a>
models, except <a href="lightgbm.html#topic+lgb.importance">lgb.importance</a> (which this method
calls internally) has an additional argument, <code>percentage</code>, that defaults to
<code>TRUE</code>, resulting in the VI scores shown as a relative percentage; pass
<code>percentage = FALSE</code> in the call to <code>vi_model()</code> to produce VI scores for
<a href="lightgbm.html#topic+lightgbm">lightgbm</a> models on the raw scale.
</p>
</li></ul>



<h3>Value</h3>

<p>A tidy data frame (i.e., a <a href="tibble.html#topic+tibble">tibble</a> object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as the average change in
performance after a random permutation (or permutations, if <code>nsim &gt; 1</code>) of
the feature in question.
</p>
</li></ul>

<p>For <a href="stats.html#topic+lm">lm</a>/<a href="stats.html#topic+glm">glm</a>-like objects, the sign (i.e., POS/NEG)
of the original coefficient is also included in a column called <code>Sign</code>.
</p>


<h3>Note</h3>

<p>Inspired by the <a href="https://cran.r-project.org/package=caret">caret</a>'s
<a href="caret.html#topic+varImp">varImp</a> function.
</p>


<h3>Source</h3>

<p>Johan Bring (1994) How to Standardize Regression Coefficients, The American
Statistician, 48:3, 209-213, DOI: 10.1080/00031305.1994.10476059.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic example using imputed titanic data set
t3 &lt;- titanic_mice[[1L]]

# Fit a simple model
set.seed(1449)  # for reproducibility
bst &lt;- lightgbm::lightgbm(
  data = data.matrix(subset(t3, select = -survived)),
  label = ifelse(t3$survived == "yes", 1, 0),
  params = list("objective" = "binary", "force_row_wise" = TRUE),
  verbose = 0
)

# Compute VI scores
vi(bst)  # defaults to `method = "model"`
vi_model(bst)  # same as above

# Same as above (since default is `method = "model"`), but returns a plot
vip(bst, geom = "point")
vi_model(bst, type = "cover")
vi_model(bst, type = "cover", percentage = FALSE)

# Compare to
lightgbm::lgb.importance(bst)

## End(Not run)

</code></pre>

<hr>
<h2 id='vi_permute'>Permutation-based variable importance</h2><span id='topic+vi_permute'></span><span id='topic+vi_permute.default'></span>

<h3>Description</h3>

<p>Compute permutation-based variable importance scores for the predictors in a
model; for details on the algorithm, see Greenwell and Boehmke (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_permute(object, ...)

## Default S3 method:
vi_permute(
  object,
  feature_names = NULL,
  train = NULL,
  target = NULL,
  metric = NULL,
  smaller_is_better = NULL,
  type = c("difference", "ratio"),
  nsim = 1,
  keep = TRUE,
  sample_size = NULL,
  sample_frac = NULL,
  reference_class = NULL,
  event_level = NULL,
  pred_wrapper = NULL,
  verbose = FALSE,
  parallel = FALSE,
  parallelize_by = c("features", "repetitions"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_permute_+3A_object">object</code></td>
<td>
<p>A fitted model object (e.g., a
<a href="randomForest.html#topic+randomForest">randomForest</a> object).</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed on to
<a href="foreach.html#topic+foreach">foreach</a> (e.g., <code>.packages</code> or <code>.export</code>).</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_feature_names">feature_names</code></td>
<td>
<p>Character string giving the names of the predictor
variables (i.e., features) of interest. If <code>NULL</code> (the default) then they
will be inferred from the <code>train</code> and <code>target</code> arguments (see below). It is
good practice to always specify this argument.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_train">train</code></td>
<td>
<p>A matrix-like R object (e.g., a data frame or matrix)
containing the training data. If <code>NULL</code> (the default) then the
internal <code>get_training_data()</code> function will be called to try and extract it
automatically. It is good practice to always specify this argument.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_target">target</code></td>
<td>
<p>Either a character string giving the name (or position) of the
target column in <code>train</code> or, if <code>train</code> only contains feature
columns, a vector containing the target values used to train <code>object</code>.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_metric">metric</code></td>
<td>
<p>Either a function or character string specifying the
performance metric to use in computing model performance (e.g., RMSE for
regression or accuracy for binary classification). If <code>metric</code> is a
function, then it requires two arguments, <code>actual</code> and <code>predicted</code>,
and should return a single, numeric value. Ideally, this should be the same
metric that was used to train <code>object</code>. See <code><a href="#topic+list_metrics">list_metrics()</a></code> for a list of
built-in metrics.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_smaller_is_better">smaller_is_better</code></td>
<td>
<p>Logical indicating whether or not a smaller value
of <code>metric</code> is better. Default is <code>NULL</code>. Must be supplied if
<code>metric</code> is a user-supplied function.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_type">type</code></td>
<td>
<p>Character string specifying how to compare the baseline and
permuted performance metrics. Current options are <code>"difference"</code> (the
default) and <code>"ratio"</code>.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_nsim">nsim</code></td>
<td>
<p>Integer specifying the number of Monte Carlo replications to
perform. Default is 1. If <code>nsim &gt; 1</code>, the results from each replication
are simply averaged together (the standard deviation will also be returned).</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_keep">keep</code></td>
<td>
<p>Logical indicating whether or not to keep the individual
permutation scores for all <code>nsim</code> repetitions. If <code>TRUE</code> (the
default) then the individual variable importance scores will be stored in an
attribute called <code>"raw_scores"</code>. (Only used when <code>nsim &gt; 1</code>.)</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_sample_size">sample_size</code></td>
<td>
<p>Integer specifying the size of the random sample to use
for each Monte Carlo repetition. Default is <code>NULL</code> (i.e., use all of the
available training data). Cannot be specified with <code>sample_frac</code>. Can be
used to reduce computation time with large data sets.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_sample_frac">sample_frac</code></td>
<td>
<p>Proportion specifying the size of the random sample to use
for each Monte Carlo repetition. Default is <code>NULL</code> (i.e., use all of the
available training data). Cannot be specified with <code>sample_size</code>. Can be
used to reduce computation time with large data sets.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_reference_class">reference_class</code></td>
<td>
<p>Deprecated, use <code>event_level</code> instead.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_event_level">event_level</code></td>
<td>
<p>String specifying which factor level of <code>truth</code> to
consider as the &quot;event&quot;. Options are <code>"first"</code> (the default) or <code>"second"</code>.
This argument is only applicable for binary classification when <code>metric</code> is
one of <code>"roc_auc"</code>, <code>"pr_auc"</code>, or <code>"youden"</code>. This argument is passed on to
the corresponding <a href="yardstick.html#topic+yardstick-package">yardstick</a> metric.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_pred_wrapper">pred_wrapper</code></td>
<td>
<p>Prediction function that requires two arguments,
<code>object</code> and <code>newdata</code>. The output of this function should be
determined by the <code>metric</code> being used:
</p>

<ul>
<li><p> Regression - A numeric vector of predicted outcomes.
</p>
</li>
<li><p> Binary classification - A vector of predicted class labels (e.g., if using
misclassification error) or a vector of predicted class probabilities for the
reference class (e.g., if using log loss or AUC).
</p>
</li>
<li><p> Multiclass classification - A vector of predicted class labels (e.g., if
using misclassification error) or a A matrix/data frame of predicted class
probabilities for each class (e.g., if using log loss or AUC).
</p>
</li></ul>
</td></tr>
<tr><td><code id="vi_permute_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print information during
the construction of variable importance scores. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_parallel">parallel</code></td>
<td>
<p>Logical indicating whether or not to run <code>vi_permute()</code>
in parallel (using a backend provided by the <a href="foreach.html#topic+foreach">foreach</a>
package). Default is <code>FALSE</code>. If <code>TRUE</code>, a
<a href="foreach.html#topic+foreach">foreach</a>-compatible backend must be provided by must be
provided. Note that <code>set.seed()</code> will not not work with
<a href="foreach.html#topic+foreach">foreach</a>'s parellelized for loops; for a workaround, see
<a href="https://github.com/koalaverse/vip/issues/145">this solution</a>.</p>
</td></tr>
<tr><td><code id="vi_permute_+3A_parallelize_by">parallelize_by</code></td>
<td>
<p>Character string specifying whether to parallelize
across features (<code>parallelize_by = "features"</code>) or repetitions
(<code>parallelize_by = "reps"</code>); the latter is only useful whenever
<code>nsim &gt; 1</code>. Default is <code>"features"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tidy data frame (i.e., a <a href="tibble.html#topic+tibble">tibble</a> object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as the average change in
performance after a random permutation (or permutations, if <code>nsim &gt; 1</code>) of
the feature in question.
</p>
</li></ul>

<p>If <code>nsim &gt; 1</code>, then an additional column (<code>StDev</code>) containing the standard
deviation of the individual permutation scores for each feature is also
returned; this helps assess the stability/variation of the individual
permutation importance for each feature.
</p>


<h3>References</h3>

<p>Brandon M. Greenwell and Bradley C. Boehmke, The R Journal (2020) 12:1,
pages 343-366.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#
# Regression example
#

library(ranger)   # for fitting random forests

# Simulate data from Friedman 1 benchmark; only x1-x5 are important!
trn &lt;- gen_friedman(500, seed = 101)  # ?vip::gen_friedman

# Prediction wrapper
pfun &lt;- function(object, newdata) {
  # Needs to return vector of predictions from a ranger object; see
  # `ranger::predcit.ranger` for details on making predictions
  predict(object, data = newdata)$predictions
}

# Fit a (default) random forest
set.seed(0803) # for reproducibility
rfo &lt;- ranger(y ~ ., data = trn)

# Compute permutation-based VI scores
set.seed(2021)  # for reproducibility
vis &lt;- vi(rfo, method = "permute", target = "y", metric = "rsq",
          pred_wrapper = pfun, train = trn)
print(vis)

# Same as above, but using `vi_permute()` directly
set.seed(2021)  # for reproducibility
vi_permute(rfo, target = "y", metric = "rsq", pred_wrapper = pfun
           train = trn)

# Plot VI scores (could also replace `vi()` with `vip()` in above example)
vip(vis, include_type = TRUE)

# Mean absolute error
mae &lt;- function(truth, estimate) {
  mean(abs(truth - estimate))
}

# Permutation-based VIP with user-defined MAE metric
set.seed(1101)  # for reproducibility
vi_permute(rfo, target = "y", metric = mae, smaller_is_better = TRUE,
           pred_wrapper = pfun, train = trn)

# Same as above, but using `yardstick` package instead of user-defined metric
set.seed(1101)  # for reproducibility
vi_permute(rfo, target = "y", metric = yardstick::mae_vec,
           smaller_is_better = TRUE, pred_wrapper = pfun, train = trn)

#
# Classification (binary) example
#

library(randomForest)  # another package for fitting random forests

# Complete (i.e., imputed version of titanic data); see `?vip::titanic_mice`
head(t1 &lt;- titanic_mice[[1L]])
t1$pclass &lt;- as.ordered(t1$pclass)  # makes more sense as an ordered factor

# Fit another (default) random forest
set.seed(2053)  # for reproducibility
(rfo2 &lt;- randomForest(survived ~ ., data = t1))

# Define prediction wrapper for predicting class labels from a
# "randomForest" object
pfun_class &lt;- function(object, newdata) {
  # Needs to return factor of classifications
  predict(object, newdata = newdata, type = "response")
}

# Sanity check
pfun_class(rfo2, newdata = head(t1))
##   1   2   3   4   5   6
## yes yes yes  no yes  no
## Levels: no yes

# Compute mean decrease in accuracy
set.seed(1359)  # for reproducibility
vi(rfo2,
   method = "permute",
   train = t1,
   target = "survived",
   metric = "accuracy",  # or pass in `yardstick::accuracy_vec` directly
   # smaller_is_better = FALSE,  # no need to set for built-in metrics
   pred_wrapper = pfun_class,
   nsim = 30  # use 30 repetitions
)
## # A tibble: 5 × 3
##   Variable Importance   StDev
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 sex          0.228  0.0110
## 2 pclass       0.0825 0.00505
## 3 age          0.0721 0.00557
## 4 sibsp        0.0346 0.00430
## 5 parch        0.0183 0.00236

# Define prediction wrapper for predicting class probabilities from a
# "randomForest" object
pfun_prob &lt;- function(object, newdata) {
  # Needs to return vector of class probabilities for event level of interest
  predict(object, newdata = newdata, type = "prob")[, "yes"]
}

# Sanity check
pfun_prob(rfo2, newdata = head(t1))  # estiated P(survived=yes | x)
##     1     2     3     4     5     6
## 0.990 0.864 0.486 0.282 0.630 0.078

# Compute mean increase in Brier score
set.seed(1411)  # for reproducibility
vi(rfo2,
   method = "permute",
   train = t1,
   target = "survived",
   metric = yardstick::brier_class_vec,  # or pass in `"brier"` directly
   smaller_is_better = FALSE,  # need to set when supplying a function
   pred_wrapper = pfun_prob,
   nsim = 30  # use 30 repetitions
)

## # A tibble: 5 × 3
## Variable Importance   StDev
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 sex          0.210  0.00869
## 2 pclass       0.0992 0.00462
## 3 age          0.0970 0.00469
## 4 parch        0.0547 0.00273
## 5 sibsp        0.0422 0.00200

# Some metrics, like AUROC, treat one class as the "event" of interest. In
# such cases, it's important to make sure the event level (which typically
# defaults to which ever event class comes first in alphabetical order)
# matches the event class that corresponds to the prediction wrappers
# returned probabilities. To do this, you can (and should) set the
# `event_class` argument. For instance, our prediction wrapper specified
# `survived = "yes"` as the event of interest, but this is considered the
# second event:
levels(t1$survived)
## [1] "no"  "yes"

# So, we need to specify the second class as the event of interest via the
# `event_level` argument (otherwise, we would get the negative of the results
# we were hoping for; a telltale sign the event level and prediction wrapper
do not match)
set.seed(1413)  # for reproducibility
vi(rfo,
   method = "permute",
   train = t1,
   target = "survived",
   metric = "roc_auc",
   event_level = "second",  # use "yes" as class label/"event" of interest
   pred_wrapper = pfun_prob,
   nsim = 30  # use 30 repetitions
)

## # A tibble: 5 × 3
## Variable Importance   StDev
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 sex          0.229  0.0137
## 2 pclass       0.0920 0.00533
## 3 age          0.0850 0.00477
## 4 sibsp        0.0283 0.00211
## 5 parch        0.0251 0.00351

## End(Not run)
</code></pre>

<hr>
<h2 id='vi_shap'>SHAP-based variable importance</h2><span id='topic+vi_shap'></span><span id='topic+vi_shap.default'></span>

<h3>Description</h3>

<p>Compute SHAP-based VI scores for the predictors in a model. See details
below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_shap(object, ...)

## Default S3 method:
vi_shap(object, feature_names = NULL, train = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_shap_+3A_object">object</code></td>
<td>
<p>A fitted model object (e.g., a
<a href="randomForest.html#topic+randomForest">randomForest</a> object).</p>
</td></tr>
<tr><td><code id="vi_shap_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed on to <code><a href="fastshap.html#topic+explain">fastshap::explain()</a></code>
(e.g., <code>nsim =  30</code>, <code>adjust = TRUE</code>, or avprediction wrapper via the
<code>pred_wrapper</code> argument); see <code>?fastshap::explain</code> for details on these and
other useful arguments.</p>
</td></tr>
<tr><td><code id="vi_shap_+3A_feature_names">feature_names</code></td>
<td>
<p>Character string giving the names of the predictor
variables (i.e., features) of interest. If <code>NULL</code> (the default) then they
will be inferred from the <code>train</code> and <code>target</code> arguments (see below). It is
good practice to always specify this argument.</p>
</td></tr>
<tr><td><code id="vi_shap_+3A_train">train</code></td>
<td>
<p>A matrix-like R object (e.g., a data frame or matrix)
containing the training data. If <code>NULL</code> (the default) then the
internal <code>get_training_data()</code> function will be called to try and extract it
automatically. It is good practice to always specify this argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This approach to computing VI scores is based on the mean absolute
value of the SHAP values for each feature; see, for example,
<a href="https://github.com/shap/shap">https://github.com/shap/shap</a> and the references therein.
</p>
<p>Strumbelj, E., and Kononenko, I. Explaining prediction models and individual
predictions with feature contributions. Knowledge and information systems
41.3 (2014): 647-665.
</p>


<h3>Value</h3>

<p>A tidy data frame (i.e., a <a href="tibble.html#topic+tibble">tibble</a> object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as the mean absolute
Shapley value.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(ggplot2)  # for theme_light() function
library(xgboost)

# Simulate training data
trn &lt;- gen_friedman(500, sigma = 1, seed = 101)  # ?vip::gen_friedman

# Feature matrix
X &lt;- data.matrix(subset(trn, select = -y))  # matrix of feature values

# Fit an XGBoost model; hyperparameters were tuned using 5-fold CV
set.seed(859)  # for reproducibility
bst &lt;- xgboost(X, label = trn$y, nrounds = 338, max_depth = 3, eta = 0.1,
               verbose = 0)

# Construct VIP using "exact" SHAP values from XGBoost's internal Tree SHAP
# functionality
vip(bst, method = "shap", train = X, exact = TRUE, include_type = TRUE,
    geom = "point", horizontal = FALSE,
    aesthetics = list(color = "forestgreen", shape = 17, size = 5)) +
  theme_light()

# Use Monte-Carlo approach, which works for any model; requires prediction
# wrapper
pfun_prob &lt;- function(object, newdata) {  # prediction wrapper
  # For Shapley explanations, this should ALWAYS return a numeric vector
  predict(object, newdata = newdata, type = "prob")[, "yes"]
}

# Compute Shapley-based VI scores
set.seed(853)  # for reproducibility
vi_shap(rfo, train = subset(t1, select = -survived), pred_wrapper = pfun_prob,
        nsim = 30)
## # A tibble: 5 × 2
## Variable Importance
##   &lt;chr&gt;         &lt;dbl&gt;
## 1 pclass       0.104
## 2 age          0.0649
## 3 sex          0.272
## 4 sibsp        0.0260
## 5 parch        0.0291

## End(Not run)
</code></pre>

<hr>
<h2 id='vip'>Variable importance plots</h2><span id='topic+vip'></span><span id='topic+vip.default'></span><span id='topic+vip.model_fit'></span><span id='topic+vip.workflow'></span><span id='topic+vip.WrappedModel'></span><span id='topic+vip.Learner'></span>

<h3>Description</h3>

<p>Plot variable importance scores for the predictors in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vip(object, ...)

## Default S3 method:
vip(
  object,
  num_features = 10L,
  geom = c("col", "point", "boxplot", "violin"),
  mapping = NULL,
  aesthetics = list(),
  horizontal = TRUE,
  all_permutations = FALSE,
  jitter = FALSE,
  include_type = FALSE,
  ...
)

## S3 method for class 'model_fit'
vip(object, ...)

## S3 method for class 'workflow'
vip(object, ...)

## S3 method for class 'WrappedModel'
vip(object, ...)

## S3 method for class 'Learner'
vip(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vip_+3A_object">object</code></td>
<td>
<p>A fitted model (e.g., of class
<a href="randomForest.html#topic+randomForest">randomForest</a> object) or a <a href="#topic+vi">vi</a> object.</p>
</td></tr>
<tr><td><code id="vip_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed on to <a href="#topic+vi">vi</a>.</p>
</td></tr>
<tr><td><code id="vip_+3A_num_features">num_features</code></td>
<td>
<p>Integer specifying the number of variable importance
scores to plot. Default is <code>10</code>.</p>
</td></tr>
<tr><td><code id="vip_+3A_geom">geom</code></td>
<td>
<p>Character string specifying which type of plot to construct.
The currently available options are described below.
</p>

<ul>
<li> <p><code>geom = "col"</code> uses <a href="ggplot2.html#topic+geom_bar">geom_col</a> to construct a bar chart
of the variable importance scores.
</p>
</li>
<li> <p><code>geom = "point"</code> uses <a href="ggplot2.html#topic+geom_point">geom_point</a> to construct a
Cleveland dot plot of the variable importance scores.
</p>
</li>
<li> <p><code>geom = "boxplot"</code> uses <a href="ggplot2.html#topic+geom_boxplot">geom_boxplot</a> to
construct a boxplot plot of the variable importance scores. This option can
only for the permutation-based importance method with <code>nsim &gt; 1</code> and
<code>keep = TRUE</code>; see <a href="#topic+vi_permute">vi_permute</a> for details.
</p>
</li>
<li> <p><code>geom = "violin"</code> uses <a href="ggplot2.html#topic+geom_violin">geom_violin</a> to
construct a violin plot of the variable importance scores. This option can
only for the permutation-based importance method with <code>nsim &gt; 1</code> and
<code>keep = TRUE</code>; see <a href="#topic+vi_permute">vi_permute</a> for details.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vip_+3A_mapping">mapping</code></td>
<td>
<p>Set of aesthetic mappings created by
<a href="ggplot2.html#topic+aes">aes</a>-related functions and/or tidy eval helpers. See example
usage below.</p>
</td></tr>
<tr><td><code id="vip_+3A_aesthetics">aesthetics</code></td>
<td>
<p>List specifying additional arguments passed on to
<a href="ggplot2.html#topic+layer">layer</a>. These are often aesthetics, used to set an aesthetic
to a fixed value, like<code>colour = "red"</code> or <code>size = 3</code>. See example usage
below.</p>
</td></tr>
<tr><td><code id="vip_+3A_horizontal">horizontal</code></td>
<td>
<p>Logical indicating whether or not to plot the importance
scores on the x-axis (<code>TRUE</code>). Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="vip_+3A_all_permutations">all_permutations</code></td>
<td>
<p>Logical indicating whether or not to plot all
permutation scores along with the average. Default is <code>FALSE</code>. (Only used for
permutation scores when <code>nsim &gt; 1</code>.)</p>
</td></tr>
<tr><td><code id="vip_+3A_jitter">jitter</code></td>
<td>
<p>Logical indicating whether or not to jitter the raw permutation
scores. Default is <code>FALSE</code>. (Only used when <code>all_permutations = TRUE</code>.)</p>
</td></tr>
<tr><td><code id="vip_+3A_include_type">include_type</code></td>
<td>
<p>Logical indicating whether or not to include the type of
variable importance computed in the axis label. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#
# A projection pursuit regression example using permutation-based importance
#

# Load the sample data
data(mtcars)

# Fit a projection pursuit regression model
model &lt;- ppr(mpg ~ ., data = mtcars, nterms = 1)

# Construct variable importance plot (permutation importance, in this case)
set.seed(825)  # for reproducibility
pfun &lt;- function(object, newdata) predict(object, newdata = newdata)
vip(model, method = "permute", train = mtcars, target = "mpg", nsim = 10,
    metric = "rmse", pred_wrapper = pfun)

# Better yet, store the variable importance scores and then plot
set.seed(825)  # for reproducibility
vis &lt;- vi(model, method = "permute", train = mtcars, target = "mpg",
          nsim = 10, metric = "rmse", pred_wrapper = pfun)
vip(vis, geom = "point", horiz = FALSE)
vip(vis, geom = "point", horiz = FALSE, aesthetics = list(size = 3))

# Plot unaggregated permutation scores (boxplot colored by feature)
library(ggplot2)  # for `aes()`-related functions and tidy eval helpers
vip(vis, geom = "boxplot", all_permutations = TRUE, jitter = TRUE,
    #mapping = aes_string(fill = "Variable"),   # for ggplot2 (&lt; 3.0.0)
    mapping = aes(fill = .data[["Variable"]]),  # for ggplot2 (&gt;= 3.0.0)
    aesthetics = list(color = "grey35", size = 0.8))

#
# A binary classification example
#
## Not run: 
library(rpart)  # for classification and regression trees

# Load Wisconsin breast cancer data; see ?mlbench::BreastCancer for details
data(BreastCancer, package = "mlbench")
bc &lt;- subset(BreastCancer, select = -Id)  # for brevity

# Fit a standard classification tree
set.seed(1032)  # for reproducibility
tree &lt;- rpart(Class ~ ., data = bc, cp = 0)

# Prune using 1-SE rule (e.g., use `plotcp(tree)` for guidance)
cp &lt;- tree$cptable
cp &lt;- cp[cp[, "nsplit"] == 2L, "CP"]
tree2 &lt;- prune(tree, cp = cp)  # tree with three splits

# Default tree-based VIP
vip(tree2)

# Computing permutation importance requires a prediction wrapper. For
# classification, the return value depends on the chosen metric; see
# `?vip::vi_permute` for details.
pfun &lt;- function(object, newdata) {
  # Need vector of predicted class probabilities when using  log-loss metric
  predict(object, newdata = newdata, type = "prob")[, "malignant"]
}

# Permutation-based importance (note that only the predictors that show up
# in the final tree have non-zero importance)
set.seed(1046)  # for reproducibility
vip(tree2, method = "permute", nsim = 10, target = "Class",
    metric = "logloss", pred_wrapper = pfun, reference_class = "malignant")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
