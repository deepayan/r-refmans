<!DOCTYPE html><html><head><title>Help for package conquer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {conquer}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#conquer-package'><p>Conquer: Convolution-Type Smoothed Quantile Regression</p></a></li>
<li><a href='#conquer'><p>Convolution-Type Smoothed Quantile Regression</p></a></li>
<li><a href='#conquer.cv.reg'><p>Cross-Validated Penalized Convolution-Type Smoothed Quantile Regression</p></a></li>
<li><a href='#conquer.process'><p>Convolution-Type Smoothed Quantile Regression Process</p></a></li>
<li><a href='#conquer.reg'><p>Penalized Convolution-Type Smoothed Quantile Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Convolution-Type Smoothed Quantile Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-05</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation and inference for conditional linear quantile regression models using a convolution smoothed approach. In the low-dimensional setting, efficient gradient-based methods are employed for fitting both a single model and a regression process over a quantile range. Normal-based and (multiplier) bootstrap confidence intervals for all slope coefficients are constructed. In high dimensions, the conquer method is complemented with flexible types of penalties (Lasso, elastic-net, group lasso, sparse group lasso, scad and mcp) to deal with complex low-dimensional structures.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/XiaoouPan/conquer">https://github.com/XiaoouPan/conquer</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.3), Matrix, matrixStats, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.9.850.1.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-06 06:51:17 UTC; xopan</td>
</tr>
<tr>
<td>Author:</td>
<td>Xuming He [aut],
  Xiaoou Pan [aut, cre],
  Kean Ming Tan [aut],
  Wen-Xin Zhou [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xiaoou Pan &lt;xip024@ucsd.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-06 08:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='conquer-package'>Conquer: Convolution-Type Smoothed Quantile Regression</h2><span id='topic+conquer-package'></span>

<h3>Description</h3>

<p>Estimation and inference for conditional linear quantile regression models using a convolution smoothed approach. 
In the low-dimensional setting, efficient gradient-based methods are employed for fitting both a single model and a regression process over a quantile range. Normal-based and (multiplier) bootstrap confidence intervals for all slope coefficients are constructed. 
In high dimensions, the conquer methods complemented with <code class="reqn">\ell_1</code>-penalization and iteratively reweighted <code class="reqn">\ell_1</code>-penalization are used to fit sparse models.
Commonly used penalities, such as the elastic-net, group lasso and sparse group lasso, are also incorporated to deal with more complex low-dimensional structures.
</p>


<h3>Author(s)</h3>

<p>Xuming He &lt;xmhe@umich.edu&gt;, Xiaoou Pan &lt;xip024@ucsd.edu&gt;, Kean Ming Tan &lt;keanming@umich.edu&gt;, and Wen-Xin Zhou &lt;wez243@ucsd.edu&gt;
</p>


<h3>References</h3>

<p>Barzilai, J. and Borwein, J. M. (1988). Two-point step size gradient methods. IMA J. Numer. Anal., 8, 141â€“148.
</p>
<p>Belloni, A. and Chernozhukov, V. (2011). <code class="reqn">\ell_1</code> penalized quantile regression in high-dimensional sparse models. Ann. Statist., 39, 82-130.
</p>
<p>Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist., 46, 814-841.
</p>
<p>Fernandes, M., Guerre, E. and Horta, E. (2021). Smoothing quantile regressions. J. Bus. Econ. Statist., 39, 338-357.
</p>
<p>He, X., Pan, X., Tan, K. M., and Zhou, W.-X. (2022+). Smoothed quantile regression for large-scale inference. J. Econometrics, in press.
</p>
<p>Koenker, R. (2005). Quantile Regression. Cambridge University Press, Cambridge.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>
<p>Tan, K. M., Wang, L. and Zhou, W.-X. (2022). High-dimensional quantile regression: convolution smoothing and concave regularization. J. Roy. Statist. Soc. Ser. B, 84(1), 205-233.
</p>

<hr>
<h2 id='conquer'>Convolution-Type Smoothed Quantile Regression</h2><span id='topic+conquer'></span>

<h3>Description</h3>

<p>Estimation and inference for conditional linear quantile regression models using a convolution smoothed approach. Efficient gradient-based methods are employed for fitting both a single model and a regression process over a quantile range. 
Normal-based and (multiplier) bootstrap confidence intervals for all slope coefficients are constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conquer(
  X,
  Y,
  tau = 0.5,
  kernel = c("Gaussian", "logistic", "uniform", "parabolic", "triangular"),
  h = 0,
  checkSing = FALSE,
  tol = 1e-04,
  iteMax = 5000,
  stepBounded = TRUE,
  stepMax = 100,
  ci = c("none", "bootstrap", "asymptotic", "both"),
  alpha = 0.05,
  B = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conquer_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">n</code> by <code class="reqn">p</code> design matrix. Each row is a vector of observations with <code class="reqn">p</code> covariates. Number of observations <code class="reqn">n</code> must be greater than number of covariates <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code id="conquer_+3A_y">Y</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional response vector.</p>
</td></tr>
<tr><td><code id="conquer_+3A_tau">tau</code></td>
<td>
<p>(<strong>optional</strong>) The desired quantile level. Default is 0.5. Value must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="conquer_+3A_kernel">kernel</code></td>
<td>
<p>(<strong>optional</strong>)  A character string specifying the choice of kernel function. Default is &quot;Gaussian&quot;. Choices are &quot;Gaussian&quot;, &quot;logistic&quot;, &quot;uniform&quot;, &quot;parabolic&quot; and &quot;triangular&quot;.</p>
</td></tr>
<tr><td><code id="conquer_+3A_h">h</code></td>
<td>
<p>(<strong>optional</strong>) Bandwidth/smoothing parameter. Default is <code class="reqn">\max\{((log(n) + p) / n)^{0.4}, 0.05\}</code>. The default will be used if the input value is less than or equal to 0.</p>
</td></tr>
<tr><td><code id="conquer_+3A_checksing">checkSing</code></td>
<td>
<p>(<strong>optional</strong>) A logical flag. Default is FALSE. If <code>checkSing = TRUE</code>, then it will check if the design matrix is singular before running conquer.</p>
</td></tr>
<tr><td><code id="conquer_+3A_tol">tol</code></td>
<td>
<p>(<strong>optional</strong>) Tolerance level of the gradient descent algorithm. The iteration will stop when the maximum magnitude of all the elements of the gradient is less than <code>tol</code>. Default is 1e-04.</p>
</td></tr>
<tr><td><code id="conquer_+3A_itemax">iteMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of iterations. Default is 5000.</p>
</td></tr>
<tr><td><code id="conquer_+3A_stepbounded">stepBounded</code></td>
<td>
<p>(<strong>optional</strong>) A logical flag. Default is TRUE. If <code>stepBounded = TRUE</code>, then the step size of gradient descent is upper bounded by <code>stepMax</code>. If <code>stepBounded = FALSE</code>, then the step size is unbounded.</p>
</td></tr>
<tr><td><code id="conquer_+3A_stepmax">stepMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum bound for the gradient descent step size. Default is 100.</p>
</td></tr>
<tr><td><code id="conquer_+3A_ci">ci</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying methods to construct confidence intervals. Choices are &quot;none&quot; (default), &quot;bootstrap&quot;, &quot;asymptotic&quot; and &quot;both&quot;. If <code>ci = "none"</code>, then confidence intervals will not be constructed. 
If <code>ci = "bootstrap"</code>, then three types of confidence intervals (percentile, pivotal and normal) will be constructed via multiplier bootstrap. 
If <code>ci = "asymptotic"</code>, then confidence intervals will be constructed based on estimated asymptotic covariance matrix. 
If <code>ci = "both"</code>, then confidence intervals from both bootstrap and asymptotic covariance will be returned.</p>
</td></tr>
<tr><td><code id="conquer_+3A_alpha">alpha</code></td>
<td>
<p>(<strong>optional</strong>) Miscoverage level for each confidence interval. Default is 0.05.</p>
</td></tr>
<tr><td><code id="conquer_+3A_b">B</code></td>
<td>
<p>(<strong>optional</strong>) The size of bootstrap samples. Default is 1000.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object containing the following items will be returned:
</p>

<dl>
<dt><code>coeff</code></dt><dd><p>A <code class="reqn">(p + 1)</code>-vector of estimated quantile regression coefficients, including the intercept.</p>
</dd>
<dt><code>ite</code></dt><dd><p>Number of iterations until convergence.</p>
</dd>
<dt><code>residual</code></dt><dd><p>An <code class="reqn">n</code>-vector of fitted residuals.</p>
</dd>
<dt><code>bandwidth</code></dt><dd><p>Bandwidth value.</p>
</dd>
<dt><code>tau</code></dt><dd><p>Quantile level.</p>
</dd>
<dt><code>kernel</code></dt><dd><p>Kernel function.</p>
</dd>
<dt><code>n</code></dt><dd><p>Sample size.</p>
</dd>
<dt><code>p</code></dt><dd><p>Number of covariates.</p>
</dd>
<dt><code>perCI</code></dt><dd><p>The percentile confidence intervals for regression coefficients. Only available if <code>ci = "bootstrap"</code> or <code>ci = "both"</code>.</p>
</dd>
<dt><code>pivCI</code></dt><dd><p>The pivotal confidence intervals for regression coefficients. Only available if <code>ci = "bootstrap"</code> or <code>ci = "both"</code>.</p>
</dd>
<dt><code>normCI</code></dt><dd><p>The normal-based confidence intervals for regression coefficients. Only available if <code>ci = "bootstrap"</code> or <code>ci = "both"</code>.</p>
</dd>
<dt><code>asyCI</code></dt><dd><p>The asymptotic confidence intervals for regression coefficients. Only available if <code>ci = "asymptotic"</code> or <code>ci = "both"</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Barzilai, J. and Borwein, J. M. (1988). Two-point step size gradient methods. IMA J. Numer. Anal., 8, 141â€“148.
</p>
<p>Fernandes, M., Guerre, E. and Horta, E. (2021). Smoothing quantile regressions. J. Bus. Econ. Statist., 39, 338-357.
</p>
<p>He, X., Pan, X., Tan, K. M., and Zhou, W.-X. (2022+). Smoothed quantile regression for large-scale inference. J. Econometrics, in press.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+conquer.process">conquer.process</a></code> for smoothed quantile regression process.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 500; p = 10
beta = rep(1, p)
X = matrix(rnorm(n * p), n, p)
Y = X %*% beta + rt(n, 2)

## Smoothed quantile regression with Gaussian kernel
fit.Gauss = conquer(X, Y, tau = 0.5, kernel = "Gaussian")
beta.hat.Gauss = fit.Gauss$coeff

## Smoothe quantile regression with uniform kernel
fit.unif = conquer(X, Y, tau = 0.5, kernel = "uniform")
beta.hat.unif = fit.unif$coeff

## Construct three types of confidence intervals via multiplier bootstrap
fit = conquer(X, Y, tau = 0.5, kernel = "Gaussian", ci = "bootstrap")
ci.per = fit$perCI
ci.piv = fit$pivCI
ci.norm = fit$normCI
</code></pre>

<hr>
<h2 id='conquer.cv.reg'>Cross-Validated Penalized Convolution-Type Smoothed Quantile Regression</h2><span id='topic+conquer.cv.reg'></span>

<h3>Description</h3>

<p>Fit sparse quantile regression models via regularized conquer methods with &quot;lasso&quot;, &quot;elastic-net&quot;, &quot;group lasso&quot;, &quot;sparse group lasso&quot;, &quot;scad&quot; and &quot;mcp&quot; penalties. The regularization parameter <code class="reqn">\lambda</code> is selected via cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conquer.cv.reg(
  X,
  Y,
  lambdaSeq = NULL,
  tau = 0.5,
  kernel = c("Gaussian", "logistic", "uniform", "parabolic", "triangular"),
  h = 0,
  penalty = c("lasso", "elastic", "group", "sparse-group", "scad", "mcp"),
  para.elastic = 0.5,
  group = NULL,
  weights = NULL,
  para.scad = 3.7,
  para.mcp = 3,
  kfolds = 5,
  numLambda = 50,
  epsilon = 0.001,
  iteMax = 500,
  phi0 = 0.01,
  gamma = 1.2,
  iteTight = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conquer.cv.reg_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">n</code> by <code class="reqn">p</code> design matrix. Each row is a vector of observations with <code class="reqn">p</code> covariates.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_y">Y</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional response vector.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_lambdaseq">lambdaSeq</code></td>
<td>
<p>(<strong>optional</strong>) A sequence of candidate regularization parameters. If unspecified, the sequence will be generated by a simulated pivotal quantity approach proposed in Belloni and Chernozhukov (2011).</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_tau">tau</code></td>
<td>
<p>(<strong>optional</strong>) Quantile level (between 0 and 1). Default is 0.5.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_kernel">kernel</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the choice of kernel function. Default is &quot;Gaussian&quot;. Choices are &quot;Gaussian&quot;, &quot;logistic&quot;, &quot;uniform&quot;, &quot;parabolic&quot; and &quot;triangular&quot;.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_h">h</code></td>
<td>
<p>(<strong>optional</strong>) The bandwidth parameter for kernel smoothing. Default is <code class="reqn">\max\{0.5 * (log(p) / n)^{0.25}, 0.05\}</code>. The default will be used if the input value is less than or equal to 0.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_penalty">penalty</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the penalty. Default is &quot;lasso&quot; (Tibshirani, 1996). The other options are &quot;elastic&quot; for elastic-net (Zou and Hastie, 2005), &quot;group&quot; for group lasso (Yuan and Lin, 2006), &quot;sparse-group&quot; for sparse group lasso (Simon et al., 2013), &quot;scad&quot; (Fan and Li, 2001) and &quot;mcp&quot; (Zhang, 2010).</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_para.elastic">para.elastic</code></td>
<td>
<p>(<strong>optional</strong>) The mixing parameter between 0 and 1 (usually noted as <code class="reqn">\alpha</code>) for elastic net. The penalty is defined as <code class="reqn">\alpha ||\beta||_1 + (1 - \alpha) ||\beta||_2^2</code>. Default is 0.5.
Setting <code>para.elastic = 1</code> gives the lasso penalty, and setting <code>para.elastic = 0</code> yields the ridge penalty. Only specify it when <code>penalty = "elastic"</code>.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_group">group</code></td>
<td>
<p>(<strong>optional</strong>) A <code class="reqn">p</code>-dimensional vector specifying group indices. Only specify it if <code>penalty = "group"</code> or <code>penalty = "sparse-group"</code>. 
For example, if <code class="reqn">p = 10</code>, and we assume the first 3 coefficients belong to the first group, and the last 7 coefficients belong to the second group, then the argument should be <code>group = c(rep(1, 3), rep(2, 7))</code>. If not specified, then the penalty will be the classical lasso.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_weights">weights</code></td>
<td>
<p>(<strong>optional</strong>) A vector specifying groups weights for group Lasso and sparse group Lasso. The length must be equal to the number of groups. If not specified, the default weights are square roots of group sizes. 
For example , if <code>group = c(rep(1, 3), rep(2, 7))</code>, then the default weights are <code class="reqn">\sqrt{3}</code> for the first group, and <code class="reqn">\sqrt{7}</code> for the second group.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_para.scad">para.scad</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for &quot;scad&quot;. Default value is 3.7. Only specify it if <code>penalty = "scad"</code>.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_para.mcp">para.mcp</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for &quot;mcp&quot;. Default value is 3. Only specify it if <code>penalty = "mcp"</code>.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_kfolds">kfolds</code></td>
<td>
<p>(<strong>optional</strong>) Number of folds for cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_numlambda">numLambda</code></td>
<td>
<p>(<strong>optional</strong>) Number of <code class="reqn">\lambda</code> values for cross-validation if <code>lambdaSeq</code> is unspeficied. Default is 50.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_epsilon">epsilon</code></td>
<td>
<p>(<strong>optional</strong>) A tolerance level for the stopping rule. The iteration will stop when the maximum magnitude of the change of coefficient updates is less than <code>epsilon</code>. Default is 0.001.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_itemax">iteMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of iterations. Default is 500.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_phi0">phi0</code></td>
<td>
<p>(<strong>optional</strong>) The initial quadratic coefficient parameter in the local adaptive majorize-minimize algorithm. Default is 0.01.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_gamma">gamma</code></td>
<td>
<p>(<strong>optional</strong>) The adaptive search parameter (greater than 1) in the local adaptive majorize-minimize algorithm. Default is 1.2.</p>
</td></tr>
<tr><td><code id="conquer.cv.reg_+3A_itetight">iteTight</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of tightening iterations in the iteratively reweighted <code class="reqn">\ell_1</code>-penalized algorithm. Only specify it if the penalty is scad or mcp. Default is 3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object containing the following items will be returned:
</p>

<dl>
<dt><code>coeff.min</code></dt><dd><p>A <code class="reqn">(p + 1)</code> vector of estimated coefficients including the intercept selected by minimizing the cross-validation errors.</p>
</dd>
<dt><code>coeff.1se</code></dt><dd><p>A <code class="reqn">(p + 1)</code> vector of estimated coefficients including the intercept. The corresponding <code class="reqn">\lambda</code> is the largest <code class="reqn">\lambda</code> such that the cross-validation error is within 1 standard error of the minimum.</p>
</dd>
<dt><code>lambdaSeq</code></dt><dd><p>The sequence of regularization parameter candidates for cross-validation.</p>
</dd>
<dt><code>lambda.min</code></dt><dd><p>Regularization parameter selected by minimizing the cross-validation errors. This is the corresponding <code class="reqn">\lambda</code> of <code>coeff.min</code>.</p>
</dd>
<dt><code>lambda.1se</code></dt><dd><p>The largest regularization parameter such that the cross-validation error is within 1 standard error of the minimum. This is the corresponding <code class="reqn">\lambda</code> of <code>coeff.1se</code>.</p>
</dd>
<dt><code>deviance</code></dt><dd><p>Cross-validation errors based on the quantile loss. The length is equal to the length of <code>lambdaSeq</code>.</p>
</dd>
<dt><code>deviance.se</code></dt><dd><p>Estimated standard errors of <code>deviance</code>. The length is equal to the length of <code>lambdaSeq</code>.</p>
</dd>
<dt><code>bandwidth</code></dt><dd><p>Bandwidth value.</p>
</dd>
<dt><code>tau</code></dt><dd><p>Quantile level.</p>
</dd>
<dt><code>kernel</code></dt><dd><p>Kernel function.</p>
</dd>
<dt><code>penalty</code></dt><dd><p>Penalty type.</p>
</dd>
<dt><code>n</code></dt><dd><p>Sample size.</p>
</dd>
<dt><code>p</code></dt><dd><p>Number of covariates.</p>
</dd>
</dl>



<h3>References</h3>

<p>Belloni, A. and Chernozhukov, V. (2011). <code class="reqn">\ell_1</code> penalized quantile regression in high-dimensional sparse models. Ann. Statist., 39, 82-130.
</p>
<p>Fan, J. and Li, R. (2001). Variable selection via nonconcave regularized likelihood and its oracle properties. J. Amer. Statist. Assoc., 96, 1348-1360.
</p>
<p>Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist., 46, 814-841.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>
<p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2013). A sparse-group lasso. J. Comp. Graph. Statist., 22, 231-245.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. Ser. B, 58, 267â€“288.
</p>
<p>Tan, K. M., Wang, L. and Zhou, W.-X. (2022). High-dimensional quantile regression: convolution smoothing and concave regularization. J. Roy. Statist. Soc. Ser. B, 84, 205-233.
</p>
<p>Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables., J. Roy. Statist. Soc. Ser. B, 68, 49-67.
</p>
<p>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38, 894-942.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. J. R. Statist. Soc. Ser. B, 67, 301-320.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+conquer.reg">conquer.reg</a></code> for regularized quantile regression with a prescribed <code class="reqn">lambda</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 100; p = 200; s = 5
beta = c(rep(1.5, s), rep(0, p - s))
X = matrix(rnorm(n * p), n, p)
Y = X %*% beta + rt(n, 2)

## Cross-validated regularized conquer with lasso penalty at tau = 0.7
fit.lasso = conquer.cv.reg(X, Y, tau = 0.7, penalty = "lasso")
beta.lasso = fit.lasso$coeff.min

## Cross-validated regularized conquer with elastic-net penalty at tau = 0.7
fit.elastic = conquer.cv.reg(X, Y, tau = 0.7, penalty = "elastic", para.elastic = 0.7)
beta.elastic = fit.elastic$coeff.min

## Cross-validated regularized conquer with scad penalty at tau = 0.7
fit.scad = conquer.cv.reg(X, Y, tau = 0.7, penalty = "scad")
beta.scad = fit.scad$coeff.min

## Regularized conquer with group lasso at tau = 0.7
beta = c(rep(1.3, 2), rep(1.5, 3), rep(0, p - s))
err = rt(n, 2)
Y = X %*% beta + err
group = c(rep(1, 2), rep(2, 3), rep(3, p - s))
fit.group = conquer.cv.reg(X, Y,tau = 0.7, penalty = "group", group = group)
beta.group = fit.group$coeff.min
</code></pre>

<hr>
<h2 id='conquer.process'>Convolution-Type Smoothed Quantile Regression Process</h2><span id='topic+conquer.process'></span>

<h3>Description</h3>

<p>Fit a smoothed quantile regression process over a quantile range. The algorithm is essentially the same as <code><a href="#topic+conquer">conquer</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conquer.process(
  X,
  Y,
  tauSeq = seq(0.1, 0.9, by = 0.05),
  kernel = c("Gaussian", "logistic", "uniform", "parabolic", "triangular"),
  h = 0,
  checkSing = FALSE,
  tol = 1e-04,
  iteMax = 5000,
  stepBounded = TRUE,
  stepMax = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conquer.process_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">n</code> by <code class="reqn">p</code> design matrix. Each row is a vector of observations with <code class="reqn">p</code> covariates. Number of observations <code class="reqn">n</code> must be greater than number of covariates <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_y">Y</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional response vector.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_tauseq">tauSeq</code></td>
<td>
<p>(<strong>optional</strong>) A sequence of quantile values (between 0 and 1). Default is <code class="reqn">\{0.1, 0.15, 0.2, ..., 0.85, 0.9\}</code>.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_kernel">kernel</code></td>
<td>
<p>(<strong>optional</strong>)  A character string specifying the choice of kernel function. Default is &quot;Gaussian&quot;. Choices are &quot;Gaussian&quot;, &quot;logistic&quot;, &quot;uniform&quot;, &quot;parabolic&quot; and &quot;triangular&quot;.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_h">h</code></td>
<td>
<p>(<strong>optional</strong>) The bandwidth/smoothing parameter. Default is <code class="reqn">\max\{((log(n) + p) / n)^{0.4}, 0.05\}</code>. The default will be used if the input value is less than or equal to 0.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_checksing">checkSing</code></td>
<td>
<p>(<strong>optional</strong>) A logical flag. Default is FALSE. If <code>checkSing = TRUE</code>, then it will check if the design matrix is singular before running conquer.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_tol">tol</code></td>
<td>
<p>(<strong>optional</strong>) Tolerance level of the gradient descent algorithm. The iteration will stop when the maximum magnitude of all the elements of the gradient is less than <code>tol</code>. Default is 1e-04.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_itemax">iteMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of iterations. Default is 5000.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_stepbounded">stepBounded</code></td>
<td>
<p>(<strong>optional</strong>) A logical flag. Default is TRUE.  If <code>stepBounded = TRUE</code>, then the step size of gradient descent is upper bounded by <code>stepMax</code>. If <code>stepBounded = FALSE</code>, then the step size is unbounded.</p>
</td></tr>
<tr><td><code id="conquer.process_+3A_stepmax">stepMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum bound for the gradient descent step size. Default is 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object containing the following items will be returned:
</p>

<dl>
<dt><code>coeff</code></dt><dd><p>A <code class="reqn">(p + 1)</code> by <code class="reqn">m</code> matrix of estimated quantile regression process coefficients, including the intercept. m is the length of <code>tauSeq</code>.</p>
</dd>
<dt><code>bandwidth</code></dt><dd><p>Bandwidth value.</p>
</dd>
<dt><code>tauSeq</code></dt><dd><p>The sequence of quantile levels.</p>
</dd>
<dt><code>kernel</code></dt><dd><p>The choice of kernel function.</p>
</dd>
<dt><code>n</code></dt><dd><p>Sample size.</p>
</dd>
<dt><code>p</code></dt><dd><p>Number the covariates.</p>
</dd>
</dl>



<h3>References</h3>

<p>Barzilai, J. and Borwein, J. M. (1988). Two-point step size gradient methods. IMA J. Numer. Anal., 8, 141â€“148.
</p>
<p>Fernandes, M., Guerre, E. and Horta, E. (2021). Smoothing quantile regressions. J. Bus. Econ. Statist., 39, 338-357.
</p>
<p>He, X., Pan, X., Tan, K. M., and Zhou, W.-X. (2022+). Smoothed quantile regression for large-scale inference. J. Econometrics, in press.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+conquer">conquer</a></code> for single-index smoothed quantile regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 500; p = 10
beta = rep(1, p)
X = matrix(rnorm(n * p), n, p)
Y = X %*% beta + rt(n, 2)

## Smoothed quantile regression process with Gaussian kernel
fit.Gauss = conquer.process(X, Y, tauSeq = seq(0.2, 0.8, by = 0.05), kernel = "Gaussian")
beta.hat.Gauss = fit.Gauss$coeff

## Smoothe quantile regression with uniform kernel
fit.unif = conquer.process(X, Y, tauSeq = seq(0.2, 0.8, by = 0.05), kernel = "uniform")
beta.hat.unif = fit.unif$coeff
</code></pre>

<hr>
<h2 id='conquer.reg'>Penalized Convolution-Type Smoothed Quantile Regression</h2><span id='topic+conquer.reg'></span>

<h3>Description</h3>

<p>Fit sparse quantile regression models in high dimensions via regularized conquer methods with &quot;lasso&quot;, &quot;elastic-net&quot;, &quot;group lasso&quot;, &quot;sparse group lasso&quot;, &quot;scad&quot; and &quot;mcp&quot; penalties. 
For &quot;scad&quot; and &quot;mcp&quot;, the iteratively reweighted <code class="reqn">\ell_1</code>-penalized algorithm is complemented with a local adpative majorize-minimize algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conquer.reg(
  X,
  Y,
  lambda = 0.2,
  tau = 0.5,
  kernel = c("Gaussian", "logistic", "uniform", "parabolic", "triangular"),
  h = 0,
  penalty = c("lasso", "elastic", "group", "sparse-group", "scad", "mcp"),
  para.elastic = 0.5,
  group = NULL,
  weights = NULL,
  para.scad = 3.7,
  para.mcp = 3,
  epsilon = 0.001,
  iteMax = 500,
  phi0 = 0.01,
  gamma = 1.2,
  iteTight = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conquer.reg_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">n</code> by <code class="reqn">p</code> design matrix. Each row is a vector of observations with <code class="reqn">p</code> covariates.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_y">Y</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional response vector.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_lambda">lambda</code></td>
<td>
<p>(<strong>optional</strong>) Regularization parameter. Can be a scalar or a sequence. If the input is a sequence, the function will sort it in ascending order, and run the regression accordingly. Default is 0.2.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_tau">tau</code></td>
<td>
<p>(<strong>optional</strong>) Quantile level (between 0 and 1). Default is 0.5.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_kernel">kernel</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the choice of kernel function. Default is &quot;Gaussian&quot;. Choices are &quot;Gaussian&quot;, &quot;logistic&quot;, &quot;uniform&quot;, &quot;parabolic&quot; and &quot;triangular&quot;.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_h">h</code></td>
<td>
<p>(<strong>optional</strong>) Bandwidth/smoothing parameter. Default is <code class="reqn">\max\{0.5 * (log(p) / n)^{0.25}, 0.05\}</code>. The default will be used if the input value is less than or equal to 0.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_penalty">penalty</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the penalty. Default is &quot;lasso&quot; (Tibshirani, 1996). The other options are &quot;elastic&quot; for elastic-net (Zou and Hastie, 2005), &quot;group&quot; for group lasso (Yuan and Lin, 2006), &quot;sparse-group&quot; for sparse group lasso (Simon et al., 2013), &quot;scad&quot; (Fan and Li, 2001) and &quot;mcp&quot; (Zhang, 2010).</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_para.elastic">para.elastic</code></td>
<td>
<p>(<strong>optional</strong>) The mixing parameter between 0 and 1 (usually noted as <code class="reqn">\alpha</code>) for elastic-net. The penalty is defined as <code class="reqn">\alpha ||\beta||_1 + (1 - \alpha) ||\beta||_2^2</code>. Default is 0.5.
Setting <code>para.elastic = 1</code> gives the lasso penalty, and setting <code>para.elastic = 0</code> yields the ridge penalty. Only specify it when <code>penalty = "elastic"</code>.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_group">group</code></td>
<td>
<p>(<strong>optional</strong>) A <code class="reqn">p</code>-dimensional vector specifying group indices. Only specify it if <code>penalty = "group"</code> or <code>penalty = "sparse-group"</code>. 
For example, if <code class="reqn">p = 10</code>, and we assume the first 3 coefficients belong to the first group, and the last 7 coefficients belong to the second group, then the argument should be <code>group = c(rep(1, 3), rep(2, 7))</code>. If not specified, then the penalty will be the classical lasso.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_weights">weights</code></td>
<td>
<p>(<strong>optional</strong>) A vector specifying groups weights for group Lasso and sparse group Lasso. The length must be equal to the number of groups. If not specified, the default weights are square roots of group sizes. 
For example , if <code>group = c(rep(1, 3), rep(2, 7))</code>, then the default weights are <code class="reqn">\sqrt{3}</code> for the first group, and <code class="reqn">\sqrt{7}</code> for the second group.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_para.scad">para.scad</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for &quot;scad&quot;. Default value is 3.7. Only specify it if <code>penalty = "scad"</code>.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_para.mcp">para.mcp</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for &quot;mcp&quot;. Default value is 3. Only specify it if <code>penalty = "mcp"</code>.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_epsilon">epsilon</code></td>
<td>
<p>(<strong>optional</strong>) A tolerance level for the stopping rule. The iteration will stop when the maximum magnitude of the change of coefficient updates is less than <code>epsilon</code>. Default is 0.001.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_itemax">iteMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of iterations. Default is 500.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_phi0">phi0</code></td>
<td>
<p>(<strong>optional</strong>) The initial quadratic coefficient parameter in the local adaptive majorize-minimize algorithm. Default is 0.01.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_gamma">gamma</code></td>
<td>
<p>(<strong>optional</strong>) The adaptive search parameter (greater than 1) in the local adaptive majorize-minimize algorithm. Default is 1.2.</p>
</td></tr>
<tr><td><code id="conquer.reg_+3A_itetight">iteTight</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of tightening iterations in the iteratively reweighted <code class="reqn">\ell_1</code>-penalized algorithm. Only specify it if the penalty is scad or mcp. Default is 3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object containing the following items will be returned:
</p>

<dl>
<dt><code>coeff</code></dt><dd><p>If the input <code>lambda</code> is a scalar, then <code>coeff</code> returns a <code class="reqn">(p + 1)</code> vector of estimated coefficients, including the intercept. If the input <code>lambda</code> is a sequence, then <code>coeff</code> returns a <code class="reqn">(p + 1)</code> by <code class="reqn">nlambda</code> matrix, where <code class="reqn">nlambda</code> refers to the length of <code>lambda</code> sequence.</p>
</dd>
<dt><code>bandwidth</code></dt><dd><p>Bandwidth value.</p>
</dd>
<dt><code>tau</code></dt><dd><p>Quantile level.</p>
</dd>
<dt><code>kernel</code></dt><dd><p>Kernel function.</p>
</dd>
<dt><code>penalty</code></dt><dd><p>Penalty type.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>Regularization parameter(s).</p>
</dd>
<dt><code>n</code></dt><dd><p>Sample size.</p>
</dd>
<dt><code>p</code></dt><dd><p>Number of the covariates.</p>
</dd>
</dl>



<h3>References</h3>

<p>Belloni, A. and Chernozhukov, V. (2011). <code class="reqn">\ell_1</code> penalized quantile regression in high-dimensional sparse models. Ann. Statist., 39, 82-130.
</p>
<p>Fan, J. and Li, R. (2001). Variable selection via nonconcave regularized likelihood and its oracle properties. J. Amer. Statist. Assoc., 96, 1348-1360.
</p>
<p>Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist., 46, 814-841.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>
<p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2013). A sparse-group lasso. J. Comp. Graph. Statist., 22, 231-245.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. Ser. B, 58, 267â€“288.
</p>
<p>Tan, K. M., Wang, L. and Zhou, W.-X. (2022). High-dimensional quantile regression: convolution smoothing and concave regularization. J. Roy. Statist. Soc. Ser. B, 84, 205-233.
</p>
<p>Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables., J. Roy. Statist. Soc. Ser. B, 68, 49-67.
</p>
<p>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38, 894-942.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. J. R. Statist. Soc. Ser. B, 67, 301-320.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+conquer.cv.reg">conquer.cv.reg</a></code> for regularized quantile regression with cross-validation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 200; p = 500; s = 10
beta = c(rep(1.5, s), rep(0, p - s))
X = matrix(rnorm(n * p), n, p)
Y = X %*% beta + rt(n, 2)

## Regularized conquer with lasso penalty at tau = 0.7
fit.lasso = conquer.reg(X, Y, lambda = 0.05, tau = 0.7, penalty = "lasso")
beta.lasso = fit.lasso$coeff

## Regularized conquer with elastic-net penalty at tau = 0.7
fit.elastic = conquer.reg(X, Y, lambda = 0.1, tau = 0.7, penalty = "elastic", para.elastic = 0.7)
beta.elastic = fit.elastic$coeff

## Regularized conquer with scad penalty at tau = 0.7
fit.scad = conquer.reg(X, Y, lambda = 0.13, tau = 0.7, penalty = "scad")
beta.scad = fit.scad$coeff

## Regularized conquer with group lasso at tau = 0.7
beta = c(rep(1.3, 5), rep(1.5, 5), rep(0, p - s))
err = rt(n, 2)
Y = X %*% beta + err
group = c(rep(1, 5), rep(2, 5), rep(3, p - s))
fit.group = conquer.reg(X, Y, lambda = 0.05, tau = 0.7, penalty = "group", group = group)
beta.group = fit.group$coeff
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
