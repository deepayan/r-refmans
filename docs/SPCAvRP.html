<!DOCTYPE html><html lang="en"><head><title>Help for package SPCAvRP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SPCAvRP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#SPCAvRP'><p>Computes the leading eigenvector using the SPCAvRP algorithm</p></a></li>
<li><a href='#SPCAvRP_deflation'><p>Computes multiple principal components using our modified deflation scheme</p></a></li>
<li><a href='#SPCAvRP_subspace'><p>Computes the leading eigenspace using</p>
the SPCAvRP algorithm for the eigenspace estimation</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Sparse Principal Component Analysis via Random Projections
(SPCAvRP)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-05-01</td>
</tr>
<tr>
<td>Author:</td>
<td>Milana Gataric, Tengyao Wang and Richard J. Samworth</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Milana Gataric &lt;m.gataric@statslab.cam.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the SPCAvRP algorithm, developed and analysed in "Sparse principal component analysis via random projections" Gataric, M., Wang, T. and Samworth, R. J. (2018) &lt;<a href="https://doi.org/10.48550/arXiv.1712.05630">doi:10.48550/arXiv.1712.05630</a>&gt;. The algorithm is based on the aggregation of eigenvector information from carefully-selected random projections of the sample covariance matrix.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0), parallel, MASS</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://arxiv.org/abs/1712.05630">https://arxiv.org/abs/1712.05630</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-05-03 22:38:13 UTC; mg617</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-05-03 23:00:04 UTC</td>
</tr>
</table>
<hr>
<h2 id='SPCAvRP'>Computes the leading eigenvector using the SPCAvRP algorithm</h2><span id='topic+SPCAvRP'></span>

<h3>Description</h3>

<p>Computes <code>l</code>-sparse leading eigenvector of the sample covariance matrix, using <code>A x B</code> random axis-aligned projections of dimension <code>d</code>. For the multiple component estimation use <code><a href="#topic+SPCAvRP_subspace">SPCAvRP_subspace</a></code> or <code><a href="#topic+SPCAvRP_deflation">SPCAvRP_deflation</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPCAvRP(data, cov = FALSE, l, d = 20, A = 600, B = 200, 
center_data = TRUE, parallel = FALSE, 
cluster_type = "PSOCK", cores = 1, machine_names = NULL)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SPCAvRP_+3A_data">data</code></td>
<td>
<p>Either the data matrix (<code>p x n</code>) or the sample covariance matrix (<code>p x p</code>).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_cov">cov</code></td>
<td>
<p><code>TRUE</code> if data is given as a sample covariance matrix.</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_l">l</code></td>
<td>
<p>Desired sparsity level in the final estimator (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_d">d</code></td>
<td>
<p>The dimension of the random projections (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_a">A</code></td>
<td>
<p>Number of projections over which to aggregate (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_b">B</code></td>
<td>
<p>Number of projections in a group from which to select (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_center_data">center_data</code></td>
<td>
<p><code>TRUE</code> if the data matrix should be centered (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_parallel">parallel</code></td>
<td>
<p><code>TRUE</code> if the selection step should be computed in parallel by uses package <code>"parallel"</code>.</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_cluster_type">cluster_type</code></td>
<td>
<p>If <code>parallel == TRUE</code>, this can be <code>"PSOCK"</code> or <code>"FORK"</code> (cf. package <code>"parallel"</code>).</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_cores">cores</code></td>
<td>
<p>If <code>parallel == TRUE</code> and <code>cluster_type == "FORK"</code>, number of cores to use.</p>
</td></tr>
<tr><td><code id="SPCAvRP_+3A_machine_names">machine_names</code></td>
<td>
<p>If <code>parallel == TRUE</code>, the names of the computers on the network.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the SPCAvRP algorithm for the principal component estimation (Algorithm 1 in the reference given below). 
</p>
<p>If the true sparsity level <code>k</code> is known, use <code>l = k</code> and <code>d = k</code>. 
</p>
<p>If the true sparsity level <code>k</code> is unknown, <code>l</code> can take an array of different values and then the estimators of the corresponding sparsity levels are computed. The final choice of <code>l</code> can then be done by the user via inspecting the explained variance computed in the output <code>value</code> or via inspecting the output <code>importance_scores</code>. The default choice for <code>d</code> is <code>20</code>, but we suggest choosing <code>d</code> equal to or slightly larger than <code>l</code>.  
</p>
<p>It is desirable to choose <code>A</code> (and <code>B = ceiling(A/3)</code>) as big as possible subject to the computational budget. In general, we suggest using <code>A = 300</code> and <code>B = 100</code> when the dimension of data is a few hundreds, while <code>A = 600</code> and <code>B = 200</code> when the dimension is on order of <code>1000</code>. 
</p>
<p>If <code>center_data == TRUE</code> and <code>data</code> is given as a data matrix, the first step is to center it by executing <code>scale(data, center_data, FALSE)</code>, which subtracts the column means of <code>data</code> from their corresponding columns.
</p>
<p>If <code>parallel == TRUE</code>, the parallelised SPCAvRP algorithm is used. We recommend to use this option if <code>p</code>, <code>A</code> and <code>B</code> are very large. 
</p>


<h3>Value</h3>

<p>Returns a list of three elements:
</p>
<table role = "presentation">
<tr><td><code>vector</code></td>
<td>
<p>A matrix of dimension <code>p x length(l)</code> with columns as the estimated eigenvectors of sparsity level <code>l</code>.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>An array with <code>length(l)</code> eigenvalues corresponding to the estimated eigenvectors returned in <code>vector</code>.</p>
</td></tr>
<tr><td><code>importance_scores</code></td>
<td>
<p>An array of length p with importance scores for each variable 1 to p.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth</p>


<h3>References</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth (2018) Sparse principal component analysis via random projections
<a href="https://arxiv.org/abs/1712.05630">https://arxiv.org/abs/1712.05630</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- 100  # data dimension
k &lt;- 10   # true sparsity level
n &lt;- 1000 # number of observations
v1 &lt;- c(rep(1/sqrt(k), k), rep(0,p-k)) # true principal component
Sigma &lt;- 2*tcrossprod(v1) + diag(p)    # population covariance
mu &lt;- rep(0, p)                        # population mean
loss = function(u,v){ 
  # the loss function
  sqrt(abs(1-sum(v*u)^2))
}
set.seed(1)
X &lt;- mvrnorm(n, mu, Sigma) # data matrix

spcavrp &lt;- SPCAvRP(data = X, cov = FALSE, l = k, d = k, A = 200, B = 70)
spcavrp.loss &lt;- loss(v1,spcavrp$vector)
print(paste0("estimation loss when l=d=k=10, A=200, B=70: ", spcavrp.loss))

##choosing sparsity level l if k unknown:
#spcavrp.choosel &lt;- SPCAvRP(data = X, cov = FALSE, l = c(1:30), d = 15, A = 200, B = 70)
#plot(1:p,spcavrp.choosel$importance_scores,xlab='variable',ylab='w',
#     main='choosing l when k unknown: \n importance scores w')
#plot(1:30,spcavrp.choosel$value,xlab='l',ylab='Var_l',
#     main='choosing l when k unknown: \n explained variance Var_l')
</code></pre>

<hr>
<h2 id='SPCAvRP_deflation'>Computes multiple principal components using our modified deflation scheme</h2><span id='topic+SPCAvRP_deflation'></span>

<h3>Description</h3>

<p>Computes <code>m</code> leading eigenvectors of the sample covariance matrix which are sparse and orthogonal, using the modified deflation scheme in conjunction with the SPCAvRP algorithm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPCAvRP_deflation(data, cov = FALSE, m, l, d = 20, 
A = 600, B = 200, center_data = TRUE)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SPCAvRP_deflation_+3A_data">data</code></td>
<td>
<p>Either the data matrix (<code>p x n</code>) or the sample covariance matrix (<code>p x p</code>).</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_cov">cov</code></td>
<td>
<p><code>TRUE</code> if data is given as a sample covariance matrix.</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_m">m</code></td>
<td>
<p>The number of principal components to estimate.</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_l">l</code></td>
<td>
<p>The array of length <code>m</code> with the desired sparsity of <code>m</code> principle components (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_d">d</code></td>
<td>
<p>The dimension of the random projections (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_a">A</code></td>
<td>
<p>Number of projections over which to aggregate (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_b">B</code></td>
<td>
<p>Number of projections in a group from which to select (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_deflation_+3A_center_data">center_data</code></td>
<td>
<p><code>TRUE</code> if the data matrix should be centered (see Details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the modified deflation scheme in conjunction with SPCAvRP (Algorithm 2 in the reference given below). 
</p>
<p>If the true sparsity level is known and for each component is equal to <code>k</code>, use <code>d = k</code> and <code>l = rep(k,m)</code>. Sparsity levels of different components may take different values. If <code>k</code> is unknown, appropriate <code>k</code> could be chosen from an array of different values by inspecting the explained variance for one component at the time and by using <code>SPCAvRP</code> in a combination with the deflation scheme implemented in <code>SPCAvRP_deflation</code>. 
</p>
<p>It is desirable to choose <code>A</code> (and <code>B = ceiling(A/3)</code>) as big as possible subject to the computational budget. In general, we suggest using <code>A = 300</code> and <code>B = 100</code> when the dimension of data is a few hundreds, while <code>A = 600</code> and <code>B = 200</code> when the dimension is on order of <code>1000</code>. 
</p>
<p>If <code>center_data == TRUE</code> and <code>data</code> is given as a data matrix, the first step is to center it by executing <code>scale(data, center_data, FALSE)</code>, which subtracts the column means of <code>data</code> from their corresponding columns.</p>


<h3>Value</h3>

<p>Returns a list of two elements:
</p>
<table role = "presentation">
<tr><td><code>vector</code></td>
<td>
<p>A matrix whose <code>m</code> columns are the estimated eigenvectors.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>An array with <code>m</code> estimated eigenvalues.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth</p>


<h3>References</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth (2018) Sparse principal component analysis via random projections
<a href="https://arxiv.org/abs/1712.05630">https://arxiv.org/abs/1712.05630</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+SPCAvRP">SPCAvRP</a></code>, <code><a href="#topic+SPCAvRP_subspace">SPCAvRP_subspace</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- 50 # data dimension
k &lt;- 8  # true sparsity of each component
v1 &lt;- 1/sqrt(k)*c(rep(1, k), rep(0, p-k)) # first principal compnent (PC)
v2 &lt;- 1/sqrt(k)*c(rep(0,4), 1, -1, 1, -1, rep(1,4), rep(0,p-12)) # 2nd PC
v3 &lt;- 1/sqrt(k)*c(rep(0,6), 1, -rep(1,4), rep(1,3), rep(0,p-14)) # 3rd PC
Sigma &lt;- diag(p) + 40*tcrossprod(v1) + 20*tcrossprod(v2) + 5*tcrossprod(v3) # population covariance 
mu &lt;- rep(0, p) # population mean
n &lt;- 2000 # number of observations
loss = function(u,v){
  sqrt(abs(1-sum(v*u)^2))
}
loss_sub = function(U,V){
  U&lt;-qr.Q(qr(U)); V&lt;-qr.Q(qr(V))
  norm(tcrossprod(U)-tcrossprod(V),"2")
}
set.seed(1)
X &lt;- mvrnorm(n, mu, Sigma) # data matrix

spcavrp.def &lt;- SPCAvRP_deflation(data = X, cov = FALSE, m = 2, l = rep(k,2), 
                                 d = k, A = 200, B = 70, center_data = FALSE)
subspace_estimation&lt;-data.frame(
  loss_sub(matrix(c(v1,v2),ncol=2),spcavrp.def$vector),
  loss(spcavrp.def$vector[,1],v1),
  loss(spcavrp.def$vector[,2],v2),
  crossprod(spcavrp.def$vector[,1],spcavrp.def$vector[,2]))
colnames(subspace_estimation)&lt;-c("loss_sub","loss_v1","loss_v2","inner_prod")
rownames(subspace_estimation)&lt;-c("")
print(subspace_estimation)
</code></pre>

<hr>
<h2 id='SPCAvRP_subspace'>Computes the leading eigenspace using 
the SPCAvRP algorithm for the eigenspace estimation</h2><span id='topic+SPCAvRP_subspace'></span>

<h3>Description</h3>

<p>Computes <code>m</code> leading eigenvectors of the sample covariance matrix which are sparse and orthogonal, using <code>A x B</code> random axis-aligned projections of dimension <code>d</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPCAvRP_subspace(data, cov = FALSE, m, l, d = 20, 
A = 600, B = 200, center_data = TRUE)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SPCAvRP_subspace_+3A_data">data</code></td>
<td>
<p>Either the data matrix (<code>p x n</code>) or the sample covariance matrix (<code>p x p</code>).</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_cov">cov</code></td>
<td>
<p><code>TRUE</code> if data is given as a sample covariance matrix.</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_m">m</code></td>
<td>
<p>The dimension of the eigenspace, i.e the number of principal components to compute.</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_l">l</code></td>
<td>
<p>Desired sparsity level of the eigenspace (i.e. the number of non-zero rows in <code>output$vector</code>) (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_d">d</code></td>
<td>
<p>The dimension of the random projections (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_a">A</code></td>
<td>
<p>Number of projections over which to aggregate (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_b">B</code></td>
<td>
<p>Number of projections in a group from which to select (see Details).</p>
</td></tr>
<tr><td><code id="SPCAvRP_subspace_+3A_center_data">center_data</code></td>
<td>
<p><code>TRUE</code> if the data matrix should be centered (see Details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the SPCAvRP algorithm for the eigenspace estimation (Algorithm 3 in the reference given below). 
</p>
<p>If the true sparsity level <code>k</code> of the eigenspace is known, use <code>l = k</code> and <code>d = k</code>. 
</p>
<p>If the true sparsity level <code>k</code> of the eigenspace is unknown, the appropriate choice of <code>l</code> can be done, for example, by running the algorithm (for any <code>l</code>) and inspecting the computed output <code>importance_scores</code>. The default choice for <code>d</code> is <code>20</code>, but we suggest choosing <code>d</code> equal to or slightly larger than <code>l</code>.  
</p>
<p>It is desirable to choose <code>A</code> (and <code>B = ceiling(A/3)</code>) as big as possible subject to the computational budget. In general, we suggest using <code>A = 300</code> and <code>B = 100</code> when the dimension of data is a few hundreds, while <code>A = 600</code> and <code>B = 200</code> when the dimension is on order of <code>1000</code>. 
</p>
<p>If <code>center_data == TRUE</code> and <code>data</code> is given as a data matrix, the first step is to center it by executing <code>scale(data, center_data, FALSE)</code>, which subtracts the column means of <code>data</code> from their corresponding columns.</p>


<h3>Value</h3>

<p>Returns a list of two elements:
</p>
<table role = "presentation">
<tr><td><code>vector</code></td>
<td>
<p>A matrix whose <code>m</code> columns are the estimated eigenvectors.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>An array with <code>m</code> estimated eigenvalues.</p>
</td></tr>
<tr><td><code>importance_scores</code></td>
<td>
<p>An array of length p with importance scores for each variable 1 to p.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth</p>


<h3>References</h3>

<p>Milana Gataric, Tengyao Wang and Richard J. Samworth (2018) Sparse principal component analysis via random projections
<a href="https://arxiv.org/abs/1712.05630">https://arxiv.org/abs/1712.05630</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+SPCAvRP">SPCAvRP</a></code>, <code><a href="#topic+SPCAvRP_deflation">SPCAvRP_deflation</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- 50 # data dimension
k1 &lt;- 8 # sparsity of each induvidual component
v1 &lt;- 1/sqrt(k1)*c(rep(1, k1), rep(0, p-k1)) # first principal compnent (PC)
v2 &lt;- 1/sqrt(k1)*c(rep(0,4), 1, -1, 1, -1, rep(1,4), rep(0,p-12)) # 2nd PC
v3 &lt;- 1/sqrt(k1)*c(rep(0,6), 1, -rep(1,4), rep(1,3), rep(0,p-14)) # 3rd PC
Sigma &lt;- diag(p) + 40*tcrossprod(v1) + 20*tcrossprod(v2) + 5*tcrossprod(v3) # population covariance 
mu &lt;- rep(0, p) # pupulation mean
n &lt;- 2000 # number of observations
loss = function(u,v){
  sqrt(abs(1-sum(v*u)^2))
}
loss_sub = function(U,V){
  U&lt;-qr.Q(qr(U)); V&lt;-qr.Q(qr(V))
  norm(tcrossprod(U)-tcrossprod(V),"2")
}
set.seed(1)
X &lt;- mvrnorm(n, mu, Sigma) # data matrix


spcavrp.sub &lt;- SPCAvRP_subspace(data = X, cov = FALSE, m = 2, l = 12, d = 12,
                             A = 200, B = 70, center_data = FALSE)

subspace_estimation&lt;-data.frame(
  loss_sub(matrix(c(v1,v2),ncol=2),spcavrp.sub$vector),
  loss(spcavrp.sub$vector[,1],v1),
  loss(spcavrp.sub$vector[,2],v2),
  crossprod(spcavrp.sub$vector[,1],spcavrp.sub$vector[,2]))
colnames(subspace_estimation)&lt;-c("loss_sub","loss_v1","loss_v2","inner_prod")
rownames(subspace_estimation)&lt;-c("")
print(subspace_estimation)

plot(1:p,spcavrp.sub$importance_scores,xlab='variable',ylab='w',
     main='importance scores w \n (may use to choose l when k unknown)')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
