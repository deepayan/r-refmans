<!DOCTYPE html><html lang="en"><head><title>Help for package GeneSelectR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {GeneSelectR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.onAttach'><p>Package Attachment Function</p></a></li>
<li><a href='#aggregate_feature_importances'><p>Aggregate Feature Importances</p></a></li>
<li><a href='#annotate_gene_lists'><p>Convert and Annotate Gene Lists</p></a></li>
<li><a href='#AnnotatedGeneLists-class'><p>AnnotatedGeneLists class</p></a></li>
<li><a href='#calculate_mean_cv_scores'><p>Calculate Mean Cross-Validation Scores for Various Feature Selection Methods</p></a></li>
<li><a href='#calculate_overlap_coefficients'><p>Calculate Overlap and Similarity Coefficients between Feature Lists</p></a></li>
<li><a href='#calculate_permutation_feature_importance'><p>Calculate Permutation Feature Importance</p></a></li>
<li><a href='#check_python_modules_available'><p>Check Python Module Availability for Examples</p></a></li>
<li><a href='#compute_GO_child_term_metrics'><p>Retrieve and Plot the Offspring Nodes of GO Terms</p></a></li>
<li><a href='#configure_environment'><p>Configure Python Environment for GeneSelectR</p></a></li>
<li><a href='#create_conda_env'><p>Create a specific Conda environment</p></a></li>
<li><a href='#create_pipelines'><p>Create Pipelines</p></a></li>
<li><a href='#create_test_metrics_df'><p>Create a Dataframe of Test Metrics</p></a></li>
<li><a href='#define_sklearn_modules'><p>Define Python modules and scikit-learn submodules</p></a></li>
<li><a href='#enable_multiprocess'><p>Enable Multiprocessing in Python Environment</p></a></li>
<li><a href='#evaluate_test_metrics'><p>Evaluate Test Metrics for a Grid Search Model</p></a></li>
<li><a href='#GeneList-class'><p>GeneList class</p></a></li>
<li><a href='#GeneSelectR'><p>Gene Selection and Evaluation with GeneSelectR</p></a></li>
<li><a href='#get_feature_importances'><p>Get Feature Importances</p></a></li>
<li><a href='#GO_enrichment_analysis'><p>Perform gene set enrichment analysis using clusterProfiler</p></a></li>
<li><a href='#import_python_packages'><p>Import Python Libraries</p></a></li>
<li><a href='#install_python_packages'><p>Install necessary Python packages in a specific Conda environment</p></a></li>
<li><a href='#load_python_packages'><p>Load Python Modules</p></a></li>
<li><a href='#perform_grid_search'><p>Perform Grid Search or Random Search for Hyperparameter Tuning</p></a></li>
<li><a href='#pipeline_to_list'><p>Convert Scikit-learn Pipeline to Named List</p></a></li>
<li><a href='#PipelineResults-class'><p>PipelineResults class</p></a></li>
<li><a href='#plot_feature_importance'><p>Plot Feature Importance</p></a></li>
<li><a href='#plot_metrics'><p>Plot Performance Metrics</p></a></li>
<li><a href='#plot_overlap_heatmaps'><p>Generate Heatmaps to Visualize Overlap and Similarity Coefficients between Feature Lists</p></a></li>
<li><a href='#plot_upset'><p>Plot Feature Overlaps Using UpSet Plots</p></a></li>
<li><a href='#python-modules'><p>Global references to Python modules</p></a></li>
<li><a href='#run_simplify_enrichment'><p>Run simplifyGOFromMultipleLists with specified measure and method</p></a></li>
<li><a href='#set_default_fs_methods'><p>Set Default Feature Selection Methods</p></a></li>
<li><a href='#set_default_param_grids'><p>Set Default Parameter Grids for Feature Selection</p></a></li>
<li><a href='#set_reticulate_python'><p>Set RETICULATE_PYTHON for the Current Session</p></a></li>
<li><a href='#skip_if_no_modules'><p>Check if Python Modules are Available</p></a></li>
<li><a href='#split_data'><p>Split Data into Training and Test Sets</p></a></li>
<li><a href='#steps_to_tuples'><p>Convert Steps to Tuples</p></a></li>
<li><a href='#TestMetrics-class'><p>Class Union for Test metrics output that could contain either a dataframe or a lists</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>'GeneSelectR' - Comprehensive Feature Selection Workflow for
Bulk RNAseq Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>The workflow is a versatile R package designed for comprehensive feature selection in bulk RNAseq datasets. Its key innovation lies in the seamless integration of the 'Python' 'scikit-learn' (<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>) machine learning framework with R-based bioinformatics tools. 'GeneSelectR' performs robust Machine Learning-driven (ML) feature selection while leveraging 'Gene Ontology' (GO) enrichment analysis as described by Thomas PD et al. (2022) &lt;<a href="https://doi.org/10.1002%2Fpro.4218">doi:10.1002/pro.4218</a>&gt;, using 'clusterProfiler' (Wu et al., 2021) &lt;<a href="https://doi.org/10.1016%2Fj.xinn.2021.100141">doi:10.1016/j.xinn.2021.100141</a>&gt; and semantic similarity analysis powered by 'simplifyEnrichment' (Gu, Huebschmann, 2021) &lt;<a href="https://doi.org/10.1016%2Fj.gpb.2022.04.008">doi:10.1016/j.gpb.2022.04.008</a>&gt;. This combination of methodologies optimizes computational and biological insights for analyzing complex RNAseq datasets.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/dzhakparov/GeneSelectR">https://github.com/dzhakparov/GeneSelectR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dzhakparov/GeneSelectR/issues">https://github.com/dzhakparov/GeneSelectR/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>cowplot (&ge; 1.1.1), dplyr (&ge; 1.1.0), ggplot2 (&ge; 3.4.2), glue
(&ge; 1.6.2), magrittr (&ge; 2.0.3), methods (&ge; 4.2.2),
RColorBrewer (&ge; 1.1.3), reshape2 (&ge; 1.4.4), reticulate (&ge;
1.28), rlang (&ge; 1.1.1), testthat (&ge; 3.0.0), tibble (&ge;
3.2.1), tidyr (&ge; 1.3.0), tmod (&ge; 0.50.13),</td>
</tr>
<tr>
<td>Suggests:</td>
<td>clusterProfiler (&ge; 4.6.2), GO.db (&ge; 3.17.0), knitr,
rmarkdown, BiocManager (&ge; 1.30.21), UpSetR (&ge; 1.4.0),
AnnotationHub (&ge; 3.8.0), ensembldb (&ge; 2.24.0), org.Hs.eg.db
(&ge; 3.17.0)</td>
</tr>
<tr>
<td>Enhances:</td>
<td>simplifyEnrichment (&ge; 1.8.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-02 16:20:22 UTC; damir</td>
</tr>
<tr>
<td>Author:</td>
<td>Damir Zhakparov <a href="https://orcid.org/0000-0001-7175-0843"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Damir Zhakparov &lt;dzhakparov@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-03 14:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='.onAttach'>Package Attachment Function</h2><span id='topic+.onAttach'></span>

<h3>Description</h3>

<p>This function is called when the package is attached. It checks for the availability of essential Bioconductor packages and alerts if any are missing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.onAttach(libname, pkgname)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".onAttach_+3A_libname">libname</code></td>
<td>
<p>The name of the library.</p>
</td></tr>
<tr><td><code id=".onAttach_+3A_pkgname">pkgname</code></td>
<td>
<p>The name of the package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function checks for the presence of specific Bioconductor packages that are essential for the package's functionality. If any required packages are missing, it displays a startup message advising the user to install the missing packages using <code>BiocManager::install()</code>.
</p>
<p>Instead of stopping the package loading process, it alerts the user about any missing dependencies, recommending their installation for full functionality.
</p>

<hr>
<h2 id='aggregate_feature_importances'>Aggregate Feature Importances</h2><span id='topic+aggregate_feature_importances'></span>

<h3>Description</h3>

<p>This function aggregates the feature importances for each method across all splits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_feature_importances(selected_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aggregate_feature_importances_+3A_selected_features">selected_features</code></td>
<td>
<p>A list of selected features. Each element of the list represents a split and should be a named list where the names are the methods and the values are data frames containing the feature importances for that method in that split.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing aggregated feature importances for each feature selection method.
Each element in the list is a dataframe with mean and standard deviation of the feature importances
for a particular method across all splits. The dataframe includes columns for feature names, mean importances,
standard deviations, and ranks.
</p>

<hr>
<h2 id='annotate_gene_lists'>Convert and Annotate Gene Lists</h2><span id='topic+annotate_gene_lists'></span>

<h3>Description</h3>

<p>This function converts a list of gene lists to different formats and returns an object of the class AnnotatedGeneLists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annotate_gene_lists(
  pipeline_results,
  custom_lists = NULL,
  annotations_ahb,
  format = c("ENTREZ", "ENSEMBL", "SYMBOL")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="annotate_gene_lists_+3A_pipeline_results">pipeline_results</code></td>
<td>
<p>A PipelineResults object containing a named list of data frames.</p>
</td></tr>
<tr><td><code id="annotate_gene_lists_+3A_custom_lists">custom_lists</code></td>
<td>
<p>(optional) A named list of character vectors containing additional user-defined gene sets.</p>
</td></tr>
<tr><td><code id="annotate_gene_lists_+3A_annotations_ahb">annotations_ahb</code></td>
<td>
<p>A data.frame object containing gene annotations with columns 'gene_id', 'gene_name', and 'entrezid'.</p>
</td></tr>
<tr><td><code id="annotate_gene_lists_+3A_format">format</code></td>
<td>
<p>The format of the gene list in 'pipeline_results' and 'custom_lists'. This should be one of &quot;ENSEMBL&quot;, &quot;ENTREZ&quot;, or &quot;SYMBOL&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the class AnnotatedGeneLists, which contains the following components:
- @field inbuilt: A list of AnnotatedGeneList objects derived from the 'inbuilt_feature_importance' field of the 'pipeline_results' parameter.
- @field permutation: A list of AnnotatedGeneList objects derived from the 'permutation_importance' field of the 'pipeline_results' parameter, if available.
Each AnnotatedGeneList object in these lists includes the gene identifiers in different formats (SYMBOL, ENSEMBL, ENTREZID) and is structured to facilitate further analysis and visualization of gene lists.
If an error occurs during the conversion or annotation process, a warning message is given and the corresponding list entry will be NULL.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simple Usage with Mock Data
# Create a mock PipelineResults object with minimal data
mock_pipeline_results &lt;- new("PipelineResults",
                             inbuilt_feature_importance = list(
                               "GeneSet1" = data.frame(feature = c("BRCA1", "TP53"))),
                             permutation_importance = list(
                               "GeneSet1" = data.frame(feature = c("BRCA1", "TP53"))))

# Mock annotations data frame
mock_annotations_ahb &lt;- data.frame(gene_id = c("BRCA1", "TP53"),
                                   gene_name = c("BRCA1", "TP53"),
                                   entrezid = c(101, 102))

# Convert and annotate gene lists
annotated_lists &lt;- annotate_gene_lists(mock_pipeline_results,
                                       custom_lists = NULL,
                                       mock_annotations_ahb,
                                       "SYMBOL")
print(annotated_lists)

# Using Custom Gene Lists
# Create custom gene lists
custom_gene_lists &lt;- list("CustomList1" = c("BRCA1", "TP53"))

# Convert and annotate gene lists with custom gene lists included
annotated_lists_custom &lt;- annotate_gene_lists(mock_pipeline_results,
                                              custom_gene_lists,
                                              mock_annotations_ahb,
                                              "SYMBOL")
print(annotated_lists_custom)


</code></pre>

<hr>
<h2 id='AnnotatedGeneLists-class'>AnnotatedGeneLists class</h2><span id='topic+AnnotatedGeneLists-class'></span>

<h3>Description</h3>

<p>A class to hold a list of GeneList objects, each representing a method.
</p>


<h3>Slots</h3>


<dl>
<dt><code>inbuilt</code></dt><dd><p>A list of GeneList objects containing annotations for genes selected with inbuilt, model-specific feature importance.</p>
</dd>
<dt><code>permutation</code></dt><dd><p>A list of GeneList objects containing annotations for genes selected with permutation importance.</p>
</dd>
</dl>

<hr>
<h2 id='calculate_mean_cv_scores'>Calculate Mean Cross-Validation Scores for Various Feature Selection Methods</h2><span id='topic+calculate_mean_cv_scores'></span>

<h3>Description</h3>

<p>Calculate Mean Cross-Validation Scores for Various Feature Selection Methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_mean_cv_scores(selected_pipelines, cv_best_score)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_mean_cv_scores_+3A_selected_pipelines">selected_pipelines</code></td>
<td>
<p>A list of pipelines for different feature selection methods.</p>
</td></tr>
<tr><td><code id="calculate_mean_cv_scores_+3A_cv_best_score">cv_best_score</code></td>
<td>
<p>A list or vector of cross-validation scores.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe containing the mean and standard deviation of cross-validation scores for each method.
</p>

<hr>
<h2 id='calculate_overlap_coefficients'>Calculate Overlap and Similarity Coefficients between Feature Lists</h2><span id='topic+calculate_overlap_coefficients'></span>

<h3>Description</h3>

<p>This function calculates the Overlap, Jaccard, and Soerensen-Dice coefficients to quantify
the similarity between feature lists. In addition to feature importance and permutation importance,
you can provide a custom list of feature names to be included in the overlap calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_overlap_coefficients(pipeline_results, custom_lists = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_overlap_coefficients_+3A_pipeline_results">pipeline_results</code></td>
<td>
<p>A PipelineResults object containing the fitted pipelines, cross-validation results, selected features,
mean performance, and mean feature importances.</p>
</td></tr>
<tr><td><code id="calculate_overlap_coefficients_+3A_custom_lists">custom_lists</code></td>
<td>
<p>An optional named list of character vectors. Each character vector should contain feature names.
The names of the list will be used as names in the resulting overlap coefficient matrices.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing lists of matrices, where each list corresponds to a different type of feature list (inbuilt feature importance, permutation importance, and custom lists if provided).
Within each of these lists, there are three matrices showing the Overlap, Jaccard, and Soerensen-Dice coefficients for the feature lists:
- @field overlap: A matrix showing the Overlap coefficients.
- @field jaccard: A matrix showing the Jaccard coefficients.
- @field soerensen: A matrix showing the Soerensen-Dice coefficients.
These matrices compare the feature lists against each other, providing a numerical measure of their similarity.
Note: If permutation importance data is not present in the <code>pipeline_results</code>, the corresponding list entry will be absent.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Basic Usage with Mock Data
# Create a mock PipelineResults object with minimal data
mock_pipeline_results &lt;- new("PipelineResults",
                             inbuilt_feature_importance = list(
                             "FeatureSet1" = data.frame(feature = c("feature1", "feature2")),
                             "FeatureSet2" = data.frame(feature = c("feature2", "feature3"))),
                             permutation_importance = list(
                             "FeatureSet1" = data.frame(feature = c("feature3", "feature4")),
                             "FeatureSet2" = data.frame(feature = c("feature1", "feature4"))))

# Calculate overlap coefficients without custom lists
overlap_results &lt;- calculate_overlap_coefficients(mock_pipeline_results)


# Including Custom Lists
# Create custom feature lists
custom_feature_lists &lt;- list("CustomList1" = c("feature5", "feature6"),
                             "CustomList2" = c("feature6", "feature7"))

# Calculate overlap coefficients with custom lists
overlap_results_custom &lt;- calculate_overlap_coefficients(mock_pipeline_results,
                                                         custom_feature_lists)

</code></pre>

<hr>
<h2 id='calculate_permutation_feature_importance'>Calculate Permutation Feature Importance</h2><span id='topic+calculate_permutation_feature_importance'></span>

<h3>Description</h3>

<p>This function calculates permutation feature importance for a Scikit-learn
pipeline with a trained classifier as the final step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_permutation_feature_importance(
  pipeline,
  X_train,
  y_train,
  n_repeats = 10L,
  random_state = 0L,
  njobs = njobs,
  pipeline_name,
  iter
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_permutation_feature_importance_+3A_pipeline">pipeline</code></td>
<td>
<p>A Scikit-learn pipeline object with a trained classifier as the final step.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_x_train">X_train</code></td>
<td>
<p>A DataFrame containing the training data.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_y_train">y_train</code></td>
<td>
<p>A DataFrame containing the training labels.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_n_repeats">n_repeats</code></td>
<td>
<p>An integer specifying the number of times to permute each feature.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_random_state">random_state</code></td>
<td>
<p>An integer specifying the seed for the random number generator.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_njobs">njobs</code></td>
<td>
<p>An integer specifying number of cores to use. Set up by the master GeneSelectR function.</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_pipeline_name">pipeline_name</code></td>
<td>
<p>Strings (names of the selected_pipelines list) representing pipeline names that were constructed for the feature selection</p>
</td></tr>
<tr><td><code id="calculate_permutation_feature_importance_+3A_iter">iter</code></td>
<td>
<p>An integer that is indicating current iteration of the train-test split</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe containing the feature names and their permutation importance scores, ranked by importance.
Each row represents a feature, with columns for feature names, importances, and ranks.
</p>

<hr>
<h2 id='check_python_modules_available'>Check Python Module Availability for Examples</h2><span id='topic+check_python_modules_available'></span>

<h3>Description</h3>

<p>Checks if the specified Python modules are available and returns TRUE if all are available,
otherwise returns FALSE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_python_modules_available(module_names)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_python_modules_available_+3A_module_names">module_names</code></td>
<td>
<p>Character vector of Python module names to check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical TRUE if all modules are available, FALSE otherwise.
</p>

<hr>
<h2 id='compute_GO_child_term_metrics'>Retrieve and Plot the Offspring Nodes of GO Terms</h2><span id='topic+compute_GO_child_term_metrics'></span>

<h3>Description</h3>

<p>This function retrieves the children nodes for a set of Gene Ontology (GO) terms
from a list of GO terms and can plot the offspring nodes' numbers and fractions for each term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_GO_child_term_metrics(GO_data, GO_terms, ontology = "BP", plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_GO_child_term_metrics_+3A_go_data">GO_data</code></td>
<td>
<p>A list of GO data where each element corresponds to a different feature list.
Each element should have a @result data frame with a column 'ID' containing GO terms.</p>
</td></tr>
<tr><td><code id="compute_GO_child_term_metrics_+3A_go_terms">GO_terms</code></td>
<td>
<p>A character vector containing GO term IDs for which offspring nodes are to be fetched.</p>
</td></tr>
<tr><td><code id="compute_GO_child_term_metrics_+3A_ontology">ontology</code></td>
<td>
<p>A character string specifying the type of ontology to be considered.
Can be one of 'BP' (Biological Process), 'MF' (Molecular Function), or 'CC' (Cellular Component).
Default is 'BP'.</p>
</td></tr>
<tr><td><code id="compute_GO_child_term_metrics_+3A_plot">plot</code></td>
<td>
<p>A logical. If TRUE, the function plots the number and fraction of offspring nodes
for each term in <code>GO_terms</code> across all feature lists in <code>GO_data</code>.
Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns:
</p>

<ul>
<li> <p><strong>feature_list</strong> - The names of the feature lists from <code>GO_data</code>.
</p>
</li>
<li> <p><strong>all_terms_number</strong> - The total number of GO terms in each feature list.
</p>
</li>
<li> <p><strong>offspring_nodes_number</strong> - The number of offspring nodes for the given GO term(s) in each feature list.
</p>
</li>
<li> <p><strong>offspring_terms</strong> - The actual offspring terms for the given GO term(s) in each feature list, concatenated by ';'.
</p>
</li>
<li> <p><strong>fraction</strong> - The fraction (percentage) of offspring nodes out of all terms in each feature list.
</p>
</li>
<li> <p><strong>GO_term</strong> - The GO term being considered.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Mock GO terms data frame
all_selection.GO_inbuilt &lt;- data.frame(
  GO_ID = c("GO:0002376", "GO:0008150", "GO:0006955", "GO:0009628"),
  Description = c("immune system process",
                  "biological_process",
                  "immune response",
                  "response to virus"),
  Parent_GO_ID = c(NA, NA, "GO:0002376", "GO:0006955"), # Simplified parent-child
  stringsAsFactors = FALSE
)

# Mock vector of GO terms to compute metrics for
GO_terms_vec &lt;- c("GO:0002376", "GO:0008150")

# Assuming compute_GO_child_term_metrics is defined and available
# df_res &lt;- compute_GO_child_term_metrics(GO_data = all_selection.GO_inbuilt,
#                                        GO_terms = GO_terms_vec,
#                                        plot = TRUE)


</code></pre>

<hr>
<h2 id='configure_environment'>Configure Python Environment for GeneSelectR</h2><span id='topic+configure_environment'></span>

<h3>Description</h3>

<p>This function checks if Conda is installed, creates a new Conda environment (if it does not already exist),
installs necessary Python packages into the environment, and sets it as the active environment for reticulate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>configure_environment(env_name = "GeneSelectR_env")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="configure_environment_+3A_env_name">env_name</code></td>
<td>
<p>The name of the Conda environment to be created. Defaults to &quot;GeneSelectR_env&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A message indicating the status of the environment configuration.
If successful, it informs the user that the environment was created and necessary packages were installed.
If Conda is not installed or an error occurs, the function stops with an error message.
The function also advises the user to restart their R session for the changes to take effect.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Configure the default environment
configure_environment()

# Configure a custom environment
configure_environment("my_env_name")


</code></pre>

<hr>
<h2 id='create_conda_env'>Create a specific Conda environment</h2><span id='topic+create_conda_env'></span>

<h3>Description</h3>

<p>This function creates a Conda environment if it doesn't already exist.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_conda_env(conda_env = "GeneSelectR_env")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_conda_env_+3A_conda_env">conda_env</code></td>
<td>
<p>The name of the Conda environment to create.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Creates conda environment for GeneSelectR package called 'GeneSelectR_env'. If environment already exists, returns a message indicating that the environment is there.
</p>

<hr>
<h2 id='create_pipelines'>Create Pipelines</h2><span id='topic+create_pipelines'></span>

<h3>Description</h3>

<p>This function creates a list of Scikit-learn pipelines using the specified feature selection methods, preprocessing steps, and classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_pipelines(
  feature_selection_methods,
  preprocessing_steps,
  selected_methods,
  classifier,
  fs_param_grids
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_pipelines_+3A_feature_selection_methods">feature_selection_methods</code></td>
<td>
<p>A list of feature selection methods to use for the pipelines.</p>
</td></tr>
<tr><td><code id="create_pipelines_+3A_preprocessing_steps">preprocessing_steps</code></td>
<td>
<p>A list of preprocessing steps to use for the pipelines.</p>
</td></tr>
<tr><td><code id="create_pipelines_+3A_selected_methods">selected_methods</code></td>
<td>
<p>A vector of names of feature selection methods to use from the default set.</p>
</td></tr>
<tr><td><code id="create_pipelines_+3A_classifier">classifier</code></td>
<td>
<p>A Scikit-learn classifier to use as the final step in the pipelines.</p>
</td></tr>
<tr><td><code id="create_pipelines_+3A_fs_param_grids">fs_param_grids</code></td>
<td>
<p>param grid</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of Scikit-learn pipeline objects. Each pipeline is constructed based on the provided
feature selection methods, preprocessing steps, and classifier. The list is named by feature selection methods.
</p>

<hr>
<h2 id='create_test_metrics_df'>Create a Dataframe of Test Metrics</h2><span id='topic+create_test_metrics_df'></span>

<h3>Description</h3>

<p>Create a Dataframe of Test Metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_test_metrics_df(test_metrics)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_test_metrics_df_+3A_test_metrics">test_metrics</code></td>
<td>
<p>A list or dataframe of test metrics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe with the processed test metrics.
</p>

<hr>
<h2 id='define_sklearn_modules'>Define Python modules and scikit-learn submodules</h2><span id='topic+define_sklearn_modules'></span>

<h3>Description</h3>

<p>Define Python modules and scikit-learn submodules
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define_sklearn_modules(python_modules)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="define_sklearn_modules_+3A_python_modules">python_modules</code></td>
<td>
<p>A list containing imported Python modules.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the initialized Python modules and scikit-learn submodules, each as a separate list element.
The list includes:
</p>

<ul>
<li><p> @field preprocessing: Module for data preprocessing.
</p>
</li>
<li><p> @field model_selection: Module for model selection and evaluation.
</p>
</li>
<li><p> @field feature_selection: Module for feature selection methods.
</p>
</li>
<li><p> @field ensemble: Module for ensemble methods.
</p>
</li>
<li><p> @field pipeline: scikit-learn pipeline object.
</p>
</li>
<li><p> @field forest: Random Forest classifier for feature selection.
</p>
</li>
<li><p> @field randomized_grid: Randomized grid search for hyperparameter tuning.
</p>
</li>
<li><p> @field grid: Grid search for hyperparameter tuning.
</p>
</li>
<li><p> @field bayesianCV: Bayesian optimization using cross-validation.
</p>
</li>
<li><p> @field lasso: Lasso method for feature selection.
</p>
</li>
<li><p> @field univariate: Univariate feature selection method.
</p>
</li>
<li><p> @field select_model: Model-based feature selection method.
</p>
</li>
<li><p> @field GradBoost: Gradient Boosting classifier.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
required_modules &lt;- c("sklearn", "boruta")
modules_available &lt;- sapply(required_modules, reticulate::py_module_available)

if (all(modules_available)) {
  # All required Python modules are available
  # Define scikit-learn modules and submodules
  sklearn_modules &lt;- define_sklearn_modules()

  # Access different modules and submodules
  preprocessing_module &lt;- sklearn_modules$preprocessing
  model_selection_module &lt;- sklearn_modules$model_selection
  feature_selection_module &lt;- sklearn_modules$feature_selection
  ensemble_module &lt;- sklearn_modules$ensemble
  # Additional code to explore each module as needed in your analysis
} else {
  unavailable_modules &lt;- names(modules_available[!modules_available])
  message(paste(
  "Required Python modules not available:",
  paste(unavailable_modules, collapse=', '), ". Skipping example."))
}

</code></pre>

<hr>
<h2 id='enable_multiprocess'>Enable Multiprocessing in Python Environment</h2><span id='topic+enable_multiprocess'></span>

<h3>Description</h3>

<p>This function sets up the necessary executable for Python's multiprocessing functionality.
Only used on Windows
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enable_multiprocess(python_modules)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="enable_multiprocess_+3A_python_modules">python_modules</code></td>
<td>
<p>a list containing imported Python modules</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Doesn't return anything, enables multiprocessing on Windows
</p>

<hr>
<h2 id='evaluate_test_metrics'>Evaluate Test Metrics for a Grid Search Model</h2><span id='topic+evaluate_test_metrics'></span>

<h3>Description</h3>

<p>This function takes a grid search object, test data, and test labels to evaluate the performance
of the best model found during grid search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate_test_metrics(grid_search, X_test, y_test, modules)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evaluate_test_metrics_+3A_grid_search">grid_search</code></td>
<td>
<p>A grid search object containing the best estimator.</p>
</td></tr>
<tr><td><code id="evaluate_test_metrics_+3A_x_test">X_test</code></td>
<td>
<p>A data frame or matrix of test features.</p>
</td></tr>
<tr><td><code id="evaluate_test_metrics_+3A_y_test">y_test</code></td>
<td>
<p>A vector of test labels.</p>
</td></tr>
<tr><td><code id="evaluate_test_metrics_+3A_modules">modules</code></td>
<td>
<p>A list of Python modules used in the function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing key performance metrics of the best model:
- @field precision: The weighted precision score.
- @field recall: The weighted recall score.
- @field f1: The weighted F1 score.
- @field accuracy: The overall accuracy score.
These metrics are crucial for evaluating the effectiveness of the model on test data.
</p>

<hr>
<h2 id='GeneList-class'>GeneList class</h2><span id='topic+GeneList-class'></span>

<h3>Description</h3>

<p>A class to hold annotated gene list for a single method.
</p>


<h3>Slots</h3>


<dl>
<dt><code>SYMBOL</code></dt><dd><p>A character vector of gene names.</p>
</dd>
<dt><code>ENSEMBL</code></dt><dd><p>A character vector of Ensembl IDs.</p>
</dd>
<dt><code>ENTREZID</code></dt><dd><p>A character vector of Entrez IDs.</p>
</dd>
</dl>

<hr>
<h2 id='GeneSelectR'>Gene Selection and Evaluation with GeneSelectR</h2><span id='topic+GeneSelectR'></span>

<h3>Description</h3>

<p>This function performs gene selection using different methods on a given
training set and evaluates their performance using cross-validation. Optionally, it
also calculates permutation feature importances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GeneSelectR(
  X,
  y,
  pipelines = NULL,
  custom_fs_methods = NULL,
  selected_methods = NULL,
  custom_fs_grids = NULL,
  classifier = NULL,
  classifier_grid = NULL,
  preprocessing_steps = NULL,
  testsize = 0.2,
  validsize = 0.2,
  scoring = "accuracy",
  njobs = -1,
  n_splits = 5,
  search_type = "random",
  n_iter = 10,
  max_features = 50,
  calculate_permutation_importance = FALSE,
  perform_test_split = FALSE,
  random_state = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GeneSelectR_+3A_x">X</code></td>
<td>
<p>A matrix or data frame with features as columns and observations as rows.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_y">y</code></td>
<td>
<p>A vector of labels corresponding to the rows of X_train.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_pipelines">pipelines</code></td>
<td>
<p>An optional list of pre-defined pipelines to use for fitting and evaluation. If this argument is provided, the feature selection methods and preprocessing steps will be ignored.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_custom_fs_methods">custom_fs_methods</code></td>
<td>
<p>An optional list of feature selection methods to use for fitting and evaluation. If this argument is not provided, a default set of feature selection methods will be used.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_selected_methods">selected_methods</code></td>
<td>
<p>An optional vector of names of feature selection methods to use from the default set. If this argument is provided, only the specified methods will be used.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_custom_fs_grids">custom_fs_grids</code></td>
<td>
<p>An optional list of hyperparameter grids for the feature selection methods. Each element of the list should be a named list of parameters for a specific feature selection method. The names of the elements should match the names of the feature selection methods. If this argument is provided, the function will perform hyperparameter tuning for the specified feature selection methods in addition to the final estimator.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_classifier">classifier</code></td>
<td>
<p>An optional sklearn classifier. If left NULL then sklearn RandomForestClassifier is used.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_classifier_grid">classifier_grid</code></td>
<td>
<p>An optional named list of classifier parameters. If none are provided then default grid is used (check vignette for exact params).</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_preprocessing_steps">preprocessing_steps</code></td>
<td>
<p>An optional named list of sklearn preprocessing procedures. If none provided defaults are used (check vignette for exact params).</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_testsize">testsize</code></td>
<td>
<p>The size of the test set used in the evaluation.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_validsize">validsize</code></td>
<td>
<p>The size of the validation set used in the evaluation.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_scoring">scoring</code></td>
<td>
<p>A string representing what scoring metric to use for hyperparameter adjustment. Default value is 'accuracy'</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_njobs">njobs</code></td>
<td>
<p>Number of jobs to run in parallel.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_n_splits">n_splits</code></td>
<td>
<p>Number of train/test splits.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_search_type">search_type</code></td>
<td>
<p>A string indicating the type of search to use. 'grid' for GridSearchCV and 'random' for RandomizedSearchCV. Default is 'random'.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_n_iter">n_iter</code></td>
<td>
<p>An integer indicating the number of parameter settings that are sampled in RandomizedSearchCV. Only applies when search_type is 'random'.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_max_features">max_features</code></td>
<td>
<p>Maximum number of features to be selected by default feature selection methods. Max features cannot exceed the total number of features in a dataset.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_calculate_permutation_importance">calculate_permutation_importance</code></td>
<td>
<p>A boolean indicating whether to calculate permutation feature importance. Default is FALSE.</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_perform_test_split">perform_test_split</code></td>
<td>
<p>Whether to perform train and test split, to have an evaluation on unseen test set. The default value is set to FALSE</p>
</td></tr>
<tr><td><code id="GeneSelectR_+3A_random_state">random_state</code></td>
<td>
<p>An integer value setting the random seed for feature selection algorithms and cross validation procedure. By default set to NULL to use different random seed every time an algorithm is used. For reproducibility could be fixed, otherwise for an unbiased estimation should be left as NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>PipelineResults</code> with the following elements:
</p>

<ul>
<li><p> @field best_pipeline: A list of the best-fitted pipelines for each feature selection method and data split.
</p>
</li>
<li><p> @field cv_results: A list containing cross-validation results for each pipeline, including scores and other metrics.
</p>
</li>
<li><p> @field inbuilt_feature_importance: A list of the inbuilt feature importance scores for each pipeline, aggregated across all data splits.
</p>
</li>
<li><p> @field test_metrics: A data frame summarizing test metrics (precision, recall, F1 score, accuracy) for each pipeline, if a test split was performed.
</p>
</li>
<li><p> @field cv_mean_score: A data frame summarizing the mean cross-validation scores for each pipeline across all data splits.
</p>
</li>
<li><p> @field permutation_importance: A list of permutation importance scores for each pipeline, if permutation importance calculation was enabled.
This comprehensive return structure allows for in-depth analysis of the feature selection methods and model performance.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
if (GeneSelectR:::check_python_modules_available(c("numpy", "pandas", "sklearn", 'boruta'))) {
  # Create a mock dataset with 29 feature columns and 1 binary label column
  set.seed(123) # for reproducibility
  n_rows &lt;- 10
  n_features &lt;- 100

  # Randomly generate feature data
  X &lt;- as.data.frame(matrix(rnorm(n_rows * n_features), nrow = n_rows, ncol = n_features))
  # Ensure each feature has a variance greater than 0.85
  for(i in 1:ncol(X)) {
    while(var(X[[i]]) &lt;= 0.85) {
      X[[i]] &lt;- X[[i]] * 1.1
    }
  }
  colnames(X) &lt;- paste0("Feature", 1:n_features)

  # Create a mock binary label column
  y &lt;- factor(sample(c("Class1", "Class2"), n_rows, replace = TRUE))

  # Set up the environment
  GeneSelectR::configure_environment()
  GeneSelectR::set_reticulate_python()

  # Run GeneSelectR
  results &lt;- GeneSelectR(X, y)

  # Perform gene selection and evaluation using user-defined methods
  fs_methods &lt;- list("Lasso" = select_model(lasso(penalty = 'l1',
                                                  C = 0.1,
                                                  solver = 'saga'),
                                            threshold = 'median'))
  custom_fs_grids &lt;- list("Lasso" = list('C' = c(0.1, 1, 10)))
  results &lt;- GeneSelectR(X,
                         y,
                         max_features = 15,
                         custom_fs_methods = fs_methods,
                         custom_fs_grids = custom_fs_grids)
} else {
  message("Skipping example as not all required Python modules are available.")
}

</code></pre>

<hr>
<h2 id='get_feature_importances'>Get Feature Importances</h2><span id='topic+get_feature_importances'></span>

<h3>Description</h3>

<p>This function extracts feature importances from a Scikit-learn pipeline
that has a Gradient Boosting Classifier as the final step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_feature_importances(pipeline, X_train, pipeline_name, iter)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_feature_importances_+3A_pipeline">pipeline</code></td>
<td>
<p>A Scikit-learn pipeline object with a Gradient Boosting Classifier
as the final step.</p>
</td></tr>
<tr><td><code id="get_feature_importances_+3A_x_train">X_train</code></td>
<td>
<p>A DataFrame containing the training data.</p>
</td></tr>
<tr><td><code id="get_feature_importances_+3A_pipeline_name">pipeline_name</code></td>
<td>
<p>Strings (names of the selected_pipelines list) representing pipeline names that were constructed for the feature selection</p>
</td></tr>
<tr><td><code id="get_feature_importances_+3A_iter">iter</code></td>
<td>
<p>An integer that is indicating current iteration of the train-test split</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe containing the selected feature names and their importances, ranked by importance, or NULL
if the classifier does not have the appropriate attributes or the feature selector
does not have the 'get_support' or 'support_' method. Each row represents a feature, with columns for
feature names, importances, and ranks.
</p>

<hr>
<h2 id='GO_enrichment_analysis'>Perform gene set enrichment analysis using clusterProfiler</h2><span id='topic+GO_enrichment_analysis'></span>

<h3>Description</h3>

<p>This function performs gene set enrichment analysis on a list of gene sets extracted from an AnnotatedGeneLists object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GO_enrichment_analysis(
  annotated_gene_lists,
  list_type = "inbuilt",
  background = NULL,
  organism = "org.Hs.eg.db",
  keyType = "ENTREZID",
  ont = "BP",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.2,
  pAdjMethod = "fdr",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GO_enrichment_analysis_+3A_annotated_gene_lists">annotated_gene_lists</code></td>
<td>
<p>An AnnotatedGeneLists object containing a list of GeneList objects.</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_list_type">list_type</code></td>
<td>
<p>A type of AnnotatedGeneList from annotate_gene_lists function. Either 'inbuilt' or 'permutation'. (default: 'inbuilt')</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_background">background</code></td>
<td>
<p>A character vector representing the background gene set.</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_organism">organism</code></td>
<td>
<p>A character string corresponding to the organism of interest. Default: &quot;org.Hs.eg.db&quot; (for human).</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_keytype">keyType</code></td>
<td>
<p>A character string indicating the type of gene identifiers. Default: &quot;ENTREZID&quot;.</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_ont">ont</code></td>
<td>
<p>A character string representing GO term ontology. Default: &quot;BP&quot; (for Biological Process).</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_pvaluecutoff">pvalueCutoff</code></td>
<td>
<p>A numeric value specifying the significance cutoff. Default: 0.05.</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_qvaluecutoff">qvalueCutoff</code></td>
<td>
<p>A numeric value specifying the q-value cutoff. Default: 0.2.</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_padjmethod">pAdjMethod</code></td>
<td>
<p>A p-value adjustment method. Should be the one from &quot;holm&quot;, &quot;hochberg&quot;, &quot;hommel&quot;, &quot;bonferroni&quot;, &quot;BH&quot;, &quot;BY&quot;, &quot;fdr&quot;, &quot;none&quot;. Default is 'fdr</p>
</td></tr>
<tr><td><code id="GO_enrichment_analysis_+3A_...">...</code></td>
<td>
<p>Other parameters to be passed to clusterProfiler::enrichGO function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the enrichment results for each gene set.
Each element in the list is named after a gene set and contains an object produced by the <code>enrichGO</code> function from <code>clusterProfiler</code>.
This object includes various details about the enrichment analysis, such as enriched GO terms, their associated p-values, q-values, and other relevant statistics.
The list excludes results for the &quot;background&quot; gene set, focusing only on the gene sets of interest.
</p>


<h3>References</h3>

<p>To use clusterProfiler in published research, please cite:
Yu G, Wang LG, Han Y, He QY. clusterProfiler: an R package for comparing biological themes among gene clusters. OMICS: A Journal of Integrative Biology. 2012;16(5):284-287. doi:10.1089/omi.2011.0118.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Creating a mock AnnotatedGeneLists object
gene_symbols &lt;- c("GENE1", "GENE2", "GENE3")
ensembl_ids &lt;- c("ENSG000001", "ENSG000002", "ENSG000003")
entrez_ids &lt;- c("1001", "1002", "1003")

create_mock_gene_list &lt;- function() {
  new("GeneList",
       SYMBOL = gene_symbols,
       ENSEMBL = ensembl_ids,
       ENTREZID = entrez_ids)
}

mock_gene_list1 &lt;- create_mock_gene_list()
mock_gene_list2 &lt;- create_mock_gene_list()

annotated_gene_lists &lt;- new("AnnotatedGeneLists",
                            inbuilt = list(Lasso = mock_gene_list1,
                                           Univariate = mock_gene_list2),
                            permutation = list(Lasso = mock_gene_list1,
                                               Univariate = mock_gene_list2))

# Define a background gene set
background &lt;- c("GENE1", "GENE2", "GENE3")

# Perform GO enrichment analysis
results &lt;- GO_enrichment_analysis(annotated_gene_lists,
                                  background = background,
                                  organism = "org.Hs.eg.db",
                                  keyType = "SYMBOL")
print(results)

</code></pre>

<hr>
<h2 id='import_python_packages'>Import Python Libraries</h2><span id='topic+import_python_packages'></span>

<h3>Description</h3>

<p>This function imports the necessary Python libraries for the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>import_python_packages()
</code></pre>


<h3>Value</h3>

<p>A list containing references to imported Python libraries used in the package:
</p>

<ul>
<li><p> @field sklearn: Scikit-learn machine learning library.
</p>
</li>
<li><p> @field pandas: Data manipulation and analysis library.
</p>
</li>
<li><p> @field numpy: Library for numerical computing.
</p>
</li>
<li><p> @field lightgbm: Gradient boosting framework.
</p>
</li>
<li><p> @field xgboost: Optimized distributed gradient boosting library.
</p>
</li>
<li><p> @field boruta: Feature selection algorithm.
</p>
</li>
<li><p> @field sys: Python system-specific parameters and functions.
</p>
</li>
<li><p> @field multiprocessing: Support for concurrent execution using processes.
</p>
</li></ul>


<hr>
<h2 id='install_python_packages'>Install necessary Python packages in a specific Conda environment</h2><span id='topic+install_python_packages'></span>

<h3>Description</h3>

<p>This function installs the necessary Python packages in a specific Conda environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_python_packages(conda_env = "GeneSelectR_env")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_python_packages_+3A_conda_env">conda_env</code></td>
<td>
<p>The name of the Conda environment to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Installs necessary version of Python packages into the GeneSelectR_env.
</p>

<hr>
<h2 id='load_python_packages'>Load Python Modules</h2><span id='topic+load_python_packages'></span>

<h3>Description</h3>

<p>This function imports necessary Python modules with delayed loading.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_python_packages()
</code></pre>


<h3>Details</h3>

<p>Imports the following Python modules with <code>delay_load</code> set to TRUE:
</p>

<ul>
<li><p> sklearn
</p>
</li>
<li><p> pandas
</p>
</li>
<li><p> numpy
</p>
</li>
<li><p> boruta
</p>
</li>
<li><p> sys
</p>
</li>
<li><p> multiprocessing
</p>
</li>
<li><p> skopt
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, the global variables for each Python module are set.
Otherwise, it will return an error message.
</p>

<hr>
<h2 id='perform_grid_search'>Perform Grid Search or Random Search for Hyperparameter Tuning</h2><span id='topic+perform_grid_search'></span>

<h3>Description</h3>

<p>Perform Grid Search or Random Search for Hyperparameter Tuning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform_grid_search(
  X_train,
  y_train,
  pipeline,
  scoring,
  params,
  search_type,
  n_iter,
  njobs,
  modules,
  random_state
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="perform_grid_search_+3A_x_train">X_train</code></td>
<td>
<p>Training data for predictors.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_y_train">y_train</code></td>
<td>
<p>Training data for outcomes.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_pipeline">pipeline</code></td>
<td>
<p>A pipeline specifying the steps for feature selection and model training.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_scoring">scoring</code></td>
<td>
<p>A string representing what scoring metric to use for hyperparameter adjustment. Default value is 'accuracy'</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_params">params</code></td>
<td>
<p>A list of parameters or parameter distributions to search over.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_search_type">search_type</code></td>
<td>
<p>A character string specifying the type of search ('grid' or 'random').</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_n_iter">n_iter</code></td>
<td>
<p>The number of parameter settings that are sampled in a random search.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_njobs">njobs</code></td>
<td>
<p>The number of CPU cores to use.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_modules">modules</code></td>
<td>
<p>A list containing the definitions for the Python modules and submodules.</p>
</td></tr>
<tr><td><code id="perform_grid_search_+3A_random_state">random_state</code></td>
<td>
<p>An integer value setting the random seed for feature selection algorithms and randomized search CV procedure. By default set to NULL to use different random seed every time an algorithm is used. For reproducibility could be fixed, otherwise for an unbiased estimation should be left as NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a scikit-learn GridSearchCV, RandomizedSearchCV, or BayesSearchCV object, depending on the <code>search_type</code> specified.
This object includes several attributes useful for analyzing the hyperparameter tuning process:
- @field best_estimator_: The best estimator chosen by the search.
- @field best_score_: The score of the best_estimator on the left-out data.
- @field best_params_: The parameter setting that gave the best results on the hold-out data.
- @field cv_results_: A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame.
- @field scorer_: Scoring method used on the held-out data.
- @field n_splits_: The number of cross-validation splits (folds/iterations).
These attributes provide insights into the model's performance and the effectiveness of the selected hyperparameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
required_modules &lt;- c("sklearn", "boruta")
modules_available &lt;- sapply(required_modules, reticulate::py_module_available)

if (all(modules_available)) {
# Assuming X_train, y_train, pipeline, and params are predefined
# Define sklearn modules (assuming 'define_sklearn_modules' is defined)
sklearn_modules &lt;- define_sklearn_modules()

# Perform a grid search
optimal_model &lt;- perform_grid_search(X_train, y_train, pipeline, "accuracy",
                                    params, "grid", NULL, 1, sklearn_modules, NULL)


# Perform a random search
optimal_model_random &lt;- perform_grid_search(X_train, y_train, pipeline, "accuracy",
                                            params, "random", 10, 1, sklearn_modules, 42)

} else {
unavailable_modules &lt;- names(modules_available[!modules_available])
message(paste("Required Python modules not available:",
  paste(unavailable_modules, collapse=', '), ". Skipping example."))
}

</code></pre>

<hr>
<h2 id='pipeline_to_list'>Convert Scikit-learn Pipeline to Named List</h2><span id='topic+pipeline_to_list'></span>

<h3>Description</h3>

<p>This function takes a Scikit-learn Pipeline object and converts it to a named list in R.
Each step in the pipeline becomes an element in the list with the name of the step as the name of the list element.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pipeline_to_list(pipeline)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pipeline_to_list_+3A_pipeline">pipeline</code></td>
<td>
<p>A Scikit-learn Pipeline object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list where each element represents a step in the Scikit-learn Pipeline.
The names of the list elements correspond to the names of the steps in the pipeline.
Each element of the list is an R representation of the respective step in the pipeline.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming a Scikit-learn pipeline object 'sklearn_pipeline' is defined in Python
# and available in R via reticulate
sklearn_pipeline &lt;- reticulate::import("sklearn.pipeline")$Pipeline(steps = list(
  list("scaler", reticulate::import("sklearn.preprocessing")$StandardScaler()),
  list("classifier", reticulate::import("sklearn.ensemble")$RandomForestClassifier())
))

# Convert the Scikit-learn pipeline to a named list in R
pipeline_list &lt;- pipeline_to_list(sklearn_pipeline)
print(pipeline_list)

</code></pre>

<hr>
<h2 id='PipelineResults-class'>PipelineResults class</h2><span id='topic+PipelineResults-class'></span>

<h3>Description</h3>

<p>A class to hold the results of GeneSelectR function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>best_pipeline</code></dt><dd><p>A named list containing parameters of the best performer pipeline</p>
</dd>
<dt><code>cv_results</code></dt><dd><p>A list of the cross-validation results for each pipeline.</p>
</dd>
<dt><code>inbuilt_feature_importance</code></dt><dd><p>A list of the inbuilt mean feature importances for each method across all splits.</p>
</dd>
<dt><code>permutation_importance</code></dt><dd><p>A list of the permutation importances for each method.</p>
</dd>
<dt><code>cv_mean_score</code></dt><dd><p>A data.frame of mean scores from cross-validation.</p>
</dd>
<dt><code>test_metrics</code></dt><dd><p>A data.frame containing metrics (F1, accuracy, precision, and recall) calculated on the unseen test set. Contains mean values across splits as well as standard deviation.</p>
</dd>
</dl>

<hr>
<h2 id='plot_feature_importance'>Plot Feature Importance</h2><span id='topic+plot_feature_importance'></span>

<h3>Description</h3>

<p>This function plots the feature importance scores from <code>inbuilt_feature_importance</code> and <code>permutation_importance</code> in the <code>PipelineResults</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_feature_importance(pipelineresults, top_n_features = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_feature_importance_+3A_pipelineresults">pipelineresults</code></td>
<td>
<p>An object of class <code>PipelineResults</code>.</p>
</td></tr>
<tr><td><code id="plot_feature_importance_+3A_top_n_features">top_n_features</code></td>
<td>
<p>An integer specifying the top N features to plot based on their mean importance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of grid plot objects (ggplot objects) for each feature selection method in the <code>PipelineResults</code> object.
Each plot visualizes the top N features based on their mean importance scores, including both inbuilt and permutation importances (if available).
The plots are arranged in a grid layout for easy comparison.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming `pipelineresults` is a PipelineResults object

pipelineresults &lt;- new("PipelineResults",
                       inbuilt_feature_importance = list("Method1" = data.frame(
                                                         feature = LETTERS[1:10],
                                                         mean_importance = runif(10)),
                                                         "Method2" = data.frame(
                                                         feature = LETTERS[1:10],
                                                         mean_importance = runif(10))),
                       permutation_importance = list("Method1" = data.frame(
                                                                 feature = LETTERS[1:10],
                                                                 mean_importance = runif(10))))

# Plot the feature importance
importance_plots &lt;- plot_feature_importance(pipelineresults, top_n_features = 5)
print(importance_plots)

</code></pre>

<hr>
<h2 id='plot_metrics'>Plot Performance Metrics</h2><span id='topic+plot_metrics'></span>

<h3>Description</h3>

<p>This function creates separate plots for f1_mean, recall_mean, precision_mean, and accuracy_mean from a given dataframe, then arranges these plots using cowplot::plot_grid. Requires ggplot2 and cowplot packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_metrics(pipeline_results)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_metrics_+3A_pipeline_results">pipeline_results</code></td>
<td>
<p>An object of class &quot;PipelineResults&quot; containing the performance metrics. This object is expected to contain a dataframe with the columns 'method', 'f1_mean', 'f1_sd', 'recall_mean', 'recall_sd', 'precision_mean', 'precision_sd', 'accuracy_mean', 'accuracy_sd'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A combined ggplot object displaying the performance metrics.
- If test metrics are provided, it includes separate plots for F1 mean, recall mean, precision mean, and accuracy mean, along with cross-validation mean score, arranged in a grid layout.
- If no test metrics are available, it returns only the cross-validation mean score plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming `pipeline_results` is a PipelineResults object with test metrics and CV mean score
pipeline_results &lt;- new("PipelineResults",
                        test_metrics = data.frame(
                        method = c("Method1", "Method2"),
                        f1_mean = c(0.8, 0.85), f1_sd = c(0.05, 0.04),
                        recall_mean = c(0.75, 0.78), recall_sd = c(0.06, 0.05),
                        precision_mean = c(0.85, 0.88), precision_sd = c(0.05, 0.04),
                        accuracy_mean = c(0.9, 0.92), accuracy_sd = c(0.03, 0.02)),
                        cv_mean_score = data.frame(
                        method = c("Method1", "Method2"),
                        mean_score = c(0.88, 0.9), sd_score = c(0.02, 0.02)))

# Plot the performance metrics
metric_plots &lt;- plot_metrics(pipeline_results)
print(metric_plots)


</code></pre>

<hr>
<h2 id='plot_overlap_heatmaps'>Generate Heatmaps to Visualize Overlap and Similarity Coefficients between Feature Lists</h2><span id='topic+plot_overlap_heatmaps'></span>

<h3>Description</h3>

<p>This function takes a list of matrices of overlap and similarity coefficients and generates heatmaps to visualize them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_overlap_heatmaps(coefficients, save_plot = FALSE, filename = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_overlap_heatmaps_+3A_coefficients">coefficients</code></td>
<td>
<p>A list of matrices showing the Overlap, Jaccard, and Soerensen-Dice coefficients for the feature lists.</p>
</td></tr>
<tr><td><code id="plot_overlap_heatmaps_+3A_save_plot">save_plot</code></td>
<td>
<p>A logical value indicating whether to save the heatmap plots to a file or not. Default is FALSE.</p>
</td></tr>
<tr><td><code id="plot_overlap_heatmaps_+3A_filename">filename</code></td>
<td>
<p>A character string specifying the filename for the saved heatmap plots (if save_plot = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A grid of ggplot2 heatmap objects visualizing the Overlap, Jaccard, and Soerensen-Dice coefficients.
The grid layout includes heatmaps for both 'inbuilt' and 'permutation' feature importance coefficients (if available).
If <code>save_plot</code> is TRUE, the heatmaps are also saved to the specified file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming `coefficients` is a list containing matrices for Overlap, Jaccard,
# and Soerensen-Dice coefficients
# For demonstration, let's create a mock coefficients list
mock_matrix &lt;- matrix(runif(25), nrow = 5)
coefficients &lt;- list(inbuilt_feature_importance_coefficient = list(overlap = mock_matrix,
                     jaccard = mock_matrix, soerensen = mock_matrix),
                     permutation_importance_coefficients = list(overlap = mock_matrix,
                     jaccard = mock_matrix, soerensen = mock_matrix))

# Plot the overlap heatmaps
heatmap_plots &lt;- plot_overlap_heatmaps(coefficients)
print(heatmap_plots)

</code></pre>

<hr>
<h2 id='plot_upset'>Plot Feature Overlaps Using UpSet Plots</h2><span id='topic+plot_upset'></span>

<h3>Description</h3>

<p>This function produces separate UpSet plots for inbuilt feature importances and permutation importances,
allowing you to visualize the overlap of feature lists. Optionally, you can include custom lists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_upset(pipeline_results, custom_lists = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_upset_+3A_pipeline_results">pipeline_results</code></td>
<td>
<p>A PipelineResults object containing the fitted pipelines, cross-validation results, selected features,
mean performance, and mean feature importances.</p>
</td></tr>
<tr><td><code id="plot_upset_+3A_custom_lists">custom_lists</code></td>
<td>
<p>An optional named list of character vectors. Each character vector should contain feature names.
The names of the list will be used as names in the UpSet plots.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing two UpSet plots:
</p>

<ul>
<li><p> @field inbuilt_importance: An UpSet plot visualizing overlaps of inbuilt feature importances.
</p>
</li>
<li><p> @field permutation_importance: An UpSet plot (if permutation importance is available) visualizing overlaps of permutation importances.
Each plot provides an interactive way to explore the intersections and unique elements of the feature lists.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Mock data for PipelineResults
pipeline_results &lt;- new("PipelineResults",
                        inbuilt_feature_importance = list(
                          Method1 = data.frame(feature = c("gene1", "gene2", "gene3")),
                          Method2 = data.frame(feature = c("gene2", "gene4"))),
                        permutation_importance = list(
                          Method1 = data.frame(feature = c("gene1", "gene5")),
                          Method2 = data.frame(feature = c("gene3", "gene6"))))

# Mock custom lists
custom_lists &lt;- list("custom1" = c("gene1", "gene2"), "custom2" = c("gene3", "gene4"))

# Generate UpSet plots
result &lt;- plot_upset(pipeline_results, custom_lists)
print(result$inbuilt_importance)
print(result$permutation_importance)

</code></pre>

<hr>
<h2 id='python-modules'>Global references to Python modules</h2><span id='topic+python-modules'></span><span id='topic+sklearn'></span>

<h3>Description</h3>

<p>These are global references to Python modules that will be initialized in .onLoad.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sklearn
</code></pre>


<h3>Format</h3>

<p>An object of class <code>NULL</code> of length 0.
</p>


<h3>Details</h3>

<p>The following Python modules are referenced:
</p>

<ul>
<li><p> sklearn
</p>
</li>
<li><p> pandas
</p>
</li>
<li><p> numpy
</p>
</li>
<li><p> lightgbm
</p>
</li>
<li><p> xgboost
</p>
</li>
<li><p> boruta
</p>
</li>
<li><p> sys
</p>
</li>
<li><p> multiprocessing
</p>
</li></ul>


<hr>
<h2 id='run_simplify_enrichment'>Run simplifyGOFromMultipleLists with specified measure and method</h2><span id='topic+run_simplify_enrichment'></span>

<h3>Description</h3>

<p>This function is simply a wrapper for the simplifyGOFromMultipleLists function in the simplifyEnrichment package,
created for the ease of data input. All credit for the underlying functionality goes to the authors
of the simplifyEnrichment package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_simplify_enrichment(
  fs_GO_results,
  padj_column = "p.adjust",
  padj_cutoff,
  ont,
  measure,
  method,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_simplify_enrichment_+3A_fs_go_results">fs_GO_results</code></td>
<td>
<p>A list of dataframes containing GO enrichment results for feature selection methods. The GO object for the simplifyGOFromMultipleLists function.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_padj_column">padj_column</code></td>
<td>
<p>Character. The column name for the p-value adjustment.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_padj_cutoff">padj_cutoff</code></td>
<td>
<p>Numeric. The cutoff for the p-value adjustment.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_ont">ont</code></td>
<td>
<p>Character. The ontology for the simplifyGOFromMultipleLists function.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_measure">measure</code></td>
<td>
<p>Character. The semantic similarity measure for the simplifyGOFromMultipleLists function.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_method">method</code></td>
<td>
<p>Character. The clustering method for the simplifyGOFromMultipleLists function.</p>
</td></tr>
<tr><td><code id="run_simplify_enrichment_+3A_...">...</code></td>
<td>
<p>Other parameters that can be passed to simplifyGOFromMultipleLists</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the <code>simplifyGOFromMultipleLists</code> function, typically comprising a heatmap or other
visualization that displays the simplified GO enrichment results. The specific output format depends
on the chosen semantic similarity measure and clustering method.
</p>


<h3>References</h3>

<p>For more information on the simplifyEnrichment package, see the original publication:
Gu Z, Hbschmann D. simplifyEnrichment: A Bioconductor Package for Clustering and Visualizing Functional Enrichment Results.
Genomics Proteomics Bioinformatics. 2023 Feb;21(1):190-202. doi: 10.1016/j.gpb.2022.04.008.
Epub 2022 Jun 6. PMID: 35680096; PMCID: PMC10373083.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Mock GO enrichment results for two feature selection methods
fs_GO_results &lt;- list(
  method1 = list(result = data.frame(GO_ID = c("GO:0008150", "GO:0009987"),
                                    Description = c("Biological Process 1", "Biological Process 2"),
                                    'p.adjust' = c(0.01, 0.02))),
  method2 = list(result = data.frame(GO_ID = c("GO:0008150", "GO:0008152"),
                                    Description = c("Biological Process 1", "Biological Process 3"),
                                    'p.adjust' = c(0.03, 0.04)))
)

# Run the wrapper function with mock data
enrichment_result &lt;- run_simplify_enrichment(fs_GO_results,
                                             padj_column = 'p.adjust',
                                             padj_cutoff = 0.05,
                                             ont = "BP",
                                             measure = "Wang",
                                             method = "kmeans")
print(enrichment_result)


</code></pre>

<hr>
<h2 id='set_default_fs_methods'>Set Default Feature Selection Methods</h2><span id='topic+set_default_fs_methods'></span>

<h3>Description</h3>

<p>Set Default Feature Selection Methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_default_fs_methods(modules, max_features, random_state)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_default_fs_methods_+3A_modules">modules</code></td>
<td>
<p>A list containing the definitions for the Python modules and submodules.</p>
</td></tr>
<tr><td><code id="set_default_fs_methods_+3A_max_features">max_features</code></td>
<td>
<p>The maximum number of features to consider.</p>
</td></tr>
<tr><td><code id="set_default_fs_methods_+3A_random_state">random_state</code></td>
<td>
<p>An integer value setting the random seed for feature selection algorithms and cross validation procedure. By default set to NULL to use different random seed every time an algorithm is used. For reproducibility could be fixed, otherwise for an unbiased estimation should be left as NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing preprocessing steps and default feature selection methods.
</p>

<hr>
<h2 id='set_default_param_grids'>Set Default Parameter Grids for Feature Selection</h2><span id='topic+set_default_param_grids'></span>

<h3>Description</h3>

<p>Set Default Parameter Grids for Feature Selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_default_param_grids(max_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_default_param_grids_+3A_max_features">max_features</code></td>
<td>
<p>An integer indicating max_features to select in Univariate select</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the default parameter grids for feature selection methods.
</p>

<hr>
<h2 id='set_reticulate_python'>Set RETICULATE_PYTHON for the Current Session</h2><span id='topic+set_reticulate_python'></span>

<h3>Description</h3>

<p>This function sets the RETICULATE_PYTHON environment variable to the path of the Python interpreter
in the specified Conda environment, but only for the current R session. The change will not persist
after the R session is closed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_reticulate_python(env_name = "GeneSelectR_env")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_reticulate_python_+3A_env_name">env_name</code></td>
<td>
<p>The name of the Conda environment. Default is 'GeneSelectR_env'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function checks if the specified Conda environment exists. If it does, the function sets the
RETICULATE_PYTHON environment variable to the path of the Python interpreter in that environment.
If the environment does not exist, the function stops with an error message.
</p>
<p>Users need to run this function in every new R session where they want to use your package. Also,
they should run this function before loading your package with library(), because the RETICULATE_PYTHON
environment variable needs to be set before reticulate is loaded.
</p>


<h3>Value</h3>

<p>This function does not return a value. Instead, it sets the RETICULATE_PYTHON environment variable for the
current R session and prints a message indicating the new value of RETICULATE_PYTHON. If the specified
environment does not exist, the function stops with an error message.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set_reticulate_python('GeneSelectR_env')

</code></pre>

<hr>
<h2 id='skip_if_no_modules'>Check if Python Modules are Available</h2><span id='topic+skip_if_no_modules'></span>

<h3>Description</h3>

<p>This helper function checks if a list of Python modules are available. If any are not, it skips the tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_if_no_modules(module_names)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="skip_if_no_modules_+3A_module_names">module_names</code></td>
<td>
<p>A vector of names of the Python modules to check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing is returned explicitly, but if a specified module is not available, the function invokes <code>testthat::skip</code> to skip the tests that require that module.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage within a test file:
module_names &lt;- c("numpy", "pandas", "sklearn")
skip_if_no_modules(module_names)


</code></pre>

<hr>
<h2 id='split_data'>Split Data into Training and Test Sets</h2><span id='topic+split_data'></span>

<h3>Description</h3>

<p>Split Data into Training and Test Sets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_data(X, y, test_size, modules)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_data_+3A_x">X</code></td>
<td>
<p>A dataframe or matrix of predictors.</p>
</td></tr>
<tr><td><code id="split_data_+3A_y">y</code></td>
<td>
<p>A vector of outcomes.</p>
</td></tr>
<tr><td><code id="split_data_+3A_test_size">test_size</code></td>
<td>
<p>Proportion of the data to be used as the test set.</p>
</td></tr>
<tr><td><code id="split_data_+3A_modules">modules</code></td>
<td>
<p>A list containing the definitions for the Python modules and submodules.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the split datasets:
</p>

<ul>
<li><p> @field X_train: Training set for predictors, converted to Python format.
</p>
</li>
<li><p> @field X_test: Test set for predictors, converted to Python format.
</p>
</li>
<li><p> @field y_train: Training set for outcomes, converted to Python format.
</p>
</li>
<li><p> @field y_test: Test set for outcomes, converted to Python format.
The function ensures that the data is appropriately partitioned and formatted for use in Python-based analysis.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming 'data' is your dataset with predictors and 'outcome' is the target variable
# Define sklearn modules (assuming 'define_sklearn_modules' is defined)
sklearn_modules &lt;- define_sklearn_modules()

# Split the data into training and test sets
split_results &lt;- split_data(data, outcome, test_size = 0.2, modules = sklearn_modules)



</code></pre>

<hr>
<h2 id='steps_to_tuples'>Convert Steps to Tuples</h2><span id='topic+steps_to_tuples'></span>

<h3>Description</h3>

<p>This function converts a list of steps to tuples for use in a Scikit-learn pipeline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>steps_to_tuples(steps)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="steps_to_tuples_+3A_steps">steps</code></td>
<td>
<p>A list of steps to convert to tuples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of tuples, where each tuple represents a step in a Scikit-learn pipeline.
The tuple contains the name of the step and the corresponding step object.
</p>

<hr>
<h2 id='TestMetrics-class'>Class Union for Test metrics output that could contain either a dataframe or a lists</h2><span id='topic+TestMetrics-class'></span>

<h3>Description</h3>

<p>A class union that can contain either a data frame or a list.
</p>


<h3>See Also</h3>

<p><a href="methods.html#topic+setClassUnion">setClassUnion</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
