<!DOCTYPE html><html><head><title>Help for package clusterMI</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {clusterMI}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#clusterMI-package'><p>clusterMI: Cluster Analysis with Missing Values by Multiple Imputation</p></a></li>
<li><a href='#chooseB'><p>Diagnostic plot for the number of iterations used in the varselbest function</p></a></li>
<li><a href='#chooseB.intern'><p>Tune the number of iterations for variable selection using varselbest</p></a></li>
<li><a href='#choosem'><p>Graphical investigation for the number of datasets generated by multiple imputation</p></a></li>
<li><a href='#choosemaxit'><p>Diagnostic plot for the number of iterations used in sequential imputation methods</p></a></li>
<li><a href='#choosenbclust'><p>Tune the number of clusters according to the partition instability</p></a></li>
<li><a href='#chooser'><p>Kfold cross-validation for specifying threshold r</p></a></li>
<li><a href='#cluster.intern'><p>Apply clustering method after multiple imputation</p></a></li>
<li><a href='#clusterMI'><p>Cluster analysis and pooling after multiple imputation</p></a></li>
<li><a href='#fastnmf'><p>Consensus clustering using non-negative matrix factorization</p></a></li>
<li><a href='#imputedata'><p>Multiple imputation methods for cluster analysis</p></a></li>
<li><a href='#mclustboot.intern'><p>MclustBootstrap with nboot = 1 and the same output as Mclust</p></a></li>
<li><a href='#myem.mix'><p>internal function</p></a></li>
<li><a href='#onefold.chooser'><p>one fold cross-validation for specifying threshold r</p></a></li>
<li><a href='#overimpute'><p>Overimputation diagnostic plot</p></a></li>
<li><a href='#prodna'><p>Introduce missing values using a missing completely at random mechanism</p></a></li>
<li><a href='#Rcpp_modelobject-class'><p>Class <code>"Rcpp_modelobject"</code></p></a></li>
<li><a href='#Silhouette.intern'><p>Compute Silhouette index</p></a></li>
<li><a href='#varselbest'><p>Variable selection for specifying conditional imputation models</p></a></li>
<li><a href='#wine'><p>Chemical analysis of wines from three different cultivars</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Cluster Analysis with Missing Values by Multiple Imputation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-07-05</td>
</tr>
<tr>
<td>Description:</td>
<td>Allows clustering of incomplete observations by addressing missing values using multiple imputation. For achieving this goal, the methodology consists in three steps, following Audigier and Niang 2022 &lt;<a href="https://doi.org/10.1007%2Fs11634-022-00519-1">doi:10.1007/s11634-022-00519-1</a>&gt;. I) Missing data imputation using dedicated models. Four multiple imputation methods are proposed, two are based on joint modelling and two are fully sequential methods, as discussed in  Audigier et al. (2021) &lt;<a href="https://doi.org/10.48550%2FarXiv.2106.04424">doi:10.48550/arXiv.2106.04424</a>&gt;. II) cluster analysis of imputed data sets. Six clustering methods are available (distances-based or model-based), but custom methods can also be easily used. III) Partition pooling. The set of partitions is aggregated using Non-negative Matrix Factorization based method. An associated instability measure is computed by bootstrap (see Fang, Y. and Wang, J., 2012 &lt;<a href="https://doi.org/10.1016%2Fj.csda.2011.09.003">doi:10.1016/j.csda.2011.09.003</a>&gt;). Among applications, this instability measure can be used to choose a number of clusters with missing values.
    The package also proposes several diagnostic tools to tune the number of imputed data sets, to tune the number of iterations in fully sequential imputation, to check the fit of imputation models, etc.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, parallel, mice, micemd, mclust, mix, fpc,
knockoff, withr, glmnet, ClusterR, FactoMineR, diceR,
NPBayesImputeCat, e1071, Rfast, cat, utils, ggplot2, gridExtra,
reshape2, methods, Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, stargazer, VIM, missMDA, clustrd,
clusterCrit, bookdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RcppModules:</td>
<td>IO_module</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-05 10:41:59 UTC; vince</td>
</tr>
<tr>
<td>Author:</td>
<td>Vincent Audigier [aut, cre] (CNAM MSDMA team),
  Hang Joon Kim [ctb] (University of Cincinnati)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Vincent Audigier &lt;vincent.audigier@cnam.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-07 16:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='clusterMI-package'>clusterMI: Cluster Analysis with Missing Values by Multiple Imputation</h2><span id='topic+clusterMI-package'></span>

<h3>Description</h3>

<p><code>clusterMI</code> is an R package to perform clustering with missing values. For achieving this goal, multiple imputation is used.
The package offers various multiple imputation methods dedicated to clustered individuals, as discussed in Audigier et al. (2021) &lt;arXiv:2106.04424&gt;.
In addition, it allows pooling results both in terms of partition and instability, as proposed in Audigier and Niang (2022) &lt;doi:10.1007/s11634-022-00519-1&gt;. Among applications, this instability measure can be used to choose a number of clusters with missing values.
</p>


<h3>References</h3>

<p>Audigier, V. and Niang, N., Clustering with missing data: which equivalent for Rubin's rules? Advances in Data Analysis and Classification &lt;doi:10.1007/s11634-022-00519-1&gt;, 2022.
</p>
<p>Audigier, V., Niang, N., &amp; Resche-Rigon, M. (2021). Clustering with missing data: which imputation model for which cluster analysis method?. arXiv preprint &lt;arXiv:2106.04424&gt;.
</p>
<p>Audigier, V. (2024). Clustering on incomplete data using clusterMI. 10emes Rencontres R, Vannes, France. hal preprint &lt;hal:04550305&gt;.
</p>
<p>Bar-Hen, A. and Audigier, V., An ensemble learning method for variable selection: application to high dimensional data and missing values, Journal of Statistical Computation and Simulation, &lt;doi:10.1080/00949655.2022.2070621&gt;, 2022.
</p>
<p>Fang, Y. and Wang, J., Selection of the number of clusters via the bootstrap method. Computational Statistics and Data Analysis, 56, 468-477 &lt;doi:10.1016/j.csda.2011.09.003&gt; 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)


# imputation
m &lt;- 5 # number of imputed data sets. Should be larger in practice
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)

# cluster analysis by kmeans and pooling
nnodes &lt;- 2 # Number of CPU cores for parallel computing
res.pool &lt;- clusterMI(res.imp, nnodes = nnodes)

res.pool$instability
table(ref, res.pool$part)

# choice of nb.clust

res.nbclust &lt;- choosenbclust(res.pool)
res.nbclust$nb.clust


</code></pre>

<hr>
<h2 id='chooseB'>Diagnostic plot for the number of iterations used in the varselbest function</h2><span id='topic+chooseB'></span>

<h3>Description</h3>

<p><code>chooseB</code> plots the proportion of times an explanatory variable is selected according to the number of iterations (B).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chooseB(
  res.varselbest,
  plotvar = NULL,
  linewidth = 1,
  linetype = "dotdash",
  xlab = "B",
  ylab = "Proportion",
  nrow = 2,
  ncol = 2,
  graph = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chooseB_+3A_res.varselbest">res.varselbest</code></td>
<td>
<p>an output from the varselbest function</p>
</td></tr>
<tr><td><code id="chooseB_+3A_plotvar">plotvar</code></td>
<td>
<p>index of variables for which a curve is ploted</p>
</td></tr>
<tr><td><code id="chooseB_+3A_linewidth">linewidth</code></td>
<td>
<p>a numerical value setting the widths of lines</p>
</td></tr>
<tr><td><code id="chooseB_+3A_linetype">linetype</code></td>
<td>
<p>what type of plot should be drawn</p>
</td></tr>
<tr><td><code id="chooseB_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="chooseB_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="chooseB_+3A_nrow">nrow</code></td>
<td>
<p>argument of gtable. Default value is 2.</p>
</td></tr>
<tr><td><code id="chooseB_+3A_ncol">ncol</code></td>
<td>
<p>argument of gtable. Default value is 2.</p>
</td></tr>
<tr><td><code id="chooseB_+3A_graph">graph</code></td>
<td>
<p>a boolean. If FALSE, no graphics are ploted. Default value is TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>varselbest</code> performs variable selection on random subsets of variables and, then, combines them to recover which explanatory variables are related to the response, following Bar-Hen and Audigier (2022) &lt;doi:10.1080/00949655.2022.2070621&gt;.
More precisely, the outline of the algorithm are as follows: let consider a random subset of <code>sizeblock</code> among p variables.
Then,  any selection variables scheme can be applied.
By resampling <code>B</code> times, a sample of size <code>sizeblock</code> among the p variables, we may count how many times a variable is considered as significantly related to the response and how many times it is not.
The number of iterations <code>B</code> should be large so that the proportion of times a variable is selected becomes stable. <code>chooseB</code> plots the values of proportion according to the number of iterations.
</p>


<h3>Value</h3>

<p>a list of matrices where each row corresponds to the vector of proportions (for all explanatory variables) obtained for a given value of B
</p>


<h3>References</h3>

<p>Bar-Hen, A. and Audigier, V., An ensemble learning method for variable selection: application to high dimensional data and missing values, Journal of Statistical Computation and Simulation, &lt;doi:10.1080/00949655.2022.2070621&gt;, 2022.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varselbest">varselbest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na&lt;-wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

nnodes &lt;- 2 # Number of CPU cores for parallel computing
B &lt;- 80 # Number of iterations for variable selection

# variable selection

res.varsel &lt;- varselbest(data.na = wine.na,
                        listvar = "alco",
                        B = B,
                        nnodes = nnodes,
                        nb.clust = nb.clust,
                        graph = FALSE)
# convergence
res.chooseB &lt;- chooseB(res.varsel)

</code></pre>

<hr>
<h2 id='chooseB.intern'>Tune the number of iterations for variable selection using varselbest</h2><span id='topic+chooseB.intern'></span>

<h3>Description</h3>

<p>Tune the number of iterations for variable selection using varselbest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chooseB.intern(
  res.varselbest,
  linewidth = 1,
  linetype = "dotdash",
  xlab = "B",
  ylab = "Proportion",
  graph = TRUE,
  title = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chooseB.intern_+3A_res.varselbest">res.varselbest</code></td>
<td>
<p>an output from the varselbest function</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_linewidth">linewidth</code></td>
<td>
<p>a numerical value setting the widths of lines</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_linetype">linetype</code></td>
<td>
<p>what type of plot should be drawn</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_graph">graph</code></td>
<td>
<p>a boolean. If FALSE, no graphics are ploted. Default value is TRUE</p>
</td></tr>
<tr><td><code id="chooseB.intern_+3A_title">title</code></td>
<td>
<p>the main title</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bar-Hen, A. and Audigier, V. An ensemble learning method for variable selection: application to high dimensional data and missing values. ArXiv e-prints &lt;arXiv:1808.06952&gt;
</p>

<hr>
<h2 id='choosem'>Graphical investigation for the number of datasets generated by multiple imputation</h2><span id='topic+choosem'></span>

<h3>Description</h3>

<p>For an object generated by the function <code>clusterMI</code>, the <code>choosem</code> function browses the sequence of the contributory partitions and computes the consensus partition at each step. Then, the rand index between successive consensus partitions is plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choosem(output, graph = TRUE, nnodes = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choosem_+3A_output">output</code></td>
<td>
<p>an output from the clusterMI function</p>
</td></tr>
<tr><td><code id="choosem_+3A_graph">graph</code></td>
<td>
<p>a boolean indicating if a graphic is plotted</p>
</td></tr>
<tr><td><code id="choosem_+3A_nnodes">nnodes</code></td>
<td>
<p>number of CPU cores for parallel computing. By default <code>nnodes = 1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of imputed datasets (<code>m</code>) should be sufficiently large to improve the partition accuracy.
The <code>choosem</code> function can be used to check if this number is suitable.
This function computes the consensus partition by considering only the first imputed datasets.
By this way, a sequence of <code>m</code> consensus partitions is obtained.
Then, the rand index between successive partitions is computed and reported in a graph.
The rand index measures the proximity between two partitions.
If the rand index between the last consensus partitions of the sequence reaches its maximum values (1),
then it means last imputed dataset does not modify the consensus partition.
Consequently, the number of imputed datasets can be considered as sufficiently large.
</p>


<h3>Value</h3>

<p>A list of two objects
</p>
<table>
<tr><td><code>part</code></td>
<td>
<p><code>m</code>-columns matrix that contains in column p the consensus partition using only the p first imputed datasets</p>
</td></tr>
<tr><td><code>rand</code></td>
<td>
<p>a <code>m</code>-1 vector given the rand index between the <code>m</code> successive consensus partitions</p>
</td></tr>
</table>


<h3>References</h3>

<p>Audigier, V. and Niang, N., Clustering with missing data: which equivalent for Rubin's rules? Advances in Data Analysis and Classification &lt;doi:10.1007/s11634-022-00519-1&gt;, 2022.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusterMI">clusterMI</a></code>, <code><a href="#topic+imputedata">imputedata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

#imputation
m &lt;- 5 # number of imputed data sets. Should be larger in practice
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)

#pooling
nnodes &lt;- 2 # number of CPU cores for parallel computing
res.pool &lt;- clusterMI(res.imp, instability = FALSE, nnodes = nnodes)

res.choosem &lt;- choosem(res.pool)
</code></pre>

<hr>
<h2 id='choosemaxit'>Diagnostic plot for the number of iterations used in sequential imputation methods</h2><span id='topic+choosemaxit'></span>

<h3>Description</h3>

<p>The <code>choosemaxit</code> function plots the within and between variance for each variable (specified in <code>plotvars</code>) against the iteration number for each of the replications (specified in <code>plotm</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choosemaxit(
  output,
  plotvars = NULL,
  plotm = 1:5,
  size = 0.5,
  linewidth = 1,
  linetype = "dotdash",
  xlab = "iterations",
  ylab = "var",
  title = "Within and between variance plots",
  nvar_by_row = 5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choosemaxit_+3A_output">output</code></td>
<td>
<p>an outpout from the imputedata function</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_plotvars">plotvars</code></td>
<td>
<p>index of variables for which a curve is plotted</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_plotm">plotm</code></td>
<td>
<p>a vector indicating which imputed datasets must be plotted</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_size">size</code></td>
<td>
<p>size of points</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_linewidth">linewidth</code></td>
<td>
<p>a numerical value setting the widths of lines</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_linetype">linetype</code></td>
<td>
<p>what type of plot should be drawn</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_title">title</code></td>
<td>
<p>the main title</p>
</td></tr>
<tr><td><code id="choosemaxit_+3A_nvar_by_row">nvar_by_row</code></td>
<td>
<p>the number of variables that are plotted per window. Default value is 5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)
set.seed(123456)
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)
nb.clust &lt;- 3 # number of clusters
m &lt;- 3 # number of imputed data sets
maxit &lt;- 50 # number of iterations for FCS imputation

res.imp &lt;- imputedata(data.na = wine.na, method = "FCS-homo",
                      nb.clust = nb.clust, m = m, maxit = maxit)
choosemaxit(res.imp)

</code></pre>

<hr>
<h2 id='choosenbclust'>Tune the number of clusters according to the partition instability</h2><span id='topic+choosenbclust'></span>

<h3>Description</h3>

<p><code>choosenbclust</code> reports the cluster instability according to the number of clusters chosen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choosenbclust(output, grid = 2:5, graph = TRUE, verbose = TRUE, nnodes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choosenbclust_+3A_output">output</code></td>
<td>
<p>an output from the clusterMI function</p>
</td></tr>
<tr><td><code id="choosenbclust_+3A_grid">grid</code></td>
<td>
<p>a vector indicating the grid of values tested for nb.clust. By default 2:5</p>
</td></tr>
<tr><td><code id="choosenbclust_+3A_graph">graph</code></td>
<td>
<p>a boolean indicating if a graphic is plotted</p>
</td></tr>
<tr><td><code id="choosenbclust_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, choosenbclust will print messages on console</p>
</td></tr>
<tr><td><code id="choosenbclust_+3A_nnodes">nnodes</code></td>
<td>
<p>number of CPU cores for parallel computing. By default, the value used in the call to the clusterMI function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>choosenbclust</code> function browses a grid of values for the number of clusters and for each one imputes the data and computes the instability.
</p>


<h3>Value</h3>

<p>a list of two objects
</p>
<table>
<tr><td><code>nb.clust</code></td>
<td>
<p>the number of clusters in <code>grid</code> minimizing the instability</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>
<p>a vector indicating the instability for each value in the grid</p>
</td></tr>
</table>


<h3>References</h3>

<p>Audigier, V. and Niang, N., Clustering with missing data: which equivalent for Rubin's rules? Advances in Data Analysis and Classification &lt;doi:10.1007/s11634-022-00519-1&gt;, 2022.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+imputedata">imputedata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

# imputation
res.imp &lt;- imputedata(data.na=wine.na, nb.clust = nb.clust, m = 5)

# pooling
nnodes &lt;- 2 # number of CPU cores for parallel computing
res.pool &lt;- clusterMI(res.imp, nnodes = nnodes, instability = FALSE)

# choice of nb.clust

choosenbclust(res.pool)


</code></pre>

<hr>
<h2 id='chooser'>Kfold cross-validation for specifying threshold r</h2><span id='topic+chooser'></span>

<h3>Description</h3>

<p><code>chooser</code> returns a list specifying the optimal threshold r for each outcome as well as the associated set of explanatory variables selected, and the cross-validation errror for each value of the grid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chooser(
  res.varsel,
  K = 10,
  seed = 12345,
  listvar = NULL,
  grid.r = seq(0, 1, 1/1000),
  graph = TRUE,
  printflag = TRUE,
  nb.clust = NULL,
  nnodes = NULL,
  sizeblock = NULL,
  method.select = NULL,
  B = NULL,
  modelNames = NULL,
  nbvarused = NULL,
  path.outfile = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chooser_+3A_res.varsel">res.varsel</code></td>
<td>
<p>an output from the varselbest function</p>
</td></tr>
<tr><td><code id="chooser_+3A_k">K</code></td>
<td>
<p>an integer given the number of folds</p>
</td></tr>
<tr><td><code id="chooser_+3A_seed">seed</code></td>
<td>
<p>a integer</p>
</td></tr>
<tr><td><code id="chooser_+3A_listvar">listvar</code></td>
<td>
<p>a vector of characters specifiying variables (outcomes) for which cross-validation should be done. By default, all variables that have been considered for varselbest are used.</p>
</td></tr>
<tr><td><code id="chooser_+3A_grid.r">grid.r</code></td>
<td>
<p>a grid for the tuning parameter r</p>
</td></tr>
<tr><td><code id="chooser_+3A_graph">graph</code></td>
<td>
<p>a boolean. If TRUE, cross-validation results are printed</p>
</td></tr>
<tr><td><code id="chooser_+3A_printflag">printflag</code></td>
<td>
<p>a boolean. If TRUE, messages are printed</p>
</td></tr>
<tr><td><code id="chooser_+3A_nb.clust">nb.clust</code></td>
<td>
<p>number of clusters. By default, the same as the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_nnodes">nnodes</code></td>
<td>
<p>an integer specifying the number of nodes for parallel computing. By default, the same as  the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_sizeblock">sizeblock</code></td>
<td>
<p>number of sampled variables at each iteration. By default, the same as the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_method.select">method.select</code></td>
<td>
<p>variable selection method used. By default, the same as the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_b">B</code></td>
<td>
<p>number of iterations. By default, the same as the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_modelnames">modelNames</code></td>
<td>
<p>mixture model specification for imputation of subsets. By default, the same as the one used in varselbest</p>
</td></tr>
<tr><td><code id="chooser_+3A_nbvarused">nbvarused</code></td>
<td>
<p>a maximal number of selected variables (can be required for a dataset with a large number of variables)</p>
</td></tr>
<tr><td><code id="chooser_+3A_path.outfile">path.outfile</code></td>
<td>
<p>a path for message redirection</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>varselbest</code> performs variable selection on random subsets of variables and, then, combines them to recover which explanatory variables are related to the response.
More precisely, the outline of the algorithm are as follows: let consider a random subset of <code>sizeblock</code> among p variables.
By choosing <code>sizeblock</code> small, this subset is low dimensional, allowing treatment of missing values by standard imputation method for clustered individuals.
Then, any selection variable scheme can be applied (lasso, stepwise and knockoff are proposed by tuning the <code>method.select</code> argument).
By resampling <code>B</code> times, a sample of size <code>sizeblock</code> among the p variables, we may count how many times, a variable is considered as significantly related to the response and how many times it is not.
We need to define a threshold (<code>r</code>) to conclude if a given variable is significantly related to the response. <code>chooser</code> aims at finding the optimal value for the threshold r using Kfold cross-validation.
</p>


<h3>Value</h3>

<p>A list where each object refers to an outcome variable called in the listvar argument. Each element is composed of three objects
</p>
<table>
<tr><td><code>r</code></td>
<td>
<p>the optimal value for the threshold</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>the cross-validation error for each value in <code>grid.r</code></p>
</td></tr>
<tr><td><code>selection</code></td>
<td>
<p>the subset of selected variables for the optimal threshold</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bar-Hen, A. and Audigier, V., An ensemble learning method for variable selection: application to high dimensional data and missing values, Journal of Statistical Computation and Simulation, &lt;doi:10.1080/00949655.2022.2070621&gt;, 2022.
</p>
<p>Schafer, J. L. (1997) Analysis of Incomplete Multivariate Data. Chapman &amp; Hall, Chapter 9.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)


nnodes &lt;- 2 # parallel::detectCores()
B &lt;- 100 #  Number of iterations
m &lt;- 5 # Number of imputed data sets

# variables selection for incomplete variable "alco"
listvar &lt;- "alco"
res.varsel &lt;- varselbest(data.na = wine.na,
                         nb.clust = nb.clust,
                         listvar = listvar,
                         B = B,
                         nnodes = nnodes)

# frequency of selection
propselect &lt;- res.varsel$proportion[listvar, ]

#predictormatrix with the default threshold value                         
predictmat &lt;- res.varsel$predictormatrix

# r optimal and associated predictor matrix 
res.chooser &lt;- chooser(res.varsel = res.varsel)
thresh &lt;- res.chooser[[listvar]]$r
is.selected &lt;- propselect&gt;=thresh
predictmat[listvar, names(is.selected)] &lt;- as.numeric(is.selected)


# imputation
res.imp.select &lt;- imputedata(data.na = wine.na, method = "FCS-homo",
                     nb.clust = nb.clust, predictmat = predictmat, m = m)


</code></pre>

<hr>
<h2 id='cluster.intern'>Apply clustering method after multiple imputation</h2><span id='topic+cluster.intern'></span>

<h3>Description</h3>

<p>From a list of imputed datasets <code>clusterMI</code> performs cluster analysis on each imputed data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.intern(
  res.imp,
  method.clustering = "kmeans",
  scaling = TRUE,
  nb.clust = NULL,
  method.hclust = "average",
  method.dist = "euclidean",
  modelNames = NULL,
  modelName.hc = "VVV",
  nstart.kmeans = 100,
  iter.max.kmeans = 10,
  m.cmeans = 2,
  samples.clara = 500,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.intern_+3A_res.imp">res.imp</code></td>
<td>
<p>a list of imputed data sets</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_method.clustering">method.clustering</code></td>
<td>
<p>a single string specifying the clustering algorithm used (&quot;kmeans&quot;, &quot;pam&quot;, &quot;clara&quot;, &quot;hclust&quot; or &quot;mixture&quot;,&quot;cmeans&quot;)</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_scaling">scaling</code></td>
<td>
<p>boolean. If TRUE, variables are scaled. Default value is TRUE</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_nb.clust">nb.clust</code></td>
<td>
<p>an integer specifying the number of clusters</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_method.hclust">method.hclust</code></td>
<td>
<p>character string defining the clustering method for hierarchical clustering (required only if method.clustering = &quot;hclust&quot;)</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_method.dist">method.dist</code></td>
<td>
<p>character string defining the method use for computing dissimilarity matrices in hierarchical clustering (required only if method.clustering = &quot;hclust&quot;)</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_modelnames">modelNames</code></td>
<td>
<p>character string indicating the models to be fitted in the EM phase of clustering (required only if method.clustering = &quot;mixture&quot;). By default modelNames = NULL.</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_modelname.hc">modelName.hc</code></td>
<td>
<p>A character string indicating the model to be used in model-based agglomerative hierarchical clustering.(required only if method.clustering = &quot;mixture&quot;). By default modelNames.hc = &quot;VVV&quot;.</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_nstart.kmeans">nstart.kmeans</code></td>
<td>
<p>how many random sets should be chosen for kmeans initalization. Default value is 100 (required only if method.clustering = &quot;kmeans&quot;)</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_iter.max.kmeans">iter.max.kmeans</code></td>
<td>
<p>how many iterations should be chosen for kmeans. Default value is 10 (required only if method.clustering = &quot;kmeans&quot;)</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_m.cmeans">m.cmeans</code></td>
<td>
<p>degree of fuzzification in cmeans clustering. By default m.cmeans = 2</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_samples.clara">samples.clara</code></td>
<td>
<p>number of samples to be drawn from the dataset when performing clustering using clara algorithm. Default value is 500.</p>
</td></tr>
<tr><td><code id="cluster.intern_+3A_verbose">verbose</code></td>
<td>
<p>logical</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs cluster analysis (according to the <code>method.clustering</code> argument). For achieving this goal, the function uses as an input an output from the <code>imputedata</code> function and applies the cluster analysis method on each imputed data set
</p>
<p>Step 1 can be tuned by specifying the cluster analysis method used (<code>method.clustering</code> argument).
If <code>method.clustering = "kmeans"</code> or <code>"pam"</code>, then the number of clusters can be specified by tuning the <code>nb.clust</code> argument. By default, the same number as the one used for imputation is used.
The number of random initializations can also be tuned through the <code>nstart.kmeans</code> argument.
If <code>method.clustering = "hclust"</code> (hierarchical clustering), the method used can be specified (see <code><a href="stats.html#topic+hclust">hclust</a></code>). By default <code>"average"</code> is used. Furthermore, the number of clusters can be specified, but it can also be automatically chosen if <code>nb.clust</code> &lt; 0.
If <code>method.clustering = "mixture"</code> (model-based clustering using gaussian mixture models), the model to be fitted can be tuned by modifying the <code>modelNames</code> argument (see <code><a href="mclust.html#topic+Mclust">Mclust</a></code>).
If <code>method.clustering = "cmeans"</code> (clustering using the fuzzy c-means algorithm), then the fuzziness parameter can be modfied by tuning the<code>m.cmeans</code> argument. By default, <code>m.cmeans = 2</code>.
</p>
<p>Can be performed in parallel by specifying the number of CPU cores (<code>nnodes</code> argument).
</p>


<h3>Value</h3>

<p>A list with clustering results
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="mclust.html#topic+Mclust">Mclust</a></code>, <code><a href="#topic+imputedata">imputedata</a></code>, <code><a href="e1071.html#topic+cmeans">cmeans</a></code>,<code><a href="stats.html#topic+dist">dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
m &lt;- 5 # number of imputed data sets. Should be larger in practice
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

#imputation
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)

#analysis by kmeans and pooling
nnodes &lt;- 2 # parallel::detectCores()
res.pool &lt;- clusterMI(res.imp, nnodes = nnodes)

res.pool$instability
table(ref, res.pool$part)

</code></pre>

<hr>
<h2 id='clusterMI'>Cluster analysis and pooling after multiple imputation</h2><span id='topic+clusterMI'></span>

<h3>Description</h3>

<p>From a list of imputed datasets <code>clusterMI</code> performs cluster analysis on each imputed data set, estimates the instability of each partition using bootstrap (following Fang, Y. and Wang, J., 2012 &lt;doi:10.1016/j.csda.2011.09.003&gt;) and pools results as proposed in Audigier and Niang (2022) &lt;doi:10.1007/s11634-022-00519-1&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterMI(
  output,
  method.clustering = "kmeans",
  method.consensus = "NMF",
  scaling = TRUE,
  nb.clust = NULL,
  Cboot = 50,
  method.hclust = "average",
  method.dist = "euclidean",
  modelNames = NULL,
  modelName.hc = "VVV",
  nstart.kmeans = 100,
  iter.max.kmeans = 10,
  m.cmeans = 2,
  samples.clara = 500,
  nnodes = 1,
  instability = TRUE,
  verbose = TRUE,
  nmf.threshold = 10^(-5),
  nmf.nstart = 100,
  nmf.early_stop_iter = 10,
  nmf.initializer = "random",
  nmf.batch_size = NULL,
  nmf.iter.max = 50
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterMI_+3A_output">output</code></td>
<td>
<p>an output from the imputedata function</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_method.clustering">method.clustering</code></td>
<td>
<p>a single string specifying the clustering algorithm used (&quot;kmeans&quot;, &quot;pam&quot;, &quot;clara&quot;, &quot;hclust&quot; or &quot;mixture&quot;,&quot;cmeans&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_method.consensus">method.consensus</code></td>
<td>
<p>a single string specifying the consensus method used to pool the contributory partitions (&quot;NMF&quot; or &quot;CSPA&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_scaling">scaling</code></td>
<td>
<p>boolean. If TRUE, variables are scaled. Default value is TRUE</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nb.clust">nb.clust</code></td>
<td>
<p>an integer specifying the number of clusters</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_cboot">Cboot</code></td>
<td>
<p>an integer specifying the number of bootstrap replications. Default value is 50</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_method.hclust">method.hclust</code></td>
<td>
<p>character string defining the clustering method for hierarchical clustering (required only if method.clustering = &quot;hclust&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_method.dist">method.dist</code></td>
<td>
<p>character string defining the method use for computing dissimilarity matrices in hierarchical clustering (required only if method.clustering = &quot;hclust&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_modelnames">modelNames</code></td>
<td>
<p>character string indicating the models to be fitted in the EM phase of clustering (required only if method.clustering = &quot;mixture&quot;). By default modelNames = NULL.</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_modelname.hc">modelName.hc</code></td>
<td>
<p>A character string indicating the model to be used in model-based agglomerative hierarchical clustering.(required only if method.clustering = &quot;mixture&quot;). By default modelNames.hc = &quot;VVV&quot;.</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nstart.kmeans">nstart.kmeans</code></td>
<td>
<p>how many random sets should be chosen for kmeans initalization. Default value is 100 (required only if method.clustering = &quot;kmeans&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_iter.max.kmeans">iter.max.kmeans</code></td>
<td>
<p>how many iterations should be chosen for kmeans. Default value is 10 (required only if method.clustering = &quot;kmeans&quot;)</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_m.cmeans">m.cmeans</code></td>
<td>
<p>degree of fuzzification in cmeans clustering. By default m.cmeans = 2</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_samples.clara">samples.clara</code></td>
<td>
<p>number of samples to be drawn from the dataset when performing clustering using clara algorithm. Default value is 500.</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nnodes">nnodes</code></td>
<td>
<p>number of CPU cores for parallel computing. By default, nnodes = 1</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_instability">instability</code></td>
<td>
<p>a boolean indicating if cluster instability must be computed. Default value is TRUE</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_verbose">verbose</code></td>
<td>
<p>a boolean. If TRUE, a message is printed at each step. Default value is TRUE</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.threshold">nmf.threshold</code></td>
<td>
<p>Default value is 10^(-5),</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.nstart">nmf.nstart</code></td>
<td>
<p>Default value is 100,</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.early_stop_iter">nmf.early_stop_iter</code></td>
<td>
<p>Default value is 10,</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.initializer">nmf.initializer</code></td>
<td>
<p>Default value is 'random',</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.batch_size">nmf.batch_size</code></td>
<td>
<p>Default value is 20,</p>
</td></tr>
<tr><td><code id="clusterMI_+3A_nmf.iter.max">nmf.iter.max</code></td>
<td>
<p>Default value is 50</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusterMI</code> performs cluster analysis (according to the <code>method.clustering</code> argument) and pooling after multiple imputation. For achieving this goal, the <code>clusterMI</code> function uses as an input an output from the <code>imputedata</code> function and then
</p>

<ol>
<li><p> applies the cluster analysis method on each imputed data set
</p>
</li>
<li><p> pools contributory partitions using non-negative matrix factorization
</p>
</li>
<li><p> computes the instability of each partition by bootstrap
</p>
</li>
<li><p> computes the total instability
</p>
</li></ol>

<p>Step 1 can be tuned by specifying the cluster analysis method used (<code>method.clustering</code> argument).
If <code>method.clustering = "kmeans"</code> or <code>"pam"</code>, then the number of clusters can be specified by tuning the <code>nb.clust</code> argument. By default, the same number as the one used for imputation is used.
The number of random initializations can also be tuned through the <code>nstart.kmeans</code> argument.
If <code>method.clustering = "hclust"</code> (hierarchical clustering), the method used can be specified (see <code><a href="stats.html#topic+hclust">hclust</a></code>). By default <code>"average"</code> is used. Furthermore, the number of clusters can be specified, but it can also be automatically chosen if <code>nb.clust</code> &lt; 0.
If <code>method.clustering = "mixture"</code> (model-based clustering using gaussian mixture models), the model to be fitted can be tuned by modifying the <code>modelNames</code> argument (see <code><a href="mclust.html#topic+Mclust">Mclust</a></code>).
If <code>method.clustering = "cmeans"</code> (clustering using the fuzzy c-means algorithm), then the fuzziness parameter can be modfied by tuning the<code>m.cmeans</code> argument. By default, <code>m.cmeans = 2</code>.
</p>
<p>Step 2 performs consensus clustering by Non-Negative Matrix Factorization, following Li and Ding (2007) &lt;doi:10.1109/ICDM.2007.98&gt;.
</p>
<p>Step 3 applies the <code><a href="fpc.html#topic+nselectboot">nselectboot</a></code> function on each imputed data set and returns the instability of each cluster obtained at step 1. The method is based on bootstrap sampling, followong Fang, Y. and Wang, J. (2012) &lt;doi:10.1016/j.csda.2011.09.003&gt;. The number of iterations can be tuned using the <code>Cboot</code> argument.
</p>
<p>Step 4 averages the previous instability measures given a within instability (<code>Ubar</code>), computes a between instability (<code>B</code>) and a total instability (<code>T</code> = B + Ubar). See Audigier and Niang (2022) &lt;doi:10.1007/s11634-022-00519-1&gt; for details.
</p>
<p>All steps can be performed in parallel by specifying the number of CPU cores (<code>nnodes</code> argument). Steps 3 and 4 are more time consuming. To compute only steps 1 and 2 use <code>instability = FALSE</code>.
</p>


<h3>Value</h3>

<p>A list with three objects
</p>
<table>
<tr><td><code>part</code></td>
<td>
<p>the consensus partition</p>
</td></tr>
<tr><td><code>instability</code></td>
<td>
<p>a list of four objects: <code>U</code> the within instability measure for each imputed data set, <code>Ubar</code> the associated average, <code>B</code> the between instability measure, <code>Tot</code> the total instability measure</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matching call</p>
</td></tr>
</table>


<h3>References</h3>

<p>Audigier, V. and Niang, N. (2022) Clustering with missing data: which equivalent for Rubin's rules? Advances in Data Analysis and Classification &lt;doi:10.1007/s11634-022-00519-1&gt;
</p>
<p>Fang, Y. and Wang, J. (2012) Selection of the number of clusters via the bootstrap method. Computational Statistics and Data Analysis, 56, 468-477. &lt;doi:10.1016/j.csda.2011.09.003&gt;
</p>
<p>T. Li, C. Ding, and M. I. Jordan (2007) Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization.  In Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, ICDM'07, page 577-582, USA. IEEE Computer Society. &lt;doi:10.1109/ICDM.2007.98&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="fpc.html#topic+nselectboot">nselectboot</a></code>, <code><a href="mclust.html#topic+Mclust">Mclust</a></code>, <code><a href="#topic+imputedata">imputedata</a></code>, <code><a href="e1071.html#topic+cmeans">cmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
m &lt;- 5 # number of imputed data sets. Should be larger in practice
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

#imputation
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)

#analysis by kmeans and pooling
nnodes &lt;- 2 # parallel::detectCores()
res.pool &lt;- clusterMI(res.imp, nnodes = nnodes)

res.pool$instability
table(ref, res.pool$part)

</code></pre>

<hr>
<h2 id='fastnmf'>Consensus clustering using non-negative matrix factorization</h2><span id='topic+fastnmf'></span>

<h3>Description</h3>

<p>From a list of partitions <code>fastnmf</code> pools partition as proposed in Li and Ding (2007) &lt;doi:10.1109/ICDM.2007.98&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastnmf(
  listpart,
  nb.clust,
  threshold = 10^(-5),
  printflag = TRUE,
  nstart = 100,
  early_stop_iter = 10,
  initializer = "random",
  batch_size = NULL,
  iter.max = 50
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastnmf_+3A_listpart">listpart</code></td>
<td>
<p>a list of partitions</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_nb.clust">nb.clust</code></td>
<td>
<p>an integer specifying the number of clusters</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_threshold">threshold</code></td>
<td>
<p>a real specifying when the NMF algorithm is stoped. Default value is 10^(-5)</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_printflag">printflag</code></td>
<td>
<p>a boolean. If TRUE, nmf will print messages on console. Default value is TRUE</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_nstart">nstart</code></td>
<td>
<p>how many random sets should be chosen for kmeans initalization. Default value is 100</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_early_stop_iter">early_stop_iter</code></td>
<td>
<p>continue that many iterations after calculation of the best within-cluster-sum-of-squared-error. Default value is 10. See MiniBatchKmeans help page.</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_initializer">initializer</code></td>
<td>
<p>the method of initialization. One of, optimal_init, quantile_init, kmeans++ and random. See MiniBatchKmeans help page.</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_batch_size">batch_size</code></td>
<td>
<p>the size of the mini batches for kmeans clustering. Default value is NULL.</p>
</td></tr>
<tr><td><code id="fastnmf_+3A_iter.max">iter.max</code></td>
<td>
<p>the maximum number of iterations allowed for kmeans. Default value is 50</p>
</td></tr>
</table>


<h3>Details</h3>

<p>fastnmf performs consensus clustering using non-negative matrix factorization following Li and Ding (2007) &lt;doi:10.1109/ICDM.2007.98&gt;. The set of partitions that are aggregated needs to be given as a list where each element is a vector of numeric values. Note that the number of classes for each partition can vary. The number of classes for the consensus partition should be given using the nb.clust argument. The NMF algorithm is iterative and required an initial partition. This latter is based on kmeans clustering on the average of connectivity matrices. If batchsize is NULL, then kmeans clustering is performed using <code>nstart</code> initial values and <code>iter.max</code> iterations. Otherwise, Mini Batch Kmeans is used. This algorithm could be faster than kmeans if the number of invididuals is large.
</p>


<h3>Value</h3>

<p>a list of 5 objets
</p>
<table>
<tr><td><code>Htilde</code></td>
<td>
<p>A fuzzy disjunctive table</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>A positive matrix</p>
</td></tr>
<tr><td><code>Mtilde</code></td>
<td>
<p>The average of connectivity matrices</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>
<p>A vector with the optimized criterion at each iteration</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>the consensus partition in nb.clust classes</p>
</td></tr>
</table>


<h3>References</h3>

<p>T. Li, C. Ding, and M. I. Jordan (2007) Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization.  In Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, ICDM'07, page 577-582, USA. IEEE Computer Society. &lt;doi:10.1109/ICDM.2007.98&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code> <code><a href="ClusterR.html#topic+MiniBatchKmeans">MiniBatchKmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)
require(clustrd)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
m &lt;- 3 # number of imputed data sets. Should be larger in practice
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

#imputation
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)

#analysis using reduced kmeans

## apply the cluspca function on each imputed data set
res.ana.rkm &lt;- lapply(res.imp$res.imp,
                      FUN = cluspca,
                      nclus = nb.clust,
                      ndim = 2,
                      method= "RKM")
## extract the set of partitions (under "list" format)
res.ana.rkm &lt;-lapply(res.ana.rkm,"[[","cluster")

# pooling by NMF
res.pool.rkm &lt;- fastnmf(res.ana.rkm, nb.clust = nb.clust)$clust

</code></pre>

<hr>
<h2 id='imputedata'>Multiple imputation methods for cluster analysis</h2><span id='topic+imputedata'></span>

<h3>Description</h3>

<p><code>imputedata</code> returns a list of imputed datasets by using imputation methods dedicated to individuals clustered in (unknown) groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imputedata(
  data.na,
  method = "JM-GL",
  nb.clust = NULL,
  m = 20,
  maxit = 50,
  Lstart = 100,
  L = 20,
  method.mice = NULL,
  predictmat = NULL,
  verbose = TRUE,
  seed = 1234,
  bootstrap = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imputedata_+3A_data.na">data.na</code></td>
<td>
<p>an incomplete dataframe</p>
</td></tr>
<tr><td><code id="imputedata_+3A_method">method</code></td>
<td>
<p>a single string specifying the imputation method used among &quot;FCS-homo&quot;,&quot;FCS-hetero&quot;,&quot;JM-DP&quot;,&quot;JM-GL&quot;. By default method = &quot;JM-GL&quot;. See the details section</p>
</td></tr>
<tr><td><code id="imputedata_+3A_nb.clust">nb.clust</code></td>
<td>
<p>number of clusters</p>
</td></tr>
<tr><td><code id="imputedata_+3A_m">m</code></td>
<td>
<p>number of imputed datasets. By default, m = 20.</p>
</td></tr>
<tr><td><code id="imputedata_+3A_maxit">maxit</code></td>
<td>
<p>number of iterations for FCS methods (only used for method = FCS-homo or method = FCS-hetero)</p>
</td></tr>
<tr><td><code id="imputedata_+3A_lstart">Lstart</code></td>
<td>
<p>number of iterations for the burn-in period (only used if method =&quot;JM-DP&quot; or &quot;JM-GL&quot;)</p>
</td></tr>
<tr><td><code id="imputedata_+3A_l">L</code></td>
<td>
<p>number of skipped iterations to keep one imputed data set after the burn-in period (only used if method =&quot;JM-DP&quot; or &quot;JM-GL&quot;)</p>
</td></tr>
<tr><td><code id="imputedata_+3A_method.mice">method.mice</code></td>
<td>
<p>a vector of strings (or a single string) giving the imputation method for each variable (only used for method = FCS-homo or method = FCS-hetero). Default value is &quot;pmm&quot; (predictive mean matching) for FCS-homo and &quot;mice.impute.2l.jomo&quot; for FCS-hetero</p>
</td></tr>
<tr><td><code id="imputedata_+3A_predictmat">predictmat</code></td>
<td>
<p>predictor matrix used for FCS imputation (only used for method = FCS-homo or method = FCS-hetero)</p>
</td></tr>
<tr><td><code id="imputedata_+3A_verbose">verbose</code></td>
<td>
<p>a boolean. If TRUE, a message is printed at each iteration. Use verbose = FALSE for silent imputation</p>
</td></tr>
<tr><td><code id="imputedata_+3A_seed">seed</code></td>
<td>
<p>a positive integer initializing the random generator</p>
</td></tr>
<tr><td><code id="imputedata_+3A_bootstrap">bootstrap</code></td>
<td>
<p>a boolean. Use bootstrap = TRUE for proper imputation with FCS methods (Mclust sometimes fails with multiple points)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>imputedata</code> offers various multiple imputation methods dedicated to clustered individuals.
In particular, two fully conditional imputation methods are proposed (<code>FCS-homo</code> and <code>FCS-hetero</code>) which essentially differ by the assumption about the covariance in each cluster (constant or not respectively).
The imputation requires a pre-specified number of clusters (<code>nb.clust</code>). See <code><a href="#topic+choosenbclust">choosenbclust</a></code> if this number is unknown.
The <code>imputedata</code> function alternates clustering and imputation given the partition of individuals.
When the clustering is performed, the function calls the <code><a href="mice.html#topic+mice">mice</a></code> function from the <code>mice</code> R package to perform imputation.
The <code>mice</code> package proposes various methods for imputation which can be specified by tuning the <code>method.mice</code> argument.
Note that two other joint modelling methods are also available: <code>JM-GL</code> from the R package <code>mix</code> and <code>JM-DP</code> from the R package <code>DPImputeCont</code> <a href="https://github.com/hang-j-kim/DPImputeCont">https://github.com/hang-j-kim/DPImputeCont</a>
</p>


<h3>Value</h3>

<p>a list of 3 objets
</p>
<table>
<tr><td><code>res.imp</code></td>
<td>
<p> a list with the several imputed datasets</p>
</td></tr>
<tr><td><code>res.conv</code></td>
<td>
<p> for FCS methods, an array given the between (and within) inertia of each imputed variable at each iteration and for each imputed dataset. For JM methods, a matrix given the between inertia for each variable and each imputed dataset.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p> the matching call</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kim, H. J., Reiter, J. P., Wang, Q., Cox, L. H. and Karr, A. F. (2014), Multiple imputation of missing or faulty values under linear constraints, Journal of Business and Economics Statistics, 32, 375-386 &lt;doi:10.1080/07350015.2014.885435&gt;
</p>
<p>Schafer, J. L. (1997) Analysis of Incomplete Multivariate Data. Chapman &amp; Hall, Chapter 9.
</p>
<p>Audigier, V., Niang, N., &amp; Resche-Rigon, M. (2021). Clustering with missing data: which imputation model for which cluster analysis method?. arXiv preprint &lt;arXiv:2106.04424&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="mice.html#topic+mice">mice</a></code> <code><a href="#topic+choosenbclust">choosenbclust</a></code> <code><a href="#topic+choosemaxit">choosemaxit</a></code> <code><a href="#topic+varselbest">varselbest</a></code> <code><a href="mix.html#topic+imp.mix">imp.mix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)
set.seed(123456)
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)
nb.clust &lt;- 3 # number of clusters
m &lt;- 20 # number of imputed data sets
res.imp &lt;- imputedata(data.na = wine.na, nb.clust = nb.clust, m = m)
lapply(res.imp$res.imp, summary)
</code></pre>

<hr>
<h2 id='mclustboot.intern'>MclustBootstrap with nboot = 1 and the same output as Mclust</h2><span id='topic+mclustboot.intern'></span>

<h3>Description</h3>

<p>MclustBootstrap with nboot = 1 and the same output as Mclust
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mclustboot.intern(don, G = NULL, modelNames = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mclustboot.intern_+3A_don">don</code></td>
<td>
<p>matrix, or data frame</p>
</td></tr>
<tr><td><code id="mclustboot.intern_+3A_g">G</code></td>
<td>
<p>An integer vector specifying the numbers of mixture components</p>
</td></tr>
<tr><td><code id="mclustboot.intern_+3A_modelnames">modelNames</code></td>
<td>
<p>A vector of character strings indicating the models to be fitted</p>
</td></tr>
<tr><td><code id="mclustboot.intern_+3A_verbose">verbose</code></td>
<td>
<p>A logical controlling if a text progress bar is displayed</p>
</td></tr>
</table>

<hr>
<h2 id='myem.mix'>internal function</h2><span id='topic+myem.mix'></span>

<h3>Description</h3>

<p>prend en entrée un jeu de données, ajoute une variable classe, et renvoie les parametre du general location model. Une des particularité est l'initialisation du EM par un modèle de mélange. En effet em.mix a sa propre façon d'initialiser qui parfois renvoie des paramètres identiques pour plusieurs groupes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myem.mix(dataset, K, boot = FALSE, silent = TRUE, nbquali = 0, eps = 1e-04)
</code></pre>

<hr>
<h2 id='onefold.chooser'>one fold cross-validation for specifying threshold r</h2><span id='topic+onefold.chooser'></span>

<h3>Description</h3>

<p>one fold cross-validation for specifying threshold r
</p>


<h3>Usage</h3>

<pre><code class='language-R'>onefold.chooser(
  data.train.k,
  data.test.k,
  jj,
  grid.r,
  nb.clust,
  nnodes,
  sizeblock,
  method.select,
  B,
  modelNames,
  K,
  path.outfile,
  nbvarused
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="onefold.chooser_+3A_data.train.k">data.train.k</code></td>
<td>
<p>a train set</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_data.test.k">data.test.k</code></td>
<td>
<p>a test set</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_jj">jj</code></td>
<td>
<p>name of the outcome used for variable selection</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_grid.r">grid.r</code></td>
<td>
<p>a grid for the tuning parameter r</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_nb.clust">nb.clust</code></td>
<td>
<p>number of clusters</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_sizeblock">sizeblock</code></td>
<td>
<p>number of sampled variables at each iteration</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_method.select">method.select</code></td>
<td>
<p>variable selection method</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_b">B</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_modelnames">modelNames</code></td>
<td>
<p>mixture model specification for imputation of subsets</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_k">K</code></td>
<td>
<p>number of fold</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_path.outfile">path.outfile</code></td>
<td>
<p>a path for message redirection</p>
</td></tr>
<tr><td><code id="onefold.chooser_+3A_nbvarused">nbvarused</code></td>
<td>
<p>a maximal number of selected variables (can be required with a large number of variables)</p>
</td></tr>
</table>

<hr>
<h2 id='overimpute'>Overimputation diagnostic plot</h2><span id='topic+overimpute'></span>

<h3>Description</h3>

<p><code>overimpute</code> assesses the fit of the predictive distribution after performing multiple imputation with the <code><a href="#topic+imputedata">imputedata</a></code> function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overimpute(
  res.imputedata,
  plotvars = NULL,
  plotinds = NULL,
  nnodes = 2,
  path.outfile = NULL,
  alpha = 0.1,
  mfrow = NULL,
  mar = c(5, 4, 4, 2) - 1.9
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="overimpute_+3A_res.imputedata">res.imputedata</code></td>
<td>
<p>an output from the imputedata function</p>
</td></tr>
<tr><td><code id="overimpute_+3A_plotvars">plotvars</code></td>
<td>
<p>column index of the variables overimputed</p>
</td></tr>
<tr><td><code id="overimpute_+3A_plotinds">plotinds</code></td>
<td>
<p>row index of the individuals overimputed</p>
</td></tr>
<tr><td><code id="overimpute_+3A_nnodes">nnodes</code></td>
<td>
<p>an integer indicating the number of nodes for parallel calculation. Default value is 5</p>
</td></tr>
<tr><td><code id="overimpute_+3A_path.outfile">path.outfile</code></td>
<td>
<p>a vector of strings indicating the path for redirection of print messages. Default value is NULL, meaning that silent imputation is performed. Otherwise, print messages are saved in the files path.outfile/output.txt. One file per node is generated.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for prediction intervals</p>
</td></tr>
<tr><td><code id="overimpute_+3A_mfrow">mfrow</code></td>
<td>
<p>a vector of the form c(nr, nc)</p>
</td></tr>
<tr><td><code id="overimpute_+3A_mar">mar</code></td>
<td>
<p>a numerical vector of the form c(bottom, left, top, right)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function imputes each observed value from each conditional imputation model obtained from the imputedata function. The comparison between the &quot;overimputed&quot; values and the observed values is made by building a confidence interval for each observed value using the quantiles of the overimputed values (see Blackwell et al. (2015) &lt;doi:10.1177/0049124115585360&gt;). Note that confidence intervals built with quantiles require a large number of imputations. If the model fits well the data, then the 90% confidence interval should contain the observed value in 90% of the cases. The function overimpute takes as an input an output of the <code><a href="#topic+imputedata">imputedata</a></code> function (<code>res.imputedata</code> argument), the indices of the incomplete continuous variables that are plotted (<code>plotvars</code>), the indices of individuals (can be useful for time consuming imputation methods), the number of CPU cores for parallel computation, and the path for exporting print message generated during the parallel process (<code>path.outfile</code>).
</p>


<h3>Value</h3>

<p>A list of two matrices
</p>
<table>
<tr><td><code>res.plot</code></td>
<td>
<p>7-columns matrix that contains (1) the variable which is overimputed, (2) the observed value of the observation, (3) the mean of the overimputations, (4) the lower bound of the confidence interval of the overimputations, (5) the upper bound of the confidence interval of the overimputations, (6) the proportion of the other variables that were missing for that observation in the original data, and (7) the color for graphical representation</p>
</td></tr>
<tr><td><code>res.values</code></td>
<td>
<p>a matrix with overimputed values for each cell. The number of columns corresponds to the number of values generated (i.e. the number of imputed datasets)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Blackwell, M., Honaker, J. and King. G. 2015. A Unified Approach to Measurement Error and Missing Data: Overview and Applications. Sociological Methods and Research, 1-39. &lt;doi:10.1177/0049124115585360&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)

nnodes &lt;- 2 # Number of CPU cores used for parallel computation

# Multiple imputation using m = 100 (should be larger in practice)

res.imp.over &lt;- imputedata(data.na = wine.na,
                           nb.clust = nb.clust,
                           m = 100)
# Overimputation

## overimputed variable
plotvars &lt;- "alco" 

## selection of 20 complete individuals on variable "alco"
plotinds &lt;- sample(which(!is.na(wine.na[, plotvars])),
                    size = 20)
## overimputation                   
res.over &lt;- overimpute(res.imp.over,
                       nnodes = nnodes,
                       plotvars = plotvars,
                       plotinds = plotinds,
                       )

</code></pre>

<hr>
<h2 id='prodna'>Introduce missing values using a missing completely at random mechanism</h2><span id='topic+prodna'></span>

<h3>Description</h3>

<p><code>prodna</code> generates an incomplete dataset by removing a proportion of observed values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prodna(X, pct = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prodna_+3A_x">X</code></td>
<td>
<p>a data frame (or matrix).</p>
</td></tr>
<tr><td><code id="prodna_+3A_pct">pct</code></td>
<td>
<p>proportion of missing values. By default pct = 0.3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an incomplete data frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
p &lt;- 5
X &lt;- matrix(runif(n*p), nrow = n, ncol = p)
summary(X)
X.na &lt;- prodna(X)
colMeans(is.na(X.na))
</code></pre>

<hr>
<h2 id='Rcpp_modelobject-class'>Class <code>"Rcpp_modelobject"</code></h2><span id='topic+Rcpp_modelobject-class'></span>

<h3>Description</h3>

<p>A class required when using the <code>imputedata</code> function with argument <code>method = "JM-DP"</code>
</p>


<h3>Extends</h3>

<p>Class &quot;C++Object&quot;, directly.
</p>
<p>All reference classes extend and inherit methods from &quot;envRefClass&quot;.
</p>


<h3>Fields</h3>


<dl>
<dt><code>.Mu</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.Z_vec</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.a_Phi</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.a_alpha</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.alpha</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.b_Phi</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.b_alpha</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.cube_UT_cholSigma</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.h_0</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.logpi</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.missing_flag_mat</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.vec_Phi</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>.where_we_are</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>Y_mat</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>max_Y_obs</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>min_Y_obs</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>msg_level</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt><code>vec_HyperParameters</code>:</dt><dd><p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt><code>initialize(...)</code>:</dt><dd><p> ~~ </p>
</dd>
<dt><code>finalize()</code>:</dt><dd><p> ~~ </p>
</dd>
<dt><code>Run(...)</code>:</dt><dd><p> ~~ </p>
</dd>
<dt><code>Iterate()</code>:</dt><dd><p> ~~ </p>
</dd>
</dl>



<h3>Note</h3>

<p>From the R package <code>DPImputeCont</code> <a href="https://github.com/hang-j-kim/DPImputeCont">https://github.com/hang-j-kim/DPImputeCont</a>.
</p>


<h3>Author(s)</h3>

<p>Kim, Hang Joon. University of Cincinnati.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Rcpp_modelobject")
</code></pre>

<hr>
<h2 id='Silhouette.intern'>Compute Silhouette index</h2><span id='topic+Silhouette.intern'></span>

<h3>Description</h3>

<p>Compute Silhouette index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Silhouette.intern(d, cl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Silhouette.intern_+3A_d">d</code></td>
<td>
<p>a distance matrix</p>
</td></tr>
<tr><td><code id="Silhouette.intern_+3A_cl">cl</code></td>
<td>
<p>a vector of integer defining the partition</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a real
</p>

<hr>
<h2 id='varselbest'>Variable selection for specifying conditional imputation models</h2><span id='topic+varselbest'></span>

<h3>Description</h3>

<p><code>varselbest</code> performs variable selection from an incomplete dataset (see Bar-Hen and Audigier (2022) &lt;doi:10.1080/00949655.2022.2070621&gt;) in order to specify the imputation models to use for FCS imputation methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varselbest(
  data.na = NULL,
  res.imputedata = NULL,
  listvar = NULL,
  nb.clust = NULL,
  nnodes = 1,
  sizeblock = 5,
  method.select = "knockoff",
  B = 200,
  r = 0.3,
  graph = TRUE,
  printflag = TRUE,
  path.outfile = NULL,
  mar = c(2, 4, 2, 0.5) + 0.1,
  cex.names = 0.7,
  modelNames = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varselbest_+3A_data.na">data.na</code></td>
<td>
<p>a dataframe with only numeric variables</p>
</td></tr>
<tr><td><code id="varselbest_+3A_res.imputedata">res.imputedata</code></td>
<td>
<p>an output from <code><a href="#topic+imputedata">imputedata</a></code></p>
</td></tr>
<tr><td><code id="varselbest_+3A_listvar">listvar</code></td>
<td>
<p>a character vector indicating for which subset of incomplete variables variable selection must be performed. By default all column names.</p>
</td></tr>
<tr><td><code id="varselbest_+3A_nb.clust">nb.clust</code></td>
<td>
<p>the number of clusters used for imputation</p>
</td></tr>
<tr><td><code id="varselbest_+3A_nnodes">nnodes</code></td>
<td>
<p>number of CPU cores for parallel computing. By default, nnodes = 1</p>
</td></tr>
<tr><td><code id="varselbest_+3A_sizeblock">sizeblock</code></td>
<td>
<p>an integer indicating the number of variables sampled at each iteration</p>
</td></tr>
<tr><td><code id="varselbest_+3A_method.select">method.select</code></td>
<td>
<p>a single string indicating the variable selection method applied on each subset of variables</p>
</td></tr>
<tr><td><code id="varselbest_+3A_b">B</code></td>
<td>
<p>number of iterations, by default B = 200</p>
</td></tr>
<tr><td><code id="varselbest_+3A_r">r</code></td>
<td>
<p>a numerical vector (or a single real number) indicating the threshold used for each variable in listvar. Each value of r should be between 0 and 1. See details.</p>
</td></tr>
<tr><td><code id="varselbest_+3A_graph">graph</code></td>
<td>
<p>a boolean. If TRUE two graphics are plotted per variable in <code>listvar</code>: a graphic reporting the variable importance measure of each explanatory variable and a graphic reporting the influence of the number iterations (B) on the importance measures</p>
</td></tr>
<tr><td><code id="varselbest_+3A_printflag">printflag</code></td>
<td>
<p>a boolean. If TRUE, a message is printed at each iteration. Use printflag = FALSE for silent selection.</p>
</td></tr>
<tr><td><code id="varselbest_+3A_path.outfile">path.outfile</code></td>
<td>
<p>a vector of strings indicating the path for redirection of print messages. Default value is NULL, meaning that silent imputation is performed. Otherwise, print messages are saved in the files path.outfile/output.txt. One file per node is generated.</p>
</td></tr>
<tr><td><code id="varselbest_+3A_mar">mar</code></td>
<td>
<p>a numerical vector of the form c(bottom, left, top, right). Only used if graph = TRUE</p>
</td></tr>
<tr><td><code id="varselbest_+3A_cex.names">cex.names</code></td>
<td>
<p>expansion factor for axis names (bar labels) (only used if graph = TRUE)</p>
</td></tr>
<tr><td><code id="varselbest_+3A_modelnames">modelNames</code></td>
<td>
<p>a vector of character strings indicating the models to be fitted in the EM phase of clustering</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>varselbest</code> performs variable selection on random subsets of variables and, then, combines them to recover which explanatory variables are related to the response.
More precisely, the outline of the algorithm are as follows: let consider a random subset of <code>sizeblock</code> among p variables.
By choosing <code>sizeblock</code> small, this subset is low dimensional, allowing treatment of missing values by standard imputation method for clustered individuals.
Then, any selection variable scheme can be applied (lasso, stepwise and knockoff are proposed by tuning the <code>method.select</code> argument).
By resampling <code>B</code> times, a sample of size <code>sizeblock</code> among the p variables, we may count how many times, a variable is considered as significantly related to the response and how many times it is not.
We need to define a threshold (<code>r</code>) to conclude if a given variable is significantly related to the response.
</p>


<h3>Value</h3>

<p>a list of four objects
</p>
<table>
<tr><td><code>predictormatrix</code></td>
<td>
<p>a numeric matrix containing 0 and 1 specifying on each line the set of predictors to be used for each target column of the incomplete dataset.</p>
</td></tr>
<tr><td><code>res.varsel</code></td>
<td>
<p>a list given details on the variable selection procedure (only required for checking convergence by the <code>chooseB</code> function)</p>
</td></tr>
<tr><td><code>proportion</code></td>
<td>
<p>a numeric matrix of proportion indicating on each line the variable importance of each predictor</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matching call</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bar-Hen, A. and Audigier, V., An ensemble learning method for variable selection: application to high dimensional data and missing values, Journal of Statistical Computation and Simulation, &lt;doi:10.1080/00949655.2022.2070621&gt;, 2022.
</p>


<h3>See Also</h3>

<p><code><a href="mice.html#topic+mice">mice</a></code>,<code><a href="#topic+clusterMI">clusterMI</a></code>, <code><a href="#topic+imputedata">imputedata</a></code>,<code><a href="knockoff.html#topic+knockoff">knockoff</a></code>,<code><a href="glmnet.html#topic+glmnet">glmnet</a></code>,<code><a href="mix.html#topic+imp.mix">imp.mix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)

require(parallel)
set.seed(123456)
ref &lt;- wine$cult
nb.clust &lt;- 3
wine.na &lt;- wine
wine.na$cult &lt;- NULL
wine.na &lt;- prodna(wine.na)


nnodes &lt;- 2 # parallel::detectCores()
B &lt;- 150 #  Number of iterations
m &lt;- 5 # Number of imputed data sets

# variable selection
res.varsel &lt;- varselbest(data.na = wine.na,
                         nb.clust = nb.clust,
                         listvar = c("alco","malic"),
                         B = B,
                         nnodes = nnodes)
predictmat &lt;- res.varsel$predictormatrix

# imputation
res.imp.select &lt;- imputedata(data.na = wine.na, method = "FCS-homo",
                     nb.clust = nb.clust, predictmat = predictmat, m = m)


</code></pre>

<hr>
<h2 id='wine'>Chemical analysis of wines from three different cultivars</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>Data are the results of a chemical analysis of
wines grown in the same region in Italy but derived from three
different cultivars. The analysis determined the quantities of 13 constituents
found in each of the three types of wines
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wine)
</code></pre>


<h3>Format</h3>

<p>A data frame with 178 rows and 14 variables:
</p>

<dl>
<dt>cult</dt><dd><p>Cultivar</p>
</dd>
<dt>alco</dt><dd><p>Alcohol</p>
</dd>
<dt>malic</dt><dd><p>Malic acid</p>
</dd>
<dt>ash</dt><dd><p>Ash</p>
</dd>
<dt>alca</dt><dd><p>Alcalinity of ash</p>
</dd>
<dt>mg</dt><dd><p>Magnesium</p>
</dd>
<dt>phe</dt><dd><p>Total phenols</p>
</dd>
<dt>fla</dt><dd><p>Flavanoids</p>
</dd>
<dt>nfla</dt><dd><p>Nonflavanoid phenols</p>
</dd>
<dt>pro</dt><dd><p>Proanthocyanins</p>
</dd>
<dt>col</dt><dd><p>Color intensity</p>
</dd>
<dt>hue</dt><dd><p>Hue</p>
</dd>
<dt>ratio</dt><dd><p>OD280/OD315 of diluted wines</p>
</dd>
<dt>prol</dt><dd><p>Proline</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/wine">https://archive.ics.uci.edu/ml/datasets/wine</a>
</p>


<h3>References</h3>

<p>M. Forina, C. Armanino, M. Castino and M. Ubigli. Vitis, 25:189-201 (1986)
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
