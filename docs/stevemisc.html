<!DOCTYPE html><html><head><title>Help for package stevemisc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stevemisc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25nin+25'><p>Find Non-Matching Elements</p></a></li>
<li><a href='#binred_plot'><p>Generate a Binned-Residual Plot from a Fitted Generalized Linear Model</p></a></li>
<li><a href='#carrec'><p>Recode a Variable</p></a></li>
<li><a href='#center_at'><p>Scoped Helper Verbs</p></a></li>
<li><a href='#charitable_contributions'><p>Charitable Contributions Panel Data</p></a></li>
<li><a href='#cor2data'><p>Simulate Data from Correlation Matrix</p></a></li>
<li><a href='#corvectors'><p>Create multivariate data by permutation</p></a></li>
<li><a href='#db_lselect'><p>Lazily select variables from multiple tables in a relational database</p></a></li>
<li><a href='#ess9_labelled'><p>Some Labeled Data in the European Social Survey (Round 9)</p></a></li>
<li><a href='#fct_reorg'><p>Reorganize a factor after &quot;re-leveling&quot; it</p></a></li>
<li><a href='#filter_refs'><p>Filter a Data Frame of Citations and Return the Entries as a Character</p></a></li>
<li><a href='#fra_leaderyears'><p>French Leader-Years, 1874-2015</p></a></li>
<li><a href='#get_sims'><p>Get Simulations from a Model Object (with New Data)</p></a></li>
<li><a href='#get_var_info'><p>Get a small data frame of the variable label and values.</p></a></li>
<li><a href='#gmy_dyadyears'><p>German Dyad-Years, 1816-2020</p></a></li>
<li><a href='#jenny'><p>Set the Only Reproducible Seed That Matters</p></a></li>
<li><a href='#linloess_plot'><p>Compare Linear Smoother to LOESS Smoother for Your OLS Model</p></a></li>
<li><a href='#make_perclab'><p>Make Percentage Label for Proportion and Add Percentage Sign</p></a></li>
<li><a href='#make_scale'><p>Rescale Vector to Arbitrary Minimum and Maximum</p></a></li>
<li><a href='#map_quiz'><p>Map Quiz Wrong Guesses Across Five Intro to IR Courses</p></a></li>
<li><a href='#mround'><p>Multiply a Number by 100 and Round It (By Default: 2)</p></a></li>
<li><a href='#normal_dist'><p>Make and annotate a normal distribution with <span class="pkg">ggplot2</span></p></a></li>
<li><a href='#p_z'><p>Convert the p-value you want to the z-value it actually is</p></a></li>
<li><a href='#prepare_refs'><p>Prepare <span class="pkg">bib2df</span> Data Frame for Formatting to Various Outputs</p></a></li>
<li><a href='#print_refs'><p>Print and Format <code>.bib</code> Entries as References</p></a></li>
<li><a href='#ps_btscs'><p>Create &quot;peace years&quot; or &quot;spells&quot; by cross-sectional unit, more generally</p></a></li>
<li><a href='#ps_spells'><p>Create &quot;spells&quot; by cross-sectional unit, even more generally</p></a></li>
<li><a href='#r1sd'><p>Scale a vector by one standard deviation</p></a></li>
<li><a href='#r2sd'><p>Scale a vector (or vectors) by two standard deviations</p></a></li>
<li><a href='#rbnorm'><p>Bounded Normal (Really: Scaled Beta) Distribution</p></a></li>
<li><a href='#rd_plot'><p>Residual Density Plot for Linear Models</p></a></li>
<li><a href='#revcode'><p>Reverse code a numeric variable</p></a></li>
<li><a href='#sbayesboot'><p>Bootstrap a Regression Model, the Bayesian Way</p></a></li>
<li><a href='#sbtscs'><p>Create &quot;peace years&quot; or &quot;spells&quot; by cross-sectional unit</p></a></li>
<li><a href='#show_ranef'><p>Get a caterpillar plot of random effects from a mixed model</p></a></li>
<li><a href='#smvrnorm'><p>Simulate from a Multivariate Normal Distribution</p></a></li>
<li><a href='#stevepubs'><p>An Incomplete List of My Publications, All of Which You Should Cite</p></a></li>
<li><a href='#strategic_rivalries'>
<p>Strategic Rivalries, 1494-2010</p></a></li>
<li><a href='#studentt'><p>The Student-t Distribution (Location-Scale)</p></a></li>
<li><a href='#tbl_df'><p>Convert data frame to an object of class &quot;tibble&quot;</p></a></li>
<li><a href='#theme_steve_web'><p>Legacy functions for Steve's Preferred <span class="pkg">ggplot2</span> Themes and Assorted Stuff</p></a></li>
<li><a href='#usa_mids'><p>United States Militarized Interstate Disputes (MIDs)</p></a></li>
<li><a href='#wls'><p>Get Weighted Least Squares of Your OLS Model</p></a></li>
<li><a href='#wom'><p>Generate Week of the Month from a Date</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Steve's Miscellaneous Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), stats</td>
</tr>
<tr>
<td>Description:</td>
<td>These are miscellaneous functions that I find useful for my research and teaching.
    The contents include themes for plots, functions for simulating
    quantities of interest from regression models, functions for simulating various 
    forms of fake data for instructional/research purposes, and many more. All told, the functions
    provided here are broadly useful for data organization, data presentation, data recoding, 
    and data simulation. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/svmiller/stevemisc/issues">https://github.com/svmiller/stevemisc/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2 (&ge; 3.3.0), magrittr, labelled, arm, parallel, purrr,
tibble, dplyr, methods, lme4, rlang, forcats, stringr, httr,
rmarkdown, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, DBI, RSQLite, dbplyr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-06 08:23:58 UTC; steve</td>
</tr>
<tr>
<td>Author:</td>
<td>Steve Miller [aut, cre],
  Ben Bolker [ctb],
  Dave Armstrong [ctb],
  John Fox [ctb],
  Winston Chang [ctb],
  Brian Ripley [ctb],
  Bill Venables [ctb],
  Pascal van Kooten [ctb],
  Gerko Vink [ctb],
  Paul Williamson [ctb],
  Andreas Beger <a href="https://orcid.org/0000-0003-1883-3169"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Vincent Arel-Bundock
    <a href="https://orcid.org/0000-0003-2042-7063"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Grant McDermott <a href="https://orcid.org/0000-0001-7883-8573"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Hadley Wickham <a href="https://orcid.org/0000-0003-4757-117X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Steve Miller &lt;steven.v.miller@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-06 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25nin+25'>Find Non-Matching Elements</h2><span id='topic++25nin+25'></span>

<h3>Description</h3>

<p><code>%nin%</code> finds non-matching elements in a given vector. It is the negation of <code>%in%</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>a %nin% b
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25nin+2B25_+3A_a">a</code></td>
<td>
<p>a vector (character, factor, or numeric)</p>
</td></tr>
<tr><td><code id="+2B25nin+2B25_+3A_b">b</code></td>
<td>
<p>a vector (character, factor, or numeric)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple negation of <code>%in%</code>. I use it mostly for columns in a data frame.
</p>


<h3>Value</h3>

<p><code>%nin%</code> finds non-matching elements and returns one of two things, depending on the use. For two simple vectors,
it will report what matches and what does not. For comparing a vector within a data frame, it has the effect of reporting the rows
in the data frame that do not match the supplied (second) vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tibble)
library(dplyr)

# Watch this subset stuff

dat &lt;- tibble(x = seq(1:10), d = rnorm(10))
filter(dat, x %nin% c(3, 6, 9))

</code></pre>

<hr>
<h2 id='binred_plot'>Generate a Binned-Residual Plot from a Fitted Generalized Linear Model</h2><span id='topic+binred_plot'></span>

<h3>Description</h3>

<p><code>binred_plot()</code> provides a diagnostic of the fit of
the generalized linear model by &quot;binning&quot; the fitted and residual values
from the model and showing where they may fall outside 95% error bounds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binred_plot(model, nbins, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binred_plot_+3A_model">model</code></td>
<td>
<p>a fitted GLM model, assuming link is &quot;logit&quot;</p>
</td></tr>
<tr><td><code id="binred_plot_+3A_nbins">nbins</code></td>
<td>
<p>number of &quot;bins&quot; for the calculation. Defaults to the rounded
square root of the number of observations in the model in the absence of a
user-specified override here.</p>
</td></tr>
<tr><td><code id="binred_plot_+3A_plot">plot</code></td>
<td>
<p>logical, defaults to TRUE. If TRUE, the function plots the
binned residuals. If FALSE, the function returns a data frame of the
binned residuals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of bins the user wants is arbitrary. Gelman and Hill
(2007) say that, for larger data sets (n &gt;= 100), the number of bins should
be the rounded-down square root of the number of observations from the model.
For models with a number of observations between 10 and 100, the number of
bins should be 10. For models with fewer than 10 observations, the number
of bins should be the rounded-down number of observations (divided by 2).
The default is the rounded square root of the number of observations in
the model. Be smart about what you want here.
</p>


<h3>Value</h3>

<p><code>bindred_plot()</code> returns a plot as a <span class="pkg">ggplot2</span> object, as
a default. The <em>y</em>-axis is the mean residuals of the particular bin. The
<em>x</em>-axis is the mean fitted values from the bin. Error bounds are 95%.
A LOESS smoother is overlaid as a solid blue line.
</p>
<p>If <code>plot = FALSE</code>, the function returns a data frame of the binned residuals
and a summary about whether the residuals are in the error bounds.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- glm(vs ~ mpg + cyl + drat, data=mtcars, family=binomial(link="logit"))

binred_plot(M1)
</code></pre>

<hr>
<h2 id='carrec'>Recode a Variable</h2><span id='topic+carrec'></span><span id='topic+carr'></span>

<h3>Description</h3>

<p>This recodes a numeric vector, character vector, or factor
according to fairly simple recode specifications that former Stata users
will appreciate. Yes, this is taken from John Fox's <code>recode()</code> unction in
his <span class="pkg">car</span> package. I'm going with <code>carrec()</code> (i.e. shorthand for
<code>car::recode()</code>, phonetically here: &quot;car-wreck&quot;) for this package, with
an additional shorthand of <code>carr</code> that does the same thing.
</p>
<p>The goal here is to minimize the number of function clashes with
multiple packages that I use in my workflow. For example: <span class="pkg">car</span>,
<span class="pkg">dplyr</span>, and <span class="pkg">Hmisc</span> all have <code>recode()</code> functions. I rely on
the <span class="pkg">car</span> package just for this function, but it conflicts with some
other <span class="pkg">tidyverse</span> functions that are vital to my workflow.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>carrec(var, recodes, as_fac, as_num = TRUE, levels)

carr(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="carrec_+3A_var">var</code></td>
<td>
<p>numeric vector, character vector, or factor</p>
</td></tr>
<tr><td><code id="carrec_+3A_recodes">recodes</code></td>
<td>
<p>character string of recode specifications: see below, but
former Stata users will find this stuff familiar</p>
</td></tr>
<tr><td><code id="carrec_+3A_as_fac">as_fac</code></td>
<td>
<p>return a factor; default is <code>TRUE</code> if <code>var</code> is a
factor,  <code>FALSE</code> otherwise</p>
</td></tr>
<tr><td><code id="carrec_+3A_as_num">as_num</code></td>
<td>
<p>if <code>TRUE</code> (which is the default) and <code>as.factor</code> is <code>FALSE</code>,
the result will be coerced to a numeric if all values in the result are numeric.
This should be what you want in 99% of applications for regression analysis.</p>
</td></tr>
<tr><td><code id="carrec_+3A_levels">levels</code></td>
<td>
<p>an optional argument specifying the order of the levels in the
returned factor; the default is to use the sort order of the level names.</p>
</td></tr>
<tr><td><code id="carrec_+3A_...">...</code></td>
<td>
<p>optional, only to make the shortcut (<code>carr()</code>) work</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recode specifications appear in a character string, separated by
semicolons (see the examples below), of the form input=output. If an input
value satisfies more than one specification, then the first (from left to
right) applies. If no specification is satisfied, then the input value is
carried over to the result. NA is allowed on input and output.
</p>


<h3>Value</h3>

<p><code>carrec()</code> returns a vector, recoded to the specifications of the
user. <code>carr()</code> is a simple shortcut for<code>carrec()</code>.
</p>


<h3>Author(s)</h3>

<p>John Fox
</p>


<h3>References</h3>

<p>Fox, J. and Weisberg, S. (2019). <em>An R Companion to Applied Regression</em>, Third Edition, Sage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(1,10)
carrec(x,"0=0;1:2=1;3:5=2;6:10=3")

</code></pre>

<hr>
<h2 id='center_at'>Scoped Helper Verbs</h2><span id='topic+center_at'></span><span id='topic+diff_at'></span><span id='topic+group_mean_center_at'></span><span id='topic+lag_at'></span><span id='topic+log_at'></span><span id='topic+mean_at'></span><span id='topic+r1sd_at'></span><span id='topic+r2sd_at'></span>

<h3>Description</h3>

<p>Scoped helper verbs included in this R Documentation file
allow for targeted commands on specified columns. They also rename
the ensuing output to conform to my preferred style. The commands here
are multiple and explained in the details section below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>center_at(data, x, prefix = "c", na = TRUE, .by = NULL)

diff_at(data, x, o = 1, prefix = "d", .by = NULL)

group_mean_center_at(
  data,
  x,
  mean_prefix = "mean",
  prefix = "b",
  na = TRUE,
  .by
)

lag_at(data, x, prefix = "l", o = 1, .by = NULL)

log_at(data, x, prefix = "ln", plus_1 = FALSE)

mean_at(data, x, prefix = "mean", na = TRUE, .by = NULL)

r1sd_at(data, x, prefix = "s", na = TRUE, .by = NULL)

r2sd_at(data, x, prefix = "z", na = TRUE, .by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="center_at_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="center_at_+3A_x">x</code></td>
<td>
<p>a vector, likely in your data frame</p>
</td></tr>
<tr><td><code id="center_at_+3A_prefix">prefix</code></td>
<td>
<p>Allows the user to rename the prefix of the new variables.  Each
function has defaults (see details section).</p>
</td></tr>
<tr><td><code id="center_at_+3A_na">na</code></td>
<td>
<p>a logical about whether missing values should be ignored in the
creation of means and re-scaled variables. Defaults to TRUE (i.e. pass
over/remove missing observations). Not applicable to <code>diff_at</code>,
<code>lag_at</code>, and <code>log_at</code>.</p>
</td></tr>
<tr><td><code id="center_at_+3A_.by">.by</code></td>
<td>
<p>a selection of columns by which to group the operation. Defaults
to NULL. This will eventually become a standard feature of the functions
as this operator moves beyond the experimental in <span class="pkg">dplyr</span>. The argument
is not applicable to <code>log_at</code> (why would it be) and is optional for all
functions except <code>group_mean_center_at</code>. <code>group_mean_center_at</code>
must have something specified for grouped mean-centering.</p>
</td></tr>
<tr><td><code id="center_at_+3A_o">o</code></td>
<td>
<p>The order of lags for calculating differences or lags in
<code>diff_at</code> or <code>lag_at</code>. Applicable only to these functions.</p>
</td></tr>
<tr><td><code id="center_at_+3A_mean_prefix">mean_prefix</code></td>
<td>
<p>Applicable only to <code>group_mean_center_at</code>. Specifies
the prefix of the (assumed) total population mean variables. Default is &quot;mean&quot;,
though the user can change this as they see fit.</p>
</td></tr>
<tr><td><code id="center_at_+3A_plus_1">plus_1</code></td>
<td>
<p>Applicable only to <code>log_at</code>. If TRUE, adds 1 to the
variables prior to log transformation. If FALSE, performs logarithmic
transformation on variables no matter whether 0 occurs (i.e. 0s will
come back as -Inf). Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>center_at</code> is a wrapper for <code>mutate_at</code> and <code>rename_at</code> from
<span class="pkg">dplyr</span>. It takes supplied vectors and effectively centers them from the
mean. It then renames these new variables to have a prefix of <code>c_</code>. The
default prefix (&quot;c&quot;) can be changed by way of an argument in the function.
</p>
<p><code>diff_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It takes supplied vectors and creates differences from the
previous value recorded above it. It then renames these new variables to have
a prefix of <code>d_</code> (in the case of a first difference), or something like
<code>d2_</code> in the case of second differences, or <code>d3_</code> in the case of
third differences (and so on). The exact prefix depends on the <code>o</code>
argument, which communicates the order of lags you want. It defaults to 1. The
default prefix (&quot;d&quot;) can be changed by way of an argument in the function,
though the naming convention will omit a numerical prefix for first
differences.
</p>
<p><code>group_mean_center_at</code> is a wrapper for <code>mutate</code> and <code>across</code>
in <span class="pkg">dplyr</span>. It takes supplied vectors and centers an (assumed) group mean
of the variables from an (assumed) total population mean of the variables
provided to it. It then returns the new variables with a prefix, whose default
is <code>b_</code>. This prefix communicates, if you will, a kind of &quot;between&quot;
variable in the panel model context, in juxtaposition to &quot;within&quot; variables
in the panel model context.
</p>
<p><code>lag_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It takes supplied vector(s) and creates lag variables from them.
These new variables have a prefix of <code>l[o]_</code> where <code>o</code> corresponds
to the order of the lag (specified by an argument in the function, which
defaults to 1). This default prefix (&quot;l&quot;) can be changed by way of an
another argument in the function.
</p>
<p><code>log_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It takes supplied vectors and creates a variable that takes
a natural logarithmic transformation of them. It then renames these new
variables to have a prefix of <code>ln_</code>. This default prefix (&quot;ln&quot;) can be
changed by way of an argument in the function. Users can optionally specify
that they want to add 1 to the vector before taking its natural logarithm,
which is a popular thing to do when positive reals have naturally occurring
zeroes.
</p>
<p><code>mean_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It takes supplied vectors and creates a variable communicating
the mean of the variable. It then renames these new variables to have a
prefix of <code>mean_</code>. This default prefix (&quot;mean&quot;) can be changed by way of
an argument in the function.
</p>
<p><code>r1sd_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It both rescales the supplied vectors to new vectors and renames
the vectors to each have a prefix of <code>s_</code>. Note the rescaling here is
just by one standard deviation and not two. The default prefix (&quot;s&quot;) can be
changed by way of an argument in the function.
</p>
<p><code>r2sd_at</code> is a wrapper for <code>mutate</code> and <code>across</code> from
<span class="pkg">dplyr</span>. It both rescales the supplied vectors to new vectors and renames
the vectors to each have a prefix of <code>z_</code>. Note the rescaling here is by
two standard deviations and not one. The default prefix (&quot;z&quot;) can be
changed by way of an argument in the function.
</p>
<p>All functions, except for <code>lag_at</code>, will fail in the absence of a
character vector of a length of one. They are intended to work across multiple
columns instead of just one. If you are wanting to create one new variable,
you should think about using some other <span class="pkg">dplyr</span> verb on its own.
</p>


<h3>Value</h3>

<p>The function returns a set of new vectors in a data frame after
performing relevant functions. The new vectors have distinct prefixes
corresponding with the action performed on them.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(8675309)
Example &lt;- data.frame(category = c(rep("A", 5),
                                   rep("B", 5),
                                   rep("C", 5)),
                      x = runif(15), y = runif(15),
                      z = sample(1:20, 15, replace=TRUE))

my_vars &lt;- c("x", "y", "z")
center_at(Example, my_vars)

diff_at(Example, my_vars)

diff_at(Example, my_vars, o=3)

lag_at(Example, my_vars)

lag_at(Example, my_vars, o=3)

log_at(Example, my_vars)

log_at(Example, my_vars, plus_1 = TRUE)

mean_at(Example, my_vars)

r1sd_at(Example, my_vars)

r2sd_at(Example, my_vars)
</code></pre>

<hr>
<h2 id='charitable_contributions'>Charitable Contributions Panel Data</h2><span id='topic+charitable_contributions'></span>

<h3>Description</h3>

<p>This is a toy panel data set on charitable contributions across 10 years for
47 taxpayers. It's useful for illustrating the estimation of panel models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>charitable_contributions
</code></pre>


<h3>Format</h3>

<p>A data frame with 470 observations on the following 8 variables.
</p>

<dl>
<dt><code>subject</code></dt><dd><p>a numeric identifier for the subject</p>
</dd>
<dt><code>time</code></dt><dd><p>a numeric time identifier, as a simple integer from 1 to 10</p>
</dd>
<dt><code>charity</code></dt><dd><p>the sum of cash and other property contributions, excluding carry-overs from previous years</p>
</dd>
<dt><code>income</code></dt><dd><p>adjusted gross income</p>
</dd>
<dt><code>price</code></dt><dd><p>1 minus the marginal income tax rate, which is defined on income prior to contributions</p>
</dd>
<dt><code>age</code></dt><dd><p>a dummy variable that equals 1 if the respondent is over 64, 0 otherwise</p>
</dd>
<dt><code>ms</code></dt><dd><p>a dummy variable that equals 1 if the respondent is married, 0 otherwise</p>
</dd>
<dt><code>deps</code></dt><dd><p>the number of claimed dependents, as an integer</p>
</dd></dl>



<h3>Details</h3>

<p>Frees (2003) is the nominal source for these data, as they appear as toy data
sets for use in his book. He in turn cites Banerjee and Frees (1995), though
this citation may have been meant for a 1997 article in <em>Journal of the
American Statistical Association</em>. The
actual source for these data as I obtained them is Gujarati (2012). The
underlying source of the raw data are supposedly the 1979-1988
<em>Statistics of Income</em> Panel of Individual Tax Returns. Given the opacity of
the data, and its temporal limitations, these data should only be used for
illustration and not inference.
</p>
<p>The charitable contributions variable and income variables are very clearly
log-transformed. Banerjee and Price (1997) seem to imply the price variable
is as well.
</p>

<hr>
<h2 id='cor2data'>Simulate Data from Correlation Matrix</h2><span id='topic+cor2data'></span>

<h3>Description</h3>

<p>A function to simulate data from a correlation matrix.
This is useful for illustrating some theoretical properties of
regressions when population parameters are known and set in advance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor2data(cor, n, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor2data_+3A_cor">cor</code></td>
<td>
<p>A correlation matrix (of class <code>matrix</code>)</p>
</td></tr>
<tr><td><code id="cor2data_+3A_n">n</code></td>
<td>
<p>A number of observations to simulate</p>
</td></tr>
<tr><td><code id="cor2data_+3A_seed">seed</code></td>
<td>
<p>An optional parameter to set a seed. Omitting this generates new simulations every time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>cor2data()</code> returns a data frame where all observations are simulated from a standard
normal distribution, but with those pre-set correlations.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vars &lt;- c("control", "treat", "instr", "e")
Correlations &lt;- matrix(cbind(1, 0.001, 0.001, 0.001,
                             0.001, 1, 0.85, -0.5,
                            0.001, 0.85, 1, 0.001,
                           0.001, -0.5, 0.001, 1),nrow=4)

rownames(Correlations) &lt;- colnames(Correlations) &lt;- vars

cor2data(Correlations, 1000, 8675309)

</code></pre>

<hr>
<h2 id='corvectors'>Create multivariate data by permutation</h2><span id='topic+corvectors'></span>

<h3>Description</h3>

<p><code>corvectors()</code> is a function to obtain a multivariate dataset by specifying
the relation between those specified variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corvectors(
  data,
  corm,
  tol = 0.005,
  conv = 10000,
  cores = 2,
  splitsize = 1000,
  verbose = FALSE,
  seed
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corvectors_+3A_data">data</code></td>
<td>
<p>a data matrix containing the data</p>
</td></tr>
<tr><td><code id="corvectors_+3A_corm">corm</code></td>
<td>
<p>A value containing the desired correlation or a vector or data matrix containing the desired correlations</p>
</td></tr>
<tr><td><code id="corvectors_+3A_tol">tol</code></td>
<td>
<p>A single value or a vector of tolerances with length <code>ncol(data) - 1</code>. The default is 0.005</p>
</td></tr>
<tr><td><code id="corvectors_+3A_conv">conv</code></td>
<td>
<p>The maximum iterations allowed. Defaults to 1000.</p>
</td></tr>
<tr><td><code id="corvectors_+3A_cores">cores</code></td>
<td>
<p>The number of cores to be used for parallel computing</p>
</td></tr>
<tr><td><code id="corvectors_+3A_splitsize">splitsize</code></td>
<td>
<p>The size to use for splitting the data</p>
</td></tr>
<tr><td><code id="corvectors_+3A_verbose">verbose</code></td>
<td>
<p>Logical statement. Default is FALSE</p>
</td></tr>
<tr><td><code id="corvectors_+3A_seed">seed</code></td>
<td>
<p>An optional seed to set</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is liberally copy-pasted from van Kooten and Vink's wonderful-but-no-longer-supported <span class="pkg">correlate</span> package.
They call it <code>correlate()</code> in their package, but I opt for <code>corvectors()</code> here.
</p>


<h3>Value</h3>

<p><code>corvectors()</code> returns a matrix given the specified multivariate relation.
</p>


<h3>Author(s)</h3>

<p>Pascal van Kooten and Gerko Vink
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
set.seed(8675309)
library(tibble)
# bivariate example, start with zero correlation
as_tibble(data.frame(corvectors(replicate(2, rnorm(100)), .5)))

# multivariate example
as_tibble(data.frame(corvectors(replicate(4, rnorm(100)), c(.5, .6, .7))))


## End(Not run)

</code></pre>

<hr>
<h2 id='db_lselect'>Lazily select variables from multiple tables in a relational database</h2><span id='topic+db_lselect'></span>

<h3>Description</h3>

<p><code>db_lselect()</code> allows you to select variables from multiple
tables in an SQL database. It returns a lazy query that combines all the
variables together into one data frame (as a <code>tibble</code>). The user can
choose to run <code>collect()</code> after this query if they see fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_lselect(.data, connection, vars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="db_lselect_+3A_.data">.data</code></td>
<td>
<p>a character vector of the tables in a relational database</p>
</td></tr>
<tr><td><code id="db_lselect_+3A_connection">connection</code></td>
<td>
<p>the name of the connection object</p>
</td></tr>
<tr><td><code id="db_lselect_+3A_vars">vars</code></td>
<td>
<p>the variables (entered as class &quot;character&quot;) to select from the tables in the database</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper function in which <span class="pkg">purrr</span> and <span class="pkg">dplyr</span>
are doing the heavy lifting. The tables in the database are declared as a
character (or character vector). The variables to select are also declared
as a character (or character vector), which are then wrapped in a
<code>one_of()</code> function within <code>select()</code> in <span class="pkg">dplyr</span>.
</p>


<h3>Value</h3>

<p>Assuming a particular structure to the database, the function returns a
combined table including all the requested variables from all the tables listed
in the <code>data</code> character vector. The returned table will have other attributes
inherited from how <span class="pkg">dplyr</span> interfaces with SQL, allowing the user to extract
some information about the query (e.g. through <code>show_query()</code>).
</p>


<h3>References</h3>

<p>Miller, Steven V. 2020. &quot;Clever Uses of Relational (SQL) Databases to Store Your Wider Data (with Some Assistance from <code>dplyr</code> and <code>purrr</code>)&quot; <a href="http://svmiller.com/blog/2020/11/smarter-ways-to-store-your-wide-data-with-sql-magic-purrr/">http://svmiller.com/blog/2020/11/smarter-ways-to-store-your-wide-data-with-sql-magic-purrr/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(DBI)
library(RSQLite)
library(dplyr)
library(dbplyr)
set.seed(8675309)

A &lt;- data.frame(uid = c(1:10),
                a = rnorm(10),
                b = sample(letters, 10),
                c = rbinom(10, 1, .5))

B &lt;- data.frame(uid = c(11:20),
                a = rnorm(10),
                b = sample(letters, 10),
                c = rbinom(10, 1, .5))

C &lt;- data.frame(uid = c(21:30), a = rnorm(10),
                b = sample(letters, 10),
                c = rbinom(10, 1, .5),
                d = rnorm(10))

con &lt;- dbConnect(SQLite(), ":memory:")

copy_to(con, A, "A",
        temporary=FALSE)

copy_to(con, B, "B",
        temporary=FALSE)

copy_to(con, C, "C",
        temporary=FALSE)

# This returns no warning because columns "a" and "b" are in all tables
c("A", "B", "C") %&gt;% db_lselect(con, c("uid", "a", "b"))

# This returns two warnings because column "d" is not in 2 of 3 tables.
# ^ this is by design. It'll inform the user about data availability.
c("A", "B", "C") %&gt;% db_lselect(con, c("uid", "a", "b", "d"))
dbDisconnect(con)

</code></pre>

<hr>
<h2 id='ess9_labelled'>Some Labeled Data in the European Social Survey (Round 9)</h2><span id='topic+ess9_labelled'></span>

<h3>Description</h3>

<p>These are data to illustrate labeled data and how to process them with
<code>get_var_info()</code> in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ess9_labelled
</code></pre>


<h3>Format</h3>

<p>A data frame with 109 observations on the following 4 variables.
</p>

<dl>
<dt><code>essround</code></dt><dd><p>a numeric constant</p>
</dd>
<dt><code>edition</code></dt><dd><p>another numeric constant</p>
</dd>
<dt><code>cntry</code></dt><dd><p>a character vector (with label) for the country in the data</p>
</dd>
<dt><code>netusoft</code></dt><dd><p>a numeric vector (with label) for self-reported internet consumption of a respondent</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are condensed summaries from the raw data. They amount to every unique combination of country and
self-reported internet consumption. The data are here to illustrate the <code>get_var_info()</code> function in this package.
</p>

<hr>
<h2 id='fct_reorg'>Reorganize a factor after &quot;re-leveling&quot; it</h2><span id='topic+fct_reorg'></span>

<h3>Description</h3>

<p><code>fct_reorg()</code> is a <span class="pkg">forcats</span> hack that reorganizes a factor after re-leveling it. It has been
situationally useful in my coefficient plots over the years.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fct_reorg(fac, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fct_reorg_+3A_fac">fac</code></td>
<td>
<p>a character or factor vector</p>
</td></tr>
<tr><td><code id="fct_reorg_+3A_...">...</code></td>
<td>
<p>optional parameters to be supplied to <span class="pkg">forcats</span> functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solution comes by way of this issue on Github: <a href="https://github.com/tidyverse/forcats/issues/45">https://github.com/tidyverse/forcats/issues/45</a>
</p>


<h3>Value</h3>

<p>This function takes a character or factor vector and first re-levels it before re-coding certain values. The end
result is a factor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x&lt;-factor(c("a","b","c"))
fct_reorg(x, B="b", C="c")

</code></pre>

<hr>
<h2 id='filter_refs'>Filter a Data Frame of Citations and Return the Entries as a Character</h2><span id='topic+filter_refs'></span>

<h3>Description</h3>

<p><code>filter_refs()</code> is a convenience function I wrote for
filtering a data frame of citations returning the entries as a valid
<code>.bib</code> entry (as a character vector). I wrote this for more easily passing
on citations to the <code>print_refs()</code> function also included in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_refs(bibdat, criteria, type = "bibtexkey")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_refs_+3A_bibdat">bibdat</code></td>
<td>
<p>a data frame of citations, like the one created by the
<span class="pkg">bib2df</span> package</p>
</td></tr>
<tr><td><code id="filter_refs_+3A_criteria">criteria</code></td>
<td>
<p>criteria, specified as a character vector, by which to
filter the data frame of citations</p>
</td></tr>
<tr><td><code id="filter_refs_+3A_type">type</code></td>
<td>
<p>the particular type of citation entry on which to filter.
Defaults to &quot;bibtexkey&quot; (which filters based on a column of unique citation
keys). When <code>type == "year"</code>, the function filters on a character vector of
years.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>filter_refs()</code> assumes some familiarity with <code>BibTeX</code>, <code>.bib</code>
entries, and depends on the <span class="pkg">bib2df</span> package.
</p>


<h3>Value</h3>

<p><code>filter_refs()</code> takes a data frame of citations, like the one
created by the <span class="pkg">bib2df</span> package, and returns a character vector
(amounting to a valid <code>.bib</code> entry) of citations the user wants. This can
then be easily passed to the <code>print_refs()</code> function also included in this
package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Based on `stevepubs` configuration, filter on `BIBTEXKEY` where
# the citation key matches one of these.
filter_refs(stevepubs, c("miller2017etst", "miller2017etjc", "miller2013tdpi"))

# Based on `stevepubs` configuration, filter on `YEAR` where
# the publication year is 2017, 2018, 2019, 2020, or 2021.
filter_refs(stevepubs, c(2017:2021), type = "year")
</code></pre>

<hr>
<h2 id='fra_leaderyears'>French Leader-Years, 1874-2015</h2><span id='topic+fra_leaderyears'></span>

<h3>Description</h3>

<p>These are data generated in <span class="pkg">peacesciencer</span> for all French leader-years from 1874 to 2015. I'm going to use
these data for stress-testing the calculation of so-called &quot;peace spells&quot; for data that are decidedly imbalanced,
as these are.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fra_leaderyears
</code></pre>


<h3>Format</h3>

<p>A data frame with 255 observations on the following 10 variables.
</p>

<dl>
<dt><code>obsid</code></dt><dd><p>the unique observation ID in the Archigos data</p>
</dd>
<dt><code>ccode</code></dt><dd><p>the Correlates of War state code for France (220)</p>
</dd>
<dt><code>leader</code></dt><dd><p>a name&mdash;typically last name&mdash;for the leader</p>
</dd>
<dt><code>year</code></dt><dd><p>an observation year for the leader</p>
</dd>
<dt><code>startdate</code></dt><dd><p>the start date for the leader's period in office</p>
</dd>
<dt><code>enddate</code></dt><dd><p>the end date for the leader's period in office</p>
</dd>
<dt><code>gmlmidongoing</code></dt><dd><p>was there an ongoing inter-state dispute for the leader?</p>
</dd>
<dt><code>gmlmidonset</code></dt><dd><p>was there a new inter-state dispute onset for the leader?</p>
</dd>
<dt><code>gmlmidongoing_init</code></dt><dd><p>was there an ongoing inter-state dispute for the leader that the leader initiated?</p>
</dd>
<dt><code>gmlmidonset_init</code></dt><dd><p>was there a new inter-state dispute onset for the leader that the leader initiated?</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are generated in the development version (scheduled release of v. 0.7) of <span class="pkg">peacesciencer</span>. Conflict data
come from the GML MID data (v. 2.2.1). Leader data come from Archigos (v. 4.1).
</p>


<h3>References</h3>

<p>Goemans, Henk E., Kristian Skrede Gleditsch, and Giacomo Chiozza. 2009. &quot;Introducing Archigos: A Dataset of Political Leaders&quot;
<em>Journal of Peace Research</em> 46(2): 269&ndash;83.
</p>
<p>Gibler, Douglas M., Steven V. Miller, and Erin K. Little. 2016. “An Analysis of the Militarized
Interstate Dispute (MID) Dataset, 1816-2001.” International Studies Quarterly 60(4): 719-730.
</p>

<hr>
<h2 id='get_sims'>Get Simulations from a Model Object (with New Data)</h2><span id='topic+get_sims'></span>

<h3>Description</h3>

<p><code>get_sims()</code> is a function to simulate quantities of interest from a
multivariate normal distribution for &quot;new data&quot; from a regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sims(model, newdata, nsim, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sims_+3A_model">model</code></td>
<td>
<p>a model object</p>
</td></tr>
<tr><td><code id="get_sims_+3A_newdata">newdata</code></td>
<td>
<p>A data frame on some quantities of interest to be simulated</p>
</td></tr>
<tr><td><code id="get_sims_+3A_nsim">nsim</code></td>
<td>
<p>Number of simulations to be run</p>
</td></tr>
<tr><td><code id="get_sims_+3A_seed">seed</code></td>
<td>
<p>An optional seed to set</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This (should) be a flexible function that takes a <code>merMod</code> object
(estimated from <span class="pkg">lme4</span>, <span class="pkg">blme</span>, etc.) or a <code>lm</code> or <code>glm</code> object and
generates some quantities of interest when paired with new data of observations of interest.
Of note: I've really only tested this function with linear models, generalized linear models,
and their mixed model equivalents. For mixed models, this approach does not offer support for
the incorporation of the random effects or the random slopes. It's just for the fixed effects,
which is typically what most people want anyway. Users who want to better incorporate the
random intercepts or slope could find that support in the <span class="pkg">merTools</span> package.
</p>


<h3>Value</h3>

<p><code>get_sims()</code> returns a data frame (as a <code>tibble</code>) with the quantities
of interest and identifying information about the particular simulation number.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Note: these models are dumb, but they illustrate how it works.

M1 &lt;- lm(mpg ~ hp, mtcars)
# Note: this function requires the DV to appear somewhere, anywhere in the "new data"
newdat &lt;- data.frame(mpg = 0,
                     hp = c(mean(mtcars$hp) - sd(mtcars$hp),
                            mean(mtcars$hp),
                            mean(mtcars$hp) + sd(mtcars$hp)))

get_sims(M1, newdat, 100, 8675309)

# Note: this is likely a dumb model, but illustrates how it works.
mtcars$mpgd &lt;- ifelse(mtcars$mpg &gt; 25, 1, 0)

M2 &lt;- glm(mpgd ~ hp, mtcars, family=binomial(link="logit"))

# Again: this function requires the DV to be somewhere, anywhere in the "new data"
newdat$mpgd &lt;- 0

# Note: the simulations are returned on their original "link". Here, that's a "logit"
# You can adjust that accordingly. `plogis(y)` will convert those to probabilities.
get_sims(M2, newdat, 100, 8675309)

library(lme4)
M3 &lt;- lmer(mpg ~ hp + (1 | cyl), mtcars)

# Random effects are not required here since we're passing over them.
get_sims(M3, newdat, 100, 8675309)

## End(Not run)

</code></pre>

<hr>
<h2 id='get_var_info'>Get a small data frame of the variable label and values.</h2><span id='topic+get_var_info'></span><span id='topic+gvi'></span>

<h3>Description</h3>

<p><code>get_var_info()</code> allows you to peek at your labelled data,
extracting a given column's variable labels. The intended use here is mostly
&quot;peeking&quot; for the purpose of recoding column's in the absence of a codebook or
other form of documentation. <code>gvi()</code> is a shortcut for this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_var_info(.data, x)

gvi(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_var_info_+3A_.data">.data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="get_var_info_+3A_x">x</code></td>
<td>
<p>a column within the data frame</p>
</td></tr>
<tr><td><code id="get_var_info_+3A_...">...</code></td>
<td>
<p>optional, only to make the shortcut (<code>gvi</code>) work</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function leans on <code>var_label()</code> and <code>val_label()</code> in the
<code>labelled</code> package, which is a dependency for this package. The function
is designed to be used in a &quot;pipe.&quot;
</p>


<h3>Value</h3>

<p>If the column in the data frame is not labelled, the function returns a message communicating
the absence of labels. If the column in the data frame is labelled, the function returns
a small data frame communicating the <code>var_label()</code> output (<code>var</code>), the (often but not always)
numeric &quot;code&quot; coinciding with with the label (<code>code</code>), and the &quot;label&quot; attached to it (<code>label</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tibble)
library(dplyr)
library(magrittr)

ess9_labelled %&gt;% get_var_info(netusoft) # works, as intended
ess9_labelled %&gt;% get_var_info(cntry) # works, as intended
ess9_labelled %&gt;% get_var_info(ess9round) # barks at you; data are not labelled

</code></pre>

<hr>
<h2 id='gmy_dyadyears'>German Dyad-Years, 1816-2020</h2><span id='topic+gmy_dyadyears'></span>

<h3>Description</h3>

<p>These are data generated in <span class="pkg">peacesciencer</span> for all German (and Prussian) dyad-years from 1816 to 2020. These
are going to be useful in stress-testing what &quot;peace spell&quot; calculations may look like when there is a huge gap
in between years. In the Correlates of War context, Germany disappears from the international system from 1945 to 1990. It'll
also serve as a nice test for making sure spell calculations don't misbehave in the context of missing data. In this application,
there are no data for disputes between 2011 and 2020, but the dyad-years include 2011 to 2020.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmy_dyadyears
</code></pre>


<h3>Format</h3>

<p>A data frame with 11174 observations on the following 6 variables.
</p>

<dl>
<dt><code>dyad</code></dt><dd><p>a unique identifier for the dyad</p>
</dd>
<dt><code>ccode1</code></dt><dd><p>the Correlates of War state code for Germany (255)</p>
</dd>
<dt><code>ccode2</code></dt><dd><p>the Correlates of War state code for the other state in the dyad</p>
</dd>
<dt><code>year</code></dt><dd><p>an observation year for the dyad</p>
</dd>
<dt><code>gmlmidongoing</code></dt><dd><p>was there an ongoing inter-state dispute in the dyad-year?</p>
</dd>
<dt><code>gmlmidonset</code></dt><dd><p>was there a new inter-state dispute onset in the dyad-year</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are generated in the development version (scheduled release of v. 0.7) of <span class="pkg">peacesciencer</span>. Conflict data
come from the GML MID data (v. 2.2.1).
</p>


<h3>References</h3>

<p>Gibler, Douglas M., Steven V. Miller, and Erin K. Little. 2016. “An Analysis of the Militarized
Interstate Dispute (MID) Dataset, 1816-2001.” International Studies Quarterly 60(4): 719-730.
</p>

<hr>
<h2 id='jenny'>Set the Only Reproducible Seed That Matters</h2><span id='topic+jenny'></span>

<h3>Description</h3>

<p><code>jenny()</code> sets a reproducible seed of 8675309. It is the only reproducible seed you should use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jenny(x = 8675309)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jenny_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>jenny()</code> comes with some additional perks if you have the <span class="pkg">emo</span> package installed. The package
is optional.
</p>


<h3>Value</h3>

<p>When <code>x</code> is not specified or is 8675309, the function sets a reproducible seed of 8675309 and returns
a nice message congratulating you for it. If <code>x</code> is not 8675309, the function sets no reproducible seed and
gently admonishes you for wasting its time.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
jenny() # will work and reward you for it
jenny(12345) # will not work and will result in a stern message
</code></pre>

<hr>
<h2 id='linloess_plot'>Compare Linear Smoother to LOESS Smoother for Your OLS Model</h2><span id='topic+linloess_plot'></span>

<h3>Description</h3>

<p><code>linloess_plot()</code> provides a visual diagnostic of the linearity assumption of the OLS model.
Provided an OLS model fit by <code>lm()</code> in base R, the function extracts the model frame and creates a faceted
scatterplot. For each facet, a linear smoother and LOESS smoother are estimated over the points. Users who run
this function can assess just how much the linear smoother and LOESS smoother diverge. The more they diverge, the
more the user can determine how much the OLS model is a good fit as specified. The plot will also point to potential
outliers that may need further consideration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linloess_plot(mod, se = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linloess_plot_+3A_mod">mod</code></td>
<td>
<p>a fitted OLS model</p>
</td></tr>
<tr><td><code id="linloess_plot_+3A_se">se</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. If <code>TRUE</code>, gives standard error estimates with the assorted smoothers.</p>
</td></tr>
<tr><td><code id="linloess_plot_+3A_...">...</code></td>
<td>
<p>optional parameters, passed to the scatterplot (<code>geom_point()</code>) component of this function. Useful if you want to make the smoothers more legible against the points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes an implicit assumption that there is no variable in the regression
formula with the name &quot;.y&quot;.
</p>
<p>It may be in your interest (for the sake of rudimentary diagnostic checks) to
disable the standard error bands for particularly ill-fitting linear models.
</p>


<h3>Value</h3>

<p><code>linloess_plot()</code> returns a faceted scatterplot as a <span class="pkg">ggplot2</span> object. The linear smoother is in solid blue (with blue
standard error bands) and the LOESS smoother is a dashed black line (with gray/default standard error bands). You can add
cosmetic features to it after the fact. The function may spit warnings to you related to the LOESS smoother, depending your data. I think
these to be fine the extent to which this is really just a visual aid and an informal diagnostic for the linearity assumption.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- lm(mpg ~ ., data=mtcars)

linloess_plot(M1)
linloess_plot(M1, color="black", pch=21)
</code></pre>

<hr>
<h2 id='make_perclab'>Make Percentage Label for Proportion and Add Percentage Sign</h2><span id='topic+make_perclab'></span>

<h3>Description</h3>

<p><code>make_perclab()</code> takes a proportion, multiplies it by 100, optionally rounds it, and pastes a percentage sign next to it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_perclab(x, d = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_perclab_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="make_perclab_+3A_d">d</code></td>
<td>
<p>digits to round. Defaults to 2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful if you're modeling proportions in something like a bar chart
(for which proportions are more flexible) but want to label each bar as a percentage. The function here is mostly cosmetic.
</p>


<h3>Value</h3>

<p>The function takes a proportion, multiplies it by 100, (optionally) rounds it to a set decimal point, and pastes a percentage sign next to it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- runif(100)
make_perclab(x)
</code></pre>

<hr>
<h2 id='make_scale'>Rescale Vector to Arbitrary Minimum and Maximum</h2><span id='topic+make_scale'></span>

<h3>Description</h3>

<p><code>make_scale()</code> will rescale any vector to have a user-defined minimum and maximum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_scale(x, minim, maxim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_scale_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="make_scale_+3A_minim">minim</code></td>
<td>
<p>a desired numeric minimum</p>
</td></tr>
<tr><td><code id="make_scale_+3A_maxim">maxim</code></td>
<td>
<p>a desired numeric maximum</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful if you wanted to do some kind of minimum-maximum rescaling of a variable
on some given scale, prominently rescaling to a minimum of 0 and a maximum of 1 (thinking ahead to a regression).
The function is flexible enough for any minimum or maximum.
</p>


<h3>Value</h3>

<p>The function takes a numeric vector and returns a rescaled version of it with the observed (desired) minimum, the observed (desired)
maximum, and rescaled values between both extremes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- runif(100, 1, 100)
make_scale(x, 2, 5) # works
make_scale(x, 5, 2) # results in message
make_scale(x, 0, 1) # probably why you're using this.
</code></pre>

<hr>
<h2 id='map_quiz'>Map Quiz Wrong Guesses Across Five Intro to IR Courses</h2><span id='topic+map_quiz'></span>

<h3>Description</h3>

<p>This is a simple data set that records every wrong guess for map quiz
assignments I gave in my intro to IR class at Clemson University across
five semesters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>map_quiz
</code></pre>


<h3>Format</h3>

<p>A data frame with 1772 observations on the following 8 variables.
</p>

<dl>
<dt><code>class</code></dt><dd><p>an ordered factor of the semester in which the wrong
guess was recorded by a student. Levels include &quot;Spring 2018&quot;, &quot;Fall 2018&quot;,
&quot;Spring 2019&quot;, &quot;Fall 2019&quot;, and &quot;Spring 2020.&quot;</p>
</dd>
<dt><code>students</code></dt><dd><p>the number of students in the class taking the
map quiz.</p>
</dd>
<dt><code>region</code></dt><dd><p>the region map on which the country was located.
Values include &quot;Europe&quot;, &quot;Africa&quot;, &quot;Asia&quot;, &quot;Latin America&quot;, and &quot;MENA.&quot;
&quot;MENA&quot; is short for &quot;Middle East and North Africa.&quot;</p>
</dd>
<dt><code>country</code></dt><dd><p>the country I asked the student to correctly
identify</p>
</dd>
<dt><code>guess</code></dt><dd><p>the country that was the actual state incorrectly
guessed by the student</p>
</dd>
<dt><code>ccode1</code></dt><dd><p>the Correlates of War state code for
the state I wanted the student to identify in <code>country</code>.</p>
</dd>
<dt><code>ccode2</code></dt><dd><p>the Correlates of War state code for the
state that is the wrong guess for the state in <code>guess</code></p>
</dd>
<dt><code>mindist</code></dt><dd><p>the minimum distance (in kilometers) between
<code>country</code> and <code>guess</code></p>
</dd></dl>



<h3>Details</h3>

<p>Students can always not make a guess and be wrong, which explains the
<code>NA</code>s in the data. Students were given five
separate numbered maps and prompted to identify 10 countries each on
them. The maps never changed across five semesters, nor did the prompts.
Use these data as you see fit. Obviously, FERPA considerations mean I
can't share anything else of potential value here.
</p>

<hr>
<h2 id='mround'>Multiply a Number by 100 and Round It (By Default: 2)</h2><span id='topic+mround'></span>

<h3>Description</h3>

<p><code>mround()</code> is a convenience function I wrote for my annotating bar charts that I make.
Assuming a proportion variable, <code>mround()</code> will multiply each value by 100 and round it for presentation.
By default, it rounds to two. The user can adjust this.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mround(x, d = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mround_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="mround_+3A_d">d</code></td>
<td>
<p>the number of decimal points to which the user wants to round. If this is not set, it rounds to two decimal points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a sister function of <code>make_perclab()</code> in the same package. This, however, won't add a percentage sign.
</p>


<h3>Value</h3>

<p>The function takes a numeric vector, multiplies it by 100, rounds it (to two digits by default), and returns it
to the user.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- runif(100)
mround(x)
mround(x, 2) # same as above
mround(x, 3)
</code></pre>

<hr>
<h2 id='normal_dist'>Make and annotate a normal distribution with <span class="pkg">ggplot2</span></h2><span id='topic+normal_dist'></span>

<h3>Description</h3>

<p><code>normal_dist()</code> is a convenience function for making a plot of a normal distribution
with annotated areas underneath the normal curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normal_dist(curvecolor, fillcolor, fontfamily)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normal_dist_+3A_curvecolor">curvecolor</code></td>
<td>
<p>What color should the curve itself be. Any <span class="pkg">ggplot2</span>-recognized format should do here.</p>
</td></tr>
<tr><td><code id="normal_dist_+3A_fillcolor">fillcolor</code></td>
<td>
<p>What color should the area underneath the curve be. Any <span class="pkg">ggplot2</span>-recognized format should do here.</p>
</td></tr>
<tr><td><code id="normal_dist_+3A_fontfamily">fontfamily</code></td>
<td>
<p>Font family for labeling areas underneath the curve. OPTIONAL. You can omit this if you'd like.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The normal distribution is a standard normal distribution with a mean of 0 and a standard deviation of 1.
</p>


<h3>Value</h3>

<p>The function returns a fancy plot of a normal distribution annotated with areas underneath the hood. Note that
whatever color is supplied in <code>fillcolor</code> is automatically lightened for areas further from the center of the curve.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(stevemisc)
normal_dist("blue","red")
normal_dist("purple","orange")
</code></pre>

<hr>
<h2 id='p_z'>Convert the p-value you want to the z-value it actually is</h2><span id='topic+p_z'></span>

<h3>Description</h3>

<p>I <em>loathe</em> how statistical instruction privileges obtaining
a magical p-value by reference to an area underneath the standard normal curve,
only to botch what the actual z-value is corresponding to the magical p-value.
This simple function converts the p-value you want (typically .05,
thanks to R.A. Fisher) to the z-value it actually is for the kind of
claims we typically make in inferential statistics. If we're going to do
inference the wrong way, let's at least get the z-value right.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_z(x, ts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_z_+3A_x">x</code></td>
<td>
<p>a numeric vector (one or multiple) between 0 or 1</p>
</td></tr>
<tr><td><code id="p_z_+3A_ts">ts</code></td>
<td>
<p>a logical, defaults to TRUE. If TRUE, returns two-sided critical z-value.
If FALSE, the function returns a one-sized critical z-value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>p_z()</code> takes a p-value of interest and converts it, with precision,
to the z-value it actually is. The function takes a vector and returns a vector. The
function assumes you're doing something akin to calculating a confidence interval or
testing a regression coefficient against a null hypothesis of zero. This means the default
output is a two-sided critical z-value. We're taught to use two-sided z-values when we're
agnostic about the direction of the effect or statistic of interest, which is, to be frank,
hilarious given how most research is typically done.
</p>


<h3>Value</h3>

<p>This function takes a numeric vector, corresponding to the p-value you want, and
returns a numeric vector coinciding with the z-value you want under the standard normal
distribution. For example, the z-value corresponding with the magic number of .05 (the
conventional cutoff for assessing statistical significance) is not 1.96, it's something
like 1.959964 (rounding to the default six decimal points).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(stevemisc)

p_z(.05)
p_z(c(.001, .01, .05, .1))
p_z(.05, ts=FALSE)
p_z(c(.001, .01, .05, .1), ts=FALSE)
</code></pre>

<hr>
<h2 id='prepare_refs'>Prepare <span class="pkg">bib2df</span> Data Frame for Formatting to Various Outputs</h2><span id='topic+prepare_refs'></span>

<h3>Description</h3>

<p><code>prepare_refs</code> does some last-minute formatting of a data frame created by
<span class="pkg">bib2df</span> so that it can be formatted nicely to various outputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_refs(bib2df_refs, toformat = "plain")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_refs_+3A_bib2df_refs">bib2df_refs</code></td>
<td>
<p>a data frame created by <span class="pkg">bib2df</span></p>
</td></tr>
<tr><td><code id="prepare_refs_+3A_toformat">toformat</code></td>
<td>
<p>what type of output you are ultimately going to want from <code>print_refs()</code> . Default is &quot;plain&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is designed to work more generally in the absence of various fields. Assume,
for example, that your data frame has no <code>BOOK</code> field. The function uses the <code>one_of()</code> wrapper
to work around this. The &quot;warning&quot; returned by the function is more of a message. This function may be expanded as
I think of more use cases.
</p>


<h3>Value</h3>

<p><code>print_refs()</code>  does some last-minute formatting to a data frame created by
<span class="pkg">bib2df</span> so that rendering in R Markdown is a little easier and less code-heavy.
</p>


<h3>See Also</h3>

<p><code>print_refs()</code> for formatting a <code>.bib</code> references to various outputs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
prepare_refs(stevepubs)
</code></pre>

<hr>
<h2 id='print_refs'>Print and Format <code>.bib</code> Entries as References</h2><span id='topic+print_refs'></span>

<h3>Description</h3>

<p><code>print_refs()</code> is a convenience function I found and
edited that will allow a user to print and format <code>.bib</code>
entries as if they
were references. This function is useful if you want to load a <code>.bib</code>
entry or set of entries and print them in the middle of a document in
R Markdown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>print_refs(
  bib,
  csl = "american-political-science-association.csl",
  toformat = "markdown_strict",
  cslrepo = "https://raw.githubusercontent.com/citation-style-language/styles/master",
  spit_out = TRUE,
  delete_after = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_refs_+3A_bib">bib</code></td>
<td>
<p>a valid <code>.bib</code> entry</p>
</td></tr>
<tr><td><code id="print_refs_+3A_csl">csl</code></td>
<td>
<p>a CSL file, matching one available on the Github repository, that the user wants to format the references. Default is &quot;american-political-science-association.csl&quot;.</p>
</td></tr>
<tr><td><code id="print_refs_+3A_toformat">toformat</code></td>
<td>
<p>the output wanted by the user. Default is &quot;markdown_strict&quot;.</p>
</td></tr>
<tr><td><code id="print_refs_+3A_cslrepo">cslrepo</code></td>
<td>
<p>a directory of CSL files. Defaults to the one on Github.</p>
</td></tr>
<tr><td><code id="print_refs_+3A_spit_out">spit_out</code></td>
<td>
<p>logical, defaults to TRUE. If TRUE, wraps (&quot;spits out&quot;) formatted citations in a <code>writeLines()</code> output for the console. If <code>FALSE</code>, returns a character vector.</p>
</td></tr>
<tr><td><code id="print_refs_+3A_delete_after">delete_after</code></td>
<td>
<p>logical, defaults to TRUE. If TRUE, deletes CSL file when it's done. If FALSE, retains CSL for (potential) future use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print_refs()</code> assumes an active internet connection in the absence of the appropriate CSL file in the
working directory. The citation style language (CSL) file supplied by the user must match a file in the
massive Github repository of CSL files. Users interested in potential outputs should read more about Pandoc (<a href="https://pandoc.org/MANUAL.html">https://pandoc.org/MANUAL.html</a>).
The Github repository of CSL files is available here: <a href="https://github.com/citation-style-language/styles">https://github.com/citation-style-language/styles</a>.
</p>


<h3>Value</h3>

<p><code>print_refs()</code> takes a <code>.bib</code> entry and returns the
requested formatted reference or references from it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

example &lt;- "@Book{vasquez2009twp, Title = {The War Puzzle Revisited},
Author = {Vasquez, John A}, Publisher = {New York, NY: Cambridge University Press},
Year = {2009}}"

print_refs(example)

</code></pre>

<hr>
<h2 id='ps_btscs'>Create &quot;peace years&quot; or &quot;spells&quot; by cross-sectional unit, more generally</h2><span id='topic+ps_btscs'></span>

<h3>Description</h3>

<p><code>ps_btscs()</code> allows you to create spells (&quot;peace years&quot; in
the international conflict context) between observations of some event. This
will allow the researcher to better model temporal dependence in binary time-series
cross-section (&quot;BTSCS&quot;) models. It is an improvement on <code>sbtscs()</code> (included in
this package) by its ability to more flexibly work with data that have lots of <code>NAs</code>
that bracket the observed event data. It is used in the <code>peacesciencer</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ps_btscs(data, event, tvar, csunit, pad_ts = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ps_btscs_+3A_data">data</code></td>
<td>
<p>the data set with which you are working</p>
</td></tr>
<tr><td><code id="ps_btscs_+3A_event">event</code></td>
<td>
<p>some event (0, 1) for which you want spells or peace years</p>
</td></tr>
<tr><td><code id="ps_btscs_+3A_tvar">tvar</code></td>
<td>
<p>the time variable (e.g. a year)</p>
</td></tr>
<tr><td><code id="ps_btscs_+3A_csunit">csunit</code></td>
<td>
<p>the cross-sectional unit (likely a dyad if you're doing boilerplate international conflict stuff)</p>
</td></tr>
<tr><td><code id="ps_btscs_+3A_pad_ts">pad_ts</code></td>
<td>
<p>should time-series be filled when panels are unbalanced/have gaps? Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is derived from <code>sbtscs()</code>. See documentation there for more information.
</p>


<h3>Value</h3>

<p><code>ps_btscs()</code> takes a data frame and returns the data frame with a new variable
named <code>spell</code>.
</p>


<h3>Author(s)</h3>

<p>David A. Armstrong, Steven V. Miller
</p>


<h3>References</h3>

<p>Armstrong, Dave. 2016. &ldquo;<span class="pkg">DAMisc</span>: Dave Armstrong's Miscellaneous Functions.&rdquo;
<em>R package version 1.4-3</em>.
</p>
<p>Miller, Steven V. 2017. &ldquo;Quickly Create Peace Years for BTSCS Models with <code>sbtscs</code> in <code>stevemisc</code>.&rdquo;
<a href="http://svmiller.com/blog/2017/06/quickly-create-peace-years-for-btscs-models-with-stevemisc/">http://svmiller.com/blog/2017/06/quickly-create-peace-years-for-btscs-models-with-stevemisc/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(stevemisc)
data(usa_mids)

# notice: no quotes
ps_btscs(usa_mids, midongoing, year, dyad)


</code></pre>

<hr>
<h2 id='ps_spells'>Create &quot;spells&quot; by cross-sectional unit, even more generally</h2><span id='topic+ps_spells'></span>

<h3>Description</h3>

<p><code>ps_spells()</code> allows you to create spells (&quot;peace years&quot; in
the international conflict context) between observations of some event. This
will allow the researcher to better model temporal dependence in binary time-series
cross-section (&quot;BTSCS&quot;) models. The function is one of three in this package, and the contents
of this function are partly ported from the <code>add_duration()</code> function in the <span class="pkg">spduration</span>
package. That function, unlike the other two I offer here, works much better where panels are decidedly
imbalanced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ps_spells(data, event, tvar, csunit, time_type = "year", ongoing = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ps_spells_+3A_data">data</code></td>
<td>
<p>the data set with which you are working</p>
</td></tr>
<tr><td><code id="ps_spells_+3A_event">event</code></td>
<td>
<p>some event (0, 1) for which you want spells</p>
</td></tr>
<tr><td><code id="ps_spells_+3A_tvar">tvar</code></td>
<td>
<p>the time variable (e.g. a year)</p>
</td></tr>
<tr><td><code id="ps_spells_+3A_csunit">csunit</code></td>
<td>
<p>the cross-sectional unit (e.g. a dyad or leader)</p>
</td></tr>
<tr><td><code id="ps_spells_+3A_time_type">time_type</code></td>
<td>
<p>what type of time-unit are the data? Right now, this will only work with years but support for months and days are forthcoming. Don't do anything with this argument just yet.</p>
</td></tr>
<tr><td><code id="ps_spells_+3A_ongoing">ongoing</code></td>
<td>
<p>If <code>TRUE</code>, successive 1s are considered ongoing events
and treated as <code>NA</code> after the first 1. If <code>FALSE</code>, successive 1s
are all treated as failures. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is derived from <code>add_duration()</code>  in the <span class="pkg">spduration</span>
package. See documentation there for more information. I thank Andreas Beger for the blessing to port parts of
it here.
</p>


<h3>Value</h3>

<p><code>ps_spells()</code> takes a data frame and returns the data frame with a new variable
named <code>spell</code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Beger, Steven V. Miller
</p>


<h3>References</h3>

<p>Beger, Andreas, Daina Chiba, Daniel W. Hill, Jr, Nils W. Metternich, Shahryar Minhas and Michael D. Ward. 2018.
&ldquo;<span class="pkg">spduration</span>: Split-Population and Duration (Cure) Regression.&rdquo; <em>R package version 0.17.1</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
One &lt;- ps_btscs(usa_mids, midongoing, year, dyad)
Two &lt;- ps_spells(usa_mids, midongoing, year, dyad)
identical(One, Two)


</code></pre>

<hr>
<h2 id='r1sd'>Scale a vector by one standard deviation</h2><span id='topic+r1sd'></span>

<h3>Description</h3>

<p><code>r1sd</code> allows you to rescale a numeric vector such that the
ensuing output has a mean of 0 and a standard deviation of 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r1sd(x, na = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r1sd_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="r1sd_+3A_na">na</code></td>
<td>
<p>what to do with NAs in the vector. Defaults to TRUE (i.e. passes over the missing observations)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a convenience function since the default <code>rescale()</code> function
has some additional weirdness that is not welcome for my use cases. By default,
<code>na.rm</code> is set to TRUE.
</p>


<h3>Value</h3>

<p>The function returns a numeric vector rescaled with a mean of 0 and a
standard deviation of 1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnorm(100)
r1sd(x)
</code></pre>

<hr>
<h2 id='r2sd'>Scale a vector (or vectors) by two standard deviations</h2><span id='topic+r2sd'></span>

<h3>Description</h3>

<p><code>r2sd</code> allows you to rescale a numeric vector such that the
ensuing output has a mean of 0 and a standard deviation of .5. <code>r2sd_at</code> is a wrapper for
<code>mutate_at</code> and <code>rename_at</code> from <span class="pkg">dplyr</span>. It both rescales the supplied vectors to
new vectors and renames the vectors to each have a prefix of <code>z_</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2sd(x, na = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2sd_+3A_x">x</code></td>
<td>
<p>a vector, likely in your data frame</p>
</td></tr>
<tr><td><code id="r2sd_+3A_na">na</code></td>
<td>
<p>what to do with NAs in the vector. Defaults to TRUE (i.e. passes over the missing observations)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code>na.rm</code> is set to TRUE. If you have missing data, the function will just pass
over them.
</p>
<p>Gelman (2008) argues that rescaling by two standard deviations puts regression inputs
on roughly the same scale no matter their original scale. This allows for some honest, if preliminary,
assessment of relative effect sizes from the regression output. This does that, but
without requiring the <code>rescale</code> function from <span class="pkg">arm</span>.
I'm trying to reduce the packages on which my workflow relies.
</p>
<p>Importantly, I tend to rescale only the ordinal and interval inputs and leave the binary inputs as 0/1.
So, my <code>r2sd</code> function doesn't have any of the fancier if-else statements that Gelman's <code>rescale</code>
function has.
</p>


<h3>Value</h3>

<p>The function returns a numeric vector rescaled with a mean of 0 and a
standard deviation of .5.
</p>


<h3>References</h3>

<p>Gelman, Andrew. 2008. &quot;Scaling Regression Inputs by Dividing by Two Standard Deviations.&quot; <em>Statistics in Medicine</em> 27: 2865&ndash;2873.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnorm(100)
r2sd(x)

r2sd_at(mtcars, c("mpg", "hp", "disp"))
</code></pre>

<hr>
<h2 id='rbnorm'>Bounded Normal (Really: Scaled Beta) Distribution</h2><span id='topic+rbnorm'></span>

<h3>Description</h3>

<p><code>rbnorm()</code> is a function to randomly generate values from a bounded normal
(really: a scaled beta) distribution with specified mean, standard deviation, and upper/lower bounds.
I use this function to randomly generate data that we treat as interval for sake of getting
means and standard  deviations, but have discernible bounds (and even skew) to teach students
about things like random sampling and central limit theorem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbnorm(n, mean, sd, lowerbound, upperbound, round = FALSE, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbnorm_+3A_n">n</code></td>
<td>
<p>the number of observations to simulate</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_mean">mean</code></td>
<td>
<p>a mean to approximate</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_sd">sd</code></td>
<td>
<p>a standard deviation to approximate</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_lowerbound">lowerbound</code></td>
<td>
<p>a lower bound for the data to be generated</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_upperbound">upperbound</code></td>
<td>
<p>an upper bound for the data to be generated</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_round">round</code></td>
<td>
<p>whether to round the values to whole integers. Defaults to FALSE</p>
</td></tr>
<tr><td><code id="rbnorm_+3A_seed">seed</code></td>
<td>
<p>set an optional seed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>I call it &quot;bounded normal&quot; when it's really a beta distribution. I'm aware of this. I took
much of this code from somewhere. I forget where.
</p>


<h3>Value</h3>

<p>The function returns a vector of simulated data approximating the user-specified conditions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tibble)

tibble(x = rbnorm(10000, 57, 14, 0, 100))
tibble(x = rbnorm(10000, 57, 14, 0, 100, round = TRUE))
tibble(x = rbnorm(10000, 57, 14, 0, 100, seed = 8675309))
</code></pre>

<hr>
<h2 id='rd_plot'>Residual Density Plot for Linear Models</h2><span id='topic+rd_plot'></span>

<h3>Description</h3>

<p><code>rd_plot()</code> provides a visual diagnostic of the normality
assumption of the linear model. Provided an OLS model fit by <code>lm()</code> in
base R, the function extracts the residuals of the model and creates
a density plot of those residuals (solid black line) against a standard
normal distribution with a mean of 0 and a standard deviation matching the
standard deviation of the residuals from the model. The function may be used
for diagnostic purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rd_plot(mod)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rd_plot_+3A_mod">mod</code></td>
<td>
<p>a fitted linear model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user can always add <span class="pkg">ggplot2</span> elements on top of this for
greater legibility/clarity. For example, density plots can be finicky about
making observations appear where they don't. Perhaps adjusting the scale
of <code>x</code> ad hoc, after the fact, may be warranted.
</p>
<p>The goal of this function is to emphasize that in many real world applications,
the normality assumption of the residuals is never held but can often be
reasonably approximated upon visual inspection.
</p>


<h3>Value</h3>

<p><code>rd_plot()</code> returns a density plot a <span class="pkg">ggplot2</span> object. A
density plot of the actual residuals is a solid black line. A stylized normal
distribution matching the description of the residuals is the blue dashed
line.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- lm(mpg ~ ., data=mtcars)

rd_plot(M1)
</code></pre>

<hr>
<h2 id='revcode'>Reverse code a numeric variable</h2><span id='topic+revcode'></span>

<h3>Description</h3>

<p><code>revcode</code> allows you to reverse code a numeric variable. If, say,
you have a Likert item that has values of 1, 2, 3, 4, and 5, the function inverts
the scale so that 1 = 5, 2 = 4, 3 = 3, 4 = 2, and 5 = 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revcode(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revcode_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function passes over NAs you may have in your variable. It does assume,
reasonably might I add, that the observed values include both the minimum and the maximum.
This is usually the case in a discrete ordered-categorical variable (like a Likert item). It
also assumes that the numeric vector supplied to it contains all possible values and that
the minimum observed value is 1. This is usually a safe assumption in survey data where the variable of
interest is ordinal (either on a 1:4 scale, or 1:5 scale, or 1:10 scale). No matter, use the function
with that in mind.
</p>


<h3>Value</h3>

<p>The function returns a numeric vector that reverse codes the the
numeric vector that was supplied to it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.frame(x1 = rep(c(1:7, NA), 2),
      x2 = c(1:10, 1:4, NA, NA),
     x3 = rep(c(1:4), 4)) -&gt; example_data

library(dplyr)
library(magrittr)

example_data %&gt;% mutate_at(vars("x1", "x2", "x3"), ~revcode(.))
</code></pre>

<hr>
<h2 id='sbayesboot'>Bootstrap a Regression Model, the Bayesian Way</h2><span id='topic+sbayesboot'></span>

<h3>Description</h3>

<p><code>sbayesboot()</code> performs a Bayesian bootstrap of a regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbayesboot(object, reps = 1000L, seed, cluster = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbayesboot_+3A_object">object</code></td>
<td>
<p>a regression model object</p>
</td></tr>
<tr><td><code id="sbayesboot_+3A_reps">reps</code></td>
<td>
<p>how many bootstrap replicates the user wants. Defaults to 1000</p>
</td></tr>
<tr><td><code id="sbayesboot_+3A_seed">seed</code></td>
<td>
<p>set an optional seed for reproducibility</p>
</td></tr>
<tr><td><code id="sbayesboot_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster for calibrating the weights</p>
</td></tr>
<tr><td><code id="sbayesboot_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The code underpinning <code>sbayesboot()</code> is largely derived from
code provided by Grant McDermott and Vincent Arel-Bundock. My approach here
takes the flexibility of McDermott's model-agnostic code (along with the
ease of specifying clusters) and combines it with Arel-Bundock's
<code>update()</code> approach to the actual bootstrapping. I may have screwed
something up, so feel free to point to cases where I did screw up.
</p>


<h3>Value</h3>

<p><code>sbayesboot()</code> takes a fitted regression model and returns a matrix
of bootstrapped coefficients (with intercept). These could be easily
converted to a data frame for ease of summary.
</p>


<h3>Author(s)</h3>

<p>Grant McDermott, Vincent Arel-Bundock
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- lm(mpg ~ disp + wt + hp, mtcars)

# Default options

BB1 &lt;- sbayesboot(M1)

# Cluster bootstrap on cylinder variable

BB2 &lt;- sbayesboot(M1, cluster=~cyl)

</code></pre>

<hr>
<h2 id='sbtscs'>Create &quot;peace years&quot; or &quot;spells&quot; by cross-sectional unit</h2><span id='topic+sbtscs'></span>

<h3>Description</h3>

<p><code>sbtscs()</code> allows you to create spells (&quot;peace years&quot; in
the international conflict context) between observations of some event. This
will allow the researcher to better model temporal dependence in binary time-series
cross-section (&quot;BTSCS&quot;) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbtscs(data, event, tvar, csunit, pad_ts = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbtscs_+3A_data">data</code></td>
<td>
<p>the data set with which you are working</p>
</td></tr>
<tr><td><code id="sbtscs_+3A_event">event</code></td>
<td>
<p>some event (0, 1) for which you want spells or peace years</p>
</td></tr>
<tr><td><code id="sbtscs_+3A_tvar">tvar</code></td>
<td>
<p>the time variable (e.g. a year)</p>
</td></tr>
<tr><td><code id="sbtscs_+3A_csunit">csunit</code></td>
<td>
<p>the cross-sectional unit (likely a dyad if you're doing boilerplate international conflict stuff)</p>
</td></tr>
<tr><td><code id="sbtscs_+3A_pad_ts">pad_ts</code></td>
<td>
<p>should time-series be filled when panels are unbalanced/have gaps? Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>I should confess outright, and it should be obvious to anyone
who looks at the code, that I liberally copy from Dave Armstrong's
<code>btscs()</code> function in the <span class="pkg">DAMisc</span> package. I offer two such improvements.
One, the <code>btscs()</code> function chokes when a large number of cross-sectional units
have no recorded &quot;event.&quot; I don't know why this happens but it does. Further, &quot;tidying&quot;
up the code by leaning on <span class="pkg">dplyr</span> substantially speeds up computation. Incidentally,
this concerns the same cross-sectional units with no recorded events that can choke the
<code>btscs()</code> function in large numbers.
</p>


<h3>Value</h3>

<p><code>sbtscs()</code> takes a data frame and returns the data frame with a new variable
named <code>spell</code>.
</p>


<h3>Author(s)</h3>

<p>David A. Armstrong, Steven V. Miller
</p>


<h3>References</h3>

<p>Armstrong, Dave. 2016. &ldquo;<span class="pkg">DAMisc</span>: Dave Armstrong's Miscellaneous Functions.&rdquo;
<em>R package version 1.4-3</em>.
</p>
<p>Miller, Steven V. 2017. &ldquo;Quickly Create Peace Years for BTSCS Models with <code>sbtscs</code> in <code>stevemisc</code>.&rdquo;
<a href="http://svmiller.com/blog/2017/06/quickly-create-peace-years-for-btscs-models-with-stevemisc/">http://svmiller.com/blog/2017/06/quickly-create-peace-years-for-btscs-models-with-stevemisc/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)
library(stevemisc)
data(usa_mids)

# notice: no quotes
sbtscs(usa_mids, midongoing, year, dyad)

## End(Not run)

</code></pre>

<hr>
<h2 id='show_ranef'>Get a caterpillar plot of random effects from a mixed model</h2><span id='topic+show_ranef'></span>

<h3>Description</h3>

<p><code>show_ranef()</code> allows a user estimating a mixed model to quickly
plot the random intercepts (with conditional variances) of a given random effect
in a mixed model. In cases where there is a random slope over the intercept, the function
plots the random slope as another caterpillar plot (as another facet)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_ranef(model, group, reorder = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_ranef_+3A_model">model</code></td>
<td>
<p>a fitted mixed model with random intercepts</p>
</td></tr>
<tr><td><code id="show_ranef_+3A_group">group</code></td>
<td>
<p>What random intercept/slopes do you want to see as a caterpillar plot? Declare it as a character</p>
</td></tr>
<tr><td><code id="show_ranef_+3A_reorder">reorder</code></td>
<td>
<p>optional argument. DEFAULT is TRUE, which &ldquo;re-orders&rdquo; the intercepts by the
original value in the data. If FALSE, the ensuing caterpillar plot defaults to a default method of ordering
the levels of the random effect by their estimated conditional mode.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a simple wrapper in which <code>broom.mixed</code> and, obviously
<code>ggplot2</code> are doing the heavy lifting.
</p>


<h3>Value</h3>

<p><code>show_ranef()</code> returns a caterpillar plot of the random intercepts from a given
mixed model. If <code>broom.mixed::augment()</code> can process it, this function should work just fine.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(lme4)
library(stevemisc)
data(sleepstudy)

M1 &lt;- lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy)
show_ranef(M1, "Subject")
show_ranef(M1, "Subject", reorder=FALSE)

</code></pre>

<hr>
<h2 id='smvrnorm'>Simulate from a Multivariate Normal Distribution</h2><span id='topic+smvrnorm'></span>

<h3>Description</h3>

<p><code>smvrnorm()</code> simulates data from a multivariate normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smvrnorm(
  n = 1,
  mu,
  sigma,
  tol = 1e-06,
  empirical = FALSE,
  eispack = FALSE,
  seed
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smvrnorm_+3A_n">n</code></td>
<td>
<p>the number of observations to simulate</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_mu">mu</code></td>
<td>
<p>a vector of means</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_sigma">sigma</code></td>
<td>
<p>a positive-definite symmetric matrix specifying the covariance matrix of the variables.</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_tol">tol</code></td>
<td>
<p>tolerance (relative to largest variance) for numerical lack of positive-definiteness in <code>sigma</code>.</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_empirical">empirical</code></td>
<td>
<p>logical. If true, <code>mu</code> and <code>sigma</code> specify the empirical not population mean and covariance matrix.</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_eispack">eispack</code></td>
<td>
<p>logical. values other than FALSE result in an error</p>
</td></tr>
<tr><td><code id="smvrnorm_+3A_seed">seed</code></td>
<td>
<p>set an optional seed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple port and rename of <code>mvrnorm()</code> from the <span class="pkg">MASS</span> package. I elect
to plagiarize/port it because the <span class="pkg">MASS</span> package conflicts with a lot of things in my workflow,
especially <code>select()</code>. This is useful for &quot;informal Bayes&quot; approaches to generating quantities
of interest from a regression model.
</p>


<h3>Value</h3>

<p>The function returns simulated data from a multivariate normal distribution.
</p>


<h3>References</h3>

<p>B. D. Ripley (1987) <em>Stochastic Simulation.</em> Wiley. Page 98.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- lm(mpg ~ disp + cyl, mtcars)

smvrnorm(100, coef(M1), vcov(M1))

</code></pre>

<hr>
<h2 id='stevepubs'>An Incomplete List of My Publications, All of Which You Should Cite</h2><span id='topic+stevepubs'></span>

<h3>Description</h3>

<p>These are data on my publications, barring a few things like book reviews and some forthcoming pieces.
I use these data to illustrate the <code>print_refs()</code> function. You should cite my publications more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stevepubs
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 14 variables.
</p>

<dl>
<dt><code>CATEGORY</code></dt><dd><p>the entry type</p>
</dd>
<dt><code>BIBTEXKEY</code></dt><dd><p>the unique entry key</p>
</dd>
<dt><code>AUTHOR</code></dt><dd><p>a list of authors for this entry</p>
</dd>
<dt><code>BOOKTITLE</code></dt><dd><p>the book title, if appropriate</p>
</dd>
<dt><code>JOURNAL</code></dt><dd><p>the journal title, if appropriate</p>
</dd>
<dt><code>NUMBER</code></dt><dd><p>the journal volume number, if appropriate</p>
</dd>
<dt><code>PAGES</code></dt><dd><p>the range of page numbers, if appropriate</p>
</dd>
<dt><code>PUBLISHER</code></dt><dd><p>the book publisher, if appropriate</p>
</dd>
<dt><code>TITLE</code></dt><dd><p>the title of the publication</p>
</dd>
<dt><code>VOLUME</code></dt><dd><p>the journal volume number, if appropriate</p>
</dd>
<dt><code>YEAR</code></dt><dd><p>the year of publication, as a character. Publications with no year are assumed to be forthcoming</p>
</dd>
<dt><code>DOI</code></dt><dd><p>a DOI, if I entered one</p>
</dd>
</dl>



<h3>Details</h3>

<p>Cite my publications more, you goons. <em>Extremely Smokey Bear voice</em> Only YOU can jack my h-index to infinity.
</p>

<hr>
<h2 id='strategic_rivalries'>
Strategic Rivalries, 1494-2010
</h2><span id='topic+strategic_rivalries'></span>

<h3>Description</h3>

<p>A simple summary of all strategic (inter-state) rivalries from Thompson and Dreyer (2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("strategic_rivalries")</code></pre>


<h3>Format</h3>

<p>A data frame with 197 observations on the following 10 variables.
</p>

<dl>
<dt><code>rivalryno</code></dt><dd><p>a numeric vector for the rivalry number</p>
</dd>
<dt><code>rivalryname</code></dt><dd><p>a character vector for the rivalry name</p>
</dd>
<dt><code>sidea</code></dt><dd><p>a character vector for the first country in the rivalry</p>
</dd>
<dt><code>sideb</code></dt><dd><p>a character vector for the second country in the rivalry</p>
</dd>
<dt><code>styear</code></dt><dd><p>a numeric vector for the start year of the rivalry</p>
</dd>
<dt><code>endyear</code></dt><dd><p>a numeric vector for the end year of the rivalry</p>
</dd>
<dt><code>region</code></dt><dd><p>a character vector for the region of the rivalry, per Thompson and Dreyer (2012)</p>
</dd>
<dt><code>type1</code></dt><dd><p>a character vector for the primary type of the rivalry (spatial, positional, ideological, or interventionary)</p>
</dd>
<dt><code>type2</code></dt><dd><p>a character vector for the secondary type of the rivalry, if applicable (spatial, positional, ideological, or interventionary)</p>
</dd>
<dt><code>type3</code></dt><dd><p>a character vector for the tertiary type of the rivalry, if applicable (spatial, positional, ideological, or interventionary)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Information gathered from the appendix of Thompson and Dreyer (2012). Ongoing rivalries are right-bound at 2010, the date of publication for Thompson and Dreyer's handbook. Users are free to change this if they like.
</p>


<h3>References</h3>

<p>Thompson, William R. and David Dreyer. 2012. Handbook of International Rivalries. CQ Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(strategic_rivalries)
</code></pre>

<hr>
<h2 id='studentt'>The Student-t Distribution (Location-Scale)</h2><span id='topic+studentt'></span><span id='topic+dst'></span><span id='topic+pst'></span><span id='topic+qst'></span><span id='topic+rst'></span>

<h3>Description</h3>

<p>These are density, distribution function, quantile function and random generation
for the Student-t distribution with location <code>mu</code>, scale <code>sigma</code>,
and degrees of freedom <code>df</code>. Base R gives you the so-called &quot;standard&quot; Student-t
distribution, with just the varying degrees of freedom. This generalizes that standard
Student-t to the three-parameter version.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dst(x, df, mu, sigma)

pst(q, df, mu, sigma)

qst(p, df, mu, sigma)

rst(n, df, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="studentt_+3A_x">x</code>, <code id="studentt_+3A_q">q</code></td>
<td>
<p>a vector of quantiles</p>
</td></tr>
<tr><td><code id="studentt_+3A_df">df</code></td>
<td>
<p>a vector of degrees of freedom</p>
</td></tr>
<tr><td><code id="studentt_+3A_mu">mu</code></td>
<td>
<p>a vector for the location value</p>
</td></tr>
<tr><td><code id="studentt_+3A_sigma">sigma</code></td>
<td>
<p>a vector of scale values</p>
</td></tr>
<tr><td><code id="studentt_+3A_p">p</code></td>
<td>
<p>Vector of probabilities.</p>
</td></tr>
<tr><td><code id="studentt_+3A_n">n</code></td>
<td>
<p>Number of samples to draw from the distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple hack taken from Wikipedia. It's an itch I've been wanting to scratch for a while. I can probably
generalize this outward to allow the tail and log stuff, but I wrote this mostly for the random number generation. Right now,
I haven't written this to account for the fact that sigma should be non-negative, but that's on the user to know that (for now).
</p>


<h3>Value</h3>

<p><code>dst()</code> returns the density. <code>pst()</code> returns the distribution function. <code>qst()</code> returns the quantile function.
<code>rst()</code> returns random numbers.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+TDist">TDist</a></code>
</p>

<hr>
<h2 id='tbl_df'>Convert data frame to an object of class &quot;tibble&quot;</h2><span id='topic+tbl_df'></span><span id='topic+to_tbl'></span>

<h3>Description</h3>

<p><code>tbl_df()</code> ensures legacy compatibility with some of my scripts since the function is deprecated in <span class="pkg">dplyr</span>.
<code>to_tbl()</code> also added for fun.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tbl_df(...)

to_tbl(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tbl_df_+3A_...">...</code></td>
<td>
<p>optional parameters, but don't put anything here. It's just there to quell CRAN checks.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function takes a data frame and turns it into a tibble.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tbl_df(mtcars)
tbl_df(iris)

</code></pre>

<hr>
<h2 id='theme_steve_web'>Legacy functions for Steve's Preferred <span class="pkg">ggplot2</span> Themes and Assorted Stuff</h2><span id='topic+theme_steve_web'></span><span id='topic+post_bg'></span><span id='topic+theme_steve_ms'></span><span id='topic+theme_steve_font'></span>

<h3>Description</h3>

<p><code>theme_steve()</code>, now in <span class="pkg">stevethemes</span>, was a preferred
theme of mine a few years ago. It was basically <code>theme_bw()</code> from
<span class="pkg">ggplot2</span> theme, but with me tweaking a few things. I then moved to
<code>theme_steve_web()</code> for most things now, prominently on my website.
This theme incorporates the &quot;Open Sans&quot; and &quot;Titillium Web&quot;
fonts that I like so much. <code>post_bg()</code> is a legacy function for changing the
backgrounds on plots to better match what was the background color on my website.
<code>theme_steve_ms()</code> is for <code>LaTeX</code> manuscripts that use the
<code>cochineal</code> font package. <code>theme_steve_font()</code> is for any purpose,
allowing you to supply your own font.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theme_steve_web(...)

post_bg(...)

theme_steve_ms(axis_face = "italic", caption_face = "italic", ...)

theme_steve_font(axis_face = "italic", caption_face = "italic", font, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="theme_steve_web_+3A_...">...</code></td>
<td>
<p>optional stuff, but don't put anything in here. You won't need it.</p>
</td></tr>
<tr><td><code id="theme_steve_web_+3A_axis_face">axis_face</code></td>
<td>
<p>font face (&quot;plain&quot;, &quot;italic&quot;, &quot;bold&quot;, &quot;bold.italic&quot;). Optional, defaults to &quot;italic&quot;. Applicable only to <code>theme_steve_ms()</code>.</p>
</td></tr>
<tr><td><code id="theme_steve_web_+3A_caption_face">caption_face</code></td>
<td>
<p>font face (&quot;plain&quot;, &quot;italic&quot;, &quot;bold&quot;, &quot;bold.italic&quot;). Optional, defaults to &quot;italic&quot;. Applicable only to <code>theme_steve_ms()</code>.</p>
</td></tr>
<tr><td><code id="theme_steve_web_+3A_font">font</code></td>
<td>
<p>font family for the plot. Applicable only to <code>theme_steve_font()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>theme_steve_web()</code> and <code>theme_steve_ms()</code> both explicitly
depend on having the fonts installed on your end. It's ultimately optional
for you to have them but the use of these functions imply them. All functions
that remain here should be understood as &quot;legacy&quot; functions that will no longer
be maintained or updated. The <span class="pkg">stevethemes</span> package will have all my
<span class="pkg">ggplot2</span> elements going forward.
</p>


<h3>Value</h3>

<p><code>post_bg()</code> takes a <span class="pkg">ggplot2</span> plot and changes the background to have a color of
&quot;#fdfdfd&quot;. <code>theme_steve_web()</code> extends
<code>theme_steve()</code> to add custom fonts, notably &quot;Open Sans&quot; and &quot;Titillium Web&quot;. In all cases, these
functions take a <span class="pkg">ggplot2</span> plot and return another <span class="pkg">ggplot2</span> plot, but with some cosmetic
changes. <code>theme_steve_ms()</code> takes a <span class="pkg">ggplot2</span> plot and overlays &quot;Crimson Pro&quot; fonts, which is
the basis of the <code>cochineal</code> font package in <code>LaTeX</code>. <code>theme_steve_font()</code> takes a <span class="pkg">ggplot2</span> plot and
overlays a font of your choosing.
</p>


<h3>See Also</h3>

<p><a href="ggplot2.html#topic+theme">ggplot2::theme</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(ggplot2)


ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point() + theme_steve_web() +
  labs(title = "A ggplot2 Plot from the Motor Trend Car Road Tests Data",
  subtitle = "Notice the prettier fonts, if you have them.",
  caption = "Data: ?mtcars in {datasets} in base R.")

ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point() + theme_steve_web() +
  post_bg() +
  labs(title = "A ggplot2 Plot from the Motor Trend Car Road Tests Data",
  subtitle = "Notice the slight change in background color",
  caption = "Data: ?mtcars in {datasets} in base R.")

ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point() + theme_steve_ms() +
  labs(title = "A ggplot2 Plot from the Motor Trend Car Road Tests Data",
  subtitle = "Notice the fonts will match the 'cochineal' font package in LaTeX.",
  caption = "Data: ?mtcars in {datasets} in base R.")

ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point() + theme_steve_font(font = "Comic Sans MS") +
  labs(title = "A ggplot2 Plot from the Motor Trend Car Road Tests Data",
  subtitle = "Notice that this will look ridiculous",
  caption = "Data: ?mtcars in {datasets} in base R.")

## End(Not run)
</code></pre>

<hr>
<h2 id='usa_mids'>United States Militarized Interstate Disputes (MIDs)</h2><span id='topic+usa_mids'></span>

<h3>Description</h3>

<p>This is a non-directed dyad-year data set for militarized interstate disputes involving
the United States. I created these to illustrate the <code>sbtscs()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_mids
</code></pre>


<h3>Format</h3>

<p>A data frame with 14586 observations on the following 6 variables.
</p>

<dl>
<dt><code>dyad</code></dt><dd><p>a unique identifier for the dyad</p>
</dd>
<dt><code>ccode1</code></dt><dd><p>the Correlates of War state code for the United States (2)</p>
</dd>
<dt><code>ccode2</code></dt><dd><p>the Correlates of War state code for the other state in the dyad</p>
</dd>
<dt><code>year</code></dt><dd><p>an observation year for the dyad</p>
</dd>
<dt><code>midongoing</code></dt><dd><p>was there an ongoing inter-state dispute in the dyad-year?</p>
</dd>
<dt><code>midonset</code></dt><dd><p>was there a new inter-state dispute onset in the dyad-year</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data were generated some time ago. Rare cases where there were multiple disputes ongoing
in a given dyad-year were first whittled by isolating 1) unique dispute onsets. Thereafter,
the data select the 2) highest fatality, then 3) the highest hostility level, and then 4)
the longer dispute, until 5) just picking whichever one came first. There are no duplicate non-directed dyad-year observations.
</p>


<h3>References</h3>

<p>Gibler, Douglas M., Steven V. Miller, and Erin K. Little. 2016. “An Analysis of the Militarized
Interstate Dispute (MID) Dataset, 1816-2001.” International Studies Quarterly 60(4): 719-730.
</p>

<hr>
<h2 id='wls'>Get Weighted Least Squares of Your OLS Model</h2><span id='topic+wls'></span>

<h3>Description</h3>

<p><code>wls()</code> takes an OLS model and re-estimates it using a weighted least squares
approach. Weighted least squares is often a &quot;textbook&quot; approach to dealing with the presence of
heteroskedastic standard errors, for which the weighted least squares estimates are compared
to the OLS estimates of uncertainty to check for consistency or potential inferential implications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wls(mod)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wls_+3A_mod">mod</code></td>
<td>
<p>a fitted OLS model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <em>should</em> be robust to potential model specification oddities (e.g. polynomials
and fixed effects). It also should perform nicely in the presence of missing data, if and only
if <code>na.action = na.exclude</code> is supplied first to the offending OLS model supplied to the function
for a weighted least squares re-estimation.
</p>


<h3>Value</h3>

<p><code>wls()</code> returns a new model object that is a weighted least squares re-estimation
of the OLS model supplied to it.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M1 &lt;- lm(mpg ~ ., data=mtcars)
M2 &lt;- wls(M1)

summary(M2)
</code></pre>

<hr>
<h2 id='wom'>Generate Week of the Month from a Date</h2><span id='topic+wom'></span>

<h3>Description</h3>

<p><code>wom()</code> is a convenience function I use for constructing
calendars in <span class="pkg">ggplot2</span>. It takes a date and returns, as a numeric
vector, the week of the month for the date given to it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wom(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wom_+3A_x">x</code></td>
<td>
<p>a date</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>wom()</code> assumes Sunday is the start of the week. This can
assuredly be customized later in this function, but right now the assumption
is Sunday is the start of the week (and not Monday, as it might be in
other contexts).
</p>


<h3>Value</h3>

<p><code>wom()</code> is a convenience function I use for constructing
calendars in <span class="pkg">ggplot2</span>. It takes a date and returns, as a numeric
vector, the week of the month for the date given to it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
wom(as.Date("2022-01-01"))

wom(Sys.Date())
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
