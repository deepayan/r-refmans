<!DOCTYPE html><html><head><title>Help for package GGMncv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {GGMncv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bfi'><p>Data: 25 Personality items representing 5 factors</p></a></li>
<li><a href='#boot_eip'><p>Bootstrapped Edge Inclusion 'Probabilities'</p></a></li>
<li><a href='#coef.ggmncv'><p>Regression Coefficients from <code>ggmncv</code> Objects</p></a></li>
<li><a href='#compare_edges'><p>Compare Edges Between Gaussian Graphical Models</p></a></li>
<li><a href='#confirm_edges'><p>Confirm Edges</p></a></li>
<li><a href='#constrained'><p>Precision Matrix with Known Graph</p></a></li>
<li><a href='#desparsify'><p>De-Sparsified Graphical Lasso Estimator</p></a></li>
<li><a href='#gen_net'><p>Simulate a Partial Correlation Matrix</p></a></li>
<li><a href='#get_graph'><p>Extract Graph from <code>ggmncv</code> Objects</p></a></li>
<li><a href='#ggmncv'><p>GGMncv</p></a></li>
<li><a href='#GGMncv-package'><p>GGMncv:  Gaussian Graphical Models with Nonconvex Regularization</p></a></li>
<li><a href='#head.eip'><p>Print the Head of <code>eip</code> Objects</p></a></li>
<li><a href='#inference'><p>Statistical Inference for Regularized Gaussian Graphical Models</p></a></li>
<li><a href='#kl_mvn'><p>Kullback-Leibler Divergence</p></a></li>
<li><a href='#ledoit_wolf'><p>Ledoit and Wolf Shrinkage Estimator</p></a></li>
<li><a href='#nct'><p>Network Comparison Test</p></a></li>
<li><a href='#penalty_derivative'><p>Penalty Derivative</p></a></li>
<li><a href='#penalty_function'><p>Penalty Function</p></a></li>
<li><a href='#plot.eip'><p>Plot Edge Inclusion 'Probabilities'</p></a></li>
<li><a href='#plot.ggmncv'><p>Plot <code>ggmncv</code> Objects</p></a></li>
<li><a href='#plot.graph'><p>Network Plot for <code>select</code> Objects</p></a></li>
<li><a href='#plot.penalty_derivative'><p>Plot <code>penalty_derivative</code> Objects</p></a></li>
<li><a href='#plot.penalty_function'><p>Plot <code>penalty_function</code> Objects</p></a></li>
<li><a href='#predict.ggmncv'><p>Predict method for <code>ggmncv</code> Objects</p></a></li>
<li><a href='#print.eip'><p>Print <code>eip</code> Objects</p></a></li>
<li><a href='#print.ggmncv'><p>Print <code>ggmncv</code> Objects</p></a></li>
<li><a href='#print.nct'><p>Print <code>nct</code> Objects</p></a></li>
<li><a href='#ptsd'><p>Data: Post-Traumatic Stress Disorder</p></a></li>
<li><a href='#Sachs'><p>Data: Sachs Network</p></a></li>
<li><a href='#score_binary'><p>Binary Classification</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gaussian Graphical Models with Nonconvex Regularization</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-12-13</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate Gaussian graphical models with nonconvex penalties &lt;<a href="https://doi.org/10.31234%2Fosf.io%2Fad57p">doi:10.31234/osf.io/ad57p</a>&gt;, 
  including the atan Wang and Zhu (2016) &lt;<a href="https://doi.org/10.1155%2F2016%2F6495417">doi:10.1155/2016/6495417</a>&gt;, 
  seamless L0 Dicker, Huang, and Lin (2013) &lt;<a href="https://doi.org/10.5705%2Fss.2011.074">doi:10.5705/ss.2011.074</a>&gt;,
  exponential Wang, Fan, and Zhu &lt;<a href="https://doi.org/10.1007%2Fs10463-016-0588-3">doi:10.1007/s10463-016-0588-3</a>&gt;, 
  smooth integration of counting and absolute deviation Lv and Fan (2009) &lt;<a href="https://doi.org/10.1214%2F09-AOS683">doi:10.1214/09-AOS683</a>&gt;,
  logarithm Mazumder, Friedman, and Hastie (2011) &lt;<a href="https://doi.org/10.1198%2Fjasa.2011.tm09738">doi:10.1198/jasa.2011.tm09738</a>&gt;,
  Lq, smoothly clipped absolute deviation Fan and Li (2001) &lt;<a href="https://doi.org/10.1198%2F016214501753382273">doi:10.1198/016214501753382273</a>&gt;,
  and minimax concave penalty Zhang (2010) &lt;<a href="https://doi.org/10.1214%2F09-AOS729">doi:10.1214/09-AOS729</a>&gt;. There are also extensions
  for computing variable inclusion probabilities, multiple regression coefficients, and 
  statistical inference &lt;<a href="https://doi.org/10.1214%2F15-EJS1031">doi:10.1214/15-EJS1031</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.4.6), Rdpack (&ge; 0.11-1), reshape, GGally (&ge;
1.4.0), ggplot2 (&ge; 3.3.0), glassoFast (&ge; 1.0), network (&ge;
1.15), numDeriv (&ge; 2016.8-1.1), mathjaxr (&ge; 1.0-1), MASS (&ge;
7.3-51.5), methods, parallel, pbapply, sna (&ge; 2.5), stats,
utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>car, corpcor, corrplot, dplyr, NetworkToolbox,
NetworkComparisonTest, nlshrink, rmarkdown, knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack, mathjaxr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/donaldRwilliams/GGMncv/issues">https://github.com/donaldRwilliams/GGMncv/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-12-14 12:42:28 UTC; bryce</td>
</tr>
<tr>
<td>Author:</td>
<td>Donald Williams [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Donald Williams &lt;drwwilliams@ucdavis.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-12-15 07:40:28 UTC</td>
</tr>
</table>
<hr>
<h2 id='bfi'>Data: 25 Personality items representing 5 factors</h2><span id='topic+bfi'></span>

<h3>Description</h3>

<p>This dataset and the corresponding documentation was taken from the <strong>psych</strong> package. We refer users to that
package for further details (Revelle 2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("bfi")
</code></pre>


<h3>Format</h3>

<p>A data frame with 25 variables and 2800 observations (including missing values)
</p>


<h3>Details</h3>


<ul>
<li> <p><code>A1</code> Am indifferent to the feelings of others. (q_146)
</p>
</li>
<li> <p><code>A2</code> Inquire about others' well-being. (q_1162)
</p>
</li>
<li> <p><code>A3</code> Know how to comfort others. (q_1206)
</p>
</li>
<li> <p><code>A4</code> Love children. (q_1364)
</p>
</li>
<li> <p><code>A5</code> Make people feel at ease. (q_1419)
</p>
</li>
<li> <p><code>C1</code> Am exacting in my work. (q_124)
</p>
</li>
<li> <p><code>C2</code> Continue until everything is perfect. (q_530)
</p>
</li>
<li> <p><code>C3</code> Do things according to a plan. (q_619)
</p>
</li>
<li> <p><code>C4</code> Do things in a half-way manner. (q_626)
</p>
</li>
<li> <p><code>C5</code> Waste my time. (q_1949)
</p>
</li>
<li> <p><code>E1</code> Don't talk a lot. (q_712)
</p>
</li>
<li> <p><code>E2</code> Find it difficult to approach others. (q_901)
</p>
</li>
<li> <p><code>E3</code> Know how to captivate people. (q_1205)
</p>
</li>
<li> <p><code>E4</code> Make friends easily. (q_1410)
</p>
</li>
<li> <p><code>E5</code> Take charge. (q_1768)
</p>
</li>
<li> <p><code>N1</code> Get angry easily. (q_952)
</p>
</li>
<li> <p><code>N2</code> Get irritated easily. (q_974)
</p>
</li>
<li> <p><code>N3</code> Have frequent mood swings. (q_1099)
</p>
</li>
<li> <p><code>N4</code> Often feel blue. (q_1479)
</p>
</li>
<li> <p><code>N5</code> Panic easily. (q_1505)
</p>
</li>
<li> <p><code>o1</code> Am full of ideas. (q_128)
</p>
</li>
<li> <p><code>o2</code> Avoid difficult reading material.(q_316)
</p>
</li>
<li> <p><code>o3</code> Carry the conversation to a higher level. (q_492)
</p>
</li>
<li> <p><code>o4</code> Spend time reflecting on things. (q_1738)
</p>
</li>
<li> <p><code>o5</code> Will not probe deeply into a subject. (q_1964)
</p>
</li>
<li> <p><code>gender</code> Males = 1, Females =2
</p>
</li>
<li> <p><code>education</code> 1 = HS, 2 = finished HS, 3 = some college, 4 = college graduate 5 = graduate degree
</p>
</li></ul>



<h3>References</h3>

<p>Revelle W (2019).
<em>psych: Procedures for Psychological, Psychometric, and Personality Research</em>.
 Northwestern University,  Evanston, Illinois.
R package version 1.9.12, <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a>.
</p>

<hr>
<h2 id='boot_eip'>Bootstrapped Edge Inclusion 'Probabilities'</h2><span id='topic+boot_eip'></span>

<h3>Description</h3>

<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
<p>Compute the number of times each edge was selected
when performing a non-parametric bootstrap
(see Figure 6.7, Hastie et al. 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_eip(Y, method = "pearson", samples = 500, progress = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_eip_+3A_y">Y</code></td>
<td>
<p>A matrix of dimensions <em>n</em> by <em>p</em>.</p>
</td></tr>
<tr><td><code id="boot_eip_+3A_method">method</code></td>
<td>
<p>Character string. Which correlation coefficient (or covariance)
is to be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;,
or &quot;spearman&quot;.</p>
</td></tr>
<tr><td><code id="boot_eip_+3A_samples">samples</code></td>
<td>
<p>Numeric. How many bootstrap samples (defaults to <code>500</code>)?</p>
</td></tr>
<tr><td><code id="boot_eip_+3A_progress">progress</code></td>
<td>
<p>Logical. Should a progress bar be included (defaults to <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="boot_eip_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+ggmncv">ggmncv</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>eip</code> that includes the &quot;probabilities&quot; in a
data frame.
</p>


<h3>Note</h3>

<p>Although Hastie et al. (2009) suggests
this approach provides probabilities, to avoid confusion with Bayesian inference,
these are better thought of as &quot;probabilities&quot; (or better yet proportions).
</p>


<h3>References</h3>

<p>Hastie T, Tibshirani R, Friedman J (2009).
<em>The elements of statistical learning: data mining, inference, and prediction</em>.
Springer Science \&amp; Business Media.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# data (ptsd symptoms)
Y &lt;- GGMncv::ptsd[,1:10]

# compute eip's
boot_samps &lt;- boot_eip(Y, samples  = 100, progress = FALSE)

boot_samps

</code></pre>

<hr>
<h2 id='coef.ggmncv'>Regression Coefficients from <code>ggmncv</code> Objects</h2><span id='topic+coef.ggmncv'></span>

<h3>Description</h3>

<p>There is a direct correspondence between the inverse covariance
matrix and multiple regression (Stephens 1998; Kwan 2014).
This readily allows for converting the off diagonal elements to regression coefficients,
resulting in noncovex penalization for multiple regression modeling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ggmncv'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.ggmncv_+3A_object">object</code></td>
<td>
<p>An Object of class <code>ggmncv</code>.</p>
</td></tr>
<tr><td><code id="coef.ggmncv_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of regression coefficients.
</p>


<h3>Note</h3>

<p>The coefficients can be accessed via <code>coefs[1,]</code>,
which provides the estimates for predicting the first node.
</p>
<p>Further, the estimates are essentially computed with both
the outcome and predictors scaled to have mean 0 and
standard deviation 1.
</p>


<h3>References</h3>

<p>Kwan CC (2014).
&ldquo;A regression-based interpretation of the inverse of the sample covariance matrix.&rdquo;
<em>Spreadsheets in Education</em>, <b>7</b>(1), 4613.<br /><br /> Stephens G (1998).
&ldquo;On the Inverse of the Covariance Matrix in Portfolio Analysis.&rdquo;
<em>The Journal of Finance</em>, <b>53</b>(5), 1821&ndash;1827.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# data
Y &lt;- GGMncv::ptsd[,1:5]

# correlations
S &lt;- cor(Y)

# fit model
fit &lt;- ggmncv(R = S, n = nrow(Y), progress = FALSE)

# regression
coefs &lt;- coef(fit)

coefs


# no regularization, resulting in OLS

# data
# note: scaled for lm()
Y &lt;- scale(GGMncv::ptsd[,1:5])

# correlations
S &lt;- cor(Y)

# fit model
# note: non reg
fit &lt;- ggmncv(R = S, n = nrow(Y), progress = FALSE, lambda = 0)

# regression
coefs &lt;- coef(fit)

# fit lm
fit_lm &lt;- lm(Y[,1] ~ 0 + Y[,-1])

# ggmncv
coefs[1,]

# lm
as.numeric(coef(fit_lm))



</code></pre>

<hr>
<h2 id='compare_edges'>Compare Edges Between Gaussian Graphical Models</h2><span id='topic+compare_edges'></span>

<h3>Description</h3>

<p>Establish whether each of the corresponding edges
are significantly different in two groups,
with the de-sparsified estimator of (Jankova and Van De Geer 2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_edges(object_1, object_2, method = "fdr", alpha = 0.05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_edges_+3A_object_1">object_1</code></td>
<td>
<p>object of class <code><a href="#topic+ggmncv">ggmncv</a></code> .</p>
</td></tr>
<tr><td><code id="compare_edges_+3A_object_2">object_2</code></td>
<td>
<p>An object of class <code><a href="#topic+ggmncv">ggmncv</a></code>.</p>
</td></tr>
<tr><td><code id="compare_edges_+3A_method">method</code></td>
<td>
<p>Character string. A correction method for
multiple comparisons (defaults to <code>fdr</code>), which
can be abbreviated. See <a href="stats.html#topic+p.adjust">p.adjust</a>.</p>
</td></tr>
<tr><td><code id="compare_edges_+3A_alpha">alpha</code></td>
<td>
<p>Numeric. Significance level (defaults to <code>0.05</code>).</p>
</td></tr>
<tr><td><code id="compare_edges_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code>P_diff</code> De-sparsified partial correlation differences
</p>
</li>
<li> <p><code>adj</code> Adjacency matrix based on the p-values.
</p>
</li>
<li> <p><code>pval_uncorrected</code> Uncorrected p-values
</p>
</li>
<li> <p><code>pval_corrected</code> Corrected p-values
</p>
</li>
<li> <p><code>method</code> The approach used for multiple comparisons
</p>
</li>
<li> <p><code>alpha</code> Significance level
</p>
</li></ul>



<h3>Note</h3>

<p>For low-dimensional settings, i.e., when the number of observations
far exceeds the number of nodes, this function likely has limited utility and
a non regularized approach should be used for comparing edges
(see for example <strong>GGMnonreg</strong>).
</p>
<p>Further, whether the de-sparsified estimator provides nominal error rates
remains to be seen, at least across a range of conditions. For example,
the simulation results in Williams (2021)
demonstrated that the confidence intervals
can have (severely) compromised coverage properties (whereas non-regularized methods
had coverage at the nominal level).
</p>


<h3>References</h3>

<p>Jankova J, Van De Geer S (2015).
&ldquo;Confidence intervals for high-dimensional inverse covariance estimation.&rdquo;
<em>Electronic Journal of Statistics</em>, <b>9</b>(1), 1205&ndash;1229.<br /><br /> Williams DR (2021).
&ldquo;The Confidence Interval that Wasn't: Bootstrapped &quot;Confidence Intervals&quot; in L1-Regularized Partial Correlation Networks.&rdquo;
<em>PsyArXiv</em>.
doi: <a href="https://doi.org/10.31234/osf.io/kjh2f">10.31234/osf.io/kjh2f</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data
# note: all edges equal
Y1 &lt;- MASS::mvrnorm(250, rep(0, 10), Sigma = diag(10))
Y2 &lt;- MASS::mvrnorm(250, rep(0, 10), Sigma = diag(10))

# fit models
# note: atan penalty by default

# group 1
fit1 &lt;- ggmncv(cor(Y1), n = nrow(Y1),
               progress = FALSE)

# group 2
fit2 &lt;- ggmncv(cor(Y2), n = nrow(Y2),
               progress = FALSE)

# compare
compare_ggms &lt;- compare_edges(fit1, fit2)

compare_ggms
</code></pre>

<hr>
<h2 id='confirm_edges'>Confirm Edges</h2><span id='topic+confirm_edges'></span>

<h3>Description</h3>

<p>Confirmatory hypothesis testing of edges that were initially
detected with data-driven model selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confirm_edges(object, Rnew, method, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confirm_edges_+3A_object">object</code></td>
<td>
<p>An object of class <code>ggmncv</code></p>
</td></tr>
<tr><td><code id="confirm_edges_+3A_rnew">Rnew</code></td>
<td>
<p>Matrix. A correlation matrix of dimensions <em>p</em> by <em>p</em>.</p>
</td></tr>
<tr><td><code id="confirm_edges_+3A_method">method</code></td>
<td>
<p>Character string. A correction method for multiple comparison
(defaults to <code>fdr</code>). Can be abbreviated. See <a href="stats.html#topic+p.adjust">p.adjust</a>.</p>
</td></tr>
<tr><td><code id="confirm_edges_+3A_alpha">alpha</code></td>
<td>
<p>Numeric. Significance level (defaults to <code>0.05</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic idea is to merge exploration with confirmation
(see for example,  Rodriguez et al. 2020).
This is accomplished by testing those edges (in an independent dataset)
that were initially detected via data driven model selection.
</p>
<p>Confirmatory hypothesis testing follows the approach described in
Jankova and Van De Geer (2015): (1)
graphical lasso is computed with lambda fixed to  \(\lambda = \sqrt{log(p)/n}\),
(2) the de-sparsified estimator is computed, and then (3) <em>p</em>-values are
obtained for the de-sparsified estimator.
</p>


<h3>Value</h3>

<p>An object of class <code>ggmncv</code>, including:
</p>

<ul>
<li><p><strong>P</strong>: Matrix of confirmed edges (partial correlations)
</p>
</li>
<li><p><strong>adj</strong>: Matrix of confirmed edges (adjacency)
</p>
</li></ul>



<h3>References</h3>

<p>Jankova J, Van De Geer S (2015).
&ldquo;Confidence intervals for high-dimensional inverse covariance estimation.&rdquo;
<em>Electronic Journal of Statistics</em>, <b>9</b>(1), 1205&ndash;1229.<br /><br /> Rodriguez JE, Williams DR, Rast P, Mulder J (2020).
&ldquo;On Formalizing Theoretical Expectations: Bayesian Testing of Central Structures in Psychological Networks.&rdquo;
<em>PsyArXiv</em>.
doi: <a href="https://doi.org/10.31234/osf.io/zw7pf">10.31234/osf.io/zw7pf</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Y &lt;- na.omit(bfi[,1:25])

Y_explore &lt;- Y[1:1000,]

Y_confirm &lt;- Y[1001:nrow(Y),]

fit &lt;- ggmncv(cor(Y_explore),
              n = nrow(Y_explore),
              progress = FALSE)

confirm &lt;- confirm_edges(fit,
                         Rnew = cor(Y_confirm),
                         method = "fdr",
                         alpha = 0.05)
</code></pre>

<hr>
<h2 id='constrained'>Precision Matrix with Known Graph</h2><span id='topic+constrained'></span><span id='topic+mle_known_graph'></span>

<h3>Description</h3>

<p>Compute the maximum likelihood estimate of the precision matrix,
given a known graphical structure (i.e., an adjacency matrix).
This approach was originally described in &quot;The Elements of Statistical Learning&quot;
(see pg. 631, Hastie et al. 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constrained(Sigma, adj)

mle_known_graph(Sigma, adj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="constrained_+3A_sigma">Sigma</code></td>
<td>
<p>Covariance matrix</p>
</td></tr>
<tr><td><code id="constrained_+3A_adj">adj</code></td>
<td>
<p>Adjacency matrix that encodes the constraints,
where a zero indicates that element should be zero.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following:
</p>

<ul>
<li><p><strong>Theta</strong>: Inverse of the covariance matrix (precision matrix)
</p>
</li>
<li><p><strong>Sigma</strong>: Covariance matrix.
</p>
</li>
<li><p><strong>wadj</strong>: Weighted adjacency matrix, corresponding
to the partial correlation network.
</p>
</li></ul>



<h3>Note</h3>

<p>The algorithm is written in <code>c++</code>, and should scale to high dimensions
nicely.
</p>
<p>Note there are a variety of algorithms for this purpose. Simulation
studies indicated that this approach is both accurate and computationally
efficient (HFT therein, Emmert-Streib et al. 2019)
</p>


<h3>References</h3>

<p>Emmert-Streib F, Tripathi S, Dehmer M (2019).
&ldquo;Constrained covariance matrices with a biologically realistic structure: Comparison of methods for generating high-dimensional Gaussian graphical models.&rdquo;
<em>Frontiers in Applied Mathematics and Statistics</em>, <b>5</b>, 17.<br /><br /> Hastie T, Tibshirani R, Friedman J (2009).
<em>The elements of statistical learning: data mining, inference, and prediction</em>.
Springer Science \&amp; Business Media.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# data
y &lt;- ptsd

# fit model
fit &lt;- ggmncv(cor(y), n = nrow(y),
              penalty = "lasso",
              progress = FALSE)

# set negatives to zero (sign restriction)
adj_new &lt;- ifelse( fit$P &lt;= 0, 0, 1)

check_zeros &lt;- TRUE

# track trys
iter &lt;- 0

# iterate until all positive
while(check_zeros){
  iter &lt;- iter + 1
  fit_new &lt;- constrained(cor(y), adj = adj_new)
  check_zeros &lt;- any(fit_new$wadj &lt; 0)
  adj_new &lt;- ifelse( fit_new$wadj &lt;= 0, 0, 1)
}



# alias

# data
y &lt;- ptsd

# nonreg (lambda = 0)
fit &lt;- ggmncv(cor(y), n = nrow(y),
              lambda = 0,
              progress = FALSE)

# set values less than |0.1| to zero
adj_new &lt;- ifelse( abs(fit$P) &lt;= 0.1, 0, 1)

# mle given the graph
mle_known_graph(cor(y), adj_new)

</code></pre>

<hr>
<h2 id='desparsify'>De-Sparsified Graphical Lasso Estimator</h2><span id='topic+desparsify'></span>

<h3>Description</h3>

<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
<p>Compute the de-sparsified (sometimes called &quot;de-biased&quot;) glasso estimator with
the approach described in Equation 7 of Jankova and Van De Geer (2015).
The basic idea is to <em>undo</em> \(L_1\)-regularization, in order
to compute <em>p</em>-values and confidence intervals
(i.e., to make statistical inference).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>desparsify(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="desparsify_+3A_object">object</code></td>
<td>
<p>An object of class <code>ggmncv</code>.</p>
</td></tr>
<tr><td><code id="desparsify_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>According to Jankova and Van De Geer (2015), the de-sparisifed estimator,
\(\hat{\mathrm{\bf T}}\), is defined as
</p>
\(\hat{\mathrm{\bf T}} = 2\hat{\boldsymbol{\Theta}} - \hat{\boldsymbol{\Theta}}\hat{\mathrm{\bf R}}\hat{\boldsymbol{\Theta}},\)
<p>where \(\hat{\boldsymbol{\Theta}}\) denotes the graphical lasso estimator of the precision matrix
and \(\hat{\mathrm{\bf R}}\) is the sample correlation matrix. Further details can be
found in Section 2 (&quot;Main Results&quot;) of Jankova and Van De Geer (2015).
</p>
<p>This approach is built upon earlier work on the de-sparsified lasso estimator
(Javanmard and Montanari 2014; Van de Geer et al. 2014; Zhang and Zhang 2014)
</p>


<h3>Value</h3>

<p>The de-sparsified estimates, including
</p>

<ul>
<li> <p><code>Theta</code>:  De-sparsified precision matrix
</p>
</li>
<li> <p><code>P</code>:  De-sparsified partial correlation matrix
</p>
</li></ul>



<h3>Note</h3>

<p>This assumes (reasonably) Gaussian data, and should not to be expected
to work for, say, polychoric correlations. Further, all work to date
has only looked at the graphical lasso estimator, and not de-sparsifying
nonconvex regularization. Accordingly, it is probably best to set
<code>penalty = "lasso"</code> in <code><a href="#topic+ggmncv">ggmncv</a></code>.
</p>
<p>This function only provides the de-sparsified estimator and
not <em>p</em>-values or confidence intervals (see <code><a href="#topic+inference">inference</a></code>).
</p>


<h3>References</h3>

<p>Jankova J, Van De Geer S (2015).
&ldquo;Confidence intervals for high-dimensional inverse covariance estimation.&rdquo;
<em>Electronic Journal of Statistics</em>, <b>9</b>(1), 1205&ndash;1229.<br /><br /> Javanmard A, Montanari A (2014).
&ldquo;Confidence intervals and hypothesis testing for high-dimensional regression.&rdquo;
<em>The Journal of Machine Learning Research</em>, <b>15</b>(1), 2869&ndash;2909.<br /><br /> Van de Geer S, BÃ¼hlmann P, Ritov Y, Dezeure R (2014).
&ldquo;On asymptotically optimal confidence regions and tests for high-dimensional models.&rdquo;
<em>The Annals of Statistics</em>, <b>42</b>(3), 1166&ndash;1202.<br /><br /> Zhang C, Zhang SS (2014).
&ldquo;Confidence intervals for low dimensional parameters in high dimensional linear models.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>76</b>(1), 217&ndash;242.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data
Y &lt;- GGMncv::Sachs[,1:5]

n &lt;- nrow(Y)
p &lt;- ncol(Y)

# fit model
# note: fix lambda, as in the reference
fit &lt;- ggmncv(cor(Y), n = nrow(Y),
              progress = FALSE,
              penalty = "lasso",
              lambda = sqrt(log(p)/n))

# fit model
# note: no regularization
fit_non_reg &lt;- ggmncv(cor(Y), n = nrow(Y),
                      progress = FALSE,
                      penalty = "lasso",
                      lambda = 0)


# remove (some) bias and sparsity
That &lt;- desparsify(fit)

# graphical lasso estimator
fit$P

# de-sparsified estimator
That$P

# mle
fit_non_reg$P
</code></pre>

<hr>
<h2 id='gen_net'>Simulate a Partial Correlation Matrix</h2><span id='topic+gen_net'></span>

<h3>Description</h3>

<p>Simulate a Partial Correlation Matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_net(p = 20, edge_prob = 0.3, lb = 0.05, ub = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_net_+3A_p">p</code></td>
<td>
<p>number of variables (nodes)</p>
</td></tr>
<tr><td><code id="gen_net_+3A_edge_prob">edge_prob</code></td>
<td>
<p>connectivity</p>
</td></tr>
<tr><td><code id="gen_net_+3A_lb">lb</code></td>
<td>
<p>lower bound for the partial correlations</p>
</td></tr>
<tr><td><code id="gen_net_+3A_ub">ub</code></td>
<td>
<p>upper bound for the partial correlations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following:
</p>

<ul>
<li><p><strong>pcor</strong>: Partial correlation matrix, encoding
the conditional (in)dependence structure.
</p>
</li>
<li><p><strong>cors</strong>: Correlation matrix.
</p>
</li>
<li><p><strong>adj</strong>: Adjacency matrix.
</p>
</li>
<li><p><strong>trys</strong>: Number of attempts to obtain a
positive definite matrix.
</p>
</li></ul>



<h3>Note</h3>

<p>The function checks for a valid matrix (positive definite),
but sometimes this will still fail. For example, for
larger <code>p</code>, to have large partial correlations this
requires a sparse GGM
(accomplished by setting <code>edge_prob</code>
to a small value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- 20
n &lt;- 500

true_net &lt;- gen_net(p = p, edge_prob = 0.25)

y &lt;- MASS::mvrnorm(n = n,
                   mu = rep(0, p),
                   Sigma = true_net$cors)

# default
fit_atan &lt;- ggmncv(R = cor(y),
                   n = nrow(y),
                   penalty = "atan",
                   progress = FALSE)

# lasso
fit_l1 &lt;- ggmncv(R = cor(y),
                 n = nrow(y),
                 penalty = "lasso",
                 progress = FALSE)

# atan
score_binary(estimate = true_net$adj,
             true = fit_atan$adj,
             model_name = "atan")

# lasso
score_binary(estimate = fit_l1$adj,
             true = true_net$adj,
             model_name = "lasso")

</code></pre>

<hr>
<h2 id='get_graph'>Extract Graph from <code>ggmncv</code> Objects</h2><span id='topic+get_graph'></span>

<h3>Description</h3>

<p>The fitted model from  <code><a href="#topic+ggmncv">ggmncv</a></code> contains a lot
of information, most of which is not immediately useful for most use
cases. This function extracts the weighted adjacency
(partial correlation network) and adjacency matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_graph(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_graph_+3A_x">x</code></td>
<td>
<p>An object of class <code>ggmncv</code>.</p>
</td></tr>
<tr><td><code id="get_graph_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code>P</code>: Weighted adjacency matrix (partial correlation network)
</p>
</li>
<li> <p><code>adj</code>: Adjacency matrix
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>Y &lt;- na.omit(bfi[,1:5])

fit &lt;- ggmncv(cor(Y),
              n = nrow(Y),
              progress = FALSE)

get_graph(fit)
</code></pre>

<hr>
<h2 id='ggmncv'>GGMncv</h2><span id='topic+ggmncv'></span>

<h3>Description</h3>

<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
<p>Gaussian graphical modeling with nonconvex regularization. A thorough survey
of these penalties, including simulation studies investigating their properties,
is provided in Williams (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggmncv(
  R,
  n,
  penalty = "atan",
  ic = "bic",
  select = "lambda",
  gamma = NULL,
  lambda = NULL,
  n_lambda = 50,
  lambda_min_ratio = 0.01,
  n_gamma = 50,
  initial = NULL,
  LLA = FALSE,
  unreg = FALSE,
  maxit = 10000,
  thr = 1e-04,
  store = TRUE,
  progress = TRUE,
  ebic_gamma = 0.5,
  penalize_diagonal = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggmncv_+3A_r">R</code></td>
<td>
<p>Matrix. A correlation matrix of dimensions <em>p</em> by <em>p</em>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_n">n</code></td>
<td>
<p>Numeric. The sample size used to compute the information criterion.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_penalty">penalty</code></td>
<td>
<p>Character string. Which penalty should be used (defaults to <code>"atan"</code>)?</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_ic">ic</code></td>
<td>
<p>Character string. Which information criterion should be used (defaults to <code>"bic"</code>)?
The options include <code>aic</code>, <code>ebic</code> (ebic_gamma defaults to <code>0.5</code>),
<code>ric</code>, or any of the generalized information criteria provided in section 5 of
Kim et al. (2012). The options are <code>gic_1</code>
(i.e., <code>bic</code>) to <code>gic_6</code> (see '<code>Details</code>').</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_select">select</code></td>
<td>
<p>Character string. Which tuning parameter should be selected
(defaults to <code>"lambda"</code>)? The options include <code>"lambda"</code>
(the regularization parameter), <code>"gamma"</code> (governs the 'shape'),
and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_gamma">gamma</code></td>
<td>
<p>Numeric. Hyperparameter for the penalty function.
Defaults to 3.7 (<code>scad</code>), 2 (<code>mcp</code>), 0.5 (<code>adapt</code>),
and 0.01 with all other penalties. Note care must be taken when
departing from the default values
(see the references in '<code>note</code>')</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_lambda">lambda</code></td>
<td>
<p>Numeric vector. Regularization (or tuning) parameters.
The defaults is <code>NULL</code> that provides default
values with  <code>select = "lambda"</code> and <code>sqrt(log(p)/n)</code> with
<code>select = "gamma"</code>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_n_lambda">n_lambda</code></td>
<td>
<p>Numeric. The number of \(\lambda\)'s to be evaluated. Defaults to 50.
This is disregarded if custom values are provided for <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>Numeric. The smallest value for <code>lambda</code>, as a
fraction of the upperbound of the
regularization/tuning parameter. The default is
<code>0.01</code>, which mimics the <code>R</code> package
<strong>qgraph</strong>. To mimic the <code>R</code> package
<strong>huge</strong>, set <code>lambda_min_ratio = 0.1</code>
and <code>n_lambda = 10</code>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_n_gamma">n_gamma</code></td>
<td>
<p>Numeric. The number of \(\gamma\)'s to be evaluated. Defaults to 50.
This is disregarded if custom values are provided in <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_initial">initial</code></td>
<td>
<p>A matrix (<em>p</em> by <em>p</em>) or custom function that returns
the inverse of the covariance matrix . This is used to compute
the penalty derivative. The default is <code>NULL</code>, which results
in using the inverse of <code>R</code> (see '<code>Note</code>').</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_lla">LLA</code></td>
<td>
<p>Logical. Should the local linear approximation be used (default to <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_unreg">unreg</code></td>
<td>
<p>Logical. Should the models be refitted (or unregularized) with maximum likelihood
(defaults to <code>FALSE</code>)? Setting to <code>TRUE</code> results in the approach of
Foygel and Drton (2010), but with the regularization path obtained from
nonconvex regularization, as opposed to the \(\ell_1\)-penalty.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_maxit">maxit</code></td>
<td>
<p>Numeric. The maximum number of iterations for determining convergence of the LLA
algorithm (defaults to <code>1e4</code>). Note this can be changed to, say,
<code>2</code> or <code>3</code>, which will provide  two and three-step estimators
without convergence check.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_thr">thr</code></td>
<td>
<p>Numeric. Threshold for determining convergence of the LLA algorithm
(defaults to <code>1.0e-4</code>).</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_store">store</code></td>
<td>
<p>Logical. Should all of the fitted models be saved (defaults to <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_progress">progress</code></td>
<td>
<p>Logical. Should a progress bar be included (defaults to <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_ebic_gamma">ebic_gamma</code></td>
<td>
<p>Numeric. Value for the additional hyper-parameter for the
extended Bayesian information criterion (defaults to 0.5,
must be between 0 and 1). Setting <code>ebic_gamma = 0</code> results
in BIC.</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_penalize_diagonal">penalize_diagonal</code></td>
<td>
<p>Logical. Should the diagonal of the inverse covariance
matrix be penalized (defaults to <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="ggmncv_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>initial</code> when a
function is provided and ignored otherwise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several of the penalties are (continuous) approximations to the
\(\ell_0\) penalty, that is, best subset selection. However, the solution
does not require enumerating all possible models which results in a computationally
efficient solution.
</p>
<p><strong>L0 Approximations</strong>
</p>

<ul>
<li><p> Atan: <code>penalty = "atan"</code> (Wang and Zhu 2016).
This is currently the default.
</p>
</li>
<li><p> Seamless \(\ell_0\): <code>penalty = "selo"</code> (Dicker et al. 2013).
</p>
</li>
<li><p> Exponential: <code>penalty = "exp"</code>  (Wang et al. 2018)
</p>
</li>
<li><p> Log: <code>penalty = "log"</code> (Mazumder et al. 2011).
</p>
</li>
<li><p> Sica: <code>penalty = "sica"</code>  (Lv and Fan 2009)
</p>
</li></ul>

<p><strong>Additional penalties</strong>:
</p>

<ul>
<li><p> SCAD: <code>penalty = "scad"</code>  (Fan and Li 2001).
</p>
</li>
<li><p> MCP: <code>penalty = "mcp"</code> (Zhang 2010).
</p>
</li>
<li><p> Adaptive lasso (<code>penalty = "adapt"</code>): Defaults to  \(\gamma = 0.5\)
(Zou 2006). Note that for consistency with the
other penalties, \(\gamma \rightarrow 0\) provides more penalization and
\(\gamma = 1\) results in \(\ell_1\) regularization.
</p>
</li>
<li><p> Lasso:  <code>penalty = "lasso"</code>  (Tibshirani 1996).
</p>
</li></ul>

<p><strong>gamma</strong> (\(\gamma\)):
</p>
<p>The <code>gamma</code> argument corresponds to additional hyperparameter for each penalty.
The defaults are set to the recommended values from the respective papers.
</p>
<p><strong>LLA</strong>
</p>
<p>The local linear approximate is noncovex penalties was described in
(Fan et al. 2009). This is essentially an iteratively
re-weighted (g)lasso. Note that by default <code>LLA = FALSE</code>. This is due to
the work of Zou and Li (2008), which suggested that,
so long as the starting values are good enough, then a one-step estimator is
sufficient to obtain an accurate estimate of the conditional dependence structure.
In the case of low-dimensional data, the sample based inverse
covariance matrix is used for the starting values. This is expected to work well,
assuming that \(n\) is sufficiently larger than  \(p\).
</p>
<p><strong>Generalized Information Criteria</strong>
</p>
<p>The following are the available GIC:
</p>

<ul>
<li> \(\textrm{GIC}_1:  |\textbf{E}| \cdot \textrm{log}(n)\)
<p>(<code>ic = "gic_1"</code>  or <code>ic = "bic"</code>)
</p>
</li>
<li>  \(\textrm{GIC}_2: |\textbf{E}| \cdot p^{1/3}\)
<p>(<code>ic = "gic_2"</code>)
</p>
</li>
<li>  \(\textrm{GIC}_3:  |\textbf{E}| \cdot 2 \cdot \textrm{log}(p)\)
<p>(<code>ic = "gic_3"</code> or <code>ic = "ric"</code>)
</p>
</li>
<li> \(\textrm{GIC}_4: |\textbf{E}| \cdot 2 \cdot \textrm{log}(p) +
      \textrm{log}\big(\textrm{log}(p)\big)\)
<p>(<code>ic = "gic_4"</code>)
</p>
</li>
<li> \(\textrm{GIC}_5: |\textbf{E}| \cdot \textrm{log}(p) +
       \textrm{log}\big(\textrm{log}(n)\big) \cdot \textrm{log}(p)\)
<p>(<code>ic = "gic_5"</code>)
</p>
</li>
<li> \(\textrm{GIC}_6: |\textbf{E}| \cdot \textrm{log}(n)
       \cdot \textrm{log}(p)\)
<p>(<code>ic = "gic_6"</code>)
</p>
</li></ul>

<p>Note that \(|\textbf{E}|\) denotes the number of edges (nonzero relations)
in the graph, \(p\) the number of nodes (columns), and
\(n\) the number of observations (rows).
Further each can be understood as a penalty term added to
negative 2 times the log-likelihood, that is,
</p>
\(-2 l_n(\hat{\boldsymbol{\Theta}}) = -2 \Big[\frac{n}{2} \textrm{log} \textrm{det}
\hat{\boldsymbol{\Theta}} - \textrm{tr}(\hat{\textbf{S}}\hat{\boldsymbol{\Theta}})\Big]\)
<p>where \(\hat{\boldsymbol{\Theta}}\) is the estimated precision matrix
(e.g., for a given \(\lambda\) and \(\gamma\))
and \(\hat{\textbf{S}}\) is the sample-based covariance matrix.
</p>


<h3>Value</h3>

<p>An object of class <code>ggmncv</code>, including:
</p>

<ul>
<li> <p><code>Theta</code> Inverse covariance matrix
</p>
</li>
<li> <p><code>Sigma</code> Covariance matrix
</p>
</li>
<li> <p><code>P</code> Weighted adjacency matrix
</p>
</li>
<li> <p><code>adj</code> Adjacency matrix
</p>
</li>
<li> <p><code>lambda</code> Tuning parameter(s)
</p>
</li>
<li> <p><code>fit</code> glasso fitted model (a list)
</p>
</li></ul>



<h3>Note</h3>

<p><strong>initial</strong>
</p>
<p><code>initial</code> not only affects performance (to some degree) but also
computational speed. In high dimensions (defined here as <em>p</em> &gt; <em>n</em>),
or when <em>p</em> approaches <em>n</em>, the precision matrix can become quite unstable.
As a result, with <code>initial = NULL</code>, the algorithm can take a very (very) long time.
If this occurs, provide a matrix for <code>initial</code> (e.g., using <code>lw</code>).
Alternatively, the penalty can be changed to <code>penalty = "lasso"</code>, if desired.
</p>
<p>The <code>R</code> package <strong>glassoFast</strong> is under the hood of <code>ggmncv</code>
(Sustik and Calderhead 2012), which is much faster than
<strong>glasso</strong> when there are many nodes.
</p>


<h3>References</h3>

<p>Dicker L, Huang B, Lin X (2013).
&ldquo;Variable selection and estimation with the seamless-L 0 penalty.&rdquo;
<em>Statistica Sinica</em>, 929&ndash;962.<br /><br /> Fan J, Feng Y, Wu Y (2009).
&ldquo;Network exploration via the adaptive LASSO and SCAD penalties.&rdquo;
<em>The annals of applied statistics</em>, <b>3</b>(2), 521.<br /><br /> Fan J, Li R (2001).
&ldquo;Variable selection via nonconcave penalized likelihood and its oracle properties.&rdquo;
<em>Journal of the American statistical Association</em>, <b>96</b>(456), 1348&ndash;1360.<br /><br /> Foygel R, Drton M (2010).
&ldquo;Extended Bayesian Information Criteria for Gaussian Graphical Models.&rdquo;
<em>Advances in Neural Information Processing Systems</em>, 604&ndash;612.
1011.6640.<br /><br /> Kim Y, Kwon S, Choi H (2012).
&ldquo;Consistent model selection criteria on high dimensions.&rdquo;
<em>The Journal of Machine Learning Research</em>, <b>13</b>, 1037&ndash;1057.<br /><br /> Lv J, Fan Y (2009).
&ldquo;A unified approach to model selection and sparse recovery using regularized least squares.&rdquo;
<em>The Annals of Statistics</em>, <b>37</b>(6A), 3498&ndash;3528.<br /><br /> Mazumder R, Friedman JH, Hastie T (2011).
&ldquo;Sparsenet: Coordinate descent with nonconvex penalties.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>106</b>(495), 1125&ndash;1138.<br /><br /> Sustik MA, Calderhead B (2012).
&ldquo;GLASSOFAST: An efficient GLASSO implementation.&rdquo;
<em>UTCS Technical Report TR-12-29 2012</em>.<br /><br /> Tibshirani R (1996).
&ldquo;Regression shrinkage and selection via the lasso.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <b>58</b>(1), 267&ndash;288.<br /><br /> Wang Y, Fan Q, Zhu L (2018).
&ldquo;Variable selection and estimation using a continuous approximation to the L0 penalty.&rdquo;
<em>Annals of the Institute of Statistical Mathematics</em>, <b>70</b>(1), 191&ndash;214.<br /><br /> Wang Y, Zhu L (2016).
&ldquo;Variable selection and parameter estimation with the Atan regularization method.&rdquo;
<em>Journal of Probability and Statistics</em>.<br /><br /> Williams DR (2020).
&ldquo;Beyond Lasso: A Survey of Nonconvex Regularization in Gaussian Graphical Models.&rdquo;
<em>PsyArXiv</em>.<br /><br /> Zhang C (2010).
&ldquo;Nearly unbiased variable selection under minimax concave penalty.&rdquo;
<em>The Annals of statistics</em>, <b>38</b>(2), 894&ndash;942.<br /><br /> Zou H (2006).
&ldquo;The adaptive lasso and its oracle properties.&rdquo;
<em>Journal of the American statistical association</em>, <b>101</b>(476), 1418&ndash;1429.<br /><br /> Zou H, Li R (2008).
&ldquo;One-step sparse estimates in nonconcave penalized likelihood models.&rdquo;
<em>Annals of statistics</em>, <b>36</b>(4), 1509.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# data
Y &lt;- GGMncv::ptsd

S &lt;- cor(Y)

# fit model
# note: atan default
fit_atan &lt;- ggmncv(S, n = nrow(Y),
                   progress = FALSE)

# plot
plot(get_graph(fit_atan),
     edge_magnify = 10,
     node_names = colnames(Y))

# lasso
fit_l1 &lt;- ggmncv(S, n = nrow(Y),
                 progress = FALSE,
                 penalty = "lasso")

# plot
plot(get_graph(fit_l1),
     edge_magnify = 10,
     node_names = colnames(Y))


# for these data, we might expect all relations to be positive
# and thus the red edges are spurious. The following re-estimates
# the graph, given all edges positive (sign restriction).

# set negatives to zero (sign restriction)
adj_new &lt;- ifelse( fit_atan$P &lt;= 0, 0, 1)

check_zeros &lt;- TRUE

# track trys
iter &lt;- 0

# iterate until all positive
while(check_zeros){
  iter &lt;- iter + 1
  fit_new &lt;- constrained(S, adj = adj_new)
  check_zeros &lt;- any(fit_new$wadj &lt; 0)
  adj_new &lt;- ifelse( fit_new$wadj &lt;= 0, 0, 1)
}

# make graph object
new_graph &lt;- list(P = fit_new$wadj,
                  adj = adj_new)
class(new_graph) &lt;- "graph"

plot(new_graph,
     edge_magnify = 10,
     node_names = colnames(Y))


</code></pre>

<hr>
<h2 id='GGMncv-package'>GGMncv:  Gaussian Graphical Models with Nonconvex Regularization</h2><span id='topic+GGMncv-package'></span>

<h3>Description</h3>

<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
<p>The primary goal of GGMncv is to provide non-convex penalties for estimating
Gaussian graphical models. These are known to overcome the various limitations
of lasso (least absolute shrinkage &quot;screening&quot; operator),
including inconsistent model selection (Zhao and Yu 2006),
biased estimates (Zhang 2010), and a high
false positive rate
(see for example Williams and Rast 2020;Williams et al. 2019)
</p>
<p>Several of the penalties are (continuous) approximations to the
\(\ell_0\) penalty, that is, best subset selection. However, the solution
does not require enumerating all possible models which results in a computationally
efficient solution.
</p>
<p><strong>L0 Approximations</strong>
</p>

<ul>
<li><p> Atan: <code>penalty = "atan"</code> (Wang and Zhu 2016).
This is currently the default.
</p>
</li>
<li><p> Seamless \(\ell_0\): <code>penalty = "selo"</code> (Dicker et al. 2013).
</p>
</li>
<li><p> Exponential: <code>penalty = "exp"</code>  (Wang et al. 2018)
</p>
</li>
<li><p> Log: <code>penalty = "log"</code> (Mazumder et al. 2011).
</p>
</li>
<li><p> Sica: <code>penalty = "sica"</code>  (Lv and Fan 2009)
</p>
</li></ul>

<p><strong>Additional penalties</strong>:
</p>

<ul>
<li><p> SCAD: <code>penalty = "scad"</code>  (Fan and Li 2001).
</p>
</li>
<li><p> MCP: <code>penalty = "mcp"</code> (Zhang 2010).
</p>
</li>
<li><p> Adaptive lasso: <code>penalty = "adapt"</code> (Zou 2006).
</p>
</li>
<li><p> Lasso:  <code>penalty = "lasso"</code>  (Tibshirani 1996).
</p>
</li></ul>

<p><strong>Citing GGMncv</strong>
</p>
<p>It is important to note that GGMncv merely provides a software implementation
of other researchers work. There are no methodological innovations,
although this is the most comprehensive R package for estimating GGMs
with non-convex penalties. Hence, in addition to citing the
package <code>citation("GGMncv")</code>, it is important to give credit to the primary
sources. The references are provided above and in <code><a href="#topic+ggmncv">ggmncv</a></code>.
</p>
<p>Further, a survey (or review) of these penalties can be found in
Williams (2020).
</p>


<h3>References</h3>

<p>Dicker L, Huang B, Lin X (2013).
&ldquo;Variable selection and estimation with the seamless-L 0 penalty.&rdquo;
<em>Statistica Sinica</em>, 929&ndash;962.<br /><br /> Fan J, Li R (2001).
&ldquo;Variable selection via nonconcave penalized likelihood and its oracle properties.&rdquo;
<em>Journal of the American statistical Association</em>, <b>96</b>(456), 1348&ndash;1360.<br /><br /> Lv J, Fan Y (2009).
&ldquo;A unified approach to model selection and sparse recovery using regularized least squares.&rdquo;
<em>The Annals of Statistics</em>, <b>37</b>(6A), 3498&ndash;3528.<br /><br /> Mazumder R, Friedman JH, Hastie T (2011).
&ldquo;Sparsenet: Coordinate descent with nonconvex penalties.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>106</b>(495), 1125&ndash;1138.<br /><br /> Tibshirani R (1996).
&ldquo;Regression shrinkage and selection via the lasso.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <b>58</b>(1), 267&ndash;288.<br /><br /> Wang Y, Fan Q, Zhu L (2018).
&ldquo;Variable selection and estimation using a continuous approximation to the L0 penalty.&rdquo;
<em>Annals of the Institute of Statistical Mathematics</em>, <b>70</b>(1), 191&ndash;214.<br /><br /> Wang Y, Zhu L (2016).
&ldquo;Variable selection and parameter estimation with the Atan regularization method.&rdquo;
<em>Journal of Probability and Statistics</em>.<br /><br /> Williams DR (2020).
&ldquo;Beyond Lasso: A Survey of Nonconvex Regularization in Gaussian Graphical Models.&rdquo;
<em>PsyArXiv</em>.<br /><br /> Williams DR, Rast P (2020).
&ldquo;Back to the basics: Rethinking partial correlation network methodology.&rdquo;
<em>British Journal of Mathematical and Statistical Psychology</em>, <b>73</b>(2), 187&ndash;212.<br /><br /> Williams DR, Rhemtulla M, Wysocki AC, Rast P (2019).
&ldquo;On nonregularized estimation of psychological networks.&rdquo;
<em>Multivariate behavioral research</em>, <b>54</b>(5), 719&ndash;750.<br /><br /> Zhang C (2010).
&ldquo;Nearly unbiased variable selection under minimax concave penalty.&rdquo;
<em>The Annals of statistics</em>, <b>38</b>(2), 894&ndash;942.<br /><br /> Zhao P, Yu B (2006).
&ldquo;On model selection consistency of Lasso.&rdquo;
<em>Journal of Machine learning research</em>, <b>7</b>(Nov), 2541&ndash;2563.<br /><br /> Zou H (2006).
&ldquo;The adaptive lasso and its oracle properties.&rdquo;
<em>Journal of the American statistical association</em>, <b>101</b>(476), 1418&ndash;1429.
</p>

<hr>
<h2 id='head.eip'>Print the Head of <code>eip</code> Objects</h2><span id='topic+head.eip'></span>

<h3>Description</h3>

<p>Print the Head of <code>eip</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'eip'
head(x, n = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="head.eip_+3A_x">x</code></td>
<td>
<p>An object of class <code>eip</code></p>
</td></tr>
<tr><td><code id="head.eip_+3A_n">n</code></td>
<td>
<p>Numeric. Number of rows to print.</p>
</td></tr>
<tr><td><code id="head.eip_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='inference'>Statistical Inference for Regularized Gaussian Graphical Models</h2><span id='topic+inference'></span><span id='topic+significance_test'></span>

<h3>Description</h3>

<p>Compute <em>p</em>-values for each relation based on the
de-sparsified glasso estimator (Jankova and Van De Geer 2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inference(object, method = "fdr", alpha = 0.05, ...)

significance_test(object, method = "fdr", alpha = 0.05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inference_+3A_object">object</code></td>
<td>
<p>An object of class <code>ggmncv</code></p>
</td></tr>
<tr><td><code id="inference_+3A_method">method</code></td>
<td>
<p>Character string. A correction method for multiple comparison (defaults to <code>fdr</code>).
Can be abbreviated. See <a href="stats.html#topic+p.adjust">p.adjust</a>.</p>
</td></tr>
<tr><td><code id="inference_+3A_alpha">alpha</code></td>
<td>
<p>Numeric. Significance level (defaults to <code>0.05</code>).</p>
</td></tr>
<tr><td><code id="inference_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code>Theta</code> De-sparsified precision matrix
</p>
</li>
<li> <p><code>adj</code> Adjacency matrix based on the p-values.
</p>
</li>
<li> <p><code>pval_uncorrected</code> Uncorrected p-values
</p>
</li>
<li> <p><code>pval_corrected</code> Corrected p-values
</p>
</li>
<li> <p><code>method</code> The approach used for multiple comparisons
</p>
</li>
<li> <p><code>alpha</code> Significance level
</p>
</li></ul>



<h3>Note</h3>

<p>This assumes (reasonably) Gaussian data, and should not to be expected
to work for, say, polychoric correlations. Further, all work to date
has only looked at the graphical lasso estimator, and not de-sparsifying
nonconvex regularization. Accordingly, it is probably best to set
<code>penalty = "lasso"</code> in <code><a href="#topic+ggmncv">ggmncv</a></code>.
</p>
<p>Further, whether the de-sparsified estimator provides nominal error rates
remains to be seen, at least across a range of conditions. For example,
the simulation results in Williams (2021)
demonstrated that the confidence intervals
can have (severely) compromised coverage properties (whereas non-regularized methods
had coverage at the nominal level).
</p>


<h3>References</h3>

<p>Jankova J, Van De Geer S (2015).
&ldquo;Confidence intervals for high-dimensional inverse covariance estimation.&rdquo;
<em>Electronic Journal of Statistics</em>, <b>9</b>(1), 1205&ndash;1229.<br /><br /> Williams DR (2021).
&ldquo;The Confidence Interval that Wasn't: Bootstrapped &quot;Confidence Intervals&quot; in L1-Regularized Partial Correlation Networks.&rdquo;
<em>PsyArXiv</em>.
doi: <a href="https://doi.org/10.31234/osf.io/kjh2f">10.31234/osf.io/kjh2f</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data
Y &lt;- GGMncv::ptsd[,1:5]

# fit model
fit &lt;- ggmncv(cor(Y), n = nrow(Y),
              progress = FALSE,
              penalty = "lasso")


# statistical inference
inference(fit)

# alias
all.equal(inference(fit), significance_test(fit))

</code></pre>

<hr>
<h2 id='kl_mvn'>Kullback-Leibler Divergence</h2><span id='topic+kl_mvn'></span>

<h3>Description</h3>

<p>Compute KL divergence for a multivariate normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kl_mvn(true, estimate, stein = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kl_mvn_+3A_true">true</code></td>
<td>
<p>Matrix. The true precision matrix
(inverse of the covariance matrix)</p>
</td></tr>
<tr><td><code id="kl_mvn_+3A_estimate">estimate</code></td>
<td>
<p>Matrix. The estimated precision matrix
(inverse of the covariance matrix)</p>
</td></tr>
<tr><td><code id="kl_mvn_+3A_stein">stein</code></td>
<td>
<p>Logical. Should Stein's loss be computed
(defaults to <code>TRUE</code>)? Note KL divergence is
half of Stein's loss.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric corresponding to KL divergence.
</p>


<h3>Note</h3>

<p>A lower value is better, with a score of zero indicating that
the estimated precision matrix is identical to the true precision matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# nodes
p &lt;- 20

main &lt;- gen_net(p = p, edge_prob = 0.15)

y &lt;- MASS::mvrnorm(250, rep(0, p), main$cors)

fit_l1 &lt;- ggmncv(R = cor(y),
              n = nrow(y),
              penalty = "lasso",
              progress = FALSE)

# lasso
kl_mvn(fit_l1$Theta, solve(main$cors))

fit_atan &lt;- ggmncv(R = cor(y),
              n = nrow(y),
              penalty = "atan",
              progress = FALSE)

kl_mvn(fit_atan$Theta, solve(main$cors))


</code></pre>

<hr>
<h2 id='ledoit_wolf'>Ledoit and Wolf Shrinkage Estimator</h2><span id='topic+ledoit_wolf'></span>

<h3>Description</h3>

<p>Compute the Ledoit and Wolf shrinkage estimator of
the covariance matrix (Ledoit and Wolf 2004),
which can be used for the <code>initial</code> inverse covariance matrix
in <code><a href="#topic+ggmncv">ggmncv</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ledoit_wolf(Y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ledoit_wolf_+3A_y">Y</code></td>
<td>
<p>A data matrix (or data.frame) of dimensions <em>n</em> by <em>p</em>.</p>
</td></tr>
<tr><td><code id="ledoit_wolf_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Inverse correlation matrix.
</p>


<h3>References</h3>

<p>Ledoit O, Wolf M (2004).
&ldquo;A well-conditioned estimator for large-dimensional covariance matrices.&rdquo;
<em>Journal of Multivariate Analysis</em>, <b>88</b>(2), 365&ndash;411.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ptsd
Y &lt;- ptsd[,1:5]

# shrinkage
ledoit_wolf(Y)

# non-reg
solve(cor(Y))
</code></pre>

<hr>
<h2 id='nct'>Network Comparison Test</h2><span id='topic+nct'></span>

<h3>Description</h3>

<p>A re-implementation and extension of the permutation based
network comparison test introduced in Van Borkulo et al. (2017).
Such extensions include scaling to networks with many nodes and the option to
use custom test-statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nct(
  Y_g1,
  Y_g2,
  iter = 1000,
  desparsify = TRUE,
  method = "pearson",
  FUN = NULL,
  cores = 1,
  progress = TRUE,
  update_progress = 4,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nct_+3A_y_g1">Y_g1</code></td>
<td>
<p>A matrix (or data.frame) of dimensions <em>n</em> by <em>p</em>,
corresponding to the first dataset (<em>p</em> must be the same
for <code>Y_g1</code> and <code>Y_g2</code>).</p>
</td></tr>
<tr><td><code id="nct_+3A_y_g2">Y_g2</code></td>
<td>
<p>A matrix of dimensions <em>n</em> by <em>p</em>, corresponding to the
second dataset (<em>p</em> must be the same for <code>Y_g1</code> and <code>Y_g2</code>).</p>
</td></tr>
<tr><td><code id="nct_+3A_iter">iter</code></td>
<td>
<p>Numeric. Number of (Monte Carlo) permutations (defaults to <code>1000</code>).</p>
</td></tr>
<tr><td><code id="nct_+3A_desparsify">desparsify</code></td>
<td>
<p>Logical. Should the de-sparsified glasso estimator be
computed (defaults to <code>TRUE</code>)? This is much faster,
as the tuning parameter is fixed to
\(\lambda = \sqrt{log(p)/n}\).</p>
</td></tr>
<tr><td><code id="nct_+3A_method">method</code></td>
<td>
<p>character string. Which correlation coefficient (or covariance)
is to be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;,
or &quot;spearman&quot;.</p>
</td></tr>
<tr><td><code id="nct_+3A_fun">FUN</code></td>
<td>
<p>A function or list of functions (defaults to <code>NULL</code>),
specifying custom test-statistics. See <strong>Examples</strong>.</p>
</td></tr>
<tr><td><code id="nct_+3A_cores">cores</code></td>
<td>
<p>Numeric. Number of cores to use when executing the permutations in
parallel (defaults to <code>1</code>).</p>
</td></tr>
<tr><td><code id="nct_+3A_progress">progress</code></td>
<td>
<p>Logical. Should a progress bar be included
(defaults to <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="nct_+3A_update_progress">update_progress</code></td>
<td>
<p>How many times should the progress bar be updated
(defaults to <code>4</code>)? Note that setting this to a
large value should result in the worse performance,
due to additional overhead communicating among the
parallel processes.</p>
</td></tr>
<tr><td><code id="nct_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+ggmncv">ggmncv</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>User-Defined Functions</strong>
</p>
<p>These functions must have two arguments, corresponding
to the partial correlation network for each group. An
example is provided below.
</p>
<p>For user-defined functions (<code>FUN</code>), absolute values are used
to compute the p-value, assuming more than one value is returned
(e.g., centrality). This is done to mimic the <code>R</code> package
<strong>NCT</strong>.
</p>
<p>A fail-safe method to ensure the p-value is computed correctly is
to access the permutations and observed values from the <code>nct</code>
object.
</p>
<p>Finally, comparing edges is not implemented. The most straightforward
way to do this is with <code><a href="#topic+compare_edges">compare_edges</a></code>, which
uses the de-sparsified estimator.
</p>


<h3>Value</h3>

<p>A list of class <code>nct</code>, including the following
</p>

<ul>
<li> <p><code>glstr_pvalue</code>: Global strength p-value.
</p>
</li>
<li> <p><code>sse_pvalue</code>: Sum of square error p-value.
</p>
</li>
<li> <p><code>jsd_pvalue</code>: Jensen-Shannon divergence p-value.
</p>
</li>
<li> <p><code>max_pvalue</code>: Maximum difference p-value.
</p>
</li>
<li> <p><code>glstr_obs</code>: Global strength observed.
</p>
</li>
<li> <p><code>sse_obs</code>: Sum of square error observed.
</p>
</li>
<li> <p><code>jsd_obs</code>: Jensen-Shannon divergence observed.
</p>
</li>
<li> <p><code>max_obs</code>: Maximum difference observed.
</p>
</li>
<li> <p><code>glstr_perm</code>: Global strength permutations.
</p>
</li>
<li> <p><code>sse_perm</code>: Sum of square error permutations.
</p>
</li>
<li> <p><code>jsd_perm</code>: Jensen-Shannon divergence permutations.
</p>
</li>
<li> <p><code>max_perm</code>: Maximum difference permutations.
</p>
</li></ul>

<p>For user-defined functions, i.e., those provided to <code>FUN</code>,
the function name is pasted to <code>_pvalue</code>, <code>_obs</code>, and
<code>_perm</code>.
</p>


<h3>Note</h3>

<p>In Van Borkulo et al. (2017), it was suggested that
these are tests of <em>invariance</em>. To avoid confusion, that
terminology is not used in <strong>GGMncv</strong>. This is because
these tests assume invariance or the null is <em>true</em>, and thus
can only be used to detect differences. Hence, it would be incorrect
to suggest networks are the same, or evidence for invariance,
by merely failing to reject the null hypothesis
(Williams et al. 2021).
</p>
<p>For the defaults, Jensen-Shannon divergence is a symmetrized version
of Kullback-Leibler divergence (the average of both directions).
</p>
<p><strong>Computational Speed</strong>
</p>
<p>This implementation has two key features that should make it
scale to larger networks: (1) parallel computation and (2) the
<code>R</code> package <strong>glassoFast</strong> is used under the hood
(as opposed to <strong>glasso</strong>). CPU (time) comparisons are
provided in Sustik and Calderhead (2012).
</p>
<p><strong>Non-regularized</strong>
</p>
<p>Non-regularized can be implemented by setting <code>lambda = 0</code>. Note
this is provided to <code><a href="#topic+ggmncv">ggmncv</a></code> via <code>...</code>.
</p>


<h3>References</h3>

<p>Sustik MA, Calderhead B (2012).
&ldquo;GLASSOFAST: An efficient GLASSO implementation.&rdquo;
<em>UTCS Technical Report TR-12-29 2012</em>.<br /><br /> Van Borkulo CD, Boschloo L, Kossakowski J, Tio P, Schoevers RA, Borsboom D, Waldorp LJ (2017).
&ldquo;Comparing network structures on three aspects: A permutation test.&rdquo;
<em>Manuscript submitted for publication</em>, <b>10</b>.<br /><br /> Williams DR, Briganti G, Linkowski P, Mulder J (2021).
&ldquo;On Accepting the Null Hypothesis of Conditional Independence in Partial Correlation Networks: A Bayesian Analysis.&rdquo;
<em>PsyArXiv</em>.
doi: <a href="https://doi.org/10.31234/osf.io/7uhx8">10.31234/osf.io/7uhx8</a>, <a href="https://psyarxiv.com/7uhx8">https://psyarxiv.com/7uhx8</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate network
main &lt;- gen_net(p = 10)

# assume groups are equal
y1 &lt;- MASS::mvrnorm(n = 500,
                    mu = rep(0, 10),
                    Sigma = main$cors)

y2 &lt;- MASS::mvrnorm(n = 500,
                    mu = rep(0, 10),
                    Sigma = main$cors)

compare_ggms &lt;- nct(y1, y2, iter = 500,
                    progress = FALSE)

compare_ggms

# custom function
# note: x &amp; y are partial correlation networks

# correlation
Correlation &lt;- function(x, y){
cor(x[upper.tri(x)], y[upper.tri(y)])
}

compare_ggms &lt;- nct(y1, y2,iter = 100,
                    FUN = Correlation,
                    progress = FALSE)

compare_ggms

# correlation and strength

Strength &lt;- function(x, y){
NetworkToolbox::strength(x) - NetworkToolbox::strength(y)
}

compare_ggms &lt;- nct(y1, y2, iter = 100,
                    FUN = list(Correlation = Correlation,
                               Strength = Strength),
                    progress = FALSE)

compare_ggms

</code></pre>

<hr>
<h2 id='penalty_derivative'>Penalty Derivative</h2><span id='topic+penalty_derivative'></span>

<h3>Description</h3>

<p>Compute the derivative for a nonconvex penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>penalty_derivative(
  theta = seq(-5, 5, length.out = 1e+05),
  penalty = "atan",
  lambda = 1,
  gamma = c(0.01, 0.05)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="penalty_derivative_+3A_theta">theta</code></td>
<td>
<p>Numeric vector. Values for which the derivative is computed.</p>
</td></tr>
<tr><td><code id="penalty_derivative_+3A_penalty">penalty</code></td>
<td>
<p>Character string. Which penalty should be
used (defaults to <code>"atan"</code>)?
See <code><a href="#topic+ggmncv">ggmncv</a></code> for the
available penalties.</p>
</td></tr>
<tr><td><code id="penalty_derivative_+3A_lambda">lambda</code></td>
<td>
<p>Numeric.  Regularization parameter (defaults to <code>1</code>).</p>
</td></tr>
<tr><td><code id="penalty_derivative_+3A_gamma">gamma</code></td>
<td>
<p>Numeric vector. Hyperparameter(s) for the penalty function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>penalty_derivative</code>, including the following:
</p>

<ul>
<li> <p><code>deriv</code>: Data frame including the derivative, theta, gamma,
and the chosen penalty.
</p>
</li>
<li> <p><code>lambda</code>: Regularization parameter.
</p>
</li></ul>



<h3>Note</h3>

<p>Some care is required for specifying <code>gamma</code>. For example,
the default value for <code>scad</code> is 3.7 and it <em>must</em> be some
value greater than 2 (Fan and Li 2001). The
default values in <strong>GGMncv</strong> are set to recommended values in the
respective papers.
</p>


<h3>References</h3>

<p>Fan J, Li R (2001).
&ldquo;Variable selection via nonconcave penalized likelihood and its oracle properties.&rdquo;
<em>Journal of the American statistical Association</em>, <b>96</b>(456), 1348&ndash;1360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>deriv &lt;- penalty_derivative(theta =  seq(-5,5,length.out = 10000),
                            lambda = 1,
                            gamma = c(0.01, 0.05, 0.1))

head(deriv$deriv)
</code></pre>

<hr>
<h2 id='penalty_function'>Penalty Function</h2><span id='topic+penalty_function'></span>

<h3>Description</h3>

<p>Compute the penalty function for nonconvex penalties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>penalty_function(
  theta = seq(-5, 5, length.out = 1e+05),
  penalty = "atan",
  lambda = 1,
  gamma = c(0.01, 0.05)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="penalty_function_+3A_theta">theta</code></td>
<td>
<p>Numeric vector. Values for which the derivative is computed.</p>
</td></tr>
<tr><td><code id="penalty_function_+3A_penalty">penalty</code></td>
<td>
<p>Character string. Which penalty should be
used (defaults to <code>"atan"</code>)?
See <code><a href="#topic+ggmncv">ggmncv</a></code> for the
available penalties.</p>
</td></tr>
<tr><td><code id="penalty_function_+3A_lambda">lambda</code></td>
<td>
<p>Numeric.  Regularization parameter (defaults to <code>1</code>).</p>
</td></tr>
<tr><td><code id="penalty_function_+3A_gamma">gamma</code></td>
<td>
<p>Numeric vector. Hyperparameter(s) for the penalty function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>penalty_function</code>, including the following:
</p>

<ul>
<li> <p><code>deriv</code>: Data frame including the penalty function,
theta, gamma, and the chosen penalty.
</p>
</li></ul>



<h3>Note</h3>

<p>Some care is required for specifying <code>gamma</code>. For example,
the default value for <code>scad</code> is 3.7 and it <em>must</em> be some
value greater than 2 (Fan and Li 2001). The
default values in <strong>GGMncv</strong> are set to recommended values in the
respective papers.
</p>


<h3>References</h3>

<p>Fan J, Li R (2001).
&ldquo;Variable selection via nonconcave penalized likelihood and its oracle properties.&rdquo;
<em>Journal of the American statistical Association</em>, <b>96</b>(456), 1348&ndash;1360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>func &lt;- penalty_function(theta =  seq(-5,5,length.out = 10000),
                            lambda = 1,
                            gamma = c(0.01, 0.05, 0.1))

head(func$pen)
</code></pre>

<hr>
<h2 id='plot.eip'>Plot Edge Inclusion 'Probabilities'</h2><span id='topic+plot.eip'></span>

<h3>Description</h3>

<p>Plot Edge Inclusion 'Probabilities'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'eip'
plot(x, color = "black", size = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.eip_+3A_x">x</code></td>
<td>
<p>An object of class <code>eip</code></p>
</td></tr>
<tr><td><code id="plot.eip_+3A_color">color</code></td>
<td>
<p>Character string. Color for <code>geom_point</code>.</p>
</td></tr>
<tr><td><code id="plot.eip_+3A_size">size</code></td>
<td>
<p>Numeric. Size of <code>geom_point</code>.</p>
</td></tr>
<tr><td><code id="plot.eip_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ggplot</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# data
Y &lt;- GGMncv::ptsd[,1:10]

# compute eip's
boot_samps &lt;- boot_eip(Y, B = 10, progress = FALSE)


plot(boot_samps)


</code></pre>

<hr>
<h2 id='plot.ggmncv'>Plot <code>ggmncv</code> Objects</h2><span id='topic+plot.ggmncv'></span>

<h3>Description</h3>

<p>Plot the solution path for the partial correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ggmncv'
plot(x, size = 1, alpha = 0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ggmncv_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+ggmncv">ggmncv</a></code>.</p>
</td></tr>
<tr><td><code id="plot.ggmncv_+3A_size">size</code></td>
<td>
<p>Numeric. Line size in <code>geom_line</code>.</p>
</td></tr>
<tr><td><code id="plot.ggmncv_+3A_alpha">alpha</code></td>
<td>
<p>Numeric. The transparency of the lines.</p>
</td></tr>
<tr><td><code id="plot.ggmncv_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# data
Y &lt;- GGMncv::ptsd[,1:10]

# correlations
S &lt;- cor(Y, method = "spearman")

# fit model
# default: atan
fit &lt;- ggmncv(R = S, n = nrow(Y), progress = FALSE)

# plot
plot(fit)

# lasso
fit &lt;- ggmncv(R = S, n = nrow(Y), progress = FALSE,
              penalty = "lasso")

# plot
plot(fit)

</code></pre>

<hr>
<h2 id='plot.graph'>Network Plot for <code>select</code> Objects</h2><span id='topic+plot.graph'></span>

<h3>Description</h3>

<p>Visualize the conditional dependence structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'graph'
plot(
  x,
  layout = "circle",
  neg_col = "#D55E00",
  pos_col = "#009E73",
  edge_magnify = 1,
  node_size = 10,
  palette = 2,
  node_names = NULL,
  node_groups = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.graph_+3A_x">x</code></td>
<td>
<p>An object of class <code>graph</code> obtained from <code><a href="#topic+get_graph">get_graph</a></code>.</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_layout">layout</code></td>
<td>
<p>Character string. Which graph layout (defaults is <code>circle</code>) ?
See <a href="sna.html#topic+gplot.layout">gplot.layout</a>.</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_neg_col">neg_col</code></td>
<td>
<p>Character string. Color for the positive edges
(defaults to a colorblind friendly red).</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_pos_col">pos_col</code></td>
<td>
<p>Character string.  Color for the negative edges
(defaults to a colorblind friendly green).</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_edge_magnify">edge_magnify</code></td>
<td>
<p>Numeric. A value that is multiplied by the edge weights. This increases (&gt; 1) or
decreases (&lt; 1) the line widths (defaults to 1).</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_node_size">node_size</code></td>
<td>
<p>Numeric. The size of the nodes (defaults to <code>10</code>).</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_palette">palette</code></td>
<td>
<p>A character string sepcifying the palette for the <code>groups</code>.
(default is <code>Set3</code>). See <a href="http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/">palette options here</a>.</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_node_names">node_names</code></td>
<td>
<p>Character string. Names for nodes of length <em>p</em>.</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_node_groups">node_groups</code></td>
<td>
<p>A character string of length <em>p</em> (the number of nodes in the model).
This indicates groups of nodes that should be the same color
(e.g., &quot;clusters&quot; or &quot;communities&quot;).</p>
</td></tr>
<tr><td><code id="plot.graph_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ggplot</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

Y &lt;- na.omit(bfi[,1:25])

fit &lt;- ggmncv(cor(Y), n = nrow(Y),
              progress = FALSE)

plot(get_graph(fit))


</code></pre>

<hr>
<h2 id='plot.penalty_derivative'>Plot <code>penalty_derivative</code> Objects</h2><span id='topic+plot.penalty_derivative'></span>

<h3>Description</h3>

<p>Plot <code>penalty_derivative</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'penalty_derivative'
plot(x, size = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.penalty_derivative_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+penalty_derivative">penalty_derivative</a></code>.</p>
</td></tr>
<tr><td><code id="plot.penalty_derivative_+3A_size">size</code></td>
<td>
<p>Numeric. Line size in <code>geom_line</code>.</p>
</td></tr>
<tr><td><code id="plot.penalty_derivative_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ggplot</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pen_deriv &lt;- penalty_derivative(theta =  seq(-5,5,length.out = 10000),
                            lambda = 1,
                            gamma = c(0.01, 0.05, 0.1))
plot(pen_deriv)

</code></pre>

<hr>
<h2 id='plot.penalty_function'>Plot <code>penalty_function</code> Objects</h2><span id='topic+plot.penalty_function'></span>

<h3>Description</h3>

<p>Plot <code>penalty_function</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'penalty_function'
plot(x, size = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.penalty_function_+3A_x">x</code></td>
<td>
<p>An object of class<code><a href="#topic+penalty_function">penalty_function</a></code>.</p>
</td></tr>
<tr><td><code id="plot.penalty_function_+3A_size">size</code></td>
<td>
<p>Numeric. Line size in <code>geom_line</code>.</p>
</td></tr>
<tr><td><code id="plot.penalty_function_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ggplot</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
func &lt;- penalty_function(theta =  seq(-5,5,length.out = 10000),
                            lambda = 1,
                            gamma = c(0.01, 0.05, 0.1))
plot(func)

</code></pre>

<hr>
<h2 id='predict.ggmncv'>Predict method for <code>ggmncv</code> Objects</h2><span id='topic+predict.ggmncv'></span>

<h3>Description</h3>

<p>There is a direct correspondence between the inverse covariance
matrix and multiple regression (Stephens 1998; Kwan 2014).
This readily allows for converting the off diagonal elements to regression coefficients,
opening the door to out-of-sample prediction in multiple regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ggmncv'
predict(object, train_data = NULL, newdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ggmncv_+3A_object">object</code></td>
<td>
<p>An object of class <code><a href="#topic+ggmncv">ggmncv</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ggmncv_+3A_train_data">train_data</code></td>
<td>
<p>Data used for model fitting (defaults to <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="predict.ggmncv_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame in which to look for variables with which to predict.
If omitted, the fitted values are used.</p>
</td></tr>
<tr><td><code id="predict.ggmncv_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of predicted values, of dimensions rows
(in the training/test data) by the number of nodes (columns).
</p>


<h3>References</h3>

<p>Kwan CC (2014).
&ldquo;A regression-based interpretation of the inverse of the sample covariance matrix.&rdquo;
<em>Spreadsheets in Education</em>, <b>7</b>(1), 4613.<br /><br /> Stephens G (1998).
&ldquo;On the Inverse of the Covariance Matrix in Portfolio Analysis.&rdquo;
<em>The Journal of Finance</em>, <b>53</b>(5), 1821&ndash;1827.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data
Y &lt;- scale(Sachs)

# test data
Ytest &lt;- Y[1:100,]

# training data
Ytrain &lt;- Y[101:nrow(Y),]

fit &lt;- ggmncv(cor(Ytrain), n = nrow(Ytrain),
              progress = FALSE)

pred &lt;- predict(fit, newdata = Ytest)

round(apply((pred - Ytest)^2, 2, mean), 2)
</code></pre>

<hr>
<h2 id='print.eip'>Print <code>eip</code> Objects</h2><span id='topic+print.eip'></span>

<h3>Description</h3>

<p>Print <code>eip</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'eip'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.eip_+3A_x">x</code></td>
<td>
<p>An object of class <code>eip</code></p>
</td></tr>
<tr><td><code id="print.eip_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='print.ggmncv'>Print <code>ggmncv</code> Objects</h2><span id='topic+print.ggmncv'></span>

<h3>Description</h3>

<p>Print <code>ggmncv</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ggmncv'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ggmncv_+3A_x">x</code></td>
<td>
<p>An object of class <code>ggmncv</code></p>
</td></tr>
<tr><td><code id="print.ggmncv_+3A_...">...</code></td>
<td>
<p>Currently ignored</p>
</td></tr>
</table>

<hr>
<h2 id='print.nct'>Print <code>nct</code> Objects</h2><span id='topic+print.nct'></span>

<h3>Description</h3>

<p>Print <code>nct</code> Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nct'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nct_+3A_x">x</code></td>
<td>
<p>An object of class <code>nct</code></p>
</td></tr>
<tr><td><code id="print.nct_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='ptsd'>Data: Post-Traumatic Stress Disorder</h2><span id='topic+ptsd'></span>

<h3>Description</h3>

<p>A dataset containing items that measure Post-traumatic stress disorder symptoms (Armour et al. 2017).
There are 20 variables (<em>p</em>) and  221 observations (<em>n</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("ptsd")
</code></pre>


<h3>Format</h3>

<p>A dataframe with 221 rows and 20 variables
</p>


<h3>Details</h3>


<ul>
<li><p> Intrusive Thoughts
</p>
</li>
<li><p> Nightmares
</p>
</li>
<li><p> Flashbacks
</p>
</li>
<li><p> Emotional cue reactivity
</p>
</li>
<li><p> Psychological cue reactivity
</p>
</li>
<li><p> Avoidance of thoughts
</p>
</li>
<li><p> Avoidance of reminders
</p>
</li>
<li><p> Trauma-related amnesia
</p>
</li>
<li><p> Negative beliefs
</p>
</li>
<li><p> Negative trauma-related emotions
</p>
</li>
<li><p> Loss of interest
</p>
</li>
<li><p> Detachment
</p>
</li>
<li><p> Restricted affect
</p>
</li>
<li><p> Irritability/anger
</p>
</li>
<li><p> Self-destructive/reckless behavior
</p>
</li>
<li><p> Hypervigilance
</p>
</li>
<li><p> Exaggerated startle response
</p>
</li>
<li><p> Difficulty concentrating
</p>
</li>
<li><p> Sleep disturbance
</p>
</li></ul>



<h3>References</h3>

<p>Armour C, Fried EI, Deserno MK, Tsai J, Pietrzak RH (2017).
&ldquo;A network analysis of DSM-5 posttraumatic stress disorder symptoms and correlates in US military veterans.&rdquo;
<em>Journal of anxiety disorders</em>, <b>45</b>, 49&ndash;59.
</p>

<hr>
<h2 id='Sachs'>Data: Sachs Network</h2><span id='topic+Sachs'></span>

<h3>Description</h3>

<p>Protein expression in human immune system cells
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Sachs")
</code></pre>


<h3>Format</h3>

<p>A data frame containing 7466 cells (n = 7466) and flow cytometry
measurements of 11 (p = 11) phosphorylated proteins and phospholipids
(Sachs et al. 2002)
</p>


<h3>References</h3>

<p>Sachs K, Gifford D, Jaakkola T, Sorger P, Lauffenburger DA (2002).
&ldquo;Bayesian network approach to cell signaling pathway modeling.&rdquo;
<em>Science's STKE</em>, <b>2002</b>(148), pe38&ndash;pe38.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Sachs")

</code></pre>

<hr>
<h2 id='score_binary'>Binary Classification</h2><span id='topic+score_binary'></span>

<h3>Description</h3>

<p>Binary Classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_binary(estimate, true, model_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_binary_+3A_estimate">estimate</code></td>
<td>
<p>Matrix. Estimated graph (adjacency matrix)</p>
</td></tr>
<tr><td><code id="score_binary_+3A_true">true</code></td>
<td>
<p>Matrix. True graph (adjacency matrix)</p>
</td></tr>
<tr><td><code id="score_binary_+3A_model_name">model_name</code></td>
<td>
<p>Character string. Name of the method or penalty
(defaults to <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing specificity (1 - false positive rate),
sensitivity (true positive rate), precision (1 - false discovery rate),
f1_score, and mcc (Matthews correlation coefficient).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- 20
n &lt;- 500

true_net &lt;- gen_net(p = p, edge_prob = 0.25)

y &lt;- MASS::mvrnorm(n = n,
                   mu = rep(0, p),
                   Sigma = true_net$cors)

# default
fit_atan &lt;- ggmncv(R = cor(y),
                   n = nrow(y),
                   penalty = "atan",
                   progress = FALSE)

# lasso
fit_l1 &lt;- ggmncv(R = cor(y),
                 n = nrow(y),
                 penalty = "lasso",
                 progress = FALSE)

# atan scores
score_binary(estimate = true_net$adj,
             true = fit_atan$adj,
             model_name = "atan")

score_binary(estimate = fit_l1$adj,
             true = true_net$adj,
             model_name = "lasso")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
