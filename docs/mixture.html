<!DOCTYPE html><html><head><title>Help for package mixture</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mixture}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mixture'><p>Mixture Models for Clustering and Classification</p></a></li>
<li><a href='#ARI'><p>Adjusted Rand Index</p></a></li>
<li><a href='#e_step'><p>Expectation Step</p></a></li>
<li><a href='#get_best_model'><p>Best Model Extractor</p></a></li>
<li><a href='#ghpcm'><p>Generalized Hyperbolic Parsimonious Clustering Models</p></a></li>
<li><a href='#gpcm'><p>Gaussian Parsimonious Clustering Models</p></a></li>
<li><a href='#main_loop'><p>GPCM Internal C++ Call</p></a></li>
<li><a href='#main_loop_gh'><p>GHPCM Internal C++ Call</p></a></li>
<li><a href='#main_loop_st'><p>STPCM Internal C++ Call</p></a></li>
<li><a href='#main_loop_t'><p>TPCM Internal C++ Call</p></a></li>
<li><a href='#main_loop_vg'><p>VGPCM Internal C++ Call</p></a></li>
<li><a href='#MAP'><p>Maximum <em>a posterori</em></p></a></li>
<li><a href='#pcm'><p>Parsimonious Clustering Models</p></a></li>
<li><a href='#stpcm'><p>Skew-t Parsimonious Clustering Models</p></a></li>
<li><a href='#sx2'><p>Skewed Simulated Data 1</p></a></li>
<li><a href='#sx3'><p>Skewed Simulated Data 2</p></a></li>
<li><a href='#tpcm'><p>Student T Parsimonious Clustering Models</p></a></li>
<li><a href='#vgpcm'><p>Variance Gamma Parsimonious Clustering Models</p></a></li>
<li><a href='#x2'><p>Simulated Data</p></a></li>
<li><a href='#z_ig_kmeans'><p>K-means Initialization</p></a></li>
<li><a href='#z_ig_random_hard'><p>Random Hard Initialization</p></a></li>
<li><a href='#z_ig_random_soft'><p>Random Soft Initialization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Mixture Models for Clustering and Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-29</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of 14 parsimonious mixture models for model-based clustering or model-based classification. Gaussian, Student's t, generalized hyperbolic, variance-gamma or skew-t mixtures are available. All approaches work with missing data. Celeux and Govaert (1995) &lt;<a href="https://doi.org/10.1016%2F0031-3203%2894%2900125-6">doi:10.1016/0031-3203(94)00125-6</a>&gt;, Browne and McNicholas (2014) &lt;<a href="https://doi.org/10.1007%2Fs11634-013-0139-1">doi:10.1007/s11634-013-0139-1</a>&gt;, Browne and McNicholas (2015) &lt;<a href="https://doi.org/10.1002%2Fcjs.11246">doi:10.1002/cjs.11246</a>&gt;.</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.2), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, BH, RcppGSL</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), lattice (&ge; 0.20)</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU GSL</td>
</tr>
<tr>
<td>Author:</td>
<td>Nik Pocuca <a href="https://orcid.org/0000-0001-8365-0751"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Ryan P. Browne <a href="https://orcid.org/0000-0003-4543-0218"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Paul D. McNicholas
    <a href="https://orcid.org/0000-0002-2482-523X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-29 18:16:11 UTC; paul</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-30 00:20:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='mixture'>Mixture Models for Clustering and Classification</h2><span id='topic+mixture'></span><span id='topic+mixture-package'></span>

<h3>Description</h3>

<p>An implementation of 14 parsimonious clustering models for finite mixtures with components that are Gaussian, generalized hyperbolic, variance-gamma, Student's t, or skew-t, for model-based clustering and model-based classification, even with missing data. 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> mixture </td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.1.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-01-29</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package contains the functions <code>gpcm</code>, <code>tpcm</code>, <code>ghpcm</code>, <code>vgpcm</code>, <code>stpcm</code>, <code>e_step</code>, <code>ARI</code>, and <code>get_best_model</code>, plus three simulated data sets.
</p>
<p>This package also contains advanced functions for large system use which are:
<code>main_loop</code> <code>main_loop_vg</code> , <code>main_loop_gh</code>, <code>main_loop_t</code> , <code>main_loop_st</code> ,<code>z_ig_random_soft</code>, <code>z_ig_random_hard</code>, <code>z_ig_kmeans</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne, and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>See Also</h3>

<p>Details, examples, and references are given under <code><a href="#topic+gpcm">gpcm</a></code>, <code><a href="#topic+tpcm">tpcm</a></code>, <code><a href="#topic+ghpcm">ghpcm</a></code>,  <code><a href="#topic+stpcm">stpcm</a></code>, and  <code><a href="#topic+vgpcm">vgpcm</a></code>.
</p>

<hr>
<h2 id='ARI'>Adjusted Rand Index</h2><span id='topic+ARI'></span>

<h3>Description</h3>

<p>Calculates an adjusted for chance Rand index.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARI(x,y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARI_+3A_x">x</code></td>
<td>

<p>predictor class memberships 
</p>
</td></tr>
<tr><td><code id="ARI_+3A_y">y</code></td>
<td>

<p>true class memberships
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sample(1:10, size = 100, replace = TRUE)
y &lt;- sample(1:10, size = 100, replace = TRUE)
ARI(x,y)
</code></pre>

<hr>
<h2 id='e_step'>Expectation Step</h2><span id='topic+e_step'></span>

<h3>Description</h3>

<p>Calculates the expectation of class memberships, and imputes if missing values for a given dataset.</p>


<h3>Usage</h3>

<pre><code class='language-R'>e_step(data, model_obj, start=0, nu = 1.0) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="e_step_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="e_step_+3A_start">start</code></td>
<td>
<p> Start values in this context are only used for imputation. Non-missing values have their expectation of class memberships calculated directly.
If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="e_step_+3A_model_obj">model_obj</code></td>
<td>

<p>A gpcm_best, vgpcm_best, stpcm_best, ghpcm_best, and salpcm_best object class.  
</p>
</td></tr>
<tr><td><code id="e_step_+3A_nu">nu</code></td>
<td>

<p>deterministic annealing for the class membership E-step. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This will only work on a dataset with the same dimension as estimated in the model. <code>e_step</code> will also work for missing values, provided that there is at least one non-missing entry. 
</p>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>A matrix of the original dataset plus imputed values if applicable.</p>
</td></tr>
<tr><td><code>origX</code></td>
<td>
<p>A matrix of the original dataset including missing values.</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# load dataset and perform model search. 

data(x2)
data_in &lt;- matrix(x2,ncol = 2)
mm &lt;- mixture::gpcm(data = data_in,G = 1:7,
           start = 0,
           veo = FALSE,pprogress=FALSE)

# get best model 
best = get_best_model(mm)
best 

# lets try imputing some missing data. 
x2NA &lt;- x2
x2NA[5,1] &lt;- NA
x2NA[140,2] &lt;- NA
x2NA[99,1] &lt;- NA

# calculate expectation
expect &lt;- e_step(data=x2NA,start = 0,nu = 1.0,model_obj = best)

# plot imputed entries and compare with original 
plot(x2,col = "grey")
points(expect$X[expect$row_tags+1,],col = "blue", pch = 20,cex = 2) # blue are imputed values.
points(x2[expect$row_tags+1,], col = "red" , pch = 20,cex = 2) # red are original values.
legend(-2,2,legend = c("imputed","original"),col = c("blue","red"),pch = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_best_model'>Best Model Extractor</h2><span id='topic+get_best_model'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Gaussian clustering models (GPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_best_model(gpcm_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_best_model_+3A_gpcm_model">gpcm_model</code></td>
<td>
<p>An input of class <code>gpcm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extracts the best model based on BIC. 
</p>


<h3>Value</h3>

<p>An object of class <code>gpcm_best</code> is a list with components:
</p>
<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# load dataset and perform model search. 
data(x2)
data_in &lt;- matrix(x2,ncol = 2)
mm &lt;- mixture::gpcm(data = data_in,G = 1:7,
           start = 0,
           veo = FALSE,pprogress=FALSE)

# get best model 
best = get_best_model(mm)
best

## End(Not run)
</code></pre>

<hr>
<h2 id='ghpcm'>Generalized Hyperbolic Parsimonious Clustering Models</h2><span id='topic+ghpcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Generalized Hyperbolic clustering models (GHPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>ghpcm(data=NULL, G=1:3, mnames=NULL,
		start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=FALSE, stochastic = FALSE, seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ghpcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_stochastic">stochastic</code></td>
<td>

<p>If <code>TRUE</code> , it will run stochastic E step variant. 
</p>
</td></tr>
<tr><td><code id="ghpcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Generalized Hyperbolic mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>ghpcm</code> is a list with components:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>model_objs</code></td>
<td>
<p>A list of all estimated models with parameters returned from the C++ call.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>A class of vgpcm_best containing; the number of groups for the best model, the covariance structure, and Bayesian Information Criterion (BIC) value.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood values from fitting the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>A G by mnames by 3 dimensional array with values pertaining to BIC calculations. (legacy)</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>gpar</code></td>
<td>
<p>A list object for each cluster pertaining to parameters. (legacy)</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Best Model</h4>

<p>An object of class <code>ghpcm_best</code> is a list with components:
</p>

<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Internal Objects</h4>

<p>All classes contain an internal list called <code>model_obj</code> or <code>model_objs</code> with the following components:
</p>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a posteori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of location vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector containg skewness vectors for each group</p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>A vector containing estimated gamma parameters for each group</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, <code>plot</code> and <code>summary</code> functions are available for objects of class <code>ghpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Browne, R.P. and McNicholas, P.D. (2015), 'A mixture of generalized hyperbolic distributions', Canadian Journal of Statistics 43(2), 176-198. 
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	## Not run: 

data("sx2")


### use random soft initializations. 
ax6 = ghpcm(sx2, G=1:3,start= 0)
summary(ax6)
ax6

### plot results 
plot(sx2,col = ax6$map + 1)

### use deterministic annealing for starting values
axDA = ghpcm(sx2, G=1:3, start=0,da=c(0.3,0.5,0.8,1.0))
summary(axDA)
axDA

	
## End(Not run)
</code></pre>

<hr>
<h2 id='gpcm'>Gaussian Parsimonious Clustering Models</h2><span id='topic+gpcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Gaussian clustering models (GPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpcm(data=NULL, G=1:3, mnames=NULL,
		start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=TRUE, stochastic = FALSE, seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_stochastic">stochastic</code></td>
<td>

<p>If <code>TRUE</code> , it will run stochastic E step variant. 
</p>
</td></tr>
<tr><td><code id="gpcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Gaussian mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>gpcm</code> is a list with components:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>model_objs</code></td>
<td>
<p>A list of all estimated models with parameters returned from the C++ call.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>A class of gpcm_best containing; the number of groups for the best model, the covariance structure, and Bayesian Information Criterion (BIC) value.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood values from fitting the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>A G by mnames by 3 dimensional array with values pertaining to BIC calculations. (legacy)</p>
</td></tr>
<tr><td><code>gpar</code></td>
<td>
<p>A list object for each cluster pertaining to parameters. (legacy)</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Best Model</h4>

<p>An object of class <code>gpcm_best</code> is a list with components:
</p>

<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>labs</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Internal Objects</h4>

<p>All classes contain an internal list called <code>model_obj</code> or <code>model_objs</code> with the following components:
</p>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a posteori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of mean vectors for each group</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, <code>plot</code> and <code>summary</code> functions are available for objects of class <code>gpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	## Not run: 

data("x2")
### use kmeans to find starting values
ax0 = gpcm(x2, G=1:5, mnames=c("VVV", "EVE"),start=2, pprogress=TRUE, atol=1e-2)
summary(ax0)
ax0

### use random soft initializations. 
ax6 = gpcm(x2, G=1:5, mnames=c("VVV", "EVE"),start= 0)
summary(ax6)
ax6

### use deterministic annealing for starting values
axDA = gpcm(x2, G=1:5, mnames=c("VVV", "EVE"), start=0,da=c(0.3,0.5,0.8,1.0))
summary(axDA)
axDA

### estimate all 14 covariance structures 
ax = gpcm(x2, G=1:5, mnames=NULL, start=0)
summary(ax)
ax

### model based classification
x2.label = numeric(nrow(x2))
x2.label[c(10,50, 110, 150, 210, 250)] = c(1,1,2,2,3,3)
axl = gpcm(x2, G=3, mnames=c("VVV", "EVE"), label=x2.label)
summary(axl)

plot(x2, col = axl$map + 1)
	
## End(Not run)
</code></pre>

<hr>
<h2 id='main_loop'>GPCM Internal C++ Call</h2><span id='topic+main_loop'></span>

<h3>Description</h3>

<p>This function is the internal C++ function call within the <code>gpcm</code> function. 
This is a raw C++ function call,  meaning it has no checks for proper inputs so it may fail to run without giving proper errors. 
Please ensure all arguements are valid. <code>main_loop</code> is useful for writing parallizations of the gpcm function. All arguement descriptions are given in terms of their corresponding C++ types.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>main_loop(X, G, model_id, 
        model_type, in_zigs, 
        in_nmax, in_l_tol, in_m_iter_max,
        in_m_tol, anneals, t_burn = 5L) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="main_loop_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_g">G</code></td>
<td>

<p>A single positive integer value representing number of groups. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_model_id">model_id</code></td>
<td>

<p>An integer representing the model_id, is useful for keeping track within parallizations. Not to be confused with model_type.  
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_model_type">model_type</code></td>
<td>

<p>The type of covariance model you wish to run. Lexicon is given as follows: 
&quot;0&quot; = &quot;EII&quot;, &quot;1&quot; = &quot;VII&quot;,  &quot;2&quot; = &quot;EEI&quot; ,  &quot;3&quot; = &quot;EVI&quot;, &quot;4&quot; = &quot;VEI&quot;,  &quot;5&quot; = &quot;VVI&quot;, &quot;6&quot; = &quot;EEE&quot;,  
&quot;7&quot; = &quot;VEE&quot;, &quot;8&quot; = &quot;EVE&quot;, &quot;9&quot; = &quot;EEV&quot;, &quot;10&quot; = &quot;VVE&quot;, &quot;11&quot; = &quot;EVV&quot;, &quot;12&quot; = &quot;VEV&quot;, &quot;13&quot; = &quot;VVV&quot;
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_in_zigs">in_zigs</code></td>
<td>

<p>A n times G a posteriori matrix resembling the probability of observation i belonging to group G. Rows must sum to one, have the proper dimensions, and be positive.  
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_in_nmax">in_nmax</code></td>
<td>

<p>Positive integer value resembling the maximum amount of iterations for the EM. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_in_l_tol">in_l_tol</code></td>
<td>

<p>A likelihood tolerance for convergence. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_in_m_iter_max">in_m_iter_max</code></td>
<td>

<p>For certain models, where applicable, the number of iterations for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_in_m_tol">in_m_tol</code></td>
<td>

<p>For certain models, where applicable, the tolerance for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_anneals">anneals</code></td>
<td>

<p>A vector of doubles representing the deterministic annealing settings.
</p>
</td></tr>
<tr><td><code id="main_loop_+3A_t_burn">t_burn</code></td>
<td>

<p>A positive integer representing the number of burn steps if missing data (NAs) are detected. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be extremly careful running this function, it is known to crash systems without proper exception handling. Consider using the package <code>parallel</code> to estimate all possible models at the same time. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a postereori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group (note you may have to reshape this)</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of mean vectors for each group</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

data("x2")
data_in = as.matrix(x2,ncol = 2)
n_iter = 1000

in_g = 3
n = dim(data_in)[1]
model_string &lt;- "VVE"
in_model_type &lt;- switch(model_string, "EII" = 0,"VII" = 1,  
              "EEI" = 2,  "EVI" = 3,  "VEI" = 4,  "VVI" = 5,  "EEE" = 6,  
              "VEE" = 7,  "EVE" = 8,  "EEV" = 9,  "VVE" = 10,
              "EVV" = 11,"VEV" = 12,"VVV" = 13)

zigs_in &lt;- z_ig_random_soft(n,in_g)

m2 = main_loop(X = data_in, # data in
               G = 3, # number of groups
               model_id = 1, # model id for parallelization later
               model_type = in_model_type,
               in_zigs = zigs_in, # initializaiton
               in_nmax = n_iter, # number of iterations
               in_l_tol = 1e-12, # likilihood tolerance
               in_m_iter_max = 20, # maximium iterations for matrices
               in_m_tol = 1e-8,
               anneals=c(0.5,0.7,0.9,1)) 

plot(data_in,col = MAP(m2$zigs) + 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='main_loop_gh'>GHPCM Internal C++ Call</h2><span id='topic+main_loop_gh'></span>

<h3>Description</h3>

<p>This function is the internal C++ function call within the <code>ghpcm</code> function. 
This is a raw C++ function call,  meaning it has no checks for proper inputs so it may fail to run without giving proper errors. 
Please ensure all arguements are valid. <code>main_loop_gh</code> is useful for writing parallizations of the ghpcm function. All arguement descriptions are given in terms of their corresponding C++ types.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>main_loop_gh(X, G, model_id, 
        model_type, in_zigs, 
        in_nmax, in_l_tol, in_m_iter_max,
        in_m_tol, anneals, t_burn = 5L) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="main_loop_gh_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_g">G</code></td>
<td>

<p>A single positive integer value representing number of groups. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_model_id">model_id</code></td>
<td>

<p>An integer representing the model_id, is useful for keeping track within parallizations. Not to be confused with model_type.  
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_model_type">model_type</code></td>
<td>

<p>The type of covariance model you wish to run. Lexicon is given as follows: 
&quot;0&quot; = &quot;EII&quot;, &quot;1&quot; = &quot;VII&quot;,  &quot;2&quot; = &quot;EEI&quot; ,  &quot;3&quot; = &quot;EVI&quot;, &quot;4&quot; = &quot;VEI&quot;,  &quot;5&quot; = &quot;VVI&quot;, &quot;6&quot; = &quot;EEE&quot;,  
&quot;7&quot; = &quot;VEE&quot;, &quot;8&quot; = &quot;EVE&quot;, &quot;9&quot; = &quot;EEV&quot;, &quot;10&quot; = &quot;VVE&quot;, &quot;11&quot; = &quot;EVV&quot;, &quot;12&quot; = &quot;VEV&quot;, &quot;13&quot; = &quot;VVV&quot;
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_in_zigs">in_zigs</code></td>
<td>

<p>A n times G a posteriori matrix resembling the probability of observation i belonging to group G. Rows must sum to one, have the proper dimensions, and be positive.  
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_in_nmax">in_nmax</code></td>
<td>

<p>Positive integer value resembling the maximum amount of iterations for the EM. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_in_l_tol">in_l_tol</code></td>
<td>

<p>A likelihood tolerance for convergence. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_in_m_iter_max">in_m_iter_max</code></td>
<td>

<p>For certain models, where applicable, the number of iterations for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_in_m_tol">in_m_tol</code></td>
<td>

<p>For certain models, where applicable, the tolerance for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_anneals">anneals</code></td>
<td>

<p>A vector of doubles representing the deterministic annealing settings.
</p>
</td></tr>
<tr><td><code id="main_loop_gh_+3A_t_burn">t_burn</code></td>
<td>

<p>A positive integer representing the number of burn steps if missing data (NAs) are detected. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be extremly careful running this function, it is known to crash systems without proper exception handling. Consider using the package <code>parallel</code> to estimate all possible models at the same time.
Or run several possible initializations with random seeds.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a postereori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group (note you may have to reshape this)</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of locational vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector of skewness vectors for each group</p>
</td></tr>
<tr><td><code>omegas</code></td>
<td>
<p>First set of gamma parameters for each group</p>
</td></tr>
<tr><td><code>lambdas</code></td>
<td>
<p>Second set of gamma parameters for each group</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Browne, R.P. and McNicholas, P.D. (2015), 'A mixture of generalized hyperbolic distributions', Canadian Journal of Statistics 43(2), 176-198.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("sx2")
data_in = as.matrix(sx2,ncol = 2)
n_iter = 300

in_g = 2
n = dim(data_in)[1]
model_string &lt;- "VVV"
in_model_type &lt;- switch(model_string, "EII" = 0,"VII" = 1,  
              "EEI" = 2,  "EVI" = 3,  "VEI" = 4,  "VVI" = 5,  "EEE" = 6,  
              "VEE" = 7,  "EVE" = 8,  "EEV" = 9,  "VVE" = 10,
              "EVV" = 11,"VEV" = 12,"VVV" = 13)

zigs_in &lt;- z_ig_random_soft(n,in_g)

m2 = main_loop_gh(X = t(data_in), # data in has to be in column major form 
               G = 2, # number of groups
               model_id = 1, # model id for parallelization later
               model_type = in_model_type,
               in_zigs = zigs_in, # initializaiton
               in_nmax = n_iter, # number of iterations
               in_l_tol = 1e-8, # likilihood tolerance
               in_m_iter_max = 20, # maximium iterations for matrices
               in_m_tol = 1e-8,
               anneals=c(0.5,0.7,0.9,1))
                
plot(sx2,col = MAP(m2$zigs) + 1, cex = 0.5, pch = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='main_loop_st'>STPCM Internal C++ Call</h2><span id='topic+main_loop_st'></span>

<h3>Description</h3>

<p>This function is the internal C++ function call within the <code>stpcm</code> function. 
This is a raw C++ function call,  meaning it has no checks for proper inputs so it may fail to run without giving proper errors. 
Please ensure all arguements are valid. <code>main_loop_st</code> is useful for writing parallizations of the stpcm function. All arguement descriptions are given in terms of their corresponding C++ types.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>main_loop_st(X, G, model_id, 
        model_type, in_zigs, 
        in_nmax, in_l_tol, in_m_iter_max,
        in_m_tol, anneals,
        latent_step="standard", 
        t_burn = 5L) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="main_loop_st_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_g">G</code></td>
<td>

<p>A single positive integer value representing number of groups. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_model_id">model_id</code></td>
<td>

<p>An integer representing the model_id, is useful for keeping track within parallizations. Not to be confused with model_type.  
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_model_type">model_type</code></td>
<td>

<p>The type of covariance model you wish to run. Lexicon is given as follows: 
&quot;0&quot; = &quot;EII&quot;, &quot;1&quot; = &quot;VII&quot;,  &quot;2&quot; = &quot;EEI&quot; ,  &quot;3&quot; = &quot;EVI&quot;, &quot;4&quot; = &quot;VEI&quot;,  &quot;5&quot; = &quot;VVI&quot;, &quot;6&quot; = &quot;EEE&quot;,  
&quot;7&quot; = &quot;VEE&quot;, &quot;8&quot; = &quot;EVE&quot;, &quot;9&quot; = &quot;EEV&quot;, &quot;10&quot; = &quot;VVE&quot;, &quot;11&quot; = &quot;EVV&quot;, &quot;12&quot; = &quot;VEV&quot;, &quot;13&quot; = &quot;VVV&quot;
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_in_zigs">in_zigs</code></td>
<td>

<p>A n times G a posteriori matrix resembling the probability of observation i belonging to group G. Rows must sum to one, have the proper dimensions, and be positive.  
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_in_nmax">in_nmax</code></td>
<td>

<p>Positive integer value resembling the maximum amount of iterations for the EM. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_in_l_tol">in_l_tol</code></td>
<td>

<p>A likelihood tolerance for convergence. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_in_m_iter_max">in_m_iter_max</code></td>
<td>

<p>For certain models, where applicable, the number of iterations for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_in_m_tol">in_m_tol</code></td>
<td>

<p>For certain models, where applicable, the tolerance for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_anneals">anneals</code></td>
<td>

<p>A vector of doubles representing the deterministic annealing settings.
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_t_burn">t_burn</code></td>
<td>

<p>A positive integer representing the number of burn steps if missing data (NAs) are detected. 
</p>
</td></tr>
<tr><td><code id="main_loop_st_+3A_latent_step">latent_step</code></td>
<td>

<p>If <code>"standard"</code>, it will use the standard E step for latent variable of a Normal Variance Mean Mixture, if <code>"random"</code> it will run a random draw from a GIG distribution.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be extremly careful running this function, it is known to crash systems without proper exception handling. Consider using the package <code>parallel</code> to estimate all possible models at the same time.
Or run several possible initializations with random seeds.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a postereori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group (note you may have to reshape this)</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of locational vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector of skewness vectors for each group</p>
</td></tr>
<tr><td><code>vgs</code></td>
<td>
<p>Gamma parameters for each group</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Wei, Y., Tang, Y. and McNicholas, P.D. (2019), 'Mixtures of generalized hyperbolic distributions and mixtures of skew-t distributions for model-based clustering with incomplete data', Computational Statistics and Data Analysis 130, 18-41.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("sx2")
data_in = as.matrix(sx2,ncol = 2)
n_iter = 300

in_g = 2
n = dim(data_in)[1]
model_string &lt;- "VEI"
in_model_type &lt;- switch(model_string, "EII" = 0,"VII" = 1,  
              "EEI" = 2,  "EVI" = 3,  "VEI" = 4,  "VVI" = 5,  "EEE" = 6,  
              "VEE" = 7,  "EVE" = 8,  "EEV" = 9,  "VVE" = 10,
              "EVV" = 11,"VEV" = 12,"VVV" = 13)

zigs_in &lt;- z_ig_random_soft(n,in_g)

m2 = main_loop_st(X = t(data_in), # data in has to be in column major form
               G = 2, # number of groups
               model_id = 1, # model id for parallelization later
               model_type = in_model_type,
               in_zigs = zigs_in, # initializaiton
               in_nmax = n_iter, # number of iterations
               in_l_tol = 0.5, # likilihood tolerance
               in_m_iter_max = 20, # maximium iterations for matrices
               anneals=c(1),
               in_m_tol = 1e-8) 

plot(sx2,col = MAP(m2$zigs) + 1, cex = 0.5, pch = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='main_loop_t'>TPCM Internal C++ Call</h2><span id='topic+main_loop_t'></span>

<h3>Description</h3>

<p>This function is the internal C++ function call within the <code>stpcm</code> function. 
This is a raw C++ function call,  meaning it has no checks for proper inputs so it may fail to run without giving proper errors. 
Please ensure all arguements are valid. <code>main_loop_st</code> is useful for writing parallizations of the stpcm function. All arguement descriptions are given in terms of their corresponding C++ types.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>main_loop_t(X, G, model_id, 
        model_type, in_zigs, 
        in_nmax, in_l_tol, in_m_iter_max,
        in_m_tol, anneals, t_burn = 5L) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="main_loop_t_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_g">G</code></td>
<td>

<p>A single positive integer value representing number of groups. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_model_id">model_id</code></td>
<td>

<p>An integer representing the model_id, is useful for keeping track within parallizations. Not to be confused with model_type.  
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_model_type">model_type</code></td>
<td>

<p>The type of covariance model you wish to run. Lexicon is given as follows: 
&quot;0&quot; = &quot;EII&quot;, &quot;1&quot; = &quot;VII&quot;,  &quot;2&quot; = &quot;EEI&quot; ,  &quot;3&quot; = &quot;EVI&quot;, &quot;4&quot; = &quot;VEI&quot;,  &quot;5&quot; = &quot;VVI&quot;, &quot;6&quot; = &quot;EEE&quot;,  
&quot;7&quot; = &quot;VEE&quot;, &quot;8&quot; = &quot;EVE&quot;, &quot;9&quot; = &quot;EEV&quot;, &quot;10&quot; = &quot;VVE&quot;, &quot;11&quot; = &quot;EVV&quot;, &quot;12&quot; = &quot;VEV&quot;, &quot;13&quot; = &quot;VVV&quot;
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_in_zigs">in_zigs</code></td>
<td>

<p>A n times G a posteriori matrix resembling the probability of observation i belonging to group G. Rows must sum to one, have the proper dimensions, and be positive.  
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_in_nmax">in_nmax</code></td>
<td>

<p>Positive integer value resembling the maximum amount of iterations for the EM. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_in_l_tol">in_l_tol</code></td>
<td>

<p>A likelihood tolerance for convergence. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_in_m_iter_max">in_m_iter_max</code></td>
<td>

<p>For certain models, where applicable, the number of iterations for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_in_m_tol">in_m_tol</code></td>
<td>

<p>For certain models, where applicable, the tolerance for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_anneals">anneals</code></td>
<td>

<p>A vector of doubles representing the deterministic annealing settings.
</p>
</td></tr>
<tr><td><code id="main_loop_t_+3A_t_burn">t_burn</code></td>
<td>

<p>A positive integer representing the number of burn steps if missing data (NAs) are detected. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be extremly careful running this function, it is known to crash systems without proper exception handling. Consider using the package <code>parallel</code> to estimate all possible models at the same time.
Or run several possible initializations with random seeds.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a postereori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group (note you may have to reshape this)</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of locational vectors for each group</p>
</td></tr>
<tr><td><code>vgs</code></td>
<td>
<p>Gamma parameters for each group</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>
<p>Andrews, J.L. and McNicholas, P.D. (2012), 'Model-based clustering, classification, and discriminant analysis via mixtures of multivariate t-distributions', Statistics and Computing 22(5), 1021-1029.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("x2")
data_in = as.matrix(x2,ncol = 2)
n_iter = 300

in_g = 3
n = dim(data_in)[1]
model_string &lt;- "VEI"
in_model_type &lt;- switch(model_string, "EII" = 0,"VII" = 1,  
              "EEI" = 2,  "EVI" = 3,  "VEI" = 4,  "VVI" = 5,  "EEE" = 6,  
              "VEE" = 7,  "EVE" = 8,  "EEV" = 9,  "VVE" = 10,
              "EVV" = 11,"VEV" = 12,"VVV" = 13)

zigs_in &lt;- z_ig_random_soft(n,in_g)

m2 = main_loop_t(X = data_in, 
               G = 3, # number of groups
               model_id = 1, # model id for parallelization later
               model_type = in_model_type,
               in_zigs = zigs_in, # initializaiton
               in_nmax = n_iter, # number of iterations
               in_l_tol = 0.5, # likilihood tolerance
               in_m_iter_max = 20, # maximium iterations for matrices
               anneals=c(1),
               in_m_tol = 1e-8) 

plot(x2,col = MAP(m2$zigs) + 1, cex = 0.5, pch = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='main_loop_vg'>VGPCM Internal C++ Call</h2><span id='topic+main_loop_vg'></span>

<h3>Description</h3>

<p>This function is the internal C++ function call within the <code>vgpcm</code> function. 
This is a raw C++ function call,  meaning it has no checks for proper inputs so it may fail to run without giving proper errors. 
Please ensure all arguements are valid. <code>main_loop_vg</code> is useful for writing parallizations of the stpcm function. All arguement descriptions are given in terms of their corresponding C++ types.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>main_loop_vg(X, G, model_id, 
        model_type, in_zigs, 
        in_nmax, in_l_tol, in_m_iter_max,
        in_m_tol, anneals,
        latent_step="standard", 
        t_burn = 5L) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="main_loop_vg_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_g">G</code></td>
<td>

<p>A single positive integer value representing number of groups. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_model_id">model_id</code></td>
<td>

<p>An integer representing the model_id, is useful for keeping track within parallizations. Not to be confused with model_type.  
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_model_type">model_type</code></td>
<td>

<p>The type of covariance model you wish to run. Lexicon is given as follows: 
&quot;0&quot; = &quot;EII&quot;, &quot;1&quot; = &quot;VII&quot;,  &quot;2&quot; = &quot;EEI&quot; ,  &quot;3&quot; = &quot;EVI&quot;, &quot;4&quot; = &quot;VEI&quot;,  &quot;5&quot; = &quot;VVI&quot;, &quot;6&quot; = &quot;EEE&quot;,  
&quot;7&quot; = &quot;VEE&quot;, &quot;8&quot; = &quot;EVE&quot;, &quot;9&quot; = &quot;EEV&quot;, &quot;10&quot; = &quot;VVE&quot;, &quot;11&quot; = &quot;EVV&quot;, &quot;12&quot; = &quot;VEV&quot;, &quot;13&quot; = &quot;VVV&quot;
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_in_zigs">in_zigs</code></td>
<td>

<p>A n times G a posteriori matrix resembling the probability of observation i belonging to group G. Rows must sum to one, have the proper dimensions, and be positive.  
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_in_nmax">in_nmax</code></td>
<td>

<p>Positive integer value resembling the maximum amount of iterations for the EM. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_in_l_tol">in_l_tol</code></td>
<td>

<p>A likelihood tolerance for convergence. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_in_m_iter_max">in_m_iter_max</code></td>
<td>

<p>For certain models, where applicable, the number of iterations for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_in_m_tol">in_m_tol</code></td>
<td>

<p>For certain models, where applicable, the tolerance for the maximization step. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_anneals">anneals</code></td>
<td>

<p>A vector of doubles representing the deterministic annealing settings.
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_t_burn">t_burn</code></td>
<td>

<p>A positive integer representing the number of burn steps if missing data (NAs) are detected. 
</p>
</td></tr>
<tr><td><code id="main_loop_vg_+3A_latent_step">latent_step</code></td>
<td>

<p>If <code>"standard"</code>, it will use the standard E step for latent variable of a Normal Variance Mean Mixture, if <code>"random"</code> it will run a random draw from a GIG distribution.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be extremly careful running this function, it is known to crash systems without proper exception handling. Consider using the package <code>parallel</code> to estimate all possible models at the same time.
Or run several possible initializations with random seeds.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a postereori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group (note you may have to reshape this)</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of locational vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector of skewness vectors for each group</p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>Gamma parameters for each group</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("sx2")
data_in = as.matrix(sx2,ncol = 2)
n_iter = 300

in_g = 2
n = dim(data_in)[1]
model_string &lt;- "VVV"
in_model_type &lt;- switch(model_string, "EII" = 0,"VII" = 1,  
              "EEI" = 2,  "EVI" = 3,  "VEI" = 4,  "VVI" = 5,  "EEE" = 6,  
              "VEE" = 7,  "EVE" = 8,  "EEV" = 9,  "VVE" = 10,
              "EVV" = 11,"VEV" = 12,"VVV" = 13)

zigs_in &lt;- z_ig_random_soft(n,in_g)

m2 = main_loop_vg(X = t(data_in), # data in has to be in column major form 
               G = 2, # number of groups
               model_id = 1, # model id for parallelization later
               model_type = in_model_type,
               in_zigs = zigs_in, # initializaiton
               in_nmax = n_iter, # number of iterations
               in_l_tol = 0.5, # likilihood tolerance
               in_m_iter_max = 20, # maximium iterations for matrices
               anneals=c(1),
               in_m_tol = 1e-8) 

plot(sx2,col = MAP(m2$zigs) + 1, cex = 0.5, pch = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='MAP'>Maximum <em>a posterori</em></h2><span id='topic+MAP'></span>

<h3>Description</h3>

<p>Generates labels from a classification matrix z</p>


<h3>Usage</h3>

<pre><code class='language-R'>  MAP(z_ig)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAP_+3A_z_ig">z_ig</code></td>
<td>

<p>A classification matrix of positive numbers in which all rows must sum to one. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix is returned of size n times g, with row sums adding up to 1. 
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 

# Simple example. 
MAP(z_ig_random_soft(100,2))

# import dataset. 
data(x2)
mm &lt;- gpcm(data = as.matrix(x2),G = 1:7,
           start = 2,
           veo = FALSE,pprogress=FALSE)

best = get_best_model(mm)
# You can get labels using the internal object with MAP.
labs &lt;- MAP(best$model_obj[[1]]$zigs)
# or you can just get labels directly. 
labs2 &lt;- best$map
  
## End(Not run)
</code></pre>

<hr>
<h2 id='pcm'>Parsimonious Clustering Models</h2><span id='topic+pcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious settings with any one of the GPCM,STPCM,VGPCM, or GHPCM families.</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcm(data=NULL, G=1:3, pcmfamily=c(gpcm,vgpcm,tpcm),
		mnames=NULL, start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=FALSE, seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_pcmfamily">pcmfamily</code></td>
<td>

<p>The family of models to be used. If <code>NULL</code> then all are fitted. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="pcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the EM algorithms.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="pcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="pcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Skew-t mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>pcm</code> is a list with components:
</p>
<table>
<tr><td><code>gpcm</code></td>
<td>
<p>If applicable, the output of running the Gaussian Parsimonious Family.</p>
</td></tr>
<tr><td><code>vgpcm</code></td>
<td>
<p>If applicable, the output of running the Variance-Gamma Parsimonious Family.</p>
</td></tr>
<tr><td><code>stpcm</code></td>
<td>
<p>If applicable, the output of running the Skew-T Parsimonious Family.</p>
</td></tr>
<tr><td><code>ghpcm</code></td>
<td>
<p>If applicable, the output of running the Generalized Hyperbolic Parsimonious Family.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>An object of corresponding to the output of the best performing family.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, and <code>summary</code> functions are available for objects of class <code>pcm</code>, <code>gpcm</code>, <code>ghpcm</code>, <code>stpcm</code>, or <code>vgpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Browne, R.P. and McNicholas, P.D. (2015), 'A mixture of generalized hyperbolic distributions', Canadian Journal of Statistics 43(2), 176-198. 
</p>
<p>Wei, Y., Tang, Y. and McNicholas, P.D. (2019), 'Mixtures of generalized hyperbolic distributions and mixtures of skew-t distributions for model-based clustering with incomplete data', Computational Statistics and Data Analysis 130, 18-41.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("x2")

## Not run: 

### estimate "VVV" "EVE"
ax = pcm(sx3, G=1:3, mnames=c("VVV","EVE"), start=0)
summary(ax)

print(ax)


## End(Not run)

</code></pre>

<hr>
<h2 id='stpcm'>Skew-t Parsimonious Clustering Models</h2><span id='topic+stpcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Skew-t clustering models (STPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>stpcm(data=NULL, G=1:3, mnames=NULL,
		start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=FALSE, 
		stochastic = FALSE, latent_method="standard", seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stpcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the EM algorithms.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_stochastic">stochastic</code></td>
<td>

<p>If <code>TRUE</code> , it will run stochastic E step variant. 
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_latent_method">latent_method</code></td>
<td>

<p>If <code>"standard"</code>, it will use the standard E step for latent variable of a Normal Variance Mean Mixture, if <code>"random"</code> it will run a random draw from a GIG distribution.
</p>
</td></tr>
<tr><td><code id="stpcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Skew-t mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>vgpcm</code> is a list with components:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>model_objs</code></td>
<td>
<p>A list of all estimated models with parameters returned from the C++ call.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>A class of vgpcm_best containing; the number of groups for the best model, the covariance structure, and Bayesian Information Criterion (BIC) value.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood values from fitting the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>A G by mnames by 3 dimensional array with values pertaining to BIC calculations. (legacy)</p>
</td></tr>
<tr><td><code>gpar</code></td>
<td>
<p>A list object for each cluster pertaining to parameters. (legacy)</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Best Model</h4>

<p>An object of class <code>stpcm_best</code> is a list with components:
</p>

<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Internal Objects</h4>

<p>All classes contain an internal list called <code>model_obj</code> or <code>model_objs</code> with the following components:
</p>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a posteori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of location vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector containg skewness vectors for each group</p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>A vector containing estimated gamma parameters for each group</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, <code>plot</code> and <code>summary</code> functions are available for objects of class <code>vgpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Wei, Y., Tang, Y. and McNicholas, P.D. (2019), 'Mixtures of generalized hyperbolic distributions and mixtures of skew-t distributions for model-based clustering with incomplete data', Computational Statistics and Data Analysis 130, 18-41.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("sx3")

## Not run: 

### estimate "VVV" "EVE"
ax = stpcm(sx3, G=1:3, mnames=c("VVV","EVE"), start=0)
summary(ax)
ax


### estimate all 14 covariance structures 
ax = stpcm(sx3, G=1:3, mnames=NULL, start=0)
summary(ax)
ax

### model based classification
sx3.label = c(rep(1,1000),rep(2,1000))
plot(sx3, col=sx3.label)
axl = stpcm(sx3, G=2, mnames=c("VVV", "EVE"), label=sx3.label)
summary(axl)


## End(Not run)

</code></pre>

<hr>
<h2 id='sx2'>Skewed Simulated Data 1</h2><span id='topic+sx2'></span>

<h3>Description</h3>

<p>Simulated data, with two variables and two groups, used to illustrate <code><a href="#topic+ghpcm">ghpcm</a>,<a href="#topic+stpcm">stpcm</a>,<a href="#topic+vgpcm">vgpcm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sx2)</code></pre>


<h3>Format</h3>

<p>A data frame with 2000 observations and 2 columns. 
</p>


<h3>Source</h3>

<p>These data were simulated using <code>R</code>.
</p>

<hr>
<h2 id='sx3'>Skewed Simulated Data 2</h2><span id='topic+sx3'></span>

<h3>Description</h3>

<p>Simulated data, with two variables and two groups, that are close together, used to illustrate <code><a href="#topic+ghpcm">ghpcm</a>,<a href="#topic+stpcm">stpcm</a>,<a href="#topic+vgpcm">vgpcm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sx3)</code></pre>


<h3>Format</h3>

<p>A data frame with 2000 observations and 2 columns. 
</p>


<h3>Source</h3>

<p>These data were simulated using <code>R</code>.
</p>

<hr>
<h2 id='tpcm'>Student T Parsimonious Clustering Models</h2><span id='topic+tpcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Student T clustering models (TPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>tpcm(data=NULL, G=1:3, mnames=NULL,
		start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=FALSE, stochastic=FALSE, 
		constrained = FALSE, seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tpcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the EM algorithms.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_stochastic">stochastic</code></td>
<td>

<p>If <code>TRUE</code> , it will run stochastic E step variant. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_constrained">constrained</code></td>
<td>

<p>If <code>TRUE</code>, it will constrain the degrees of freedom for student-t to be the same for all clusters. 
</p>
</td></tr>
<tr><td><code id="tpcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Skew-t mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>tpcm</code> is a list with components:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>model_objs</code></td>
<td>
<p>A list of all estimated models with parameters returned from the C++ call.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>A class of vgpcm_best containing; the number of groups for the best model, the covariance structure, and Bayesian Information Criterion (BIC) value.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood values from fitting the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>A G by mnames by 3 dimensional array with values pertaining to BIC calculations. (legacy)</p>
</td></tr>
<tr><td><code>gpar</code></td>
<td>
<p>A list object for each cluster pertaining to parameters. (legacy)</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Best Model</h4>

<p>An object of class <code>stpcm_best</code> is a list with components:
</p>

<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Internal Objects</h4>

<p>All classes contain an internal list called <code>model_obj</code> or <code>model_objs</code> with the following components:
</p>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a posteori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of location vectors for each group</p>
</td></tr>
<tr><td><code>vgs</code></td>
<td>
<p>A vector containing estimated gamma parameters for each group</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, <code>plot</code> and <code>summary</code> functions are available for objects of class <code>vgpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Andrews, J.L. and McNicholas, P.D. (2012), 'Model-based clustering, classification, and discriminant analysis via mixtures of multivariate t-distributions', Statistics and Computing 22(5), 1021-1029.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("x2")

## Not run: 

### estimate "VVV" "EVE"
ax = tpcm(x2, G=1:3, mnames=c("VVV","EVE"), start=0)
summary(ax)
ax


### estimate all 14 covariance structures 
ax = tpcm(x2, G=1:3, mnames=NULL, start=0)
summary(ax)
ax


## End(Not run)

</code></pre>

<hr>
<h2 id='vgpcm'>Variance Gamma Parsimonious Clustering Models</h2><span id='topic+vgpcm'></span>

<h3>Description</h3>

<p>Carries out model-based clustering or classification using some or all of the 14 parsimonious Variance Gamma clustering models (VGPCM).</p>


<h3>Usage</h3>

<pre><code class='language-R'>vgpcm(data=NULL, G=1:3, mnames=NULL,
		start=2, label=NULL, 
		veo=FALSE, da=c(1.0),
		nmax=1000, atol=1e-8, mtol=1e-8, mmax=10, burn=5,
		pprogress=FALSE, pwarning=FALSE, 
		stochastic = FALSE, latent_method="standard", seed=123) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vgpcm_+3A_data">data</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_g">G</code></td>
<td>

<p>A sequence of integers giving the number of components to be used.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_mnames">mnames</code></td>
<td>

<p>The models (i.e., covariance structures) to be used. If <code>NULL</code> then all 14 are fitted. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_start">start</code></td>
<td>

<p>If <code>0</code> then the random soft function is used for initialization.
If <code>1</code> then the random hard function is used for initialization.
If <code>2</code> then the kmeans function is used for initialization. 
If <code>&gt;2</code> then multiple random soft starts are used for initialization. 
If <code>is.matrix</code> then matrix is used as an initialization matrix as along as it has non-negative elements. Note: only models with the same number of columns of this matrix will be fit.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_label">label</code></td>
<td>

<p>If <code>NULL</code> then the data has no known groups.
If <code>is.integer</code> then some of the observations have known groups. If <code>label[i]=k</code> then observation belongs to group  <code>k</code>. If <code>label[i]=0</code> then observation has no known group. See Examples. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_veo">veo</code></td>
<td>

<p>Stands for &quot;Variables exceed observations&quot;. If <code>TRUE</code> then if the number variables in the model exceeds the number of observations the model is still fitted.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_da">da</code></td>
<td>

<p>Stands for Determinstic Annealing. A vector of doubles. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_nmax">nmax</code></td>
<td>

<p>The maximum number of iterations each EM algorithm is allowed to use. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_atol">atol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the EM algorithms. For each algorithm, the criterion is based on the difference between the log-likelihood at an iteration and an asymptotic estimate of the log-likelihood at that iteration. This asymptotic estimate is based on the Aitken acceleration and details are given in the References. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_mtol">mtol</code></td>
<td>

<p>A number specifying the epsilon value for the convergence criteria used in the M-step in the EM algorithms.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_mmax">mmax</code></td>
<td>

<p>The maximum number of iterations each M-step is allowed in the GEM algorithms.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_burn">burn</code></td>
<td>

<p>The burn in period for imputing data. (Missing observations are removed and a model is estimated seperately before placing an imputation step within the EM.)
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_pprogress">pprogress</code></td>
<td>

<p>If <code>TRUE</code> print the progress of the function.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_pwarning">pwarning</code></td>
<td>

<p>If <code>TRUE</code> print the warnings.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_stochastic">stochastic</code></td>
<td>

<p>If <code>TRUE</code> , it will run stochastic E step variant. 
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_latent_method">latent_method</code></td>
<td>

<p>If <code>"standard"</code>, it will use the standard E step for latent variable of a Normal Variance Mean Mixture, if <code>"random"</code> it will run a random draw from a GIG distribution.
</p>
</td></tr>
<tr><td><code id="vgpcm_+3A_seed">seed</code></td>
<td>

<p>The seed for the run, default is 123
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data <code>x</code> are either clustered or classified using Variance Gamma mixture models with some or all of the 14 parsimonious covariance structures described in Celeux &amp; Govaert (1995). The algorithms given by Celeux &amp; Govaert (1995) is used for 12 of the 14 models; the &quot;EVE&quot; and &quot;VVE&quot; models use the algorithms given in Browne &amp; McNicholas (2014). Starting values are very important to the successful operation of these algorithms and so care must be taken in the interpretation of results. 
</p>


<h3>Value</h3>

<p>An object of class <code>vgpcm</code> is a list with components:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>model_objs</code></td>
<td>
<p>A list of all estimated models with parameters returned from the C++ call.</p>
</td></tr>
<tr><td><code>best_model</code></td>
<td>
<p>A class of vgpcm_best containing; the number of groups for the best model, the covariance structure, and Bayesian Information Criterion (BIC) value.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood values from fitting the best model.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>A matrix giving the raw values upon which <code>map</code> is based.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>A G by mnames by 3 dimensional array with values pertaining to BIC calculations. (legacy)</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>gpar</code></td>
<td>
<p>A list object for each cluster pertaining to parameters. (legacy)</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Best Model</h4>

<p>An object of class <code>vgpcm_best</code> is a list with components:
</p>

<table>
<tr><td><code>model_type</code></td>
<td>
<p>A string containg summarized information about the type of model estimated (Covariance structure and number of groups).</p>
</td></tr>
<tr><td><code>model_obj</code></td>
<td>
<p>An internal list containing all parameters returned from the C++ call. </p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Index Criterion (positive scale, bigger is better).</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log liklihood from the estimated model. </p>
</td></tr>
<tr><td><code>nparam</code></td>
<td>
<p>Number of a parameters in the mode.</p>
</td></tr>
<tr><td><code>startobject</code></td>
<td>
<p>The type of object inputted into <code>start</code>.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>cov_type</code></td>
<td>
<p>A string representing the type of covariance matrix (see 14 models).</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>Convergence status of EM algorithm according to Aitken's Acceleration</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>A vector of integers indicating the maximum <em>a posteriori</em> classifications for the best model.</p>
</td></tr>
<tr><td><code>row_tags</code></td>
<td>
<p>If there were NAs in the original dataset, a vector of indices referencing the row of the imputed vectors is given.</p>
</td></tr>
</table>


<h4>Internal Objects</h4>

<p>All classes contain an internal list called <code>model_obj</code> or <code>model_objs</code> with the following components:
</p>

<table>
<tr><td><code>zigs</code></td>
<td>
<p> a posteori matrix </p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>An integer representing the number of groups.</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>A vector of covariance matrices for each group</p>
</td></tr> 
<tr><td><code>mus</code></td>
<td>
<p>A vector of location vectors for each group</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>A vector containg skewness vectors for each group</p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>A vector containing estimated gamma parameters for each group</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Dedicated <code>print</code>, <code>plot</code> and <code>summary</code> functions are available for objects of class <code>vgpcm</code>.
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>McNicholas, P.D. (2016), <em>Mixture Model-Based Classification</em>. Boca Raton: Chapman &amp; Hall/CRC Press
</p>
<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("sx2")
### use kmeans to find starting values
ax0 = vgpcm(sx2, G=1:3, mnames=c("VVV", "EVE"),start=2, pprogress=TRUE, atol=1e-2)
summary(ax0)
ax0

### use random soft initializations. 
ax6 = vgpcm(sx2, G=1:3, mnames=c("VVV", "EVE"),start= 0)
summary(ax6)
ax6

### use deterministic annealing for starting values
axDA = vgpcm(sx2, G=1:3, mnames=c("VVV", "EVE"), start=0,da=c(0.3,0.5,0.8,1.0))
summary(axDA)
axDA

### estimate all 14 covariance structures 
ax = vgpcm(sx2, G=1:3, mnames=NULL, start=0)
summary(ax)
ax

### model based classification
sx2.label = c(rep(1,1000),rep(2,1000))
plot(sx2, col=sx2.label)
axl = vgpcm(sx2, G=2, mnames=c("VVV", "EVE"), label=sx2.label)
summary(axl)

## End(Not run)
</code></pre>

<hr>
<h2 id='x2'>Simulated Data</h2><span id='topic+x2'></span>

<h3>Description</h3>

<p>Simulated data, with two variables with three groups, used to illustrate <code><a href="#topic+gpcm">gpcm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(x2)</code></pre>


<h3>Format</h3>

<p>A data frame with 300 observations and 2 columns. 
</p>


<h3>Source</h3>

<p>These data were simulated using <code>R</code>.
</p>

<hr>
<h2 id='z_ig_kmeans'>K-means Initialization</h2><span id='topic+z_ig_kmeans'></span>

<h3>Description</h3>

<p>Generates an initialization matrix for a dataset X using k-means.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  z_ig_kmeans(X,g)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z_ig_kmeans_+3A_x">X</code></td>
<td>

<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables. Note that this function currently only works with multivariate data p &gt; 1. Note. NO NAS allowed. 
</p>
</td></tr>
<tr><td><code id="z_ig_kmeans_+3A_g">g</code></td>
<td>

<p>An integer representing the number of groups. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix is returned of size n times g, with row sums adding up to 1. 
</p>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data("x2")
#z_init &lt;- z_ig_kmeans(x2,g=3)
</code></pre>

<hr>
<h2 id='z_ig_random_hard'>Random Hard Initialization</h2><span id='topic+z_ig_random_hard'></span>

<h3>Description</h3>

<p>Generates an initialization matrix of size n times g using random hard.</p>


<h3>Usage</h3>

<pre><code class='language-R'>z_ig_random_hard(n,g)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z_ig_random_hard_+3A_n">n</code></td>
<td>

<p>Number of rows, must be positive. 
</p>
</td></tr>
<tr><td><code id="z_ig_random_hard_+3A_g">g</code></td>
<td>

<p>Number of columns, must be positive. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z_init &lt;- z_ig_random_hard(100,3)
</code></pre>

<hr>
<h2 id='z_ig_random_soft'>Random Soft Initialization</h2><span id='topic+z_ig_random_soft'></span>

<h3>Description</h3>

<p>Generates an initialization matrix of size n times g using random soft.</p>


<h3>Usage</h3>

<pre><code class='language-R'>z_ig_random_soft(n,g)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z_ig_random_soft_+3A_n">n</code></td>
<td>

<p>Number of rows, must be positive. 
</p>
</td></tr>
<tr><td><code id="z_ig_random_soft_+3A_g">g</code></td>
<td>

<p>Number of columns, must be positive. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nik Pocuca, Ryan P. Browne and Paul D. McNicholas.
</p>
<p>Maintainer: Paul D. McNicholas &lt;mcnicholas@math.mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Browne, R.P. and McNicholas, P.D. (2014). Estimating common principal components in high dimensions. <em>Advances in Data Analysis and Classification</em> <b>8</b>(2), 217-226.
</p>
<p>Zhou, H. and Lange, K. (2010). On the bumpy road to the dominant mode. <em>Scandinavian Journal of Statistics</em> <b>37</b>, 612-631. 
</p>
<p>Celeux, G., Govaert, G. (1995). Gaussian parsimonious clustering models. <em>Pattern Recognition</em> <b>28</b>(5), 781-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z_init &lt;- z_ig_random_soft(100,3)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
