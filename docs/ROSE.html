<!DOCTYPE html><html><head><title>Help for package ROSE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ROSE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ROSE-package'>
<p>ROSE: Random Over-Sampling Examples</p></a></li>
<li><a href='#accuracy.meas'>
<p>Metrics to evaluate a classifier accuracy in imbalanced learning</p></a></li>
<li><a href='#hacide'><p>Half circle filled data</p></a></li>
<li><a href='#ovun.sample'>
<p>Over-sampling, under-sampling, combination of over- and under-sampling.</p></a></li>
<li><a href='#roc.curve'>
<p>ROC curve</p></a></li>
<li><a href='#ROSE'>
<p>Generation of synthetic data by Randomly Over Sampling Examples (ROSE)</p></a></li>
<li><a href='#ROSE.eval'>
<p>Evaluation of learner accuracy by ROSE</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Over-Sampling Examples</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0-4</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-06-14</td>
</tr>
<tr>
<td>Author:</td>
<td>Nicola Lunardon, Giovanna Menardi, Nicola Torelli</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nicola Lunardon &lt;nicola.lunardon@unimib.it&gt;</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, nnet, rpart, tree</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions to deal with binary classification
  problems in the presence of imbalanced classes. Synthetic balanced samples are  
  generated according to ROSE (Menardi and Torelli, 2013).  
  Functions that implement more traditional remedies to the class imbalance
  are also provided, as well as different metrics to evaluate a learner accuracy.
  These are estimated by holdout, bootstrap or cross-validation methods. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-06-14 07:29:48 UTC; nicola</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-06-14 08:10:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='ROSE-package'>
ROSE: Random Over-Sampling Examples</h2><span id='topic+ROSE-package'></span><span id='topic+ROSEpack'></span>

<h3>Description</h3>

<p>Functions to deal with binary classification
problems in the presence of imbalanced classes. Synthetic balanced samples are
generated according to ROSE (Menardi and Torelli, 2014).
Functions that implement more traditional remedies to the class imbalance
are also provided, as well as different metrics to evaluate a learner accuracy.
These are estimated by holdout, bootrstrap or cross-validation methods.
</p>


<h3>Details</h3>

<p>The package pivots on function <code><a href="#topic+ROSE">ROSE</a></code> which generates synthetic balanced
samples and thus allows to strenghten the subsequent estimation of any binary classifier.
ROSE (Random Over-Sampling Examples) is a bootstrap-based technique
which aids the task of binary classification in the presence of rare classes.
It handles both continuous and categorical data by generating synthetic examples from
a conditional density estimate of the two classes. 
Different metrics to evaluate a learner accuracy are supplied by
functions <code><a href="#topic+roc.curve">roc.curve</a></code> and <code><a href="#topic+accuracy.meas">accuracy.meas</a></code>.
Holdout, bootstrap or cross-validation estimators of these accuracy metrics are
computed by means of ROSE and provided by function <code><a href="#topic+ROSE.eval">ROSE.eval</a></code>, to be used in
conjuction with virtually any binary classifier.
Additionally, function <code><a href="#topic+ovun.sample">ovun.sample</a></code> implements more traditional remedies
to the class imbalance, such as over-sampling the minority class, under-sampling the majority
class, or a combination of over- and under- sampling.
</p>


<h3>Author(s)</h3>

<p>Nicola Lunardon, Giovanna Menardi, Nicola Torelli
</p>
<p>Maintainer:
Nicola Lunardon &lt;lunardon@stat.unipd.it&gt;
</p>


<h3>References</h3>

<p>Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. <em>R Jorunal</em>, 6:82&ndash;92.
</p>
<p>Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. <em>Data Mining and Knowledge Discovery</em>, 28:92&ndash;122.
</p>


<h3>See Also</h3>

<p><code><a href="nnet.html#topic+nnet">nnet</a></code>,
<code><a href="rpart.html#topic+rpart">rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># loading data
data(hacide)

# check imbalance
table(hacide.train$cls)

# train logistic regression on imbalanced data
log.reg.imb &lt;- glm(cls ~ ., data=hacide.train, family=binomial)

# use the trained model to predict test data
pred.log.reg.imb &lt;- predict(log.reg.imb, newdata=hacide.test,
                            type="response")

# generate new balanced data by ROSE
hacide.rose &lt;- ROSE(cls ~ ., data=hacide.train, seed=123)$data

# check (im)balance of new data
table(hacide.rose$cls)

# train logistic regression on balanced data
log.reg.bal &lt;- glm(cls ~ ., data=hacide.rose, family=binomial)

# use the trained model to predict test data
pred.log.reg.bal &lt;- predict(log.reg.bal, newdata=hacide.test,
                            type="response")

# check accuracy of the two learners by measuring auc
roc.curve(hacide.test$cls, pred.log.reg.imb)
roc.curve(hacide.test$cls, pred.log.reg.bal, add.roc=TRUE, col=2)

# determine bootstrap distribution of the AUC of logit models
# trained on ROSE balanced samples
# B has been reduced from 100 to 10 for time saving solely
boot.auc.bal &lt;- ROSE.eval(cls ~ ., data=hacide.train, learner= glm,
                          method.assess = "BOOT",
                          control.learner=list(family=binomial),
                          trace=TRUE, B=10)

summary(boot.auc.bal)
</code></pre>

<hr>
<h2 id='accuracy.meas'>
Metrics to evaluate a classifier accuracy in imbalanced learning
</h2><span id='topic+accuracy.meas'></span>

<h3>Description</h3>

<p>This function computes precision, recall and the F measure of a prediction. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy.meas(response, predicted, threshold = 0.5) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracy.meas_+3A_response">response</code></td>
<td>

<p>A vector of responses containing two classes to be used to evaluate prediction accuracy. 
It can be of class <code>"factor"</code>, <code>"numeric"</code> or <code>"character"</code>.
</p>
</td></tr>
<tr><td><code id="accuracy.meas_+3A_predicted">predicted</code></td>
<td>

<p>A vector containing a prediction for each observation. This can be of class <code>"factor"</code> or <code>"character"</code> if the predicted label classes are provided or <code>"numeric"</code> for the probabilities of the rare class (or a monotonic function of them).
</p>
</td></tr>
<tr><td><code id="accuracy.meas_+3A_threshold">threshold</code></td>
<td>

<p>When <code>predicted</code> is of class <code>numeric</code>, it defines the probability threshold to classify an example as positive. 
Default value is meant for predicted probabilities and is set to 0.5. See further details below. 
Ignored if <code>predicted</code> is of class <code>factor</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prediction of positive or negative labels depends on the classification threshold, 
here defined as the value such that observations with predicted value greater than the
threshold are assigned to the positive class. Some caution is due in setting the 
threshold as well as in using the default setting both because the default value is meant 
for predicted probabilities and because the default 0.5 is 
not necessarily the optimal 
choice for imbalanced learning. Smaller values set for the threshold correspond to assign a larger 
misclassification costs to the rare class, which is usually the case.
</p>
<p>Precision is defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\mbox{true positives}}{\mbox{true positives + false positives}}</code>
</p>

<p>Recall is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\mbox{true positives}}{\mbox{true positives + false negative}}</code>
</p>

<p>The F measure is the harmonic average between precision and recall:
</p>
<p style="text-align: center;"><code class="reqn">2 \cdot \frac{\mbox{precision} \cdot \mbox{recall}}{\mbox{precision+recall}}</code>
</p>



<h3>Value</h3>

<p>The value is an object of class <code>accuracy.meas</code> which has components
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>The selected threshold.</p>
</td></tr>
<tr><td><code>precision</code></td>
<td>
<p>A vector of length one giving the precision of the prediction</p>
</td></tr>
<tr><td><code>recall</code></td>
<td>
<p>A vector of length one giving the recall of the prediction</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>A vector of length one giving the F measure</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fawcet T. (2006). An introduction to ROC analysis. <em>Pattern Recognition Letters</em>, 27 (8), 861&ndash;875.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc.curve">roc.curve</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2-dimensional example
# loading data
data(hacide)

# imbalance on training set
table(hacide.train$cls)

# model estimation using logistic regression
fit.hacide  &lt;- glm(cls~., data=hacide.train, family="binomial")

# prediction on training set
pred.hacide.train &lt;- predict(fit.hacide, newdata=hacide.train,
                             type="response")

# compute accuracy measures (training set)
accuracy.meas(hacide.train$cls, pred.hacide.train, threshold = 0.02)

# imbalance on test set 
table(hacide.test$cls)

# prediction on test set
pred.hacide.test &lt;- predict(fit.hacide, newdata=hacide.test,
                            type="response")

# compute accuracy measures (test set)
accuracy.meas(hacide.test$cls, pred.hacide.test, threshold = 0.02)

</code></pre>

<hr>
<h2 id='hacide'>Half circle filled data</h2><span id='topic+hacide.train'></span><span id='topic+hacide.test'></span>

<h3>Description</h3>

<p>Simulated training and test set for imbalanced binary classification. The rare class may be described as a half circle depleted filled with the prevalent class, which is normally distributed and has elliptical contours. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hacide)</code></pre>


<h3>Format</h3>

<p>Data represent 2 real features (denoted as <code>x1, x2</code>) and a binary label class (denoted as <code>cls</code>). Positive examples occur in about 2% of the data. 
</p>

<dl>
<dt><code>hacide.train</code></dt><dd><p>Includes 1000 rows and 20 positive examples.</p>
</dd>
<dt><code>hacide.test</code></dt><dd><p>Includes 250 rows and 5 positive examples.</p>
</dd>
</dl>

<p>Data have been simulated as follows:
</p>

<dl>
<dt>-</dt><dd><p>if <code>cls</code> = 0 then <code>(x1, x2)</code><code class="reqn">\sim \mathbf{N}_{2} \left(\mathbf{0}_{2}, (1/4, 1) \mathbf{I}_{2}\right)</code></p>
</dd>
<dt>-</dt><dd><p>if <code>cls</code> = 1 then <code>(x1, x2)</code><code class="reqn">\sim \mathbf{N}_{2} \left(\mathbf{0}_{2}, \mathbf{I}_{2}\right) \cap \left\|\mathbf{x}\right\|^2&gt;4 \cap x_2 \leq 0</code></p>
</dd>
</dl>



<h3>References</h3>

<p>Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. <em>R Jorunal</em>, 6:82&ndash;92.
</p>
<p>Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. <em>Data Mining and Knowledge Discovery</em>, 28:92&ndash;122.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hacide)
summary(hacide.train)
summary(hacide.test)
</code></pre>

<hr>
<h2 id='ovun.sample'>
Over-sampling, under-sampling, combination of over- and under-sampling.
</h2><span id='topic+ovun.sample'></span>

<h3>Description</h3>

<p>Creates possibly balanced samples by random over-sampling minority examples, under-sampling majority examples or combination of over- and under-sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ovun.sample(formula, data, method="both", N, p=0.5, 
            subset=options("subset")$subset,
            na.action=options("na.action")$na.action, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ovun.sample_+3A_formula">formula</code></td>
<td>

<p>An object of class <code><a href="stats.html#topic+formula">formula</a></code> (or one that can be coerced to that class). 
See <code><a href="#topic+ROSE">ROSE</a></code> for information about interaction among predictors or 
their transformations.
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_data">data</code></td>
<td>

<p>An optional data frame, list or environment (or object
coercible to a data frame by <code>as.data.frame</code>) in which 
to preferentially interpret &ldquo;formula&rdquo;. 
If not specified, the variables are taken from &ldquo;environment(formula)&rdquo;.
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_method">method</code></td>
<td>

<p>One among <code>c("over", "under", "both")</code> to perform over-sampling minority examples, under-sampling majority 
examples or combination of over- and under-sampling, respectively. 
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_n">N</code></td>
<td>

<p>The desired sample size of the resulting data set. 
If missing and <code>method</code> is either <code>"over"</code> or <code>"under"</code> the sample size is determined by oversampling or, respectively, undersampling examples so that the minority class occurs approximately in proportion <code>p</code>. 
When <code>method = "both"</code> the default value is given by the length of vectors 
specified in <code>formula</code>. 
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_p">p</code></td>
<td>

<p>The probability of resampling from the rare class.
If missing and <code>method</code> is either <code>"over"</code> or <code>"under"</code> this proportion is determined by oversampling 
or, respectively, undersampling examples so that the sample size is equal to <code>N</code>. 
When <code>method ="both"</code> the default value given by 0.5.
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_subset">subset</code></td>
<td>

<p>An optional vector specifying a subset of observations to be used in the sampling process.
The default is set by the <code><a href="base.html#topic+subset">subset</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr> 
<tr><td><code id="ovun.sample_+3A_na.action">na.action</code></td>
<td>

<p>A function which indicates what should happen when the data contain 'NA's.  
The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr>
<tr><td><code id="ovun.sample_+3A_seed">seed</code></td>
<td>

<p>A single value, interpreted as an integer, recommended to specify seeds and keep trace of the  
sample.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value is an object of class <code>ovun.sample</code> which has components
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The method used to balance the sample. Possible choices are <br /> <code>c("over", "under", "both")</code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p> The resulting new data set.</p>
</td></tr> 
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ROSE">ROSE</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 2-dimensional example
# loading data
data(hacide)

# imbalance on training set
table(hacide.train$cls)

# balanced data set with both over and under sampling
data.balanced.ou &lt;- ovun.sample(cls~., data=hacide.train,
                                N=nrow(hacide.train), p=0.5, 
                                seed=1, method="both")$data

table(data.balanced.ou$cls)

# balanced data set with over-sampling
data.balanced.over &lt;- ovun.sample(cls~., data=hacide.train, 
                                  p=0.5, seed=1, 
                                  method="over")$data

table(data.balanced.over$cls)

</code></pre>

<hr>
<h2 id='roc.curve'>
ROC curve
</h2><span id='topic+roc.curve'></span>

<h3>Description</h3>

<p>This function returns the ROC curve and computes the area under the curve (AUC) for binary classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc.curve(response, predicted, plotit = TRUE, add.roc = FALSE, 
          n.thresholds=100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc.curve_+3A_response">response</code></td>
<td>
<p>A vector of responses containing two classes to be used to compute the ROC curve. It can be of class <code>"factor"</code>, <code>"numeric"</code> 
or <code>"character"</code>.</p>
</td></tr>
<tr><td><code id="roc.curve_+3A_predicted">predicted</code></td>
<td>
<p>A vector containing a prediction for each observation. This can be of class <code>"factor"</code> or <code>"character"</code> if the predicted label classes are provided or <code>"numeric"</code> for the probabilities of the rare class (or a monotonic function of them).</p>
</td></tr>
<tr><td><code id="roc.curve_+3A_plotit">plotit</code></td>
<td>
<p>Logical, if <code>TRUE</code> the ROC curve is plotted in a new window. Default value is set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="roc.curve_+3A_add.roc">add.roc</code></td>
<td>
<p>Logical, if <code>TRUE</code> the ROC curve is added to an existing window. Default value is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="roc.curve_+3A_n.thresholds">n.thresholds</code></td>
<td>
<p>Number of <code>thresholds</code> at which the ROC curve is computed. 
Default value is the minimum between 100 and the number of elements in <code>response</code>. 
A value of <code>n.thresholds</code> greater than the length of <code>response</code> is ignored.</p>
</td></tr>
<tr><td><code id="roc.curve_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed either to <code>plot</code> or <code>lines</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value is an object of class <code>roc.curve</code> which has components
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>The value of the area under the ROC curve.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>false positive rate</code></td>
<td>
<p>The false positive rate (or equivalently the complement of sensitivity) of the classifier at the evaluated <code>thresholds</code>.</p>
</td></tr>
<tr><td><code>true positive rate</code></td>
<td>
<p>The true positive rate (or equivalently the specificity) of the classifier at the evaluated <code>thresholds</code>.</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>Thresholds at which the ROC curve is evaluated.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fawcet T. (2006). An introduction to ROC analysis. <em>Pattern Recognition Letters</em>, 27 (8), 861&ndash;875.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+accuracy.meas">accuracy.meas</a></code>, <code><a href="pROC.html#topic+roc">roc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2-dimensional example
# loading data
data(hacide)

# check imbalance on training set
table(hacide.train$cls)

# model estimation using logistic regression
fit.hacide  &lt;- glm(cls~., data=hacide.train, family="binomial")

# prediction on training set
pred.hacide.train &lt;- predict(fit.hacide, newdata=hacide.train)

# plot the ROC curve (training set)
roc.curve(hacide.train$cls, pred.hacide.train, 
          main="ROC curve \n (Half circle depleted data)")

# check imbalance on test set 
table(hacide.test$cls)

# prediction using test set
pred.hacide.test &lt;- predict(fit.hacide, newdata=hacide.test)

# add the ROC curve (test set)
roc.curve(hacide.test$cls, pred.hacide.test, add=TRUE, col=2, 
          lwd=2, lty=2)
legend("topleft", c("Resubstitution estimate", "Holdout estimate"), 
        col=1:2, lty=1:2, lwd=2)
</code></pre>

<hr>
<h2 id='ROSE'>
Generation of synthetic data by Randomly Over Sampling Examples (ROSE)
</h2><span id='topic+ROSE'></span>

<h3>Description</h3>

<p>Creates a sample of synthetic data by enlarging the features space of minority and majority class examples. 
Operationally, the new examples are drawn from a conditional kernel density estimate of the two classes, as 
described in Menardi and Torelli (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> 
ROSE(formula, data, N, p=0.5, hmult.majo=1, hmult.mino=1, 
     subset=options("subset")$subset,
     na.action=options("na.action")$na.action, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROSE_+3A_formula">formula</code></td>
<td>

<p>An object of class <code><a href="stats.html#topic+formula">formula</a></code> (or one that can be coerced to that class). 
The left-hand-side (response) should be a vector specifying the class labels. 
The right-hand-side should be a series of vectors with the predictors. See &ldquo;Warning&rdquo; 
for information about interaction among predictors or their transformations.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_data">data</code></td>
<td>

<p>An optional data frame, list or environment (or object
coercible to a data frame by <code>as.data.frame</code>) in which 
to preferentially interpret &ldquo;formula&rdquo;. 
If not specified, the variables are taken from &ldquo;environment(formula)&rdquo;.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_n">N</code></td>
<td>

<p>The desired sample size of the resulting data set generated by ROSE. If missing, 
it is set equal to the length of the response variable in <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_p">p</code></td>
<td>

<p>The probability of the minority class examples in the resulting data set generated by ROSE.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_hmult.majo">hmult.majo</code></td>
<td>

<p>Optional shrink factor to be multiplied by the smoothing parameters to estimate the conditional
kernel density of the majority class. See &ldquo;References&rdquo; and &ldquo;Details&rdquo;.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_hmult.mino">hmult.mino</code></td>
<td>

<p>Optional shrink factor to be multiplied by the smoothing parameters to estimate the conditional
kernel density of the minority class. See &ldquo;References&rdquo; and &ldquo;Details&rdquo;. 
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_subset">subset</code></td>
<td>

<p>An optional vector specifying a subset of observations to be used in the sampling process.
The default is set by the <code><a href="base.html#topic+subset">subset</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr> 
<tr><td><code id="ROSE_+3A_na.action">na.action</code></td>
<td>

<p>A function which indicates what should happen when the data contain 'NA's.  
The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr>
<tr><td><code id="ROSE_+3A_seed">seed</code></td>
<td>

<p>A single value, interpreted as an integer, recommended to specify seeds and keep trace of the  
generated sample.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ROSE (Random Over-Sampling Examples) aids the task of binary classification in the presence of rare classes. It produces a synthetic, possibly balanced, sample of data simulated according to a smoothed-bootstrap approach. 
</p>
<p>Denoted by <code class="reqn">y</code> the binary response and by <code class="reqn">x</code> a vector 
of numeric predictors observed on <code class="reqn">n</code> subjects <code class="reqn">i,</code> (<code class="reqn">i=1, \ldots, n</code>), 
syntethic examples with class label <code class="reqn">k, (k=0, 1)</code> are generated from
a kernel estimate of the conditional density <code class="reqn">f(x|y = k)</code>.
The kernel is a Normal product function centered at each of the <code class="reqn">x_i</code> with
diagonal covariance matrix <code class="reqn">H_k</code>. Here, <code class="reqn">H_k</code> is the asymptotically optimal
smoothing matrix under the assumption of multivariate normality. See &ldquo;References&rdquo;
below and further references therein.
</p>
<p>Essentially, ROSE selects an observation belonging to the class <code class="reqn">k</code> 
and generates new examples in its neighbourhood, 
where the width of the neighbourhood is determined by <code class="reqn">H_k</code>. The user is allowed to 
shrink <code class="reqn">H_k</code> by varying arguments <code>h.mult.majo</code> and <code>h.mult.mino</code>.  
Balancement is regulated by argument <code>p</code>, i.e. the probability of 
generating examples from class <code class="reqn">k=1</code>.
</p>
<p>As they stand, kernel-based methods may be applied to continuous data only.
However, as ROSE includes combination of over and under-sampling as a special case when 
<code class="reqn">H_k</code> tend to zero, the assumption of continuity may be circumvented by 
using a degenerate kernel distribution to draw synthetic categorical examples. 
Basically, if the <code class="reqn">j-</code>th component of <code class="reqn">x_i</code> is categorical, a syntehic clone 
of <code class="reqn">x_i</code> will have as <code class="reqn">j-</code>th component the same value of the <code class="reqn">j-</code>th component of <code class="reqn">x_i</code>.
</p>


<h3>Value</h3>

<p>The value is an object of class <code>ROSE</code> which has components
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The method used to balance the sample. The only possible choice is <br /> <code>ROSE</code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>An object of class <code>data.frame</code> containing new examples generated by ROSE.</p>
</td></tr> 
</table>


<h3>Warning</h3>

<p>The purpose of <code>ROSE</code> is to generate new synthetic examples in the features space. The use of <code>formula</code> is intended solely to 
distinguish the response variable from the predictors. 
Hence, <code>formula</code> must not be confused with the one supplied to fit a classifier in which the specification of either tranformations 
or interactions among variables may be sensible/necessary. 
In the current version <code>ROSE</code> discards possible interactions and transformations of predictors specified in <code>formula</code> automatically. 
The automatic parsing of <code>formula</code> is able to manage virtually all cases on which it has been tested it but 
the user is warned to use caution in the specification of entangled functions of predictors. 
Any report about possible malfunctioning of the parsing mechanism is welcome.
</p>


<h3>References</h3>

<p>Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. <em>R Jorunal</em>, 6:82&ndash;92.
</p>
<p>Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. <em>Data Mining and Knowledge Discovery</em>, 28:92&ndash;122.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ovun.sample">ovun.sample</a></code>, <code><a href="#topic+ROSE.eval">ROSE.eval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2-dimensional example
# loading data
data(hacide)

# imbalance on training set
table(hacide.train$cls)
#imbalance on test set
table(hacide.test$cls)

# plot unbalanced data highlighting the majority and 
# minority class examples.
par(mfrow=c(1,2))
plot(hacide.train[, 2:3], main="Unbalanced data", xlim=c(-4,4),
     ylim=c(-4,4), col=as.numeric(hacide.train$cls), pch=20)
legend("topleft", c("Majority class","Minority class"), pch=20, col=1:2)

# model estimation using logistic regression
fit &lt;- glm(cls~., data=hacide.train, family="binomial")
# prediction using test set
pred &lt;- predict(fit, newdata=hacide.test)
roc.curve(hacide.test$cls, pred,
          main="ROC curve \n (Half circle depleted data)")

# generating data according to ROSE: p=0.5 as default
data.rose &lt;- ROSE(cls~., data=hacide.train, seed=3)$data
table(data.rose$cls)

par(mfrow=c(1,2))
# plot new data generated by ROSE highlighting the 
# majority and minority class examples.
plot(data.rose[, 2:3], main="Balanced data by ROSE",
     xlim=c(-6,6), ylim=c(-6,6), col=as.numeric(data.rose$cls), pch=20)
legend("topleft", c("Majority class","Minority class"), pch=20, col=1:2)

fit.rose &lt;- glm(cls~., data=data.rose, family="binomial")
pred.rose &lt;- predict(fit.rose, data=data.rose, type="response")
roc.curve(data.rose$cls, pred.rose, 
          main="ROC curve \n (Half circle depleted data balanced by ROSE)")
par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='ROSE.eval'>
Evaluation of learner accuracy by ROSE 
</h2><span id='topic+ROSE.eval'></span>

<h3>Description</h3>

<p>Given a classifier and a set of data, this function exploits ROSE 
generation of synthetic samples to provide holdout, bootstrap or leave-K-out 
cross-validation estimates of a specified accuracy measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROSE.eval(formula, data, learner, acc.measure="auc", extr.pred=NULL, 
          method.assess="holdout", K=1, B=100, control.rose=list(),
          control.learner=list(), control.predict=list(), 
          control.accuracy=list(), trace=FALSE, 
          subset=options("subset")$subset, 
          na.action=options("na.action")$na.action, seed)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROSE.eval_+3A_formula">formula</code></td>
<td>
 
<p>An object of class <code><a href="stats.html#topic+formula">formula</a></code> (or one that can be coerced to that class). 
The specification of the formula must be suited for the selected classifier. 
See <code><a href="#topic+ROSE">ROSE</a></code> and the &ldquo;Note&rdquo; below for information about interaction 
among predictors or their transformations.</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_data">data</code></td>
<td>
 
<p>An optional data frame, list or environment (or object
coercible to a data frame by <code>as.data.frame</code>) in which 
to preferentially interpret &ldquo;formula&rdquo;. 
If not specified, the variables are taken from &ldquo;environment(formula)&rdquo;.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_learner">learner</code></td>
<td>

<p>Either a built-in <span class="pkg">R</span> or an user defined function that fits a classifier and that returns a vector of predicted values. See &ldquo;Details&rdquo; below.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_acc.measure">acc.measure</code></td>
<td>

<p>One among <code>c("auc", "precision", "recall", "F")</code>, it defines the 
accuracy measure to be estimated. Function <code><a href="#topic+roc.curve">roc.curve</a></code> is internally called 
when <code>auc="auc"</code> while the other options entail an internal call of function <br /> 
<code><a href="#topic+accuracy.meas">accuracy.meas</a></code>. Default value is <code>"auc"</code>.  
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_extr.pred">extr.pred</code></td>
<td>

<p>An optional function that extracts from the output of a <code>predict</code>
function the vector of predicted values. 
If not specified, the value returned by &ldquo;predict&rdquo; is used. See Examples below.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_method.assess">method.assess</code></td>
<td>

<p>One among <code>c("holdout", "LKOCV", "BOOT")</code>, it is the method used 
for model assessment. When <code>"holdout"</code> is chosen, the learner is fitted
on one ROSE sample and tested on the data provided in <code>formula</code>. 
<code>"LKOCV"</code> stands for &ldquo;leave-K-out cross validation&quot;: the original data set is divided into <code class="reqn">Q</code> subsets of <code>K</code> observations; at  each round, the specified learner is estimated on a ROSE sample built on the provided data but one of these groups and then a prediction on the excluded set of observations is made. At the end of the process, the <code class="reqn">Q</code> distinct predictions are deployed to compute the
selected accuracy measure. <code>"BOOT"</code> estimates the accuracy measure by fitting a
learner on <code>B</code> ROSE samples and testing each of them on the provided data.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_k">K</code></td>
<td>

<p>An integer value indicating the size of the subsets created when 
<br /><code>method.assess="LKOCV"</code>. If <code>K</code> is not 
a multiple of the sample size <code class="reqn">n</code>, then <code class="reqn">Q-1</code> sets of size <code>K</code> are created and the remaining <code class="reqn">n-(Q-1)K</code> observations are 
used to form the last subset. Default value is 1, i.e. leave-1-out cross validation is performed.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_b">B</code></td>
<td>

<p>The number of bootstrap replications to set when <code>method.assess="BOOT"</code>. 
Ignored otherwise. Default value is 100.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_control.learner">control.learner</code></td>
<td>

<p>Further arguments to be passed to <code>learner</code></p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_control.rose">control.rose</code></td>
<td>

<p>Optional arguments to be passed to <code><a href="#topic+ROSE">ROSE</a></code>. 
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_control.predict">control.predict</code></td>
<td>

<p>Further arguments to be passed to <code><a href="stats.html#topic+predict">predict</a></code>.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_control.accuracy">control.accuracy</code></td>
<td>

<p>Optional arguments to be passed to either <code><a href="#topic+roc.curve">roc.curve</a></code> or <code><a href="#topic+accuracy.meas">accuracy.meas</a></code>
depending on the selected accuracy measure.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_trace">trace</code></td>
<td>

<p>logical, if <code>TRUE</code> traces information on the progress of model 
assessment (number of bootstrap or cross validation iterations performed). 
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_subset">subset</code></td>
<td>

<p>An optional vector specifying a subset of observations to be used in the sampling and learning process.
The default is set by the <code><a href="base.html#topic+subset">subset</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr> 
<tr><td><code id="ROSE.eval_+3A_na.action">na.action</code></td>
<td>

<p>A function which indicates what should happen when the data contain 'NA's.  
The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of <code><a href="base.html#topic+options">options</a></code>.
</p>
</td></tr>
<tr><td><code id="ROSE.eval_+3A_seed">seed</code></td>
<td>

<p>A single value, interpreted as an integer, recommended to specify seeds and keep trace of the generated ROSE 
sample/es.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates a measure of accuracy of a classifier specified by the user
by using either holdout, cross-validation, or bootstrap estimators.
Operationally, the classifier is trained over synthetic data generated by ROSE and
then evaluated on the original data. 
</p>
<p>Whatever accuracy measure and estimator are chosen, the <em>true</em> accuracy depends 
on the probability distribution underlying the training data. This is clearly affected by the imbalance 
and its estimation is then regulated by argument <code>control.rose</code>. 
A default setting of the arguments (that is, <code>p=0.5</code>) entails the estimation of the learner accuracy 
conditional to a balanced training set. In order to estimate the accuracy of a learner fitted on unbalanced data, 
the user may set argument <code>p</code> of <code>control.rose</code> to the proportion of
positive examples in the observed sample. See Example 2 below and, for further details, Menardi and Torelli (2014).
</p>
<p>To the aim of a grater flexibility, <code>ROSE.eval</code> is not linked to the use of a specific learner and works virtually with any classifier. 
The actual implementation supports the following two type of <code>learner</code>.
</p>
<p>In the first case, <code>learner</code> has a 'standard' behavior in the sense that it is a function having <code><a href="stats.html#topic+formula">formula</a></code> as a mandatory argument and retrieves an object whose class is associated to a <code><a href="stats.html#topic+predict">predict</a></code> method.
The user that is willing to define her/his own <code>learner</code> must follow the implicit convention that when a classed object is created, then the function name and the class should match (such as <code>lm</code>, <code>glm</code>, <code>rpart</code>, <code>tree</code>, <code>nnet</code>, <code>lda</code>, etc). Furthermore, since <code>predict</code> returns are very heterogeneous, the user is allowed to define some function <code>extr.pred</code> which extracts from the output of <code>predict</code> the desired vector of predicted values.
</p>
<p>In the second case, <code>learner</code> is a wrapper that allows to embed functions that do not meet the aforementioned requirements. The wrapper must have the following mandatory arguments: <code>data</code> and <code>newdata</code>, and must return a vector of predicted values. Optional arguments can be passed as well into the wrapper including the <code>...</code> and by specifiyng them through <code>control.learner</code>.
When argument <code>data</code> in <code>ROSE.eval</code> is not missing, <code>data</code> in <code>learner</code> receives a data frame structured 
as the one in input, otherwise it is constructed according to the template provided by <code>formula</code>.
The same rule applies for argument <code>newdata</code> with the exception that the class label variable is dropped. See &ldquo;Examples&rdquo; below.
</p>


<h3>Value</h3>

<p>The value is an object of class <code>ROSE.eval</code> which has components
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The selected method for model assessment.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>The selected measure to evaluate accuracy.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>The vector of the estimated measure of accuracy. It has length <code class="reqn">1</code> if <br /> <code>method.assess="holdout"</code>, 
or <code>method.assess="LKOCV"</code> and length <code>B</code> if <code>method.assess="BOOT"</code>, corresponding to the bootstrap distribution of the accuracy
estimator.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The function allows the user to include in the formula transformations of predictors or 
interactions among them. ROSE samples are generated on the original data and transformations 
or interactions are ignored. These are then retrieved in fitting the classifier, provided that 
the selected learner function can handle them. See also &ldquo;Warning&rdquo; in <code><a href="#topic+ROSE">ROSE</a></code>.
</p>


<h3>References</h3>

<p>Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. <em>R Jorunal</em>, 6:82&ndash;92.
</p>
<p>Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. <em>Data Mining and Knowledge Discovery</em>, 28:92&ndash;122.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ROSE">ROSE</a></code>, <code><a href="#topic+roc.curve">roc.curve</a></code>, <code><a href="#topic+accuracy.meas">accuracy.meas</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 2-dimensional data
# loading data
data(hacide)

# in the following examples 
# use of a small subset of observations only --&gt; argument subset

dat &lt;- hacide.train

table(dat$cls)

##Example 1
# classification with logit model
# arguments to glm are passed through control.learner
# leave-one-out cross-validation estimate of auc of classifier
# trained on balanced data
ROSE.eval(cls~., data=dat, glm, subset=c(1:50, 981:1000), 
          method.assess="LKOCV", K=5,
          control.learner=list(family=binomial), seed=1)

## Not run: 
##Example 2
# classification with decision tree 
# require package rpart
library(rpart)

# function is needed to extract predicted probability of cls 1 
f.pred.rpart &lt;- function(x) x[,2]

# holdout estimate of auc of two classifiers

# first classifier trained on ROSE unbalanced sample
# proportion of rare events in original data
p &lt;- (table(dat$cls)/sum(table(dat$cls)))[2]
ROSE.eval(cls~., data=dat, rpart, subset=c(1:50, 981:1000),
          control.rose=list(p = p), extr.pred=f.pred.rpart, seed=1)

# second classifier trained on ROSE balanced sample
# optional arguments to plot the roc.curve are passed through 
# control.accuracy 
ROSE.eval(cls~., data=dat, rpart, subset=c(1:50, 981:1000), 
          control.rose=list(p = 0.5), control.accuracy = list(add.roc = TRUE, 
          col = 2), extr.pred=f.pred.rpart, seed=1)

##Example 3
# classification with linear discriminant analysis
library(MASS)

# function is needed to extract the predicted values from predict.lda
f.pred.lda &lt;- function(z) z$posterior[,2]

# bootstrap estimate of precision of learner trained on balanced data
prec.distr &lt;- ROSE.eval(cls~., data=dat, lda, subset=c(1:50, 981:1000), 
                        extr.pred=f.pred.lda, acc.measure="precision",
                        method.assess="BOOT", B=100, trace=TRUE)

summary(prec.distr)

##Example 4
# compare auc of classification with neural network
# with auc of classification with tree 
# require package nnet
# require package tree

library(nnet)
library(tree)

# optional arguments to nnet are passed through control.learner 
ROSE.eval(cls~., data=dat, nnet, subset=c(1:50, 981:1000), 
          method.assess="holdout", control.learn=list(size=1), seed=1)

# optional arguments to plot the roc.curve are passed through 
# control.accuracy
# a function is needed to extract predicted probability of class 1 
f.pred.rpart &lt;- function(x) x[,2] 
f.pred.tree  &lt;- function(x) x[,2] 
ROSE.eval(cls~., data=dat, tree, subset=c(1:50, 981:1000), 
          method.assess="holdout", extr.pred=f.pred.tree, 
          control.acc=list(add=TRUE, col=2), seed=1)

##Example 5
# An user defined learner with a standard behavior
# Consider a dummy example for illustrative purposes only
# Note that function name and the name of the class returned match
DummyStump &lt;- function(formula, ...)
{
   mc &lt;- match.call()
   m &lt;- match(c("formula", "data", "na.action", "subset"), names(mc), 0L)
   mf &lt;- mc[c(1L, m)]
   mf[[1L]] &lt;- as.name("model.frame")
   mf &lt;- eval(mf, parent.frame())  
   data.st &lt;- data.frame(mf)
   out &lt;- list(colname=colnames(data.st)[2], threshold=1)
   class(out) &lt;- "DummyStump"
   out
}

# Associate to DummyStump a predict method
# Usual S3 definition: predic.classname
predict.DummyStump &lt;- function(object, newdata)
{
   out &lt;- newdata[,object$colname]&gt;object$threshold
   out
}

ROSE.eval(formula=cls~., data=dat, learner=DummyStump, 
          subset=c(1:50, 981:1000), method.assess="holdout", seed=3)


##Example 6
# The use of the wrapper for a function with non standard behaviour
# Consider knn in package class
# require package class

library(class)

# the wrapper require two mandatory arguments: data, newdata.
# optional arguments can be passed by including the object '...'
# note that we are going to specify data=data in ROSE.eval
# therefore data in knn.wrap will receive a data set structured
# as dat as well as newdata but with the class label variable dropped
# note that inside the wrapper we dispense to knn 
# the needed quantities accordingly

knn.wrap &lt;- function(data, newdata, ...)
{
   knn(train=data[,-1], test=newdata, cl=data[,1], ...)
}

# optional arguments to knn.wrap may be specified in control.learner
ROSE.eval(formula=cls~., data=dat, learner=knn.wrap,
          subset=c(1:50, 981:1000), method.assess="holdout", 
          control.learner=list(k=2, prob=T), seed=1)

# if we swap the columns of dat we have to change the wrapper accordingly
dat &lt;- dat[,c("x1","x2","cls")]

# now class label variable is the last one
knn.wrap &lt;- function(data, newdata, ...)
{
   knn(train=data[,-3], test=newdata, cl=data[,3], ...)
}

ROSE.eval(formula=cls~., data=dat, learner=knn.wrap,
          subset=c(1:50, 981:1000), method.assess="holdout", 
          control.learner=list(k=2, prob=T), seed=1)


## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
