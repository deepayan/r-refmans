<!DOCTYPE html><html lang="en"><head><title>Help for package tictactoe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tictactoe}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#equivalent_states'><p>Equivalent States</p></a></li>
<li><a href='#hash-ops'><p>Hash Operations for Single State</p></a></li>
<li><a href='#ttt'><p>Play Tic-Tac-Toe Game</p></a></li>
<li><a href='#ttt_ai'><p>Tic-Tac-Toe AI Player</p></a></li>
<li><a href='#ttt_game'><p>Tic-Tac-Toe Game</p></a></li>
<li><a href='#ttt_human'><p>Human Tic-Tac-Toe Player</p></a></li>
<li><a href='#ttt_qlearn'><p>Q-Learning for Training Tic-Tac-Toe AI</p></a></li>
<li><a href='#ttt_simulate'><p>Simulate Tic-Tac-Toe Games between AIs</p></a></li>
<li><a href='#vectorized-hash-ops'><p>Vectorized Hash Operations</p></a></li>
<li><a href='#xhash'><p>Create Hash Table for Generic Keys</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tic-Tac-Toe Game</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Implements tic-tac-toe game to play on console, either with human or AI players.
  Various levels of AI players are trained through the Q-learning algorithm.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>hash, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, combiter, dplyr, tidyr, reshape2, ggplot2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kota7/tictactoe">https://github.com/kota7/tictactoe</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kota7/tictactoe/issues">https://github.com/kota7/tictactoe/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-05-26 14:15:36 UTC; kota</td>
</tr>
<tr>
<td>Author:</td>
<td>Kota Mori [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kota Mori &lt;kmori05@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-05-26 15:33:31 UTC</td>
</tr>
</table>
<hr>
<h2 id='equivalent_states'>Equivalent States</h2><span id='topic+equivalent_states'></span><span id='topic+equivalent_states_actions'></span>

<h3>Description</h3>

<p>Returns a set of equivalent states and actions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equivalent_states(state)

equivalent_states_actions(state, action)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="equivalent_states_+3A_state">state</code></td>
<td>
<p>state, 3x3 matrix</p>
</td></tr>
<tr><td><code id="equivalent_states_+3A_action">action</code></td>
<td>
<p>integer vector of indices (1 to 9)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>equivalent_states</code> returns a list of state matrices
</p>
<p><code>equivalent_states_actions</code> returns a list of two lists:
<code>states</code>, the set of equivalent states and
<code>actions</code>, the set of equivalent actions
</p>

<hr>
<h2 id='hash-ops'>Hash Operations for Single State</h2><span id='topic+hash-ops'></span><span id='topic+haskey'></span><span id='topic++5B.xhash'></span><span id='topic++5B+3C-.xhash'></span><span id='topic+haskey.xhash'></span>

<h3>Description</h3>

<p>Hash Operations for Single State
</p>


<h3>Usage</h3>

<pre><code class='language-R'>haskey(x, ...)

## S3 method for class 'xhash'
x[state, ...]

## S3 replacement method for class 'xhash'
x[state, ...] &lt;- value

## S3 method for class 'xhash'
haskey(x, state, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hash-ops_+3A_x">x</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="hash-ops_+3A_...">...</code></td>
<td>
<p>additional arguments to determine the key</p>
</td></tr>
<tr><td><code id="hash-ops_+3A_state">state</code></td>
<td>
<p>state object</p>
</td></tr>
<tr><td><code id="hash-ops_+3A_value">value</code></td>
<td>
<p>value to assign</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p><code>haskey</code> returns a logical
</p>
</li>
<li><p><code>`[`</code> returns a reference to the object
</p>
</li>
<li><p><code>`[&lt;-`</code> returns a value
</p>
</li></ul>


<hr>
<h2 id='ttt'>Play Tic-Tac-Toe Game</h2><span id='topic+ttt'></span>

<h3>Description</h3>

<p>Start tic-tac-toe game on the console.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt(player1 = ttt_human(), player2 = ttt_human(), sleep = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttt_+3A_player1">player1</code>, <code id="ttt_+3A_player2">player2</code></td>
<td>
<p>objects that inherit <code>ttt_player</code> class</p>
</td></tr>
<tr><td><code id="ttt_+3A_sleep">sleep</code></td>
<td>
<p>interval to take before an AI player to make decision, in second</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At default, the game is played between humans.
Set <code>player1</code> or <code>player2</code> to <code>ttt_ai()</code> to play against
an AI player.
The strength of the AI can be adjusted by passing the <code>level</code>
argument (0 (weekest) to 5 (strongest)) to the <code>ttt_ai</code> function.
</p>
<p>To input your move, type the position like &quot;a1&quot;. Only two-length string
consisting of an alphabet and a digit is accepted.  Type &quot;exit&quot; to
finish the game.
</p>
<p>You may set both <code>player1</code> and <code>player2</code> as AI players.
In this case, the game transition is displayed on the console without
human inputs.
For conducting a large sized simulations of games between AIs, refer to
<code><a href="#topic+ttt_simulate">ttt_simulate</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttt_ai">ttt_ai</a></code>, <code><a href="#topic+ttt_human">ttt_human</a></code>,
<code><a href="#topic+ttt_simulate">ttt_simulate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ttt(ttt_human(), ttt_random())

## End(Not run)
</code></pre>

<hr>
<h2 id='ttt_ai'>Tic-Tac-Toe AI Player</h2><span id='topic+ttt_ai'></span><span id='topic+ttt_random'></span>

<h3>Description</h3>

<p>Create an AI tic-tac-toe game player
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt_ai(name = "ttt AI", level = 0L)

ttt_random(name = "random AI")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttt_ai_+3A_name">name</code></td>
<td>
<p>player name</p>
</td></tr>
<tr><td><code id="ttt_ai_+3A_level">level</code></td>
<td>
<p>AI strength. must be Integer 0 (weekest) to 5 (strongest)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>level</code> argument controls the strength of AI, from
0 (weekest) to 5 (strongest).
<code>ttt_random</code> is an alias of <code>ttt_ai(level = 0)</code>.
</p>
<p>A <code>ttt_ai</code> object has the <code>getmove</code> function, which takes
<code>ttt_game</code> object and returns a move considered as optimal.
<code>getmove</code> function is designed to take a <code>ttt_game</code> object
and returns a move using the policy function.
</p>
<p>The object has the value and policy functions.
The value function maps a game state
to the evaluation from the first player's viewpoint.
The policy function maps a game state to a set of
optimal moves in light of the value evaluation.
The functions have been trained through the Q-learning.
</p>


<h3>Value</h3>

<p><code>ttt_ai</code> object
</p>

<hr>
<h2 id='ttt_game'>Tic-Tac-Toe Game</h2><span id='topic+ttt_game'></span>

<h3>Description</h3>

<p>Object that encapsulates a tic-tac-toe game.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt_game()
</code></pre>


<h3>Value</h3>

<p><code>ttt_game</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- ttt_game()
x$play(3)
x$play(5)
x$show_board()
</code></pre>

<hr>
<h2 id='ttt_human'>Human Tic-Tac-Toe Player</h2><span id='topic+ttt_human'></span>

<h3>Description</h3>

<p>Create an human tic-tac-toe player
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt_human(name = "no name")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttt_human_+3A_name">name</code></td>
<td>
<p>player name</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ttt_human</code> object
</p>

<hr>
<h2 id='ttt_qlearn'>Q-Learning for Training Tic-Tac-Toe AI</h2><span id='topic+ttt_qlearn'></span>

<h3>Description</h3>

<p>Train a tic-tac-toe AI through Q-learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt_qlearn(player, N = 1000L, epsilon = 0.1, alpha = 0.8, gamma = 0.99,
  simulate = TRUE, sim_every = 250L, N_sim = 1000L, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttt_qlearn_+3A_player">player</code></td>
<td>
<p>AI player to train</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_n">N</code></td>
<td>
<p>number of episode, i.e. training games</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_epsilon">epsilon</code></td>
<td>
<p>fraction of random exploration move</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_alpha">alpha</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_gamma">gamma</code></td>
<td>
<p>discount factor</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_simulate">simulate</code></td>
<td>
<p>if true, conduct simulation during training</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_sim_every">sim_every</code></td>
<td>
<p>conduct simulation after this many training games</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_n_sim">N_sim</code></td>
<td>
<p>number of simulation games</p>
</td></tr>
<tr><td><code id="ttt_qlearn_+3A_verbose">verbose</code></td>
<td>
<p>if true, progress report is shown</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements Q-learning to train a tic-tac-toe AI player.
It is designed to train one AI player, which plays against itself to update its
value and policy functions.
</p>
<p>The employed algorithm is Q-learning with epsilon greedy.
For each state <code class="reqn">s</code>, the player updates its value evaluation by
</p>
<p style="text-align: center;"><code class="reqn">V(s) = (1-\alpha) V(s) + \alpha \gamma max_s' V(s')</code>
</p>

<p>if it is the first player's turn.  If it is the other player's turn, replace
<code class="reqn">max</code> by <code class="reqn">min</code>.
Note that <code class="reqn">s'</code> spans all possible states you can reach from <code class="reqn">s</code>.
The policy function is also updated analogously, that is, the set of
actions to reach <code class="reqn">s'</code> that maximizes <code class="reqn">V(s')</code>.
The parameter <code class="reqn">\alpha</code> controls the learning rate, and <code class="reqn">gamma</code> is
the discount factor (earlier win is better than later).
</p>
<p>Then the player chooses the next action by <code class="reqn">\epsilon</code>-greedy method;
Follow its policy with probability <code class="reqn">1-\epsilon</code>, and choose random
action with probability <code class="reqn">\epsilon</code>.  <code class="reqn">\epsilon</code> controls
the ratio of explorative moves.
</p>
<p>At the end of a game, the player sets the value of the final state either to
100 (if the first player wins), -100 (if the second player wins), or
0 (if draw).
</p>
<p>This learning process is repeated for <code>N</code> training games.
When <code>simulate</code> is set true, simulation is conducted after
<code>sim_every</code> training games.
This would be usefule for observing the progress of training.
In general, as the AI gets smarter, the game tends to result in draw more.
</p>
<p>See Sutton and Barto (1998) for more about the Q-learning.
</p>


<h3>Value</h3>

<p><code>data.frame</code> of simulation outcomes, if any
</p>


<h3>References</h3>

<p>Sutton, Richard S and Barto, Andrew G. Reinforcement Learning: An Introduction. The MIT Press (1998)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- ttt_ai()
o &lt;- ttt_qlearn(p, N = 200)
</code></pre>

<hr>
<h2 id='ttt_simulate'>Simulate Tic-Tac-Toe Games between AIs</h2><span id='topic+ttt_simulate'></span>

<h3>Description</h3>

<p>Simulate Tic-Tac-Toe Games between AIs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttt_simulate(player1, player2 = player1, N = 1000L, verbose = TRUE,
  showboard = FALSE, pauseif = integer(0))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttt_simulate_+3A_player1">player1</code>, <code id="ttt_simulate_+3A_player2">player2</code></td>
<td>
<p>AI players to simulate</p>
</td></tr>
<tr><td><code id="ttt_simulate_+3A_n">N</code></td>
<td>
<p>number of simulation games</p>
</td></tr>
<tr><td><code id="ttt_simulate_+3A_verbose">verbose</code></td>
<td>
<p>if true, show progress report</p>
</td></tr>
<tr><td><code id="ttt_simulate_+3A_showboard">showboard</code></td>
<td>
<p>if true, game transition is displayed</p>
</td></tr>
<tr><td><code id="ttt_simulate_+3A_pauseif">pauseif</code></td>
<td>
<p>pause the simulation when specified results occur.
This can be useful for explorative purposes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector of simulation outcomes
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res &lt;- ttt_simulate(ttt_ai(), ttt_ai())
prop.table(table(res))
</code></pre>

<hr>
<h2 id='vectorized-hash-ops'>Vectorized Hash Operations</h2><span id='topic+vectorized-hash-ops'></span><span id='topic+haskeys'></span><span id='topic+setvalues'></span><span id='topic+getvalues'></span><span id='topic+getvalues.xhash'></span><span id='topic+setvalues.xhash'></span><span id='topic+haskeys.xhash'></span>

<h3>Description</h3>

<p>Vectorized Hash Operations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>haskeys(x, ...)

setvalues(x, ...)

getvalues(x, ...)

## S3 method for class 'xhash'
getvalues(x, states, ...)

## S3 method for class 'xhash'
setvalues(x, states, values, ...)

## S3 method for class 'xhash'
haskeys(x, states, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vectorized-hash-ops_+3A_x">x</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="vectorized-hash-ops_+3A_...">...</code></td>
<td>
<p>additional arugments to determine the keys</p>
</td></tr>
<tr><td><code id="vectorized-hash-ops_+3A_states">states</code></td>
<td>
<p>state object</p>
</td></tr>
<tr><td><code id="vectorized-hash-ops_+3A_values">values</code></td>
<td>
<p>values to assign</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p><code>haskeys</code> returns a logical vector
</p>
</li>
<li><p><code>setvalues</code> returns a reference to the object
</p>
</li>
<li><p><code>getvalues</code> returns a list of values
</p>
</li></ul>


<hr>
<h2 id='xhash'>Create Hash Table for Generic Keys</h2><span id='topic+xhash'></span>

<h3>Description</h3>

<p>Create Hash Table for Generic Keys
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xhash(convfunc = function(state, ...) state, convfunc_vec = function(states,
  ...) unlist(Map(convfunc, states, ...)), default_value = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xhash_+3A_convfunc">convfunc</code></td>
<td>
<p>function that converts a game state to a key.
It must take a positional argument <code>state</code> and keyword arguments
represented by <code>...</code>, and returns a character.</p>
</td></tr>
<tr><td><code id="xhash_+3A_convfunc_vec">convfunc_vec</code></td>
<td>
<p>function for vectorized conversion from states to keys.
This function must receive a positional argument <code>states</code>
and keyword arguments <code>...</code>
and returns character vector.  By default, it tries to vectorize
<code>convfunc</code> using <code>Map</code>. User may specify more efficient function
if any.</p>
</td></tr>
<tr><td><code id="xhash_+3A_default_value">default_value</code></td>
<td>
<p>value to be returned when a state is not recorded in
the table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xhash</code> object
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
