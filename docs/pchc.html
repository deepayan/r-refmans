<!DOCTYPE html><html><head><title>Help for package pchc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pchc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pchc-package'>
<p>Bayesian Network Learning with the PCHC and Related Algorithms</p></a></li>
<li><a href='#Adjacency+20matrix+20of+20a+20Bayesian+20network'>
<p>Adjacency matrix of a Bayesian network</p></a></li>
<li><a href='#All+20pairwise+20G-square+20and+20chi-square+20tests+20of+20indepedence'>
<p>All pairwise G-square and chi-square tests of indepedence</p></a></li>
<li><a href='#Bootstrap+20versions+20of+20the+20skeleton+20of+20a+20Bayesian+20network'>
<p>Bootstrap versions of the skeleton of a Bayesian network</p></a></li>
<li><a href='#Bootstrapping+20the+20FEDHC+20and+20FEDTABU+20Bayesian+20network+20learning+20algorithms'>
<p>Bootstrapping the FEDHC and FEDTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#Bootstrapping+20the+20MMHC+20and+20MMTABU+20Bayesian+20network+20learning+20algorithms'>
<p>Bootstrapping the MMHC and MMTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#Bootstrapping+20the+20PCHC+20and+20PCTABU+20Bayesian+20network+20learning+20algorithms'>
<p>Bootstrapping the PCHC and PCTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#Check+20whether+20a+20directed+20graph+20is+20acyclic'>
<p>Check whether a directed graph is acyclic</p></a></li>
<li><a href='#Chi-square+20and+20G-square+20tests+20of+20+28unconditional+29+20indepdence'>
<p>Chi-square and G-square tests of (unconditional) indepdence</p></a></li>
<li><a href='#Continuous+20data+20simulation+20from+20a+20DAG'>
<p>Continuous data simulation from a DAG.</p></a></li>
<li><a href='#Correlation+20between+20pairs+20of+20variables'>
<p>Correlation between pairs of variables</p></a></li>
<li><a href='#Correlation+20matrix+20for+20FBM+20class+20matrices+20+28big+20matrices+29'>
<p>Correlation matrix for FBM class matrices (big matrices)</p></a></li>
<li><a href='#Correlation+20significance+20testing+20using+20Fisher+27s+20z-transformation'>
<p>Correlation significance testing using Fisher's z-transformation</p></a></li>
<li><a href='#Correlations'>
<p>Correlation between a vector and a set of variables</p></a></li>
<li><a href='#Estimation+20of+20the+20percentage+20of+20null+20p-values'>
<p>Estimation of the percentage of null p-values</p></a></li>
<li><a href='#G-square+20and+20Chi-square+20test+20of+20conditional+20indepdence'>
<p>G-square test of conditional indepdence</p></a></li>
<li><a href='#Lower+20limit+20of+20the+20confidence+20of+20an+20edge'>
<p>Lower limit of the confidence of an edge</p></a></li>
<li><a href='#Markov+20blanket+20of+20a+20node+20in+20a+20Bayesian+20network'>
<p>Markov blanket of a node in a Bayesian network</p></a></li>
<li><a href='#Outliers+20free+20data+20via+20the+20reweighted+20MCD'>
<p>Outliers free data via the reweighted MCD</p></a></li>
<li><a href='#Partial+20correlation+20between+20two+20continuous+20variables'>
<p>Partial correlation</p></a></li>
<li><a href='#Partial+20correlation+20matrix+20from+20correlation+20or+20covariance+20matrix'>
<p>Partial correlation matrix from correlation or covariance matrix</p></a></li>
<li><a href='#Plot+20of+20a+20Bayesian+20network'>
<p>Plot of a Bayesian network</p></a></li>
<li><a href='#Random+20values+20simulation+20from+20a+20Bayesian+20network'>
<p>Random values simulation from a Bayesian network</p></a></li>
<li><a href='#Read+20big+20data+20or+20a+20big.matrix+20object'>
<p>Read big data or a big.matrix object</p></a></li>
<li><a href='#ROC+20and+20AUC'>
<p>ROC and AUC</p></a></li>
<li><a href='#Skeleton+20of+20the+20FEDHC+20algorithm'>
<p>The skeleton of a Bayesian network produced by the FEDHC algorithm</p></a></li>
<li><a href='#Skeleton+20of+20the+20FEDHC+20algorithm+20using+20the+20distance+20correlation'>
<p>The skeleton of a Bayesian network produced by the FEDHC algorithm using the distance correlation</p></a></li>
<li><a href='#Skeleton+20of+20the+20MMHC+20algorithm'>
<p>The skeleton of a Bayesian network learned with the MMHC algorithm</p></a></li>
<li><a href='#Skeleton+20of+20the+20PC+20algorithm'>
<p>The skeleton of a Bayesian network learned with the PC algorithm</p></a></li>
<li><a href='#The+20FEDHC+20and+20FEDTABU+20Bayesian+20network+20learning+20algorithms'>
<p>The FEDHC and FEDTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#The+20MMHC+20and+20MMTABU+20Bayesian+20network+20learning+20algorithms'>
<p>The MMHC and MMTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#The+20PCHC+20and+20PCTABU+20Bayesian+20network+20learning+20algorithms'>
<p>The PCHC and PCTABU Bayesian network learning algorithms</p></a></li>
<li><a href='#Topological+20sort+20of+20a+20Bayesian+20network'>
<p>Topological sort of a Bayesian network</p></a></li>
<li><a href='#Utilities+20for+20the+20skeleton+20of+20a+20+28Bayesian+29+20network'>
<p>Utilities for the skeleton of a (Bayesian) Network</p></a></li>
<li><a href='#Variable+20selection+20for+20continuous+20data+20using+20the+20FBED+20algorithm'>
<p>Variable selection for continuous data using the FBED algorithm</p></a></li>
<li><a href='#Variable+20selection+20for+20continuous+20data+20using+20the+20MMPC+20algorithm'>
<p>Variable selection for continuous data using the MMPC algorithm</p></a></li>
<li><a href='#Variable+20selection+20for+20continuous+20data+20using+20the+20PC-simple+20algorithm'><p>Variable selection for continuous data using the PC-simple algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Network Learning with the PCHC and Related Algorithms</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-06</td>
</tr>
<tr>
<td>Author:</td>
<td>Michail Tsagris [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michail Tsagris &lt;mtsagris@uoc.gr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>bigstatsr, bnlearn, dcov, foreach, doParallel, parallel,
Rfast, Rfast2, robustbase, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bigreadr, Rgraphviz</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian network learning using the PCHC algorithm. PCHC stands for PC Hill-Climbing, a new hybrid algorithm that uses PC to construct the skeleton of the BN and then 
			 applies the Hill-Climbing greedy search. More algorithms and variants have been added, such as MMHC, FEDHC, and the Tabu search variants, PCTABU, MMTABU and FEDTABU. 
			 The relevant papers are:
			 a) Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics. Computational Economics, 57(1): 341-367. &lt;<a href="https://doi.org/10.1007%2Fs10614-020-10065-7">doi:10.1007/s10614-020-10065-7</a>&gt;. 
			 b) Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics 2022, 10(15): 2604. &lt;<a href="https://doi.org/10.3390%2Fmath10152604">doi:10.3390/math10152604</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-06 15:33:02 UTC; tsagris</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-06 16:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='pchc-package'>
Bayesian Network Learning with the PCHC and Related Algorithms
</h2><span id='topic+pchc-package'></span>

<h3>Description</h3>

<p>The original version of this package was to learn Bayesian networks with the PCHC algorithm. PCHC stands for PC Hill-Climbing. It is a new hybrid algorithm that used PC to construct the skeleton of the BN and then utilizes the Hill-Climbing greedy search. The package has been expanded to include the MMHC and the FEDHC algortihms. It further includes the PCTABU, MMTABU and FEDTABU algortihms which are pretty much similar. Instead of the Hill Climbing greey search the Tabu search is employed.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> pchc</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-09-06</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers</h3>

<p>Michail Tsagris &lt;mtsagris@uoc.gr&gt;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics 2022, 10(15): 2604.
</p>
<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics. Computational Economics 57(1): 341-367.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search.
The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I., Borboudakis G. (2010) Permutation Testing Improves Bayesian Network Learning.
In Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2010. 322-337.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning 65(1):31-78.
</p>
<p>Tsagris M. (2017). Conditional independence test for categorical data using Poisson log-linear model. Journal of Data Science, 15(2):347-356.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping.
Journal of Machine Learning Research, 20(8): 1-39.
</p>

<hr>
<h2 id='Adjacency+20matrix+20of+20a+20Bayesian+20network'>
Adjacency matrix of a Bayesian network
</h2><span id='topic+bnmat'></span>

<h3>Description</h3>

<p>Adjacency matrix of a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnmat(dag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Adjacency+2B20matrix+2B20of+2B20a+2B20Bayesian+2B20network_+3A_dag">dag</code></td>
<td>

<p>A BN object, an object of class &quot;bn&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is called from the &quot;bnlearn&quot; package which invokes the &quot;Rgraphviz&quot; package from Bioconductor and you need to install it first.
</p>


<h3>Value</h3>

<p>Adjacency matrix of a Bayesian network is extracted.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="Rfast.html#topic+pc.skel">pc.skel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix( rnorm(200 * 10, 1, 10), nrow = 200 )
a &lt;- pchc::pchc(x)
pchc::bnmat(a$dag)

</code></pre>

<hr>
<h2 id='All+20pairwise+20G-square+20and+20chi-square+20tests+20of+20indepedence'>
All pairwise G-square and chi-square tests of indepedence
</h2><span id='topic+g2test_univariate'></span><span id='topic+g2test_univariate_perm'></span><span id='topic+chi2test_univariate'></span>

<h3>Description</h3>

<p>All pairwise G-square and chi-square tests of indepedence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g2test_univariate(x, dc)
g2test_univariate_perm(x, dc, B)
chi2test_univariate(x, dc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="All+2B20pairwise+2B20G-square+2B20and+2B20chi-square+2B20tests+2B20of+2B20indepedence_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the data. <b>The minimum must be 0, otherwise the function can crash or will produce
wrong results</b>. The data must be consecutive numbers.
</p>
</td></tr>
<tr><td><code id="All+2B20pairwise+2B20G-square+2B20and+2B20chi-square+2B20tests+2B20of+2B20indepedence_+3A_dc">dc</code></td>
<td>

<p>A numerical value equal to the number of variables (or columns of the data matrix) indicating the number of distinct, unique values (or levels) of each variable.
Make sure you give the correct numbers here, otherwise the degrees of freedom will be wrong.
</p>
</td></tr>
<tr><td><code id="All+2B20pairwise+2B20G-square+2B20and+2B20chi-square+2B20tests+2B20of+2B20indepedence_+3A_b">B</code></td>
<td>

<p>The number of permutations. The permutations test is slower than without permutations and should be used with small sample sizes or when the contigency tables have zeros. When there are few variables, R's &quot;chisq.test&quot; function is faster, but as the number of variables increase the time difference
with R's procedure becomes larger and larger.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function does all the pairwise <code class="reqn">G^2</code> test of independence and gives the position inside the matrix.
The user must build the associations matrix now, similarly to the correlation matrix. See the examples of how to do that.
The p-value is not returned, we live this to the user. See the examples of how to obtain it.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>

<p>The <code class="reqn">G^2</code> or <code class="reqn">X^2</code> test statistic for each pair of variables.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The p-value of the test statistic for each pair of variables.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The row or variable of the data.
</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>The column or variable of the data.
</p>
</td></tr>
<tr><td><code>df</code></td>
<td>

<p>The degrees of freedom of each test.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2017). Conditional independence test for categorical data using Poisson log-linear model.
Journal of Data Science, 15(2):347-356.
</p>
<p>Tsamardinos, I. and Borboudakis, G. (2010). Permutation testing improves Bayesian network learning.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 322-337). Springer Berlin Heidelberg
</p>


<h3>See Also</h3>

<p><code><a href="#topic+g2test">g2test</a>, <a href="#topic+cat.tests">cat.tests</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nvalues &lt;- 3
nvars &lt;- 10
nsamples &lt;- 1000
x&lt;- matrix( sample( 0:(nvalues - 1), nvars * nsamples, replace = TRUE ), nsamples, nvars )
dc &lt;- rep(nvalues, nvars)
system.time( g2test_univariate(x, dc) )
a &lt;- g2test_univariate(x, dc)
</code></pre>

<hr>
<h2 id='Bootstrap+20versions+20of+20the+20skeleton+20of+20a+20Bayesian+20network'>
Bootstrap versions of the skeleton of a Bayesian network
</h2><span id='topic+pchc.skel.boot'></span><span id='topic+fedhc.skel.boot'></span><span id='topic+mmhc.skel.boot'></span>

<h3>Description</h3>

<p>Bootstrap versions of the skeleton of a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pchc.skel.boot(x, method = "pearson", alpha = 0.05, B = 200)
fedhc.skel.boot(x, method = "pearson", alpha = 0.05, B = 200)
mmhc.skel.boot(x, max_k = 3, method = "pearson", alpha = 0.05, B = 200)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap+2B20versions+2B20of+2B20the+2B20skeleton+2B20of+2B20a+2B20Bayesian+2B20network_+3A_x">x</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. <b>If you have categorical data though, the user must transform the data.frame into a matrix. In addition, the numerical matrix must have values starting from 0. For example, 0, 1, 2, instead of &quot;A&quot;, &quot;B&quot; and &quot;C&quot;</b>.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20versions+2B20of+2B20the+2B20skeleton+2B20of+2B20a+2B20Bayesian+2B20network_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20versions+2B20of+2B20the+2B20skeleton+2B20of+2B20a+2B20Bayesian+2B20network_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20versions+2B20of+2B20the+2B20skeleton+2B20of+2B20a+2B20Bayesian+2B20network_+3A_alpha">alpha</code></td>
<td>

<p>The significance level ( suitable values in (0, 1) ) for assessing the p-values. The default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20versions+2B20of+2B20the+2B20skeleton+2B20of+2B20a+2B20Bayesian+2B20network_+3A_b">B</code></td>
<td>

<p>The number of bootstrap resamples to draw. The algorithm is performed in each bootstrap sample. In the end, the adjacency matrix on the observed data is returned, along with another adjacency matrix produced by the bootstrap. The latter one contains values from 0 to 1 indicating the proportion of times an edge between two nodes was present.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>G</code></td>
<td>

<p>The observed adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
<tr><td><code>Gboot</code></td>
<td>

<p>The bootstrapped adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>A new scalable Bayesian network learning algorithm with applications to economics. 
Computational Economics, 57(1): 341-367.
</p>
<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics 2022, 10(15), 2604.
</p>
<p>Spirtes P., Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. 
The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network 
structure learning algorithm. Machine learning 65(1): 31-78.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. 
Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+bn.skel.utils">bn.skel.utils</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- pchc::rbn2(500, p = 20, nei = 3)$x
a &lt;- pchc::pchc.skel.boot(x, alpha = 0.05)

</code></pre>

<hr>
<h2 id='Bootstrapping+20the+20FEDHC+20and+20FEDTABU+20Bayesian+20network+20learning+20algorithms'>
Bootstrapping the FEDHC and FEDTABU Bayesian network learning algorithms
</h2><span id='topic+fedhc.boot'></span><span id='topic+fedtabu.boot'></span>

<h3>Description</h3>

<p>Bootstrapping the FEDHC and FEDTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fedhc.boot(x, method = "pearson", alpha = 0.05, ini.stat = NULL, R = NULL,
restart = 10, score = "bic-g", blacklist = NULL, whitelist = NULL, B = 200, ncores = 1)

fedtabu.boot(x, method = "pearson", alpha = 0.05, ini.stat = NULL, R = NULL,
tabu = 10, score = "bic-g", blacklist = NULL, whitelist = NULL, B = 200, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, you can choose either &quot;pearson&quot; or &quot;spearman&quot;. If you have categorical data though, this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The <code><a href="#topic+g2test">g2test</a></code> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_b">B</code></td>
<td>

<p>The number of bootstrap resamples to draw. The algorithm is performed in each bootstrap sample. In the end, the adjacency matrix on the observed data is returned, along with another adjacency matrix produced by the bootstrap. The latter one contains values from 0 to 1 indicating the proportion of times an edge between two nodes was present.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use, in case of parallel computing.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The FEDHC algorithm is implemented. The FBED algortihm (Borboudakis and Tsamardinos, 2019), without the backward phase, is implemented during the skeleton identification phase. Next, the Hill Climbing greedy search or the Tabu search is employed to score the network.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+pchc">pchc</a> </code> or the <code> <a href="#topic+pctabu">pctabu</a> </code>  function.
</p>
</td></tr>
<tr><td><code>Gboot</code></td>
<td>

<p>The bootstrapped adjancency matrix of the Bayesian network.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics 2022, 10(15): 2604.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping.
Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65(1):31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+fedhc">fedhc</a>, <a href="#topic+fedhc.skel">fedhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 20, 1, 10), nrow = 200 )
a &lt;- fedhc.boot(x, B = 50)
</code></pre>

<hr>
<h2 id='Bootstrapping+20the+20MMHC+20and+20MMTABU+20Bayesian+20network+20learning+20algorithms'>
Bootstrapping the MMHC and MMTABU Bayesian network learning algorithms
</h2><span id='topic+mmhc.boot'></span><span id='topic+mmtabu.boot'></span>

<h3>Description</h3>

<p>Bootstrapping the MMHC and MMTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmhc.boot(x, method = "pearson", max_k = 3, alpha = 0.05, ini.stat = NULL,
R = NULL, restart = 10, score = "bic-g", blacklist = NULL, whitelist = NULL,
B = 200, ncores = 1)

mmtabu.boot(x, method = "pearson", max_k = 3, alpha = 0.05, ini.stat = NULL,
R = NULL, tabu = 10, score = "bic-g", blacklist = NULL, whitelist = NULL,
B = 200, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0.
No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero.
The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_b">B</code></td>
<td>

<p>The number of bootstrap resamples to draw. The algorithm is performed in each bootstrap sample. In the end, the adjacency matrix on the observed data is returned, along with another adjacency matrix produced by the bootstrap. The latter one contains values from 0 to 1 indicating the proportion of times an edge between two nodes was present.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use, in case of parallel computing.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMHC algorithm is implemented without performing the backward elimination during the skeleton identification phase. The MMHC as described in Tsamardinos et al. (2006) employs the MMPC algorithm during the skeleton construction phase and the Tabu search in the scoring phase. In this package, the mmhc function employs the Hill Climbing greedy search in the scoring phase while the mmtabu employs the Tabu search.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+mmhc">mmhc</a> </code> or the <code> <a href="#topic+mmtabu">mmtabu</a> </code>  function.
</p>
</td></tr>
<tr><td><code>Gboot</code></td>
<td>

<p>The bootstrapped adjancency matrix of the Bayesian network.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1): 31-78.
</p>
<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics, 57(1):341-367.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fedhc">fedhc</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+mmhc">mmhc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 20, 1, 10), nrow = 200 )
a &lt;- mmhc.boot(x, B = 50)
</code></pre>

<hr>
<h2 id='Bootstrapping+20the+20PCHC+20and+20PCTABU+20Bayesian+20network+20learning+20algorithms'>
Bootstrapping the PCHC and PCTABU Bayesian network learning algorithms
</h2><span id='topic+pchc.boot'></span><span id='topic+pctabu.boot'></span>

<h3>Description</h3>

<p>Bootstrapping the PCHC and PCTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pchc.boot(x, method = "pearson", alpha = 0.05, ini.stat = NULL,
R = NULL, restart = 10, score = "bic-g", blacklist = NULL, whitelist = NULL,
B = 200, ncores = 1)

pctabu.boot(x, method = "pearson", alpha = 0.05, ini.stat = NULL,
R = NULL, tabu = 10, score = "bic-g", blacklist = NULL, whitelist = NULL,
B = 200, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0.
No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, you can choose either &quot;pearson&quot; or &quot;spearman&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The <code><a href="#topic+g2test">g2test</a></code>
and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_b">B</code></td>
<td>

<p>The number of bootstrap resamples to draw. The algorithm is performed in each bootstrap sample. In the end, the adjacency matrix on the observed data is returned, along with another adjacency matrix produced by the bootstrap. The latter one contains values from 0 to 1 indicating the proportion of times an edge between two nodes was present.
</p>
</td></tr>
<tr><td><code id="Bootstrapping+2B20the+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use, in case of parallel computing.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PC algorithm as proposed by Spirtes et al. (2001) is first implemented followed by a scoring phase,
such as hill climbing or tabu search. The PCHC was proposed by Tsagris (2021), while the PCTABU algorithm
is the same but instead of the hill climbing scoring phase, the tabu search is employed.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+pchc">pchc</a> </code> or the <code> <a href="#topic+pctabu">pctabu</a> </code>  function.
</p>
</td></tr>
<tr><td><code>Gboot</code></td>
<td>

<p>The bootstrapped adjancency matrix of the Bayesian network.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics, 57(1):341-367.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I. and Borboudakis G. (2010) Permutation Testing Improves Bayesian Network Learning. In Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2010, 322-337.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1): 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fedhc">fedhc</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+pchc.skel">pchc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 20, 1, 10), nrow = 200 )
a &lt;- pchc.boot(x, B = 50)
</code></pre>

<hr>
<h2 id='Check+20whether+20a+20directed+20graph+20is+20acyclic'>
Check whether a directed graph is acyclic
</h2><span id='topic+is.dag'></span>

<h3>Description</h3>

<p>Check whether a directed graph is acyclic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.dag(dag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Check+2B20whether+2B20a+2B20directed+2B20graph+2B20is+2B20acyclic_+3A_dag">dag</code></td>
<td>

<p>A square matrix representing a directed graph which contains either 0 or 1, where G[i, j] = 1, means there is an arrow from node i to node j.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The topological sort is performed. If it cannot be performed, NAs are returned. Hence, the functions checks for NAs.
</p>


<h3>Value</h3>

<p>A logical value, TRUE if the matrix represents a DAG and FALSE otherwise.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian network structures.
Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Canada, 87-98.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="#topic+fedhc">fedhc</a>, <a href="#topic+mmhc">mmhc</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>G &lt;- pchc::rbn3(100, 20, 0.3)$G
pchc::is.dag(G)  ## TRUE
</code></pre>

<hr>
<h2 id='Chi-square+20and+20G-square+20tests+20of+20+28unconditional+29+20indepdence'>
Chi-square and G-square tests of (unconditional) indepdence
</h2><span id='topic+cat.tests'></span>

<h3>Description</h3>

<p>Chi-square and G-square tests of (unconditional) indepdence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cat.tests(x, y, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Chi-square+2B20and+2B20G-square+2B20tests+2B20of+2B20+2B28unconditional+2B29+2B20indepdence_+3A_x">x</code></td>
<td>

<p>A numerical vector or a factor variable with data. The data must be consecutive numbers.
</p>
</td></tr>
<tr><td><code id="Chi-square+2B20and+2B20G-square+2B20tests+2B20of+2B20+2B28unconditional+2B29+2B20indepdence_+3A_y">y</code></td>
<td>

<p>A numerical vector or a factor variable with data. The data must be consecutive numbers.
</p>
</td></tr>
<tr><td><code id="Chi-square+2B20and+2B20G-square+2B20tests+2B20of+2B20+2B28unconditional+2B29+2B20indepdence_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates the test statistic of the <code class="reqn">X^2</code> and the <code class="reqn">G^2</code> tests of unconditional
independence between x and y. x and y need not be numerical vectors like in <code><a href="Rfast.html#topic+g2Test">g2Test</a></code>. This
function is more close to the spirit of MASS' <code><a href="MASS.html#topic+loglm">loglm</a></code> function which calculates both statistics
using Poisson log-linear models (Tsagris, 2017).
</p>


<h3>Value</h3>

<p>A matrix with two rows. In each row the X2 or G2 test statistic, its p-value and the degrees of freedom are returned.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics 57(1): 341-367.
</p>
<p>Tsagris M. (2017). Conditional independence test for categorical data using Poisson log-linear model.
Journal of Data Science, 15(2): 347-356.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+g2test">g2test</a>, <a href="#topic+cortest">cortest</a>, <a href="Rfast.html#topic+pc.skel">pc.skel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rbinom(100, 3, 0.5)
y &lt;- rbinom(100, 2, 0.5)
cat.tests(x, y)
</code></pre>

<hr>
<h2 id='Continuous+20data+20simulation+20from+20a+20DAG'>
Continuous data simulation from a DAG.
</h2><span id='topic+rbn2'></span><span id='topic+rbn3'></span>

<h3>Description</h3>

<p>Contunuous data simulation from a DAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbn2(n, G = NULL, p, nei, low = 0.1, up = 1)
rbn3(n, p, s, a = 0, m, G = NULL, seed = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_n">n</code></td>
<td>

<p>A number indicating the sample size.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_p">p</code></td>
<td>

<p>A number indicating the number of nodes (or vectices, or variables).
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_nei">nei</code></td>
<td>

<p>The average number of neighbours.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_s">s</code></td>
<td>

<p>A number in <code class="reqn">(0, 1)</code>. This defines somehow the sparseness of the model. It is the probability that a node has an edge.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_a">a</code></td>
<td>

<p>A number in <code class="reqn">(0, 1)</code>. The defines the percentage of outliers to be included in the simulated data. If <code class="reqn">a=0</code>, no outliers are generated.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_m">m</code></td>
<td>

<p>A vector equal to the number of nodes. This is the mean vector of the normal distribution from which the data are to be generated. This is used only when <code class="reqn">a&gt;0</code>
so as to define the mena vector of the multivariate normal from which the outliers will be generated.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_g">G</code></td>
<td>

<p>If you already have an an adjacency matrix in mind, plug it in here, otherwise, leave it NULL.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_seed">seed</code></td>
<td>

<p>If seed is TRUE, the simulated data will always be the same.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_low">low</code></td>
<td>

<p>Every child will be a function of some parents. The beta coefficients of the parents will be drawn uniformly from two numbers, low and up. See details for more information on this.
</p>
</td></tr>
<tr><td><code id="Continuous+2B20data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_up">up</code></td>
<td>

<p>Every child will be a function of some parents. The beta coefficients of the parents will be drawn uniformly from two numbers, low and up. See details for more information on this.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where no adjacency matrix is given, an <code class="reqn">p \times p</code> matrix with zeros everywhere is created.
Every element below the diagonal is is replaced by random values from a Bernoulli distribution with probability of success equal to s. This is the matrix B. Every value of 1 is replaced by a uniform value in <code class="reqn">0.1, 1</code>. This final matrix is called A. The data are generated from a multivariate normal distribution with a zero mean vector and covariance matrix equal to <code class="reqn">\left({\bf I}_p- A\right)^{-1}\left({\bf I}_p- A\right)</code>, where <code class="reqn">{\bf I}_p</code> is the <code class="reqn">p \times p</code> identiy matrix. If a is greater than zero, the outliers are generated from a multivariate normal with the same covariance matrix and mean vector the one specified by the user, the argument &quot;m&quot;. The flexibility of the outliers is that you cna specifiy outliers in some variables only or in all of them. For example, m = c(0,0,5) introduces outliers in the third variable only, whereas m = c(5,5,5) introduces outliers in all variables. The user is free to decide on the type of outliers to include in the data.
</p>
<p>For the &quot;rdag2&quot;, this is a different way of simulating data from DAGs. The first variable is normally generated. Every other variable can be a function of some previous ones. Suppose now that the i-th variable is a child of 4 previous variables. We need for coefficients <code class="reqn">b_j</code> to multiply the 4 variables and then generate the i-th variable from a normal with mean <code class="reqn">\sum_{j=1}b_j X_j</code> and variance 1. The <code class="reqn">b_j</code> will be either positive or negative values with equal probability. Their absolute values ranges between &quot;low&quot; and &quot;up&quot;. The code is accessible and you can see in detail what is going on. In addition, every generated data, are standardised to avoid numerical overflow.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>nout</code></td>
<td>

<p>The number of outliers.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adcacency matrix used. For the &quot;rdag&quot; if G[i, j] = 2, then G[j, i] = 3 and this means that there is an arrow from j to i. For the &quot;rdag2&quot; the entries are either G[i, j] = G[j, i] = 0 (no edge) or G[i, j] = 1 and G[j, i] = 0 (indicating i -&gt; j).
</p>
</td></tr>
<tr><td><code>A</code></td>
<td>

<p>The matrix with the with the uniform values in the interval <code class="reqn">0.1, 1</code>. This is returned only by &quot;rdag&quot;.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The simulated data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation.
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data.
International Journal of Data Science and Analytics.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Colombo Diego, and Marloes H. Maathuis (2014). Order-independent constraint-based causal structure learning. Journal of Machine Learning Research 15(1): 3741&ndash;3782.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rbn">rbn</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+fedhc">fedhc</a>, <a href="#topic+mmhc">mmhc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- pchc::rbn3(100, 20, 0.2)$x
a &lt;- pchc::pchc(x)

</code></pre>

<hr>
<h2 id='Correlation+20between+20pairs+20of+20variables'>
Correlation between pairs of variables
</h2><span id='topic+corpairs'></span>

<h3>Description</h3>

<p>Correlations between pairs of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpairs(x, y, rho = NULL, logged = FALSE, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlation+2B20between+2B20pairs+2B20of+2B20variables_+3A_x">x</code></td>
<td>

<p>A matrix with real valued data.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20between+2B20pairs+2B20of+2B20variables_+3A_y">y</code></td>
<td>

<p>A matrix with real valued data whose dimensions match those of x.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20between+2B20pairs+2B20of+2B20variables_+3A_rho">rho</code></td>
<td>

<p>This can be a vector of assumed correlations (equal to the number of variables or the columns of x or y) to
be tested. If this is not the case, leave it NULL and only the correlations will be returned.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20between+2B20pairs+2B20of+2B20variables_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)? This is taken into account only if &quot;rho&quot; is
a vector.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20between+2B20pairs+2B20of+2B20variables_+3A_parallel">parallel</code></td>
<td>

<p>Should parallel implentations take place in C++? The default value is FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The paired correlations are calculated. For each column of the matrices x and y the correlation between them is
calculated.
</p>


<h3>Value</h3>

<p>A vector of correlations in the case of &quot;rho&quot; being NULL, or a matrix with two extra columns, the test
statistic and the (logged) p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Lambert Diane (1992). Zero-Inflated Poisson Regression, with an Application to Defects in
Manufacturing. Technometrics. 34(1):1-14.
</p>
<p>Johnson Norman L., Kotz Samuel and Kemp Adrienne W. (1992). Univariate Discrete
Distributions (2nd ed.). Wiley
</p>
<p>Cohen, A. Clifford (1960). Estimating parameters in a conditional Poisson distribution. Biometrics. 16:203-211.
</p>
<p>Johnson, Norman L. Kemp, Adrianne W. Kotz, Samuel (2005). Univariate Discrete Distributions (third edition).
Hoboken, NJ: Wiley-Interscience.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+correls">correls</a>, <a href="#topic+cortest">cortest</a>, <a href="#topic+pcor">pcor</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(100 * 100), ncol = 100)
y &lt;- matrix( rnorm(100 * 100), ncol = 100)
system.time( a &lt;- corpairs(x, y) )
</code></pre>

<hr>
<h2 id='Correlation+20matrix+20for+20FBM+20class+20matrices+20+28big+20matrices+29'>
Correlation matrix for FBM class matrices (big matrices)
</h2><span id='topic+big_cor'></span>

<h3>Description</h3>

<p>Correlation matrix for FBM class matrices (big matrices).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>big_cor(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlation+2B20matrix+2B20for+2B20FBM+2B20class+2B20matrices+2B20+2B28big+2B20matrices+2B29_+3A_x">x</code></td>
<td>

<p>An FBM class matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function accepts a Filebacked Big Matrix (FBM) class matrix and returns the correlation matrix.
Check you matrix for possible NA values. For more information see the &quot;bigmemory&quot; and &quot;bigstatsr&quot; packages.
</p>


<h3>Value</h3>

<p>The correlation matrix of the big data x.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+big_read">big_read</a>, <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(bigstatsr, quietly = TRUE)
x &lt;- matrix( runif(100 * 50, 1, 100), ncol = 50 )
x &lt;- bigstatsr::as_FBM(x)
a &lt;- pchc::big_cor(x)

</code></pre>

<hr>
<h2 id='Correlation+20significance+20testing+20using+20Fisher+27s+20z-transformation'>
Correlation significance testing using Fisher's z-transformation
</h2><span id='topic+cortest'></span>

<h3>Description</h3>

<p>Correlation significance testing using Fisher's z-transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cortest(y, x, rho = 0, a = 0.05 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_rho">rho</code></td>
<td>

<p>The value of the hypothesised correlation to be used in the hypothesis testing.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_a">a</code></td>
<td>

<p>The significance level used for the confidence intervals.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the built-in function &quot;cor&quot; which is very fast, then computes a confidence interval
and produces a p-value for the hypothesis test.
</p>


<h3>Value</h3>

<p>A vector with 5 numbers; the correlation, the p-value for the hypothesis test that each of them is
equal to &quot;rho&quot;, the test statistic and the $a/2%$ lower and upper confidence limits.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics 57(1): 341-367.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcor">pcor</a>, <a href="#topic+correls">correls</a>, <a href="#topic+corpairs">corpairs</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rcauchy(60)
y &lt;- rnorm(60)
cortest(y, x)
</code></pre>

<hr>
<h2 id='Correlations'>
Correlation between a vector and a set of variables
</h2><span id='topic+correls'></span>

<h3>Description</h3>

<p>Correlation between a vector and a set of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correls(y, x, type = "pearson", rho = 0, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlations_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Correlations_+3A_x">x</code></td>
<td>

<p>A matrix with the data.
</p>
</td></tr>
<tr><td><code id="Correlations_+3A_type">type</code></td>
<td>

<p>The type of correlation you want. &quot;pearson&quot; and &quot;spearman&quot; are the two supported types because their standard error is easily calculated. For the &quot;groupcorrels&quot; you can also put &quot;kendall&quot; because no hypothesis test is performed in that function.
</p>
</td></tr>
<tr><td><code id="Correlations_+3A_rho">rho</code></td>
<td>

<p>The value of the hypothesised correlation to be used in the hypothesis testing.
</p>
</td></tr>
<tr><td><code id="Correlations_+3A_a">a</code></td>
<td>

<p>The significance level used for the confidence intervals.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions uses the built-in function &quot;cor&quot; which is very fast and then includes confidence intervals and produces a p-value for the hypothesis test.
</p>


<h3>Value</h3>

<p>A matrix with 5 column; the correlation, the p-value for the hypothesis test that each of them is
eaqual to &quot;rho&quot;, the test statistic and the <code class="reqn">a/2\%</code> lower and upper confidence limits.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corpairs">corpairs</a>, <a href="#topic+cortest">cortest</a>, <a href="#topic+pcor">pcor</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(100 * 50 ), ncol = 50)
y &lt;- rnorm(100)
r &lt;- cor(y, x)  ## correlation of y with each of the xs
b &lt;- correls(y, x)
</code></pre>

<hr>
<h2 id='Estimation+20of+20the+20percentage+20of+20null+20p-values'>
Estimation of the percentage of null p-values
</h2><span id='topic+pi0est'></span>

<h3>Description</h3>

<p>Estimation of the percentage of null p-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pi0est(p, lambda = seq(0.05, 0.95, by = 0.01), dof = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20null+2B20p-values_+3A_p">p</code></td>
<td>

<p>A vector of p-values.
</p>
</td></tr>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20null+2B20p-values_+3A_lambda">lambda</code></td>
<td>

<p>A vector of values of the tuning parameter lambda.
</p>
</td></tr>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20null+2B20p-values_+3A_dof">dof</code></td>
<td>

<p>Number of degrees of freedom to use when estimating pi_0 with smoothing splines.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimated proporiton of null p-values is estimated the algorithm by Storey and Tibshirani (2003).
</p>


<h3>Value</h3>

<p>The estimated proportion of non significant (null) p-values. In the paper Storey and Tibshirani mention that the estimate of pi0 is with lambda=1, but in their R code they use the highest value of lambda and thus we do the same here.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Storey J.D. and Tibshirani R. (2003). Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+conf.edge.lower">conf.edge.lower</a>, <a href="#topic+bn.skel.utils">bn.skel.utils</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- pchc::rbn2(1000, p = 20, nei = 3)
x &lt;- A$x
mod &lt;- pchc::mmhc.skel(x, alpha = 0.05 )
pval &lt;- exp(mod$pvalue)
pval &lt;- lower.tri(pval)
pchc::pi0est(pval)
</code></pre>

<hr>
<h2 id='G-square+20and+20Chi-square+20test+20of+20conditional+20indepdence'>
G-square test of conditional indepdence
</h2><span id='topic+g2test'></span><span id='topic+g2test_perm'></span><span id='topic+chi2test'></span>

<h3>Description</h3>

<p>G-square test of conditional indepdence with and without permutations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g2test(x, indx, indy, indz, dc)
chi2test(x, indx, indy, indz, dc)
g2test_perm(x, indx, indy, indz, dc, B)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the data. <b>The minimum must be 0, otherwise the function can crash or will produce
wrong results</b>. The data must be consecutive numbers.
</p>
</td></tr>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_indx">indx</code></td>
<td>

<p>A number between 1 and the number of columns of data. This indicates which variable to take.
</p>
</td></tr>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_indy">indy</code></td>
<td>

<p>A number between 1 and the number of columns of data (other than x). This indicates the other variable whose independence with x is to be tested.
</p>
</td></tr>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_indz">indz</code></td>
<td>

<p>A vector with the indices of the variables to condition upon. It must be non zero and between 1 and the number
of variables. If you want unconditional independence test see <code><a href="#topic+g2test_univariate">g2test_univariate</a></code>,
<code><a href="#topic+chi2test_univariate">chi2test_univariate</a></code>  and <code><a href="#topic+g2test_univariate_perm">g2test_univariate_perm</a></code>. If there is an overlap between x, y and cs you will get 0 as the value of the test statistic.
</p>
</td></tr>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_dc">dc</code></td>
<td>

<p>A numerical value equal to the number of variables (or columns of the data matrix) indicating the number of distinct, unique values (or levels) of each variable. Make sure you give the correct numbers here, otherwise the degrees of freedom will be wrong.
</p>
</td></tr>
<tr><td><code id="G-square+2B20and+2B20Chi-square+2B20test+2B20of+2B20conditional+2B20indepdence_+3A_b">B</code></td>
<td>

<p>The number of permutations. The permutations test is slower than without permutations and should be used with
small sample sizes or when the contigency tables have zeros. When there are few variables, R's &quot;chisq.test&quot; function is faster, but as the number of variables increase the time difference with R's procedure becomes
larger and larger.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions calculates the test statistic of the <code class="reqn">G^2</code> or the <code class="reqn">X^2</code> test of conditional independence between x and y conditional on a set of variable(s) cs.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>

<p>The <code class="reqn">G^2</code> or <code class="reqn">chi^2</code> test statistic.
</p>
</td></tr>
<tr><td><code>df</code></td>
<td>

<p>The degrees of freedom of the test statistic.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The row or variable of the data.
</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>The column or variable of the data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics 57(1): 341-367.
</p>
<p>Tsamardinos, I., &amp; Borboudakis, G. (2010). Permutation testing improves Bayesian network learning.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 322-337). Springer Berlin Heidelberg
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cat.tests">cat.tests</a>, <a href="#topic+g2test_univariate">g2test_univariate</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nvalues &lt;- 2
nvars &lt;- 5
nsamples &lt;- 5000
data &lt;- matrix( sample( 0:(nvalues - 1), nvars * nsamples, replace = TRUE ), nsamples, nvars )
dc &lt;- rep(nvalues, nvars)

g2test( data, 1, 2, 3, c(3, 3, 3) )
g2test_perm( data, 1, 2, 3, c(3, 3, 3), 1000 )
</code></pre>

<hr>
<h2 id='Lower+20limit+20of+20the+20confidence+20of+20an+20edge'>
Lower limit of the confidence of an edge
</h2><span id='topic+conf.edge.lower'></span>

<h3>Description</h3>

<p>Lower limit of the confidence of an edge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf.edge.lower(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lower+2B20limit+2B20of+2B20the+2B20confidence+2B20of+2B20an+2B20edge_+3A_p">p</code></td>
<td>

<p>A numerical vector with the proportion of times an edge was found in the bootstrapped PC algorithm or the confidence of the edge returned by <code><a href="#topic+bn.skel.utils2">bn.skel.utils2</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After having performed PC algorithm many times in the bootstrap samples (using <code><a href="#topic+mmhc.skel.boot">mmhc.skel.boot</a></code> for example) you get a symmetric matrix with the proportion of times an edge was discovered. Take the lower (or upper) triangular elements of that matrix and pass them as input in this function. This will tell you the minimum proportion required to be confident that an edge is trully significant.
</p>


<h3>Value</h3>

<p>The estimated cutoff limit above which an edge can be deemed significant.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Scutari M. and Nagarajan R. (2013). Identifying significant edges in graphical models of molecular networks. Artifficial Intelligence in Medicine, 57: 207-217.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc.skel.boot">pchc.skel.boot</a>, <a href="#topic+mmhc.skel.boot">mmhc.skel.boot</a>, <a href="#topic+fedhc.skel.boot">fedhc.skel.boot</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- pchc::rbn2(200, p = 30, nei = 3)
x &lt;- y$x
g &lt;- pchc::pchc.skel.boot(x, B = 100)$Gboot
a &lt;- g[ lower.tri(g) ]
pchc::conf.edge.lower(a)
</code></pre>

<hr>
<h2 id='Markov+20blanket+20of+20a+20node+20in+20a+20Bayesian+20network'>
Markov blanket of a node in a Bayesian network
</h2><span id='topic+mb'></span>

<h3>Description</h3>

<p>Markov blanket of a node in a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mb(bn, node)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Markov+2B20blanket+2B20of+2B20a+2B20node+2B20in+2B20a+2B20Bayesian+2B20network_+3A_bn">bn</code></td>
<td>

<p>This can either be a bn object or the adjacency matrix.
</p>
</td></tr>
<tr><td><code id="Markov+2B20blanket+2B20of+2B20a+2B20node+2B20in+2B20a+2B20Bayesian+2B20network_+3A_node">node</code></td>
<td>

<p>A vector with one number indicating the node or variable whose Markov blanket is to be returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Markov blanket of a variable (node) is the set of its parents, children and spouses.
</p>


<h3>Value</h3>

<table>
<tr><td><code>parents</code></td>
<td>

<p>The parents of the node of interest.
</p>
</td></tr>
<tr><td><code>children</code></td>
<td>

<p>The children of the node of interest.
</p>
</td></tr>
<tr><td><code>spouses</code></td>
<td>

<p>The spouses of the node of interest. These are the other parents of the children of the node of interest.
</p>
</td></tr>
<tr><td><code>markov.blanket</code></td>
<td>

<p>The Markov blanket of the node of interest. The collection of all the previous.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="#topic+fedhc">fedhc</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+bnplot">bnplot</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- pchc::rbn3(1000, 10, 0.3)
tru &lt;- y$G
x &lt;- y$x
mod &lt;- pchc(x)
pchc::bnplot(mod$dag)
G &lt;- pchc::bnmat(mod$dag)
pchc::mb(G, 6)

</code></pre>

<hr>
<h2 id='Outliers+20free+20data+20via+20the+20reweighted+20MCD'>
Outliers free data via the reweighted MCD
</h2><span id='topic+rmcd'></span>

<h3>Description</h3>

<p>Outliers free data via the reweighted MCD.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmcd(x, alpha = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Outliers+2B20free+2B20data+2B20via+2B20the+2B20reweighted+2B20MCD_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>.
</p>
</td></tr>
<tr><td><code id="Outliers+2B20free+2B20data+2B20via+2B20the+2B20reweighted+2B20MCD_+3A_alpha">alpha</code></td>
<td>

<p>A number controlling the size of the subsets over which the determinant is minimized; roughly alpha*n observations are used for computing the determinant. Values between 0.5 and 1 are allowed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The FEDHC algorithm.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>poia</code></td>
<td>

<p>A vector with the indices of the vectors that were removed.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The outlier free data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Rousseeuw P. J. and Leroy A. M. (1987) Robust Regression and Outlier Detection. Wiley.
</p>
<p>Rousseeuw P. J. and van Driessen K. (1999) A fast algorithm for the minimum covariance determinant estimator. Technometrics 41: 212-223.
</p>
<p>Pison G., Van Aelst S., and Willems G. (2002) Small Sample Corrections for LTS and MCD, Metrika 55: 111-123.
</p>
<p>Hubert M., Rousseeuw P. J. and Verdonck, T. (2012) A deterministic algorithm for robust location and scatter. Journal of Computational and Graphical Statistics 21: 618-637.
</p>
<p>Cerioli A. (2010). Multivariate outlier detection with high-breakdown estimators.Journal of the American Statistical Association 105(489): 147-156.
</p>
<p>Cerchiello P. and Giudici P. (2016). Big data analysis for financial risk management. Journal of Big Data 3(1): 18.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(200 * 20), nrow = 200 )
x1 &lt;- matrix( rnorm(10 * 20, 10), nrow = 10 )
x &lt;- rbind(x, x1)
a &lt;- pchc::rmcd(x)
a$poia
</code></pre>

<hr>
<h2 id='Partial+20correlation+20between+20two+20continuous+20variables'>
Partial correlation
</h2><span id='topic+pcor'></span>

<h3>Description</h3>

<p>Partial correlation between two continuous variables when a correlation matrix is given.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcor(R, indx, indy, indz, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20continuous+2B20variables_+3A_r">R</code></td>
<td>

<p>A correlation or covariance matrix.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20continuous+2B20variables_+3A_indx">indx</code></td>
<td>

<p>The index of the first variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20continuous+2B20variables_+3A_indy">indy</code></td>
<td>

<p>The index of the second variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20continuous+2B20variables_+3A_indz">indz</code></td>
<td>

<p>The index of the conditioning variables.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20continuous+2B20variables_+3A_n">n</code></td>
<td>

<p>The sample size of the data from which the correlation matrix was computed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation or a covariance matrix the function will caclulate the partial correlation between variables indx and indy conditioning on variable(s) indz and will return the logarithm of the p-value.
</p>


<h3>Value</h3>

<p>A numeric vector containing the partial correlation and logged p-value for the test of no partial correlation.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cor2pcor">cor2pcor</a>, <a href="#topic+cortest">cortest</a>, <a href="#topic+correls">correls</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- as.matrix( iris[, 1:2] )
z &lt;- cbind(1, iris[, 3] )
er &lt;- resid( .lm.fit(z, y) )
r &lt;- cor(er)[1, 2]
z &lt;- 0.5 * log( (1 + r) / (1 - r) ) * sqrt( 150 - 1 - 3 )
log(2) + pt( abs(z), 150 - 1 - 3, lower.tail = FALSE, log.p = TRUE )
r &lt;- cor(iris[, 1:3])
pcor(r, 1,2, 3, 150)
</code></pre>

<hr>
<h2 id='Partial+20correlation+20matrix+20from+20correlation+20or+20covariance+20matrix'>
Partial correlation matrix from correlation or covariance matrix
</h2><span id='topic+cor2pcor'></span>

<h3>Description</h3>

<p>Partial correlation matrix from correlation or covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor2pcor(R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20matrix+2B20from+2B20correlation+2B20or+2B20covariance+2B20matrix_+3A_r">R</code></td>
<td>

<p>A correlation or covariance matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation or covariance matrix the function will caclulate the pairwise partial correlation conditional on all other variables.
</p>


<h3>Value</h3>

<p>A matrix where each entry is the partial correlation matrix between each pair of variables conditional on all other variables.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pcor">pcor</a>, <a href="#topic+cortest">cortest</a>, <a href="#topic+correls">correls</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
R &lt;- cor(x)
cor2pcor(R)
pcor(R, 1, 2, 3:4, n = 150)
</code></pre>

<hr>
<h2 id='Plot+20of+20a+20Bayesian+20network'>
Plot of a Bayesian network
</h2><span id='topic+bnplot'></span>

<h3>Description</h3>

<p>Plot of a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnplot(dag, shape = "ellipse", main = NULL, sub = NULL, highlight = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Plot+2B20of+2B20a+2B20Bayesian+2B20network_+3A_dag">dag</code></td>
<td>

<p>A BN object, an object of class &quot;bn&quot;.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20a+2B20Bayesian+2B20network_+3A_shape">shape</code></td>
<td>

<p>A character string defining the shape of the nodes, &quot;ellipse&quot; (default value), &quot;circle&quot; or &quot;rectangle&quot;.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20a+2B20Bayesian+2B20network_+3A_main">main</code></td>
<td>

<p>The main title of the graph displayed on the top.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20a+2B20Bayesian+2B20network_+3A_sub">sub</code></td>
<td>

<p>The subtitle of the graph displayed at the bottom.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20a+2B20Bayesian+2B20network_+3A_highlight">highlight</code></td>
<td>

<p>A list with options specifying which nodes to plot with different colours. You can also check the package bnlearn or
the package Rgraphviz for more information on this, or simply check the example below.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is called from the &quot;bnlearn&quot; package which invokes the &quot;Rgraphviz&quot; package from Bioconductor and you need to install it first.
</p>


<h3>Value</h3>

<p>The Bayesian network is visualised.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="Rfast.html#topic+pc.skel">pc.skel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require("Rgraphviz") ) {
# simulate a dataset with continuous data
x &lt;- matrix( rnorm(100 * 15, 1, 5), nrow = 100 )
colnames(x) &lt;- paste("X", 1:15, sep = "")
nam &lt;- colnames(x)
a &lt;- pchc(x)
bnplot(a$dag)
bnplot( a$dag, highlight = list(nodes = nam[c(2, 3)],
col = "tomato", fill = "orange") )
}

</code></pre>

<hr>
<h2 id='Random+20values+20simulation+20from+20a+20Bayesian+20network'>
Random values simulation from a Bayesian network
</h2><span id='topic+rbn'></span>

<h3>Description</h3>

<p>Random values simulation from a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbn(n, dagobj, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20a+2B20Bayesian+2B20network_+3A_n">n</code></td>
<td>

<p>The number of observations to generate.
</p>
</td></tr>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20a+2B20Bayesian+2B20network_+3A_dagobj">dagobj</code></td>
<td>

<p>A &quot;bn&quot; object. See the examples for more information.
</p>
</td></tr>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20a+2B20Bayesian+2B20network_+3A_x">x</code></td>
<td>

<p>The data used to fit the Bayesian network in a data.frame format.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This information is taken directly from the R package &quot;bnlearn&quot;. This function implements forward/logic sampling: values for the root nodes are sampled from their (un-conditional) distribution, then those of their children conditional on the respective parent sets. Thisis done iteratively until values have been sampled for all nodes.If &quot;dagobj&quot; contains NA parameter estimates (because of unobserved discrete parents configurations in the data the parameters were learned from), rbn will produce observations that contain NAs when thoseparents configurations appear in the simulated samples.
</p>


<h3>Value</h3>

<p>A data frame with the same structure (column names and data types) of the argument &quot;data&quot;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Korb K. and Nicholson A.E. (2010).Bayesian Artificial Intelligence. Chapman &amp; Hall/CRC, 2nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 20, 1, 10), nrow = 200 )
a &lt;- pchc::pchc(x)
sim &lt;- pchc::rbn( 100, dagobj = a$dag, x = x )
</code></pre>

<hr>
<h2 id='Read+20big+20data+20or+20a+20big.matrix+20object'>
Read big data or a big.matrix object
</h2><span id='topic+big_read'></span>

<h3>Description</h3>

<p>Read big data or a big.matrix object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>big_read(big_path, select, header = TRUE, sep = ",")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_big_path">big_path</code></td>
<td>

<p>The path (including the name) where the big.matrix object is.
</p>
</td></tr>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_select">select</code></td>
<td>

<p>Indices of columns to read (sorted). The length of select will be the number of columns of the
resulting filebacked Big Matrix.
</p>
</td></tr>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_header">header</code></td>
<td>

<p>If there are column names, then this should be TRUE.
</p>
</td></tr>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_sep">sep</code></td>
<td>

<p>A field delimiter, for example &quot;;&quot; or &quot;,&quot; (comma separated). See also <code><a href="utils.html#topic+read.csv">read.csv</a></code> for more information.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data (matrix) which will be read and compressed into a big.matrix object must be of type &quot;numeric&quot;.
I tested it and it works with &quot;integer&quot; as well. But, in general, bear in mind that only matrices will be read.
I have not tested with data.frame for example. However, in the help page of &quot;bigmemory&quot; this is mentioned:
Any non-numeric entry will be ignored and replaced with NA, so reading something that traditionally would
be a data.frame won't cause an error. A warning is issued. In all cases, the big.matrix is turned into a
Filebacked Big Matrix (FBM) of type 'double' the object size is alwasy 680 bytes! If the initial dataset
has row names these will be ignored and a column with NAs will apear. So check your final FBM matrix.
For more information see the &quot;bigmemory&quot; and &quot;bigstatsr&quot; packages.
</p>


<h3>Value</h3>

<p>A Filebacked Big Matrix (FBM) matrix.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+big_cor">big_cor</a>, <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( runif(100 * 5, 1, 100), ncol = 5 )
</code></pre>

<hr>
<h2 id='ROC+20and+20AUC'>
ROC and AUC
</h2><span id='topic+auc'></span>

<h3>Description</h3>

<p>Receiver operating curve and area under the curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auc(group, preds, roc = FALSE, cutoffs = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_group">group</code></td>
<td>

<p>A numerical vector with the predicted values of each group as 0 and 1.
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_preds">preds</code></td>
<td>

<p>The predicted values of each group.
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_roc">roc</code></td>
<td>

<p>If you want the ROC to appear set it to TRUE.
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_cutoffs">cutoffs</code></td>
<td>

<p>If you provide a vector with decreasing numbers from 1 to 0 that will be used for the ROC, otherwise, the values from 1 to 0 with a step equal to -0.01 will be used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ara under the curve is returned. The user has the option of getting the receiver operating curve as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>cutoffs</code></td>
<td>

<p>The cutoff values.
</p>
</td></tr>
<tr><td><code>sensitivity</code></td>
<td>

<p>The sensitivity values for each cutoff value.
</p>
</td></tr>
<tr><td><code>specificity</code></td>
<td>

<p>The specificity value for each cutoff value.
</p>
</td></tr>
<tr><td><code>youden</code></td>
<td>

<p>The pair of of 1- specificity and sensitivity where the Youden's J appears on the graph and the Youden index which is defined as the maximum value of sensitivity - specificity + 1.
</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>

<p>The area under the curve, plus a circle with the point where Youden's J is located. If &quot;roc&quot; is set to FALSE, this is the only item in the list to be returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bn.skel.utils">bn.skel.utils</a>, <a href="#topic+conf.edge.lower">conf.edge.lower</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>g &lt;- rbinom(150, 1, 0.6)
f &lt;- rnorm(150)
pchc::auc(g, f, roc = FALSE)
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20FEDHC+20algorithm'>
The skeleton of a Bayesian network produced by the FEDHC algorithm
</h2><span id='topic+fedhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network produced by the FEDHC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fedhc.skel(x, method = "pearson", alpha = 0.05, robust = FALSE,
ini.stat = NULL, R = NULL, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them
into a matrix using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data,
the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero.
The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the PCHC algorithm? If yes, set this to
TRUE to utilise the MCD.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_parallel">parallel</code></td>
<td>

<p>Set this to TRUE if you have millions of observations. In that instance it can reduce the
computaitonal time by 1/3.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to MMHC and PCHC the first phase consists of a variable selection procedure,
the FBED algortihm (Borboudakis and Tsamardinos, 2019).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the logarithm of the p-values of the updated associations.
This final p-value is the maximum p-value among the two p-values in the end.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j
have an edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics, 10(25): 2604.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping.
Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning 65(1):31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+fedhc">fedhc</a>, <a href="#topic+fedhc.skel.boot">fedhc.skel.boot</a>, <a href="#topic+dcor.fedhc.skel">dcor.fedhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 50, 1, 10), nrow = 200 )
a &lt;- fedhc.skel(x)
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20FEDHC+20algorithm+20using+20the+20distance+20correlation'>
The skeleton of a Bayesian network produced by the FEDHC algorithm using the distance correlation
</h2><span id='topic+dcor.fedhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network produced by the FEDHC algorithm using the distance correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcor.fedhc.skel(x, alpha = 0.05, ini.stat = NULL, R = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm+2B20using+2B20the+2B20distance+2B20correlation_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them
into a matrix using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data,
the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm+2B20using+2B20the+2B20distance+2B20correlation_+3A_alpha">alpha</code></td>
<td>

<p>The significance level (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm+2B20using+2B20the+2B20distance+2B20correlation_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm+2B20using+2B20the+2B20distance+2B20correlation_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As in FEDHC the first phase consists of a variable selection procedure,
the FBED algortihm (Borboudakis and Tsamardinos, 2019) which is performed
though by utilizing the distance correlation (Szekely et al., 2007,
Szekely and Rizzo 2014, Huo and Szekely, 2016).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the logarithm of the p-values of the updated associations.
This final p-value is the maximum p-value among the two p-values in the end.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j
have an edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm.
Mathematics, 10(25): 2604.
</p>
<p>Szekely G.J., Rizzo M.L. and Bakirov N.K. (2007). Measuring and Testing Independence
by Correlation of Distances. Annals of Statistics, 35(6):2769-2794.
</p>
<p>Szekely G.J. and Rizzo M. L. (2014). Partial distance correlation with methods for dissimilarities.
Annals of Statistics, 42(6), 2382-2412.
</p>
<p>Huo X. and Szekely G.J. (2016). Fast computing for distance covariance.
Technometrics, 58(4), 435-447.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+fedhc.skel.boot">fedhc.skel.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(500 * 30, 1, 10), nrow = 500 )
a &lt;- dcor.fedhc.skel(x)
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20MMHC+20algorithm'>
The skeleton of a Bayesian network learned with the MMHC algorithm
</h2><span id='topic+mmhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network learned with the MMHC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmhc.skel(x, method = "pearson", max_k = 3, alpha = 0.05,
robust = FALSE, ini.stat = NULL, R = NULL, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0.
No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the PCHC algorithm? If yes, set this to TRUE to utilise the
MCD.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_parallel">parallel</code></td>
<td>

<p>Set this to TRUE if you have millions of observations. In that instance it can reduce the
computaitonal time by 1/3.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The max_k option: the maximum size of the conditioning set to use in the conditioning independence test. Larger values provide more accurate results, at the cost of higher computational times. When the sample size is small (e.g., <code class="reqn">&lt;50</code> observations) the max_k parameter should be 3 for example, otherwise the conditional independence test may not be able to provide reliable results.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the logarithm of the p-values of the updated associations. This final p-value is the maximum p-value among the two p-values in the end.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsamardinos, I., Aliferis, C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown, L. E., Tsamardinos, I. and Aliferis, C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. Medinfo, 711-715.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning 65(1):31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+mmhc.skel.boot">mmhc.skel.boot</a>  </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 100), nrow = 300 )
a &lt;- mmhc.skel(x)
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20PC+20algorithm'>
The skeleton of a Bayesian network learned with the PC algorithm
</h2><span id='topic+pchc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network learned with the PC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pchc.skel(x, method = "pearson", alpha = 0.05,
robust = FALSE, ini.stat = NULL, R = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance leve for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the PCHC algorithm? If yes, set this to TRUE to utilise the
MCD.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PC algorithm as proposed by Spirtes et al. (2000) is implemented. The variables must be either continuous or
categorical, only. The skeleton of the PC algorithm is order independent, since we are using the third heuristic
(Spirte et al., 2000, pg. 90). At every stage of the algorithm use the pairs which are least statistically associated. The conditioning set consists of variables which are most statistically associated with each other of the pair of variables.
</p>
<p>For example, for the pair (X, Y) there can be two conditioning sets for example (Z1, Z2) and (W1, W2). All p-values and test statistics and degrees of freedom have been computed at the first step of the algorithm. Take the p-values between (Z1, Z2) and (X, Y) and between (Z1, Z2) and (X, Y). The conditioning set with the minimum p-value is used first. If the minimum p-values are the same, use the second lowest p-value. If the unlikely, but not impossible, event of all p-values being the same, the test statistic divided by the degrees of freedom is used as a means of choosing which conditioning set is to be used first.
</p>
<p>If two or more p-values are below the machine epsilon (.Machine$double.eps which is equal to 2.220446e-16), all of them are set to 0. To make the comparison or the ordering feasible we use the logarithm of p-value. Hence,
the logarithm of the p-values is always calculated and used.
</p>
<p>In the case of the <code class="reqn">G^2</code> test of independence (for categorical data) with no permutations, we have incorporated a rule of thumb. If the number of samples is at least 5 times the number of the parameters to be estimated, the test is performed, otherwise, independence is not rejected according to Tsamardinos et al. (2006). We have modified it so that it calculates the p-value using permutations.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The logarithm of the p-values of the univariate associations.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>

<p>The maximum value of k, the maximum cardinality of the conditioning set at which the algorithm stopped.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
<tr><td><code>sepset</code></td>
<td>

<p>A list with the separating sets for every value of k.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. 
The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics. 
Computational Economics, 57(1):341-367.
</p>
<p>Tsamardinos I. and Borboudakis G. (2010) Permutation Testing Improves Bayesian Network Learning. 
In Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2010. 322-337.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+pchc.skel.boot">pchc.skel.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 100), nrow = 300 )
a &lt;- pchc::pchc.skel(x)
</code></pre>

<hr>
<h2 id='The+20FEDHC+20and+20FEDTABU+20Bayesian+20network+20learning+20algorithms'>
The FEDHC and FEDTABU Bayesian network learning algorithms
</h2><span id='topic+fedhc'></span><span id='topic+fedtabu'></span>

<h3>Description</h3>

<p>The FEDHC and FEDTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fedhc(x, method = "pearson", alpha = 0.05, robust = FALSE, skel = NULL,
ini.stat = NULL, R = NULL, restart = 10, score = "bic-g", blacklist = NULL,
whitelist = NULL)

fedtabu(x, method = "pearson", alpha = 0.05, robust = FALSE, skel = NULL,
ini.stat = NULL, R = NULL, tabu = 10, score = "bic-g", blacklist = NULL,
whitelist = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, you can choose either &quot;pearson&quot; or &quot;spearman&quot;. If you have categorical data though, this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The <code><a href="#topic+g2test">g2test</a></code> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the FEDHC algorithm? If yes, set this to TRUE to utilise the MCD.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_skel">skel</code></td>
<td>

<p>If you have the output of the skeleton phase, the output from the function <code><a href="#topic+fedhc.skel">fedhc.skel</a></code> plug it here. This can save time.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="The+2B20FEDHC+2B20and+2B20FEDTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The FEDHC algorithm is implemented. The FBED algortihm (Borboudakis and Tsamardinos, 2019), without the
backward phase, is implemented during the skeleton identification phase. Next, the Hill Climbing greedy
search or the Tabu search is employed to score the network.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+fedhc.skel">fedhc.skel</a> </code> function.
</p>
</td></tr>
<tr><td><code>dag</code></td>
<td>

<p>A &quot;bn&quot; class output. A list including the outcome of the Hill-Climbing or the Tabu search phase.
See the package &quot;bnlearn&quot; for more details.
</p>
</td></tr>
<tr><td><code>scoring</code></td>
<td>

<p>The score value.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2022). The FEDHC Bayesian Network Learning Algorithm. Mathematics 2022, 10(15): 2604.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping.
Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1):31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pchc">pchc</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+fedhc.boot">fedhc.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 10), nrow = 300 )
a &lt;- fedhc(x)
</code></pre>

<hr>
<h2 id='The+20MMHC+20and+20MMTABU+20Bayesian+20network+20learning+20algorithms'>
The MMHC and MMTABU Bayesian network learning algorithms
</h2><span id='topic+mmhc'></span><span id='topic+mmtabu'></span>

<h3>Description</h3>

<p>The MMHC and MMTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmhc(x, method = "pearson", max_k = 3, alpha = 0.05, robust = FALSE,
skel = NULL, ini.stat = NULL, R = NULL, restart = 10, score = "bic-g",
blacklist = NULL, whitelist = NULL)

mmtabu(x, method = "pearson", max_k = 3, alpha = 0.05, robust = FALSE,
skel = NULL, ini.stat = NULL, R = NULL, tabu = 10, score = "bic-g",
blacklist = NULL, whitelist = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0.
No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero.
The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the MMHC algorithm? If yes, set this to TRUE to utilise the MCD.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_skel">skel</code></td>
<td>

<p>If you have the output of the skeleton phase, the output from the function <code><a href="#topic+mmhc.skel">mmhc.skel</a></code> plug it here. This can save time.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="The+2B20MMHC+2B20and+2B20MMTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMHC algorithm is implemented without performing the backward elimination during the skeleton identification phase.
The MMHC as described in Tsamardinos et al. (2006) employs the MMPC algorithm during the skeleton construction phase and the Tabu search in the scoring phase. In this package, the mmhc function employs the Hill Climbing greedy search in the scoring phase while the mmtabu employs the Tabu search.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+mmhc.skel">mmhc.skel</a> </code> function.
</p>
</td></tr>
<tr><td><code>dag</code></td>
<td>

<p>A &quot;bn&quot; class output. A list including the outcome of the Hill-Climbing or the Tabu search phase. See the package &quot;bnlearn&quot; for more details.
</p>
</td></tr>
<tr><td><code>scoring</code></td>
<td>

<p>The score value.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1): 31-78.
</p>
<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics, 57(1):341-367.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fedhc">fedhc</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+mmhc.boot">mmhc.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 10), nrow = 300 )
a &lt;- mmhc(x)
</code></pre>

<hr>
<h2 id='The+20PCHC+20and+20PCTABU+20Bayesian+20network+20learning+20algorithms'>
The PCHC and PCTABU Bayesian network learning algorithms
</h2><span id='topic+pchc'></span><span id='topic+pctabu'></span>

<h3>Description</h3>

<p>The PCHC and PCTABU Bayesian network learning algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pchc(x, method = "pearson", alpha = 0.05, robust = FALSE, skel = NULL,
ini.stat = NULL, R = NULL, restart = 10, score = "bic-g", blacklist = NULL,
whitelist = NULL)

pctabu(x, method = "pearson", alpha = 0.05, robust = FALSE, skel = NULL,
ini.stat = NULL, R = NULL, tabu = 10, score = "bic-g", blacklist = NULL,
whitelist = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code><a href="Rfast.html#topic+data.frame.to_matrix">data.frame.to_matrix</a></code>. Note, that for the categorical case data, the numbers must start from 0.
No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_method">method</code></td>
<td>

<p>If you have continuous data, you can choose either &quot;pearson&quot; or &quot;spearman&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The <code><a href="#topic+g2test">g2test</a></code>
and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_alpha">alpha</code></td>
<td>

<p>The significance level for assessing the p-values.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_robust">robust</code></td>
<td>

<p>Do you want outliers to be removed prior to applying the PCHC algorithm? If yes, set this to TRUE to utilise the MCD.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_skel">skel</code></td>
<td>

<p>If you have the output of the skeleton phase, the output from the function <code><a href="#topic+pchc.skel">pchc.skel</a></code> plug it here. This can save time.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_restart">restart</code></td>
<td>

<p>An integer, the number of random restarts.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_tabu">tabu</code></td>
<td>

<p>An integer, the length of the tabu list used in the tabu function.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_score">score</code></td>
<td>

<p>A character string, the label of the network score to be used in the algorithm. If none is specified,
the default score is the Bayesian Information Criterion for both discrete and continuous data sets.
The available score for continuous variables are: &quot;bic-g&quot; (default), &quot;loglik-g&quot;, &quot;aic-g&quot;, &quot;bic-g&quot; or &quot;bge&quot;.
The available score categorical variables are: &quot;bde&quot;, &quot;loglik&quot; or &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs not to
be included in the graph.
</p>
</td></tr>
<tr><td><code id="The+2B20PCHC+2B20and+2B20PCTABU+2B20Bayesian+2B20network+2B20learning+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>

<p>A data frame with two columns (optionally labeled &quot;from&quot; and &quot;to&quot;), containing a set of arcs to be
included in the graph.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PC algorithm as proposed by Spirtes et al. (2001) is first implemented followed by a scoring phase,
such as hill climbing or tabu search. The PCHC was proposed by Tsagris (2021), while the PCTABU algorithm
is the same but instead of the hill climbing scoring phase, the tabu search is employed.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini</code></td>
<td>

<p>A list including the output of the <code> <a href="#topic+pchc.skel">pchc.skel</a> </code> function.
</p>
</td></tr>
<tr><td><code>dag</code></td>
<td>

<p>A &quot;bn&quot; class output. A list including the outcome of the Hill-Climbing or the Tabu search phase.
See the package &quot;bnlearn&quot; for more details.
</p>
</td></tr>
<tr><td><code>scoring</code></td>
<td>

<p>The score value.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2021). A new scalable Bayesian network learning algorithm with applications to economics.
Computational Economics, 57(1):341-367.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I. and Borboudakis G. (2010) Permutation Testing Improves Bayesian Network Learning. In Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2010, 322-337.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1): 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fedhc">fedhc</a>, <a href="#topic+mmhc">mmhc</a>, <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+pchc.boot">pchc.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 10), nrow = 300 )
a &lt;- pchc(x)
</code></pre>

<hr>
<h2 id='Topological+20sort+20of+20a+20Bayesian+20network'>
Topological sort of a Bayesian network
</h2><span id='topic+topological_sort'></span>

<h3>Description</h3>

<p>Topological sort of a Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topological_sort(dag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Topological+2B20sort+2B20of+2B20a+2B20Bayesian+2B20network_+3A_dag">dag</code></td>
<td>

<p>A square matrix representing a directed graph which contains 0s and 1s. If G[i, j] = 1 it
means there is an arrow from node i to node j. When there is no edge between nodes i and j if G[i, j] = 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is an R translation from an old matlab code.
</p>


<h3>Value</h3>

<p>A vector with numbers indicating the sorting. If the matrix does not correspond to a Bayesian network (or a DAG),
NA will be returned.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian network structures.
Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Canada, 87-98.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bnplot">bnplot</a>, <a href="#topic+pchc">pchc</a>, <a href="#topic+pchc.skel">pchc.skel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- pchc::rbn3(100, 20, 0.2)
Gtrue &lt;- y$G
a &lt;- pchc::pchc(y$x)
G &lt;- bnmat(a$dag)
topological_sort(G)
</code></pre>

<hr>
<h2 id='Utilities+20for+20the+20skeleton+20of+20a+20+28Bayesian+29+20network'>
Utilities for the skeleton of a (Bayesian) Network
</h2><span id='topic+bn.skel.utils'></span><span id='topic+bn.skel.utils2'></span>

<h3>Description</h3>

<p>Utilities for the skeleton of a (Bayesian) Network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bn.skel.utils(bnskel.obj, G = NULL, roc = TRUE, alpha = 0.05)
bn.skel.utils2(bnskel.obj, G = NULL, roc = TRUE, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20network_+3A_bnskel.obj">bnskel.obj</code></td>
<td>

<p>An object as retured by pc.skel, glmm.pc.skel or mmhc.skel.
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20network_+3A_g">G</code></td>
<td>

<p>The true adjacency matrix with 1 indicating an edge and zero its absence. Symmetric or not is not important. If this is not available, leave it NULL.
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20network_+3A_roc">roc</code></td>
<td>

<p>Do you want a graph with the ROC curve be returned? Default value is TRUE.
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20network_+3A_alpha">alpha</code></td>
<td>

<p>The significance level ( suitable values in (0, 1) ) for assessing the p-values. Default value is 0.01.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the true adjaceny matrix one can evaluate the estimated adjacency matrix, skeleton, of the PC or the MMHC algorithm.
</p>
<p>The bn.skels.utils give you the area under the curve, false discovery rate and sorting of the edges based on their p-values.
</p>
<p>The bn.skel.utils2 estimates the confidence of each edge. The estimated proportion of null p-values is estimated the algorithm by Storey and Tibshirani (2003).
</p>


<h3>Value</h3>

<p>For the &quot;bn.skel.utils&quot; a list including:
</p>
<table>
<tr><td><code>fdr</code></td>
<td>

<p>The false discovery rate as estimated using the Benjamini-Hochberg correction.
</p>
</td></tr>
<tr><td><code>area</code></td>
<td>

<p>This is a list with the elements of the <code><a href="#topic+auc">auc</a></code> function. The area under the curve, the sensitivy and specificity for a range of values, the Youden index, etc.
</p>
</td></tr>
<tr><td><code>sig.pvalues</code></td>
<td>

<p>A matrix with the row and column of each significant p-value sorted in asending order. As we move down the matrix, the p-values increase and hence the strength of the associations decreases.
</p>
</td></tr>
</table>
<p>For the &quot;bn.skel.utils2&quot; a list including:
</p>
<table>
<tr><td><code>area</code></td>
<td>

<p>This is a list with the elements of the <code><a href="#topic+auc">auc</a></code> function. The area under the curve, the sensitivy and specificity for a range of values, the Youden index, etc.
</p>
</td></tr>
<tr><td><code>pxy</code></td>
<td>

<p>A matrix with the row and column of the confidence of each p-value sorted in asending order. As we move down the matrix, the confidences decrease.
</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>

<p>The lower confidcence limit of an edge as estimated by <code><a href="#topic+conf.edge.lower">conf.edge.lower</a></code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsamardinos I. and Brown L.E. Bounding the False Discovery Rate in Local Bayesian Network Learning.
AAAI, 2008.
</p>
<p>Triantafillou S., Tsamardinos I. and Roumpelaki A. (2014). Learning neighborhoods of high confidence in
constraint-based causal discovery. In European Workshop on Probabilistic Graphical Models, pp. 487-502.
</p>
<p>Storey J.D. and Tibshirani R. (2003). Statistical significance for genome-wide experiments. Proceedings of
the National Academy of Sciences, 100: 9440-9445.
</p>
<p>Benjamini Y. and Hochberg Y. (1995). Controlling the false discovery rate: a practical and powerful approach
to multiple testing. Journal of the Royal Statistical Society Series B, 57(1), 289-300.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press,
Cambridge, MA, USA, 3nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+conf.edge.lower">conf.edge.lower</a>, <a href="#topic+pchc.skel">pchc.skel</a>, <a href="#topic+rbn2">rbn2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- pchc::rbn2(200, p = 25, nei = 3)
x &lt;- y$x
G &lt;- y$G
mod &lt;- pchc::pchc.skel(x, method = "pearson", alpha = 0.05)
G &lt;- G + t(G)
bn.skel.utils(mod, G, roc = FALSE, alpha = 0.05)
bn.skel.utils2(mod, G, roc = FALSE, alpha = 0.05)
</code></pre>

<hr>
<h2 id='Variable+20selection+20for+20continuous+20data+20using+20the+20FBED+20algorithm'>
Variable selection for continuous data using the FBED algorithm
</h2><span id='topic+cor.fbed'></span>

<h3>Description</h3>

<p>Variable selection for continuous data using the FBED algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.fbed(y, x, ystand = TRUE, xstand = TRUE, alpha = 0.05, K = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_y">y</code></td>
<td>

<p>The response variable, a numeric vector.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples and the columns are the variables.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_ystand">ystand</code></td>
<td>

<p>If this is TRUE the response variable is centered. The mean is subtracted from every value.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_xstand">xstand</code></td>
<td>

<p>If this is TRUE the independent variables are standardised.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20FBED+2B20algorithm_+3A_k">K</code></td>
<td>

<p>The number of times to repeat the process. The default value is 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>FBED stands for Forward Backward with Earcly Dropping. It is a variation of the classical forward selection, where at each step, only the
statistically significant variables carry on. The rest are dropped. The process stops when no other variables can be selected. If K = 1, the process
is repeated testing sequentially again all those that have not been selected. If K &gt; 1, then this is repeated.
</p>
<p>In the end, the backward selection is performed to remove any falsely included variables. This backward phase has not been implemented yet.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the process.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the index of the selected variable, their test statistic value and the associated p-value.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with two columns. The cumulative number of variables selected and the number of tests for each value of K.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping.
Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.sel">pc.sel</a>, <a href="#topic+mmpc">mmpc</a>, <a href="#topic+cortest">cortest</a>, <a href="#topic+correls">correls</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(50 * 50), ncol = 50 )
y &lt;- rnorm(50)
a &lt;- pchc::cor.fbed(y, x)
a
</code></pre>

<hr>
<h2 id='Variable+20selection+20for+20continuous+20data+20using+20the+20MMPC+20algorithm'>
Variable selection for continuous data using the MMPC algorithm
</h2><span id='topic+mmpc'></span>

<h3>Description</h3>

<p>Variable selection for continuous data using the MMPC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc(y, x, max_k = 3, alpha = 0.05, method = "pearson",
ini = NULL, hash = FALSE, hashobject = NULL, backward = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_y">y</code></td>
<td>

<p>The class variable. Provide a numeric vector.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>The main dataset. Provide a numeric matrix.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional independence test. Provide an integer.
</p>
<p>The default value set is 3.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>Threshold for assessing p-values' significance.
Provide a double value, between 0.0 and 1.0.
</p>
<p>The default value set is 0.05.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>Currently only &quot;pearson&quot; is supported.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_ini">ini</code></td>
<td>

<p>This argument is used for the avoidance of the univariate associations re-calculations, in the case of them being present. Provide it in the form of a list.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_hash">hash</code></td>
<td>

<p>Boolean value for the activation of the statistics storage in a hash type object.
</p>
<p>The default value is false.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_hashobject">hashobject</code></td>
<td>

<p>This argument is used for the avoidance of the hash re-calculation, in the case of them being present, similarly to ini argument. Provide it in the form of a hash.
</p>
<p>Please note that the generated hash object should be used only when the same dataset is re-analyzed, possibly with different values of max_k and alpha.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20MMPC+2B20algorithm_+3A_backward">backward</code></td>
<td>

<p>Boolean value for the activation of the backward/symmetry correction phase. This option removes and falsely included variables in the parents and children set of the target variable. The backward option seems dubious. Please do not use at the moment.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMPC function implements the MMPC algorithm as presented in
&quot;Tsamardinos, Brown and Aliferis. The max-min hill-climbing
Bayesian network structure learning algorithm&quot;
http://www.dsl-lab.org/supplements/mmhc_paper/paper_online.pdf
</p>


<h3>Value</h3>

<p>The output of the algorithm is an list including:
</p>
<table>
<tr><td><code>selected</code></td>
<td>

<p>The order of the selected variables according to the increasing pvalues.
</p>
</td></tr>
<tr><td><code>hashobject</code></td>
<td>

<p>The hash object containing the statistics calculated in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. Particularly,
this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate
higher association.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to the aforementioned pvalues (higher values indicate higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations; the test
statistics and their corresponding logged p-values.
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k value used in the current execution.
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>

<p>The alpha value used in the current execution.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If hash = TRUE, the number of tests performed will be returned.
If hash != TRUE, the number of univariate associations will be returned.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The time (in seconds) that was needed for the execution of algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M. and Tsamardinos I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>
<p>Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets, Lagani V. and Athineou
G. and Farcomeni A. and Tsagris M. and Tsamardinos I. (2017). Journal of Statistical Software, 80(7).
</p>
<p>Tsamardinos, I., Aliferis, C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal
relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown L. E., Tsamardinos, I. and Aliferis C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning.
Medinfo, 711-715.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning,
65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cor.fbed">cor.fbed</a>, <a href="#topic+pc.sel">pc.sel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(50 * 50), ncol = 50 )
y &lt;- rnorm(50)
a &lt;- pchc::mmpc(y, x)
</code></pre>

<hr>
<h2 id='Variable+20selection+20for+20continuous+20data+20using+20the+20PC-simple+20algorithm'>Variable selection for continuous data using the PC-simple algorithm
</h2><span id='topic+pc.sel'></span>

<h3>Description</h3>

<p>Variable selection for continuous data using the PC-simple algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.sel(y, x, ystand = TRUE, xstand = TRUE, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_y">y</code></td>
<td>

<p>A numerical vector with continuous data.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data; the independent variables, of which some will probably be selected.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_ystand">ystand</code></td>
<td>

<p>If this is TRUE the response variable is centered. The mean is subtracted from every value.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_xstand">xstand</code></td>
<td>

<p>If this is TRUE the independent variables are standardised.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20for+2B20continuous+2B20data+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variable selection for continuous data only is performed using the PC-simple algorithm
(Buhlmann, Kalisch and Maathuis, 2010). The PC algorithm used to infer the skeleton of a Bayesian
Network has been adopted in the context of variable selection. In other words, the PC algorithm
is used for a single node.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>A vector with the selected variables.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests performed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Buhlmann P., Kalisch M. and Maathuis M. H. (2010). Variable selection in high-dimensional linear models:
partially faithful distributions and the PC-simple algorithm. Biometrika, 97(2): 261-278.
<a href="https://arxiv.org/pdf/0906.3204.pdf">https://arxiv.org/pdf/0906.3204.pdf</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmpc">mmpc</a>, <a href="#topic+cor.fbed">cor.fbed</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(50 * 50), ncol = 50 )
y &lt;- rnorm(50)
a &lt;- pchc::pc.sel(y, x)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
