<!DOCTYPE html><html lang="en"><head><title>Help for package filling</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {filling}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#filling-package'><p>Matrix Completion, Imputation, and Inpainting Methods</p></a></li>
<li><a href='#aux.rndmissing'><p>Randomly assign NAs to the data matrix with probability <code>x</code></p></a></li>
<li><a href='#fill.HardImpute'><p>HardImpute : Generalized Spectral Regularization</p></a></li>
<li><a href='#fill.KNNimpute'><p>Imputation using Weighted K-nearest Neighbors</p></a></li>
<li><a href='#fill.nuclear'><p>Low-Rank Completion with Nuclear Norm Optimization</p></a></li>
<li><a href='#fill.OptSpace'><p>OptSpace</p></a></li>
<li><a href='#fill.simple'><p>Imputation by Simple Rules</p></a></li>
<li><a href='#fill.SoftImpute'><p>SoftImpute : Spectral Regularization</p></a></li>
<li><a href='#fill.SVDimpute'><p>Iterative Regression against Right Singular Vectors</p></a></li>
<li><a href='#fill.SVT'><p>Singular Value Thresholding for Nuclear Norm Optimization</p></a></li>
<li><a href='#fill.USVT'><p>Matrix Completion by Universal Singular Value Thresholding</p></a></li>
<li><a href='#lena128'><p>lena image at size of <code class="reqn">(128 \times 128)</code></p></a></li>
<li><a href='#lena256'><p>lena image at size of <code class="reqn">(256 \times 256)</code></p></a></li>
<li><a href='#lena64'><p>lena image at size of <code class="reqn">(64 \times 64)</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Matrix Completion, Imputation, and Inpainting Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Filling in the missing entries of a partially observed data is one of fundamental problems in various disciplines of mathematical science. For many cases, data at our interests have canonical form of matrix in that the problem is posed upon a matrix with missing values to fill in the entries under preset assumptions and models. We provide a collection of methods from multiple disciplines under Matrix Completion, Imputation, and Inpainting. See Davenport and Romberg (2016) &lt;<a href="https://doi.org/10.1109%2FJSTSP.2016.2539100">doi:10.1109/JSTSP.2016.2539100</a>&gt; for an overview of the topic.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>CVXR (&ge; 1.0), Rcpp, Rdpack, ROptSpace, RSpectra, nabor,
stats, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-21 03:41:59 UTC; kisung</td>
</tr>
<tr>
<td>Author:</td>
<td>Kisung You <a href="https://orcid.org/0000-0002-8584-459X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kisung You &lt;kisungyou@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-21 15:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='filling-package'>Matrix Completion, Imputation, and Inpainting Methods</h2><span id='topic+filling-package'></span>

<h3>Description</h3>

<p>Filling in the missing entries of a partially observed data is one of fundamental problems
in various disciplines of mathematical science. For many cases, data at our interests
have canonical form of matrix in that the problem is posed upon a matrix with missing values
to fill in the entries under preset assumptions and models.
We provide a collection of methods from multiple disciplines under Matrix Completion, Imputation, and Inpainting.
Currently, we have following methods implemented,
</p>

<table>
<tr>
 <td style="text-align: left;">
<em>Name of Function</em> </td><td style="text-align: left;"> <em>Method</em> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.HardImpute</code> </td><td style="text-align: left;"> Generalized Spectral Regularization </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.KNNimpute</code>  </td><td style="text-align: left;"> Weighted <code class="reqn">K</code>-nearest Neighbors </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.nuclear</code> </td><td style="text-align: left;"> Nuclear Norm Optimization </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.OptSpace</code> </td><td style="text-align: left;"> OptSpace </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.simple</code> </td><td style="text-align: left;"> Simple Rules of Mean, Median, and Random </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.SoftImpute</code> </td><td style="text-align: left;"> Spectral Regularization </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.SVDimpute</code> </td><td style="text-align: left;"> Iterative Regression against Right Singular Vectors </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.SVT</code> </td><td style="text-align: left;"> Singular Value Thresholding for Nuclear Norm Optimization </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>fill.USVT</code> </td><td style="text-align: left;"> Universal Singular Value Thresholding
</td>
</tr>

</table>


<hr>
<h2 id='aux.rndmissing'>Randomly assign NAs to the data matrix with probability <code>x</code></h2><span id='topic+aux.rndmissing'></span>

<h3>Description</h3>

<p><code>aux.rndmissing</code> randomly selects <code class="reqn">100\cdot x</code>% of entries from
a given data matrix and turns them into missing entries, i.e., their values
become <code>NA</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.rndmissing(A, x = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aux.rndmissing_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> data matrix.</p>
</td></tr>
<tr><td><code id="aux.rndmissing_+3A_x">x</code></td>
<td>
<p>percentage of turning current entries into missing (<code>NA</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(n\times p)</code> data matrix with missing entries at proportion <code class="reqn">x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load lena64 image matrix
data(lena64)

# generate 10% of missing values
lena64_miss &lt;- aux.rndmissing(lena64)

# visualize
par(mfrow=c(1,2))
image(lena64, axes=FALSE, main="original image")
image(lena64_miss, axes=FALSE, main="10% missing entries")

</code></pre>

<hr>
<h2 id='fill.HardImpute'>HardImpute : Generalized Spectral Regularization</h2><span id='topic+fill.HardImpute'></span>

<h3>Description</h3>

<p>If the assumed underlying model has sufficiently many zeros, the LASSO type
shrinkage estimator is known to overestimate the number of non-zero coefficients.
<code>fill.HardImpute</code> aims at overcoming such difficulty via low-rank assumption
and hard thresholding idea, well-known concept in conventional regression analysis.
In algorithmic aspect, it takes output of <code>SoftImpute</code> as warm-start matrices
for iterative estimation process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.HardImpute(
  A,
  lambdas = c(10, 1, 0.1),
  maxiter = 100,
  tol = 0.001,
  rk = (min(dim(A)) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.HardImpute_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.HardImpute_+3A_lambdas">lambdas</code></td>
<td>
<p>a length-<code class="reqn">t</code> vector regularization parameters.</p>
</td></tr>
<tr><td><code id="fill.HardImpute_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations to be performed.</p>
</td></tr>
<tr><td><code id="fill.HardImpute_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for an incremental progress.</p>
</td></tr>
<tr><td><code id="fill.HardImpute_+3A_rk">rk</code></td>
<td>
<p>assumed rank of the matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p\times t)</code> cubic array after completion at each <code>lambda</code> value.</p>
</dd>
</dl>



<h3>References</h3>

<p>Mazumder R, Hastie T, Tibshirani R (2010).
&ldquo;Spectral Regularization Algorithms for Learning Large Incomplete Matrices.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>11</b>, 2287&ndash;2322.
ISSN 1532-4435.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fill.SoftImpute">fill.SoftImpute</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method with 3 rank conditions
fill1 &lt;- fill.HardImpute(A, lambdas=c(500,100,50), rk=10)
fill2 &lt;- fill.HardImpute(A, lambdas=c(500,100,50), rk=50)
fill3 &lt;- fill.HardImpute(A, lambdas=c(500,100,50), rk=100)

## visualize only the last ones from each run
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill1$X[,,3], col=gray((0:100)/100), axes=FALSE, main="Rank 10")
image(fill2$X[,,3], col=gray((0:100)/100), axes=FALSE, main="Rank 50")
image(fill3$X[,,3], col=gray((0:100)/100), axes=FALSE, main="Rank 100")
par(opar)

</code></pre>

<hr>
<h2 id='fill.KNNimpute'>Imputation using Weighted K-nearest Neighbors</h2><span id='topic+fill.KNNimpute'></span>

<h3>Description</h3>

<p>One of the simplest idea to <em>guess</em> missing entry is to use
portion of the data that has most similar characteristics across
all covariates. <code>fill.KNNimpute</code> follows such reasoning in that
it finds <code class="reqn">K</code>-nearest neighbors based on observed variables and
uses weighted average of nearest elements to fill in the missing entry.
Note that when there are many missing entries, it's possible that there are
no <em>surrogates</em> to be computed upon. Therefore, if there exists an entire
row or column full of missing entries, the algorithm stops.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.KNNimpute(A, k = ceiling(nrow(A)/2))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.KNNimpute_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.KNNimpute_+3A_k">k</code></td>
<td>
<p>the number of neighbors to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
</dl>



<h3>References</h3>

<p>Troyanskaya O, Cantor M, Sherlock G, Brown P, Hastie T, Tibshirani R, Botstein D, Altman RB (2001).
&ldquo;Missing value estimation methods for DNA microarrays.&rdquo;
<em>Bioinformatics</em>, <b>17</b>(6), 520&ndash;525.
ISSN 1367-4803.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fill.SVDimpute">fill.SVDimpute</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
set.seed(5)
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method with 3 different neighborhood size
fill1 &lt;- fill.KNNimpute(A, k=5)
fill2 &lt;- fill.KNNimpute(A, k=25)
fill3 &lt;- fill.KNNimpute(A, k=50)

## visualize only the last ones from each run
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill1$X, col=gray((0:100)/100), axes=FALSE, main="5-neighbor")
image(fill2$X, col=gray((0:100)/100), axes=FALSE, main="25-neighbor")
image(fill3$X, col=gray((0:100)/100), axes=FALSE, main="50-neighbor")
par(opar)

</code></pre>

<hr>
<h2 id='fill.nuclear'>Low-Rank Completion with Nuclear Norm Optimization</h2><span id='topic+fill.nuclear'></span>

<h3>Description</h3>

<p>In many circumstances, it is natural to assume that there exists an underlying
low-rank structure. The assumption of <em>low-rank</em> property leads to an optimization problem
for matrix completion problem,
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{minimize}\quad rank(X)</code>
</p>

<p style="text-align: center;"><code class="reqn">\mathrm{s.t}~~ X_{ij}=A_{ij} ~~\mathrm{for}~~ A_{ij} \in E</code>
</p>

<p>where <code class="reqn">A_{ij}\in E</code> means the <code class="reqn">(i,j)</code>-th entry of data matrix <code class="reqn">A</code> is not missing. The objective
function can be further relaxed by nuclear norm
</p>
<p style="text-align: center;"><code class="reqn">\|X\|_* = \sum \sigma_i(X)</code>
</p>

<p>where <code class="reqn">\sigma_i (X)</code> is <code class="reqn">i</code>-th singular value of the matrix <code class="reqn">X</code>. Note that
for modeling purpose, we adopted closeness parameter <code>tolerance</code> for equality constraint.
<span class="pkg">CVXR</span> package was used in implementation. Computational efficiency may not be guaranteed for large data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.nuclear(A, tolerance = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.nuclear_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.nuclear_+3A_tolerance">tolerance</code></td>
<td>
<p>level of tolerance for entrywise equality condition.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
<dt>norm</dt><dd><p>solution of the minimization problem; approximate rank.</p>
</dd>
<dt>cvxr.status</dt><dd><p>&ldquo;optimal&rdquo; denotes the problem was solved. See <code><a href="CVXR.html#topic+psolve">psolve</a></code> for more details on solvability.</p>
</dd>
<dt>cvxr.niters</dt><dd><p>the number of iterations taken.</p>
</dd>
<dt>cvxr.solver</dt><dd><p>type of solver used by <span class="pkg">CVXR</span>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Candès EJ, Recht B (2009).
&ldquo;Exact Matrix Completion via Convex Optimization.&rdquo;
<em>Foundations of Computational Mathematics</em>, <b>9</b>(6), 717&ndash;772.
ISSN 1615-3375, 1615-3383.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load image data of 'lena64'
data(lena64)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena64, x=0.05)

## apply the method
filled &lt;- fill.nuclear(A)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(filled$X, col=gray((0:100)/100), axes=FALSE, main="processed")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='fill.OptSpace'>OptSpace</h2><span id='topic+fill.OptSpace'></span>

<h3>Description</h3>

<p>OptSpace is an algorithm for matrix completion when a matrix is partially observed. It
performs what authors called <em>trimming</em> and <em>projection</em> repeatedly based on
singular value decompositions. Original implementation is borrowed from <span class="pkg">ROptSpace</span> package,
which was independently developed by the maintainer. See <code><a href="ROptSpace.html#topic+OptSpace">OptSpace</a></code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.OptSpace(A, ropt = NA, niter = 50, tol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.OptSpace_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.OptSpace_+3A_ropt">ropt</code></td>
<td>
<p><code>NA</code> to guess the rank, or a positive integer as a pre-defined rank.</p>
</td></tr>
<tr><td><code id="fill.OptSpace_+3A_niter">niter</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="fill.OptSpace_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for reconstruction in Frobenius norm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
<dt>error</dt><dd><p>a vector of reconstruction errors for each successive iteration.</p>
</dd>
</dl>



<h3>References</h3>

<p>Keshavan RH, Montanari A, Oh S (2010).
&ldquo;Matrix Completion From a Few Entries.&rdquo;
<em>IEEE Transactions on Information Theory</em>, <b>56</b>(6), 2980&ndash;2998.
ISSN 0018-9448.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load image data of 'lena64'
data(lena64)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena64, x=0.05)

## apply the method with different rank assumptions
filled10 &lt;- fill.OptSpace(A, ropt=10)
filled20 &lt;- fill.OptSpace(A, ropt=20)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(filled10$X, col=gray((0:100)/100), axes=FALSE, main="rank 10")
image(filled20$X, col=gray((0:100)/100), axes=FALSE, main="rank 20")
par(opar)

## End(Not run)


</code></pre>

<hr>
<h2 id='fill.simple'>Imputation by Simple Rules</h2><span id='topic+fill.simple'></span>

<h3>Description</h3>

<p>One of the most simplest ways to fill in the missing entries is
to apply any simple rule for each variable. In this example, we provide
3 options, <code>"mean"</code>, <code>"median"</code>, and <code>"random"</code>. It assumes
that every column has at least one non-missing entries in that for each column,
the rule is applied from the subset of non-missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.simple(A, method = c("mean", "median", "random"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.simple_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.simple_+3A_method">method</code></td>
<td>
<p>simple rule to fill in the missing entries in a columnwise manner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
</dl>



<h3>References</h3>

<p>Gelman A, Hill J (2007).
<em>Data analysis using regression and multilevel/hierarchical models</em>,  Analytical methods for social research.
Cambridge University Press, Cambridge ; New York.
ISBN 978-0-521-86706-1 978-0-521-68689-1, OCLC: ocm67375137.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply all three methods#'
fill1 &lt;- fill.simple(A, method="mean")
fill2 &lt;- fill.simple(A, method="median")
fill3 &lt;- fill.simple(A, method="random")

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="original")
image(fill1$X, col=gray((0:100)/100), axes=FALSE, main="method:mean")
image(fill2$X, col=gray((0:100)/100), axes=FALSE, main="method:median")
image(fill3$X, col=gray((0:100)/100), axes=FALSE, main="method:random")
par(opar)

</code></pre>

<hr>
<h2 id='fill.SoftImpute'>SoftImpute : Spectral Regularization</h2><span id='topic+fill.SoftImpute'></span>

<h3>Description</h3>

<p><code>fill.SoftImpute</code> implements convex relaxation techniques to generate
a sequence of regularized low-rank solutions for matrix completion problems.
For the nuclear norm optimization problem, it uses soft thresholding technique
iteratively in that the algorithm returns several matrices in accordance with
the provided vector of regularization parameters <code class="reqn">\lambda</code> (<code>lambdas</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.SoftImpute(A, lambdas = c(10, 1, 0.1), maxiter = 100, tol = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.SoftImpute_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.SoftImpute_+3A_lambdas">lambdas</code></td>
<td>
<p>a length-<code class="reqn">t</code> vector regularization parameters.</p>
</td></tr>
<tr><td><code id="fill.SoftImpute_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations to be performed.</p>
</td></tr>
<tr><td><code id="fill.SoftImpute_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for an incremental progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p\times t)</code> cubic array after completion at each <code>lambda</code> value.</p>
</dd>
</dl>



<h3>References</h3>

<p>Mazumder R, Hastie T, Tibshirani R (2010).
&ldquo;Spectral Regularization Algorithms for Learning Large Incomplete Matrices.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>11</b>, 2287&ndash;2322.
ISSN 1532-4435.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fill.HardImpute">fill.HardImpute</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method with 3 lambda values
fill &lt;- fill.SoftImpute(A, lambdas=c(500,100,50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill$X[,,1], col=gray((0:100)/100), axes=FALSE, main="lambda=500")
image(fill$X[,,2], col=gray((0:100)/100), axes=FALSE, main="lambda=100")
image(fill$X[,,3], col=gray((0:100)/100), axes=FALSE, main="lambda=50")
par(opar)

</code></pre>

<hr>
<h2 id='fill.SVDimpute'>Iterative Regression against Right Singular Vectors</h2><span id='topic+fill.SVDimpute'></span>

<h3>Description</h3>

<p>Singular Value Decomposition (SVD) is the best low-rank approximation of a given matrix.
<code>fill.SVDimpute</code> exploits such idea. First, it starts with simple filling using
column mean values for filling. Second, it finds SVD of a current matrix. Then,
each row vector is regressed upon top-<code class="reqn">k</code> right singular vectors. Missing entries are
then filled with predicted estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.SVDimpute(A, k = ceiling(ncol(A)/2), maxiter = 100, tol = 0.01)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.SVDimpute_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.SVDimpute_+3A_k">k</code></td>
<td>
<p>the number of regressors to be used.</p>
</td></tr>
<tr><td><code id="fill.SVDimpute_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations to be performed.</p>
</td></tr>
<tr><td><code id="fill.SVDimpute_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for an incremental progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
</dl>



<h3>References</h3>

<p>Troyanskaya O, Cantor M, Sherlock G, Brown P, Hastie T, Tibshirani R, Botstein D, Altman RB (2001).
&ldquo;Missing value estimation methods for DNA microarrays.&rdquo;
<em>Bioinformatics</em>, <b>17</b>(6), 520&ndash;525.
ISSN 1367-4803.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fill.KNNimpute">fill.KNNimpute</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
set.seed(5)
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method with 3 different number of regressors
fill1 &lt;- fill.SVDimpute(A, k=5)
fill2 &lt;- fill.SVDimpute(A, k=25)
fill3 &lt;- fill.SVDimpute(A, k=50)

## visualize only the last ones from each run
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill1$X, col=gray((0:100)/100), axes=FALSE, main="5 regressors")
image(fill2$X, col=gray((0:100)/100), axes=FALSE, main="25 regressors")
image(fill3$X, col=gray((0:100)/100), axes=FALSE, main="50 regressors")
par(opar)

## End(Not run)


</code></pre>

<hr>
<h2 id='fill.SVT'>Singular Value Thresholding for Nuclear Norm Optimization</h2><span id='topic+fill.SVT'></span>

<h3>Description</h3>

<p><code>fill.SVT</code> is an iterative updating scheme for Nuclear Norm Minimization problem. An unconstrained
parahrase of the problem introduced in <code><a href="#topic+fill.nuclear">fill.nuclear</a></code> is
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{minimize}\quad \frac{1}{2}\|P_{\Omega}(X-A) \|_F^2 + \lambda \| X \|_*</code>
</p>

<p>where <code class="reqn">P_{\Omega}(X)=X_{ij}</code> if it is observed, or <code class="reqn">0</code> otherwise.
It performs iterative shrinkage on newly computed singular values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.SVT(A, lambda = 1, maxiter = 100, tol = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.SVT_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.SVT_+3A_lambda">lambda</code></td>
<td>
<p>a regularization parameter.</p>
</td></tr>
<tr><td><code id="fill.SVT_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations to be performed.</p>
</td></tr>
<tr><td><code id="fill.SVT_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for an incremental progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after completion.</p>
</dd>
</dl>



<h3>References</h3>

<p>Cai J, Candès EJ, Shen Z (2010).
&ldquo;A Singular Value Thresholding Algorithm for Matrix Completion.&rdquo;
<em>SIAM Journal on Optimization</em>, <b>20</b>(4), 1956&ndash;1982.
ISSN 1052-6234, 1095-7189.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fill.nuclear">fill.nuclear</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method
fill1 &lt;- fill.SVT(A, lambda=0.1)
fill2 &lt;- fill.SVT(A, lambda=1.0)
fill3 &lt;- fill.SVT(A, lambda=20)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill1$X, col=gray((0:100)/100), axes=FALSE, main="lbd=0.1")
image(fill2$X, col=gray((0:100)/100), axes=FALSE, main="lbd=1")
image(fill3$X, col=gray((0:100)/100), axes=FALSE, main="lbd=10")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='fill.USVT'>Matrix Completion by Universal Singular Value Thresholding</h2><span id='topic+fill.USVT'></span>

<h3>Description</h3>

<p><code>fill.USVT</code> is a matrix <em>estimation</em> method suitable for low-rank structure. In the context of
our package, we provide this method under the matrix <em>completion</em> problem category. It aims at
exploiting the idea of thresholding the singular values to minimize the mean-squared error, defined as
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{MSE}(\hat{A}):={E} \left\{ \frac{1}{np} \sum_{i=1}^{n} \sum_{j=1}^{p} (\hat{a}_{ij} - a_{ij})^2  \right\}</code>
</p>

<p>where <code class="reqn">A</code> is an <code class="reqn">(n\times p)</code> matrix with some missing values and <code class="reqn">\hat{A}</code> is an estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fill.USVT(A, eta = 0.01)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fill.USVT_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> partially observed matrix.</p>
</td></tr>
<tr><td><code id="fill.USVT_+3A_eta">eta</code></td>
<td>
<p>control for thresholding <code class="reqn">\in (0,1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>X</dt><dd><p>an <code class="reqn">(n\times p)</code> estimated matrix after completion, which is <code class="reqn">\hat{A}</code> in the equation above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Chatterjee S (2015).
&ldquo;Matrix estimation by Universal Singular Value Thresholding.&rdquo;
<em>Ann. Statist.</em>, <b>43</b>(1), 177&ndash;214.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load image data of 'lena128'
data(lena128)

## transform 5% of entries into missing
set.seed(5)
A &lt;- aux.rndmissing(lena128, x=0.05)

## apply the method with 3 different control 'eta'
fill1 &lt;- fill.USVT(A, eta=0.01)
fill2 &lt;- fill.USVT(A, eta=0.5)
fill3 &lt;- fill.USVT(A, eta=0.99)

## visualize only the last ones from each run
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
image(A, col=gray((0:100)/100), axes=FALSE, main="5% missing")
image(fill1$X, col=gray((0:100)/100), axes=FALSE, main="eta=0.01")
image(fill2$X, col=gray((0:100)/100), axes=FALSE, main="eta=0.5")
image(fill3$X, col=gray((0:100)/100), axes=FALSE, main="eta=0.99")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='lena128'>lena image at size of <code class="reqn">(128 \times 128)</code></h2><span id='topic+lena128'></span>

<h3>Description</h3>

<p><em>Lena</em> is probably one of the most well-known example in image processing and computer vision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lena128)
</code></pre>


<h3>Format</h3>

<p>matrix of size <code class="reqn">(128\times 128)</code>
</p>


<h3>Source</h3>

<p>USC SIPI Image Database.
</p>


<h3>References</h3>

<p>Gonzalez, Rafael C. and Woods, Richard E. (2017) <em>Digital Image Processing</em> (4th ed.). ISBN 0133356728.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lena128)
image(lena128, col=gray((0:100)/100), axes=FALSE, main="lena128")
</code></pre>

<hr>
<h2 id='lena256'>lena image at size of <code class="reqn">(256 \times 256)</code></h2><span id='topic+lena256'></span>

<h3>Description</h3>

<p><em>Lena</em> is probably one of the most well-known example in image processing and computer vision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lena256)
</code></pre>


<h3>Format</h3>

<p>matrix of size <code class="reqn">(256\times 256)</code>
</p>


<h3>Source</h3>

<p>USC SIPI Image Database.
</p>


<h3>References</h3>

<p>Gonzalez, Rafael C. and Woods, Richard E. (2017) <em>Digital Image Processing</em> (4th ed.). ISBN 0133356728.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lena256)
image(lena256, col=gray((0:100)/100), axes=FALSE, main="lena256")
</code></pre>

<hr>
<h2 id='lena64'>lena image at size of <code class="reqn">(64 \times 64)</code></h2><span id='topic+lena64'></span>

<h3>Description</h3>

<p><em>Lena</em> is probably one of the most well-known example in image processing and computer vision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lena64)
</code></pre>


<h3>Format</h3>

<p>matrix of size <code class="reqn">(64\times 64)</code>
</p>


<h3>Source</h3>

<p>USC SIPI Image Database.
</p>


<h3>References</h3>

<p>Gonzalez, Rafael C. and Woods, Richard E. (2017) <em>Digital Image Processing</em> (4th ed.). ISBN 0133356728.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lena64)
image(lena64, col=gray((0:100)/100), axes=FALSE, main="lena64")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
