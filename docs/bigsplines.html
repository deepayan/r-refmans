<!DOCTYPE html><html><head><title>Help for package bigsplines</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bigsplines}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bigspline'>
<p>Fits Smoothing Spline</p></a></li>
<li><a href='#bigsplines-internal'><p>Internal functions for big splines package</p></a></li>
<li><a href='#bigsplines-package'>
<p>Smoothing Splines for Large Samples</p></a></li>
<li><a href='#bigssa'>
<p>Fits Smoothing Spline ANOVA Models</p></a></li>
<li><a href='#bigssg'>
<p>Fits Generalized Smoothing Spline ANOVA Models</p></a></li>
<li><a href='#bigssp'>
<p>Fits Smoothing Splines with Parametric Effects</p></a></li>
<li><a href='#bigtps'>
<p>Fits Cubic Thin-Plate Splines</p></a></li>
<li><a href='#binsamp'>
<p>Bin-Samples Strategic Knot Indices</p></a></li>
<li><a href='#imagebar'>
<p>Displays a Color Image with Colorbar</p></a></li>
<li><a href='#makessa'>
<p>Makes Objects to Fit Smoothing Spline ANOVA Models</p></a></li>
<li><a href='#makessg'>
<p>Makes Objects to Fit Generalized Smoothing Spline ANOVA Models</p></a></li>
<li><a href='#makessp'>
<p>Makes Objects to Fit Smoothing Splines with Parametric Effects</p></a></li>
<li><a href='#ordspline'>
<p>Fits Ordinal Smoothing Spline</p></a></li>
<li><a href='#plotbar'>
<p>Generic X-Y Plotting with Colorbar</p></a></li>
<li><a href='#plotci'>
<p>Generic X-Y Plotting with Confidence Intervals</p></a></li>
<li><a href='#predict.bigspline'>
<p>Predicts for &quot;bigspline&quot; Objects</p></a></li>
<li><a href='#predict.bigssa'>
<p>Predicts for &quot;bigssa&quot; Objects</p></a></li>
<li><a href='#predict.bigssg'>
<p>Predicts for &quot;bigssg&quot; Objects</p></a></li>
<li><a href='#predict.bigssp'>
<p>Predicts for &quot;bigssp&quot; Objects</p></a></li>
<li><a href='#predict.bigtps'>
<p>Predicts for &quot;bigtps&quot; Objects</p></a></li>
<li><a href='#predict.ordspline'>
<p>Predicts for &quot;ordspline&quot; Objects</p></a></li>
<li><a href='#print'>
<p>Prints Fit Information for bigsplines Model</p></a></li>
<li><a href='#ssBasis'>
<p>Smoothing Spline Basis for Polynomial Splines</p></a></li>
<li><a href='#summary'>
<p>Summarizes Fit Information for bigsplines Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Smoothing Splines for Large Samples</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-05-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>quadprog</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, grDevices</td>
</tr>
<tr>
<td>Description:</td>
<td>Fits smoothing spline regression models using scalable algorithms designed for large samples. Seven marginal spline types are supported: linear, cubic, different cubic, cubic periodic, cubic thin-plate, ordinal, and nominal. Random effects and parametric effects are also supported. Response can be Gaussian or non-Gaussian: Binomial, Poisson, Gamma, Inverse Gaussian, or Negative Binomial.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-25 05:53:06 UTC; Nate</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-25 06:47:54 UTC</td>
</tr>
</table>
<hr>
<h2 id='bigspline'>
Fits Smoothing Spline
</h2><span id='topic+bigspline'></span>

<h3>Description</h3>

<p>Given a real-valued response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code> and a real-valued predictor vector <code class="reqn">\mathbf{x}=\{x_{i}\}_{n\times 1}</code> with <code class="reqn">a \leq x_{i} \leq b \ \forall i</code>, a smoothing spline model has the form </p>
<p style="text-align: center;"><code class="reqn">y_{i}=\eta(x_{i})+e_{i}</code>
</p>
<p> where <code class="reqn">y_{i}</code> is the <code class="reqn">i</code>-th observation's respone, <code class="reqn">x_{i}</code> is the <code class="reqn">i</code>-th observation's predictor, <code class="reqn">\eta</code> is an unknown smooth function relating the response and predictor, and <code class="reqn">e_{i}\sim\mathrm{N}(0,\sigma^{2})</code> is iid Gaussian error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigspline(x,y,type="cub",nknots=30,rparm=0.01,xmin=min(x),
          xmax=max(x),alpha=1,lambdas=NULL,se.fit=FALSE,
          rseed=1234,knotcheck=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigspline_+3A_x">x</code></td>
<td>

<p>Predictor vector.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_y">y</code></td>
<td>

<p>Response vector. Must be same length as <code>x</code>.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_type">type</code></td>
<td>

<p>Type of spline for <code>x</code>. Options include <code>type="lin"</code> for linear, <code>type="cub"</code> for cubic, <code>type="cub0"</code> for different cubic, and <code>type="per"</code> for cubic periodic. See Spline Types section.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_nknots">nknots</code></td>
<td>

<p>Scalar giving maximum number of knots to bin-sample. Use more knots for more jagged functions.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_rparm">rparm</code></td>
<td>

<p>Rounding parameter for <code>x</code>. Use <code>rparm=NA</code> to fit unrounded solution. Rounding parameter must be in interval (0,1].
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_xmin">xmin</code></td>
<td>

<p>Minimum <code>x</code> value (i.e., <code class="reqn">a</code>). Used to transform data to interval [0,1].
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_xmax">xmax</code></td>
<td>

<p>Maximum <code>x</code> value (i.e., <code class="reqn">b</code>). Used to transform data to interval [0,1].
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_alpha">alpha</code></td>
<td>

<p>Manual tuning parameter for GCV score. Using <code>alpha=1</code> gives unbaised esitmate. Using a larger alpha enforces a smoother estimate.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default estimates smoothing parameter that minimizes GCV score.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of fitted values should be estimated. 
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_rseed">rseed</code></td>
<td>

<p>Random seed. Input to <code><a href="base.html#topic+set.seed">set.seed</a></code> to reproduce same knots when refitting same model. Use <code>rseed=NULL</code> to generate a different sample of knots each time.
</p>
</td></tr>
<tr><td><code id="bigspline_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the penalized least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\eta(x_{i}))^{2}+\lambda \int [\ddot{\eta}(x)]^2 dx</code>
</p>
<p> where <code class="reqn">\ddot{\eta}</code> denotes the second derivative of <code class="reqn">\eta</code> and <code class="reqn">\lambda\geq0</code> is a smoothing parameter that controls the trade-off between fitting and smoothing the data. 
</p>
<p>Default use of the function estimates <code class="reqn">\lambda</code> by minimizing the GCV score: </p>
<p style="text-align: center;"><code class="reqn">\mbox{GCV}(\lambda) = \frac{n\|(\mathbf{I}_{n}-\mathbf{S}_{\lambda})\mathbf{y}\|^{2}}{[n-\mathrm{tr}(\mathbf{S}_{\lambda})]^2}</code>
</p>
<p> where <code class="reqn">\mathbf{I}_{n}</code> is the identity matrix and <code class="reqn">\mathbf{S}_{\lambda}</code> is the smoothing matrix (see Computational Details).
</p>
<p>Using the rounding parameter input <code>rparm</code> can greatly speed-up and stabilize the fitting for large samples. When <code>rparm</code> is used, the spline is fit to a set of unique data points after rounding; the unique points are determined using the efficient algorithm described in Helwig (2013). For typical cases, I recommend using <code>rparm=0.01</code>, but smaller rounding parameters (e,g., <code>rparm=0.001</code>) may be needed for particularly jagged functions (or when <code>x</code> has outliers). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values corresponding to the original data points in <code>x</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of <code>fitted.values</code> (if input <code>se.fit=TRUE)</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>Predictor vector (same as input).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response vector (same as input).</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Type of spline that was used.</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique elements of <code>x</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>Mean of <code>y</code> for unique elements of <code>x</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>funique</code></td>
<td>
<p>Vector giving frequency of each element of <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation, i.e., <code class="reqn">\hat{\sigma}</code>.</p>
</td></tr>
<tr><td><code>ndf</code></td>
<td>
<p>Data frame with two elements: <code>n</code> is total sample size, and <code>df</code> is effective degrees of freedom of fit model (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model (assuming Gaussian error).</p>
</td></tr>
<tr><td><code>xrng</code></td>
<td>
<p>Predictor range: <code>xrng=c(xmin,xmax)</code>. </p>
</td></tr>
<tr><td><code>myknots</code></td>
<td>
<p>Bin-sampled spline knots used for fit.</p>
</td></tr>
<tr><td><code>rparm</code></td>
<td>
<p>Rounding parameter for <code>x</code> (same as input).</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Optimal smoothing parameter.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Spline basis function coefficients.</p>
</td></tr>
<tr><td><code>coef.csqrt</code></td>
<td>
<p>Matrix square-root of covariace matrix of <code>coef</code>. Use <code>tcrossprod(coef.csqrt)</code> to get covariance matrix of <code>coef</code>.</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>Cubic and cubic periodic splines transform the predictor to the interval [0,1] before fitting. So input <code>xmin</code> must be less than or equal to <code>min(x)</code>, and input <code>xmax</code> must be greater than or equal to <code>max(x)</code>.
</p>
<p>When using rounding parameters, output <code>fitted.values</code> corresponds to unique rounded predictor scores in output <code>xunique</code>. Use <code><a href="#topic+predict.bigspline">predict.bigspline</a></code> function to get fitted values for full <code>y</code> vector.
</p>


<h3>Computational Details </h3>

<p>According to smoothing spline theory, the function <code class="reqn">\eta</code> can be approximated as </p>
<p style="text-align: center;"><code class="reqn">\eta(x) = d_{0} + d_{1}\phi_{1}(x) + \sum_{h=1}^{q}c_{h}\rho(x,x_{h}^{*})</code>
</p>
<p> where the <code class="reqn">\phi_{1}</code>  is a linear function, <code class="reqn">\rho</code> is the reproducing kernel of the contrast (nonlinear) space, and <code class="reqn">\{x_{h}^{*}\}_{h=1}^{q}</code> are the selected spline knots.
</p>
<p>This implies that the penalized least-squares functional can be rewritten as </p>
<p style="text-align: center;"><code class="reqn"> \|\mathbf{y} - \mathbf{K}\mathbf{d} - \mathbf{J}\mathbf{c}\|^{2} + n\lambda\mathbf{c}'\mathbf{Q}\mathbf{c} </code>
</p>

<p>where <code class="reqn">\mathbf{K}=\{\phi(x_{i})\}_{n \times 2}</code> is the null space basis function matrix, <code class="reqn">\mathbf{J}=\{\rho(x_{i},x_{h}^{*})\}_{n \times q}</code> is the contrast space basis funciton matrix, <code class="reqn">\mathbf{Q}=\{\rho(x_{g}^{*},x_{h}^{*})\}_{q \times q}</code> is the penalty matrix, and <code class="reqn">\mathbf{d}=(d_{0},d_{1})'</code> and <code class="reqn">\mathbf{c}=(c_{1},\ldots,c_{q})'</code> are the unknown basis function coefficients.
</p>
<p>Given the smoothing parameter <code class="reqn">\lambda</code>, the optimal basis function coefficients have the form 
</p>
<p style="text-align: center;"><code class="reqn"> \left(\begin{array}{cc} \hat{\mathbf{d}} \\ \hat{\mathbf{c}} \end{array}\right) =
\left(\begin{array}{cc} \mathbf{K'K} &amp; \mathbf{K}'\mathbf{J} \\
 \mathbf{J}'\mathbf{K} &amp; \mathbf{J}'\mathbf{J} + n\lambda\mathbf{Q} \end{array}\right)^{\dagger} \left(\begin{array}{c} \mathbf{K}' \\ \mathbf{J}' \end{array}\right)\mathbf{y} </code>
</p>

<p>where <code class="reqn">(\cdot)^{\dagger}</code> denotes the pseudoinverse of the input matrix.
</p>
<p>Given the optimal coefficients, the fitted values are given by <code class="reqn">\hat{\mathbf{y}} = \mathbf{K}\hat{\mathbf{d}}+\mathbf{J}\hat{\mathbf{c}} = \mathbf{S}_{\lambda}\mathbf{y}</code>, where </p>
<p style="text-align: center;"><code class="reqn"> \mathbf{S}_{\lambda} = \left(\begin{array}{cc} \mathbf{K} &amp; \mathbf{J} \end{array}\right)
\left(\begin{array}{cc} \mathbf{K'K} &amp; \mathbf{K}'\mathbf{J} \\
 \mathbf{J}'\mathbf{K} &amp; \mathbf{J}'\mathbf{J} + n\lambda\mathbf{Q} \end{array}\right)^{\dagger} \left(\begin{array}{c} \mathbf{K}' \\ \mathbf{J}' \end{array}\right) </code>
</p>

<p>is the smoothing matrix, which depends on <code class="reqn">\lambda</code>.
</p>


<h3>Spline Types </h3>

<p>For a linear spline (<code>type="lin"</code>) with <code class="reqn">x \in [0,1]</code>, the needed functions are 
</p>
<p style="text-align: center;"><code class="reqn"> \phi_{1}(x) = 0 \qquad \mbox{and} \qquad \rho(x,z) = k_{1}(x)k_{1}(z)+k_{2}(|x-z|)</code>
</p>
<p> where <code class="reqn">k_{1}(x)=x-0.5</code>, <code class="reqn">k_{2}(x)=\frac{1}{2}\left(k_{1}^{2}(x) - \frac{1}{12} \right)</code>; in this case <code class="reqn">\mathbf{K}=\mathbf{1}_{n}</code> and <code class="reqn">\mathbf{d}=d_{0}</code>.
</p>
<p>For a cubic spline (<code>type="cub"</code>) with <code class="reqn">x \in [0,1]</code>, the needed functions are 
</p>
<p style="text-align: center;"><code class="reqn"> \phi_{1}(x) = k_{1}(x) \qquad \mbox{and} \qquad \rho(x,z) =  k_{2}(x)k_{2}(z)-k_{4}(|x-z|)</code>
</p>
<p> where <code class="reqn">k_{1}</code> and <code class="reqn">k_{2}</code> are defined above, and <code class="reqn">k_{4}(x)=\frac{1}{24}\left(k_{1}^{4}(x) - \frac{k_{1}^{2}(x)}{2} + \frac{7}{240} \right)</code>. 
</p>
<p>For a different cubic spline (<code>type="cub0"</code>) with <code class="reqn">x \in [0,1]</code>, the needed functions are </p>
<p style="text-align: center;"><code class="reqn"> \phi_{1}(x) = x \qquad \mbox{and} \qquad \rho(x,z) = (x \wedge z)^2[3(x \vee z) - (x \wedge z)]/6</code>
</p>

<p>where <code class="reqn">(x \wedge z) = \min(x,z)</code> and <code class="reqn">(x \vee z) = \max(x,z)</code>. 
</p>
<p>Note that <code>type="cub"</code> and <code>type="cub0"</code> use different definitions of the averaging operator in the null space. The overall spline estimates should be the same (up to approximation accuracy), but the null and constrast space effect functions will differ (see <code><a href="#topic+predict.bigspline">predict.bigspline</a></code>). See Helwig (2013) and Gu (2013) for a further discussion of polynomial splines.  
</p>
<p>For a periodic cubic spline (<code>type="per"</code>) with <code class="reqn">x \in [0,1]</code>, the needed functions are 
</p>
<p style="text-align: center;"><code class="reqn"> \phi_{1}(x) = 0 \qquad \mbox{and} \qquad \rho(x,z) = -k_{4}(|x-z|)</code>
</p>
<p> where <code class="reqn">k_{4}(x)</code> is defined as it was for <code>type="cub"</code>; in this case <code class="reqn">\mathbf{K}=\mathbf{1}_{n}</code> and <code class="reqn">\mathbf{d}=d_{0}</code>.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized least-squares, which does not require the Gaussian error assumption. However, the spline inference information (e.g., standard errors and fit information) requires the Gaussian error assumption.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define relatively smooth function
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(10^6)
y &lt;- myfun(x) + rnorm(10^6)

# linear, cubic, different cubic, and periodic splines
linmod &lt;- bigspline(x,y,type="lin")
linmod
cubmod &lt;- bigspline(x,y)
cubmod
cub0mod &lt;- bigspline(x,y,type="cub0")
cub0mod
permod &lt;- bigspline(x,y,type="per")
permod


##########   EXAMPLE 2   ##########

# define more jagged function
set.seed(773)
myfun &lt;- function(x){ 2*x + cos(4*pi*x) }
x &lt;- runif(10^6)*4
y &lt;- myfun(x) + rnorm(10^6)

# try different numbers of knots
r1mod &lt;- bigspline(x,y,nknots=20)
crossprod( myfun(r1mod$xunique) - r1mod$fitted )/length(r1mod$fitted)
r2mod &lt;- bigspline(x,y,nknots=30)
crossprod( myfun(r2mod$xunique) - r2mod$fitted )/length(r2mod$fitted)
r3mod &lt;- bigspline(x,y,nknots=40)
crossprod( myfun(r3mod$xunique) - r3mod$fitted )/length(r3mod$fitted)


##########   EXAMPLE 3   ##########

# define more jagged function
set.seed(773)
myfun &lt;- function(x){ 2*x + cos(4*pi*x) }
x &lt;- runif(10^6)*4
y &lt;- myfun(x) + rnorm(10^6)

# try different rounding parameters
r1mod &lt;- bigspline(x,y,rparm=0.05)
crossprod( myfun(r1mod$xunique) - r1mod$fitted )/length(r1mod$fitted)
r2mod &lt;- bigspline(x,y,rparm=0.02)
crossprod( myfun(r2mod$xunique) - r2mod$fitted )/length(r2mod$fitted)
r3mod &lt;- bigspline(x,y,rparm=0.01)
crossprod( myfun(r3mod$xunique) - r3mod$fitted )/length(r3mod$fitted)

</code></pre>

<hr>
<h2 id='bigsplines-internal'>Internal functions for big splines package</h2><span id='topic+gcvcss'></span><span id='topic+gcvgss'></span><span id='topic+gcvoss'></span><span id='topic+gcvssa'></span><span id='topic+gcvssg'></span><span id='topic+gcvssp'></span><span id='topic+getRandom'></span><span id='topic+lamcoef'></span><span id='topic+lamcoefg'></span><span id='topic+lamloop'></span><span id='topic+lamloopg'></span><span id='topic+makerkm'></span><span id='topic+makeZtX'></span><span id='topic+makeZtZ'></span><span id='topic+MPinv'></span><span id='topic+nbmle'></span><span id='topic+num2col'></span><span id='topic+pdsXty'></span><span id='topic+pinvsm'></span><span id='topic+postvar'></span><span id='topic+rkron'></span><span id='topic+remlri'></span><span id='topic+remlvc'></span><span id='topic+smartssa'></span><span id='topic+smartssg'></span><span id='topic+smartssp'></span><span id='topic+ssawork'></span><span id='topic+ssadpm'></span><span id='topic+ssblup'></span><span id='topic+ssgwork'></span><span id='topic+sspwork'></span><span id='topic+sspdpm'></span><span id='topic+tcprod'></span><span id='topic+unifqsum'></span><span id='topic+unifqsumg'></span><span id='topic+cubker'></span><span id='topic+cubkersym'></span><span id='topic+cubkerz'></span><span id='topic+cubkerzsym'></span><span id='topic+linker'></span><span id='topic+linkersym'></span><span id='topic+nomker'></span><span id='topic+nomkersym'></span><span id='topic+ordker'></span><span id='topic+ordkermon'></span><span id='topic+ordkersym'></span><span id='topic+perker'></span><span id='topic+perkersym'></span><span id='topic+sumfreq'></span><span id='topic+tpsker'></span><span id='topic+tpskersym'></span>

<h3>Description</h3>

<p>Internal functions for big splines package.
</p>


<h3>Details</h3>

<p>These functions are not to be called by the user.
</p>

<hr>
<h2 id='bigsplines-package'>
Smoothing Splines for Large Samples
</h2><span id='topic+bigsplines-package'></span><span id='topic+bigsplines'></span>

<h3>Description</h3>

<p>Fits smoothing spline regression models using scalable algorithms designed for large samples. Seven marginal spline types are supported: linear, cubic, different cubic, cubic periodic, cubic thin-plate, ordinal, and nominal. Random effects and parametric effects are also supported. Response can be Gaussian or non-Gaussian: Binomial, Poisson, Gamma, Inverse Gaussian, or Negative Binomial.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> bigsplines</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Smoothing Splines for Large Samples</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1-1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2018-05-25</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Nathaniel E. Helwig &lt;helwig@umn.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Nathaniel E. Helwig &lt;helwig@umn.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> quadprog</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> stats, graphics, grDevices</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Fits smoothing spline regression models using scalable algorithms designed for large samples. Seven marginal spline types are supported: linear, cubic, different cubic, cubic periodic, cubic thin-plate, ordinal, and nominal. Random effects and parametric effects are also supported. Response can be Gaussian or non-Gaussian: Binomial, Poisson, Gamma, Inverse Gaussian, or Negative Binomial.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Index of help topics:
</p>
<pre>
bigspline               Fits Smoothing Spline
bigsplines-package      Smoothing Splines for Large Samples
bigssa                  Fits Smoothing Spline ANOVA Models
bigssg                  Fits Generalized Smoothing Spline ANOVA Models
bigssp                  Fits Smoothing Splines with Parametric Effects
bigtps                  Fits Cubic Thin-Plate Splines
binsamp                 Bin-Samples Strategic Knot Indices
imagebar                Displays a Color Image with Colorbar
makessa                 Makes Objects to Fit Smoothing Spline ANOVA
                        Models
makessg                 Makes Objects to Fit Generalized Smoothing
                        Spline ANOVA Models
makessp                 Makes Objects to Fit Smoothing Splines with
                        Parametric Effects
ordspline               Fits Ordinal Smoothing Spline
plotbar                 Generic X-Y Plotting with Colorbar
plotci                  Generic X-Y Plotting with Confidence Intervals
predict.bigspline       Predicts for "bigspline" Objects
predict.bigssa          Predicts for "bigssa" Objects
predict.bigssg          Predicts for "bigssg" Objects
predict.bigssp          Predicts for "bigssp" Objects
predict.bigtps          Predicts for "bigtps" Objects
predict.ordspline       Predicts for "ordspline" Objects
print.bigspline         Prints Fit Information for bigsplines Model
ssBasis                 Smoothing Spline Basis for Polynomial Splines
summary.bigspline       Summarizes Fit Information for bigsplines Model
</pre>
<p>The function <code><a href="#topic+bigspline">bigspline</a></code> fits one-dimensional cubic smoothing splines (unconstrained or periodic). The function <code><a href="#topic+bigssa">bigssa</a></code> fits Smoothing Spline Anova (SSA) models (Gaussian data). The function <code><a href="#topic+bigssg">bigssg</a></code> fits Generalized Smoothing Spline Anova (GSSA) models (non-Gaussian data). The function <code><a href="#topic+bigssp">bigssp</a></code> is for fitting Smoothing Splines with Parametric effects (semi-parametric regression). The function <code><a href="#topic+bigtps">bigtps</a></code> fits one-, two-, and three-dimensional cubic thin-plate splines. There are corresponding predict, print, and summary functions for these methods.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>
<p>Maintainer: Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Wahba, G. (1991). Minimizing GCV/GML scores with multiple smoothing parameters via the Newton method. <em>SIAM Journal on Scientific and Statistical Computing, 12</em>, 383-398.
</p>
<p>Gu, C. and Xiang, D. (2001). Cross-validating non-Gaussian data: Generalized approximate cross-validation revisited. <em>Journal of Computational and Graphical Statistics, 10</em>, 581-591.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples for bigspline, bigssa, bigssg, bigssp, and bigtps
</code></pre>

<hr>
<h2 id='bigssa'>
Fits Smoothing Spline ANOVA Models
</h2><span id='topic+bigssa'></span>

<h3>Description</h3>

<p>Given a real-valued response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code>, a Smoothing Spline Anova (SSA) has the form </p>
<p style="text-align: center;"><code class="reqn">y_{i}= \eta(\mathbf{x}_{i}) + e_{i}</code>
</p>
<p> where <code class="reqn">y_{i}</code> is the <code class="reqn">i</code>-th observation's respone, <code class="reqn">\mathbf{x}_{i}=(x_{i1},\ldots,x_{ip})</code> is the <code class="reqn">i</code>-th observation's nonparametric predictor vector, <code class="reqn">\eta</code> is an unknown smooth function relating the response and nonparametric predictors, and <code class="reqn">e_{i}\sim\mathrm{N}(0,\sigma^{2})</code> is iid Gaussian error. Function can fit additive models, and also allows for 2-way and 3-way interactions between any number of predictors (see Details and Examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigssa(formula,data=NULL,type=NULL,nknots=NULL,rparm=NA,
       lambdas=NULL,skip.iter=TRUE,se.fit=FALSE,rseed=1234,
       gcvopts=NULL,knotcheck=TRUE,gammas=NULL,weights=NULL,
       random=NULL,remlalg=c("FS","NR","EM","none"),remliter=500,
       remltol=10^-4,remltau=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigssa_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>. Or an object of class &quot;makessa&quot;, which is output from <code><a href="#topic+makessa">makessa</a></code>.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="cub0"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, <code>type="ord"</code> for ordinal, and <code>type="nom"</code> for nominal.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default <code>lambdas=10^-c(9:0)</code>.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Skip Iteration section.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of the fitted values should be estimated.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 3 elements: (a) <code>maxit</code>: maximum number of algorithm iterations, (b) <code>gcvtol</code>: covergence tolerance for iterative GCV update, and (c) <code>alpha</code>: tuning parameter for GCV minimization. Default: <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1)</code>
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_gammas">gammas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_random">random</code></td>
<td>

<p>Adds random effects to model (see Random Effects section).
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_remlalg">remlalg</code></td>
<td>

<p>REML algorithm for estimating variance components (see Random Effects section). Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_remliter">remliter</code></td>
<td>

<p>Maximum number of iterations for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_remltol">remltol</code></td>
<td>

<p>Convergence tolerance for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssa_+3A_remltau">remltau</code></td>
<td>

<p>Initial estimate of variance parameters for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>formula</code> syntax is similar to that used in <code><a href="stats.html#topic+lm">lm</a></code> and many other R regression functions. Use <code>y~x</code> to predict the response <code>y</code> from the predictor <code>x</code>. Use <code>y~x1+x2</code> to fit an additive model of the predictors <code>x1</code> and <code>x2</code>, and use <code>y~x1*x2</code> to fit an interaction model. The syntax <code>y~x1*x2</code> includes the interaction and main effects, whereas the syntax <code>y~x1:x2</code> is not supported. See Computational Details for specifics about how nonparametric effects are estimated.
</p>
<p>See <code><a href="#topic+bigspline">bigspline</a></code> for definitions of <code>type="cub"</code>, <code>type="cub0"</code>, and <code>type="per"</code> splines, which can handle one-dimensional predictors. See Appendix of Helwig and Ma (2015) for information about <code>type="tps"</code> and <code>type="nom"</code> splines. Note that <code>type="tps"</code> can handle one-, two-, or three-dimensional predictors. I recommend using <code>type="cub"</code> if the predictor scores have no extreme outliers; when outliers are present, <code>type="tps"</code> may produce a better result. 
</p>
<p>Using the rounding parameter input <code>rparm</code> can greatly speed-up and stabilize the fitting for large samples. For typical cases, I recommend using <code>rparm=0.01</code> for cubic and periodic splines, but smaller rounding parameters may be needed for particularly jagged functions. For thin-plate splines, the data are NOT transformed to the interval [0,1] before fitting, so the rounding parameter should be on the raw data scale. Also, for <code>type="tps"</code> you can enter one rounding parameter for each predictor dimension. Use <code>rparm=1</code> for ordinal and nominal splines.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values corresponding to the original data points in <code>xvars</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of <code>fitted.values</code> (if input <code>se.fit=TRUE)</code>.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code>xvars</code></td>
<td>
<p>List of predictors.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Type of smoothing spline that was used for each predictor.</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>Mean of <code>yvar</code> for unique points after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique rows of <code>xvars</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation, i.e., <code class="reqn">\hat{\sigma}</code>.</p>
</td></tr>
<tr><td><code>ndf</code></td>
<td>
<p>Data frame with two elements: <code>n</code> is total sample size, and <code>df</code> is effective degrees of freedom of fit model (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model (assuming Gaussian error).</p>
</td></tr>
<tr><td><code>modelspec</code></td>
<td>
<p>List containing specifics of fit model (needed for prediction).</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Convergence status: <code>converged=TRUE</code> if iterative update converged, <code>converged=FALSE</code> if iterative update failed to converge, and <code>converged=NA</code> if option <code>skip.iter=TRUE</code> was used.</p>
</td></tr>
<tr><td><code>tnames</code></td>
<td>
<p>Names of the terms in model.</p>
</td></tr>
<tr><td><code>random</code></td>
<td>
<p>Random effects formula (same as input).</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>Variance parameters such that <code>sigma*sqrt(tau)</code> gives standard deviation of random effects (if <code>!is.null(random)</code>).</p>
</td></tr>
<tr><td><code>blup</code></td>
<td>
<p>Best linear unbiased predictors (if <code>!is.null(random)</code>).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Called model in input <code>formula</code>.</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>Cubic and cubic periodic splines transform the predictor to the interval [0,1] before fitting.
</p>
<p>When using rounding parameters, output <code>fitted.values</code> corresponds to unique rounded predictor scores in output <code>xunique</code>. Use <code><a href="#topic+predict.bigssa">predict.bigssa</a></code> function to get fitted values for full <code>yvar</code> vector.
</p>


<h3>Computational Details </h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the penalized least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}\left(y_{i} - \eta(\mathbf{x}_{i}) \right)^{2} + \lambda J(\eta)</code>
</p>
<p> where <code class="reqn">J(\cdot)</code> is a nonnegative penalty functional quantifying the roughness of <code class="reqn">\eta</code> and <code class="reqn">\lambda&gt;0</code> is a smoothing parameter controlling the trade-off between fitting and smoothing the data. Note that for <code class="reqn">p&gt;1</code> nonparametric predictors, there are additional <code class="reqn">\theta_{k}</code> smoothing parameters embedded in <code class="reqn">J</code>. 
</p>
<p>The penalized least squares functioncal can be rewritten as </p>
<p style="text-align: center;"><code class="reqn"> \|\mathbf{y} - \mathbf{K}\mathbf{d} - \mathbf{J}_{\theta}\mathbf{c}\|^{2} + n\lambda\mathbf{c}'\mathbf{Q}_{\theta}\mathbf{c} </code>
</p>

<p>where <code class="reqn">\mathbf{K}=\{\phi(x_{i})\}_{n \times m}</code> is the null (parametric) space basis function matrix, <code class="reqn">\mathbf{J}_{\theta}=\sum_{k=1}^{s}\theta_{k}\mathbf{J}_{k}</code> with <code class="reqn">\mathbf{J}_{k}=\{\rho_{k}(\mathbf{x}_{i},\mathbf{x}_{h}^{*})\}_{n \times q}</code> denoting the <code class="reqn">k</code>-th contrast space basis funciton matrix, <code class="reqn">\mathbf{Q}_{\theta}=\sum_{k=1}^{s}\theta_{k}\mathbf{Q}_{k}</code> with <code class="reqn">\mathbf{Q}_{k}=\{\rho_{k}(\mathbf{x}_{g}^{*},\mathbf{x}_{h}^{*})\}_{q \times q}</code> denoting the <code class="reqn">k</code>-th penalty matrix, and <code class="reqn">\mathbf{d}=(d_{0},\ldots,d_{m})'</code> and <code class="reqn">\mathbf{c}=(c_{1},\ldots,c_{q})'</code> are the unknown basis function coefficients. The optimal smoothing parameters are chosen by minimizing the GCV score (see <code><a href="#topic+bigspline">bigspline</a></code>). 
</p>
<p>Note that this function uses the efficient SSA reparameterization described in Helwig (2013) and Helwig and Ma (2015); using is parameterization, there is one unique smoothing parameter per predictor (<code class="reqn">\gamma_{j}</code>), and these <code class="reqn">\gamma_{j}</code> parameters determine the structure of the <code class="reqn">\theta_{k}</code> parameters in the tensor product space. To evaluate the GCV score, this function uses the improved (scalable) SSA algorithm discussed in Helwig (2013) and Helwig and Ma (2015).
</p>


<h3>Skip Iteration </h3>

<p>For <code class="reqn">p&gt;1</code> predictors, initial values for the <code class="reqn">\gamma_{j}</code> parameters (that determine the structure of the <code class="reqn">\theta_{k}</code> parameters) are estimated using the smart starting algorithm described in Helwig (2013) and Helwig and Ma (2015). 
</p>
<p>Default use of this function (<code>skip.iter=TRUE</code>) fixes the <code class="reqn">\gamma_{j}</code> parameters afer the smart start, and then finds the global smoothing parameter <code class="reqn">\lambda</code> (among the input <code>lambdas</code>) that minimizes the GCV score. This approach typically produces a solution very similar to the more optimal solution using <code>skip.iter=FALSE</code>.
</p>
<p>Setting <code>skip.iter=FALSE</code> uses the same smart starting algorithm as setting <code>skip.iter=TRUE</code>. However, instead of fixing the <code class="reqn">\gamma_{j}</code> parameters afer the smart start, using <code>skip.iter=FALSE</code> iterates between estimating the optimal <code class="reqn">\lambda</code> and the optimal <code class="reqn">\gamma_{j}</code> parameters. The R function <code><a href="stats.html#topic+nlm">nlm</a></code> is used to minimize the GCV score with respect to the <code class="reqn">\gamma_{j}</code> parameters, which can be time consuming for models with many predictors and/or a large number of knots.
</p>


<h3>Random Effects </h3>

<p>The input <code>random</code> adds random effects to the model assuming a variance components structure. Both nested and crossed random effects are supported. In all cases, the random effects are assumed to be indepedent zero-mean Gaussian variables with the variance depending on group membership.
</p>
<p>Random effects are distinguished by vertical bars (&quot;|&quot;), which separate expressions for design matrices (left) from group factors (right). For example, the syntax <code>~1|group</code> includes a random intercept for each level of <code>group</code>, whereas the syntax <code>~1+x|group</code> includes both a random intercept and a random slope for each level of <code>group</code>. For crossed random effects, parentheses are needed to distinguish different terms, e.g., <code>~(1|group1)+(1|group2)</code> includes a random intercept for each level of <code>group1</code> and a random intercept for each level of <code>group2</code>, where both <code>group1</code> and <code>group2</code> are factors. For nested random effects, the syntax <code>~group|subject</code> can be used, where both <code>group</code> and <code>subject</code> are factors such that the levels of <code>subject</code> are nested within those of <code>group</code>. 
</p>
<p>The input <code>remlalg</code> determines the REML algorithm used to estimate the variance components. Setting <code>remlalg="FS"</code> uses a Fisher Scoring algorithm (default). Setting <code>remlalg="NR"</code> uses a Newton-Raphson algorithm. Setting <code>remlalg="EM"</code> uses an Expectation Maximization algorithm. Use <code>remlalg="none"</code> to fit a model with known variance components (entered through <code>remltau</code>). 
</p>
<p>The input <code>remliter</code> sets the maximum number of iterations for the REML estimation. The input <code>remltol</code> sets the convergence tolerance for the REML estimation, which is determined via relative change in the REML log-likelihood. The input <code>remltau</code> sets the initial estimates of variance parameters; default is <code>remltau = rep(1,ntau)</code> where <code>ntau</code> is the number of variance components.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized least-squares, which does not require the Gaussian error assumption. However, the spline inference information (e.g., standard errors and fit information) requires the Gaussian error assumption.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(500)
y &lt;- myfun(x) + rnorm(500)

# cubic, periodic, and thin-plate spline models with 20 knots
cubmod &lt;- bigssa(y~x,type="cub",nknots=20,se.fit=TRUE)
cubmod
permod &lt;- bigssa(y~x,type="per",nknots=20,se.fit=TRUE)
permod
tpsmod &lt;- bigssa(y~x,type="tps",nknots=20,se.fit=TRUE)
tpsmod


##########   EXAMPLE 2   ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){sin(2*pi*x1v)+log(x2v+.1)+cos(pi*(x1v-x2v))}
x1v &lt;- runif(500)
x2v &lt;- runif(500)
y &lt;- myfun(x1v,x2v) + rnorm(500)

# cubic splines with 50 randomly selected knots
intmod &lt;- bigssa(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
intmod
crossprod( myfun(x1v,x2v) - intmod$fitted.values )/500

# fit additive model (with same knots)
addmod &lt;- bigssa(y~x1v+x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
addmod
crossprod( myfun(x1v,x2v) - addmod$fitted.values )/500


##########   EXAMPLE 3   ##########

# function with two continuous and one nominal predictor (3 levels)
set.seed(773)
myfun &lt;- function(x1v,x2v,x3v){
  fval &lt;- rep(0,length(x1v))
  xmeans &lt;- c(-1,0,1)
  for(j in 1:3){
    idx &lt;- which(x3v==letters[j])
    fval[idx] &lt;- xmeans[j]
  }
  fval[idx] &lt;- fval[idx] + cos(4*pi*(x1v[idx]))
  fval &lt;- (fval + sin(3*pi*x1v*x2v+pi)) / sqrt(2)
}
x1v &lt;- runif(500)
x2v &lt;- runif(500)
x3v &lt;- sample(letters[1:3],500,replace=TRUE)
y &lt;- myfun(x1v,x2v,x3v) + rnorm(500)

# 3-way interaction with 50 knots
cuimod &lt;- bigssa(y~x1v*x2v*x3v,type=list(x1v="cub",x2v="cub",x3v="nom"),nknots=50)
crossprod( myfun(x1v,x2v,x3v) - cuimod$fitted.values )/500

# fit correct interaction model with 50 knots
cubmod &lt;- bigssa(y~x1v*x2v+x1v*x3v,type=list(x1v="cub",x2v="cub",x3v="nom"),nknots=50)
crossprod( myfun(x1v,x2v,x3v) - cubmod$fitted.values )/500

# fit model using 2-dimensional thin-plate and nominal
x1new &lt;- cbind(x1v,x2v)
x2new &lt;- x3v
tpsmod &lt;- bigssa(y~x1new*x2new,type=list(x1new="tps",x2new="nom"),nknots=50)
crossprod( myfun(x1v,x2v,x3v) - tpsmod$fitted.values )/500


##########   EXAMPLE 4   ##########

# function with four continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v,x3v,x4v){
  sin(2*pi*x1v) + log(x2v+.1) + x3v*cos(pi*(x4v))
  }
x1v &lt;- runif(500)
x2v &lt;- runif(500)
x3v &lt;- runif(500)
x4v &lt;- runif(500)
y &lt;- myfun(x1v,x2v,x3v,x4v) + rnorm(500)

# fit cubic spline model with x3v*x4v interaction
cubmod &lt;- bigssa(y~x1v+x2v+x3v*x4v,type=list(x1v="cub",x2v="cub",x3v="cub",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

</code></pre>

<hr>
<h2 id='bigssg'>
Fits Generalized Smoothing Spline ANOVA Models
</h2><span id='topic+bigssg'></span>

<h3>Description</h3>

<p>Given an exponential family response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code>, a Generalized Smoothing Spline Anova (GSSA) has the form </p>
<p style="text-align: center;"><code class="reqn">g(\mu_{i}) = \eta(\mathbf{x}_{i})</code>
</p>
<p> where <code class="reqn">\mu_{i}</code> is the expected value of the <code class="reqn">i</code>-th observation's respone, <code class="reqn">g(\cdot)</code> is some invertible link function, <code class="reqn">\mathbf{x}_{i}=(x_{i1},\ldots,x_{ip})</code> is the <code class="reqn">i</code>-th observation's nonparametric predictor vector, and <code class="reqn">\eta</code> is an unknown smooth function relating the response and nonparametric predictors. Function can fit additive models, and also allows for 2-way and 3-way interactions between any number of predictors. Response can be one of five non-Gaussian distributions: Binomial, Poisson, Gamma, Inverse Gaussian, or Negative Binomial (see Details and Examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigssg(formula,family,data=NULL,type=NULL,nknots=NULL,rparm=NA,
       lambdas=NULL,skip.iter=TRUE,se.lp=FALSE,rseed=1234,
       gcvopts=NULL,knotcheck=TRUE,gammas=NULL,weights=NULL,
       gcvtype=c("acv","gacv","gacv.old"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigssg_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_family">family</code></td>
<td>

<p>Distribution for response. One of five options: <code>"binomial"</code>, <code>"poisson"</code>, <code>"Gamma"</code>, <code>"inverse.gaussian"</code>, or <code>"negbin"</code>. See Response section.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>. Or an object of class &quot;makessg&quot;, which is output from <code><a href="#topic+makessg">makessg</a></code>.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="cub0"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, <code>type="ord"</code> for ordinal, and <code>type="nom"</code> for nominal.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default <code>lambdas=10^-c(9:0)</code>.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Skip Iteration section.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_se.lp">se.lp</code></td>
<td>

<p>Logical indicating if the standard errors of the linear predictors (<code class="reqn">\eta</code>) should be estimated.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 6 elements: (i) <code>maxit</code>: maximum number of outer iterations, (ii) <code>gcvtol</code>: covergence tolerance for iterative GACV update, (iii) <code>alpha</code>: tuning parameter for GACV minimization, (iv) <code>inmaxit</code>: maximum number of inner iterations for iteratively reweighted fitting, (v) <code>intol</code>: inner convergence tolerance for iteratively reweighted fitting, and (vi) <code>insub</code>: number of data points to subsample when checking inner convergence. <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1,inmaxit=100,intol=10^-5,insub=10^4)</code>
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_gammas">gammas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="bigssg_+3A_gcvtype">gcvtype</code></td>
<td>

<p>Cross-validation criterion for selecting smoothing parameters (see Details).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>formula</code> syntax is similar to that used in <code><a href="stats.html#topic+lm">lm</a></code> and many other R regression functions. Use <code>y~x</code> to predict the response <code>y</code> from the predictor <code>x</code>. Use <code>y~x1+x2</code> to fit an additive model of the predictors <code>x1</code> and <code>x2</code>, and use <code>y~x1*x2</code> to fit an interaction model. The syntax <code>y~x1*x2</code> includes the interaction and main effects, whereas the syntax <code>y~x1:x2</code> is not supported. See Computational Details for specifics about how nonparametric effects are estimated.
</p>
<p>See <code><a href="#topic+bigspline">bigspline</a></code> for definitions of <code>type="cub"</code>, <code>type="cub0"</code>, and <code>type="per"</code> splines, which can handle one-dimensional predictors. See Appendix of Helwig and Ma (2015) for information about <code>type="tps"</code> and <code>type="nom"</code> splines. Note that <code>type="tps"</code> can handle one-, two-, or three-dimensional predictors. I recommend using <code>type="cub"</code> if the predictor scores have no extreme outliers; when outliers are present, <code>type="tps"</code> may produce a better result. 
</p>
<p>Using the rounding parameter input <code>rparm</code> can greatly speed-up and stabilize the fitting for large samples. For typical cases, I recommend using <code>rparm=0.01</code> for cubic and periodic splines, but smaller rounding parameters may be needed for particularly jagged functions. For thin-plate splines, the data are NOT transformed to the interval [0,1] before fitting, so rounding parameter should be on raw data scale. Also, for <code>type="tps"</code> you can enter one rounding parameter for each predictor dimension. Use <code>rparm=1</code> for ordinal and nominal splines.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values (data scale) corresponding to the original data points in <code>xvars</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>Vector of fitted values (link scale) corresponding to the original data points in <code>xvars</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>se.lp</code></td>
<td>
<p>Vector of standard errors of <code>linear.predictors</code> (if input <code>se.lp=TRUE)</code>.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code>xvars</code></td>
<td>
<p>List of predictors.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Type of smoothing spline that was used for each predictor.</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>Mean of <code>yvar</code> for unique points after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique rows of <code>xvars</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>Estimated dispersion parameter (see Response section).</p>
</td></tr>
<tr><td><code>ndf</code></td>
<td>
<p>Data frame with two elements: <code>n</code> is total sample size, and <code>df</code> is effective degrees of freedom of fit model (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model.</p>
</td></tr>
<tr><td><code>modelspec</code></td>
<td>
<p>List containing specifics of fit model (needed for prediction).</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Convergence status: <code>converged=TRUE</code> if iterative update converged, <code>converged=FALSE</code> if iterative update failed to converge, and <code>converged=NA</code> if option <code>skip.iter=TRUE</code> was used.</p>
</td></tr>
<tr><td><code>tnames</code></td>
<td>
<p>Names of the terms in model.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>Distribution family (same as input).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Called model in input <code>formula</code>.</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>Cubic and cubic periodic splines transform the predictor to the interval [0,1] before fitting.
</p>
<p>When using rounding parameters, output <code>fitted.values</code> corresponds to unique rounded predictor scores in output <code>xunique</code>. Use <code><a href="#topic+predict.bigssg">predict.bigssg</a></code> function to get fitted values for full <code>yvar</code> vector.
</p>


<h3>Response</h3>

<p>Only one link is permitted for each family:
</p>
<p><code>family="binomial"</code>
Logit link. Response should be vector of proportions in the interval [0,1]. If response is a sample proportion, the total count should be input through <code>weights</code> argument.
</p>
<p><code>family="poisson"</code>
Log link. Response should be vector of counts (non-negative integers).
</p>
<p><code>family="Gamma"</code>
Inverse link. Response should be vector of positive real-valued data. Estimated <code>dispersion</code> parameter is the inverse of the <code>shape</code> parameter, so that the variance of the response increases as <code>dispersion</code> increases.
</p>
<p><code>family="inverse.gaussian"</code>
Inverse-square link. Response should be vector of positive real-valued data. Estimated <code>dispersion</code> parameter is the inverse of the <code>shape</code> parameter, so that the variance of the response increases as <code>dispersion</code> increases.
</p>
<p><code>family="negbin"</code>
Log link. Response should be vector of counts (non-negative integers). Estimated <code>dispersion</code> parameter is the inverse of the <code>size</code> parameter, so that the variance of the response increases as <code>dispersion</code> increases. 
</p>
<p><code>family=list("negbin",2)</code>
Log link. Response should be vector of counts (non-negative integers). Second element is the known (common) <code>dispersion</code> parameter (2 in this case). The input <code>dispersion</code> parameter should be the inverse of the <code>size</code> parameter, so that the variance of the response increases as <code>dispersion</code> increases.
</p>


<h3>Computational Details </h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the (negative of the) penalized log likelihood </p>
<p style="text-align: center;"><code class="reqn">-\frac{1}{n}\sum_{i=1}^{n}\left\{y_{i}\eta(\mathbf{x}_{i}) - b(\eta(\mathbf{x}_{i})) \right\} + \frac{\lambda}{2} J(\eta)</code>
</p>
<p> where <code class="reqn">J(\cdot)</code> is a nonnegative penalty functional quantifying the roughness of <code class="reqn">\eta</code> and <code class="reqn">\lambda&gt;0</code> is a smoothing parameter controlling the trade-off between fitting and smoothing the data. Note that for <code class="reqn">p&gt;1</code> nonparametric predictors, there are additional <code class="reqn">\theta_{k}</code> smoothing parameters embedded in <code class="reqn">J</code>. 
</p>
<p>Following standard exponential family theory, <code class="reqn">\mu_{i} = \dot{b}(\eta(\mathbf{x}_{i}))</code> and <code class="reqn">v_{i} = \ddot{b}(\eta(\mathbf{x}_{i}))a(\xi)</code>, where <code class="reqn">\dot{b}(\cdot)</code> and <code class="reqn">\ddot{b}(\cdot)</code> denote the first and second derivatives of <code class="reqn">b(\cdot)</code>, <code class="reqn">v_{i}</code> is the variance of <code class="reqn">y_{i}</code>,and <code class="reqn">\xi</code> is the dispersion parameter. Given fixed smoothing parameters, the optimal <code class="reqn">\eta</code> can be estimated by iteratively minimizing the penalized reweighted least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}v_{i}^{*}\left(y_{i}^{*} - \eta(\mathbf{x}_{i}) \right)^{2} + \lambda J(\eta)</code>
</p>
<p> where <code class="reqn">v_{i}^{*}=v_{i}/a(\xi)</code> is the weight, <code class="reqn">y_{i}^{*}=\hat{\eta}(\mathbf{x}_{i})+(y_{i}-\hat{\mu}_{i})/v_{i}^{*}</code> is the adjusted dependent variable, and <code class="reqn">\hat{\eta}(\mathbf{x}_{i})</code> is the current estimate of <code class="reqn">\eta</code>.
</p>
<p>The optimal smoothing parameters are chosen via direct cross-validation (see Gu &amp; Xiang, 2001). 
</p>
<p>Setting <code>gcvtype="acv"</code> uses the Approximate Cross-Validation (ACV) score: </p>
<p style="text-align: center;"><code class="reqn">-\frac{1}{n}\sum_{i=1}^{n}\{y_{i}\hat{\eta}(\mathbf{x}_{i}) - b(\hat{\eta}(\mathbf{x}_{i}))\} + \frac{1}{n}\sum_{i=1}^{n}\frac{s_{ii}}{(1-s_{ii})v_{i}^{*}}y_{i}(y_{i}-\hat{\mu}_{i}) </code>
</p>
<p> where <code class="reqn">s_{ii}</code> is the i-th diagonal of the smoothing matrix <code class="reqn">\mathbf{S}_{\boldsymbol\lambda}</code>.
</p>
<p>Setting <code>gcvtype="gacv"</code> uses the Generalized ACV (GACV) score: </p>
<p style="text-align: center;"><code class="reqn">-\frac{1}{n}\sum_{i=1}^{n}\{y_{i}\hat{\eta}(\mathbf{x}_{i}) - b(\hat{\eta}(\mathbf{x}_{i}))\} + \frac{\mathrm{tr}(\mathbf{S}_{\boldsymbol\lambda}\mathbf{V}^{-1})}{n-\mathrm{tr}(\mathbf{S}_{\boldsymbol\lambda})}\frac{1}{n}\sum_{i=1}^{n}y_{i}(y_{i}-\hat{\mu}_{i}) </code>
</p>
<p> where <code class="reqn">\mathbf{S}_{\boldsymbol\lambda}</code> is the smoothing matrix, and <code class="reqn">\mathbf{V}=\mathrm{diag}(v_{1}^{*},\ldots,v_{n}^{*})</code>. 
</p>
<p>Setting <code>gcvtype="gacv.old"</code> uses an approximation of the GACV where <code class="reqn">\frac{1}{n}\mathrm{tr}(\mathbf{S}_{\boldsymbol\lambda}\mathbf{V}^{-1})</code> is approximated using <code class="reqn">\frac{1}{n^2}\mathrm{tr}(\mathbf{S}_{\boldsymbol\lambda})\mathrm{tr}(\mathbf{V}^{-1})</code>. This option is included for back-compatibility (ver 1.0-4 and earlier), and is not recommended because the ACV or GACV often perform better.
</p>
<p>Note that this function uses the efficient SSA reparameterization described in Helwig (2013) and Helwig and Ma (2015); using is parameterization, there is one unique smoothing parameter per predictor (<code class="reqn">\gamma_{j}</code>), and these <code class="reqn">\gamma_{j}</code> parameters determine the structure of the <code class="reqn">\theta_{k}</code> parameters in the tensor product space. To evaluate the ACV/GACV score, this function uses the improved (scalable) GSSA algorithm discussed in Helwig (in preparation).
</p>


<h3>Skip Iteration </h3>

<p>For <code class="reqn">p&gt;1</code> predictors, initial values for the <code class="reqn">\gamma_{j}</code> parameters (that determine the structure of the <code class="reqn">\theta_{k}</code> parameters) are estimated using an extension of the smart starting algorithm described in Helwig (2013) and Helwig and Ma (2015). 
</p>
<p>Default use of this function (<code>skip.iter=TRUE</code>) fixes the <code class="reqn">\gamma_{j}</code> parameters afer the smart start, and then finds the global smoothing parameter <code class="reqn">\lambda</code> (among the input <code>lambdas</code>) that minimizes the GCV score. This approach typically produces a solution very similar to the more optimal solution using <code>skip.iter=FALSE</code>.
</p>
<p>Setting <code>skip.iter=FALSE</code> uses the same smart starting algorithm as setting <code>skip.iter=TRUE</code>. However, instead of fixing the <code class="reqn">\gamma_{j}</code> parameters afer the smart start, using <code>skip.iter=FALSE</code> iterates between estimating the optimal <code class="reqn">\lambda</code> and the optimal <code class="reqn">\gamma_{j}</code> parameters. The R function <code><a href="stats.html#topic+nlm">nlm</a></code> is used to minimize the approximate GACV score with respect to the <code class="reqn">\gamma_{j}</code> parameters, which can be time consuming for models with many predictors and/or a large number of knots.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized likelihood estimation. Standard errors of the linear predictors are formed using Bayesian confidence intervals.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Xiang, D. (2001). Cross-validating non-Gaussian data: Generalized approximate cross-validation revisited. <em>Journal of Computational and Graphical Statistics, 10</em>, 581-591.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1 (1-way GSSA)   ##########

# define univariate function and data
set.seed(1)
myfun &lt;- function(x){ sin(2*pi*x) }
ndpts &lt;- 1000
x &lt;- runif(ndpts)

# binomial response (no weights)
set.seed(773)
lp &lt;- myfun(x)
p &lt;- 1/(1+exp(-lp))
y &lt;- rbinom(n=ndpts,size=1,p=p)     ## y is binary data
gmod &lt;- bigssg(y~x,family="binomial",type="cub",nknots=20)
crossprod( lp - gmod$linear.predictor )/length(lp)

# binomial response (with weights)
set.seed(773)
lp &lt;- myfun(x)
p &lt;- 1/(1+exp(-lp))
w &lt;- sample(c(10,20,30,40,50),length(p),replace=TRUE)
y &lt;- rbinom(n=ndpts,size=w,p=p)/w   ## y is proportion correct
gmod &lt;- bigssg(y~x,family="binomial",type="cub",nknots=20,weights=w)
crossprod( lp - gmod$linear.predictor )/length(lp)

# poisson response
set.seed(773)
lp &lt;- myfun(x)
mu &lt;- exp(lp)
y &lt;- rpois(n=ndpts,lambda=mu)
gmod &lt;- bigssg(y~x,family="poisson",type="cub",nknots=20)
crossprod( lp - gmod$linear.predictor )/length(lp)

# Gamma response
set.seed(773)
lp &lt;- myfun(x) + 2
mu &lt;- 1/lp
y &lt;- rgamma(n=ndpts,shape=4,scale=mu/4)
gmod &lt;- bigssg(y~x,family="Gamma",type="cub",nknots=20)
1/gmod$dispersion   ## dispersion = 1/shape
crossprod( lp - gmod$linear.predictor )/length(lp)

# inverse gaussian response (not run: requires statmod package)
# require(statmod)
# set.seed(773)
# lp &lt;- myfun(x) + 2
# mu &lt;- sqrt(1/lp)
# y &lt;- rinvgauss(n=ndpts,mean=mu,shape=2)
# gmod &lt;- bigssg(y~x,family="inverse.gaussian",type="cub",nknots=20)
# 1/gmod$dispersion   ## dispersion = 1/shape
# crossprod( lp - gmod$linear.predictor )/length(lp)

# negative binomial response (known dispersion)
set.seed(773)
lp &lt;- myfun(x)
mu &lt;- exp(lp)
y &lt;- rnbinom(n=ndpts,size=.5,mu=mu)
gmod &lt;- bigssg(y~x,family=list("negbin",2),type="cub",nknots=20)
1/gmod$dispersion   ## dispersion = 1/size
crossprod( lp - gmod$linear.predictor )/length(lp)

# negative binomial response (unknown dispersion)
set.seed(773)
lp &lt;- myfun(x)
mu &lt;- exp(lp)
y &lt;- rnbinom(n=ndpts,size=.5,mu=mu)
gmod &lt;- bigssg(y~x,family="negbin",type="cub",nknots=20)
1/gmod$dispersion   ## dispersion = 1/size
crossprod( lp - gmod$linear.predictor )/length(lp)

## Not run: 

##########   EXAMPLE 2 (2-way GSSA)   ##########

# function with two continuous predictors
set.seed(1)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
ndpts &lt;- 1000
x1v &lt;- runif(ndpts)
x2v &lt;- runif(ndpts)

# binomial response (no weights)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
p &lt;- 1/(1+exp(-lp))
y &lt;- rbinom(n=ndpts,size=1,p=p)     ## y is binary data
gmod &lt;- bigssg(y~x1v*x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=50)
crossprod( lp - gmod$linear.predictor )/length(lp)

# binomial response (with weights)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
p &lt;- 1/(1+exp(-lp))
w &lt;- sample(c(10,20,30,40,50),length(p),replace=TRUE)
y &lt;- rbinom(n=ndpts,size=w,p=p)/w   ## y is proportion correct
gmod &lt;- bigssg(y~x1v*x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=50,weights=w)
crossprod( lp - gmod$linear.predictor )/length(lp)

# poisson response
set.seed(773)
lp &lt;- myfun(x1v,x2v)
mu &lt;- exp(lp)
y &lt;- rpois(n=ndpts,lambda=mu)
gmod &lt;- bigssg(y~x1v*x2v,family="poisson",type=list(x1v="cub",x2v="cub"),nknots=50)
crossprod( lp - gmod$linear.predictor )/length(lp)

# Gamma response
set.seed(773)
lp &lt;- myfun(x1v,x2v)+6
mu &lt;- 1/lp
y &lt;- rgamma(n=ndpts,shape=4,scale=mu/4)
gmod &lt;- bigssg(y~x1v*x2v,family="Gamma",type=list(x1v="cub",x2v="cub"),nknots=50)
1/gmod$dispersion   ## dispersion = 1/shape
crossprod( lp - gmod$linear.predictor )/length(lp)

# inverse gaussian response (not run: requires 'statmod' package)
# require(statmod)
# set.seed(773)
# lp &lt;- myfun(x1v,x2v)+6
# mu &lt;- sqrt(1/lp)
# y &lt;- rinvgauss(n=ndpts,mean=mu,shape=2)
# gmod &lt;- bigssg(y~x1v*x2v,family="inverse.gaussian",type=list(x1v="cub",x2v="cub"),nknots=50)
# 1/gmod$dispersion   ## dispersion = 1/shape
# crossprod( lp - gmod$linear.predictor )/length(lp)

# negative binomial response (known dispersion)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
mu &lt;- exp(lp)
y &lt;- rnbinom(n=ndpts,size=.5,mu=mu)
gmod &lt;- bigssg(y~x1v*x2v,family=list("negbin",2),type=list(x1v="cub",x2v="cub"),nknots=50)
1/gmod$dispersion   ## dispersion = 1/size
crossprod( lp - gmod$linear.predictor )/length(lp)

# negative binomial response (unknown dispersion)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
mu &lt;- exp(lp)
y &lt;- rnbinom(n=ndpts,size=.5,mu=mu)
gmod &lt;- bigssg(y~x1v*x2v,family="negbin",type=list(x1v="cub",x2v="cub"),nknots=50)
1/gmod$dispersion   ## dispersion = 1/size
crossprod( lp - gmod$linear.predictor )/length(lp)

## End(Not run)

</code></pre>

<hr>
<h2 id='bigssp'>
Fits Smoothing Splines with Parametric Effects
</h2><span id='topic+bigssp'></span>

<h3>Description</h3>

<p>Given a real-valued response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code>, a semiparametric regression model has the form </p>
<p style="text-align: center;"><code class="reqn">y_{i}= \eta(\mathbf{x}_{i}) + \sum_{j=1}^{t}b_{j}z_{ij} + e_{i}</code>
</p>
<p> where <code class="reqn">y_{i}</code> is the <code class="reqn">i</code>-th observation's respone, <code class="reqn">\mathbf{x}_{i}=(x_{i1},\ldots,x_{ip})</code> is the <code class="reqn">i</code>-th observation's nonparametric predictor vector, <code class="reqn">\eta</code> is an unknown smooth function relating the response and nonparametric predictors, <code class="reqn">\mathbf{z}_{i}=(z_{i1},\ldots,z_{it})</code> is the <code class="reqn">i</code>-th observation's parametric predictor vector, and <code class="reqn">e_{i}\sim\mathrm{N}(0,\sigma^{2})</code> is iid Gaussian error. Function can fit both additive and interactive non/parametric effects, and allows for 2-way and 3-way interactions between nonparametric and parametric effects (see Details and Examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigssp(formula,data=NULL,type=NULL,nknots=NULL,rparm=NA,
       lambdas=NULL,skip.iter=TRUE,se.fit=FALSE,rseed=1234,
       gcvopts=NULL,knotcheck=TRUE,thetas=NULL,weights=NULL,
       random=NULL,remlalg=c("FS","NR","EM","none"),remliter=500,
       remltol=10^-4,remltau=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigssp_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>. Or an object of class &quot;makessp&quot;, which is output from <code><a href="#topic+makessp">makessp</a></code>.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="cub0"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, <code>type="ord"</code> for ordinal, and <code>type="nom"</code> for nominal. Use <code>type="prm"</code> for parametric effect.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default <code>lambdas=10^-c(9:0)</code>.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Skip Iteration section.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of the fitted values should be estimated.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 3 elements: (a) <code>maxit</code>: maximum number of algorithm iterations, (b) <code>gcvtol</code>: covergence tolerance for iterative GCV update, and (c) <code>alpha</code>: tuning parameter for GCV minimization. Default: <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1)</code>
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_thetas">thetas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor subspace. See Details. 
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_random">random</code></td>
<td>

<p>Adds random effects to model (see Random Effects section).
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_remlalg">remlalg</code></td>
<td>

<p>REML algorithm for estimating variance components (see Random Effects section). Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_remliter">remliter</code></td>
<td>

<p>Maximum number of iterations for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_remltol">remltol</code></td>
<td>

<p>Convergence tolerance for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="bigssp_+3A_remltau">remltau</code></td>
<td>

<p>Initial estimate of variance parameters for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>formula</code> syntax is similar to that used in <code><a href="stats.html#topic+lm">lm</a></code> and many other R regression functions. Use <code>y~x</code> to predict the response <code>y</code> from the predictor <code>x</code>. Use <code>y~x1+x2</code> to fit an additive model of the predictors <code>x1</code> and <code>x2</code>, and use <code>y~x1*x2</code> to fit an interaction model. The syntax <code>y~x1*x2</code> includes the interaction and main effects, whereas the syntax <code>y~x1:x2</code> only includes the interaction. See Computational Details for specifics about how non/parametric effects are estimated.
</p>
<p>See <code><a href="#topic+bigspline">bigspline</a></code> for definitions of <code>type="cub"</code>, <code>type="cub0"</code>, and <code>type="per"</code> splines, which can handle one-dimensional predictors. See Appendix of Helwig and Ma (2015) for information about <code>type="tps"</code> and <code>type="nom"</code> splines. Note that <code>type="tps"</code> can handle one-, two-, or three-dimensional predictors. I recommend using <code>type="cub"</code> if the predictor scores have no extreme outliers; when outliers are present, <code>type="tps"</code> may produce a better result. 
</p>
<p>Using the rounding parameter input <code>rparm</code> can greatly speed-up and stabilize the fitting for large samples. For typical cases, I recommend using <code>rparm=0.01</code> for cubic and periodic splines, but smaller rounding parameters may be needed for particularly jagged functions. For thin-plate splines, the data are NOT transformed to the interval [0,1] before fitting, so the rounding parameter should be on the raw data scale. Also, for <code>type="tps"</code> you can enter one rounding parameter for each predictor dimension. Use <code>rparm=1</code> for ordinal and nominal splines.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values corresponding to the original data points in <code>xvars</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of <code>fitted.values</code> (if input <code>se.fit=TRUE)</code>.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code>xvars</code></td>
<td>
<p>List of predictors.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Type of smoothing spline that was used for each predictor.</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>Mean of <code>yvar</code> for unique points after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique rows of <code>xvars</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation, i.e., <code class="reqn">\hat{\sigma}</code>.</p>
</td></tr>
<tr><td><code>ndf</code></td>
<td>
<p>Data frame with two elements: <code>n</code> is total sample size, and <code>df</code> is effective degrees of freedom of fit model (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model (assuming Gaussian error).</p>
</td></tr>
<tr><td><code>modelspec</code></td>
<td>
<p>List containing specifics of fit model (needed for prediction).</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Convergence status: <code>converged=TRUE</code> if iterative update converged, <code>converged=FALSE</code> if iterative update failed to converge, and <code>converged=NA</code> if option <code>skip.iter=TRUE</code> was used.</p>
</td></tr>
<tr><td><code>tnames</code></td>
<td>
<p>Names of the terms in model.</p>
</td></tr>
<tr><td><code>random</code></td>
<td>
<p>Random effects formula (same as input).</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>Variance parameters such that <code>sigma*sqrt(tau)</code> gives standard deviation of random effects (if <code>!is.null(random)</code>).</p>
</td></tr>
<tr><td><code>blup</code></td>
<td>
<p>Best linear unbiased predictors (if <code>!is.null(random)</code>).</p>
</td></tr>  
<tr><td><code>call</code></td>
<td>
<p>Called model in input <code>formula</code>.</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>Cubic and cubic periodic splines transform the predictor to the interval [0,1] before fitting.
</p>
<p>When using rounding parameters, output <code>fitted.values</code> corresponds to unique rounded predictor scores in output <code>xunique</code>. Use <code><a href="#topic+predict.bigssp">predict.bigssp</a></code> function to get fitted values for full <code>yvar</code> vector.
</p>


<h3>Computational Details </h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the penalized least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}\left(y_{i} - \eta(\mathbf{x}_{i}) - \textstyle\sum_{j=1}^{t}b_{j}z_{ij} \right)^{2} + \lambda J(\eta)</code>
</p>
<p> where <code class="reqn">J(\cdot)</code> is a nonnegative penalty functional quantifying the roughness of <code class="reqn">\eta</code> and <code class="reqn">\lambda&gt;0</code> is a smoothing parameter controlling the trade-off between fitting and smoothing the data. Note that for <code class="reqn">p&gt;1</code> nonparametric predictors, there are additional <code class="reqn">\theta_{k}</code> smoothing parameters embedded in <code class="reqn">J</code>. 
</p>
<p>The penalized least squares functioncal can be rewritten as </p>
<p style="text-align: center;"><code class="reqn"> \|\mathbf{y} - \mathbf{K}\mathbf{d} - \mathbf{J}_{\theta}\mathbf{c}\|^{2} + n\lambda\mathbf{c}'\mathbf{Q}_{\theta}\mathbf{c} </code>
</p>

<p>where <code class="reqn">\mathbf{K}=\{\phi(x_{i}),\mathbf{z}_{i}\}_{n \times m}</code> is the parametric space basis function matrix, <code class="reqn">\mathbf{J}_{\theta}=\sum_{k=1}^{s}\theta_{k}\mathbf{J}_{k}</code> with <code class="reqn">\mathbf{J}_{k}=\{\rho_{k}(\mathbf{x}_{i},\mathbf{x}_{h}^{*})\}_{n \times q}</code> denoting the <code class="reqn">k</code>-th contrast space basis funciton matrix, <code class="reqn">\mathbf{Q}_{\theta}=\sum_{k=1}^{s}\theta_{k}\mathbf{Q}_{k}</code> with <code class="reqn">\mathbf{Q}_{k}=\{\rho_{k}(\mathbf{x}_{g}^{*},\mathbf{x}_{h}^{*})\}_{q \times q}</code> denoting the <code class="reqn">k</code>-th penalty matrix, and <code class="reqn">\mathbf{d}=(d_{0},\ldots,d_{m})'</code> and <code class="reqn">\mathbf{c}=(c_{1},\ldots,c_{q})'</code> are the unknown basis function coefficients. The optimal smoothing parameters are chosen by minimizing the GCV score (see <code><a href="#topic+bigspline">bigspline</a></code>). 
</p>
<p>Note that this function uses the classic smoothing spline parameterization (see Gu, 2013), so there is more than one smoothing parameter per predictor (if interactions are included in the model). To evaluate the GCV score, this function uses the improved (scalable) SSA algorithm discussed in Helwig (2013) and Helwig and Ma (2015).
</p>


<h3>Skip Iteration </h3>

<p>For <code class="reqn">p&gt;1</code> predictors, initial values for the <code class="reqn">\theta_{k}</code> parameters are estimated using Algorithm 3.2 described in Gu and Wahba (1991). 
</p>
<p>Default use of this function (<code>skip.iter=TRUE</code>) fixes the <code class="reqn">\theta_{k}</code> parameters afer the smart start, and then finds the global smoothing parameter <code class="reqn">\lambda</code> (among the input <code>lambdas</code>) that minimizes the GCV score. This approach typically produces a solution very similar to the more optimal solution using <code>skip.iter=FALSE</code>.
</p>
<p>Setting <code>skip.iter=FALSE</code> uses the same smart starting algorithm as setting <code>skip.iter=TRUE</code>. However, instead of fixing the <code class="reqn">\theta_{k}</code> parameters afer the smart start, using <code>skip.iter=FALSE</code> iterates between estimating the optimal <code class="reqn">\lambda</code> and the optimal <code class="reqn">\theta_{k}</code> parameters. The R function <code><a href="stats.html#topic+nlm">nlm</a></code> is used to minimize the GCV score with respect to the <code class="reqn">\theta_{k}</code> parameters, which can be time consuming for models with many predictors.
</p>


<h3>Random Effects </h3>

<p>The input <code>random</code> adds random effects to the model assuming a variance components structure. Both nested and crossed random effects are supported. In all cases, the random effects are assumed to be indepedent zero-mean Gaussian variables with the variance depending on group membership.
</p>
<p>Random effects are distinguished by vertical bars (&quot;|&quot;), which separate expressions for design matrices (left) from group factors (right). For example, the syntax <code>~1|group</code> includes a random intercept for each level of <code>group</code>, whereas the syntax <code>~1+x|group</code> includes both a random intercept and a random slope for each level of <code>group</code>. For crossed random effects, parentheses are needed to distinguish different terms, e.g., <code>~(1|group1)+(1|group2)</code> includes a random intercept for each level of <code>group1</code> and a random intercept for each level of <code>group2</code>, where both <code>group1</code> and <code>group2</code> are factors. For nested random effects, the syntax <code>~group|subject</code> can be used, where both <code>group</code> and <code>subject</code> are factors such that the levels of <code>subject</code> are nested within those of <code>group</code>.
</p>
<p>The input <code>remlalg</code> determines the REML algorithm used to estimate the variance components. Setting <code>remlalg="FS"</code> uses a Fisher Scoring algorithm (default). Setting <code>remlalg="NR"</code> uses a Newton-Raphson algorithm. Setting <code>remlalg="EM"</code> uses an Expectation Maximization algorithm. Use <code>remlalg="none"</code> to fit a model with known variance components (entered through <code>remltau</code>).
</p>
<p>The input <code>remliter</code> sets the maximum number of iterations for the REML estimation. The input <code>remltol</code> sets the convergence tolerance for the REML estimation, which is determined via relative change in the REML log-likelihood. The input <code>remltau</code> sets the initial estimates of variance parameters; default is <code>remltau = rep(1,ntau)</code> where <code>ntau</code> is the number of variance components.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized least-squares, which does not require the Gaussian error assumption. However, the spline inference information (e.g., standard errors and fit information) requires the Gaussian error assumption.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Wahba, G. (1991). Minimizing GCV/GML scores with multiple smoothing parameters via the Newton method. <em>SIAM Journal on Scientific and Statistical Computing, 12</em>, 383-398.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE   ##########

# function with four continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v,x3v,x4v){
  sin(2*pi*x1v) + log(x2v+.1) + x3v*cos(pi*(x4v))
  }
x1v &lt;- runif(500)
x2v &lt;- runif(500)
x3v &lt;- runif(500)
x4v &lt;- runif(500)
y &lt;- myfun(x1v,x2v,x3v,x4v) + rnorm(500)

# fit cubic spline model with x3v*x4v interaction and x3v as "cub" 
# (includes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v*x4v,type=list(x1v="cub",x2v="cub",x3v="cub",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

# fit cubic spline model with x3v*x4v interaction and x3v as "cub0"
# (includes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v*x4v,type=list(x1v="cub",x2v="cub",x3v="cub0",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

# fit model with x3v*x4v interaction treating x3v as parametric effect
# (includes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v*x4v,type=list(x1v="cub",x2v="cub",x3v="prm",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

# fit cubic spline model with x3v:x4v interaction and x3v as "cub"
# (excludes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v:x4v,type=list(x1v="cub",x2v="cub",x3v="cub",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

# fit cubic spline model with x3v:x4v interaction and x3v as "cub0"
# (excludes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v:x4v,type=list(x1v="cub",x2v="cub",x3v="cub0",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

# fit model with x3v:x4v interaction treating x3v as parametric effect
# (excludes x3v and x4v main effects)
cubmod &lt;- bigssp(y~x1v+x2v+x3v:x4v,type=list(x1v="cub",x2v="cub",x3v="prm",x4v="cub"),nknots=50)
crossprod( myfun(x1v,x2v,x3v,x4v) - cubmod$fitted.values )/500

</code></pre>

<hr>
<h2 id='bigtps'>
Fits Cubic Thin-Plate Splines
</h2><span id='topic+bigtps'></span>

<h3>Description</h3>

<p>Given a real-valued response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code>, a thin-plate spline model has the form </p>
<p style="text-align: center;"><code class="reqn">y_{i}=\eta(\mathbf{x}_{i})+e_{i}</code>
</p>
<p> where <code class="reqn">y_{i}</code> is the <code class="reqn">i</code>-th observation's respone, <code class="reqn">\mathbf{x}_{i}=(x_{i1},\ldots,x_{id})</code> is the <code class="reqn">i</code>-th observation's nonparametric predictor vector, <code class="reqn">\eta</code> is an unknown smooth function relating the response and predictor, and <code class="reqn">e_{i}\sim\mathrm{N}(0,\sigma^{2})</code> is iid Gaussian error. Function only fits interaction models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigtps(x,y,nknots=NULL,nvec=NULL,rparm=NA,
       alpha=1,lambdas=NULL,se.fit=FALSE,
       rseed=1234,knotcheck=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigtps_+3A_x">x</code></td>
<td>

<p>Predictor vector or matrix with three or less columns.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_y">y</code></td>
<td>

<p>Response vector. Must be same length as <code>x</code> has rows.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>x</code> to use as knots.
</p>
</td></tr> 
<tr><td><code id="bigtps_+3A_nvec">nvec</code></td>
<td>

<p>Number of eigenvectors (and eigenvalues) to use in approximation. Must be less than or equal to the number of knots and greater than or equal to <code>ncol(x)+2</code>. Default sets <code>nvec&lt;-nknots</code>. Can also input <code>0&lt;nvec&lt;1</code> to retain <code>nvec</code> percentage of eigenbasis variation.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_rparm">rparm</code></td>
<td>

<p>Rounding parameter(s) for <code>x</code>. Use <code>rparm=NA</code> to fit unrounded solution. Can provide one (positive) rounding parameter for each column of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_alpha">alpha</code></td>
<td>

<p>Manual tuning parameter for GCV score. Using <code>alpha=1</code> gives unbaised esitmate. Using a larger alpha enforces a smoother estimate.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default estimates smoothing parameter that minimizes GCV score.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of fitted values should be estimated. 
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="bigtps_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the penalized least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\eta(\mathbf{x}_{i}))^{2}+\lambda J(\eta)</code>
</p>
<p> where <code class="reqn">J(\eta)</code> is the thin-plate penalty (see Helwig and Ma) and <code class="reqn">\lambda\geq0</code> is a smoothing parameter that controls the trade-off between fitting and smoothing the data. Default use of the function estimates <code class="reqn">\lambda</code> by minimizing the GCV score (see <code><a href="#topic+bigspline">bigspline</a></code>).
</p>
<p>Using the rounding parameter input <code>rparm</code> can greatly speed-up and stabilize the fitting for large samples. When <code>rparm</code> is used, the spline is fit to a set of unique data points after rounding; the unique points are determined using the efficient algorithm described in Helwig (2013). Rounding parameter should be on the raw data scale. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values corresponding to the original data points in <code>x</code> (if <code>rparm=NA</code>) or the rounded data points in <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of <code>fitted.values</code> (if input <code>se.fit=TRUE)</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>Predictor vector (same as input).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response vector (same as input).</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique elements of <code>x</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>Mean of <code>y</code> for unique elements of <code>x</code> after rounding (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>funique</code></td>
<td>
<p>Vector giving frequency of each element of <code>xunique</code> (if <code>rparm</code> is used).</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation, i.e., <code class="reqn">\hat{\sigma}</code>.</p>
</td></tr>
<tr><td><code>ndf</code></td>
<td>
<p>Data frame with two elements: <code>n</code> is total sample size, and <code>df</code> is effective degrees of freedom of fit model (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model (assuming Gaussian error).</p>
</td></tr>
<tr><td><code>myknots</code></td>
<td>
<p>Spline knots used for fit.</p>
</td></tr>
<tr><td><code>nvec</code></td>
<td>
<p>Number of eigenvectors used for solution.</p>
</td></tr>
<tr><td><code>rparm</code></td>
<td>
<p>Rounding parameter for <code>x</code> (same as input).</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Optimal smoothing parameter.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Spline basis function coefficients.</p>
</td></tr>
<tr><td><code>coef.csqrt</code></td>
<td>
<p>Matrix square-root of covariace matrix of <code>coef</code>. Use <code>tcrossprod(coef.csqrt)</code> to get covariance matrix of <code>coef</code>.</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>Input <code>nvec</code> must be greater than <code>ncol(x)+1</code>.
</p>
<p>When using rounding parameters, output <code>fitted.values</code> corresponds to unique rounded predictor scores in output <code>xunique</code>. Use <code><a href="#topic+predict.bigtps">predict.bigtps</a></code> function to get fitted values for full <code>y</code> vector.
</p>


<h3>Computational Details </h3>

<p>According to thin-plate spline theory, the function <code class="reqn">\eta</code> can be approximated as </p>
<p style="text-align: center;"><code class="reqn">\eta(x) = \sum_{k=1}^{M}d_{k}\phi_{k}(\mathbf{x}) + \sum_{h=1}^{q}c_{h}\xi(\mathbf{x},\mathbf{x}_{h}^{*})</code>
</p>
<p> where the <code class="reqn">\{\phi_{k}\}_{k=1}^{M}</code> are linear functions, <code class="reqn">\xi</code> is the thin-plate spline semi-kernel, <code class="reqn">\{\mathbf{x}_{h}^{*}\}_{h=1}^{q}</code> are the knots, and the <code class="reqn">c_{h}</code> coefficients are constrained to be orthongonal to the <code class="reqn">\{\phi_{k}\}_{k=1}^{M}</code> functions.
</p>
<p>This implies that the penalized least-squares functional can be rewritten as </p>
<p style="text-align: center;"><code class="reqn"> \|\mathbf{y} - \mathbf{K}\mathbf{d} - \mathbf{J}\mathbf{c}\|^{2} + n\lambda\mathbf{c}'\mathbf{Q}\mathbf{c} </code>
</p>

<p>where <code class="reqn">\mathbf{K}=\{\phi(\mathbf{x}_{i})\}_{n \times M}</code> is the null space basis function matrix, <code class="reqn">\mathbf{J}=\{\xi(\mathbf{x}_{i},\mathbf{x}_{h}^{*})\}_{n \times q}</code> is the contrast space basis funciton matrix, <code class="reqn">\mathbf{Q}=\{\xi(\mathbf{x}_{g}^{*},\mathbf{x}_{h}^{*})\}_{q \times q}</code> is the penalty matrix, and <code class="reqn">\mathbf{d}=(d_{0},\ldots,d_{M})'</code> and <code class="reqn">\mathbf{c}=(c_{1},\ldots,c_{q})'</code> are the unknown basis function coefficients, where <code class="reqn">\mathbf{c}</code> are constrained to be orthongonal to the <code class="reqn">\{\phi_{k}\}_{k=1}^{M}</code> functions.
</p>
<p>See Helwig and Ma for specifics about how the constrained estimation is handled.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized least-squares, which does not require the Gaussian error assumption. However, the spline inference information (e.g., standard errors and fit information) requires the Gaussian error assumption.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define relatively smooth function
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(500)
y &lt;- myfun(x) + rnorm(500)

# fit thin-plate spline (default 1 dim: 30 knots)
tpsmod &lt;- bigtps(x,y)
tpsmod


##########   EXAMPLE 2   ##########

# define more jagged function
set.seed(773)
myfun &lt;- function(x){ 2*x+cos(2*pi*x) }
x &lt;- runif(500)*4
y &lt;- myfun(x) + rnorm(500)

# try different numbers of knots
r1mod &lt;- bigtps(x,y,nknots=20,rparm=0.01)
crossprod( myfun(r1mod$xunique) - r1mod$fitted )/length(r1mod$fitted)
r2mod &lt;- bigtps(x,y,nknots=35,rparm=0.01)
crossprod( myfun(r2mod$xunique) - r2mod$fitted )/length(r2mod$fitted)
r3mod &lt;- bigtps(x,y,nknots=50,rparm=0.01)
crossprod( myfun(r3mod$xunique) - r3mod$fitted )/length(r3mod$fitted)


##########   EXAMPLE 3   ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x &lt;- cbind(runif(500),runif(500))
y &lt;- myfun(x[,1],x[,2]) + rnorm(500)

# fit thin-plate spline with 50 knots (default 2 dim: 100 knots)
tpsmod &lt;- bigtps(x,y,nknots=50)
tpsmod
crossprod( myfun(x[,1],x[,2]) - tpsmod$fitted.values )/500


##########   EXAMPLE 4   ##########

# function with three continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v,x3v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*x3v)
  }
x &lt;- cbind(runif(500),runif(500),runif(500))
y &lt;- myfun(x[,1],x[,2],x[,3]) + rnorm(500)

# fit thin-plate spline with 50 knots (default 3 dim: 200 knots)
tpsmod &lt;- bigtps(x,y,nknots=50)
tpsmod
crossprod( myfun(x[,1],x[,2],x[,3]) - tpsmod$fitted.values )/500

</code></pre>

<hr>
<h2 id='binsamp'>
Bin-Samples Strategic Knot Indices
</h2><span id='topic+binsamp'></span>

<h3>Description</h3>

<p>Breaks the predictor domain into a user-specified number of disjoint subregions, and randomly samples a user-specified number of observations from each (nonempty) subregion. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binsamp(x,xrng=NULL,nmbin=11,nsamp=1,alg=c("new","old"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binsamp_+3A_x">x</code></td>
<td>

<p>Matrix of predictors <code class="reqn">\mathbf{X}=\{x_{ij}\}_{n \times p}</code> where <code class="reqn">n</code> is the number of observations, and <code class="reqn">p</code> is the number of predictors.
</p>
</td></tr>
<tr><td><code id="binsamp_+3A_xrng">xrng</code></td>
<td>

<p>Optional matrix of predictor ranges: <code class="reqn">\mathbf{R}=\{r_{kj}\}_{2 \times p}</code> where <code class="reqn">r_{1j}=\min_{i}x_{ij}</code> and <code class="reqn">r_{2j}=\max_{i}x_{ij}</code>.
</p>
</td></tr>
<tr><td><code id="binsamp_+3A_nmbin">nmbin</code></td>
<td>

<p>Vector <code class="reqn">\mathbf{b}=(b_{1},\ldots,b_{p})'</code>, where <code class="reqn">b_{j}\geq1</code> is the number of marginal bins to use for the <code class="reqn">j</code>-th predictor. If <code>length(nmbin)&lt;ncol(x)</code>, then <code>nmbin[1]</code> is used for all columns. Default is <code>nmbin=11</code> marginal bins for each dimension.
</p>
</td></tr>
<tr><td><code id="binsamp_+3A_nsamp">nsamp</code></td>
<td>

<p>Scalar <code class="reqn">s\geq1</code> giving the number of observations to sample from each bin. Default is sample <code>nsamp=1</code> observation from each bin.
</p>
</td></tr>
<tr><td><code id="binsamp_+3A_alg">alg</code></td>
<td>

<p>Bin-sampling algorithm. New algorithm forms equidistant grid, whereas old algorithm forms approximately equidistant grid. New algorithm is default for versions 1.0-1 and later.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an index vector indicating the rows of <code>x</code> that were bin-sampled. 
</p>


<h3>Warnings </h3>

<p>If <code class="reqn">x_{ij}</code> is nominal with <code class="reqn">g</code> levels, the function requires <code class="reqn">b_{j}=g</code> and <code class="reqn">x_{ij}\in\{1,\ldots,g\}</code> for <code class="reqn">i\in\{1,\ldots,n\}</code>.
</p>


<h3>Note</h3>

<p>The number of returned knots will depend on the distribution of the covariate scores. The maximum number of possible bin-sampled knots is <code class="reqn">s\prod_{j=1}^{p}b_{j}</code>, but fewer knots will be returned if one (or more) of the bins is empty (i.e., if there is no data in one or more bins).
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##########   EXAMPLE 1   ##########

# create 2-dimensional predictor (both continuous)
set.seed(123)
xmat &lt;- cbind(runif(10^6),runif(10^6))

# Default use:
#   10 marginal bins for each predictor
#   sample 1 observation from each subregion
xind &lt;- binsamp(xmat)

# get the corresponding knots
bknots &lt;- xmat[xind,]

# compare to randomly-sampled knots
rknots &lt;- xmat[sample(1:(10^6),100),]
par(mfrow=c(1,2))
plot(bknots,main="bin-sampled")
plot(rknots,main="randomly sampled")



##########   EXAMPLE 2   ##########

# create 2-dimensional predictor (continuous and nominal)
set.seed(123)
xmat &lt;- cbind(runif(10^6),sample(1:3,10^6,replace=TRUE))

# use 10 marginal bins for x1 and 3 marginal bins for x2 
# and sample one observation from each subregion
xind &lt;- binsamp(xmat,nmbin=c(10,3))

# get the corresponding knots
bknots &lt;- xmat[xind,]

# compare to randomly-sampled knots
rknots &lt;- xmat[sample(1:(10^6),30),]
par(mfrow=c(1,2))
plot(bknots,main="bin-sampled")
plot(rknots,main="randomly sampled")



##########   EXAMPLE 3   ##########

# create 3-dimensional predictor (continuous, continuous, nominal)
set.seed(123)
xmat &lt;- cbind(runif(10^6),runif(10^6),sample(1:2,10^6,replace=TRUE))

# use 10 marginal bins for x1 and x2, and 2 marginal bins for x3 
# and sample one observation from each subregion
xind &lt;- binsamp(xmat,nmbin=c(10,10,2))

# get the corresponding knots
bknots &lt;- xmat[xind,]

# compare to randomly-sampled knots
rknots &lt;- xmat[sample(1:(10^6),200),]
par(mfrow=c(2,2))
plot(bknots[1:100,1:2],main="bin-sampled, x3=1")
plot(bknots[101:200,1:2],main="bin-sampled, x3=2")
plot(rknots[rknots[,3]==1,1:2],main="randomly sampled, x3=1")
plot(rknots[rknots[,3]==2,1:2],main="randomly sampled, x3=2")

</code></pre>

<hr>
<h2 id='imagebar'>
Displays a Color Image with Colorbar
</h2><span id='topic+imagebar'></span>

<h3>Description</h3>

<p>This is a modification to the R function <code><a href="Matrix.html#topic+image">image</a></code> that adds a colorbar to the margin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imagebar(x,y,z,xlim=NULL,ylim=NULL,zlim=NULL,
         zlab=NULL,zcex.axis=NULL,zcex.lab=NULL,
         zaxis.at=NULL,zaxis.labels=TRUE,
         col=NULL,ncolor=21,drawbar=TRUE,zline=2,
         pltimage=c(.2,.8,.2,.8),pltbar=c(.82,.85,.2,.8),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imagebar_+3A_x">x</code>, <code id="imagebar_+3A_y">y</code></td>
<td>

<p>Locations of grid lines at which the values in <code>z</code> are measured. These must be finite, non-missing and in (strictly) ascending order. If only <code>x</code> is given, the input <code>x</code> is treated as <code>z</code>.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_z">z</code></td>
<td>

<p>A matrix containing the values to be plotted (<code>NA</code>s are allowed). If <code>x</code> and <code>y</code> are missing, a sequence from 0 to 1 is used for plotting.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_xlim">xlim</code>, <code id="imagebar_+3A_ylim">ylim</code></td>
<td>

<p>Ranges for the plotted <code>x</code> and <code>y</code> values, defaulting to the ranges of <code>x</code> and <code>y</code>.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zlim">zlim</code></td>
<td>

<p>The minimum and maximum <code>z</code> values for which colors should be plotted, defaulting to the range of the finite values of <code>z</code>.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zlab">zlab</code></td>
<td>

<p>Label for the colorbar.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zcex.axis">zcex.axis</code></td>
<td>

<p>The magnification to be used for the z-axis annotation (colorbar scale).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zcex.lab">zcex.lab</code></td>
<td>

<p>The magnification to be used for the z-axis label (<code>zlab</code>).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zaxis.at">zaxis.at</code></td>
<td>

<p>The points at which tick-marks are to be drawn for the colorbar. Points outside of the range of <code>zlim</code> will not be plotted.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zaxis.labels">zaxis.labels</code></td>
<td>

<p>This can either be a logical value specifying whether (numerical) annotations are to be made at the tickmarks, or a character or expression vector of labels to be placed at the tickpoints.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_col">col</code></td>
<td>

<p>Color scheme to use. Default is from <code>blueviolet</code> (low) to <code>red</code> (high).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_ncolor">ncolor</code></td>
<td>

<p>The number of colors to use in the color scheme.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_drawbar">drawbar</code></td>
<td>

<p>Logical indicating if the colorbar should be drawn.
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_zline">zline</code></td>
<td>

<p>Number of lines into the margin at which the axis line will be drawn (see <code><a href="graphics.html#topic+axis">axis</a></code>).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_pltimage">pltimage</code></td>
<td>

<p>A vector of the form c(x1, x2, y1, y2) giving the coordinates of the image region as fractions of the current figure region (see <code><a href="graphics.html#topic+par">par</a></code>).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_pltbar">pltbar</code></td>
<td>

<p>A vector of the form c(x1, x2, y1, y2) giving the coordinates of the colorbar region as fractions of the current figure region (see <code><a href="graphics.html#topic+par">par</a></code>).
</p>
</td></tr>
<tr><td><code id="imagebar_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to <code>image</code> (e.g., <code>xlab</code>, <code>ylab</code>, <code>main</code>, <code>cex</code>, <code>cex.axis</code>, <code>cex.lab</code>, etc.)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Produces an <code>image</code> plot with a colorbar.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##########   EXAMPLE 1   ##########

myfun &lt;- function(x){
  2*sin(sqrt(x[,1]^2+x[,2]^2+.1))/sqrt(x[,1]^2+x[,2]^2+.1)
}
x &lt;- expand.grid(seq(-8,8,l=100),seq(-8,8,l=100))
imagebar(seq(-8,8,l=100),seq(-8,8,l=100),matrix(myfun(x),100,100),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))),zlim=c(-0.5,2),zaxis.at=seq(-0.5,2,by=0.5))


##########   EXAMPLE 2   ##########

myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + 2*sin(sqrt(x2v^2+.1))/sqrt(x2v^2+.1)
}
x &lt;- expand.grid(x1v=seq(0,1,l=100),x2v=seq(-8,8,l=100))
imagebar(seq(0,1,l=100),seq(-8,8,l=100),matrix(myfun(x$x1v,x$x2v),100,100),
         col=c("red","orange","yellow","white"),xlab="x1v",ylab="x2v",
         zlab=expression(hat(italic(y))),zlim=c(-1.5,3),zaxis.at=seq(-1.5,3,by=0.5))


##########   EXAMPLE 3   ##########

myfun &lt;- function(x1v,x2v){
  sin(3*pi*x1v) + sin(2*pi*x2v) + 3*cos(pi*(x1v-x2v))
}
x &lt;- expand.grid(x1v=seq(-1,1,l=100),x2v=seq(-1,1,l=100))
imagebar(seq(-1,1,l=100),seq(-1,1,l=100),matrix(myfun(x$x1v,x$x2v),100,100),
         col=c("blue","green","light green","yellow"),xlab="x1v",ylab="x2v",
         zlab=expression(hat(italic(y))),zlim=c(-5,5),zaxis.at=c(-5,0,5),
         zaxis.labels=c("low","med","high"))

</code></pre>

<hr>
<h2 id='makessa'>
Makes Objects to Fit Smoothing Spline ANOVA Models
</h2><span id='topic+makessa'></span>

<h3>Description</h3>

<p>This function creates a list containing the necessary information to fit a smoothing spline anova model (see <code><a href="#topic+bigssa">bigssa</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makessa(formula,data=NULL,type=NULL,nknots=NULL,rparm=NA,
        lambdas=NULL,skip.iter=TRUE,se.fit=FALSE,rseed=1234,
        gcvopts=NULL,knotcheck=TRUE,gammas=NULL,weights=NULL,
        random=NULL,remlalg=c("FS","NR","EM","none"),remliter=500,
       remltol=10^-4,remltau=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makessa_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="makessa_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="acub"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, and <code>type="nom"</code> for nominal.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="makessa_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default uses <code>lambdas=10^-c(9:0)</code>
</p>
</td></tr>
<tr><td><code id="makessa_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Computational Details.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of the fitted values should be estimated.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="makessa_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 3 elements: (a) <code>maxit</code>: maximum number of algorithm iterations, (b) <code>gcvtol</code>: covergence tolerance for iterative GCV update, and (c) <code>alpha</code>: tuning parameter for GCV minimization. Default: <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1)</code>
</p>
</td></tr>
<tr><td><code id="makessa_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="makessa_+3A_gammas">gammas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="makessa_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="makessa_+3A_random">random</code></td>
<td>

<p>Adds random effects to model (see Random Effects section).
</p>
</td></tr>
<tr><td><code id="makessa_+3A_remlalg">remlalg</code></td>
<td>

<p>REML algorithm for estimating variance components (see Random Effects section). Input is ignored if <code>is.null(random)</code>.  
</p>
</td></tr>
<tr><td><code id="makessa_+3A_remliter">remliter</code></td>
<td>

<p>Maximum number of iterations for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="makessa_+3A_remltol">remltol</code></td>
<td>

<p>Convergence tolerance for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="makessa_+3A_remltau">remltau</code></td>
<td>

<p>Initial estimate of variance parameters for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bigssa">bigssa</a></code> and below example for more details.
</p>


<h3>Value</h3>

<p>An object of class &quot;makessa&quot;, which can be input to <code><a href="#topic+bigssa">bigssa</a></code>.
</p>


<h3>Warning </h3>

<p>When inputting a &quot;makessa&quot; class object into <code><a href="#topic+bigssa">bigssa</a></code>, the formula input to <code>bigssa</code> must be a nested version of the original formula input to <code>makessa</code>. In other words, you cannot add any new effects after a &quot;makessa&quot; object has been created, but you can drop (remove) effects from the model.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE  ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x1v &lt;- runif(500)
x2v &lt;- runif(500)
y &lt;- myfun(x1v,x2v) + rnorm(500)

# fit 2 possible models (create information 2 separate times)
system.time({
  intmod &lt;- bigssa(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
  addmod &lt;- bigssa(y~x1v+x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
})

# fit 2 possible models (create information 1 time)
system.time({
  makemod &lt;- makessa(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
  int2mod &lt;- bigssa(y~x1v*x2v,makemod)
  add2mod &lt;- bigssa(y~x1v+x2v,makemod)
})

# check difference (no difference)
crossprod( intmod$fitted.values - int2mod$fitted.values )
crossprod( addmod$fitted.values - add2mod$fitted.values )

</code></pre>

<hr>
<h2 id='makessg'>
Makes Objects to Fit Generalized Smoothing Spline ANOVA Models
</h2><span id='topic+makessg'></span>

<h3>Description</h3>

<p>This function creates a list containing the necessary information to fit a generalized smoothing spline anova model (see <code><a href="#topic+bigssg">bigssg</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makessg(formula,family,data,type=NULL,nknots=NULL,rparm=NA,
        lambdas=NULL,skip.iter=TRUE,se.lp=FALSE,rseed=1234,
        gcvopts=NULL,knotcheck=TRUE,gammas=NULL,weights=NULL,
        gcvtype=c("acv","gacv","gacv.old"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makessg_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="makessg_+3A_family">family</code></td>
<td>

<p>Distribution for response. One of five options: <code>"binomial"</code>, <code>"poisson"</code>, <code>"Gamma"</code>, <code>"inverse.gaussian"</code>, or <code>"negbin"</code>. See <code><a href="#topic+bigssg">bigssg</a></code>.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="acub"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, and <code>type="nom"</code> for nominal.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="makessg_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default uses <code>lambdas=10^-c(9:0)</code>
</p>
</td></tr>
<tr><td><code id="makessg_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Computational Details.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_se.lp">se.lp</code></td>
<td>

<p>Logical indicating if the standard errors of the linear predictors (<code class="reqn">\eta</code>) should be estimated.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="makessg_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 6 elements: (i) <code>maxit</code>: maximum number of outer iterations, (ii) <code>gcvtol</code>: covergence tolerance for iterative GACV update, (iii) <code>alpha</code>: tuning parameter for GACV minimization, (iv) <code>inmaxit</code>: maximum number of inner iterations for iteratively reweighted fitting, (v) <code>intol</code>: inner convergence tolerance for iteratively reweighted fitting, and (vi) <code>insub</code>: number of data points to subsample when checking inner convergence. <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1,inmaxit=100,intol=10^-5,insub=10^4)</code>
</p>
</td></tr>
<tr><td><code id="makessg_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="makessg_+3A_gammas">gammas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="makessg_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="makessg_+3A_gcvtype">gcvtype</code></td>
<td>

<p>Cross-validation criterion for selecting smoothing parameters (see Details).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bigssg">bigssg</a></code> and below example for more details.
</p>


<h3>Value</h3>

<p>An object of class &quot;makessg&quot;, which can be input to <code><a href="#topic+bigssg">bigssg</a></code>.
</p>


<h3>Warning </h3>

<p>When inputting a &quot;makessg&quot; class object into <code><a href="#topic+bigssg">bigssg</a></code>, the formula input to <code>bigssg</code> must be a nested version of the original formula input to <code>makessg</code>. In other words, you cannot add any new effects after a &quot;makessg&quot; object has been created, but you can drop (remove) effects from the model.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Xiang, D. (2001). Cross-validating non-Gaussian data: Generalized approximate cross-validation revisited. <em>Journal of Computational and Graphical Statistics, 10</em>, 581-591.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE  ##########

# function with two continuous predictors
set.seed(1)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
ndpts &lt;- 1000
x1v &lt;- runif(ndpts)
x2v &lt;- runif(ndpts)

# binomial response (no weights)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
p &lt;- 1/(1+exp(-lp))
y &lt;- rbinom(n=ndpts,size=1,p=p)

# fit 2 possible models (create information 2 separate times)
system.time({
  intmod &lt;- bigssg(y~x1v*x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=50)
  addmod &lt;- bigssg(y~x1v+x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=50)
})

# fit 2 possible models (create information 1 time)
system.time({
  makemod &lt;- makessg(y~x1v*x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=50)
  int2mod &lt;- bigssg(y~x1v*x2v,data=makemod)
  add2mod &lt;- bigssg(y~x1v+x2v,data=makemod)
})

# check difference (no difference)
crossprod( intmod$fitted.values - int2mod$fitted.values )
crossprod( addmod$fitted.values - add2mod$fitted.values )

</code></pre>

<hr>
<h2 id='makessp'>
Makes Objects to Fit Smoothing Splines with Parametric Effects
</h2><span id='topic+makessp'></span>

<h3>Description</h3>

<p>This function creates a list containing the necessary information to fit a smoothing spline with parametric effects (see <code><a href="#topic+bigssp">bigssp</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makessp(formula,data=NULL,type=NULL,nknots=NULL,rparm=NA,
        lambdas=NULL,skip.iter=TRUE,se.fit=FALSE,rseed=1234,
        gcvopts=NULL,knotcheck=TRUE,thetas=NULL,weights=NULL,
        random=NULL,remlalg=c("FS","NR","EM","none"),remliter=500,
       remltol=10^-4,remltau=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makessp_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;<code>formula</code>&quot;: a symbolic description of the model to be fitted (see Details and Examples for more information).
</p>
</td></tr>
<tr><td><code id="makessp_+3A_data">data</code></td>
<td>

<p>Optional data frame, list, or environment containing the variables in <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_type">type</code></td>
<td>

<p>List of smoothing spline types for predictors in <code>formula</code> (see Details). Options include <code>type="cub"</code> for cubic, <code>type="acub"</code> for another cubic, <code>type="per"</code> for cubic periodic, <code>type="tps"</code> for cubic thin-plate, and <code>type="nom"</code> for nominal. Use <code>type="prm"</code> for parametric effect.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_nknots">nknots</code></td>
<td>

<p>Two possible options: (a) scalar giving total number of random knots to sample, or (b) vector indexing which rows of <code>data</code> to use as knots.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_rparm">rparm</code></td>
<td>

<p>List of rounding parameters for each predictor. See Details. 
</p>
</td></tr>
<tr><td><code id="makessp_+3A_lambdas">lambdas</code></td>
<td>

<p>Vector of global smoothing parameters to try. Default uses <code>lambdas=10^-c(9:0)</code>
</p>
</td></tr>
<tr><td><code id="makessp_+3A_skip.iter">skip.iter</code></td>
<td>

<p>Logical indicating whether to skip the iterative smoothing parameter update. Using <code>skip.iter=FALSE</code> should provide a more optimal solution, but the fitting time may be substantially longer. See Computational Details.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating if the standard errors of the fitted values should be estimated.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_rseed">rseed</code></td>
<td>

<p>Random seed for knot sampling. Input is ignored if <code>nknots</code> is an input vector of knot indices. Set <code>rseed=NULL</code> to obtain a different knot sample each time, or set <code>rseed</code> to any positive integer to use a different seed than the default.
</p>
</td></tr>
<tr><td><code id="makessp_+3A_gcvopts">gcvopts</code></td>
<td>

<p>Control parameters for optimization. List with 3 elements: (a) <code>maxit</code>: maximum number of algorithm iterations, (b) <code>gcvtol</code>: covergence tolerance for iterative GCV update, and (c) <code>alpha</code>: tuning parameter for GCV minimization. Default: <code>gcvopts=list(maxit=5,gcvtol=10^-5,alpha=1)</code>
</p>
</td></tr>
<tr><td><code id="makessp_+3A_knotcheck">knotcheck</code></td>
<td>

<p>If <code>TRUE</code>, only unique knots are used (for stability).  
</p>
</td></tr>
<tr><td><code id="makessp_+3A_thetas">thetas</code></td>
<td>

<p>List of initial smoothing parameters for each predictor subspace. See Details. 
</p>
</td></tr>
<tr><td><code id="makessp_+3A_weights">weights</code></td>
<td>

<p>Vector of positive weights for fitting (default is vector of ones).  
</p>
</td></tr>
<tr><td><code id="makessp_+3A_random">random</code></td>
<td>

<p>Adds random effects to model (see Random Effects section).
</p>
</td></tr>
<tr><td><code id="makessp_+3A_remlalg">remlalg</code></td>
<td>

<p>REML algorithm for estimating variance components (see Random Effects section). Input is ignored if <code>is.null(random)</code>.  
</p>
</td></tr>
<tr><td><code id="makessp_+3A_remliter">remliter</code></td>
<td>

<p>Maximum number of iterations for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="makessp_+3A_remltol">remltol</code></td>
<td>

<p>Convergence tolerance for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
<tr><td><code id="makessp_+3A_remltau">remltau</code></td>
<td>

<p>Initial estimate of variance parameters for REML estimation of variance components. Input is ignored if <code>random=NULL</code>.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bigssp">bigssp</a></code> and below example for more details.
</p>


<h3>Value</h3>

<p>An object of class &quot;makessp&quot;, which can be input to <code><a href="#topic+bigssp">bigssp</a></code>.
</p>


<h3>Warning </h3>

<p>When inputting a &quot;makessp&quot; class object into <code><a href="#topic+bigssp">bigssp</a></code>, the formula input to <code>bigssp</code> must be a nested version of the original formula input to <code>makessp</code>. In other words, you cannot add any new effects after a &quot;makessp&quot; object has been created, but you can drop (remove) effects from the model.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE  ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x1v &lt;- runif(500)
x2v &lt;- runif(500)
y &lt;- myfun(x1v,x2v) + rnorm(500)

# fit 2 possible models (create information 2 separate times)
system.time({
  intmod &lt;- bigssp(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
  addmod &lt;- bigssp(y~x1v+x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
})

# fit 2 possible models (create information 1 time)
system.time({
  makemod &lt;- makessp(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
  int2mod &lt;- bigssp(y~x1v*x2v,makemod)
  add2mod &lt;- bigssp(y~x1v+x2v,makemod)
})

# check difference (no difference)
crossprod( intmod$fitted.values - int2mod$fitted.values )
crossprod( addmod$fitted.values - add2mod$fitted.values )

</code></pre>

<hr>
<h2 id='ordspline'>
Fits Ordinal Smoothing Spline
</h2><span id='topic+ordspline'></span>

<h3>Description</h3>

<p>Given a real-valued response vector <code class="reqn">\mathbf{y}=\{y_{i}\}_{n\times1}</code> and an ordinal predictor vector <code class="reqn">\mathbf{x}=\{x_{i}\}_{n\times 1}</code> with <code class="reqn">x_{i} \in \{1,\ldots,K\} \ \forall i</code>, an ordinal smoothing spline model has the form </p>
<p style="text-align: center;"><code class="reqn">y_{i}=\eta(x_{i})+e_{i}</code>
</p>
<p> where <code class="reqn">y_{i}</code> is the <code class="reqn">i</code>-th observation's respone, <code class="reqn">x_{i}</code> is the <code class="reqn">i</code>-th observation's predictor, <code class="reqn">\eta</code> is an unknown function relating the response and predictor, and <code class="reqn">e_{i}\sim\mathrm{N}(0,\sigma^{2})</code> is iid Gaussian error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordspline(x, y, knots, weights, lambda, monotone=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordspline_+3A_x">x</code></td>
<td>

<p>Predictor vector.
</p>
</td></tr>
<tr><td><code id="ordspline_+3A_y">y</code></td>
<td>

<p>Response vector. Must be same length as <code>x</code>.
</p>
</td></tr>
<tr><td><code id="ordspline_+3A_knots">knots</code></td>
<td>

<p>Either a scalar giving the number of equidistant knots to use, or a vector of values to use as the spline knots. If left blank, the number of knots is <code>min(50, nu)</code> where <code>nu = length(unique(x)).</code>
</p>
</td></tr>
<tr><td><code id="ordspline_+3A_weights">weights</code></td>
<td>

<p>Weights vector (for weighted penalized least squares). Must be same length as <code>x</code> and contain non-negative values.
</p>
</td></tr>
<tr><td><code id="ordspline_+3A_lambda">lambda</code></td>
<td>

<p>Smoothing parameter. If left blank, <code>lambda</code> is tuned via Generalized Cross-Validation.
</p>
</td></tr>
<tr><td><code id="ordspline_+3A_monotone">monotone</code></td>
<td>

<p>If <code>TRUE</code>, the relationship between <code>x</code> and <code>y</code> is constrained to be monotonic increasing.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To estimate <code class="reqn">\eta</code> I minimize the penalized least-squares functional </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\eta(x_{i}))^{2}+\lambda \sum_{x=2}^K [\eta(x)-\eta(x-1)]^2 dx</code>
</p>
<p> where <code class="reqn">\lambda\geq0</code> is a smoothing parameter that controls the trade-off between fitting and smoothing the data. 
</p>
<p>Default use of the function estimates <code class="reqn">\lambda</code> by minimizing the GCV score: </p>
<p style="text-align: center;"><code class="reqn">\mbox{GCV}(\lambda) = \frac{n\|(\mathbf{I}_{n}-\mathbf{S}_{\lambda})\mathbf{y}\|^{2}}{[n-\mathrm{tr}(\mathbf{S}_{\lambda})]^2}</code>
</p>
<p> where <code class="reqn">\mathbf{I}_{n}</code> is the identity matrix and <code class="reqn">\mathbf{S}_{\lambda}</code> is the smoothing matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values.</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation, i.e., <code class="reqn">\hat{\sigma}</code>.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Chosen smoothing parameter.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, R-squared, AIC, and BIC of fit model (assuming Gaussian error).</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Spline basis function coefficients.</p>
</td></tr>
<tr><td><code>coef.csqrt</code></td>
<td>
<p>Matrix square-root of covariace matrix of <code>coef</code>. Use <code>tcrossprod(coef.csqrt)</code> to get covariance matrix of <code>coef</code>.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Number of data points, i.e., <code>length(x)</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Effective degrees of freedom (trace of smoothing matrix).</p>
</td></tr>
<tr><td><code>xunique</code></td>
<td>
<p>Unique elements of <code>x</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>Predictor vector (same as input).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response vector (same as input).</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residual vector, i.e., <code>y - fitted.values</code>.</p>
</td></tr>
<tr><td><code>knots</code></td>
<td>
<p>Spline knots used for fit.</p>
</td></tr>
<tr><td><code>monotone</code></td>
<td>
<p>Logical (same as input).</p>
</td></tr>
</table>


<h3>Warnings </h3>

<p>When inputting user-specified <code>knots</code>, all values in <code>knots</code> must match a corresponding value in <code>x</code>.
</p>


<h3>Note</h3>

<p>The spline is estimated using penalized least-squares, which does not require the Gaussian error assumption. However, the spline inference information (e.g., standard errors and fit information) requires the Gaussian error assumption.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE   ##########

# generate some data
n &lt;- 100
nk &lt;- 50
x &lt;- seq(-3,3,length.out=n)
eta &lt;- (sin(2*x/pi) + 0.25*x^3 + 0.05*x^5)/15
set.seed(1)
y &lt;- eta + rnorm(n, sd=0.5)

# plot data and true eta
plot(x, y)
lines(x, eta, col="blue", lwd=2)

# fit ordinal smoothing spline
ossmod &lt;- ordspline(x, y, knots=nk)
lines(ossmod$x, ossmod$fit, col="red", lwd=2)

# fit monotonic smoothing spline
mssmod &lt;- ordspline(x, y, knots=nk, monotone=TRUE)
lines(mssmod$x, mssmod$fit, col="purple", lwd=2)

</code></pre>

<hr>
<h2 id='plotbar'>
Generic X-Y Plotting with Colorbar
</h2><span id='topic+plotbar'></span>

<h3>Description</h3>

<p>This is a modification to the R function <code><a href="graphics.html#topic+plot">plot</a></code> that adds a colorbar to the margin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotbar(x,y,z,xlim=NULL,ylim=NULL,zlim=NULL,
        zlab=NULL,zcex.axis=NULL,zcex.lab=NULL,
        zaxis.at = NULL, zaxis.labels = TRUE,
        col=NULL,ncolor=21,drawbar=TRUE,zline=2,
        pltimage=c(.2,.8,.2,.8),pltbar=c(.82,.85,.2,.8),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotbar_+3A_x">x</code>, <code id="plotbar_+3A_y">y</code></td>
<td>

<p>The x and y coordinates of the points to plot.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_z">z</code></td>
<td>

<p>Numeric vector the same length as <code>x</code> and <code>y</code> containing the values to be plotted in color.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_xlim">xlim</code>, <code id="plotbar_+3A_ylim">ylim</code></td>
<td>

<p>Ranges for the plotted <code>x</code> and <code>y</code> values, defaulting to the ranges of <code>x</code> and <code>y</code>.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zlim">zlim</code></td>
<td>

<p>The minimum and maximum <code>z</code> values for which colors should be plotted, defaulting to the range of the finite values of <code>z</code>.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zlab">zlab</code></td>
<td>

<p>Label for the colorbar.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zcex.axis">zcex.axis</code></td>
<td>

<p>The magnification to be used for the z-axis annotation (colorbar scale).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zcex.lab">zcex.lab</code></td>
<td>

<p>The magnification to be used for the z-axis label (<code>zlab</code>).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zaxis.at">zaxis.at</code></td>
<td>

<p>The points at which tick-marks are to be drawn for the colorbar. Points outside of the range of <code>zlim</code> will not be plotted.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zaxis.labels">zaxis.labels</code></td>
<td>

<p>This can either be a logical value specifying whether (numerical) annotations are to be made at the tickmarks, or a character or expression vector of labels to be placed at the tickpoints.
</p>
</td></tr>  
<tr><td><code id="plotbar_+3A_col">col</code></td>
<td>

<p>Color scheme to use. Default is from <code>blueviolet</code> (low) to <code>red</code> (high).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_ncolor">ncolor</code></td>
<td>

<p>The number of colors to use in the color scheme.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_drawbar">drawbar</code></td>
<td>

<p>Logical indicating if the colorbar should be drawn.
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_zline">zline</code></td>
<td>

<p>Number of lines into the margin at which the axis line will be drawn (see <code><a href="graphics.html#topic+axis">axis</a></code>).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_pltimage">pltimage</code></td>
<td>

<p>A vector of the form c(x1, x2, y1, y2) giving the coordinates of the image region as fractions of the current figure region (see <code><a href="graphics.html#topic+par">par</a></code>).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_pltbar">pltbar</code></td>
<td>

<p>A vector of the form c(x1, x2, y1, y2) giving the coordinates of the colorbar region as fractions of the current figure region (see <code><a href="graphics.html#topic+par">par</a></code>).
</p>
</td></tr>
<tr><td><code id="plotbar_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to <code>plot</code> (e.g., <code>xlab</code>, <code>ylab</code>, <code>main</code>, <code>cex</code>, <code>cex.axis</code>, <code>cex.lab</code>, etc.)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Produces a <code>plot</code> with a colorbar.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##########   EXAMPLE 1   ##########

myfun &lt;- function(x){
  2*sin(sqrt(x[,1]^2+x[,2]^2+.1))/sqrt(x[,1]^2+x[,2]^2+.1)
}
x &lt;- expand.grid(seq(-8,8,l=100),seq(-8,8,l=100))
plotbar(x[,1],x[,2],myfun(x),
        xlab=expression(italic(x)[1]),
        ylab=expression(italic(x)[2]),
        zlab=expression(hat(italic(y))))


##########   EXAMPLE 2   ##########

myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + 2*sin(sqrt(x2v^2+.1))/sqrt(x2v^2+.1)
}
x &lt;- expand.grid(x1v=seq(0,1,l=100),x2v=seq(-8,8,l=100))
plotbar(x[,1],x[,2],myfun(x$x1v,x$x2v),
         col=c("red","orange","yellow","white"),
         xlab="x1v",ylab="x2v",zlab=expression(hat(italic(y))))


##########   EXAMPLE 3   ##########

myfun &lt;- function(x1v,x2v){
  sin(3*pi*x1v) + sin(2*pi*x2v) + 3*cos(pi*(x1v-x2v))
}
x &lt;- expand.grid(x1v=seq(-1,1,l=100),x2v=seq(-1,1,l=100))
plotbar(x[,1],x[,2],myfun(x$x1v,x$x2v),
         col=c("blue","green","light green","yellow"),
         xlab="x1v",ylab="x2v",zlab=expression(hat(italic(y))))

</code></pre>

<hr>
<h2 id='plotci'>
Generic X-Y Plotting with Confidence Intervals
</h2><span id='topic+plotci'></span>

<h3>Description</h3>

<p>This is a modification to the R function <code><a href="graphics.html#topic+plot">plot</a></code> that adds confidence intervals to the plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotci(x, y, se, level=0.95, cval=NULL, col="blue",
       col.ci="cyan", alpha=0.65, add=FALSE, 
       type="l", link=function(y){y}, axes=TRUE, 
       bars=FALSE, barlty=1, barlwd=2, bw=0.2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotci_+3A_x">x</code>, <code id="plotci_+3A_y">y</code></td>
<td>

<p>The x and y coordinates of the points to plot.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_se">se</code></td>
<td>

<p>Numeric vector the same length as <code>x</code> and <code>y</code> containing the standard errors of the <code>y</code> values.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_level">level</code></td>
<td>

<p>Significance level for the confidence interval. Default forms 95% interval.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_cval">cval</code></td>
<td>

<p>Critical value for the confidence interval. Default uses <code>cval=qnorm(1-(1-level)/2)</code>.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_col">col</code></td>
<td>

<p>Color for plotting the relationship between <code>x</code> and <code>y</code>.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_col.ci">col.ci</code></td>
<td>

<p>Color for plotting the confidence interval.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_alpha">alpha</code></td>
<td>

<p>Transparency used for plotting confidence polygons. Only used when <code>bars=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_add">add</code></td>
<td>

<p>Logical indicating whether lines should be added to current plot.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_type">type</code></td>
<td>

<p>Type of plot to create (defaults to &quot;l&quot; for lines).
</p>
</td></tr>
<tr><td><code id="plotci_+3A_link">link</code></td>
<td>

<p>Link function to apply. See Details.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_axes">axes</code></td>
<td>

<p>Logical indicating if the axes should be drawn.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_bars">bars</code></td>
<td>

<p>Logical indicating if confidence bars should be plotted instead of polygons.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_barlty">barlty</code>, <code id="plotci_+3A_barlwd">barlwd</code></td>
<td>

<p>Line type and width for confidence bars. Only used when <code>bars=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_bw">bw</code></td>
<td>

<p>Positive scalar giving the width of the confidence bars. Only used when <code>bars=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plotci_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to <code>plot</code> (e.g., <code>xlab</code>, <code>ylab</code>, <code>main</code>, <code>cex</code>, <code>cex.axis</code>, <code>cex.lab</code>, etc.)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plotted confidence interval is <code>c(link(y-cval*se), link(y+cval*se))</code> where <code>link</code> is the user-specified link function and <code>cval</code> is the user-sepcified critival value, which defaults to <code>cval = qnorm(1-(1-level)/2)</code>.
</p>


<h3>Value</h3>

<p>Produces a <code>plot</code> with a colorbar.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##########   EXAMPLE   ##########

# define relatively smooth function
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# fit cubic smoothing spline
cubmod &lt;- bigspline(x,y)
newdata &lt;- data.frame(x=seq(0,1,length=20))
ypred &lt;- predict(cubmod, newdata, se.fit=TRUE)

# plot predictions with CIs in two ways
plotci(newdata$x, ypred$fit, ypred$se.fit)
plotci(newdata$x, ypred$fit, ypred$se.fit, type="p", bars=TRUE, bw=0.02)

</code></pre>

<hr>
<h2 id='predict.bigspline'>
Predicts for &quot;bigspline&quot; Objects
</h2><span id='topic+predict.bigspline'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for cubic smoothing splines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigspline'
predict(object,newdata=NULL,se.fit=FALSE,
        effect=c("all","0","lin","non"),
        design=FALSE,smoothMatrix=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bigspline_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigspline&quot;, which is output from <code><a href="#topic+bigspline">bigspline</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_newdata">newdata</code></td>
<td>

<p>Vector containing new data points for prediction. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating whether the standard errors of the fitted values should be estimated. Default is <code>se.fit=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_effect">effect</code></td>
<td>

<p>Which effect to estimate: <code>effect="all"</code> gives full <code class="reqn">\hat{y}</code>, <code>effect="0"</code> gives the intercept (constant) portion of <code class="reqn">\hat{y}</code>,  <code>effect="lin"</code> gives linear portion of <code class="reqn">\hat{y}</code>, and <code>effect="non"</code> gives nonlinear portion of <code class="reqn">\hat{y}</code>. 
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_design">design</code></td>
<td>

<p>Logical indicating whether the design matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_smoothmatrix">smoothMatrix</code></td>
<td>

<p>Logical indicating whether the smoothing matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigspline_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit cubic smoothing spline (estimated by <code><a href="#topic+bigspline">bigspline</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>If <code>se.fit=FALSE</code>, <code>design=FALSE</code>, and <code>smoothMatrix=FALSE</code>, returns vector of fitted values.
</p>
<p>Otherwise returns list with elements:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fitted values</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of fitted values (if <code>se.fit=TRUE</code>)</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>Design matrix used to create fitted values (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>Index vector such that <code>fit=X%*%object$coef[ix]</code> (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Smoothing matrix corresponding to fitted values (if <code>smoothMatrix=TRUE</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x + sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# fit cubic spline model
cubmod &lt;- bigspline(x,y)
crossprod( predict(cubmod) - myfun(x) )/10^4

# define new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
yc &lt;- predict(cubmod,newdata,se.fit=TRUE)

# plot results with 95% Bayesian confidence interval
plot(newdata$x,yc$fit,type="l")
lines(newdata$x,yc$fit+qnorm(.975)*yc$se.fit,lty=3)
lines(newdata$x,yc$fit-qnorm(.975)*yc$se.fit,lty=3)

# predict constant, linear, and nonlinear effects
yc0 &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="0")
ycl &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="lin")
ycn &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="non")
crossprod( yc$fit - (yc0$fit + ycl$fit + ycn$fit) )

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x,ycl$fit,type="l",main="Linear effect")
lines(newdata$x,ycl$fit+qnorm(.975)*ycl$se.fit,lty=3)
lines(newdata$x,ycl$fit-qnorm(.975)*ycl$se.fit,lty=3)
plot(newdata$x,ycn$fit,type="l",main="Nonlinear effect")
lines(newdata$x,ycn$fit+qnorm(.975)*ycn$se.fit,lty=3)
lines(newdata$x,ycn$fit-qnorm(.975)*ycn$se.fit,lty=3)


##########   EXAMPLE 2   ##########

# define (same) univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x + sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# fit a different cubic spline model
cubamod &lt;- bigspline(x,y,type="cub0")
crossprod( predict(cubamod) - myfun(x) )/10^4

# define (same) new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
ya &lt;- predict(cubamod,newdata,se.fit=TRUE)

# plot results with 95% Bayesian confidence interval
plot(newdata$x,ya$fit,type="l")
lines(newdata$x,ya$fit+qnorm(.975)*ya$se.fit,lty=3)
lines(newdata$x,ya$fit-qnorm(.975)*ya$se.fit,lty=3)

# predict constant, linear, and nonlinear effects
ya0 &lt;- predict(cubamod,newdata,se.fit=TRUE,effect="0")
yal &lt;- predict(cubamod,newdata,se.fit=TRUE,effect="lin")
yan &lt;- predict(cubamod,newdata,se.fit=TRUE,effect="non")
crossprod( ya$fit - (ya0$fit + yal$fit + yan$fit) )

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x,yal$fit,type="l",main="Linear effect")
lines(newdata$x,yal$fit+qnorm(.975)*yal$se.fit,lty=3)
lines(newdata$x,yal$fit-qnorm(.975)*yal$se.fit,lty=3)
plot(newdata$x,yan$fit,type="l",main="Nonlinear effect")
lines(newdata$x,yan$fit+qnorm(.975)*yan$se.fit,lty=3)
lines(newdata$x,yan$fit-qnorm(.975)*yan$se.fit,lty=3)

</code></pre>

<hr>
<h2 id='predict.bigssa'>
Predicts for &quot;bigssa&quot; Objects
</h2><span id='topic+predict.bigssa'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for smoothing spline anova models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigssa'
predict(object,newdata=NULL,se.fit=FALSE,include=object$tnames,
        effect=c("all","0","lin","non"),includeint=FALSE,
        design=FALSE,smoothMatrix=FALSE,intercept=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bigssa_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigssa&quot;, which is output from <code><a href="#topic+bigssa">bigssa</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_newdata">newdata</code></td>
<td>

<p>Data frame or list containing the new data points for prediction. Variable names must match those used in the <code>formula</code> input of <code><a href="#topic+bigssa">bigssa</a></code>. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating whether the standard errors of the fitted values should be estimated. Default is <code>se.fit=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_include">include</code></td>
<td>

<p>Which terms to include in the estimate. You can get fitted values for any combination of terms in the <code>tnames</code> element of an &quot;bigssa&quot; object.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_effect">effect</code></td>
<td>

<p>Which effect to estimate: <code>effect="all"</code> gives <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, <code>effect="lin"</code> gives linear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, and <code>effect="non"</code> gives nonlinear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>. Use <code>effect="0"</code> to return the intercept.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_includeint">includeint</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. If <code>include=object$tnames</code> and <code>effect="all"</code> (default), then this input is ignored and the intercept is automatically included in the prediction.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_design">design</code></td>
<td>

<p>Logical indicating whether the design matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_smoothmatrix">smoothMatrix</code></td>
<td>

<p>Logical indicating whether the smoothing matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_intercept">intercept</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. When used, this input overrides the <code>includeint</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigssa_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit smoothing spline anova (estimated by <code><a href="#topic+bigssa">bigssa</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>If <code>se.fit=FALSE</code>, <code>design=FALSE</code>, and <code>smoothMatrix=FALSE</code>, returns vector of fitted values.
</p>
<p>Otherwise returns list with elements:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fitted values</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of fitted values (if <code>se.fit=TRUE</code>)</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>Design matrix used to create fitted values (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>Index vector such that <code>fit=X%*%object$modelspec$coef[ix]</code> (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Smoothing matrix corresponding to fitted values (if <code>smoothMatrix=TRUE</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x + sin(2*pi*x) }
x &lt;- runif(500)
y &lt;- myfun(x) + rnorm(500)

# fit cubic spline model
cubmod &lt;- bigssa(y~x,type="cub",nknots=30)
crossprod( predict(cubmod) - myfun(x) )/500

# define new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
yc &lt;- predict(cubmod,newdata,se.fit=TRUE)

# plot results with 95% Bayesian confidence interval
plot(newdata$x,yc$fit,type="l")
lines(newdata$x,yc$fit+qnorm(.975)*yc$se.fit,lty=3)
lines(newdata$x,yc$fit-qnorm(.975)*yc$se.fit,lty=3)

# predict constant, linear, and nonlinear effects
yc0 &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="0")
ycl &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="lin")
ycn &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="non")
crossprod( yc$fit - (yc0$fit + ycl$fit + ycn$fit) )

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x,ycl$fit,type="l",main="Linear effect")
lines(newdata$x,ycl$fit+qnorm(.975)*ycl$se.fit,lty=3)
lines(newdata$x,ycl$fit-qnorm(.975)*ycl$se.fit,lty=3)
plot(newdata$x,ycn$fit,type="l",main="Nonlinear effect")
lines(newdata$x,ycn$fit+qnorm(.975)*ycn$se.fit,lty=3)
lines(newdata$x,ycn$fit-qnorm(.975)*ycn$se.fit,lty=3)
         
         
##########   EXAMPLE 2   ##########

# define bivariate function and data
set.seed(773)
myfun&lt;-function(x){
  2 + x[,1]/10 - x[,2]/5 + 2*sin(sqrt(x[,1]^2+x[,2]^2+.1))/sqrt(x[,1]^2+x[,2]^2+.1)
}
x1v &lt;- runif(500)*16-8
x2v &lt;- runif(500)*16-8
y &lt;- myfun(cbind(x1v,x2v)) + rnorm(500)

# tensor product cubic splines with 50 knots
cubmod &lt;- bigssa(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=50)
crossprod( predict(cubmod) - myfun(cbind(x1v,x2v)) )/500

# define new data for prediction
xnew &lt;- as.matrix(expand.grid(seq(-8,8,l=50),seq(-8,8,l=50)))
newdata &lt;- list(x1v=xnew[,1],x2v=xnew[,2])

# get fitted values for new data
yp &lt;- predict(cubmod,newdata)

# plot results
imagebar(seq(-8,8,l=50),seq(-8,8,l=50),matrix(yp,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))))

# predict linear and nonlinear effects for x1v
newdata &lt;- list(x1v=seq(-8,8,length.out=100))
yl &lt;- predict(cubmod,newdata,include="x1v",effect="lin",se.fit=TRUE)
yn &lt;- predict(cubmod,newdata,include="x1v",effect="non",se.fit=TRUE)

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x1v,yl$fit,type="l",main="Linear effect")
lines(newdata$x1v,yl$fit+qnorm(.975)*yl$se.fit,lty=3)
lines(newdata$x1v,yl$fit-qnorm(.975)*yl$se.fit,lty=3)
plot(newdata$x1v,yn$fit,type="l",main="Nonlinear effect",ylim=c(-.3,.4))
lines(newdata$x1v,yn$fit+qnorm(.975)*yn$se.fit,lty=3)
lines(newdata$x1v,yn$fit-qnorm(.975)*yn$se.fit,lty=3)

</code></pre>

<hr>
<h2 id='predict.bigssg'>
Predicts for &quot;bigssg&quot; Objects
</h2><span id='topic+predict.bigssg'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for generalized smoothing spline anova models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigssg'
predict(object,newdata=NULL,se.lp=FALSE,include=object$tnames,
        effect=c("all","0","lin","non"),includeint=FALSE,
        design=FALSE,smoothMatrix=FALSE,intercept=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bigssg_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigssg&quot;, which is output from <code><a href="#topic+bigssg">bigssg</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_newdata">newdata</code></td>
<td>

<p>Data frame or list containing the new data points for prediction. Variable names must match those used in the <code>formula</code> input of <code><a href="#topic+bigssg">bigssg</a></code>. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_se.lp">se.lp</code></td>
<td>

<p>Logical indicating if the standard errors of the linear predictors (<code class="reqn">\eta</code>) should be estimated. Default is <code>se.lp=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_include">include</code></td>
<td>

<p>Which terms to include in the estimate. You can get fitted values for any combination of terms in the <code>tnames</code> element of an &quot;bigssg&quot; object.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_effect">effect</code></td>
<td>

<p>Which effect to estimate: <code>effect="all"</code> gives <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, <code>effect="lin"</code> gives linear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, and <code>effect="non"</code> gives nonlinear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>. Use <code>effect="0"</code> to return the intercept.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_includeint">includeint</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. If <code>include=object$tnames</code> and <code>effect="all"</code> (default), then this input is ignored and the intercept is automatically included in the prediction.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_design">design</code></td>
<td>

<p>Logical indicating whether the design matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_smoothmatrix">smoothMatrix</code></td>
<td>

<p>Logical indicating whether the smoothing matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_intercept">intercept</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. When used, this input overrides the <code>includeint</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigssg_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit generalized smoothing spline anova (estimated by <code><a href="#topic+bigssg">bigssg</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>Returns list with elements:
</p>
<table>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values (on data scale)</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>Vector of fitted values (on link scale)</p>
</td></tr>
<tr><td><code>se.lp</code></td>
<td>
<p>Vector of standard errors of linear predictors (if <code>se.lp=TRUE</code>)</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>Design matrix used to create linear predictors (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>Index vector such that <code>linear.predictors=X%*%object$modelspec$coef[ix]</code> (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Smoothing matrix corresponding to fitted values (if <code>smoothMatrix=TRUE</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Xiang, D. (2001). Cross-validating non-Gaussian data: Generalized approximate cross-validation revisited. <em>Journal of Computational and Graphical Statistics, 10</em>, 581-591.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(1)
myfun &lt;- function(x){ sin(2*pi*x) }
ndpts &lt;- 1000
x &lt;- runif(ndpts)

# negative binomial response (unknown dispersion)
set.seed(773)
lp &lt;- myfun(x)
mu &lt;- exp(lp)
y &lt;- rnbinom(n=ndpts,size=2,mu=mu)

# fit cubic spline model
cubmod &lt;- bigssg(y~x,family="negbin",type="cub",nknots=20)
1/cubmod$dispersion   ## dispersion = 1/size
crossprod( lp - cubmod$linear.predictor )/length(lp)

# define new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
yc &lt;- predict(cubmod,newdata,se.lp=TRUE)

# plot results with 95% Bayesian confidence interval (link scale)
plot(newdata$x,yc$linear.predictor,type="l")
lines(newdata$x,yc$linear.predictor+qnorm(.975)*yc$se.lp,lty=3)
lines(newdata$x,yc$linear.predictor-qnorm(.975)*yc$se.lp,lty=3)

# plot results with 95% Bayesian confidence interval (data scale)
plot(newdata$x,yc$fitted,type="l")
lines(newdata$x,exp(yc$linear.predictor+qnorm(.975)*yc$se.lp),lty=3)
lines(newdata$x,exp(yc$linear.predictor-qnorm(.975)*yc$se.lp),lty=3)

# predict constant, linear, and nonlinear effects
yc0 &lt;- predict(cubmod,newdata,se.lp=TRUE,effect="0")
ycl &lt;- predict(cubmod,newdata,se.lp=TRUE,effect="lin")
ycn &lt;- predict(cubmod,newdata,se.lp=TRUE,effect="non")
crossprod( yc$linear - (yc0$linear + ycl$linear + ycn$linear) )

# plot results with 95% Bayesian confidence intervals (link scale)
par(mfrow=c(1,2))
plot(newdata$x,ycl$linear,type="l",main="Linear effect")
lines(newdata$x,ycl$linear+qnorm(.975)*ycl$se.lp,lty=3)
lines(newdata$x,ycl$linear-qnorm(.975)*ycl$se.lp,lty=3)
plot(newdata$x,ycn$linear,type="l",main="Nonlinear effect")
lines(newdata$x,ycn$linear+qnorm(.975)*ycn$se.lp,lty=3)
lines(newdata$x,ycn$linear-qnorm(.975)*ycn$se.lp,lty=3)
         
# plot results with 95% Bayesian confidence intervals (data scale)
par(mfrow=c(1,2))
plot(newdata$x,ycl$fitted,type="l",main="Linear effect")
lines(newdata$x,exp(ycl$linear+qnorm(.975)*ycl$se.lp),lty=3)
lines(newdata$x,exp(ycl$linear-qnorm(.975)*ycl$se.lp),lty=3)
plot(newdata$x,ycn$fitted,type="l",main="Nonlinear effect")
lines(newdata$x,exp(ycn$linear+qnorm(.975)*ycn$se.lp),lty=3)
lines(newdata$x,exp(ycn$linear-qnorm(.975)*ycn$se.lp),lty=3)

         
         
##########   EXAMPLE 2   ##########

# define bivariate function and data
set.seed(1)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
ndpts &lt;- 1000
x1v &lt;- runif(ndpts)
x2v &lt;- runif(ndpts)

# binomial response (with weights)
set.seed(773)
lp &lt;- myfun(x1v,x2v)
p &lt;- 1/(1+exp(-lp))
w &lt;- sample(c(10,20,30,40,50),length(p),replace=TRUE)
y &lt;- rbinom(n=ndpts,size=w,p=p)/w   ## y is proportion correct
cubmod &lt;- bigssg(y~x1v*x2v,family="binomial",type=list(x1v="cub",x2v="cub"),nknots=100,weights=w)
crossprod( lp - cubmod$linear.predictor )/length(lp)

# define new data for prediction
xnew &lt;- as.matrix(expand.grid(seq(0,1,length=50),seq(0,1,length=50)))
newdata &lt;- list(x1v=xnew[,1],x2v=xnew[,2])

# get fitted values for new data
yp &lt;- predict(cubmod,newdata)

# plot linear predictor and fitted values
par(mfrow=c(2,2))
imagebar(seq(0,1,l=50),seq(0,1,l=50),matrix(myfun(newdata$x1v,newdata$x2v),50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))),zlim=c(-4.5,1.5),main="True Linear Predictor")
imagebar(seq(0,1,l=50),seq(0,1,l=50),matrix(yp$linear.predictor,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))),zlim=c(-4.5,1.5),main="Estimated Linear Predictor")
newprob &lt;- 1/(1+exp(-myfun(newdata$x1v,newdata$x2v)))
imagebar(seq(0,1,l=50),seq(0,1,l=50),matrix(newprob,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))),zlim=c(0,0.8),main="True Probabilities")
imagebar(seq(0,1,l=50),seq(0,1,l=50),matrix(yp$fitted.values,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))),zlim=c(0,0.8),main="Estimated Probabilities")         

# predict linear and nonlinear effects for x1v (link scale)
newdata &lt;- list(x1v=seq(0,1,length.out=100))
yl &lt;- predict(cubmod,newdata,include="x1v",effect="lin",se.lp=TRUE)
yn &lt;- predict(cubmod,newdata,include="x1v",effect="non",se.lp=TRUE)

# plot results with 95% Bayesian confidence intervals (link scale)
par(mfrow=c(1,2))
plot(newdata$x1v,yl$linear,type="l",main="Linear effect")
lines(newdata$x1v,yl$linear+qnorm(.975)*yl$se.lp,lty=3)
lines(newdata$x1v,yl$linear-qnorm(.975)*yl$se.lp,lty=3)
plot(newdata$x1v,yn$linear,type="l",main="Nonlinear effect")
lines(newdata$x1v,yn$linear+qnorm(.975)*yn$se.lp,lty=3)
lines(newdata$x1v,yn$linear-qnorm(.975)*yn$se.lp,lty=3)

</code></pre>

<hr>
<h2 id='predict.bigssp'>
Predicts for &quot;bigssp&quot; Objects
</h2><span id='topic+predict.bigssp'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for smoothing splines with parametric effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigssp'
predict(object,newdata=NULL,se.fit=FALSE,include=object$tnames,
        effect=c("all","0","lin","non"),includeint=FALSE,
        design=FALSE,smoothMatrix=FALSE,intercept=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bigssp_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigssp&quot;, which is output from <code><a href="#topic+bigssp">bigssp</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_newdata">newdata</code></td>
<td>

<p>Data frame or list containing the new data points for prediction. Variable names must match those used in the <code>formula</code> input of <code><a href="#topic+bigssp">bigssp</a></code>. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating whether the standard errors of the fitted values should be estimated. Default is <code>se.fit=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_include">include</code></td>
<td>

<p>Which terms to include in the estimate. You can get fitted values for any combination of terms in the <code>tnames</code> element of an &quot;bigssp&quot; object.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_effect">effect</code></td>
<td>

<p>Which effect to estimate: <code>effect="all"</code> gives <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, <code>effect="lin"</code> gives linear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>, and <code>effect="non"</code> gives nonlinear portion of <code class="reqn">\hat{y}</code> for given terms in <code>include</code>. Use <code>effect="0"</code> to return the intercept.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_includeint">includeint</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. If <code>include=object$tnames</code> and <code>effect="all"</code> (default), then this input is ignored and the intercept is automatically included in the prediction.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_design">design</code></td>
<td>

<p>Logical indicating whether the design matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_smoothmatrix">smoothMatrix</code></td>
<td>

<p>Logical indicating whether the smoothing matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigssp_+3A_intercept">intercept</code></td>
<td>

<p>Logical indicating whether the intercept should be included in the prediction. When used, this input overrides the <code>includeint</code> input.
</p>
</td></tr>  
<tr><td><code id="predict.bigssp_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit smoothing spline with parametric effects (estimated by <code><a href="#topic+bigssp">bigssp</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>If <code>se.fit=FALSE</code>, <code>design=FALSE</code>, and <code>smoothMatrix=FALSE</code>, returns vector of fitted values.
</p>
<p>Otherwise returns list with elements:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fitted values</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of fitted values (if <code>se.fit=TRUE</code>)</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>Design matrix used to create fitted values (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>Index vector such that <code>fit=X%*%object$modelspec$coef[ix]</code> (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Smoothing matrix corresponding to fitted values (if <code>smoothMatrix=TRUE</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Gu, C. and Wahba, G. (1991). Minimizing GCV/GML scores with multiple smoothing parameters via the Newton method. <em>SIAM Journal on Scientific and Statistical Computing, 12</em>, 383-398.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-effects models with large samples. <em>Statistics and Computing, 26</em>, 1319-1336.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x + sin(2*pi*x) }
x &lt;- runif(500)
y &lt;- myfun(x) + rnorm(500)

# fit cubic spline model
cubmod &lt;- bigssp(y~x,type="cub",nknots=30)
crossprod( predict(cubmod) - myfun(x) )/500

# define new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
yc &lt;- predict(cubmod,newdata,se.fit=TRUE)

# plot results with 95% Bayesian confidence interval
plot(newdata$x,yc$fit,type="l")
lines(newdata$x,yc$fit+qnorm(.975)*yc$se.fit,lty=3)
lines(newdata$x,yc$fit-qnorm(.975)*yc$se.fit,lty=3)

# predict constant, linear, and nonlinear effects
yc0 &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="0")
ycl &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="lin")
ycn &lt;- predict(cubmod,newdata,se.fit=TRUE,effect="non")
sum( yc$fit - (yc0$fit + ycl$fit + ycn$fit) )

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x,ycl$fit,type="l",main="Linear effect")
lines(newdata$x,ycl$fit+qnorm(.975)*ycl$se.fit,lty=3)
lines(newdata$x,ycl$fit-qnorm(.975)*ycl$se.fit,lty=3)
plot(newdata$x,ycn$fit,type="l",main="Nonlinear effect")
lines(newdata$x,ycn$fit+qnorm(.975)*ycn$se.fit,lty=3)
lines(newdata$x,ycn$fit-qnorm(.975)*ycn$se.fit,lty=3)


##########   EXAMPLE 2   ##########

# define bivariate function and data
set.seed(773)
myfun &lt;- function(x){
  2 + x[,1]/10 - x[,2]/5 + 2*sin(sqrt(x[,1]^2+x[,2]^2+.1))/sqrt(x[,1]^2+x[,2]^2+.1)
}
x &lt;- cbind(runif(500),runif(500))*16 - 8
y &lt;- myfun(x)+rnorm(500)

# bidimensional thin-plate spline with 50 knots
tpsmod &lt;- bigssp(y~x,type="tps",nknots=50)
crossprod( predict(tpsmod) - myfun(x) )/500

# define new data for prediction
xnew &lt;- as.matrix(expand.grid(seq(-8,8,length=50),seq(-8,8,length=50)))
newdata &lt;- list(x=xnew)

# get fitted values for new data
yp &lt;- predict(tpsmod,newdata)

# plot results
imagebar(seq(-8,8,l=50),seq(-8,8,l=50),matrix(yp,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))))

# predict linear and nonlinear effects
yl &lt;- predict(tpsmod,newdata,effect="lin")
yn &lt;- predict(tpsmod,newdata,effect="non")

# plot results
par(mfrow=c(1,2))
imagebar(seq(-8,8,l=50),seq(-8,8,l=50),matrix(yl,50,50),
         main="Linear effect",xlab=expression(italic(x)[1]),
         ylab=expression(italic(x)[2]),zlab=expression(hat(italic(y))))
imagebar(seq(-8,8,l=50),seq(-8,8,l=50),matrix(yn,50,50),
         main="Nonlinear effect",xlab=expression(italic(x)[1]),
         ylab=expression(italic(x)[2]),zlab=expression(hat(italic(y))))
         
</code></pre>

<hr>
<h2 id='predict.bigtps'>
Predicts for &quot;bigtps&quot; Objects
</h2><span id='topic+predict.bigtps'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for thin-plate splines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigtps'
predict(object,newdata=NULL,se.fit=FALSE,
        effect=c("all","0","lin","non"),
        design=FALSE,smoothMatrix=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bigtps_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigtps&quot;, which is output from <code><a href="#topic+bigtps">bigtps</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_newdata">newdata</code></td>
<td>

<p>Vector or matrix containing new data points for prediction. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating whether the standard errors of the fitted values should be estimated. Default is <code>se.fit=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_effect">effect</code></td>
<td>

<p>Which effect to estimate: <code>effect="all"</code> gives full <code class="reqn">\hat{y}</code>, <code>effect="0"</code> gives the intercept (constant) portion of <code class="reqn">\hat{y}</code>,  <code>effect="lin"</code> gives linear portion of <code class="reqn">\hat{y}</code>, and <code>effect="non"</code> gives nonlinear portion of <code class="reqn">\hat{y}</code>. 
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_design">design</code></td>
<td>

<p>Logical indicating whether the design matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_smoothmatrix">smoothMatrix</code></td>
<td>

<p>Logical indicating whether the smoothing matrix should be returned.
</p>
</td></tr>
<tr><td><code id="predict.bigtps_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit thin-plate spline (estimated by <code><a href="#topic+bigtps">bigtps</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>If <code>se.fit=FALSE</code>, <code>design=FALSE</code>, and <code>smoothMatrix=FALSE</code>, returns vector of fitted values.
</p>
<p>Otherwise returns list with elements:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fitted values</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of fitted values (if <code>se.fit=TRUE</code>)</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>Design matrix used to create fitted values (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>Index vector such that <code>fit=X%*%object$coef[ix]</code> (if <code>design=TRUE</code>)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Smoothing matrix corresponding to fitted values (if <code>smoothMatrix=TRUE</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>
<p>Helwig, N. E. and Ma, P. (2016). Smoothing spline ANOVA for super-large samples: Scalable computation via rounding parameters. <em>Statistics and Its Interface, 9</em>, 433-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x + sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# fit thin-plate spline (default 1 dim: 30 knots)
tpsmod &lt;- bigtps(x,y)
crossprod( predict(tpsmod) - myfun(x) )/10^4

# define new data for prediction
newdata &lt;- data.frame(x=seq(0,1,length.out=100))

# get fitted values and standard errors for new data
yc &lt;- predict(tpsmod,newdata,se.fit=TRUE)

# plot results with 95% Bayesian confidence interval
plot(newdata$x,yc$fit,type="l")
lines(newdata$x,yc$fit+qnorm(.975)*yc$se.fit,lty=3)
lines(newdata$x,yc$fit-qnorm(.975)*yc$se.fit,lty=3)

# predict constant, linear, and nonlinear effects
yc0 &lt;- predict(tpsmod,newdata,se.fit=TRUE,effect="0")
ycl &lt;- predict(tpsmod,newdata,se.fit=TRUE,effect="lin")
ycn &lt;- predict(tpsmod,newdata,se.fit=TRUE,effect="non")
crossprod( yc$fit - (yc0$fit + ycl$fit + ycn$fit) )

# plot results with 95% Bayesian confidence intervals
par(mfrow=c(1,2))
plot(newdata$x,ycl$fit,type="l",main="Linear effect")
lines(newdata$x,ycl$fit+qnorm(.975)*ycl$se.fit,lty=3)
lines(newdata$x,ycl$fit-qnorm(.975)*ycl$se.fit,lty=3)
plot(newdata$x,ycn$fit,type="l",main="Nonlinear effect")
lines(newdata$x,ycn$fit+qnorm(.975)*ycn$se.fit,lty=3)
lines(newdata$x,ycn$fit-qnorm(.975)*ycn$se.fit,lty=3)


##########   EXAMPLE 2   ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x &lt;- cbind(runif(10^4),runif(10^4))
y &lt;- myfun(x[,1],x[,2]) + rnorm(10^4)

# fit thin-plate spline (default 2 dim: 100 knots)
tpsmod &lt;- bigtps(x,y)

# define new data
newdata &lt;- as.matrix(expand.grid(seq(0,1,length=50),seq(0,1,length=50)))

# get fitted values for new data
yp &lt;- predict(tpsmod,newdata)

# plot results
imagebar(seq(0,1,length=50),seq(0,1,length=50),matrix(yp,50,50),
         xlab=expression(italic(x)[1]),ylab=expression(italic(x)[2]),
         zlab=expression(hat(italic(y))))

# predict linear and nonlinear effects
yl &lt;- predict(tpsmod,newdata,effect="lin")
yn &lt;- predict(tpsmod,newdata,effect="non")

# plot results
par(mfrow=c(1,2))
imagebar(seq(0,1,length=50),seq(0,1,length=50),matrix(yl,50,50),
         main="Linear effect",xlab=expression(italic(x)[1]),
         ylab=expression(italic(x)[2]),zlab=expression(hat(italic(y))))
imagebar(seq(0,1,length=50),seq(0,1,length=50),matrix(yn,50,50),
         main="Nonlinear effect",xlab=expression(italic(x)[1]),
         ylab=expression(italic(x)[2]),zlab=expression(hat(italic(y))))

</code></pre>

<hr>
<h2 id='predict.ordspline'>
Predicts for &quot;ordspline&quot; Objects
</h2><span id='topic+predict.ordspline'></span>

<h3>Description</h3>

<p>Get fitted values and standard error estimates for ordinal smoothing splines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ordspline'
predict(object,newdata=NULL,se.fit=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ordspline_+3A_object">object</code></td>
<td>

<p>Object of class &quot;ordspline&quot;, which is output from <code><a href="#topic+ordspline">ordspline</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.ordspline_+3A_newdata">newdata</code></td>
<td>

<p>Vector containing new data points for prediction. See Details and Example. Default of <code>newdata=NULL</code> uses original data in <code>object</code> input.
</p>
</td></tr>
<tr><td><code id="predict.ordspline_+3A_se.fit">se.fit</code></td>
<td>

<p>Logical indicating whether the standard errors of the fitted values should be estimated. Default is <code>se.fit=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predict.ordspline_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the coefficient and smoothing parameter estimates from a fit ordinal smoothing spline (estimated by <code><a href="#topic+ordspline">ordspline</a></code>) to predict for new data.
</p>


<h3>Value</h3>

<p>If <code>se.fit=FALSE</code>, returns vector of fitted values.
</p>
<p>Otherwise returns list with elements:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fitted values</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Vector of standard errors of fitted values</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE   ##########

# define univariate function and data
set.seed(773)
myfun &lt;- function(x){ 2 + x/2 + sin(x) }
x &lt;- sample(1:20, size=500, replace=TRUE)
y &lt;- myfun(x) + rnorm(500)

# fit ordinal spline model
ordmod &lt;- ordspline(x, y)
monmod &lt;- ordspline(x, y, monotone=TRUE)
crossprod( predict(ordmod) - myfun(x) ) / 500
crossprod( predict(monmod) - myfun(x) ) / 500

# plot truth and predictions
ordfit &lt;- predict(ordmod, 1:20, se.fit=TRUE)
monfit &lt;- predict(monmod, 1:20, se.fit=TRUE)
plotci(1:20, ordfit$fit, ordfit$se.fit, ylab="f(x)")
plotci(1:20, monfit$fit, monfit$se.fit, col="red", col.ci="pink", add=TRUE)
points(1:20, myfun(1:20))

</code></pre>

<hr>
<h2 id='print'>
Prints Fit Information for bigsplines Model
</h2><span id='topic+print.bigspline'></span><span id='topic+print.bigssa'></span><span id='topic+print.bigssg'></span><span id='topic+print.bigssp'></span><span id='topic+print.bigtps'></span><span id='topic+print.ordspline'></span><span id='topic+print.summary.bigspline'></span><span id='topic+print.summary.bigssa'></span><span id='topic+print.summary.bigssg'></span><span id='topic+print.summary.bigssp'></span><span id='topic+print.summary.bigtps'></span>

<h3>Description</h3>

<p>This function prints basic model fit information for a fit <code>bigsplines</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigspline'
print(x,...)
## S3 method for class 'bigssa'
print(x,...)
## S3 method for class 'bigssg'
print(x,...)
## S3 method for class 'bigssp'
print(x,...)
## S3 method for class 'bigtps'
print(x,...)
## S3 method for class 'ordspline'
print(x,...)
## S3 method for class 'summary.bigspline'
print(x,digits=4,...)
## S3 method for class 'summary.bigssa'
print(x,digits=4,...)
## S3 method for class 'summary.bigssg'
print(x,digits=4,...)
## S3 method for class 'summary.bigssp'
print(x,digits=4,...)
## S3 method for class 'summary.bigtps'
print(x,digits=4,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_+3A_x">x</code></td>
<td>

<p>Object of class &quot;bigspline&quot; (output from <code><a href="#topic+bigspline">bigspline</a></code>), class &quot;summary.bigspline&quot; (output from <code><a href="#topic+summary.bigspline">summary.bigspline</a></code>), class &quot;bigssa&quot; (output from <code><a href="#topic+bigssa">bigssa</a></code>), class &quot;summary.bigssa&quot; (output from <code><a href="#topic+summary.bigssa">summary.bigssa</a></code>), class &quot;bigssg&quot; (output from <code><a href="#topic+bigssg">bigssg</a></code>), class &quot;summary.bigssg&quot; (output from <code><a href="#topic+summary.bigssg">summary.bigssg</a></code>), class &quot;bigssp&quot; (output from <code><a href="#topic+bigssp">bigssp</a></code>), class &quot;summary.bigssp&quot; (output from <code><a href="#topic+summary.bigssp">summary.bigssp</a></code>), class &quot;bigtps&quot; (output from <code><a href="#topic+bigtps">bigtps</a></code>), class &quot;summary.bigtps&quot; (output from <code><a href="#topic+summary.bigtps">summary.bigtps</a></code>), or class &quot;ordspline&quot; (output from <code><a href="#topic+ordspline">ordspline</a></code>).
</p>
</td></tr>
<tr><td><code id="print_+3A_digits">digits</code></td>
<td>

<p>Number of decimal places to print.
</p>
</td></tr> 
<tr><td><code id="print_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bigspline">bigspline</a></code>, <code><a href="#topic+bigssa">bigssa</a></code>, <code><a href="#topic+bigssg">bigssg</a></code>, <code><a href="#topic+bigssp">bigssp</a></code>, <code><a href="#topic+bigtps">bigtps</a></code>, and <code><a href="#topic+ordspline">ordspline</a></code> for more details.
</p>


<h3>Value</h3>

<p>&quot;bigspline&quot; objects: prints Spline Type, Fit Statistic information, and Smoothing Parameter.
</p>
<p>&quot;summary.bigspline&quot; objects: prints Spline Type, five number summary of Residuals, Error Standard Deviation Estimate, Fit Statistics, and Smoothing Parameter.
</p>
<p>&quot;bigssa&quot; objects: prints Spline Types, Fit Statistic information, and Algorithm Convergence status.
</p>
<p>&quot;summary.bigssa&quot; objects: prints the formula Call, five number summary of Residuals, Error Standard Deviation Estimate, Fit Statistics, and Smoothing Parameters.
</p>
<p>&quot;bigssg&quot; objects: prints Family, Spline Types, Fit Statistic information, and Algorithm Convergence status.
</p>
<p>&quot;summary.bigssg&quot; objects: prints the Family, formula Call, five number summary of Residuals, Dispersion Estimate, Fit Statistics, and Smoothing Parameters (with selection criterion).
</p>
<p>&quot;bigssp&quot; objects: prints Predictor Types, Fit Statistic information, and Algorithm Convergence status.
</p>
<p>&quot;summary.bigssp&quot; objects: prints formula Call, five number summary of Residuals, Error Standard Deviation Estimate, Fit Statistics, and Smoothing Parameters.
</p>
<p>&quot;bigtps&quot; objects: prints Spline Type, Fit Statistic information, and Smoothing Parameter.
</p>
<p>&quot;summary.bigtps&quot; objects: prints Spline Type, five number summary of Residuals, Error Standard Deviation Estimate, Fit Statistics, and Smoothing Parameter.
</p>
<p>&quot;ordspline&quot; objects: prints Monotonic, Fit Statistic information, and Smoothing Parameter.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### see examples for bigspline, bigssa, bigssg, bigssp, bigtps, and ordspline

</code></pre>

<hr>
<h2 id='ssBasis'>
Smoothing Spline Basis for Polynomial Splines
</h2><span id='topic+ssBasis'></span>

<h3>Description</h3>

<p>Generate the smoothing spline basis matrix for a polynomial spline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssBasis(x, knots, m=2, d=0, xmin=min(x), xmax=max(x), periodic=FALSE, intercept=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssBasis_+3A_x">x</code></td>
<td>

<p>Predictor variable.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_knots">knots</code></td>
<td>

<p>Spline knots.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_m">m</code></td>
<td>

<p>Penalty order. 'm=1' for linear smoothing spline, 'm=2' for cubic, and 'm=3' for quintic.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_d">d</code></td>
<td>

<p>Derivative order. 'd=0' for smoothing spline basis, 'd=1' for 1st derivative of basis, and 'd=2' for 2nd derivative of basis.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_xmin">xmin</code></td>
<td>

<p>Minimum value of 'x'.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_xmax">xmax</code></td>
<td>

<p>Maximum value of 'x'.
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_periodic">periodic</code></td>
<td>

<p>If <code>TRUE</code>, the smoothing spline basis is periodic w.r.t. the interval [<code>xmin</code>, <code>xmax</code>].
</p>
</td></tr>
<tr><td><code id="ssBasis_+3A_intercept">intercept</code></td>
<td>

<p>If <code>TRUE</code>, the first column of the basis will be a column of ones.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>X</code></td>
<td>
<p>Spline Basis.</p>
</td></tr>
<tr><td><code>knots</code></td>
<td>
<p>Spline knots.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>Penalty order.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>Derivative order.</p>
</td></tr>
<tr><td><code>xlim</code></td>
<td>
<p>Inputs <code>xmin</code> and <code>xmax</code>.</p>
</td></tr>
<tr><td><code>periodic</code></td>
<td>
<p>Same as input.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Same as input.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Inputs <code>x</code> and <code>knots</code> should be within the interval [<code>xmin</code>, <code>xmax</code>].
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, C. (2013). <em>Smoothing spline ANOVA models, 2nd edition</em>. New York: Springer.
</p>
<p>Helwig, N. E. (2013). <em>Fast and stable smoothing spline analysis of variance models for large samples with applications to electroencephalography data analysis</em>. Unpublished doctoral dissertation. University of Illinois at Urbana-Champaign.
</p>
<p>Helwig, N. E. (2017). <a href="http://dx.doi.org/10.3389/fams.2017.00015">Regression with ordered predictors via ordinal smoothing splines</a>. Frontiers in Applied Mathematics and Statistics, 3(15), 1-13.
</p>
<p>Helwig, N. E. and Ma, P. (2015). Fast and stable multiple smoothing parameter selection in smoothing spline analysis of variance models with large samples. <em>Journal of Computational and Graphical Statistics, 24</em>, 715-732.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##########   EXAMPLE   ##########

# define function and its derivatives
n &lt;- 500
x &lt;- seq(0, 1, length.out=n)
knots &lt;- seq(0, 1, length=20)
y &lt;- sin(4 * pi * x)
d1y &lt;- 4 * pi * cos(4 * pi * x)
d2y &lt;- - (4 * pi)^2 * sin(4 * pi * x)

# linear smoothing spline
linmat0 &lt;- ssBasis(x, knots, m=1)
lincoef &lt;- pinvsm(crossprod(linmat0$X)) %*% crossprod(linmat0$X, y)
linyhat &lt;- linmat0$X %*% lincoef
linmat1 &lt;- ssBasis(x, knots, m=1, d=1)
linyd1 &lt;- linmat1$X %*% lincoef

# plot linear smoothing spline results
par(mfrow=c(1,2))
plot(x, y, type="l", main="Function")
lines(x, linyhat, lty=2, col="red")
plot(x, d1y, type="l", main="First Derivative")
lines(x, linyd1, lty=2, col="red")

# cubic smoothing spline
cubmat0 &lt;- ssBasis(x, knots)
cubcoef &lt;- pinvsm(crossprod(cubmat0$X)) %*% crossprod(cubmat0$X, y)
cubyhat &lt;- cubmat0$X %*% cubcoef
cubmat1 &lt;- ssBasis(x, knots, d=1)
cubyd1 &lt;- cubmat1$X %*% cubcoef
cubmat2 &lt;- ssBasis(x, knots, d=2)
cubyd2 &lt;- cubmat2$X %*% cubcoef

# plot cubic smoothing spline results
par(mfrow=c(1,3))
plot(x, y, type="l", main="Function")
lines(x, cubyhat, lty=2, col="red")
plot(x, d1y, type="l", main="First Derivative")
lines(x, cubyd1, lty=2, col="red")
plot(x, d2y, type="l", main="Second Derivative")
lines(x, cubyd2, lty=2, col="red")

# quintic smoothing spline
quimat0 &lt;- ssBasis(x, knots, m=3)
quicoef &lt;- pinvsm(crossprod(quimat0$X)) %*% crossprod(quimat0$X, y)
quiyhat &lt;- quimat0$X %*% quicoef
quimat1 &lt;- ssBasis(x, knots, m=3, d=1)
quiyd1 &lt;- quimat1$X %*% quicoef
quimat2 &lt;- ssBasis(x, knots, m=3, d=2)
quiyd2 &lt;- quimat2$X %*% quicoef

# plot quintic smoothing spline results
par(mfrow=c(1,3))
plot(x, y, type="l", main="Function")
lines(x, quiyhat, lty=2, col="red")
plot(x, d1y, type="l", main="First Derivative")
lines(x, quiyd1, lty=2, col="red")
plot(x, d2y, type="l", main="Second Derivative")
lines(x, quiyd2, lty=2, col="red")

</code></pre>

<hr>
<h2 id='summary'>
Summarizes Fit Information for bigsplines Model
</h2><span id='topic+summary.bigspline'></span><span id='topic+summary.bigssa'></span><span id='topic+summary.bigssg'></span><span id='topic+summary.bigssp'></span><span id='topic+summary.bigtps'></span>

<h3>Description</h3>

<p>This function summarizes basic model fit information for a fit <code>bigsplines</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bigspline'
summary(object, fitresid = TRUE, chunksize = 10000, ...)
## S3 method for class 'bigssa'
summary(object, fitresid = TRUE, chunksize = 10000, diagnostics = FALSE,...)
## S3 method for class 'bigssg'
summary(object, fitresid = TRUE, chunksize = 10000, diagnostics = FALSE,...)
## S3 method for class 'bigssp'
summary(object, fitresid = TRUE, chunksize = 10000, diagnostics = FALSE,...)
## S3 method for class 'bigtps'
summary(object, fitresid = TRUE, chunksize = 10000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_+3A_object">object</code></td>
<td>

<p>Object of class &quot;bigspline&quot; (output from <code><a href="#topic+bigspline">bigspline</a></code>), class &quot;bigssa&quot; (output from <code><a href="#topic+bigssa">bigssa</a></code>), class &quot;bigssg&quot; (output from <code><a href="#topic+bigssg">bigssg</a></code>), class &quot;bigssp&quot; (output from <code><a href="#topic+bigssp">bigssp</a></code>), or class &quot;bigtps&quot; (output from <code><a href="#topic+bigtps">bigtps</a></code>).
</p>
</td></tr>
<tr><td><code id="summary_+3A_fitresid">fitresid</code></td>
<td>

<p>Logical indicating whether the fitted values and residuals should be calculated for all data points in input <code>object</code>. 
</p>
</td></tr>
<tr><td><code id="summary_+3A_chunksize">chunksize</code></td>
<td>

<p>If <code>fitresid=TRUE</code>, fitted values are calculated in chunks of size <code>chunksize</code>.
</p>
</td></tr>
<tr><td><code id="summary_+3A_diagnostics">diagnostics</code></td>
<td>

<p>If <code>diagnostics=TRUE</code>, cosine diagnostics are calculated for each term in the model. These give an approximate break-down of the model R-squared into that accounted for by each term in the model.
</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>

<p>Ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bigspline">bigspline</a></code>, <code><a href="#topic+bigssa">bigssa</a></code>, <code><a href="#topic+bigssg">bigssg</a></code>, <code><a href="#topic+bigssp">bigssp</a></code>, and <code><a href="#topic+bigtps">bigtps</a></code> for more details.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>Called model in input <code>formula</code>.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Type of smoothing spline that was used for each predictor.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of fitted values (if <code>fitresid=TRUE</code>).</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>Vector of linear predictors (only for class &quot;bigssg&quot; with <code>fitresid=TRUE</code>).</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Vector of residuals (if <code>fitresid=TRUE</code>). For class &quot;bigssg&quot; these are deviance residuals.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated error standard deviation.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Model deviance (only for class &quot;bigssg&quot;).</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>Estimated dispersion parameter (only for class &quot;bigssg&quot;).</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Total sample size.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Effective degrees of freedom of the model.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Model fit information: vector containing the GCV, multiple R-squared, AIC, and BIC of fit model.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Convergence status: <code>converged=TRUE</code> if the iterative theta update converged, <code>converged=FALSE</code> if the iterative theta update failed to converge, and <code>converged=NA</code> if option <code>skip.iter=TRUE</code> was used.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterative updates (<code>iter=NA</code> if option <code>skip.iter=TRUE</code> was used).</p>
</td></tr>
<tr><td><code>rparm</code></td>
<td>
<p>Rounding parameters used for model fitting.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Global smoothing parameter used for model fitting.</p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>Vector of additional smoothing parameters (only for class &quot;bigssa&quot;).</p>
</td></tr>
<tr><td><code>thetas</code></td>
<td>
<p>Vector of additional smoothing parameters (only for class &quot;bigssp&quot;).</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>Vector of cosine diagnostics.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>Distribution family (only for class &quot;bigssg&quot;).</p>
</td></tr>
<tr><td><code>gcvtype</code></td>
<td>
<p>Smoothing parameter selection criterion (only for class &quot;bigssg&quot;).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For &quot;bigspline&quot; and &quot;bigtps&quot; objects, the outputs <code>call</code>, <code>converged</code>, and <code>iter</code> are NA.
</p>


<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##########   EXAMPLE 1   ##########

# define relatively smooth function
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# cubic spline
cubmod &lt;- bigspline(x,y)
summary(cubmod)


##########   EXAMPLE 2   ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x1v &lt;- runif(10^4)
x2v &lt;- runif(10^4)
y &lt;- myfun(x1v,x2v) + rnorm(10^4)

# cubic splines with 100 randomly selected knots (efficient parameterization)
cubmod &lt;- bigssa(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=100)
summary(cubmod)


##########   EXAMPLE 3   ##########

# function with two continuous predictors
set.seed(1)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
ndpts &lt;- 1000
x1v &lt;- runif(ndpts)
x2v &lt;- runif(ndpts)

# poisson response
set.seed(773)
lp &lt;- myfun(x1v,x2v)
mu &lt;- exp(lp)
y &lt;- rpois(n=ndpts,lambda=mu)

# generalized smoothing spline anova
genmod &lt;- bigssg(y~x1v*x2v,family="poisson",type=list(x1v="cub",x2v="cub"),nknots=50)
summary(genmod)


##########   EXAMPLE 4   ##########

# function with two continuous predictors
set.seed(773)
myfun &lt;- function(x1v,x2v){
  sin(2*pi*x1v) + log(x2v+.1) + cos(pi*(x1v-x2v))
}
x1v &lt;- runif(10^4)
x2v &lt;- runif(10^4)
y &lt;- myfun(x1v,x2v) + rnorm(10^4)

# cubic splines with 100 randomly selected knots (classic parameterization)
cubmod &lt;- bigssp(y~x1v*x2v,type=list(x1v="cub",x2v="cub"),nknots=100)
summary(cubmod)


##########   EXAMPLE 5   ##########

# define relatively smooth function
set.seed(773)
myfun &lt;- function(x){ sin(2*pi*x) }
x &lt;- runif(10^4)
y &lt;- myfun(x) + rnorm(10^4)

# thin-plate with default (30 knots)
tpsmod &lt;- bigtps(x,y)
summary(tpsmod)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
