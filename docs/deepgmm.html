<!DOCTYPE html><html lang="en"><head><title>Help for package deepgmm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepgmm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#deepgmm'>
<p>Fits Deep Gaussian Mixture Models Using</p>
Stochastic EM algorithm.</a></li>
<li><a href='#model_selection'>
<p>Function to compare different models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Gaussian Mixture Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-21</td>
</tr>
<tr>
<td>Author:</td>
<td>Cinzia Viroli, Geoffrey J. McLachlan</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Suren Rathnayake &lt;surenr@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Deep Gaussian mixture models as proposed by Viroli and McLachlan (2019) 
    &lt;<a href="https://doi.org/10.1007%2Fs11222-017-9793-z">doi:10.1007/s11222-017-9793-z</a>&gt; provide a generalization of classical Gaussian mixtures 
    to multiple layers. Each layer contains a set of latent variables that follow a mixture of 
    Gaussian distributions. To avoid overparameterized solutions, dimension reduction is 
    applied at each layer by way of factor models.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/suren-rathnayake/deepgmm">https://github.com/suren-rathnayake/deepgmm</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>mvtnorm, corpcor, mclust</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-20 21:14:26 UTC; suren</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-20 21:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='deepgmm'>
Fits Deep Gaussian Mixture Models Using
Stochastic EM algorithm.
</h2><span id='topic+deepgmm'></span>

<h3>Description</h3>

<p>Fits a deep Gaussian mixture model to multivariate data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepgmm(y, layers, k, r,
        it = 250, eps = 0.001, init = "kmeans", init_est = "factanal",
        seed = NULL, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deepgmm_+3A_y">y</code></td>
<td>

<p>A matrix or a data frame in which the rows correspond to
observations and the columns to variables.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_layers">layers</code></td>
<td>

<p>The number of layers in the deep Gaussian mixture model.
Limited to 1, 2 or 3.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_k">k</code></td>
<td>

<p>A vector of integers of length <code>layers</code>
containing the number of groups in the different layers.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_r">r</code></td>
<td>

<p>A vector of integers of length <code>layers</code>
containing the dimensions at the different layers.
Dimension of the layers must be in decreasing
size. See details.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_it">it</code></td>
<td>

<p>Maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_eps">eps</code></td>
<td>

<p>The EM algorithm terminates if the relative increment of the log-likelihood
falls below this value.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_init">init</code></td>
<td>

<p>Procedure to obtain an initial partition of the observations. See Details.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_init_est">init_est</code></td>
<td>

<p>Procedure for computing the initial parameter values for the given initial
partition of the data. See Details.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_seed">seed</code></td>
<td>

<p>Integer value to be passed to the <code>set.seed</code> function at the biginning of
the <code>deepgmm</code> function.
</p>
</td></tr>
<tr><td><code id="deepgmm_+3A_scale">scale</code></td>
<td>

<p>If <code>scale = TRUE</code>, the columns of data, <code>y</code>, will be scaled to
zero mean and unit variance.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The deep Gaussian mixture model is an hierarchical model organized
in a multilayered architecture where, at each layer,
the variables follow a mixture of Gaussian distributions.
This set of nested mixtures of linear models provides a globally
nonlinear model that can model the data in a very flexible way.
In order to avoid overparameterized solutions,
dimension reduction by factor models can be applied at each layer of
the architecture, thus resulting in deep mixtures of factor analyzers.
</p>
<p>The data <code>y</code> must be a matrix or a data frame containing
numerical values, with no missing values. The rows must correspond to
observations and the columns to variables.
</p>
<p>Presently, the maximum number of layers <code>layers</code> implemented
is 3.
</p>
<p>The ith element of <code>k</code> contain number of groups in the ith layer. Thus
the length <code>k</code> must equal to <code>layers</code>.
</p>
<p>The parameter vector <code>r</code> contains the latent variable dimension of
each layer.
Variables at different layers have progressively decreasing dimension,
<code class="reqn">r_1</code>, <code class="reqn">r_2</code>, ..., <code class="reqn">r_h</code>, where <code class="reqn">p &gt; r_1 &gt; r_2 &gt;
\dots &gt; r_h \geq 1</code>.
</p>
<p>The EM algorithm used by <code>dgmm</code> requires initialization.
The initialization is done by first partitioning the dataset,
and then estimating the initial values for model parameters
based on the partition.
There are four options available in <code>dgmm</code> for the
initial partitioning of the data;
random partitioning (<code>init = "random"</code>),
clustering using the <em>k</em>-means algorithm of &quot;Hartigan-Wong&quot;
(<code>init = "kmeans"</code>),
agglomerative hierarchical clustering (<code>init = "hclass"</code>).
and Gaussian mixture model based clustering
(<code>init = "mclust"</code>).
</p>
<p>After the initial partitioning has been chosen, initial values of
the parameters in the component analyzers need to be
calculated. Curently only one option available.
This default option, <code>init_est = "factanal"</code> provides initial
estimates of the parameters based on factor analysis.
</p>


<h3>Value</h3>

<p>An object of class <code>"dgmm"</code> containing fitted values.
It contains
</p>
<table role = "presentation">
<tr><td><code>H</code></td>
<td>
<p>A list in which the <em>i</em>th element is the loading
matrix for the <em>i</em>th layer</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>A list containing mixing proportions for each layer. (i.e.
the element w[[i]][j] contain the mixing proportion of the <em>j</em>th
component in the <em>i</em> layer.)</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>A list of matrices containing components means in the columns.
(i.e. the element mu[[i]][, j] contain the component mean of the <em>j</em>th
component in the <em>i</em> layer.)</p>
</td></tr>
<tr><td><code>psi</code></td>
<td>
<p>A list of arrays which contain covariance matrices for
the random error components of each component (i.e. the element
psi[[i]][j, ,, ] contain the error covariance matrix
for the <em>j</em>th component in the <em>i</em> layer.)</p>
</td></tr>
<tr><td><code>lik</code></td>
<td>
<p>The log-likelihood after each EM iteration</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>The Bayesian information criterion for the model fit</p>
</td></tr>
<tr><td><code>acl</code></td>
<td>
<p>The Akaike information criterion for the model fit</p>
</td></tr>
<tr><td><code>clc</code></td>
<td>
<p>The Classification likelihood information criterion for the model fit</p>
</td></tr>
<tr><td><code>icl.bic</code></td>
<td>
<p>The integrated classification criterion for the model fit</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Clustering of the observations</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>Value of the seed used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cinzia Viroli, Geoffrey J. McLachlan
</p>


<h3>References</h3>

<p>Viroli, C. and McLachlan, G.J. (2019). Deep Gaussian mixture models. Statistics and Computing 29, 43-51.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
layers &lt;- 2
k &lt;- c(3, 4)
r &lt;- c(3, 2)
it &lt;- 50
eps &lt;- 0.001
y &lt;- scale(mtcars)

set.seed(1)
fit &lt;-deepgmm(y = y, layers = layers, k = k, r = r,
                  it = it, eps = eps)
fit

summary(fit)

</code></pre>

<hr>
<h2 id='model_selection'>
Function to compare different models
</h2><span id='topic+model_selection'></span>

<h3>Description</h3>

<p>Compares different models and return the best one selected according to
criterion (BIC or AIC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_selection(y, layers, g, seeds = sample(.Machine$integer.max, 10),
                             it = 50, eps = 0.001, init = "kmeans",
                             init_est = "factanal", criterion = "BIC")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="model_selection_+3A_y">y</code></td>
<td>

<p>A matrix or a data frame in which rows correspond to
observations and columns to variables.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_layers">layers</code></td>
<td>

<p>The number of layers in the deep Gaussian mixture model.
Admitted values are 1, 2 or 3.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_g">g</code></td>
<td>

<p>The number of clusters.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_seeds">seeds</code></td>
<td>

<p>Integer vector containing seeds to try.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_it">it</code></td>
<td>

<p>Maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_eps">eps</code></td>
<td>

<p>The EM algorithm terminates the relative increment of the log-likelihod
falls below this value.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_init">init</code></td>
<td>

<p>Initial paritioning of the observations to determine initial
parameter values. See Details.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_init_est">init_est</code></td>
<td>

<p>To determine how the initial parameter values are computed. See Details.
</p>
</td></tr>
<tr><td><code id="model_selection_+3A_criterion">criterion</code></td>
<td>

<p>Model selection criterion, either <code>"AIC"</code> of <code>"BIC"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compares different models and return the best one selected according to
criterion (BIC or AIC). One can use diffefrent number of seeds.
</p>


<h3>Value</h3>

<p>A list containing
an object of class <code>"dgmm"</code> containing fitted values
and list of BIC and AIC values.
</p>


<h3>References</h3>

<p>Viroli, C. and McLachlan, G.J. (2019). Deep Gaussian mixture models.
Statistics and Computing 29, 43-51.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

y &lt;- scale(mtcars)

sel &lt;- model_selection(y, layers = 2, g = 3, seeds = c(1, 2, 12334),
                      it = 250, eps = 0.001, init = "kmeans", criterion = "BIC")
sel

summary(sel)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
