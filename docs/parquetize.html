<!DOCTYPE html><html><head><title>Help for package parquetize</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {parquetize}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#parquetize-package'><p>parquetize: Convert Files to Parquet Format</p></a></li>
<li><a href='#check_parquet'><p>Check if parquet file or dataset is readable and return basic informations</p></a></li>
<li><a href='#csv_to_parquet'><p>Convert a csv or a txt file to parquet format</p></a></li>
<li><a href='#dbi_to_parquet'><p>Convert a SQL Query on a DBI connection to parquet format</p></a></li>
<li><a href='#download_extract'><p>download and uncompress file if needed</p></a></li>
<li><a href='#expect_missing_argument'><p>Check if missing argument error is raised</p></a></li>
<li><a href='#expect_parquet'><p>Check if parquet dataset/file is readable and has the good number of rows</p></a></li>
<li><a href='#fst_to_parquet'><p>Convert a fst file to parquet format</p></a></li>
<li><a href='#get_partitions'><p>get unique values from table's column</p></a></li>
<li><a href='#json_to_parquet'><p>Convert a json file to parquet format</p></a></li>
<li><a href='#parquetize_example'><p>Get path to parquetize example</p></a></li>
<li><a href='#rbind_parquet'><p>Function to bind multiple parquet files by row</p></a></li>
<li><a href='#rds_to_parquet'><p>Convert a rds file to parquet format</p></a></li>
<li><a href='#sqlite_to_parquet'><p>Convert a sqlite file to parquet format</p></a></li>
<li><a href='#table_to_parquet'><p>Convert an input file to parquet format</p></a></li>
<li><a href='#write_parquet_at_once'><p>write parquet file or dataset based on partition argument <br /></p></a></li>
<li><a href='#write_parquet_by_chunk'><p>read input by chunk on function and create dataset <br /></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Convert Files to Parquet Format</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.7</td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of functions to get files in parquet format.
    Parquet is a columnar storage file format <a href="https://parquet.apache.org/">https://parquet.apache.org/</a>. 
    The files to convert can be of several formats 
    ("csv", "RData", "rds", "RSQLite", 
    "json", "ndjson", "SAS", "SPSS"...).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ddotta.github.io/parquetize/">https://ddotta.github.io/parquetize/</a>,
<a href="https://github.com/ddotta/parquetize">https://github.com/ddotta/parquetize</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ddotta/parquetize/issues">https://github.com/ddotta/parquetize/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>haven (&ge; 2.4.0), arrow, curl, readr, jsonlite, DBI, RSQLite,
cli, tidyselect, lifecycle, tools, glue, fst, rlang, dplyr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-04 13:40:09 UTC; damien.dotta</td>
</tr>
<tr>
<td>Author:</td>
<td>Damien Dotta [aut, cre],
  Nicolas Chuche [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Damien Dotta &lt;damien.dotta@live.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-04 14:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='parquetize-package'>parquetize: Convert Files to Parquet Format</h2><span id='topic+parquetize'></span><span id='topic+parquetize-package'></span>

<h3>Description</h3>

<p>Collection of functions to get files in parquet format. Parquet is a columnar storage file format <a href="https://parquet.apache.org/">https://parquet.apache.org/</a>. The files to convert can be of several formats (&quot;csv&quot;, &quot;RData&quot;, &quot;rds&quot;, &quot;RSQLite&quot;, &quot;json&quot;, &quot;ndjson&quot;, &quot;SAS&quot;, &quot;SPSS&quot;...).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Damien Dotta <a href="mailto:damien.dotta@live.fr">damien.dotta@live.fr</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Nicolas Chuche <a href="mailto:nicolas.chuche@barna.be">nicolas.chuche@barna.be</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://ddotta.github.io/parquetize/">https://ddotta.github.io/parquetize/</a>
</p>
</li>
<li> <p><a href="https://github.com/ddotta/parquetize">https://github.com/ddotta/parquetize</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ddotta/parquetize/issues">https://github.com/ddotta/parquetize/issues</a>
</p>
</li></ul>


<hr>
<h2 id='check_parquet'>Check if parquet file or dataset is readable and return basic informations</h2><span id='topic+check_parquet'></span>

<h3>Description</h3>

<p>This function checks if a file/dataset is a valid parquet format.
It will print the number of lines/columns and return a tibble on columns
information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_parquet(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_parquet_+3A_path">path</code></td>
<td>
<p>path to the file or dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will :
</p>
<p>* open the parquet dataset/file to check if it's valid
* print the number of lines
* print the number of columns
* return a tibble with 2 columns :
</p>
<p>* the column name (string)
* the arrow type (string)
</p>
<p>You can find a list of arrow type in the documentation
<a href="https://arrow.apache.org/docs/r/articles/data_types.html">on this page</a>.
</p>


<h3>Value</h3>

<p>a tibble with information on parquet dataset/file's columns with
three columns : field name, arrow type and nullable
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# check a parquet file
check_parquet(parquetize_example("iris.parquet"))

# check a parquet dataset
check_parquet(parquetize_example("iris_dataset"))
</code></pre>

<hr>
<h2 id='csv_to_parquet'>Convert a csv or a txt file to parquet format</h2><span id='topic+csv_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a csv or a txt file to parquet format. <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>csv_to_parquet(
  path_to_file,
  url_to_csv = lifecycle::deprecated(),
  csv_as_a_zip = lifecycle::deprecated(),
  filename_in_zip,
  path_to_parquet,
  columns = "all",
  compression = "snappy",
  compression_level = NULL,
  partition = "no",
  encoding = "UTF-8",
  read_delim_args = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_url_to_csv">url_to_csv</code></td>
<td>
<p>DEPRECATED use path_to_file instead</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_csv_as_a_zip">csv_as_a_zip</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_filename_in_zip">filename_in_zip</code></td>
<td>
<p>name of the csv/txt file in the zip. Required if several csv/txt are included in the zip.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_columns">columns</code></td>
<td>
<p>Character vector of columns to select from the input file (by default, all columns are selected).</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_encoding">encoding</code></td>
<td>
<p>String that indicates the character encoding for the input file.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_read_delim_args">read_delim_args</code></td>
<td>
<p>list of arguments for 'read_delim'.</p>
</td></tr>
<tr><td><code id="csv_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Note</h3>

<p>Be careful, if the zip size exceeds 4 GB, the function may truncate
the data (because unzip() won't work reliably in this case -
see <a href="https://rdrr.io/r/utils/unzip.html">here</a>).
In this case, it's advised to unzip your csv/txt file by hand
(for example with <a href="https://www.7-zip.org/">7-Zip</a>)
then use the function with the argument 'path_to_file'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a local csv file to a single parquet file :

csv_to_parquet(
  path_to_file = parquetize_example("region_2022.csv"),
  path_to_parquet = tempfile(fileext=".parquet")
)

# Conversion from a local txt file to a single parquet file :

csv_to_parquet(
  path_to_file = parquetize_example("region_2022.txt"),
  path_to_parquet = tempfile(fileext=".parquet")
)

# Conversion from a local csv file to a single parquet file and select only
# few columns :

csv_to_parquet(
  path_to_file = parquetize_example("region_2022.csv"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  columns = c("REG","LIBELLE")
)

# Conversion from a local csv file to a partitioned parquet file  :

csv_to_parquet(
  path_to_file = parquetize_example("region_2022.csv"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  partition = "yes",
  partitioning =  c("REG")
)

# Conversion from a URL and a zipped file (csv) :

csv_to_parquet(
  path_to_file = "https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip",
  filename_in_zip = "census2021-ts007-ctry.csv",
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a URL and a zipped file (txt) :

csv_to_parquet(
  path_to_file = "https://sourceforge.net/projects/irisdss/files/latest/download",
  filename_in_zip = "IRIS TEST data.txt",
  path_to_parquet = tempfile(fileext=".parquet")
)

## Not run: 
# Conversion from a URL and a csv file with "gzip" compression :

csv_to_parquet(
  path_to_file =
  "https://github.com/sidsriv/Introduction-to-Data-Science-in-python/raw/master/census.csv",
  path_to_parquet = tempfile(fileext = ".parquet"),
  compression = "gzip",
  compression_level = 5
)

## End(Not run)
</code></pre>

<hr>
<h2 id='dbi_to_parquet'>Convert a SQL Query on a DBI connection to parquet format</h2><span id='topic+dbi_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a SQL query from a DBI to parquet format.<br />
</p>
<p>It handles all DBI supported databases.
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>

<p>Examples explain how to convert a query to a chunked dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbi_to_parquet(
  conn,
  sql_query,
  path_to_parquet,
  max_memory,
  max_rows,
  chunk_memory_sample_lines = 10000,
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dbi_to_parquet_+3A_conn">conn</code></td>
<td>
<p>A DBIConnection object, as return by DBI::dbConnect</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_sql_query">sql_query</code></td>
<td>
<p>a character string containing an SQL query (this argument is passed to DBI::dbSendQuery)</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_max_memory">max_memory</code></td>
<td>
<p>Memory size (in Mb) in which data of one parquet file should roughly fit.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_max_rows">max_rows</code></td>
<td>
<p>Number of lines that defines the size of the chunk.
This argument can not be filled in if max_memory is used.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_chunk_memory_sample_lines">chunk_memory_sample_lines</code></td>
<td>
<p>Number of lines to read to evaluate max_memory. Default to 10 000.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="dbi_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a sqlite dbi connection to a single parquet file :

dbi_connection &lt;- DBI::dbConnect(RSQLite::SQLite(),
  system.file("extdata","iris.sqlite",package = "parquetize"))

# Reading iris table from local sqlite database
# and conversion to one parquet file :

dbi_to_parquet(
  conn = dbi_connection,
  sql_query = "SELECT * FROM iris",
  path_to_parquet = tempfile(fileext=".parquet"),
)

# Reading iris table from local sqlite database by chunk (using
# `max_memory` argument) and conversion to multiple parquet files

dbi_to_parquet(
  conn = dbi_connection,
  sql_query = "SELECT * FROM iris",
  path_to_parquet = tempdir(),
  max_memory = 2 / 1024
)

# Using chunk and partition together is not possible directly but easy to do :
# Reading iris table from local sqlite database by chunk (using
# `max_memory` argument) and conversion to arrow dataset partitioned by
# species

# get unique values of column "iris from table "iris"
partitions &lt;- get_partitions(dbi_connection, table = "iris", column = "Species")

# loop over those values
for (species in partitions) {
  dbi_to_parquet(
    conn = dbi_connection,
    # use glue_sql to create the query filtering the partition
    sql_query = glue::glue_sql("SELECT * FROM iris where Species = {species}",
                               .con = dbi_connection),
    # add the partition name in the output dir to respect parquet partition schema
    path_to_parquet = file.path(tempdir(), "iris", paste0("Species=", species)),
    max_memory = 2 / 1024,
  )
}

# If you need a more complicated query to get your partitions, you can use
# dbGetQuery directly :
col_to_partition &lt;- DBI::dbGetQuery(dbi_connection, "SELECT distinct(`Species`) FROM `iris`")[,1]

</code></pre>

<hr>
<h2 id='download_extract'>download and uncompress file if needed</h2><span id='topic+download_extract'></span>

<h3>Description</h3>

<p>This function will download the file if the file is remote and
unzip it if it is zipped.  It will just return the input path argument if
it's neither. <br />
</p>
<p>If the zip contains multiple files, you can use 'filename_in_zip' to set the file you want to unzip and use.
</p>
<p>You can pipe output on all '*_to_parquet' functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_extract(path, filename_in_zip)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="download_extract_+3A_path">path</code></td>
<td>
<p>the input  file's path or url.</p>
</td></tr>
<tr><td><code id="download_extract_+3A_filename_in_zip">filename_in_zip</code></td>
<td>
<p>name of the csv file in the zip. Required if
several csv are included in the zip.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the path to the usable (uncompressed) file, invisibly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 1. unzip a local zip file
# 2. parquetize it

file_path &lt;- download_extract(system.file("extdata","mtcars.csv.zip", package = "readr"))
csv_to_parquet(
  file_path,
  path_to_parquet = tempfile(fileext = ".parquet")
)

# 1. download a remote file
# 2. extract the file census2021-ts007-ctry.csv
# 3. parquetize it

file_path &lt;- download_extract(
  "https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip",
  filename_in_zip = "census2021-ts007-ctry.csv"
)
csv_to_parquet(
  file_path,
  path_to_parquet = tempfile(fileext = ".parquet")
)

# the file is local and not zipped so :
# 1. parquetize it

file_path &lt;- download_extract(parquetize_example("region_2022.csv"))
csv_to_parquet(
  file_path,
  path_to_parquet = tempfile(fileext = ".parquet")
)

</code></pre>

<hr>
<h2 id='expect_missing_argument'>Check if missing argument error is raised</h2><span id='topic+expect_missing_argument'></span>

<h3>Description</h3>

<p>Check if missing argument error is raised
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expect_missing_argument(object, regexp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expect_missing_argument_+3A_object">object</code></td>
<td>
<p>the object to check</p>
</td></tr>
<tr><td><code id="expect_missing_argument_+3A_regexp">regexp</code></td>
<td>
<p>a regexp with the message we must find</p>
</td></tr>
</table>


<h3>Value</h3>

<p>same as expect_error
</p>

<hr>
<h2 id='expect_parquet'>Check if parquet dataset/file is readable and has the good number of rows</h2><span id='topic+expect_parquet'></span>

<h3>Description</h3>

<p>Check if parquet dataset/file is readable and has the good number of rows
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expect_parquet(
  path,
  with_lines,
  with_partitions = NULL,
  with_columns = NULL,
  with_files = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expect_parquet_+3A_path">path</code></td>
<td>
<p>to the parquet file or dataset</p>
</td></tr>
<tr><td><code id="expect_parquet_+3A_with_lines">with_lines</code></td>
<td>
<p>number of lines the file/dataset should have</p>
</td></tr>
<tr><td><code id="expect_parquet_+3A_with_partitions">with_partitions</code></td>
<td>
<p>NULL or a vector with the partition names the dataset should have</p>
</td></tr>
<tr><td><code id="expect_parquet_+3A_with_columns">with_columns</code></td>
<td>
<p>NULL or a column's name vector the dataset/file should have</p>
</td></tr>
<tr><td><code id="expect_parquet_+3A_with_files">with_files</code></td>
<td>
<p>NULL or number of files a dataset should have</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset handle
</p>

<hr>
<h2 id='fst_to_parquet'>Convert a fst file to parquet format</h2><span id='topic+fst_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a fst file to parquet format. <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>fst_to_parquet(
  path_to_file,
  path_to_parquet,
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fst_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="fst_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="fst_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="fst_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="fst_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="fst_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a local fst file to a single parquet file ::

fst_to_parquet(
  path_to_file = system.file("extdata","iris.fst",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a local fst file to a partitioned parquet file  ::

fst_to_parquet(
  path_to_file = system.file("extdata","iris.fst",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  partition = "yes",
  partitioning =  c("Species")
)
</code></pre>

<hr>
<h2 id='get_partitions'>get unique values from table's column</h2><span id='topic+get_partitions'></span>

<h3>Description</h3>

<p>This function allows you to extract unique values from a table's column to use as partitions.<br />
</p>
<p>Internally, this function does &quot;SELECT DISTINCT('mycolumn') FROM 'mytable';&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_partitions(conn, table, column)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_partitions_+3A_conn">conn</code></td>
<td>
<p>A 'DBIConnection' object, as return by 'DBI::dbConnect'</p>
</td></tr>
<tr><td><code id="get_partitions_+3A_table">table</code></td>
<td>
<p>a DB table name</p>
</td></tr>
<tr><td><code id="get_partitions_+3A_column">column</code></td>
<td>
<p>a column name for the table passed in param</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with unique values for the column of the table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dbi_connection &lt;- DBI::dbConnect(RSQLite::SQLite(),
  system.file("extdata","iris.sqlite",package = "parquetize"))

get_partitions(dbi_connection, "iris", "Species")
</code></pre>

<hr>
<h2 id='json_to_parquet'>Convert a json file to parquet format</h2><span id='topic+json_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a <a href="https://www.json.org/json-en.html">json</a>
or <a href="https://docs.mulesoft.com/dataweave/latest/dataweave-formats-ndjson">ndjson</a> file to parquet format. <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>json_to_parquet(
  path_to_file,
  path_to_parquet,
  format = "json",
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="json_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_format">format</code></td>
<td>
<p>string that indicates if the format is &quot;json&quot; (by default) or &quot;ndjson&quot;</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="json_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a local json file to a single parquet file ::

json_to_parquet(
  path_to_file = system.file("extdata","iris.json",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a local ndjson file to a partitioned parquet file  ::

json_to_parquet(
  path_to_file = system.file("extdata","iris.ndjson",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  format = "ndjson"
)
</code></pre>

<hr>
<h2 id='parquetize_example'>Get path to parquetize example</h2><span id='topic+parquetize_example'></span>

<h3>Description</h3>

<p>parquetize comes bundled with a number of sample files in its 'inst/extdata'
directory. This function make them easy to access
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parquetize_example(file = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parquetize_example_+3A_file">file</code></td>
<td>
<p>Name of file or directory. If 'NULL', the example files will be listed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string
</p>


<h3>Examples</h3>

<pre><code class='language-R'>parquetize_example()
parquetize_example("region_2022.csv")
parquetize_example("iris_dataset")
</code></pre>

<hr>
<h2 id='rbind_parquet'>Function to bind multiple parquet files by row</h2><span id='topic+rbind_parquet'></span>

<h3>Description</h3>

<p>This function read all parquet files in 'folder' argument that starts with 'output_name',
combine them using rbind and write the result to a new parquet file. <br />
</p>
<p>It can also delete the initial files if 'delete_initial_files' argument is TRUE. <br />
</p>
<p>Be careful, this function will not work if files with different structures
are present in the folder given with the argument 'folder'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbind_parquet(
  folder,
  output_name,
  delete_initial_files = TRUE,
  compression = "snappy",
  compression_level = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbind_parquet_+3A_folder">folder</code></td>
<td>
<p>the folder where the initial files are stored</p>
</td></tr>
<tr><td><code id="rbind_parquet_+3A_output_name">output_name</code></td>
<td>
<p>name of the output parquet file</p>
</td></tr>
<tr><td><code id="rbind_parquet_+3A_delete_initial_files">delete_initial_files</code></td>
<td>
<p>Boolean. Should the function delete the initial files ? By default TRUE.</p>
</td></tr>
<tr><td><code id="rbind_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="rbind_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Parquet files, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(arrow)
if (file.exists('output')==FALSE) {
  dir.create("output")
}

file.create(fileext = "output/test_data1-4.parquet")
write_parquet(data.frame(
  x = c("a","b","c"),
  y = c(1L,2L,3L)
),
"output/test_data1-4.parquet")

file.create(fileext = "output/test_data4-6.parquet")
write_parquet(data.frame(
  x = c("d","e","f"),
  y = c(4L,5L,6L)
), "output/test_data4-6.parquet")

test_data &lt;- rbind_parquet(folder = "output",
                           output_name = "test_data",
                           delete_initial_files = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='rds_to_parquet'>Convert a rds file to parquet format</h2><span id='topic+rds_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a rds file to parquet format. <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>rds_to_parquet(
  path_to_file,
  path_to_parquet,
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rds_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="rds_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="rds_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="rds_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="rds_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="rds_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a local rds file to a single parquet file ::

rds_to_parquet(
  path_to_file = system.file("extdata","iris.rds",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a local rds file to a partitioned parquet file  ::

rds_to_parquet(
  path_to_file = system.file("extdata","iris.rds",package = "parquetize"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  partition = "yes",
  partitioning =  c("Species")
)
</code></pre>

<hr>
<h2 id='sqlite_to_parquet'>Convert a sqlite file to parquet format</h2><span id='topic+sqlite_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert a table from a sqlite file to parquet format. <br />
The following extensions are supported :
&quot;db&quot;,&quot;sdb&quot;,&quot;sqlite&quot;,&quot;db3&quot;,&quot;s3db&quot;,&quot;sqlite3&quot;,&quot;sl3&quot;,&quot;db2&quot;,&quot;s2db&quot;,&quot;sqlite2&quot;,&quot;sl2&quot;. <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sqlite_to_parquet(
  path_to_file,
  table_in_sqlite,
  path_to_parquet,
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sqlite_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_table_in_sqlite">table_in_sqlite</code></td>
<td>
<p>string that indicates the name of the table to convert in the sqlite file</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="sqlite_to_parquet_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments, see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parquet file, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Conversion from a local sqlite file to a single parquet file :

sqlite_to_parquet(
  path_to_file = system.file("extdata","iris.sqlite",package = "parquetize"),
  table_in_sqlite = "iris",
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a local sqlite file to a partitioned parquet file  :

sqlite_to_parquet(
  path_to_file = system.file("extdata","iris.sqlite",package = "parquetize"),
  table_in_sqlite = "iris",
  path_to_parquet = tempfile(),
  partition = "yes",
  partitioning =  c("Species")
)
</code></pre>

<hr>
<h2 id='table_to_parquet'>Convert an input file to parquet format</h2><span id='topic+table_to_parquet'></span>

<h3>Description</h3>

<p>This function allows to convert an input file to parquet format. <br />
</p>
<p>It handles SAS, SPSS and Stata files in a same function. There is only one function to use for these 3 cases.
For these 3 cases, the function guesses the data format using the extension of the input file (in the 'path_to_file' argument). <br />
</p>
<p>Two conversions possibilities are offered :
</p>

<ul>
<li><p>Convert to a single parquet file. Argument 'path_to_parquet' must then be used;
</p>
</li>
<li><p>Convert to a partitioned parquet file. Additionnal arguments 'partition' and 'partitioning' must then be used;
</p>
</li></ul>

<p>To avoid overcharging R's RAM, the conversion can be done by chunk. One of arguments 'max_memory' or 'max_rows' must then be used.
This is very useful for huge tables and for computers with little RAM because the conversion is then done
with less memory consumption. For more information, see <a href="https://ddotta.github.io/parquetize/articles/aa-conversions.html">here</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>table_to_parquet(
  path_to_file,
  path_to_parquet,
  max_memory = NULL,
  max_rows = NULL,
  chunk_size = lifecycle::deprecated(),
  chunk_memory_size = lifecycle::deprecated(),
  columns = "all",
  by_chunk = lifecycle::deprecated(),
  skip = 0,
  partition = "no",
  encoding = NULL,
  chunk_memory_sample_lines = 10000,
  compression = "snappy",
  compression_level = NULL,
  user_na = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="table_to_parquet_+3A_path_to_file">path_to_file</code></td>
<td>
<p>String that indicates the path to the input file (don't forget the extension).</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where the parquet files will be stored.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_max_memory">max_memory</code></td>
<td>
<p>Memory size (in Mb) in which data of one parquet file should roughly fit.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_max_rows">max_rows</code></td>
<td>
<p>Number of lines that defines the size of the chunk.
This argument can not be filled in if max_memory is used.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_chunk_size">chunk_size</code></td>
<td>
<p>DEPRECATED use max_rows</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_chunk_memory_size">chunk_memory_size</code></td>
<td>
<p>DEPRECATED use max_memory</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_columns">columns</code></td>
<td>
<p>Character vector of columns to select from the input file (by default, all columns are selected).</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_by_chunk">by_chunk</code></td>
<td>
<p>DEPRECATED use max_memory or max_rows instead</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_skip">skip</code></td>
<td>
<p>By default 0. This argument must be filled in if 'by_chunk' is TRUE. Number of lines to ignore when converting.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_partition">partition</code></td>
<td>
<p>String (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.
Be careful, this argument can not be &quot;yes&quot; if 'max_memory' or 'max_rows' argument are not NULL.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_encoding">encoding</code></td>
<td>
<p>String that indicates the character encoding for the input file.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_chunk_memory_sample_lines">chunk_memory_sample_lines</code></td>
<td>
<p>Number of lines to read to evaluate max_memory. Default to 10 000.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_user_na">user_na</code></td>
<td>
<p>If 'TRUE' variables with user defined missing will be read
into [haven::labelled_spss()] objects. If 'FALSE', the default, user-defined missings will be converted to 'NA'.</p>
</td></tr>
<tr><td><code id="table_to_parquet_+3A_...">...</code></td>
<td>
<p>Additional format-specific arguments,  see <a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a>
and <a href="https://arrow.apache.org/docs/r/reference/write_dataset.html">arrow::write_dataset()</a> for more informations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Parquet files, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Conversion from a SAS file to a single parquet file :

table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Conversion from a SPSS file to a single parquet file :

table_to_parquet(
  path_to_file = system.file("examples","iris.sav", package = "haven"),
  path_to_parquet = tempfile(fileext = ".parquet"),
)
# Conversion from a Stata file to a single parquet file without progress bar :

table_to_parquet(
  path_to_file = system.file("examples","iris.dta", package = "haven"),
  path_to_parquet = tempfile(fileext = ".parquet")
)

# Reading SPSS file by chunk (using `max_rows` argument)
# and conversion to multiple parquet files :

table_to_parquet(
  path_to_file = system.file("examples","iris.sav", package = "haven"),
  path_to_parquet = tempfile(),
  max_rows = 50,
)

# Reading SPSS file by chunk (using `max_memory` argument)
# and conversion to multiple parquet files of 5 Kb when loaded (5 Mb / 1024)
# (in real files, you should use bigger value that fit in memory like 3000
# or 4000) :

table_to_parquet(
  path_to_file = system.file("examples","iris.sav", package = "haven"),
  path_to_parquet = tempfile(),
  max_memory = 5 / 1024
)

# Reading SAS file by chunk of 50 lines with encoding
# and conversion to multiple files :

table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(),
  max_rows = 50,
  encoding = "utf-8"
)

# Conversion from a SAS file to a single parquet file and select only
# few columns  :

table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  columns = c("Species","Petal_Length")
)

# Conversion from a SAS file to a partitioned parquet file  :

table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(),
  partition = "yes",
  partitioning =  c("Species") # vector use as partition key
)

# Reading SAS file by chunk of 50 lines
# and conversion to multiple files with zstd, compression level 10

if (isTRUE(arrow::arrow_info()$capabilities[['zstd']])) {
  table_to_parquet(
    path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
    path_to_parquet = tempfile(),
    max_rows = 50,
    compression = "zstd",
    compression_level = 10
  )
}
</code></pre>

<hr>
<h2 id='write_parquet_at_once'>write parquet file or dataset based on partition argument <br /></h2><span id='topic+write_parquet_at_once'></span>

<h3>Description</h3>

<p>Low level function that implements the logic to write a parquet file or a dataset from data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_parquet_at_once(
  data,
  path_to_parquet,
  partition = "no",
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_parquet_at_once_+3A_data">data</code></td>
<td>
<p>the data.frame/tibble to write</p>
</td></tr>
<tr><td><code id="write_parquet_at_once_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where
the output parquet file or dataset will be stored.</p>
</td></tr>
<tr><td><code id="write_parquet_at_once_+3A_partition">partition</code></td>
<td>
<p>string (&quot;yes&quot; or &quot;no&quot; - by default) that indicates whether you want to create a partitioned parquet file.
If &quot;yes&quot;, '&quot;partitioning&quot;' argument must be filled in. In this case, a folder will be created for each modality of the variable filled in '&quot;partitioning&quot;'.</p>
</td></tr>
<tr><td><code id="write_parquet_at_once_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="write_parquet_at_once_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="write_parquet_at_once_+3A_...">...</code></td>
<td>
<p>Additional format-specific arguments,  see
<a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataset as return by arrow::open_dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
write_parquet_at_once(iris, tempfile())

write_parquet_at_once(iris, tempfile(), partition = "yes", partitioning = c("Species"))

## Not run: 
write_parquet_at_once(iris, tempfile(), compression="gzip", compression_level = 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='write_parquet_by_chunk'>read input by chunk on function and create dataset <br /></h2><span id='topic+write_parquet_by_chunk'></span>

<h3>Description</h3>

<p>Low level function that implements the logic to to read input file by chunk and write a
dataset. <br />
</p>
<p>It will:
</p>

<ul>
<li><p>calculate the number of row by chunk if needed;
</p>
</li>
<li><p>loop over the input file by chunk;
</p>
</li>
<li><p>write each output files.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>write_parquet_by_chunk(
  read_method,
  input,
  path_to_parquet,
  max_rows = NULL,
  max_memory = NULL,
  chunk_memory_sample_lines = 10000,
  compression = "snappy",
  compression_level = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_parquet_by_chunk_+3A_read_method">read_method</code></td>
<td>
<p>a method to read input files. This method take only three
arguments
</p>
<p>'input' : some kind of data. Can be a
'skip' : the number of row to skip
'n_max' : the number of row to return
</p>
<p>This method will be called until it returns a dataframe/tibble with zero row.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_input">input</code></td>
<td>
<p>that indicates the path to the input. It can be anything you
want but more often a file's path or a data.frame.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_path_to_parquet">path_to_parquet</code></td>
<td>
<p>String that indicates the path to the directory where
the output parquet file or dataset will be stored.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_max_rows">max_rows</code></td>
<td>
<p>Number of lines that defines the size of the chunk. This
argument can not be filled in if max_memory is used.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_max_memory">max_memory</code></td>
<td>
<p>Memory size (in Mb) in which data of one parquet file
should roughly fit.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_chunk_memory_sample_lines">chunk_memory_sample_lines</code></td>
<td>
<p>Number of lines to read to evaluate
max_memory. Default to 10 000.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression algorithm.</p>
</td></tr>
<tr><td><code id="write_parquet_by_chunk_+3A_...">...</code></td>
<td>
<p>Additional format-specific arguments,  see
<a href="https://arrow.apache.org/docs/r/reference/write_parquet.html">arrow::write_parquet()</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataset as return by arrow::open_dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# example with a dataframe

# we create the function to loop over the data.frame

read_method &lt;- function(input, skip = 0L, n_max = Inf) {
  # if we are after the end of the input we return an empty data.frame
  if (skip+1 &gt; nrow(input)) { return(data.frame()) }

  # return the n_max row from skip + 1
  input[(skip+1):(min(skip+n_max, nrow(input))),]
}

# we use it

write_parquet_by_chunk(
  read_method = read_method,
  input = mtcars,
  path_to_parquet = tempfile(),
  max_rows = 10,
)


#
# Example with haven::read_sas
#

# we need to pass two argument beside the 3 input, skip and n_max.
# We will use a closure :

my_read_closure &lt;- function(encoding, columns) {
  function(input, skip = OL, n_max = Inf) {
    haven::read_sas(data_file = input,
                    n_max = n_max,
                    skip = skip,
                    encoding = encoding,
                    col_select = all_of(columns))
  }
}

# we initialize the closure

read_method &lt;- my_read_closure(encoding = "WINDOWS-1252", columns = c("Species", "Petal_Width"))

# we use it
write_parquet_by_chunk(
  read_method = read_method,
  input = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(),
  max_rows = 75,
)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
