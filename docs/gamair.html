<!DOCTYPE html><html><head><title>Help for package gamair</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gamair}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aral'><p>Aral sea remote sensed chlorophyll data</p></a></li>
<li><a href='#bird'><p>Bird distribution data from Portugal</p></a></li>
<li><a href='#blowfly'><p>Nicholson's Blowfly data</p></a></li>
<li><a href='#bone'><p>Bone marrow treatemtn survival data</p></a></li>
<li><a href='#brain'><p>Brain scan data</p></a></li>
<li><a href='#cairo'><p>Daily temperature data for Cairo</p></a></li>
<li><a href='#CanWeather'><p>Canadian Weather data</p></a></li>
<li><a href='#ch1'><p>Code for Chapter 1: Linear Models</p></a></li>
<li><a href='#ch1.solutions'><p>Solution code for Chapter 1: Linear Models</p></a></li>
<li><a href='#ch2'><p>Code for Chapter 2: Linear Mixed Models</p></a></li>
<li><a href='#ch2.solutions'><p>Solution code for Chapter 2: Linear Mixed Models</p></a></li>
<li><a href='#ch3'><p>Code for Chapter 3: Generalized Linear Models</p></a></li>
<li><a href='#ch3.solutions'><p>Solution code for Chapter 3: Generalized Linear Models</p></a></li>
<li><a href='#ch4'><p>Code for Chapter 4: Introducing GAMs</p></a></li>
<li><a href='#ch4.solutions'><p>Solution code for Chapter 4: Introducing GAMs</p></a></li>
<li><a href='#ch5'><p>Code for Chapter 5: Smoothers</p></a></li>
<li><a href='#ch5.solutions'><p>Solution code for Chapter 5: Smoothers</p></a></li>
<li><a href='#ch6'><p>Code for Chapter 6: GAM Theory</p></a></li>
<li><a href='#ch6.solutions'><p>Solution code for Chapter 6: GAM Theory</p></a></li>
<li><a href='#ch7'><p>Code for Chapter 7: GAMs in Practice: mgcv</p></a></li>
<li><a href='#ch7.solutions'><p>Solution code for Chapter 7 GAMs in Practice: mgcv</p></a></li>
<li><a href='#chicago'><p>Chicago air pollution and death rate data</p></a></li>
<li><a href='#chl'><p>Chlorophyll data</p></a></li>
<li><a href='#co2s'><p>Atmospheric CO2 at South Pole</p></a></li>
<li><a href='#coast'><p>European coastline from -11 to 0 East and</p>
from 43 to 59 North</a></li>
<li><a href='#engine'><p>Engine wear versus size data</p></a></li>
<li><a href='#gamair-package'>
<p>Data and scripts for &lsquo;Generalized Additive Models: An Introduction with R&rsquo;</p></a></li>
<li><a href='#gas'><p>Octane rating data</p></a></li>
<li><a href='#harrier'><p>Hen Harriers Eating Grouse</p></a></li>
<li><a href='#hubble'><p>Hubble Space Telescope Data</p></a></li>
<li><a href='#ipo'><p>Initial Public Offering Data</p></a></li>
<li><a href='#Larynx'><p>Cancer of the larynx in Germany</p></a></li>
<li><a href='#mack'><p>Egg data from 1992 mackerel survey</p></a></li>
<li><a href='#mackp'><p>Prediction grid data for 1992 mackerel egg model</p></a></li>
<li><a href='#meh'><p>Data from 2010 horse mackerel and mackerel egg survey</p></a></li>
<li><a href='#mpg'><p>Data on automobile efficiency on town streets and highway.</p></a></li>
<li><a href='#prostate'><p>Prostate cancer screening data</p></a></li>
<li><a href='#sitka'><p>Sitka spruce growth data.</p></a></li>
<li><a href='#sole'><p>Sole Eggs in the Bristol Channel</p></a></li>
<li><a href='#sperm.comp1'><p>Sperm competition data I</p></a></li>
<li><a href='#sperm.comp2'><p>Sperm competition data II</p></a></li>
<li><a href='#stomata'><p>Stomatal area and CO2</p></a></li>
<li><a href='#swer'><p>Swiss 12 hour extreme rainfall</p></a></li>
<li><a href='#wesdr'><p>Diabetic retinopathy in Wisconsin</p></a></li>
<li><a href='#wine'><p>Bordeaux Wines</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0-2</td>
</tr>
<tr>
<td>Author:</td>
<td>Simon Wood &lt;simon.wood@r-project.org&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Simon Wood &lt;simon.wood@r-project.org&gt;</td>
</tr>
<tr>
<td>Title:</td>
<td>Data for 'GAMs: An Introduction with R'</td>
</tr>
<tr>
<td>Description:</td>
<td>Data sets and scripts used in the book 'Generalized Additive 
             Models: An Introduction with R', Wood (2006,2017) CRC.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mgcv, lattice, MASS, nlme, lme4, geoR, survival</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-08-23 12:14:27 UTC; sw283</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-08-23 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aral'>Aral sea remote sensed chlorophyll data</h2><span id='topic+aral'></span><span id='topic+aral.bnd'></span>

<h3>Description</h3>

<p>SeaWifs satellite chlorophyll measurements for the 38th 8-day observation period of the year in the Aral sea, averaged over 1998-2002, along with an Aral sea boundary file. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(aral)
data(aral.bnd)
</code></pre>


<h3>Format</h3>

<p>The <code>aral</code> data frame has the following columns
</p>

<dl>
<dt>lon</dt><dd><p>longitude of pixel or boundary vertex.</p>
</dd>
<dt>lat</dt><dd><p>latitude of pixel or boundary vertex.</p>
</dd>
<dt>chl</dt><dd><p>chlorophyll measurement</p>
</dd>
<dt>exra</dt><dd><p>The highest rainfall observed in any 12 hour period in that year, in mm. </p>
</dd>
</dl>


<h3>Details</h3>

<p>Trying to smooth the data with a conventional smoother, such as a thin plate spline, leads to linkage between the two arms of the sea, which is clearly an artefact. A soap film smoother avoids this problem.
</p>


<h3>Source</h3>

<p><a href="https://seawifs.gsfc.nasa.gov/">https://seawifs.gsfc.nasa.gov/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(aral); data(aral.bnd)

## define some knots...
knt &lt;- list(lon=c(58.55,59.09,59.36,59.64,59.91,60.18,58.27,58.55,59.09,
59.36,59.64,59.91,60.18,60.45,58.27,58.55,58.82,59.09,59.36,59.64,59.91,
60.18,60.45,58.27,58.55,59.36,59.64,59.91,60.18,58.55,59.36,59.64,59.91,
60.18,58.55,58.82,59.36,59.64,59.91,60.18,60.45,58.82,59.09,59.64,59.91,
60.18,59.64),
lat=c(44.27,44.27,44.27,44.27,44.27,44.27,44.55,44.55,44.55,44.55,44.55,
44.55,44.55,44.55,44.82,44.82,44.82,44.82,44.82,44.82,44.82,44.82,44.82,
45.09,45.09,45.09,45.09,45.09,45.09,45.36,45.36,45.36,45.36,45.36,45.64,
45.64,45.64,45.64,45.64,45.64,45.64,45.91,45.91,45.91,45.91,45.91,46.18))

## fit soap film...
b &lt;-  gam(chl~s(lon,lat,k=30,bs="so",xt=list(bnd=list(aral.bnd),
                nmax=150)),knots=knt,data=aral)

## plot results...
plot(b)
</code></pre>

<hr>
<h2 id='bird'>Bird distribution data from Portugal</h2><span id='topic+bird'></span>

<h3>Description</h3>

<p>Data from the compilation of the Portuguese Atlas of Breeding Birds.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bird)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 columns and 25100 rows. Each row refers to one 2km by 2km
square (tetrad). The columns are:
</p>

<dl>
<dt>QUADRICULA</dt><dd><p>An identifier for the 10km by 10km square that this tetrad
belongs to.</p>
</dd>
<dt>TET</dt><dd><p>Which tetrad the observation is from.</p>
</dd>
<dt>crestlark</dt><dd><p>Were crested lark (or possibly thekla lark!) found (1), not
found (0) breading in this tetrad, or was the tetrad not visited (<code>NA</code>). </p>
</dd>
<dt>linnet</dt><dd><p>As <code>crestlark</code>, but for linnet.</p>
</dd>
<dt>x</dt><dd><p>location of tetrad (km east of an origin).</p>
</dd>
<dt>y</dt><dd><p>location of tetrad (km north of an origin).</p>
</dd>
</dl>


<h3>Details</h3>

<p>At least 6 tetrads from each 10km square were visited, to establish
whether each species was breeding there, or not. Each Tetrad was visited twice
for one hour each visit. These data are not
definitive: at time of writing the fieldwork was not quite complete.
</p>
<p>The data were kindly supplied by Jose Pedro Granadeiro.
</p>


<h3>Source</h3>

 
<p>The Atlas of the Portuguese Breeding Birds.
</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(bird)
  species &lt;- "crestlark"
  op&lt;-par(bg="white",mfrow=c(1,1),mar=c(5,5,1,1))
  ind &lt;- bird[[species]]==0&amp;!is.na(bird[[species]])
  plot(bird$y[ind]/1000,1000-bird$x[ind]/1000,pch=19,cex=.3,col="white",
     ylab="km west",xlab="km north",cex.lab=1.4,cex.axis=1.3,type="n")
  polygon(c(4000,4700,4700,4000),c(250,250,600,600),col="grey",border="black")
  points(bird$y[ind]/1000,1000-bird$x[ind]/1000,pch=19,cex=.3,col="white")
  ind &lt;- bird[[species]]==1&amp;!is.na(bird[[species]])
  with(bird,points(y[ind]/1000,1000-x[ind]/1000,pch=19,cex=.3))
  par(op)
</code></pre>

<hr>
<h2 id='blowfly'>Nicholson's Blowfly data</h2><span id='topic+blowfly'></span>

<h3>Description</h3>

<p>Data on a laboratory population of Blowfies, from the classic
ecological studies of Nicholson.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(blowfly)
</code></pre>


<h3>Format</h3>

<p>A data frame with 2 columns and 180 rows. The columns are:
</p>

<dl>
<dt>pop</dt><dd><p>Counts (!) of the population of adult Blowflies in one of the experiments.</p>
</dd>
<dt>day</dt><dd><p>Day of experiment.</p>
</dd>
</dl>


<h3>Details</h3>

<p>The population counts are actually obtained by counting dead
blowflies and back calculating.
</p>


<h3>References</h3>

<p>Nicholson, A.J. (1954a) Compensatory reactions of populations to stresses and
their evolutionary significance. Australian Journal of Zoology 2, 1-8.
</p>
<p>Nicholson, A.J. (1954b) An outline of the dynamics of animal populations. 
Australian Journal of Zoology 2, 9-65.
</p>
<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(blowfly)
  with(blowfly,plot(day,pop,type="l"))
</code></pre>

<hr>
<h2 id='bone'>Bone marrow treatemtn survival data</h2><span id='topic+bone'></span>

<h3>Description</h3>

<p>Data from Klein and Moeschberger (2003), for 23 patients with non-Hodgkin's lymphoma.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bone)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 columns and 23 rows. Each row refers to one patient. The columns are:
</p>

<dl>
<dt>t</dt><dd><p>Time of death, relapse or last follow up after treatment, in days.</p>
</dd>
<dt>d</dt><dd><p>1 for death or relapse. 0 otherwise.</p>
</dd>
<dt>trt</dt><dd><p>2 level factor. <code>allo</code> or <code>auto</code> depending on treatment recieved.</p>
</dd>
</dl>


<h3>Details</h3>

<p>The data were collected at the Ohio State University bone marrow transplant unit. The <code>allo</code> treatment is bone marrow transplant from a matched sibling donor. The <code>auto</code> treatment consists of bone marrow removal and replacement after chemotherapy. 
</p>


<h3>Source</h3>

 
<p>Klein and Moeschberger (2003).
</p>


<h3>References</h3>

<p>Klein and Moeschberger (2003) Survival Analysis: techniques for censored and truncated data.
</p>
<p>Wood, S.N. (2017) Generalized Additive Models: An Introduction with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example of fitting a Cox PH model as a Poisson GLM... 
## First a function to convert data frame of raw data
## to data frame of artificial data...

psurv &lt;- function(surv,time="t",censor="d",event="z") {
## create data frame to fit Cox PH as Poisson model.
## surv[[censor]] should be 1 for event or zero for censored.   
  if (event %in% names(surv)) warning("event name already in use in data frame")
  surv &lt;- as.data.frame(surv)[order(surv[[time]]),] ## ascending time order
  et &lt;- unique(surv[[time]][surv[[censor]]==1]) ## unique times requiring record
  es &lt;- match(et,surv[[time]]) ## starts of risk sets in surv
  n &lt;- nrow(surv); t &lt;- rep(et,1+n-es) ## times for risk sets
  st &lt;- cbind(0,surv[unlist(apply(matrix(es),1,function(x,n) x:n,n=n)),])
  st[st[[time]]==t&amp;st[[censor]]!=0,1] &lt;- 1 ## signal events 
  st[[time]] &lt;- t ## reset time field to time for this risk set
  names(st)[1] &lt;- event
  st
} ## psurv

## Now fit the model...
require(gamair)
data(bone);bone$id &lt;- 1:nrow(bone)
pb &lt;- psurv(bone); pb$tf &lt;- factor(pb$t)
## Note that time factor tf should go first to ensure
## it has no contrasts applied... 
b &lt;- glm(z ~ tf + trt - 1,poisson,pb)
drop1(b,test="Chisq") ## test for effect - no evidence

## martingale and deviance residuals
chaz &lt;- tapply(fitted(b),pb$id,sum) ## cum haz by subject
mrsd &lt;- bone$d - chaz
drsd &lt;- sign(mrsd)*sqrt(-2*(mrsd + bone$d*log(chaz)))

## Estimate and plot survivor functions and standard
## errors for the two groups...

te &lt;- sort(unique(bone$t[bone$d==1])) ## event times

## predict survivor function for "allo"...
pd &lt;- data.frame(tf=factor(te),trt=bone$trt[1])
fv &lt;- predict(b,pd)
H &lt;- cumsum(exp(fv)) ## cumulative hazard
plot(stepfun(te,c(1,exp(-H))),do.points=FALSE,ylim=c(0,1),xlim=c(0,550),
     main="black allo, grey auto",ylab="S(t)",xlab="t (days)")
## add s.e. bands...     
X &lt;- model.matrix(~tf+trt-1,pd)
J &lt;- apply(exp(fv)*X,2,cumsum)
se &lt;- diag(J%*%vcov(b)%*%t(J))^.5
lines(stepfun(te,c(1,exp(-H+se))),do.points=FALSE,lty=2)
lines(stepfun(te,c(1,exp(-H-se))),do.points=FALSE,lty=2)

## same for "auto"...
pd &lt;- data.frame(tf=factor(te),trt=bone$trt[23])
fv &lt;- predict(b,pd); H &lt;- cumsum(exp(fv))
lines(stepfun(te,c(1,exp(-H))),col="grey",lwd=2,do.points=FALSE)
X &lt;- model.matrix(~tf+trt-1,pd)
J &lt;- apply(exp(fv)*X,2,cumsum)
se &lt;- diag(J%*%vcov(b)%*%t(J))^.5
lines(stepfun(te,c(1,exp(-H+se))),do.points=FALSE,lty=2,col="grey",lwd=2)
lines(stepfun(te,c(1,exp(-H-se))),do.points=FALSE,lty=2,col="grey",lwd=2)

</code></pre>

<hr>
<h2 id='brain'>Brain scan data</h2><span id='topic+brain'></span>

<h3>Description</h3>

<p> Functional magnetic resonance imaging measurements for a human
brain subject to a particular experimental stimulus. One slice of the image is
provided, described as a near-axial slice through the dorsal cerebral cortex.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(brain)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5 columns and 1567 rows. Each row refers to one &lsquo;voxel&rsquo; of
the image. The columns are:
</p>

<dl>
<dt>X</dt><dd><p>voxel position on horizontal axis.</p>
</dd>
<dt>Y</dt><dd><p>voxel position on vertical axis.</p>
</dd>
<dt>medFPQ</dt><dd><p>median of three replicate 'Fundamental 
Power Quotient' values at the voxel: this is the main measurement of brain 
activivity.</p>
</dd>
<dt>region</dt><dd><p>code indicating which of several regions of the brain the voxel
belongs to. The regions are defined by the experimenters. <code>0</code> is the base
region; <code>1</code> is the
region of interest; <code>2</code> is the region activated by the experimental 
stimulus; <code>NA</code> denotes a voxel with no allocation.</p>
</dd>
<dt>meanTheta</dt><dd><p>mean phase shift at the Voxel, over three measurments.</p>
</dd>
</dl>



<h3>Details</h3>

<p> See the source article for fuller details.
</p>


<h3>Source</h3>

<p> S. Landau et al (2003)
&lsquo;Tests for a difference in timing of physiological response between two brain regions measured by using functional magnetic resonance imaging&rsquo;.
Journal of the Royal Statistical Society, Series C, Applied Statistics, 53(1):63-82
</p>

<hr>
<h2 id='cairo'>Daily temperature data for Cairo</h2><span id='topic+cairo'></span>

<h3>Description</h3>

<p>The average air temperature (F) in Cairo from Jan 1st 1995.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cairo)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 columns and 3780 rows. The columns are:
</p>

<dl>
<dt>month</dt><dd><p>month of year from 1 to 12.</p>
</dd>
<dt>day.of.month</dt><dd><p>day of month, from 1 to 31.</p>
</dd>
<dt>year</dt><dd><p>Year, starting 1995.</p>
</dd>
<dt>temp</dt><dd><p>Average temperature (F).</p>
</dd>
<dt>day.of.year</dt><dd><p>Day of year from 1 to 366.</p>
</dd>
<dt>time</dt><dd><p>Number of days since 1st Jan 1995.</p>
</dd>
</dl>


<h3>Source</h3>

<p><a href="http://academic.udayton.edu/kissock/http/Weather/citylistWorld.htm">http://academic.udayton.edu/kissock/http/Weather/citylistWorld.htm</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(cairo)
  with(cairo,plot(time,temp,type="l"))
</code></pre>

<hr>
<h2 id='CanWeather'>Canadian Weather data</h2><span id='topic+CanWeather'></span>

<h3>Description</h3>

<p>Data on temperature throughout the year at 35 Canadian locations, originally form the <code>fda</code> package.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(canWeather)
</code></pre>


<h3>Format</h3>

<p>The <code>CanWeather</code> data frame has the following 5 columns
</p>

<dl>
<dt>time</dt><dd><p>Day of year from 1 to 365.</p>
</dd>
<dt>T</dt><dd><p>Mean temperature for that day in centigrade.</p>
</dd>
<dt>region</dt><dd><p>A four level factor classifiying locations as <code>Arctic</code>, <code>Atlantic</code>, <code>Continental</code> or <code>Pacific</code>.</p>
</dd>
<dt>latitude</dt><dd><p>Degrees north of the equator.</p>
</dd>
<dt>place</dt><dd><p>A factor with 35 levels: the names of each locagtion.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data provide quite a nice application of function on scalar regression. Note that the data are for a single year, so will not generally be cyclic.  
</p>


<h3>Source</h3>

<p>Data are from the <code>fda</code> package. 
</p>
<p><a href="https://cran.r-project.org/package=fda">https://cran.r-project.org/package=fda</a>
</p>


<h3>References</h3>

<p>Ramsay J.O. and B.W. Silverman (2006) Functional data analysis (2nd ed). Springer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(canWeather)
reg &lt;- unique(CanWeather$region)
place &lt;- unique(CanWeather$place)
col &lt;- 1:4;names(col) &lt;- reg
for (k in 1:35) {
  if (k==1) plot(1:365,CanWeather$T[CanWeather$place==place[k]],
            ylim=range(CanWeather$T),type="l",
	    col=col[CanWeather$region],xlab="day",ylab="temperature") else
	    lines(1:365,CanWeather$T[CanWeather$place==place[k]],
            col=col[CanWeather$region[CanWeather$place==place[k]]])
}

## Function on scalar regression.
## T(t) = f_r(t) + f(t)*latitude + e(t)
## where e(t) is AR1 Gaussian and f_r is
## a smooth for region r.
## 'rho' chosen to minimize AIC or (-ve) REML score. 

b &lt;- bam(T ~ region + s(time,k=20,bs="cr",by=region) +
         s(time,k=40,bs="cr",by=latitude),
         data=CanWeather,AR.start=time==1,rho=.97)

## Note: 'discrete==TRUE' option even faster.

par(mfrow=c(2,3))
plot(b,scale=0,scheme=1)
acf(b$std)

</code></pre>

<hr>
<h2 id='ch1'>Code for Chapter 1: Linear Models</h2><span id='topic+ch1'></span>

<h3>Description</h3>

<p>R code from Chapter 1 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch1.solutions">ch1.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 1.1.2
data(hubble)
hub.mod &lt;- lm(y ~ x - 1, data=hubble)
summary(hub.mod)
plot(fitted(hub.mod),residuals(hub.mod),xlab="fitted values",
     ylab="residuals")
hub.mod1 &lt;- lm(y ~ x - 1,data=hubble[-c(3,15),])
summary(hub.mod1)
plot(fitted(hub.mod1),residuals(hub.mod1),
     xlab="fitted values",ylab="residuals")
hubble.const &lt;- c(coef(hub.mod),coef(hub.mod1))/3.09e19
age &lt;- 1/hubble.const
age/(60^2*24*365)

## 1.1.3
cs.hubble &lt;- 163000000
t.stat &lt;- (coef(hub.mod1)-cs.hubble)/vcov(hub.mod1)[1,1]^0.5
pt(t.stat,df=21)*2 # 2 because of |T| in p-value defn.

sigb &lt;- summary(hub.mod1)$coefficients[2]
h.ci &lt;- coef(hub.mod1)+qt(c(0.025,0.975),df=21)*sigb
h.ci
h.ci &lt;- h.ci*60^2*24*365.25/3.09e19 # convert to 1/years
sort(1/h.ci)

## 1.5.1
data(sperm.comp1)
pairs(sperm.comp1[,-1])
sc.mod1 &lt;- lm(count~time.ipc+prop.partner,sperm.comp1)
model.matrix(sc.mod1)
par(mfrow=c(2,2))  # split the graphics device into 4 panels
plot(sc.mod1)      # (uses plot.lm as sc.mod1 is class `lm')
sperm.comp1[9,]
sc.mod1
sc.mod2 &lt;- lm(count~time.ipc+I(prop.partner*time.ipc),
              sperm.comp1)

## 1.5.2
summary(sc.mod1)

## 1.5.3
sc.mod3 &lt;- lm(count~prop.partner,sperm.comp1)
summary(sc.mod3)
sc.mod4 &lt;- lm(count~1,sperm.comp1) # null model
AIC(sc.mod1,sc.mod3,sc.mod4)

## 1.5.4
data(sperm.comp2)
sc2.mod1 &lt;- lm(count~f.age+f.height+f.weight+m.age+m.height+
               m.weight+m.vol,sperm.comp2)
plot(sc2.mod1)
summary(sc2.mod1)
sc2.mod2 &lt;- lm(count~f.age+f.height+f.weight+m.height+
               m.weight+m.vol,sperm.comp2)
summary(sc2.mod2)
sc2.mod7 &lt;- lm(count~f.weight,sperm.comp2)
summary(sc2.mod7)
sc &lt;- sperm.comp2[-19,]
sc3.mod1 &lt;- lm(count~f.age+f.height+f.weight+m.age+m.height+
               m.weight+m.vol,sc)
summary(sc3.mod1)
sperm.comp1$m.vol &lt;-
   sperm.comp2$m.vol[sperm.comp2$pair%in%sperm.comp1$subject]
sc1.mod1 &lt;- lm(count~m.vol,sperm.comp1)
summary(sc1.mod1)

## 1.5.5
sc.c &lt;- summary(sc1.mod1)$coefficients
sc.c   # check info extracted from summary
sc.c[2,1]+qt(c(.025,.975),6)*sc.c[2,2]

## 1.5.6
df &lt;- data.frame(m.vol=c(10,15,20,25))
predict(sc1.mod1,df,se=TRUE)

## 1.5.7
set.seed(1); n &lt;- 100; x &lt;- runif(n)
z &lt;- x + rnorm(n)*.05
y &lt;- 2 + 3 * x + rnorm(n)
summary(lm(y~z))
summary(lm(y~x+z))

## 1.6.4
z &lt;- c(1,1,1,2,2,1,3,3,3,3,4)
z
z &lt;- as.factor(z)
z
x &lt;- c("A","A","C","C","C","er","er")
x
x &lt;- factor(x)
x
PlantGrowth$group
# PlantGrowth$group &lt;- as.factor(PlantGrowth$group)
pgm.1 &lt;- lm(weight ~ group,data=PlantGrowth)
plot(pgm.1)
summary(pgm.1)
pgm.0 &lt;- lm(weight~1,data=PlantGrowth)
anova(pgm.0,pgm.1)

</code></pre>

<hr>
<h2 id='ch1.solutions'>Solution code for Chapter 1: Linear Models</h2><span id='topic+ch1.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 1 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch1">ch1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## Q.8 Rubber
## a)
library(MASS)
m1 &lt;- lm(loss~hard+tens+I(hard*tens)+I(hard^2)+I(tens^2)+
I(hard^2*tens)+I(tens^2*hard)+I(tens^3)+I(hard^3),Rubber)
plot(m1)    ## residuals OK
summary(m1) ## p-values =&gt; drop I(tens^2*hard)
m2 &lt;- update(m1,.~.-I(tens^2*hard))
summary(m2)
m3 &lt;- update(m2,.~.-hard)
summary(m3)
m4 &lt;- update(m3,.~.-1)
summary(m4)
m5 &lt;- update(m4,.~.-I(hard^2))
summary(m5) ## p-values =&gt; keep all remaining
plot(m5)    ## residuals OK

## b)
AIC(m1,m2,m3,m4,m5)
m6 &lt;- step(m1)

## c)
m &lt;- 40;attach(Rubber)
mt &lt;- seq(min(tens),max(tens),length=m)
mh &lt;- seq(min(hard),max(hard),length=m)
lp &lt;- predict(m6,data.frame(hard=rep(mh,rep(m,m)),
                            tens=rep(mt,m)))
contour(mt,mh,matrix(lp,m,m),xlab="tens",ylab="hard")
points(tens,hard)
detach(Rubber)

## Q.9 warpbreaks
wm &lt;- lm(breaks~wool*tension,warpbreaks)
par(mfrow=c(2,2))
plot(wm)    # residuals OK
anova(wm)
## ... so there is evidence for a wool:tension interaction.
par(mfrow=c(1,1))
with(warpbreaks,interaction.plot(tension,wool,breaks))

## Q.10 cars
## a)
cm1 &lt;- lm(dist ~ speed + I(speed^2),cars)
summary(cm1)
## Intercept has very high p-value, so drop it
cm2 &lt;- lm(dist ~ speed + I(speed^2)-1,cars)
summary(cm2)
## both terms now significant, but try the alternative of
## dropping `speed'
cm3 &lt;- lm(dist ~ I(speed^2),cars)
AIC(cm1,cm2,cm3)
plot(cm2)
# Clearly cm2, with speed and speed squared terms, is to be preferred,
# but note that variance seems to be increasing with mean a little:
# perhaps a GLM, better?

## b)
# In seconds, the answer is obtained as follows..
b &lt;- coef(cm2)
5280/(b[1]*60^2)
# This is a long time, but would have a rather wide associated confidence
# interval.

## Q.11 QR
# The following is a version of the function that you should end up with.

fitlm &lt;- function(y,X)
{ qrx &lt;- qr(X)                ## get QR decomposition
  y &lt;- qr.qty(qrx,y)          ## form Q'y efficiently
  R &lt;- qr.R(qrx)              ## extract R
  p &lt;- ncol(R);n &lt;- length(y) ## get dimensions
  f &lt;- y[1:p]; r &lt;- y[(p+1):n]## partition Q'y
  beta &lt;- backsolve(R,f)      ## parameter estimates (a)
  sig2 &lt;- sum(r^2)/(n-p)      ## resid variance estimate (c)
  Ri &lt;- backsolve(R,diag(ncol(R))) ## inverse of R matrix
  Vb &lt;- Ri%*%t(Ri)*sig2       ## covariance matrix
  se &lt;- diag(Vb)^.5           ## standard errors (c)
  F.ratio &lt;- f^2/sig2         ## sequential F-ratios
  seq.p.val &lt;- 1-pf(F.ratio,1,n-p) ## seq. p-values (e)
  list(beta=beta,se=se,sig2=sig2,seq.p.val=seq.p.val,df=n-p)
} ## fitlm

# The following code uses the function to answer some of the question parts.

## get example X ...
X &lt;- model.matrix(dist ~ speed + I(speed^2),cars)
cm &lt;- fitlm(cars$dist,X) # used fitting function
cm$beta;cm$se            # print estimates and s.e.s (a,c)
cm1&lt;-lm(dist ~ speed + I(speed^2),cars) # equiv. lm call
summary(cm1)             # check estimates and s.e.s (b,c)
t.ratio &lt;- cm$beta/cm$se # form t-ratios
p.val &lt;- pt(-abs(t.ratio),df=cm$df)*2
p.val                    # print evaluated p-values (d)
## print sequential ANOVA p-values, and check them (e)
cm$seq.p.val
anova(cm1)

## Q.12 InsectSprays
X &lt;- model.matrix(~spray-1,InsectSprays)
X &lt;- cbind(rep(1,nrow(X)),X)   # redundant model matrix
C &lt;- matrix(c(0,rep(1,6)),1,7) # constraints
qrc &lt;- qr(t(C))                # QR decomp. of C'
## use fact that Q=[D:Z] and XQ = (Q'X')' to form XZ ...
XZ &lt;- t(qr.qty(qrc,t(X)))[,2:7]
m1 &lt;- lm(InsectSprays$count~XZ-1) # fit model
bz &lt;- coef(m1) # estimates in constrained parameterization
## form b = Z b_z, using fact that Q=[D:Z], again
b &lt;- c(0,bz)
b &lt;- qr.qy(qrc,b)
sum(b[2:7])

## Q.13 trees
## a)
EV.func &lt;- function(b,g,h)
{ mu &lt;- b[1]*g^b[2]*h^b[3]
  J &lt;- cbind(g^b[2]*h^b[3],mu*log(g),mu*log(h))
  list(mu=mu,J=J)
}

## b)
attach(trees)
b &lt;- c(.002,2,1);b.old &lt;- 100*b+100
while (sum(abs(b-b.old))&gt;1e-7*sum(abs(b.old))) {
   EV &lt;- EV.func(b,Girth,Height)
   z &lt;- (Volume-EV$mu) + EV$J%*%b
   b.old &lt;- b
   b &lt;- coef(lm(z~EV$J-1))
}
b

## c)
sig2 &lt;- sum((Volume - EV$mu)^2)/(nrow(trees)-3)
Vb &lt;- solve(t(EV$J)%*%EV$J)*sig2
se &lt;- diag(Vb)^.5;se

</code></pre>

<hr>
<h2 id='ch2'>Code for Chapter 2: Linear Mixed Models</h2><span id='topic+ch2'></span>

<h3>Description</h3>

<p>R code from Chapter 2 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch2.solutions">ch2.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 2.1.1
data(stomata)
m1 &lt;- lm(area ~ CO2 + tree,stomata)
m0 &lt;- lm(area ~ CO2,stomata)
anova(m0,m1)
m2 &lt;- lm(area ~ tree,stomata)
anova(m2,m1)
st &lt;- aggregate(data.matrix(stomata),
                by=list(tree=stomata$tree),mean)
st$CO2 &lt;- as.factor(st$CO2);st
m3 &lt;- lm(area~CO2,st)
anova(m3)
summary(m3)$sigma^2 - summary(m1)$sigma^2/4

## 2.1.3
library(nlme)  # load nlme `library', which contains data
data(Rail)     # load data
Rail
m1 &lt;- lm(travel ~ Rail,Rail)
anova(m1)
rt &lt;- aggregate(data.matrix(Rail),by=list(Rail$Rail),mean)
rt
m0 &lt;- lm(travel ~ 1,rt)   # fit model to aggregated data
sigb &lt;- (summary(m0)$sigma^2-summary(m1)$sigma^2/3)^0.5
# sigb^2 is variance component for rail
sig &lt;- summary(m1)$sigma # sig^2 is resid. var. component
sigb
sig
summary(m0)

## 2.1.4
library(nlme)
data(Machines)
names(Machines)
attach(Machines)  # make data available without `Machines$'
interaction.plot(Machine,Worker,score)
m1 &lt;- lm(score ~ Worker*Machine,Machines)
m0 &lt;- lm(score ~ Worker + Machine,Machines)
anova(m0,m1)
summary(m1)$sigma^2
Mach &lt;- aggregate(data.matrix(Machines),by=
        list(Machines$Worker,Machines$Machine),mean)
Mach$Worker &lt;- as.factor(Mach$Worker)
Mach$Machine &lt;- as.factor(Mach$Machine)
m0 &lt;- lm(score ~ Worker + Machine,Mach)
anova(m0)
summary(m0)$sigma^2 - summary(m1)$sigma^2/3
M &lt;- aggregate(data.matrix(Mach),by=list(Mach$Worker),mean)
m00 &lt;- lm(score ~ 1,M)
summary(m00)$sigma^2 - (summary(m0)$sigma^2)/3

## 2.4.4
llm &lt;- function(theta,X,Z,y) {
  ## untransform parameters...
  sigma.b &lt;- exp(theta[1])
  sigma &lt;- exp(theta[2])
  ## extract dimensions...
  n &lt;- length(y); pr &lt;- ncol(Z); pf &lt;- ncol(X)
  ## obtain \hat \beta, \hat b...
  X1 &lt;- cbind(X,Z)
  ipsi &lt;- c(rep(0,pf),rep(1/sigma.b^2,pr))
  b1 &lt;- solve(crossprod(X1)/sigma^2+diag(ipsi),
              t(X1)%*%y/sigma^2)
  ## compute log|Z'Z/sigma^2 + I/sigma.b^2|...
  ldet &lt;- sum(log(diag(chol(crossprod(Z)/sigma^2 + 
              diag(ipsi[-(1:pf)])))))
  ## compute log profile likelihood...
  l &lt;- (-sum((y-X1%*%b1)^2)/sigma^2 - sum(b1^2*ipsi) - 
  n*log(sigma^2) - pr*log(sigma.b^2) - 2*ldet - n*log(2*pi))/2
  attr(l,"b") &lt;- as.numeric(b1) ## return \hat beta and \hat b
  -l 
}
library(nlme) ## for Rail data
options(contrasts=c("contr.treatment","contr.treatment"))
Z &lt;- model.matrix(~Rail$Rail-1) ## r.e. model matrix
X &lt;- matrix(1,18,1)             ## fixed model matrix
## fit the model...
rail.mod &lt;- optim(c(0,0),llm,hessian=TRUE,
                           X=X,Z=Z,y=Rail$travel)
exp(rail.mod$par) ## variance components
solve(rail.mod$hessian) ## approx cov matrix for theta 
attr(llm(rail.mod$par,X,Z,Rail$travel),"b")

## 2.5.1
library(nlme)
lme(travel~1,Rail,list(Rail=~1))

## 2.5.2

Loblolly$age &lt;- Loblolly$age - mean(Loblolly$age)
lmc &lt;- lmeControl(niterEM=500,msMaxIter=100)
m0 &lt;- lme(height ~ age + I(age^2) + I(age^3),Loblolly,
          random=list(Seed=~age+I(age^2)+I(age^3)),
          correlation=corAR1(form=~age|Seed),control=lmc)
plot(m0)
m1 &lt;- lme(height ~ age+I(age^2)+I(age^3)+I(age^4),Loblolly,
          list(Seed=~age+I(age^2)+I(age^3)),
          cor=corAR1(form=~age|Seed),control=lmc)
plot(m1)
m2 &lt;- lme(height~age+I(age^2)+I(age^3)+I(age^4)+I(age^5),
          Loblolly,list(Seed=~age+I(age^2)+I(age^3)),
          cor=corAR1(form=~age|Seed),control=lmc)
plot(m2)
plot(m2,Seed~resid(.))
qqnorm(m2,~resid(.))
qqnorm(m2,~ranef(.))

m3 &lt;- lme(height~age+I(age^2)+I(age^3)+I(age^4)+I(age^5),
          Loblolly,list(Seed=~age+I(age^2)+I(age^3)),control=lmc)
anova(m3,m2)
m4 &lt;- lme(height~age+I(age^2)+I(age^3)+I(age^4)+I(age^5),
          Loblolly,list(Seed=~age+I(age^2)),
          correlation=corAR1(form=~age|Seed),control=lmc)
anova(m4,m2)
m5 &lt;- lme(height~age+I(age^2)+I(age^3)+I(age^4)+I(age^5),
          Loblolly,list(Seed=pdDiag(~age+I(age^2)+I(age^3))),
          correlation=corAR1(form=~age|Seed),control=lmc)
anova(m2,m5)
plot(augPred(m2))

## 2.5.3
lme(score~Machine,Machines,list(Worker=~1,Machine=~1))

## 2.5.4
library(lme4)
a1 &lt;- lmer(score~Machine+(1|Worker)+(1|Worker:Machine),
           data=Machines)
a1
a2 &lt;- lmer(score~Machine+(1|Worker)+(Machine-1|Worker),
           data=Machines)
AIC(a1,a2)
anova(a1,a2)

## 2.5.5
library(mgcv)
b1 &lt;- gam(score~ Machine + s(Worker,bs="re") + 
      s(Machine,Worker,bs="re"),data=Machines,method="REML")
gam.vcomp(b1)
b2 &lt;- gam(score~ Machine + s(Worker,bs="re") + 
      s(Worker,bs="re",by=Machine),data=Machines,method="REML")
gam.vcomp(b2)

</code></pre>

<hr>
<h2 id='ch2.solutions'>Solution code for Chapter 2: Linear Mixed Models</h2><span id='topic+ch2.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 2 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch2">ch2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## Q.6
## c)
library(nlme)
options(contrasts=c("contr.treatment",
                    "contr.treatment"))
m1 &lt;- lme(Thickness~Site+Source,Oxide,~1|Lot/Wafer)
plot(m1)               # check resids vs. fitted vals
qqnorm(residuals(m1))  # check resids for normality
abline(0,sd(resid(m1)))# adding a "reference line"
qqnorm(m1,~ranef(.,level=1)) # check normality of b_k
qqnorm(m1,~ranef(.,level=2)) # check normality of c_(k)l
m2 &lt;- lme(Thickness~Site+Source,Oxide,~1|Lot)
anova(m1,m2)
anova(m1)
intervals(m1)

## Q.7

library(nlme)
attach(Machines)
interaction.plot(Machine,Worker,score) # note 6B
## base model
m1&lt;-lme(score~Machine,Machines,~1|Worker/Machine)
## check it...
plot(m1)
plot(m1,Machine~resid(.),abline=0)
plot(m1,Worker~resid(.),abline=0)
qqnorm(m1,~resid(.))
qqnorm(m1,~ranef(.,level=1))
qqnorm(m1,~ranef(.,level=2)) ## note outlier
## try more general r.e. structure
m2&lt;-lme(score~Machine,Machines,~Machine|Worker)
## check it...
qqnorm(m2,~resid(.))
qqnorm(m2,~ranef(.,level=1)) ## still an outlier
## simplified model...
m0 &lt;- lme(score~Machine,Machines,~1|Worker)
## formal comparison
anova(m0,m1,m2) ## m1 most appropriate
anova(m1)     ## significant Machine effect
intervals(m1) ## Machines B and C better than A
## remove problematic worker 6, machine B
Machines &lt;- Machines[-(34:36),]
## re-running improves plots, but conclusions same.

## It seems that (2.6) is the most appropriate model of those tried,
## and broadly the same conclusions are reached with or without
## worker 6, on Machine B, which causes outliers on several checking
## plots. See next question for comparison of machines B and C.

## Q.8
# Using the data without worker 6 machine B:

intervals(m1,level=1-0.05/3,which="fixed")
levels(Machines$Machine)
Machines$Machine &lt;- relevel(Machines$Machine,"B")
m1a &lt;- lme(score ~ Machine, Machines, ~ 1|Worker/Machine)
intervals(m1a,level=1-0.05/3,which="fixed")

# So, there is evidence for differences between machine A and the
# other 2, but not between B and C, at the 5% level. However,
# depending on how economically significant the point estimate of
# the B-C difference is, it might be worth conducting a study with
# more workers in order to check whether the possible small
# difference might actually be real.

## Q.9
## c)
library(nlme);data(Gun)
options(contrasts=c("contr.treatment","contr.treatment"))
with(Gun,plot(Method,rounds))
with(Gun,plot(Physique,rounds))
m1 &lt;- lme(rounds~Method+Physique,Gun,~1|Team)
plot(m1)              # fitted vs. resid plot
qqnorm(residuals(m1))
abline(0,m1$sigma)  # add line of "perfect Normality"
anova(m1)  # balanced data: don't need type="marginal"
m2 &lt;- lme(rounds~Method,Gun,~1|Team)
intervals(m2)
</code></pre>

<hr>
<h2 id='ch3'>Code for Chapter 3: Generalized Linear Models</h2><span id='topic+ch3'></span>

<h3>Description</h3>

<p>R code from Chapter 3 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch3.solutions">ch3.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 3.2.2
x &lt;- c(.6,1.5); y &lt;- c(.02,.9)
ms &lt;- exp(-x*4)   # set initial values at lower left
glm(y ~ I(-x)-1,family=gaussian(link=log),mustart=ms)
ms &lt;- exp(-x*0.1)  # set initial values at upper right
glm(y ~ I(-x)-1,family=gaussian(link=log),mustart=ms)

## 3.3.1

heart &lt;- data.frame(ck = 0:11*40+20,
ha=c(2,13,30,30,21,19,18,13,19,15,7,8),
ok=c(88,26,8,5,0,1,1,1,1,0,0,0))

p &lt;- heart$ha/(heart$ha+heart$ok)
plot(heart$ck,p,xlab="Creatinine kinase level",
     ylab="Proportion Heart Attack")
mod.0 &lt;- glm(cbind(ha,ok) ~ ck, family=binomial(link=logit),
             data=heart)
mod.0 &lt;- glm(cbind(ha,ok) ~ ck, family=binomial, data=heart)
mod.0
(271.7-36.93)/271.7
1-pchisq(36.93,10)
par(mfrow=c(2,2))
plot(mod.0)
plot(heart$ck, p, xlab="Creatinine kinase level",
     ylab="Proportion Heart Attack")
lines(heart$ck, fitted(mod.0))
mod.2 &lt;- glm(cbind(ha,ok)~ck+I(ck^2)+I(ck^3),family=binomial,
             data=heart)
mod.2
par(mfrow=c(2,2))
plot(mod.2)
par(mfrow=c(1,1))
plot(heart$ck,p,xlab="Creatinine kinase level",
     ylab="Proportion Heart Attack")
lines(heart$ck,fitted(mod.2))
anova(mod.0,mod.2,test="Chisq")

## 3.3.2
y &lt;- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t &lt;- 1:13
plot(t+1980,y,xlab="Year",ylab="New AIDS cases",ylim=c(0,280))
m0 &lt;- glm(y~t,poisson)
m0
par(mfrow=c(2,2))
plot(m0)
m1 &lt;- glm(y~t+I(t^2),poisson)
plot(m1)
summary(m1)
anova(m0,m1,test="Chisq")
beta.1 &lt;- summary(m1)$coefficients[2,]
ci &lt;- c(beta.1[1]-1.96*beta.1[2],beta.1[1]+1.96*beta.1[2])
ci ## print 95% CI for beta_1
new.t &lt;- seq(1,13,length=100)
fv &lt;- predict(m1,data.frame(t=new.t),se=TRUE)
par(mfrow=c(1,1))
plot(t+1980,y,xlab="Year",ylab="New AIDS cases",ylim=c(0,280))
lines(new.t+1980,exp(fv$fit))
lines(new.t+1980,exp(fv$fit+2*fv$se.fit),lty=2)
lines(new.t+1980,exp(fv$fit-2*fv$se.fit),lty=2)

## 3.3.3
psurv &lt;- function(surv,time="t",censor="d",event="z") {
## create data frame to fit Cox PH as Poisson model.
## surv[[censor]] should be 1 for event or zero for censored.   
  if (event %in% names(surv)) warning("event name clashes")
  surv &lt;- as.data.frame(surv)[order(surv[[time]]),] # t order
  et &lt;- unique(surv[[time]][surv[[censor]]==1]) # unique times 
  es &lt;- match(et,surv[[time]]) # starts of risk sets in surv
  n &lt;- nrow(surv); t &lt;- rep(et,1+n-es) # times for risk sets
  st &lt;- cbind(0,
     surv[unlist(apply(matrix(es),1,function(x,n) x:n,n=n)),])
  st[st[[time]]==t&amp;st[[censor]]!=0,1] &lt;- 1 # signal events 
  st[[time]] &lt;- t ## reset event time to risk set time
  names(st)[1] &lt;- event
  st
} ## psurv

require(gamair); data(bone); bone$id &lt;- 1:nrow(bone)
pb &lt;- psurv(bone); pb$tf &lt;- factor(pb$t)
b &lt;- glm(z ~ tf + trt - 1,poisson,pb)

chaz &lt;- tapply(fitted(b),pb$id,sum) ## by subject cum. hazard
mrsd &lt;- bone$d - chaz ## Martingale residuals

drop1(b,test="Chisq") ## test for effect - no evidence

te &lt;- sort(unique(bone$t[bone$d==1])) ## event times
## predict survivor function for "allo"...
pd &lt;- data.frame(tf=factor(te),trt=bone$trt[1])
fv &lt;- predict(b,pd)
H &lt;- cumsum(exp(fv)) ## cumulative hazard
plot(stepfun(te,c(1,exp(-H))),do.points=FALSE,ylim=c(0,1),
     xlim=c(0,550),main="",ylab="S(t)",xlab="t (days)")
## add s.e. bands...     
X &lt;- model.matrix(~tf+trt-1,pd)
J &lt;- apply(exp(fv)*X,2,cumsum)
se &lt;- diag(J%*%vcov(b)%*%t(J))^.5
lines(stepfun(te,c(1,exp(-H+se))),do.points=FALSE,lty=2)
lines(stepfun(te,c(1,exp(-H-se))),do.points=FALSE,lty=2)

## 3.3.4
al &lt;- data.frame(y=c(435,147,375,134),gender=
   as.factor(c("F","F","M","M")),faith=as.factor(c(1,0,1,0)))
al
mod.0 &lt;- glm(y ~ gender + faith, data=al, family=poisson)
model.matrix(mod.0)
mod.0
fitted(mod.0)
mod.1 &lt;- glm(y~gender*faith,data=al,family=poisson)
model.matrix(mod.1)
mod.1
anova(mod.0,mod.1,test="Chisq")

## 3.3.5
data(sole)
sole$off &lt;- log(sole$a.1-sole$a.0)# model offset term
sole$a&lt;-(sole$a.1+sole$a.0)/2     # mean stage age
solr&lt;-sole                        # make copy for rescaling
solr$t&lt;-solr$t-mean(sole$t)
solr$t&lt;-solr$t/var(sole$t)^0.5
solr$la&lt;-solr$la-mean(sole$la)
solr$lo&lt;-solr$lo-mean(sole$lo)
b &lt;- glm(eggs ~ offset(off)+lo+la+t+I(lo*la)+I(lo^2)+I(la^2)
          +I(t^2)+I(lo*t)+I(la*t)+I(lo^3)+I(la^3)+I(t^3)+
          I(lo*la*t)+I(lo^2*la)+I(lo*la^2)+I(lo^2*t)+
          I(la^2*t)+I(la*t^2)+I(lo*t^2)+ a +I(a*t)+I(t^2*a),
          family=quasi(link=log,variance="mu"),data=solr)
summary(b)
b1 &lt;- update(b, ~ . - I(lo*t))
b4 &lt;- update(b1, ~ . - I(lo*la*t) - I(lo*t^2) - I(lo^2*t))
anova(b,b4,test="F")
par(mfrow=c(1,2)) # split graph window into 2 panels
plot(fitted(b4)^0.5,solr$eggs^0.5) # fitted vs. data plot
plot(fitted(b4)^0.5,residuals(b4)) # resids vs. sqrt(fitted)

## 3.5.1
rf &lt;- residuals(b4,type="d") # extract deviance residuals
## create an identifier for each sampling station
solr$station &lt;- factor(with(solr,paste(-la,-lo,-t,sep="")))
## is there evidence of a station effect in the residuals?
solr$rf &lt;-rf
rm &lt;- lme(rf~1,solr,random=~1|station)
rm0 &lt;- lm(rf~1,solr)
anova(rm,rm0)
## following is slow...
## Not run: 
library(MASS)
form &lt;- eggs ~ offset(off)+lo+la+t+I(lo*la)+I(lo^2)+
            I(la^2)+I(t^2)+I(lo*t)+I(la*t)+I(lo^3)+I(la^3)+
            I(t^3)+I(lo*la*t)+I(lo^2*la)+I(lo*la^2)+I(lo^2*t)+
            I(la^2*t)+I(la*t^2)+I(lo*t^2)+ # end log spawn
            a +I(a*t)+I(t^2*a)
b &lt;- glmmPQL(form,random=list(station=~1),
            family=quasi(link=log,variance="mu"),data=solr)

summary(b)

form4 &lt;- eggs ~ offset(off)+lo+la+t+I(lo*la)+I(lo^2)+
            I(la^2)+I(t^2)+I(lo*t)+I(la*t)+I(lo^3)+I(la^3)+
            I(t^3)+I(lo^2*la)+I(lo*la^2)+
            I(la^2*t)+I(lo*t^2)+ # end log spawn
            a +I(a*t)+I(t^2*a)

b4 &lt;- glmmPQL(form4,random=list(station=~1),
            family=quasi(link=log,variance="mu"),data=solr)

fv &lt;- exp(fitted(b4)+solr$off) # note need to add offset
resid &lt;- solr$egg-fv          # raw residuals
plot(fv^.5,solr$eggs^.5)
abline(0,1,lwd=2)
plot(fv^.5,resid/fv^.5)
plot(fv^.5,resid)
fl&lt;-sort(fv^.5)
## add 1 s.d. and 2 s.d. reference lines
lines(fl,fl);lines(fl,-fl);lines(fl,2*fl,lty=2)
lines(fl,-2*fl,lty=2)

intervals(b4,which="var-cov")

## 3.5.2

form5 &lt;- eggs ~ offset(off)+lo+la+t+I(lo*la)+I(lo^2)+
            I(la^2)+I(t^2)+I(lo*t)+I(la*t)+I(lo^3)+I(la^3)+
            I(t^3)+I(lo^2*la)+I(lo*la^2)+
            I(la^2*t)+I(lo*t^2)+ # end log spawn
            a +I(a*t)+I(t^2*a) + s(station,bs="re")

b &lt;- gam(form5,family=quasi(link=log,variance="mu"),data=solr,
         method="REML")

## 3.5.3
library(lme4)
solr$egg1 &lt;- round(solr$egg * 5)
form &lt;- egg1 ~ offset(off)+lo+la+t+I(lo*la)+I(lo^2)+
            I(la^2)+I(t^2)+I(lo*t)+I(la*t)+I(lo^3)+I(la^3)+
            I(t^3)+I(lo*la*t)+I(lo^2*la)+I(lo*la^2)+I(lo^2*t)+
            I(la^2*t)+I(la*t^2)+I(lo*t^2)+ # end log spawn
            a +I(a*t)+I(t^2*a) + (1|station)

glmer(form,family=poisson,data=solr)

## End(Not run)
</code></pre>

<hr>
<h2 id='ch3.solutions'>Solution code for Chapter 3: Generalized Linear Models</h2><span id='topic+ch3.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 3 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch3">ch3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## Q.2 Residuals

n &lt;- 100; m &lt;- 10
x &lt;- runif(n)
lp &lt;- 3*x-1
mu &lt;- binomial()$linkinv(lp)
y &lt;- rbinom(1:n,m,mu)
par(mfrow=c(2,2))
plot(glm(y/m ~ x,family=binomial,weights=rep(m,n)))

## example glm fit...
b &lt;- glm(y/m ~ x,family=binomial,weights=rep(m,n))
reps &lt;- 200;mu &lt;- fitted(b)
rsd &lt;- matrix(0,reps,n) # array for simulated resids
runs &lt;- rep(0,reps)  # array for simulated run counts
for (i in 1:reps) {  # simulation loop
  ys &lt;- rbinom(1:n,m,mu) # simulate from fitted model
  ## refit model to simulated data
  br &lt;- glm(ys/m ~ x,family=binomial,weights=rep(m,n))
  rs &lt;- residuals(br) # simulated resids (meet assumptions)
  rsd[i,] &lt;- sort(rs) # store sorted residuals
  fv.sort &lt;- sort(fitted(br),index.return=TRUE)
  rs &lt;- rs[fv.sort$ix] # order resids by sorted fit values
  rs &lt;- rs &gt; 0         # check runs of +ve, -ve resids
  runs[i] &lt;- sum(rs[1:(n-1)]!=rs[2:n])
}
# plot original ordered residuals, and simulation envelope
for (i in 1:n) rsd[,i] &lt;- sort(rsd[,i])
par(mfrow=c(1,1))
plot(sort(residuals(b)),(1:n-.5)/n) # original
## plot 95% envelope ....
lines(rsd[5,],(1:n-.5)/n);lines(rsd[reps-5,],(1:n-.5)/n)

# compare original runs to distribution under independence
rs &lt;- residuals(b)
fv.sort &lt;- sort(fitted(b),index.return=TRUE)
rs &lt;- rs[fv.sort$ix]
rs &lt;- rs &gt; 0
obs.runs &lt;- sum(rs[1:(n-1)]!=rs[2:n])
sum(runs&gt;obs.runs)

## Q.3 Death penalty

## read in data...
count &lt;- c(53,414,11,37,0,16,4,139)
death &lt;- factor(c(1,0,1,0,1,0,1,0))
defendant &lt;- factor(c(0,0,1,1,0,0,1,1))
victim &lt;- factor(c(0,0,0,0,1,1,1,1))
levels(death) &lt;- c("no","yes")
levels(defendant) &lt;- c("white","black")
levels(victim) &lt;- c("white","black")

## a)
sum(count[death=="yes"&amp;defendant=="black"])/
    sum(count[defendant=="black"])
sum(count[death=="yes"&amp;defendant=="white"])/
    sum(count[defendant=="white"])

## b)
dm &lt;- glm(count~death*victim+death*defendant+
          victim*defendant,family=poisson(link=log))
summary(dm)
dm0 &lt;- glm(count~death*victim+victim*defendant,
           family=poisson(link=log))
anova(dm0,dm,test="Chisq")

## Q.7 IRLS
y &lt;- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t &lt;-1:13
X &lt;- cbind(rep(1,13),t,t^2) # model matrix
mu &lt;- y;eta &lt;- log(mu) # initial values
ok &lt;- TRUE
while (ok) {
 ## evaluate pseudodata and weights
 z &lt;- (y-mu)/mu + eta
 w &lt;- as.numeric(mu)
 ## fit weighted working linear model
 z &lt;- sqrt(w)*z; WX &lt;- sqrt(w)*X
 beta &lt;- coef(lm(z~WX-1))
 ## evaluate new eta and mu
 eta.old &lt;- eta
 eta &lt;- X%*%beta
 mu &lt;- exp(eta)
 ## test for convergence...
 if (max(abs(eta-eta.old))&lt;1e-7*max(abs(eta))) ok &lt;- FALSE
}
plot(t,y);lines(t,mu) # plot fit

## Q.8
## b)
data(harrier)
m &lt;- 1
b &lt;- glm(Consumption.Rate~I(1/Grouse.Density^m),
     family=quasi(link=inverse,variance=mu),data=harrier)

## c)
plot(harrier$Grouse.Density,residuals(b))
## clear pattern if $m=1$, and the parameter estimates lead to a rather odd curve.

## d)
## search leads to...
m &lt;- 3.25
b &lt;- glm(Consumption.Rate~I(1/Grouse.Density^m),
     family=quasi(link=inverse,variance=mu),data=harrier)

## e)
pd &lt;- data.frame(Grouse.Density = seq(0,130,length=200))
pr &lt;- predict(b,newdata=pd,se=TRUE)
with(harrier,plot(Grouse.Density,Consumption.Rate))
lines(pd$Grouse.Density,1/pr$fit,col=2)
lines(pd$Grouse.Density,1/(pr$fit-pr$se*2),col=3)
lines(pd$Grouse.Density,1/(pr$fit+pr$se*2),col=3)

## f)
ll &lt;- function(b,cr,d)
## evalates -ve quasi-log likelihood of model
## b is parameters, cr is consumption, d is density
{ ## get expected consumption...
  dm &lt;- d^b[3]
  Ec &lt;- exp(b[1])*dm/(1+exp(b[1])*exp(b[2])*dm)
  ## appropriate quasi-likelihood...
  ind &lt;- cr&gt;0    ## have to deal with cr==0 case
  ql &lt;- cr - Ec
  ql[ind] &lt;- ql[ind] + cr[ind]*log(Ec[ind]/cr[ind])
  -sum(ql)
}
## Now fit model ...
fit &lt;- optim(c(log(.4),log(10),3),ll,method="L-BFGS-B",
             hessian=TRUE,cr=harrier$Consumption.Rate,
             d=harrier$Grouse.Density)
## and plot results ...
b &lt;- fit$par
d &lt;- seq(0,130,length=200); dm &lt;- d^b[3]
Ec &lt;- exp(b[1])*dm/(1+exp(b[1])*exp(b[2])*dm)
with(harrier,plot(Grouse.Density,Consumption.Rate))
lines(d,Ec,col=2)

## Q.9
death &lt;- as.numeric(ldeaths)
month &lt;- rep(1:12,6)
time &lt;- 1:72
ldm &lt;- glm(death ~ sin(month/12*2*pi)+cos(month/12*2*pi),
           family=poisson(link=identity))
plot(time,death,type="l");lines(time,fitted(ldm),col=2)
summary(ldm)
plot(ldm)

## Q.10
y &lt;- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t &lt;- 1:13
b &lt;- glm(y ~ t + I(t^2),family=poisson)
log.lik &lt;- b1 &lt;- seq(.4,.7,length=100)
for (i in 1:100)
{ log.lik[i] &lt;- logLik(glm(y~offset(b1[i]*t)+I(t^2),
                           family=poisson))
}
plot(b1,log.lik,type="l")
points(coef(b)[2],logLik(b),pch=19)
abline(logLik(b)[1]-qchisq(.95,df=1),0,lty=2)

## Q.11 Soybean
## a)
library(nlme)
attach(Soybean)
lmc &lt;- lmeControl(niterEM=300) ## needed for convergence
m1&lt;-lme(weight~Variety*Time+Variety*I(Time^2)+
        Variety*I(Time^3),Soybean,~Time|Plot,control=lmc)
plot(m1) ## clear increasing variance with mean

## b)
library(MASS)
m2&lt;-glmmPQL(weight~Variety*Time+Variety*I(Time^2)+
    Variety*I(Time^3),data=Soybean,random=~Time|Plot,
    family=Gamma(link=log),control=lmc)
plot(m2) ## much better

## c)
m0&lt;-glmmPQL(weight~Variety*Time+Variety*I(Time^2)+
    Variety*I(Time^3),data=Soybean,random=~1|Plot,
    family=Gamma(link=log),control=lmc) # simpler r.e.'s
m3&lt;-glmmPQL(weight~Variety*Time+Variety*I(Time^2)+
    Variety*I(Time^3),data=Soybean,random=~Time+
    I(Time^2)|Plot,family=Gamma(link=log),control=lmc)
## ... m3 has more complex r.e. structure
## Following not strictly valid, but gives a rough
## guide. Suggests m2 is best...
AIC(m0,m2,m3)
summary(m2) ## drop Variety:Time
m4&lt;-glmmPQL(weight~Variety+Time+Variety*I(Time^2)+
    Variety*I(Time^3),data=Soybean,random=~Time|Plot,
    family=Gamma(link=log),control=lmc)
summary(m4) ## perhaps drop Variety:I(Time^3)?
m5&lt;-glmmPQL(weight~Variety+Time+Variety*I(Time^2)+
    I(Time^3),data=Soybean,random=~Time|Plot,
    family=Gamma(link=log),control=lmc)
summary(m5) ## don't drop any more
AIC(m2,m4,m5) ## supports m4
intervals(m5,which="fixed")

## So m4 or m5 are probably the best models to use, and
## both suggest that variety P has a higher weight on average.
</code></pre>

<hr>
<h2 id='ch4'>Code for Chapter 4: Introducing GAMs</h2><span id='topic+ch4'></span>

<h3>Description</h3>

<p>R code from Chapter 4 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch4.solutions">ch4.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 4.2.1
data(engine); attach(engine)

plot(size,wear,xlab="Engine capacity",ylab="Wear index")

tf &lt;- function(x,xj,j) {
## generate jth tent function from set defined by knots xj
  dj &lt;- xj*0;dj[j] &lt;- 1
  approx(xj,dj,x)$y
}

tf.X &lt;- function(x,xj) {
## tent function basis matrix given data x
## and knot sequence xj
  nk &lt;- length(xj); n &lt;- length(x)
  X &lt;- matrix(NA,n,nk)
  for (j in 1:nk) X[,j] &lt;- tf(x,xj,j)
  X
}

sj &lt;- seq(min(size),max(size),length=6) ## generate knots
X &lt;- tf.X(size,sj)                      ## get model matrix
b &lt;- lm(wear~X-1)                       ## fit model
s &lt;- seq(min(size),max(size),length=200)## prediction data
Xp &lt;- tf.X(s,sj)                        ## prediction matrix
plot(size,wear)                         ## plot data 
lines(s,Xp%*%coef(b))                   ## overlay estimated f

## 4.2.2
prs.fit &lt;- function(y,x,xj,sp) {
  X &lt;- tf.X(x,xj)       ## model matrix
  D &lt;- diff(diag(length(xj)),differences=2) ## sqrt penalty
  X &lt;- rbind(X,sqrt(sp)*D) ## augmented model matrix
  y &lt;- c(y,rep(0,nrow(D))) ## augmented data
  lm(y~X-1)   ## penalized least squares fit
}

sj &lt;- seq(min(size),max(size),length=20) ## knots
b &lt;- prs.fit(wear,size,sj,2) ## penalized fit
plot(size,wear)   ## plot data
Xp &lt;- tf.X(s,sj)  ## prediction matrix
lines(s,Xp%*%coef(b)) ## plot the smooth 

## 4.2.3
rho = seq(-9,11,length=90)
n &lt;- length(wear)
V &lt;- rep(NA,90) 
for (i in 1:90) { ## loop through smoothing params
  b &lt;- prs.fit(wear,size,sj,exp(rho[i])) ## fit model
  trF &lt;- sum(influence(b)$hat[1:n])      ## extract EDF
  rss &lt;- sum((wear-fitted(b)[1:n])^2)    ## residual SS
  V[i] &lt;- n*rss/(n-trF)^2                ## GCV score
}

plot(rho,V,type="l",xlab=expression(log(lambda)),
                    main="GCV score")
sp &lt;- exp(rho[V==min(V)])     ## extract optimal sp
b &lt;- prs.fit(wear,size,sj,sp) ## re-fit
plot(size,wear,main="GCV optimal fit")
lines(s,Xp%*%coef(b))

## 4.2.3 mixed model connection

## copy of llm from 2.2.4...
llm &lt;- function(theta,X,Z,y) {
  ## untransform parameters...
  sigma.b &lt;- exp(theta[1])
  sigma &lt;- exp(theta[2])
  ## extract dimensions...
  n &lt;- length(y); pr &lt;- ncol(Z); pf &lt;- ncol(X)
  ## obtain \hat \beta, \hat b...
  X1 &lt;- cbind(X,Z)
  ipsi &lt;- c(rep(0,pf),rep(1/sigma.b^2,pr))
  b1 &lt;- solve(crossprod(X1)/sigma^2+diag(ipsi),
              t(X1)%*%y/sigma^2)
  ## compute log|Z'Z/sigma^2 + I/sigma.b^2|...
  ldet &lt;- sum(log(diag(chol(crossprod(Z)/sigma^2 + 
              diag(ipsi[-(1:pf)])))))
  ## compute log profile likelihood...
  l &lt;- (-sum((y-X1%*%b1)^2)/sigma^2 - sum(b1^2*ipsi) - 
  n*log(sigma^2) - pr*log(sigma.b^2) - 2*ldet - n*log(2*pi))/2
  attr(l,"b") &lt;- as.numeric(b1) ## return \hat beta and \hat b
  -l 
}

X0 &lt;- tf.X(size,sj)            ## X in original parameterization
D &lt;- rbind(0,0,diff(diag(20),difference=2)) 
diag(D) &lt;- 1                   ## augmented D
X &lt;- t(backsolve(t(D),t(X0)))  ## re-parameterized X
Z &lt;- X[,-c(1,2)]; X &lt;- X[,1:2] ## mixed model matrices
## estimate smoothing and variance parameters...
m &lt;- optim(c(0,0),llm,method="BFGS",X=X,Z=Z,y=wear)
b &lt;- attr(llm(m$par,X,Z,wear),"b") ## extract coefficients
## plot results...
plot(size,wear)
Xp1 &lt;- t(backsolve(t(D),t(Xp))) ## re-parameterized pred. mat.
lines(s,Xp1%*%as.numeric(b),col="grey",lwd=2)

library(nlme)
g &lt;- factor(rep(1,nrow(X)))        ## dummy factor
m &lt;- lme(wear~X-1,random=list(g=pdIdent(~Z-1)))
lines(s,Xp1%*%as.numeric(coef(m))) ## and to plot


## 4.3.1 Additive

tf.XD &lt;- function(x,xk,cmx=NULL,m=2) {
## get X and D subject to constraint
  nk &lt;- length(xk)
  X &lt;- tf.X(x,xk)[,-nk]                   ## basis matrix
  D &lt;- diff(diag(nk),differences=m)[,-nk] ## root penalty
  if (is.null(cmx)) cmx &lt;- colMeans(X)
  X &lt;- sweep(X,2,cmx)        ## subtract cmx from columns
  list(X=X,D=D,cmx=cmx)
} ## tf.XD

am.fit &lt;- function(y,x,v,sp,k=10) {
  ## setup bases and penalties...
  xk &lt;- seq(min(x),max(x),length=k)
  xdx &lt;- tf.XD(x,xk)
  vk &lt;- seq(min(v),max(v),length=k)
  xdv &lt;- tf.XD(v,vk)
  ## create augmented model matrix and response...
  nD &lt;- nrow(xdx$D)*2
  sp &lt;- sqrt(sp)
  X &lt;- cbind(c(rep(1,nrow(xdx$X)),rep(0,nD)),
             rbind(xdx$X,sp[1]*xdx$D,xdv$D*0),
             rbind(xdv$X,xdx$D*0,sp[2]*xdv$D))  
  y1 &lt;- c(y,rep(0,nD))
  ## fit model..
  b &lt;- lm(y1~X-1)
  ## compute some useful quantities...
  n &lt;- length(y)
  trA &lt;- sum(influence(b)$hat[1:n]) ## EDF
  rsd &lt;- y-fitted(b)[1:n] ## residuals
  rss &lt;- sum(rsd^2)       ## residual SS
  sig.hat &lt;- rss/(n-trA)       ## residual variance
  gcv &lt;- sig.hat*n/(n-trA)     ## GCV score
  Vb &lt;- vcov(b)*sig.hat/summary(b)$sigma^2 ## coeff cov matrix
  ## return fitted model...
  list(b=coef(b),Vb=Vb,edf=trA,gcv=gcv,fitted=fitted(b)[1:n],
       rsd=rsd,xk=list(xk,vk),cmx=list(xdx$cmx,xdv$cmx))
} ## am.fit

am.gcv &lt;- function(lsp,y,x,v,k) {
## function suitable for GCV optimization by optim 
  am.fit(y,x,v,exp(lsp),k)$gcv
}

## find GCV optimal smoothing parameters... 
fit &lt;- optim(c(0,0), am.gcv, y=trees$Volume, x=trees$Girth,
             v=trees$Height,k=10)
sp &lt;- exp(fit$par) ## best fit smoothing parameters
## Get fit at GCV optimal smoothing parameters...
fit &lt;- am.fit(trees$Volume,trees$Girth,trees$Height,sp,k=10)

am.plot &lt;- function(fit,xlab,ylab) {
## produces effect plots for simple 2 term 
## additive model 
  start &lt;- 2 ## where smooth coeffs start in beta
  for (i in 1:2) {
    ## sequence of values at which to predict...
    x &lt;- seq(min(fit$xk[[i]]),max(fit$xk[[i]]),length=200)
    ## get prediction matrix for this smooth...
    Xp &lt;- tf.XD(x,fit$xk[[i]],fit$cmx[[i]])$X 
    ## extract coefficients and cov matrix for this smooth
    stop &lt;- start + ncol(Xp)-1; ind &lt;- start:stop
    b &lt;- fit$b[ind];Vb &lt;- fit$Vb[ind,ind]
    ## values for smooth at x...
    fv &lt;- Xp%*%b
    ## standard errors of smooth at x....
    se &lt;- rowSums((Xp%*%Vb)*Xp)^.5
    ## 2 s.e. limits for smooth...
    ul &lt;- fv + 2*se;ll &lt;- fv - 2 * se
    ## plot smooth and limits...
    plot(x,fv,type="l",ylim=range(c(ul,ll)),xlab=xlab[i],
         ylab=ylab[i])
    lines(x,ul,lty=2);lines(x,ll,lty=2)
    start &lt;- stop + 1
  }
} ## am.plot

par(mfrow=c(1,3))
plot(fit$fitted,trees$Vol,xlab="fitted volume ",
     ylab="observed volume")
am.plot(fit,xlab=c("Girth","Height"),
        ylab=c("s(Girth)","s(Height)"))

## 4.4 Generalized additive

gam.fit &lt;- function(y,x,v,sp,k=10) {
## gamma error log link 2 term gam fit...
  eta &lt;- log(y) ## get initial eta
  not.converged &lt;- TRUE
  old.gcv &lt;- -100 ## don't converge immediately
  while (not.converged) {
    mu &lt;- exp(eta)  ## current mu estimate  
    z &lt;- (y - mu)/mu + eta ## pseudodata
    fit &lt;- am.fit(z,x,v,sp,k) ## penalized least squares
    if (abs(fit$gcv-old.gcv)&lt;1e-5*fit$gcv) { 
      not.converged &lt;- FALSE
    }
    old.gcv &lt;- fit$gcv 
    eta &lt;- fit$fitted ## updated eta
  }
  fit$fitted &lt;- exp(fit$fitted) ## mu
  fit
} ## gam.fit

gam.gcv &lt;- function(lsp,y,x,v,k=10) {
  gam.fit(y,x,v,exp(lsp),k=k)$gcv
}

fit &lt;- optim(c(0,0),gam.gcv,y=trees$Volume,x=trees$Girth,
             v=trees$Height,k=10)
sp &lt;- exp(fit$par)
fit &lt;- gam.fit(trees$Volume,trees$Girth,trees$Height,sp)
par(mfrow=c(1,3))
plot(fit$fitted,trees$Vol,xlab="fitted volume ",
     ylab="observed volume")
am.plot(fit,xlab=c("Girth","Height"),
        ylab=c("s(Girth)","s(Height)"))

## 4.6 mgcv

library(mgcv)   ## load the package
library(gamair) ## load the data package
data(trees)
ct1 &lt;- gam(Volume~s(Height)+s(Girth),
           family=Gamma(link=log),data=trees)
ct1
plot(ct1,residuals=TRUE)

## 4.6.1
ct2 &lt;- gam(Volume~s(Height,bs="cr")+s(Girth,bs="cr"),
           family=Gamma(link=log),data=trees)
ct2
ct3 &lt;- gam(Volume ~ s(Height) + s(Girth,bs="cr",k=20),
           family=Gamma(link=log),data=trees)
ct3

ct4 &lt;- gam(Volume ~ s(Height) + s(Girth),
           family=Gamma(link=log),data=trees,gamma=1.4)
ct4
plot(ct4,residuals=TRUE)

## 4.6.2

ct5 &lt;- gam(Volume ~ s(Height,Girth,k=25),
           family=Gamma(link=log),data=trees)
ct5
plot(ct5,too.far=0.15)
ct6 &lt;- gam(Volume ~ te(Height,Girth,k=5),
           family=Gamma(link=log),data=trees)
ct6
plot(ct6,too.far=0.15)

## 4.6.3

gam(Volume~Height+s(Girth),family=Gamma(link=log),data=trees)

trees$Hclass &lt;- factor(floor(trees$Height/10)-5,
                labels=c("small","medium","large"))

ct7 &lt;- gam(Volume ~ Hclass+s(Girth),
           family=Gamma(link=log),data=trees)
par(mfrow=c(1,2))
plot(ct7,all.terms=TRUE)
anova(ct7)
AIC(ct7)
summary(ct7)

</code></pre>

<hr>
<h2 id='ch4.solutions'>Solution code for Chapter 4: Introducing GAMs</h2><span id='topic+ch4.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 4 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch4">ch4</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)
## Q.1
set.seed(1)
x &lt;- sort(runif(40)*10)^.5
y &lt;- sort(runif(40))^0.1
## polynomial fits ...
xx &lt;- seq(min(x),max(x),length=200)
plot(x,y)
b&lt;-lm(y~poly(x,5))
lines(xx,predict(b,data.frame(x=xx)))
b&lt;-lm(y~poly(x,10))
lines(xx,predict(b,data.frame(x=xx)),col=2)
## spline fits ...
sb &lt;- function(x,xk) { abs(x-xk)^3}
q&lt;-11
xk&lt;-((1:(q-2)/(q-1))*10)^.5
## lazy person's formula construction ...
form&lt;-paste("sb(x,xk[",1:(q-2),"])",sep="",collapse="+")
form &lt;- paste("y~x+",form)
b&lt;-lm(formula(form))
lines(xx,predict(b,data.frame(x=xx)),col=3)

## Q.2
## x,y, and xx from previous question
b1 &lt;- lm(form)
plot(x,y)
lines(xx,predict(b1,data.frame(x=xx)),col=4)
X &lt;- model.matrix(b1)   # extract model matrix
beta &lt;- solve(t(X)%*%X,t(X)%*%y,tol=0)
b1$coefficients &lt;- beta # trick for simple prediction
lines(xx,predict(b1,data.frame(x=xx)),col=5)
## ... upping the basis dimension to 11 makes the
## normal equations estimates perform very badly.

## Q.8 Additive model as a mixed model

## from 4.2.1 and 4.3.1...
tf &lt;- function(x,xj,j) {
## generate jth tent function from set defined by knots xj
  dj &lt;- xj*0;dj[j] &lt;- 1
  approx(xj,dj,x)$y
}

tf.X &lt;- function(x,xj) {
## tent function basis matrix given data x
## and knot sequence xj
  nk &lt;- length(xj); n &lt;- length(x)
  X &lt;- matrix(NA,n,nk)
  for (j in 1:nk) X[,j] &lt;- tf(x,xj,j)
  X
}

tf.XD &lt;- function(x,xk,cmx=NULL,m=2) {
## get X and D subject to constraint
  nk &lt;- length(xk)
  X &lt;- tf.X(x,xk)[,-nk]                   ## basis matrix
  D &lt;- diff(diag(nk),differences=m)[,-nk] ## root penalty
  if (is.null(cmx)) cmx &lt;- colMeans(X)
  X &lt;- sweep(X,2,cmx)        ## subtract cmx from columns
  list(X=X,D=D,cmx=cmx)
} ## tf.XD

## Solution code...
## a)

XZmixed &lt;- function(x,xk=NULL,k=10,sep=TRUE) {
## Get re-parameterized model matrix/matrices...
  if (is.null(xk)) xk &lt;- seq(min(x),max(x),length=k)
  xd &lt;- tf.XD(x,xk)
  D &lt;- rbind(0,xd$D); D[1,1] &lt;- 1
  X &lt;- t(solve(t(D),t(xd$X)))
  if (sep) list(X=X[,1,drop=FALSE],Z=X[,-1],xk=xk)
  else list(X=X,xk=xk) 
} ## XZmixed

## b)
## get components of smooths for Height and Girth...
xh &lt;- XZmixed(trees$Height)
xg &lt;- XZmixed(trees$Girth)

## Fit as mixed model...
X &lt;- cbind(1,xh$X,xg$X)
Zg &lt;- xg$Z; Zh &lt;- xh$Z
g1 &lt;- g &lt;- factor(rep(1,nrow(X)))
vol &lt;- trees$Volume
b &lt;- lme(vol~X-1,random=list(g=pdIdent(~Zh-1),
         g1=pdIdent(~Zg-1)))

## c)
## raw vs. fitted and residual plot 
par(mfrow=c(1,2))
plot(fitted(b),vol)
rsd &lt;- vol - fitted(b)
plot(fitted(b),rsd)

## extract coefs for each smooth...
bh &lt;- as.numeric(coef(b)[c(2,4:11)]) ## coefs for s(Height)
bg &lt;- as.numeric(coef(b)[c(3,12:19)]) ## coefs for s(Height)

## get smooth specific prediction matrices...
Xh &lt;- XZmixed(trees$Height,xk=xh$xk,sep=FALSE)$X
Xg &lt;- XZmixed(trees$Girth,xk=xg$xk,sep=FALSE)$X

## d)
## plot smooths over partial residuals... 
sh &lt;- Xh%*%bh
sg &lt;- Xg%*%bg

par(mfrow=c(1,2))
plot(trees$Girth,sg+rsd,pch=19,col="grey",
     xlab="Girth",ylab="s(Girth)")
lines(trees$Girth,sg)
plot(trees$Height,sh+rsd,pch=19,col="grey",
     xlab="Height",ylab="s(Height)")
lines(trees$Height,sh)

## Q.9 Generalized version of 8 by PQL
## a)

gamm.fit &lt;- function(y,X,Zh,Zg) {
## gamma error log link 2 term gam fit via PQL...
  eta &lt;- log(y) ## get initial eta
  g &lt;- g1 &lt;- factor(rep(1,nrow(X)))
  not.converged &lt;- TRUE
  old.reml &lt;- 1e100 ## don't converge immediately
  while (not.converged) {
    mu &lt;- exp(eta)  ## current mu estimate  
    z &lt;- (y - mu)/mu + eta ## pseudodata
    fit &lt;- lme(z~X-1,random=list(g=pdIdent(~Zh-1),g1=pdIdent(~Zg-1)))
    if (abs(logLik(fit)-old.reml)&lt;1e-5*abs(logLik(fit))) { 
      not.converged &lt;- FALSE
    }
    old.reml &lt;- logLik(fit) 
    eta &lt;- fitted(fit) ## updated eta
  }
  fit
} ## gamm.fit

## b) re-using arguments from Q.8...
m &lt;- gamm.fit(vol,X,Zh,Zg)

## c)
rsd &lt;- residuals(m)
par(mfrow=c(1,2))
plot(exp(fitted(m)),vol);abline(0,1)
plot(fitted(m),rsd)

## d)
bh &lt;- as.numeric(coef(m)[c(2,4:11)]) ## coefs for s(Height)
bg &lt;- as.numeric(coef(m)[c(3,12:19)]) ## coefs for s(Height)

sh &lt;- Xh%*%bh
sg &lt;- Xg%*%bg

par(mfrow=c(1,2))
plot(trees$Girth,sg+rsd,pch=19,col="grey",
     xlab="Girth",ylab="s(Girth)")
lines(trees$Girth,sg)
plot(trees$Height,sh+rsd,pch=19,col="grey",
     xlab="Height",ylab="s(Height)")
lines(trees$Height,sh)


</code></pre>

<hr>
<h2 id='ch5'>Code for Chapter 5: Smoothers</h2><span id='topic+ch5'></span>

<h3>Description</h3>

<p>R code from Chapter 5 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch5.solutions">ch5.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 5.3.3 P-splines

bspline &lt;- function(x,k,i,m=2)
# evaluate ith b-spline basis function of order m at the values
# in x, given knot locations in k
{ if (m==-1) # base of recursion
  { res &lt;- as.numeric(x&lt;k[i+1]&amp;x&gt;=k[i])
  } else     # construct from call to lower order basis
  { z0 &lt;- (x-k[i])/(k[i+m+1]-k[i])
    z1 &lt;- (k[i+m+2]-x)/(k[i+m+2]-k[i+1])
    res &lt;- z0*bspline(x,k,i,m-1)+ z1*bspline(x,k,i+1,m-1)
  }
  res
} ## bspline

k&lt;-6                              # example basis dimension
P &lt;- diff(diag(k),differences=1)  # sqrt of penalty matrix
S &lt;- t(P)%*%P 

## 5.3.6 SCOP-splines
x &lt;- 0:200/200
set.seed(32)
y &lt;- binomial()$linkinv((x-.5)*10) + rnorm(length(x))*.1
plot(x,y)
k &lt;- 7
ssp &lt;- s(x,bs="ps",k=k); ssp$mono &lt;- 1
sm &lt;- smoothCon(ssp,data.frame(x))[[1]]
X &lt;- sm$X; XX &lt;- crossprod(X); sp &lt;- .005
gamma &lt;- rep(0,k); S &lt;- sm$S[[1]]
for (i in 1:20) {
  gt &lt;- c(gamma[1],exp(gamma[2:k]))
  dg &lt;- c(1,gt[2:k]) 
  g &lt;- -dg*(t(X)%*%(y-X%*%gt)) + sp*S%*%gamma
  H &lt;- dg*t(dg*XX)
  gamma &lt;- gamma - solve(H+sp*S,g)
}
lines(x,X%*%gt)

</code></pre>

<hr>
<h2 id='ch5.solutions'>Solution code for Chapter 5: Smoothers</h2><span id='topic+ch5.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 5 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch5">ch5</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)
## Q.4 P-spline
## a)
library(splines)
pspline.XB &lt;- function(x,q=10,m=2,p.m=2)
# Get model matrix and sqrt Penalty matrix for P-spline
{ # first make knot sequence, k
  k &lt;- seq(min(x),max(x),length=q-m)
  dk &lt;- k[2]-k[1]
  k &lt;- c(k[1]-dk*((m+1):1),k,k[q-m]+dk*(1:(m+1)))
  # now get model matrix and root penalty
  X &lt;- splineDesign(k,x,ord=m+2)
  B &lt;- diff(diag(q),difference=p.m)
  list(X=X,B=B)
} ## pspline.XB

## b)
n&lt;-100
x &lt;- sort(runif(n))
ps &lt;- pspline.XB(x,q=9,m=2,p.m=2)
par(mfrow=c(3,3)) # plot the original basis functions
for (i in 1:9) plot(x,ps$X[,i],type="l")

## c)
S &lt;-  t(ps$B)%*%ps$B
es &lt;- eigen(S);U &lt;- es$vectors
XU &lt;- ps$X%*%U # last p.m cols are penalty null space
par(mfrow=c(3,3)) # plot penalty eigenbasis functions
for (i in 1:9) plot(x,XU[,i],type="l")

## d)
qrx &lt;- qr(ps$X) # QR of X
R &lt;- qr.R(qrx)
RSR &lt;- solve(t(R),S);RSR &lt;- t(solve(t(R),t(RSR)))
ersr &lt;- eigen(RSR)
U &lt;- ersr$vectors
Q &lt;- qr.Q(qrx)
QU &lt;- Q%*%U
par(mfrow=c(3,3)) # plot the natural basis functions
for (i in 1:9) plot(x,QU[,i],type="l")

## Q.5

test1&lt;-function(x,z,sx=0.3,sz=0.4)
{ 1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
  0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2)
}
n &lt;- 200
x &lt;- matrix(runif(2*n),n,2)
f &lt;- test1(x[,1],x[,2])
y &lt;- f + rnorm(n)*.1

eta &lt;- function(r)
{ # thin plate spline basis functions
  ind &lt;- r&lt;=0
  eta &lt;- r
  eta[!ind] &lt;- r[!ind]^2*log(r[!ind])/(8*pi)
  eta[ind] &lt;- 0
  eta
} ## eta

XSC &lt;- function(x,xk=x)
{ # set up t.p.s., given covariates, x, and knots, xk
  n &lt;- nrow(x);k &lt;- nrow(xk)
  X &lt;- matrix(1,n,k+3)  # tps model matrix
  for (j in 1:k) {
    r &lt;- sqrt((x[,1]-xk[j,1])^2+(x[,2]-xk[j,2])^2)
    X[,j] &lt;- eta(r)
  }
  X[,j+2] &lt;- x[,1];X[,j+3] &lt;- x[,2]
  C &lt;- matrix(0,3,k+3)  # tps constraint matrix
  S &lt;- matrix(0,k+3,k+3)# tps penalty matrix
  for (i in 1:k) {
   C[1,i]&lt;-1;C[2,i] &lt;- xk[i,1];C[3,i] &lt;- xk[i,2]
   for (j in i:k) S[j,i]&lt;-S[i,j] &lt;-
                eta(sqrt(sum((xk[i,]-xk[j,])^2)))
  }
  list(X=X,S=S,C=C)
} ## XSC

absorb.con &lt;- function(X,S,C)
{ # get constraint null space, Z...
  qrc &lt;- qr(t(C)) # QR=C', Q=[Y,Z]
  m &lt;- nrow(C);k &lt;- ncol(X)
  X &lt;- t(qr.qty(qrc,t(X)))[,(m+1):k] # form XZ
  # now form Z'SZ ...
  S &lt;- qr.qty(qrc,t(qr.qty(qrc,t(S))))[(m+1):k,(m+1):k]
  list(X=X,S=S,qrc=qrc)
} ## absorb.con

fit.tps &lt;- function(y,x,xk=x,lambda=0)
{ tp &lt;- XSC(x,xk)                  # get tps matrices
  tp &lt;- absorb.con(tp$X,tp$S,tp$C) # make unconstrained
  ev &lt;- eigen(tp$S,symmetric=TRUE) # get sqrt penalty, rS
  rS &lt;- ev$vectors%*%(ev$values^.5*t(ev$vectors))
  X &lt;- rbind(tp$X,rS*sqrt(lambda)) # augmented model matrix
  z &lt;- c(y,rep(0,ncol(rS)))        # augmented data
  beta &lt;- coef(lm(z~X-1))          # fit model
  beta &lt;- qr.qy(tp$qrc,c(0,0,0,beta)) # backtransform beta
} ## fit.tps

eval.tps &lt;- function(x,beta,xk)
{ # evaluate tps at x, given parameters, beta, and knots, xk.
  k &lt;- nrow(xk);n &lt;- nrow(x)
  f &lt;- rep(beta[k+1],n)
  for (i in 1:k) {
    r &lt;- sqrt((x[,1]-xk[i,1])^2+(x[,2]-xk[i,2])^2)
    f &lt;- f + beta[i]*eta(r)
  }
  f &lt;- f + beta[k+2]*x[,1] + beta[k+3]*x[,2]
} ## eval.tps

## select some `knots', xk ...
ind &lt;- sample(1:n,100,replace=FALSE)
xk &lt;- x[ind,]
## fit model ...
beta &lt;- fit.tps(y,x,xk=xk,lambda=.01)

## contour truth and fit
par(mfrow=c(1,2))
xp &lt;- matrix(0,900,2)
x1&lt;-seq(0,1,length=30);x2&lt;-seq(0,1,length=30)
xp[,1]&lt;-rep(x1,30);xp[,2]&lt;-rep(x2,rep(30,30))
truth&lt;-matrix(test1(xp[,1],xp[,2]),30,30)
contour(x1,x2,truth)
fit &lt;- matrix(eval.tps(xp,beta,xk),30,30)
contour(x1,x2,fit)

## Q.6 smooth.construct

tf &lt;- function(x,xj,j) {
## generate jth tent function from set defined by knots xj
  dj &lt;- xj*0;dj[j] &lt;- 1
  approx(xj,dj,x)$y
}

tf.X &lt;- function(x,xj) {
## tent function basis matrix given data x
## and knot sequence xj
  nk &lt;- length(xj); n &lt;- length(x)
  X &lt;- matrix(NA,n,nk)
  for (j in 1:nk) X[,j] &lt;- tf(x,xj,j)
  X
}

smooth.construct.pl.smooth.spec&lt;-function(object,data,knots) {
## a piecewise linear smooth constructor method function
  m &lt;- object$p.order[1]
  if (is.na(m)) m &lt;- 2 ## default 
  if (m&lt;1) stop("silly m supplied")
  if (object$bs.dim&lt;0) object$bs.dim &lt;- 20 ## default
  x &lt;- data[[object$term]]  ## the data
  k &lt;- knots[[object$term]] ## will be NULL if none supplied
  if (is.null(k)) { # space knots through data
    k &lt;- seq(min(x),max(x),length=object$bs.dim)
  } else {
    if (length(k)!=object$bs.dim) # right number of knots?
    k &lt;- seq(min(k),max(k),length=object$bs.dim)
  }
  object$X &lt;- tf.X(x,k)
  if (!object$fixed) { # create the penalty matrix 
    object$S[[1]] &lt;- crossprod(diff(diag(object$bs.dim),difference=m))
  }
  object$rank &lt;- object$bs.dim - m  # penalty rank
  object$null.space.dim &lt;- m  # dim. of unpenalized space
  ## store "tr" specific stuff ...
  object$knots &lt;- k
 
  object$df &lt;- ncol(object$X)     # maximum DoF (if unconstrained)
 
  class(object) &lt;- "pl.smooth"  # Give object a class
  object
}

Predict.matrix.pl.smooth&lt;-function(object,data)
## prediction method function for the `pl' smooth class
{ x &lt;- data[[object$term]]
  X &lt;- tf.X(x,object$knots)
  X # return the prediction matrix
}

# an example, using the new class....
require(mgcv)
set.seed(10)
dat &lt;- gamSim(1,n=400,scale=2)
b &lt;- gam(y~s(x0,bs="pl",m=2)+s(x1,bs="pl",m=2) +
         s(x2,bs="pl",m=3)+s(x3,bs="pl",m=2),
	 data=dat,method="REML")
plot(b,pages=1)


</code></pre>

<hr>
<h2 id='ch6'>Code for Chapter 6: GAM Theory</h2><span id='topic+ch6'></span>

<h3>Description</h3>

<p>R code from Chapter 6 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch6.solutions">ch6.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## 6.13.2 backfitting

set.seed(2) ## simulate some data... 
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
edf &lt;- c(3,3,8,3)
y &lt;- dat$y
x &lt;- cbind(dat$x0,dat$x1,dat$x2,dat$x3)
f &lt;- x*0; alpha &lt;- mean(y); ok &lt;- TRUE; rss0 &lt;- 0
while (ok) { # backfitting loop
  for (i in 1:ncol(x)) { # loop through the smooth terms
    ep &lt;- y - rowSums(f[,-i]) - alpha
    b &lt;- smooth.spline(x[,i],ep,df=edf[i])
    f[,i] &lt;- predict(b,x[,i])$y
  }
  rss &lt;- sum((y-rowSums(f))^2)
  if (abs(rss-rss0)&lt;1e-6*rss) ok &lt;- FALSE
  rss0 &lt;- rss
}
par(mfrow=c(2,2))
for (i in 1:ncol(x)) {
  plot(x[,i],y-mean(y),col="grey",pch=19,cex=.3)
  ii &lt;- order(x[,i])
  lines(x[ii,i],f[ii,i],col=2,lwd=2)
}

</code></pre>

<hr>
<h2 id='ch6.solutions'>Solution code for Chapter 6: GAM Theory</h2><span id='topic+ch6.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 6 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch6">ch6</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## code from Chapter 5 solutions...

## Q.3

pspline.XB &lt;- function(x,q=10,m=2,p.m=2)
# Get model matrix and sqrt Penalty matrix for P-spline
{ # first make knot sequence, k
  k &lt;- seq(min(x),max(x),length=q-m)
  dk &lt;- k[2]-k[1]
  k &lt;- c(k[1]-dk*((m+1):1),k,k[q-m]+dk*(1:(m+1)))
  # now get model matrix and root penalty
  X &lt;- splineDesign(k,x,ord=m+2)
  B &lt;- diff(diag(q),difference=p.m)
  list(X=X,B=B)
} ## pspline.XB

## a) and b)
fit.wPs &lt;- function(y,X,B,lambda=0,w=rep(1,length(y)))
# fit to y by weighted penalized least squares, X is
# model matrix, B is sqrt penalty, lambda is smoothing p.
{ w &lt;- as.numeric(w^.5)
  n &lt;- nrow(X)
  X&lt;-rbind(w*X,sqrt(lambda)*B)
  y&lt;-c(w*y,rep(0,nrow(B)))
  b &lt;- lm(y~X-1) # actually estimate model
  trA &lt;- sum(influence(b)$hat[1:n])
  rss &lt;- sum((y-fitted(b))[1:n]^2) ## not really needed here
  list(trA=trA,rss=rss,b=coef(b))
}

fitPoiPs &lt;- function(y,X,B,lambda=0)
# Fit Poisson model with log-link by P-IRLS
{ mu &lt;- y;mu[mu==0] &lt;- .1
  eta &lt;- log(mu)
  converged &lt;- FALSE
  dev &lt;- ll.sat &lt;- sum(dpois(y,y,log=TRUE))
  while (!converged) {
    z &lt;- (y-mu)/mu + eta
    w &lt;- mu
    fPs &lt;- fit.wPs(z,X,B,lambda,w)
    eta &lt;- X%*%fPs$b
    mu=exp(eta)
    old.dev &lt;- dev
    dev &lt;- 2*(ll.sat-sum(dpois(y,mu,log=TRUE)))
    if (abs(dev-old.dev)&lt;1e-6*dev) converged &lt;- TRUE
  }
  list(dev=dev,rss=fPs$rss,trA=fPs$trA,b=fPs$b,fv=mu)
}

## c)
## simulate data as in question...
set.seed(1)
f &lt;- function(x) .04*x^11*(10*(1-x))^6+2*(10*x)^3*(1-x)^10
n &lt;- 100;x &lt;- sort(runif(n))
y &lt;- rpois(rep(1,n),exp(f(x)))

## fitting...
library(splines)
ps &lt;- pspline.XB(x,q=10,m=2,p.m=2)
lambda &lt;- 1e-4;reps &lt;- 60
sp &lt;- trA &lt;- gcv &lt;- rep(0,reps)
for (i in 1:reps) { # loop through trial s.p.s
  fps &lt;- fitPoiPs(y,ps$X,ps$B,lambda=lambda)
  trA[i] &lt;- fps$trA;sp[i] &lt;- lambda
  gcv[i] &lt;- n*fps$dev/(n-trA[i])^2
  lambda &lt;- lambda*1.3
}
plot(trA,gcv,type="l")
fps1 &lt;- fitPoiPs(y,ps$X,ps$B,lambda=sp[gcv==min(gcv)])
plot(x,y);lines(x,fps1$fv)

## Q.6 Fellner-Schall for GCV and AIC...

## b)
library(mgcv);library(MASS)
sm &lt;- smoothCon(s(times,k=20),data=mcycle)[[1]]
X &lt;- sm$X; S &lt;- sm$S[[1]]; y &lt;- mcycle$accel
lambda &lt;- 1; n &lt;- length(y)
XX &lt;- crossprod(X);
with(mcycle,plot(times,accel))
for (i in 1:20) {
  R &lt;- chol(XX+lambda*S)
  b &lt;- backsolve(R,forwardsolve(t(R),t(X) %*% y))
  f &lt;- X %*% b
  lines(mcycle$times,f,col="grey")
  HiS &lt;- backsolve(R,forwardsolve(t(R),S))
  HiH &lt;- backsolve(R,forwardsolve(t(R),XX))
  tau &lt;- sum(diag(HiH))
  if (i&gt;1) { ## convergence test
    if (abs(tau-tau0)&lt;1e-5*tau) break  
  } 
  tau0 &lt;- tau
  dt.dl &lt;- -sum(t(HiH)*HiS)
  db.dl &lt;- -HiS %*% b
  dD.db &lt;- 2*t(X) %*% (f - y)
  lambda &lt;- -sum(2*(y-f)^2)/(n-tau)*dt.dl/sum(db.dl*dD.db) * lambda
}
lines(mcycle$times,f)

## c)
y &lt;- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t &lt;- 1:13
sm &lt;- smoothCon(s(t),data=data.frame(t=t,y=y))[[1]]
X &lt;- sm$X; S &lt;- sm$S[[1]]; lambda &lt;- .001; n &lt;- length(y)
plot(t,y)
mu &lt;- y; eta &lt;- log(mu)
for (i in 1:50) {
  w &lt;- mu; z &lt;- (y-mu)/mu + eta
  XWX &lt;- crossprod(sqrt(w)*X)
  R &lt;- chol(XWX+lambda*S)
  b &lt;- backsolve(R,forwardsolve(t(R),t(X) %*% (w*z)))
  eta &lt;- drop(X %*% b);mu &lt;- exp(eta)
  lines(t,mu,col="grey")
  HiS &lt;- backsolve(R,forwardsolve(t(R),S))
  HiH &lt;- backsolve(R,forwardsolve(t(R),XWX))
  tau &lt;- sum(diag(HiH))
  if (i&gt;1) { ## convergence test
    if (abs(tau-tau0)&lt;1e-5*tau) break  
  } 
  tau0 &lt;- tau
  dt.dl &lt;- -sum(t(HiH)*HiS)
  db.dl &lt;- -HiS %*% b
  dl.db &lt;- t(X) %*% (y-mu) ## especially simple for this case
  lambda &lt;- dt.dl/sum(db.dl*dl.db) * lambda
}
i;tau;lines(t,mu)

## Q.8 log det stabilty (or lack of)

set.seed(1);lam &lt;- 1
A1 &lt;- crossprod(diff(diag(3),diff=1))
A2 &lt;- crossprod(matrix(runif(9),3,3))
A &lt;- matrix(0,5,5);A[1:3,1:3] &lt;- A1
A[3:5,3:5] &lt;- A[3:5,3:5] + lam * A2

ldetA.qr &lt;- ldetA.ev &lt;- ldetA.svd &lt;- ldetA &lt;-
            rho &lt;- seq(-40,-25,length=100)
for (i in 1:length(rho)) {
  lam &lt;- exp(rho[i])
  A &lt;- matrix(0,5,5);A[1:3,1:3] &lt;- A1
  A[3:5,3:5] &lt;- A[3:5,3:5] + lam * A2
  ea1 &lt;- eigen(A1)
  Q &lt;- diag(5);Q[1:3,1:3] &lt;- ea1$vectors
  At &lt;- matrix(0,5,5)
  At[3:5,3:5] &lt;- At[3:5,3:5] + lam * A2
  At &lt;- t(Q)%*%At%*%Q
  diag(At)[1:2] &lt;- diag(At)[1:2]+ea1$values[1:2]

  ldetA[i] &lt;- sum(log(abs(diag(qr.R(qr(At))))))
  ldetA.qr[i] &lt;- sum(log(abs(diag(qr.R(qr(A))))))
  ldetA.ev[i] &lt;- sum(log(abs(eigen(A)$values))) 
  ldetA.svd[i] &lt;- sum(log(abs(svd(A)$d)))
}
plot(rho,ldetA,type="l") ## nice and stable
## not...
lines(rho,ldetA.qr,lty=2)
lines(rho,ldetA.ev,lty=3)
lines(rho,ldetA.svd,lty=4)



</code></pre>

<hr>
<h2 id='ch7'>Code for Chapter 7: GAMs in Practice: mgcv</h2><span id='topic+ch7'></span>

<h3>Description</h3>

<p>R code from Chapter 7 of the second edition of &lsquo;Generalized Additive Models: An Introduction with R&rsquo; is in the examples section below.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch7.solutions">ch7.solutions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)
## NOTE: Examples are marked 'Not run' to save CRAN check time

## 7.1.1 using smooth constructors

library(mgcv); library(MASS) ## load for mcycle data.
## set up a smoother...
sm &lt;- smoothCon(s(times,k=10),data=mcycle,knots=NULL)[[1]]
## use it to fit a regression spline model...
beta &lt;- coef(lm(mcycle$accel~sm$X-1))
with(mcycle,plot(times,accel)) ## plot data
times &lt;- seq(0,60,length=200)  ## create prediction times
## Get matrix mapping beta to spline prediction at 'times'
Xp &lt;- PredictMat(sm,data.frame(times=times))
lines(times,Xp%*%beta) ## add smooth to plot

## Not run: 
## 7.2 Brain scan
## 7.2.1 preliminary modelling
require(gamair); require(mgcv); data(brain)
brain &lt;- brain[brain$medFPQ&gt;5e-3,] # exclude 2 outliers
m0 &lt;- gam(medFPQ~s(Y,X,k=100),data=brain)
gam.check(m0)

e &lt;- residuals(m0); fv &lt;- fitted(m0)
lm(log(e^2)~log(fv))

m1&lt;-gam(medFPQ^.25~s(Y,X,k=100),data=brain)
gam.check(m1)
m2&lt;-gam(medFPQ~s(Y,X,k=100),data=brain,family=Gamma(link=log))
mean(fitted(m1)^4);mean(fitted(m2));mean(brain$medFPQ)

m2
vis.gam(m2,plot.type="contour",too.far=0.03,
        color="gray",n.grid=60,zlim=c(-1,2))

## 7.2.2 additive?
m3 &lt;- gam(medFPQ~s(Y,k=30)+s(X,k=30),data=brain,
          family=Gamma(link=log))
m3
AIC(m2,m3)

## 7.2.3 isotropic or tensor 
tm &lt;- gam(medFPQ~te(Y,X,k=10),data=brain,family=Gamma(link=log))
tm1 &lt;- gam(medFPQ ~ s(Y,k=10,bs="cr") + s(X,bs="cr",k=10) +
           ti(X,Y,k=10), data=brain, family=Gamma(link=log))
AIC(m2,tm,tm1)
anova(tm1)

## 7.2.4 Detecting symmetry
brain$Xc &lt;- abs(brain$X - 64.5)
brain$right &lt;- as.numeric(brain$X&lt;64.5)
m.sy &lt;- gam(medFPQ~s(Y,Xc,k=100),data=brain,
            family=Gamma(link=log))
m.as &lt;- gam(medFPQ~s(Y,Xc,k=100)+s(Y,Xc,k=100,by=right),
            data=brain,family=Gamma(link=log))
m.sy
m.as

anova(m.as)

vis.gam(m.sy,plot.type="contour",view=c("Xc","Y"),too.far=.03,
        color="gray",n.grid=60,zlim=c(-1,2),main="both sides")
vis.gam(m.as,plot.type="contour",view=c("Xc","Y"),
        cond=list(right=0),too.far=.03,color="gray",n.grid=60,
        zlim=c(-1,2),main="left side")
vis.gam(m.as,plot.type="contour",view=c("Xc","Y"),
        cond=list(right=1),too.far=.03,color="gray",n.grid=60,
        zlim=c(-1,2),main="right side")

## 7.2.5 Comparing surfaces
brain1 &lt;- brain
mu &lt;- fitted(m2)
n&lt;-length(mu)
ind &lt;- brain1$X&lt;60 &amp; brain1$Y&lt;20
mu[ind] &lt;- mu[ind]/3
set.seed(1)
brain1$medFPQ &lt;- rgamma(rep(1,n),mu/m2$sig2,scale=m2$sig2)

brain2=rbind(brain,brain1)
brain2$sample1 &lt;- c(rep(1,n),rep(0,n))
brain2$sample0 &lt;- 1 - brain2$sample1

m.same&lt;-gam(medFPQ~s(Y,X,k=100),data=brain2,
            family=Gamma(link=log))
m.diff&lt;-gam(medFPQ~s(Y,X,k=100)+s(Y,X,by=sample1,k=100),
            data=brain2,family=Gamma(link=log))
AIC(m.same,m.diff)
anova(m.diff)

## 7.2.6 Prediction
predict(m2)[1:5]
pv &lt;- predict(m2,se=TRUE)
pv$fit[1:5]
pv$se[1:5]

predict(m2,type="response")[1:5]
pv &lt;- predict(m2,type="response",se=TRUE)
pv$se[1:5]

pd &lt;- data.frame(X=c(80.1,68.3),Y=c(41.8,41.8))
predict(m2,newdata=pd)
predict(m2,newdata=pd,type="response",se=TRUE)

predict(m3,newdata=pd,type="terms",se=TRUE)

Xp &lt;- predict(m2,newdata=pd,type="lpmatrix")
fv &lt;- Xp%*%coef(m2)
fv
d &lt;- t(c(1,-1))
d%*%fv
d%*%Xp%*%m2$Vp%*%t(Xp)%*%t(d)

## 7.2.7 Variance of non-linear function

ind &lt;- brain$region==1&amp; ! is.na(brain$region)
Xp &lt;- predict(m2,newdata=brain[ind,],type="lpmatrix")
set.seed(8) ## for repeatability
br &lt;- rmvn(n=1000,coef(m2),vcov(m2)) # simulate from posterior
mean.FPQ&lt;-rep(0,1000)
for (i in 1:1000)
{ lp &lt;- Xp%*%br[i,]  # replicate linear predictor
  mean.FPQ[i] &lt;- mean(exp(lp)) # replicate region 1 mean FPQ
}
mean.FPQ &lt;- colMeans(exp(Xp%*%t(br)))

## 7.3 Retinopathy
require(gamair); require(mgcv); data(wesdr)
k &lt;- 7
b &lt;- gam(ret ~ s(dur,k=k) + s(gly,k=k) + s(bmi,k=k) + 
         ti(dur,gly,k=k) + ti(dur,bmi,k=k) + ti(gly,bmi,k=k),
         select=TRUE, data=wesdr, family=binomial(), method="ML")
b

## 7.4 Air pollution
data(chicago)
ap0 &lt;- gam(death~s(time,bs="cr",k=200)+pm10median+so2median+
           o3median+tmpd,data=chicago,family=poisson)
gam.check(ap0)

par(mfrow=c(2,1))
plot(ap0,n=1000)  # n increased to make plot smooth
plot(ap0,residuals=TRUE,n=1000)

chicago$death[3111:3125]

ap1&lt;-gam(death~s(time,bs="cr",k=200)+s(pm10median,bs="cr")+
     s(so2median,bs="cr")+s(o3median,bs="cr")+s(tmpd,bs="cr"),
     data=chicago,family=poisson)

## 7.4.1 single index

lagard &lt;- function(x,n.lag=6) {
  n &lt;- length(x); X &lt;- matrix(NA,n,n.lag)
  for (i in 1:n.lag) X[i:n,i] &lt;- x[i:n-i+1] 
  X
}
dat &lt;- list(lag=matrix(0:5,nrow(chicago),6,byrow=TRUE),
            death=chicago$death,time=chicago$time)
dat$pm10 &lt;- lagard(chicago$pm10median)
dat$tmp &lt;- lagard(chicago$tmpd)
dat$o3 &lt;- lagard(chicago$o3median)

si &lt;- function(theta,dat,opt=TRUE) {
## Return ML if opt==TRUE or fitted gam otherwise.
  alpha &lt;- c(1,theta) ## alpha defined via unconstrained theta
  kk &lt;- sqrt(sum(alpha^2)); alpha &lt;- alpha/kk  ## ||alpha||=1
  o3 &lt;- dat$o3%*%alpha; tmp &lt;- dat$tmp%*%alpha
  pm10 &lt;- dat$pm10%*%alpha ## re-weight lagged covariates
  b&lt;- bam(dat$death~s(dat$time,k=200,bs="cr")+s(pm10,bs="cr")+
          te(o3,tmp,k=8),family=poisson) ## fit model
  cat(".") ## give user something to watch
  if (opt) return(b$gcv.ubre) else {
    b$alpha &lt;- alpha  ## add alpha to model object
    b$J &lt;- outer(alpha,-theta/kk^2) ## get dalpha_i/dtheta_j
    for (j in 1:length(theta)) b$J[j+1,j] &lt;- b$J[j+1,j] + 1/kk
    return(b)
  }
} ## si

## WARNING: the next line takes around half an hour to run

f1 &lt;- optim(rep(1,5),si,method="BFGS",hessian=TRUE,dat=dat)

apsi &lt;- si(f1$par,dat,opt=FALSE)
apsi$alpha

## 7.4.2 distributed lag...

apl &lt;- bam(death~s(time,bs="cr",k=200)+te(pm10,lag,k=c(10,5))+
       te(o3,tmp,lag,k=c(8,8,5)),family=poisson,data=dat)

## 7.5 Egg survey - less than a minute
## 7.5.1 Model development
data(mack)
mack$log.net.area &lt;- log(mack$net.area)

gmtw &lt;- gam(egg.count ~ s(lon,lat,k=100) + s(I(b.depth^.5))+ 
       s(c.dist) + s(salinity) + s(temp.surf) + s(temp.20m)+
       offset(log.net.area),data=mack,family=tw,method="REML",
       select=TRUE)
gm2 &lt;- gam(egg.count ~ s(lon,lat,k=100) + s(I(b.depth^.5)) + 
         s(c.dist) + s(temp.20m) + offset(log.net.area),
         data=mack,family=tw,method="REML")
gm2

## 7.5.2 model predictions
par(mfrow=c(1,3))
data(mackp); data(coast)
mackp$log.net.area &lt;- rep(0,nrow(mackp))
lon &lt;- seq(-15,-1,1/4); lat &lt;- seq(44,58,1/4)
zz&lt;-array(NA,57*57); zz[mackp$area.index]&lt;-predict(gm2,mackp)  
image(lon,lat,matrix(zz,57,57),col=gray(0:32/32),
      cex.lab=1.5,cex.axis=1.4)
contour(lon,lat,matrix(zz,57,57),add=TRUE)
lines(coast$lon,coast$lat,col=1)

set.seed(4) ## make reproducable
br1 &lt;- rmvn(n=1000,coef(gm2),vcov(gm2))
Xp &lt;- predict(gm2,newdata=mackp,type="lpmatrix")
mean.eggs1 &lt;- colMeans(exp(Xp%*%t(br1)))
hist(mean.eggs1)

## 7.5.3 alternative

gmgr &lt;- gam(egg.count ~s(lon,lat,k=100)+s(lon,lat,by=temp.20m)
        +s(lon,lat,by=I(b.depth^.5)) +offset(log.net.area),
        data=mack,family=tw,method="REML")

## 7.6 Larks - about a minute
library(gamair); data(bird)
bird$n &lt;- bird$y/1000;bird$e &lt;- bird$x/1000
m1 &lt;- gam(crestlark~s(e,n,k=100),data=bird,family=binomial,
          method="REML")
m1

m2 &lt;- gam(crestlark ~ s(e,n,bs="ds",m=c(1,.5),k=100),data=bird,family=binomial,
          method="REML")
	  
REML &lt;- r &lt;- 1:10*10 
for (i in 1:length(r)) { 
  mt &lt;- gam(crestlark ~ s(e,n,bs="gp",m=c(3,r[i]),k=100),
        data=bird,family=binomial,method="REML")
  REML[i] &lt;- mt$gcv.ubre
  if (i==1||REML[i]==REML0) { m3 &lt;- mt; REML0 &lt;- REML[i]}
}
AIC(m1,m2,m3)

bird$tet.n &lt;- bird$N &lt;- rep(1,nrow(bird))
bird$N[is.na(as.vector(bird$crestlark))] &lt;- NA
ba &lt;- aggregate(data.matrix(bird), by=list(bird$QUADRICULA),
                FUN=sum, na.rm=TRUE)
ba$e &lt;- ba$e/ba$tet.n; ba$n &lt;- ba$n/ba$tet.n

m10 &lt;- gam(cbind(crestlark,N-crestlark) ~ s(e,n,k=100),
           data=ba, family=binomial, method="REML")
library(geoR)
coords&lt;-matrix(0,nrow(ba),2);coords[,1]&lt;-ba$e;coords[,2]&lt;-ba$n
gb&lt;-list(data=residuals(m10,type="d"),coords=coords)
plot(variog(gb,max.dist=100))
plot(fitted(m10),residuals(m10))

## 7.7.1 Sole egg GAMM
## Chapter 3 preliminaries...
data(sole)
sole$off &lt;- log(sole$a.1-sole$a.0)# model offset term
sole$a&lt;-(sole$a.1+sole$a.0)/2     # mean stage age
solr&lt;-sole                        # make copy for rescaling
solr$t&lt;-solr$t-mean(sole$t)
solr$t&lt;-solr$t/var(sole$t)^0.5
solr$la&lt;-solr$la-mean(sole$la)
solr$lo&lt;-solr$lo-mean(sole$lo)

## GAMM fit...
solr$station &lt;- factor(with(solr,paste(-la,-lo,-t,sep="")))     
som &lt;- gamm(eggs~te(lo,la,t,bs=c("tp","tp"),k=c(25,5),d=c(2,1))
            +s(t,k=5,by=a)+offset(off), family=quasipoisson,
            data=solr,random=list(station=~1))
som$gam
som1 &lt;- bam(eggs~te(lo,la,t,bs=c("tp","tp"),k=c(25,5),d=c(2,1))
            + s(t,k=5,by=a)+offset(off)+s(station,bs="re"),
	        family=quasipoisson,data=solr)
gam.vcomp(som1)
som$lme
## boundary and knots for soap...
bnd &lt;- list(list(lo=c(-6.74,-5.72,-5.7 ,-5.52,-5.37,-5.21,-5.09,-5.02,
          -4.92,-4.76,-4.64,-4.56,-4.53,-4.3,-4.16,-3.8 ,-3.8,-5.04,-6.76,
	  -6.74),
          la=c(50.01,50.02,50.13,50.21,50.24,50.32,50.41,50.54,50.59,50.64,
	  50.74,50.86,51.01,51  ,51.2,51.22,51.61,51.7,51.7,50.01)))

knt &lt;- list(lo=c(-4.643,-5.172,-5.638,-6.159,-6.665,-6.158,-5.656,-5.149,
  -4.652,-4.154,-3.901,-4.146,-4.381,-4.9,-5.149,-5.37,-5.866,-6.36,-6.635,
  -6.12,-5.626,-5.117,-4.622,-4.695,-4.875,-5.102,-5.609,-5.652,-5.141,
  -5.354,-5.843,-6.35,-6.628,-6.127,-5.63,-5.154,-5.356,-5.652,-5.853,
  -6.123),
   la=c(51.626,51.61,51.639,51.638,51.376,51.377,51.373,51.374,51.374,
   51.376,51.379,51.226,51.129,51.194,51.083,51.147,51.129,51.151,50.901,
   50.891,50.959,50.958,50.942,50.728,50.676,50.818,50.825,50.684,50.693,
   50.568,50.564,50.626,50.397,50.451,50.443,50.457,50.325,50.193,50.322,
   50.177))
   
sole$station &lt;- solr$station ## station to sole data

som2 &lt;- bam(eggs ~ te(lo,la,t,bs=c("sw","cr"),k=c(40,5),
            d=c(2,1),xt=list(list(bnd=bnd),NULL)) +
            s(t,k=5,by=a) + offset(off) + s(station,bs="re"),
            knots=knt, family=quasipoisson, data=sole)

## 7.7.2 Cairo temperature
data(cairo)
ctamm &lt;- gamm(temp~s(day.of.year,bs="cc",k=20)+s(time,bs="cr"),
         data=cairo,correlation=corAR1(form=~1|year))
summary(ctamm$gam)
intervals(ctamm$lme,which="var-cov")
ctamm$gam$sig2/ctamm$gam$sp
plot(ctamm$gam,scale=0,pages=1)

REML &lt;- rho &lt;- 0.6+0:20/100
for (i in 1:length(rho)) {
  ctbam &lt;- bam(temp~s(day.of.year,bs="cc",k=20)+s(time,bs="cr"),
               data=cairo,rho=rho[i])
  REML[i] &lt;- ctbam$gcv.ubre
}
rho[REML==min(REML)]

## 7.7.3 Fully Bayesian
## Not currently included (requires editing of JAGS file)

## 7.7.4 Random wiggly curves
data(sitka)
sitka$id.num &lt;- as.factor(sitka$id.num)
b &lt;- gamm(log.size~s(days) + ozone + ozone:days +
          s(days,id.num,bs="fs",k=5),data=sitka)
plot(b$gam,pages=1)


## 7.8 survival
require(survival)
data(pbc) ## loads pbcseq also
pbc$status1 &lt;- as.numeric(pbc$status==2)
pbc$stage &lt;- factor(pbc$stage)
b0 &lt;- gam(time ~ trt+sex+stage+s(sqrt(protime))+s(platelet)+ 
          s(age)+s(bili)+s(albumin)+s(sqrt(ast))+s(alk.phos),
          weights=status1,family=cox.ph,data=pbc)

b &lt;- gam(time ~ trt+sex+s(sqrt(protime))+s(platelet)+ 
          s(age)+s(bili)+s(albumin),
          weights=status1,family=cox.ph,data=pbc)

anova(b)
par(mfrow=c(2,3))
plot(b); plot(b$linear.predictors,residuals(b))

par(mfrow=c(1,1))
## create prediction data frame...
np &lt;- 300
newd &lt;- data.frame(matrix(0,np,0))
for (n in names(pbc)) newd[[n]] &lt;- rep(pbc[[n]][25],np)
newd$time &lt;- seq(0,4500,length=np)
## predict and plot the survival function... 
fv &lt;- predict(b,newdata=newd,type="response",se=TRUE)
plot(newd$time,fv$fit,type="l",ylim=c(0.,1),xlab="time",
     ylab="survival",lwd=2)
## add crude one s.e. intervals...
lines(newd$time,fv$fit+fv$se.fit,col="grey")
lines(newd$time,fv$fit-fv$se.fit,col="grey")
## and intervals based on cumulative hazard s.e...
se &lt;- fv$se.fit/fv$fit
lines(newd$time,exp(log(fv$fit)+se))
lines(newd$time,exp(log(fv$fit)-se))

## 7.8.1 time dependent
## copy functions from ?cox.pht in mgcv...

app &lt;- function(x,t,to) {
## wrapper to approx for calling from apply...
   y &lt;- if (sum(!is.na(x))&lt;1) rep(NA,length(to)) else
        approx(t,x,to,method="constant",rule=2)$y
   if (is.factor(x)) factor(levels(x)[y],levels=levels(x)) else y
} ## app

tdpois &lt;- function(dat,event="z",et="futime",t="day",
                             status="status1",id="id") {
## dat is data frame. id is patient id; et is event time; t is
## observation time; status is 1 for death 0 otherwise;
## event is name for Poisson response.
   if (event %in% names(dat)) warning("event name in use")
   require(utils) ## for progress bar
   te &lt;- sort(unique(dat[[et]][dat[[status]]==1])) ## event times
   sid &lt;- unique(dat[[id]])
   prg &lt;- txtProgressBar(min = 0, max = length(sid), initial = 0,
              char = "=",width = NA, title="Progress", style = 3)
   ## create dataframe for poisson model data
   dat[[event]] &lt;- 0; start &lt;- 1
   dap &lt;- dat[rep(1:length(sid),length(te)),]
   for (i in 1:length(sid)) { ## work through patients
     di &lt;- dat[dat[[id]]==sid[i],] ## ith patient's data
     tr &lt;- te[te &lt;= di[[et]][1]] ## times required for this patient
     ## Now do the interpolation of covariates to event times...
     um &lt;- data.frame(lapply(X=di,FUN=app,t=di[[t]],to=tr))
     ## Mark the actual event...
     if (um[[et]][1]==max(tr)&amp;&amp;um[[status]]==1) um[[event]][nrow(um)] &lt;- 1 
     um[[et]] &lt;- tr ## reset time to relevant event times
     dap[start:(start-1+nrow(um)),] &lt;- um ## copy to dap
     start &lt;- start + nrow(um)
     setTxtProgressBar(prg, i)
   }
   close(prg)
   dap[1:(start-1),]
} ## tdpois

## model fitting...

data(pbc)
pbcseq$status1 &lt;- as.numeric(pbcseq$status==2) ## deaths
pb &lt;- tdpois(pbcseq) ## conversion
pb$tf &lt;- factor(pb$futime) ## add factor for event time

b &lt;- bam(z ~ tf - 1  +  trt + s(sqrt(protime)) + s(platelet) + 
         s(age) + s(bili) + s(albumin) + s(sqrt(ast)),
         family=poisson,data=pb,discrete=TRUE,nthreads=2)

chaz &lt;- tapply(fitted(b),pb$id,sum) ## cum. hazard by subject
d &lt;- tapply(pb$z,pb$id,sum) ## censoring indicator
mrsd &lt;- d - chaz ## Martingale residuals
drsd &lt;- sign(mrsd)*sqrt(-2*(mrsd + d*log(chaz))) ## deviance

te &lt;- sort(unique(pb$futime)) ## event times
di &lt;- pbcseq[pbcseq$id==25,] ## data for subject 25
## interpolate to te using app from ?cox.pht...
pd &lt;- data.frame(lapply(X=di,FUN=app,t=di$day,to=te)) 
pd$tf &lt;- factor(te)
X &lt;- predict(b,newdata=pd,type="lpmatrix")
eta &lt;- drop(X%*%coef(b)); H &lt;- cumsum(exp(eta))
J &lt;- apply(exp(eta)*X,2,cumsum)
se &lt;- diag(J%*%vcov(b)%*%t(J))^.5
par(mfrow=c(1,2))
plot(stepfun(te,c(1,exp(-H))),do.points=FALSE,ylim=c(0.7,1),
     ylab="S(t)",xlab="t (days)",main="",lwd=2)
lines(stepfun(te,c(1,exp(-H+se))),do.points=FALSE)
lines(stepfun(te,c(1,exp(-H-se))),do.points=FALSE)
rug(pbcseq$day[pbcseq$id==25]) ## measurement times

er &lt;- pbcseq[pbcseq$id==25,]
plot(er$day,er$ast);lines(te,pd$ast)

## 7.9 Location scale

library(MASS);library(mgcv)
b &lt;- gam(list(accel~s(times,bs="ad"),~s(times,bs="ad")),
         family=gaulss,data=mcycle)

## 7.9.1 Extreme rainfall
library(mgcv);library(gamair);data(swer)
b0 &lt;- gam(list(exra ~ s(nao)+ s(elevation)+ climate.region+
                      te(N,E,year,d=c(2,1),k=c(20,5)),
      ~ s(year)+ s(nao)+ s(elevation)+ climate.region+ s(N,E),
	      ~ s(elevation)+ climate.region),family=gevlss,data=swer)

b &lt;- gam(list(exra~ s(nao)+s(elevation)+climate.region+s(N,E),
         ~ s(year)+ s(elevation)+ climate.region+ s(N,E),
	        ~ climate.region),family=gevlss,data=swer)
plot(b,scale=0,scheme=c(1,1,3,1,1,3),contour.col="white",pages=1)

mu &lt;- fitted(b)[,1];rho &lt;- fitted(b)[,2]; xi &lt;- fitted(b)[,3]
fv &lt;- mu + exp(rho)*(gamma(1-xi)-1)/xi

Fi.gev &lt;- function(z,mu,sigma,xi) { ## GEV inverse cdf.
  xi[abs(xi)&lt;1e-8] &lt;- 1e-8 ## approximate xi=0, by small xi
  x &lt;- mu + ((-log(z))^-xi-1)*sigma/xi
}
mb &lt;- coef(b);Vb &lt;- vcov(b) ## posterior mean and cov
b1 &lt;- b ## copy fitted model object to modify
n.rep &lt;- 1000; br &lt;- rmvn(n.rep,mb,Vb) ## posterior sim
n &lt;- length(fitted(b))
sim.dat &lt;- cbind(data.frame(rep(0,n*n.rep)),swer$code)
for (i in 1:n.rep) {
  b1$coefficients &lt;- br[i,] ## copy sim coefs to gam object
  X &lt;- predict(b1,type="response");ii &lt;- 1:n + (i-1)*n
  sim.dat[ii,1] &lt;- Fi.gev(runif(n),X[,1],exp(X[,2]),X[,3])
}

stm &lt;- tapply(sim.dat[,1],sim.dat[,2],mean)
st98 &lt;- tapply(sim.dat[,1],sim.dat[,2],quantile,probs=0.98)

## 7.10 Multivariate
library(mgcv); library(gamair); data(mpg)
b &lt;- gam(list(city.mpg ~ fuel +style +drive +s(weight) +s(hp)
                         + s(make,bs="re"),
                hw.mpg ~ fuel +style +drive +s(weight) +s(hp)
                         + s(make,bs="re")),
               family = mvn(d=2) , data = mpg)

b1 &lt;- gam(list(city.mpg ~ fuel +style +drive +s(hp) +s(weight)
                          + s(make,bs="re"),
                 hw.mpg ~ fuel +style +drive +s(make,bs="re"),
                    1+2 ~ s(weight) +s(hp) -1),
               family = mvn(d=2) , data = mpg)

## 7.11 FDA
## 7.11.1 scalar-on-function
data(gas)
b &lt;- gam(octane~s(nm,by=NIR,k=50),data=gas)
par(mfrow=c(1,2))
plot(b,scheme=1,col=1)
plot(fitted(b),gas$octane)

## Prostate...
data(prostate)
b &lt;- gam(type ~ s(MZ,by=intensity,k=100),family=ocat(R=3),
         data=prostate,method="ML")
par(mfrow=c(1,3))
plot(b,rug=FALSE,scheme=1,xlab="Daltons",ylab="f(D)",
     cex.lab=1.6,cex.axis=1.4)
pb &lt;- predict(b,type="response") ## matrix of class probs
plot(factor(prostate$type),pb[,3])
qq.gam(b,rep=100,lev=.95)

prostate$type1 &lt;- prostate$type - 1 ## recode for multinom
b1 &lt;- gam(list(type1 ~ s(MZ,by=intensity,k=100),
               ~ s(MZ,by=intensity,k=100)),
          family=multinom(K=2),data=prostate)
plot(b1,pages=1,scheme=1,rug=FALSE)

## 7.11.2 Canadian weather
require(gamair);require(lattice);data(canWeather)
xyplot(T~time|region,data=CanWeather,type="l",groups=place)

aic &lt;- reml &lt;- rho &lt;- seq(0.9,0.99,by=.01)
for (i in 1:length(rho)) {
  b &lt;- bam(T ~ region + s(time,k=20,bs="cr",by=region) +
           s(time,k=40,bs="cr",by=latitude),
           data=CanWeather,AR.start=time==1,rho=rho[i])
  aic[i] &lt;- AIC(b); reml[i] &lt;- b$gcv.ubre
}

## End(Not run)

</code></pre>

<hr>
<h2 id='ch7.solutions'>Solution code for Chapter 7 GAMs in Practice: mgcv</h2><span id='topic+ch7.solutions'></span>

<h3>Description</h3>

<p>R code for Chapter 7 exercise solutions.</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code>, <code><a href="#topic+ch7">ch7</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gamair); library(mgcv)

## Q.1
## a)
data(hubble)
h1 &lt;- gam(y~s(x),data=hubble)
plot(h1) ## model is curved
h0 &lt;- gam(y~x,data=hubble)
h1;h0
AIC(h1,h0)

## b)
gam.check(h1) # oh dear
h2 &lt;- gam(y~s(x),data=hubble,family=quasi(var=mu))
gam.check(h2) # not great, but better
h2

## Q.2
## a)
library(MASS)
par(mfrow=c(2,2))
mc &lt;- gam(accel~s(times,k=40),data=mcycle)
plot(mc,residuals=TRUE,se=FALSE,pch=1)

## b)
mc1 &lt;- lm(accel~poly(times,11),data=mcycle)
termplot(mc1,partial.resid=TRUE)

## c)
mc2 &lt;- gam(accel~s(times,k=11,fx=TRUE),data=mcycle)
plot(mc2,residuals=TRUE,se=FALSE,pch=1)

## d)
mc3 &lt;- gam(accel~s(times,k=11,fx=TRUE,bs="cr"),data=mcycle)
plot(mc3,residuals=TRUE,se=FALSE,pch=1)

## e)
par(mfrow=c(1,1))
plot(mcycle$times,residuals(mc))

## f)
mcw &lt;- gam(accel~s(times,k=40),data=mcycle,
           weights=c(rep(400,20),rep(1,113)))
plot(mcw,residuals=TRUE,pch=1)
rsd &lt;- residuals(mcw)
plot(mcycle$times,rsd)
var(rsd[21:133])/var(rsd[1:20])

## g)
gam(accel~s(times,k=40,m=3),data=mcycle,
    weights=c(rep(400,20),rep(1,113)))

## Q.3
## b)
library(MASS)
n &lt;- nrow(mcycle)
A &lt;- matrix(0,n,n)
for (i in 1:n) {
  mcycle$y&lt;-mcycle$accel*0;mcycle$y[i] &lt;- 1
  A[,i] &lt;- fitted(gam(y~s(times,k=40),data=mcycle,sp=mc$sp))
}

## d)
plot(mcycle$times,A[,65],type="l",ylim=c(-0.05,0.15))

## e)
for (i in 1:n) lines(mcycle$times,A[,i])

## f)
par(mfrow=c(2,2))
mcycle$y&lt;-mcycle$accel*0;mcycle$y[65] &lt;- 1
for (k in 1:4) plot(mcycle$times,fitted(
     gam(y~s(times,k=40),data=mcycle,sp=mc$sp*10^(k-1.5))
     ),type="l",ylab="A[65,]",ylim=c(-0.01,0.12))

## Q.4
## a)
par(mfrow=c(1,1))
w &lt;- c(rep(400,20),rep(1,113))
m &lt;- 40;par(mfrow=c(1,1))
sp &lt;- seq(-13,12,length=m) ## trial log(sp)'s
AC1 &lt;- EDF &lt;- rep(0,m)
for (i in 1:m) { ## loop through s.p.'s
 b &lt;- gam(accel~s(times,k=40),data=mcycle,weights=w,
          sp=exp(sp[i]))
 EDF[i] &lt;- sum(b$edf)
 AC1[i] &lt;- acf(residuals(b),plot=FALSE)$acf[2]
}
plot(EDF,AC1,type="l");abline(0,0,col=2)

## Not run: 
## Q.5 - a bit slow - few seconds
## a)
data(co2s)
attach(co2s)
plot(c.month,co2,type="l")

## b)
b&lt;-gam(co2~s(c.month,k=300,bs="cr"))

## c)
pd &lt;- data.frame(c.month=1:(n+36))
fv &lt;- predict(b,pd,se=TRUE)
plot(pd$c.month,fv$fit,type="l")
lines(pd$c.month,fv$fit+2*fv$se,col=2)
lines(pd$c.month,fv$fit-2*fv$se,col=2)

## d)
b2 &lt;- gam(co2~s(month,bs="cc")+s(c.month,bs="cr",k=300),
           knots=list(month=seq(1,13,length=10)))

## e)
pd2 &lt;- data.frame(c.month=1:(n+36),
                  month=rep(1:12,length.out=n+36))
fv &lt;- predict(b2,pd2,se=TRUE)
plot(pd$c.month,fv$fit,type="l")
lines(pd$c.month,fv$fit+2*fv$se,col=2)
lines(pd$c.month,fv$fit-2*fv$se,col=2)

## End(Not run)

## Not run: 
## Q.6 - a bit slow - a few seconds
data(ipo)
n&lt;-nrow(ipo)
## create lagged variables ...
ipo$ir1 &lt;- c(NA,ipo$ir[1:(n-1)])
ipo$ir2 &lt;- c(NA,NA,ipo$ir[1:(n-2)])
ipo$ir3 &lt;- c(NA,NA,NA,ipo$ir[1:(n-3)])
ipo$ir4 &lt;- c(NA,NA,NA,NA,ipo$ir[1:(n-4)])
ipo$dp1 &lt;- c(NA,ipo$dp[1:(n-1)])
ipo$dp2 &lt;- c(NA,NA,ipo$dp[1:(n-2)])
ipo$dp3 &lt;- c(NA,NA,NA,ipo$dp[1:(n-3)])
ipo$dp4 &lt;- c(NA,NA,NA,NA,ipo$dp[1:(n-4)])
## fit initial model and look at it ...
b&lt;-gam(n.ipo~s(ir1)+s(ir2)+s(ir3)+s(ir4)+s(log(reg.t))+
   s(dp1)+s(dp2)+s(dp3)+s(dp4)+s(month,bs="cc")+s(t,k=20),
   data=ipo,knots=list(month=seq(1,13,length=10)),
   family=poisson,gamma=1.4)
par(mfrow=c(3,4))
plot(b,scale=0)
summary(b)
## re-fit model dropping ir4 ...
b1 &lt;- gam(n.ipo~s(ir1)+s(ir2)+s(ir3)+s(log(reg.t))+s(dp1)+
          s(dp2)+s(dp3)+s(dp4)+s(month,bs="cc")+s(t,k=20),
          data=ipo,knots=list(month=seq(1,13,length=10)),
          family=poisson,gamma=1.4)
par(mfrow=c(3,4))
plot(b1,scale=0)
summary(b1)
## residual checking ...
gam.check(b1)
par(mfrow=c(1,1))
acf(residuals(b1))

## End(Not run)

## Q.7
data(wine)
wm&lt;-gam(price~s(h.rain)+s(s.temp)+s(h.temp)+s(year),
    data=wine,family=Gamma(link=identity),gamma=1.4)
plot(wm,pages=1,residuals=TRUE,pch=1,scale=0)
acf(residuals(wm))
gam.check(wm)
predict(wm,wine,se=TRUE)

## Q.8
## a)
par(mfrow=c(1,1))
data(blowfly)
bf &lt;- blowfly
plot(bf$day,bf$pop,type="l")

## b)
## prepare differenced and lagged data ...
n &lt;- nrow(bf)
bf$dn &lt;- c(NA,bf$pop[2:n]-bf$pop[1:(n-1)])
lag &lt;- 6
bf$n.lag &lt;- c(rep(NA,lag),bf$pop[1:(n-lag)])
bf1 &lt;- bf[(lag+1):n,] # strip out NAs, for convenience
## fit model, note no intercept ...
b&lt;-gam(dn~n.lag+pop+s(log(n.lag),by=n.lag)+
       s(log(pop),by=-pop)-1,data=bf1)
plot(b,pages=1,scale=-1,se=FALSE) ## effects
plot(abs(fitted(b)),residuals(b))
acf(residuals(b))

## c)
fv &lt;- bf$pop
e &lt;- rnorm(n)*0 ## increase multiplier for noisy version
min.pop &lt;- min(bf$pop);max.pop &lt;- max(bf$pop)
for (i in (lag+1):n) { ## iteration loop
  dn &lt;- predict(b,data.frame(n.lag=fv[i-lag],pop=fv[i-1]))
  fv[i] &lt;- fv[i-1]+dn + e[i];
  fv[i]&lt;-min(max.pop,max(min.pop,fv[i]))
}
plot(bf$day,fv,type="l")

## Not run: 
## Q.9 - takes several minutes
## a)
data(chl)
pairs(chl,pch=".")

## b)
fam &lt;- quasi(link=log,var=mu^2)
cm &lt;- gam(chl ~ s(I(chl.sw^.4),bs="cr",k=20)+
      s(I(bath^.25),bs="cr",k=60)+s(jul.day,bs="cr",k=20),
      data=chl,family=fam,gamma=1.4)
gam.check(cm)
summary(cm)

## c)
## create fit and validation sets ...
set.seed(2)
n&lt;-nrow(chl);nf &lt;- floor(n*.9)
ind &lt;- sample(1:n,nf,replace=FALSE)
chlf &lt;- chl[ind,];chlv &lt;- chl[-ind,]
## fit to the fit set
cmf&lt;-gam(chl ~ s(I(chl.sw^.4),bs="cr",k=20)+
     s(I(bath^.25),bs="cr",k=60)+s(jul.day,bs="cr",k=20),
     data=chlf,family=fam,gamma=1.4)
## evaluate prop. dev. explained for validation set
y &lt;- chlv$chl;w &lt;- y*0+1
mu &lt;- predict(cmf,chlv,type="response")
pred.dev &lt;- sum(fam$dev.resids(y,mu,w))
null.dev &lt;- sum(fam$dev.resids(y,mean(y),w))
1-pred.dev/null.dev # prop dev. explained

## End(Not run)

## Not run: 
## Q.10 - a few seconds run time
## a)
g1&lt;-gamm(weight ~ Variety + s(Time)+
    s(Time,by=ordered(Variety)),data=Soybean,
    family=Gamma(link=log), random=list(Plot=~Time))
plot(g1$lme) ## standard mean variance plot
par(mfrow=c(1,3))
plot(g1$gam,residuals=TRUE,all.terms=TRUE,scale=0) ## gam plot

## b)
summary(g1$gam) ## evidence for variety dependence
## could also do following ....
g2 &lt;- gamm(weight~s(Time),family=Gamma(link=log),
      data=Soybean,random=list(Plot=~Time))
g3 &lt;- gamm(weight~Variety+s(Time),family=Gamma(link=log),
      data=Soybean,random=list(Plot=~Time))
## following only a rough guide, but also supports g1 ...
AIC(g1$lme,g2$lme,g3$lme)

## Q.11
data(med); head(med) ## look at data
data(coast)

## initial plots...
plot(med$lo,med$la,cex=0.2+med$count^.5/10,col="grey",
     pch=19,xlab="lo",ylab="la",main="mackerel")
ind &lt;- med$count==0
points(med$lo[ind],med$la[ind],cex=0.1,pch=19)
lines(coast)
## ... survey seems to cover spawning area this time!

require(mgcv)
m1 &lt;- gam(count~s(lo,la,k=100)+s(T.surf)+s(T.20)+s(I(b.depth^.5))+s(Sal20)+
          s(ship,bs="re")+offset(log(vol)),data=med,select=TRUE,family=tw)
gam.check(m1) ## mean variance relationship not quite right?

m2 &lt;- gam(count~s(lo,la,k=100)+s(T.surf)+s(T.20)+s(I(b.depth^.5))+s(Sal20)+
          s(ship,bs="re")+offset(log(vol)),data=med,select=TRUE,family=nb)
gam.check(m2)

par(mfrow=c(1,2)) ## re-check residuals vs fitted
plot(fitted(m1)^.5,residuals(m1));plot(fitted(m2)^.5,residuals(m2))

AIC(m1,m2) ## neg bin much better
plot(m2,pages=1) ## effects


## End(Not run)
</code></pre>

<hr>
<h2 id='chicago'>Chicago air pollution and death rate data</h2><span id='topic+chicago'></span>

<h3>Description</h3>

<p> Daily air pollution and death rate data for Chicago.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(chicago)
</code></pre>


<h3>Format</h3>

<p>A data frame with 7 columns and 5114 rows. Each row refers to one day.
The columns are:
</p>

<dl>
<dt>death</dt><dd><p> total deaths (per day).</p>
</dd>
<dt>pm10median</dt><dd><p>median particles in 2.5-10 per cubic m</p>
</dd>
<dt>pm25median</dt><dd><p>median particles &lt; 2.5 mg per cubic m (more dangerous).</p>
</dd>
<dt>o3median</dt><dd><p>Ozone in parts per billion</p>
</dd>
<dt>so2median</dt><dd><p>Median Sulpher dioxide measurement</p>
</dd>
<dt>time</dt><dd><p>time in days</p>
</dd>
<dt>tmpd</dt><dd><p>temperature in fahrenheit</p>
</dd>
</dl>


<h3>Details</h3>

<p> See the <code>NMMAPSdata</code> package for fuller details. Note that
there are missing values in some fields.
</p>


<h3>Source</h3>

 
<p>Roger D. Peng, Leah J. Welty and Aiden McDermott. R package NMMAPSdata.
</p>


<h3>References</h3>

<p>Peng, R.D. and Welty, L.J. (2004) The NMMAPSdata package. R News 4(2).
</p>
<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R
</p>

<hr>
<h2 id='chl'>Chlorophyll data</h2><span id='topic+chl'></span>

<h3>Description</h3>

<p>Data relating to the callibration of remote sensed satellite
data. The SeaWifs satellite provides estimates of chlorophyll concentration
at the ocean surface from measurements of ocean surface colour. It is of
interest to attempt to use these data to predict direct bottle measurements of
chl. conc.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(chl)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 columns and 13840 rows. The columns are:
</p>

<dl>
<dt>lon</dt><dd><p>longitude</p>
</dd>
<dt>lat</dt><dd><p>latitude</p>
</dd>
<dt>jul.day</dt><dd><p>Julian day (i.e. day of year starting at Jan 1st.)</p>
</dd>
<dt>bath</dt><dd><p>Ocean depth in metres.</p>
</dd>
<dt>chl</dt><dd><p>direct chlorophyll concentration measured at given location from
a bottle sample.</p>
</dd>
<dt>chl.sw</dt><dd><p>chl. conc. as measured by Seawifs Satellite</p>
</dd>
</dl>


<h3>Source</h3>

<p><a href="https://oceancolor.gsfc.nasa.gov/SeaWiFS/">https://oceancolor.gsfc.nasa.gov/SeaWiFS/</a>
</p>
<p>and the World Ocean Database.
</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(chl)
  with(chl,plot(chl,chl.sw))
</code></pre>

<hr>
<h2 id='co2s'>Atmospheric CO2 at South Pole</h2><span id='topic+co2s'></span>

<h3>Description</h3>

<p>Monthly CO2 concentration in parts per million at the South Pole.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(co2s)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 columns and 507 rows. The columns are:
</p>

<dl>
<dt>co2</dt><dd><p>atmospheric CO2 concentration in parts per million</p>
</dd>
<dt>c.month</dt><dd><p>cumulative number of months since Jan 1957</p>
</dd>
<dt>month</dt><dd><p>month of year</p>
</dd>
</dl>


<h3>Source</h3>

<p><a href="http://cdiac.esd.ornl.gov/trends/co2/">http://cdiac.esd.ornl.gov/trends/co2/</a>
</p>


<h3>References</h3>

<p>Keeling C.P. and T.P Whorf (2000) Atmospheric CO2 records from sites in the
SIO air sampling network. In Trends: A Compedium of Data on Global
Change. Carbon Dioxide Analyis Center, Oak Ridge National Laboratory,
U.S. Department of Energy, Oak Ridge Tenn., USA
</p>
<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(co2s)
  with(co2s,plot(c.month,co2,type="l",ylab=
  expression(paste(CO[2]," in ppm.")),xlab="Month since Jan. 1957"))
</code></pre>

<hr>
<h2 id='coast'>European coastline from -11 to 0 East and 
from 43 to 59 North</h2><span id='topic+coast'></span>

<h3>Description</h3>

<p> The data are longitudes (degrees E) and latitudes (degrees N) defining points that can be joined up to get the European coastline in the rectangle (-11E,43N)-(0E,59N). Discontinuous sections of coast are separated by NA's. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(coast)
</code></pre>


<h3>Format</h3>

<p> A data frame with 2 columns.
</p>
 
<dl>
<dt>lon</dt><dd><p>Longitude in degrees East for points used to define the coast.</p>
</dd>
<dt>lat</dt><dd><p>Latitude in degrees North for points used to define the coast.</p>
</dd>
</dl>


<h3>Details</h3>

 <p><code>lon</code>, <code>lat</code> together define the co-ordinates of points that can be joined up in order to 
plot the coastline. The original data come from the NOAA www site given below,
but have been substantially thinned, to a much lower resultion than the source.
</p>


<h3>Author(s)</h3>

<p> Simon Wood.
</p>


<h3>References</h3>

<p> Originally from...
<code>http://rimmer.ngdc.noaa.gov/coast/</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(coast)
# plot the entire coast .....
plot(coast$lon,coast$lat,type="l")
# or draw it clipped to  whatever the current plot is....
lines(coast$lon,coast$lat,col="blue")
</code></pre>

<hr>
<h2 id='engine'>Engine wear versus size data</h2><span id='topic+engine'></span>

<h3>Description</h3>

<p> Data on engine wear against engine size for 19 Volvo car engines.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(engine)
</code></pre>


<h3>Format</h3>

<p>A data frame with 2 columns and 19 rows. Each row refers to one engine model.
The columns are:
</p>

<dl>
<dt>wear</dt><dd><p>an index of engine wear rate.</p>
</dd>
<dt>size</dt><dd><p>cylinder capacity in litres.</p>
</dd>
</dl>


<h3>Details</h3>

<p> See the source for further details.
</p>


<h3>Source</h3>

<p>Originally from... 
<code>http://www3.bc.sympatico.ca/Volvo_Books/engine3.html</code>
</p>

<hr>
<h2 id='gamair-package'>
Data and scripts for &lsquo;Generalized Additive Models: An Introduction with R&rsquo;
</h2><span id='topic+gamair-package'></span><span id='topic+gamair'></span>

<h3>Description</h3>

<p>This package contains the data sets used in the book
<em>Generalized Additive Models: An Introduction with R</em>, which covers linear and generalized linear models, GAMs as implemented in package <code>mgcv</code> and
mixed model extensions of these.
</p>
<p>There are help files containing the R code for each chapter and its exercise solutions, for the second edition of the book.
</p>
<p>The script files for the first edition of the book can be found in the 'scripts' folder of the 'inst' folder of the source package. They have been modified slightly to work with recent versions of mgcv (e.g. &gt;= 1.7-0).
</p>


<h3>Details</h3>

<p>Each dataset has its own help page, which describes the dataset, and
gives the original source and associated references. All datasets have been
reformatted into standard R data frames. Some smaller datasets from the book
have not been included. Datasets from other R packages have not been
included, with the exception of a distillation of one set from the 
<code>NMMAPSdata</code> package.
</p>
<p>Index:
</p>
<pre>
aral                    Aral sea chlorophyll
aral.bnd                Aral sea boundary
bird                    Bird distribution data from Portugal
blowfly                 Nicholson's Blowfly data
bone                    Bone marrow treatment survival data.
brain                   Brain scan data
cairo                   Daily temperature data for Cairo
CanWeather              Canadian annual temperature curves
chicago                 Chicago air pollution and death rate data
chl                     Chlorophyll data
co2s                    Atmospheric CO2 at South Pole
coast                   European coastline from -11 to 0 East and from
                        43 to 59 North.
engine                  Engine wear versus size data
german.polys.rda        Polygons defining german local regions
gamair                  Generalized Additive Models: An Introduction
                        With R
harrier                 Hen Harriers Eating Grouse
hubble                  Hubble Space Telescope Data
ipo                     Initial Public Offering Data
larynx                  Cancer of the larynx in Germany
mack                    Egg data from 1992 mackerel survey
mackp                   Prediction grid data for 1992 mackerel egg
                        model
med                     2010 mackerel egg survey data
meh                     2010 horse mackerel egg survey data
prostate                Protein mass spectra for prostate diagnosis
sitka                   Sitka spruce growth and ozone data
sole                    Sole Eggs in the Bristol Channel
sperm.comp1             Sperm competition data I
sperm.comp2             Sperm competition data II
swer                    Swiss extreme ranfall data
stomata                 Artifical stomatal area data
wesdr                   Diabetic retinopathy data
wine                    Bordeaux Wines
</pre>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon@r-project.org&gt;
</p>
<p>Maintainer: Simon Wood &lt;simon@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2006,2017) <em>Generalized Additive Models: An Introduction with R</em>, CRC
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+mgcv-package">mgcv</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(help=gamair)
</code></pre>

<hr>
<h2 id='gas'>Octane rating data</h2><span id='topic+gas'></span>

<h3>Description</h3>

<p>The octane rating of fuel determines its &lsquo;knocking&rsquo; resistance. So the higher the octane rating the higher the compression ratio that an engine can run at. Traditionally octane measurement involves comparing the knocking resistance of fuel samples to standard mixtures in special variable compression ratio engines. This is an expensive process relative to obtaining the near infra-red spectrum of a sample. It would be good to be able to predict octane rating from the spectrum.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(gas)
</code></pre>


<h3>Format</h3>

<p>A three item list
</p>

<dl>
<dt>octane</dt><dd><p>Octane rating of gasoline (petrol) sample. </p>
</dd>
<dt>NIR</dt><dd><p>A matrix each row of which contains the near infra-red reflectance spectrum of the corresponding gasoline sample.</p>
</dd>
<dt>nm</dt><dd><p> Matrix of same dimension as <code>NIR</code> containing wavelengths at which measurements were taken.</p>
</dd>
</dl>



<h3>Details</h3>

<p>A scalar-on-function regression (also known as &lsquo;signal regression&rsquo;) works quite well for these data.
</p>


<h3>Source</h3>

<p>Originally from the <code>pls</code> package
</p>
<p><a href="https://cran.r-project.org/package=pls">https://cran.r-project.org/package=pls</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(gas)
## plot some spectra...
with(gas,plot(nm[1,],NIR[1,],type="l",ylab="log(1/R)",
     xlab="wavelength (nm)",col=1))
text(1000,1.2,"octane");text(1000,1.2-.1,gas$octane[1],col=1)
for (i in 2:8) { lines(gas$nm[i,],gas$NIR[i,],col=i)
  text(1000,1.2-.1*i,gas$octane[i],col=i)
}

## Fit scalar on function regression...

b &lt;- gam(octane~s(nm,by=NIR,k=50),data=gas)

gam.check(b)

par(mfrow=c(1,2))
plot(b,scheme=1)
plot(fitted(b),gas$octane,xlab="fitted octane",
     ylab="observed octane");abline(0,1)

</code></pre>

<hr>
<h2 id='harrier'>Hen Harriers Eating Grouse</h2><span id='topic+harrier'></span>

<h3>Description</h3>

<p>Data on the rate at which Hen Harriers consume Grouse as a
function of Grouse density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(harrier)
</code></pre>


<h3>Format</h3>

<p>A data frame with 2 columns and 37 rows. The columns are:
</p>

<dl>
<dt>Grouse.Density</dt><dd><p>Density of Grouse per square kilometre.</p>
</dd>
<dt>Consumption.Rate</dt><dd><p>Number of Grouse consumed per Hen Harrier per day.</p>
</dd>
</dl>


<h3>Details</h3>

<p>Data have been read from Figure 1 of Asseburg et al. (2005)</p>


<h3>Source</h3>

 
<p>Asseburg, C., S. Smout, J. Matthiopoulos, C. Fernandez, S. Redpath,
S. Thirgood and J. Harwood (2005) The functional response of a generalist
predator. Web preprint 
</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(harrier)
  with(harrier,plot(Grouse.Density,Consumption.Rate))
</code></pre>

<hr>
<h2 id='hubble'>Hubble Space Telescope Data</h2><span id='topic+hubble'></span>

<h3>Description</h3>

<p> Data on distances and velocities of 24 galaxies containing
Cepheid stars, from the Hubble space telescope key project to measure the
Hubble constant.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hubble)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 columns and 24 rows. The columns are:
</p>

<dl>
<dt>Galaxy</dt><dd><p> A (factor) label identifying the galaxy.</p>
</dd>
<dt>y</dt><dd><p> The galaxy's relative velocity in kilometres per second.</p>
</dd>
<dt>x</dt><dd><p> The galaxy's distance in Mega parsecs. 1 parsec is 3.09e13 km.</p>
</dd>
</dl>


<h3>Details</h3>

<p>Cepheids are variable stars which have a known relationship between 
brightness and period. Hence the distance to galaxies containing these stars
can be estimated from the observed brightness of the Cepheid, relative to its
absolute brightness as predicted by its period. The velocity of the galaxy can
be estimated from its mean red-shift.
</p>
<p>The data can be used to get a reasonably good idea of the age of the universe. 
A data free alternative estimate of 6000 years is given in the reference (not
the source!).
</p>


<h3>Source</h3>

<p>Tables 4 and 5 of Freedman et al. 2001. The Astrophysical Journal
553:47-72</p>


<h3>References</h3>

<p>Freedman et al. (2001) Final results from the Hubble space telescope key
project to measure the Hubble constant. The Astrophysical Journal (553), 47-72.
</p>
<p>http://www.icr.org/pubs/imp/imp-352.htm
</p>
<p>NUCLEAR DECAY: EVIDENCE FOR A YOUNG WORLD
- IMPACT No. 352 October 2002
by D. Russell Humphreys, Ph.D.
</p>
<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>

<hr>
<h2 id='ipo'>Initial Public Offering Data</h2><span id='topic+ipo'></span>

<h3>Description</h3>

<p> Data on the relationship between the number of initial public
offerings (of shares in a company) and other potentially important variables.
It is probably necessary to lag some of the explanatory variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ipo)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 columns and 156 rows. The columns are:
</p>

<dl>
<dt>n.ipo</dt><dd><p>number of initial pubilc offerings each month.</p>
</dd>
<dt>ir</dt><dd><p>the average initial return (volume weighted): this is the percentage
difference between the offer proce of shares and the price after the first day
of trading.</p>
</dd>
<dt>dp</dt><dd><p>the average percentage difference between middle of the price range
proposed at first filing of the IPO, and the eventual offer price.</p>
</dd>
<dt>reg.t</dt><dd><p>the average time between filing and offer.</p>
</dd>
<dt>t</dt><dd><p>time, in months.</p>
</dd>
<dt>month</dt><dd><p>month of the year (1 = January).</p>
</dd>
</dl>


<h3>Source</h3>

 
<p><a href="http://schwert.ssb.rochester.edu">http://schwert.ssb.rochester.edu</a>
</p>


<h3>References</h3>

<p>Lowry, M. and G.W. Schwert (2002) IPO market cycles: Bubbles or sequential
learning? The Journal of Finance 67(3), 1171-1198
</p>
<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(ipo)
  pairs(ipo)
</code></pre>

<hr>
<h2 id='Larynx'>Cancer of the larynx in Germany</h2><span id='topic+Larynx'></span><span id='topic+german.polys'></span>

<h3>Description</h3>

<p>The data give counts of deaths from cancer of the Larynx by region of Germany from 1986 to 1990, along with the expected count according to the populaiton of the region and the total deaths for the whle of Germany. A list of polygons defining the boundaries of the districts is also provided. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(larynx)
data(german.polys)
</code></pre>


<h3>Format</h3>

<p>The <code>Larynx</code> data frame has the following columns
</p>

<dl>
<dt>region</dt><dd><p>A factor with 544 levels identifying the health reporting region.</p>
</dd>
<dt>E</dt><dd><p>Expected number of deaths according to population of region and pan-German total.</p>
</dd>
<dt>Y</dt><dd><p>Number of deaths from cancer of the Larynx in the region.</p>
</dd>
<dt>x</dt><dd><p>A measure of level of smoking in the region.</p>
</dd>
</dl>

<p><code>german.polys</code> is a list with one item per health reporting region in <code>Larynx</code>. The name of each item identifies the region using the same labels as <code>Larynx$region</code>. Each item is a two column matrix defining a polygon approximating the outline of the region it relates to. Each row of the matrix defines a polygon vertex. <code>NA</code> rows separate geographically disjoint areas which are part of the same region. 
</p>


<h3>Details</h3>

<p>Note that the polygons are set up to exactly share vertices with their neighbours, which facilitates the auto-identification of neighbourhood structures.   
</p>


<h3>Source</h3>

<p>Data are from the INLA website:
</p>
<p><a href="http://www.r-inla.org/">http://www.r-inla.org/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(larynx);data(german.polys)

## plot raw deaths over expected deaths by region...
polys.plot(german.polys,Larynx$Y/Larynx$E)

## Fit additive model with Gauss MRF for space and smooth of
## smoking level. k somewhat low to reduce computational time
b &lt;- gam(Y~s(region,k=60,bs="mrf",xt=list(polys=german.polys)) +
offset(log(E))+s(x,k=10),family=poisson,data=Larynx,method="REML")

summary(b)
plot(b,scheme=c(0,1),pages=1)
</code></pre>

<hr>
<h2 id='mack'>Egg data from 1992 mackerel survey</h2><span id='topic+mack'></span>

<h3>Description</h3>

<p> The data relate to the distribution of mackerel eggs and were collected as part of the 1992 mackerel survey aimed at assessing the mackerel spawning stock biomass using the daily egg production method. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mack)
</code></pre>


<h3>Format</h3>

<p> A data frame with 16 columns. Each row corresponds to one sample of 
eggs. 
</p>

<dl>
<dt>egg.count</dt><dd><p>The number of stage I eggs in this sample.</p>
</dd>
<dt>egg.dens</dt><dd><p>The number of stage I eggs per square metre of sea surface, produced per day. This is calculated from <code>egg.count</code> and other information about sampling net size, and egg stage duration.</p>
</dd>
<dt>b.depth</dt><dd><p>The sea bed depth at the sampling location.</p>
</dd>
<dt>c.dist</dt><dd><p>The distance from the sample location to the 200m contour measured in degrees as if degrees latitude equalled degrees longitude, which actually they don't.</p>
</dd>
<dt>lon</dt><dd><p>The longitude of the sample station in degrees east.</p>
</dd>
<dt>lat</dt><dd><p>The latitude of the sample station in degrees north.</p>
</dd>
<dt>time</dt><dd><p>The time of day (in hours) at which the sample was taken.</p>
</dd>
<dt>salinity</dt><dd><p>The salinity (saltiness) of the water at the sampling location.</p>
</dd>
<dt>flow</dt><dd><p>Reading from the flow meter attached to the sampling net - used for calibration.</p>
</dd>
<dt>s.depth</dt><dd><p>The depth that the sampling net started sampling from (the net is dropped to this depth and then hauled up to the surface, filtering eggs etc out of the water as it goes).</p>
</dd>
<dt>temp.surf</dt><dd><p>The temperature at the sea surface at the sampling location.</p>
</dd>
<dt>temp.20m</dt><dd><p>The temperature 20m down at the sampling location.</p>
</dd>
<dt>net.area</dt><dd><p>The area of the sampling net in square metres.</p>
</dd>
<dt>country</dt><dd><p>A code identifying the country responsible for the boat that took  this sample.</p>
</dd>
<dt>vessel</dt><dd><p>A code identifying the boat that took this sample.</p>
</dd>
<dt>vessel.haul</dt><dd><p>A code uniquely identifying this sample, given that the vessel is known.</p>
</dd>
</dl>



<h3>Details</h3>

<p> At each of a number of stations located as defined in <code>lon</code>
and <code>lat</code>, mackerel eggs were sampled by hauling a fine net up from deep below the sea surface to the sea surface. The egg count data are obtained from the resulting samples, and these have been converted to (stage I) eggs produced per metre squared per day - the egg density data. Other possibly useful predictor variable information has been recorded, along with identification information, and some information that is probably useless!  
</p>


<h3>Source</h3>

<p> The data are effectively a combination of datasets <code>mackerel</code>
and <code>smacker</code> from the <code>sm</code> library. They were originally analyzed
using GAMs by:
</p>
<p>Borchers, D.L., S.T. Buckland, I.G. Priede and S. Ahmadi (1997) &quot;Improving the precision of the daily egg 
production method using generalized additive models&quot;. Can. J. Fish. Aquat. Sci. 54:2727-2742. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mack)
# plot the egg densities against location
plot(mack$lon,mack$lat,cex=0.2+mack$egg.dens/150,col="red")
</code></pre>

<hr>
<h2 id='mackp'>Prediction grid data for 1992 mackerel egg model</h2><span id='topic+mackp'></span>

<h3>Description</h3>

<p> This data frame provides a regular grid of values of some predictor variables useful for modelling mackerel egg abundances. Its main purpose is to enable mackerel egg densities to be predicted over a regular spatial grid within the area covered by the 1992 mackerel egg survey (see <code>mack</code>), using a fitted generalised additive model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mackp)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5 columns. Each row corresponds to one spatial location within the survey area. 
The columns are as follows:
</p>

<dl>
<dt>lon</dt><dd><p>Longitude of the gridpoint in degrees east</p>
</dd>
<dt>lat</dt><dd><p>Latitude of the gridpoint in degrees north.</p>
</dd>
<dt>b.depth</dt><dd><p>The sea bed depth at the gridpoint.</p>
</dd>
<dt>c.dist</dt><dd><p>The distance from the gridpoint to the 200m sea bed depth contour.</p>
</dd>
<dt>salinity</dt><dd><p>Salinity interpolated onto the grid (from <code>mack</code> measurements).</p>
</dd>
<dt>temp.surf</dt><dd><p>Surface temperature interpolated onto grid (from <code>mack</code> data).</p>
</dd>
<dt>temp.20m</dt><dd><p>Temperature at 20m interpolated from <code>mack</code> data.</p>
</dd>
<dt>area.index</dt><dd><p> An indexing vector that enables straightforward copying of
the other variables into a matrix suitable for plotting against longitude and
lattitude using <code>image()</code>. See the example below.</p>
</dd>
</dl>


<h3>Details</h3>

<p> The grid is defined on a series of 1/4 degree lon-lat squares.
</p>


<h3>References</h3>

<p>Borchers, D.L., S.T. Buckland, I.G. Priede and S. Ahmadi (1997) &quot;Improving the precision of the daily egg 
production method using generalized additive models&quot;. Can. J. Fish. Aquat. Sci. 54:2727-2742. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example of how to use `area.index' to paste gridded info.
## into a square grid (of NA's) for plotting
data(mackp)
lon&lt;-seq(-15,-1,1/4);lat&lt;-seq(44,58,1/4)
zz&lt;-array(NA,57*57)
zz[mackp$area.index]&lt;-mackp$b.depth
image(lon,lat,matrix(zz,57,57))
</code></pre>

<hr>
<h2 id='meh'>Data from 2010 horse mackerel and mackerel egg survey</h2><span id='topic+meh'></span><span id='topic+med'></span>

<h3>Description</h3>

<p> The data relate to the distribution of horse mackerel (<code>meh</code>, Trachurus trachurus) eggs and mackerel (<code>med</code>, Scomber scombrus) eggs and were collected as part of the 2010 mackerel survey aimed at assessing the mackerel spawning stock biomass using the daily egg production method. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(med)
data(meh)
</code></pre>


<h3>Format</h3>

<p> A data frame with the following columns. Each row corresponds to one sample of eggs. 
</p>

<dl>
<dt>count</dt><dd><p>The number of stage I eggs in this sample.</p>
</dd>
<dt>la</dt><dd><p>sample station latitude</p>
</dd>
<dt>lo</dt><dd><p>sample station longitude</p>
</dd>
<dt>vol</dt><dd><p>volume of water sampled</p>
</dd>
<dt>T.surf</dt><dd><p>surface temperature in centigrade</p>
</dd>
<dt>T.x</dt><dd><p>temperature at x metres depth.</p>
</dd>
<dt>T1.x</dt><dd><p>Second temperature measurements.</p>
</dd>
<dt>Sal20</dt><dd><p>Salinity at 20m depth</p>
</dd>
<dt>b.depth</dt><dd><p>seabed depth in metres for <code>med</code> only.</p>
</dd>
<dt>lon</dt><dd><p>The longitude of the sample station in degrees east.</p>
</dd>
<dt>lat</dt><dd><p>The latitude of the sample station in degrees north.</p>
</dd>
<dt>time</dt><dd><p>The time of day (in hours) at which the sample was taken.</p>
</dd>
<dt>salinity</dt><dd><p>The salinity (saltiness) of the water at the sampling location.</p>
</dd>
<dt>period</dt><dd><p>sampling period</p>
</dd>
<dt>country</dt><dd><p>Country responsible for sample</p>
</dd>
<dt>ship</dt><dd><p>Vessel ID</p>
</dd>
<dt>DT</dt><dd><p>sample data and time</p>
</dd>
<dt>ID</dt><dd><p>Sample ID</p>
</dd>
<dt>gear</dt><dd><p>type of sampling gear used</p>
</dd>
</dl>

<p>The remaining fields are undocumented.
</p>


<h3>Details</h3>

<p>The original data files do not always exactly match the file documentation, so these data should not be treated as definitive.  
</p>


<h3>Source</h3>

<p>ICES Eggs and Larvae Dataset 2012, ICES, Copenhagen
</p>
<p><a href="http://www.ices.dk/">http://www.ices.dk/</a>
</p>
<p><a href="http://eggsandlarvae.ices.dk/Download.aspx">http://eggsandlarvae.ices.dk/Download.aspx</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair)
par(mfrow=c(1,2))
data(meh);data(med);data(coast)
# plot the egg counts against location
plot(meh$lo,meh$la,cex=0.2+meh$count^.5/10,col="grey",
     pch=19,xlab="lo",ylab="la",main="horse mackerel")
ind &lt;- meh$count==0
points(meh$lo[ind],meh$la[ind],cex=0.1,pch=19)
lines(coast)

# same for med
plot(med$lo,med$la,cex=0.2+med$count^.5/10,col="grey",
     pch=19,xlab="lo",ylab="la",main="mackerel")
ind &lt;- med$count==0
points(med$lo[ind],med$la[ind],cex=0.1,pch=19)
lines(coast)

</code></pre>

<hr>
<h2 id='mpg'>Data on automobile efficiency on town streets and highway.</h2><span id='topic+mpg'></span>

<h3>Description</h3>

<p>Fuel efficiency in miles per gallon for a variety of cars in the USA. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mpg)
</code></pre>


<h3>Format</h3>

<p>A data frame listing fuel efficiency and other car characteristics including
</p>

<dl>
<dt>symbol</dt><dd><p>Insurers measure of relative riskiness of car from -3 (safe) to 3 (risky)</p>
</dd>
<dt>loss</dt><dd><p>average insurance loss payment per insured vehicle per year.</p>
</dd>
<dt>hw.mpg</dt><dd><p>Fuel consumption on highway as miles per US gallon.</p>
</dd>
<dt>city.mpg</dt><dd><p>Fuel consumption in town as miles per US gallon.</p>
</dd>
<dt>make</dt><dd><p>Name of car maker.</p>
</dd>
<dt>fuel</dt><dd><p>2 level factor. <code>gas</code> or <code>diesel</code>.</p>
</dd>
<dt>aspir</dt><dd><p>2 level factor. <code>std</code> or <code>turbo</code>.</p>
</dd>
<dt>doors</dt><dd><p>2 level factor. <code>two</code> or <code>four</code>.</p>
</dd>
<dt>style</dt><dd><p>Factor indicating style of car.</p>
</dd>
<dt>drive</dt><dd><p>3 level factor indicating front, rear or all wheel drive: <code>fwd</code>, <code>rwd</code> or <code>4wd</code>.</p>
</dd>
<dt>eng.loc</dt><dd><p>Engine location</p>
</dd>
<dt>wb</dt><dd><p>wheel base in inches</p>
</dd>
<dt>length</dt><dd><p>in inches</p>
</dd>
<dt>width</dt><dd><p>in inches</p>
</dd>
<dt>height</dt><dd><p>in inches</p>
</dd>
<dt>weight</dt><dd><p>in pounds</p>
</dd>
<dt>eng.type</dt><dd><p>Factor giving engine type</p>
</dd>
<dt>cylinders</dt><dd><p>Factor for number of cylinders</p>
</dd>
<dt>eng.cc</dt><dd><p>cubic capicity of engine in cubic inches.</p>
</dd>
<dt>fuel.sys</dt><dd><p>fuel system</p>
</dd>
<dt>bore</dt><dd><p>in inches</p>
</dd>
<dt>stroke</dt><dd><p>in inches</p>
</dd>
<dt>comp.ratio</dt><dd><p>compression ratio</p>
</dd>
<dt>hp</dt><dd><p>horse power</p>
</dd>
<dt>rpm</dt><dd><p>maximum RPM</p>
</dd>
<dt>price</dt><dd><p>in US dollars</p>
</dd>
</dl>


<h3>Details</h3>

<p>Data were collected by Jeffrey C. Schlimmer from
1) 1985 Model Import Car and Truck Specifications, 1985 Ward's
Automotive Yearbook.
2) Personal Auto Manuals, Insurance Services Office, 160 Water
Street, New York, NY 10038 
3) Insurance Collision Report, Insurance Institute for Highway
Safety, Watergate 600, Washington, DC 20037
</p>


<h3>Source</h3>

 
<p><a href="https://archive.ics.uci.edu/ml/datasets/Automobile">https://archive.ics.uci.edu/ml/datasets/Automobile</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2006) Generalized Additive Models: An Introduction with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(mpg)
b &lt;- gam(list(city.mpg~fuel+style+drive+s(weight)+s(hp)+s(make,bs="re"),
          hw.mpg~fuel+style+drive++s(weight)+s(hp)+s(make,bs="re")),
          family=mvn(d=2),data=mpg)
plot(b,pages=1,scheme=1)
</code></pre>

<hr>
<h2 id='prostate'>Prostate cancer screening data</h2><span id='topic+prostate'></span>

<h3>Description</h3>

<p>Protein mass spectographs for patients with normal, benign enlargement and cancer of the prostate gland. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(prostate)
</code></pre>


<h3>Format</h3>

<p>A three item list
</p>

<dl>
<dt>type</dt><dd><p>1 for normal, 2 for benign enlargement and 3 for cancerous.</p>
</dd>
<dt>intensity</dt><dd><p>A matrix with rows corresponding to measurements in <code>type</code>. Each row is a normalized spectral intensity measurement for the protein mass given in <code>MZ</code> </p>
</dd>
<dt>MZ</dt><dd><p> Matrix corresponding to <code>intensity</code> giving the protein masses in Daltons.Actually all rows are identical.</p>
</dd>
</dl>



<h3>Details</h3>

<p> See the source article for fuller details. The intensity data here have been smoothed so that each measurement is an average of 40 adjacent measurements from the raw spectrum. The intensity data have also been rounded to 3 significant figures. This pre-processing was done to reduce the dataset size to something reasonable for distribution.
</p>


<h3>Source</h3>

<p>Originally from the msProstate package version 1.0.2.
</p>


<h3>References</h3>

<p>Adam, B-L. Y. Qu, J.W. Davis et al. (2002) Serum Protein Fingerprinting Coupled with a Pattern-matching Algorithm Distinguishes Prostate Cancer from Benign Prostate
Hyperplasia and Healthy Men. Cancer Research 62:3609-3614
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(prostate)
## plot some spectra...
par(mfrow=c(2,3),mar=c(5,5,3,1))
ind &lt;- c(1,163,319)
lab &lt;- list("Healthy","Enlarged","Cancer")
for (i in 1:3) {
  plot(prostate$MZ[ind[i],],prostate$intensity[ind[i],],type="l",ylim=c(0,60),
  xlab="Daltons",ylab="Intensity",main=lab[[i]],cex.axis=1.4,cex.lab=1.6)
  lines(prostate$MZ[ind[i],],prostate$intensity[ind[i]+2,]+5,col=2)
  lines(prostate$MZ[ind[i],],prostate$intensity[ind[i]+4,]+10,col=4)
}
## treat as ordered cat control, bph, cancer
b &lt;- gam(type ~ s(MZ,by=intensity,k=100),family=ocat(R=3),
         data=prostate,method="ML")
## results...
pb &lt;- predict(b,type="response")
plot(b,rug=FALSE,scheme=1,xlab="Daltons",ylab="f(D)",
cex.lab=1.6,cex.axis=1.4,main="a")
plot(factor(prostate$type),pb[,3],cex.lab=1.6,cex.axis=1.4,main="b")
qq.gam(b,rep=100,lev=.95,cex.lab=1.6,cex.axis=1.4,main="c")
</code></pre>

<hr>
<h2 id='sitka'>Sitka spruce growth data.</h2><span id='topic+sitka'></span>

<h3>Description</h3>

<p>Tree growth data under enhanced ozone and control conditions.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sitka)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1027 rows and 5 columns columns:
</p>

<dl>
<dt>id.num</dt><dd><p>identity of the tree: 1...79.</p>
</dd>
<dt>order</dt><dd><p>time order ranking within each tree.</p>
</dd>
<dt>days</dt><dd><p>since 1st January, 1988.</p>
</dd>
<dt>log.size</dt><dd><p>log of tree &lsquo;size&rsquo;.</p>
</dd>
<dt>ozone</dt><dd><p>1 - enhanced ozone treatment; 0 - control.</p>
</dd>
</dl>



<h3>Details</h3>

<p> The data were analysed in Crainiceanu CM, Ruppert D, Wand MP (2005) using WinBUGS, and in Wood (2016) using auto-generated JAGS code. 
</p>


<h3>Source</h3>

<p>The <code>SemiPar</code> package, from:
</p>
<p>Diggle, P.J, Heagery, P., Liang, K.-Y. and Zeger, S.L. (2002) Analysis of Longitudinal Data (2nd ed.) OUP.
</p>


<h3>References</h3>

<p>Wood SN (2016) &quot;Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv&quot; Journal of Statistical Software 75
</p>
<p>Crainiceanu C.M., Ruppert D. and Wand M.P. (2005). &quot;Bayesian Analysis for Penalized Spline Regression Using WinBUGS.&quot; Journal of Statistical Software, 14(14).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(gamair); require(lattice)
  data(sitka)
  xyplot(log.size~days|as.factor(ozone),data=sitka,type="l",groups=id.num)

</code></pre>

<hr>
<h2 id='sole'>Sole Eggs in the Bristol Channel</h2><span id='topic+sole'></span>

<h3>Description</h3>

<p> Data on Sole Egg densities in the Bristol Channel (West Coast of
England, UK.) The data are from 5 research cruises undertaken for the purpose
of measuring Sole egg densities. Samples were taken at each of a number of
sampling stations, by hauling a net vertically through the water column. Sole
eggs were counted and assigned to one of four developmental stages.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sole)
</code></pre>


<h3>Format</h3>

<p>A data frame with 7 columns and 1575 rows. The columns are:
</p>

<dl>
<dt>la</dt><dd><p>latitude of sampling station</p>
</dd>
<dt>lo</dt><dd><p>longitude of sampling station</p>
</dd>
<dt>t</dt><dd><p>time of sampling station: actually time of midpoint of the cruise on
which this sample was taken. Measured in Julian days (days since January 1st).</p>
</dd>
<dt>eggs</dt><dd><p>egg density per square metre of sea surface.</p>
</dd>
<dt>stage</dt><dd><p>to which of 4 stages the sample relates.</p>
</dd>
<dt>a.0</dt><dd><p>lower age limit for the stage (i.e. age of youngest possible egg in
this sample).</p>
</dd>
<dt>a.1</dt><dd><p>upper age limit of this stage (i.e. age of oldest possible egg in sample).</p>
</dd>
</dl>


<h3>Source</h3>

 
<p>Dixon (2003)
</p>


<h3>References</h3>

<p>Dixon, C.E. (2003) Multi-dimensional modelling of physiologically and
temporally structured populations. PhD thesis. University of St Andrews
</p>
<p>Horwood, J. (1993) The Bristol Channel Sole (solea solea (L.)): A fisheries
case study. Advances in Marine Biology 29, 215-367
</p>
<p>Horwood, J. and M. Greer Walker (1990) Determinacy of fecundity in Sole (solea
solea) from the Bristol Channel. Journal of the Marine Biology Association of
the United Kingdom. 70, 803-813.
</p>
<p>Wood (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(gamair)
  data(sole);data(coast)
  par(mfrow=c(2,3))
  sample.t &lt;- unique(sole$t)
  stage &lt;- 1
  for (i in 1:5)
  { egg&lt;-sole[sole$stage==stage&amp;sole$t==sample.t[i],] 
    plot(egg$lo,egg$la,xlab="lo",ylab="la",main=paste("day",sample.t[i]),cex=egg$eggs/4,
    xlim=range(sole$lo),ylim=range(sole$la),cex.axis=1.5,cex.lab=1.5,cex.main=1.5)
    points(egg$lo,egg$la,pch=".",col=2)
    lines(coast)
  }
  ## boundary definition list and knots suitable for soap film smoothing
  bnd &lt;- list(list(lo=c(-6.74,-5.72,-5.7 ,-5.52,-5.37,-5.21,-5.09,-5.02,
          -4.92,-4.76,-4.64,-4.56,-4.53,-4.3,-4.16,-3.8 ,-3.8,-5.04,-6.76,
	  -6.74),
          la=c(50.01,50.02,50.13,50.21,50.24,50.32,50.41,50.54,50.59,50.64,
	  50.74,50.86,51.01,51  ,51.2,51.22,51.61,51.7,51.7,50.01)))

  knt &lt;- list(lo=c(-4.643,-5.172,-5.638,-6.159,-6.665,-6.158,-5.656,-5.149,
  -4.652,-4.154,-3.901,-4.146,-4.381,-4.9,-5.149,-5.37,-5.866,-6.36,-6.635,
  -6.12,-5.626,-5.117,-4.622,-4.695,-4.875,-5.102,-5.609,-5.652,-5.141,
  -5.354,-5.843,-6.35,-6.628,-6.127,-5.63,-5.154,-5.356,-5.652,-5.853,
  -6.123),
   la=c(51.626,51.61,51.639,51.638,51.376,51.377,51.373,51.374,51.374,
   51.376,51.379,51.226,51.129,51.194,51.083,51.147,51.129,51.151,50.901,
   50.891,50.959,50.958,50.942,50.728,50.676,50.818,50.825,50.684,50.693,
   50.568,50.564,50.626,50.397,50.451,50.443,50.457,50.325,50.193,50.322,
   50.177))

   points(knt$lo,knt$la,pch=19,col=2,cex=.6)
   lines(bnd[[1]]$lo,bnd[[1]]$la,col=2)
</code></pre>

<hr>
<h2 id='sperm.comp1'>Sperm competition data I</h2><span id='topic+sperm.comp1'></span>

<h3>Description</h3>

<p>Data relating sperm count to time since last inter-pair
copulation and proportion of that time spent together for 15 couples living in
Manchester UK.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sperm.comp1)
</code></pre>


<h3>Format</h3>

<p>A data frame with 4 columns and 15 rows. The columns are:
</p>

<dl>
<dt>subject</dt><dd><p> An identifier for the subject/couple.</p>
</dd>
<dt>time.ipc</dt><dd><p> Time since last inter-pair copulation, in hours.</p>
</dd>
<dt>prop.partner</dt><dd><p> Proportion of <code>time.ipc</code> that the couple had spent
together.</p>
</dd>
<dt>count</dt><dd><p>Sperm count in millions.</p>
</dd>
</dl>


<h3>Details</h3>

<p>The sperm counts reported are total counts in ejaculate from a single
copulation, for each of 15 couples. Also recorded are the time since the
couple's previous copulation, and the proportion of that time that the couple
had spent together. The data are from volunteers from Manchester University
and were gathered to test theories about human sperm competition.
See the source article for further details.
</p>


<h3>Source</h3>

<p>Baker, RR and Bellis M.A. (1993) 
'Human sperm competition: ejaculate adjustment by males and the function
of masturbation'.
Animal behaviour 46:861-885
</p>

<hr>
<h2 id='sperm.comp2'>Sperm competition data II</h2><span id='topic+sperm.comp2'></span>

<h3>Description</h3>

<p>Data relating average number of sperm ejaculated per copulation
to physical characterisics of partners involved, for 24 heterosexual couples
from Manchester, UK. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sperm.comp2)
</code></pre>


<h3>Format</h3>

<p>A data frame with 10 columns and 24 rows. The columns are:
</p>

<dl>
<dt>pair</dt><dd><p> an identifier for the couple. These labels correspond to those
given in <code><a href="#topic+sperm.comp1">sperm.comp1</a></code>.</p>
</dd>
<dt>n</dt><dd><p> the number of copulations over which the average sperm count has
been calculated.</p>
</dd>
<dt>count</dt><dd><p> the average sperm count in millions, per copulation.</p>
</dd>
<dt>f.age</dt><dd><p> age of the female, in years.</p>
</dd>
<dt>f.height</dt><dd><p> height of the female, in cm.</p>
</dd>
<dt>f.weight</dt><dd><p>weight of the female, in kg.</p>
</dd>
<dt>m.age</dt><dd><p>age of the male, in years.</p>
</dd>
<dt>m.height</dt><dd><p>height of the male, in cm.</p>
</dd>
<dt>m.weight</dt><dd><p>weight of the male, in kg.</p>
</dd>
<dt>m.vol</dt><dd><p>volume of one male teste in cubic cm.</p>
</dd>
</dl>


<h3>Details</h3>

<p> In the source article, these data are used to argue that males
invest more reproductive effort in heavier females, on the basis of regression
modelling. It is worth checking for outliers.
</p>


<h3>Source</h3>

<p>Baker, RR and Bellis M.A. (1993) 
'Human sperm competition: ejaculate adjustment by males and the function
of masturbation'.
Animal behaviour 46:861-885
</p>

<hr>
<h2 id='stomata'>Stomatal area and CO2 </h2><span id='topic+stomata'></span>

<h3>Description</h3>

<p>Fake data on average stomatal area for 6 trees grown under 
one of two CO2 concentrations</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(stomata)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 columns and 24 rows. The columns are:
</p>

<dl>
<dt>area</dt><dd><p>mean stomatal area.</p>
</dd>
<dt>CO2</dt><dd><p>label for which CO2 treatment the measurement relates to.</p>
</dd>
<dt>tree</dt><dd><p>label for individual tree.</p>
</dd>
</dl>


<h3>Details</h3>

<p>The context for these simulated data is given in section 6.1 of 
the source book.
</p>


<h3>Source</h3>

<p>The reference.</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>

<hr>
<h2 id='swer'>Swiss 12 hour extreme rainfall</h2><span id='topic+swer'></span>

<h3>Description</h3>

<p>Records the most extreme 12 hourly total rainfall each year for 65 Swiss weather stations. The data period is 1981-2015, although not all stations start in 1981.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(swer)
</code></pre>


<h3>Format</h3>

<p>The <code>swer</code> data frame has the following columns
</p>

<dl>
<dt>year</dt><dd><p>The year of observation.</p>
</dd>
<dt>exra</dt><dd><p>The highest rainfall observed in any 12 hour period in that year, in mm. </p>
</dd>
<dt>nao</dt><dd><p>Annual North Atlantic Oscillation index, based on the difference of normalized sea level pressure (SLP) between Lisbon, Portugal and Stykkisholmur/Reykjavik, Iceland. Positive values are generally associated with wetter and milder weather over Western Europe.</p>
</dd>
<dt>location</dt><dd><p>The measuring station location name.</p>
</dd>
<dt>code</dt><dd><p>Three letter code identifying the station.</p>
</dd>
<dt>elevation</dt><dd><p>metres above sea level.</p>
</dd>
<dt>climate.region</dt><dd><p>One of 12 distinct climate regions.</p>
</dd>
<dt>N</dt><dd><p>Degrees north.</p>
</dd>
<dt>E</dt><dd><p>Degrees east. </p>
</dd>
</dl>


<h3>Details</h3>

<p>The actual extreme rainfall measurements are digitized from plots in the MeteoSwiss reports for each station. The error associated with digitization can be estimated from the error in the digitized year values, since the true values are then known exactly. This translates into a mean square error in rainfall of about 0.1% of the station maximum, and a maximum error of about 0.3% of station maximum. 
</p>


<h3>Source</h3>

<p>Mostly from the MeteoSwiss website:
</p>
<p><a href="http://www.meteoswiss.admin.ch/home/climate/past/climate-extremes/extreme-value-analyses/standard-period.html?">http://www.meteoswiss.admin.ch/home/climate/past/climate-extremes/extreme-value-analyses/standard-period.html?</a>
</p>
<p>NAO data from:
</p>
<p>Hurrell, James &amp; National Center for Atmospheric Research Staff (Eds). Last modified 16 Aug 2016. &quot;The Climate Data Guide: Hurrell North Atlantic Oscillation (NAO) Index (station-based).&quot;
</p>
<p><a href="https://climatedataguide.ucar.edu/climate-data/hurrell-north-atlantic-oscillation-nao-index-station-based.">https://climatedataguide.ucar.edu/climate-data/hurrell-north-atlantic-oscillation-nao-index-station-based.</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(swer)
## GEV model, over-simplified for speed...
system.time(b &lt;- gam(list(exra~s(elevation)+ climate.region,
         ~s(elevation),~1),family=gevlss,data=swer))
plot(b,pages=1,scale=0,scheme=1)
</code></pre>

<hr>
<h2 id='wesdr'>Diabetic retinopathy in Wisconsin</h2><span id='topic+wesdr'></span>

<h3>Description</h3>

<p>The data, originally from the <code>gss</code> package, record whether or not diabetic patients developed retinopathy along with three possible predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wesdr)
</code></pre>


<h3>Format</h3>

<p>The <code>wesdr</code> data frame has the following columns
</p>

<dl>
<dt>ret</dt><dd><p>binary variable: 1 = retinopathy, 0 = not.</p>
</dd>
<dt>bmi</dt><dd><p>Body mass index (weight in kg divided by square of height in metres)</p>
</dd>
<dt>gly</dt><dd><p>Glycosylated hemoglobin - the percentage of hemoglobin bound to glucuse in the blood. This reflects long term average blood glucose levels: less than 6% is typical of non-diabetics, but is only rarely acheived by diabetic patients.</p>
</dd>
<dt>dur</dt><dd><p>Duration of disease in years.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Retinopathy is a common problem in diabetic patients and the interst is in predicting the risk using the measured predictors.
</p>


<h3>Source</h3>

<p>Data are from Chong Gu's <code>gss</code> package. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(gamair);require(mgcv)
data(wesdr)
## Smooth ANOVA model...
k &lt;- 5
b &lt;- gam(ret~s(dur,k=k)+s(gly,k=k)+s(bmi,k=k)+ti(dur,gly,k=k)+
         ti(dur,bmi,k=k)+ti(gly,bmi,k=k),select=TRUE,
         data=wesdr,family=binomial(),method="ML")
ow &lt;- options(warn=-1) ## avoid complaint about zlim 
plot(b,pages=1,scheme=1,zlim=c(-3,3))
options(ow)
</code></pre>

<hr>
<h2 id='wine'>Bordeaux Wines</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p> Data on prices and growing characteristics of 25 Bordeaux wines
from 1952 to 1998.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wine)
</code></pre>


<h3>Format</h3>

<p>A data frame with 7 columns and 47 rows. The columns are:
</p>

<dl>
<dt>year</dt><dd><p>year of production</p>
</dd>
<dt>price</dt><dd><p>average price of the wines as a percentage of the 1961 price.</p>
</dd>
<dt>h.rain</dt><dd><p>mm of rain in the harvest month.</p>
</dd>
<dt>s.temp</dt><dd><p>Average temperature (C) over the summer preceding harvest.</p>
</dd>
<dt>w.rain</dt><dd><p>mm of rain in the winter preceding harvest.</p>
</dd>
<dt>h.temp</dt><dd><p>average temperature (C) at harvest.</p>
</dd>
<dt>parker</dt><dd><p>a rating of the wine quality (see source for details).</p>
</dd>
</dl>


<h3>Source</h3>

 
<p><a href="http://schwert.ssb.rochester.edu/a425/a425.htm">http://schwert.ssb.rochester.edu/a425/a425.htm</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2006, 2017) Generalized Additive Models: An Introduction with R. CRC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(wine)
  pairs(wine[,-7])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
