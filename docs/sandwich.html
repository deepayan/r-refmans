<!DOCTYPE html><html lang="en"><head><title>Help for package sandwich</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sandwich}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bread'><p>Bread for Sandwiches</p></a></li>
<li><a href='#estfun'><p>Extract Empirical Estimating Functions</p></a></li>
<li><a href='#InstInnovation'><p>Innovation and Institutional Ownership</p></a></li>
<li><a href='#Investment'><p>US Investment Data</p></a></li>
<li><a href='#isoacf'><p>Isotonic Autocorrelation Function</p></a></li>
<li><a href='#kweights'><p>Kernel Weights</p></a></li>
<li><a href='#lrvar'><p>Long-Run Variance of the Mean</p></a></li>
<li><a href='#meat'><p>A Simple Meat Matrix Estimator</p></a></li>
<li><a href='#NeweyWest'><p>Newey-West HAC Covariance Matrix Estimation</p></a></li>
<li><a href='#PetersenCL'><p>Petersen's Simulated Data for Assessing Clustered Standard Errors</p></a></li>
<li><a href='#PublicSchools'><p>US Expenditures for Public Schools</p></a></li>
<li><a href='#sandwich'><p>Making Sandwiches with Bread and Meat</p></a></li>
<li><a href='#vcovBS'><p>(Clustered) Bootstrap Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovCL'><p>Clustered Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovHAC'><p>Heteroscedasticity and Autocorrelation Consistent (HAC) Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovHC'><p>Heteroscedasticity-Consistent Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovJK'><p>(Clustered) Jackknife Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovOPG'><p>Outer-Product-of-Gradients Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovPC'><p>Panel-Corrected Covariance Matrix Estimation</p></a></li>
<li><a href='#vcovPL'><p>Clustered Covariance Matrix Estimation for Panel Data</p></a></li>
<li><a href='#weightsAndrews'><p>Kernel-based HAC Covariance Matrix Estimation</p></a></li>
<li><a href='#weightsLumley'><p>Weighted Empirical Adaptive Variance Estimation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>3.1-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-16</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Covariance Matrix Estimators</td>
</tr>
<tr>
<td>Description:</td>
<td>Object-oriented software for model-robust covariance matrix estimators. Starting out from the basic 
             robust Eicker-Huber-White sandwich covariance methods include: heteroscedasticity-consistent (HC)
	     covariances for cross-section data; heteroscedasticity- and autocorrelation-consistent (HAC)
	     covariances for time series data (such as Andrews' kernel HAC, Newey-West, and WEAVE estimators);
	     clustered covariances (one-way and multi-way); panel and panel-corrected covariances;
	     outer-product-of-gradients covariances; and (clustered) bootstrap covariances. All methods are
	     applicable to (generalized) linear model objects fitted by lm() and glm() but can also be adapted
	     to other classes through S3 methods. Details can be found in Zeileis et al. (2020) &lt;<a href="https://doi.org/10.18637%2Fjss.v095.i01">doi:10.18637/jss.v095.i01</a>&gt;,
	     Zeileis (2004) &lt;<a href="https://doi.org/10.18637%2Fjss.v011.i10">doi:10.18637/jss.v011.i10</a>&gt; and Zeileis (2006) &lt;<a href="https://doi.org/10.18637%2Fjss.v016.i09">doi:10.18637/jss.v016.i09</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, utils, zoo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>AER, car, geepack, lattice, lme4, lmtest, MASS, multiwayvcov,
parallel, pcse, plm, pscl, scatterplot3d, stats4, strucchange,
survival</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://sandwich.R-Forge.R-project.org/">https://sandwich.R-Forge.R-project.org/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://sandwich.R-Forge.R-project.org/contact.html">https://sandwich.R-Forge.R-project.org/contact.html</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-15 20:36:02 UTC; zeileis</td>
</tr>
<tr>
<td>Author:</td>
<td>Achim Zeileis <a href="https://orcid.org/0000-0003-0918-3766"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Thomas Lumley <a href="https://orcid.org/0000-0003-4255-5437"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Nathaniel Graham <a href="https://orcid.org/0009-0002-1215-5256"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Susanne Koell [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-15 21:50:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='bread'>Bread for Sandwiches</h2><span id='topic+bread'></span><span id='topic+bread.default'></span><span id='topic+bread.lm'></span><span id='topic+bread.mlm'></span><span id='topic+bread.survreg'></span><span id='topic+bread.coxph'></span><span id='topic+bread.gam'></span><span id='topic+bread.nls'></span><span id='topic+bread.rlm'></span><span id='topic+bread.hurdle'></span><span id='topic+bread.zeroinfl'></span><span id='topic+bread.mlogit'></span><span id='topic+bread.polr'></span><span id='topic+bread.clm'></span>

<h3>Description</h3>

<p>Generic function for extracting an estimator for the bread of
sandwiches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bread(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bread_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="bread_+3A_...">...</code></td>
<td>
<p>arguments passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing an estimator for the expectation of the negative
derivative of the estimating functions, usually the Hessian.
Typically, this should be an <code class="reqn">k \times k</code> matrix corresponding
to <code class="reqn">k</code> parameters. The rows and columns should be named
as in <code><a href="stats.html#topic+coef">coef</a></code> or <code><a href="stats.html#topic+terms">terms</a></code>, respectively.
</p>
<p>The default method tries to extract <code><a href="stats.html#topic+vcov">vcov</a></code> and <code><a href="stats.html#topic+nobs">nobs</a></code>
and simply computes their product.
</p>


<h3>References</h3>

<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## linear regression
x &lt;- sin(1:10)
y &lt;- rnorm(10)
fm &lt;- lm(y ~ x)

## bread: n * (x'x)^{-1}
bread(fm)
solve(crossprod(cbind(1, x))) * 10
</code></pre>

<hr>
<h2 id='estfun'>Extract Empirical Estimating Functions</h2><span id='topic+estfun'></span><span id='topic+estfun.lm'></span><span id='topic+estfun.glm'></span><span id='topic+estfun.mlm'></span><span id='topic+estfun.rlm'></span><span id='topic+estfun.polr'></span><span id='topic+estfun.clm'></span><span id='topic+estfun.survreg'></span><span id='topic+estfun.coxph'></span><span id='topic+estfun.nls'></span><span id='topic+estfun.hurdle'></span><span id='topic+estfun.zeroinfl'></span><span id='topic+estfun.mlogit'></span>

<h3>Description</h3>

<p>Generic function for extracting the empirical estimating functions
of a fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estfun(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estfun_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="estfun_+3A_...">...</code></td>
<td>
<p>arguments passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing the empirical estimating functions.
Typically, this should be an <code class="reqn">n \times k</code> matrix corresponding
to <code class="reqn">n</code> observations and <code class="reqn">k</code> parameters. The columns should be named
as in <code><a href="stats.html#topic+coef">coef</a></code> or <code><a href="stats.html#topic+terms">terms</a></code>, respectively.
</p>
<p>The estimating function (or score function) for a model is the derivative of the objective function
with respect to the parameter vector. The empirical estimating functions is
the evaluation of the estimating function at the observed data (<code class="reqn">n</code> observations)
and the estimated parameters (of dimension <code class="reqn">k</code>).
</p>


<h3>References</h3>

<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## linear regression
x &lt;- 1:9
y &lt;- sin(1:9/5)
m &lt;- lm(y ~ x)

## estimating function: (y - x'beta) * x
estfun(m)
residuals(m) * cbind(1, x)
</code></pre>

<hr>
<h2 id='InstInnovation'>Innovation and Institutional Ownership</h2><span id='topic+InstInnovation'></span>

<h3>Description</h3>

<p>Firm-level panel data on innovation and institutional ownership from 1991 to
1999 over 803 firms. The observations refer to different firms over different years.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("InstInnovation")</code></pre>


<h3>Format</h3>

<p>A data frame containing 6208 observations on 25 variables.
</p>

<dl>
<dt>company</dt><dd><p>factor. Company names.</p>
</dd>
<dt>sales</dt><dd><p>numeric. Sales (in millions of dollars).</p>
</dd>
<dt>acompetition</dt><dd><p>numeric. Constant inverse Lerner index.</p>
</dd>
<dt>competition</dt><dd><p>numeric. Varying inverse Lerner index.</p>
</dd>
<dt>capital</dt><dd><p>numeric. Net stock of property, plant, and equipment.</p>
</dd>
<dt>cites</dt><dd><p>integer. Future cite-weighted patents.</p>
</dd>
<dt>precites</dt><dd><p>numeric. Presample average of cite-weighted patents.</p>
</dd>
<dt>dprecites</dt><dd><p>factor. Indicates zero precites.</p>
</dd>
<dt>patents</dt><dd><p>integer. Granted patents.</p>
</dd>
<dt>drandd</dt><dd><p>factor. Indicates a zero R&amp;D stock.</p>
</dd>
<dt>randd</dt><dd><p>numeric. R&amp;D stock (in millions of dollars).</p>
</dd>
<dt>employment</dt><dd><p>numeric. Employment (in 1000s).</p>
</dd>
<dt>sp500</dt><dd><p>factor. Membership of firms in the S&amp;P500 index.</p>
</dd>
<dt>tobinq</dt><dd><p>numeric. Tobin's q.</p>
</dd>
<dt>value</dt><dd><p>numeric. Stock market value.</p>
</dd>
<dt>institutions</dt><dd><p>numeric. Proportion of stock owned by institutions.</p>
</dd>
<dt>industry</dt><dd><p>factor. Four-digit industry code.</p>
</dd>
<dt>year</dt><dd><p>factor. Estimation period.</p>
</dd>
<dt>top1</dt><dd><p>numeric. Share of the largest institution.</p>
</dd>
<dt>quasiindexed</dt><dd><p>numeric. Share of &quot;quasi-indexed&quot; institutional owners.</p>
</dd>
<dt>nonquasiindexed</dt><dd><p>numeric. Share of &quot;non-quasi-indexed&quot;
institutional owners.</p>
</dd>
<dt>transient</dt><dd><p>numeric. Share of &quot;transient&quot; institutional owners.</p>
</dd>
<dt>dedicated</dt><dd><p>numeric. Share of &quot;dedicated&quot; institutional owners.</p>
</dd>
<dt>competition4</dt><dd><p>numeric. Varying inverse Lerner index in the firm's four-digit industry.</p>
</dd>
<dt>subsample</dt><dd><p>factor. Subsample for the replication of columns
1&ndash;5 from Table 4 in Aghion et al. (2013).</p>
</dd>
</dl>



<h3>Details</h3>

<p>Aghion et al. (2013) combine several firm level panel datasets (e.g.,
USPTO, SEC and Compustat) to examine the role of institutional investors
in the governance of innovation. Their baseline to model innovation is the Poisson model,
but they also consider negative binomial models.
Berger et al. (2017) argue that nonlinearities in the innovation process emerge in case that the first
innovation is especially hard to obtain in comparison to succeeding innovations.
Then, hurdle models offer a useful way that allows for a distinction between these two processes.
Berger et al. (2017) show that an extended analysis with negative binomial hurdle models
differs materially from the outcomes of the single-equation Poisson
approach of Aghion et al. (2013).
</p>
<p>Institutional ownership (institutions) is defined as the proportion of
stock owney by institutions. According to Aghion et al. (2013), an
institutional owner is defined as an institution that files a Form 13-F
with the Securities and Exchange Commission (SEC).
</p>
<p>Future cite-weighted patents (cites) are used as a proxy for
innovation. They are calculated using ultimately granted patent, dated
by year of application, and weight these by future citations through
2002 (see Aghion et al. (2013)).
</p>
<p>The presample average of cite-weighted patents (precites) is used by Aghion et
al. (2013) as a proxy for unobserved heterogeneity, employing the
&quot;presample mean scaling&quot; method of Blundell et al. (1999).
</p>
<p>The inverse Lerner index in the firm's three-digit industry is used
as a time-varying measure for product market competition (competition), where the Lerner is
calculated as the median gross margin from the entire Compustat database
in the firm's three-digit industry (see Aghion et al. (2013)).
A time-invariant measure for competition (acompetition) is constructed by averaging the
Lerner over the sample period.
</p>
<p>The classification of institutions into &quot;quasiindexed&quot;, &quot;transient&quot; and
&quot;dedicated&quot; follows Bushee (1998) and distinguishes between institutional
investors based on their type of investing. Quasiindexed institutions are
do not trade much and are widely diversified, dedicated institution do
not trade much and have more concentrated holdings, and transient
institutions often trade and have diversified holdings (see Aghion et
al. (2013) and Bushee (1998)).
</p>


<h3>Source</h3>

<p>Data and online appendix of Aghion et al. (2013).
</p>


<h3>References</h3>

<p>Aghion P, Van Reenen J, Zingales L (2013).
&ldquo;Innovation and Institutional Ownership.&rdquo;
<em>The American Economic Review</em>, <b>103</b>(1), 277&ndash;304.
<a href="https://doi.org/10.1257/aer.103.1.277">doi:10.1257/aer.103.1.277</a>
</p>
<p>Berger S, Stocker H, Zeileis A (2017).
&ldquo;Innovation and Institutional Ownership Revisited: An Empirical Investigation with Count Data Models.&rdquo;
<em>Empirical Economics</em>, <b>52</b>(4), 1675&ndash;1688.
<a href="https://doi.org/10.1007/s00181-016-1118-0">doi:10.1007/s00181-016-1118-0</a>
</p>
<p>Blundell R, Griffith R, Van Reenen J (1999).
&ldquo;Market Share, Market Value and Innovation in a Panel of
British Manufacturing Firms.&rdquo;
<em>Review of Economic Studies</em>, 66(3), 529&ndash;554.
</p>
<p>Bushee B (1998).
&ldquo;The Influence of Institutional Investors on Myopic R&amp;D Investment Behavior.&rdquo;
<em>Accounting Review</em>, <b>73</b>(3), 655&ndash;679.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Poisson models from Table I in Aghion et al. (2013)

## load data set
data("InstInnovation", package = "sandwich")

## log-scale variable
InstInnovation$lograndd &lt;- log(InstInnovation$randd)
InstInnovation$lograndd[InstInnovation$lograndd == -Inf] &lt;- 0

## regression formulas
f1 &lt;- cites ~ institutions + log(capital/employment) + log(sales) + industry + year
f2 &lt;- cites ~ institutions + log(capital/employment) + log(sales) +
  industry + year + lograndd + drandd
f3 &lt;- cites ~ institutions + log(capital/employment) + log(sales) +
  industry + year + lograndd + drandd + dprecites + log(precites)

## Poisson models
tab_I_3_pois &lt;- glm(f1, data = InstInnovation, family = poisson)
tab_I_4_pois &lt;- glm(f2, data = InstInnovation, family = poisson)
tab_I_5_pois &lt;- glm(f3, data = InstInnovation, family = poisson)

## one-way clustered covariances
vCL_I_3 &lt;- vcovCL(tab_I_3_pois, cluster = ~ company)
vCL_I_4 &lt;- vcovCL(tab_I_4_pois, cluster = ~ company)
vCL_I_5 &lt;- vcovCL(tab_I_5_pois, cluster = ~ company)

## replication of columns 3 to 5 from Table I in Aghion et al. (2013)
cbind(coef(tab_I_3_pois), sqrt(diag(vCL_I_3)))[2:4, ]
cbind(coef(tab_I_4_pois), sqrt(diag(vCL_I_4)))[c(2:4, 148), ]
cbind(coef(tab_I_5_pois), sqrt(diag(vCL_I_5)))[c(2:4, 148), ]
</code></pre>

<hr>
<h2 id='Investment'>US Investment Data</h2><span id='topic+Investment'></span>

<h3>Description</h3>

<p>US data for fitting an investment equation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Investment)</code></pre>


<h3>Format</h3>

<p>An annual time series from 1963 to 1982 with 7 variables.
</p>

<dl>
<dt>GNP</dt><dd><p>nominal gross national product (in billion USD),</p>
</dd>
<dt>Investment</dt><dd><p>nominal gross private domestic investment (in billion USD),</p>
</dd>
<dt>Price</dt><dd><p>price index, implicit price deflator for GNP,</p>
</dd>
<dt>Interest</dt><dd><p>interest rate, average yearly discount rate
charged by the New York Federal Reserve Bank,</p>
</dd>
<dt>RealGNP</dt><dd><p>real GNP (= GNP/Price),</p>
</dd>
<dt>RealInv</dt><dd><p>real investment (= Investment/Price),</p>
</dd>
<dt>RealInt</dt><dd><p>approximation to the real interest rate
(= Interest - 100 * diff(Price)/Price).</p>
</dd>
</dl>



<h3>Source</h3>

<p>Table 15.1 in Greene (1993)</p>


<h3>References</h3>

<p>Greene W.H. (1993). <em>Econometric Analysis</em>, 2nd edition.
Macmillan Publishing Company, New York.
</p>
<p>Executive Office of the President (1984). <em>Economic Report of the
President</em>. US Government Printing Office, Washington, DC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Willam H. Greene, Econometric Analysis, 2nd Ed.
## Chapter 15
## load data set, p. 411, Table 15.1
data(Investment)

## fit linear model, p. 412, Table 15.2
fm &lt;- lm(RealInv ~ RealGNP + RealInt, data = Investment)
summary(fm)

## visualize residuals, p. 412, Figure 15.1
plot(ts(residuals(fm), start = 1964),
  type = "b", pch = 19, ylim = c(-35, 35), ylab = "Residuals")
sigma &lt;- sqrt(sum(residuals(fm)^2)/fm$df.residual) ## maybe used df = 26 instead of 16 ??
abline(h = c(-2, 0, 2) * sigma, lty = 2)

if(require(lmtest)) {
## Newey-West covariances, Example 15.3
coeftest(fm, vcov = NeweyWest(fm, lag = 4))
## Note, that the following is equivalent:
coeftest(fm, vcov = kernHAC(fm, kernel = "Bartlett", bw = 5, prewhite = FALSE, adjust = FALSE))

## Durbin-Watson test, p. 424, Example 15.4
dwtest(fm)

## Breusch-Godfrey test, p. 427, Example 15.6
bgtest(fm, order = 4)
}

## visualize fitted series
plot(Investment[, "RealInv"], type = "b", pch = 19, ylab = "Real investment")
lines(ts(fitted(fm), start = 1964), col = 4)


## 3-d visualization of fitted model
if(require(scatterplot3d)) {
s3d &lt;- scatterplot3d(Investment[,c(5,7,6)],
  type = "b", angle = 65, scale.y = 1, pch = 16)
s3d$plane3d(fm, lty.box = "solid", col = 4)
}
</code></pre>

<hr>
<h2 id='isoacf'>Isotonic Autocorrelation Function</h2><span id='topic+isoacf'></span><span id='topic+pava.blocks'></span>

<h3>Description</h3>

<p>Autocorrelation function (forced to be decreasing by isotonic regression).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isoacf(x, lagmax = NULL, weave1 = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isoacf_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="isoacf_+3A_lagmax">lagmax</code></td>
<td>
<p>numeric. The maximal lag of the autocorrelations.</p>
</td></tr>
<tr><td><code id="isoacf_+3A_weave1">weave1</code></td>
<td>
<p>logical. If set to <code>TRUE</code> <code>isoacf</code>
uses the <code>acf.R</code> and <code>pava.blocks</code> function from the
original <code>weave</code> package, otherwise R's own <code>acf</code> and
<code>isoreg</code> functions are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>isoacf</code> computes the autocorrelation function (ACF)
of <code>x</code> enforcing the ACF to be decreasing by isotonic regression.
See also Robertson et al. (1988).
</p>


<h3>Value</h3>

<p><code>isoacf</code> returns a numeric vector containing the ACF.
</p>


<h3>References</h3>

<p>Lumley T &amp; Heagerty P (1999).
&ldquo;Weighted Empirical Adaptive Variance Estimators for Correlated Data Regression.&rdquo;
<em>Journal of the Royal Statistical Society B</em>, <b>61</b>,
459&ndash;477.
</p>
<p>Robertson T, Wright FT, Dykstra RL (1988).
<em>Order Restricted Statistical Inference</em>.
John Wiley and Sons, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weave">weave</a></code>, <code><a href="#topic+weightsLumley">weightsLumley</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- filter(rnorm(100), 0.9, "recursive")
isoacf(x)
acf(x, plot = FALSE)$acf
</code></pre>

<hr>
<h2 id='kweights'>Kernel Weights</h2><span id='topic+kweights'></span>

<h3>Description</h3>

<p>Kernel weights for kernel-based heteroscedasticity
and autocorrelation consistent (HAC) covariance matrix estimators
as introduced by Andrews (1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kweights(x, kernel = c("Truncated", "Bartlett", "Parzen",
  "Tukey-Hanning", "Quadratic Spectral"), normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kweights_+3A_x">x</code></td>
<td>
<p>numeric.</p>
</td></tr>
<tr><td><code id="kweights_+3A_kernel">kernel</code></td>
<td>
<p>a character specifying the kernel used. All kernels used
are described in Andrews (1991).</p>
</td></tr>
<tr><td><code id="kweights_+3A_normalize">normalize</code></td>
<td>
<p>logical. If set to <code>TRUE</code> the kernels are
normalized as described in Andrews (1991).</p>
</td></tr>    
</table>


<h3>Value</h3>

<p>Value of the kernel function at <code>x</code>.
</p>


<h3>References</h3>

<p>Andrews DWK (1991).
&ldquo;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.&rdquo;
<em>Econometrica</em>, <b>59</b>,
817&ndash;858.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernHAC">kernHAC</a></code>, <code><a href="#topic+weightsAndrews">weightsAndrews</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>curve(kweights(x, kernel = "Quadratic", normalize = TRUE),
      from = 0, to = 3.2, xlab = "x", ylab = "k(x)")
curve(kweights(x, kernel = "Bartlett", normalize = TRUE),
      from = 0, to = 3.2, col = 2, add = TRUE)
curve(kweights(x, kernel = "Parzen", normalize = TRUE),
      from = 0, to = 3.2, col = 3, add = TRUE)
curve(kweights(x, kernel = "Tukey", normalize = TRUE),
      from = 0, to = 3.2, col = 4, add = TRUE)
curve(kweights(x, kernel = "Truncated", normalize = TRUE),
      from = 0, to = 3.2, col = 5, add = TRUE)
</code></pre>

<hr>
<h2 id='lrvar'>Long-Run Variance of the Mean</h2><span id='topic+lrvar'></span>

<h3>Description</h3>

<p>Convenience function for computing the long-run variance (matrix) of a
(possibly multivariate) series of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lrvar(x, type = c("Andrews", "Newey-West"), prewhite = TRUE, adjust = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lrvar_+3A_x">x</code></td>
<td>
<p>numeric vector, matrix, or time series.</p>
</td></tr>
<tr><td><code id="lrvar_+3A_type">type</code></td>
<td>
<p>character specifying the type of estimator, i.e., whether
<code><a href="#topic+kernHAC">kernHAC</a></code> for the Andrews quadratic spectral kernel HAC estimator
is used or <code><a href="#topic+NeweyWest">NeweyWest</a></code> for the Newey-West Bartlett HAC
estimator.</p>
</td></tr>
<tr><td><code id="lrvar_+3A_prewhite">prewhite</code></td>
<td>
<p>logical or integer. Should the series be prewhitened?
Passed to <code><a href="#topic+kernHAC">kernHAC</a></code> or <code><a href="#topic+NeweyWest">NeweyWest</a></code>.</p>
</td></tr>
<tr><td><code id="lrvar_+3A_adjust">adjust</code></td>
<td>
<p>logical.  Should a finite sample adjustment be made?
Passed to <code><a href="#topic+kernHAC">kernHAC</a></code> or <code><a href="#topic+NeweyWest">NeweyWest</a></code>.</p>
</td></tr>
<tr><td><code id="lrvar_+3A_...">...</code></td>
<td>
<p>further arguments passed on to <code><a href="#topic+kernHAC">kernHAC</a></code> or
<code><a href="#topic+NeweyWest">NeweyWest</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lrvar</code> is a simple wrapper function for computing the long-run variance
(matrix) of a (possibly multivariate) series <code>x</code>. First, this simply fits
a linear regression model <code>x ~ 1</code> by <code><a href="stats.html#topic+lm">lm</a></code>. Second,
the corresponding variance of the mean(s) is estimated either by <code><a href="#topic+kernHAC">kernHAC</a></code>
(Andrews quadratic spectral kernel HAC estimator) or by <code><a href="#topic+NeweyWest">NeweyWest</a></code>
(Newey-West Bartlett HAC estimator).
</p>


<h3>Value</h3>

<p>For a univariate series <code>x</code> a scalar variance is computed. For a
multivariate series <code>x</code> the covariance matrix is computed.</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernHAC">kernHAC</a></code>, <code><a href="#topic+NeweyWest">NeweyWest</a></code>, <code><a href="#topic+vcovHAC">vcovHAC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>suppressWarnings(RNGversion("3.5.0"))
set.seed(1)
## iid series (with variance of mean 1/n)
## and Andrews kernel HAC (with prewhitening)
x &lt;- rnorm(100)
lrvar(x)

## analogous multivariate case with Newey-West estimator (without prewhitening)
y &lt;- matrix(rnorm(200), ncol = 2)
lrvar(y, type = "Newey-West", prewhite = FALSE)

## AR(1) series with autocorrelation 0.9
z &lt;- filter(rnorm(100), 0.9, method = "recursive")
lrvar(z)
</code></pre>

<hr>
<h2 id='meat'>A Simple Meat Matrix Estimator</h2><span id='topic+meat'></span>

<h3>Description</h3>

<p>Estimating the variance of the estimating functions of
a regression model by cross products of the empirical
estimating functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meat(x, adjust = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="meat_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="meat_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="meat_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code><a href="#topic+estfun">estfun</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For some theoretical background along with implementation
details see Zeileis (2006).
</p>


<h3>Value</h3>

<p>A <code class="reqn">k \times k</code> matrix corresponding containing
the scaled cross products of the empirical estimating functions.</p>


<h3>References</h3>

<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sandwich">sandwich</a></code>, <code><a href="#topic+bread">bread</a></code>, <code><a href="#topic+estfun">estfun</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sin(1:10)
y &lt;- rnorm(10)
fm &lt;- lm(y ~ x)

meat(fm)
meatHC(fm, type = "HC")
meatHAC(fm)
</code></pre>

<hr>
<h2 id='NeweyWest'>Newey-West HAC Covariance Matrix Estimation</h2><span id='topic+bwNeweyWest'></span><span id='topic+NeweyWest'></span>

<h3>Description</h3>

<p>A set of functions implementing the Newey &amp; West (1987, 1994) heteroscedasticity
and autocorrelation consistent (HAC) covariance matrix estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NeweyWest(x, lag = NULL, order.by = NULL, prewhite = TRUE, adjust = FALSE, 
  diagnostics = FALSE, sandwich = TRUE, ar.method = "ols", data = list(),
  verbose = FALSE)

bwNeweyWest(x, order.by = NULL, kernel = c("Bartlett", "Parzen",
  "Quadratic Spectral", "Truncated", "Tukey-Hanning"), weights = NULL,
  prewhite = 1, ar.method = "ols", data = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NeweyWest_+3A_x">x</code></td>
<td>
<p>a fitted model object. For <code>bwNeweyWest</code> it can also
be a score matrix (as returned by <code>estfun</code>) directly.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_lag">lag</code></td>
<td>
<p>integer specifying the maximum lag with positive 
weight for the Newey-West estimator. If set to <code>NULL</code>
<code>floor(bwNeweyWest(x, ...))</code> is used.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_order.by">order.by</code></td>
<td>
<p>Either a vector <code>z</code> or a formula with a single explanatory
variable like <code>~ z</code>. The observations in the model
are ordered by the size of <code>z</code>. If set to <code>NULL</code> (the
default) the observations are assumed to be ordered (e.g., a
time series).</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_prewhite">prewhite</code></td>
<td>
<p>logical or integer. Should the estimating functions
be prewhitened? If <code>TRUE</code> or greater than 0 a VAR model of
order <code>as.integer(prewhite)</code> is fitted via <code>ar</code> with
method <code>"ols"</code> and <code>demean = FALSE</code>. The default is to
use VAR(1) prewhitening.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_kernel">kernel</code></td>
<td>
<p>a character specifying the kernel used. All kernels used
are described in Andrews (1991). <code>bwNeweyWest</code> can only
compute bandwidths for <code>"Bartlett"</code>, <code>"Parzen"</code> and
<code>"Quadratic Spectral"</code>.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_diagnostics">diagnostics</code></td>
<td>
<p>logical. Should additional model diagnostics be returned?
See <code><a href="#topic+vcovHAC">vcovHAC</a></code> for details.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the middle matrix is returned.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_ar.method">ar.method</code></td>
<td>
<p>character. The <code>method</code> argument passed to
<code><a href="stats.html#topic+ar">ar</a></code> for prewhitening (only, not for bandwidth selection).</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the <code>order.by</code> 
model. By default the variables are taken from the environment which
the function is called from.</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_verbose">verbose</code></td>
<td>
<p>logical. Should the lag truncation parameter used be
printed?</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_weights">weights</code></td>
<td>
<p>numeric. A vector of weights used for weighting the estimated
coefficients of the approximation model (as specified by <code>approx</code>). By
default all weights are 1 except that for the intercept term (if there is more than
one variable).</p>
</td></tr>
<tr><td><code id="NeweyWest_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>NeweyWest</code> is a convenience interface to <code><a href="#topic+vcovHAC">vcovHAC</a></code> using 
Bartlett kernel weights as described in Newey &amp; West (1987, 1994).
The automatic bandwidth selection procedure described in Newey &amp; West (1994)
is used as the default and can also be supplied to <code>kernHAC</code> for the
Parzen and quadratic spectral kernel. It is implemented in <code>bwNeweyWest</code>
which does not truncate its results - if the results for the Parzen and Bartlett
kernels should be truncated, this has to be applied afterwards. For Bartlett 
weights this is implemented in <code>NeweyWest</code>.
</p>
<p>To obtain the estimator described in Newey &amp; West (1987), prewhitening has to
be suppressed.
</p>


<h3>Value</h3>

<p><code>NeweyWest</code> returns the same type of object as <code><a href="#topic+vcovHAC">vcovHAC</a></code>
which is typically just the covariance matrix.
</p>
<p><code>bwNeweyWest</code> returns the selected bandwidth parameter.
</p>


<h3>References</h3>

<p>Andrews DWK (1991).
&ldquo;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.&rdquo;
<em>Econometrica</em>, <b>59</b>, 817&ndash;858.
</p>
<p>Newey WK &amp; West KD (1987).
&ldquo;A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.&rdquo;
<em>Econometrica</em>, <b>55</b>, 703&ndash;708.
</p>
<p>Newey WK &amp; West KD (1994).
&ldquo;Automatic Lag Selection in Covariance Matrix Estimation.&rdquo;
<em>Review of Economic Studies</em>, <b>61</b>, 631&ndash;653.
</p>
<p>Zeileis A (2004).
&ldquo;Econometric Computing with HC and HAC Covariance Matrix Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovHAC">vcovHAC</a></code>, <code><a href="#topic+weightsAndrews">weightsAndrews</a></code>,
<code><a href="#topic+kernHAC">kernHAC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## fit investment equation
data(Investment)
fm &lt;- lm(RealInv ~ RealGNP + RealInt, data = Investment)

## Newey &amp; West (1994) compute this type of estimator
NeweyWest(fm)

## The Newey &amp; West (1987) estimator requires specification
## of the lag and suppression of prewhitening
NeweyWest(fm, lag = 4, prewhite = FALSE)

## bwNeweyWest() can also be passed to kernHAC(), e.g.
## for the quadratic spectral kernel
kernHAC(fm, bw = bwNeweyWest)
</code></pre>

<hr>
<h2 id='PetersenCL'>Petersen's Simulated Data for Assessing Clustered Standard Errors</h2><span id='topic+PetersenCL'></span>

<h3>Description</h3>

<p>Artificial balanced panel data set from Petersen (2009) for
illustrating and benchmarking clustered standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("PetersenCL")</code></pre>


<h3>Format</h3>

<p>A data frame containing 5000 observations on 4 variables.
</p>

<dl>
<dt>firm</dt><dd><p>integer. Firm identifier (500 firms).</p>
</dd>
<dt>year</dt><dd><p>integer. Time variable (10 years per firm).</p>
</dd>
<dt>x</dt><dd><p>numeric. Independent regressor variable.</p>
</dd>
<dt>y</dt><dd><p>numeric. Dependent response variable.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This simulated data set was created to illustrate and benchmark clustered standard errors.
The residual and the regressor variable both contain a firm effect, but no year effect.
Thus, standard errors clustered by firm are different from the OLS standard errors
and similarly double-clustered standard errors (by firm and year) are different from
the standard errors clustered by year.
</p>


<h3>Source</h3>

<p><a href="https://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm">https://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm</a>
</p>


<h3>References</h3>

<p>Petersen MA (2009).
&ldquo;Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches&rdquo;,
<em>The Review of Financial Studies</em>, <b>22</b>(1), 435&ndash;480.
<a href="https://doi.org/10.1093/rfs/hhn053">doi:10.1093/rfs/hhn053</a>
</p>

<hr>
<h2 id='PublicSchools'>US Expenditures for Public Schools</h2><span id='topic+PublicSchools'></span>

<h3>Description</h3>

<p>Per capita expenditure on public schools and per capita income
by state in 1979.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("PublicSchools")</code></pre>


<h3>Format</h3>

<p>A data frame containing 51 observations of 2 variables.
</p>

<dl>
<dt>Expenditure</dt><dd><p>per capita expenditure on public schools,</p>
</dd>
<dt>Income</dt><dd><p>per capita income.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Table 14.1 in Greene (1993)</p>


<h3>References</h3>

<p>Cribari-Neto F. (2004).
&ldquo;Asymptotic Inference Under Heteroskedasticity of Unknown Form.&rdquo;
<em>Computational Statistics &amp; Data Analysis</em>,
<b>45</b>, 215-233.
</p>
<p>Greene W.H. (1993). <em>Econometric Analysis</em>, 2nd edition.
Macmillan Publishing Company, New York.
</p>
<p>US Department of Commerce (1979). <em>Statistical Abstract of the
United States</em>. US Government Printing Office, Washington, DC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Willam H. Greene, Econometric Analysis, 2nd Ed.
## Chapter 14
## load data set, p. 385, Table 14.1
data("PublicSchools", package = "sandwich")

## omit NA in Wisconsin and scale income
ps &lt;- na.omit(PublicSchools)
ps$Income &lt;- ps$Income * 0.0001

## fit quadratic regression, p. 385, Table 14.2
fmq &lt;- lm(Expenditure ~ Income + I(Income^2), data = ps)
summary(fmq)

## compare standard and HC0 standard errors
## p. 391, Table 14.3
coef(fmq)
sqrt(diag(vcovHC(fmq, type = "const")))
sqrt(diag(vcovHC(fmq, type = "HC0")))


if(require(lmtest)) {
## compare t ratio
coeftest(fmq, vcov = vcovHC(fmq, type = "HC0"))

## White test, p. 393, Example 14.5
wt &lt;- lm(residuals(fmq)^2 ~ poly(Income, 4), data = ps)
wt.stat &lt;- summary(wt)$r.squared * nrow(ps)
c(wt.stat, pchisq(wt.stat, df = 3, lower = FALSE))

## Bresch-Pagan test, p. 395, Example 14.7
bptest(fmq, studentize = FALSE)
bptest(fmq)

## Francisco Cribari-Neto, Asymptotic Inference, CSDA 45
## quasi z-tests, p. 229, Table 8
## with Alaska
coeftest(fmq, df = Inf)[3,4]
coeftest(fmq, df = Inf, vcov = vcovHC(fmq, type = "HC0"))[3,4]
coeftest(fmq, df = Inf, vcov = vcovHC(fmq, type = "HC3"))[3,4]
coeftest(fmq, df = Inf, vcov = vcovHC(fmq, type = "HC4"))[3,4]
## without Alaska (observation 2)
fmq1 &lt;- lm(Expenditure ~ Income + I(Income^2), data = ps[-2,])
coeftest(fmq1, df = Inf)[3,4]
coeftest(fmq1, df = Inf, vcov = vcovHC(fmq1, type = "HC0"))[3,4]
coeftest(fmq1, df = Inf, vcov = vcovHC(fmq1, type = "HC3"))[3,4]
coeftest(fmq1, df = Inf, vcov = vcovHC(fmq1, type = "HC4"))[3,4]
}

## visualization, p. 230, Figure 1
plot(Expenditure ~ Income, data = ps,
  xlab = "per capita income",
  ylab = "per capita spending on public schools")
inc &lt;- seq(0.5, 1.2, by = 0.001)
lines(inc, predict(fmq, data.frame(Income = inc)), col = 4)
fml &lt;- lm(Expenditure ~ Income, data = ps)
abline(fml)
text(ps[2,2], ps[2,1], rownames(ps)[2], pos = 2)
</code></pre>

<hr>
<h2 id='sandwich'>Making Sandwiches with Bread and Meat</h2><span id='topic+sandwich'></span>

<h3>Description</h3>

<p>Constructing sandwich covariance matrix estimators by
multiplying bread and meat matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sandwich(x, bread. = bread, meat. = meat, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sandwich_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="sandwich_+3A_bread.">bread.</code></td>
<td>
<p>either a bread matrix or a function for computing
this via <code>bread.(x)</code>.</p>
</td></tr>
<tr><td><code id="sandwich_+3A_meat.">meat.</code></td>
<td>
<p>either a bread matrix or a function for computing
this via <code>meat.(x, ...)</code>.</p>
</td></tr>
<tr><td><code id="sandwich_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code>meat</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sandwich</code> is a simple convenience function that
takes a bread matrix (i.e., estimator of the expectation of the negative
derivative of the estimating functions) and a meat matrix (i.e.,
estimator of the variance of the estimating functions) and multiplies
them to a sandwich with meat between two slices of bread. By default
<code><a href="#topic+bread">bread</a></code> and <code><a href="#topic+meat">meat</a></code> are called. 
</p>
<p>Some theoretical background along with implementation details is introduced
in Zeileis (2006) and also used in Zeileis et al. (2020).
</p>


<h3>Value</h3>

<p>A matrix containing the sandwich covariance matrix estimate.
Typically, this should be an <code class="reqn">k \times k</code> matrix corresponding
to <code class="reqn">k</code> parameters.</p>


<h3>References</h3>

<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bread">bread</a></code>, <code><a href="#topic+meat">meat</a></code>, <code><a href="#topic+meatHC">meatHC</a></code>, <code><a href="#topic+meatHAC">meatHAC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sin(1:10)
y &lt;- rnorm(10)
fm &lt;- lm(y ~ x)

sandwich(fm)
vcovHC(fm, type = "HC")
</code></pre>

<hr>
<h2 id='vcovBS'>(Clustered) Bootstrap Covariance Matrix Estimation</h2><span id='topic+vcovBS'></span><span id='topic+vcovBS.default'></span><span id='topic+vcovBS.lm'></span><span id='topic+vcovBS.glm'></span><span id='topic+.vcovBSenv'></span>

<h3>Description</h3>

<p>Object-oriented estimation of basic bootstrap covariances, using
simple (clustered) case-based resampling, plus more refined methods
for <code><a href="stats.html#topic+lm">lm</a></code> and <code><a href="stats.html#topic+glm">glm</a></code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovBS(x, ...)

## Default S3 method:
vcovBS(x, cluster = NULL, R = 250, start = FALSE, type = "xy", ...,
  fix = FALSE, use = "pairwise.complete.obs", applyfun = NULL, cores = NULL,
  center = "mean")

## S3 method for class 'lm'
vcovBS(x, cluster = NULL, R = 250, type = "xy", ...,
  fix = FALSE, use = "pairwise.complete.obs", applyfun = NULL, cores = NULL,
  qrjoint = FALSE, center = "mean")

## S3 method for class 'glm'
vcovBS(x, cluster = NULL, R = 250, start = FALSE, type = "xy", ...,
  fix = FALSE, use = "pairwise.complete.obs", applyfun = NULL, cores = NULL,
  center = "mean")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovBS_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_cluster">cluster</code></td>
<td>
<p>a variable indicating the clustering of observations,
a <code>list</code> (or <code>data.frame</code>) thereof, or a formula specifying
which variables from the fitted model should be used (see examples).
By default (<code>cluster = NULL</code>), either <code>attr(x, "cluster")</code> is used
(if any) or otherwise every observation is assumed to be its own cluster.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_r">R</code></td>
<td>
<p>integer. Number of bootstrap replications.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_start">start</code></td>
<td>
<p>logical. Should <code>coef(x)</code> be passed as <code>start</code>
to the <code>update(x, subset = ...)</code> call? In case the model <code>x</code>
is computed by some numeric iteration, this may speed up the bootstrapping.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_type">type</code></td>
<td>
<p>character (or function). The character string specifies the type of
bootstrap to use: In the default and <code>glm</code> method the three types
<code>"xy"</code>, <code>"fractional"</code>, and <code>"jackknife"</code> are available.
In the <code>lm</code> method there are additionally <code>"residual"</code>, <code>"wild"</code>
(or equivalently: <code>"wild-rademacher"</code> or <code>"rademacher"</code>),
<code>"mammen"</code> (or <code>"wild-mammen"</code>), <code>"norm"</code>
(or <code>"wild-norm"</code>), <code>"webb"</code> (or <code>"wild-webb"</code>).
Finally, for the <code>lm</code> method <code>type</code> can be a <code>function(n)</code>
for drawing wild bootstrap factors.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_...">...</code></td>
<td>
<p>arguments passed to methods. For the default method, this is
passed to <code>update</code>, and for the <code>lm</code> method to <code>lm.fit</code>.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_fix">fix</code></td>
<td>
<p>logical. Should the covariance matrix be fixed to be
positive semi-definite in case it is not?</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_use">use</code></td>
<td>
<p>character. Specification passed to <code><a href="stats.html#topic+cov">cov</a></code> for
handling missing coefficients/parameters.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_applyfun">applyfun</code></td>
<td>
<p>an optional <code><a href="base.html#topic+lapply">lapply</a></code>-style function with arguments
<code>function(X, FUN, ...)</code>. It is used for refitting the model to the
bootstrap samples. The default is to use the basic <code>lapply</code>
function unless the <code>cores</code> argument is specified (see below).</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_cores">cores</code></td>
<td>
<p>numeric. If set to an integer the <code>applyfun</code> is set to    
<code><a href="parallel.html#topic+mclapply">mclapply</a></code> with the desired number of <code>cores</code>,
except on Windows where <code><a href="parallel.html#topic+parLapply">parLapply</a></code> with
<code>makeCluster(cores)</code> is used.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_center">center</code></td>
<td>
<p>character. For <code>type = "jackknife"</code> the coefficients from
all jacknife samples (each dropping one observational unit/cluster) can be
centered by their <code>"mean"</code> (default) or by the original full-sample
<code>"estimate"</code>.</p>
</td></tr>
<tr><td><code id="vcovBS_+3A_qrjoint">qrjoint</code></td>
<td>
<p>logical. For residual-based and wild boostrap (i.e.,
<code>type != "xy"</code>), should the bootstrap sample the dependent variable
and then apply the QR decomposition jointly only once? If <code>FALSE</code>,
the boostrap applies the QR decomposition separately in each iteration
and samples coefficients directly. If the sample size (and the number of
coefficients) is large, then <code>qrjoint = TRUE</code> maybe significantly
faster while requiring much more memory.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clustered sandwich estimators are used to adjust inference when errors
are correlated within (but not between) clusters. See the documentation for <code><a href="#topic+vcovCL">vcovCL</a></code> 
for specifics about covariance clustering. This function allows
for clustering in arbitrarily many cluster dimensions (e.g., firm, time, industry), given all
dimensions have enough clusters (for more details, see Cameron et al. 2011).
Unlike <code>vcovCL</code>, <code>vcovBS</code> uses a bootstrap rather than an asymptotic solution.
</p>
<p>Basic (clustered) bootstrap covariance matrix estimation is provided by
the default <code>vcovBS</code> method. It samples clusters (where each observation
is its own cluster by default), i.e., using case-based resampling. For obtaining
a covariance matrix estimate it is assumed that an <code><a href="stats.html#topic+update">update</a></code>
of the model with the resampled <code>subset</code> can be obtained, the <code><a href="stats.html#topic+coef">coef</a></code>
extracted, and finally the covariance computed with <code><a href="stats.html#topic+cov">cov</a></code>.
</p>
<p>The <code>update</code> model is evaluated in the <code>environment(terms(x))</code> (if available).
To speed up computations two further arguments can be leveraged.
</p>

<ol>
<li><p> Instead of <code><a href="base.html#topic+lapply">lapply</a></code> a parallelized function such as
<code><a href="parallel.html#topic+parLapply">parLapply</a></code> or <code><a href="parallel.html#topic+mclapply">mclapply</a></code>
can be specified to iterate over the bootstrap replications. For the latter,
specifying <code>cores = ...</code> is a convenience shortcut.
</p>
</li>
<li><p> When specifying <code>start = TRUE</code>, the <code>coef(x)</code> are passed to
<code>update</code> as <code>start = coef(x)</code>. This may not be supported by all
model fitting functions and is hence not turned on by default.
</p>
</li></ol>

<p>The &ldquo;xy&rdquo; or &ldquo;pairs&rdquo; bootstrap is consistent for heteroscedasticity and clustered errors, 
and converges to the asymptotic solution used in <code>vcovCL</code>
as <code>R</code>, <code class="reqn">n</code>, and <code class="reqn">g</code> become large (<code class="reqn">n</code> and <code class="reqn">g</code> are the number of
observations and the number of clusters, respectively; see Efron 1979, or Mammen 1992, for a 
discussion of bootstrap asymptotics). For small <code class="reqn">g</code> &ndash; particularly under 30 groups &ndash; the
bootstrap will converge to a slightly different value than the asymptotic method, due to
the limited number of distinct bootstrap replications possible (see Webb 2014 for a discussion
of this phenomonon). The bootstrap will not necessarily converge to an asymptotic estimate
that has been corrected for small samples.
</p>
<p>The xy approach to bootstrapping is generally only of interest to the
practitioner when the asymptotic solution is unavailable (this can happen when using
estimators that have no <code>estfun</code> function, for example). The residual bootstrap,
by contrast, is rarely of practical interest, because while it provides consistent
inference for clustered standard errors, it is not robust to heteroscedasticity.
More generally, bootstrapping is useful when the bootstrap makes different assumptions than the asymptotic
estimator, in particular when the number of clusters is small and large <code class="reqn">n</code> or
<code class="reqn">g</code> assumptions are unreasonable. Bootstrapping is also often effective for nonlinear models,
particularly in smaller samples, where asymptotic approaches often perform relatively poorly.
See Cameron and Miller (2015) for further discussion of bootstrap techniques in practical applications,
and Zeileis et al. (2020) show simulations comparing <code>vcovBS</code> to <code>vcovCL</code> in several
settings.
</p>
<p>The jackknife approach is of particular interest in practice because it can be shown to be
exactly equivalent to the HC3 (without cluster adjustment, also known as CV3)
covariance matrix estimator in linear models (see MacKinnon,
Nielsen, Webb 2022). If the number of observations per cluster is large it may become
impossible to compute this estimator via <code><a href="#topic+vcovCL">vcovCL</a></code> while using the jackknife
approach will still be feasible. In nonlinear models (including non-Gaussian GLMs) the
jackknife and the HC3 estimator do not coincide but the jackknife might still be a useful
alternative when the HC3 cannot be computed. A convenience interface <code><a href="#topic+vcovJK">vcovJK</a></code>
is provided whose default method simply calls <code>vcovBS(..., type = "jackknife")</code>.
</p>
<p>The fractional-random-weight bootstrap (see Xu et al. 2020), first introduced by
Rubin (1981) as Bayesian bootstrap, is an alternative to the xy bootstrap when it is
computationally challenging or even impractical to reestimate the model on subsets, e.g.,
when &quot;successes&quot; in binary responses are rare or when the number of parameters is close
to the sample size. In these situations excluding some observations completely is the
source of the problems, i.e., giving some observations zero weight while others receive
integer weights of one ore more. The fractional bootstrap mitigates this by giving
every observation a positive fractional weight, drawn from a Dirichlet distribution.
These may become close to zero but never exclude an observation completly, thus stabilizing
the computation of the reweighted models.
</p>
<p>The <code><a href="stats.html#topic+glm">glm</a></code> method works essentially like the default method but calls
<code><a href="stats.html#topic+glm.fit">glm.fit</a></code> instead of <code>update</code>.
</p>
<p>The <code><a href="stats.html#topic+lm">lm</a></code> method provides additional bootstrapping <code>type</code>s
and computes the bootstrapped coefficient estimates somewhat more efficiently using
<code><a href="stats.html#topic+lm.fit">lm.fit</a></code> (for case-based resampling) or <code><a href="base.html#topic+qr.coef">qr.coef</a></code>
rather than <code>update</code>. The default <code>type</code> is case-based resampling
(<code>type = "xy"</code>) as in the default method. Alternative <code>type</code> specifications are:
</p>

<ul>
<li> <p><code>"residual"</code>. The residual cluster bootstrap resamples the residuals (as above,
by cluster) which are subsequently added to the fitted values to obtain the bootstrapped
response variable: <code class="reqn">y^{*} = \hat{y} + e^{*}</code>.
Coefficients can then be estimated using <code>qr.coef()</code>, reusing the
QR decomposition from the original fit. As Cameron et al. (2008) point out,
the residual cluster bootstrap is not well-defined when the clusters are unbalanced as 
residuals from one cluster cannot be easily assigned to another cluster with different size.
Hence a warning is issued in that case.
</p>
</li>
<li> <p><code>"wild"</code> (or equivalently <code>"wild-rademacher"</code> or <code>"rademacher"</code>).
The wild cluster bootstrap does not actually resample the residuals but instead reforms the
dependent variable by multiplying the residual by a randomly drawn value and adding the
result to the fitted value: <code class="reqn">y^{*} = \hat{y} + e \cdot w</code>
(see Cameron et al. 2008). By default, the factors are drawn from the Rademacher distribution:
<code>function(n) sample(c(-1, 1), n, replace = TRUE)</code>.
</p>
</li>
<li> <p><code>"mammen"</code> (or <code>"wild-mammen"</code>). This draws the wild bootstrap factors as
suggested by Mammen (1993):
<code>sample(c(-1, 1) * (sqrt(5) + c(-1, 1))/2, n, replace = TRUE, prob = (sqrt(5) + c(1, -1))/(2 * sqrt(5)))</code>.
</p>
</li>
<li> <p><code>"webb"</code> (or <code>"wild-webb"</code>). This implements the six-point distribution
suggested by Webb (2014), which may improve inference when the number of clusters is small:
<code>sample(c(-sqrt((3:1)/2), sqrt((1:3)/2)), n, replace = TRUE)</code>.
</p>
</li>
<li> <p><code>"norm"</code> (or <code>"wild-norm"</code>). The standard normal/Gaussian distribution
is used for drawing the wild bootstrap factors: <code>function(n) rnorm(n)</code>.
</p>
</li>
<li><p> User-defined function. This needs of the form as above, i.e., a <code>function(n)</code>
returning a vector of random wild bootstrap factors of corresponding length.
</p>
</li></ul>



<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Cameron AC, Gelbach JB, Miller DL (2008).
&ldquo;Bootstrap-Based Improvements for Inference with Clustered Errors&rdquo;,
<em>The Review of Economics and Statistics</em>, <b>90</b>(3), 414&ndash;427.
<a href="https://doi.org/10.3386/t0344">doi:10.3386/t0344</a>
</p>
<p>Cameron AC, Gelbach JB, Miller DL (2011).
&ldquo;Robust Inference with Multiway Clustering&rdquo;,
<em>Journal of Business &amp; Economic Statistics</em>, <b>29</b>(2), 238&ndash;249.
<a href="https://doi.org/10.1198/jbes.2010.07136">doi:10.1198/jbes.2010.07136</a>
</p>
<p>Cameron AC, Miller DL (2015). 
&ldquo;A Practitioner's Guide to Cluster-Robust Inference&rdquo;, 
<em>Journal of Human Resources</em>, <b>50</b>(2), 317&ndash;372.
<a href="https://doi.org/10.3368/jhr.50.2.317">doi:10.3368/jhr.50.2.317</a>
</p>
<p>Efron B (1979). 
&ldquo;Bootstrap Methods: Another Look at the Jackknife&rdquo;, 
<em>The Annals of Statistics</em>, <b>7</b>(1), 1&ndash;26.
<a href="https://doi.org/10.1214/aos/1176344552">doi:10.1214/aos/1176344552</a>
</p>
<p>MacKinnon JG, Nielsen MØ, Webb MD (2022). 
&ldquo;Cluster-Robust Inference: A Guide to Empirical Practice&rdquo;, 
<em>Journal of Econometrics</em>, Forthcoming.
<a href="https://doi.org/10.1016/j.jeconom.2022.04.001">doi:10.1016/j.jeconom.2022.04.001</a>
</p>
<p>Mammen E (1992). 
&ldquo;When Does Bootstrap Work?: Asymptotic Results and Simulations&rdquo;, 
<em>Lecture Notes in Statistics</em>, <b>77</b>.
Springer Science &amp; Business Media.
</p>
<p>Mammen E (1993). 
&ldquo;Bootstrap and Wild Bootstrap for High Dimensional Linear Models&rdquo;, 
<em>The Annals of Statistics</em>, <b>21</b>(1), 255&ndash;285.
<a href="https://doi.org/10.1214/aos/1176349025">doi:10.1214/aos/1176349025</a>
</p>
<p>Rubin DB (1981).
&ldquo;The Bayesian Bootstrap&rdquo;,
<em>The Annals of Statistics</em>, <b>9</b>(1), 130&ndash;134.
<a href="https://doi.org/10.1214/aos/1176345338">doi:10.1214/aos/1176345338</a>
</p>
<p>Webb MD (2014).
&ldquo;Reworking Wild Bootstrap Based Inference for Clustered Errors&rdquo;,
Working Paper 1315, <em>Queen's Economics Department.</em>
<a href="https://www.econ.queensu.ca/sites/econ.queensu.ca/files/qed_wp_1315.pdf">https://www.econ.queensu.ca/sites/econ.queensu.ca/files/qed_wp_1315.pdf</a>.
</p>
<p>Xu L, Gotwalt C, Hong Y, King CB, Meeker WQ (2020).
&ldquo;Applications of the Fractional-Random-Weight Bootstrap&rdquo;,
<em>The American Statistician</em>, <b>74</b>(4), 345&ndash;358.
<a href="https://doi.org/10.1080/00031305.2020.1731599">doi:10.1080/00031305.2020.1731599</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovCL">vcovCL</a></code>, <code><a href="#topic+vcovJK">vcovJK</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Petersen's data
data("PetersenCL", package = "sandwich")
m &lt;- lm(y ~ x, data = PetersenCL)

## comparison of different standard errors
suppressWarnings(RNGversion("3.5.0"))
set.seed(1)
cbind(
  "classical" = sqrt(diag(vcov(m))),
  "HC-cluster" = sqrt(diag(vcovCL(m, cluster = ~ firm))),
  "BS-cluster" = sqrt(diag(vcovBS(m, cluster = ~ firm))),
  "FW-cluster" = sqrt(diag(vcovBS(m, cluster = ~ firm, type = "fractional")))
)

## two-way wild cluster bootstrap with Mammen distribution
vcovBS(m, cluster = ~ firm + year, type = "wild-mammen")

## jackknife estimator coincides with HC3 (aka CV3)
all.equal(
  vcovBS(m, cluster = ~ firm, type = "jackknife"),
  vcovCL(m, cluster = ~ firm, type = "HC3", cadjust = FALSE),
  tolerance = 1e-7
)
</code></pre>

<hr>
<h2 id='vcovCL'>Clustered Covariance Matrix Estimation</h2><span id='topic+vcovCL'></span><span id='topic+meatCL'></span>

<h3>Description</h3>

<p>Estimation of one-way and multi-way clustered
covariance matrices using an object-oriented approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovCL(x, cluster = NULL, type = NULL, sandwich = TRUE, fix = FALSE, ...)
meatCL(x, cluster = NULL, type = NULL, cadjust = TRUE, multi0 = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovCL_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_cluster">cluster</code></td>
<td>
<p>a variable indicating the clustering of observations,
a <code>list</code> (or <code>data.frame</code>) thereof, or a formula specifying
which variables from the fitted model should be used (see examples).
By default (<code>cluster = NULL</code>), either <code>attr(x, "cluster")</code> is used
(if any) or otherwise every observation is assumed to be its own cluster.</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_type">type</code></td>
<td>
<p>a character string specifying the estimation type (HC0&ndash;HC3). 
The default is to use <code>"HC1"</code> for <code>lm</code> objects and
<code>"HC0"</code> otherwise.</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the meat matrix is returned.</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_fix">fix</code></td>
<td>
<p>logical. Should the covariance matrix be fixed to be
positive semi-definite in case it is not?</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_cadjust">cadjust</code></td>
<td>
<p>logical. Should a cluster adjustment be applied?</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_multi0">multi0</code></td>
<td>
<p>logical. Should the HC0 estimate be used for
the final adjustment in multi-way clustered covariances?</p>
</td></tr>
<tr><td><code id="vcovCL_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>meatCL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clustered sandwich estimators are used to adjust inference when errors
are correlated within (but not between) clusters. <code>vcovCL</code> allows
for clustering in arbitrary many cluster dimensions (e.g., firm, time, industry), given all
dimensions have enough clusters (for more details, see Cameron et al. 2011).
If each observation is its own cluster, the clustered sandwich
collapses to the basic sandwich covariance.
</p>
<p>The function <code>meatCL</code> is the work horse for estimating
the meat of clustered sandwich estimators. <code>vcovCL</code> is a wrapper calling
<code><a href="#topic+sandwich">sandwich</a></code> and <code><a href="#topic+bread">bread</a></code> (Zeileis 2006).
<code>vcovCL</code> is applicable beyond <code>lm</code> or <code>glm</code> class objects.
</p>
<p><code><a href="#topic+bread">bread</a></code> and <code><a href="#topic+meat">meat</a></code> matrices are multiplied to
construct clustered sandwich estimators.
The meat of a clustered sandwich estimator is the cross product of
the clusterwise summed estimating functions. Instead of summing over
all individuals, first sum over cluster.
</p>
<p>A two-way clustered sandwich estimator <code class="reqn">M</code> (e.g., for cluster dimensions
&quot;firm&quot; and &quot;industry&quot; or &quot;id&quot; and &quot;time&quot;) is a linear combination of
one-way clustered sandwich estimators for both dimensions
(<code class="reqn">M_{id}, M_{time}</code>) minus the
clustered sandwich estimator, with clusters formed out of the
intersection of both dimensions (<code class="reqn">M_{id \cap time}</code>):
</p>
<p style="text-align: center;"><code class="reqn">M = M_{id} + M_{time} - M_{id \cap time}</code>
</p>
<p>.
Additionally, each of the three terms can be weighted by the corresponding
cluster bias adjustment factor (see below and Equation 20 in Zeileis et al. 2020).
Instead of subtracting <code class="reqn">M_{id \cap time}</code> as the last
subtracted matrix, Ma (2014) suggests to subtract the basic HC0
covariance matrix when only a single observation is in each
intersection of <code class="reqn">id</code>  and <code class="reqn">time</code>.
Set <code>multi0 = TRUE</code> to subtract the basic HC0 covariance matrix as
the last subtracted matrix in multi-way clustering. For details,
see also Petersen (2009) and Thompson (2011).
</p>
<p>With the <code>type</code> argument, HC0 to HC3 types of
bias adjustment can be employed, following the terminology used by
MacKinnon and White (1985) for heteroscedasticity corrections. HC0 applies no small sample bias adjustment.
HC1 applies a degrees of freedom-based correction, <code class="reqn">(n-1)/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> is the number of explanatory or predictor variables in the model.
HC1 is the most commonly used approach for linear models, and HC0 otherwise. Hence these
are the defaults in <code>vcovCL</code>. However, HC0 and HC1 are less effective
than HC2 and HC3 when the number of clusters is relatively small (Cameron et al. 2008).
HC2 and HC3 types of bias adjustment are geared towards the linear
model, but they are also applicable for GLMs (see Bell and McCaffrey
2002, and Kauermann and Carroll 2001, for details).
A precondition for HC2 and HC3 types of bias adjustment is the availability
of a hat matrix (or a weighted version therof for GLMs) and hence
these two types are currently only implemented for <code><a href="stats.html#topic+lm">lm</a></code>
and <code><a href="stats.html#topic+glm">glm</a></code> objects.
</p>
<p>An alternative to the clustered HC3 estimator is the clustered jackknife estimator
which is available in <code><a href="#topic+vcovBS">vcovBS</a></code> with <code>type = "jackknife"</code>. In linear
models the HC3 and the jackknife estimator coincide (MacKinnon et al. 2022) with the
latter still being computationally feasible if the number of observations per cluster
is large. In nonlinear models (including non-Gaussian GLMs) the jackknife and the HC3
estimator do not coincide but the jackknife might still be a useful alternative when
the HC3 cannot be computed.
</p>
<p>The <code>cadjust</code> argument allows to
switch the cluster bias adjustment factor <code class="reqn">G/(G-1)</code> on and
off (where <code class="reqn">G</code> is the number of clusters in a cluster dimension <code class="reqn">g</code>)
See Cameron et al. (2008) and Cameron et al. (2011) for more details about
small-sample modifications.
</p>
<p>The <code>cluster</code> specification can be made in a number of ways: The <code>cluster</code>
can be a single variable or a <code>list</code>/<code>data.frame</code> of multiple
clustering variables. If <code><a href="stats.html#topic+expand.model.frame">expand.model.frame</a></code> works
for the model object <code>x</code>, the <code>cluster</code> can also be a <code>formula</code>.
By default (<code>cluster = NULL</code>), <code>attr(x, "cluster")</code> is checked and
used if available. If not, every observation is assumed to be its own cluster.
If the number of observations in the model <code>x</code> is smaller than in the
original <code>data</code> due to <code>NA</code> processing, then the same <code>NA</code> processing
can be applied to <code>cluster</code> if necessary (and <code>x$na.action</code> being
available).
</p>
<p>Cameron et al. (2011) observe that sometimes the covariance matrix is
not positive-semidefinite and recommend to employ the eigendecomposition of the estimated
covariance matrix, setting any negative eigenvalue(s) to zero. This fix
is applied, if necessary, when <code>fix = TRUE</code> is specified.
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Bell RM, McCaffrey DF (2002).
&ldquo;Bias Reduction in Standard Errors for Linear Regression with Multi-Stage Samples&rdquo;,
<em>Survey Methodology</em>, <b>28</b>(2), 169&ndash;181.
</p>
<p>Cameron AC, Gelbach JB, Miller DL (2008).
&ldquo;Bootstrap-Based Improvements for Inference with Clustered Errors&rdquo;,
<em>The Review of Economics and Statistics</em>, <b>90</b>(3),
414&ndash;427.
<a href="https://doi.org/10.3386/t0344">doi:10.3386/t0344</a>
</p>
<p>Cameron AC, Gelbach JB, Miller DL (2011).
&ldquo;Robust Inference with Multiway Clustering&rdquo;,
<em>Journal of Business &amp; Ecomomic Statistics</em>, <b>29</b>(2),
238&ndash;249.
<a href="https://doi.org/10.1198/jbes.2010.07136">doi:10.1198/jbes.2010.07136</a>
</p>
<p>Kauermann G, Carroll RJ (2001).
&ldquo;A Note on the Efficiency of Sandwich Covariance Matrix
Estimation&rdquo;,
<em>Journal of the American Statistical Association</em>,
<b>96</b>(456), 1387&ndash;1396.
<a href="https://doi.org/10.1198/016214501753382309">doi:10.1198/016214501753382309</a>
</p>
<p>Ma MS (2014).
&ldquo;Are We Really Doing What We Think We Are Doing? A Note on
Finite-Sample Estimates of Two-Way Cluster-Robust Standard Errors&rdquo;,
<em>Mimeo, Availlable at SSRN.</em>
<a href="https://doi.org/10.2139/ssrn.2420421">doi:10.2139/ssrn.2420421</a>
</p>
<p>MacKinnon JG, Nielsen MØ, Webb MD (2022). 
&ldquo;Cluster-Robust Inference: A Guide to Empirical Practice&rdquo;, 
<em>Journal of Econometrics</em>, Forthcoming.
<a href="https://doi.org/10.1016/j.jeconom.2022.04.001">doi:10.1016/j.jeconom.2022.04.001</a>
</p>
<p>MacKinnon JG, White H (1985). 
&ldquo;Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties&rdquo;
<em>Journal of Econometrics</em>, <b>29</b>(3), 305&ndash;325.
<a href="https://doi.org/10.1016/0304-4076%2885%2990158-7">doi:10.1016/0304-4076(85)90158-7</a>
</p>
<p>Petersen MA (2009).
&ldquo;Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches&rdquo;,
<em>The Review of Financial Studies</em>, <b>22</b>(1), 435&ndash;480.
<a href="https://doi.org/10.1093/rfs/hhn053">doi:10.1093/rfs/hhn053</a>
</p>
<p>Thompson SB (2011).
&ldquo;Simple Formulas for Standard Errors That Cluster by Both Firm
and Time&rdquo;,
<em>Journal of Financial Economics</em>, <b>99</b>(1), 1&ndash;10.
<a href="https://doi.org/10.1016/j.jfineco.2010.08.016">doi:10.1016/j.jfineco.2010.08.016</a>
</p>
<p>Zeileis A (2004).
&ldquo;Econometric Computing with HC and HAC Covariance Matrix Estimator&rdquo;,
<em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>
</p>
<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators&rdquo;,
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovHC">vcovHC</a></code>, <code><a href="#topic+vcovBS">vcovBS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Petersen's data
data("PetersenCL", package = "sandwich")
m &lt;- lm(y ~ x, data = PetersenCL)

## clustered covariances
## one-way
vcovCL(m, cluster = ~ firm)
vcovCL(m, cluster = PetersenCL$firm) ## same
## one-way with HC2
vcovCL(m, cluster = ~ firm, type = "HC2")
## two-way
vcovCL(m, cluster = ~ firm + year)
vcovCL(m, cluster = PetersenCL[, c("firm", "year")]) ## same

## comparison with cross-section sandwiches
## HC0
all.equal(sandwich(m), vcovCL(m, type = "HC0", cadjust = FALSE))
## HC2
all.equal(vcovHC(m, type = "HC2"), vcovCL(m, type = "HC2"))
## HC3
all.equal(vcovHC(m, type = "HC3"), vcovCL(m, type = "HC3"))

## Innovation data
data("InstInnovation", package = "sandwich")

## replication of one-way clustered standard errors for model 3, Table I
## and model 1, Table II in Berger et al. (2017), see ?InstInnovation

## count regression formula
f1 &lt;- cites ~ institutions + log(capital/employment) + log(sales) + industry + year

## model 3, Table I: Poisson model
## one-way clustered standard errors
tab_I_3_pois &lt;- glm(f1, data = InstInnovation, family = poisson)
vcov_pois &lt;- vcovCL(tab_I_3_pois, InstInnovation$company)
sqrt(diag(vcov_pois))[2:4]

## coefficient tables
if(require("lmtest")) {
coeftest(tab_I_3_pois, vcov = vcov_pois)[2:4, ]
}

## Not run: 
## model 1, Table II: negative binomial hurdle model
## (requires "pscl" or alternatively "countreg" from R-Forge)
library("pscl")
library("lmtest")
tab_II_3_hurdle &lt;- hurdle(f1, data = InstInnovation, dist = "negbin")
#  dist = "negbin", zero.dist = "negbin", separate = FALSE)
vcov_hurdle &lt;- vcovCL(tab_II_3_hurdle, InstInnovation$company)
sqrt(diag(vcov_hurdle))[c(2:4, 149:151)]
coeftest(tab_II_3_hurdle, vcov = vcov_hurdle)[c(2:4, 149:151), ]

## End(Not run)
</code></pre>

<hr>
<h2 id='vcovHAC'>Heteroscedasticity and Autocorrelation Consistent (HAC) Covariance Matrix Estimation</h2><span id='topic+vcovHAC'></span><span id='topic+vcovHAC.default'></span><span id='topic+meatHAC'></span>

<h3>Description</h3>

<p>Heteroscedasticity and autocorrelation consistent (HAC) estimation
of the covariance matrix of the coefficient estimates in a (generalized)
linear regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovHAC(x, ...)

## Default S3 method:
vcovHAC(x, order.by = NULL, prewhite = FALSE, weights = weightsAndrews,
  adjust = TRUE, diagnostics = FALSE, sandwich = TRUE, ar.method = "ols",
  data = list(), ...)

meatHAC(x, order.by = NULL, prewhite = FALSE, weights = weightsAndrews,
  adjust = TRUE, diagnostics = FALSE, ar.method = "ols", data = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovHAC_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_order.by">order.by</code></td>
<td>
<p>Either a vector <code>z</code> or a formula with a single explanatory
variable like <code>~ z</code>. The observations in the model
are ordered by the size of <code>z</code>. If set to <code>NULL</code> (the
default) the observations are assumed to be ordered (e.g., a
time series).</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_prewhite">prewhite</code></td>
<td>
<p>logical or integer. Should the estimating functions
be prewhitened? If <code>TRUE</code> or greater than 0 a VAR model of
order <code>as.integer(prewhite)</code> is fitted via <code>ar</code> with
method <code>"ols"</code> and <code>demean = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_weights">weights</code></td>
<td>
<p>Either a vector of weights for the autocovariances or a
function to compute these weights based on <code>x</code>, <code>order.by</code>,
<code>prewhite</code>, <code>ar.method</code> and <code>data</code>. If <code>weights</code>
is a function it has to take these arguments. See also details.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_diagnostics">diagnostics</code></td>
<td>
<p>logical. Should additional model diagnostics be returned?
See below for details.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the meat matrix is returned.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_ar.method">ar.method</code></td>
<td>
<p>character. The <code>method</code> argument passed to
<code><a href="stats.html#topic+ar">ar</a></code> for prewhitening.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the <code>order.by</code> 
model. By default the variables are taken from the environment which
<code>vcovHAC</code> is called from.</p>
</td></tr>
<tr><td><code id="vcovHAC_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="#topic+sandwich">sandwich</a></code> (in <code>vcovHAC</code>)
and <code><a href="#topic+estfun">estfun</a></code> (in <code>meatHAC</code>), respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>meatHAC</code> is the real work horse for estimating
the meat of HAC sandwich estimators &ndash; the default <code>vcovHAC</code> method
is a wrapper calling
<code><a href="#topic+sandwich">sandwich</a></code> and <code><a href="#topic+bread">bread</a></code>. See Zeileis (2006) for
more implementation details. The theoretical background, exemplified
for the linear regression model, is described in Zeileis (2004).
</p>
<p>Both functions construct weighted information sandwich variance estimators
for parametric models fitted to time series data. These are basically
constructed from weighted sums of autocovariances of the estimating functions
(as extracted by <code><a href="#topic+estfun">estfun</a></code>). The crucial step is the specification
of weights: the user can either supply <code>vcovHAC</code> with some vector of 
weights or with a function that computes these weights adaptively (based on
the arguments <code>x</code>, <code>order.by</code>, <code>prewhite</code> and <code>data</code>). 
Two functions for adaptively choosing weights are implemented in
<code><a href="#topic+weightsAndrews">weightsAndrews</a></code> implementing the results of Andrews (1991) and
in <code><a href="#topic+weightsLumley">weightsLumley</a></code> implementing the results of Lumley (1999).
The functions <code><a href="#topic+kernHAC">kernHAC</a></code> and <code><a href="#topic+weave">weave</a></code> respectively
are to more convenient interfaces for <code>vcovHAC</code> with these functions.
</p>
<p>Prewhitening based on VAR approximations is described as suggested in
Andrews &amp; Monahan (1992).
</p>
<p>The covariance matrix estimators have been improved by the addition of a bias correction and an
approximate denominator degrees of freedom for test and confidence interval
construction. See Lumley &amp; Heagerty (1999) for details.
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate. If <code>diagnostics</code>
was set to <code>TRUE</code> this has an attribute <code>"diagnostics"</code> which is a list 
with
</p>
<table role = "presentation">
<tr><td><code>bias.correction</code></td>
<td>
<p>multiplicative bias correction</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Approximate denominator degrees of freedom</p>
</td></tr>
</table>


<h3>References</h3>

<p>Andrews DWK (1991).
&ldquo;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.&rdquo;
<em>Econometrica</em>, <b>59</b>, 817&ndash;858.
</p>
<p>Andrews DWK &amp; Monahan JC (1992).
&ldquo;An Improved Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimator.&rdquo;
<em>Econometrica</em>, <b>60</b>, 953&ndash;966.
</p>
<p>Lumley T &amp; Heagerty P (1999).
&ldquo;Weighted Empirical Adaptive Variance Estimators for Correlated Data Regression.&rdquo;
<em>Journal of the Royal Statistical Society B</em>, <b>61</b>, 459&ndash;477.
</p>
<p>Newey WK &amp; West KD (1987).
&ldquo;A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.&rdquo;
<em>Econometrica</em>, <b>55</b>, 703&ndash;708.
</p>
<p>Zeileis A (2004).
&ldquo;Econometric Computing with HC and HAC Covariance Matrix Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>  
</p>
<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightsLumley">weightsLumley</a></code>, <code><a href="#topic+weightsAndrews">weightsAndrews</a></code>,
<code><a href="#topic+weave">weave</a></code>, <code><a href="#topic+kernHAC">kernHAC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sin(1:100)
y &lt;- 1 + x + rnorm(100)
fm &lt;- lm(y ~ x)
vcovHAC(fm)
vcov(fm)
</code></pre>

<hr>
<h2 id='vcovHC'>Heteroscedasticity-Consistent Covariance Matrix Estimation</h2><span id='topic+vcovHC'></span><span id='topic+vcovHC.default'></span><span id='topic+vcovHC.mlm'></span><span id='topic+meatHC'></span>

<h3>Description</h3>

<p>Heteroscedasticity-consistent estimation of the covariance matrix of the
coefficient estimates in regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovHC(x, ...)

## Default S3 method:
vcovHC(x,
  type = c("HC3", "const", "HC", "HC0", "HC1", "HC2", "HC4", "HC4m", "HC5"),
  omega = NULL, sandwich = TRUE, ...)

meatHC(x, type = , omega = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovHC_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovHC_+3A_type">type</code></td>
<td>
<p>a character string specifying the estimation type. For
details see below.</p>
</td></tr>
<tr><td><code id="vcovHC_+3A_omega">omega</code></td>
<td>
<p>a vector or a
function depending on the arguments <code>residuals</code>
(the working residuals of the model), <code>diaghat</code> (the diagonal 
of the corresponding hat matrix) and <code>df</code> (the residual degrees of
freedom). For details see below.</p>
</td></tr>
<tr><td><code id="vcovHC_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the meat matrix is returned.</p>
</td></tr>
<tr><td><code id="vcovHC_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="#topic+sandwich">sandwich</a></code> (in <code>vcovHC</code>)
and <code><a href="#topic+estfun">estfun</a></code> (in <code>meatHC</code>), respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>meatHC</code> is the real work horse for estimating
the meat of HC sandwich estimators &ndash; the default <code>vcovHC</code> method
is a wrapper calling
<code><a href="#topic+sandwich">sandwich</a></code> and <code><a href="#topic+bread">bread</a></code>. See Zeileis (2006) for
more implementation details. The theoretical background, exemplified
for the linear regression model, is described below and in Zeileis (2004).
Analogous formulas are employed for other types of models, provided that
they depend on a single linear predictor and the estimating functions
can be represented as a product of &ldquo;working residual&rdquo; and regressor
vector (Zeileis 2006, Equation 7).
</p>
<p>When <code>type = "const"</code> constant variances are assumed and
and <code>vcovHC</code> gives the usual estimate of the covariance matrix of
the coefficient estimates:
</p>
<p style="text-align: center;"><code class="reqn">\hat \sigma^2 (X^\top X)^{-1}</code>
</p>

<p>All other methods do not assume constant variances and are suitable in case of
heteroscedasticity. <code>"HC"</code> (or equivalently <code>"HC0"</code>) gives White's
estimator, the other estimators are refinements of this. They are all of form
</p>
<p style="text-align: center;"><code class="reqn">(X^\top X)^{-1} X^\top \Omega X (X^\top X)^{-1}</code>
</p>

<p>and differ in the choice of Omega. This is in all cases a diagonal matrix whose 
elements can be either supplied as a vector <code>omega</code> or as a
a function <code>omega</code> of the residuals, the diagonal elements of the hat matrix and
the residual degrees of freedom. For White's estimator
</p>
<p><code>omega &lt;- function(residuals, diaghat, df) residuals^2</code>
</p>
<p>Instead of specifying the diagonal <code>omega</code> or a function for
estimating it, the <code>type</code> argument can be used to specify the 
HC0 to HC5 estimators. If <code>omega</code> is used, <code>type</code> is ignored.
</p>
<p>Long &amp; Ervin (2000) conduct a simulation study of HC estimators (HC0 to HC3) in
the linear regression model, recommending to use HC3 which is thus the
default in <code>vcovHC</code>. Cribari-Neto (2004), Cribari-Neto, Souza, &amp; Vasconcellos (2007),
and Cribari-Neto &amp; Da Silva (2011), respectively, suggest the HC4, HC5, and
modified HC4m type estimators. All of them are tailored to take into account
the effect of leverage points in the design matrix. For more details see the references.
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Cribari-Neto F. (2004). &ldquo;Asymptotic Inference under Heteroskedasticity
of Unknown Form.&rdquo; <em>Computational Statistics &amp; Data Analysis</em>
<b>45</b>, 215&ndash;233.
</p>
<p>Cribari-Neto F., Da Silva W.B. (2011). &ldquo;A New Heteroskedasticity-Consistent
Covariance Matrix Estimator for the Linear Regression Model.&rdquo;
<em>Advances in Statistical Analysis</em>, <b>95</b>(2), 129&ndash;146.
</p>
<p>Cribari-Neto F., Souza T.C., Vasconcellos, K.L.P. (2007). &ldquo;Inference under
Heteroskedasticity and Leveraged Data.&rdquo; <em>Communications in Statistics &ndash; Theory and
Methods</em>, <b>36</b>, 1877&ndash;1888. Errata: <b>37</b>, 3329&ndash;3330, 2008.
</p>
<p>Long J. S., Ervin L. H. (2000). &ldquo;Using Heteroscedasticity Consistent Standard
Errors in the Linear Regression Model.&rdquo; <em>The American Statistician</em>,
<b>54</b>, 217&ndash;224.
</p>
<p>MacKinnon J. G., White H. (1985). &ldquo;Some Heteroskedasticity-Consistent
Covariance Matrix Estimators with Improved Finite Sample Properties.&rdquo;
<em>Journal of Econometrics</em>, <b>29</b>, 305&ndash;325.
</p>
<p>White H. (1980). &ldquo;A Heteroskedasticity-Consistent Covariance Matrix and
a Direct Test for Heteroskedasticity.&rdquo; <em>Econometrica</em> <b>48</b>,
817&ndash;838.
</p>
<p>Zeileis A (2004). &ldquo;Econometric Computing with HC and HAC Covariance Matrix
Estimators.&rdquo; <em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>
</p>
<p>Zeileis A (2006). &ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="car.html#topic+hccm">hccm</a></code>,
<code><a href="lmtest.html#topic+bptest">bptest</a></code>, <code><a href="car.html#topic+ncv.test">ncv.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate linear regression relationship
## with homoscedastic variances
x &lt;- sin(1:100)
y &lt;- 1 + x + rnorm(100)
## model fit and HC3 covariance
fm &lt;- lm(y ~ x)
vcovHC(fm)
## usual covariance matrix
vcovHC(fm, type = "const")
vcov(fm)

sigma2 &lt;- sum(residuals(lm(y ~ x))^2)/98
sigma2 * solve(crossprod(cbind(1, x)))
</code></pre>

<hr>
<h2 id='vcovJK'>(Clustered) Jackknife Covariance Matrix Estimation</h2><span id='topic+vcovJK'></span><span id='topic+vcovJK.default'></span>

<h3>Description</h3>

<p>Object-oriented estimation of jackknife covariances, i.e., based on
the centered outer product of leave-on-out estimates of the model
coefficients/parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovJK(x, ...)

## Default S3 method:
vcovJK(x, cluster = NULL, center = "mean", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovJK_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovJK_+3A_cluster">cluster</code></td>
<td>
<p>a variable indicating the clustering of observations,
a <code>list</code> (or <code>data.frame</code>) thereof, or a formula specifying
which variables from the fitted model should be used (see examples).
By default (<code>cluster = NULL</code>), either <code>attr(x, "cluster")</code> is used
(if any) or otherwise every observation is assumed to be its own cluster.</p>
</td></tr>
<tr><td><code id="vcovJK_+3A_center">center</code></td>
<td>
<p>character specifying how to center the coefficients from
all jacknife samples (each dropping one observational unit/cluster).
By default the coefficients are centered by their <code>"mean"</code> across the
sample or, alternatively, by the original full-sample <code>"estimate"</code>.</p>
</td></tr>
<tr><td><code id="vcovJK_+3A_...">...</code></td>
<td>
<p>arguments passed to methods. For the default method, this is
passed to <code><a href="#topic+vcovBS">vcovBS</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Jackknife covariance estimation is based on leave-one-out estimates of the
coefficients/parameters of a model. This means that the model is reestimated
after dropping each observational unit once, i.e., each individual observation
in independent observations or each cluster in dependent data. The covariance
matrix is then constructed from the scaled outer product of the centered
jackknife estimates. Centering can either be done by the mean of the jackknife
coefficients (default) or by the original full-sample estimates. Scaling is done
by (N - 1)/N where N is the number of observational units.
</p>
<p>Recent research has shown that the jackknife covariance estimate have particularly
useful properties in practice: they are not downward biased and yield better
coverage rates for confidence intervals compared to other &quot;robust&quot; covariance
estimates. See MacKinnon et al. (2022) and Hansen (2022) for more details.
</p>
<p>As jackknife covariances are also based on reestimation of the coefficients on
subsamples, their computation is very similar to bootstrap covariances. Hence,
the <code><a href="#topic+vcovBS">vcovBS</a></code> methods provided in the package all offer an argument
<code>vcovBS(..., type = "jackknife")</code>. This is called by the default
<code>vcovJK</code> method. Therefore, see the arguments of <code>vcovBS</code> for further
details, e.g., for leveraging multicore computations etc.
</p>
<p>In the linear regression model, the jackknife covariance can actually be computed
without reestimating the coefficients but using only the full-sample estimates and
certain elements of the so-called hat matrix. Namly the diagonal elements or
blocks of elements from the hat matrix are needed for independent observations and
clustered data, respectively. These alternative computations of the jackknife
covariances are available in <code><a href="#topic+vcovHC">vcovHC</a></code> and <code><a href="#topic+vcovCL">vcovCL</a></code>, respectively,
in both cases with argument <code>type = "HC3"</code>. To obtain HC3 covariances that exactly
match the jackknife covariances, the jackknife has to be centered with the full-sample
estimates and the right finite-sample adjustment has to be selected for the HC3.
</p>
<p>In small to moderate sample sizes, the HC3 estimation techniques are typically much
faster than the jackknife. However, in large samples it may become impossible to
compute the HC3 covariances while the jackknife approach is still feasible.
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Bell RM, McCaffrey DF (2002).
&ldquo;Bias Reduction in Standard Errors for Linear Regression with Multi-Stage Samples&rdquo;,
<em>Survey Methodology</em>, <b>28</b>(2), 169&ndash;181.
</p>
<p>Hansen BE (2022).
&ldquo;Jackknife Standard Errors for Clustered Regression&rdquo;, Working Paper, August 2022.
<a href="https://www.ssc.wisc.edu/~bhansen/papers/tcauchy.html">https://www.ssc.wisc.edu/~bhansen/papers/tcauchy.html</a>
</p>
<p>MacKinnon JG, Nielsen MØ, Webb MD (2022). 
&ldquo;Cluster-Robust Inference: A Guide to Empirical Practice&rdquo;, 
<em>Journal of Econometrics</em>, Forthcoming.
<a href="https://doi.org/10.1016/j.jeconom.2022.04.001">doi:10.1016/j.jeconom.2022.04.001</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovJK">vcovJK</a></code>, <code><a href="#topic+vcovHC">vcovHC</a></code>, <code><a href="#topic+vcovCL">vcovCL</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## cross-section data
data("PublicSchools", package = "sandwich")
m1 &lt;- lm(Expenditure ~ poly(Income, 2), data = PublicSchools)
vcovJK(m1, center = "estimate")
vcovHC(m1, type = "HC3") * (nobs(m1) - 1)/nobs(m1)

## clustered data
data("PetersenCL", package = "sandwich")
m2 &lt;- lm(y ~ x, data = PetersenCL)

## jackknife estimator coincides with HC3 (aka CV3)
vcovJK(m2, cluster = ~ firm, center = "estimate")
vcovCL(m2, cluster = ~ firm, type = "HC3", cadjust = FALSE)
</code></pre>

<hr>
<h2 id='vcovOPG'>Outer-Product-of-Gradients Covariance Matrix Estimation</h2><span id='topic+vcovOPG'></span>

<h3>Description</h3>

<p>Outer product of gradients estimation for the covariance matrix of the coefficient estimates
in regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovOPG(x, adjust = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovOPG_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovOPG_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="vcovOPG_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code><a href="#topic+estfun">estfun</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In correctly specified models, the &ldquo;meat&rdquo; matrix (cross product of estimating
functions, see <code><a href="#topic+meat">meat</a></code>) and the inverse of the &ldquo;bread&rdquo; matrix
(inverse of the derivative of the estimating functions, see <code><a href="#topic+bread">bread</a></code>) are equal
and correspond to the Fisher information matrix.
Typically, an empirical version of the bread is used for estimation of the information
but alternatively it is also possible to use the meat. This method is also known as
the outer product of gradients (OPG) estimator (Cameron &amp; Trivedi 2005).
</p>
<p>Using the <span class="pkg">sandwich</span> infrastructure, the OPG estimator could easily be computed via
<code>solve(meat(obj))</code> (modulo scaling). To employ numerically more stable implementation
of the inversion, this simple convenience function can be used: <code>vcovOPG(obj)</code>.
</p>
<p>Note that this only works if the <code>estfun()</code> method computes the maximum
likelihood scores (and not a scaled version such as least squares scores for
<code>"lm"</code> objects).
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Cameron AC and Trivedi PK (2005). <em>Microeconometrics: Methods and Applications</em>.
Cambridge University Press, Cambridge.
</p>
<p>Zeileis A (2006). &ldquo;Object-Oriented Computation of Sandwich Estimators.&rdquo;
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+meat">meat</a></code>, <code><a href="#topic+bread">bread</a></code>, <code><a href="#topic+sandwich">sandwich</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate poisson regression relationship
x &lt;- sin(1:100)
y &lt;- rpois(100, exp(1 + x))
## compute usual covariance matrix of coefficient estimates
fm &lt;- glm(y ~ x, family = poisson)
vcov(fm)
vcovOPG(fm)
</code></pre>

<hr>
<h2 id='vcovPC'>Panel-Corrected Covariance Matrix Estimation</h2><span id='topic+vcovPC'></span><span id='topic+meatPC'></span>

<h3>Description</h3>

<p>Estimation of sandwich covariances a la Beck and Katz (1995) for panel data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovPC(x, cluster = NULL, order.by = NULL,
  pairwise = FALSE, sandwich = TRUE, fix = FALSE, ...)

meatPC(x, cluster = NULL, order.by = NULL,
  pairwise = FALSE, kronecker = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovPC_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_cluster">cluster</code></td>
<td>
<p>a single variable indicating the clustering of observations,
or a <code>list</code> (or <code>data.frame</code>) of one or two variables, or a
formula specifying which one ore two variables from the fitted model should
be used (see examples). In case two variables are specified, the second variable
is assumed to provide the time ordering (instead of using the argument
<code>order.by</code>).
By default (<code>cluster = NULL</code>), either <code>attr(x, "cluster")</code> is used
(if any) or otherwise every observation is assumed to be its own cluster.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_order.by">order.by</code></td>
<td>
<p>a variable, list/data.frame, or formula indicating the
aggregation within time periods. By default <code>attr(x, "order.by")</code> is used
(if any) or specified through the second variable in <code>cluster</code> (see above).
If neither is available,  observations within clusters are assumed to be ordered.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_pairwise">pairwise</code></td>
<td>
<p>logical. For unbalanced panels. Indicating whether
the meat should be estimated pair- or casewise.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the meat matrix is returned.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_fix">fix</code></td>
<td>
<p>logical. Should the covariance matrix be fixed to be
positive semi-definite in case it is not?</p>
</td></tr>  
<tr><td><code id="vcovPC_+3A_kronecker">kronecker</code></td>
<td>
<p>logical. Calculate the meat via the
Kronecker-product, shortening the computation time for small
matrices. For large matrices, set <code>kronecker = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vcovPC_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code>meatPC</code> or <code>estfun</code> function,
respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vcovPC</code> is a function for estimating Beck and Katz (1995)
panel-corrected covariance matrix.
</p>
<p>The function <code>meatPC</code> is the work horse for estimating
the meat of Beck and Katz (1995) covariance matrix estimators.
<code>vcovPC</code> is a wrapper calling
<code><a href="#topic+sandwich">sandwich</a></code> and <code><a href="#topic+bread">bread</a></code>
(Zeileis 2006).
</p>
<p>Following Bailey and Katz (2011), there are two alternatives to
estimate the meat for unbalanced panels.
For <code>pairwise = FALSE</code>, a balanced subset of the panel is used,
whereas for <code>pairwise = TRUE</code>, a pairwise balanced sample is
employed.
</p>
<p>The <code>cluster</code>/<code>order.by</code> specification can be made in a number of ways:
Either both can be a single variable or <code>cluster</code> can be a 
<code>list</code>/<code>data.frame</code> of two variables.
If <code><a href="stats.html#topic+expand.model.frame">expand.model.frame</a></code> works for the model object <code>x</code>,
the <code>cluster</code> (and potentially additionally <code>order.by</code>) can also be
a <code>formula</code>. By default (<code>cluster = NULL, order.by = NULL</code>),
<code>attr(x, "cluster")</code> and <code>attr(x, "order.by")</code> are checked and
used if available. If not, every observation is assumed to be its own cluster,
and observations within clusters are assumed to be ordered accordingly.
If the number of observations in the model <code>x</code> is smaller than in the
original <code>data</code> due to <code>NA</code> processing, then the same <code>NA</code> processing
can be applied to <code>cluster</code> if necessary (and <code>x$na.action</code> being
available).
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Bailey D, Katz JN (2011).
&ldquo;Implementing Panel-Corrected Standard Errors in R: The pcse Package&rdquo;,
<em>Journal of Statistical Software, Code Snippets</em>, <b>42</b>(1), 1&ndash;11.
<a href="https://doi.org/10.18637/jss.v042.c01">doi:10.18637/jss.v042.c01</a>
</p>
<p>Beck N, Katz JN (1995).
&ldquo;What To Do (and Not To Do) with Time-Series-Cross-Section Data in Comparative Politics&rdquo;,
<em>American Political Science Review</em>, <b>89</b>(3), 634&ndash;647.
<a href="https://doi.org/10.2307/2082979">doi:10.2307/2082979</a>
</p>
<p>Zeileis A (2004).
&ldquo;Econometric Computing with HC and HAC Covariance Matrix Estimator&rdquo;,
<em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>
</p>
<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators&rdquo;,
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovCL">vcovCL</a></code>, <code><a href="#topic+vcovPL">vcovPL</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Petersen's data
data("PetersenCL", package = "sandwich")
m &lt;- lm(y ~ x, data = PetersenCL)

## Beck and Katz (1995) standard errors
## balanced panel
sqrt(diag(vcovPC(m, cluster = ~ firm + year)))

## unbalanced panel
PU &lt;- subset(PetersenCL, !(firm == 1 &amp; year == 10))
pu_lm &lt;- lm(y ~ x, data = PU)
sqrt(diag(vcovPC(pu_lm, cluster = ~ firm + year, pairwise = TRUE)))
sqrt(diag(vcovPC(pu_lm, cluster = ~ firm + year, pairwise = FALSE)))


## the following specifications of cluster/order.by are equivalent
vcovPC(m, cluster = ~ firm + year)
vcovPC(m, cluster = PetersenCL[, c("firm", "year")])
vcovPC(m, cluster = ~ firm, order.by = ~ year)
vcovPC(m, cluster = PetersenCL$firm, order.by = PetersenCL$year)

## these are also the same when observations within each
## cluster are already ordered
vcovPC(m, cluster = ~ firm)
vcovPC(m, cluster = PetersenCL$firm)

</code></pre>

<hr>
<h2 id='vcovPL'>Clustered Covariance Matrix Estimation for Panel Data</h2><span id='topic+vcovPL'></span><span id='topic+meatPL'></span>

<h3>Description</h3>

<p>Estimation of sandwich covariances a la Newey-West (1987)
and Driscoll and Kraay (1998) for panel data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovPL(x, cluster = NULL, order.by = NULL,
  kernel = "Bartlett", sandwich = TRUE, fix = FALSE, ...)

meatPL(x, cluster = NULL, order.by = NULL,
  kernel = "Bartlett", lag = "NW1987", bw = NULL,
  adjust = TRUE, aggregate = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vcovPL_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_cluster">cluster</code></td>
<td>
<p>a single variable indicating the clustering of observations,
or a <code>list</code> (or <code>data.frame</code>) of one or two variables, or a
formula specifying which one ore two variables from the fitted model should
be used (see examples). In case two variables are specified, the second variable
is assumed to provide the time ordering (instead of using the argument
<code>order.by</code>).
By default (<code>cluster = NULL</code>), either <code>attr(x, "cluster")</code> is used
(if any) or otherwise every observation is assumed to be its own cluster.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_order.by">order.by</code></td>
<td>
<p>a variable, list/data.frame, or formula indicating the
aggregation within time periods. By default <code>attr(x, "order.by")</code> is used
(if any) or specified through the second variable in <code>cluster</code> (see above).
If neither is available,  observations within clusters are assumed to be ordered.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_kernel">kernel</code></td>
<td>
<p>a character specifying the kernel used. All kernels
described in Andrews (1991) are supported, see <code><a href="#topic+kweights">kweights</a></code>.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_lag">lag</code></td>
<td>
<p>character or numeric, indicating the lag length used.
Three rules of thumb (<code>"max"</code> or equivalently <code>"P2009"</code>,
<code>"NW1987"</code>, or <code>"NW1994"</code>) can be specified, or a numeric
number of lags can be specified directly. By default, <code>"NW1987"</code> is used.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_bw">bw</code></td>
<td>
<p>numeric. The bandwidth of the kernel which by default corresponds
to <code>lag + 1</code>. Only one of <code>lag</code> and <code>bw</code> should be
used.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the meat matrix is returned.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_fix">fix</code></td>
<td>
<p>logical. Should the covariance matrix be fixed to be
positive semi-definite in case it is not?</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made? This
amounts to multiplication with <code class="reqn">n/(n - k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> is the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_aggregate">aggregate</code></td>
<td>
<p>logical. Should the <code>estfun</code> be aggregated within each
time period (yielding Driscoll and Kraay 1998) or not (restricting cross-sectional
and cross-serial correlation to zero, yielding panel Newey-West)?</p>
</td></tr>
<tr><td><code id="vcovPL_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code>metaPL</code> or <code>estfun</code> function,
respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vcovPL</code> is a function for estimating the Newey-West (1987) and
Driscoll and Kraay (1998) covariance matrix.
Driscoll and Kraay (1998) apply a Newey-West type correction to the
sequence of cross-sectional averages of the moment conditions (see
Hoechle (2007)). For large <code class="reqn">T</code> (and regardless of the length of the
cross-sectional dimension), the Driscoll and Kraay (1998)
standard errors are robust to general forms of cross-sectional and
serial correlation (Hoechle (2007)).
The Newey-West (1987) covariance matrix restricts the Driscoll and
Kraay (1998) covariance matrix to no cross-sectional correlation.
</p>
<p>The function <code>meatPL</code> is the work horse for estimating
the meat of Newey-West (1987) and Driscoll and Kraay (1998)
covariance matrix estimators. <code>vcovPL</code> is a wrapper calling
<code><a href="#topic+sandwich">sandwich</a></code> and <code><a href="#topic+bread">bread</a></code> (Zeileis 2006).
</p>
<p>Default lag length is the <code>"NW1987"</code>. 
For <code>lag = "NW1987"</code>, the lag length is chosen from the heuristic
<code class="reqn">floor[T^{(1/4)}]</code>. More details on lag length selection in Hoechle (2007). 
For <code>lag = "NW1994"</code>, the lag length is taken from the first step
of Newey and West's (1994) plug-in procedure.
</p>
<p>The <code>cluster</code>/<code>order.by</code> specification can be made in a number of ways:
Either both can be a single variable or <code>cluster</code> can be a 
<code>list</code>/<code>data.frame</code> of two variables.
If <code><a href="stats.html#topic+expand.model.frame">expand.model.frame</a></code> works for the model object <code>x</code>,
the <code>cluster</code> (and potentially additionally <code>order.by</code>) can also be
a <code>formula</code>. By default (<code>cluster = NULL, order.by = NULL</code>),
<code>attr(x, "cluster")</code> and <code>attr(x, "order.by")</code> are checked and
used if available. If not, every observation is assumed to be its own cluster,
and observations within clusters are assumed to be ordered accordingly.
If the number of observations in the model <code>x</code> is smaller than in the
original <code>data</code> due to <code>NA</code> processing, then the same <code>NA</code> processing
can be applied to <code>cluster</code> if necessary (and <code>x$na.action</code> being
available).
</p>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>References</h3>

<p>Andrews DWK (1991).
&ldquo;Heteroscedasticity and Autocorrelation Consistent Covariance Matrix Estimation&rdquo;,
<em>Econometrica</em>, 817&ndash;858.
</p>
<p>Driscoll JC &amp;  Kraay AC (1998).
&ldquo;Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data&rdquo;,
<em>The Review of Economics and Statistics</em>, <b>80</b>(4), 549&ndash;560.
</p>
<p>Hoechle D (2007).
&ldquo;Robust Standard Errors for Panel Regressions with Cross-Sectional Dependence&rdquo;,
<em>Stata Journal</em>, <b>7</b>(3), 281&ndash;312.
</p>
<p>Newey WK &amp; West KD (1987).
&ldquo;Hypothesis Testing with Efficient Method of Moments Estimation&rdquo;,
<em>International Economic Review</em>, 777-787.
</p>
<p>Newey WK &amp; West KD (1994).
&ldquo;Automatic Lag Selection in Covariance Matrix Estimation&rdquo;,
<em>The Review of Economic Studies</em>, <b>61</b>(4), 631&ndash;653.
</p>
<p>White H (1980).
&ldquo;A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity&rdquo;,
<em>Econometrica</em>, 817&ndash;838.
<a href="https://doi.org/10.2307/1912934">doi:10.2307/1912934</a>
</p>
<p>Zeileis A (2004).
&ldquo;Econometric Computing with HC and HAC Covariance Matrix Estimator&rdquo;,
<em>Journal of Statistical Software</em>, <b>11</b>(10), 1&ndash;17.
<a href="https://doi.org/10.18637/jss.v011.i10">doi:10.18637/jss.v011.i10</a>
</p>
<p>Zeileis A (2006).
&ldquo;Object-Oriented Computation of Sandwich Estimators&rdquo;,
<em>Journal of Statistical Software</em>, <b>16</b>(9), 1&ndash;16.
<a href="https://doi.org/10.18637/jss.v016.i09">doi:10.18637/jss.v016.i09</a>
</p>
<p>Zeileis A, Köll S, Graham N (2020).
&ldquo;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>95</b>(1), 1&ndash;36.
<a href="https://doi.org/10.18637/jss.v095.i01">doi:10.18637/jss.v095.i01</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovCL">vcovCL</a></code>, <code><a href="#topic+vcovPC">vcovPC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Petersen's data
data("PetersenCL", package = "sandwich")
m &lt;- lm(y ~ x, data = PetersenCL)

## Driscoll and Kraay standard errors
## lag length set to: T - 1 (maximum lag length)
## as proposed by Petersen (2009)
sqrt(diag(vcovPL(m, cluster = ~ firm + year, lag = "max", adjust = FALSE)))

## lag length set to: floor(4 * (T / 100)^(2/9))
## rule of thumb proposed by Hoechle (2007) based on Newey &amp; West (1994)
sqrt(diag(vcovPL(m, cluster = ~ firm + year, lag = "NW1994")))

## lag length set to: floor(T^(1/4))
## rule of thumb based on Newey &amp; West (1987)
sqrt(diag(vcovPL(m, cluster = ~ firm + year, lag = "NW1987")))

## the following specifications of cluster/order.by are equivalent
vcovPL(m, cluster = ~ firm + year)
vcovPL(m, cluster = PetersenCL[, c("firm", "year")])
vcovPL(m, cluster = ~ firm, order.by = ~ year)
vcovPL(m, cluster = PetersenCL$firm, order.by = PetersenCL$year)

## these are also the same when observations within each
## cluster are already ordered
vcovPL(m, cluster = ~ firm)
vcovPL(m, cluster = PetersenCL$firm)
</code></pre>

<hr>
<h2 id='weightsAndrews'>Kernel-based HAC Covariance Matrix Estimation</h2><span id='topic+weightsAndrews'></span><span id='topic+bwAndrews'></span><span id='topic+kernHAC'></span>

<h3>Description</h3>

<p>A set of functions implementing a class of kernel-based heteroscedasticity
and autocorrelation consistent (HAC) covariance matrix estimators
as introduced by Andrews (1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernHAC(x, order.by = NULL, prewhite = 1, bw = bwAndrews,
  kernel = c("Quadratic Spectral", "Truncated", "Bartlett", "Parzen", "Tukey-Hanning"),
  approx = c("AR(1)", "ARMA(1,1)"), adjust = TRUE, diagnostics = FALSE,
  sandwich = TRUE, ar.method = "ols", tol = 1e-7, data = list(), verbose = FALSE, ...)

weightsAndrews(x, order.by = NULL, bw = bwAndrews,
  kernel = c("Quadratic Spectral", "Truncated", "Bartlett", "Parzen", "Tukey-Hanning"),
  prewhite = 1, ar.method = "ols", tol = 1e-7, data = list(), verbose = FALSE, ...)

bwAndrews(x, order.by = NULL, kernel = c("Quadratic Spectral", "Truncated",
  "Bartlett", "Parzen", "Tukey-Hanning"), approx = c("AR(1)", "ARMA(1,1)"),
  weights = NULL, prewhite = 1, ar.method = "ols", data = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightsAndrews_+3A_x">x</code></td>
<td>
<p>a fitted model object. For <code>bwAndrews</code> it can also
be a score matrix (as returned by <code>estfun</code>) directly.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_order.by">order.by</code></td>
<td>
<p>Either a vector <code>z</code> or a formula with a single explanatory
variable like <code>~ z</code>. The observations in the model
are ordered by the size of <code>z</code>. If set to <code>NULL</code> (the
default) the observations are assumed to be ordered (e.g., a
time series).</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_prewhite">prewhite</code></td>
<td>
<p>logical or integer. Should the estimating functions
be prewhitened? If <code>TRUE</code> or greater than 0 a VAR model of
order <code>as.integer(prewhite)</code> is fitted via <code>ar</code> with
method <code>"ols"</code> and <code>demean = FALSE</code>. The default is to
use VAR(1) prewhitening.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_bw">bw</code></td>
<td>
<p>numeric or a function. The bandwidth of the kernel (corresponds to the
truncation lag). If set to to a function (the default is <code>bwAndrews</code>) it is adaptively
chosen.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_kernel">kernel</code></td>
<td>
<p>a character specifying the kernel used. All kernels used
are described in Andrews (1991).</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_approx">approx</code></td>
<td>
<p>a character specifying the approximation method if the
bandwidth <code>bw</code> has to be chosen by <code>bwAndrews</code>.</p>
</td></tr>    
<tr><td><code id="weightsAndrews_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_diagnostics">diagnostics</code></td>
<td>
<p>logical. Should additional model diagnostics be returned?
See <code><a href="#topic+vcovHAC">vcovHAC</a></code> for details.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the middle matrix is returned.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_ar.method">ar.method</code></td>
<td>
<p>character. The <code>method</code> argument passed to
<code><a href="stats.html#topic+ar">ar</a></code> for prewhitening (only, not for bandwidth selection).</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_tol">tol</code></td>
<td>
<p>numeric. Weights that exceed <code>tol</code> are used for computing
the covariance matrix, all other weights are treated as 0.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the <code>order.by</code> 
model. By default the variables are taken from the environment which
the function is called from.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_verbose">verbose</code></td>
<td>
<p>logical. Should the bandwidth parameter used be
printed?</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code>bwAndrews</code>.</p>
</td></tr>
<tr><td><code id="weightsAndrews_+3A_weights">weights</code></td>
<td>
<p>numeric. A vector of weights used for weighting the estimated
coefficients of the approximation model (as specified by <code>approx</code>). By
default all weights are 1 except that for the intercept term (if there is more than
one variable).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kernHAC</code> is a convenience interface to <code><a href="#topic+vcovHAC">vcovHAC</a></code> using 
<code>weightsAndrews</code>: first a weights function is defined and then <code>vcovHAC</code>
is called.
</p>
<p>The kernel weights underlying <code>weightsAndrews</code>
are directly accessible via the function <code><a href="#topic+kweights">kweights</a></code> and require
the specification of the bandwidth parameter <code>bw</code>. If this is not specified
it can be chosen adaptively by the function <code>bwAndrews</code> (except for the
<code>"Truncated"</code> kernel). The automatic bandwidth selection is based on
an approximation of the estimating functions by either AR(1) or ARMA(1,1) processes.
To aggregate the estimated parameters from these approximations a weighted sum
is used. The <code>weights</code> in this aggregation are by default all equal to 1
except that corresponding to the intercept term which is set to 0 (unless there
is no other variable in the model) making the covariance matrix scale invariant.
</p>
<p>Further details can be found in Andrews (1991).
</p>
<p>The estimator of Newey &amp; West (1987) is a special case of the class of estimators
introduced by Andrews (1991). It can be obtained using the <code>"Bartlett"</code>
kernel and setting <code>bw</code> to <code>lag + 1</code>. A convenience interface is 
provided in <code><a href="#topic+NeweyWest">NeweyWest</a></code>.
</p>


<h3>Value</h3>

<p><code>kernHAC</code> returns the same type of object as <code><a href="#topic+vcovHAC">vcovHAC</a></code>
which is typically just the covariance matrix.
</p>
<p><code>weightsAndrews</code> returns a vector of weights.
</p>
<p><code>bwAndrews</code> returns the selected bandwidth parameter.
</p>


<h3>References</h3>

<p>Andrews DWK (1991).
&ldquo;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.&rdquo;
<em>Econometrica</em>, <b>59</b>,
817&ndash;858.
</p>
<p>Newey WK &amp; West KD (1987).
&ldquo;A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.&rdquo;
<em>Econometrica</em>, <b>55</b>,
703&ndash;708.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovHAC">vcovHAC</a></code>, <code><a href="#topic+NeweyWest">NeweyWest</a></code>, <code><a href="#topic+weightsLumley">weightsLumley</a></code>,
<code><a href="#topic+weave">weave</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>curve(kweights(x, kernel = "Quadratic", normalize = TRUE),
      from = 0, to = 3.2, xlab = "x", ylab = "k(x)")
curve(kweights(x, kernel = "Bartlett", normalize = TRUE),
      from = 0, to = 3.2, col = 2, add = TRUE)
curve(kweights(x, kernel = "Parzen", normalize = TRUE),
      from = 0, to = 3.2, col = 3, add = TRUE)
curve(kweights(x, kernel = "Tukey", normalize = TRUE),
      from = 0, to = 3.2, col = 4, add = TRUE)
curve(kweights(x, kernel = "Truncated", normalize = TRUE),
      from = 0, to = 3.2, col = 5, add = TRUE)

## fit investment equation
data(Investment)
fm &lt;- lm(RealInv ~ RealGNP + RealInt, data = Investment)

## compute quadratic spectral kernel HAC estimator
kernHAC(fm)
kernHAC(fm, verbose = TRUE)

## use Parzen kernel instead, VAR(2) prewhitening, no finite sample
## adjustment and Newey &amp; West (1994) bandwidth selection
kernHAC(fm, kernel = "Parzen", prewhite = 2, adjust = FALSE,
  bw = bwNeweyWest, verbose = TRUE)

## compare with estimate under assumption of spheric errors
vcov(fm)
</code></pre>

<hr>
<h2 id='weightsLumley'>Weighted Empirical Adaptive Variance Estimation</h2><span id='topic+weightsLumley'></span><span id='topic+weave'></span>

<h3>Description</h3>

<p>A set of functions implementing weighted empirical adaptive
variance estimation (WEAVE) as introduced by Lumley and Heagerty (1999).
This is implemented as a special case of the general class of
kernel-based heteroscedasticity and autocorrelation consistent (HAC)
covariance matrix estimators as introduced by Andrews (1991), using
a special choice of weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weave(x, order.by = NULL, prewhite = FALSE, C = NULL,
  method = c("truncate", "smooth"), acf = isoacf, adjust = FALSE,
  diagnostics = FALSE, sandwich = TRUE, tol = 1e-7, data = list(), ...)

weightsLumley(x, order.by = NULL, C = NULL,
  method = c("truncate", "smooth"), acf = isoacf, tol = 1e-7, data = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightsLumley_+3A_x">x</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_order.by">order.by</code></td>
<td>
<p>Either a vector <code>z</code> or a formula with a single explanatory
variable like <code>~ z</code>. The observations in the model
are ordered by the size of <code>z</code>. If set to <code>NULL</code> (the
default) the observations are assumed to be ordered (e.g., a
time series).</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_prewhite">prewhite</code></td>
<td>
<p>logical or integer. Should the estimating functions
be prewhitened? If <code>TRUE</code> or greater than 0 a VAR model of
order <code>as.integer(prewhite)</code> is fitted via <code>ar</code> with
method <code>"ols"</code> and <code>demean = FALSE</code>.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_c">C</code></td>
<td>
<p>numeric. The cutoff constant <code>C</code> is by default
4 for method <code>"truncate"</code> and 1 for method <code>"smooth"</code>.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_method">method</code></td>
<td>
<p>a character specifying the method used, see details.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_acf">acf</code></td>
<td>
<p>a function that computes the autocorrelation function of 
a vector, by default <code><a href="#topic+isoacf">isoacf</a></code> is used.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should a finite sample adjustment be made?
This amounts to multiplication with <code class="reqn">n/(n-k)</code> where <code class="reqn">n</code> is the
number of observations and <code class="reqn">k</code> the number of estimated parameters.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_diagnostics">diagnostics</code></td>
<td>
<p>logical. Should additional model diagnostics be returned?
See <code><a href="#topic+vcovHAC">vcovHAC</a></code> for details.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_sandwich">sandwich</code></td>
<td>
<p>logical. Should the sandwich estimator be computed?
If set to <code>FALSE</code> only the middle matrix is returned.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_tol">tol</code></td>
<td>
<p>numeric. Weights that exceed <code>tol</code> are used for computing
the covariance matrix, all other weights are treated as 0.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the <code>order.by</code> 
model. By default the variables are taken from the environment which
the function is called from.</p>
</td></tr>
<tr><td><code id="weightsLumley_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>weave</code> is a convenience interface to <code><a href="#topic+vcovHAC">vcovHAC</a></code> using 
<code>weightsLumley</code>: first a weights function is defined and then <code>vcovHAC</code>
is called.
</p>
<p>Both weighting methods are based on some estimate of the autocorrelation
function <code class="reqn">\rho</code> (as computed by <code>acf</code>) of the residuals of
the model <code>x</code>. The weights for the <code>"truncate"</code> method are 
</p>
<p style="text-align: center;"><code class="reqn">I\{n \rho^2 &gt; C\}</code>
</p>

<p>and the weights for the <code>"smooth"</code> method are
</p>
<p style="text-align: center;"><code class="reqn">\min\{1, C n \rho^2\}</code>
</p>

<p>where n is the number of observations in the model an C is the truncation 
constant <code>C</code>.
</p>
<p>Further details can be found in Lumley &amp; Heagerty (1999).
</p>


<h3>Value</h3>

<p><code>weave</code> returns the same type of object as <code><a href="#topic+vcovHAC">vcovHAC</a></code>
which is typically just the covariance matrix.
</p>
<p><code>weightsLumley</code> returns a vector of weights.
</p>


<h3>References</h3>

<p>Lumley T &amp; Heagerty P (1999).
&ldquo;Weighted Empirical Adaptive Variance Estimators for Correlated Data Regression.&rdquo;
<em>Journal of the Royal Statistical Society B</em>, <b>61</b>,
459&ndash;477.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcovHAC">vcovHAC</a></code>, <code><a href="#topic+weightsAndrews">weightsAndrews</a></code>,
<code><a href="#topic+kernHAC">kernHAC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sin(1:100)
y &lt;- 1 + x + rnorm(100)
fm &lt;- lm(y ~ x)
weave(fm)
vcov(fm)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
