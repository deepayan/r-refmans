<!DOCTYPE html><html lang="en"><head><title>Help for package sparcl</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparcl}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sparcl-package'>
<p>Performs sparse hierarchical and sparse K-means clustering</p></a></li>
<li><a href='#ColorDendrogram'><p>Color the leaves in a hierarchical clustering dendrogram</p></a></li>
<li><a href='#HierarchicalSparseCluster'><p>Hierarchical sparse clustering</p></a></li>
<li><a href='#HierarchicalSparseCluster.permute'><p>Choose tuning parameter for sparse hierarchical clustering</p></a></li>
<li><a href='#HierarchicalSparseCluster.wrapper'><p>A wrapper for the hierarchical sparse clustering algorithm</p></a></li>
<li><a href='#KMeansSparseCluster'><p>Performs sparse k-means clustering</p></a></li>
<li><a href='#KMeansSparseCluster.permute'><p>Choose tuning parameter for sparse k-means clustering</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Perform Sparse Hierarchical Clustering and Sparse K-Means
Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-10-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniela M. Witten and Robert Tibshirani</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniela Witten &lt;dwitten@u.washington.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the sparse clustering methods of Witten and
        Tibshirani (2010): "A framework for feature selection in
        clustering"; published in Journal of the American Statistical
        Association 105(490): 713-726.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-10-22 22:19:08 UTC; robtibshirani</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-10-24 13:50:03 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='sparcl-package'>
Performs sparse hierarchical and sparse K-means clustering
</h2><span id='topic+sparcl-package'></span><span id='topic+sparcl'></span>

<h3>Description</h3>

<p>Implements the sparse clustering methods of Witten and Tibshirani (2010)
&quot;A framework for feature selection in clustering&quot;, Journal
Amer. Stat. Assocn. 105(490): 713-726.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> sparcl</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2013-1-02</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 </td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The main functions are KMeansSparseCluster and
HierarchicalSparseCluster. Tuning parameters for these methods are
chosen by KMeansSparseCluster.permute and HierarchicalSparseCluster.permute.
</p>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani
</p>
<p>Maintainer: Daniela Witten &lt;dwitten@u.washington.edu&gt;
</p>


<h3>References</h3>

<p>Witten and Tibshirani (2010) A framework for feature selection in
clustering. Journal
Amer. Stat. Assocn. 105(490): 713-726.
</p>

<hr>
<h2 id='ColorDendrogram'>Color the leaves in a hierarchical clustering dendrogram</h2><span id='topic+ColorDendrogram'></span>

<h3>Description</h3>

<p>Pass in the output of &quot;hclust&quot; and a class label for each
observation. A colored dendrogram will result, with the leaf colors indicating
the classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ColorDendrogram(hc, y, main = "", branchlength = 0.7, labels = NULL, xlab = NULL, 
sub="NULL",ylab = "", cex.main = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ColorDendrogram_+3A_hc">hc</code></td>
<td>
<p>The output of running &quot;hclust&quot; on a nxn dissimilarity matrix</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_y">y</code></td>
<td>
<p>A vector of n class labels for the observations that were clustered
using &quot;hclust&quot;. If labels are numeric from 1 to K, then colors will
be determine automatically. Otherwise the labels can take the form
of colors (e.g. c(&quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;orange&quot;)).</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_main">main</code></td>
<td>
<p>The main title for the dendrogram.</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_branchlength">branchlength</code></td>
<td>
<p>How long to make the colored part of the
branches. Adjustment will be needed for each dissimilarity matrix</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_labels">labels</code></td>
<td>
<p>The labels for the n observations.</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_xlab">xlab</code></td>
<td>
<p>X-axis label.</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_sub">sub</code></td>
<td>
<p>Sub-x-axis label.</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_ylab">ylab</code></td>
<td>
<p>Y-axis label.</p>
</td></tr>
<tr><td><code id="ColorDendrogram_+3A_cex.main">cex.main</code></td>
<td>
<p>The amount by which to enlarge the main title for the figure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p>HierarchicalSparseCluster, HierarchicalSparseCluster.permute</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate 2-class data
set.seed(1)
x &lt;- matrix(rnorm(100*20),ncol=20)
y &lt;- c(rep(1,50),rep(2,50))
x[y==1,] &lt;- x[y==1,]+2
# Perform hierarchical clustering
hc &lt;- hclust(dist(x),method="complete")
# Plot
ColorDendrogram(hc,y=y,main="My Simulated Data",branchlength=3)
</code></pre>

<hr>
<h2 id='HierarchicalSparseCluster'>Hierarchical sparse clustering</h2><span id='topic+HierarchicalSparseCluster'></span><span id='topic+print.HierarchicalSparseCluster'></span><span id='topic+plot.HierarchicalSparseCluster'></span>

<h3>Description</h3>

<p>Performs sparse hierarchical  clustering. If $d_ii'j$ is the
dissimilarity between observations i and i' for feature j, seek a sparse
weight
vector w and then use $(sum_j (d_ii'j w_j))_ii'$ as a nxn dissimilarity
matrix for hierarchical clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalSparseCluster(x=NULL, dists=NULL,
method=c("average","complete", "single","centroid"),
wbound=NULL,niter=15,dissimilarity=c("squared.distance","absolute.value"),
 uorth=NULL,
silent=FALSE,cluster.features=FALSE,method.features=c("average", "complete",
"single","centroid"),output.cluster.files=FALSE,
outputfile.prefix="output",genenames=NULL,genedesc=NULL,standardize.arrays=FALSE)
## S3 method for class 'HierarchicalSparseCluster'
print(x,...)
## S3 method for class 'HierarchicalSparseCluster'
plot(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HierarchicalSparseCluster_+3A_x">x</code></td>
<td>
<p>A nxp data matrix; n is the number of observations and p the
number of features. If NULL, then specify dists instead.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_dists">dists</code></td>
<td>
<p>For advanced users, can be entered instead of x. If
HierarchicalSparseCluster has already been run on this data, then
the dists value of the previous output can be entered here. 
Under normal circumstances, leave this argument NULL and
pass in x instead.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_method">method</code></td>
<td>
<p>The type of linkage to use in the hierarchical
clustering - &quot;single&quot;, &quot;complete&quot;, &quot;centroid&quot;, or &quot;average&quot;.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_wbound">wbound</code></td>
<td>
<p>The L1 bound on w to use; this is the tuning parameter
for sparse hierarchical clustering. Should be greater than 1.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_niter">niter</code></td>
<td>
<p>The number of iterations to perform in the sparse
hierarchical clustering algorithm.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_dissimilarity">dissimilarity</code></td>
<td>
<p>The type of dissimilarity measure to use. One of
&quot;squared.distance&quot; or &quot;absolute.value&quot;. Only use this if x was
passed in (rather than dists).</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_uorth">uorth</code></td>
<td>
<p>If complementary sparse clustering is desired, then this
is the nxn dissimilarity matrix obtained in the original sparse
clustering.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_standardize.arrays">standardize.arrays</code></td>
<td>
<p>Should the arrays be standardized? Default
is FALSE.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_silent">silent</code></td>
<td>
<p>Print out progress?</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_cluster.features">cluster.features</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_method.features">method.features</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_output.cluster.files">output.cluster.files</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_outputfile.prefix">outputfile.prefix</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_genenames">genenames</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_genedesc">genedesc</code></td>
<td>
<p>Not for use.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster_+3A_...">...</code></td>
<td>
<p> not used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>We seek a p-vector of weights w (one per feature) and a nxn matrix U
that optimize
</p>
<p>$maximize_U,w sum_j w_j sum_ii' d_ii'j U_ii'$ subject to $||w||_2 &lt;= 1,
||w||_1 &lt;= wbound, w_j &gt;= 0, sum_ii' U_ii'^2 &lt;= 1$.
</p>
<p>Here, $d_ii'j$ is the dissimilarity between observations i and i' with
along feature j. The resulting matrix U is used as a dissimilarity
matrix for hierarchical clustering. &quot;wbound&quot; is a tuning parameter for
this method, which controls the L1 bound on w, and as a result the
number of features with non-zero $w_j$ weights.
The non-zero elements of w indicate features that are used in the
sparse clustering.
</p>
<p>We optimize the above criterion with an iterative approach: hold U
fixed and optimize with respect to w. Then, hold w fixed and optimize
with respect to U.
</p>
<p>Note that the arguments described as &quot;Not for use&quot; are included for
the sparcl package to function with GenePattern but should be ignored
by the R user.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>hc</code></td>
<td>
<p>The output of a call to &quot;hclust&quot;, giving the results of
hierarchical sparse clustering.</p>
</td></tr>
<tr><td><code>ws</code></td>
<td>
<p>The p-vector of feature weights.</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The nxn dissimilarity matrix passed into hclust, of the form
$(sum_j w_j d_ii'j)_ii'$.</p>
</td></tr>
<tr><td><code>dists</code></td>
<td>
<p>The (n*n)xp dissimilarity matrix for the data matrix
x. This is useful if additional calls to HierarchicalSparseCluster
will be made.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p><a href="#topic+HierarchicalSparseCluster.permute">HierarchicalSparseCluster.permute</a>,<a href="#topic+KMeansSparseCluster">KMeansSparseCluster</a>,<a href="#topic+KMeansSparseCluster.permute">KMeansSparseCluster.permute</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Generate 2-class data
  set.seed(1)
  x &lt;- matrix(rnorm(100*50),ncol=50)
  y &lt;- c(rep(1,50),rep(2,50))
  x[y==1,1:25] &lt;- x[y==1,1:25]+2
  # Do tuning parameter selection for sparse hierarchical clustering
  perm.out &lt;- HierarchicalSparseCluster.permute(x, wbounds=c(1.5,2:6),
nperms=5)
  print(perm.out)
  plot(perm.out)
  # Perform sparse hierarchical clustering
  sparsehc &lt;- HierarchicalSparseCluster(dists=perm.out$dists,
wbound=perm.out$bestw, method="complete")
  # faster than   sparsehc &lt;- HierarchicalSparseCluster(x=x,wbound=perm.out$bestw, 
#  method="complete")
  par(mfrow=c(1,2))
  plot(sparsehc)
  plot(sparsehc$hc, labels=rep("", length(y)))
  print(sparsehc)
  # Plot using knowledge of class labels in order to compare true class
  #   labels to clustering obtained
  par(mfrow=c(1,1))
  ColorDendrogram(sparsehc$hc,y=y,main="My Simulated Data",branchlength=.007)
  # Now, what if we want to see if out data contains a *secondary*
  #   clustering after accounting for the first one obtained. We
  #   look for a complementary sparse clustering:
  sparsehc.comp &lt;- HierarchicalSparseCluster(x,wbound=perm.out$bestw,
     method="complete",uorth=sparsehc$u)
  # Redo the analysis, but this time use "absolute value" dissimilarity:
  perm.out &lt;- HierarchicalSparseCluster.permute(x, wbounds=c(1.5,2:6),
    nperms=5, dissimilarity="absolute.value")
  print(perm.out)
  plot(perm.out)
  # Perform sparse hierarchical clustering
  sparsehc &lt;- HierarchicalSparseCluster(dists=perm.out$dists, wbound=perm.out$bestw, 
method="complete",
 dissimilarity="absolute.value")
  par(mfrow=c(1,2))
  plot(sparsehc)
</code></pre>

<hr>
<h2 id='HierarchicalSparseCluster.permute'>Choose tuning parameter for sparse hierarchical clustering</h2><span id='topic+HierarchicalSparseCluster.permute'></span><span id='topic+print.HierarchicalSparseCluster.permute'></span><span id='topic+plot.HierarchicalSparseCluster.permute'></span>

<h3>Description</h3>

<p>The tuning parameter controls the L1 bound on w, the feature weights. A permutation approach is used to select the tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalSparseCluster.permute(x, nperms = 10, wbounds = NULL,
dissimilarity=c("squared.distance",
"absolute.value"),standardize.arrays=FALSE)
## S3 method for class 'HierarchicalSparseCluster.permute'
plot(x,...) 
## S3 method for class 'HierarchicalSparseCluster.permute'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_x">x</code></td>
<td>
<p>A nxp data matrix, with n observations and p feaures.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_nperms">nperms</code></td>
<td>
<p>The number of permutations to perform.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_wbounds">wbounds</code></td>
<td>
<p>The sequence of tuning parameters to consider. The
tuning parameters are the L1 bound on w, the feature weights. If
NULL, then a default sequence will be used. If non-null, should be
greater than 1.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_dissimilarity">dissimilarity</code></td>
<td>
<p>How should dissimilarity be computed? Default is
squared.distance.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_standardize.arrays">standardize.arrays</code></td>
<td>
<p>Should the arrays first be standardized?
Default is FALSE.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.permute_+3A_...">...</code></td>
<td>
<p> not used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let $d_ii'j$ denote the dissimilarity between observations i and i'
along feature j.
</p>
<p>Sparse hierarchical clustering seeks a p-vector of weights w (one per
feature) and a nxn  matrix U that optimize
$maximize_U,w sum_j w_j  sum_ii' d_ii'j U_ii'$ subject to $||w||_2 &lt;= 1,
||w||_1 &lt;= s, w_j &gt;= 0, sum_ii' U_ii'^2 &lt;= 1$,
where  s is a value for the L1 bound on w. Let O(s) denote the
objective function with tuning parameter s: i.e. $O(s)=sum_j w_j
sum_ii' d_ii'j U_ii'$.
</p>
<p>We permute the data as follows: within each feature, we permute the
observations. Using the permuted data, we can run sparse hierarchical clustering
with tuning parameter s, yielding the objective function O*(s). If we do
this repeatedly we can get a number of O*(s) values.
</p>
<p>Then, the Gap statistic is given by
$Gap(s)=log(O(s))-mean(log(O*(s)))$. The
optimal s is that which results in the highest Gap
statistic. Or, we
can choose the smallest s such that its Gap statistic is within
$sd(log(O*(s)))$ of the largest Gap statistic.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>gaps</code></td>
<td>
<p>The gap statistics obtained (one for each of the tuning
parameters tried). If O(s) is the objective function evaluated at
the tuning parameter s, and O*(s) is the same quantity but for the
permuted data, then Gap(s)=log(O(s))-mean(log(O*(s))).</p>
</td></tr>
<tr><td><code>sdgaps</code></td>
<td>
<p>The standard deviation of log(O*(s)), for each value of the
tuning parameter s.</p>
</td></tr>
<tr><td><code>nnonzerows</code></td>
<td>
<p>The number of features with non-zero weights, for
each value of the tuning parameter.</p>
</td></tr>
<tr><td><code>wbounds</code></td>
<td>
<p>The tuning parameters considered.</p>
</td></tr>
<tr><td><code>bestw</code></td>
<td>
<p>The value of the tuning parameter corresponding to the
highest gap statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p><a href="#topic+HierarchicalSparseCluster">HierarchicalSparseCluster</a>, <a href="#topic+KMeansSparseCluster">KMeansSparseCluster</a>, <a href="#topic+KMeansSparseCluster.permute">KMeansSparseCluster.permute</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Generate 2-class data
  set.seed(1)
  x &lt;- matrix(rnorm(100*50),ncol=50)
  y &lt;- c(rep(1,50),rep(2,50))
  x[y==1,1:25] &lt;- x[y==1,1:25]+2
  # Do tuning parameter selection for sparse hierarchical clustering
  perm.out &lt;- HierarchicalSparseCluster.permute(x, wbounds=c(1.5,2:6),
nperms=5)
  print(perm.out)
  plot(perm.out)
  # Perform sparse hierarchical clustering
  sparsehc &lt;- HierarchicalSparseCluster(dists=perm.out$dists, wbound=perm.out$bestw, 
method="complete")
  par(mfrow=c(1,2))
  plot(sparsehc)
  plot(sparsehc$hc, labels=rep("", length(y)))
  print(sparsehc)
  # Plot using knowledge of class labels in order to compare true class
  #   labels to clustering obtained
  par(mfrow=c(1,1))
  ColorDendrogram(sparsehc$hc,y=y,main="My Simulated
Data",branchlength=.007)
</code></pre>

<hr>
<h2 id='HierarchicalSparseCluster.wrapper'>A wrapper for the hierarchical sparse clustering algorithm</h2><span id='topic+HierarchicalSparseCluster.wrapper'></span>

<h3>Description</h3>

<p>A wrapper for HierarchicalSparseCluster which reads in the data in  GCT file
format, and then  automatically chooses the
optimal tuning parameter value using HierarchicalSparseCluster.permute
if not specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalSparseCluster.wrapper(file,  method=c("average", "complete", "single",
 "centroid"),
wbound=NULL, silent=FALSE, cluster.features=FALSE,
method.features=c("average", "complete",
"single","centroid"),output.cluster.files=TRUE,outputfile.prefix=NULL,maxnumgenes=5000,
standardize.arrays=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_file">file</code></td>
<td>
<p>A GCT filename in the working directory containing the
data to be clustered.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_method">method</code></td>
<td>
<p>The type of linkage to use in the hierarchical
clustering - &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, or &quot;centroid&quot;.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_wbound">wbound</code></td>
<td>
<p>The L1 bound on w to use; this is the tuning parameter
for sparse hierarchical clustering. If NULL, then it will be
chosen via HierarchicalSparseCluster.permute.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_silent">silent</code></td>
<td>
<p>Print out progress?</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_cluster.features">cluster.features</code></td>
<td>
<p>Is a clustering for the features with non-zero
weights also desired? Default is FALSE.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_method.features">method.features</code></td>
<td>
<p>If cluster.features is TRUE, then the type of
linkage used to cluster the features with non-zero weights: one of
&quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, or &quot;centroid&quot;.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_output.cluster.files">output.cluster.files</code></td>
<td>
<p>Should files containing the clustering be output? Default is
TRUE.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_outputfile.prefix">outputfile.prefix</code></td>
<td>
<p>The prefix for the output files. If NULL,
then the prefix of the input file is used.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_maxnumgenes">maxnumgenes</code></td>
<td>
<p>Limit the analysis to some number of genes with highest marginal
variance, for computational reasons. This is recommended when the number of genes is very
large. If NULL, then all genes are used.</p>
</td></tr>
<tr><td><code id="HierarchicalSparseCluster.wrapper_+3A_standardize.arrays">standardize.arrays</code></td>
<td>
<p>Should the arrays first be standardized?
Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>hc</code></td>
<td>
<p>The output of a call to &quot;hclust&quot;, giving the results of
hierarchical sparse clustering.</p>
</td></tr>
<tr><td><code>ws</code></td>
<td>
<p>The p-vector of feature weights.</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The nxn dissimilarity matrix passed into hclust, of the form
$(sum_j w_j d_ii'j)_ii'$.</p>
</td></tr>
<tr><td><code>dists</code></td>
<td>
<p>The (n*n)xp dissimilarity matrix for the data matrix
x. This is useful if additional calls to HierarchicalSparseCluster
will be made.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p><a href="#topic+HierarchicalSparseCluster.permute">HierarchicalSparseCluster.permute</a>,<a href="#topic+KMeansSparseCluster">KMeansSparseCluster</a>,<a href="#topic+KMeansSparseCluster.permute">KMeansSparseCluster.permute</a></p>

<hr>
<h2 id='KMeansSparseCluster'>Performs sparse k-means clustering</h2><span id='topic+KMeansSparseCluster'></span><span id='topic+print.KMeansSparseCluster'></span><span id='topic+plot.KMeansSparseCluster'></span>

<h3>Description</h3>

<p>This function performs sparse k-means clustering. You must specify a number of clusters K and an L1 bound on w, the feature weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMeansSparseCluster(x, K=NULL, wbounds = NULL, nstart = 20, silent =
FALSE, maxiter=6, centers=NULL)
## S3 method for class 'KMeansSparseCluster'
plot(x,...)
## S3 method for class 'KMeansSparseCluster'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KMeansSparseCluster_+3A_x">x</code></td>
<td>
<p>An nxp data matrix. There are n observations and p features.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_k">K</code></td>
<td>
<p>The number of clusters desired (&quot;K&quot; in K-means
clustering). Must provide either K or centers.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_wbounds">wbounds</code></td>
<td>
<p>A single L1 bound on w (the feature weights), or a vector of L1 bounds on w. If wbound is small,
then few features will have non-zero weights. If wbound is large then all
features will have non-zero weights. Should be greater than 1.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_nstart">nstart</code></td>
<td>
<p>The number of random starts for the k-means
algorithm.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_silent">silent</code></td>
<td>
<p>Print out progress?</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_centers">centers</code></td>
<td>
<p>Optional argument. If you want to run the k-means
algorithm starting from a particular set of clusters, then you can
enter the Kxp matrix of cluster centers here. Default use case
involves taking centers=NULL and instead specifying K.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster_+3A_...">...</code></td>
<td>
<p> not used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>We seek a p-vector of weights w (one per feature) and a set of clusters
C1,...,CK that optimize
</p>
<p>$maximize_C1,...,CK,w sum_j w_j BCSS_j$ subject to $||w||_2 &lt;= 1,
||w||_1 &lt;= wbound, w_j &gt;= 0$
</p>
<p>where $BCSS_j$ is the between cluster sum of squares for feature j. An
iterative approach is taken: with w fixed, optimize with respect to
C1,...,CK, and with C1,...,CK fixed, optimize with respect to w. Here,
wbound is a tuning parameter which determines the L1 bound on w.
</p>
<p>The non-zero elements of w indicate features that are used in the sparse clustering.
</p>


<h3>Value</h3>

<p>If wbounds is a vector, then a list with elements as follows (one per
element of wbounds). If
wbounds is just a single value, then elements as follows:
</p>
<table role = "presentation">
<tr><td><code>ws</code></td>
<td>
<p>The p-vector of feature weights.</p>
</td></tr>
<tr><td><code>Cs</code></td>
<td>
<p>The clustering obtained.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p><a href="#topic+KMeansSparseCluster.permute">KMeansSparseCluster.permute</a>,<a href="#topic+HierarchicalSparseCluster">HierarchicalSparseCluster</a></p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data
set.seed(11)
x &lt;- matrix(rnorm(50*70),ncol=70)
x[1:25,1:20] &lt;- x[1:25,1:20]+1
x &lt;- scale(x, TRUE, TRUE)
# choose tuning parameter
km.perm &lt;- KMeansSparseCluster.permute(x,K=2,wbounds=seq(3,7,len=15),nperms=5)
print(km.perm)
plot(km.perm)
# run sparse k-means
km.out &lt;- KMeansSparseCluster(x,K=2,wbounds=km.perm$bestw)
print(km.out)
plot(km.out)
# run sparse k-means for a range of tuning parameter values
km.out &lt;- KMeansSparseCluster(x,K=2,wbounds=seq(1.3,4,len=8))
print(km.out)
plot(km.out)
# Run sparse k-means starting from a particular set of cluster centers
#in the k-means algorithm.
km.out &lt;- KMeansSparseCluster(x,wbounds=2:7,centers=x[c(1,3,5),])
</code></pre>

<hr>
<h2 id='KMeansSparseCluster.permute'>Choose tuning parameter for sparse k-means clustering</h2><span id='topic+KMeansSparseCluster.permute'></span><span id='topic+print.KMeansSparseCluster.permute'></span><span id='topic+plot.KMeansSparseCluster.permute'></span>

<h3>Description</h3>

<p>The tuning parameter controls the L1 bound on w, the feature weights. A
permutation approach is used to select the tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMeansSparseCluster.permute(x, K=NULL, nperms = 25, wbounds = NULL,
silent = FALSE, nvals = 10, centers=NULL)
## S3 method for class 'KMeansSparseCluster.permute'
print(x,...)
## S3 method for class 'KMeansSparseCluster.permute'
plot(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KMeansSparseCluster.permute_+3A_x">x</code></td>
<td>
<p>The nxp data matrix, n is the number of observations and p
the number of features.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_k">K</code></td>
<td>
<p>The number of clusters desired - that is, the &quot;K&quot; in K-means
clustering. Must specify K or centers.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_nperms">nperms</code></td>
<td>
<p>Number of permutations.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_wbounds">wbounds</code></td>
<td>
<p>The range of tuning parameters to consider. This is the
L1 bound on w, the feature weights. If NULL, then a range of values
will be chosen automatically. Should be greater than 1 if non-null.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_silent">silent</code></td>
<td>
<p>Print out progress?</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_nvals">nvals</code></td>
<td>
<p>If wbounds is NULL, then the number of candidate tuning
parameter values to consider.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_centers">centers</code></td>
<td>
<p>Optional argument. If you want to run the k-means
algorithm starting from a particular set of clusters, then you
can enter the Kxp matrix of cluster centers here. Default use
case involves taking centers=NULL and instead specifying K.</p>
</td></tr>
<tr><td><code id="KMeansSparseCluster.permute_+3A_...">...</code></td>
<td>
<p> not used. </p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Sparse k-means clustering seeks a p-vector of weights w (one per feature) and a set of
clusters  C1,...,CK that optimize
$maximize_C1,...,CK,w sum_j w_j BCSS_j$ subject to $||w||_2 &lt;= 1,
||w||_1 &lt;= s, w_j &gt;= 0$,
where $BCSS_j$ is the between cluster sum of squares for feature
j, and s is a value for the L1 bound on w. Let O(s) denote the
objective function with tuning parameter s: i.e. $O(s)=sum_j w_j
BCSS_j$.
</p>
<p>We permute the data as follows: within each feature, we permute the
observations. Using the permuted data, we can run sparse K-means with
tuning parameter s, yielding the objective function O*(s). If we do
this repeatedly we can get a number of O*(s) values.
</p>
<p>Then, the Gap statistic is given by $Gap(s)=log(O(s))-mean(log(O*(s)))$. The
optimal s is that which results in the highest Gap statistic. Or, we
can choose the smallest s such that its Gap statistic is within
$sd(log(O*(s)))$ of the largest Gap statistic.
</p>


<h3>Value</h3>

 
<table role = "presentation">
<tr><td><code>gaps</code></td>
<td>
<p>The gap statistics obtained (one for each of the tuning
parameters tried). If O(s) is the objective function evaluated at
the tuning parameter s, and O*(s) is the same quantity but for the
permuted data, then Gap(s)=log(O(s))-mean(log(O*(s))).</p>
</td></tr>
<tr><td><code>sdgaps</code></td>
<td>
<p>The standard deviation of log(O*(s)), for each value of the
tuning parameter s.</p>
</td></tr>
<tr><td><code>nnonzerows</code></td>
<td>
<p>The number of features with non-zero weights, for
each value of the tuning parameter.</p>
</td></tr>
<tr><td><code>wbounds</code></td>
<td>
<p>The tuning parameters considered.</p>
</td></tr>
<tr><td><code>bestw</code></td>
<td>
<p>The value of the tuning parameter corresponding to the
highest gap statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p><a href="#topic+KMeansSparseCluster">KMeansSparseCluster</a>, <a href="#topic+HierarchicalSparseCluster">HierarchicalSparseCluster</a>, <a href="#topic+HierarchicalSparseCluster.permute">HierarchicalSparseCluster.permute</a></p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data
set.seed(11)
x &lt;- matrix(rnorm(50*70),ncol=70)
x[1:25,1:10] &lt;- x[1:25,1:10]+1.5
x &lt;- scale(x, TRUE, TRUE)
# choose tuning parameter
km.perm &lt;-
KMeansSparseCluster.permute(x,K=2,wbounds=seq(2,5,len=8),nperms=3)
print(km.perm)
plot(km.perm)
# run sparse k-means
km.out &lt;- KMeansSparseCluster(x,K=2,wbounds=km.perm$bestw)
print(km.out)
plot(km.out)
# run sparse k-means for a range of tuning parameter values
km.out &lt;- KMeansSparseCluster(x,K=2,wbounds=2:7)
print(km.out)
plot(km.out)
# Repeat, but this time start with a particular choice of cluster
# centers.
# This will do 4-means clustering starting with this particular choice
# of cluster centers.
km.perm.out &lt;- KMeansSparseCluster.permute(x,wbounds=2:6, centers=x[1:4,],nperms=3)
print(km.out)
plot(km.out)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
