<!DOCTYPE html><html lang="en"><head><title>Help for package ollamar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ollamar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#append_message'><p>Append message to a list</p></a></li>
<li><a href='#chat'><p>Generate a chat completion with message history</p></a></li>
<li><a href='#check_option_valid'><p>Check if an option is valid</p></a></li>
<li><a href='#check_options'><p>Check if a vector of options are valid</p></a></li>
<li><a href='#copy'><p>Copy a model</p></a></li>
<li><a href='#create'><p>Create a model from a Modelfile</p></a></li>
<li><a href='#create_message'><p>Create a message</p></a></li>
<li><a href='#create_messages'><p>Create a list of messages</p></a></li>
<li><a href='#create_request'><p>Create a httr2 request object</p></a></li>
<li><a href='#delete'><p>Delete a model and its data</p></a></li>
<li><a href='#delete_message'><p>Delete a message in a specified position from a list</p></a></li>
<li><a href='#embed'><p>Generate embedding for inputs</p></a></li>
<li><a href='#embeddings'><p>Generate embeddings for a single prompt - deprecated in favor of <code>embed()</code></p></a></li>
<li><a href='#encode_images_in_messages'><p>Encode images in messages to base64 format</p></a></li>
<li><a href='#generate'><p>Generate a response for a given prompt</p></a></li>
<li><a href='#get_tool_calls'><p>Get tool calls helper function</p></a></li>
<li><a href='#image_encode_base64'><p>Read image file and encode it to base64</p></a></li>
<li><a href='#insert_message'><p>Insert message into a list at a specified position</p></a></li>
<li><a href='#list_models'><p>List models that are available locally</p></a></li>
<li><a href='#model_avail'><p>Check if model is available locally</p></a></li>
<li><a href='#model_options'><p>Model options</p></a></li>
<li><a href='#ohelp'><p>Chat with a model in real-time in R console</p></a></li>
<li><a href='#package_config'><p>Package configuration</p></a></li>
<li><a href='#prepend_message'><p>Prepend message to a list</p></a></li>
<li><a href='#ps'><p>List models that are currently loaded into memory</p></a></li>
<li><a href='#pull'><p>Pull/download a model from the Ollama library</p></a></li>
<li><a href='#push'><p>Push or upload a model to a model library</p></a></li>
<li><a href='#resp_process'><p>Process httr2 response object</p></a></li>
<li><a href='#resp_process_stream'><p>Process httr2 response object for streaming</p></a></li>
<li><a href='#search_options'><p>Search for options based on a query</p></a></li>
<li><a href='#show'><p>Show model information</p></a></li>
<li><a href='#stream_handler'><p>Stream handler helper function</p></a></li>
<li><a href='#test_connection'><p>Test connection to Ollama server</p></a></li>
<li><a href='#validate_message'><p>Validate a message</p></a></li>
<li><a href='#validate_messages'><p>Validate a list of messages</p></a></li>
<li><a href='#validate_options'><p>Validate additional options or parameters provided to the API call</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>'Ollama' Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Description:</td>
<td>An interface to easily run local language models with 'Ollama' <a href="https://ollama.com">https://ollama.com</a> server
    and API endpoints (see <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">https://github.com/ollama/ollama/blob/main/docs/api.md</a> for details). It lets 
    you run open-source large language models locally on your machine. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>base64enc, crayon, glue, httr2, jsonlite, tibble</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/hauselin/ollama-r/issues">https://github.com/hauselin/ollama-r/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://hauselin.github.io/ollama-r/">https://hauselin.github.io/ollama-r/</a>,
<a href="https://github.com/hauselin/ollama-r">https://github.com/hauselin/ollama-r</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-08 05:08:27 UTC; hause</td>
</tr>
<tr>
<td>Author:</td>
<td>Hause Lin <a href="https://orcid.org/0000-0003-4590-7039"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  Tawab Safi <a href="https://orcid.org/0009-0000-5659-9890"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hause Lin &lt;hauselin@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-08 05:40:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='append_message'>Append message to a list</h2><span id='topic+append_message'></span>

<h3>Description</h3>

<p>Appends a message (add to end of a list) to a list of messages. The role and content will be converted to a list and appended to the input list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>append_message(content, role = "user", x = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="append_message_+3A_content">content</code></td>
<td>
<p>The content of the message.</p>
</td></tr>
<tr><td><code id="append_message_+3A_role">role</code></td>
<td>
<p>The role of the message. Can be &quot;user&quot;, &quot;system&quot;, &quot;assistant&quot;. Default is &quot;user&quot;.</p>
</td></tr>
<tr><td><code id="append_message_+3A_x">x</code></td>
<td>
<p>A list of messages. Default is NULL.</p>
</td></tr>
<tr><td><code id="append_message_+3A_...">...</code></td>
<td>
<p>Additional arguments such as images.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages with the new message appended.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>append_message("user", "Hello")
append_message("system", "Always respond nicely")
</code></pre>

<hr>
<h2 id='chat'>Generate a chat completion with message history</h2><span id='topic+chat'></span>

<h3>Description</h3>

<p>Generate a chat completion with message history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat(
  model,
  messages,
  tools = list(),
  stream = FALSE,
  format = list(),
  keep_alive = "5m",
  output = c("resp", "jsonlist", "raw", "df", "text", "req", "tools", "structured"),
  endpoint = "/api/chat",
  host = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_messages">messages</code></td>
<td>
<p>A list with list of messages for the model (see examples below).</p>
</td></tr>
<tr><td><code id="chat_+3A_tools">tools</code></td>
<td>
<p>Tools for the model to use if supported. Requires stream = FALSE. Default is an empty list.</p>
</td></tr>
<tr><td><code id="chat_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="chat_+3A_format">format</code></td>
<td>
<p>Format to return a response in. Format can be json/list (structured response).</p>
</td></tr>
<tr><td><code id="chat_+3A_keep_alive">keep_alive</code></td>
<td>
<p>The duration to keep the connection alive. Default is &quot;5m&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;resp&quot;. Other options are &quot;jsonlist&quot;, &quot;raw&quot;, &quot;df&quot;, &quot;text&quot;, &quot;req&quot; (httr2_request object), &quot;tools&quot; (tool calling), &quot;structured&quot; (structured output)</p>
</td></tr>
<tr><td><code id="chat_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to chat with the model. Default is &quot;/api/chat&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
<tr><td><code id="chat_+3A_...">...</code></td>
<td>
<p>Additional options to pass to the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one message
messages &lt;- list(
    list(role = "user", content = "How are you doing?")
)
chat("llama3", messages) # returns response by default
chat("llama3", messages, output = "text") # returns text/vector
chat("llama3", messages, temperature = 2.8) # additional options
chat("llama3", messages, stream = TRUE) # stream response
chat("llama3", messages, output = "df", stream = TRUE) # stream and return dataframe

# multiple messages
messages &lt;- list(
    list(role = "user", content = "Hello!"),
    list(role = "assistant", content = "Hi! How are you?"),
    list(role = "user", content = "Who is the prime minister of the uk?"),
    list(role = "assistant", content = "Rishi Sunak"),
    list(role = "user", content = "List all the previous messages.")
)
chat("llama3", messages, stream = TRUE)

# image
image_path &lt;- file.path(system.file("extdata", package = "ollamar"), "image1.png")
messages &lt;- list(
   list(role = "user", content = "What is in the image?", images = image_path)
)
chat("benzie/llava-phi-3", messages, output = 'text')

</code></pre>

<hr>
<h2 id='check_option_valid'>Check if an option is valid</h2><span id='topic+check_option_valid'></span>

<h3>Description</h3>

<p>Check if an option is valid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_option_valid(opt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_option_valid_+3A_opt">opt</code></td>
<td>
<p>An option (character) to check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns TRUE if the option is valid, FALSE otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>check_option_valid("mirostat")
check_option_valid("invalid_option")
</code></pre>

<hr>
<h2 id='check_options'>Check if a vector of options are valid</h2><span id='topic+check_options'></span>

<h3>Description</h3>

<p>Check if a vector of options are valid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_options(opts = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_options_+3A_opts">opts</code></td>
<td>
<p>A vector of options to check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with two elements: valid_options and invalid_options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>check_options(c("mirostat", "invalid_option"))
check_options(c("mirostat", "num_predict"))
</code></pre>

<hr>
<h2 id='copy'>Copy a model</h2><span id='topic+copy'></span>

<h3>Description</h3>

<p>Creates a model with another name from an existing model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>copy(source, destination, endpoint = "/api/copy", host = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="copy_+3A_source">source</code></td>
<td>
<p>The name of the model to copy.</p>
</td></tr>
<tr><td><code id="copy_+3A_destination">destination</code></td>
<td>
<p>The name for the new model.</p>
</td></tr>
<tr><td><code id="copy_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to copy the model. Default is &quot;/api/copy&quot;.</p>
</td></tr>
<tr><td><code id="copy_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#copy-a-model">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
copy("llama3", "llama3_copy")
delete("llama3_copy")  # delete the model was just got copied

</code></pre>

<hr>
<h2 id='create'>Create a model from a Modelfile</h2><span id='topic+create'></span>

<h3>Description</h3>

<p>It is recommended to set <code>modelfile</code> to the content of the Modelfile rather than just set path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create(
  name,
  modelfile = NULL,
  stream = FALSE,
  path = NULL,
  endpoint = "/api/create",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_+3A_name">name</code></td>
<td>
<p>Name of the model to create.</p>
</td></tr>
<tr><td><code id="create_+3A_modelfile">modelfile</code></td>
<td>
<p>Contents of the Modelfile as character string. Default is NULL.</p>
</td></tr>
<tr><td><code id="create_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="create_+3A_path">path</code></td>
<td>
<p>The path to the Modelfile. Default is NULL.</p>
</td></tr>
<tr><td><code id="create_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to create the model. Default is &quot;/api/create&quot;.</p>
</td></tr>
<tr><td><code id="create_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
create("mario", "FROM llama3\nSYSTEM You are mario from Super Mario Bros.")
generate("mario", "who are you?", output = "text")  # model should say it's Mario
delete("mario")  # delete the model created above

</code></pre>

<hr>
<h2 id='create_message'>Create a message</h2><span id='topic+create_message'></span>

<h3>Description</h3>

<p>Create a message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_message(content, role = "user", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_message_+3A_content">content</code></td>
<td>
<p>The content of the message.</p>
</td></tr>
<tr><td><code id="create_message_+3A_role">role</code></td>
<td>
<p>The role of the message. Can be &quot;user&quot;, &quot;system&quot;, &quot;assistant&quot;. Default is &quot;user&quot;.</p>
</td></tr>
<tr><td><code id="create_message_+3A_...">...</code></td>
<td>
<p>Additional arguments such as images.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_message("Hello", "user")
create_message("Always respond nicely", "system")
create_message("I am here to help", "assistant")
</code></pre>

<hr>
<h2 id='create_messages'>Create a list of messages</h2><span id='topic+create_messages'></span>

<h3>Description</h3>

<p>Create messages for <code>chat()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_messages(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_messages_+3A_...">...</code></td>
<td>
<p>A list of messages, each of list class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages, each of list class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>messages &lt;- create_messages(
    create_message("be nice", "system"),
    create_message("tell me a 3-word joke")
)

messages &lt;- create_messages(
    list(role = "system", content = "be nice"),
    list(role = "user", content = "tell me a 3-word joke")
)
</code></pre>

<hr>
<h2 id='create_request'>Create a httr2 request object</h2><span id='topic+create_request'></span>

<h3>Description</h3>

<p>Creates a httr2 request object with base URL, headers and endpoint. Used by other functions in the package and not intended to be used directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_request(endpoint, host = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_request_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to create the request</p>
</td></tr>
<tr><td><code id="create_request_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses http://127.0.0.1:11434</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 request object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_request("/api/tags")
create_request("/api/chat")
create_request("/api/embeddings")
</code></pre>

<hr>
<h2 id='delete'>Delete a model and its data</h2><span id='topic+delete'></span>

<h3>Description</h3>

<p>Delete a model from your local machine that you downloaded using the pull() function. To see which models are available, use the list_models() function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete(name, endpoint = "/api/delete", host = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_+3A_name">name</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="delete_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to delete the model. Default is &quot;/api/delete&quot;.</p>
</td></tr>
<tr><td><code id="delete_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#delete-a-model">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
delete("llama3")

## End(Not run)
</code></pre>

<hr>
<h2 id='delete_message'>Delete a message in a specified position from a list</h2><span id='topic+delete_message'></span>

<h3>Description</h3>

<p>Delete a message using positive or negative positions/indices.
Negative positions/indices can be used to refer to
elements/messages from the end of the sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_message(x, position = -1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_message_+3A_x">x</code></td>
<td>
<p>A list of messages.</p>
</td></tr>
<tr><td><code id="delete_message_+3A_position">position</code></td>
<td>
<p>The position of the message to delete.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages with the message at the specified position removed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>messages &lt;- list(
    list(role = "system", content = "Be friendly"),
    list(role = "user", content = "How are you?")
)
delete_message(messages, 1) # delete first message
delete_message(messages, -2) # same as above (delete first message)
delete_message(messages, 2) # delete second message
delete_message(messages, -1) # same as above (delete second message)
</code></pre>

<hr>
<h2 id='embed'>Generate embedding for inputs</h2><span id='topic+embed'></span>

<h3>Description</h3>

<p>Supercedes the <code>embeddings()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed(
  model,
  input,
  truncate = TRUE,
  normalize = TRUE,
  keep_alive = "5m",
  endpoint = "/api/embed",
  host = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embed_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="embed_+3A_input">input</code></td>
<td>
<p>A vector of characters that you want to get the embeddings for.</p>
</td></tr>
<tr><td><code id="embed_+3A_truncate">truncate</code></td>
<td>
<p>Truncates the end of each input to fit within context length. Returns error if FALSE and context length is exceeded. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="embed_+3A_normalize">normalize</code></td>
<td>
<p>Normalize the vector to length 1. Default is TRUE.</p>
</td></tr>
<tr><td><code id="embed_+3A_keep_alive">keep_alive</code></td>
<td>
<p>The time to keep the connection alive. Default is &quot;5m&quot; (5 minutes).</p>
</td></tr>
<tr><td><code id="embed_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to get the vector embedding. Default is &quot;/api/embeddings&quot;.</p>
</td></tr>
<tr><td><code id="embed_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
<tr><td><code id="embed_+3A_...">...</code></td>
<td>
<p>Additional options to pass to the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix of the embedding. Each column is the embedding for one input.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
embed("nomic-embed-text:latest", "The quick brown fox jumps over the lazy dog.")
# pass multiple inputs
embed("nomic-embed-text:latest", c("Good bye", "Bye", "See you."))
# pass model options to the model
embed("nomic-embed-text:latest", "Hello!", temperature = 0.1, num_predict = 3)

</code></pre>

<hr>
<h2 id='embeddings'>Generate embeddings for a single prompt - deprecated in favor of <code>embed()</code></h2><span id='topic+embeddings'></span>

<h3>Description</h3>

<p>This function will be deprecated over time and has been superceded by <code>embed()</code>. See <code>embed()</code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embeddings(
  model,
  prompt,
  normalize = TRUE,
  keep_alive = "5m",
  endpoint = "/api/embeddings",
  host = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embeddings_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_prompt">prompt</code></td>
<td>
<p>A character string of the prompt that you want to get the vector embedding for.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_normalize">normalize</code></td>
<td>
<p>Normalize the vector to length 1. Default is TRUE.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_keep_alive">keep_alive</code></td>
<td>
<p>The time to keep the connection alive. Default is &quot;5m&quot; (5 minutes).</p>
</td></tr>
<tr><td><code id="embeddings_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to get the vector embedding. Default is &quot;/api/embeddings&quot;.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_...">...</code></td>
<td>
<p>Additional options to pass to the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of the embedding.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embedding">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
embeddings("nomic-embed-text:latest", "The quick brown fox jumps over the lazy dog.")
# pass model options to the model
embeddings("nomic-embed-text:latest", "Hello!", temperature = 0.1, num_predict = 3)

</code></pre>

<hr>
<h2 id='encode_images_in_messages'>Encode images in messages to base64 format</h2><span id='topic+encode_images_in_messages'></span>

<h3>Description</h3>

<p>Encode images in messages to base64 format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encode_images_in_messages(messages)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="encode_images_in_messages_+3A_messages">messages</code></td>
<td>
<p>A list of messages, each of list class. Generally used in the <code>chat()</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages with images encoded in base64 format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>image &lt;- file.path(system.file("extdata", package = "ollamar"), "image1.png")
message &lt;- create_message(content = "what is in the image?", images = image)
message_updated &lt;- encode_images_in_messages(message)
</code></pre>

<hr>
<h2 id='generate'>Generate a response for a given prompt</h2><span id='topic+generate'></span>

<h3>Description</h3>

<p>Generate a response for a given prompt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate(
  model,
  prompt,
  suffix = "",
  images = "",
  format = list(),
  system = "",
  template = "",
  context = list(),
  stream = FALSE,
  raw = FALSE,
  keep_alive = "5m",
  output = c("resp", "jsonlist", "raw", "df", "text", "req", "structured"),
  endpoint = "/api/generate",
  host = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_prompt">prompt</code></td>
<td>
<p>A character string of the prompt like &quot;The sky is...&quot;</p>
</td></tr>
<tr><td><code id="generate_+3A_suffix">suffix</code></td>
<td>
<p>A character string after the model response. Default is &quot;&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_images">images</code></td>
<td>
<p>A path to an image file to include in the prompt. Default is &quot;&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_format">format</code></td>
<td>
<p>Format to return a response in. Format can be json/list (structured response).</p>
</td></tr>
<tr><td><code id="generate_+3A_system">system</code></td>
<td>
<p>A character string of the system prompt (overrides what is defined in the Modelfile). Default is &quot;&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_template">template</code></td>
<td>
<p>A character string of the prompt template (overrides what is defined in the Modelfile). Default is &quot;&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_context">context</code></td>
<td>
<p>A list of context from a previous response to include previous conversation in the prompt. Default is an empty list.</p>
</td></tr>
<tr><td><code id="generate_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="generate_+3A_raw">raw</code></td>
<td>
<p>If TRUE, no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API. Default is FALSE.</p>
</td></tr>
<tr><td><code id="generate_+3A_keep_alive">keep_alive</code></td>
<td>
<p>The time to keep the connection alive. Default is &quot;5m&quot; (5 minutes).</p>
</td></tr>
<tr><td><code id="generate_+3A_output">output</code></td>
<td>
<p>A character vector of the output format. Default is &quot;resp&quot;. Options are &quot;resp&quot;, &quot;jsonlist&quot;, &quot;raw&quot;, &quot;df&quot;, &quot;text&quot;, &quot;req&quot; (httr2_request object).</p>
</td></tr>
<tr><td><code id="generate_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to generate the completion. Default is &quot;/api/generate&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
<tr><td><code id="generate_+3A_...">...</code></td>
<td>
<p>Additional options to pass to the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# text prompt
generate("llama3", "The sky is...", stream = FALSE, output = "df")
# stream and increase temperature
generate("llama3", "The sky is...", stream = TRUE, output = "text", temperature = 2.0)

# image prompt
# something like "image1.png"
image_path &lt;- file.path(system.file("extdata", package = "ollamar"), "image1.png")
# use vision or multimodal model such as https://ollama.com/benzie/llava-phi-3
generate("benzie/llava-phi-3:latest", "What is in the image?", images = image_path, output = "text")

</code></pre>

<hr>
<h2 id='get_tool_calls'>Get tool calls helper function</h2><span id='topic+get_tool_calls'></span>

<h3>Description</h3>

<p>Get tool calls from response object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tool_calls(resp)
</code></pre>

<hr>
<h2 id='image_encode_base64'>Read image file and encode it to base64</h2><span id='topic+image_encode_base64'></span>

<h3>Description</h3>

<p>Read image file and encode it to base64
</p>


<h3>Usage</h3>

<pre><code class='language-R'>image_encode_base64(image_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="image_encode_base64_+3A_image_path">image_path</code></td>
<td>
<p>The path to the image file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A base64 encoded string.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>image_path &lt;- file.path(system.file("extdata", package = "ollamar"), "image1.png")
substr(image_encode_base64(image_path), 1, 5) # truncate output
</code></pre>

<hr>
<h2 id='insert_message'>Insert message into a list at a specified position</h2><span id='topic+insert_message'></span>

<h3>Description</h3>

<p>Inserts a message at a specified position in a list of messages.
The role and content are converted to a list and inserted into the input list at the given position.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>insert_message(content, role = "user", x = NULL, position = -1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="insert_message_+3A_content">content</code></td>
<td>
<p>The content of the message.</p>
</td></tr>
<tr><td><code id="insert_message_+3A_role">role</code></td>
<td>
<p>The role of the message. Can be &quot;user&quot;, &quot;system&quot;, &quot;assistant&quot;. Default is &quot;user&quot;.</p>
</td></tr>
<tr><td><code id="insert_message_+3A_x">x</code></td>
<td>
<p>A list of messages. Default is NULL.</p>
</td></tr>
<tr><td><code id="insert_message_+3A_position">position</code></td>
<td>
<p>The position at which to insert the new message. Default is -1 (end of list).</p>
</td></tr>
<tr><td><code id="insert_message_+3A_...">...</code></td>
<td>
<p>Additional arguments such as images.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages with the new message inserted at the specified position.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>messages &lt;- list(
    list(role = "system", content = "Be friendly"),
    list(role = "user", content = "How are you?")
)
insert_message("INSERT MESSAGE AT THE END", "user", messages)
insert_message("INSERT MESSAGE AT THE BEGINNING", "user", messages, 2)
</code></pre>

<hr>
<h2 id='list_models'>List models that are available locally</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>List models that are available locally
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(
  output = c("df", "resp", "jsonlist", "raw", "text"),
  endpoint = "/api/tags",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_models_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;df&quot;. Other options are &quot;resp&quot;, &quot;jsonlist&quot;, &quot;raw&quot;, &quot;text&quot;.</p>
</td></tr>
<tr><td><code id="list_models_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to get the models. Default is &quot;/api/tags&quot;.</p>
</td></tr>
<tr><td><code id="list_models_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
list_models() # returns dataframe
list_models("df") # returns dataframe
list_models("resp") # httr2 response object
list_models("jsonlist")
list_models("raw")

</code></pre>

<hr>
<h2 id='model_avail'>Check if model is available locally</h2><span id='topic+model_avail'></span>

<h3>Description</h3>

<p>Check if model is available locally
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_avail(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="model_avail_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value indicating if the model exists.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
model_avail("codegemma:7b")
model_avail("abc")
model_avail("llama3")

</code></pre>

<hr>
<h2 id='model_options'>Model options</h2><span id='topic+model_options'></span>

<h3>Description</h3>

<p>Model options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_options
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 13.
</p>

<hr>
<h2 id='ohelp'>Chat with a model in real-time in R console</h2><span id='topic+ohelp'></span>

<h3>Description</h3>

<p>Chat with a model in real-time in R console
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ohelp(model = "codegemma:7b", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ohelp_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;. Defaults to &quot;codegemma:7b&quot; which is a decent coding model as of 2024-07-27.</p>
</td></tr>
<tr><td><code id="ohelp_+3A_...">...</code></td>
<td>
<p>Additional options. No options are currently available at this time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return anything. It prints the conversation in the console.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
ohelp(first_prompt = "quit")
# regular usage: ohelp()

</code></pre>

<hr>
<h2 id='package_config'>Package configuration</h2><span id='topic+package_config'></span>

<h3>Description</h3>

<p>Package configuration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>package_config
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 3.
</p>

<hr>
<h2 id='prepend_message'>Prepend message to a list</h2><span id='topic+prepend_message'></span>

<h3>Description</h3>

<p>Prepends a message (add to beginning of a list) to a list of messages.
The role and content will be converted to a list and prepended to the input list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepend_message(content, role = "user", x = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepend_message_+3A_content">content</code></td>
<td>
<p>The content of the message.</p>
</td></tr>
<tr><td><code id="prepend_message_+3A_role">role</code></td>
<td>
<p>The role of the message. Can be &quot;user&quot;, &quot;system&quot;, &quot;assistant&quot;.</p>
</td></tr>
<tr><td><code id="prepend_message_+3A_x">x</code></td>
<td>
<p>A list of messages. Default is NULL.</p>
</td></tr>
<tr><td><code id="prepend_message_+3A_...">...</code></td>
<td>
<p>Additional arguments such as images.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of messages with the new message prepended.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prepend_message("user", "Hello")
prepend_message("system", "Always respond nicely")
</code></pre>

<hr>
<h2 id='ps'>List models that are currently loaded into memory</h2><span id='topic+ps'></span>

<h3>Description</h3>

<p>List models that are currently loaded into memory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ps(
  output = c("df", "resp", "jsonlist", "raw", "text"),
  endpoint = "/api/ps",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ps_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;df&quot;. Supported formats are &quot;df&quot;, &quot;resp&quot;, &quot;jsonlist&quot;, &quot;raw&quot;, and &quot;text&quot;.</p>
</td></tr>
<tr><td><code id="ps_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to list the running models. Default is &quot;/api/ps&quot;.</p>
</td></tr>
<tr><td><code id="ps_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
ps("text")

</code></pre>

<hr>
<h2 id='pull'>Pull/download a model from the Ollama library</h2><span id='topic+pull'></span>

<h3>Description</h3>

<p>See https://ollama.com/library for a list of available models. Use the list_models() function to get the list of models already downloaded/installed on your machine. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pull(
  name,
  stream = FALSE,
  insecure = FALSE,
  endpoint = "/api/pull",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pull_+3A_name">name</code></td>
<td>
<p>A character string of the model name to download/pull, such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="pull_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="pull_+3A_insecure">insecure</code></td>
<td>
<p>Allow insecure connections Only use this if you are pulling from your own library during development. Default is FALSE.</p>
</td></tr>
<tr><td><code id="pull_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to pull the model. Default is &quot;/api/pull&quot;.</p>
</td></tr>
<tr><td><code id="pull_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#pull-a-model">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pull("llama3")
pull("all-minilm", stream = FALSE)

</code></pre>

<hr>
<h2 id='push'>Push or upload a model to a model library</h2><span id='topic+push'></span>

<h3>Description</h3>

<p>Push or upload a model to an Ollama model library. Requires registering for ollama.ai and adding a public key first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>push(
  name,
  insecure = FALSE,
  stream = FALSE,
  output = c("resp", "jsonlist", "raw", "text", "df"),
  endpoint = "/api/push",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="push_+3A_name">name</code></td>
<td>
<p>A character string of the model name to upload, in the form of <code style="white-space: pre;">&#8288;&lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;&#8288;</code></p>
</td></tr>
<tr><td><code id="push_+3A_insecure">insecure</code></td>
<td>
<p>Allow insecure connections. Only use this if you are pushing to your own library during development. Default is FALSE.</p>
</td></tr>
<tr><td><code id="push_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="push_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;resp&quot;. Other options are &quot;jsonlist&quot;, &quot;raw&quot;, &quot;text&quot;, and &quot;df&quot;.</p>
</td></tr>
<tr><td><code id="push_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to push the model. Default is &quot;/api/push&quot;.</p>
</td></tr>
<tr><td><code id="push_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#push-a-model">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
push("mattw/pygmalion:latest")

</code></pre>

<hr>
<h2 id='resp_process'>Process httr2 response object</h2><span id='topic+resp_process'></span>

<h3>Description</h3>

<p>Process httr2 response object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resp_process(
  resp,
  output = c("df", "jsonlist", "raw", "resp", "text", "tools")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="resp_process_+3A_resp">resp</code></td>
<td>
<p>A httr2 response object.</p>
</td></tr>
<tr><td><code id="resp_process_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;df&quot;. Other options are &quot;jsonlist&quot;, &quot;raw&quot;, &quot;resp&quot; (httr2 response object), &quot;text&quot;, &quot;tools&quot; (tool_calls), &quot;structured&quot; (structured output).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, json list, raw or httr2 response object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
resp &lt;- list_models("resp")
resp_process(resp, "df") # parse response to dataframe/tibble
resp_process(resp, "jsonlist") # parse response to list
resp_process(resp, "raw") # parse response to raw string
resp_process(resp, "text") # return text/character vector
resp_process(resp, "tools") # return tool_calls

</code></pre>

<hr>
<h2 id='resp_process_stream'>Process httr2 response object for streaming</h2><span id='topic+resp_process_stream'></span>

<h3>Description</h3>

<p>Process httr2 response object for streaming
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resp_process_stream(resp, output)
</code></pre>

<hr>
<h2 id='search_options'>Search for options based on a query</h2><span id='topic+search_options'></span>

<h3>Description</h3>

<p>Search for options based on a query
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_options(query)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="search_options_+3A_query">query</code></td>
<td>
<p>A query (character) to search for in the options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of matching options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>search_options("learning rate")
search_options("tokens")
search_options("invalid query")
</code></pre>

<hr>
<h2 id='show'>Show model information</h2><span id='topic+show'></span>

<h3>Description</h3>

<p>Model information includes details, modelfile, template, parameters, license, system prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show(
  name,
  verbose = FALSE,
  output = c("jsonlist", "resp", "raw"),
  endpoint = "/api/show",
  host = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show_+3A_name">name</code></td>
<td>
<p>Name of the model to show</p>
</td></tr>
<tr><td><code id="show_+3A_verbose">verbose</code></td>
<td>
<p>Returns full data for verbose response fields. Default is FALSE.</p>
</td></tr>
<tr><td><code id="show_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;jsonlist&quot;. Other options are &quot;resp&quot;, &quot;raw&quot;.</p>
</td></tr>
<tr><td><code id="show_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to show the model. Default is &quot;/api/show&quot;.</p>
</td></tr>
<tr><td><code id="show_+3A_host">host</code></td>
<td>
<p>The base URL to use. Default is NULL, which uses Ollama's default base URL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>References</h3>

<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information">API documentation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# show("llama3") # returns jsonlist
show("llama3", output = "resp") # returns response object

</code></pre>

<hr>
<h2 id='stream_handler'>Stream handler helper function</h2><span id='topic+stream_handler'></span>

<h3>Description</h3>

<p>Function to handle streaming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_handler(x, env, endpoint)
</code></pre>

<hr>
<h2 id='test_connection'>Test connection to Ollama server</h2><span id='topic+test_connection'></span>

<h3>Description</h3>

<p>Tests whether the Ollama server is running or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_connection(url = "http://localhost:11434", logical = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="test_connection_+3A_url">url</code></td>
<td>
<p>The URL of the Ollama server. Default is http://localhost:11434</p>
</td></tr>
<tr><td><code id="test_connection_+3A_logical">logical</code></td>
<td>
<p>Logical. If TRUE, returns a boolean value. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boolean value or httr2 response object, where status_code is either 200 (success) or 503 (error).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_connection(logical = TRUE)
test_connection("http://localhost:11434") # default url
test_connection("http://127.0.0.1:11434")
</code></pre>

<hr>
<h2 id='validate_message'>Validate a message</h2><span id='topic+validate_message'></span>

<h3>Description</h3>

<p>Validate a message to ensure it has the required fields and the correct data types for the <code>chat()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_message(message)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_message_+3A_message">message</code></td>
<td>
<p>A list with a single message of list class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if message is valid, otherwise an error is thrown.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>validate_message(create_message("Hello"))
validate_message(list(role = "user", content = "Hello"))
</code></pre>

<hr>
<h2 id='validate_messages'>Validate a list of messages</h2><span id='topic+validate_messages'></span>

<h3>Description</h3>

<p>Validate a list of messages to ensure they have the required fields and the correct data types for the <code>chat()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_messages(messages)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_messages_+3A_messages">messages</code></td>
<td>
<p>A list of messages, each of list class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if all messages are valid, otherwise warning messages are printed and FALSE is returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>validate_messages(create_messages(
   create_message("Be friendly", "system"),
   create_message("Hello")
))
</code></pre>

<hr>
<h2 id='validate_options'>Validate additional options or parameters provided to the API call</h2><span id='topic+validate_options'></span>

<h3>Description</h3>

<p>Validate additional options or parameters provided to the API call
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_options(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_options_+3A_...">...</code></td>
<td>
<p>Additional options or parameters provided to the API call</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if all additional options are valid, FALSE otherwise
</p>


<h3>Examples</h3>

<pre><code class='language-R'>validate_options(mirostat = 1, mirostat_eta = 0.2, num_ctx = 1024)
validate_options(mirostat = 1, mirostat_eta = 0.2, invalid_opt = 1024)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
