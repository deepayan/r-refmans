<!DOCTYPE html><html><head><title>Help for package ollamar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ollamar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#chat'><p>Chat with Ollama models</p></a></li>
<li><a href='#create_request'><p>Create a httr2 request object.</p></a></li>
<li><a href='#delete'><p>Delete a model</p></a></li>
<li><a href='#embeddings'><p>Get vector embedding for a prompt</p></a></li>
<li><a href='#generate'><p>Generate a completion.</p></a></li>
<li><a href='#list_models'><p>Get available local models</p></a></li>
<li><a href='#package_config'><p>Package configuration</p></a></li>
<li><a href='#pull'><p>Pull/download a model</p></a></li>
<li><a href='#resp_process'><p>Process httr2 response object.</p></a></li>
<li><a href='#test_connection'><p>Test connection to Ollama server</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>'Ollama' Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>An interface to easily run local language models with 'Ollama' <a href="https://ollama.com">https://ollama.com</a> server
    and API endpoints (see <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">https://github.com/ollama/ollama/blob/main/docs/api.md</a> for details). It lets 
    you run open-source large language models locally on your machine. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>glue, httr2, jsonlite, tibble</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/hauselin/ollama-r/issues">https://github.com/hauselin/ollama-r/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://hauselin.github.io/ollama-r/">https://hauselin.github.io/ollama-r/</a>,
<a href="https://github.com/hauselin/ollama-r">https://github.com/hauselin/ollama-r</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-02 01:09:48 UTC; hause</td>
</tr>
<tr>
<td>Author:</td>
<td>Hause Lin <a href="https://orcid.org/0000-0003-4590-7039"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hause Lin &lt;hauselin@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-02 15:12:40 UTC</td>
</tr>
</table>
<hr>
<h2 id='chat'>Chat with Ollama models</h2><span id='topic+chat'></span>

<h3>Description</h3>

<p>Chat with Ollama models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat(
  model,
  messages,
  stream = FALSE,
  output = c("resp", "jsonlist", "raw", "df"),
  endpoint = "/api/chat"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_messages">messages</code></td>
<td>
<p>A list with list of messages for the model (see examples below).</p>
</td></tr>
<tr><td><code id="chat_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="chat_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;resp&quot;. Other options are &quot;jsonlist&quot;, &quot;raw&quot;, &quot;df&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to chat with the model. Default is &quot;/api/chat&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object, json list, raw or data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one message
messages &lt;- list(
 list(role = "user", content = "How are you doing?")
)
chat("llama3", messages)
chat("llama3", messages, stream = TRUE)
chat("llama3", messages, stream = TRUE, output = "df")

# multiple messages
messages &lt;- list(
 list(role = "user", content = "Hello!"),
 list(role = "assistant", content = "Hi! How are you?"),
 list(role = "user", content = "Who is the prime minister of the uk?"),
 list(role = "assistant", content = "Rishi Sunak"),
 list(role = "user", content = "List all the previous messages.")
)
chat("llama3", messages, stream = TRUE)

</code></pre>

<hr>
<h2 id='create_request'>Create a httr2 request object.</h2><span id='topic+create_request'></span>

<h3>Description</h3>

<p>Creates a httr2 request object with base URL, headers and endpoint. Used by other functions in the package and not intended to be used directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_request(endpoint)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_request_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to create the request</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 request object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_request("/api/tags")
create_request("/api/chat")
create_request("/api/embeddings")
</code></pre>

<hr>
<h2 id='delete'>Delete a model</h2><span id='topic+delete'></span>

<h3>Description</h3>

<p>Delete a model from your local machine that you downlaoded using the pull() function. To see which models are available, use the list_models() function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete(model, endpoint = "/api/delete")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="delete_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to delete the model. Default is &quot;/api/delete&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
delete("llama3")

## End(Not run)
</code></pre>

<hr>
<h2 id='embeddings'>Get vector embedding for a prompt</h2><span id='topic+embeddings'></span>

<h3>Description</h3>

<p>Get vector embedding for a prompt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embeddings(model, prompt, normalize = TRUE, endpoint = "/api/embeddings")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embeddings_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_prompt">prompt</code></td>
<td>
<p>A character string of the prompt that you want to get the vector embedding for.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_normalize">normalize</code></td>
<td>
<p>Normalize the vector to length 1. Default is TRUE.</p>
</td></tr>
<tr><td><code id="embeddings_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to get the vector embedding. Default is &quot;/api/embeddings&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of the embedding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
embeddings("nomic-embed-text:latest", "The quick brown fox jumps over the lazy dog.")

</code></pre>

<hr>
<h2 id='generate'>Generate a completion.</h2><span id='topic+generate'></span>

<h3>Description</h3>

<p>Generate a response for a given prompt with a provided model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate(
  model,
  prompt,
  stream = FALSE,
  output = c("resp", "jsonlist", "raw", "df"),
  endpoint = "/api/generate"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_prompt">prompt</code></td>
<td>
<p>A character string of the promp like &quot;The sky is...&quot;</p>
</td></tr>
<tr><td><code id="generate_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is FALSE.</p>
</td></tr>
<tr><td><code id="generate_+3A_output">output</code></td>
<td>
<p>A character vector of the output format. Default is &quot;resp&quot;. Options are &quot;resp&quot;, &quot;jsonlist&quot;, &quot;raw&quot;, &quot;df&quot;.</p>
</td></tr>
<tr><td><code id="generate_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to generate the completion. Default is &quot;/api/generate&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A response in the format specified in the output parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
generate("llama3", "The sky is...", stream = FALSE, output = "df")
generate("llama3", "The sky is...", stream = TRUE, output = "df")
generate("llama3", "The sky is...", stream = FALSE, output = "jsonlist")

</code></pre>

<hr>
<h2 id='list_models'>Get available local models</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>Get available local models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(
  output = c("df", "resp", "jsonlist", "raw"),
  endpoint = "/api/tags"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list_models_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;df&quot;. Other options are &quot;resp&quot;, &quot;jsonlist&quot;, &quot;raw&quot;.</p>
</td></tr>
<tr><td><code id="list_models_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to get the models. Default is &quot;/api/tags&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object, json list, raw or data frame. Default is &quot;df&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
list_models()  # returns dataframe/tibble by default
list_models("df")
list_models("resp")
list_models("jsonlist")
list_models("raw")

</code></pre>

<hr>
<h2 id='package_config'>Package configuration</h2><span id='topic+package_config'></span>

<h3>Description</h3>

<p>Package configuration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>package_config
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 3.
</p>

<hr>
<h2 id='pull'>Pull/download a model</h2><span id='topic+pull'></span>

<h3>Description</h3>

<p>See https://ollama.com/library for a list of available models. Use the list_models() function to get the list of models already downloaded/installed on your machine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pull(model, stream = TRUE, endpoint = "/api/pull")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pull_+3A_model">model</code></td>
<td>
<p>A character string of the model name such as &quot;llama3&quot;.</p>
</td></tr>
<tr><td><code id="pull_+3A_stream">stream</code></td>
<td>
<p>Enable response streaming. Default is TRUE.</p>
</td></tr>
<tr><td><code id="pull_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint to pull the model. Default is &quot;/api/pull&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pull("llama3")

</code></pre>

<hr>
<h2 id='resp_process'>Process httr2 response object.</h2><span id='topic+resp_process'></span>

<h3>Description</h3>

<p>Process httr2 response object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resp_process(resp, output = c("df", "jsonlist", "raw", "resp"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resp_process_+3A_resp">resp</code></td>
<td>
<p>A httr2 response object.</p>
</td></tr>
<tr><td><code id="resp_process_+3A_output">output</code></td>
<td>
<p>The output format. Default is &quot;df&quot;. Other options are &quot;jsonlist&quot;, &quot;raw&quot;, &quot;resp&quot; (httr2 response object).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, json list, raw or httr2 response object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
resp &lt;- list_models("resp")
resp_process(resp, "df")  # parse response to dataframe/tibble
resp_process(resp, "jsonlist")  # parse response to list
resp_process(resp, "raw")  # parse response to raw string
resp_process(resp, "resp")  # return input response object

</code></pre>

<hr>
<h2 id='test_connection'>Test connection to Ollama server</h2><span id='topic+test_connection'></span>

<h3>Description</h3>

<p>Test connection to Ollama server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_connection(url = "http://localhost:11434")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_connection_+3A_url">url</code></td>
<td>
<p>The URL of the Ollama server. Default is http://localhost:11434</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A httr2 response object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_connection()
test_connection("http://localhost:11434")
test_connection("http://127.0.0.1:11434")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
