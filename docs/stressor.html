<!DOCTYPE html><html><head><title>Help for package stressor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stressor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#boston'><p>Boston Housing Data</p></a></li>
<li><a href='#create_groups'><p>Create Groups for CV</p></a></li>
<li><a href='#create_virtualenv'><p>Create 'Python' Virtual Environment</p></a></li>
<li><a href='#cv'><p>Cross Validation</p></a></li>
<li><a href='#cv_cluster'><p>Spatial Cluster-Based Partitions for Cross-Validation</p></a></li>
<li><a href='#cv_core'><p>Cross Validation Function</p></a></li>
<li><a href='#data_gen_asym'><p>Data Generation Asymptotic</p></a></li>
<li><a href='#data_gen_lm'><p>Data Generation for Linear Regression</p></a></li>
<li><a href='#data_gen_sine'><p>Data Generation for Sinusoidal Regression</p></a></li>
<li><a href='#dist_cent'><p>Distance to Center</p></a></li>
<li><a href='#kappa_class'><p>Kappa function</p></a></li>
<li><a href='#mlm_classification'><p>Fit Machine Learning Classification Models</p></a></li>
<li><a href='#mlm_init'><p>Compare Machine Learning Models</p></a></li>
<li><a href='#mlm_refit'><p>Refit Machine Learning Models</p></a></li>
<li><a href='#mlm_regressor'><p>Fit Machine Learning Regressor Models</p></a></li>
<li><a href='#predict'><p>Prediction Methods for Various Models</p></a></li>
<li><a href='#python_avail'><p>Check if 'Python' is Available</p></a></li>
<li><a href='#reg_asym'><p>Asymptotic Regression</p></a></li>
<li><a href='#reg_sine'><p>Sinusoidal Regression</p></a></li>
<li><a href='#rmse'><p>Root Mean Squarred Error (RMSE)</p></a></li>
<li><a href='#score'><p>Score Function for Metrics</p></a></li>
<li><a href='#score_classification'><p>Score Function for Binary Classification</p></a></li>
<li><a href='#score_regression'><p>Score Function for Regression</p></a></li>
<li><a href='#split_data_prob'><p>Create Train Index Set</p></a></li>
<li><a href='#thinning'><p>Thinning Algorithm for Models with Predict Function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Algorithms for Testing Models under Stress</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Traditional model evaluation metrics fail to capture model 
    performance under less than ideal conditions. This package employs 
    techniques to evaluate models "under-stress". This includes testing 
    models' extrapolation ability, or testing accuracy on specific 
    sub-samples of the overall model space. Details describing stress-testing 
    methods in this package are provided in 
    Haycock (2023) &lt;<a href="https://doi.org/10.26076%2F2am5-9f67">doi:10.26076/2am5-9f67</a>&gt;. The other primary contribution of
    this package is provided to R users access to the 'Python' library 'PyCaret'
    <a href="https://pycaret.org/">https://pycaret.org/</a> for quick and easy access to auto-tuned 
    machine learning models. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>python(&gt;=3.8.10)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, ggplot2, mlbench, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate, stats, dplyr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-01 02:41:58 UTC; 14357</td>
</tr>
<tr>
<td>Author:</td>
<td>Sam Haycock [aut, cre],
  Brennan Bean [aut],
  Utah State University [cph, fnd],
  Thermo Fisher Scientific Inc. [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sam Haycock &lt;haycock.sam@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-01 04:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='boston'>Boston Housing Data</h2><span id='topic+boston'></span>

<h3>Description</h3>

<p>A subset of data from the Housing data for 506 census tracts of Boston from
the 1970 Census. Original data set can be found in the
<a href="mlbench.html#topic+BostonHousing2">mlbench</a> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(boston)
</code></pre>


<h3>Format</h3>

<p>A data.frame with 506 rows and 13 columns:
</p>

<dl>
<dt>cmedv</dt><dd><p>corrected median value of owner-occupied homes in USD 1000's</p>
</dd>
<dt>crim</dt><dd><p>per capita crime rate by town</p>
</dd>
<dt>zn</dt><dd><p>proportion of residential land zoned for lots over 25,000 sq.ft</p>
</dd>
<dt>indus</dt><dd><p>proportion of non-retail business acres per town</p>
</dd>
<dt>nox</dt><dd><p>nitric oxides concentration (parts per 10 million)</p>
</dd>
<dt>rm</dt><dd><p>average number of rooms per dwelling</p>
</dd>
<dt>age</dt><dd><p>proportion of owner-occupied units built prior to 1940</p>
</dd>
<dt>dis</dt><dd><p>weighted distances to five Boston employment centres</p>
</dd>
<dt>rad</dt><dd><p>index of accessibility to radial highways</p>
</dd>
<dt>tax</dt><dd><p>full-value property-tax rate per USD 10,000</p>
</dd>
<dt>ptratio</dt><dd><p>pupil-teacher ratio by town</p>
</dd>
<dt>chas</dt><dd><p>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</p>
</dd>
<dt>lstat</dt><dd><p>percentage of lower status of the population</p>
</dd>
</dl>



<h3>Source</h3>

<p>mlbench package
</p>

<hr>
<h2 id='create_groups'>Create Groups for CV</h2><span id='topic+create_groups'></span>

<h3>Description</h3>

<p>Create groups for the data by separating them either into 10 fold
cross-validation, LOO cross-validation, or k-means grouping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_groups(
  formula,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_groups_+3A_formula">formula</code></td>
<td>
<p>A formula object that specifies the model to be fit.</p>
</td></tr>
<tr><td><code id="create_groups_+3A_data">data</code></td>
<td>
<p>The data that will be separated into each group.</p>
</td></tr>
<tr><td><code id="create_groups_+3A_n_folds">n_folds</code></td>
<td>
<p>An integer value defaulted to 10 fold cross-validation. If NULL
uses Leave One Out(LOO) instead.</p>
</td></tr>
<tr><td><code id="create_groups_+3A_k_mult">k_mult</code></td>
<td>
<p>When specified, this is passed onto the <a href="#topic+cv_cluster">cv_cluster</a>
to fit the data into k_groups.</p>
</td></tr>
<tr><td><code id="create_groups_+3A_repl">repl</code></td>
<td>
<p>A Boolean value defaulted to 'FALSE', change to 'TRUE' when
replicates need to be included in the same group.</p>
</td></tr>
<tr><td><code id="create_groups_+3A_grouping_formula">grouping_formula</code></td>
<td>
<p>A formula object that specifies how the groups will
be gathered.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If 'k_mult' is specified as an integer, the formula object will be
used to help determine the features specified by the user. This will be
passed to the <a href="#topic+cv_cluster">cv_cluster</a> function, which takes a scaled
matrix of features.
</p>
<p>This function is called by the <a href="#topic+cv">cv</a> methods as it forms the
groups necessary to perform the cross-validation. If you want to
use this, it is a nice function that separates the 'data' into
groups for training and testing.
</p>


<h3>Value</h3>

<p>A vector of the length equal to number of rows of data.frame from the
data argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # data generation
 lm_data &lt;- data_gen_lm(1000)

 # 10 Fold CV group
 create_groups(Y ~ ., lm_data)

 # Spatial CV
 create_groups(Y ~ ., lm_data, n_folds = 10, k_mult = 5)

 # LOO CV group
 create_groups(Y ~ ., lm_data, n_folds = NULL)
</code></pre>

<hr>
<h2 id='create_virtualenv'>Create 'Python' Virtual Environment</h2><span id='topic+create_virtualenv'></span>

<h3>Description</h3>

<p>Allows the user to create a stressor 'python' environment with 'PyCaret'
installed in the environment. This function assumes that
you have properly installed 'python'. We recommend version 3.8.10. It uses
existing stressor environments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_virtualenv(python = Sys.which("python"), delete_env = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_virtualenv_+3A_python">python</code></td>
<td>
<p>Defaults to your install of 'python'. We prefer version 3.8.10.
This is assuming that you installed python from python.org. Currently 'Anaconda'
installations of python are not implemented.</p>
</td></tr>
<tr><td><code id="create_virtualenv_+3A_delete_env">delete_env</code></td>
<td>
<p>Boolean value to indicate if the environments need to be
deleted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To install 'python', it is recommended using 'python' version 3.8.10 from
<a href="https://www.python.org/downloads/release/python-3810/">python.org</a>.
This is the same version recommended by 'PyCaret', as it is the most stable.
Users have reported troubles using the 'Anaconda' distribution of 'python'.
</p>
<p>For MacOS and Linux Users note that in order to run this package, 'LightGBM'
package on python requires the install of an additional compiler 'cmake' and
the 'libomp' (Open Multi-Processing interface). Troubleshoot link from the
'LightGBM'documentation
<a href="https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html">here</a>.
</p>


<h3>Value</h3>

<p>A message indicating which environment is being used.
</p>


<h3>Troubleshoot</h3>

<p>If 'python' is not being found properly, trying setting the
'RETICULATE_PYTHON' to blank string. Also ensure that you do not have other
'python' objects in your environment.
</p>
<p>Also note that on some instances that a warning message may be displayed as
to which version of 'python' is being used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 create_virtualenv()

</code></pre>

<hr>
<h2 id='cv'>Cross Validation</h2><span id='topic+cv'></span><span id='topic+cv.lm'></span><span id='topic+cv.mlm_stressor'></span><span id='topic+cv.reg_asym'></span><span id='topic+cv.reg_sine'></span>

<h3>Description</h3>

<p>This is the core of cross-validation- both standard and using k-mean groups.
This method is called by other cv methods of classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(
  object,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)

## S3 method for class 'lm'
cv(
  object,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)

## S3 method for class 'mlm_stressor'
cv(
  object,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)

## S3 method for class 'reg_asym'
cv(
  object,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)

## S3 method for class 'reg_sine'
cv(
  object,
  data,
  n_folds = 10,
  k_mult = NULL,
  repl = FALSE,
  grouping_formula = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_object">object</code></td>
<td>
<p>One of the four objects that is accepted: mlm_stressor,
reg_sine, reg_asym, or lm.</p>
</td></tr>
<tr><td><code id="cv_+3A_data">data</code></td>
<td>
<p>A data.frame object that contains all the entries to be
cross-validated on.</p>
</td></tr>
<tr><td><code id="cv_+3A_n_folds">n_folds</code></td>
<td>
<p>An integer value for the number of folds defaulted to 10. If
NULL, it will run LOO cross-validation.</p>
</td></tr>
<tr><td><code id="cv_+3A_k_mult">k_mult</code></td>
<td>
<p>Used to specify if k-means clustering is to be used, defaulted
to NULL.</p>
</td></tr>
<tr><td><code id="cv_+3A_repl">repl</code></td>
<td>
<p>A Boolean value defaulted to 'FALSE', change to 'TRUE' when
replicates need to be included in the same group.</p>
</td></tr>
<tr><td><code id="cv_+3A_grouping_formula">grouping_formula</code></td>
<td>
<p>A formula object that specifies how the groups will
be gathered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the object is of class mlm_stressor, then a data.frame will be
returned. Otherwise, a vector of the predictions will be returned.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>cv(lm)</code>: Cross-Validation for lm
</p>
</li>
<li> <p><code>cv(mlm_stressor)</code>: Cross-Validation for mlm_stressor
</p>
</li>
<li> <p><code>cv(reg_asym)</code>: Cross-Validation for reg_asym
</p>
</li>
<li> <p><code>cv(reg_sine)</code>: Cross-Validation for reg_sine
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'> # lm example
 lm_test &lt;- data_gen_lm(20)
 lm &lt;- lm(Y ~ ., lm_test)
 cv(lm, lm_test, n_folds = 2)


 lm_test &lt;- data_gen_lm(20)
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_test)
 cv(mlm_lm, lm_test, n_folds = 2)

 # Asymptotic example
 asym_data &lt;- data_gen_asym(10)
 asym_fit &lt;- reg_asym(Y ~ ., asym_data)
 cv(asym_fit, asym_data, n_folds = 2)

 # Sine example
 sine_data &lt;- data_gen_sine(10)
 sine_fit &lt;- reg_sine(Y ~ ., sine_data)
 cv(sine_fit, sine_data, n_folds = 2)
</code></pre>

<hr>
<h2 id='cv_cluster'>Spatial Cluster-Based Partitions for Cross-Validation</h2><span id='topic+cv_cluster'></span>

<h3>Description</h3>

<p>This function creates cluster-based partitions of a sample space based on
k-means clustering. Included in the function are algorithms that attempt
to produce clusters of roughly equal size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_cluster(features, k, k_mult = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_cluster_+3A_features">features</code></td>
<td>
<p>A scaled matrix of features to be used in the clustering.
Scaling usually done with <a href="base.html#topic+scale">scale</a> and should not include the
predictor variable.</p>
</td></tr>
<tr><td><code id="cv_cluster_+3A_k">k</code></td>
<td>
<p>The number of partitions for k-fold cross-validation.</p>
</td></tr>
<tr><td><code id="cv_cluster_+3A_k_mult">k_mult</code></td>
<td>
<p>k*k_mult determines the number of subgroups that will be
created as part of the balancing algorithm.</p>
</td></tr>
<tr><td><code id="cv_cluster_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="stats.html#topic+kmeans">kmeans</a> as needed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>More information regarding spatial cross-validation can be found in
Robin Lovelace's explanation of spatial
cross-validation in his
<a href="https://r.geocompx.org/spatial-cv.html?q=cross%20validation#intro-cv">textbook</a>.
</p>


<h3>Value</h3>

<p>An integer vector that is number of rows of features with indices of
each group.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Creating a matrix of predictor variables
 x_data &lt;- base::scale(data_gen_lm(30)[, -1])
 groups &lt;- cv_cluster(x_data, 5, k_mult = 5)
 groups
</code></pre>

<hr>
<h2 id='cv_core'>Cross Validation Function</h2><span id='topic+cv_core'></span>

<h3>Description</h3>

<p>This is the machinery to run cross validation. It subsets the test and train
set based on the groups it receives.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_core(object, data, t_groups, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_core_+3A_object">object</code></td>
<td>
<p>Currently '&quot;reg_sine&quot;, &quot;reg_asym&quot;, &quot;lm&quot;, &quot;mlm_stressor&quot;'
objects are accepted.</p>
</td></tr>
<tr><td><code id="cv_core_+3A_data">data</code></td>
<td>
<p>A data.frame object that has the same formula that was fitted on
the data.</p>
</td></tr>
<tr><td><code id="cv_core_+3A_t_groups">t_groups</code></td>
<td>
<p>The groups for cross validation: standard cross validation,
LOO cross_validation, or spatial cross validation.</p>
</td></tr>
<tr><td><code id="cv_core_+3A_...">...</code></td>
<td>
<p>Additional arguments that are passed to the predict function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector of predictions for '&quot;reg_sine&quot;, &quot;reg_asym&quot;, &quot;lm&quot;' and
a data frame for '&quot;mlm_stressor&quot;'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # lm example
 lm_test &lt;- data_gen_lm(20)
 lm &lt;- lm(Y ~ ., lm_test)
 cv(lm, lm_test, n_folds = 2)


 lm_test &lt;- data_gen_lm(20)
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_test)
 cv(mlm_lm, lm_test, n_folds = 2)

 # Asymptotic example
 asym_data &lt;- data_gen_asym(10)
 asym_fit &lt;- reg_asym(Y ~ ., asym_data)
 cv(asym_fit, asym_data, n_folds = 2)

 # Sine example
 sine_data &lt;- data_gen_sine(10)
 sine_fit &lt;- reg_sine(Y ~ ., sine_data)
 cv(sine_fit, sine_data, n_folds = 2)
</code></pre>

<hr>
<h2 id='data_gen_asym'>Data Generation Asymptotic</h2><span id='topic+data_gen_asym'></span>

<h3>Description</h3>

<p>Creates a synthetic data set for an additive asymptotic model. See the details
section for clarification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_gen_asym(
  n,
  weight_mat = matrix(rlnorm(10), nrow = 2, ncol = 5),
  y_int = 0,
  resp_sd = 1,
  window = 1e-05,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_gen_asym_+3A_n">n</code></td>
<td>
<p>The number of observations for each parameter.</p>
</td></tr>
<tr><td><code id="data_gen_asym_+3A_weight_mat">weight_mat</code></td>
<td>
<p>The parameter coefficients, where each column represents
the coefficients and is two rows as each additive equation contains two
parameters. Defaulted to be 10 random numbers from the log-normal
distribution. The second row of the matrix needs to be positive.</p>
</td></tr>
<tr><td><code id="data_gen_asym_+3A_y_int">y_int</code></td>
<td>
<p>The y-intercept term of the additive model.</p>
</td></tr>
<tr><td><code id="data_gen_asym_+3A_resp_sd">resp_sd</code></td>
<td>
<p>The standard deviation of the epsilon term to be added for
noise.</p>
</td></tr>
<tr><td><code id="data_gen_asym_+3A_window">window</code></td>
<td>
<p>Used to determine for any given X variable to get you within
distance to capture the asymptotic behavior.</p>
</td></tr>
<tr><td><code id="data_gen_asym_+3A_...">...</code></td>
<td>
<p>Additional arguments that are not currently implemented.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Observations are generated from the following model:
</p>
<p style="text-align: center;"><code class="reqn">y = \sum_{i = 1}^n -\alpha_ie^{-\beta_i \cdot x_i} + y_{int}</code>
</p>

<p>Where 'n' is the number of parameters to be used, <code class="reqn">\alpha_i</code>'s
are the scaling parameter and the <code class="reqn">\beta_i</code>'s are the weights
associated with each <code class="reqn">x_i</code>. With the <code class="reqn">y_{int}</code> being where it
crosses the y-axis.
</p>


<h3>Value</h3>

<p>A data.frame object with the n rows and the response variable with
the number of parameters being equal to the number of columns from the
weight matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Generates 10 observations
 asym_data &lt;- data_gen_asym(10)
 asym_data
</code></pre>

<hr>
<h2 id='data_gen_lm'>Data Generation for Linear Regression</h2><span id='topic+data_gen_lm'></span>

<h3>Description</h3>

<p>Creates a synthetic data set for an additive linear model. See details for
clarification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_gen_lm(n, weight_vec = rep(1, 5), y_int = 0, resp_sd = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_gen_lm_+3A_n">n</code></td>
<td>
<p>The number of observations for each parameter.</p>
</td></tr>
<tr><td><code id="data_gen_lm_+3A_weight_vec">weight_vec</code></td>
<td>
<p>The parameter coefficients where each entry represents the
coefficients for the additive linear model.</p>
</td></tr>
<tr><td><code id="data_gen_lm_+3A_y_int">y_int</code></td>
<td>
<p>The y-intercept term of the additive model.</p>
</td></tr>
<tr><td><code id="data_gen_lm_+3A_resp_sd">resp_sd</code></td>
<td>
<p>The standard deviation of the epsilon term to be added for
noise.</p>
</td></tr>
<tr><td><code id="data_gen_lm_+3A_...">...</code></td>
<td>
<p>Additional arguments that are not currently implemented.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Observations are generated from the following model:
</p>
<p style="text-align: center;"><code class="reqn">y = \sum_{i = 1}^n \alpha_i\cdot x_i + y_{int}</code>
</p>

<p>Where 'n' is the number of parameters to be used and the <code class="reqn">\alpha_i</code>'s
are the weights associated with each <code class="reqn">x_i</code>. With the <code class="reqn">y_{int}</code>
being where it crosses the y-axis.
</p>


<h3>Value</h3>

<p>A data.frame object with the n rows and the response variable with
the number of parameters being equal to the number of columns from the
weight matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Generates 10 observations
 lm_data &lt;- data_gen_lm(10)
 lm_data
</code></pre>

<hr>
<h2 id='data_gen_sine'>Data Generation for Sinusoidal Regression</h2><span id='topic+data_gen_sine'></span>

<h3>Description</h3>

<p>Creates a synthetic data set for an additive sinusoidal regression model. See
the details section for clarification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_gen_sine(
  n,
  weight_mat = matrix(rnorm(15), nrow = 3, ncol = 5),
  y_int = 0,
  resp_sd = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_gen_sine_+3A_n">n</code></td>
<td>
<p>The number of observations for each parameter.</p>
</td></tr>
<tr><td><code id="data_gen_sine_+3A_weight_mat">weight_mat</code></td>
<td>
<p>The parameter coefficients, where each column represents
the coefficients and is three rows as each additive equation contains three
parameters. Defaulted to be 15 random numbers from the normal distribution.</p>
</td></tr>
<tr><td><code id="data_gen_sine_+3A_y_int">y_int</code></td>
<td>
<p>The y-intercept term of the additive model.</p>
</td></tr>
<tr><td><code id="data_gen_sine_+3A_resp_sd">resp_sd</code></td>
<td>
<p>The standard deviation of the epsilon term to be added for
noise.</p>
</td></tr>
<tr><td><code id="data_gen_sine_+3A_...">...</code></td>
<td>
<p>Additional arguments that are not currently implemented.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Observations are generated from the following model:
</p>
<p style="text-align: center;"><code class="reqn">y = \sum_{i = 1}^n \alpha_i \ \sin{(\beta_i(x_i - \gamma_i)))} +
  y_{int}</code>
</p>

<p>Where 'n' is the number of parameters to be used, <code class="reqn">\alpha_i</code>'s
are the amplitude of each sine wave, <code class="reqn">\beta_i</code>'s are the periods for
each sine wave and indirectly the weight on each <code class="reqn">x_i</code>, and the
<code class="reqn">\gamma_i</code>'s are the phase shift associated with each sine wave. With
the <code class="reqn">y_{int}</code> being where it crosses the y-axis.
</p>


<h3>Value</h3>

<p>A data.frame object with the n rows and the response variable with
the number of parameters being equal to the number of columns from the
weight matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Generates 10 observations
 sine_data &lt;- data_gen_sine(10)
 sine_data
</code></pre>

<hr>
<h2 id='dist_cent'>Distance to Center</h2><span id='topic+dist_cent'></span>

<h3>Description</h3>

<p>Calculates the distance from center of the matrix of predictor variables
using a euclidean distance, or the average of all x-dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_cent(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_cent_+3A_formula">formula</code></td>
<td>
<p>A formula object.</p>
</td></tr>
<tr><td><code id="dist_cent_+3A_data">data</code></td>
<td>
<p>A data.frame object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formula used to calculate the center point:
</p>
<p style="text-align: center;"><code class="reqn">\bar{x} = \frac{1}{N}\sum_{j = 1}^N x_{ij}</code>
</p>

<p>Where <strong><code class="reqn">\bar{x}</code></strong> is a vector of the center of the x-dimensions,
<code class="reqn">N</code> is the number of rows in the matrix, and <code class="reqn">x_{ij}</code> is the
<code class="reqn">i,j^{th}</code> entry in the matrix.
</p>


<h3>Value</h3>

<p>A vector of distances from the center.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data &lt;- data_gen_lm(10)
  dist &lt;- dist_cent(Y ~ ., data)
  dist
</code></pre>

<hr>
<h2 id='kappa_class'>Kappa function</h2><span id='topic+kappa_class'></span>

<h3>Description</h3>

<p>A function to calculate the Kappa of binary classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappa_class(confusion_matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kappa_class_+3A_confusion_matrix">confusion_matrix</code></td>
<td>
<p>A matrix or table that is the confusion matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value representing the kappa value.
</p>

<hr>
<h2 id='mlm_classification'>Fit Machine Learning Classification Models</h2><span id='topic+mlm_classification'></span>

<h3>Description</h3>

<p>Through the <a href="https://pycaret.gitbook.io/docs/get-started/quickstart#classification">PyCaret</a>
module from 'python', this function fits many machine
learning models simultaneously without requiring any 'python'
programming on the part of the user. This function is specifically
designed for the classification models fitted by 'PyCaret'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlm_classification(
  formula,
  train_data,
  fit_models = c("ada", "et", "lightgbm", "dummy", "lr", "rf", "ridge", "knn", "dt",
    "gbc", "svm", "lda", "nb", "qda"),
  sort_v = c("Accuracy", "AUC", "Recall", "Precision", "F1", "Kappa", "MCC"),
  n_models = 9999,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlm_classification_+3A_formula">formula</code></td>
<td>
<p>The classification formula, as a formula object.</p>
</td></tr>
<tr><td><code id="mlm_classification_+3A_train_data">train_data</code></td>
<td>
<p>A data.frame object that includes data to be trained on.</p>
</td></tr>
<tr><td><code id="mlm_classification_+3A_fit_models">fit_models</code></td>
<td>
<p>A character vector with all the possible Machine Learning
classifiers that are currently being fit, the user may specify a subset of
them using a character vector.
</p>

<table>
<tr>
 <td style="text-align: right;">
  ada </td><td style="text-align: left;"> AdaBoost Classifier </td>
</tr>
<tr>
 <td style="text-align: right;">
  dt </td><td style="text-align: left;"> Decision Tree Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  dummy </td><td style="text-align: left;"> Dummy Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  et </td><td style="text-align: left;"> Extra Trees Classifier </td>
</tr>
<tr>
 <td style="text-align: right;">
  gbc </td><td style="text-align: left;"> Gradient Boosting Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  knn </td><td style="text-align: left;"> K Neighbors Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  lda </td><td style="text-align: left;"> Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: right;">
  lightgbm </td><td style="text-align: left;"> Light Gradient Boosting Machine</td>
</tr>
<tr>
 <td style="text-align: right;">
  lr </td><td style="text-align: left;"> Logistic Regression</td>
</tr>
<tr>
 <td style="text-align: right;">
  nb </td><td style="text-align: left;"> Naive Bayes </td>
</tr>
<tr>
 <td style="text-align: right;">
  qda </td><td style="text-align: left;"> Quadratic Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: right;">
  rf </td><td style="text-align: left;"> Random Forest Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  ridge </td><td style="text-align: left;"> Ridge Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
  svm </td><td style="text-align: left;"> SVM - Linear Kernel
</td>
</tr>

</table>
</td></tr>
<tr><td><code id="mlm_classification_+3A_sort_v">sort_v</code></td>
<td>
<p>A character vector indicating what to sort the tuned models on.</p>
</td></tr>
<tr><td><code id="mlm_classification_+3A_n_models">n_models</code></td>
<td>
<p>An integer value defaulted to a large integer value to
return all possible models.</p>
</td></tr>
<tr><td><code id="mlm_classification_+3A_seed">seed</code></td>
<td>
<p>An integer value to set the seed of the 'python' environment.
Default value is set to 'NULL'.</p>
</td></tr>
<tr><td><code id="mlm_classification_+3A_...">...</code></td>
<td>
<p>Additional arguments passed onto <a href="#topic+mlm_init">mlm_init</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'PyCaret' is a 'python' module where machine learning models can be fitted with
little coding by the user. The pipeline that 'PyCaret' uses is a
setup function to parameterize the data that is easy for all the models to
fit on. Then the compare models function is executed, which fits all the models
that are currently available. This process takes less than five minutes for
data.frame objects that are less than 10,000 rows.
</p>


<h3>Value</h3>

<p>A list object where the first entry is the models fitted and the
second is the initial predictive accuracy on the random test data. Returns
as two classes '&quot;mlm_stressor&quot;' and '&quot;classifier&quot;'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 lm_test &lt;- data_gen_lm(20)
 binary_response &lt;- sample(c(0, 1), 20, replace = TRUE)
 lm_test$Y &lt;- binary_response
 mlm_class &lt;- mlm_classification(Y ~ ., lm_test)

</code></pre>

<hr>
<h2 id='mlm_init'>Compare Machine Learning Models</h2><span id='topic+mlm_init'></span>

<h3>Description</h3>

<p>Through the <a href="https://pycaret.gitbook.io/docs/get-started/quickstart">PyCaret</a>
module from 'python', this function fits many machine
learning models simultaneously without requiring any 'python'
programming on the part of the user. This is the core function to fitting
the initial models. This function is the backbone to fitting all the models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlm_init(
  formula,
  train_data,
  fit_models,
  sort_v = NULL,
  n_models = 9999,
  classification = FALSE,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlm_init_+3A_formula">formula</code></td>
<td>
<p>The regression formula or classification formula. This formula
should be linear.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_train_data">train_data</code></td>
<td>
<p>A data.frame object that includes data to be trained on.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_fit_models">fit_models</code></td>
<td>
<p>A character vector with all the possible Machine Learning
regressors that are currently being fit. The user may specify a subset of
them using a character vector.
</p>

<table>
<tr>
 <td style="text-align: right;">
   ada </td><td style="text-align: left;"> AdaBoost Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   br </td><td style="text-align: left;"> Bayesian Ridge </td>
</tr>
<tr>
 <td style="text-align: right;">
   dt </td><td style="text-align: left;"> Decision Tree Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   dummy </td><td style="text-align: left;"> Dummy Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   en </td><td style="text-align: left;"> Elastic Net </td>
</tr>
<tr>
 <td style="text-align: right;">
   et </td><td style="text-align: left;"> Extra Trees Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   gbr </td><td style="text-align: left;"> Gradient Boosting Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   huber </td><td style="text-align: left;"> Huber Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   knn </td><td style="text-align: left;"> K Neighbors Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   lar </td><td style="text-align: left;"> Least Angle Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
   lasso </td><td style="text-align: left;"> Lasso Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
   lightgbm </td><td style="text-align: left;"> Light Gradient Boosting Machine </td>
</tr>
<tr>
 <td style="text-align: right;">
   llar </td><td style="text-align: left;"> Lasso Least Angle Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
   lr </td><td style="text-align: left;"> Linear Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
   omp </td><td style="text-align: left;"> Orthogonal Matching Pursuit </td>
</tr>
<tr>
 <td style="text-align: right;">
   par </td><td style="text-align: left;"> Passive Aggressive Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
   rf </td><td style="text-align: left;"> Random Forest Regressor</td>
</tr>
<tr>
 <td style="text-align: right;">
   ridge </td><td style="text-align: left;"> Ridge Regression
 </td>
</tr>

</table>

<p>If classification is set to 'TRUE', these models can be used depending on user.
These are the default values for classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
   ada </td><td style="text-align: left;"> AdaBoost Classifier </td>
</tr>
<tr>
 <td style="text-align: right;">
   dt </td><td style="text-align: left;"> Decision Tree Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   dummy </td><td style="text-align: left;"> Dummy Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   et </td><td style="text-align: left;"> Extra Trees Classifier </td>
</tr>
<tr>
 <td style="text-align: right;">
   gbc </td><td style="text-align: left;"> Gradient Boosting Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   knn </td><td style="text-align: left;"> K Neighbors Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   lda </td><td style="text-align: left;"> Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: right;">
   lightgbm </td><td style="text-align: left;"> Light Gradient Boosting Machine</td>
</tr>
<tr>
 <td style="text-align: right;">
   lr </td><td style="text-align: left;"> Logistic Regression</td>
</tr>
<tr>
 <td style="text-align: right;">
   nb </td><td style="text-align: left;"> Naive Bayes </td>
</tr>
<tr>
 <td style="text-align: right;">
   qda </td><td style="text-align: left;"> Quadratic Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: right;">
   rf </td><td style="text-align: left;"> Random Forest Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   ridge </td><td style="text-align: left;"> Ridge Classifier</td>
</tr>
<tr>
 <td style="text-align: right;">
   svm </td><td style="text-align: left;"> SVM - Linear Kernel
 </td>
</tr>

</table>
</td></tr>
<tr><td><code id="mlm_init_+3A_sort_v">sort_v</code></td>
<td>
<p>A character vector indicating what to sort the tuned models on.
Default value is 'NULL'.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_n_models">n_models</code></td>
<td>
<p>A defaulted integer to return the maximum number of models.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_classification">classification</code></td>
<td>
<p>A Boolean value tag to indicate if classification
methods should be used.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_seed">seed</code></td>
<td>
<p>An integer value to set the seed of the python environment.
Default value is set to 'NULL'.</p>
</td></tr>
<tr><td><code id="mlm_init_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the setup function in 'PyCaret'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula should be linear. However, that does not imply a linear
fit. The formula is a convenient way to separate predictor
variables from explanatory variables.
</p>
<p>'PyCaret' is a 'python' module where machine learning models can be fitted with
little coding by the user. The pipeline that 'PyCaret' uses has a
setup function to parameterize the data that is easy for all the models to
fit on. Then compare models function is executed which fits all the models
that are currently available. This process takes less than five minutes for
data.frame objects that are less than 10,000 rows.
</p>


<h3>Value</h3>

<p>A list object that contains all the fitted models and the CV
predictive accuracy. With a class attribute of '&quot;mlm_stressor&quot;'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 lm_test &lt;- data_gen_lm(20)
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_test)

</code></pre>

<hr>
<h2 id='mlm_refit'>Refit Machine Learning Models</h2><span id='topic+mlm_refit'></span>

<h3>Description</h3>

<p>Refits models fitted in the <a href="#topic+mlm_init">mlm_init</a>, and returns the
predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlm_refit(mlm_object, train_data, test_data, classification = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlm_refit_+3A_mlm_object">mlm_object</code></td>
<td>
<p>A '&quot;mlm_stressor&quot;' object.</p>
</td></tr>
<tr><td><code id="mlm_refit_+3A_train_data">train_data</code></td>
<td>
<p>A data.frame object used for refitting excludes the test
data. Can be 'NULL' to allow for predictions to be used on the current
model.</p>
</td></tr>
<tr><td><code id="mlm_refit_+3A_test_data">test_data</code></td>
<td>
<p>A data.frame object used for predictions.</p>
</td></tr>
<tr><td><code id="mlm_refit_+3A_classification">classification</code></td>
<td>
<p>A Boolean value used to represent if classification
methods need to be used to refit the data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with the predictions of the various machine learning
methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 lm_train &lt;- data_gen_lm(20)
 train_idx &lt;- sample.int(20, 5)
 train &lt;- lm_train[train_idx, ]
 test &lt;- lm_train[-train_idx, ]
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_train)
 mlm_refit(mlm_lm, train, test, classification = FALSE)

</code></pre>

<hr>
<h2 id='mlm_regressor'>Fit Machine Learning Regressor Models</h2><span id='topic+mlm_regressor'></span>

<h3>Description</h3>

<p>Through the <a href="https://pycaret.gitbook.io/docs/get-started/quickstart#regression">PyCaret</a>
module from 'python', this function fits many machine
learning models simultaneously with without requiring any 'python'
programming on the part of the user. This function is
specifically designed for the regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlm_regressor(
  formula,
  train_data,
  fit_models = c("ada", "et", "lightgbm", "gbr", "lr", "rf", "ridge", "knn", "dt",
    "dummy", "lar", "br", "huber", "omp", "lasso", "en", "llar", "par"),
  sort_v = c("MAE", "MSE", "RMSE", "R2", "RMSLE", "MAPE"),
  n_models = 9999,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlm_regressor_+3A_formula">formula</code></td>
<td>
<p>A linear formula object.</p>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_train_data">train_data</code></td>
<td>
<p>A data.frame object that includes data to be trained on.</p>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_fit_models">fit_models</code></td>
<td>
<p>A character vector with all the possible Machine Learning
regressors that are currently being fit. The user may specify a subset of
them using a character vector.
</p>

<table>
<tr>
 <td style="text-align: right;">
  ada </td><td style="text-align: left;"> AdaBoost Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  br </td><td style="text-align: left;"> Bayesian Ridge </td>
</tr>
<tr>
 <td style="text-align: right;">
  dt </td><td style="text-align: left;"> Decision Tree Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  dummy </td><td style="text-align: left;"> Dummy Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  en </td><td style="text-align: left;"> Elastic Net </td>
</tr>
<tr>
 <td style="text-align: right;">
  et </td><td style="text-align: left;"> Extra Trees Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  gbr </td><td style="text-align: left;"> Gradient Boosting Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  huber </td><td style="text-align: left;"> Huber Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  knn </td><td style="text-align: left;"> K Neighbors Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  lar </td><td style="text-align: left;"> Least Angle Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
  lasso </td><td style="text-align: left;"> Lasso Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
  lightgbm </td><td style="text-align: left;"> Light Gradient Boosting Machine </td>
</tr>
<tr>
 <td style="text-align: right;">
  llar </td><td style="text-align: left;"> Lasso Least Angle Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
  lr </td><td style="text-align: left;"> Linear Regression </td>
</tr>
<tr>
 <td style="text-align: right;">
  omp </td><td style="text-align: left;"> Orthogonal Matching Pursuit </td>
</tr>
<tr>
 <td style="text-align: right;">
  par </td><td style="text-align: left;"> Passive Aggressive Regressor </td>
</tr>
<tr>
 <td style="text-align: right;">
  rf </td><td style="text-align: left;"> Random Forest Regressor</td>
</tr>
<tr>
 <td style="text-align: right;">
  ridge </td><td style="text-align: left;"> Ridge Regression
</td>
</tr>

</table>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_sort_v">sort_v</code></td>
<td>
<p>A character vector indicating what to sort the tuned models on.</p>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_n_models">n_models</code></td>
<td>
<p>An integer value defaulted to a large integer value to
return all possible models.</p>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_seed">seed</code></td>
<td>
<p>An integer value to set the seed of the 'python' environment.
Default value is set to 'NULL'.</p>
</td></tr>
<tr><td><code id="mlm_regressor_+3A_...">...</code></td>
<td>
<p>Additional arguments passed onto <a href="#topic+mlm_init">mlm_init</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'PyCaret' is a 'python' module where machine learning models can be fitted with
little coding by the user. The pipeline that 'PyCaret' uses is a
setup function to parameterize the data that is easy for all the models to
fit on. Then the compare models function is executed, which fits all the models
that are currently available. This process takes less than five minutes for
data.frame objects that are less than 10,000 rows.
</p>


<h3>Value</h3>

<p>A list object where the first entry is the models fitted and the
second is the initial predictive accuracy on the random test data. Returns
as two classes '&quot;mlm_stressor&quot;' and '&quot;regressor&quot;'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 lm_test &lt;- data_gen_lm(20)
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_test)

</code></pre>

<hr>
<h2 id='predict'>Prediction Methods for Various Models</h2><span id='topic+predict'></span><span id='topic+predict.mlm_stressor'></span><span id='topic+predict.reg_asym'></span><span id='topic+predict.reg_sine'></span>

<h3>Description</h3>

<p>Predict values on 'mlm_stressor', 'reg_asym', or 'reg_sine'
objects. This expands the <a href="stats.html#topic+predict">predict</a> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mlm_stressor'
predict(object, newdata, train_data = NULL, ...)

## S3 method for class 'reg_asym'
predict(object, newdata, ...)

## S3 method for class 'reg_sine'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>A 'mlm_stressor', 'reg_asym', or 'reg_sine' object.</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p>A data.frame object that is the data to be predicted on.</p>
</td></tr>
<tr><td><code id="predict_+3A_train_data">train_data</code></td>
<td>
<p>A data.frame object defaulted to 'NULL'. This is only used
when an 'mlm_stressor' object needs to be refitted.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>Extending the <a href="stats.html#topic+predict">predict</a> function default. In this
case, it is ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of predictions if 'mlm_stressor' object or vector of
predicted values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 # mlm_stressor example
 lm_test &lt;- data_gen_lm(20)
 create_virtualenv()
 mlm_lm &lt;- mlm_regressor(Y ~ ., lm_test)
 predict(mlm_lm, lm_test)

 # Asymptotic Examples
 asym_data &lt;- data_gen_asym(10)
 asym_fit &lt;- reg_asym(Y ~ ., asym_data)
 predict(asym_fit, asym_data)
 # Sinusoidal Examples
 sine_data &lt;- data_gen_sine(10)
 sine_fit &lt;- reg_sine(Y ~ ., sine_data)
 predict(sine_fit, sine_data)
</code></pre>

<hr>
<h2 id='python_avail'>Check if 'Python' is Available</h2><span id='topic+python_avail'></span>

<h3>Description</h3>

<p>A function that allows examples to run when appropriate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>python_avail()
</code></pre>


<h3>Value</h3>

<p>A Boolean value is returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> python_avail()
</code></pre>

<hr>
<h2 id='reg_asym'>Asymptotic Regression</h2><span id='topic+reg_asym'></span>

<h3>Description</h3>

<p>A simple example of asymptotic regression that is in the form of
<code class="reqn">y = -e^{-x}</code> and is the sum of multiple of these exponential
functions with a common intercept term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_asym(
  formula,
  data,
  method = "BFGS",
  init_guess = rep(1, ncol(data) * 2 - 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_asym_+3A_formula">formula</code></td>
<td>
<p>A formula object to describe the relationship.</p>
</td></tr>
<tr><td><code id="reg_asym_+3A_data">data</code></td>
<td>
<p>The response and predictor variables.</p>
</td></tr>
<tr><td><code id="reg_asym_+3A_method">method</code></td>
<td>
<p>The method that is passed to the optim function. By default, it
is the BFGS method which uses a gradient.</p>
</td></tr>
<tr><td><code id="reg_asym_+3A_init_guess">init_guess</code></td>
<td>
<p>The initial parameter guesses for the optim function. By
default, it is all ones.</p>
</td></tr>
<tr><td><code id="reg_asym_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the optim function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A &quot;reg_asym&quot; object is returned which contains the results from the
optim function that was returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> asym_data &lt;- data_gen_asym(10)
 reg_asym(Y ~ ., asym_data)
</code></pre>

<hr>
<h2 id='reg_sine'>Sinusoidal Regression</h2><span id='topic+reg_sine'></span>

<h3>Description</h3>

<p>A simple example of sinusoidal regression that is in the form of
<code class="reqn">y = asin(b(x - c))</code> and is the sum of of multiple of these sine
functions with a common intercept term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_sine(
  formula,
  data,
  method = "BFGS",
  init_guess = rep(1, ncol(data) * 3 - 2),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_sine_+3A_formula">formula</code></td>
<td>
<p>A formula object to describe the relationship.</p>
</td></tr>
<tr><td><code id="reg_sine_+3A_data">data</code></td>
<td>
<p>The response and predictor variables.</p>
</td></tr>
<tr><td><code id="reg_sine_+3A_method">method</code></td>
<td>
<p>The method that is passed to the optim function. By default, it
is the BFGS method which uses a gradient.</p>
</td></tr>
<tr><td><code id="reg_sine_+3A_init_guess">init_guess</code></td>
<td>
<p>The initial parameter guesses for the optim function. By
default, it is all ones.</p>
</td></tr>
<tr><td><code id="reg_sine_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the optim function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A &quot;reg_sine&quot; object is returned which contains the results from the
optim function that was returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> sine_data &lt;- data_gen_sine(10)
 reg_sine(Y ~ ., sine_data)
</code></pre>

<hr>
<h2 id='rmse'>Root Mean Squarred Error (RMSE)</h2><span id='topic+rmse'></span>

<h3>Description</h3>

<p>A function to calculate the RMSE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse(predicted, observed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmse_+3A_predicted">predicted</code></td>
<td>
<p>A data.frame or vector object that is the same number of
rows or length as the length of observed values.</p>
</td></tr>
<tr><td><code id="rmse_+3A_observed">observed</code></td>
<td>
<p>A vector of the observed results.</p>
</td></tr>
</table>

<hr>
<h2 id='score'>Score Function for Metrics</h2><span id='topic+score'></span>

<h3>Description</h3>

<p>A score function takes the observed and predicted values and returns a
vector or data.frame of the various metrics that are reported from 'PyCaret'.
For regression, the following metrics are available: 'RMSE', 'MAE', 'MSE',
'R2', 'RMSLE', and 'MAPE'. For classification, the following metrics are
available:'Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'MCC', and 'Kappa'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score(observed, predicted, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_+3A_observed">observed</code></td>
<td>
<p>A vector of the observed results.</p>
</td></tr>
<tr><td><code id="score_+3A_predicted">predicted</code></td>
<td>
<p>A data.frame or vector object that is the same number of
rows or length as the length of observed values.</p>
</td></tr>
<tr><td><code id="score_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+score_classification">score_classification</a></code>, <code><a href="#topic+score_regression">score_regression</a></code>
</p>

<dl>
<dt><code>metrics</code></dt><dd><p>A character vector of the metrics to be fitted. This is
defaulted to be the metrics from 'PyCaret'.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with the various metrics reported.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lm_data &lt;- data_gen_lm(100)
indices &lt;- split_data_prob(lm_data, .2)
train &lt;- lm_data[!indices,]
test &lt;- lm_data[indices,]
model &lt;- lm(Y ~ ., train)
pred_lm &lt;- predict(model, test)
score(test$Y, pred_lm)
</code></pre>

<hr>
<h2 id='score_classification'>Score Function for Binary Classification</h2><span id='topic+score_classification'></span>

<h3>Description</h3>

<p>This function takes the observed and predicted values and computes metrics
that are found in 'PyCaret' such as: 'Accuracy', 'AUC', 'Recall', 'Prec.',
'F1', 'MCC', and 'Kappa'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_classification(
  observed,
  predicted,
  metrics = c("Accuracy", "AUC", "Recall", "Prec.", "F1", "MCC", "Kappa")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_classification_+3A_observed">observed</code></td>
<td>
<p>A vector of the observed results.</p>
</td></tr>
<tr><td><code id="score_classification_+3A_predicted">predicted</code></td>
<td>
<p>A data.frame or vector object that is the same number of
rows or length as the length of observed values.</p>
</td></tr>
<tr><td><code id="score_classification_+3A_metrics">metrics</code></td>
<td>
<p>A character vector of the metrics to be fitted. This is
defaulted to be the metrics from 'PyCaret'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector or data.frame of the methods and metrics.
</p>

<hr>
<h2 id='score_regression'>Score Function for Regression</h2><span id='topic+score_regression'></span>

<h3>Description</h3>

<p>This function takes the observed and predicted values and
computes metrics that are found in 'PyCaret' such as: 'RMSE', 'MAE', 'MSE',
'R2', 'RMSLE', and 'MAPE'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_regression(
  observed,
  predicted,
  metrics = c("RMSE", "MAE", "MSE", "R2", "RMSLE", "MAPE")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_regression_+3A_observed">observed</code></td>
<td>
<p>A vector of the observed results.</p>
</td></tr>
<tr><td><code id="score_regression_+3A_predicted">predicted</code></td>
<td>
<p>A data.frame or vector object that is the same number of
rows or length as the length of observed values.</p>
</td></tr>
<tr><td><code id="score_regression_+3A_metrics">metrics</code></td>
<td>
<p>A character vector of the metrics to be fitted. This is
defaulted to be the metrics from 'PyCaret'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector or data.frame of the methods and metrics.
</p>

<hr>
<h2 id='split_data_prob'>Create Train Index Set</h2><span id='topic+split_data_prob'></span>

<h3>Description</h3>

<p>This function takes in a data.frame object and the training size and returns
a logical vector indicating which entries to include.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_data_prob(data, test_prop)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_data_prob_+3A_data">data</code></td>
<td>
<p>A data.frame object used to determine the length of the vector.</p>
</td></tr>
<tr><td><code id="split_data_prob_+3A_test_prop">test_prop</code></td>
<td>
<p>A numeric that is between zero and one that represents the
proportion of observations to be included in the test data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical vector is returned that is the same length as the number of
rows of the data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  lm_data &lt;- data_gen_lm(10)
  indices &lt;- split_data_prob(lm_data, .8)
  train &lt;- lm_data[indices, ]
  test &lt;- lm_data[!indices, ]
</code></pre>

<hr>
<h2 id='thinning'>Thinning Algorithm for Models with Predict Function</h2><span id='topic+thinning'></span>

<h3>Description</h3>

<p>Fits various train size and test sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thinning(
  model,
  data,
  max = 0.95,
  min = 0.05,
  iter = 0.05,
  classification = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thinning_+3A_model">model</code></td>
<td>
<p>A model that is currently of class type &quot;reg_sine&quot;, &quot;reg_asym&quot;,
&quot;lm&quot;, or &quot;mlm_stressor&quot;.</p>
</td></tr>
<tr><td><code id="thinning_+3A_data">data</code></td>
<td>
<p>A data frame with all the data.</p>
</td></tr>
<tr><td><code id="thinning_+3A_max">max</code></td>
<td>
<p>A numeric value in (0, 1] and greater than 'min',
defaulted to .95.</p>
</td></tr>
<tr><td><code id="thinning_+3A_min">min</code></td>
<td>
<p>A numeric value in (0, 1) and less than 'max', defaulted to .05.</p>
</td></tr>
<tr><td><code id="thinning_+3A_iter">iter</code></td>
<td>
<p>A numeric value to indicate the step size, defaulted to .05.</p>
</td></tr>
<tr><td><code id="thinning_+3A_classification">classification</code></td>
<td>
<p>A Boolean value defaulted 'FALSE', used for
'mlm_classification'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of objects, where the first element is the RMSE values at each
iteration and the second element is the predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> lm_data &lt;- data_gen_lm(1000)
 lm_model &lt;- lm(Y ~ ., lm_data)
 thin_results &lt;- thinning(lm_model, lm_data)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
