<!DOCTYPE html><html><head><title>Help for package bnlearn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bnlearn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bnlearn-package'><p>Bayesian network structure learning, parameter learning and inference</p></a></li>
<li><a href='#alarm'><p>ALARM monitoring system (synthetic) data set</p></a></li>
<li><a href='#alpha.star'><p>Estimate the optimal imaginary sample size for BDe(u)</p></a></li>
<li><a href='#arc operations'><p>Drop, add or set the direction of an arc or an edge</p></a></li>
<li><a href='#arc.strength'><p>Measure arc strength</p></a></li>
<li><a href='#asia'><p>Asia (synthetic) data set by Lauritzen and Spiegelhalter</p></a></li>
<li><a href='#BF'><p>Bayes factor between two network structures</p></a></li>
<li><a href='#bn class'><p>The bn class structure</p></a></li>
<li><a href='#bn.boot'><p>Nonparametric bootstrap of Bayesian networks</p></a></li>
<li><a href='#bn.cv'><p>Cross-validation for Bayesian networks</p></a></li>
<li><a href='#bn.fit'><p>Fit the parameters of a Bayesian network</p></a></li>
<li><a href='#bn.fit class'><p>The bn.fit class structure</p></a></li>
<li><a href='#bn.fit plots'><p>Plot fitted Bayesian networks</p></a></li>
<li><a href='#bn.fit utilities'><p>Utilities to manipulate fitted Bayesian networks</p></a></li>
<li><a href='#bn.kcv class'><p>The bn.kcv class structure</p></a></li>
<li><a href='#bn.strength class'><p>The bn.strength class structure</p></a></li>
<li><a href='#ci.test'><p>Independence and conditional independence tests</p></a></li>
<li><a href='#clgaussian.test'><p>Synthetic (mixed) data set to test learning algorithms</p></a></li>
<li><a href='#compare'><p>Compare two or more different Bayesian networks</p></a></li>
<li><a href='#configs'><p>Construct configurations of discrete variables</p></a></li>
<li><a href='#constraint-based algorithms'><p>Constraint-based structure learning algorithms</p></a></li>
<li><a href='#coronary'><p>Coronary heart disease data set</p></a></li>
<li><a href='#cpdag'><p>Equivalence classes, moral graphs and consistent extensions</p></a></li>
<li><a href='#cpquery'><p>Perform conditional probability queries</p></a></li>
<li><a href='#data preprocessing'><p>Pre-process data to better learn Bayesian networks</p></a></li>
<li><a href='#dsep'><p>Test d-separation</p></a></li>
<li><a href='#foreign files utilities'><p>Read and write BIF, NET, DSC and DOT files</p></a></li>
<li><a href='#gaussian.test'><p>Synthetic (continuous) data set to test learning algorithms</p></a></li>
<li><a href='#gRain integration'><p>Import and export networks from the gRain package</p></a></li>
<li><a href='#graph enumeration'><p>Count graphs with specific characteristics</p></a></li>
<li><a href='#graph generation utilities'><p>Generate empty, complete or random graphs</p></a></li>
<li><a href='#graph integration'><p>Import and export networks from the graph package</p></a></li>
<li><a href='#graph utilities'><p>Utilities to manipulate graphs</p></a></li>
<li><a href='#graphviz.chart'><p>Plotting networks with probability bars</p></a></li>
<li><a href='#graphviz.plot'><p>Advanced Bayesian network plots</p></a></li>
<li><a href='#hailfinder'><p>The HailFinder weather forecast system (synthetic) data set</p></a></li>
<li><a href='#hybrid algorithms'><p>Hybrid structure learning algorithms</p></a></li>
<li><a href='#igraph integration'><p>Import and export networks from the igraph package</p></a></li>
<li><a href='#independence-tests'><p>Conditional independence tests</p></a></li>
<li><a href='#insurance'><p>Insurance evaluation network (synthetic) data set</p></a></li>
<li><a href='#KL'><p>Compute the distance between two fitted Bayesian networks</p></a></li>
<li><a href='#learning.test'><p>Synthetic (discrete) data set to test learning algorithms</p></a></li>
<li><a href='#lizards'><p>Lizards' perching behaviour data set</p></a></li>
<li><a href='#lm integration'><p>Produce lm objects from Bayesian networks</p></a></li>
<li><a href='#local discovery algorithms'><p>Local discovery structure learning algorithms</p></a></li>
<li><a href='#marks'><p>Examination marks data set</p></a></li>
<li><a href='#misc utilities'><p>Miscellaneous utilities</p></a></li>
<li><a href='#model string utilities'><p>Build a model string from a Bayesian network and vice versa</p></a></li>
<li><a href='#multivariate normal distribution'><p>Gaussian Bayesian networks and multivariate normals</p></a></li>
<li><a href='#naive.bayes'><p>Naive Bayes classifiers</p></a></li>
<li><a href='#network-classifiers'><p>Bayesian network Classifiers</p></a></li>
<li><a href='#network-scores'><p>Network scores</p></a></li>
<li><a href='#node operations'><p>Manipulate nodes in a graph</p></a></li>
<li><a href='#node ordering utilities'><p>Partial node orderings</p></a></li>
<li><a href='#pcalg integration'><p>Import and export networks from the pcalg package</p></a></li>
<li><a href='#plot.bn'><p>Plot a Bayesian network</p></a></li>
<li><a href='#plot.bn.strength'><p>Plot arc strengths derived from bootstrap</p></a></li>
<li><a href='#predict and impute'><p>Predict or impute missing data from a Bayesian network</p></a></li>
<li><a href='#rbn'><p>Simulate random samples from a given Bayesian network</p></a></li>
<li><a href='#ROCR integration'><p>Generating a prediction object for ROCR</p></a></li>
<li><a href='#score'><p>Score of the Bayesian network</p></a></li>
<li><a href='#score-based algorithms'><p>Score-based structure learning algorithms</p></a></li>
<li><a href='#single-node local discovery'><p>Discover the structure around a single node</p></a></li>
<li><a href='#strength.plot'><p>Arc strength plot</p></a></li>
<li><a href='#structural.em'><p>Structure learning from missing data</p></a></li>
<li><a href='#structure-learning'><p>Structure learning algorithms</p></a></li>
<li><a href='#test counter'><p>Manipulating the test counter</p></a></li>
<li><a href='#utilities for whitelists and blacklists'><p>Get or create whitelists and blacklists</p></a></li>
<li><a href='#whitelists-blacklists'><p>Whitelists and blacklists in structure learning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Network Structure Learning, Parameter Learning and
Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>4.9.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-15</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.3.0), methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>parallel, graph, Rgraphviz, igraph, lattice, gRbase, gRain
(&ge; 1.3-3), ROCR, Rmpfr, gmp</td>
</tr>
<tr>
<td>Author:</td>
<td>Marco Scutari [aut, cre], Tomi Silander [ctb], Robert Ness [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marco Scutari &lt;scutari@bnlearn.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian network structure learning, parameter learning and inference.
  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,
  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu
  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,
  Gaussian and conditional Gaussian networks, along with many score functions and
  conditional independence tests.
  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.
  Some utility functions (model comparison and manipulation, random data generation, arc
  orientation testing, simple and advanced plots) are included, as well as support for
  parameter estimation (maximum likelihood and Bayesian) and inference, conditional
  probability queries, cross-validation, bootstrap and model averaging.
  Development snapshots with the latest bugfixes are available from <a href="https://www.bnlearn.com/">https://www.bnlearn.com/</a>.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.bnlearn.com/">https://www.bnlearn.com/</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>USE_C17</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-15 11:07:37 UTC; fizban</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-15 13:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bnlearn-package'>Bayesian network structure learning, parameter learning and inference</h2><span id='topic+bnlearn-package'></span><span id='topic+bnlearn'></span>

<h3>Description</h3>

<p>Bayesian network structure learning (via constraint-based, score-based and
hybrid algorithms), parameter learning (via ML and Bayesian estimators) and
inference (via approximate inference algorithms).
</p>


<h3>Details</h3>

<p><span class="pkg">bnlearn</span> implements key algorithms covering all stages of Bayesian
network modelling: data preprocessing, structure learning combining data and
expert/prior knowledge, parameter learning, and inference (including causal
inference via do-calculus). <span class="pkg">bnlearn</span> aims to be a one-stop shop for
Bayesian networks in R, providing the tools needed for learning and working
with discrete Bayesian networks, Gaussian Bayesian networks and conditional
linear Gaussian Bayesian networks on real-world data. Incomplete data with
missing values are also supported. Furthermore the modular nature of
<span class="pkg">bnlearn</span> makes it easy to use it for simulation studies.
</p>
<p>Implemented structure learning algorithms include:
</p>

<ul>
<li> <p><em>Constraint-based algorithms</em>, which use conditional independence
tests to learn conditional independence constraints from data. The
constraints in turn are used to learn the structure of the Bayesian
network under the assumption that conditional independence implies
graphical separation (so, two variables that are independent cannot be
connected by an arc).
</p>
</li>
<li> <p><em>Score-based algorithms</em>, which are general-purpose optimization
algorithms that rank network structures with respect to a goodness-of-fit
score.
</p>
</li>
<li> <p><em>Hybrid algorithms</em> combine aspects of both constraint-based and
score-based algorithms, as they use conditional independence tests
(usually to reduce the search space) and network scores (to find the
optimal network in the reduced space) at the same time.
</p>
</li></ul>

<p>For more details about structure learning algorithms see
<a href="#topic+structure+20learning">structure learning</a>; available conditional independence tests are
described in <a href="#topic+independence+20tests">independence tests</a> and available network scores are
described in <a href="#topic+network+20scores">network scores</a>. Specialized algorithms to learn the
structure of Bayesian network classifiers are described in
<a href="#topic+network+20classifiers">network classifiers</a>. All algorithms support the use of whitelists and
blacklists to include and exclude arcs from the networks (see
<a href="#topic+whitelists+20and+20blacklists">whitelists and blacklists</a>); and many have parallel implementation
built on the <span class="pkg">parallel</span> package. Bayesian network scores support the use
of graphical priors.
</p>
<p>Parameter learning approaches include both frequentist and Bayesian
estimators. Inference is implemented using approximate algorithms via particle
filters approaches such as likelihood weighting, and covers conditional
probability queries, prediction and imputation.
</p>
<p>Additional facilities include support for bootstrap and cross-validation;
advanced plotting capabilities implemented on top of <span class="pkg">Rgraphviz</span> and
<span class="pkg">lattice</span>; model averaging; random graphs and random samples generation;
import/export functions to integrate <span class="pkg">bnlearn</span> with software such as
Hugin and GeNIe; an associated Bayesian network repository of golden-standard
networks at <a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>
<p>Use <code>citation("bnlearn")</code> to find out how to cite <span class="pkg">bnlearn</span> in
publications and other materials; and visit <a href="https://www.bnlearn.com/">https://www.bnlearn.com/</a> for
more examples and code from publications using <span class="pkg">bnlearn</span>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari<br />
Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA)<br />
</p>
<p>Maintainer: Marco Scutari <a href="mailto:scutari@bnlearn.com">scutari@bnlearn.com</a>
</p>


<h3>References</h3>

<p><b>reference books:</b>
</p>
<p>Koller D, Friedman N (2009). <em>Probabilistic Graphical Models: Principles
and Techniques</em>. MIT Press.
</p>
<p>Korb K, Nicholson AE (2010). <em>Bayesian Artificial Intelligence</em>.
Chapman &amp; Hall/CRC, 2nd edition.
</p>
<p>Pearl J (1988). <em>Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference</em>. Morgan Kaufmann.
</p>
<p><b>from the author:</b>
</p>
<p>Nagarajan R, Scutari M, Lebre S (2013). &quot;Bayesian Networks in R with
Applications in Systems Biology&quot;. Springer.
</p>
<p>Scutari M (2010). &quot;Learning Bayesian Networks with the bnlearn R Package&quot;.
<em>Journal of Statistical Software</em>, <strong>35</strong>(3):1&ndash;22.
</p>
<p>Scutari M (20107). &quot;Bayesian Network Constraint-Based Structure Learning
Algorithms: Parallel and Optimized Implementations in the bnlearn R
Package&quot;. <em>Journal of Statistical Software</em>, <strong>77</strong>(2):1&ndash;20.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## the workflow of Bayesian network modelling in bnlearn:
# choose the data set to work on...
data(learning.test)
# ... choose an algorithm and learn the structure of the network from the data...
net = hc(learning.test)
# ... plot it...
## Not run: graphviz.plot(net)
# ... learn the parameters of the network...
bn = bn.fit(net, learning.test)
# ... explore the network with a classic barchart...
## Not run: graphviz.chart(bn)
# ... and perform inference to answer any question that interests you!
cpquery(bn, event = (A == "a"), evidence = (C == "a"))
</code></pre>

<hr>
<h2 id='alarm'>ALARM monitoring system (synthetic) data set</h2><span id='topic+alarm'></span>

<h3>Description</h3>

<p>The ALARM (&quot;A Logical Alarm Reduction Mechanism&quot;) is a Bayesian network
designed to provide an alarm message system for patient monitoring.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(alarm)
</code></pre>


<h3>Format</h3>

<p>The <code>alarm</code> data set contains the following 37 variables:
</p>

<ul>
<li> <p><code>CVP</code> (<em>central venous pressure</em>): a three-level factor
with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>PCWP</code> (<em>pulmonary capillary wedge pressure</em>): a
three-level factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>HIST</code> (<em>history</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>TPR</code> (<em>total peripheral resistance</em>): a three-level
factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>BP</code> (<em>blood pressure</em>): a three-level factor with levels
<code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>CO</code> (<em>cardiac output</em>): a three-level factor with levels
<code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>HRBP</code> (<em>heart rate / blood pressure</em>): a three-level
factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>HREK</code> (<em>heart rate measured by an EKG monitor</em>): a
three-level factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>HRSA</code> (<em>heart rate / oxygen saturation</em>): a three-level
factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>PAP</code> (<em>pulmonary artery pressure</em>): a three-level factor
with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>SAO2</code> (<em>arterial oxygen saturation</em>): a three-level
factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>FIO2</code> (<em>fraction of inspired oxygen</em>): a two-level factor
with levels <code>LOW</code> and <code>NORMAL</code>.
</p>
</li>
<li> <p><code>PRSS</code> (<em>breathing pressure</em>): a four-level factor with
levels <code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>ECO2</code> (<em>expelled CO2</em>): a four-level factor with levels
<code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>MINV</code> (<em>minimum volume</em>): a four-level factor with levels
<code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>MVS</code> (<em>minimum volume set</em>): a three-level factor with
levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>HYP</code> (<em>hypovolemia</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>LVF</code> (<em>left ventricular failure</em>): a two-level factor with
levels <code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>APL</code> (<em>anaphylaxis</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>ANES</code> (<em>insufficient anesthesia/analgesia</em>): a two-level
factor with levels <code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>PMB</code> (<em>pulmonary embolus</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>INT</code> (<em>intubation</em>): a three-level factor with levels
<code>NORMAL</code>, <code>ESOPHAGEAL</code> and <code>ONESIDED</code>.
</p>
</li>
<li> <p><code>KINK</code> (<em>kinked tube</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>DISC</code> (<em>disconnection</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>LVV</code> (<em>left ventricular end-diastolic volume</em>): a
three-level factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>STKV</code> (<em>stroke volume</em>): a three-level factor with levels
<code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>CCHL</code> (<em>catecholamine</em>): a two-level factor with levels
<code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>ERLO</code> (<em>error low output</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>HR</code> (<em>heart rate</em>): a three-level factor with levels
<code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>ERCA</code> (<em>electrocauter</em>): a two-level factor with levels
<code>TRUE</code> and <code>FALSE</code>.
</p>
</li>
<li> <p><code>SHNT</code> (<em>shunt</em>): a two-level factor with levels
<code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>PVS</code> (<em>pulmonary venous oxygen saturation</em>): a three-level
factor with levels <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>ACO2</code> (<em>arterial CO2</em>): a three-level factor with levels
<code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>VALV</code> (<em>pulmonary alveoli ventilation</em>): a four-level
factor with levels <code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>VLNG</code> (<em>lung ventilation</em>): a four-level factor with
levels <code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>VTUB</code> (<em>ventilation tube</em>): a four-level factor with
levels <code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li>
<li> <p><code>VMCH</code> (<em>ventilation machine</em>): a four-level factor with
levels <code>ZERO</code>, <code>LOW</code>, <code>NORMAL</code> and <code>HIGH</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The complete BN can be downloaded from
<a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>


<h3>Source</h3>

<p>Beinlich I, Suermondt HJ, Chavez RM, Cooper GF (1989). &quot;The ALARM Monitoring
System: A Case Study with Two Probabilistic Inference Techniques for Belief
Networks&quot;. <em>Proceedings of the 2nd European Conference on Artificial
Intelligence in Medicine</em>, 247&ndash;256.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(alarm)
# create and plot the network structure.
modelstring = paste0("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
  "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
  "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
  "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
  "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG]",
  "[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]")
dag = model2network(modelstring)
## Not run: graphviz.plot(dag)
</code></pre>

<hr>
<h2 id='alpha.star'>Estimate the optimal imaginary sample size for BDe(u)</h2><span id='topic+alpha.star'></span>

<h3>Description</h3>

<p>Estimate the optimal value of the imaginary sample size for the BDe score,
assuming a uniform prior and given a network structure and a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  alpha.star(x, data, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha.star_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> (for <code>bn.fit</code> and <code>custom.fit</code>)
or an object of class <code>bn.fit</code> (for <code>bn.net</code>).</p>
</td></tr>
<tr><td><code id="alpha.star_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="alpha.star_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>alpha.star()</code> returns a positive number, the estimated optimal imaginary
sample size value.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Steck H (2008). &quot;Learning the Bayesian Network Structure: Dirichlet Prior
versus Data&quot;. <em>Proceedings of the 24th Conference on Uncertainty in
Artificial Intelligence</em>, 511&ndash;518.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test, score = "bic")

for (i in 1:3) {

  a = alpha.star(dag, learning.test)
  dag = hc(learning.test, score = "bde", iss = a)

}#FOR
</code></pre>

<hr>
<h2 id='arc+20operations'>Drop, add or set the direction of an arc or an edge</h2><span id='topic+arc+20operations'></span><span id='topic+set.arc'></span><span id='topic+drop.arc'></span><span id='topic+reverse.arc'></span><span id='topic+set.edge'></span><span id='topic+drop.edge'></span>

<h3>Description</h3>

<p>Drop, add or set the direction of a directed or undirected arc (also
known as edge).
</p>


<h3>Usage</h3>

<pre><code class='language-R'># arc operations.
set.arc(x, from, to, check.cycles = TRUE, check.illegal = TRUE, debug = FALSE)
drop.arc(x, from, to, debug = FALSE)
reverse.arc(x, from, to, check.cycles = TRUE, check.illegal = TRUE, debug = FALSE)

# edge (i.e. undirected arc) operations
set.edge(x, from, to, check.cycles = TRUE, check.illegal = TRUE, debug = FALSE)
drop.edge(x, from, to, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arc+2B20operations_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="arc+2B20operations_+3A_from">from</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="arc+2B20operations_+3A_to">to</code></td>
<td>
<p>a character string, the label of another node.</p>
</td></tr>
<tr><td><code id="arc+2B20operations_+3A_check.cycles">check.cycles</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the graph is tested for
acyclicity; otherwise the graph is returned anyway.</p>
</td></tr>
<tr><td><code id="arc+2B20operations_+3A_check.illegal">check.illegal</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> arcs that break the
parametric assumptions of <code>x</code>, such as those from continuous to
discrete nodes in conditional Gaussian networks, cause an error.</p>
</td></tr>
<tr><td><code id="arc+2B20operations_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>set.arc()</code> function operates in the following way:
</p>

<ul>
<li><p> if there is no arc between <code>from</code> and <code>to</code>, the arc
<code>from</code> <code class="reqn">\rightarrow</code> <code>to</code> is added.
</p>
</li>
<li><p> if there is an undirected arc between <code>from</code> and <code>to</code>, its
direction is set to <code>from</code> <code class="reqn">\rightarrow</code> <code>to</code>.
</p>
</li>
<li><p> if the arc <code>to</code> <code class="reqn">\rightarrow</code> <code>from</code> is present,
it is reversed.
</p>
</li>
<li><p> if the arc <code>from</code> <code class="reqn">\rightarrow</code> <code>to</code> is present, no
action is taken.
</p>
</li></ul>

<p>The <code>drop.arc()</code> function operates in the following way:
</p>

<ul>
<li><p> if there is no arc between <code>from</code> and <code>to</code>, no action is
taken.
</p>
</li>
<li><p> if there is a directed or an undirected arc between <code>from</code> and
<code>to</code>, it is dropped regardless of its direction.
</p>
</li></ul>

<p>The <code>reverse.arc()</code> function operates in the following way:
</p>

<ul>
<li><p> if there is no arc between <code>from</code> and <code>to</code>, it returns an
error.
</p>
</li>
<li><p> if there is an undirected arc between <code>from</code> and <code>to</code>, it
returns an error.
</p>
</li>
<li><p> if the arc <code>to</code> <code class="reqn">\rightarrow</code> <code>from</code> is present, it
is reversed.
</p>
</li>
<li><p> if the arc <code>from</code> <code class="reqn">\rightarrow</code> <code>to</code> is present, it
is reversed.
</p>
</li></ul>

<p>The <code>set.edge()</code> function operates in the following way:
</p>

<ul>
<li><p> if there is no arc between <code>from</code> and <code>to</code>, the undirected
arc <code>from</code> - <code>to</code> is added.
</p>
</li>
<li><p> if there is an undirected arc between <code>from</code> and <code>to</code>, no
action is taken.
</p>
</li>
<li><p> if either the arc <code>from</code> <code class="reqn">\rightarrow</code> <code>to</code> or the
arc <code>to</code> <code class="reqn">\rightarrow</code> <code>from</code> are present, they are
replaced with the undirected arc <code>from</code> - <code>to</code>.
</p>
</li></ul>

<p>The <code>drop.edge()</code> function operates in the following way:
</p>

<ul>
<li><p> if there is no undirected arc between <code>from</code> and <code>to</code>, no
action is taken.
</p>
</li>
<li><p> if there is an undirected arc between <code>from</code> and <code>to</code>, it
is removed.
</p>
</li>
<li><p> if there is a directed arc between <code>from</code> and <code>to</code>, no
action is taken.
</p>
</li></ul>



<h3>Value</h3>

<p>All functions return invisibly an updated copy of <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>dag = cpdag(model2network("[A][C][F][B|A][D|A:C][E|B:F]"))
dag

## use debug = TRUE to get more information.
updated = set.arc(dag, "A", "B")
updated
updated = drop.arc(dag, "A", "B")
updated
updated = drop.edge(dag, "A", "B")
updated
updated = reverse.arc(dag, "A", "D")
updated
</code></pre>

<hr>
<h2 id='arc.strength'>Measure arc strength</h2><span id='topic+arc.strength'></span><span id='topic+boot.strength'></span><span id='topic+custom.strength'></span><span id='topic+bf.strength'></span><span id='topic+mean.bn.strength'></span><span id='topic+averaged.network'></span><span id='topic+inclusion.threshold'></span>

<h3>Description</h3>

<p>Measure the strength of the probabilistic relationships expressed by the arcs
of a Bayesian network, and use model averaging to build a network containing
only the significant arcs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># strength of the arcs present in x.
arc.strength(x, data, criterion = NULL, ..., debug = FALSE)
# strength of all possible arcs, as learned from bootstrapped data.
boot.strength(data, cluster, R = 200, m = nrow(data),
  algorithm, algorithm.args = list(), cpdag = TRUE, shuffle = TRUE,
  debug = FALSE)
# strength of all possible arcs, from a list of custom networks.
custom.strength(networks, nodes, weights = NULL, cpdag = TRUE, debug = FALSE)
# strength of all possible arcs, computed using Bayes factors.
bf.strength(x, data, score, ..., debug = FALSE)

# average arc strengths.
## S3 method for class 'bn.strength'
mean(x, ..., weights = NULL)

# averaged network structure.
averaged.network(strength, threshold)
# strength threshold for inclusion in the averaged network structure.
inclusion.threshold(strength)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arc.strength_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.strength</code> (for <code>mean()</code>) or of
class <code>bn</code> (for all other functions).</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_networks">networks</code></td>
<td>
<p>a list, containing either object of class <code>bn</code> or arc
sets (matrices or data frames with two columns, optionally labeled &quot;from&quot;
and &quot;to&quot;); or an object of class <code>bn.kcv</code> or <code>bn.kcv.list</code>
from <code>bn.cv()</code>.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_data">data</code></td>
<td>
<p>a data frame containing the data the Bayesian network was
learned from (for <code>arc.strength()</code>) or that will be used to compute
the arc strengths (for <code>boot.strength()</code> and <code>bf.strength()</code>).</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_strength">strength</code></td>
<td>
<p>an object of class <code>bn.strength</code>, see below.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_threshold">threshold</code></td>
<td>
<p>a numeric value, the minimum strength required for an
arc to be included in the averaged network. The default value is the
<code>threshold</code> attribute of the <code>strength</code> argument.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_nodes">nodes</code></td>
<td>
<p>a vector of character strings, the labels of the nodes in the
network.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_criterion">criterion</code>, <code id="arc.strength_+3A_score">score</code></td>
<td>
<p>a character string. For <code>arc.strength()</code>, the
label of a score function or an independence test; see
<code><a href="#topic+network+20scores">network scores</a></code> for details.</p>
</td></tr></table>
<p> For <code>bf.strength()</code>, the
label of the score used to compute the Bayes factors; see <code><a href="#topic+BF">BF</a></code>
for details.
</p>
<table>
<tr><td><code id="arc.strength_+3A_r">R</code></td>
<td>
<p>a positive integer, the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_m">m</code></td>
<td>
<p>a positive integer, the size of each bootstrap replicate.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_weights">weights</code></td>
<td>
<p>a vector of non-negative numbers, to be used as weights
when averaging arc strengths (in <code>mean()</code>) or network structures (in
<code>custom.strength()</code>) to compute strength coefficients. If <code>NULL</code>,
weights are assumed to be uniform.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_cpdag">cpdag</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the (PDAG of) the equivalence
class is used instead of the network structure itself. It should make it
easier to identify score-equivalent arcs.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_shuffle">shuffle</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the columns in the data are
permuted in each bootstrap sample to enforce the fact that the ordering of
the variables in the data should be an invariant.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_algorithm">algorithm</code></td>
<td>
<p>a character string, the structure learning algorithm to be
applied to the bootstrap replicates. See <code><a href="#topic+structure+20learning">structure learning</a></code>
and the documentation of each algorithm for details.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_algorithm.args">algorithm.args</code></td>
<td>
<p>a list of extra arguments to be passed to the learning
algorithm.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_...">...</code></td>
<td>
<p>in <code>arc.strength()</code>, the additional tuning parameters for
the network score (if <code>criterion</code> is the label of a score function,
see <code><a href="#topic+score">score</a></code> for details), the conditional independence test
(currently the only one is <code>B</code>, the number of permutations). In
<code>mean()</code>, additional objects of class <code>bn.strength</code> to average.</p>
</td></tr>
<tr><td><code id="arc.strength_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>arc.strength()</code> computes a measure of confidence or strength for each
arc, while keeping fixed the rest of the network structure.
</p>
<p>If <code>criterion</code> is a conditional independence test, the strength is a
p-value (so the lower the value, the stronger the relationship). The
conditional independence test would be that to drop the arc from the network.
The only two possible additional arguments are <code>alpha</code>, which sets the
significance threshold that is used in <code>strength.plot()</code>; and <code>B</code>,
the number of permutations to be generated for each permutation test.
</p>
<p>If <code>criterion</code> is the label of a score function, the strength is
measured by the score gain/loss which would be caused by the arc's removal.
In other words, it is the difference between the score of the network
in which the arc is not present and the score of the network in which the arc
is present. Negative values correspond to decreases in the network score
and positive values correspond to increases in the network score (the stronger
the relationship, the more negative the difference). There may be additional
arguments depending on the choice of the score, see <code><a href="#topic+score">score</a></code> for
details. The significance threshold is set to <code>0</code>.
</p>
<p><code>boot.strength()</code> estimates the strength of each arc as its empirical
frequency over a set of networks learned from bootstrap samples. It computes
the probability of each arc (modulo its direction) and the probabilities of
each arc's directions conditional on the arc being present in the graph (in
either direction). The significance threshold is computed automatically from
the strength estimates.
</p>
<p><code>bf.strength()</code> estimates the strength of each arc using Bayes factors
to overcome the fact that Bayesian posterior scores are not normalised, and
uses the latter to estimate the probabilities of all possible states of an
arc given the rest of the network. The significance threshold is set to
<code>1</code>.
</p>
<p><code>custom.strength()</code> takes a list of networks and estimates arc strength
in the same way as <br /> <code>boot.strength()</code>.
</p>
<p>Model averaging is supported for objects of class <code>bn.strength</code> returned
by <code><a href="#topic+boot.strength">boot.strength</a></code>, <code><a href="#topic+custom.strength">custom.strength</a></code> and
<code><a href="#topic+bf.strength">bf.strength</a></code>. The returned network contains the arcs whose
strength is greater than the <code>threshold</code> attribute of the
<code>bn.strength</code> object passed to <code>averaged.network()</code>.
</p>


<h3>Value</h3>

<p><code>arc.strength()</code>, <code>boot.strength()</code>, <code>custom.strength()</code>,
<code>bf.strength()</code> and <code>mean()</code> return an object of class
<code>bn.strength</code>; <code>boot.strength()</code> and <code>custom.strength()</code> also
include information about the relative probabilities of arc directions.
</p>
<p><code>averaged.network()</code> returns an object of class <code>bn</code>.
</p>
<p>See <code><a href="#topic+bn.strength+20class">bn.strength class</a></code> and <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>


<h3>Note</h3>

<p><code>averaged.network()</code> typically returns a completely directed graph; an
arc can be undirected if and only if the probability of each of its directions
is exactly 0.5. This may happen, for example, if the arc is undirected in all
the networks being averaged.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p><b>for model averaging and boostrap strength (confidence):</b>
</p>
<p>Friedman N, Goldszmidt M, Wyner A (1999). &quot;Data Analysis with Bayesian
Networks: A Bootstrap Approach&quot;. <em>Proceedings of the 15th Annual
Conference on Uncertainty in Artificial Intelligence</em>, 196&ndash;201.
</p>
<p><b>for the computation of the bootstrap strength (confidence) significance
threshold:</b>
</p>
<p>Scutari M, Nagarajan R (2013). &quot;On Identifying Significant Edges in Graphical
Models of Molecular Networks&quot;. <em>Artificial Intelligence in Medicine</em>,
<strong>57</strong>(3):207&ndash;217.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+strength.plot">strength.plot</a></code>, <code><a href="#topic+score">score</a></code>,
<code><a href="#topic+ci.test">ci.test</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test)
arc.strength(dag, learning.test)

## Not run: 
arcs = boot.strength(learning.test, algorithm = "hc")
arcs[(arcs$strength &gt; 0.85) &amp; (arcs$direction &gt;= 0.5), ]
averaged.network(arcs)

start = random.graph(nodes = names(learning.test), num = 50)
netlist = lapply(start, function(net) {
  hc(learning.test, score = "bde", iss = 10, start = net) })
arcs = custom.strength(netlist, nodes = names(learning.test),
         cpdag = FALSE)
arcs[(arcs$strength &gt; 0.85) &amp; (arcs$direction &gt;= 0.5), ]
modelstring(averaged.network(arcs))

## End(Not run)

bf.strength(dag, learning.test, score = "bds", prior = "marginal")
</code></pre>

<hr>
<h2 id='asia'>Asia (synthetic) data set by Lauritzen and Spiegelhalter</h2><span id='topic+asia'></span>

<h3>Description</h3>

<p>Small synthetic data set from Lauritzen and Spiegelhalter (1988) about lung
diseases (tuberculosis, lung cancer or bronchitis) and visits to Asia.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(asia)
</code></pre>


<h3>Format</h3>

<p>The <code>asia</code> data set contains the following variables:
</p>

<ul>
<li> <p><code>D</code> (<em>dyspnoea</em>), a two-level factor with levels <code>yes</code>
and <code>no</code>.
</p>
</li>
<li> <p><code>T</code> (<em>tuberculosis</em>), a two-level factor with levels
<code>yes</code> and <code>no</code>.
</p>
</li>
<li> <p><code>L</code> (<em>lung cancer</em>), a two-level factor with levels
<code>yes</code> and <code>no</code>.
</p>
</li>
<li> <p><code>B</code> (<em>bronchitis</em>), a two-level factor with levels
<code>yes</code> and <code>no</code>.
</p>
</li>
<li> <p><code>A</code> (<em>visit to Asia</em>), a two-level factor with levels
<code>yes</code> and <code>no</code>.
</p>
</li>
<li> <p><code>S</code> (<em>smoking</em>), a two-level factor with levels <code>yes</code>
and <code>no</code>.
</p>
</li>
<li> <p><code>X</code> (<em>chest X-ray</em>), a two-level factor with levels
<code>yes</code> and <code>no</code>.
</p>
</li>
<li> <p><code>E</code> (<em>tuberculosis versus lung cancer/bronchitis</em>), a
two-level factor with levels <code>yes</code> and <code>no</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>Lauritzen and Spiegelhalter (1988) motivate this example as follows:
</p>
<p>&ldquo;Shortness-of-breath (dyspnoea) may be due to tuberculosis, lung
cancer or bronchitis, or none of them, or more than one of them. A recent
visit to Asia increases the chances of tuberculosis, while smoking is known
to be a risk factor for both lung cancer and bronchitis. The results of a
single chest X-ray do not discriminate between lung cancer and tuberculosis,
as neither does the presence or absence of dyspnoea.&rdquo;
</p>
<p>Standard learning algorithms are not able to recover the true structure of
the network because of the presence of a node (<code>E</code>) with conditional
probabilities equal to both 0 and 1. Monte Carlo tests seems to behave
better than their parametric counterparts.
</p>
<p>The complete BN can be downloaded from
<a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>


<h3>Source</h3>

<p>Lauritzen S, Spiegelhalter D (1988). &quot;Local Computation with Probabilities
on Graphical Structures and their Application to Expert Systems (with
discussion)&quot;. <em>Journal of the Royal Statistical Society: Series B</em>,
<strong>50</strong>(2):157&ndash;224.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(asia)
# create and plot the network structure.
dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
## Not run: graphviz.plot(dag)
</code></pre>

<hr>
<h2 id='BF'>Bayes factor between two network structures</h2><span id='topic+BF'></span>

<h3>Description</h3>

<p>Compute the Bayes factor between the structures of two Bayesian networks..
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BF(num, den, data, score, ..., log = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BF_+3A_num">num</code>, <code id="BF_+3A_den">den</code></td>
<td>
<p>two objects of class <code>bn</code>, corresponding to the numerator
and the denominator models in the Bayes factor.</p>
</td></tr>
<tr><td><code id="BF_+3A_data">data</code></td>
<td>
<p>a data frame containing the data to be used to compute the Bayes
factor.</p>
</td></tr>
<tr><td><code id="BF_+3A_score">score</code></td>
<td>
<p>a character string, the label of a posterior network score or
<code>custom</code> for the custom score. If none is specified, the default score
is the <em>Bayesian Dirichlet equivalent</em> score (<code>bde</code>) for discrete
networks and the <em>Bayesian Gaussian score</em> (<code>bge</code>) for Gaussian
networks. Other kinds of Bayesian networks are not currently supported.</p>
</td></tr>
<tr><td><code id="BF_+3A_...">...</code></td>
<td>
<p>extra tuning arguments for the posterior scores. See
<code><a href="#topic+score">score</a></code> for details.</p>
</td></tr>
<tr><td><code id="BF_+3A_log">log</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the Bayes factor is given as
log(BF).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value, the Bayes factor of the two network structures
<code>num</code> and <code>den</code>.
</p>


<h3>Note</h3>

<p>The Bayes factor for two network structures, by definition, is the ratio of
the respective marginal likelihoods which is equivalent to the ration of
the corresponding posterior probabilities if we assume the <code>uniform</code>
prior over all possible DAGs. However, note that it is possible to specify
different priors using the &ldquo;<code>...</code>&rdquo; arguments of <code>BF()</code>; in
that case the value returned by the function will not be the classic Bayes
factor.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+score">score</a></code>, <code><a href="#topic+compare">compare</a></code>,
<code><a href="#topic+bf.strength">bf.strength</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)

dag1 = model2network("[A][B][F][C|B][E|B][D|A:B:C]")
dag2 = model2network("[A][C][B|A][D|A][E|D][F|A:C:E]")
BF(dag1, dag2, learning.test, score = "bds", iss = 1)
</code></pre>

<hr>
<h2 id='bn+20class'>The bn class structure</h2><span id='topic+bn+20class'></span><span id='topic+bn-class'></span>

<h3>Description</h3>

<p>The structure of an object of S3 class <code>bn</code>.
</p>


<h3>Details</h3>

<p>An object of class <code>bn</code> is a list containing at least the following
components:
</p>

<ul>
<li> <p><code>learning</code>: a list containing some information about the results
of the learning algorithm. It's never changed afterward.
</p>

<ul>
<li> <p><code>whitelist</code>: a copy of the <code>whitelist</code> argument (a
two-column matrix, whose columns are labeled <code>from</code> and <code>to</code>)
as transformed by sanitization functions.
</p>
</li>
<li> <p><code>blacklist</code>: a copy of the <code>blacklist</code> argument (a
two-column matrix, whose columns are labeled <code>from</code> and <code>to</code>)
as transformed by sanitization functions.
</p>
</li>
<li> <p><code>test</code>: the label of the conditional independence test used by
the learning algorithm (a character string); the label of the network
score is used for score-based algorithms; the label of the network score
used in the &ldquo;Maximize&rdquo; phase of hybrid algorithms; &quot;none&quot; for
randomly generated graphs. For hybrid algorithms, <code>test</code> always
has the same value as <code>maxscore</code> (see below).
</p>
</li>
<li> <p><code>ntests</code>: the number of conditional independence tests or
score comparisons used in the learning (an integer value).
</p>
</li>
<li> <p><code>algo</code>: the label of the learning algorithm or the random
generation algorithm used to generate the network (a character string).
</p>
</li>
<li> <p><code>args</code>: a list. The values of the parameters of either the
conditional tests or the scores used in the learning process. Only the
relevant ones are stored, so this may be an empty list.
</p>

<ul>
<li> <p><code>alpha</code>: the target nominal type I error rate (a numeric
value) of the conditional independence tests.
</p>
</li>
<li> <p><code>iss</code>: a positive numeric value, the imaginary sample size
used by the <code>bge</code> and <code>bde</code> scores.
</p>
</li>
<li> <p><code>k</code>: a positive numeric value, the penalty coefficient
used by the <code>aic</code>, <code>aic-g</code>, <code>aic-cg</code>, <code>bic</code>,
<code>bic-g</code>, <code>bic-cg</code>, <code>pnal</code>, <code>pnal-g</code> and
<code>pnal-cg</code> scores.
</p>
</li>
<li> <p><code>prob</code>: the probability of each arc to be present in a
graph generated by the <code>ordered</code> graph generation algorithm.
</p>
</li>
<li> <p><code>burn.in</code>: the number of iterations for the <code>ic-dag</code>
graph generation algorithm to converge to a stationary (and uniform)
probability distribution.
</p>
</li>
<li> <p><code>max.degree</code>: the maximum degree for any node in a graph
generated by the <code>ic-dag</code> graph generation algorithm.
</p>
</li>
<li> <p><code>max.in.degree</code>: the maximum in-degree for any node in a
graph generated by the <code>ic-dag</code> graph generation algorithm.
</p>
</li>
<li> <p><code>max.out.degree</code>: the maximum out-degree for any node in
a graph generated by the <code>ic-dag</code> graph generation algorithm.
</p>
</li>
<li> <p><code>training</code>: a character string, the label of the training
node in a Bayesian network classifier.
</p>
</li>
<li> <p><code>threshold</code>: the threshold used to determine which arcs
are significant when averaging network structures.
</p>
</li>
<li> <p><code>prior</code>: the graphical prior used in combination with a
Bayesian score such as <code>bde</code> or <code>bge</code>.
</p>
</li>
<li> <p><code>beta</code>: the parameters of the graphical prior.
</p>
</li></ul>

</li></ul>

</li>
<li> <p><code>nodes</code>: a list. Each element is named after a node and contains
the following elements:
</p>

<ul>
<li> <p><code>mb</code>: the Markov blanket of the node (a vector of character
strings).
</p>
</li>
<li> <p><code>nbr</code>: the neighbourhood of the node (a vector of character
strings).
</p>
</li>
<li> <p><code>parents</code>: the parents of the node (a vector of character
strings).
</p>
</li>
<li> <p><code>children</code>: the children of the node (a vector of character
strings).
</p>
</li></ul>

</li>
<li> <p><code>arcs</code>: the arcs of the Bayesian network (a two-column matrix,
whose columns are labeled <code>from</code> and <code>to</code>). Undirected arcs
are stored as two directed arcs with opposite directions between the
corresponding incident nodes.
</p>
</li></ul>

<p>Additional (optional) components under <code>learning</code>:
</p>

<ul>
<li> <p><code>optimized</code>: whether additional optimizations have been used in
the learning algorithm (a boolean value).
</p>
</li>
<li> <p><code>illegal</code>: arcs that are illegal according to the parametric
assumptions used to learn the network structure (a two-column matrix,
whose columns are labeled <code>from</code> and <code>to</code>).
</p>
</li>
<li> <p><code>restrict</code>: the label of the constraint-based algorithm used in
the &ldquo;Restrict&rdquo; phase of a hybrid learning algorithm (a character
string).
</p>
</li>
<li> <p><code>rtest</code>: the label of the conditional independence test used in
the &ldquo;Restrict&rdquo; phase of a hybrid learning algorithm (a character
string).
</p>
</li>
<li> <p><code>maximize</code>: the label of the score-based algorithm used in the
&ldquo;Maximize&rdquo; phase of a hybrid learning algorithm (a character
string).
</p>
</li>
<li> <p><code>maxscore</code>: the label of the network score used in the
&ldquo;Maximize&rdquo; phase of a hybrid learning algorithm (a character
string).
</p>
</li>
<li> <p><code>max.sx</code>: the maximum allowed size of the conditioning sets
in the conditional independence tests used in constraint-based algorithms.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='bn.boot'>Nonparametric bootstrap of Bayesian networks</h2><span id='topic+bn.boot'></span>

<h3>Description</h3>

<p>Apply a user-specified function to the Bayesian network structures learned
from bootstrap samples of the original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bn.boot(data, statistic, R = 200, m = nrow(data), algorithm,
  algorithm.args = list(), statistic.args = list(), cluster,
  debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bn.boot_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_statistic">statistic</code></td>
<td>
<p>a function or a character string (the name of a function)
to be applied to each bootstrap replicate.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_r">R</code></td>
<td>
<p>a positive integer, the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_m">m</code></td>
<td>
<p>a positive integer, the size of each bootstrap replicate.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_algorithm">algorithm</code></td>
<td>
<p>a character string, the learning algorithm to be applied
to the bootstrap replicates. See <code><a href="#topic+structure+20learning">structure learning</a></code> and the
documentation of each algorithm for details.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_algorithm.args">algorithm.args</code></td>
<td>
<p>a list of extra arguments to be passed to the learning
algorithm.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_statistic.args">statistic.args</code></td>
<td>
<p>a list of extra arguments to be passed to the function
specified by <code>statistic</code>.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="bn.boot_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first argument of <code>statistic</code> is the <code>bn</code> object encoding the
network structure learned from the bootstrap sample; the arguments specified
in <code>statistics.args</code> are extracted from the list and passed to
<code>statitstics</code> as the 2nd, 3rd, etc. arguments.
</p>


<h3>Value</h3>

<p>A list containing the results of the calls to <code>statistic</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Friedman N, Goldszmidt M, Wyner A (1999). &quot;Data Analysis with Bayesian
Networks: A Bootstrap Approach&quot;. <em>Proceedings of the 15th Annual
Conference on Uncertainty in Artificial Intelligence</em>, 196&ndash;201.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bn.cv">bn.cv</a></code>, <code><a href="#topic+rbn">rbn</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(learning.test)
bn.boot(data = learning.test, R = 2, m = 500, algorithm = "gs",
  statistic = arcs)

## End(Not run)
</code></pre>

<hr>
<h2 id='bn.cv'>Cross-validation for Bayesian networks</h2><span id='topic+bn.cv'></span><span id='topic+plot.bn.kcv'></span><span id='topic+plot.bn.kcv.list'></span><span id='topic+loss'></span>

<h3>Description</h3>

<p>Perform a k-fold or hold-out cross-validation for a learning algorithm or a
fixed network structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bn.cv(data, bn, loss = NULL, ..., algorithm.args = list(),
  loss.args = list(), fit, fit.args = list(), method = "k-fold",
  cluster, debug = FALSE)

## S3 method for class 'bn.kcv'
plot(x, ..., main, xlab, ylab, connect = FALSE)
## S3 method for class 'bn.kcv.list'
plot(x, ..., main, xlab, ylab, connect = FALSE)

loss(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bn.cv_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_bn">bn</code></td>
<td>
<p>either a character string (the label of the learning algorithm to
be applied to the training data in each iteration) or an object of class
<code>bn</code> (a fixed network structure).</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_loss">loss</code></td>
<td>
<p>a character string, the label of a loss function. If none is
specified, the default loss function is the <em>Classification Error</em>
for Bayesian networks classifiers; otherwise, the <em>Log-Likelihood
Loss</em> for both discrete and continuous data sets. See below for
additional details.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_algorithm.args">algorithm.args</code></td>
<td>
<p>a list of extra arguments to be passed to the learning
algorithm.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_loss.args">loss.args</code></td>
<td>
<p>a list of extra arguments to be passed to the loss function
specified by <code>loss</code>.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_fit">fit</code></td>
<td>
<p>a character string, the label of the method used to fit the
parameters of the network. See <code><a href="#topic+bn.fit">bn.fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_fit.args">fit.args</code></td>
<td>
<p>additional arguments for the parameter estimation procedure,
see again <code><a href="#topic+bn.fit">bn.fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_method">method</code></td>
<td>
<p>a character string, either <code>k-fold</code>, <code>custom-folds</code>
or <code>hold-out</code>. See below for details.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.kcv</code> or <code>bn.kcv.list</code> returned by
<code>bn.cv()</code>.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_...">...</code></td>
<td>
<p>additional objects of class <code>bn.kcv</code> or <code>bn.kcv.list</code>
to plot alongside the first.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_main">main</code>, <code id="bn.cv_+3A_xlab">xlab</code>, <code id="bn.cv_+3A_ylab">ylab</code></td>
<td>
<p>the title of the plot, an array of labels for the
boxplot, the label for the y axis.</p>
</td></tr>
<tr><td><code id="bn.cv_+3A_connect">connect</code></td>
<td>
<p>a logical value. If <code>TRUE</code>, the medians points in the
boxplots will be connected by a segmented line.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>bn.cv()</code> returns an object of class <code>bn.kcv.list</code> if <code>runs</code>
is at least 2, an object of class <code>bn.kcv</code> if <code>runs</code> is equal to 1.
</p>
<p><code>loss()</code> returns a numeric vector with a length equal to <code>runs</code>.
</p>


<h3>Cross-Validation Strategies</h3>

<p>The following cross-validation methods are implemented:
</p>

<ul>
<li> <p><em>k-fold</em>: the <code>data</code> are split in <code>k</code> subsets of equal
size. For each subset in turn, <code>bn</code> is fitted (and possibly learned
as well) on the other <code>k - 1</code> subsets and the loss function is then
computed using that subset. Loss estimates for each of the <code>k</code>
subsets are then combined to give an overall loss for <code>data</code>.
</p>
</li>
<li> <p><em>custom-folds</em>: the data are manually partitioned by the user
into subsets, which are then used as in k-fold cross-validation. Subsets
are not constrained to have the same size, and every observation must be
assigned to one subset.
</p>
</li>
<li> <p><em>hold-out</em>: <code>k</code> subsamples of size <code>m</code> are sampled
independently without replacement from the <code>data</code>. For each subsample,
<code>bn</code> is fitted (and possibly learned) on the remaining
<code>m - nrow(data)</code> samples and the loss function is computed on the
<code>m</code> observations in the subsample. The overall loss estimate is the
average of the <code>k</code> loss estimates from the subsamples.
</p>
</li></ul>

<p>If cross-validation is used with multiple <code>runs</code>, the overall loss is the
averge of the loss estimates from the different runs.
</p>
<p>To clarify, cross-validation methods accept the following optional arguments:
</p>

<ul>
<li> <p><code>k</code>: a positive integer number, the number of groups into which the
data will be split (in k-fold cross-validation) or the number of times
the data will be split in training and test samples (in hold-out
cross-validation).
</p>
</li>
<li> <p><code>m</code>: a positive integer number, the size of the test set in
hold-out cross-validation.
</p>
</li>
<li> <p><code>runs</code>: a positive integer number, the number of times
k-fold or hold-out cross-validation will be run.
</p>
</li>
<li> <p><code>folds</code>: a list in which element corresponds to one fold and
contains the indices for the observations that are included to that fold;
or a list with an element for each run, in which each element is itself a
list of the folds to be used for that run.
</p>
</li></ul>



<h3>Loss Functions</h3>

<p>The following loss functions are implemented:
</p>

<ul>
<li> <p><em>Log-Likelihood Loss</em> (<code>logl</code>): also known as <em>negative
entropy</em> or <em>negentropy</em>, it is the negated expected log-likelihood
of the test set for the Bayesian network fitted from the training set.
Lower valuer are better.
</p>
</li>
<li> <p><em>Gaussian Log-Likelihood Loss</em> (<code>logl-g</code>): the negated
expected log-likelihood for Gaussian Bayesian networks. Lower values are
better.
</p>
</li>
<li> <p><em>Classification Error</em> (<code>pred</code>): the <em>prediction error</em>
for a single node in a discrete network. Frequentist predictions are used,
so the values of the target node are predicted using only the information
present in its local distribution (from its parents). Lower values are
better.
</p>
</li>
<li> <p><em>Posterior Classification Error</em> (<code>pred-lw</code> and
<code>pred-lw-cg</code>): similar to the above, but predictions are computed
from an arbitrary set of nodes using likelihood weighting to obtain
Bayesian posterior estimates. <code>pred-lw</code> applies to discrete Bayesian
networks, <code>pred-lw-cg</code> to (discrete nodes in) hybrid networks. Lower
values are better.
</p>
</li>
<li> <p><em>Exact Classification Error</em> (<code>pred-exact</code>): closed-form
exact posterior predictions are available for Bayesian network
classifiers. Lower values are better.
</p>
</li>
<li> <p><em>Predictive Correlation</em> (<code>cor</code>): the <em>correlation</em>
between the observed and the predicted values for a single node in a
Gaussian Bayesian network. Higher values are better.
</p>
</li>
<li> <p><em>Posterior Predictive Correlation</em> (<code>cor-lw</code> and
<code>cor-lw-cg</code>): similar to the above, but predictions are computed from
an arbitrary set of nodes using likelihood weighting to obtain Bayesian
posterior estimates. <code>cor-lw</code> applies to Gaussian networks and
<code>cor-lw-cg</code> to (continuous nodes in) hybrid networks. Higher values
are better.
</p>
</li>
<li> <p><em>Mean Squared Error</em> (<code>mse</code>): the <em>mean squared error</em>
between the observed and the predicted values for a single node in a
Gaussian Bayesian network. Lower values are better.
</p>
</li>
<li> <p><em>Posterior Mean Squared Error</em> (<code>mse-lw</code> and
<code>mse-lw-cg</code>): similar to the above, but predictions are computed from
an arbitrary set of nodes using likelihood weighting to obtain Bayesian
posterior estimates. <code>mse-lw</code> applies to Gaussian networks and
<code>mse-lw-cg</code> to (continuous nodes in) hybrid networks. Lower values
are better.
</p>
</li></ul>

<p>Optional arguments that can be specified in <code>loss.args</code> are:
</p>

<ul>
<li> <p><code>target</code>: a character string, the label of target node for
prediction in all loss functions but <code>logl</code>, <code>logl-g</code> and
<code>logl-cg</code>.
</p>
</li>
<li> <p><code>from</code>: a vector of character strings, the labels of the nodes
used to predict the <code>target</code> node in <code>pred-lw</code>, <code>pred-lw-cg</code>,
<code>cor-lw</code>, <code>cor-lw-cg</code>, <code>mse-lw</code> and <code>mse-lw-cg</code>. The
default is to use all the other nodes in the network. Loss functions
<code>pred</code>, <code>cor</code> and <code>mse</code> implicitly predict only from the
parents of the <code>target</code> node.
</p>
</li>
<li> <p><code>n</code>: a positive integer, the number of particles used by
likelihood weighting for <code>pred-lw</code>, <code>pred-lw-cg</code>, <code>cor-lw</code>,
<code>cor-lw-cg</code>, <code>mse-lw</code> and <code>mse-lw-cg</code>.
The default value is <code>500</code>.
</p>
</li></ul>

<p>Note that if <code>bn</code> is a Bayesian network classifier, <code>pred</code> and
<code>pred-lw</code> both give exact posterior predictions computed using the
closed-form formulas for naive Bayes and TAN.
</p>


<h3>Plotting Results from Cross-Validation</h3>

<p>Both plot methods accept any combination of objects of class <code>bn.kcv</code> or
<code>bn.kcv.list</code> (the first as the <code>x</code> argument, the remaining as the
<code>...</code> argument) and plot the respected expected loss values side by side.
For a <code>bn.kcv</code> object, this mean a single point; for a <code>bn.kcv.list</code>
object this means a boxplot.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Koller D, Friedman N (2009). <em>Probabilistic Graphical Models: Principles
and Techniques</em>. MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bn.boot">bn.boot</a></code>, <code><a href="#topic+rbn">rbn</a></code>, <code><a href="#topic+bn.kcv-class">bn.kcv-class</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>bn.cv(learning.test, 'hc', loss = "pred", loss.args = list(target = "F"))

folds = list(1:2000, 2001:3000, 3001:5000)
bn.cv(learning.test, 'hc', loss = "logl", method = "custom-folds",
  folds = folds)

xval = bn.cv(gaussian.test, 'mmhc', method = "hold-out",
         k = 5, m = 50, runs = 2)
xval
loss(xval)

## Not run: 
# comparing algorithms with multiple runs of cross-validation.
gaussian.subset = gaussian.test[1:50, ]
cv.gs = bn.cv(gaussian.subset, 'gs', runs = 10)
cv.iamb = bn.cv(gaussian.subset, 'iamb', runs = 10)
cv.inter = bn.cv(gaussian.subset, 'inter.iamb', runs = 10)
plot(cv.gs, cv.iamb, cv.inter,
  xlab = c("Grow-Shrink", "IAMB", "Inter-IAMB"), connect = TRUE)

# use custom folds.
folds = split(sample(nrow(gaussian.subset)), seq(5))
bn.cv(gaussian.subset, "hc", method = "custom-folds", folds = folds)

# multiple runs, with custom folds.
folds = replicate(5, split(sample(nrow(gaussian.subset)), seq(5)),
          simplify = FALSE)
bn.cv(gaussian.subset, "hc", method = "custom-folds", folds = folds)

## End(Not run)</code></pre>

<hr>
<h2 id='bn.fit'>Fit the parameters of a Bayesian network</h2><span id='topic+bn.fit'></span><span id='topic+custom.fit'></span><span id='topic+bn.net'></span><span id='topic++24+3C-.bn.fit'></span>

<h3>Description</h3>

<p>Fit, assign or replace the parameters of a Bayesian network conditional on
its structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bn.fit(x, data, cluster, method, ..., keep.fitted = TRUE,
  debug = FALSE)
custom.fit(x, dist, ordinal, debug = FALSE)
bn.net(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bn.fit_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> (for <code>bn.fit()</code> and
<code>custom.fit()</code>) or an object of class <code>bn.fit</code> (for
<code>bn.net</code>).</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_dist">dist</code></td>
<td>
<p>a named list, with element for each node of <code>x</code>. See below.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_method">method</code></td>
<td>
<p>a character string, see below for details.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_...">...</code></td>
<td>
<p>additional arguments for the parameter estimation procedure, see
below.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_ordinal">ordinal</code></td>
<td>
<p>a vector of character strings, the labels of the discrete
nodes which should be saved as ordinal random variables
(<code>bn.fit.onode</code>) instead of unordered  factors (<code>bn.fit.dnode</code>).</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_keep.fitted">keep.fitted</code></td>
<td>
<p>a boolean value. If <code>TRUE</code>, the object returned by
<code>bn.fit</code> will contain fitted values and residuals for all Gaussian and
conditional Gaussian nodes, and the configurations of the discrete parents
for conditional Gaussian nodes.</p>
</td></tr>
<tr><td><code id="bn.fit_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bn.fit()</code> fits the parameters of a Bayesian network given its structure
and a data set; <code>bn.net</code> returns the structure underlying a fitted
Bayesian network.
</p>
<p><code>bn.fit()</code> accepts data with missing values encoded as <code>NA</code>. If the
parameter estimation method was not specifically designed to deal with
incomplete data, <code>bn.fit()</code> uses locally complete observations to fit the
parameters of each local distribution.
</p>
<p>Available methods for <em>discrete Bayesian networks</em> are:
</p>

<ul>
<li> <p><code>mle</code>: the maximum likelihood estimator for conditional
probabilities.
</p>
</li>
<li> <p><code>bayes</code>: the classic Bayesian posterior estimator with a uniform
prior matching that in the Bayesian Dirichlet equivalent (<code>bde</code>)
score.
</p>
</li>
<li> <p><code>hdir</code>: the hierarchical Dirichlet posterior estimator for
related data sets from Azzimonti, Corani and Zaffalon (2019).
</p>
</li>
<li> <p><code>hard-em</code>: the Expectation-Maximization implementation of the
estimators above.
</p>
</li></ul>

<p>Available methods for <em>hybrid Bayesian networks</em> are:
</p>

<ul>
<li> <p><code>mle-g</code>: the maximum likelihood estimator for least squares
regression models.
</p>
</li>
<li> <p><code>hard-em-g</code>: the Expectation-Maximization implementation of the
estimators above.
</p>
</li></ul>

<p>Available methods for <em>discrete Bayesian networks</em> are:
</p>

<ul>
<li> <p><code>mle-cg</code>: a combination of the maximum likelihood estimators
<code>mle</code> and <code>mle-g</code>.
</p>
</li>
<li> <p><code>hard-em-cg</code>: the Expectation-Maximization implementation of the
estimators above.
</p>
</li></ul>

<p>Additional arguments for the <code>bn.fit()</code> function:
</p>

<ul>
<li> <p><code>iss</code>: a numeric value, the imaginary sample size used by the
<code>bayes</code> method to estimate the conditional probability tables
associated with discrete nodes (see <code><a href="#topic+score">score</a></code> for details).
</p>
</li>
<li> <p><code>replace.unidentifiable</code>: a boolean value. If <code>TRUE</code> and
<code>method</code> one of <code>mle</code>, <code>mle-g</code> or <code>mle-cg</code>,
unidentifiable parameters are replaced by zeroes (in the case of
regression coefficients and standard errors in Gaussian and conditional
Gaussian nodes) or by uniform conditional probabilities (in discrete
nodes).
</p>
<p>If <code>FALSE</code> (the default), the conditional probabilities in the local
distributions of discrete nodes have a mximum likelihood estimate of
<code>NaN</code> for all parents configurations that are not observed in
<code>data</code>. Similarly, regression coefficients are set to <code>NA</code>
if the linear regressions correspoding to the local distributions of
continuous nodes are singular. Such missing values propagate to the
results of functions such as <code>predict()</code>.
</p>
</li>
<li> <p><code>alpha0</code>: a positive number, the amount of information pooling
between the related data sets in the <code>hdir</code> estimator.
</p>
</li>
<li> <p><code>group</code>: a character string, the label of the node with the
grouping of the observations into the related data sets in the <code>hdir</code>
estimator.
</p>
</li>
<li> <p><code>impute</code> and <code>impute.args</code>: a character string, the label of
the imputation method (and its arguments) used by <code>hard-em</code>,
<code>hard-em-g</code> and <code>hard-em-cg</code> to complete the data in the
<em>expectation</em> step. The default method is the same as for
<code>impute()</code>.
</p>
</li>
<li> <p><code>fit</code> and <code>fit.args</code>: a character string, the label of the
parameter estimation method used by <code>hard-em</code>, <code>hard-em-g</code> and
<code>hard-em-cg</code> to estimate the parameters in the <em>maximization</em>
step. The default method is the same as for <code>bn.fit()</code>.
</p>
</li>
<li> <p><code>threshold</code>: a positive numeric value, the minimum improvement
threshold used to step iterating in <code>hard-em</code>, <code>hard-em-g</code> and
<code>hard-em-cg</code>. The threshold is defined as the relative likelihood
improvement divided by the sample size of <code>data</code>, and defaults to
<code>1e-3</code>.
</p>
</li>
<li> <p><code>max.iter</code>: a positive integer value, the maximum number of
iterations in <code>hard-em</code>, <code>hard-em-g</code> and <code>hard-em-cg</code>. The
default value is <code>5</code>.
</p>
</li>
<li> <p><code>start</code>: a <code>bn.fit</code> object, the fitted network used to
initialize the <code>hard-em</code>, <code>hard-em-g</code> and <code>hard-em-cg</code>
estimators. The default is to use the <code>bn.fit</code> object obtained from
<code>x</code> with the default parameter estimator for the data, which will use
locally complete data to fit the local distributions.
</p>
</li>
<li> <p><code>newdata</code>: a data frame, a separate set of data used to assess
the convergence of the <code>hard-em</code>, <code>hard-em-g</code> and
<code>hard-em-cg</code> estimators. The data in <code>data</code> are used by default
for this purpose.
</p>
</li></ul>

<p>An in-place replacement method is available to change the parameters of each
node in a <code>bn.fit</code> object; see the examples for discrete, continuous and
hybrid networks below. For a discrete node (class <code>bn.fit.dnode</code> or
<code>bn.fit.onode</code>), the new parameters must be in a <code>table</code> object.
For a Gaussian node (class <code>bn.fit.gnode</code>), the new parameters can be
defined either by an <code>lm</code>, <code>glm</code> or <code>pensim</code> object (the
latter is from the <code>penalized</code> package) or in a list with elements named
<code>coef</code>, <code>sd</code> and optionally <code>fitted</code> and <code>resid</code>. For
a conditional Gaussian node (class <code>bn.fit.cgnode</code>), the new parameters
can be defined by a list with elements named <code>coef</code>, <code>sd</code> and
optionally <code>fitted</code>, <code>resid</code> and <code>configs</code>. In both cases
<code>coef</code> should contain the new regression coefficients, <code>sd</code> the
standard deviation of the residuals, <code>fitted</code> the fitted values and
<code>resid</code> the residuals. <code>configs</code> should contain the configurations
if the discrete parents of the conditional Gaussian node, stored as a factor.
</p>
<p><code>custom.fit()</code> takes a set of user-specified distributions and their
parameters and uses them to build a <code>bn.fit</code> object. Its purpose is to
specify a Bayesian network (complete with the parameters, not only the
structure) using knowledge from experts in the field instead of learning it
from a data set. The distributions must be passed to the function in a list,
with elements named after the nodes of the network structure <code>x</code>. Each
element of the list must be in one of the formats described above for
in-place replacement.
</p>


<h3>Value</h3>

<p><code>bn.fit()</code> and <code>custom.fit()</code>returns an object of class
<code>bn.fit</code>, <code>bn.net()</code> an object of class <code>bn</code>. See
<code><a href="#topic+bn+20class">bn class</a></code> and <code><a href="#topic+bn.fit+20class">bn.fit class</a></code> for details.
</p>


<h3>Note</h3>

<p>Due to the way Bayesian networks are defined it is possible to estimate their
parameters only if the network structure is completely directed (i.e. there
are no undirected arcs). See <code><a href="#topic+set.arc">set.arc</a></code> and <code><a href="#topic+cextend">cextend</a></code>
for two ways of manually setting the direction of one or more arcs.
</p>
<p>In the case of maximum likelihood estimators, <code>bn.fit()</code> produces
<code>NA</code> parameter estimates for discrete and conditional Gaussian nodes when
there are (discrete) parents configurations that are not observed in
<code>data</code>. To avoid this either set <code>replace.unidentifiable</code> to
<code>TRUE</code> or, in the case of discrete networks, use <code>method = "bayes"</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Azzimonti L, Corani G, Zaffalon M (2019). &quot;Hierarchical Estimation of
Parameters in Bayesian Networks&quot;.  <em>Computational Statistics &amp; Data
Analysis</em>, <strong>137</strong>:67&ndash;91.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bn.fit+20utilities">bn.fit utilities</a></code>, <code><a href="#topic+bn.fit+20plots">bn.fit plots</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)

# learn the network structure.
cpdag = pc.stable(learning.test)
# set the direction of the only undirected arc, A - B.
dag = set.arc(cpdag, "A", "B")
# estimate the parameters of the Bayesian network.
fitted = bn.fit(dag, learning.test)
# replace the parameters of the node B.
new.cpt = matrix(c(0.1, 0.2, 0.3, 0.2, 0.5, 0.6, 0.7, 0.3, 0.1),
            byrow = TRUE, ncol = 3,
            dimnames = list(B = c("a", "b", "c"), A = c("a", "b", "c")))
fitted$B = as.table(new.cpt)
# the network structure is still the same.
all.equal(dag, bn.net(fitted))

# learn the network structure.
dag = hc(gaussian.test)
# estimate the parameters of the Bayesian network.
fitted = bn.fit(dag, gaussian.test)
# replace the parameters of the node F.
fitted$F = list(coef = c(1, 2, 3, 4, 5), sd = 3)
# set again the original parameters
fitted$F = lm(F ~ A + D + E + G, data = gaussian.test)

# discrete Bayesian network from expert knowledge.
dag = model2network("[A][B][C|A:B]")
cptA = matrix(c(0.4, 0.6), ncol = 2, dimnames = list(NULL, c("LOW", "HIGH")))
cptB = matrix(c(0.8, 0.2), ncol = 2, dimnames = list(NULL, c("GOOD", "BAD")))
cptC = c(0.5, 0.5, 0.4, 0.6, 0.3, 0.7, 0.2, 0.8)
dim(cptC) = c(2, 2, 2)
dimnames(cptC) = list("C" = c("TRUE", "FALSE"), "A" =  c("LOW", "HIGH"),
                   "B" = c("GOOD", "BAD"))
cfit = custom.fit(dag, dist = list(A = cptA, B = cptB, C = cptC))
# for ordinal nodes it is nearly the same.
cfit = custom.fit(dag, dist = list(A = cptA, B = cptB, C = cptC),
         ordinal = c("A", "B"))

# Gaussian Bayesian network from expert knowledge.
distA = list(coef = c("(Intercept)" = 2), sd = 1)
distB = list(coef = c("(Intercept)" = 1), sd = 1.5)
distC = list(coef = c("(Intercept)" = 0.5, "A" = 0.75, "B" = 1.32), sd = 0.4)
cfit = custom.fit(dag, dist = list(A = distA, B = distB, C = distC))

# conditional Gaussian Bayesian network from expert knowledge.
cptA = matrix(c(0.4, 0.6), ncol = 2, dimnames = list(NULL, c("LOW", "HIGH")))
distB = list(coef = c("(Intercept)" = 1), sd = 1.5)
distC = list(coef = matrix(c(1.2, 2.3, 3.4, 4.5), ncol = 2,
               dimnames = list(c("(Intercept)", "B"), NULL)),
          sd = c(0.3, 0.6))
cgfit = custom.fit(dag, dist = list(A = cptA, B = distB, C = distC))
</code></pre>

<hr>
<h2 id='bn.fit+20class'>The bn.fit class structure</h2><span id='topic+bn.fit+20class'></span><span id='topic+bn.fit-class'></span><span id='topic+bn.fit.dnode'></span><span id='topic+bn.fit.gnode'></span>

<h3>Description</h3>

<p>The structure of an object of S3 class <code>bn.fit</code>.
</p>


<h3>Details</h3>

<p>An object of class <code>bn.fit</code> is a list whose elements correspond to the
nodes of the Bayesian network. If the latter is discrete (i.e. the nodes are
multinomial random variables), the object also has class <code>bn.fit.dnet</code>;
each node has class <code>bn.fit.dnode</code> and contains the following elements:
</p>

<ul>
<li> <p><code>node</code>: a character string, the label of the node.
</p>
</li>
<li> <p><code>parents</code>: a vector of character strings, the labels of the
parents of the node.
</p>
</li>
<li> <p><code>children</code>: a vector of character strings, the labels of the
children of the node.
</p>
</li>
<li> <p><code>prob</code>: a (multi)dimensional numeric table, the conditional
probability table of the node given its parents.
</p>
</li></ul>

<p>Nodes encoding ordinal variables (i.e. ordered factors) have class
<code>bn.fit.onode</code> and contain the same elements as <code>bn.fit.dnode</code>
nodes. Networks containing only ordinal nodes also have class
<code>bn.fit.onet</code>, while those containing both ordinal and multinomial
nodes also have class <code>bn.fit.donet</code>.
</p>
<p>If on the other hand the network is continuous (i.e. the nodes are Gaussian
random variables), the object also has class <code>bn.fit.gnet</code>; each node
has class <code>bn.fit.gnode</code> and contains the following elements:
</p>

<ul>
<li> <p><code>node</code>: a character string, the label of the node.
</p>
</li>
<li> <p><code>parents</code>: a vector of character strings, the labels of the
parents of the node.
</p>
</li>
<li> <p><code>children</code>: a vector of character strings, the labels of the
children of the node.
</p>
</li>
<li> <p><code>coefficients</code>: a numeric vector, the linear regression
coefficients of the parents against the node.
</p>
</li>
<li> <p><code>residuals</code>: a numeric vector, the residuals of the linear
regression.
</p>
</li>
<li> <p><code>fitted.values</code>: a numeric vector, the fitted mean values of
the linear regression.
</p>
</li>
<li> <p><code>sd</code>: a numeric value, the standard deviation of the residuals
(i.e. the standard error).
</p>
</li></ul>

<p>Hybrid (i.e. conditional linear Gaussian) networks also have class
<code>bn.fit.gnet</code>. Gaussian nodes have class <code>bn.fit.gnode</code>, discrete
nodes have class <code>bn.fit.dnode</code> and conditional Gaussian nodes have
class <code>bn.fit.cgnode</code>. Each node contains the following elements:
</p>

<ul>
<li> <p><code>node</code>: a character string, the label of the node.
</p>
</li>
<li> <p><code>parents</code>: a vector of character strings, the labels of the
parents of the node.
</p>
</li>
<li> <p><code>children</code>: a vector of character strings, the labels of the
children of the node.
</p>
</li>
<li> <p><code>dparents</code>: an integer vector, the indexes of the discrete
parents in <code>parents</code>.
</p>
</li>
<li> <p><code>gparents</code>: an integer vector, the indexes of the continuous
parents in <code>parents</code>.
</p>
</li>
<li> <p><code>dlevels</code>: a list containing the levels of the discrete
parents in <code>parents</code>.
</p>
</li>
<li> <p><code>coefficients</code>: a numeric matrix, the linear regression
coefficients of the continuous parents. Each column corresponds to
a configuration of the discrete parents.
</p>
</li>
<li> <p><code>residuals</code>: a numeric vector, the residuals of the linear
regression.
</p>
</li>
<li> <p><code>fitted.values</code>: a numeric vector, the fitted mean values of the
linear regression.
</p>
</li>
<li> <p><code>configs</code>: an integer vector, the indexes of the configurations
of the discrete parents.
</p>
</li>
<li> <p><code>sd</code>: a numeric vector, the standard deviation of the residuals
(i.e. the standard error) for each configuration of the discrete parents.
</p>
</li></ul>

<p>Furthermore, Bayesian network classifiers store the label of the training node
in an additional attribute named <code>training</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='bn.fit+20plots'>Plot fitted Bayesian networks</h2><span id='topic+bn.fit+20plots'></span><span id='topic+bn.fit.qqplot'></span><span id='topic+bn.fit.xyplot'></span><span id='topic+bn.fit.histogram'></span><span id='topic+bn.fit.barchart'></span><span id='topic+bn.fit.dotplot'></span>

<h3>Description</h3>

<p>Plot functions for the <code>bn.fit</code>, <code>bn.fit.dnode</code> and
<code>bn.fit.gnode</code> classes, based on the <span class="pkg">lattice</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## for Gaussian Bayesian networks.
bn.fit.qqplot(fitted, xlab = "Theoretical Quantiles",
  ylab = "Sample Quantiles", main, ...)
bn.fit.histogram(fitted, density = TRUE, xlab = "Residuals",
  ylab = ifelse(density, "Density", ""), main, ...)
bn.fit.xyplot(fitted, xlab = "Fitted values", ylab = "Residuals", main, ...)
## for discrete (multinomial and ordinal) Bayesian networks.
bn.fit.barchart(fitted, xlab = "Probabilities", ylab = "Levels", main, ...)
bn.fit.dotplot(fitted, xlab = "Probabilities", ylab = "Levels", main, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bn.fit+2B20plots_+3A_fitted">fitted</code></td>
<td>
<p>an object of class <code>bn.fit</code>, <code>bn.fit.dnode</code> or
<code>bn.fit.gnode</code>.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20plots_+3A_xlab">xlab</code>, <code id="bn.fit+2B20plots_+3A_ylab">ylab</code>, <code id="bn.fit+2B20plots_+3A_main">main</code></td>
<td>
<p>the label of the x axis, of the y axis, and the plot
title.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20plots_+3A_density">density</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the histogram is plotted using
relative frequencies, and the matching normal density is added to the plot.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20plots_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <span class="pkg">lattice</span> functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bn.fit.qqplot()</code> draws a quantile-quantile plot of the residuals.
</p>
<p><code>bn.fit.histogram()</code> draws a histogram of the residuals, using either
absolute or relative frequencies.
</p>
<p><code>bn.fit.xyplot()</code> plots the residuals versus the fitted values.
</p>
<p><code>bn.fit.barchart()</code> and <code>bn.fit.dotplot</code> plot the probabilities in
the conditional probability table associated with each node.
</p>


<h3>Value</h3>

<p>The <span class="pkg">lattice</span> plot objects. Note that if auto-printing is turned off (for
example when the code is loaded with the <code>source</code> function), the return
value must be printed explicitly for the plot to be displayed.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+bn.fit">bn.fit</a></code>, <code><a href="#topic+bn.fit+20class">bn.fit class</a></code>.
</p>

<hr>
<h2 id='bn.fit+20utilities'>Utilities to manipulate fitted Bayesian networks</h2><span id='topic+bn.fit+20utilities'></span><span id='topic+sigma'></span><span id='topic+fitted.bn.fit'></span><span id='topic+coef.bn.fit'></span><span id='topic+residuals.bn.fit'></span><span id='topic+sigma.bn.fit'></span><span id='topic+logLik.bn.fit'></span><span id='topic+AIC.bn.fit'></span><span id='topic+BIC.bn.fit'></span><span id='topic+fitted.bn.fit.gnode'></span><span id='topic+coef.bn.fit.gnode'></span><span id='topic+residuals.bn.fit.gnode'></span><span id='topic+sigma.bn.fit.gnode'></span><span id='topic+fitted.bn.fit.dnode'></span><span id='topic+coef.bn.fit.dnode'></span><span id='topic+residuals.bn.fit.dnode'></span><span id='topic+coef.bn.fit.onode'></span><span id='topic+fitted.bn.fit.cgnode'></span><span id='topic+coef.bn.fit.cgnode'></span><span id='topic+residuals.bn.fit.cgnode'></span><span id='topic+sigma.bn.fit.cgnode'></span>

<h3>Description</h3>

<p>Assign, extract or compute various quantities of interest from an object of
class <code>bn.fit</code>, <code>bn.fit.dnode</code>, <code>bn.fit.gnode</code>,
<code>bn.fit.cgnode</code> or <code>bn.fit.onode</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## methods available for "bn.fit"
## S3 method for class 'bn.fit'
fitted(object, ...)
## S3 method for class 'bn.fit'
coef(object, ...)
## S3 method for class 'bn.fit'
residuals(object, ...)
## S3 method for class 'bn.fit'
sigma(object, ...)
## S3 method for class 'bn.fit'
logLik(object, data, nodes, by.sample = FALSE, na.rm = FALSE, debug = FALSE, ...)
## S3 method for class 'bn.fit'
AIC(object, data, ..., k = 1)
## S3 method for class 'bn.fit'
BIC(object, data, ...)

## methods available for "bn.fit.dnode"
## S3 method for class 'bn.fit.dnode'
coef(object, for.parents, ...)

## methods available for "bn.fit.onode"
## S3 method for class 'bn.fit.onode'
coef(object, for.parents, ...)

## methods available for "bn.fit.gnode"
## S3 method for class 'bn.fit.gnode'
fitted(object, ...)
## S3 method for class 'bn.fit.gnode'
coef(object, ...)
## S3 method for class 'bn.fit.gnode'
residuals(object, ...)
## S3 method for class 'bn.fit.gnode'
sigma(object, ...)

## methods available for "bn.fit.cgnode"
## S3 method for class 'bn.fit.cgnode'
fitted(object, ...)
## S3 method for class 'bn.fit.cgnode'
coef(object, for.parents, ...)
## S3 method for class 'bn.fit.cgnode'
residuals(object,  ...)
## S3 method for class 'bn.fit.cgnode'
sigma(object, for.parents, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bn.fit+2B20utilities_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn.fit</code>, <code>bn.fit.dnode</code>,
<code>bn.fit.gnode</code>, <code>bn.fit.cgnode</code> or <code>bn.fit.onode</code>.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_nodes">nodes</code></td>
<td>
<p>a vector of character strings, the label of a nodes whose
log-likelihood components are to be computed.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_...">...</code></td>
<td>
<p>additional arguments, currently ignored.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_k">k</code></td>
<td>
<p>a numeric value, the penalty coefficient to be used; the default
<code>k = 1</code> gives the expression used to compute AIC.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_by.sample">by.sample</code></td>
<td>
<p>a boolean value. If <code>TRUE</code>, <code>logLik()</code> returns
a vector containing the the log-likelihood of each observations in the
sample. If <code>FALSE</code>, <code>logLik()</code> returns a single value, the
likelihood of the whole sample.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_na.rm">na.rm</code></td>
<td>
<p>a boolean value, whether missing values should be used in
computing the log-likelihood. See below for details. The default value is
<code>FALSE</code>, and it only has an effect if <code>by.sample = FALSE</code>.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="bn.fit+2B20utilities_+3A_for.parents">for.parents</code></td>
<td>
<p>a named list in which each element contains a set of values
for the discrete parents of the nodes. <code>codef()</code> and <code>sigma()</code>
will only return the parameters associated with those parent configurations.
(Only relevant for conditional Gaussian nodes.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>coef()</code> (and its alias <code>coefficients()</code>) extracts model
coefficients (which are conditional probabilities for discrete nodes and
linear regression coefficients for Gaussian and conditional Gaussian nodes).
</p>
<p><code>residuals()</code> (and its alias <code>resid()</code>) extracts model residuals and
<code>fitted()</code> (and its alias <br /> <code>fitted.values()</code>) extracts fitted
values from Gaussian and conditional Gaussian nodes. If the <code>bn.fit</code>
object does not include the residuals or the fitted values for the node of
interest both functions return <code>NULL</code>.
</p>
<p><code>sigma()</code> extracts the standard deviations of the residuals from Gaussian
and conditional Gaussian networks and nodes.
</p>
<p><code>logLik()</code> returns the log-likelihood for the observations in
<code>data</code>. If <code>na.rm</code> is set to <code>TRUE</code>, the log-likelihood will be
<code>NA</code> if the data contain missing values. If <code>na.rm</code> is set to
<code>FALSE</code>, missing values will be dropped and the log-likelihood will be
computed using only locally-complete observations (effectively returning the
node-average log-likelihood times the sample size). Note that the
log-likelihood may be <code>NA</code> even if <code>na.rm = TRUE</code> if the network
contains <code>NA</code> parameters or is singular.
</p>
<p>The <code>for.parents</code> argument in the methods for <code>coef()</code> and
<code>sigma()</code> can be used to have both functions return the parameters
associated with a specific configuration of the discrete parents of a node.
If <code>for.parents</code> is not specified, all relevant parameters are returned.
</p>


<h3>Value</h3>

<p><code>logLik()</code> returns a numeric vector or a single numeric value, depending
on the value of <code>by.sample</code>. <code>AIC</code> and <code>BIC</code> always return a
single numeric value.
</p>
<p>All the other functions return a list with an element for each node in the
network (if <code>object</code> has class <code>bn.fit</code>) or a numeric vector or
matrix (if <code>object</code> has class <code>bn.fit.dnode</code>, <code>bn.fit.gnode</code>,
<code>bn.fit.cgnode</code> or <code>bn.fit.onode</code>).
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+bn.fit">bn.fit</a></code>, <code><a href="#topic+bn.fit-class">bn.fit-class</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gaussian.test)
dag = hc(gaussian.test)
fitted = bn.fit(dag, gaussian.test)
coefficients(fitted)
coefficients(fitted$C)
str(residuals(fitted))

data(learning.test)
dag2 = hc(learning.test)
fitted2 = bn.fit(dag2, learning.test)
coefficients(fitted2$E)
coefficients(fitted2$E, for.parents = list(F = "a", B = "b"))
</code></pre>

<hr>
<h2 id='bn.kcv+20class'>The bn.kcv class structure</h2><span id='topic+bn.kcv+20class'></span><span id='topic+bn.kcv-class'></span><span id='topic+bn.kcv.list+20class'></span><span id='topic+bn.kcv.list-class'></span>

<h3>Description</h3>

<p>The structure of an object of S3 class <code>bn.kcv</code> or <code>bn.kcv.list</code>.
</p>


<h3>Details</h3>

<p>An object of class <code>bn.kcv.list</code> is a list whose elements are objects
of class <code>bn.kcv</code>.
</p>
<p>An object of class <code>bn.kcv</code> is a list whose elements correspond to the
iterations of a k-fold cross-validation. Each element contains the following
objects:
</p>

<ul>
<li> <p><code>test</code>: an integer vector, the indexes of the observations
used as a test set.
</p>
</li>
<li> <p><code>fitted</code>: an object of class <code>bn.fit</code>, the Bayesian network
fitted from the training set.
</p>
</li>
<li> <p><code>learning</code>: the <code>learning</code> element of the <code>bn</code> object
that was used for parameter learning from the training set (either learned
from the training set as well or specified by the user).
</p>
</li>
<li> <p><code>loss</code>: the value of the loss function.
</p>
</li></ul>

<p>If the loss function requires to predict values from the test sets, each
element also contains:
</p>

<ul>
<li> <p><code>predicted</code>: a factor or a numeric vector, the predicted values
for the target node in the test set.
</p>
</li>
<li> <p><code>observed</code>: a factor or a numeric vector, the observed values
for the target node in the test set.
</p>
</li></ul>

<p>In addition, an object of class <code>bn.kcv</code> has the following attributes:
</p>

<ul>
<li> <p><code>loss</code>: a character string, the label of the loss function.
</p>
</li>
<li> <p><code>mean</code>: the mean of the values of the loss function computed in
the <code>k</code> iterations of the cross-validation, which is printed as the
&quot;expected loss&quot; or averaged to compute the &quot;average loss over the runs&quot;.
</p>
</li>
<li> <p><code>bn</code>: either a character string (the label of the learning
algorithm to be applied to the training data in each iteration) or an
object of class <code>bn</code> (a fixed network structure).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='bn.strength+20class'>The bn.strength class structure</h2><span id='topic+bn.strength+20class'></span><span id='topic+bn.strength-class'></span><span id='topic+bn.strength'></span>

<h3>Description</h3>

<p>The structure of an object of S3 class <code>bn.strength</code>.
</p>


<h3>Details</h3>

<p>An object of class <code>bn.strength</code> is a data frame with the following
columns (one row for each arc):
</p>

<ul>
<li> <p><code>from, to</code>: the nodes incident on the arc.
</p>
</li>
<li> <p><code>strength</code>: the strength of the arc. See
<code><a href="#topic+arc.strength">arc.strength</a></code>, <code><a href="#topic+boot.strength">boot.strength</a></code>,
<code><a href="#topic+custom.strength">custom.strength</a></code> and <code><a href="#topic+strength.plot">strength.plot</a></code>
for details.
</p>
</li></ul>

<p>and some additional attributes:
</p>

<ul>
<li> <p><code>nodes</code>: a vector of character strings, the labels of the nodes
of the network(s) the strength were computed from.
</p>
</li>
<li> <p><code>method</code>: a character string, the method used to compute the
strength coefficients. It can be equal to <code>test</code>, <code>score</code> or
<code>bootstrap</code>.
</p>
</li>
<li> <p><code>threshold</code>: a numeric value, the threshold used to determine
if a strength coefficient is significant.
</p>
</li></ul>

<p>An optional column called <code>direction</code> may also be present, giving the
probability of the direction of an arc given its presence in the graph.
</p>
<p>Only the <code>plot()</code> method is defined for this class; therefore, it can
be manipulated as a standard data frame.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='ci.test'>Independence and conditional independence tests</h2><span id='topic+ci.test'></span>

<h3>Description</h3>

<p>Perform an independence or a conditional independence test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci.test(x, y, z, data, test, B, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci.test_+3A_x">x</code></td>
<td>
<p>a character string (the name of a variable), a data frame, a numeric
vector or a factor object.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_y">y</code></td>
<td>
<p>a character string (the name of another variable), a numeric vector
or a factor object.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_z">z</code></td>
<td>
<p>a vector of character strings (the names of the conditioning
variables), a numeric vector, a factor object or a data frame. If
<code>NULL</code> an independence test will be executed.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables to be tested.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_test">test</code></td>
<td>
<p>a character string, the label of the conditional independence
test to be used in the algorithm. If none is specified, the default test
statistic is the <em>mutual information</em> for categorical variables, the
Jonckheere-Terpstra test for ordered factors and the <em>linear
correlation</em> for continuous variables. See <code><a href="#topic+independence+20tests">independence tests</a></code>
for details.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_b">B</code></td>
<td>
<p>a positive integer, the number of permutations considered for each
permutation test. It will be ignored with a warning if the conditional
independence test specified by the <code>test</code> argument is not a
permutation test.</p>
</td></tr>
<tr><td><code id="ci.test_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>htest</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value the test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate chi-squared or t
distribution of the test statistic; the number of permutations computed by
Monte Carlo tests. Semiparametric tests have both.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the type of test performed, and
whether Monte Carlo simulation or continuity correction was used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the value of the test statistic under the null hypothesis,
always 0.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative hypothesis.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+independence+20tests">independence tests</a></code>, <code><a href="#topic+arc.strength">arc.strength</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gaussian.test)
data(learning.test)

# using a data frame and column labels.
ci.test(x = "F" , y = "B", z = c("C", "D"), data = gaussian.test)
# using a data frame.
ci.test(gaussian.test)
# using factor objects.
attach(learning.test)
ci.test(x = F , y = B, z = data.frame(C, D))
</code></pre>

<hr>
<h2 id='clgaussian.test'>Synthetic (mixed) data set to test learning algorithms</h2><span id='topic+clgaussian.test'></span>

<h3>Description</h3>

<p>This a synthetic data set used as a test case in the <span class="pkg">bnlearn</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(clgaussian.test)
</code></pre>


<h3>Format</h3>

<p>The <code>clgaussian.test</code> data set contains one normal (Gaussian) variable,
4 discrete variables and 3 conditional Gaussian variables.
</p>


<h3>Note</h3>

<p>The R script to generate data from this network is available from
<a href="https://www.bnlearn.com/documentation/networks/">https://www.bnlearn.com/documentation/networks/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(clgaussian.test)
# create and plot the network structure.
dag = model2network("[A][B][C][H][D|A:H][F|B:C][E|B:D][G|A:D:E:F]")
## Not run: graphviz.plot(dag)
</code></pre>

<hr>
<h2 id='compare'>Compare two or more different Bayesian networks</h2><span id='topic+compare'></span><span id='topic+all.equal.bn'></span><span id='topic+shd'></span><span id='topic+hamming'></span><span id='topic+graphviz.compare'></span>

<h3>Description</h3>

<p>Compare two different Bayesian networks; compute their Structural Hamming
Distance (SHD) or the Hamming distance between their skeletons. Or
graphically compare them by plotting them side by side,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare(target, current, arcs = FALSE)
## S3 method for class 'bn'
all.equal(target, current, ...)

shd(learned, true, wlbl = FALSE, debug = FALSE)
hamming(learned, true, debug = FALSE)

graphviz.compare(x, ..., groups, layout = "dot", shape = "rectangle",
  fontsize = 12, main = NULL, sub = NULL, diff = "from-first",
  diff.args = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_+3A_target">target</code>, <code id="compare_+3A_learned">learned</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_current">current</code>, <code id="compare_+3A_true">true</code></td>
<td>
<p>another object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (for <code>all.equal()</code>,
currently ignored); or a set of one or more objects of class <code>bn</code>
(for <code>graphviz.compare</code>).</p>
</td></tr>
<tr><td><code id="compare_+3A_wlbl">wlbl</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> arcs whose directions have been
fixed by a whitelist or a by blacklist are preserved when constructing
the CPDAGs of <code>learned</code> and <code>true</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="compare_+3A_arcs">arcs</code></td>
<td>
<p>a boolean value. See below.</p>
</td></tr>
<tr><td><code id="compare_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_groups">groups</code></td>
<td>
<p>a list of character vectors, representing groups of node labels
of nodes that should be plotted close to each other.</p>
</td></tr>
<tr><td><code id="compare_+3A_layout">layout</code></td>
<td>
<p>a character string, the layout argument that will be passed to
<span class="pkg">Rgraphviz</span>. Possible values are <code>dots</code>, <code>neato</code>,
<code>twopi</code>, <code>circo</code> and <code>fdp</code>. See <span class="pkg">Rgraphviz</span>
documentation for details.</p>
</td></tr>
<tr><td><code id="compare_+3A_shape">shape</code></td>
<td>
<p>a character string, the shape of the nodes. Can be <code>circle</code>,
<code>ellipse</code> or <code>rectangle</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_fontsize">fontsize</code></td>
<td>
<p>a positive number, the font size for the node labels.</p>
</td></tr>
<tr><td><code id="compare_+3A_main">main</code></td>
<td>
<p>a vector of character strings, one for each network. They
are plotted at the top of the corresponding figure(s).</p>
</td></tr>
<tr><td><code id="compare_+3A_sub">sub</code></td>
<td>
<p>a vector of character strings, the subtitles that are plotted at
the bottom of the corresponding figure(s).</p>
</td></tr>
<tr><td><code id="compare_+3A_diff">diff</code></td>
<td>
<p>a character string, the label of the method used to compare and
format the figure(s) created by <code>graphviz.compare()</code>. The default value
is <code>from-first</code>, se below for details.</p>
</td></tr>
<tr><td><code id="compare_+3A_diff.args">diff.args</code></td>
<td>
<p>a list of optional arguments to control the formatting of
the figure(s) created by <code>graphviz.compare()</code>. See below for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>graphviz.compare()</code> can visualize differences between graphs in various
way depending on the value of the <code>diff</code> and <code>diff.args</code> arguments:
</p>

<ul>
<li> <p><code>none</code>: differences are not highlighted.
</p>
</li>
<li> <p><code>from-first</code>: the first <code>bn</code> object, <code>x</code>, is taken as
the reference network. All the other networks, passed via the <code>...</code>
argument, are compared to that first network and their true positive,
false positive, false negative arcs relative to that first network are
highlighted. Colours, line types and line widths for each category of
arcs can be specified as the elements of a list via the <code>diff.args</code>
argument, with names <code>tp.col</code>, <code>tp.lty</code>, <code>tp.lwd</code>,
<code>fp.col</code>, <code>fp.lty</code>, <code>fp.lwd</code>, <code>fn.col</code>, <code>fn.lty</code>,
<code>tp.lwd</code>. In addition, it is possible not to plot the reference
network at all by setting <code>show.first</code> to <code>FALSE</code>.
</p>
</li></ul>

<p>Regardless of the visualization, the nodes are arranged to be in the same
position for all the networks to make it easier to compare them.
</p>


<h3>Value</h3>

<p><code>compare()</code> returns a list containing the number of true positives
(<code>tp</code>, the number of arcs in <code>current</code> also present in
<code>target</code>), of false positives (<code>fp</code>, the number of arcs in
<code>current</code> not present in <code>target</code>) and of false negatives
(<code>fn</code>, the number of arcs not in <code>current</code> but present in
<code>target</code>) if <code>arcs</code> is <code>FALSE</code>; or the corresponding arc sets
if <code>arcs</code> is <code>TRUE</code>.
</p>
<p><code>all.equal()</code> returns either <code>TRUE</code> or a character string describing
the differences between <code>target</code> and <code>current</code>.
</p>
<p><code>shd()</code> and <code>hamming()</code> return a non-negative integer number.
</p>
<p><code>graphviz.compare()</code> plots one or more figures and returns invisibly a
list containing the <code>graph</code> objects generated from the networks that were
passed as arguments (in the same order). They can be further modified using
the <span class="pkg">graph</span> and <span class="pkg">Rgraphviz</span> packages.
</p>


<h3>Note</h3>

<p>Note that SHD, as defined in the reference, is defined on CPDAGs; therefore
<code>cpdag()</code> is called on both <code>learned</code> and <code>true</code> before computing
the distance.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Tsamardinos I, Brown LE, Aliferis CF (2006). &quot;The Max-Min Hill-Climbing
Bayesian Network Structure Learning Algorithm&quot;. <em>Machine Learning</em>,
<strong>65</strong>(1):31&ndash;78.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)

e1 = model2network("[A][B][C|A:B][D|B][E|C][F|A:E]")
e2 = model2network("[A][B][C|A:B][D|B][E|C:F][F|A]")
shd(e2, e1, debug = TRUE)
unlist(compare(e1,e2))
compare(target = e1, current = e2, arcs = TRUE)
## Not run: graphviz.compare(e1, e2, diff = "none")
</code></pre>

<hr>
<h2 id='configs'>Construct configurations of discrete variables</h2><span id='topic+configs'></span>

<h3>Description</h3>

<p>Create configurations of discrete variables, which can be used in modelling
conditional probability tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  configs(data, all = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="configs_+3A_data">data</code></td>
<td>
<p>a data frame containing factor columns.</p>
</td></tr>
<tr><td><code id="configs_+3A_all">all</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> all configuration are included as
levels in the return value;  otherwise only configurations which are actually
observed are considered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A factor with one element for each row of <code>data</code>, and levels as
specified by <code>all</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
configs(learning.test, all = TRUE)
configs(learning.test, all = FALSE)
</code></pre>

<hr>
<h2 id='constraint-based+20algorithms'>Constraint-based structure learning algorithms</h2><span id='topic+constraint-based+20algorithms'></span><span id='topic+pc.stable'></span><span id='topic+gs'></span><span id='topic+iamb'></span><span id='topic+fast.iamb'></span><span id='topic+inter.iamb'></span><span id='topic+iamb.fdr'></span><span id='topic+mmpc'></span><span id='topic+si.hiton.pc'></span><span id='topic+hpc'></span>

<h3>Description</h3>

<p>Learn the equivalence class of a directed acyclic graph (DAG) from data using
the PC, Grow-Shrink (GS), Incremental Association (IAMB), Fast Incremental
Association (Fast-IAMB), Interleaved Incremental Association (Inter-IAMB),
Incremental Association with FDR (IAMB-FDR), Max-Min Parents and Children
(MMPC), Semi-Interleaved HITON-PC or Hybrid Parents and Children (HPC)
constraint-based algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.stable(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
gs(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
iamb(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
fast.iamb(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
inter.iamb(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
iamb.fdr(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = FALSE)
mmpc(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = TRUE)
si.hiton.pc(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = TRUE)
hpc(x, cluster, whitelist = NULL, blacklist = NULL, test = NULL,
  alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE, undirected = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="constraint-based+2B20algorithms_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs to be included in the graph.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs not to be included in the graph.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_test">test</code></td>
<td>
<p>a character string, the label of the conditional independence
test to be used in the algorithm. If none is specified, the default test
statistic is the <em>mutual information</em> for categorical variables, the
Jonckheere-Terpstra test for ordered factors and the <em>linear
correlation</em> for continuous variables. See <code><a href="#topic+independence+20tests">independence tests</a></code>
for details.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_alpha">alpha</code></td>
<td>
<p>a numeric value, the target nominal type I error rate.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_b">B</code></td>
<td>
<p>a positive integer, the number of permutations considered for each
permutation test. It will be ignored with a warning if the conditional
independence test specified by the <code>test</code> argument is not a
permutation test.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_max.sx">max.sx</code></td>
<td>
<p>a positive integer, the maximum allowed size of the conditioning
sets used in conditional independence tests. The default is that there is
no limit on size.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="constraint-based+2B20algorithms_+3A_undirected">undirected</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> no attempt will be made to
determine the orientation of the arcs; the returned (undirected) graph
will represent the underlying structure of the Bayesian network.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bn</code>.
See <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>


<h3>Note</h3>

<p>Note that even when <code>undirected</code> is set to <code>FALSE</code> there is no
guarantee that all arcs in the returned network will be directed; some arc
directions are impossible to learn just from data due to score equivalence.
<code>cextend()</code> provides a consistent extension of partially directed
networks into directed acyclic graphs, which can then be used (for instance)
for parameter learning.
</p>
<p>See <code><a href="#topic+structure+20learning">structure learning</a></code> for a complete list of structure learning
algorithms with the respective references. All algorithms accept incomplete
data, which they handle by computing individual conditional independence tests
on locally complete observations.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+independence+20tests">independence tests</a></code>, <a href="#topic+local+20discovery+20algorithms">local discovery algorithms</a>,
<a href="#topic+score-based+20algorithms">score-based algorithms</a>, <a href="#topic+hybrid+20algorithms">hybrid algorithms</a>, <a href="#topic+cextend">cextend</a>.</p>

<hr>
<h2 id='coronary'>Coronary heart disease data set</h2><span id='topic+coronary'></span>

<h3>Description</h3>

<p>Probable risk factors for coronary thrombosis, comprising data from 1841 men.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(coronary)
</code></pre>


<h3>Format</h3>

<p>The <code>coronary</code> data set contains the following 6 variables:
</p>

<ul>
<li> <p><code>Smoking</code> (<em>smoking</em>): a two-level factor with levels
<code>no</code> and <code>yes</code>.
</p>
</li>
<li> <p><code>M. Work</code> (<em>strenuous mental work</em>): a two-level factor
with levels <code>no</code> and <code>yes</code>.
</p>
</li>
<li> <p><code>P. Work</code> (<em>strenuous physical work</em>): a two-level factor
with levels <code>no</code> and <code>yes</code>.
</p>
</li>
<li> <p><code>Pressure</code> (<em>systolic blood pressure</em>): a two-level factor
with levels <code>&lt;140</code> and <code>&gt;140</code>.
</p>
</li>
<li> <p><code>Proteins</code> (<em>ratio of beta and alpha lipoproteins</em>): a
two-level factor with levels <code>&lt;3</code> and <code>&gt;3</code>.
</p>
</li>
<li> <p><code>Family</code> (<em>family anamnesis of coronary heart disease</em>): a
two-level factor with levels <code>neg</code> and <code>pos</code>.
</p>
</li></ul>



<h3>Source</h3>

<p>Edwards DI (2000). <em>Introduction to Graphical Modelling</em>. Springer, 2nd
edition.
</p>
<p>Reinis Z, Pokorny J, Basika V, Tiserova J, Gorican K, Horakova D, Stuchlikova
E, Havranek T, Hrabovsky F (1981). &quot;Prognostic Significance of the Risk
Profile in the Prevention of Coronary Heart Disease&quot;. <em>Bratisl Lek
Listy</em>, <strong>76</strong>:137&ndash;150. Published on Bratislava Medical Journal,
in Czech.
</p>
<p>Whittaker J (1990). <em>Graphical Models in Applied Multivariate
Statistics</em>. Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This is the undirected graphical model from Whittaker (1990).
data(coronary)
ug = empty.graph(names(coronary))
arcs(ug, check.cycles = FALSE) = matrix(
  c("Family", "M. Work", "M. Work", "Family",
    "M. Work", "P. Work", "P. Work", "M. Work",
    "M. Work", "Proteins", "Proteins", "M. Work",
    "M. Work", "Smoking", "Smoking", "M. Work",
    "P. Work", "Smoking", "Smoking", "P. Work",
    "P. Work", "Proteins", "Proteins", "P. Work",
    "Smoking", "Proteins", "Proteins", "Smoking",
    "Smoking", "Pressure", "Pressure", "Smoking",
    "Pressure", "Proteins", "Proteins", "Pressure"),
  ncol = 2, byrow = TRUE,
  dimnames = list(c(), c("from", "to")))
## Not run: graphviz.plot(ug, shape = "ellipse")
</code></pre>

<hr>
<h2 id='cpdag'>Equivalence classes, moral graphs and consistent extensions</h2><span id='topic+cpdag'></span><span id='topic+cextend'></span><span id='topic+moral'></span><span id='topic+colliders'></span><span id='topic+shielded.colliders'></span><span id='topic+unshielded.colliders'></span><span id='topic+vstructs'></span>

<h3>Description</h3>

<p>Find the equivalence class and the v-structures of a Bayesian network,
construct its moral graph, or create a consistent extension of an equivalent
class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpdag(x, wlbl = FALSE, debug = FALSE)
cextend(x, strict = TRUE, debug = FALSE)
moral(x, debug = FALSE)

colliders(x, arcs = FALSE, debug = FALSE)
shielded.colliders(x, arcs = FALSE, debug = FALSE)
unshielded.colliders(x, arcs = FALSE, debug = FALSE)
vstructs(x, arcs = FALSE, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpdag_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code> (with the exception of
<code>cextend</code>, which only accepts objects of class <code>bn</code>).</p>
</td></tr>
<tr><td><code id="cpdag_+3A_arcs">arcs</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the arcs that are part of at least
one v-structure are returned instead of the v-structures themselves.</p>
</td></tr>
<tr><td><code id="cpdag_+3A_wlbl">wlbl</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> arcs whose directions have been
fixed by a whitelist or a by blacklist are preserved when constructing
the CPDAG.</p>
</td></tr>
<tr><td><code id="cpdag_+3A_strict">strict</code></td>
<td>
<p>a boolean value. If no consistent extension is possible and
<code>strict</code> is <code>TRUE</code>, an error is generated; otherwise a partially
extended graph is returned with a warning.</p>
</td></tr>
<tr><td><code id="cpdag_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arcs whose directions are dictated by the parametric assumptions of
the network are preserved as directed arcs in  <code>cpdag()</code>. This means
that, in a conditional Gaussian network, arcs from discrete nodes to
continuous nodes will be treated as whitelisted in their only valid direction.
</p>


<h3>Value</h3>

<p><code>cpdag()</code> returns an object of class <code>bn</code>, representing the
equivalence class. <code>moral</code> on the other hand returns the moral graph.
See <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>
<p><code>cextend()</code> returns an object of class <code>bn</code>, representing a DAG that
is the consistent extension of <code>x</code>.
</p>
<p><code>vstructs()</code> returns a matrix with either 2 or 3 columns, according to the
value of the <code>arcs</code> argument.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Dor D (1992). <em>A Simple Algorithm to Construct a Consistent Extension of
a Partially Oriented Graph</em>. UCLA, Cognitive Systems Laboratory.
</p>
<p>Koller D, Friedman N (2009). <em>Probabilistic Graphical Models: Principles
and Techniques</em>. MIT Press.
</p>
<p>Pearl J (2009). <em>Causality: Models, Reasoning and Inference</em>. Cambridge
University Press, 2nd edition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test)
cpdag(dag)
vstructs(dag)
</code></pre>

<hr>
<h2 id='cpquery'>Perform conditional probability queries</h2><span id='topic+cpquery'></span><span id='topic+cpdist'></span><span id='topic+mutilated'></span>

<h3>Description</h3>

<p>Perform conditional probability queries (CPQs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpquery(fitted, event, evidence, cluster, method = "ls", ...,
  debug = FALSE)
cpdist(fitted, nodes, evidence, cluster, method = "ls", ...,
  debug = FALSE)

mutilated(x, evidence)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpquery_+3A_fitted">fitted</code></td>
<td>
<p>an object of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_event">event</code>, <code id="cpquery_+3A_evidence">evidence</code></td>
<td>
<p>see below.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_nodes">nodes</code></td>
<td>
<p>a vector of character strings, the labels of the nodes whose
conditional distribution we are interested in.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_method">method</code></td>
<td>
<p>a character string, the method used to perform the conditional
probability query. Currently only <em>logic sampling</em> (<code>ls</code>, the
default) and <em>likelihood weighting</em> (<code>lw</code>) are implemented.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_...">...</code></td>
<td>
<p>additional tuning parameters.</p>
</td></tr>
<tr><td><code id="cpquery_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cpquery</code> estimates the conditional probability of <code>event</code> given
<code>evidence</code> using the method specified in the <code>method</code> argument.
</p>
<p><code>cpdist</code> generates random samples conditional on the <code>evidence</code>
using the method specified in the <code>method</code> argument.
</p>
<p><code>mutilated</code> constructs the mutilated network arising from an ideal
intervention setting the nodes involved to the values specified by
<code>evidence</code>. In this case <code>evidence</code> must be provided as a list
in the same format as for likelihood weighting (see below).
</p>
<p>Note that both <code>cpquery</code> and <code>cpdist</code> are based on Monte Carlo
particle filters, and therefore they may return slightly different values
on different runs due to simulation noise.
</p>


<h3>Value</h3>

<p><code>cpquery()</code> returns a numeric value, the conditional probability of
<code>event()</code> conditional on <code>evidence</code>.
</p>
<p><code>cpdist()</code> returns a data frame containing the samples generated from
the conditional distribution of the <code>nodes</code> conditional on
<code>evidence()</code>. The data frame has class <code>c("bn.cpdist", "data.frame")</code>,
and a <code>meth, -8od</code> attribute storing the value of the <code>method</code>
argument. In the case of likelihood weighting, the weights are also attached
as an attribute called <code>weights</code>.
</p>
<p><code>mutilated</code> returns a <code>bn</code> or <code>bn.fit</code> object, depending on the
class of <code>x</code>.
</p>


<h3>Logic Sampling</h3>

<p>Logic sampling is an <em>approximate inference</em> algorithm.
</p>
<p>The <code>event</code> and <code>evidence</code> arguments must be two expressions
describing the event of interest and the conditioning evidence in a format
such that, if we denote with <code>data</code> the data set the network was learned
from, <code>data[evidence, ]</code> and <code>data[event, ]</code> return the correct
observations. If either <code>event</code> or <code>evidence</code> is set to <code>TRUE</code>
an unconditional probability query is performed with respect to that argument.
</p>
<p>Three tuning parameters are available:
</p>

<ul>
<li> <p><code>n</code>: a positive integer number, the number of random samples
to generate from <code>fitted</code>. The default value is
<code>5000 * log10(nparams(fitted))</code> for discrete and conditional Gaussian
networks and <code>500 * nparams(fitted)</code> for Gaussian networks.
</p>
</li>
<li> <p><code>batch</code>: a positive integer number, the number of random samples
that are generated at one time. Defaults to <code>10^4</code>. If the <code>n</code>
is very large (e.g. <code>10^12</code>), R would run out of memory if it tried
to generate them all at once. Instead random samples are generated in
batches of size <code>batch</code>, discarding each batch before generating the
next.
</p>
</li>
<li> <p><code>query.nodes</code>: a vector of character strings, the labels of
the nodes involved in <code>event</code> and <code>evidence</code>. Simple queries do
not require to generate samples from all the nodes in the network,
so <code>cpquery</code> and <code>cpdist</code> try to identify which nodes are used
in <code>event</code> and <code>evidence</code> and reduce the network to their upper
closure. <code>query.nodes</code> may be used to manually specify these nodes
when automatic identification fails; there is no reason to use it
otherwise.
</p>
</li></ul>

<p>Note that the number of samples returned by <code>cpdist()</code> is always smaller
than <code>n</code>, because logic sampling is a form of rejection sampling.
Therefore, only the observations matching <code>evidence</code> (out of the <code>n</code>
that are generated) are returned, and their number depends on the probability
of <code>evidence</code>. Furthermore, the probabilities returned by
<code>cpquery()</code> are approximate estimates and they will not sum up to 1 even
when the corresponding underlying values do if they are computed in separate
calls to cpquery().
</p>


<h3>Likelihood Weighting</h3>

<p>Likelihood weighting is an <em>approximate inference</em> algorithm based on
Monte Carlo sampling.
</p>
<p>The <code>event</code> argument must be an expression describing the event of
interest, as in logic sampling. The <code>evidence</code> argument must be a named
list:
</p>

<ul>
<li><p> Each element corresponds to one node in the network and must contain
the value that node will be set to when sampling.
</p>
</li>
<li><p> In the case of a continuous node, two values can also be provided. In
that case, the value for that node will be sampled from a uniform
distribution on the interval delimited by the specified values.
</p>
</li>
<li><p> In the case of a discrete or ordinal node, two or more values can also
be provided. In that case, the value for that node will be sampled with
uniform probability from the set of specified values.
</p>
</li></ul>

<p>If either <code>event</code> or <code>evidence</code> is set to <code>TRUE</code> an
unconditional probability query is performed with respect to that argument.
</p>
<p>Tuning parameters are the same as for logic sampling: <code>n</code>, <code>batch</code>
and <code>query.nodes</code>.
</p>
<p>Note that the samples returned by <code>cpdist()</code> are generated from the
mutilated network, and need to be weighted appropriately when computing
summary statistics (for more details, see the references below).
<code>cpquery</code> does that automatically when computing the final conditional
probability. Also note that the <code>batch</code> argument is ignored in
<code>cpdist()</code> for speed and memory efficiency. Furthermore, the
probabilities returned by <code>cpquery()</code> are approximate estimates and they
will not sum up to 1 even when the corresponding underlying values do if they
are computed in separate calls to cpquery().
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Koller D, Friedman N (2009). <em>Probabilistic Graphical Models: Principles
and Techniques</em>. MIT Press.
</p>
<p>Korb K, Nicholson AE (2010). <em>Bayesian Artificial Intelligence</em>. Chapman
&amp; Hall/CRC, 2nd edition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## discrete Bayesian network (it is the same with ordinal nodes).
data(learning.test)
fitted = bn.fit(hc(learning.test), learning.test)
# the result should be around 0.025.
cpquery(fitted, (B == "b"), (A == "a"))
# programmatically build a conditional probability query...
var = names(learning.test)
obs = 2
str = paste("(", names(learning.test)[-3], " == '",
        sapply(learning.test[obs, -3], as.character), "')",
        sep = "", collapse = " &amp; ")
str
str2 = paste("(", names(learning.test)[3], " == '",
         as.character(learning.test[obs, 3]), "')", sep = "")
str2

cmd = paste("cpquery(fitted, ", str2, ", ", str, ")", sep = "")
eval(parse(text = cmd))
# ... but note that predict works better in this particular case.
attr(predict(fitted, "C", learning.test[obs, -3], prob = TRUE), "prob")
# do the same with likelihood weighting.
cpquery(fitted, event = eval(parse(text = str2)),
  evidence = as.list(learning.test[2, -3]), method = "lw")
attr(predict(fitted, "C", learning.test[obs, -3],
               method = "bayes-lw", prob = TRUE), "prob")
# conditional distribution of A given C == "c".
table(cpdist(fitted, "A", (C == "c")))

## Gaussian Bayesian network.
data(gaussian.test)
fitted = bn.fit(hc(gaussian.test), gaussian.test)
# the result should be around 0.04.
cpquery(fitted,
  event = ((A &gt;= 0) &amp; (A &lt;= 1)) &amp; ((B &gt;= 0) &amp; (B &lt;= 3)),
  evidence = (C + D &lt; 10))

## ideal interventions and mutilated networks.
mutilated(fitted, evidence = list(F = 42))
</code></pre>

<hr>
<h2 id='data+20preprocessing'>Pre-process data to better learn Bayesian networks</h2><span id='topic+discretize'></span><span id='topic+dedup'></span>

<h3>Description</h3>

<p>Screen and transform the data to make them more suitable for structure and
parameter learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  # discretize continuous data into factors.
  discretize(data, method, breaks = 3, ordered = FALSE, ..., debug = FALSE)
  # screen continuous data for highly correlated pairs of variables.
  dedup(data, threshold, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data+2B20preprocessing_+3A_data">data</code></td>
<td>
<p>a data frame containing numeric columns (for <code>dedup()</code>) or a
combination of numeric or factor columns (for <code>discretize()</code>).</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_threshold">threshold</code></td>
<td>
<p>a numeric value between zero and one, the absolute
correlation used a threshold in screening highly correlated pairs.</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_method">method</code></td>
<td>
<p>a character string, either <code>interval</code> for <em>interval
discretization</em>, <code>quantile</code> for <em>quantile discretization</em>
(the default) or <code>hartemink</code> for <em>Hartemink's pairwise mutual
information</em> method.</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_breaks">breaks</code></td>
<td>
<p>an integer number, the number of levels the variables will be
discretized into; or a vector of integer numbers, one for each column of the
data set, specifying the number of levels for each variable.</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_ordered">ordered</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the discretized variables are
returned as ordered factors instead of unordered ones.</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_...">...</code></td>
<td>
<p>additional tuning parameters, see below.</p>
</td></tr>
<tr><td><code id="data+2B20preprocessing_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>discretize()</code> takes a data frame as its first argument and returns a
secdond data frame of discrete variables, transformed using of three methods:
<code>interval</code>, <code>quantile</code> or <code>hartemink</code>. Discrete variables are
left unchanged.
</p>
<p>The <code>hartemink</code> method has two additional tuning parameters:
</p>

<ul>
<li> <p><code>idisc</code>: the method used for the initial marginal discretization
of the variables, either <code>interval</code> or <code>quantile</code>.
</p>
</li>
<li> <p><code>ibreaks</code>: the number of levels the variables are initially
discretized into, in the same format as in the <code>breaks</code> argument.
</p>
</li></ul>

<p>It is sometimes the case that the <code>quantile</code> method cannot discretize one
or more variables in the data without generating zero-length intervals because
the quantiles are not unique. If <code>method = "quantile"</code>,
<code>discretize()</code> will produce an error. If <code>method = "quantile"</code> and
<code>idisc = "quantile"</code>, <code>discretize()</code> will try to lower the number of
breaks set by the <code>ibreaks</code> argument until quantiles are distinct. If
this is not possible without making <code>ibreaks</code> smaller than <code>breaks</code>,
<code>discretize()</code> will produce an error.
</p>
<p><code>dedup()</code> screens the data for pairs of highly correlated variables, and
discards one in each pair.
</p>
<p>Both <code>discretize()</code> and <code>dedup()</code> accept data with missing values.
</p>


<h3>Value</h3>

<p><code>discretize()</code> returns a data frame with the same structure (number of
columns, column names, etc.) as <code>data</code>, containing the discretized
variables.
</p>
<p><code>dedup()</code> returns a data frame with a subset of the columns of <code>data</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Hartemink A (2001). <em>Principled Computational Methods for the Validation
and Discovery of Genetic Regulatory Networks</em>. Ph.D. thesis, School of
Electrical Engineering and Computer Science, Massachusetts Institute of
Technology.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gaussian.test)
d = discretize(gaussian.test, method = 'hartemink', breaks = 4, ibreaks = 10)
plot(hc(d))
d2 = dedup(gaussian.test)
</code></pre>

<hr>
<h2 id='dsep'>Test d-separation</h2><span id='topic+dsep'></span>

<h3>Description</h3>

<p>Check whether two nodes are d-separated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsep(bn, x, y, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dsep_+3A_bn">bn</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="dsep_+3A_x">x</code>, <code id="dsep_+3A_y">y</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="dsep_+3A_z">z</code></td>
<td>
<p>an optional vector of character strings, the label of the
(candidate) d-separating nodes. It defaults to the empty set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dsep()</code> returns <code>TRUE</code> if <code>x</code> and <code>y</code> are
d-separated by <code>z</code>, and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Koller D, Friedman N (2009). <em>Probabilistic Graphical Models: Principles
and Techniques</em>. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bn = model2network("[A][C|A][B|C]")
dsep(bn, "A", "B", "C")
bn = model2network("[A][C][B|A:C]")
dsep(bn, "A", "B", "C")
</code></pre>

<hr>
<h2 id='foreign+20files+20utilities'>Read and write BIF, NET, DSC and DOT files</h2><span id='topic+read.bif'></span><span id='topic+write.bif'></span><span id='topic+read.dsc'></span><span id='topic+write.dsc'></span><span id='topic+read.net'></span><span id='topic+write.net'></span><span id='topic+write.dot'></span>

<h3>Description</h3>

<p>Read networks saved from other programs into <code>bn.fit</code> objects, and dump
<code>bn</code> and <code>bn.fit</code> objects into files for other programs to read.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># Old (non-XML) Bayesian Interchange format.
read.bif(file, debug = FALSE)
write.bif(file, fitted)

# Microsoft Interchange format.
read.dsc(file, debug = FALSE)
write.dsc(file, fitted)

# HUGIN flat network format.
read.net(file, debug = FALSE)
write.net(file, fitted)

# Graphviz DOT format.
write.dot(file, graph)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="foreign+2B20files+2B20utilities_+3A_file">file</code></td>
<td>
<p>a connection object or a character string.</p>
</td></tr>
<tr><td><code id="foreign+2B20files+2B20utilities_+3A_fitted">fitted</code></td>
<td>
<p>an object of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="foreign+2B20files+2B20utilities_+3A_graph">graph</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="foreign+2B20files+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>read.bif()</code>, <code>read.dsc()</code> and <code>read.net()</code> return an object of class
<code>bn.fit</code>.
</p>
<p><code>write.bif()</code>, <code>write.dsc()</code>, <code>write.net()</code> and <code>write.dot()</code>
return <code>NULL</code> invisibly.
</p>


<h3>Note</h3>

<p>All the networks present in the Bayesian Network Repository have associated
BIF, DSC and NET files that can be imported with <code>read.bif()</code>,
<code>read.dsc()</code> and <code>read.net()</code>.
</p>
<p>HUGIN can import and export NET files; Netica can read (but not write) DSC
files; and GeNIe can read and write both DSC and NET files.
</p>
<p>DOT files can be read by Graphviz, Gephi and a variety of other programs.
</p>
<p>Please note that these functions work on a &quot;best effort&quot; basis, as the parsing
of these formats have been implemented by reverse engineering the file format
from publicly available examples.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Bayesian Network Repository, <a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>

<hr>
<h2 id='gaussian.test'>Synthetic (continuous) data set to test learning algorithms</h2><span id='topic+gaussian.test'></span>

<h3>Description</h3>

<p>This a synthetic data set used as a test case in the <span class="pkg">bnlearn</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(gaussian.test)
</code></pre>


<h3>Format</h3>

<p>The <code>gaussian.test</code> data set contains seven normal (Gaussian) variables.
</p>


<h3>Note</h3>

<p>The R script to generate data from this network is available from
<a href="https://www.bnlearn.com/documentation/networks/">https://www.bnlearn.com/documentation/networks/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(gaussian.test)
# create and plot the network structure.
dag = model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]")
## Not run: graphviz.plot(dag)
</code></pre>

<hr>
<h2 id='gRain+20integration'>Import and export networks from the gRain package</h2><span id='topic+gRain+20integration'></span><span id='topic+as.bn.fit'></span><span id='topic+as.bn.fit.grain'></span><span id='topic+as.bn.grain'></span><span id='topic+as.grain'></span><span id='topic+as.grain.bn.fit'></span><span id='topic+as.grain.bn'></span>

<h3>Description</h3>

<p>Convert <code>bn.fit</code> objects to <code>grain</code> objects and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'grain'
as.bn.fit(x, including.evidence = FALSE, ...)
## S3 method for class 'bn.fit'
as.grain(x)
## S3 method for class 'grain'
as.bn(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gRain+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>grain(code)</code> (for <code>as.bn.fit</code>) or
<code>bn.fit()</code> (for <code>as.grain</code>).</p>
</td></tr>
<tr><td><code id="gRain+2B20integration_+3A_including.evidence">including.evidence</code></td>
<td>
<p>a boolean value. If <code>FALSE</code>, the <code>grain</code>
object is converted without considering any evidence that has been set into
it. If <code>TRUE</code>, any hard evidence is carried over into the <code>bn.fit</code>
object as a zero-one probability distribution.</p>
</td></tr>
<tr><td><code id="gRain+2B20integration_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>grain</code> (for <code>as.grain</code>), <code>bn.fit</code> (for
<code>as.bn.fit</code>) or <code>bn</code> (for <code>as.bn</code>).
</p>


<h3>Note</h3>

<p>Conditional probability tables in <code>grain</code> objects must be completely
specified; on the other hand, <code>bn.fit</code> allows <code>NaN</code> values for
unobserved parents' configurations. Such <code>bn.fit</code> objects will be
converted to $m$ <code>grain</code> objects by replacing the missing conditional
probability distributions with uniform distributions.
</p>
<p>Another solution to this problem is to fit another <code>bn.fit</code> with
<code>method = "bayes"</code> and a low <code>iss</code> value, using the same data
and network structure.
</p>
<p>Ordinal nodes will be treated as categorical by <code>as.grain</code>,
disregarding the ordering of the levels.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(gRain)
a = bn.fit(hc(learning.test), learning.test)
b = as.grain(a)
c = as.bn.fit(b)
## End(Not run)</code></pre>

<hr>
<h2 id='graph+20enumeration'>Count graphs with specific characteristics</h2><span id='topic+count.graphs'></span><span id='topic+graph+20enumeration'></span>

<h3>Description</h3>

<p>Count directed acyclic graphs of various sizes with specific characteristics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>count.graphs(type = "all.dags", nodes, ..., debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graph+2B20enumeration_+3A_type">type</code></td>
<td>
<p>a character string, the label describing the types of graphs to
be counted (see below).</p>
</td></tr>
<tr><td><code id="graph+2B20enumeration_+3A_nodes">nodes</code></td>
<td>
<p>a vector of positive integers, the graph sizes as given by the
numbers of nodes.</p>
</td></tr>
<tr><td><code id="graph+2B20enumeration_+3A_...">...</code></td>
<td>
<p>additional parameters (see below).</p>
</td></tr>
<tr><td><code id="graph+2B20enumeration_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent. Ignored in some
generation methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The types of graphs, and the associated additional parameters, are:
</p>

<ul>
<li> <p><code>all-dags</code>: all directed acyclic graphs.
</p>
</li>
<li> <p><code>dags-given-ordering</code>: all directed acyclic graphs
with a specific topological ordering.
</p>
</li>
<li> <p><code>dags-with-k-roots</code>: all directed acyclic graphs with <code>k</code>
root nodes.
</p>
</li>
<li> <p><code>dags-with-r-arcs</code>: all directed acyclic graphs with <code>r</code>
arcs.
</p>
</li></ul>



<h3>Value</h3>

<p><code>count.graphs()</code> returns an objects of class <code>bigz</code> from the
<span class="pkg">gmp</span> package, a vector with the graph counts.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Harary F, Palmer EM (1973). &quot;Graphical Enumeration&quot;. Academic Press.
</p>
<p>Rodionov VI (1992). &quot;On the Number of Labeled Acyclic Digraphs&quot;.
<em>Discrete Mathematics</em>, <strong>105</strong>:319&ndash;321.
</p>
<p>Liskovets VA (1976). &quot;On the Number of Maximal Vertices of a Random Acyclic
Digraph&quot;. <em>Theory of Probability and its Applications</em>,
<strong>20</strong>(2):401&ndash;409.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
count.graphs("dags.with.r.arcs", nodes = 3:6, r = 2)

## End(Not run)</code></pre>

<hr>
<h2 id='graph+20generation+20utilities'>Generate empty, complete or random graphs</h2><span id='topic+graph+20generation+20utilities'></span><span id='topic+empty.graph'></span><span id='topic+complete.graph'></span><span id='topic+random.graph'></span>

<h3>Description</h3>

<p>Generate empty, complete or random directed acyclic graphs from a given set
of nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>empty.graph(nodes, num = 1)
complete.graph(nodes, num = 1)
random.graph(nodes, num = 1, method = "ordered", ..., debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graph+2B20generation+2B20utilities_+3A_nodes">nodes</code></td>
<td>
<p>a vector of character strings, the labels of the nodes.</p>
</td></tr>
<tr><td><code id="graph+2B20generation+2B20utilities_+3A_num">num</code></td>
<td>
<p>an integer, the number of graphs to be generated.</p>
</td></tr>
<tr><td><code id="graph+2B20generation+2B20utilities_+3A_method">method</code></td>
<td>
<p>a character string, the label of a score. Possible values are
<code>ordered</code> (<em>full ordering</em> based generation), <code>ic-dag</code>
(Ide's and Cozman's <em>Generating Multi-connected DAGs</em> algorithm) and
<code>melancon</code> (Melancon's and Philippe's <em>Uniform Random Acyclic
Digraphs</em> algorithm).</p>
</td></tr>
<tr><td><code id="graph+2B20generation+2B20utilities_+3A_...">...</code></td>
<td>
<p>additional tuning parameters (see below).</p>
</td></tr>
<tr><td><code id="graph+2B20generation+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent. Ignored in some
generation methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Graph generation algorithms available in <code>random.graph()</code> are:
</p>

<ul>
<li> <p><em>full ordering</em> based generation (<code>ordered</code>): generates
graphs whose node ordering is given by the order of the labels in the
<code>nodes</code> argument. The same algorithm is used in the
<code>randomDAG</code> function in package <span class="pkg">pcalg</span>.
</p>
</li>
<li><p> Ide's and Cozman's <em>Generating Multi-connected DAGs</em> algorithm
(<code>ic-dag</code>): generates graphs with a uniform probability
distribution over the set of multiconnected graphs.
</p>
</li>
<li><p> Melancon's and Philippe's <em>Uniform Random Acyclic Digraphs</em>
algorithm (<code>melancon</code>): generates graphs with a uniform probability
distribution over the set of all possible graphs.
</p>
</li></ul>

<p>Additional arguments for the <code>random.graph()</code> function are:
</p>

<ul>
<li> <p><code>prob</code>: the probability of each arc to be present in a graph
generated by the <code>ordered</code> algorithm. The default value is
<code>2 / (length(nodes) - 1)</code>, which results in a sparse graph (the
number of arcs should be of the same order as the number of nodes).
</p>
</li>
<li> <p><code>burn.in</code>: the number of iterations for the <code>ic-dag</code> and
<code>melancon</code> algorithms to converge to a stationary (and uniform)
probability distribution. The default value is <code>6 * length(nodes)^2</code>.
</p>
</li>
<li> <p><code>every</code>: return only one graph <code>every</code> number of steps
instead of all the graphs generated with <code>ic-dag</code> and
<code>melancon</code>. Since both algorithms are based on Markov Chain Monte
Carlo approaches, high values of <code>every</code> result in a more diverse
set of networks. The default value is <code>1</code>, i.e. to return all the
networks that are generated.
</p>
</li>
<li> <p><code>max.degree</code>: the maximum degree for any node in a graph
generated by the <code>ic-dag</code> and <code>melancon</code> algorithms. The default
value is <code>Inf</code>.
</p>
</li>
<li> <p><code>max.in.degree</code>: the maximum in-degree for any node in a graph
generated by the <code>ic-dag</code> and <code>melancon</code> algorithms. The default
value is <code>Inf</code>.
</p>
</li>
<li> <p><code>max.out.degree</code>: the maximum out-degree for any node in a graph
generated by the <code>ic-dag</code> and <code>melancon</code> algorithms. The default
value is <code>Inf</code>.
</p>
</li></ul>

<p><code>empty.graph()</code> generates <code>num</code> identical empty graphs, while
<code>complete.graph()</code> generates <code>num</code> identical complete directed
acyclic graphs.
</p>


<h3>Value</h3>

<p><code>empty.graph()</code>, <code>complete.graph()</code> and <code>random[.graph()</code>
return an object of class <code>bn</code> (if <code>num</code> is equal to <code>1</code>) or a
list of objects of class <code>bn</code> (otherwise). If <code>every</code> is greated
than <code>1</code>, <code>random.graph()</code> always returns a list, regardless of the
number of graphs it contains.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Ide JS, Cozman FG (2002). &quot;Random Generation of Bayesian Networks&quot;.
<em>Proceedings of the 16th Brazilian Symposium on Artificial
Intelligence</em>, 366&ndash;375.
</p>
<p>Melancon G, Dutour I, Bousquet-Melou M (2001). &quot;Random Generation of Directed
Acyclic Graphs&quot;. <em>Electronic Notes in Discrete Mathematics</em>,
<strong>10</strong>:202&ndash;207.
</p>
<p>Melancon G, Philippe F (2004). &quot;Generating Connected Acyclic Digraphs
Uniformly at Random&quot;. <em>Information Processing Letters</em>,
<strong>90</strong>(4):209&ndash;213.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>empty.graph(LETTERS[1:8])
random.graph(LETTERS[1:8])
plot(random.graph(LETTERS[1:8], method = "ic-dag", max.in.degree = 2))
plot(random.graph(LETTERS[1:8]))
plot(random.graph(LETTERS[1:8], prob = 0.2))
</code></pre>

<hr>
<h2 id='graph+20integration'>Import and export networks from the graph package</h2><span id='topic+graph+20integration'></span><span id='topic+as.bn.graphNEL'></span><span id='topic+as.bn.graphAM'></span><span id='topic+as.graphNEL'></span><span id='topic+as.graphAM'></span><span id='topic+as.graphNEL.bn'></span><span id='topic+as.graphNEL.bn.fit'></span><span id='topic+as.graphAM.bn'></span><span id='topic+as.graphAM.bn.fit'></span>

<h3>Description</h3>

<p>Convert <code>bn</code> and <code>bn.fit</code> objects to <code>graphNEL</code> and
<code>graphAM</code> objects and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'graphNEL'
as.bn(x, ..., check.cycles = TRUE)
## S3 method for class 'graphAM'
as.bn(x, ..., check.cycles = TRUE)
## S3 method for class 'bn'
as.graphNEL(x)
## S3 method for class 'bn.fit'
as.graphNEL(x)
## S3 method for class 'bn'
as.graphAM(x)
## S3 method for class 'bn.fit'
as.graphAM(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graph+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>, <code>bn.fit</code>, <code>graphNEL</code>,
<code>graphAM</code>.</p>
</td></tr>
<tr><td><code id="graph+2B20integration_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
<tr><td><code id="graph+2B20integration_+3A_check.cycles">check.cycles</code></td>
<td>
<p>a boolean value. If <code>FALSE</code> the returned network will
not be checked for cycles.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the relevant class.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(graph)
a = bn.fit(hc(learning.test), learning.test)
b = as.graphNEL(a)
c = as.bn(b)
## End(Not run)</code></pre>

<hr>
<h2 id='graph+20utilities'>Utilities to manipulate graphs</h2><span id='topic+graph+20utilities'></span><span id='topic+acyclic'></span><span id='topic+directed'></span><span id='topic+path.exists'></span><span id='topic+path'></span><span id='topic+path+2Cbn-method'></span><span id='topic+path+2Cbn.fit-method'></span><span id='topic+path+2Cbn.naive-method'></span><span id='topic+path+2Cbn.tan-method'></span><span id='topic+skeleton'></span><span id='topic+pdag2dag'></span><span id='topic+subgraph'></span>

<h3>Description</h3>

<p>Check and manipulate graph-related properties of an object of class <code>bn</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># check whether the graph is acyclic/completely directed.
acyclic(x, directed = FALSE, debug = FALSE)
directed(x)
# check whether there is a path between two nodes.
path.exists(x, from, to, direct = TRUE, underlying.graph = FALSE, debug = FALSE)
# build the skeleton or a complete orientation of the graph.
skeleton(x)
pdag2dag(x, ordering)
# build a subgraph spanning a subset of nodes.
subgraph(x, nodes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graph+2B20utilities_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>. <code>skeleton()</code>, <code>acyclic()</code>,
<code>directed()</code> and <code>path.exixsts()</code> also accept objects of class
<code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_from">from</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_to">to</code></td>
<td>
<p>a character string, the label of a node (different from
<code>from</code>).</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_direct">direct</code></td>
<td>
<p>a boolean value. If <code>FALSE</code> ignore any arc between
<code>from</code> and <code>to</code> when looking for a path.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_underlying.graph">underlying.graph</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the underlying
undirected graph is used instead of the (directed) one from the <code>x</code>
argument.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_ordering">ordering</code></td>
<td>
<p>the labels of all the nodes in the graph; their order is the
node ordering used to set the direction of undirected arcs.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_nodes">nodes</code></td>
<td>
<p>the labels of the nodes that induce the subgraph.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_directed">directed</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> only completely directed
cycles are considered; otherwise undirected arcs will also be considered
and treated as arcs present in both directions.</p>
</td></tr>
<tr><td><code id="graph+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>acyclic()</code>, <code>path()</code> and <code>directed()</code> return a boolean value. <br />
<code>skeleton()</code>, <code>pdag2dag()</code> and <code>subgraph()</code> return an object of
class <code>bn</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Bang-Jensen J, Gutin G (2009). <em>Digraphs: Theory, Algorithms and
Applications</em>. Springer, 2nd edition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
cpdag = pc.stable(learning.test)

acyclic(cpdag)
directed(cpdag)
dag = pdag2dag(cpdag, ordering = LETTERS[1:6])
dag
directed(dag)
skeleton(dag)
</code></pre>

<hr>
<h2 id='graphviz.chart'>Plotting networks with probability bars</h2><span id='topic+graphviz.chart'></span>

<h3>Description</h3>

<p>Plot a Bayesian network as a graph whose nodes are barplots representing
the marginal distributions of the corresponding variables.
Requires the <span class="pkg">Rgraphviz</span> and <span class="pkg">gRain</span> packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graphviz.chart(x, type = "barchart", layout = "dot", draw.labels = TRUE,
  grid = FALSE, scale = c(0.75, 1.1), col = "black", bg = "transparent",
  text.col = "black", bar.col = "black", strip.bg = bg, main = NULL,
  sub = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graphviz.chart_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_type">type</code></td>
<td>
<p>a character string, the type of graph used to plot the probability
distributions in the nodes. Possible values are <code>barchart</code>, <code>dotplot</code>
and <code>barprob</code> (a barchart with parameter values printed over the bars).</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_layout">layout</code></td>
<td>
<p>a character string, the layout argument that will be passed to
<span class="pkg">Rgraphviz</span>. Possible values are <code>dots</code>, <code>neato</code>,
<code>twopi</code>, <code>circo</code> and <code>fdp</code>. See <span class="pkg">Rgraphviz</span> documentation
for details.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_draw.labels">draw.labels</code></td>
<td>
<p>a boolean value, whether to print the labels of the
parameters of each variable.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_grid">grid</code></td>
<td>
<p>a boolean value, whether to draw to a reference grid for the
probability distributions. If <code>grid</code> is <code>TRUE</code>, a vertical grid is
drawn at probabilities <code>c(0, 0.25,</code> <code>0.50, 0.75)</code> for discrete
nodes, and at the quartiles of the regression coefficients range for
continuous nodes. If <code>grid</code> is a numeric vector, a verical grid is
drawn at the specified values. If grid is a named list, each element is
a set of grid points can be specificed for the corresponding node.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_scale">scale</code></td>
<td>
<p>a vector of two positive numbers, used by <span class="pkg">Rgraphviz</span> to
determine the size and the aspect ratio of the nodes.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_col">col</code>, <code id="graphviz.chart_+3A_bg">bg</code>, <code id="graphviz.chart_+3A_text.col">text.col</code>, <code id="graphviz.chart_+3A_bar.col">bar.col</code>, <code id="graphviz.chart_+3A_strip.bg">strip.bg</code></td>
<td>
<p>the colours of the node border,
of the barchart background, of the text, of the bars and of the strip
background.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_main">main</code></td>
<td>
<p>a character string, the main title of the graph. It's plotted at
the top of the graph.</p>
</td></tr>
<tr><td><code id="graphviz.chart_+3A_sub">sub</code></td>
<td>
<p>a character string, a subtitle which is plotted at the bottom of
the graph.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>graphviz.chart()</code> invisibly returns <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
modelstring = paste("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
  "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
  "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
  "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
  "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG][ACO2|VALV]",
  "[CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]", sep = "")
dag = model2network(modelstring)
fitted = bn.fit(dag, alarm)

# Netica style.
graphviz.chart(fitted, grid = TRUE, bg = "beige", bar.col = "black")
# Hugin style.
graphviz.chart(fitted, type = "barprob", grid = TRUE, bar.col = "green",
  strip.bg = "lightyellow")
# GeNIe style.
graphviz.chart(fitted, col = "darkblue", bg = "azure", bar.col = "darkblue")
# personal favourites.
graphviz.chart(fitted, type = "barprob", grid = TRUE, bar.col = "darkgreen",
  strip.bg = "lightskyblue")
graphviz.chart(fitted, type = "barprob", grid = TRUE, bar.col = "gold",
  strip.bg = "lightskyblue")
# dot-plot version.
graphviz.chart(fitted, type = "dotplot")

## End(Not run)
</code></pre>

<hr>
<h2 id='graphviz.plot'>Advanced Bayesian network plots</h2><span id='topic+graphviz.plot'></span>

<h3>Description</h3>

<p>Plot the graph associated with a Bayesian network using the <span class="pkg">Rgraphviz</span>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graphviz.plot(x, highlight = NULL, groups, layout = "dot",
  shape = "rectangle", fontsize = 12, main = NULL, sub = NULL,
  render = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graphviz.plot_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_highlight">highlight</code></td>
<td>
<p>a list, see below.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_groups">groups</code></td>
<td>
<p>a list of character vectors, representing groups of node labels
of nodes that should be plotted close to each other.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_layout">layout</code></td>
<td>
<p>a character string, the layout argument that will be passed to
<span class="pkg">Rgraphviz</span>. Possible values are <code>dots</code>, <code>neato</code>,
<code>twopi</code>, <code>circo</code> and <code>fdp</code>. See <span class="pkg">Rgraphviz</span>
documentation for details.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_shape">shape</code></td>
<td>
<p>a character string, the shape of the nodes. Can be <code>circle</code>,
<code>ellipse</code> or <code>rectangle</code>.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_fontsize">fontsize</code></td>
<td>
<p>a positive number, the font size for the node labels.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_main">main</code></td>
<td>
<p>a character string, the main title of the graph. It's plotted at
the top of the graph.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_sub">sub</code></td>
<td>
<p>a character string, a subtitle which is plotted at the bottom of
the graph.</p>
</td></tr>
<tr><td><code id="graphviz.plot_+3A_render">render</code></td>
<td>
<p>a logical value. If <code>TRUE</code>, <code>graphviz.plot()</code> actually
draws the figure in addition to returning the corresponding <code>graph</code>
object. If <code>FALSE</code>, no figure is produced.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>highlight</code> argument is a list with at least one of the following
elements:
</p>

<ul>
<li> <p><code>nodes</code>: a character vector, the labels of the nodes to be
highlighted.
</p>
</li>
<li> <p><code>arcs</code>: the arcs to be highlighted (a two-column matrix, whose
columns are labeled <code>from</code> and <code>to</code>).
</p>
</li></ul>

<p>and optionally one or more of the following graphical parameters:
</p>

<ul>
<li> <p><code>col</code>: an integer or character string (the highlight colour for
the arcs and the node frames). The default value is <code>red</code>.
</p>
</li>
<li> <p><code>textCol</code>: an integer or character string (the highlight colour
for the labels of the nodes). The default value is <code>black</code>.
</p>
</li>
<li> <p><code>fill</code>: an integer or character string (the colour used as a
background colour for the nodes). The default value is <code>transparent</code>.
</p>
</li>
<li> <p><code>lwd</code>: a positive number (the line width of highlighted arcs).
It overrides the line width settings in <code>strength.plot()</code>. The
default value is to use the global settings of <span class="pkg">Rgraphviz</span>.
</p>
</li>
<li> <p><code>lty</code>: the line type of highlighted arcs. Possible values are
0, 1, 2, 3, 4, 5, 6, &quot;blank&quot;, &quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;, &quot;dotdash&quot;,
&quot;longdash&quot; and &quot;twodash&quot;. The default value is to use the global
settings of <span class="pkg">Rgraphviz</span>.
</p>
</li></ul>

<p>Note that all these parameters take a single value that is then applied to all
nodes and arcs that will be highlighted.
</p>


<h3>Value</h3>

<p><code>graphviz.plot()</code> returns invisibly the <code>graph</code> object produced from
the network passed as the <code>x</code> argument. It can be further modified using
the <span class="pkg">graph</span> and <span class="pkg">Rgraphviz</span> packages.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.bn">plot.bn</a></code>.</p>

<hr>
<h2 id='hailfinder'>The HailFinder weather forecast system (synthetic) data set</h2><span id='topic+hailfinder'></span>

<h3>Description</h3>

<p>Hailfinder is a Bayesian network designed to forecast severe summer hail in
northeastern Colorado.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hailfinder)
</code></pre>


<h3>Format</h3>

<p>The <code>hailfinder</code> data set contains the following 56 variables:
</p>

<ul>
<li> <p><code>N07muVerMo</code> (<em>10.7mu vertical motion</em>): a four-level factor
with levels <code>StrongUp</code>, <code>WeakUp</code>, <code>Neutral</code> and <code>Down</code>.
</p>
</li>
<li> <p><code>SubjVertMo</code> (<em>subjective judgment of vertical motion</em>): a
four-level factor with levels <code>StrongUp</code>, <code>WeakUp</code>,
<code>Neutral</code> and <code>Down</code>.
</p>
</li>
<li> <p><code>QGVertMotion</code> (<em>quasigeostrophic vertical motion</em>): a
four-level factor with levels <code>StrongUp</code>, <code>WeakUp</code>,
<code>Neutral</code> and <code>Down</code>.
</p>
</li>
<li> <p><code>CombVerMo</code> (<em>combined vertical motion</em>): a four-level
factor with levels <code>StrongUp</code>, <code>WeakUp</code>, <code>Neutral</code> and
<code>Down</code>.
</p>
</li>
<li> <p><code>AreaMesoALS</code> (<em>area of meso-alpha</em>): a four-level factor
with levels <code>StrongUp</code>, <code>WeakUp</code>, <code>Neutral</code> and
<code>Down</code>.
</p>
</li>
<li> <p><code>SatContMoist</code> (<em>satellite contribution to moisture</em>): a
four-level factor with levels <code>VeryWet</code>, <code>Wet</code>, <code>Neutral</code>
and <code>Dry</code>.
</p>
</li>
<li> <p><code>RaoContMoist</code> (<em>reading at the forecast center for
moisture</em>): a four-level factor with levels <code>VeryWet</code>, <code>Wet</code>,
<code>Neutral</code> and <code>Dry</code>.
</p>
</li>
<li> <p><code>CombMoisture</code> (<em>combined moisture</em>): a four-level factor
with levels <code>VeryWet</code>, <code>Wet</code>, <code>Neutral</code> and <code>Dry</code>.
</p>
</li>
<li> <p><code>AreaMoDryAir</code> (<em>area of moisture and adry air</em>): a
four-level factor with levels <code>VeryWet</code>, <code>Wet</code>, <code>Neutral</code>
and <code>Dry</code>.
</p>
</li>
<li> <p><code>VISCloudCov</code> (<em>visible cloud cover</em>): a three-level factor
with levels <code>Cloudy</code>, <code>PC</code> and <code>Clear</code>.
</p>
</li>
<li> <p><code>IRCloudCover</code> (<em>infrared cloud cover</em>): a three-level
factor with levels <code>Cloudy</code>, <code>PC</code> and <code>Clear</code>.
</p>
</li>
<li> <p><code>CombClouds</code> (<em>combined cloud cover</em>): a three-level factor
with levels <code>Cloudy</code>, <code>PC</code> and <code>Clear</code>.
</p>
</li>
<li> <p><code>CldShadeOth</code> (<em>cloud shading, other</em>): a three-level factor
with levels <code>Cloudy</code>, <code>PC</code> and <code>Clear</code>.
</p>
</li>
<li> <p><code>AMInstabMt</code> (<em>AM instability in the mountains</em>): a
three-level factor with levels <code>None</code>, <code>Weak</code> and <code>Strong</code>.
</p>
</li>
<li> <p><code>InsInMt</code> (<em>instability in the mountains</em>): a three-level
factor with levels <code>None</code>, <code>Weak</code> and <code>Strong</code>.
</p>
</li>
<li> <p><code>WndHodograph</code> (<em>wind hodograph</em>): a four-level factor with
levels <code>DCVZFavor</code>, <code>StrongWest</code>, <code>Westerly</code> and
<code>Other</code>.
</p>
</li>
<li> <p><code>OutflowFrMt</code> (<em>outflow from mountains</em>): a three-level
factor with levels <code>None</code>, <code>Weak</code> and <code>Strong</code>.
</p>
</li>
<li> <p><code>MorningBound</code> (<em>morning boundaries</em>): a three-level factor
with levels <code>None</code>, <code>Weak</code> and <code>Strong</code>.
</p>
</li>
<li> <p><code>Boundaries</code> (<em>boundaries</em>): a three-level factor with
levels <code>None</code>, <code>Weak</code> and <code>Strong</code>.
</p>
</li>
<li> <p><code>CldShadeConv</code> (<em>cloud shading, convection</em>): a three-level
factor with levels <code>None</code>, <code>Some</code> and <code>Marked</code>.
</p>
</li>
<li> <p><code>CompPlFcst</code> (<em>composite plains forecast</em>): a three-level
factor with levels <code>IncCapDecIns</code>, <code>LittleChange</code> and
<code>DecCapIncIns</code>.
</p>
</li>
<li> <p><code>CapChange</code> (<em>capping change</em>): a three-level factor with
levels <code>Decreasing</code>, <code>LittleChange</code> and <code>Increasing</code>.
</p>
</li>
<li> <p><code>LoLevMoistAd</code> (<em>low-level moisture advection</em>): a
four-level factor with levels <code>StrongPos</code>, <code>WeakPos</code>,
<code>Neutral</code> and <code>Negative</code>.
</p>
</li>
<li> <p><code>InsChange</code> (<em>instability change</em>):  three-level factor with
levels <code>Decreasing</code>, <code>LittleChange</code> and <code>Increasing</code>.
</p>
</li>
<li> <p><code>MountainFcst</code> (<em>mountains (region 1) forecast</em>): a
three-level factor with levels <code>XNIL</code>, <code>SIG</code> and <code>SVR</code>.
</p>
</li>
<li> <p><code>Date</code> (<em>date</em>): a six-level factor with levels
<code>May15_Jun14</code>, <code>Jun15_Jul1</code>, <code>Jul2_Jul15</code>,
<code>Jul16_Aug10</code>, <code>Aug11_Aug20</code> and <code>Aug20_Sep15</code>.
</p>
</li>
<li> <p><code>Scenario</code> (<em>scenario</em>): an eleven-level factor with levels
<code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code>, <code>F</code>, <code>G</code>,
<code>H</code>, <code>I</code>, <code>J</code> and <code>K</code>.
</p>
</li>
<li> <p><code>ScenRelAMCIN</code> (<em>scenario relevant to AM convective
inhibition</em>): a two-level factor with levels <code>AB</code> and <code>CThruK</code>.
</p>
</li>
<li> <p><code>MorningCIN</code> (<em>morning convective inhibition</em>): a four-level
factor with levels <code>None</code>, <code>PartInhibit</code>, <code>Stifling</code> and
<code>TotalInhibit</code>.
</p>
</li>
<li> <p><code>AMCINInScen</code> (<em>AM convective inhibition in scenario</em>): a
three-level factor with levels <code>LessThanAve</code>, <code>Average</code> and
<code>MoreThanAve</code>.
</p>
</li>
<li> <p><code>CapInScen</code> (<em>capping withing scenario</em>): a three-level
factor with levels <code>LessThanAve</code>, <code>Average</code> and
<code>MoreThanAve</code>.
</p>
</li>
<li> <p><code>ScenRelAMIns</code> (<em>scenario relevant to AM instability</em>): a
six-level factor with levels <code>ABI</code>, <code>CDEJ</code>, <code>F</code>, <code>G</code>,
<code>H</code> and <code>K</code>.
</p>
</li>
<li> <p><code>LIfr12ZDENSd</code> (<em>LI from 12Z DEN sounding</em>): a four-level
factor with levels <code>LIGt0</code>, <code>N1GtLIGt_4</code>, <code>N5GtLIGt_8</code>
and <code>LILt_8</code>.
</p>
</li>
<li> <p><code>AMDewptCalPl</code> (<em>AM dewpoint calculations, plains</em>): a
three-level factor with levels <code>Instability</code>, <code>Neutral</code> and
<code>Stability</code>.
</p>
</li>
<li> <p><code>AMInsWliScen</code> (<em>AM instability within scenario</em>): a
three-level factor with levels <code>LessUnstable</code>, <code>Average</code> and
<code>MoreUnstable</code>.
</p>
</li>
<li> <p><code>InsSclInScen</code> (<em>instability scaling within scenario</em>): a
three-level factor with levels <code>LessUnstable</code>, <code>Average</code> and
<code>MoreUnstable</code>.
</p>
</li>
<li> <p><code>ScenRel34</code> (<em>scenario relevant to regions 2/3/4</em>): a
five-level factor with levels <code>ACEFK</code>, <code>B</code>, <code>D</code>, <code>GJ</code>
and <code>HI</code>.
</p>
</li>
<li> <p><code>LatestCIN</code> (<em>latest convective inhibition</em>): a four-level
factor with levels <code>None</code>, <code>PartInhibit</code>, <code>Stifling</code> and
<code>TotalInhibit</code>.
</p>
</li>
<li> <p><code>LLIW</code> (<em>LLIW severe weather index</em>): a four-level factor
with levels <code>Unfavorable</code>, <code>Weak</code>, <code>Moderate</code> and
<code>Strong</code>.
</p>
</li>
<li> <p><code>CurPropConv</code> (<em>current propensity to convection</em>): a
four-level factor with levels <code>None</code>, <code>Slight</code>, <code>Moderate</code>
and <code>Strong</code>.
</p>
</li>
<li> <p><code>ScnRelPlFcst</code> (<em>scenario relevant to plains forecast</em>): an
eleven-level factor with levels <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>,
<code>E</code>, <code>F</code>, <code>G</code>, <code>H</code>, <code>I</code>, <code>J</code> and <code>K</code>.
</p>
</li>
<li> <p><code>PlainsFcst</code> (<em>plains forecast</em>): a three-level factor with
levels <code>XNIL</code>, <code>SIG</code> and <code>SVR</code>.
</p>
</li>
<li> <p><code>N34StarFcst</code> (<em>regions 2/3/4 forecast</em>): a three-level
factor with levels <code>XNIL</code>, <code>SIG</code> and <code>SVR</code>.
</p>
</li>
<li> <p><code>R5Fcst</code> (<em>region 5 forecast</em>): a three-level factor with
levels <code>XNIL</code>, <code>SIG</code> and <code>SVR</code>.
</p>
</li>
<li> <p><code>Dewpoints</code> (<em>dewpoints</em>): a seven-level factor with levels
<code>LowEverywhere</code>, <code>LowAtStation</code>, <code>LowSHighN</code>,
<code>LowNHighS</code>, <code>LowMtsHighPl</code>, <code>HighEverywher</code>, <code>Other</code>.
</p>
</li>
<li> <p><code>LowLLapse</code> (<em>low-level lapse rate</em>): a four-level factor
with levels <code>CloseToDryAd</code>, <code>Steep</code>, <code>ModerateOrLe</code> and
<code>Stable</code>.
</p>
</li>
<li> <p><code>MeanRH</code> (<em>mean relative humidity</em>): a three-level factor
with levels <code>VeryMoist</code>, <code>Average</code> and <code>Dry</code>.
</p>
</li>
<li> <p><code>MidLLapse</code> (<em>mid-level lapse rate</em>): a three-level factor
with levels <code>CloseToDryAd</code>, <code>Steep</code> and <code>ModerateOrLe</code>.
</p>
</li>
<li> <p><code>MvmtFeatures</code> (<em>movement of features</em>): a four-level factor
with levels <code>StrongFront</code>, <code>MarkedUpper</code>, <code>OtherRapid</code> and
<code>NoMajor</code>.
</p>
</li>
<li> <p><code>RHRatio</code> (<em>realtive humidity ratio</em>): a three-level factor
with levels <code>MoistMDryL</code>, <code>DryMMoistL</code> and <code>other</code>.
</p>
</li>
<li> <p><code>SfcWndShfDis</code> (<em>surface wind shifts and discontinuities</em>):
a seven-level factor with levels <code>DenvCyclone</code>, <code>E_W_N</code>,
<code>E_W_S</code>, <code>MovigFtorOt</code>, <code>DryLine</code>, <code>None</code> and
<code>Other</code>.
</p>
</li>
<li> <p><code>SynForcng</code> (<em>synoptic forcing</em>): a five-level factor with
levels <code>SigNegative</code>, <code>NegToPos</code>, <code>SigPositive</code>,
<code>PosToNeg</code> and <code>LittleChange</code>.
</p>
</li>
<li> <p><code>TempDis</code> (<em>temperature discontinuities</em>): a four-level
factor with levels <code>QStationary</code>, <code>Moving</code>, <code>None</code>,
<code>Other</code>.
</p>
</li>
<li> <p><code>WindAloft</code> (<em>wind aloft</em>): a four-level factor with levels
<code>LV</code>, <code>SWQuad</code>, <code>NWQuad</code>, <code>AllElse</code>.
</p>
</li>
<li> <p><code>WindFieldMt</code> (<em>wind fields, mountains</em>): a two-level
factor with levels <code>Westerly</code> and <code>LVorOther</code>.
</p>
</li>
<li> <p><code>WindFieldPln</code> (<em>wind fields, plains</em>): a six-level factor
with levels <code>LV</code>, <code>DenvCyclone</code>, <code>LongAnticyc</code>,
<code>E_NE</code>, <code>SEquad</code> and <code>WidespdDnsl</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The complete BN can be downloaded from
<a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>


<h3>Source</h3>

<p>Abramson B, Brown J, Edwards W, Murphy A, Winkler RL (1996). &quot;Hailfinder: A
Bayesian system for forecasting severe weather&quot;. <em>International
Journal of Forecasting</em>, <strong>12</strong>(1):57&ndash;71.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(hailfinder)
# create and plot the network structure.
modelstring = paste0("[N07muVerMo][SubjVertMo][QGVertMotion][SatContMoist][RaoContMoist]",
  "[VISCloudCov][IRCloudCover][AMInstabMt][WndHodograph][MorningBound][LoLevMoistAd][Date]",
  "[MorningCIN][LIfr12ZDENSd][AMDewptCalPl][LatestCIN][LLIW]",
  "[CombVerMo|N07muVerMo:SubjVertMo:QGVertMotion][CombMoisture|SatContMoist:RaoContMoist]",
  "[CombClouds|VISCloudCov:IRCloudCover][Scenario|Date][CurPropConv|LatestCIN:LLIW]",
  "[AreaMesoALS|CombVerMo][ScenRelAMCIN|Scenario][ScenRelAMIns|Scenario][ScenRel34|Scenario]",
  "[ScnRelPlFcst|Scenario][Dewpoints|Scenario][LowLLapse|Scenario][MeanRH|Scenario]",
  "[MidLLapse|Scenario][MvmtFeatures|Scenario][RHRatio|Scenario][SfcWndShfDis|Scenario]",
  "[SynForcng|Scenario][TempDis|Scenario][WindAloft|Scenario][WindFieldMt|Scenario]",
  "[WindFieldPln|Scenario][AreaMoDryAir|AreaMesoALS:CombMoisture]",
  "[AMCINInScen|ScenRelAMCIN:MorningCIN][AMInsWliScen|ScenRelAMIns:LIfr12ZDENSd:AMDewptCalPl]",
  "[CldShadeOth|AreaMesoALS:AreaMoDryAir:CombClouds][InsInMt|CldShadeOth:AMInstabMt]",
  "[OutflowFrMt|InsInMt:WndHodograph][CldShadeConv|InsInMt:WndHodograph][MountainFcst|InsInMt]",
  "[Boundaries|WndHodograph:OutflowFrMt:MorningBound][N34StarFcst|ScenRel34:PlainsFcst]",
  "[CompPlFcst|AreaMesoALS:CldShadeOth:Boundaries:CldShadeConv][CapChange|CompPlFcst]",
  "[InsChange|CompPlFcst:LoLevMoistAd][CapInScen|CapChange:AMCINInScen]",
  "[InsSclInScen|InsChange:AMInsWliScen][R5Fcst|MountainFcst:N34StarFcst]",
  "[PlainsFcst|CapInScen:InsSclInScen:CurPropConv:ScnRelPlFcst]")
dag = model2network(modelstring)
## Not run: graphviz.plot(dag, shape = "ellipse")
</code></pre>

<hr>
<h2 id='hybrid+20algorithms'>Hybrid structure learning algorithms</h2><span id='topic+hybrid+20algorithms'></span><span id='topic+rsmax2'></span><span id='topic+mmhc'></span><span id='topic+h2pc'></span>

<h3>Description</h3>

<p>Learn the structure of a Bayesian network with Max-Min Hill Climbing (MMHC),
Hybrid HPC (H2PC), and the more general 2-phase Restricted Maximization
(RSMAX2) hybrid algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsmax2(x, whitelist = NULL, blacklist = NULL, restrict = "si.hiton.pc",
  maximize = "hc", restrict.args = list(), maximize.args = list(), debug = FALSE)
mmhc(x, whitelist = NULL, blacklist = NULL, restrict.args = list(),
  maximize.args = list(), debug = FALSE)
h2pc(x, whitelist = NULL, blacklist = NULL, restrict.args = list(),
  maximize.args = list(), debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hybrid+2B20algorithms_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot; and
&quot;to&quot;), containing a set of arcs to be included in the graph.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot; and
&quot;to&quot;), containing a set of arcs not to be included in the graph.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_restrict">restrict</code></td>
<td>
<p>a character string, the constraint-based or local search
algorithm to be used in the &ldquo;restrict&rdquo; phase. See
<code><a href="#topic+structure+20learning">structure learning</a></code> and the documentation of each algorithm for
details.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_maximize">maximize</code></td>
<td>
<p>a character string, the score-based algorithm to be used in
the &ldquo;maximize&rdquo; phase. Possible values are <code>hc</code> and <code>tabu</code>.
See <code><a href="#topic+structure+20learning">structure learning</a></code> for details.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_restrict.args">restrict.args</code></td>
<td>
<p>a list of arguments to be passed to the algorithm
specified by <code>restrict</code>, such as <code>test</code> or <code>alpha</code>.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_maximize.args">maximize.args</code></td>
<td>
<p>a list of arguments to be passed to the algorithm
specified by <code>maximize</code>, such as <code>restart</code> for hill-climbing or
<code>tabu</code> for tabu search.</p>
</td></tr>
<tr><td><code id="hybrid+2B20algorithms_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bn</code>. See <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>


<h3>Note</h3>

<p><code>mmhc()</code> is simply <code>rsmax2()</code> with <code>restrict</code> set to
<code>mmpc</code> and <code>maximize</code> set to <code>hc</code>. Similarly, <code>h2pc</code> is
simply <code>rsmax2()</code> with <code>restrict</code> set to <code>hpc</code>and
<code>maximize</code> set to <code>hc</code>.
</p>
<p>See <code><a href="#topic+structure+20learning">structure learning</a></code> for a complete list of structure learning
algorithms with the respective references.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><a href="#topic+local+20discovery+20algorithms">local discovery algorithms</a>,
<a href="#topic+score-based+20algorithms">score-based algorithms</a>, <a href="#topic+constraint-based+20algorithms">constraint-based algorithms</a>.</p>

<hr>
<h2 id='igraph+20integration'>Import and export networks from the igraph package</h2><span id='topic+igraph+20integration'></span><span id='topic+as.bn.igraph'></span><span id='topic+as.igraph'></span><span id='topic+as.igraph.bn'></span><span id='topic+as.igraph.bn.fit'></span>

<h3>Description</h3>

<p>Convert <code>bn</code> and <code>bn.fit</code> objects to <code>igraph</code> and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'igraph'
as.bn(x, ..., check.cycles = TRUE)
## S3 method for class 'bn'
as.igraph(x, ...)
## S3 method for class 'bn.fit'
as.igraph(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="igraph+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>, <code>bn.fit</code>, or <code>igraph</code>.</p>
</td></tr>
<tr><td><code id="igraph+2B20integration_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
<tr><td><code id="igraph+2B20integration_+3A_check.cycles">check.cycles</code></td>
<td>
<p>a boolean value. If <code>FALSE</code> the returned network will
not be checked for cycles.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the relevant class.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
a = bn.fit(hc(learning.test), learning.test)
b = as.igraph(a)
plot(b, edge.arrow.mode = 2L * !igraph::which_mutual(b))
c = as.bn(b)
## End(Not run)</code></pre>

<hr>
<h2 id='independence-tests'>Conditional independence tests</h2><span id='topic+independence-tests'></span><span id='topic+independence+20tests'></span>

<h3>Description</h3>

<p>Overview of the conditional independence tests implemented in <span class="pkg">bnlearn</span>,
with the respective reference publications.
</p>


<h3>Details</h3>

<p>Unless otherwise noted, the reference publication for conditional independence
tests is:
</p>
<p>Edwards DI (2000). <em>Introduction to Graphical Modelling</em>. Springer, 2nd
edition.
</p>
<p>Additionally for continuous permutation tests:
</p>
<p>Legendre P (2000). &quot;Comparison of Permutation Methods for the Partial
Correlation and Partial Mantel Tests&quot;. <em>Journal of Statistical
Computation and Simulation</em>, <strong>67</strong>:37&ndash;73.
</p>
<p>and for semiparametric discrete tests:
</p>
<p>Tsamardinos I, Borboudakis G (2010). &quot;Permutation Testing Improves Bayesian
Network Learning&quot;. <em>Machine Learning and Knowledge Discovery in
Databases</em>, 322&ndash;337.
</p>
<p>Available conditional independence tests (and the respective labels) for
<em>discrete Bayesian networks</em> (categorical variables) are:
</p>

<ul>
<li> <p><em>mutual information</em>: an information-theoretic distance measure.
It's proportional to the log-likelihood ratio (they differ by a
<code class="reqn">2n</code> factor) and is related to the deviance of the tested models.
The asymptotic <code class="reqn">\chi^2</code> test (<code>mi</code> and <code>mi-adf</code>,
with adjusted degrees of freedom), the Monte Carlo permutation test
(<code>mc-mi</code>), the sequential Monte Carlo permutation test
(<code>smc-mi</code>), and the semiparametric test (<code>sp-mi</code>) are
implemented.
</p>
</li>
<li> <p><em>shrinkage estimator</em> for the <em>mutual information</em>
(<code>mi-sh</code>): an improved asymptotic <code class="reqn">\chi^2</code> test
based on the James-Stein estimator for the mutual information.
</p>
<p>Hausser J, Strimmer K (2009). &quot;Entropy inference and the James-Stein
estimator, with application to nonlinear gene association networks&quot;.
<em>Statistical Applications in Genetics and Molecular Biology</em>,
<strong>10</strong>:1469&ndash;1484.
</p>
</li>
<li> <p><em>Pearson's <code class="reqn">X^2</code></em>: the classical Pearson's
<code class="reqn">X^2</code> test for contingency tables. The asymptotic
<code class="reqn">\chi^2</code> test (<code>x2</code> and <code>x2-adf</code>, with adjusted
degrees of freedom), the Monte Carlo permutation test (<code>mc-x2</code>), the
sequential Monte Carlo permutation test (<code>smc-x2</code>) and semiparametric
test (<code>sp-x2</code>) are implemented.
</p>
</li></ul>

<p>Available conditional independence tests (and the respective labels) for
<em>discrete Bayesian networks</em> (ordered factors) are:
</p>

<ul>
<li> <p><em>Jonckheere-Terpstra</em>: a trend test for ordinal variables. The
asymptotic normal test (<code>jt</code>), the Monte Carlo permutation test
(<code>mc-jt</code>) and the sequential Monte Carlo permutation test
(<code>smc-jt</code>) are implemented.
</p>
</li></ul>

<p>Available conditional independence tests (and the respective labels) for
<em>Gaussian Bayesian networks</em> (normal variables) are:
</p>

<ul>
<li> <p><em>linear correlation</em>: Pearson's linear correlation. The exact
Student's t test (<code>cor</code>), the Monte Carlo permutation test
(<code>mc-cor</code>) and the sequential Monte Carlo permutation test
(<code>smc-cor</code>) are implemented.
</p>
<p>Hotelling H (1953). &quot;New Light on the Correlation Coefficient and its
Transforms&quot;. <em>Journal of the Royal Statistical Society: Series B</em>,
<strong>15</strong>(2):193&ndash;225.
</p>
</li>
<li> <p><em>Fisher's Z</em>: a transformation of the linear correlation with
asymptotic normal distribution. The asymptotic normal test (<code>zf</code>),
the Monte Carlo permutation test (<code>mc-zf</code>) and the sequential Monte
Carlo permutation test (<code>smc-zf</code>) are implemented.
</p>
</li>
<li> <p><em>mutual information</em>: an information-theoretic distance measure.
Again it is proportional to the log-likelihood ratio (they differ by a
<code class="reqn">2n</code> factor). The asymptotic <code class="reqn">\chi^2</code> test
(<code>mi-g</code>), the Monte Carlo permutation test (<code>mc-mi-g</code>) and the
sequential Monte Carlo permutation test (<code>smc-mi-g</code>) are implemented.
</p>
</li>
<li> <p><em>shrinkage estimator</em> for the <em>mutual information</em>
(<code>mi-g-sh</code>): an improved asymptotic <code class="reqn">\chi^2</code> test
based on the James-Stein estimator for the mutual information.
</p>
<p>Ledoit O, Wolf M (2003). &quot;Improved Estimation of the Covariance Matrix
of Stock Returns with an Application to Portfolio Selection&quot;.
<em>Journal of Empirical Finance</em>, <strong>10</strong>:603&ndash;621.
</p>
</li></ul>

<p>Available conditional independence tests (and the respective labels) for
<em>hybrid Bayesian networks</em> (mixed discrete and normal variables) are:
</p>

<ul>
<li> <p><em>mutual information</em>: an information-theoretic distance measure.
Again it is proportional to the log-likelihood ratio (they differ by a
<code class="reqn">2n</code> factor). Only the asymptotic <code class="reqn">\chi^2</code> test
(<code>mi-cg</code>) is implemented.
</p>
</li></ul>


<hr>
<h2 id='insurance'>Insurance evaluation network (synthetic) data set</h2><span id='topic+insurance'></span>

<h3>Description</h3>

<p>Insurance is a network for evaluating car insurance risks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(insurance)
</code></pre>


<h3>Format</h3>

<p>The <code>insurance</code> data set contains the following 27 variables:
</p>

<ul>
<li> <p><code>GoodStudent</code> (<em>good student</em>): a two-level factor with
levels <code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>Age</code> (<em>age</em>): a three-level factor with levels
<code>Adolescent</code>, <code>Adult</code> and <code>Senior</code>.
</p>
</li>
<li> <p><code>SocioEcon</code> (<em>socio-economic status</em>): a four-level factor
with levels <code>Prole</code>, <code>Middle</code>, <code>UpperMiddle</code> and
<code>Wealthy</code>.
</p>
</li>
<li> <p><code>RiskAversion</code> (<em>risk aversion</em>): a four-level factor with
levels <code>Psychopath</code>, <code>Adventurous</code>, <code>Normal</code> and
<code>Cautious</code>.
</p>
</li>
<li> <p><code>VehicleYear</code> (<em>vehicle age</em>): a two-level factor with
levels <code>Current</code> and <code>older</code>.
</p>
</li>
<li> <p><code>ThisCarDam</code> (<em>damage to this car</em>): a four-level factor
with levels <code>None</code>, <code>Mild</code>, <code>Moderate</code> and <code>Severe</code>.
</p>
</li>
<li> <p><code>RuggedAuto</code> (<em>ruggedness of the car</em>): a three-level
factor with levels <code>EggShell</code>, <code>Football</code> and <code>Tank</code>.
</p>
</li>
<li> <p><code>Accident</code> (<em>severity of the accident</em>): a four-level factor
with levels <code>None</code>, <code>Mild</code>, <code>Moderate</code> and <code>Severe</code>.
</p>
</li>
<li> <p><code>MakeModel</code> (<em>car's model</em>): a five-level factor with levels
<code>SportsCar</code>, <code>Economy</code>, <code>FamilySedan</code>, <code>Luxury</code> and
<code>SuperLuxury</code>.
</p>
</li>
<li> <p><code>DrivQuality</code> (<em>driving quality</em>): a three-level factor with
levels <code>Poor</code>, <code>Normal</code> and <code>Excellent</code>.
</p>
</li>
<li> <p><code>Mileage</code> (<em>mileage</em>): a four-level factor with levels
<code>FiveThou</code>, <code>TwentyThou</code>, <code>FiftyThou</code> and <code>Domino</code>.
</p>
</li>
<li> <p><code>Antilock</code> (<em>ABS</em>): a two-level factor with levels
<code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>DrivingSkill</code> (<em>driving skill</em>): a three-level factor with
levels <code>SubStandard</code>, <code>Normal</code> and <code>Expert</code>.
</p>
</li>
<li> <p><code>SeniorTrain</code> (<em>senior training</em>): a two-level factor with
levels <code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>ThisCarCost</code> (<em>costs for the insured car</em>): a four-level
factor with levels <code>Thousand</code>, <code>TenThou</code>, <code>HundredThou</code>
and <code>Million</code>.
</p>
</li>
<li> <p><code>Theft</code> (<em>theft</em>): a two-level factor with levels
<code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>CarValue</code> (<em>value of the car</em>): a five-level factor with
levels <code>FiveThou</code>, <code>TenThou</code>, <code>TwentyThou</code>,
<code>FiftyThou</code> and <code>Million</code>.
</p>
</li>
<li> <p><code>HomeBase</code> (<em>neighbourhood type</em>): a four-level factor with
levels <code>Secure</code>, <code>City</code>, <code>Suburb</code> and <code>Rural</code>.
</p>
</li>
<li> <p><code>AntiTheft</code> (<em>anti-theft system</em>): a two-level factor with
levels <code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>PropCost</code> (<em>ratio of the cost for the two cars</em>): a
four-level factor with levels <code>Thousand</code>, <code>TenThou</code>,
<code>HundredThou</code> and <code>Million</code>.
</p>
</li>
<li> <p><code>OtherCarCost</code> (<em>costs for the other car</em>): a four-level
factor with levels <code>Thousand</code>, <code>TenThou</code>, <code>HundredThou</code>
and <code>Million</code>.
</p>
</li>
<li> <p><code>OtherCar</code> (<em>other cars involved in the accident</em>): a
two-level factor with levels <code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>MedCost</code> (<em>cost of the medical treatment</em>): a four-level
factor with levels <code>Thousand</code>, <code>TenThou</code>, <code>HundredThou</code>
and <code>Million</code>.
</p>
</li>
<li> <p><code>Cushioning</code> (<em>cushioning</em>): a four-level factor with
levels <code>Poor</code>, <code>Fair</code>, <code>Good</code> and <code>Excellent</code>.
</p>
</li>
<li> <p><code>Airbag</code> (<em>airbag</em>): a two-level factor with levels
<code>False</code> and <code>True</code>.
</p>
</li>
<li> <p><code>ILiCost</code> (<em>inspection cost</em>): a four-level factor with
levels <code>Thousand</code>, <code>TenThou</code>, <code>HundredThou</code> and
<code>Million</code>.
</p>
</li>
<li> <p><code>DrivHist</code> (<em>driving history</em>): a three-level factor with
levels <code>Zero</code>, <code>One</code> and <code>Many</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The complete BN can be downloaded from
<a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a>.
</p>


<h3>Source</h3>

<p>Binder J, Koller D, Russell S, Kanazawa K (1997). &quot;Adaptive Probabilistic
Networks with Hidden Variables&quot;. <em>Machine Learning</em>,
<strong>29</strong>(2&ndash;3):213&ndash;244.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(insurance)
# create and plot the network structure.
modelstring = paste0("[Age][Mileage][SocioEcon|Age][GoodStudent|Age:SocioEcon]",
  "[RiskAversion|Age:SocioEcon][OtherCar|SocioEcon][VehicleYear|SocioEcon:RiskAversion]",
  "[MakeModel|SocioEcon:RiskAversion][SeniorTrain|Age:RiskAversion]",
  "[HomeBase|SocioEcon:RiskAversion][AntiTheft|SocioEcon:RiskAversion]",
  "[RuggedAuto|VehicleYear:MakeModel][Antilock|VehicleYear:MakeModel]",
  "[DrivingSkill|Age:SeniorTrain][CarValue|VehicleYear:MakeModel:Mileage]",
  "[Airbag|VehicleYear:MakeModel][DrivQuality|RiskAversion:DrivingSkill]",
  "[Theft|CarValue:HomeBase:AntiTheft][Cushioning|RuggedAuto:Airbag]",
  "[DrivHist|RiskAversion:DrivingSkill][Accident|DrivQuality:Mileage:Antilock]",
  "[ThisCarDam|RuggedAuto:Accident][OtherCarCost|RuggedAuto:Accident]",
  "[MedCost|Age:Accident:Cushioning][ILiCost|Accident]",
  "[ThisCarCost|ThisCarDam:Theft:CarValue][PropCost|ThisCarCost:OtherCarCost]")
dag = model2network(modelstring)
## Not run: graphviz.plot(dag, shape = "ellipse")
</code></pre>

<hr>
<h2 id='KL'>Compute the distance between two fitted Bayesian networks</h2><span id='topic+KL'></span>

<h3>Description</h3>

<p>Compute the Kullback-Leibler divergence between two fitted Bayesian networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL(P, Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KL_+3A_p">P</code>, <code id="KL_+3A_q">Q</code></td>
<td>
<p>two objects of class <code>bn.fit</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>KL()</code> returns a numeric value.
</p>


<h3>Note</h3>

<p><code>KL()</code> only supports discrete (<code>bn.fit.dnet</code>) and Gaussian
(<code>bn.fit.gnet</code>) networks. Note that in the case of Gaussian netwoks
the divergence can be negative. Regardless of the type of network, if at least
one of the two networks is singular the divergence can be <code>+Inf</code>.
</p>
<p>If any of the parameters of the two networks are <code>NA</code>s, the divergence
will also be <code>NA</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'># discrete networks
dag = model2network("[A][C][F][B|A][D|A:C][E|B:F]")
fitted1 = bn.fit(dag, learning.test, method = "mle")
fitted2 = bn.fit(dag, learning.test, method = "bayes", iss = 20)

KL(fitted1, fitted1)
KL(fitted2, fitted2)
KL(fitted1, fitted2)

# continuous, singular networks.
dag = model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]")
singular = fitted1 = bn.fit(dag, gaussian.test)
singular$A = list(coef = coef(fitted1[["A"]]) + runif(1), sd = 0)

KL(singular, fitted1)
KL(fitted1, singular)
</code></pre>

<hr>
<h2 id='learning.test'>Synthetic (discrete) data set to test learning algorithms</h2><span id='topic+learning.test'></span>

<h3>Description</h3>

<p>This a synthetic data set used as a test case in the <span class="pkg">bnlearn</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(learning.test)
</code></pre>


<h3>Format</h3>

<p>The <code>learning.test</code> data set contains the following variables:
</p>

<ul>
<li> <p><code>A</code>, a three-level factor with levels <code>a</code>, <code>b</code> and
<code>c</code>.
</p>
</li>
<li> <p><code>B</code>, a three-level factor with levels <code>a</code>, <code>b</code> and
<code>c</code>.
</p>
</li>
<li> <p><code>C</code>, a three-level factor with levels <code>a</code>, <code>b</code> and
<code>c</code>.
</p>
</li>
<li> <p><code>D</code>, a three-level factor with levels <code>a</code>, <code>b</code> and
<code>c</code>.
</p>
</li>
<li> <p><code>E</code>, a three-level factor with levels <code>a</code>, <code>b</code> and
<code>c</code>.
</p>
</li>
<li> <p><code>F</code>, a two-level factor with levels <code>a</code> and <code>b</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The R script to generate data from this network is available from
<a href="https://www.bnlearn.com/documentation/networks/">https://www.bnlearn.com/documentation/networks/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(learning.test)
# create and plot the network structure.
dag = model2network("[A][C][F][B|A][D|A:C][E|B:F]")
## Not run: graphviz.plot(dag)
</code></pre>

<hr>
<h2 id='lizards'>Lizards' perching behaviour data set</h2><span id='topic+lizards'></span>

<h3>Description</h3>

<p>Real-world data set about the perching behaviour of two species of lizards in
the South Bimini island, from Shoener (1968).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lizards)
</code></pre>


<h3>Format</h3>

<p>The <code>lizards</code> data set contains the following variables:
</p>

<ul>
<li> <p><code>Species</code> (<em>the species of the lizard</em>): a two-level factor
with levels <code>Sagrei</code> and <code>Distichus</code>.
</p>
</li>
<li> <p><code>Height</code> (<em>perch height</em>): a two-level factor with levels
<code>high</code> (greater than 4.75 feet) and <code>low</code> (lesser or equal to
4.75 feet).
</p>
</li>
<li> <p><code>Diameter</code> (<em>perch diameter</em>): a two-level factor with
levels <code>narrow</code> (greater than 4 inches) and <code>wide</code> (lesser or
equal to 4 inches).
</p>
</li></ul>



<h3>Source</h3>

<p>Edwards DI (2000). <em>Introduction to Graphical Modelling</em>. Springer, 2nd
edition.
</p>
<p>Fienberg SE (1980). <em>The Analysis of Cross-Classified Categorical Data</em>.
Springer, 2nd edition.
</p>
<p>Schoener TW (1968). &quot;The Anolis Lizards of Bimini: Resource Partitioning in a
Complex Fauna&quot;. <em>Ecology</em>, <strong>49</strong>(4):704&ndash;726.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the data.
data(lizards)
# create and plot the network structure.
dag = model2network("[Species][Diameter|Species][Height|Species]")
## Not run: graphviz.plot(dag, shape = "ellipse")

# This data set is useful as it offers nominal values for
# the conditional mutual information and X^2 tests.
ci.test("Height", "Diameter", "Species", test = "mi", data = lizards)
ci.test("Height", "Diameter", "Species", test = "x2", data = lizards)
</code></pre>

<hr>
<h2 id='lm+20integration'>Produce lm objects from Bayesian networks</h2><span id='topic+lm+20integration'></span><span id='topic+as.lm'></span><span id='topic+as.lm.bn'></span><span id='topic+as.lm.bn.fit'></span><span id='topic+as.lm.bn.fit.gnode'></span>

<h3>Description</h3>

<p>Take a <code>bn</code> object or <code>bn.fit</code> object encoding a Gaussian network
and refit all the local distributions using <code>lm()</code>. This makes it
possible to use all the functions provided by R for <code>lm</code> objects 
(<code>summary</code>, <code>anova</code>, etc.) to investigate the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bn'
as.lm(x, data, ...)
## S3 method for class 'bn.fit'
as.lm(x, data, ...)
## S3 method for class 'bn.fit.gnode'
as.lm(x, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>, <code>bn.fit</code> or <code>bn.fit.gnode</code>.</p>
</td></tr>
<tr><td><code id="lm+2B20integration_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="lm+2B20integration_+3A_...">...</code></td>
<td>
<p>additional arguments, currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>x</code> is an object of class <code>bn</code> or <code>bn.fit</code>, <code>as.lm()</code>
returns a list of <code>lm</code> objects, one for each node in <code>x</code>. If
<code>x</code> is an object of class <code>bn</code> or <code>bn.fit.gnode</code>,
<code>as.lm()</code> returns a single <code>lm</code> object.
</p>


<h3>Author(s)</h3>

<p>Marco</p>


<h3>Examples</h3>

<pre><code class='language-R'>dag = hc(gaussian.test)
fitted = bn.fit(dag, gaussian.test)
as.lm(dag, gaussian.test)
as.lm(fitted, gaussian.test)
as.lm(fitted$F, gaussian.test)
</code></pre>

<hr>
<h2 id='local+20discovery+20algorithms'>Local discovery structure learning algorithms</h2><span id='topic+local+20discovery+20algorithms'></span><span id='topic+aracne'></span><span id='topic+chow.liu'></span>

<h3>Description</h3>

<p>ARACNE and Chow-Liu learn simple graphs structures from data using pairwise
mutual information coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aracne(x, whitelist = NULL, blacklist = NULL, mi = NULL, debug = FALSE)
chow.liu(x, whitelist = NULL, blacklist = NULL, mi = NULL, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local+2B20discovery+2B20algorithms_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="local+2B20discovery+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs to be included in the graph.</p>
</td></tr>
<tr><td><code id="local+2B20discovery+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs not to be included in the graph.</p>
</td></tr>
<tr><td><code id="local+2B20discovery+2B20algorithms_+3A_mi">mi</code></td>
<td>
<p>a character string, the estimator used for the pairwise (i.e.
unconditional) mutual information coefficients in the ARACNE and Chow-Liu
algorithms. Possible values are <code>mi</code> (discrete mutual information)
and <code>mi-g</code> (Gaussian mutual information).</p>
</td></tr>
<tr><td><code id="local+2B20discovery+2B20algorithms_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bn</code>. See <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><a href="#topic+constraint-based+20algorithms">constraint-based algorithms</a>, <a href="#topic+score-based+20algorithms">score-based algorithms</a>,
<a href="#topic+hybrid+20algorithms">hybrid algorithms</a>.</p>

<hr>
<h2 id='marks'>Examination marks data set</h2><span id='topic+marks'></span>

<h3>Description</h3>

<p>Examination marks of 88 students on five different topics, from Mardia (1979).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(marks)
</code></pre>


<h3>Format</h3>

<p>The <code>marks</code> data set contains the following variables, one for each
topic in the examination:
</p>

<ul>
<li> <p><code>MECH</code> (<em>mechanics</em>)
</p>
</li>
<li> <p><code>VECT</code> (<em>vectors</em>)
</p>
</li>
<li> <p><code>ALG</code> (<em>algebra</em>)
</p>
</li>
<li> <p><code>ANL</code> (<em>analysis</em>)
</p>
</li>
<li> <p><code>STAT</code> (<em>statistics</em>)
</p>
</li></ul>

<p>All are measured on the same scale (0-100).
</p>


<h3>Source</h3>

<p>Edwards DI (2000). <em>Introduction to Graphical Modelling</em>. Springer, 2nd
edition.
</p>
<p>Mardia KV, Kent JT, Bibby JM (1979). <em>Multivariate Analysis</em>. Academic
Press.
</p>
<p>Whittaker J (1990). <em>Graphical Models in Applied Multivariate
Statistics</em>. Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This is the undirected graphical model from Edwards (2000).
data(marks)
ug = empty.graph(names(marks))
arcs(ug, check.cycles = FALSE) = matrix(
  c("MECH", "VECT", "MECH", "ALG", "VECT", "MECH", "VECT", "ALG",
    "ALG",  "MECH", "ALG", "VECT", "ALG",  "ANL", "ALG",  "STAT",
    "ANL",  "ALG", "ANL",  "STAT", "STAT", "ALG", "STAT", "ANL"),
  ncol = 2, byrow = TRUE,
  dimnames = list(c(), c("from", "to")))
## Not run: graphviz.plot(ug)
</code></pre>

<hr>
<h2 id='misc+20utilities'>Miscellaneous utilities</h2><span id='topic+misc+20utilities'></span><span id='topic+mb'></span><span id='topic+nbr'></span><span id='topic+arcs'></span><span id='topic+arcs+3C-'></span><span id='topic+directed.arcs'></span><span id='topic+undirected.arcs'></span><span id='topic+incoming.arcs'></span><span id='topic+outgoing.arcs'></span><span id='topic+incident.arcs'></span><span id='topic+compelled.arcs'></span><span id='topic+reversible.arcs'></span><span id='topic+narcs'></span><span id='topic+nnodes'></span><span id='topic+nodes'></span><span id='topic+nodes+2Cbn-method'></span><span id='topic+nodes+2Cbn.fit-method'></span><span id='topic+nodes+2Cbn.naive-method'></span><span id='topic+nodes+2Cbn.tan-method'></span><span id='topic+amat'></span><span id='topic+amat+3C-'></span><span id='topic+parents'></span><span id='topic+parents+3C-'></span><span id='topic+children'></span><span id='topic+children+3C-'></span><span id='topic+spouses'></span><span id='topic+ancestors'></span><span id='topic+descendants'></span><span id='topic+in.degree'></span><span id='topic+out.degree'></span><span id='topic+degree'></span><span id='topic+degree+2Cbn-method'></span><span id='topic+degree+2Cbn.fit-method'></span><span id='topic+degree+2Cbn.naive-method'></span><span id='topic+degree+2Cbn.tan-method'></span><span id='topic+root.nodes'></span><span id='topic+leaf.nodes'></span><span id='topic+nparams'></span><span id='topic+ntests'></span>

<h3>Description</h3>

<p>Assign or extract various quantities of interest from an object of class
<code>bn</code> of <code>bn.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## nodes
mb(x, node)
nbr(x, node)
parents(x, node)
parents(x, node, debug = FALSE) &lt;- value
children(x, node)
children(x, node, debug = FALSE) &lt;- value
spouses(x, node)
ancestors(x, node)
descendants(x, node)
in.degree(x, node)
out.degree(x, node)
root.nodes(x)
leaf.nodes(x)
nnodes(x)

## arcs
arcs(x)
arcs(x, check.cycles = TRUE, check.illegal = TRUE, debug = FALSE) &lt;- value
directed.arcs(x)
undirected.arcs(x)
incoming.arcs(x, node)
outgoing.arcs(x, node)
incident.arcs(x, node)
compelled.arcs(x)
reversible.arcs(x)
narcs(x)

## adjacency matrix
amat(x)
amat(x, check.cycles = TRUE, check.illegal = TRUE, debug = FALSE) &lt;- value

## graphs
nparams(x, data, effective = FALSE, debug = FALSE)
ntests(x)

## shared with the graph package.
# these used to be a simple nodes(x) function.
## S4 method for signature 'bn'
nodes(object)
## S4 method for signature 'bn.fit'
nodes(object)
# these used to be a simple degree(x, node) function.
## S4 method for signature 'bn'
degree(object, Nodes)
## S4 method for signature 'bn.fit'
degree(object, Nodes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misc+2B20utilities_+3A_x">x</code>, <code id="misc+2B20utilities_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>. The replacement
form of <code>parents</code>, <code>children</code>, <code>arcs</code> and <code>amat</code>
requires an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_node">node</code>, <code id="misc+2B20utilities_+3A_nodes">Nodes</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_value">value</code></td>
<td>
<p>either a vector of character strings (for <code>parents</code> and
<code>children</code>), an adjacency matrix (for <code>amat</code>) or a data frame with
two columns (optionally labeled &quot;from&quot; and &quot;to&quot;, for <code>arcs</code>).</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_data">data</code></td>
<td>
<p>a data frame containing the data the Bayesian network was learned
from. It's only needed if <code>x</code> is an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_check.cycles">check.cycles</code></td>
<td>
<p>a boolean value. If <code>FALSE</code> the returned network will
not be checked for cycles.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_check.illegal">check.illegal</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> arcs that break the
parametric assumptions of <code>x</code>, such as those from continuous to
discrete nodes in conditional Gaussian networks, cause an error.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_effective">effective</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the number of non-zero free
parameters is returned, that is, the effective degrees of freedom of the
network; otherwise the theoretical number of parameters is returned.</p>
</td></tr>
<tr><td><code id="misc+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of parameters of a discrete Bayesian network is defined as the
sum of the number of logically independent parameters of each node given its
parents (Chickering, 1995). For Gaussian Bayesian networks the distribution
of each node can be viewed as a linear regression, so it has a number of
parameters equal to the number of the parents of the node plus one (the
intercept) as per Neapolitan (2003). For conditional linear Gaussian networks,
the number of parameters of discrete and Gaussian nodes is as above. The
number of parameters of conditional Gaussian nodes is equal to <code>1</code> plus
the number of continuous parents (who get one regression coefficient each,
plus the intercept) times the number of configurations of the discrete parents
(each configuration has an associated regression model).
</p>


<h3>Value</h3>

<p><code>mb</code>, <code>nbr</code>, <code>nodes</code>, <code>parents</code>, <code>children</code>,
<code>spouses</code>, <code>ancestors</code>, <code>descendants</code>, <code>root.nodes</code> and
<code>leaf.nodes</code> return a vector of character strings.
</p>
<p><code>arcs</code>, <code>directed.arcs</code>, <code>undirected.arcs</code>,
<code>incoming.arcs</code>, <code>outgoing.arcs</code>, <code>incident.arcs</code>, <br />
<code>compelled.arcs</code>, <code>reversible.arcs</code>, return a matrix of two
columns of character strings.
</p>
<p><code>narcs</code> and <code>nnodes</code> return the number of arcs and nodes in the
graph, respectively.
</p>
<p><code>amat</code> returns a matrix of 0/1 integer values.
</p>
<p><code>degree</code>, <code>in.degree</code>, <code>out.degree</code>, <code>nparams</code> and
<code>ntests</code> return an integer.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Chickering DM (1995). &quot;A Transformational Characterization of Equivalent
Bayesian Network Structures&quot;. <em>Proceedings of the Eleventh Annual
Conference on Uncertainty in Artificial Intelligence</em>, 87&ndash;98.
</p>
<p>Neapolitan RE (2003). <em>Learning Bayesian Networks</em>. Prentice Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
cpdag = pc.stable(learning.test)

##  the Markov blanket of A.
mb(cpdag, "A")
## the neighbourhood of F.
nbr(cpdag, "F")
## the arcs in the graph.
arcs(cpdag)
## the nodes of the graph.
nodes(cpdag)
## the adjacency matrix for the nodes of the graph.
amat(cpdag)
## the parents of D.
parents(cpdag, "D")
## the children of A.
children(cpdag, "A")
## the root nodes of the graph.
root.nodes(cpdag)
## the leaf nodes of the graph.
leaf.nodes(cpdag)
## number of parameters of the Bayesian network.
dag = set.arc(cpdag, "A", "B")
nparams(dag, learning.test)
</code></pre>

<hr>
<h2 id='model+20string+20utilities'>Build a model string from a Bayesian network and vice versa</h2><span id='topic+model+20string+20utilities'></span><span id='topic+modelstring'></span><span id='topic+modelstring+3C-'></span><span id='topic+model2network'></span><span id='topic+as.character.bn'></span><span id='topic+as.bn'></span><span id='topic+as.bn.character'></span>

<h3>Description</h3>

<p>Build a model string from a Bayesian network and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelstring(x)
modelstring(x, debug = FALSE) &lt;- value

model2network(string, ordering = NULL, debug = FALSE)

## S3 method for class 'bn'
as.character(x, ...)
## S3 method for class 'character'
as.bn(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model+2B20string+2B20utilities_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>. <code>modelstring()</code> (but not its
replacement form) accepts also objects of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="model+2B20string+2B20utilities_+3A_string">string</code></td>
<td>
<p>a character string describing the Bayesian network.</p>
</td></tr>
<tr><td><code id="model+2B20string+2B20utilities_+3A_ordering">ordering</code></td>
<td>
<p>the labels of all the nodes in the graph; their order is the
node ordering used in the construction of the <code>bn</code> object. If
<code>NULL</code> the nodes are sorted alphabetically.</p>
</td></tr>
<tr><td><code id="model+2B20string+2B20utilities_+3A_value">value</code></td>
<td>
<p>a character string, the same as the <code>string</code>.</p>
</td></tr>
<tr><td><code id="model+2B20string+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="model+2B20string+2B20utilities_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The strings returned by <code>modelstringi()</code> have the same format as the ones
returned by the <code>modelstring()</code> function in package <span class="pkg">deal</span>; network
structures may be easily exported to and imported from that package (via the
<code>model2network</code> function).
</p>
<p>The format of the model strings is as follows. The local structure of each
node is enclosed in square brackets (&quot;<code>[]</code>&quot;); the first string is the
label of that node. The parents of the node (if any) are listed after a
(&quot;<code>|</code>&quot;) and separated by colons (&quot;<code>:</code>&quot;). All nodes (including
isolated and root nodes) must be listed.
</p>


<h3>Value</h3>

<p><code>model2network()</code> and <code>as.bn()</code> return an object of class <code>bn</code>;
<code>modelstring()</code> and <code>as.character.bn()</code> return a character string.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test)
dag
modelstring(dag)
dag2 = model2network(modelstring(dag))
dag2
all.equal(dag, dag2)
</code></pre>

<hr>
<h2 id='multivariate+20normal+20distribution'>Gaussian Bayesian networks and multivariate normals</h2><span id='topic+mvnorm2gbn'></span><span id='topic+gbn2mvnorm'></span>

<h3>Description</h3>

<p>Convert a Gaussian Bayesian network into the multivariate normal distribution
that is its global distribution, and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbn2mvnorm(fitted)
mvnorm2gbn(dag, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multivariate+2B20normal+2B20distribution_+3A_fitted">fitted</code></td>
<td>
<p>an object of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="multivariate+2B20normal+2B20distribution_+3A_dag">dag</code></td>
<td>
<p>an object of class <code>bn</code>, the structure of the network that
will be returned.</p>
</td></tr>
<tr><td><code id="multivariate+2B20normal+2B20distribution_+3A_mu">mu</code></td>
<td>
<p>a numeric vector, the expectation of the multivariate normal.</p>
</td></tr>
<tr><td><code id="multivariate+2B20normal+2B20distribution_+3A_sigma">sigma</code></td>
<td>
<p>a square numeric matrix, the covariance matrix of the
multivariate normal.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gbn2mvnorm()</code> returns a list with elements <code>"mu"</code> (the vector of
expectations) and <code>"sigma"</code> (the covariance matrix).
</p>
<p><code>mvnorm2gbn()</code> returns an object of class <code>bn.fit</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Pourahmadi M (2011). &quot;Covariance Estimation: The GLM and Regularization
Perspectives&quot;. <em>Statistical Science</em>, 26(3), 369&ndash;387.
</p>


<h3>See Also</h3>

<p><a href="#topic+bn.fit">bn.fit</a>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gaussian.test)
dag = model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]")
bn = bn.fit(dag, gaussian.test)
mvn = gbn2mvnorm(bn)
bn2 = mvnorm2gbn(dag, mu = mvn$mu, sigma = mvn$sigma)
all.equal(bn, bn2)
</code></pre>

<hr>
<h2 id='naive.bayes'>Naive Bayes classifiers</h2><span id='topic+naive.bayes'></span><span id='topic+tree.bayes'></span><span id='topic+predict.bn.naive'></span><span id='topic+predict.bn.tan'></span>

<h3>Description</h3>

<p>Create, fit and perform predictions with naive Bayes and Tree-Augmented
naive Bayes (TAN) classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>naive.bayes(x, training, explanatory)
## S3 method for class 'bn.naive'
predict(object, data, prior, ..., prob = FALSE, debug = FALSE)

tree.bayes(x, training, explanatory, whitelist = NULL, blacklist = NULL,
  mi = NULL, root = NULL, debug = FALSE)
## S3 method for class 'bn.tan'
predict(object, data, prior, ..., prob = FALSE, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naive.bayes_+3A_training">training</code></td>
<td>
<p>a character string, the label of the training variable.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_explanatory">explanatory</code></td>
<td>
<p>a vector of character strings, the labels of the
explanatory variables.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn.naive</code>, either fitted or not.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_x">x</code>, <code id="naive.bayes_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model, which
must all be factors.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_prior">prior</code></td>
<td>
<p>a numeric vector, the prior distribution for the training
variable. It is automatically normalized if not already so. The default
prior is the probability distribution of the training variable in
<code>object</code>.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_whitelist">whitelist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs to be included in the graph.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_blacklist">blacklist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot;
and &quot;to&quot;), containing a set of arcs not to be included in the graph.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_mi">mi</code></td>
<td>
<p>a character string, the estimator used for the mutual information
coefficients for the Chow-Liu algorithm in TAN. Possible values are
<code>mi</code> (discrete mutual information) and <code>mi-g</code> (Gaussian mutual
information).</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_root">root</code></td>
<td>
<p>a character string, the label of the explanatory variable to be
used as the root of the tree in the TAN classifier.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_prob">prob</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> the posterior probabilities used
for prediction are attached to the predicted values as an attribute called
<code>prob</code>.</p>
</td></tr>
<tr><td><code id="naive.bayes_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>naive.bayes()</code> function creates the star-shaped Bayesian network
form of a naive Bayes classifier; the training variable (the one holding the
group each observation belongs to) is at the center of the star, and it has
an outgoing arc for each explanatory variable.
</p>
<p>If <code>data</code> is specified, <code>explanatory</code> will be ignored and the
labels of the explanatory variables will be extracted from the data.
</p>
<p><code>predict()</code> performs a supervised classification of the observations by
assigning them to the group with the maximum posterior probability.
</p>


<h3>Value</h3>

<p><code>naive.bayes()</code> returns an object of class <code>c("bn.naive", "bn")</code>,
which behaves like a normal <code>bn</code> object unless passed to <code>predict()</code>.
<code>tree.bayes()</code> returns an object of class <code>c("bn.tan", "bn")</code>, which
again behaves like a normal <code>bn</code> object unless passed to <code>predict()</code>.
</p>
<p><code>predict()</code> returns a factor with the same levels as the <code>training</code>
variable from <code>data</code>. If <code>prob = TRUE</code>, the posterior probabilities
used for prediction are attached to the predicted values as an attribute
called <code>prob</code>.
</p>
<p>See <code><a href="#topic+network+20classifiers">network classifiers</a></code> for a complete list of network
classifiers with the respective references.
</p>


<h3>Note</h3>

<p>Since <span class="pkg">bnlearn</span> does not support networks containing both continuous and
discrete variables, all variables in <code>data</code> must be discrete.
</p>
<p>Ties in prediction are broken using <em>Bayesian tie breaking</em>, i.e.
sampling at random from the tied values. Therefore, setting the random seed
is required to get reproducible results.
</p>
<p><code>tan.tree()</code> supports whitelisting and blacklisting arcs but not their
directions. Morevoer it is not possible to whitelist or blacklist arcs
incident on <code>training</code>.
</p>
<p><code>predict()</code> accepts either a <code>bn</code> or a <code>bn.fit</code> object as its
first argument. For the former, the parameters of the network are fitted on
<code>data</code>, that is, the observations whose class labels the function is
trying to predict.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Borgelt C, Kruse R, Steinbrecher M (2009). <em>Graphical Models:
Representations for Learning, Reasoning and Data Mining</em>. Wiley, 2nd
edition.
</p>
<p>Friedman N, Geiger D, Goldszmidt M (1997). &quot;Bayesian Network Classifiers&quot;.
<em>Machine Learning</em>, <strong>29</strong>(2&ndash;3):131&ndash;163.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
# this is an in-sample prediction with naive Bayes (parameter learning
# is performed implicitly during the prediction).
bn = naive.bayes(learning.test, "A")
pred = predict(bn, learning.test)
table(pred, learning.test[, "A"])

# this is an in-sample prediction with TAN (parameter learning is
# performed explicitly with bn.fit).
tan = tree.bayes(learning.test, "A")
fitted = bn.fit(tan, learning.test, method = "bayes")
pred = predict(fitted, learning.test)
table(pred, learning.test[, "A"])

# this is an out-of-sample prediction, from a training test to a separate
# test set.
training.set = learning.test[1:4000, ]
test.set = learning.test[4001:5000, ]
bn = naive.bayes(training.set, "A")
fitted = bn.fit(bn, training.set)
pred = predict(fitted, test.set)
table(pred, test.set[, "A"])
</code></pre>

<hr>
<h2 id='network-classifiers'>Bayesian network Classifiers</h2><span id='topic+network-classifiers'></span><span id='topic+network+20classifiers'></span>

<h3>Description</h3>

<p>Structure learning algorithms for Bayesian network classifiers.
</p>


<h3>Details</h3>

<p>The algorithms are aimed at classification, and favour predictive power over
the ability to recover the correct network structure. The implementation in
<span class="pkg">bnlearn</span> assumes that all variables, including the classifiers, are
discrete.
</p>

<ul>
<li> <p><em>Naive Bayes</em> (<code><a href="#topic+naive.bayes">naive.bayes</a></code>): a very simple
algorithm assuming that all classifiers are independent and using the
posterior probability of the target variable for classification.
</p>
</li>
<li> <p><em>Tree-Augmented Naive Bayes</em> (<code><a href="#topic+tree.bayes">tree.bayes</a></code>): an
improvement over naive Bayes, this algorithms uses Chow-Liu to approximate
the dependence structure of the classifiers.
</p>
<p>Friedman N, Geiger D, Goldszmit M (1997). &quot;Bayesian Network Classifiers&quot;.
<em>Machine Learning</em>, <strong>29</strong>:131&ndash;163.
</p>
</li></ul>


<hr>
<h2 id='network-scores'>Network scores</h2><span id='topic+network-scores'></span><span id='topic+network+20scores'></span>

<h3>Description</h3>

<p>Overview of the network scores implemented in <span class="pkg">bnlearn</span>, with the
respective reference publications.
</p>


<h3>Details</h3>

<p>Available scores (and the respective labels) for <em>discrete Bayesian
networks</em> (categorical variables) are:
</p>

<ul>
<li><p> the multinomial <em>log-likelihood</em> (<code>loglik</code>) score, which is
equivalent to the <em>entropy measure</em> used in Weka.
</p>
</li>
<li><p> the <em>Akaike Information Criterion</em> (AIC) score (<code>aic</code>).
</p>
</li>
<li><p> the <em>Bayesian Information Criterion</em> (BIC) score (<code>bic</code>),
which is equivalent to the <em>Minimum Description Length</em> (MDL) and is
also known as <em>Schwarz Information Criterion</em>.
</p>
<p>Chickering DM (1995). &quot;A Transformational Characterization of Equivalent
Bayesian Network Structures&quot;. <em>Proceedings of the Eleventh Annual
Conference on Uncertainty in Artificial Intelligence</em>, 87&ndash;98.
</p>
</li>
<li><p> the <em>extended Bayesian Information Criterion</em> (<code>ebic</code>),
which adds a second penalty to BIC to penalize dense networks.
</p>
<p>Foygel R, Drton M (2010). &quot;Extended Bayesian Information Criteria for
Gaussian Graphical Models&quot;. NIPS 23, 604&ndash;612.
</p>
</li>
<li><p> the <em>predictive log-likelihood</em> (<code>pred-loglik</code>) computed on
a separate test set.
</p>
<p>Chickering DM, Heckerman D (2000). &quot;A Comparison of Scientific and
Engineering Criteria for Bayesian Model Selection&quot;. <em>Statistics
and Computing</em>, <strong>10</strong>:55&ndash;62.
</p>
<p>Scutari M, Vitolo C, Tucker A (2019). &quot;Learning Bayesian Networks from
Big Data with Greedy Search: Computational Complexity and Efficient
Implementation&quot;. <em>Statistics and Computing</em>,
<strong>25</strong>(9):1095&ndash;1108.
</p>
</li>
<li><p> the logarithm of the <em>Bayesian Dirichlet equivalent (uniform)</em>
score (<code>bde</code>) (also denoted BDeu), a score equivalent Dirichlet
posterior density.
</p>
<p>Heckerman D, Geiger D, Chickering DM (1995). &quot;Learning Bayesian Networks:
The Combination of Knowledge and Statistical Data&quot;.  <em>Machine
Learning</em>, <strong>20</strong>(3):197&ndash;243.
</p>
<p>Castelo R, Siebes A (2000). &quot;Priors on Network Structures. Biasing the
Search for Bayesian Networks&quot;. <em>International Journal of
Approximate	Reasoning</em>, <strong>24</strong>(1):39&ndash;57.
</p>
</li>
<li><p> the logarithm of the <em>Bayesian Dirichlet sparse</em> score
(<code>bds</code>) (BDs), a sparsity-inducing Dirichlet posterior density (not
score equivalent).
</p>
<p>Scutari M (2016). &quot;An Empirical-Bayes Score for Discrete Bayesian
Networks&quot;. <em>Journal of Machine Learning Research</em>,
<strong>52</strong>:438&ndash;448.
</p>
</li>
<li><p> the logarithm of the <em>Bayesian Dirichlet</em> score with
<em>Jeffrey's prior</em> (not score equivalent).
</p>
<p>Suzuki J (2016). &quot;A Theoretical Analysis of the BDeu Scores in Bayesian
Network Structure Learning&quot;. <em>Behaviormetrika</em>,
<strong>44</strong>(1):97&ndash;116.
</p>
</li>
<li><p> the logarithm of the modified <em>Bayesian Dirichlet equivalent</em>
score (<code>mbde</code>) for mixtures of experimental and observational data
(not score equivalent).
</p>
<p>Cooper GF, Yoo C (1999). &quot;Causal Discovery from a Mixture of Experimental
and Observational Data&quot;. <em>Proceedings of the Fifteenth Annual
Conference on Uncertainty in Artificial Intelligence</em>, 116&ndash;125.
</p>
</li>
<li><p> the logarithm of the <em>locally averaged Bayesian Dirichlet</em> score
(<code>bdla</code>, not score equivalent).
</p>
<p>Cano A, Gomez-Olmedo M, Masegosa AR, Moral S (2013). &quot;Locally Averaged
Bayesian Dirichlet Metrics for Learning the Structure and the
Parameters of Bayesian Networks&quot;. <em>International Journal of
Approximate Reasoning</em>, <strong>54</strong>:526&ndash;540.
</p>
</li>
<li><p> the logarithm of the <em>K2</em> score (<code>k2</code>), a Dirichlet
posterior density (not score equivalent).
</p>
<p>Korb K, Nicholson AE (2010). <em>Bayesian Artificial Intelligence</em>.
Chapman &amp; Hall/CRC, 2nd edition.
</p>
</li>
<li><p> the logarithm of the <em>factorized normalized maximum likelihood</em>
score (<code>fnml</code>, not score equivalent).
</p>
<p>Silander T, Roos T, Kontkanen P, Myllymaki P (2008). &quot;Factorized
Normalized Maximum Likelihood Criterion for Learning Bayesian Network
Structures&quot;. <em>Proceedings of the 4th European Workshop on
Probabilistic Graphical Models</em>, 257&ndash;272.
</p>
</li>
<li><p> the logarithm of the <em>quotient normalized maximum likelihood</em>
(<code>qnml</code>).
</p>
<p>Silander T, Leppa-Abo J, Jaasaari, Roos T (2018). &quot;Quotient Normalized
Maximum Likelihood Criterion for Learning Bayesian Network Structures&quot;.
<em>Proceedings of Machine Learning Research</em>, <strong>84</strong>:948&ndash;957.
</p>
</li>
<li><p> the node-average (log-)likelihood (<code>nal</code>) and the penalized
node-average (log-)likelihood (<code>pnal</code>).
</p>
<p>Bodewes T, Scutari M (2021). &quot;Learning Bayesian Networks from Incomplete
Data with the Node-Averaged Likelihood&quot;. <em>International Journal of
Approximate Reasoning</em>, <strong>138</strong>:145&ndash;160.
</p>
</li></ul>

<p>Available scores (and the respective labels) for <em>Gaussian Bayesian
networks</em> (normal variables) are:
</p>

<ul>
<li><p> the multivariate Gaussian <em>log-likelihood</em> (<code>loglik-g</code>)
score.
</p>
</li>
<li><p> the corresponding <em>Akaike Information Criterion</em> (AIC) score
(<code>aic-g</code>).
</p>
</li>
<li><p> the corresponding <em>Bayesian Information Criterion</em> (BIC) score
(<code>bic-g</code>).
</p>
<p>Geiger D, Heckerman D (1994). &quot;Learning Gaussian Networks&quot;.
<em>Proceedings of the Tenth Annual Conference on Uncertainty in
Artificial Intelligence</em>, 235&ndash;243.
</p>
</li>
<li><p> the <em>extended Bayesian Information Criterion</em> (<code>ebic-g</code>),
which adds a second penalty to BIC to penalize dense networks.
</p>
<p>Foygel R, Drton M (2010). &quot;Extended Bayesian Information Criteria for
Gaussian Graphical Models&quot;. NIPS 23, 604&ndash;612.
</p>
</li>
<li><p> the <em>predictive log-likelihood</em> (<code>pred-loglik-g</code>) computed
on a separate test set. The reference paper is the same as that for
<code>pred-loglik</code>. It is currently implemented to be score-equivalent
like <code>pred-loglik</code>, but that may be subject to change.
</p>
</li>
<li><p> a score equivalent <em>Gaussian posterior density</em> (<code>bge</code>).
</p>
<p>Kuipers J, Moffa G, Heckerman D (2014). &quot;Addendum on the Scoring of
Gaussian Directed Acyclic Graphical Models&quot;. <em>The Annals of
Statistics</em>, <strong>42</strong>(4):1689&ndash;1691.
</p>
</li>
<li><p> the node-average (log-)likelihood (<code>nal-g</code>) and the penalized
node-average (log-)likelihood (<code>pnal-g</code>).
</p>
<p>Bodewes T, Scutari M (2021). &quot;Learning Bayesian Networks from Incomplete
Data with the Node-Averaged Likelihood&quot;. <em>International Journal of
Approximate Reasoning</em>, <strong>138</strong>:145&ndash;160.
</p>
</li></ul>

<p>Available scores (and the respective labels) for <em>hybrid Bayesian
networks</em> (mixed categorical and normal variables) are:
</p>

<ul>
<li><p> the conditional linear Gaussian <em>log-likelihood</em>
(<code>loglik-cg</code>) score.
</p>
</li>
<li><p> the corresponding <em>Akaike Information Criterion</em> (AIC) score
(<code>aic-cg</code>).
</p>
</li>
<li><p> the corresponding <em>Bayesian Information Criterion</em> (BIC) score
(<code>bic-cg</code>).
</p>
</li>
<li><p> the <em>extended Bayesian Information Criterion</em> (<code>ebic-cg</code>),
which adds a second penalty to BIC to penalize dense networks.
</p>
<p>Foygel R, Drton M (2010). &quot;Extended Bayesian Information Criteria for
Gaussian Graphical Models&quot;. NIPS 23, 604&ndash;612.
</p>
</li>
<li><p> the <em>predictive log-likelihood</em> (<code>pred-loglik-cg</code>) computed
on a separate test set. The reference paper is the same as that for
<code>pred-loglik</code>.
</p>
</li>
<li><p> the node-average (log-)likelihood (<code>nal-cg</code>) and the penalized
node-average (log-)likelihood (<code>pnal-cg</code>).
</p>
<p>Bodewes T, Scutari M (2021). &quot;Learning Bayesian Networks from Incomplete
Data with the Node-Averaged Likelihood&quot;. <em>International Journal of
Approximate Reasoning</em>, <strong>138</strong>:145&ndash;160.
</p>
</li></ul>

<p>Other scores (and the respective labels):
</p>

<ul>
<li><p> a <em>custom decomposable</em> (<code>custom</code>) score interface that
takes an R function as an argument. It can be used to trial experimental
score functions without having to code them in C and hook them up to the
internals of <span class="pkg">bnlearn</span>.
</p>
</li></ul>


<hr>
<h2 id='node+20operations'>Manipulate nodes in a graph</h2><span id='topic+node+20operations'></span><span id='topic+add.node'></span><span id='topic+remove.node'></span><span id='topic+rename.nodes'></span><span id='topic+nodes+3C-'></span><span id='topic+nodes+3C-+2Cbn-method'></span><span id='topic+nodes+3C-+2Cbn.fit-method'></span><span id='topic+nodes+3C-+2Cbn.naive-method'></span><span id='topic+nodes+3C-+2Cbn.tan-method'></span>

<h3>Description</h3>

<p>Add, remove and rename nodes in a graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># add and remove nodes.
add.node(x, node)
remove.node(x, node)

# re-label nodes.
rename.nodes(x, names)
## S4 replacement method for signature 'bn'
nodes(object) &lt;- value
## S4 replacement method for signature 'bn.fit'
nodes(object) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="node+2B20operations_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> for <code>add.node()</code> and
<code>remove.node()</code>; an object of class <code>bn</code> or <code>bn.fit</code> for
<code>rename.nodes()</code>.</p>
</td></tr>
<tr><td><code id="node+2B20operations_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="node+2B20operations_+3A_node">node</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="node+2B20operations_+3A_value">value</code>, <code id="node+2B20operations_+3A_names">names</code></td>
<td>
<p>a vector of character strings, the new set of labels
that wll be used as to rename the nodes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>add.node()</code> adds a new (isolated) node to an existing <code>bn</code> object.
</p>
<p><code>remove.node()</code> removes a node from a <code>bn</code> object.
</p>
<p><code>rename.nodes()</code> replaces the node labels with new ones, relabelling the
whole node set. The assignment method for <code>nodes()</code> is an alias of
<code>rename.nodes()</code>.
</p>


<h3>Value</h3>

<p><code>add.node()</code>, <code>remove.node()</code> and <code>rename.nodes()</code> return an
updated <code>bn</code> object.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>dag = random.graph(LETTERS[1:5])
add.node(dag, "Z")
remove.node(dag, "A")

nodes(dag)
nodes(dag) = LETTERS[6:10]
nodes(dag)
</code></pre>

<hr>
<h2 id='node+20ordering+20utilities'>Partial node orderings</h2><span id='topic+node+20ordering+20utilities'></span><span id='topic+node.ordering'></span>

<h3>Description</h3>

<p>Find the partial node ordering implied by a network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>node.ordering(x, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="node+2B20ordering+2B20utilities_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code> or <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="node+2B20ordering+2B20utilities_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>node.ordering()</code> returns a vector of character strings, an ordered set
of node labels.
</p>


<h3>Note</h3>

<p><code>node.ordering()</code> supports only completely directed Bayesian networks.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>dag = random.graph(LETTERS[1:10])
ord = node.ordering(dag)
ord
</code></pre>

<hr>
<h2 id='pcalg+20integration'>Import and export networks from the pcalg package</h2><span id='topic+pcalg+20integration'></span><span id='topic+as.bn.pcAlgo'></span>

<h3>Description</h3>

<p>Convert <code>pcAlgo</code> objects to <code>bn</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcAlgo'
as.bn(x, ..., check.cycles = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcalg+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>pcAlgo</code>.</p>
</td></tr>
<tr><td><code id="pcalg+2B20integration_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (currently ignored).</p>
</td></tr>
<tr><td><code id="pcalg+2B20integration_+3A_check.cycles">check.cycles</code></td>
<td>
<p>a boolean value. If <code>FALSE</code> the returned network will
not be checked for cycles.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bn</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='plot.bn'>Plot a Bayesian network</h2><span id='topic+plot.bn'></span>

<h3>Description</h3>

<p>Plot the graph associated with a small Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bn'
plot(x, ylim = c(0,600), xlim = ylim, radius = 250,
  arrow = 35, highlight = NULL, color = "red", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bn_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_ylim">ylim</code></td>
<td>
<p>a numeric vector with two components containing the range of the
y-axis.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_xlim">xlim</code></td>
<td>
<p>a numeric vector with two components containing the range of the
x-axis.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_radius">radius</code></td>
<td>
<p>a numeric value containing the radius of the nodes.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_arrow">arrow</code></td>
<td>
<p>a numeric value containing the length of the arrow heads.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_highlight">highlight</code></td>
<td>
<p>a vector of character strings, representing the labels of
the nodes (and corresponding arcs) to be highlighted.</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_color">color</code></td>
<td>
<p>an integer or character string (the highlight colour).</p>
</td></tr>
<tr><td><code id="plot.bn_+3A_...">...</code></td>
<td>
<p>other graphical parameters to be passed through to plotting
functions.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The following arguments are always overridden:
</p>

<ul>
<li> <p><code>axes</code> is set to <code>FALSE</code>.
</p>
</li>
<li> <p><code>xlab</code> is set to an empty string.
</p>
</li>
<li> <p><code>ylab</code> is set to an empty string.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+graphviz.plot">graphviz.plot</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
cpdag = pc.stable(learning.test)

plot(cpdag)

## highlight node B and related arcs.
plot(cpdag, highlight = "B")
## highlight B and its Markov blanket.
plot(cpdag, highlight = c("B", mb(cpdag, "B")))

## a more compact plot.
par(oma = rep(0, 4), mar = rep(0, 4), mai = rep(0, 4),
  plt = c(0.06, 0.94, 0.12, 0.88))
plot(cpdag)
</code></pre>

<hr>
<h2 id='plot.bn.strength'>Plot arc strengths derived from bootstrap</h2><span id='topic+plot.bn.strength'></span>

<h3>Description</h3>

<p>Plot arc strengths derived from bootstrap resampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bn.strength'
plot(x, draw.threshold = TRUE, main = NULL,
  xlab = "arc strengths", ylab = "CDF(arc strengths)", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bn.strength_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.strength</code>.</p>
</td></tr>
<tr><td><code id="plot.bn.strength_+3A_draw.threshold">draw.threshold</code></td>
<td>
<p>a boolean value. If <code>TRUE</code>, a dashed vertical line
is drawn at the threshold.</p>
</td></tr>
<tr><td><code id="plot.bn.strength_+3A_main">main</code>, <code id="plot.bn.strength_+3A_xlab">xlab</code>, <code id="plot.bn.strength_+3A_ylab">ylab</code></td>
<td>
<p>character strings, the main title and the axes labels.</p>
</td></tr>
<tr><td><code id="plot.bn.strength_+3A_...">...</code></td>
<td>
<p>other graphical parameters to be passed through to plotting
functions.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>xlim</code> and <code>ylim</code> arguments are always overridden.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)

start = random.graph(nodes = names(learning.test), num = 50)
netlist = lapply(start, function(net) {
  hc(learning.test, score = "bde", iss = 10, start = net) })
arcs = custom.strength(netlist, nodes = names(learning.test), cpdag = FALSE)
plot(arcs)
</code></pre>

<hr>
<h2 id='predict+20and+20impute'>Predict or impute missing data from a Bayesian network</h2><span id='topic+impute'></span><span id='topic+predict.bn.fit'></span>

<h3>Description</h3>

<p>Impute missing values in a data set or predict a variable from a Bayesian
network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bn.fit'
predict(object, node, data, cluster, method = "parents", ...,
  prob = FALSE, debug = FALSE)

impute(object, data, cluster, method, ..., strict = TRUE, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict+2B20and+2B20impute_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn.fit</code> for <code>impute</code>; or an
object of class <code>bn</code> or <code>bn.fit</code> for <code>predict</code>.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_data">data</code></td>
<td>
<p>a data frame containing the data to be imputed. Complete
observations will be ignored.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_node">node</code></td>
<td>
<p>a character string, the label of a node.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_method">method</code></td>
<td>
<p>a character string, the method used to impute the missing
values or predict new ones. The default value is <code>parents</code>.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_...">...</code></td>
<td>
<p>additional arguments for the imputation method. See below.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_prob">prob</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> and <code>object</code> is a discrete
network, the probabilities used for prediction are attached to the
predicted values as an attribute called <code>prob</code>.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_strict">strict</code></td>
<td>
<p>a boolean value. If <code>TRUE</code>, <code>impute()</code> will produce an
error if the data were not imputed successfully, that is, if they still
contain missing values. If <code>FALSE</code>, it will return the partially
imputed data with a warning.</p>
</td></tr>
<tr><td><code id="predict+2B20and+2B20impute_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict()</code> returns the predicted values for <code>node</code> given the data
specified by <code>data</code> and the fitted network. Depending on the value of
<code>method</code>, the predicted values are computed as follows.
</p>

<ul>
<li> <p><code>parents</code>: the predicted values are computed by plugging in
the new values for the parents of <code>node</code> in the local probability
distribution of <code>node</code> extracted from <code>fitted</code>.
</p>
</li>
<li> <p><code>bayes-lw</code>: the predicted values are computed by averaging
likelihood weighting simulations performed using all the available nodes
as evidence (obviously, with the exception of the node whose values we
are predicting). The number of random samples which are averaged for each
new observation is controlled by the <code>n</code> optional argument; the
default is <code>500</code>. If the variable being predicted is discrete, the
predicted level is that with the highest conditional probability. If the
variable is continuous, the predicted value is the expected value of the
conditional distribution. The variables that are used to compute the
predicted values can be specified with the <code>from</code> optional argument;
the default is to use all the relevant variables from the data. Note that
the predicted values will differ in each call to <code>predict()</code> since
this method is based on a stochastic simulation.
</p>
</li>
<li> <p><code>exact</code>: the predicted values are computed using exact inference.
They are maximum a posteriori estimates obtained using junction trees and
belief propagation in the case of discrete networks, or posterior
expectations computed using closed-form results for the multivariate
normal distribution for Gaussian networks. Conditional Gaussian networks
are not supported. The variables that are used to compute the predicted
values can be specified with the <code>from</code> optional argument; the
default is to use those in the Markov blanket of <code>node</code>.
</p>
</li></ul>

<p><code>impute()</code> is based on <code>predict()</code>, and can impute missing values
with the same <code>methods</code> (<code>parents</code>, <code>bayes-lw</code> and
<code>exact</code>). The method <code>bayes-lw</code> can take an additional argument
<code>n</code> with the number of random samples which are averaged for each
observation. As in <code>predict()</code>, imputed values will differ in each call
to <code>impute()</code> when <code>method</code> is set to <code>bayes-lw</code>.
</p>
<p>If <code>object</code> contains <code>NA</code> parameter estimates (because of
unobserved discrete parents configurations in the data the parameters were
learned from), <code>predict</code> will predict <code>NA</code>s when those parents
configurations appear in <code>data</code>. See <code><a href="#topic+bn.fit">bn.fit</a></code> for details on
how to make sure <code>bn.fit</code> objects contain no <code>NA</code> parameter
estimates.
</p>


<h3>Value</h3>

<p><code>predict()</code> returns a numeric vector (for Gaussian and conditional
Gaussian nodes), a factor (for categorical nodes) or an ordered factor (for
ordinal nodes). If <code>prob = TRUE</code> and the network is discrete, the
probabilities used for prediction are attached to the predicted values as
an attribute called <code>prob</code>.
</p>
<p><code>impute()</code> returns a data frame with the same structure as <code>data</code>.
</p>


<h3>Note</h3>

<p>Ties in prediction are broken using <em>Bayesian tie breaking</em>, i.e.
sampling at random from the tied values. Therefore, setting the random seed
is required to get reproducible results.
</p>
<p>Classifiers have a separate <code>predict()</code> method, see <a href="#topic+naive.bayes">naive.bayes</a>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'># missing data imputation.
with.missing.data = gaussian.test
with.missing.data[sample(nrow(with.missing.data), 500), "F"] = NA
fitted = bn.fit(model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]"),
           gaussian.test)
imputed = impute(fitted, with.missing.data)

# predicting a variable in the test set.
training = bn.fit(model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]"),
           gaussian.test[1:2000, ])
test = gaussian.test[2001:nrow(gaussian.test), ]
predicted = predict(training, node = "F", data = test)

# obtain the conditional probabilities for the values of a single variable
# given a subset of the rest, they are computed to determine the predicted
# values.
fitted = bn.fit(model2network("[A][C][F][B|A][D|A:C][E|B:F]"), learning.test)
evidence = data.frame(A = factor("a", levels = levels(learning.test$A)),
                      F = factor("b", levels = levels(learning.test$F)))
predicted = predict(fitted, "C", evidence,
              method = "bayes-lw", prob = TRUE)
attr(predicted, "prob")
</code></pre>

<hr>
<h2 id='rbn'>Simulate random samples from a given Bayesian network</h2><span id='topic+rbn'></span>

<h3>Description</h3>

<p>Simulate random samples from a given Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbn(x, n = 1, ..., debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbn_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.fit</code>.</p>
</td></tr>
<tr><td><code id="rbn_+3A_n">n</code></td>
<td>
<p>a positive integer giving the number of observations to generate.</p>
</td></tr>
<tr><td><code id="rbn_+3A_...">...</code></td>
<td>
<p>additional arguments for the parameter estimation prcoedure, see
again <code><a href="#topic+bn.fit">bn.fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="rbn_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>rbn()</code> implements forward/logic sampling: values for the root nodes are
sampled from their (unconditional) distribution, then those of their children
conditional on the respective parent sets. This is done iteratively until
values have been sampled for all nodes.
</p>
<p>If <code>x</code> contains <code>NA</code> parameter estimates (because of unobserved
discrete parents configurations in the data the parameters were learned from),
<code>rbn</code> will produce samples that contain <code>NA</code>s when those parents
configurations appear in the simulated samples. See <code><a href="#topic+bn.fit">bn.fit</a></code> for
details on how to make sure <code>bn.fit</code> objects contain no <code>NA</code>
parameter estimates.
</p>


<h3>Value</h3>

<p>A data frame with the same structure as the data originally used to to fit the
parameters of the Bayesian network.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Korb K, Nicholson AE (2010). <em>Bayesian Artificial Intelligence</em>.
Chapman &amp; Hall/CRC, 2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpdist">cpdist</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test)
fitted = bn.fit(dag, learning.test)
rbn(fitted, 5)
</code></pre>

<hr>
<h2 id='ROCR+20integration'>Generating a prediction object for ROCR</h2><span id='topic+ROCR+20integration'></span><span id='topic+as.prediction'></span><span id='topic+as.prediction.bn.strength'></span>

<h3>Description</h3>

<p>Evaluate structure learning accuracy with <span class="pkg">ROCR</span>. This function views the
arcs in a <code>bn.strength</code> object as a set of predictions and the arcs in a
<code>true</code> reference graph as a set of labels, and produces a <code>prediction</code>
object from the <span class="pkg">ROCR</span> package. This facilitates evaluation of structure
learning with traditional machine learning metrics such as ROC curves and AUC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bn.strength'
as.prediction(x, true, ..., consider.direction = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROCR+2B20integration_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn.strength</code> returned by <code>boot.strength()</code>,
representing learning results targeting the object of class <code>bn</code>
specified by the <code>true</code> argument.</p>
</td></tr>
<tr><td><code id="ROCR+2B20integration_+3A_true">true</code></td>
<td>
<p>an object of class <code>bn</code>, the target of structure learning.</p>
</td></tr>
<tr><td><code id="ROCR+2B20integration_+3A_...">...</code></td>
<td>
<p>additional arguments, currently ignored.</p>
</td></tr>
<tr><td><code id="ROCR+2B20integration_+3A_consider.direction">consider.direction</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> an arc's prediction
value is set to the product of its <code>strength</code> and <code>direction</code>
values in <code>x</code> (interpreted as the probability an arc is both present
and has the specified direction). If <code>FALSE</code> the arc's prediction value
is set to its <code>strength</code> value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>One way of evaluating the overall performance of a network structure learning
algorithm is to evaluate how well it detects individual arcs.
<code>as.prediction()</code> takes each pair of nodes in a ground truth network and
labels them with a <code>1</code> if an arc exists between them and <code>0</code> if not.
It uses the arc presence probabilities in a <code>bn.strength</code> object
returned by <code>boot.strength()</code> as the predictions.
</p>


<h3>Value</h3>

<p>An object of class <code>prediction</code> from the <span class="pkg">ROCR</span> package.
</p>


<h3>Author(s)</h3>

<p>Robert Ness</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(ROCR)

modelstring = paste0("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
  "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
  "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
  "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
  "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG][ACO2|VALV]",
  "[CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]")
true.dag = model2network(modelstring)
strength = boot.strength(alarm, R = 200, m = 30, algorithm = "hc")
pred = as.prediction(strength, true.dag)
perf = performance(pred, "tpr", "fpr")
plot(perf, main = "Arc Detection")
performance(pred, "auc")

## End(Not run)
</code></pre>

<hr>
<h2 id='score'>Score of the Bayesian network</h2><span id='topic+score'></span><span id='topic+score+2Cbn-method'></span><span id='topic+score+2Cbn.naive-method'></span><span id='topic+score+2Cbn.tan-method'></span><span id='topic+logLik.bn'></span><span id='topic+AIC.bn'></span><span id='topic+BIC.bn'></span>

<h3>Description</h3>

<p>Compute the score of the Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'bn'
score(x, data, type = NULL, ..., by.node = FALSE, debug = FALSE)
## S4 method for signature 'bn.naive'
score(x, data, type = NULL, ..., by.node = FALSE, debug = FALSE)
## S4 method for signature 'bn.tan'
score(x, data, type = NULL, ..., by.node = FALSE, debug = FALSE)

## S3 method for class 'bn'
logLik(object, data, ...)
## S3 method for class 'bn'
AIC(object, data, ..., k = 1)
## S3 method for class 'bn'
BIC(object, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_+3A_x">x</code>, <code id="score_+3A_object">object</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="score_+3A_data">data</code></td>
<td>
<p>a data frame containing the data the Bayesian network that will
be used to compute the score.</p>
</td></tr>
<tr><td><code id="score_+3A_type">type</code></td>
<td>
<p>a character string, the label of a network score. If none is
specified, the default score is the <em>Bayesian Information Criterion</em>
for both discrete and continuous data sets. See <code><a href="#topic+network+20scores">network scores</a></code>
for details.</p>
</td></tr>
<tr><td><code id="score_+3A_by.node">by.node</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> and the score is decomposable,
the function returns the score terms corresponding to each node; otherwise
it returns their sum (the overall score of <code>x</code>).</p>
</td></tr>
<tr><td><code id="score_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="score_+3A_...">...</code></td>
<td>
<p>extra arguments from the generic method (for the <code>AIC</code> and
<code>logLik</code> functions, currently ignored) or additional tuning parameters
(for the <code>score</code> function).</p>
</td></tr>
<tr><td><code id="score_+3A_k">k</code></td>
<td>
<p>a numeric value, the penalty coefficient to be used; the default
<code>k = 1</code> gives the expression used to compute the AIC in the context of
scoring Bayesian networks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Additional arguments of the <code>score()</code> function:
</p>

<ul>
<li> <p><code>iss</code>: the imaginary sample size used by the Bayesian Dirichlet
scores (<code>bde</code>, <code>mbde</code>, <code>bds</code>, <code>bdj</code>). It is also known
as &ldquo;equivalent sample size&rdquo;. The default value is equal to <code>1</code>.
</p>
</li>
<li> <p><code>iss.mu</code>: the imaginary sample size for the normal component of
the normal-Wishart prior in the Bayesian Gaussian score (<code>bge</code>). The
default value is <code>1</code>.
</p>
</li>
<li> <p><code>iss.w</code>: the imaginary sample size for the Wishart component of
the normal-Wishart prior in the Bayesian Gaussian score (<code>bge</code>). The
default value is <code>ncol(data) + 2</code>.
</p>
</li>
<li> <p><code>nu</code>: the mean vector of the normal component of the
normal-Wishart prior in the Bayesian Gaussian score (<code>bge</code>). The
default value is equal to <code>colMeans(data)</code>.
</p>
</li>
<li> <p><code>l</code>: the number of scores to average in the locally averaged
Bayesian Dirichlet score (<code>bdla</code>). The default value is <code>5</code>.
</p>
</li>
<li> <p><code>exp</code>: a list of indexes of experimental observations (those that
have been artificially manipulated). Each element of the list must be
named after one of the nodes, and must contain a numeric vector with
indexes of the observations whose value has been manipulated for that node.
</p>
</li>
<li> <p><code>k</code>: the penalty coefficient to be used by the AIC, BIC and
penalized node-average log-likelihood scores. The default value is
<code>1</code> for AIC, <code>log(nrow(data)) / 2</code> for BIC and
<code>1 / nnnodes(x) * nrow(data) ^ -0.25</code> for the node-average
log-likelihood scores.
</p>
</li>
<li> <p><code>gamma</code>: the additional penalty in the extended BIC scores. The
default value is <code>0.5</code>.
</p>
</li>
<li> <p><code>prior</code>: the prior distribution to be used with the various
Bayesian Dirichlet scores (<code>bde</code>, <code>mbde</code>, <code>bds</code>,
<code>bdj</code>, <code>bdla</code>) and the Bayesian Gaussian score (<code>bge</code>).
Possible values are:
</p>

<ul>
<li> <p><code>uniform</code> (the default).
</p>
</li>
<li> <p><code>vsp</code>: the Bayesian variable selection prior, which puts a
probability of inclusion on parents.
</p>
</li>
<li> <p><code>marginal</code>: an independent marginal uniform for each arc.
</p>
</li>
<li> <p><code>cs</code>: the Castelo &amp; Siebes prior, which puts an independent
prior probability on each arc and direction).
</p>
</li></ul>

</li>
<li> <p><code>beta</code>: the parameter associated with <code>prior</code>.
</p>

<ul>
<li><p> If <code>prior</code> is <code>uniform</code>, <code>beta</code> is ignored.
</p>
</li>
<li><p> If <code>prior</code> is <code>vsp</code>, <code>beta</code> is the probability of
inclusion of an additional parent. The default is <code>1/ncol(data)</code>.
</p>
</li>
<li><p> If <code>prior</code> is <code>marginal</code>, <code>beta</code> is the probability
of inclusion of an arc. Each direction has a probability of inclusion
of <code>beta / 2</code> and the probability that the arc is not included is
therefore  <code>1 - beta</code>. The default value is <code>0.5</code>, so that
arc inclusion and arc exclusion have the same probability.
</p>
</li>
<li><p> If <code>prior</code> is <code>cs</code>, <code>beta</code> is a data frame with
columns <code>from</code>, <code>to</code> and <code>prob</code> specifying the prior
probability for a set of arcs. A uniform probability distribution is
assumed for the remaining arcs.
</p>
</li></ul>

</li>
<li> <p><code>newdata</code>: the test set whose predictive likelihood will be
computed by <code>pred-loglik</code>, <code>pred-loglik-g</code> or
<code>pred-loglik-cg</code>. It should be a data frame with the same variables
as <code>data</code>.
</p>
</li>
<li> <p><code>fun</code>: the function that computes the score component for a
single node in the <code>custom</code> score. <code>fun</code> must have arguments
<code>node</code>, <code>parents</code>, <code>data</code> and <code>args</code>, in this order;
in other words, it must have signature <code>function(node, parents, data,
      args)</code>. <code>node</code> will contain the label of the node to be scored (a
character string); <code>parents</code> will contain the labels of its parents
(a character vector); <code>data</code> will contain the complete data sets,
with all the variables (a data frame); and <code>args</code> will be a list
containing any additional arguments to the score.
</p>
</li>
<li> <p><code>args</code>: a list containing the optional arguments to <code>fun</code>,
for tuning <code>custom</code> score functions.
</p>
</li></ul>



<h3>Value</h3>

<p>For <code>score()</code> with <code>by.node = TRUE</code>, a vector of numeric values, the
individual node contributions to the score of the Bayesian network.
Otherwise, a single numeric value, the score of the Bayesian network.
</p>


<h3>Note</h3>

<p>AIC and BIC are computed as <code>logLik(x) - k * nparams(x)</code>, that is, the
classic definition rescaled by -2. Therefore higher values are better, and
for large sample sizes BIC converges to log(BDe).
</p>
<p>When using the Castelo &amp; Siebes prior in structure learning, the prior
probabilities associated with an arc are bound away from zero and one by
shrinking them towards the uniform distribution as per Hausser and Strimmer
(2009) with a lambda equal to <code>3 * sqrt(.Machine$double.eps)</code>. This
dramatically improves structure learning, which is less likely to get stuck
when starting from an empty graph. As an alternative to prior probabilities,
a blacklist can be used to prevent arcs from being included in the network,
and a whitelist can be used to force the inclusion of particular arcs.
<code>beta</code> is not modified when the prior is used from functions other than
those implementing score-based and hybrid structure learning.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+network+20scores">network scores</a></code>, <code><a href="#topic+arc.strength">arc.strength</a></code>,
<code><a href="#topic+alpha.star">alpha.star</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
dag = hc(learning.test)
score(dag, learning.test, type = "bde")

## let's see score equivalence in action!
dag2 = set.arc(dag, "B", "A")
score(dag2, learning.test, type = "bde")

## K2 score on the other hand is not score equivalent.
score(dag, learning.test, type = "k2")
score(dag2, learning.test, type = "k2")

## BDe with a prior.
beta = data.frame(from = c("A", "D"), to = c("B", "F"),
         prob = c(0.2, 0.5), stringsAsFactors = FALSE)
score(dag, learning.test, type = "bde", prior = "cs", beta = beta)

## equivalent to logLik(dag, learning.test)
score(dag, learning.test, type = "loglik")

## equivalent to AIC(dag, learning.test)
score(dag, learning.test, type = "aic")

## custom score, computing BIC manually.
my.bic = function(node, parents, data, args) {

  n = nrow(data)

  if (length(parents) == 0) {

    counts = table(data[, node])
    nparams = dim(counts) - 1
    sum(counts * log(counts / n)) - nparams * log(n) / 2

  }#THEN
  else {

    counts = table(data[, node], configs(data[, parents, drop = FALSE]))
    nparams = ncol(counts) * (nrow(counts) - 1)
    sum(counts * log(prop.table(counts, 2))) - nparams * log(n) / 2

  }#ELSE

}#MY.BIC
score(dag, learning.test, type = "custom", fun = my.bic, by.node = TRUE)
score(dag, learning.test, type = "bic", by.node = TRUE)
</code></pre>

<hr>
<h2 id='score-based+20algorithms'>Score-based structure learning algorithms</h2><span id='topic+score-based+20algorithms'></span><span id='topic+hc'></span><span id='topic+tabu'></span>

<h3>Description</h3>

<p>Learn the structure of a Bayesian network using a hill-climbing (HC) or a
Tabu search (TABU) greedy search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hc(x, start = NULL, whitelist = NULL, blacklist = NULL, score = NULL, ...,
  debug = FALSE, restart = 0, perturb = 1, max.iter = Inf, maxp = Inf, optimized = TRUE)
tabu(x, start = NULL, whitelist = NULL, blacklist = NULL, score = NULL, ...,
  debug = FALSE, tabu = 10, max.tabu = tabu, max.iter = Inf, maxp = Inf, optimized = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score-based+2B20algorithms_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_start">start</code></td>
<td>
<p>an object of class <code>bn</code>, the preseeded directed acyclic
graph used to initialize the algorithm. If none is specified, an empty one
(i.e. without any arc) is used.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_whitelist">whitelist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot; and
&quot;to&quot;), containing a set of arcs to be included in the graph.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_blacklist">blacklist</code></td>
<td>
<p>a data frame with two columns (optionally labeled &quot;from&quot; and
&quot;to&quot;), containing a set of arcs not to be included in the graph.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_score">score</code></td>
<td>
<p>a character string, the label of the network score to be used in
the algorithm. If none is specified, the default score is the <em>Bayesian
Information Criterion</em> for both discrete and continuous data sets. See
<code><a href="#topic+network+20scores">network scores</a></code> for details.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_...">...</code></td>
<td>
<p>additional tuning parameters for the network score. See
<code><a href="#topic+score">score</a></code> for details.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_restart">restart</code></td>
<td>
<p>an integer, the number of random restarts.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_tabu">tabu</code></td>
<td>
<p>a positive integer number, the length of the tabu list used in the
<code>tabu</code> function.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_max.tabu">max.tabu</code></td>
<td>
<p>a positive integer number, the iterations tabu search can
perform without improving the best network score.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_perturb">perturb</code></td>
<td>
<p>an integer, the number of attempts to randomly
insert/remove/reverse an arc on every random restart.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_max.iter">max.iter</code></td>
<td>
<p>an integer, the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_maxp">maxp</code></td>
<td>
<p>the maximum number of parents allowed for a node in any network
that is considered in the search, including that that is returned. The
default value is <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="score-based+2B20algorithms_+3A_optimized">optimized</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> (the default), score caching
is used to speed up structure learning.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bn</code>. See <code><a href="#topic+bn-class">bn-class</a></code> for details.
</p>


<h3>Note</h3>

<p>See <code><a href="#topic+structure+20learning">structure learning</a></code> for a complete list of structure learning
algorithms with the respective references.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><code><a href="#topic+network+20scores">network scores</a></code>, <a href="#topic+constraint-based+20algorithms">constraint-based algorithms</a>,
<a href="#topic+hybrid+20algorithms">hybrid algorithms</a>, <a href="#topic+local+20discovery+20algorithms">local discovery algorithms</a>,
<a href="#topic+alpha.star">alpha.star</a>.</p>

<hr>
<h2 id='single-node+20local+20discovery'>Discover the structure around a single node</h2><span id='topic+single-node+20local+20discovery'></span><span id='topic+learn.mb'></span><span id='topic+learn.nbr'></span>

<h3>Description</h3>

<p>Learn the Markov blanket or the neighbourhood centered on a node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn.mb(x, node, method, whitelist = NULL, blacklist = NULL, start = NULL,
  test = NULL, alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE)
learn.nbr(x, node, method, whitelist = NULL, blacklist = NULL,
  test = NULL, alpha = 0.05, B = NULL, max.sx = NULL, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_node">node</code></td>
<td>
<p>a character string, the label of the node whose local structure
is being learned.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_method">method</code></td>
<td>
<p>a character string, the label of a structure learning algorithm.
Possible choices are listed in <a href="#topic+structure+20learning">structure learning</a>.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_whitelist">whitelist</code></td>
<td>
<p>a vector of character strings, the labels of the whitelisted
nodes.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_blacklist">blacklist</code></td>
<td>
<p>a vector of character strings, the labels of the blacklisted
nodes.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_start">start</code></td>
<td>
<p>a vector of character strings, the labels of the nodes to be
included in the Markov blanket before the learning process (in
<code>learn.mb</code>). Note that the nodes in <code>start</code> can be removed from
the Markov blanket by the learning algorithm, unlike the nodes included due
to whitelisting.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_test">test</code></td>
<td>
<p>a character string, the label of the conditional independence test
to be used in the algorithm. If none is specified, the default test
statistic is the <em>mutual information</em> for categorical variables, the
Jonckheere-Terpstra test for ordered factors and the <em>linear
correlation</em> for continuous variables. See <code><a href="#topic+independence+20tests">independence tests</a></code>
for details.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_alpha">alpha</code></td>
<td>
<p>a numeric value, the target nominal type I error rate.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_b">B</code></td>
<td>
<p>a positive integer, the number of permutations considered for each
permutation test. It will be ignored with a warning if the conditional
independence test specified by the <code>test</code> argument is not a permutation
test.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_max.sx">max.sx</code></td>
<td>
<p>a positive integer, the maximum allowed size of the conditioning
sets used in conditional independence tests. The default is that there is
no limit on size.</p>
</td></tr>
<tr><td><code id="single-node+2B20local+2B20discovery_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of character strings, the labels of the nodes in the Markov blanket
(for <code>learn.mb()</code>) or in the neighbourhood (for <code>learn.nbr()</code>).
</p>


<h3>Note</h3>

<p>All algorithms used by <code>learn.mb()</code> and <code>learn.nbr()</code> accept
incomplete data, which they handle by computing individual conditional
independence tests on locally complete observations.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>See Also</h3>

<p><a href="#topic+constraint-based+20algorithms">constraint-based algorithms</a>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>learn.mb(learning.test, node = "D", method = "iamb")
learn.mb(learning.test, node = "D", method = "iamb", blacklist = c("A", "F"))

learn.nbr(gaussian.test, node = "F", method = "si.hiton.pc", whitelist = "D")
</code></pre>

<hr>
<h2 id='strength.plot'>Arc strength plot</h2><span id='topic+strength.plot'></span>

<h3>Description</h3>

<p>Plot a Bayesian network and format its arcs according to the strength of the
dependencies they represent. Requires the <span class="pkg">Rgraphviz</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strength.plot(x, strength, threshold, cutpoints, highlight = NULL, groups,
  layout = "dot", shape = "rectangle", fontsize = 12, main = NULL, sub = NULL,
  render = TRUE, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strength.plot_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_strength">strength</code></td>
<td>
<p>an object of class <code>bn.strength</code> computed from the object
of class <code>bn</code> corresponding to the <code>x</code> argument.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_threshold">threshold</code></td>
<td>
<p>a numeric value. See below.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_cutpoints">cutpoints</code></td>
<td>
<p>an array of numeric values. See below.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_highlight">highlight</code></td>
<td>
<p>a list, see <code><a href="#topic+graphviz.plot">graphviz.plot</a></code> for details.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_groups">groups</code></td>
<td>
<p>a list of character vectors, representing groups of node labels
of nodes that should be plotted close to each other.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_layout">layout</code></td>
<td>
<p>a character string, the layout argument that will be passed to
<span class="pkg">Rgraphviz</span>. Possible values are <code>dots</code>, <code>neato</code>,
<code>twopi</code>, <code>circo</code> and <code>fdp</code>. See <span class="pkg">Rgraphviz</span> documentation
for details.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_shape">shape</code></td>
<td>
<p>a character string, the shape of the nodes. Can be <code>circle</code>,
<code>ellipse</code> or <code>rectangle</code>.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_fontsize">fontsize</code></td>
<td>
<p>a positive number, the font size for the node labels.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_main">main</code></td>
<td>
<p>a character string, the main title of the graph. It's plotted at
the top of the graph.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_sub">sub</code></td>
<td>
<p>a character string, a subtitle which is plotted at the bottom of
the graph.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
<tr><td><code id="strength.plot_+3A_render">render</code></td>
<td>
<p>a logical value. If <code>TRUE</code>, <code>strength.plot()</code> actually
draws the figure in addition to returning the corresponding <code>graph</code>
object. If <code>FALSE</code>, no figure is produced.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>threshold</code> argument is used to determine which arcs are supported
strongly enough by the data to be deemed significant:
</p>

<ul>
<li><p> if arc strengths have been computed using conditional independence
tests, any strength coefficient (which is the p-value of the test) lesser
than or equal to the threshold is considered significant.
</p>
</li>
<li><p> if arc strengths have been computed using network scores, any strength
coefficient (which is the increase/decrease of the network score caused by
the removal of the arc) lesser than the threshold is considered
significant.
</p>
</li>
<li><p> if arc strengths have been computed using bootstrap or using Bayes
factors, any strength coefficient (which can be interpreted as a
probability for inclusion) greater or equal than the threshold is
considered significant.
</p>
</li></ul>

<p>The default value is the value of the <code>strength</code> attribute of the
<code>bn.strength</code> object passed via the <code>strength</code> argument.
</p>
<p>Non-significant arcs are plotted as dashed lines.
</p>
<p>The <code>cutpoints</code> argument is an array of numeric values used to divide
the range of the strength coefficients into intervals. The interval each
strength coefficient falls into determines the line width of the corresponding
arc in the plot. The default intervals are delimited by
</p>
<p><code>  unique(c(0, threshold/c(10, 5, 2, 1.5, 1), 1))</code>
</p>
<p>if the coefficients are computed from conditional independence tests, by
</p>
<p><code>  unique(c(0, (1 - threshold)/c(10, 5, 2, 1.5, 1), 1))</code>
</p>
<p>for bootstrap estimates or by the quantiles
</p>
<p><code>  quantile(s[s &lt; threshold], 1 - c(0.50, 0.75, 0.90, 0.95, 1))</code>
</p>
<p>of the significant differences if network scores are used.
</p>


<h3>Value</h3>

<p><code>graphviz.plot()</code> returns invisibly the <code>graph</code> object produced by
<span class="pkg">Rgraphviz</span>. It can be further modified using the commands present in the
<span class="pkg">graph</span> and <span class="pkg">Rgraphviz</span> packages, and it contains the arc strengths
in the edge weight attribute.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# plot the network learned by hc().
dag = hc(learning.test)
strength = arc.strength(dag, learning.test, criterion = "x2")
strength.plot(dag, strength)
# add another (non-significant) arc and plot the network again.
dag = set.arc(dag, "A", "C")
strength = arc.strength(dag, learning.test, criterion = "x2")
strength.plot(dag, strength)

## End(Not run)
</code></pre>

<hr>
<h2 id='structural.em'>Structure learning from missing data</h2><span id='topic+em-based+20algorithms'></span><span id='topic+structural.em'></span>

<h3>Description</h3>

<p>Learn the structure of a Bayesian network from a data set containing missing
values using Structural EM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structural.em(x, maximize = "hc", maximize.args = list(), fit,
    fit.args = list(), impute, impute.args = list(), return.all = FALSE,
    start = NULL, max.iter = 5, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="structural.em_+3A_x">x</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_maximize">maximize</code></td>
<td>
<p>a character string, the score-based algorithm to be used in
the &ldquo;maximization&rdquo; step. See <code><a href="#topic+structure+20learning">structure learning</a></code> for
details.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_maximize.args">maximize.args</code></td>
<td>
<p>a list of arguments to be passed to the algorithm
specified by <code>maximize</code>, such as <code>restart</code> for hill-climbing or
<code>tabu</code> for tabu search.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_fit">fit</code></td>
<td>
<p>a character string, the parameter learning method to be used in
the &ldquo;maximization&rdquo; step. See <code><a href="#topic+bn.fit">bn.fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_fit.args">fit.args</code></td>
<td>
<p>a list of arguments to be passed to the parameter learning
method specified by <code>fit</code>.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_impute">impute</code></td>
<td>
<p>a character string, the imputation method to be used in the
&ldquo;expectation&rdquo; step. See <code><a href="#topic+impute">impute</a></code> for details.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_impute.args">impute.args</code></td>
<td>
<p>a list of arguments to be passed to the imputation method
specified by <code>impute</code>.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_return.all">return.all</code></td>
<td>
<p>a boolean value. See below for details.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_start">start</code></td>
<td>
<p>a <code>bn</code> or <code>bn.fit</code> object, the network used to perform
the first imputation and as a starting point for the score-based
algorithm specified by <code>maximize</code>.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_max.iter">max.iter</code></td>
<td>
<p>an integer, the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="structural.em_+3A_debug">debug</code></td>
<td>
<p>a boolean value. If <code>TRUE</code> a lot of debugging output is
printed; otherwise the function is completely silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>return.all</code> is <code>FALSE</code>, <code>structural.em()</code> returns an object
of class <code>bn</code>. (See <code><a href="#topic+bn-class">bn-class</a></code> for details.)
</p>
<p>If <code>return.all</code> is <code>TRUE</code>, <code>structural.em()</code> returns a list
with three elements named <code>dag</code> (an object of class <code>bn</code>),
<code>imputed</code> (a data frame containing the imputed data from the last
iteration) and <code>fitted</code> (an object of class <code>bn.fit</code>, again from
the last iteration; see <code><a href="#topic+bn.fit-class">bn.fit-class</a></code> for details).
</p>


<h3>Note</h3>

<p>If at least one of the variables in the data <code>x</code> does not contain any
observed value, the <code>start</code> network must be specified and it must be a
<code>bn.fit</code> object. Otherwise, <code>structural.em()</code> is unable to complete
the first <em>maximization</em> step because it cannot fit the corresponding
local distribution(s).
</p>
<p>Note that if <code>impute</code> is set to <code>bayes-lw</code>, each call to
<code>structural.em</code> may produce a different model since the imputation is
based on a stochastic simulation.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Friedman N (1997). &quot;Learning Belief Networks in the Presence of Missing Values
and Hidden Variables&quot;. <em>Proceedings of the 14th International
Conference on Machine Learning</em>, 125&ndash;133.
</p>


<h3>See Also</h3>

<p><a href="#topic+score-based+20algorithms">score-based algorithms</a>, <a href="#topic+bn.fit">bn.fit</a>, <a href="#topic+impute">impute</a>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)

# learn with incomplete data.
incomplete.data = learning.test
incomplete.data[1:100, 1] = NA
incomplete.data[101:200, 2] = NA
incomplete.data[1:200, 5] = NA
structural.em(incomplete.data)

## Not run: 
# learn with a latent variable.
incomplete.data = learning.test
incomplete.data[seq(nrow(incomplete.data)), 1] = NA
start = bn.fit(empty.graph(names(learning.test)), learning.test)
wl = data.frame(from = c("A", "A"), to = c("B", "D"))
structural.em(incomplete.data, start = start,
  maximize.args = list(whitelist = wl))

## End(Not run)
</code></pre>

<hr>
<h2 id='structure-learning'>Structure learning algorithms</h2><span id='topic+structure-learning'></span><span id='topic+structure+20learning'></span>

<h3>Description</h3>

<p>Overview of the structure learning algorithms implemented in <span class="pkg">bnlearn</span>,
with the respective reference publications.
</p>


<h3>Available Constraint-Based Learning Algorithms</h3>


<ul>
<li> <p><em>PC</em> (<code><a href="#topic+pc.stable">pc.stable</a></code>), a modern implementation of the
first practical constraint-based structure learning algorithm.
</p>
<p>Colombo D, Maathuis MH (2014). &quot;Order-Independent Constraint-Based Causal
Structure Learning&quot;. <em>Journal of Machine Learning Research</em>,
<strong>15</strong>:3921&ndash;3962.
</p>
</li>
<li> <p><em>Grow-Shrink</em> (<code><a href="#topic+gs">gs</a></code>): based on the <em>Grow-Shrink
Markov Blanket</em>, the first (and simplest) Markov blanket detection
algorithm used in a structure learning algorithm.
</p>
<p>Margaritis D (2003). <em>Learning Bayesian Network Model Structure from
Data</em>. Ph.D. thesis, School of Computer Science, Carnegie-Mellon
University, Pittsburgh, PA.
</p>
</li>
<li> <p><em>Incremental Association</em> (<code><a href="#topic+iamb">iamb</a></code>): based on the
Markov blanket detection algorithm of the same name, which is based on
a two-phase selection scheme (a forward selection followed by an attempt
to remove false positives).
</p>
<p>Tsamardinos I, Aliferis CF, Statnikov A (2003). &quot;Algorithms for Large
Scale Markov Blanket Discovery&quot;. <em>Proceedings of the Sixteenth
International Florida Artificial Intelligence Research Society
Conference</em>, 376&ndash;381.
</p>
</li>
<li> <p><em>Fast Incremental Association</em> (<code><a href="#topic+fast.iamb">fast.iamb</a></code>): a
variant of IAMB which uses speculative stepwise forward selection to
reduce the number of conditional independence tests.
</p>
</li>
<li> <p><em>Interleaved Incremental Association</em> (<code><a href="#topic+inter.iamb">inter.iamb</a></code>):
another variant of IAMB which uses forward stepwise selection to avoid
false positives in the Markov blanket detection phase.
</p>
<p>Yaramakala S, Margaritis D (2005). &quot;Speculative Markov Blanket Discovery
for Optimal Feature Selection&quot;. <em>Proceedings of the Fifth IEEE
International Conference on Data Mining</em>, 809&ndash;812.
</p>
</li>
<li> <p><em>Incremental Association with FDR</em> (<code><a href="#topic+iamb.fdr">iamb.fdr</a></code>): a
variant of IAMB which adjusts the tests significance threshold with FDR.
</p>
<p>Pena JM (2008). &quot;Learning Gaussian Graphical Models of Gene Networks with
False Discovery Rate Control&quot;. <em>Proceedings of the Sixth European
Conference on Evolutionary Computation, Machine Learning and Data Mining
in Bioinformatics</em>, 165&ndash;176.
</p>
<p>Gasse M, Aussem A, Elghazel H (2014). &quot;A Hybrid Algorithm for Bayesian
Network Structure Learning with Application to Multi-Label Learning&quot;.
<em>Expert Systems with Applications</em>, <strong>41</strong>(15):6755&ndash;6772.
</p>
</li></ul>

<p><span class="pkg">bnlearn</span> includes two implementations of each algorithm: a vanilla
implementation, and a parallel one that requires a running cluster set
up with the <code>makeCluster</code> function from the <span class="pkg">parallel</span> package.
</p>


<h3>Available Score-based Learning Algorithms</h3>


<ul>
<li> <p><em>Hill-Climbing</em> (<code><a href="#topic+hc">hc</a></code>): a <em>hill climbing</em>
greedy search that explores the space of the directed acyclic graphs by
single-arc addition, removal and reversals; with random restarts to avoid
local optima. The optimized implementation uses score caching, score
decomposability and score equivalence to reduce the number of duplicated
tests.
</p>
</li>
<li> <p><em>Tabu Search</em> (<code><a href="#topic+tabu">tabu</a></code>): a modified hill-climbing
able to escape local optima by selecting a network that minimally
decreases the score function.
</p>
<p>Russell SJ, Norvig P (2009). <em>Artificial Intelligence: A Modern
Approach</em>. Prentice Hall, 3rd edition.
</p>
</li></ul>



<h3>Available Hybrid Learning Algorithms</h3>


<ul>
<li> <p><em>Max-Min Hill-Climbing</em> (<code><a href="#topic+mmhc">mmhc</a></code>): a hybrid algorithm
which combines the Max-Min Parents and Children algorithm (to restrict the
search space) and the Hill-Climbing algorithm (to find the optimal network
structure in the restricted space).
</p>
<p>Tsamardinos I, Brown LE, Aliferis CF (2006). &quot;The Max-Min Hill-Climbing
Bayesian Network Structure Learning Algorithm&quot;. <em>Machine Learning</em>,
<strong>65</strong>(1):31&ndash;78.
</p>
</li>
<li> <p><em>Restricted Maximization</em> (<code><a href="#topic+rsmax2">rsmax2</a></code>): a general
implementation of the Sparse Candidate algorithms, which can use any
combination of constraint-based and score-based algorithms.
</p>
<p>Friedman N, Nachman I, Pe'er D (1999). &quot;Learning Bayesian Network
Structure from Massive Datasets: the Sparse Candidate Algorithm.&quot;
<em>Proceedings of the Fifteenth Conference on Uncertainty in
Artificial Intelligence (UAI)</em>, 206&ndash;215.
</p>
</li>
<li> <p><em>Hybrid HPC</em> (<code><a href="#topic+h2pc">h2pc</a></code>): a hybrid algorithm combining
HPC and hill-climbing.
</p>
<p>Gasse M, Aussem A, Elghazel H (2014). &quot;A Hybrid Algorithm for Bayesian
Network Structure Learning with Application to Multi-Label Learning&quot;.
<em>Expert Systems with Applications</em>, <strong>41</strong>(15):6755&ndash;6772.
</p>
</li></ul>



<h3>Other (Constraint-Based) Local Discovery Algorithms</h3>

<p>These algorithms learn the structure of the undirected graph underlying the
Bayesian network, which is known as the <em>skeleton</em> of the network.
Therefore by default all arcs are undirected, and no attempt is made to
detect their orientation. They are often used in hybrid learning algorithms.
</p>

<ul>
<li> <p><em>Max-Min Parents and Children</em> (<code><a href="#topic+mmpc">mmpc</a></code>): a forward
selection technique for neighbourhood detection based on the maximization
of the minimum association measure observed with any subset of the nodes
selected in the previous iterations.
</p>
<p>Tsamardinos I, Aliferis CF, Statnikov A (2003). &quot;Time and Sample Efficient
Discovery of Markov Blankets and Direct Causal Relations&quot;.
<em>Proceedings of the Ninth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining</em>, 673&ndash;678.
</p>
</li>
<li> <p><em>Hiton Parents and Children</em> (<code><a href="#topic+si.hiton.pc">si.hiton.pc</a></code>): a fast
forward selection technique for neighbourhood detection designed to
exclude nodes early based on the marginal association. The implementation
follows the Semi-Interleaved variant of the algorithm.
</p>
<p>Aliferis FC, Statnikov A, Tsamardinos I, Subramani M, Koutsoukos XD (2010).
&quot;Local Causal and Markov Blanket Induction for Causal Discovery and
Feature Selection for Classification Part I: Algorithms and Empirical
Evaluation&quot;.  <em>Journal of Machine Learning Research</em>,
<strong>11</strong>:171&ndash;234.
</p>
</li>
<li> <p><em>Hybrid Parents and Children</em> (<code><a href="#topic+hpc">hpc</a></code>): an algorithm
building on <code>iamb.fdr</code> to learn the parents and children of each node
like <code>mmpc</code> and <code>si.hiton.pc</code>. The reference publication is the
same as that for Hybrid HPC.
</p>
</li></ul>



<h3>Pairwise Mutual Information Algorithms</h3>

<p>These algorithms learn approximate network structures using only pairwise
mutual information.
</p>

<ul>
<li> <p><em>Chow-Liu</em> (<code><a href="#topic+chow.liu">chow.liu</a></code>): an application of the
minimum-weight spanning tree and the information inequality. It learns
the tree structure closest to the true one in the probability space.
</p>
<p>Chow CK, Liu CN (1968). &quot;Approximating Discrete Probability Distributions
with Dependence Trees&quot;, IEEE Transactions on Information Theory, IT-14
<strong>3</strong>:462&ndash;467.
</p>
</li>
<li> <p><em>ARACNE</em> (<code><a href="#topic+aracne">aracne</a></code>): an improved version of the
Chow-Liu algorithm that is able to learn polytrees.
</p>
<p>Margolin AA, Nemenman I, Basso K, Wiggins C, Stolovitzky G, Dalla Favera R,
Califano A (2006). &quot;ARACNE: An Algorithm for the Reconstruction of Gene
Regulatory Networks in a Mammalian Cellular Context&quot;. <em>BMC
Bioinformatics</em>, <strong>7</strong>(Suppl 1):S7.
</p>
</li></ul>

<p>All these algorithms have two implementations (vanilla and parallel) like
other constraint-based algorithms.
</p>

<hr>
<h2 id='test+20counter'>Manipulating the test counter</h2><span id='topic+test.counter'></span><span id='topic+increment.test.counter'></span><span id='topic+reset.test.counter'></span>

<h3>Description</h3>

<p>Check, increment or reset the test/score counter used in structure learning
algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.counter()
increment.test.counter(i = 1)
reset.test.counter()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test+2B20counter_+3A_i">i</code></td>
<td>
<p>a numeric value, which is added to the test counter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value, the current value of the test counter.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(learning.test)
hc(learning.test)
test.counter()
reset.test.counter()
test.counter()
</code></pre>

<hr>
<h2 id='utilities+20for+20whitelists+20and+20blacklists'>Get or create whitelists and blacklists</h2><span id='topic+whitelist'></span><span id='topic+blacklist'></span><span id='topic+ordering2blacklist'></span><span id='topic+tiers2blacklist'></span><span id='topic+set2blacklist'></span>

<h3>Description</h3>

<p>Extract whitelists and blacklists from an object of class <code>bn</code>, or create
them for use in structure learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>whitelist(x)
blacklist(x)

ordering2blacklist(nodes)
tiers2blacklist(tiers)
set2blacklist(set)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="utilities+2B20for+2B20whitelists+2B20and+2B20blacklists_+3A_x">x</code></td>
<td>
<p>an object of class <code>bn</code>.</p>
</td></tr>
<tr><td><code id="utilities+2B20for+2B20whitelists+2B20and+2B20blacklists_+3A_nodes">nodes</code>, <code id="utilities+2B20for+2B20whitelists+2B20and+2B20blacklists_+3A_set">set</code></td>
<td>
<p>a vector of character strings, the labels of the nodes.</p>
</td></tr>
<tr><td><code id="utilities+2B20for+2B20whitelists+2B20and+2B20blacklists_+3A_tiers">tiers</code></td>
<td>
<p>a vector of character strings or a list, see below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ordering2blacklist()</code> takes a vector of character strings (the labels
of the nodes), which specifies a complete node ordering. An object of class
<code>bn</code> or <code>bn.fit</code>; in that case, the node ordering is derived by the
graph. In both cases, the blacklist returned by <code>ordering2blacklist()</code>
contains all the possible arcs that violate the specified node ordering.
</p>
<p><code>tiers2blacklist()</code> takes (again) a vector of character strings (the
labels of the nodes), which specifies a complete node ordering, or a list of
character vectors, which specifies a partial node ordering. In the latter
case, all arcs going from a node in a particular element of the list
(sometimes known as <em>tier</em>) to a node in one of the previous elements
are blacklisted. Arcs between nodes in the same element are not blacklisted.
</p>
<p><code>set2blacklist()</code> creates a blacklist containing all the arcs between any
two of the nodes whose labels are passed as the argument <code>set</code>.
</p>


<h3>Value</h3>

<p><code>whitelist()</code> and <code>blacklist()</code> return a matrix of character string
with two columns, named <code>from</code> and <code>to</code>, if whitelist or a blacklist
have been used to learn the <code>bn</code> object passed as their argument.
</p>
<p><code>ordering2blacklist()</code>, <code>tiers2blacklist()</code> and
<code>set2blacklist()</code> return a sanitized <code>blacklist</code> (a two-column
matrix, whose columns are labeled <code>from</code> and <code>to</code>).
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>tiers2blacklist(list(LETTERS[1:3], LETTERS[4:6]))
set2blacklist(LETTERS[1:3])
ordering2blacklist(LETTERS[1:6])
</code></pre>

<hr>
<h2 id='whitelists-blacklists'>Whitelists and blacklists in structure learning</h2><span id='topic+whitelists-blacklists'></span><span id='topic+whitelists+20and+20blacklists'></span>

<h3>Description</h3>

<p>How whitelists and blacklists are used in structure learning.
</p>


<h3>Constraint-based Algorithms</h3>

<p>Constraint-based algorithms support arc whitelisting and blacklisting as
follows:
</p>

<ul>
<li><p> blacklisted arcs are never present in the learned graph.
</p>
</li>
<li><p> arcs whitelisted in one direction only (i.e.
<code class="reqn">A \rightarrow B</code> is whitelisted but
<code class="reqn">B \rightarrow A</code> is not) have the respective reverse arcs
blacklisted, and are always present in the learned graph.
</p>
</li>
<li><p> arcs whitelisted in both directions (i.e. both
<code class="reqn">A \rightarrow B</code> and <code class="reqn">B \rightarrow A</code> are
whitelisted) are present in the learned graph, but their direction is set
by the learning algorithm.
</p>
</li></ul>

<p>Any arc whitelisted and blacklisted at the same time is assumed to be
whitelisted, and is thus removed from the blacklist.
</p>


<h3>Score-based Algorithms</h3>

<p>Score-based algorithms support arc whitelisting and blacklisting as follows:
</p>

<ul>
<li><p> blacklisted arcs are never present in the learned graph.
</p>
</li>
<li><p> arcs can only be whitelisted in a single direction, and are always
present in the learned graph; it is not possible to whitelist arcs in both
directions.
</p>
</li></ul>



<h3>Hybrid Algorithms</h3>

<p>Hybrid algorithms use constraint-based (or pairwise mutual information)
algorithms in the <em>restrict phase</em> and score-based algorithms in the
<em>maximize phase</em>. Hence whitelists and blacklists are supported as
follows:
</p>

<ul>
<li><p> whitelists and blacklists should be specified for the algorithm used
in the restrict phase.
</p>
</li>
<li><p> if the whitelist contains any undirected arc, its consistent extension
is used instead in the maximize phase.
</p>
</li></ul>



<h3>Pairwise Mutual Information Algorithms</h3>

<p>In algorithms that learn undirected graphs, such as ARACNE and Chow-Liu, arcs
are treated as being whitelisted or blacklisted in both directions even if
only one direction is listed in the whitelist or blacklist. Again blacklisted
arcs are never present in the learned graph and whitelisted arcs are
guaranteed to be present in the learned graph.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
