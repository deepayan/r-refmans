<!DOCTYPE html><html><head><title>Help for package tune</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tune}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.get_tune_parameters'><p>Various accessor functions</p></a></li>
<li><a href='#.stash_last_result'><p>Save most recent results to search path</p></a></li>
<li><a href='#.use_case_weights_with_yardstick'><p>Determine if case weights should be passed on to yardstick</p></a></li>
<li><a href='#augment.tune_results'><p>Augment data with holdout predictions</p></a></li>
<li><a href='#autoplot.tune_results'><p>Plot tuning search results</p></a></li>
<li><a href='#check_rset'><p>Get colors for tune text.</p></a></li>
<li><a href='#choose_metric'><p>Tools for selecting metrics and evaluation times</p></a></li>
<li><a href='#collect_predictions'><p>Obtain and format results produced by tuning functions</p></a></li>
<li><a href='#compute_metrics'><p>Calculate and format metrics from tuning functions</p></a></li>
<li><a href='#conf_mat_resampled'><p>Compute average confusion matrix across resamples</p></a></li>
<li><a href='#control_bayes'><p>Control aspects of the Bayesian search process</p></a></li>
<li><a href='#control_grid'><p>Control aspects of the grid search process</p></a></li>
<li><a href='#control_last_fit'><p>Control aspects of the last fit process</p></a></li>
<li><a href='#coord_obs_pred'><p>Use same scale for plots of observed vs predicted values</p></a></li>
<li><a href='#example_ames_knn'><p>Example Analysis of Ames Housing Data</p></a></li>
<li><a href='#expo_decay'><p>Exponential decay function</p></a></li>
<li><a href='#extract_model'><p>Convenience functions to extract model</p></a></li>
<li><a href='#extract-tune'><p>Extract elements of <code>tune</code> objects</p></a></li>
<li><a href='#filter_parameters'><p>Remove some tuning parameter results</p></a></li>
<li><a href='#finalize_model'><p>Splice final parameters into objects</p></a></li>
<li><a href='#fit_best'><p>Fit a model to the numerically optimal configuration</p></a></li>
<li><a href='#fit_resamples'><p>Fit multiple models via resampling</p></a></li>
<li><a href='#forge_from_workflow'><p>Internal functions used by other tidymodels packages</p></a></li>
<li><a href='#get_metric_time'><p>Get time for analysis of dynamic survival metrics</p></a></li>
<li><a href='#int_pctl.tune_results'><p>Bootstrap confidence intervals for performance metrics</p></a></li>
<li><a href='#last_fit'><p>Fit the final best model to the training set and evaluate the test set</p></a></li>
<li><a href='#load_pkgs'><p>Quietly load package namespace</p></a></li>
<li><a href='#merge.recipe'><p>Merge parameter grid values into objects</p></a></li>
<li><a href='#message_wrap'><p>Write a message that respects the line width</p></a></li>
<li><a href='#min_grid.model_spec'><p>Determine the minimum set of model fits</p></a></li>
<li><a href='#outcome_names'><p>Determine names of the outcome data in a workflow</p></a></li>
<li><a href='#parameters.workflow'><p>Determination of parameter sets for other objects</p></a></li>
<li><a href='#prob_improve'><p>Acquisition function for scoring parameter combinations</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#show_best'><p>Investigate best tuning parameters</p></a></li>
<li><a href='#show_notes'><p>Display distinct errors from tune objects</p></a></li>
<li><a href='#tune_bayes'><p>Bayesian optimization of model parameters.</p></a></li>
<li><a href='#tune_grid'><p>Model tuning via grid search</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Tidy Tuning Tools</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>The ability to tune models is important. 'tune' contains
    functions and classes to be used in conjunction with other
    'tidymodels' packages for finding reasonable values of
    hyper-parameters in models, pre-processing methods, and
    post-processing steps.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://tune.tidymodels.org/">https://tune.tidymodels.org/</a>, <a href="https://github.com/tidymodels/tune">https://github.com/tidymodels/tune</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/tune/issues">https://github.com/tidymodels/tune/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli (&ge; 3.3.0), dials (&ge; 1.0.0), doFuture (&ge; 1.0.0), dplyr
(&ge; 1.1.0), foreach, future (&ge; 1.33.0), generics (&ge; 0.1.2),
ggplot2, glue (&ge; 1.6.2), GPfit, hardhat (&ge; 1.2.0), lifecycle
(&ge; 1.0.0), parsnip (&ge; 1.2.0), purrr (&ge; 1.0.0), recipes (&ge;
1.0.4), rlang (&ge; 1.1.0), rsample (&ge; 1.2.0), tibble (&ge;
3.1.0), tidyr (&ge; 1.2.0), tidyselect (&ge; 1.1.2), vctrs (&ge;
0.6.1), withr, workflows (&ge; 1.1.4), yardstick (&ge; 1.3.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>C50, censored (&ge; 0.3.0), covr, kernlab, kknn, knitr,
modeldata, scales, spelling, testthat (&ge; 3.0.0), xgboost, xml2</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>pkgdown, tidymodels, kknn, doParallel, doFuture,
tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-20 12:08:23 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-20 18:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.get_tune_parameters'>Various accessor functions</h2><span id='topic+.get_tune_parameters'></span><span id='topic+.get_tune_parameter_names'></span><span id='topic+.get_extra_col_names'></span><span id='topic+.get_tune_metrics'></span><span id='topic+.get_tune_metric_names'></span><span id='topic+.get_tune_eval_times'></span><span id='topic+.get_tune_eval_time_target'></span><span id='topic+.get_tune_outcome_names'></span><span id='topic+.get_tune_workflow'></span><span id='topic+.get_fingerprint.tune_results'></span>

<h3>Description</h3>

<p>These functions return different attributes from objects with class
<code>tune_result</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_tune_parameters(x)

.get_tune_parameter_names(x)

.get_extra_col_names(x)

.get_tune_metrics(x)

.get_tune_metric_names(x)

.get_tune_eval_times(x)

.get_tune_eval_time_target(x)

.get_tune_outcome_names(x)

.get_tune_workflow(x)

## S3 method for class 'tune_results'
.get_fingerprint(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".get_tune_parameters_+3A_x">x</code></td>
<td>
<p>An object of class <code>tune_result</code>.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code>.get_tune_parameters()</code> returns a <code>dials</code> <code>parameter</code> object or a tibble.
</p>
</li>
<li> <p><code>.get_tune_parameter_names()</code>, <code>.get_tune_metric_names()</code>, and
<code>.get_tune_outcome_names()</code> return a character string.
</p>
</li>
<li> <p><code>.get_tune_metrics()</code> returns a metric set or NULL.
</p>
</li>
<li> <p><code>.get_tune_workflow()</code> returns the workflow used to fit the
resamples (if <code>save_workflow</code> was set to <code>TRUE</code> during fitting) or NULL.
</p>
</li></ul>


<hr>
<h2 id='.stash_last_result'>Save most recent results to search path</h2><span id='topic+.stash_last_result'></span>

<h3>Description</h3>

<p>Save most recent results to search path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.stash_last_result(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".stash_last_result_+3A_x">x</code></td>
<td>
<p>An object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function will assign <code>x</code> to <code>.Last.tune.result</code> and put it in
the search path.
</p>


<h3>Value</h3>

<p>NULL, invisibly.
</p>

<hr>
<h2 id='.use_case_weights_with_yardstick'>Determine if case weights should be passed on to yardstick</h2><span id='topic+.use_case_weights_with_yardstick'></span><span id='topic+.use_case_weights_with_yardstick.hardhat_importance_weights'></span><span id='topic+.use_case_weights_with_yardstick.hardhat_frequency_weights'></span>

<h3>Description</h3>

<p>This S3 method defines the logic for deciding when a case weight vector
should be passed to yardstick metric functions and used to measure model
performance. The current logic is that frequency weights (i.e.
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>) are the only situation where this should
occur.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.use_case_weights_with_yardstick(x)

## S3 method for class 'hardhat_importance_weights'
.use_case_weights_with_yardstick(x)

## S3 method for class 'hardhat_frequency_weights'
.use_case_weights_with_yardstick(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".use_case_weights_with_yardstick_+3A_x">x</code></td>
<td>
<p>A vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single <code>TRUE</code> or <code>FALSE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(parsnip)
library(dplyr)

frequency_weights(1:10) %&gt;%
  .use_case_weights_with_yardstick()

importance_weights(seq(1, 10, by = .1))%&gt;%
  .use_case_weights_with_yardstick()
</code></pre>

<hr>
<h2 id='augment.tune_results'>Augment data with holdout predictions</h2><span id='topic+augment.tune_results'></span><span id='topic+augment.resample_results'></span><span id='topic+augment.last_fit'></span>

<h3>Description</h3>

<p>For <code>tune</code> objects that use resampling, these <code>augment()</code> methods will add
one or more columns for the hold-out predictions (i.e. from the assessment
set(s)).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune_results'
augment(x, ..., parameters = NULL)

## S3 method for class 'resample_results'
augment(x, ...)

## S3 method for class 'last_fit'
augment(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="augment.tune_results_+3A_x">x</code></td>
<td>
<p>An object resulting from one of the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions,
<code>fit_resamples()</code>, or <code>last_fit()</code>. The control specifications for these
objects should have used the option <code>save_pred = TRUE</code>.</p>
</td></tr>
<tr><td><code id="augment.tune_results_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="augment.tune_results_+3A_parameters">parameters</code></td>
<td>
<p>A data frame with a single row that indicates what
tuning parameters should be used to generate the predictions (for <code style="white-space: pre;">&#8288;tune_*()&#8288;</code>
objects only). If <code>NULL</code>, <code>select_best(x)</code> will be used with the first
metric and, if applicable, the first evaluation time point, used to
create <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For some resampling methods where rows may be replicated in multiple
assessment sets, the prediction columns will be averages of the holdout
results. Also, for these methods, it is possible that all rows of the
original data do not have holdout predictions (like a single bootstrap
resample). In this case, all rows are return and a warning is issued.
</p>
<p>For objects created by <code>last_fit()</code>, the test set data and predictions are
returned.
</p>
<p>Unlike other <code>augment()</code> methods, the predicted values for regression models
are in a column called <code>.pred</code> instead of <code>.fitted</code> (to be consistent with
other tidymodels conventions).
</p>
<p>For regression problems, an additional <code>.resid</code> column is added to the
results.
</p>


<h3>Value</h3>

<p>A data frame with one or more additional columns for model
predictions.
</p>

<hr>
<h2 id='autoplot.tune_results'>Plot tuning search results</h2><span id='topic+autoplot.tune_results'></span>

<h3>Description</h3>

<p>Plot tuning search results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune_results'
autoplot(
  object,
  type = c("marginals", "parameters", "performance"),
  metric = NULL,
  eval_time = NULL,
  width = NULL,
  call = rlang::current_env(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.tune_results_+3A_object">object</code></td>
<td>
<p>A tibble of results from <code><a href="#topic+tune_grid">tune_grid()</a></code> or <code><a href="#topic+tune_bayes">tune_bayes()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_type">type</code></td>
<td>
<p>A single character value. Choices are <code>"marginals"</code> (for a plot
of each predictor versus performance; see Details below), <code>"parameters"</code>
(each parameter versus search iteration), or <code>"performance"</code> (performance
versus iteration). The latter two choices are only used for <code><a href="#topic+tune_bayes">tune_bayes()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_metric">metric</code></td>
<td>
<p>A character vector or <code>NULL</code> for which metric to plot. By
default, all metrics will be shown via facets. Possible options are
the entries in <code>.metric</code> column of <code>collect_metrics(object)</code>.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be chosen (e.g. the time-dependent ROC curve, etc). The
values should be consistent with the values used to create <code>object</code>.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_width">width</code></td>
<td>
<p>A number for the width of the confidence interval bars when
<code>type = "performance"</code>. A value of zero prevents them from being shown.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_call">call</code></td>
<td>
<p>The call to be displayed in warnings or errors.</p>
</td></tr>
<tr><td><code id="autoplot.tune_results_+3A_...">...</code></td>
<td>
<p>For plots with a regular grid, this is passed to <code>format()</code> and is
applied to a parameter used to color points. Otherwise, it is not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the results of <code>tune_grid()</code> are used with <code>autoplot()</code>, it tries to
determine whether a <em>regular grid</em> was used.
</p>


<h4>Regular grids</h4>

<p>For regular grids with one or more numeric tuning parameters, the parameter
with the most unique values is used on the x-axis. If there are categorical
parameters, the first is used to color the geometries. All other parameters
are used in column faceting.
</p>
<p>The plot has the performance metric(s) on the y-axis. If there are multiple
metrics, these are row-faceted.
</p>
<p>If there are more than five tuning parameters, the &quot;marginal effects&quot; plots
are used instead.
</p>



<h4>Irregular grids</h4>

<p>For space-filling or random grids, a <em>marginal</em> effect plot is created. A
panel is made for each numeric parameter so that each parameter is on the
x-axis and performance is on the y-xis. If there are multiple metrics, these
are row-faceted.
</p>
<p>A single categorical parameter is shown as colors. If there are two or more
non-numeric parameters, an error is given. A similar result occurs is only
non-numeric parameters are in the grid. In these cases, we suggest using
<code>collect_metrics()</code> and <code>ggplot()</code> to create a plot that is appropriate for
the data.
</p>
<p>If a parameter has an associated transformation associated with it (as
determined by the parameter object used to create it), the plot shows the
values in the transformed units (and is labeled with the transformation type).
</p>
<p>Parameters are labeled using the labels found in the parameter object
<em>except</em> when an identifier was used (e.g. <code>neighbors = tune("K")</code>).
</p>



<h3>Value</h3>

<p>A <code>ggplot2</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tune_grid">tune_grid()</a></code>, <code><a href="#topic+tune_bayes">tune_bayes()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# For grid search:
data("example_ames_knn")

# Plot the tuning parameter values versus performance
autoplot(ames_grid_search, metric = "rmse")


# For iterative search:
# Plot the tuning parameter values versus performance
autoplot(ames_iter_search, metric = "rmse", type = "marginals")

# Plot tuning parameters versus iterations
autoplot(ames_iter_search, metric = "rmse", type = "parameters")

# Plot performance over iterations
autoplot(ames_iter_search, metric = "rmse", type = "performance")

</code></pre>

<hr>
<h2 id='check_rset'>Get colors for tune text.</h2><span id='topic+check_rset'></span><span id='topic+check_parameters'></span><span id='topic+check_workflow'></span><span id='topic+check_metrics'></span><span id='topic+check_initial'></span><span id='topic+val_class_or_null'></span><span id='topic+val_class_and_single'></span><span id='topic+.config_key_from_metrics'></span><span id='topic+estimate_tune_results'></span><span id='topic+metrics_info'></span><span id='topic+new_iteration_results'></span><span id='topic+get_tune_colors'></span><span id='topic+encode_set'></span><span id='topic+check_time'></span><span id='topic+pull_rset_attributes'></span><span id='topic+empty_ellipses'></span><span id='topic+is_recipe'></span><span id='topic+is_preprocessor'></span><span id='topic+is_workflow'></span>

<h3>Description</h3>

<p>These are not intended for use by the general public.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_rset(x)

check_parameters(wflow, pset = NULL, data, grid_names = character(0))

check_workflow(x, ..., pset = NULL, check_dials = FALSE, call = caller_env())

check_metrics(x, object)

check_initial(
  x,
  pset,
  wflow,
  resamples,
  metrics,
  eval_time,
  ctrl,
  checks = "grid"
)

val_class_or_null(x, cls = "numeric", where = NULL)

val_class_and_single(x, cls = "numeric", where = NULL)

.config_key_from_metrics(x)

estimate_tune_results(x, ..., col_name = ".metrics")

metrics_info(x)

new_iteration_results(
  x,
  parameters,
  metrics,
  eval_time,
  eval_time_target,
  outcomes = character(0),
  rset_info,
  workflow
)

get_tune_colors()

encode_set(x, pset, ..., as_matrix = FALSE)

check_time(origin, limit)

pull_rset_attributes(x)

empty_ellipses(...)

is_recipe(x)

is_preprocessor(x)

is_workflow(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_rset_+3A_x">x</code></td>
<td>
<p>An object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_wflow">wflow</code></td>
<td>
<p>A <code>workflow</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_pset">pset</code></td>
<td>
<p>A <code>parameters</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_data">data</code></td>
<td>
<p>The training data.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_grid_names">grid_names</code></td>
<td>
<p>A character vector of column names from the grid.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_...">...</code></td>
<td>
<p>Other options</p>
</td></tr>
<tr><td><code id="check_rset_+3A_check_dials">check_dials</code></td>
<td>
<p>A logical for check for a NULL parameter object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_object">object</code></td>
<td>
<p>A <code>workflow</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_metrics">metrics</code></td>
<td>
<p>A metric set.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc).</p>
</td></tr>
<tr><td><code id="check_rset_+3A_ctrl">ctrl</code></td>
<td>
<p>A <code>control_grid</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_cls">cls</code></td>
<td>
<p>A character vector of possible classes</p>
</td></tr>
<tr><td><code id="check_rset_+3A_where">where</code></td>
<td>
<p>A character string for the calling function.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_parameters">parameters</code></td>
<td>
<p>A <code>parameters</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_outcomes">outcomes</code></td>
<td>
<p>A character vector of outcome names.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_rset_info">rset_info</code></td>
<td>
<p>Attributes from an <code>rset</code> object.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_workflow">workflow</code></td>
<td>
<p>The workflow used to fit the iteration results.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_as_matrix">as_matrix</code></td>
<td>
<p>A logical for the return type.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_origin">origin</code></td>
<td>
<p>The calculation start time.</p>
</td></tr>
<tr><td><code id="check_rset_+3A_limit">limit</code></td>
<td>
<p>The allowable time (in minutes).</p>
</td></tr>
</table>

<hr>
<h2 id='choose_metric'>Tools for selecting metrics and evaluation times</h2><span id='topic+choose_metric'></span><span id='topic+check_metric_in_tune_results'></span><span id='topic+choose_eval_time'></span><span id='topic+maybe_choose_eval_time'></span><span id='topic+first_metric'></span><span id='topic+first_eval_time'></span><span id='topic+.filter_perf_metrics'></span><span id='topic+check_metrics_arg'></span><span id='topic+check_eval_time_arg'></span>

<h3>Description</h3>

<p>Tools for selecting metrics and evaluation times
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choose_metric(x, metric, ..., call = rlang::caller_env())

check_metric_in_tune_results(mtr_info, metric, ..., call = rlang::caller_env())

choose_eval_time(
  x,
  metric,
  ...,
  eval_time = NULL,
  quietly = FALSE,
  call = rlang::caller_env()
)

maybe_choose_eval_time(x, mtr_set, eval_time)

first_metric(mtr_set)

first_eval_time(
  mtr_set,
  ...,
  metric = NULL,
  eval_time = NULL,
  quietly = FALSE,
  call = rlang::caller_env()
)

.filter_perf_metrics(x, metric, eval_time)

check_metrics_arg(mtr_set, wflow, ..., call = rlang::caller_env())

check_eval_time_arg(eval_time, mtr_set, ..., call = rlang::caller_env())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choose_metric_+3A_x">x</code></td>
<td>
<p>An object with class <code>tune_results</code>.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_metric">metric</code></td>
<td>
<p>A character value for which metric is being used.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_...">...</code></td>
<td>
<p>These dots are for future extensions and must be empty.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_call">call</code></td>
<td>
<p>The call to be displayed in warnings or errors.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_eval_time">eval_time</code></td>
<td>
<p>An optional vector of times to compute dynamic and/or
integrated metrics.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_quietly">quietly</code></td>
<td>
<p>Logical. Should warnings be muffled?</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_mtr_set">mtr_set</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>.</p>
</td></tr>
<tr><td><code id="choose_metric_+3A_wflow">wflow</code></td>
<td>
<p>A <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are developer-facing functions used to compute and validate choices
for performance metrics. For survival analysis models, there are similar
functions for the evaluation time(s) required for dynamic and/or integrated
metrics.
</p>
<p><code>choose_metric()</code> is used with functions such as <code><a href="#topic+show_best">show_best()</a></code> or
<code><a href="#topic+select_best">select_best()</a></code> where a single valid metric is required to rank models. If
no value is given by the user, the first metric value is used (with a
warning).
</p>
<p>For evaluation times, one is only required when the metric type is dynamic
(e.g. <code><a href="yardstick.html#topic+brier_survival">yardstick::brier_survival()</a></code> or <code><a href="yardstick.html#topic+roc_auc_survival">yardstick::roc_auc_survival()</a></code>). For
these metrics, we require a single numeric value that was originally given
to the function used to produce <code>x</code> (such as <code><a href="#topic+tune_grid">tune_grid()</a></code>).
</p>
<p>If a time is required and none is given, the first value in the vector
originally given in the <code>eval_time</code> argument is used (with a warning).
</p>
<p><code>maybe_choose_eval_time()</code> is for cases where multiple evaluation times are
acceptable but you need to choose a good default. The &quot;maybe&quot; is because
the function that would use <code>maybe_choose_eval_time()</code> can accept multiple
metrics (like <code><a href="#topic+autoplot">autoplot()</a></code>).
</p>

<hr>
<h2 id='collect_predictions'>Obtain and format results produced by tuning functions</h2><span id='topic+collect_predictions'></span><span id='topic+collect_predictions.default'></span><span id='topic+collect_predictions.tune_results'></span><span id='topic+collect_metrics'></span><span id='topic+collect_metrics.tune_results'></span><span id='topic+collect_notes'></span><span id='topic+collect_notes.tune_results'></span><span id='topic+collect_extracts'></span><span id='topic+collect_extracts.tune_results'></span>

<h3>Description</h3>

<p>Obtain and format results produced by tuning functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collect_predictions(x, ...)

## Default S3 method:
collect_predictions(x, ...)

## S3 method for class 'tune_results'
collect_predictions(x, ..., summarize = FALSE, parameters = NULL)

collect_metrics(x, ...)

## S3 method for class 'tune_results'
collect_metrics(x, ..., summarize = TRUE, type = c("long", "wide"))

collect_notes(x, ...)

## S3 method for class 'tune_results'
collect_notes(x, ...)

collect_extracts(x, ...)

## S3 method for class 'tune_results'
collect_extracts(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collect_predictions_+3A_x">x</code></td>
<td>
<p>The results of <code><a href="#topic+tune_grid">tune_grid()</a></code>, <code><a href="#topic+tune_bayes">tune_bayes()</a></code>, <code><a href="#topic+fit_resamples">fit_resamples()</a></code>,
or <code><a href="#topic+last_fit">last_fit()</a></code>. For <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, the control option <code>save_pred = TRUE</code> should have been used.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_summarize">summarize</code></td>
<td>
<p>A logical; should metrics be summarized over resamples
(<code>TRUE</code>) or return the values for each individual resample. Note that, if <code>x</code>
is created by <code><a href="#topic+last_fit">last_fit()</a></code>, <code>summarize</code> has no effect. For the other object
types, the method of summarizing predictions is detailed below.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_parameters">parameters</code></td>
<td>
<p>An optional tibble of tuning parameter values that can be
used to filter the predicted values before processing. This tibble should
only have columns for each tuning parameter identifier (e.g. <code>"my_param"</code>
if <code>tune("my_param")</code> was used).</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_type">type</code></td>
<td>
<p>One of <code>"long"</code> (the default) or <code>"wide"</code>. When <code>type = "long"</code>,
output has columns <code>.metric</code> and one of <code>.estimate</code> or <code>mean</code>.
<code>.estimate</code>/<code>mean</code> gives the values for the <code>.metric</code>. When <code>type = "wide"</code>,
each metric has its own column and the <code>n</code> and <code>std_err</code> columns are removed,
if they exist.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble. The column names depend on the results and the mode of the
model.
</p>
<p>For <code><a href="#topic+collect_metrics">collect_metrics()</a></code> and <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, when unsummarized,
there are columns for each tuning parameter (using the <code>id</code> from <code><a href="#topic+tune">tune()</a></code>,
if any).
</p>
<p><code><a href="#topic+collect_metrics">collect_metrics()</a></code> also has columns <code>.metric</code>, and <code>.estimator</code> by default.
For <code><a href="#topic+collect_metrics">collect_metrics()</a></code> methods that have a <code>type</code> argument, supplying
<code>type = "wide"</code> will pivot the output such that each metric has its own
column. When the results are summarized, there are columns for <code>mean</code>, <code>n</code>,
and <code>std_err</code>. When not summarized, the additional columns for the resampling
identifier(s) and <code>.estimate</code>.
</p>
<p>For <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, there are additional columns for the resampling
identifier(s), columns for the predicted values (e.g., <code>.pred</code>,
<code>.pred_class</code>, etc.), and a column for the outcome(s) using the original
column name(s) in the data.
</p>
<p><code><a href="#topic+collect_predictions">collect_predictions()</a></code> can summarize the various results over
replicate out-of-sample predictions. For example, when using the bootstrap,
each row in the original training set has multiple holdout predictions
(across assessment sets). To convert these results to a format where every
training set same has a single predicted value, the results are averaged
over replicate predictions.
</p>
<p>For regression cases, the numeric predictions are simply averaged.
</p>
<p>For classification models, the problem is more complex. When class probabilities
are used, these are averaged and then re-normalized to make sure that they
add to one. If hard class predictions also exist in the data, then these are
determined from the summarized probability estimates (so that they match).
If only hard class predictions are in the results, then the mode is used to
summarize.
</p>
<p>With censored outcome models, the predicted survival probabilities (if any)
are averaged while the static predicted event times are summarized using the
median.
</p>
<p><code><a href="#topic+collect_notes">collect_notes()</a></code> returns a tibble with columns for the resampling
indicators, the location (preprocessor, model, etc.), type (error or warning),
and the notes.
</p>
<p><code><a href="#topic+collect_extracts">collect_extracts()</a></code> collects objects extracted from fitted workflows
via the <code>extract</code> argument to <a href="#topic+control_grid">control functions</a>. The
function returns a tibble with columns for the resampling
indicators, the location (preprocessor, model, etc.), and extracted objects.
</p>


<h3>Hyperparameters and extracted objects</h3>

<p>When making use of submodels, tune can generate predictions and calculate
metrics for multiple model <code>.config</code>urations using only one model fit.
However, this means that if a function was supplied to a
<a href="#topic+control_grid">control function's</a> <code>extract</code> argument, tune can only
execute that extraction on the one model that was fitted. As a result,
in the <code>collect_extracts()</code> output, tune opts to associate the
extracted objects with the hyperparameter combination used to
fit that one model workflow, rather than the hyperparameter
combination of a submodel. In the output, this appears like
a hyperparameter entry is recycled across many <code>.config</code>
entries&mdash;this is intentional.
</p>
<p>See <a href="https://parsnip.tidymodels.org/articles/Submodels.html">https://parsnip.tidymodels.org/articles/Submodels.html</a> to learn
more about submodels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("example_ames_knn")
# The parameters for the model:
extract_parameter_set_dials(ames_wflow)

# Summarized over resamples
collect_metrics(ames_grid_search)

# Per-resample values
collect_metrics(ames_grid_search, summarize = FALSE)


# ---------------------------------------------------------------------------

library(parsnip)
library(rsample)
library(dplyr)
library(recipes)
library(tibble)

lm_mod &lt;- linear_reg() %&gt;% set_engine("lm")
set.seed(93599150)
car_folds &lt;- vfold_cv(mtcars, v = 2, repeats = 3)
ctrl &lt;- control_resamples(save_pred = TRUE, extract = extract_fit_engine)

spline_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp, deg_free = tune("df"))

grid &lt;- tibble(df = 3:6)

resampled &lt;-
  lm_mod %&gt;%
  tune_grid(spline_rec, resamples = car_folds, control = ctrl, grid = grid)

collect_predictions(resampled) %&gt;% arrange(.row)
collect_predictions(resampled, summarize = TRUE) %&gt;% arrange(.row)
collect_predictions(
  resampled,
  summarize = TRUE,
  parameters = grid[1, ]
) %&gt;% arrange(.row)

collect_extracts(resampled)

</code></pre>

<hr>
<h2 id='compute_metrics'>Calculate and format metrics from tuning functions</h2><span id='topic+compute_metrics'></span><span id='topic+compute_metrics.default'></span><span id='topic+compute_metrics.tune_results'></span>

<h3>Description</h3>

<p>This function computes metrics from tuning results. The arguments and
output formats are closely related to those from <code><a href="#topic+collect_metrics">collect_metrics()</a></code>, but
this function additionally takes a <code>metrics</code> argument with a
<a href="yardstick.html#topic+metric_set">metric set</a> for new metrics to compute. This
allows for computing new performance metrics without requiring users to
re-evaluate models against resamples.
</p>
<p>Note that the <a href="#topic+control_grid">control option</a> <code>save_pred = TRUE</code> must
have been supplied when generating <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_metrics(x, metrics, summarize, event_level, ...)

## Default S3 method:
compute_metrics(x, metrics, summarize = TRUE, event_level = "first", ...)

## S3 method for class 'tune_results'
compute_metrics(x, metrics, ..., summarize = TRUE, event_level = "first")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_metrics_+3A_x">x</code></td>
<td>
<p>The results of a tuning function like <code><a href="#topic+tune_grid">tune_grid()</a></code> or
<code><a href="#topic+fit_resamples">fit_resamples()</a></code>, generated with the control option <code>save_pred = TRUE</code>.</p>
</td></tr>
<tr><td><code id="compute_metrics_+3A_metrics">metrics</code></td>
<td>
<p>A <a href="yardstick.html#topic+metric_set">metric set</a> of new metrics
to compute. See the &quot;Details&quot; section below for more information.</p>
</td></tr>
<tr><td><code id="compute_metrics_+3A_summarize">summarize</code></td>
<td>
<p>A single logical value indicating whether metrics should
be summarized over resamples (<code>TRUE</code>) or return the values for each
individual resample. See <code><a href="#topic+collect_metrics">collect_metrics()</a></code> for more details on how
metrics are summarized.</p>
</td></tr>
<tr><td><code id="compute_metrics_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="compute_metrics_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each metric in the set supplied to the <code>metrics</code> argument must have a metric
type (usually <code>"numeric"</code>, <code>"class"</code>, or <code>"prob"</code>) that matches some metric
evaluated when generating <code>x</code>. e.g. For example, if <code>x</code> was generated with
only hard <code>"class"</code> metrics, this function can't compute metrics that take in
class probabilities (<code>"prob"</code>.) By default, the tuning functions used to
generate <code>x</code> compute metrics of all needed types.
</p>


<h3>Value</h3>

<p>A tibble. See <code><a href="#topic+collect_metrics">collect_metrics()</a></code> for more details on the return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load needed packages:
library(parsnip)
library(rsample)
library(yardstick)

# evaluate a linear regression against resamples.
# note that we pass `save_pred = TRUE`:
res &lt;-
  fit_resamples(
    linear_reg(),
    mpg ~ cyl + hp,
    bootstraps(mtcars, 5),
    control = control_grid(save_pred = TRUE)
  )

# to return the metrics supplied to `fit_resamples()`:
collect_metrics(res)

# to compute new metrics:
compute_metrics(res, metric_set(mae))

# if `metrics` is the same as that passed to `fit_resamples()`,
# then `collect_metrics()` and `compute_metrics()` give the same
# output, though `compute_metrics()` is quite a bit slower:
all.equal(
  collect_metrics(res),
  compute_metrics(res, metric_set(rmse, rsq))
)

</code></pre>

<hr>
<h2 id='conf_mat_resampled'>Compute average confusion matrix across resamples</h2><span id='topic+conf_mat_resampled'></span>

<h3>Description</h3>

<p>For classification problems, <code>conf_mat_resampled()</code> computes a separate
confusion matrix for each resample then averages the cell counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf_mat_resampled(x, ..., parameters = NULL, tidy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf_mat_resampled_+3A_x">x</code></td>
<td>
<p>An object with class <code>tune_results</code> that was used with a
classification model that was run with <code>control_*(save_pred = TRUE)</code>.</p>
</td></tr>
<tr><td><code id="conf_mat_resampled_+3A_...">...</code></td>
<td>
<p>Currently unused, must be empty.</p>
</td></tr>
<tr><td><code id="conf_mat_resampled_+3A_parameters">parameters</code></td>
<td>
<p>A tibble with a single tuning parameter combination. Only
one tuning parameter combination (if any were used) is allowed here.</p>
</td></tr>
<tr><td><code id="conf_mat_resampled_+3A_tidy">tidy</code></td>
<td>
<p>Should the results come back in a tibble (<code>TRUE</code>) or a <code>conf_mat</code>
object like <code>yardstick::conf_mat()</code> (<code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble or <code>conf_mat</code> with the average cell count across resamples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(parsnip)
library(rsample)
library(dplyr)

data(two_class_dat, package = "modeldata")

set.seed(2393)
res &lt;-
  logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit_resamples(
    Class ~ .,
    resamples = vfold_cv(two_class_dat, v = 3),
    control = control_resamples(save_pred = TRUE)
  )

conf_mat_resampled(res)
conf_mat_resampled(res, tidy = FALSE)
</code></pre>

<hr>
<h2 id='control_bayes'>Control aspects of the Bayesian search process</h2><span id='topic+control_bayes'></span>

<h3>Description</h3>

<p>Control aspects of the Bayesian search process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_bayes(
  verbose = FALSE,
  verbose_iter = FALSE,
  no_improve = 10L,
  uncertain = Inf,
  seed = sample.int(10^5, 1),
  extract = NULL,
  save_pred = FALSE,
  time_limit = NA,
  pkgs = NULL,
  save_workflow = FALSE,
  save_gp_scoring = FALSE,
  event_level = "first",
  parallel_over = NULL,
  backend_options = NULL,
  allow_par = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_bayes_+3A_verbose">verbose</code></td>
<td>
<p>A logical for logging results (other than warnings and errors,
which are always shown) as they are generated during training in a single
R process. When using most parallel backends, this argument typically will
not result in any logging. If using a dark IDE theme, some logging messages
might be hard to see; try setting the <code>tidymodels.dark</code> option with
<code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_verbose_iter">verbose_iter</code></td>
<td>
<p>A logical for logging results of the Bayesian search
process. Defaults to FALSE. If using a dark IDE theme, some logging
messages might be hard to see; try setting the <code>tidymodels.dark</code> option
with <code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_no_improve">no_improve</code></td>
<td>
<p>The integer cutoff for the number of iterations without
better results.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_uncertain">uncertain</code></td>
<td>
<p>The number of iterations with no improvement before an
uncertainty sample is created where a sample with high predicted variance is
chosen (i.e., in a region that has not yet been explored). The iteration
counter is reset after each uncertainty sample. For example, if <code>uncertain = 10</code>, this condition is triggered every 10 samples with no improvement.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_seed">seed</code></td>
<td>
<p>An integer for controlling the random number stream. Tuning
functions are sensitive to both the state of RNG set outside of tuning
functions with <code>set.seed()</code> as well as the value set here. The value of the
former determines RNG for the higher-level tuning process, like grid
generation and setting the value of this argument if left as default. The
value of this argument determines RNG state in workers for each iteration
of model fitting, determined by the value of <code>parallel_over</code>.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_extract">extract</code></td>
<td>
<p>An optional function with at least one argument (or <code>NULL</code>)
that can be used to retain arbitrary objects from the model fit object,
recipe, or other elements of the workflow.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_save_pred">save_pred</code></td>
<td>
<p>A logical for whether the out-of-sample predictions should
be saved for each model <em>evaluated</em>.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_time_limit">time_limit</code></td>
<td>
<p>A number for the minimum number of <em>minutes</em> (elapsed) that
the function should execute. The elapsed time is evaluated at internal
checkpoints and, if over time, the results at that time are returned (with
a warning). This means that the <code>time_limit</code> is not an exact limit, but a
minimum time limit.
</p>
<p>Note that timing begins immediately on execution. Thus, if the
<code>initial</code> argument to <code><a href="#topic+tune_bayes">tune_bayes()</a></code> is supplied as a number, the elapsed
time will include the time needed to generate initialization results.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_pkgs">pkgs</code></td>
<td>
<p>An optional character string of R package names that should be
loaded (by namespace) during parallel processing.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_save_workflow">save_workflow</code></td>
<td>
<p>A logical for whether the workflow should be appended
to the output as an attribute.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_save_gp_scoring">save_gp_scoring</code></td>
<td>
<p>A logical to save the intermediate Gaussian process
models for each iteration of the search. These are saved to
<code>tempdir()</code> with names <code style="white-space: pre;">&#8288;gp_candidates_{i}.RData&#8288;</code> where <code>i</code> is the iteration.
These results are deleted when the R session ends. This option is only
useful for teaching purposes.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_parallel_over">parallel_over</code></td>
<td>
<p>A single string containing either <code>"resamples"</code> or
<code>"everything"</code> describing how to use parallel processing. Alternatively,
<code>NULL</code> is allowed, which chooses between <code>"resamples"</code> and <code>"everything"</code>
automatically.
</p>
<p>If <code>"resamples"</code>, then tuning will be performed in parallel over resamples
alone. Within each resample, the preprocessor (i.e. recipe or formula) is
processed once, and is then reused across all models that need to be fit.
</p>
<p>If <code>"everything"</code>, then tuning will be performed in parallel at two levels.
An outer parallel loop will iterate over resamples. Additionally, an
inner parallel loop will iterate over all unique combinations of
preprocessor and model tuning parameters for that specific resample. This
will result in the preprocessor being re-processed multiple times, but
can be faster if that processing is extremely fast.
</p>
<p>If <code>NULL</code>, chooses <code>"resamples"</code> if there are more than one resample,
otherwise chooses <code>"everything"</code> to attempt to maximize core utilization.
</p>
<p>Note that switching between <code>parallel_over</code> strategies is not guaranteed
to use the same random number generation schemes. However, re-tuning a
model using the same <code>parallel_over</code> strategy is guaranteed to be
reproducible between runs.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_backend_options">backend_options</code></td>
<td>
<p>An object of class <code>"tune_backend_options"</code> as created
by <code>tune::new_backend_options()</code>, used to pass arguments to specific tuning
backend. Defaults to <code>NULL</code> for default backend options.</p>
</td></tr>
<tr><td><code id="control_bayes_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>extract</code>, this function can be used to output the model object, the
recipe (if used), or some components of either or both. When evaluated, the
function's sole argument has a fitted workflow If the formula method is used,
the recipe element will be <code>NULL</code>.
</p>
<p>The results of the <code>extract</code> function are added to a list column in the
output called <code>.extracts</code>. Each element of this list is a tibble with tuning
parameter column and a list column (also called <code>.extracts</code>) that contains
the results of the function. If no extraction function is used, there is no
<code>.extracts</code> column in the resulting object. See <code><a href="#topic+tune_bayes">tune_bayes()</a></code> for more
specific details.
</p>
<p>Note that for <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, it is possible that each row of the
original data point might be represented multiple times per tuning
parameter. For example, if the bootstrap or repeated cross-validation are
used, there will be multiple rows since the sample data point has been
evaluated multiple times. This may cause issues when merging the predictions
with the original data.
</p>


<h3>Hyperparameters and extracted objects</h3>

<p>When making use of submodels, tune can generate predictions and calculate
metrics for multiple model <code>.config</code>urations using only one model fit.
However, this means that if a function was supplied to a
<a href="#topic+control_grid">control function's</a> <code>extract</code> argument, tune can only
execute that extraction on the one model that was fitted. As a result,
in the <code>collect_extracts()</code> output, tune opts to associate the
extracted objects with the hyperparameter combination used to
fit that one model workflow, rather than the hyperparameter
combination of a submodel. In the output, this appears like
a hyperparameter entry is recycled across many <code>.config</code>
entries&mdash;this is intentional.
</p>
<p>See <a href="https://parsnip.tidymodels.org/articles/Submodels.html">https://parsnip.tidymodels.org/articles/Submodels.html</a> to learn
more about submodels.
</p>

<hr>
<h2 id='control_grid'>Control aspects of the grid search process</h2><span id='topic+control_grid'></span><span id='topic+control_resamples'></span><span id='topic+new_backend_options'></span>

<h3>Description</h3>

<p>Control aspects of the grid search process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_grid(
  verbose = FALSE,
  allow_par = TRUE,
  extract = NULL,
  save_pred = FALSE,
  pkgs = NULL,
  save_workflow = FALSE,
  event_level = "first",
  parallel_over = NULL,
  backend_options = NULL
)

control_resamples(
  verbose = FALSE,
  allow_par = TRUE,
  extract = NULL,
  save_pred = FALSE,
  pkgs = NULL,
  save_workflow = FALSE,
  event_level = "first",
  parallel_over = NULL,
  backend_options = NULL
)

new_backend_options(..., class = character())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_grid_+3A_verbose">verbose</code></td>
<td>
<p>A logical for logging results (other than warnings and errors,
which are always shown) as they are generated during training in a single
R process. When using most parallel backends, this argument typically will
not result in any logging. If using a dark IDE theme, some logging messages
might be hard to see; try setting the <code>tidymodels.dark</code> option with
<code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
<tr><td><code id="control_grid_+3A_extract">extract</code></td>
<td>
<p>An optional function with at least one argument (or <code>NULL</code>)
that can be used to retain arbitrary objects from the model fit object,
recipe, or other elements of the workflow.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_save_pred">save_pred</code></td>
<td>
<p>A logical for whether the out-of-sample predictions should
be saved for each model <em>evaluated</em>.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_pkgs">pkgs</code></td>
<td>
<p>An optional character string of R package names that should be
loaded (by namespace) during parallel processing.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_save_workflow">save_workflow</code></td>
<td>
<p>A logical for whether the workflow should be appended
to the output as an attribute.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_parallel_over">parallel_over</code></td>
<td>
<p>A single string containing either <code>"resamples"</code> or
<code>"everything"</code> describing how to use parallel processing. Alternatively,
<code>NULL</code> is allowed, which chooses between <code>"resamples"</code> and <code>"everything"</code>
automatically.
</p>
<p>If <code>"resamples"</code>, then tuning will be performed in parallel over resamples
alone. Within each resample, the preprocessor (i.e. recipe or formula) is
processed once, and is then reused across all models that need to be fit.
</p>
<p>If <code>"everything"</code>, then tuning will be performed in parallel at two levels.
An outer parallel loop will iterate over resamples. Additionally, an
inner parallel loop will iterate over all unique combinations of
preprocessor and model tuning parameters for that specific resample. This
will result in the preprocessor being re-processed multiple times, but
can be faster if that processing is extremely fast.
</p>
<p>If <code>NULL</code>, chooses <code>"resamples"</code> if there are more than one resample,
otherwise chooses <code>"everything"</code> to attempt to maximize core utilization.
</p>
<p>Note that switching between <code>parallel_over</code> strategies is not guaranteed
to use the same random number generation schemes. However, re-tuning a
model using the same <code>parallel_over</code> strategy is guaranteed to be
reproducible between runs.</p>
</td></tr>
<tr><td><code id="control_grid_+3A_backend_options">backend_options</code></td>
<td>
<p>An object of class <code>"tune_backend_options"</code> as created
by <code>tune::new_backend_options()</code>, used to pass arguments to specific tuning
backend. Defaults to <code>NULL</code> for default backend options.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>extract</code>, this function can be used to output the model object, the
recipe (if used), or some components of either or both. When evaluated, the
function's sole argument has a fitted workflow If the formula method is used,
the recipe element will be <code>NULL</code>.
</p>
<p>The results of the <code>extract</code> function are added to a list column in the
output called <code>.extracts</code>. Each element of this list is a tibble with tuning
parameter column and a list column (also called <code>.extracts</code>) that contains
the results of the function. If no extraction function is used, there is no
<code>.extracts</code> column in the resulting object. See <code><a href="#topic+tune_bayes">tune_bayes()</a></code> for more
specific details.
</p>
<p>Note that for <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, it is possible that each row of the
original data point might be represented multiple times per tuning
parameter. For example, if the bootstrap or repeated cross-validation are
used, there will be multiple rows since the sample data point has been
evaluated multiple times. This may cause issues when merging the predictions
with the original data.
</p>
<p><code><a href="#topic+control_resamples">control_resamples()</a></code> is an alias for <code><a href="#topic+control_grid">control_grid()</a></code> and is meant to be
used with <code><a href="#topic+fit_resamples">fit_resamples()</a></code>.
</p>


<h3>Hyperparameters and extracted objects</h3>

<p>When making use of submodels, tune can generate predictions and calculate
metrics for multiple model <code>.config</code>urations using only one model fit.
However, this means that if a function was supplied to a
<a href="#topic+control_grid">control function's</a> <code>extract</code> argument, tune can only
execute that extraction on the one model that was fitted. As a result,
in the <code>collect_extracts()</code> output, tune opts to associate the
extracted objects with the hyperparameter combination used to
fit that one model workflow, rather than the hyperparameter
combination of a submodel. In the output, this appears like
a hyperparameter entry is recycled across many <code>.config</code>
entries&mdash;this is intentional.
</p>
<p>See <a href="https://parsnip.tidymodels.org/articles/Submodels.html">https://parsnip.tidymodels.org/articles/Submodels.html</a> to learn
more about submodels.
</p>

<hr>
<h2 id='control_last_fit'>Control aspects of the last fit process</h2><span id='topic+control_last_fit'></span>

<h3>Description</h3>

<p>Control aspects of the last fit process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_last_fit(verbose = FALSE, event_level = "first", allow_par = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_last_fit_+3A_verbose">verbose</code></td>
<td>
<p>A logical for logging results (other than warnings and errors,
which are always shown) as they are generated during training in a single
R process. When using most parallel backends, this argument typically will
not result in any logging. If using a dark IDE theme, some logging messages
might be hard to see; try setting the <code>tidymodels.dark</code> option with
<code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_last_fit_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="control_last_fit_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+control_last_fit">control_last_fit()</a></code> is a wrapper around <code><a href="#topic+control_resamples">control_resamples()</a></code> and is meant
to be used with <code><a href="#topic+last_fit">last_fit()</a></code>.
</p>

<hr>
<h2 id='coord_obs_pred'>Use same scale for plots of observed vs predicted values</h2><span id='topic+coord_obs_pred'></span>

<h3>Description</h3>

<p>For regression models, <code>coord_obs_pred()</code> can be used in a ggplot to make the
x- and y-axes have the same exact scale along with an aspect ratio of one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coord_obs_pred(ratio = 1, xlim = NULL, ylim = NULL, expand = TRUE, clip = "on")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coord_obs_pred_+3A_ratio">ratio</code></td>
<td>
<p>Aspect ratio, expressed as <code>y / x</code>. Defaults to 1.0.</p>
</td></tr>
<tr><td><code id="coord_obs_pred_+3A_xlim">xlim</code>, <code id="coord_obs_pred_+3A_ylim">ylim</code></td>
<td>
<p>Limits for the x and y axes.</p>
</td></tr>
<tr><td><code id="coord_obs_pred_+3A_expand">expand</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="coord_obs_pred_+3A_clip">clip</code></td>
<td>
<p>Should drawing be clipped to the extent of the plot panel? A setting
of &quot;on&quot; (the default) means yes, and a setting of &quot;off&quot; means no. In most
cases, the default of &quot;on&quot; should not be changed, as setting <code>clip = "off"</code>
can cause unexpected results. It allows drawing of data points anywhere on
the plot, including in the plot margins. If limits are set via <code>xlim</code> and
<code>ylim</code> and some data points fall outside those limits, then those data points
may show up in places such as the axes, the legend, the plot title, or the
plot margins.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggproto</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(solubility_test, package = "modeldata")

library(ggplot2)
p &lt;- ggplot(solubility_test, aes(x = solubility, y = prediction)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 0.5)

p

p + coord_fixed()

p + coord_obs_pred()
</code></pre>

<hr>
<h2 id='example_ames_knn'>Example Analysis of Ames Housing Data</h2><span id='topic+example_ames_knn'></span><span id='topic+ames_wflow'></span><span id='topic+ames_grid_search'></span><span id='topic+ames_iter_search'></span>

<h3>Description</h3>

<p>Example Analysis of Ames Housing Data
</p>


<h3>Details</h3>

<p>These objects are the results of an analysis of the Ames
housing data. A K-nearest neighbors model was used with a small
predictor set that included natural spline transformations of
the <code>Longitude</code> and <code>Latitude</code> predictors. The code used to
generate these examples was:
</p>
<div class="sourceCode"><pre>library(tidymodels)
library(tune)
library(AmesHousing)

# ------------------------------------------------------------------------------

ames &lt;- make_ames()

set.seed(4595)
data_split &lt;- initial_split(ames, strata = "Sale_Price")

ames_train &lt;- training(data_split)

set.seed(2453)
rs_splits &lt;- vfold_cv(ames_train, strata = "Sale_Price")

# ------------------------------------------------------------------------------

ames_rec &lt;-
  recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;%
  step_YeoJohnson(Lot_Area, Gr_Liv_Area) %&gt;%
  step_other(Neighborhood, threshold = .1)  %&gt;%
  step_dummy(all_nominal()) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_ns(Longitude, deg_free = tune("lon")) %&gt;%
  step_ns(Latitude, deg_free = tune("lat"))

knn_model &lt;-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K"),
    weight_func = tune(),
    dist_power = tune()
  ) %&gt;%
  set_engine("kknn")

ames_wflow &lt;-
  workflow() %&gt;%
  add_recipe(ames_rec) %&gt;%
  add_model(knn_model)

ames_set &lt;-
  extract_parameter_set_dials(ames_wflow) %&gt;%
  update(K = neighbors(c(1, 50)))

set.seed(7014)
ames_grid &lt;-
  ames_set %&gt;%
  grid_max_entropy(size = 10)

ames_grid_search &lt;-
  tune_grid(
    ames_wflow,
    resamples = rs_splits,
    grid = ames_grid
  )

set.seed(2082)
ames_iter_search &lt;-
  tune_bayes(
    ames_wflow,
    resamples = rs_splits,
    param_info = ames_set,
    initial = ames_grid_search,
    iter = 15
  )
</pre></div>
<p><strong>important note</strong>: Since the <code>rsample</code> split columns contain a reference
to the same data, saving them to disk can results in large object sizes when
the object is later used. In essence, R replaces all of those references with
the actual data. For this reason, we saved zero-row tibbles in their place.
This doesn't affect how we use these objects in examples but be advised that
using some <code>rsample</code> functions on them will cause issues.
</p>


<h3>Value</h3>

<table>
<tr><td><code>ames_wflow</code></td>
<td>
<p>A workflow object</p>
</td></tr>
<tr><td><code>ames_grid_search</code>, <code>ames_iter_search</code></td>
<td>
<p>Results of model tuning. </p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(tune)

ames_grid_search
ames_iter_search
</code></pre>

<hr>
<h2 id='expo_decay'>Exponential decay function</h2><span id='topic+expo_decay'></span>

<h3>Description</h3>

<p><code><a href="#topic+expo_decay">expo_decay()</a></code> can be used to increase or decrease a function exponentially
over iterations. This can be used to dynamically set parameters for
acquisition functions as iterations of Bayesian optimization proceed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expo_decay(iter, start_val, limit_val, slope = 1/5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expo_decay_+3A_iter">iter</code></td>
<td>
<p>An integer for the current iteration number.</p>
</td></tr>
<tr><td><code id="expo_decay_+3A_start_val">start_val</code></td>
<td>
<p>The number returned for the first iteration.</p>
</td></tr>
<tr><td><code id="expo_decay_+3A_limit_val">limit_val</code></td>
<td>
<p>The number that the process converges to over iterations.</p>
</td></tr>
<tr><td><code id="expo_decay_+3A_slope">slope</code></td>
<td>
<p>A coefficient for the exponent to control the rate of decay. The
sign of the slope controls the direction of decay.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that, when used with the acquisition functions in <code><a href="#topic+tune">tune()</a></code>, a wrapper
would be required since only the first argument would be evaluated during
tuning.
</p>


<h3>Value</h3>

<p>A single numeric value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tibble)
library(purrr)
library(ggplot2)
library(dplyr)
tibble(
  iter = 1:40,
  value = map_dbl(
    1:40,
    expo_decay,
    start_val = .1,
    limit_val = 0,
    slope = 1 / 5
  )
) %&gt;%
  ggplot(aes(x = iter, y = value)) +
  geom_path()

</code></pre>

<hr>
<h2 id='extract_model'>Convenience functions to extract model</h2><span id='topic+extract_model'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#soft-deprecated"><img src="../help/figures/lifecycle-soft-deprecated.svg" alt='[Soft-deprecated]' /></a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_model(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_model_+3A_x">x</code></td>
<td>
<p>A fitted workflow object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use <code><a href="#topic+extract_fit_engine.tune_results">extract_fit_engine()</a></code> instead of <code>extract_model()</code>.
</p>
<p>When extracting the fitted results, the workflow is easily accessible. If
there is only interest in the model, this functions can be used
as a shortcut
</p>


<h3>Value</h3>

<p>A fitted model.
</p>

<hr>
<h2 id='extract-tune'>Extract elements of <code>tune</code> objects</h2><span id='topic+extract-tune'></span><span id='topic+extract_workflow.last_fit'></span><span id='topic+extract_workflow.tune_results'></span><span id='topic+extract_spec_parsnip.tune_results'></span><span id='topic+extract_recipe.tune_results'></span><span id='topic+extract_fit_parsnip.tune_results'></span><span id='topic+extract_fit_engine.tune_results'></span><span id='topic+extract_mold.tune_results'></span><span id='topic+extract_preprocessor.tune_results'></span>

<h3>Description</h3>

<p>These functions extract various elements from a tune object. If they do
not exist yet, an error is thrown.
</p>

<ul>
<li> <p><code><a href="#topic+extract_preprocessor.tune_results">extract_preprocessor()</a></code> returns
the formula, recipe, or variable
expressions used for preprocessing.
</p>
</li>
<li> <p><code><a href="#topic+extract_spec_parsnip.tune_results">extract_spec_parsnip()</a></code> returns
the parsnip model specification.
</p>
</li>
<li> <p><code><a href="#topic+extract_fit_parsnip.tune_results">extract_fit_parsnip()</a></code> returns the
parsnip model fit object.
</p>
</li>
<li> <p><code><a href="#topic+extract_fit_engine.tune_results">extract_fit_engine()</a></code> returns the
engine specific fit embedded within
a parsnip model fit. For example, when using <code><a href="parsnip.html#topic+linear_reg">parsnip::linear_reg()</a></code>
with the <code>"lm"</code> engine, this returns the underlying <code>lm</code> object.
</p>
</li>
<li> <p><code><a href="#topic+extract_mold.tune_results">extract_mold()</a></code> returns the preprocessed
&quot;mold&quot; object returned
from <code><a href="hardhat.html#topic+mold">hardhat::mold()</a></code>. It contains information about the preprocessing,
including either the prepped recipe, the formula terms object, or
variable selectors.
</p>
</li>
<li> <p><code><a href="#topic+extract_recipe.tune_results">extract_recipe()</a></code> returns the recipe.
The <code>estimated</code> argument specifies
whether the fitted or original recipe is returned.
</p>
</li>
<li> <p><code><a href="#topic+extract_workflow.tune_results">extract_workflow()</a></code> returns the
workflow object if the control option
<code>save_workflow = TRUE</code> was used. The workflow will only have been
estimated for objects produced by <code><a href="#topic+last_fit">last_fit()</a></code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'last_fit'
extract_workflow(x, ...)

## S3 method for class 'tune_results'
extract_workflow(x, ...)

## S3 method for class 'tune_results'
extract_spec_parsnip(x, ...)

## S3 method for class 'tune_results'
extract_recipe(x, ..., estimated = TRUE)

## S3 method for class 'tune_results'
extract_fit_parsnip(x, ...)

## S3 method for class 'tune_results'
extract_fit_engine(x, ...)

## S3 method for class 'tune_results'
extract_mold(x, ...)

## S3 method for class 'tune_results'
extract_preprocessor(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract-tune_+3A_x">x</code></td>
<td>
<p>A <code>tune_results</code> object.</p>
</td></tr>
<tr><td><code id="extract-tune_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="extract-tune_+3A_estimated">estimated</code></td>
<td>
<p>A logical for whether the original (unfit) recipe or the
fitted recipe should be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions supersede <code>extract_model()</code>.
</p>


<h3>Value</h3>

<p>The extracted value from the <code>tune</code> tune_results, <code>x</code>, as described in the
description section.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(recipes)
library(rsample)
library(parsnip)

set.seed(6735)
tr_te_split &lt;- initial_split(mtcars)

spline_rec &lt;- recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp)

lin_mod &lt;- linear_reg() %&gt;%
  set_engine("lm")

spline_res &lt;- last_fit(lin_mod, spline_rec, split = tr_te_split)

extract_preprocessor(spline_res)

# The `spec` is the parsnip spec before it has been fit.
# The `fit` is the fitted parsnip model.
extract_spec_parsnip(spline_res)
extract_fit_parsnip(spline_res)
extract_fit_engine(spline_res)

# The mold is returned from `hardhat::mold()`, and contains the
# predictors, outcomes, and information about the preprocessing
# for use on new data at `predict()` time.
extract_mold(spline_res)

# A useful shortcut is to extract the fitted recipe from the workflow
extract_recipe(spline_res)

# That is identical to
identical(
  extract_mold(spline_res)$blueprint$recipe,
  extract_recipe(spline_res)
)
</code></pre>

<hr>
<h2 id='filter_parameters'>Remove some tuning parameter results</h2><span id='topic+filter_parameters'></span>

<h3>Description</h3>

<p>For objects produced by the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, there may only be a subset
of tuning parameter combinations of interest. For large data sets, it might be
helpful to be able to remove some results. This function trims the <code>.metrics</code>
column of unwanted results as well as columns <code>.predictions</code> and <code>.extracts</code>
(if they were requested).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_parameters(x, ..., parameters = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_parameters_+3A_x">x</code></td>
<td>
<p>An object of class <code>tune_results</code> that has multiple tuning parameters.</p>
</td></tr>
<tr><td><code id="filter_parameters_+3A_...">...</code></td>
<td>
<p>Expressions that return a logical value, and are defined in terms
of the tuning parameter values. If multiple expressions are included, they
are combined with the <code>&amp;</code> operator. Only rows for which all conditions
evaluate to <code>TRUE</code> are kept.</p>
</td></tr>
<tr><td><code id="filter_parameters_+3A_parameters">parameters</code></td>
<td>
<p>A tibble of tuning parameter values that can be used to
filter the predicted values before processing. This tibble should only have
columns for tuning parameter identifiers (e.g. <code>"my_param"</code> if
<code>tune("my_param")</code> was used). There can be multiple rows and one or more
columns. <strong>If used, this parameter must be named.</strong></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Removing some parameter combinations might affect the results of <code>autoplot()</code>
for the object.
</p>


<h3>Value</h3>

<p>A version of <code>x</code> where the lists columns only retain the parameter
combinations in <code>parameters</code> or satisfied by the filtering logic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(tibble)

# For grid search:
data("example_ames_knn")

## -----------------------------------------------------------------------------
# select all combinations using the 'rank' weighting scheme

ames_grid_search %&gt;%
  collect_metrics()

filter_parameters(ames_grid_search, weight_func == "rank") %&gt;%
  collect_metrics()

rank_only &lt;- tibble::tibble(weight_func = "rank")
filter_parameters(ames_grid_search, parameters = rank_only) %&gt;%
  collect_metrics()

## -----------------------------------------------------------------------------
# Keep only the results from the numerically best combination

ames_iter_search %&gt;%
  collect_metrics()

best_param &lt;- select_best(ames_iter_search, metric = "rmse")
ames_iter_search %&gt;%
  filter_parameters(parameters = best_param) %&gt;%
  collect_metrics()
</code></pre>

<hr>
<h2 id='finalize_model'>Splice final parameters into objects</h2><span id='topic+finalize_model'></span><span id='topic+finalize_recipe'></span><span id='topic+finalize_workflow'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;finalize_*&#8288;</code> functions take a list or tibble of tuning parameter values and
update objects with those values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finalize_model(x, parameters)

finalize_recipe(x, parameters)

finalize_workflow(x, parameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="finalize_model_+3A_x">x</code></td>
<td>
<p>A recipe, <code>parsnip</code> model specification, or workflow.</p>
</td></tr>
<tr><td><code id="finalize_model_+3A_parameters">parameters</code></td>
<td>
<p>A list or 1-row tibble of parameter values. Note that the
column names of the tibble should be the <code>id</code> fields attached to <code>tune()</code>.
For example, in the <code>Examples</code> section below, the model has <code>tune("K")</code>. In
this case, the parameter tibble should be &quot;K&quot; and not &quot;neighbors&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated version of <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("example_ames_knn")

library(parsnip)
knn_model &lt;-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K"),
    weight_func = tune(),
    dist_power = tune()
  ) %&gt;%
  set_engine("kknn")

lowest_rmse &lt;- select_best(ames_grid_search, metric = "rmse")
lowest_rmse

knn_model
finalize_model(knn_model, lowest_rmse)

</code></pre>

<hr>
<h2 id='fit_best'>Fit a model to the numerically optimal configuration</h2><span id='topic+fit_best'></span><span id='topic+fit_best.default'></span><span id='topic+fit_best.tune_results'></span>

<h3>Description</h3>

<p><code>fit_best()</code> takes the results from model tuning and fits it to the training
set using tuning parameters associated with the best performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_best(x, ...)

## Default S3 method:
fit_best(x, ...)

## S3 method for class 'tune_results'
fit_best(
  x,
  ...,
  metric = NULL,
  eval_time = NULL,
  parameters = NULL,
  verbose = FALSE,
  add_validation_set = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_best_+3A_x">x</code></td>
<td>
<p>The results of class <code>tune_results</code> (coming from functions such as
<code><a href="#topic+tune_grid">tune_grid()</a></code>, <code><a href="#topic+tune_bayes">tune_bayes()</a></code>, etc). The control option
<code><a href="#topic+control_grid">save_workflow = TRUE</a></code> should have been used.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_...">...</code></td>
<td>
<p>Not currently used, must be empty.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_metric">metric</code></td>
<td>
<p>A character string (or <code>NULL</code>) for which metric to optimize. If
<code>NULL</code>, the first metric is used.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_eval_time">eval_time</code></td>
<td>
<p>A single numeric time point where dynamic event time
metrics should be chosen (e.g., the time-dependent ROC curve, etc). The
values should be consistent with the values used to create <code>x</code>. The <code>NULL</code>
default will automatically use the first evaluation time used by <code>x</code>.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_parameters">parameters</code></td>
<td>
<p>An optional 1-row tibble of tuning parameter settings, with
a column for each tuning parameter. This tibble should have columns for each
tuning parameter identifier (e.g. <code>"my_param"</code> if <code>tune("my_param")</code> was used).
If <code>NULL</code>, this argument will be set to
<code><a href="#topic+select_best.tune_results">select_best(metric, eval_time)</a></code>.
If not <code>NULL</code>, <code>parameters</code> overwrites the specification via <code>metric</code>, and
<code>eval_time</code>.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_verbose">verbose</code></td>
<td>
<p>A logical for printing logging.</p>
</td></tr>
<tr><td><code id="fit_best_+3A_add_validation_set">add_validation_set</code></td>
<td>
<p>When the resamples embedded in <code>x</code> are a split into
training set and validation set, should the validation set be included in the
data set used to train the model? If not, only the training set is used. If
<code>NULL</code>, the validation set is not used for resamples originating from
<code><a href="rsample.html#topic+validation_set">rsample::validation_set()</a></code> while it is used for resamples originating
from <code><a href="rsample.html#topic+validation_split">rsample::validation_split()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a shortcut for the manual steps of:
</p>
<pre>
  best_param &lt;- select_best(tune_results, metric) # or other `select_*()`
  wflow &lt;- finalize_workflow(wflow, best_param)  # or just `finalize_model()`
  wflow_fit &lt;- fit(wflow, data_set)
</pre>


<h3>Value</h3>

<p>A fitted workflow.
</p>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code><a href="parsnip.html#topic+case_weights_allowed">parsnip::case_weights_allowed()</a></code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code><a href="hardhat.html#topic+importance_weights">hardhat:: importance_weights()</a></code> or
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.
</p>
<p>For functions such as <code><a href="#topic+fit_resamples">fit_resamples()</a></code> and the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, the
model must be contained inside of a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>. To declare that
case weights are used, invoke <code><a href="workflows.html#topic+add_case_weights">workflows::add_case_weights()</a></code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>See also</h3>

<p><code><a href="#topic+last_fit">last_fit()</a></code> is closely related to <code><a href="#topic+fit_best">fit_best()</a></code>. They both
give you access to a workflow fitted on the training data but are situated
somewhat differently in the modeling workflow. <code><a href="#topic+fit_best">fit_best()</a></code> picks up
after a tuning function like <code><a href="#topic+tune_grid">tune_grid()</a></code> to take you from tuning results
to fitted workflow, ready for you to predict and assess further. <code><a href="#topic+last_fit">last_fit()</a></code>
assumes you have made your choice of hyperparameters and finalized your
workflow to then take you from finalized workflow to fitted workflow and
further to performance assessment on the test data. While <code><a href="#topic+fit_best">fit_best()</a></code> gives
a fitted workflow, <code><a href="#topic+last_fit">last_fit()</a></code> gives you the performance results. If you
want the fitted workflow, you can extract it from the result of <code><a href="#topic+last_fit">last_fit()</a></code>
via <a href="#topic+extract_workflow.tune_results">extract_workflow()</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(rsample)
library(parsnip)
library(dplyr)

data(meats, package = "modeldata")
meats &lt;- meats %&gt;% select(-water, -fat)

set.seed(1)
meat_split &lt;- initial_split(meats)
meat_train &lt;- training(meat_split)
meat_test  &lt;- testing(meat_split)

set.seed(2)
meat_rs &lt;- vfold_cv(meat_train, v = 10)

pca_rec &lt;-
  recipe(protein ~ ., data = meat_train) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;%
  step_pca(all_numeric_predictors(), num_comp = tune())

knn_mod &lt;- nearest_neighbor(neighbors = tune()) %&gt;% set_mode("regression")

ctrl &lt;- control_grid(save_workflow = TRUE)

set.seed(128)
knn_pca_res &lt;-
  tune_grid(knn_mod, pca_rec, resamples = meat_rs, grid = 10, control = ctrl)

knn_fit &lt;- fit_best(knn_pca_res, verbose = TRUE)
predict(knn_fit, meat_test)

</code></pre>

<hr>
<h2 id='fit_resamples'>Fit multiple models via resampling</h2><span id='topic+fit_resamples'></span><span id='topic+fit_resamples.model_spec'></span><span id='topic+fit_resamples.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+fit_resamples">fit_resamples()</a></code> computes a set of performance metrics across one or more
resamples. It does not perform any tuning (see <code><a href="#topic+tune_grid">tune_grid()</a></code> and
<code><a href="#topic+tune_bayes">tune_bayes()</a></code> for that), and is instead used for fitting a single
model+recipe or model+formula combination across many resamples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_resamples(object, ...)

## S3 method for class 'model_spec'
fit_resamples(
  object,
  preprocessor,
  resamples,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_resamples()
)

## S3 method for class 'workflow'
fit_resamples(
  object,
  resamples,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_resamples()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_resamples_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or an unfitted
<a href="workflows.html#topic+workflow">workflow()</a>. No tuning parameters are allowed; if arguments
have been marked with <a href="hardhat.html#topic+tune">tune()</a>, their values must be
<a href="#topic+finalize_model">finalized</a>.</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset</code> resampling object created from an <code>rsample</code>
function, such as <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>, or <code>NULL</code> to compute a standard
set of metrics.</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="fit_resamples_+3A_control">control</code></td>
<td>
<p>A <code><a href="#topic+control_resamples">control_resamples()</a></code> object used to fine tune the resampling
process.</p>
</td></tr>
</table>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code><a href="parsnip.html#topic+case_weights_allowed">parsnip::case_weights_allowed()</a></code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code><a href="hardhat.html#topic+importance_weights">hardhat:: importance_weights()</a></code> or
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.
</p>
<p>For functions such as <code><a href="#topic+fit_resamples">fit_resamples()</a></code> and the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, the
model must be contained inside of a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>. To declare that
case weights are used, invoke <code><a href="workflows.html#topic+add_case_weights">workflows::add_case_weights()</a></code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>Censored Regression Models</h3>

<p>Three types of metrics can be used to assess the quality of censored
regression models:
</p>

<ul>
<li><p> static: the prediction is independent of time.
</p>
</li>
<li><p> dynamic: the prediction is a time-specific probability (e.g., survival
probability) and is measured at one or more particular times.
</p>
</li>
<li><p> integrated: same as the dynamic metric but returns the integral of the
different metrics from each time point.
</p>
</li></ul>

<p>Which metrics are chosen by the user affects how many evaluation times
should be specified. For example:
</p>
<div class="sourceCode"><pre># Needs no `eval_time` value
metric_set(concordance_survival)

# Needs at least one `eval_time`
metric_set(brier_survival)
metric_set(brier_survival, concordance_survival)

# Needs at least two eval_time` values
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival, brier_survival)
</pre></div>
<p>Values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>


<h3>Performance Metrics</h3>

<p>To use your own performance metrics, the <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> function
can be used to pick what should be measured for each model. If multiple
metrics are desired, they can be bundled. For example, to estimate the area
under the ROC curve as well as the sensitivity and specificity (under the
typical probability cutoff of 0.50), the <code>metrics</code> argument could be given:
</p>
<pre>
  metrics = metric_set(roc_auc, sens, spec)
</pre>
<p>Each metric is calculated for each candidate model.
</p>
<p>If no metric set is provided, one is created:
</p>

<ul>
<li><p> For regression models, the root mean squared error and coefficient
of determination are computed.
</p>
</li>
<li><p> For classification, the area under the ROC curve and overall accuracy
are computed.
</p>
</li></ul>

<p>Note that the metrics also determine what type of predictions are estimated
during tuning. For example, in a classification problem, if metrics are used
that are all associated with hard class predictions, the classification
probabilities are not created.
</p>
<p>The out-of-sample estimates of these metrics are contained in a list column
called <code>.metrics</code>. This tibble contains a row for each metric and columns
for the value, the estimator type, and so on.
</p>
<p><code><a href="#topic+collect_metrics">collect_metrics()</a></code> can be used for these objects to collapse the results
over the resampled (to obtain the final resampling estimates per tuning
parameter combination).
</p>


<h3>Obtaining Predictions</h3>

<p>When <code>control_grid(save_pred = TRUE)</code>, the output tibble contains a list
column called <code>.predictions</code> that has the out-of-sample predictions for each
parameter combination in the grid and each fold (which can be very large).
</p>
<p>The elements of the tibble are tibbles with columns for the tuning
parameters, the row number from the original data object (<code>.row</code>), the
outcome data (with the same name(s) of the original data), and any columns
created by the predictions. For example, for simple regression problems, this
function generates a column called <code>.pred</code> and so on. As noted above, the
prediction columns that are returned are determined by the type of metric(s)
requested.
</p>
<p>This list column can be <code>unnested</code> using <code><a href="tidyr.html#topic+unnest">tidyr::unnest()</a></code> or using the
convenience function <code><a href="#topic+collect_predictions">collect_predictions()</a></code>.
</p>


<h3>Extracting Information</h3>

<p>The <code>extract</code> control option will result in an additional function to be
returned called <code>.extracts</code>. This is a list column that has tibbles
containing the results of the user's function for each tuning parameter
combination. This can enable returning each model and/or recipe object that
is created during resampling. Note that this could result in a large return
object, depending on what is returned.
</p>
<p>The control function contains an option (<code>extract</code>) that can be used to
retain any model or recipe that was created within the resamples. This
argument should be a function with a single argument. The value of the
argument that is given to the function in each resample is a workflow
object (see <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> for more information). Several
helper functions can be used to easily pull out the preprocessing
and/or model information from the workflow, such as
<code><a href="workflows.html#topic+extract-workflow">extract_preprocessor()</a></code> and
<code><a href="workflows.html#topic+extract-workflow">extract_fit_parsnip()</a></code>.
</p>
<p>As an example, if there is interest in getting each parsnip model fit back,
one could use:
</p>
<pre>
  extract = function (x) extract_fit_parsnip(x)
</pre>
<p>Note that the function given to the <code>extract</code> argument is evaluated on
every model that is <em>fit</em> (as opposed to every model that is <em>evaluated</em>).
As noted above, in some cases, model predictions can be derived for
sub-models so that, in these cases, not every row in the tuning parameter
grid has a separate R object associated with it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+control_resamples">control_resamples()</a></code>, <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, <code><a href="#topic+collect_metrics">collect_metrics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(rsample)
library(parsnip)
library(workflows)

set.seed(6735)
folds &lt;- vfold_cv(mtcars, v = 5)

spline_rec &lt;- recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp) %&gt;%
  step_ns(wt)

lin_mod &lt;- linear_reg() %&gt;%
  set_engine("lm")

control &lt;- control_resamples(save_pred = TRUE)

spline_res &lt;- fit_resamples(lin_mod, spline_rec, folds, control = control)

spline_res

show_best(spline_res, metric = "rmse")

# You can also wrap up a preprocessor and a model into a workflow, and
# supply that to `fit_resamples()` instead. Here, a workflows "variables"
# preprocessor is used, which lets you supply terms using dplyr selectors.
# The variables are used as-is, no preprocessing is done to them.
wf &lt;- workflow() %&gt;%
  add_variables(outcomes = mpg, predictors = everything()) %&gt;%
  add_model(lin_mod)

wf_res &lt;- fit_resamples(wf, folds)

</code></pre>

<hr>
<h2 id='forge_from_workflow'>Internal functions used by other tidymodels packages</h2><span id='topic+forge_from_workflow'></span><span id='topic+finalize_workflow_preprocessor'></span><span id='topic+tune-internal-functions'></span><span id='topic+.estimate_metrics'></span><span id='topic+.load_namespace'></span><span id='topic+initialize_catalog'></span><span id='topic+.catch_and_log'></span><span id='topic+.catch_and_log_fit'></span>

<h3>Description</h3>

<p>These are not to be meant to be invoked directly by users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forge_from_workflow(new_data, workflow)

finalize_workflow_preprocessor(workflow, grid_preprocessor)

.estimate_metrics(
  dat,
  metric,
  param_names,
  outcome_name,
  event_level,
  metrics_info = metrics_info(metrics)
)

.load_namespace(x)

initialize_catalog(control, env = rlang::caller_env())

.catch_and_log(.expr, ..., bad_only = FALSE, notes, catalog = TRUE)

.catch_and_log_fit(.expr, ..., notes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forge_from_workflow_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix of predictors to process.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_workflow">workflow</code></td>
<td>
<p>A workflow.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_grid_preprocessor">grid_preprocessor</code></td>
<td>
<p>A tibble with parameter information.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_dat">dat</code></td>
<td>
<p>A data set.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_metric">metric</code></td>
<td>
<p>A metric set.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_param_names">param_names</code></td>
<td>
<p>A character vector of tuning parameter names.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_outcome_name">outcome_name</code></td>
<td>
<p>A character string for the column of <code>dat</code> that is the
outcome.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_event_level">event_level</code></td>
<td>
<p>A logical passed from the control function.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_metrics_info">metrics_info</code></td>
<td>
<p>The output of <code>tune:::metrics_info(metrics)</code>&mdash;only
included as an argument to allow for pre-computing.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_x">x</code></td>
<td>
<p>A character vector of package names.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_.expr">.expr</code></td>
<td>
<p>Code to execute.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_...">...</code></td>
<td>
<p>Object to pass to the internal <code>tune_log()</code> function.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_bad_only">bad_only</code></td>
<td>
<p>A logical for whether warnings and errors should be caught.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_notes">notes</code></td>
<td>
<p>Character data to add to the logging.</p>
</td></tr>
<tr><td><code id="forge_from_workflow_+3A_catalog">catalog</code></td>
<td>
<p>A logical passed to <code>tune_log()</code> giving whether the message
is compatible with the issue cataloger. Defaults to <code>TRUE</code>. Updates that are
always unique and do not represent a tuning &quot;issue&quot; can bypass the cataloger
by setting <code>catalog = FALSE</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='get_metric_time'>Get time for analysis of dynamic survival metrics</h2><span id='topic+get_metric_time'></span>

<h3>Description</h3>

<p>Get time for analysis of dynamic survival metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_metric_time(metrics, eval_time)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_metric_time_+3A_metrics">metrics</code></td>
<td>
<p>A metric set.</p>
</td></tr>
<tr><td><code id="get_metric_time_+3A_eval_time">eval_time</code></td>
<td>
<p>A vector of evaluation times.</p>
</td></tr>
</table>

<hr>
<h2 id='int_pctl.tune_results'>Bootstrap confidence intervals for performance metrics</h2><span id='topic+int_pctl.tune_results'></span>

<h3>Description</h3>

<p>Using out-of-sample predictions, the bootstrap is used to create percentile
confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune_results'
int_pctl(
  .data,
  metrics = NULL,
  eval_time = NULL,
  times = 1001,
  parameters = NULL,
  alpha = 0.05,
  allow_par = TRUE,
  event_level = "first",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int_pctl.tune_results_+3A_.data">.data</code></td>
<td>
<p>A object with class <code>tune_results</code> where the <code>save_pred = TRUE</code>
option was used in the control function.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>. By default, it uses the same
metrics as the original object.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_eval_time">eval_time</code></td>
<td>
<p>A vector of evaluation times for censored regression models.
<code>NULL</code> is appropriate otherwise. If <code>NULL</code> is used with censored models, a
evaluation time is selected, and a warning is issued.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_times">times</code></td>
<td>
<p>The number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_parameters">parameters</code></td>
<td>
<p>An optional tibble of tuning parameter values that can be
used to filter the predicted values before processing. This tibble should
only have columns for each tuning parameter identifier (e.g. <code>"my_param"</code>
if <code>tune("my_param")</code> was used).</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_alpha">alpha</code></td>
<td>
<p>Level of significance.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of truth to consider as the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="int_pctl.tune_results_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each model configuration (if any), this function takes bootstrap samples
of the out-of-sample predicted values. For each bootstrap sample, the metrics
are computed and these are used to compute confidence intervals.
See <code><a href="rsample.html#topic+int_pctl">rsample::int_pctl()</a></code> and the references therein for more details.
</p>
<p>Note that the <code>.estimate</code> column is likely to be different from the results
given by <code><a href="#topic+collect_metrics">collect_metrics()</a></code> since a different estimator is used. Since
random numbers are used in sampling, set the random number seed prior to
running this function.
</p>
<p>The number of bootstrap samples should be large to have reliable intervals.
The defaults reflect the fewest samples that should be used.
</p>
<p>The computations for each configuration can be extensive. To increase
computational efficiency parallel processing can be used. The <span class="pkg">future</span>
package is used here. To execute the resampling iterations in parallel,
specify a <a href="future.html#topic+plan">plan</a> with future first. The <code>allow_par</code> argument
can be used to avoid parallelism.
</p>
<p>Also, if a censored regression model used numerous evaluation times, the
computations can take a long time unless the times are filtered with the
<code>eval_time</code> argument.
</p>


<h3>Value</h3>

<p>A tibble of metrics with additional columns for <code>.lower</code> and
<code>.upper</code>.
</p>


<h3>References</h3>

<p>Davison, A., &amp; Hinkley, D. (1997). <em>Bootstrap Methods and their
Application</em>. Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511802843
</p>


<h3>See Also</h3>

<p><code><a href="rsample.html#topic+int_pctl">rsample::int_pctl()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sacramento, package = "modeldata")
library(rsample)
library(parsnip)

set.seed(13)
sac_rs &lt;- vfold_cv(Sacramento)

lm_res &lt;-
  linear_reg() %&gt;%
  fit_resamples(
    log10(price) ~ beds + baths + sqft + type + latitude + longitude,
    resamples = sac_rs,
    control = control_resamples(save_pred = TRUE)
  )

set.seed(31)
int_pctl(lm_res)

</code></pre>

<hr>
<h2 id='last_fit'>Fit the final best model to the training set and evaluate the test set</h2><span id='topic+last_fit'></span><span id='topic+last_fit.model_spec'></span><span id='topic+last_fit.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+last_fit">last_fit()</a></code> emulates the process where, after determining the best model,
the final fit on the entire training set is needed and is then evaluated on
the test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>last_fit(object, ...)

## S3 method for class 'model_spec'
last_fit(
  object,
  preprocessor,
  split,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_last_fit(),
  add_validation_set = FALSE
)

## S3 method for class 'workflow'
last_fit(
  object,
  split,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_last_fit(),
  add_validation_set = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="last_fit_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or an unfitted
<a href="workflows.html#topic+workflow">workflow()</a>. No tuning parameters are allowed; if arguments
have been marked with <a href="hardhat.html#topic+tune">tune()</a>, their values must be
<a href="#topic+finalize_model">finalized</a>.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_split">split</code></td>
<td>
<p>An <code>rsplit</code> object created from <code><a href="rsample.html#topic+initial_split">rsample::initial_split()</a></code> or
<code><a href="rsample.html#topic+initial_validation_split">rsample::initial_validation_split()</a></code>.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>, or <code>NULL</code> to compute a standard
set of metrics.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="last_fit_+3A_control">control</code></td>
<td>
<p>A <code><a href="#topic+control_last_fit">control_last_fit()</a></code> object used to fine tune the last fit
process.</p>
</td></tr>
<tr><td><code id="last_fit_+3A_add_validation_set">add_validation_set</code></td>
<td>
<p>For 3-way splits into training, validation, and test
set via <code><a href="rsample.html#topic+initial_validation_split">rsample::initial_validation_split()</a></code>, should the validation set be
included in the data set used to train the model. If not, only the training
set is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is intended to be used after fitting a <em>variety of models</em>
and the final tuning parameters (if any) have been finalized. The next step
would be to fit using the entire training set and verify performance using
the test data.
</p>


<h3>Value</h3>

<p>A single row tibble that emulates the structure of <code>fit_resamples()</code>.
However, a list column called <code>.workflow</code> is also attached with the fitted
model (and recipe, if any) that used the training set. Helper functions
for formatting tuning results like <code><a href="#topic+collect_metrics">collect_metrics()</a></code> and
<code><a href="#topic+collect_predictions">collect_predictions()</a></code> can be used with <code>last_fit()</code> output.
</p>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code><a href="parsnip.html#topic+case_weights_allowed">parsnip::case_weights_allowed()</a></code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code><a href="hardhat.html#topic+importance_weights">hardhat:: importance_weights()</a></code> or
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.
</p>
<p>For functions such as <code><a href="#topic+fit_resamples">fit_resamples()</a></code> and the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, the
model must be contained inside of a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>. To declare that
case weights are used, invoke <code><a href="workflows.html#topic+add_case_weights">workflows::add_case_weights()</a></code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>Censored Regression Models</h3>

<p>Three types of metrics can be used to assess the quality of censored
regression models:
</p>

<ul>
<li><p> static: the prediction is independent of time.
</p>
</li>
<li><p> dynamic: the prediction is a time-specific probability (e.g., survival
probability) and is measured at one or more particular times.
</p>
</li>
<li><p> integrated: same as the dynamic metric but returns the integral of the
different metrics from each time point.
</p>
</li></ul>

<p>Which metrics are chosen by the user affects how many evaluation times
should be specified. For example:
</p>
<div class="sourceCode"><pre># Needs no `eval_time` value
metric_set(concordance_survival)

# Needs at least one `eval_time`
metric_set(brier_survival)
metric_set(brier_survival, concordance_survival)

# Needs at least two eval_time` values
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival, brier_survival)
</pre></div>
<p>Values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>


<h3>See also</h3>

<p><code><a href="#topic+last_fit">last_fit()</a></code> is closely related to <code><a href="#topic+fit_best">fit_best()</a></code>. They both
give you access to a workflow fitted on the training data but are situated
somewhat differently in the modeling workflow. <code><a href="#topic+fit_best">fit_best()</a></code> picks up
after a tuning function like <code><a href="#topic+tune_grid">tune_grid()</a></code> to take you from tuning results
to fitted workflow, ready for you to predict and assess further. <code><a href="#topic+last_fit">last_fit()</a></code>
assumes you have made your choice of hyperparameters and finalized your
workflow to then take you from finalized workflow to fitted workflow and
further to performance assessment on the test data. While <code><a href="#topic+fit_best">fit_best()</a></code> gives
a fitted workflow, <code><a href="#topic+last_fit">last_fit()</a></code> gives you the performance results. If you
want the fitted workflow, you can extract it from the result of <code><a href="#topic+last_fit">last_fit()</a></code>
via <a href="#topic+extract_workflow.tune_results">extract_workflow()</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(rsample)
library(parsnip)

set.seed(6735)
tr_te_split &lt;- initial_split(mtcars)

spline_rec &lt;- recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp)

lin_mod &lt;- linear_reg() %&gt;%
  set_engine("lm")

spline_res &lt;- last_fit(lin_mod, spline_rec, split = tr_te_split)
spline_res

# test set metrics
collect_metrics(spline_res)

# test set predictions
collect_predictions(spline_res)

# or use a workflow

library(workflows)
spline_wfl &lt;-
  workflow() %&gt;%
  add_recipe(spline_rec) %&gt;%
  add_model(lin_mod)

last_fit(spline_wfl, split = tr_te_split)

</code></pre>

<hr>
<h2 id='load_pkgs'>Quietly load package namespace</h2><span id='topic+load_pkgs'></span>

<h3>Description</h3>

<p>For one or more packages, load the namespace. This is used during parallel
processing since the different parallel backends handle the package
environments differently.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_pkgs(x, ..., infra = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_pkgs_+3A_x">x</code></td>
<td>
<p>A character vector of packages.</p>
</td></tr>
<tr><td><code id="load_pkgs_+3A_infra">infra</code></td>
<td>
<p>Should base tidymodels packages be loaded as well?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An invisible NULL.
</p>

<hr>
<h2 id='merge.recipe'>Merge parameter grid values into objects</h2><span id='topic+merge.recipe'></span><span id='topic+merge.model_spec'></span>

<h3>Description</h3>

<p><code>merge()</code> can be used to easily update any of the arguments in a
<span class="pkg">parsnip</span> model or recipe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'recipe'
merge(x, y, ...)

## S3 method for class 'model_spec'
merge(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge.recipe_+3A_x">x</code></td>
<td>
<p>A recipe or model specification object.</p>
</td></tr>
<tr><td><code id="merge.recipe_+3A_y">y</code></td>
<td>
<p>A data frame or a parameter grid resulting from one of the
<code style="white-space: pre;">&#8288;grid_*&#8288;</code> functions. The column names should correspond to the parameter
names (or their annotations) in the object.</p>
</td></tr>
<tr><td><code id="merge.recipe_+3A_...">...</code></td>
<td>
<p>Not used but required for S3 completeness.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with a column <code>x</code> that has as many rows as were in <code>y</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tibble)
library(recipes)
library(parsnip)
library(dials)

pca_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_impute_knn(all_predictors(), neighbors = tune()) %&gt;%
  step_pca(all_predictors(), num_comp = tune())

pca_grid &lt;-
  tribble(
    ~neighbors, ~num_comp,
             1,         1,
             5,         1,
             1,         2,
             5,         2
  )

merge(pca_rec, pca_grid)

spline_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp, deg_free = tune("disp df")) %&gt;%
  step_ns(wt, deg_free = tune("wt df"))

spline_grid &lt;-
  tribble(
    ~"disp df", ~ "wt df",
    3,         3,
    5,         3,
    3,         5,
    5,         5
  )

merge(pca_rec, pca_grid)

data(hpc_data, package = "modeldata")

xgb_mod &lt;-
  boost_tree(trees = tune(), min_n = tune()) %&gt;%
  set_engine("xgboost")

set.seed(254)
xgb_grid &lt;-
  extract_parameter_set_dials(xgb_mod) %&gt;%
  finalize(hpc_data) %&gt;%
  grid_max_entropy(size = 3)

merge(xgb_mod, xgb_grid)

</code></pre>

<hr>
<h2 id='message_wrap'>Write a message that respects the line width</h2><span id='topic+message_wrap'></span>

<h3>Description</h3>

<p>Write a message that respects the line width
</p>


<h3>Usage</h3>

<pre><code class='language-R'>message_wrap(
  x,
  width = options()$width - 2,
  prefix = "",
  color_text = NULL,
  color_prefix = color_text
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="message_wrap_+3A_x">x</code></td>
<td>
<p>A character string of the message text.</p>
</td></tr>
<tr><td><code id="message_wrap_+3A_width">width</code></td>
<td>
<p>An integer for the width.</p>
</td></tr>
<tr><td><code id="message_wrap_+3A_prefix">prefix</code></td>
<td>
<p>An optional string to go on the first line of the message.</p>
</td></tr>
<tr><td><code id="message_wrap_+3A_color_text">color_text</code>, <code id="message_wrap_+3A_color_prefix">color_prefix</code></td>
<td>
<p>A function (or <code>NULL</code>) that is used to color
the text and/or prefix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The processed text is returned (invisibly) but a message is written.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cli)
Gaiman &lt;-
  paste(
    '"Good point." Bod was pleased with himself, and glad he had thought of',
    "asking the poet for advice. Really, he thought, if you couldn't trust a",
    "poet to offer sensible advice, who could you trust?",
    collapse = ""
  )
message_wrap(Gaiman)
message_wrap(Gaiman, width = 20, prefix = "-")
message_wrap(Gaiman,
  width = 30, prefix = "-",
  color_text = cli::col_silver
)
message_wrap(Gaiman,
  width = 30, prefix = "-",
  color_text = cli::style_underline,
  color_prefix = cli::col_green
)
</code></pre>

<hr>
<h2 id='min_grid.model_spec'>Determine the minimum set of model fits</h2><span id='topic+min_grid.model_spec'></span><span id='topic+fit_max_value'></span><span id='topic+min_grid.boost_tree'></span><span id='topic+min_grid.linear_reg'></span><span id='topic+min_grid.logistic_reg'></span><span id='topic+min_grid.mars'></span><span id='topic+min_grid.multinom_reg'></span><span id='topic+min_grid.nearest_neighbor'></span><span id='topic+min_grid.cubist_rules'></span><span id='topic+min_grid.C5_rules'></span><span id='topic+min_grid.rule_fit'></span><span id='topic+min_grid.pls'></span><span id='topic+min_grid.poisson_reg'></span>

<h3>Description</h3>

<p><code>min_grid()</code> determines exactly what models should be fit in order to
evaluate the entire set of tuning parameter combinations. This is for
internal use only and the API may change in the near future.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_spec'
min_grid(x, grid, ...)

fit_max_value(x, grid, ...)

## S3 method for class 'boost_tree'
min_grid(x, grid, ...)

## S3 method for class 'linear_reg'
min_grid(x, grid, ...)

## S3 method for class 'logistic_reg'
min_grid(x, grid, ...)

## S3 method for class 'mars'
min_grid(x, grid, ...)

## S3 method for class 'multinom_reg'
min_grid(x, grid, ...)

## S3 method for class 'nearest_neighbor'
min_grid(x, grid, ...)

## S3 method for class 'cubist_rules'
min_grid(x, grid, ...)

## S3 method for class 'C5_rules'
min_grid(x, grid, ...)

## S3 method for class 'rule_fit'
min_grid(x, grid, ...)

## S3 method for class 'pls'
min_grid(x, grid, ...)

## S3 method for class 'poisson_reg'
min_grid(x, grid, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="min_grid.model_spec_+3A_x">x</code></td>
<td>
<p>A model specification.</p>
</td></tr>
<tr><td><code id="min_grid.model_spec_+3A_grid">grid</code></td>
<td>
<p>A tibble with tuning parameter combinations.</p>
</td></tr>
<tr><td><code id="min_grid.model_spec_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit_max_value()</code> can be used in other packages to implement a <code>min_grid()</code>
method.
</p>


<h3>Value</h3>

<p>A tibble with the minimum tuning parameters to fit and an additional
list column with the parameter combinations used for prediction.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(dials)
library(parsnip)

## -----------------------------------------------------------------------------
## No ability to exploit submodels:

svm_spec &lt;-
  svm_poly(cost = tune(), degree = tune()) %&gt;%
  set_engine("kernlab") %&gt;%
  set_mode("regression")

svm_grid &lt;-
  svm_spec %&gt;%
  extract_parameter_set_dials() %&gt;%
  grid_regular(levels = 3)

min_grid(svm_spec, svm_grid)

## -----------------------------------------------------------------------------
## Can use submodels

xgb_spec &lt;-
  boost_tree(trees = tune(), min_n = tune()) %&gt;%
  set_engine("xgboost") %&gt;%
  set_mode("regression")

xgb_grid &lt;-
  xgb_spec %&gt;%
  extract_parameter_set_dials() %&gt;%
  grid_regular(levels = 3)

min_grid(xgb_spec, xgb_grid)
</code></pre>

<hr>
<h2 id='outcome_names'>Determine names of the outcome data in a workflow</h2><span id='topic+outcome_names'></span><span id='topic+outcome_names.terms'></span><span id='topic+outcome_names.formula'></span><span id='topic+outcome_names.recipe'></span><span id='topic+outcome_names.workflow'></span><span id='topic+outcome_names.tune_results'></span>

<h3>Description</h3>

<p>Determine names of the outcome data in a workflow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outcome_names(x, ...)

## S3 method for class 'terms'
outcome_names(x, ...)

## S3 method for class 'formula'
outcome_names(x, ...)

## S3 method for class 'recipe'
outcome_names(x, ...)

## S3 method for class 'workflow'
outcome_names(x, ...)

## S3 method for class 'tune_results'
outcome_names(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outcome_names_+3A_x">x</code></td>
<td>
<p>An object.</p>
</td></tr>
<tr><td><code id="outcome_names_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string of variable names
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
lm(cbind(mpg, wt) ~ ., data = mtcars) %&gt;%
  purrr::pluck(terms) %&gt;%
  outcome_names()
</code></pre>

<hr>
<h2 id='parameters.workflow'>Determination of parameter sets for other objects</h2><span id='topic+parameters.workflow'></span><span id='topic+parameters.model_spec'></span><span id='topic+parameters.recipe'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>These methods have been deprecated in favor of <code><a href="#topic+extract_parameter_set_dials">extract_parameter_set_dials()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'workflow'
parameters(x, ...)

## S3 method for class 'model_spec'
parameters(x, ...)

## S3 method for class 'recipe'
parameters(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parameters.workflow_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
<tr><td><code id="parameters.workflow_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parameter set object
</p>

<hr>
<h2 id='prob_improve'>Acquisition function for scoring parameter combinations</h2><span id='topic+prob_improve'></span><span id='topic+exp_improve'></span><span id='topic+conf_bound'></span>

<h3>Description</h3>

<p>These functions can be used to score candidate tuning parameter combinations
as a function of their predicted mean and variation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prob_improve(trade_off = 0, eps = .Machine$double.eps)

exp_improve(trade_off = 0, eps = .Machine$double.eps)

conf_bound(kappa = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prob_improve_+3A_trade_off">trade_off</code></td>
<td>
<p>A number or function that describes the trade-off between
exploitation and exploration. Smaller values favor exploitation.</p>
</td></tr>
<tr><td><code id="prob_improve_+3A_eps">eps</code></td>
<td>
<p>A small constant to avoid division by zero.</p>
</td></tr>
<tr><td><code id="prob_improve_+3A_kappa">kappa</code></td>
<td>
<p>A positive number (or function) that corresponds to the
multiplier of the standard deviation in a confidence bound (e.g. 1.96 in
normal-theory 95 percent confidence intervals). Smaller values lean more
towards exploitation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The acquisition functions often combine the mean and variance
predictions from the Gaussian process model into an objective to be
optimized.
</p>
<p>For this documentation, we assume that the metric in question is better when
<em>maximized</em> (e.g. accuracy, the coefficient of determination, etc).
</p>
<p>The expected improvement of a point <code>x</code> is based on the predicted mean and
variation at that point as well as the current best value (denoted here as
<code>x_b</code>). The vignette linked below contains the formulas for this acquisition
function. When the <code>trade_off</code> parameter is greater than zero, the
acquisition function will down-play the effect of the <em>mean</em> prediction and
give more weight to the variation. This has the effect of searching for new
parameter combinations that are in areas that have yet to be sampled.
</p>
<p>Note that for <code><a href="#topic+exp_improve">exp_improve()</a></code> and <code><a href="#topic+prob_improve">prob_improve()</a></code>, the <code>trade_off</code> value is
in the units of the outcome. The functions are parameterized so that the
<code>trade_off</code> value should always be non-negative.
</p>
<p>The confidence bound function does not take into account the current best
results in the data.
</p>
<p>If a function is passed to  <code><a href="#topic+exp_improve">exp_improve()</a></code> or <code><a href="#topic+prob_improve">prob_improve()</a></code>, the function
can have multiple arguments but only the first (the current iteration number)
is given to the function. In other words, the function argument should have
defaults for all but the first argument. See <code><a href="#topic+expo_decay">expo_decay()</a></code> as an example of
a function.
</p>


<h3>Value</h3>

<p>An object of class <code>prob_improve</code>, <code>exp_improve</code>, or <code>conf_bounds</code>
along with an extra class of <code>acquisition_function</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tune_bayes">tune_bayes()</a></code>, <code><a href="#topic+expo_decay">expo_decay()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prob_improve()
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+dplyr_reconstruct'></span><span id='topic+parameters'></span><span id='topic+autoplot'></span><span id='topic+required_pkgs'></span><span id='topic+tune'></span><span id='topic+tunable'></span><span id='topic+tune_args'></span><span id='topic+min_grid'></span><span id='topic+augment'></span><span id='topic+.get_fingerprint'></span><span id='topic+int_pctl'></span><span id='topic+extract_spec_parsnip'></span><span id='topic+extract_recipe'></span><span id='topic+extract_fit_parsnip'></span><span id='topic+extract_fit_engine'></span><span id='topic+extract_mold'></span><span id='topic+extract_preprocessor'></span><span id='topic+extract_workflow'></span><span id='topic+extract_parameter_set_dials'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>dials</dt><dd><p><code><a href="dials.html#topic+parameters">parameters</a></code></p>
</dd>
<dt>dplyr</dt><dd><p><code><a href="dplyr.html#topic+dplyr_extending">dplyr_reconstruct</a></code></p>
</dd>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+min_grid">min_grid</a></code>, <code><a href="generics.html#topic+required_pkgs">required_pkgs</a></code>, <code><a href="generics.html#topic+tunable">tunable</a></code>, <code><a href="generics.html#topic+tune_args">tune_args</a></code></p>
</dd>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
<dt>hardhat</dt><dd><p><code><a href="hardhat.html#topic+hardhat-extract">extract_fit_engine</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_fit_parsnip</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_mold</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_parameter_set_dials</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_preprocessor</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_recipe</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_spec_parsnip</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_workflow</a></code>, <code><a href="hardhat.html#topic+tune">tune</a></code></p>
</dd>
<dt>rsample</dt><dd><p><code><a href="rsample.html#topic+get_fingerprint">.get_fingerprint</a></code>, <code><a href="rsample.html#topic+int_pctl">int_pctl</a></code></p>
</dd>
</dl>

<hr>
<h2 id='show_best'>Investigate best tuning parameters</h2><span id='topic+show_best'></span><span id='topic+show_best.default'></span><span id='topic+show_best.tune_results'></span><span id='topic+select_best'></span><span id='topic+select_best.default'></span><span id='topic+select_best.tune_results'></span><span id='topic+select_by_pct_loss'></span><span id='topic+select_by_pct_loss.default'></span><span id='topic+select_by_pct_loss.tune_results'></span><span id='topic+select_by_one_std_err'></span><span id='topic+select_by_one_std_err.default'></span><span id='topic+select_by_one_std_err.tune_results'></span>

<h3>Description</h3>

<p><code><a href="#topic+show_best">show_best()</a></code> displays the top sub-models and their performance estimates.
</p>
<p><code><a href="#topic+select_best">select_best()</a></code> finds the tuning parameter combination with the best
performance values.
</p>
<p><code><a href="#topic+select_by_one_std_err">select_by_one_std_err()</a></code> uses the &quot;one-standard error rule&quot; (Breiman _el
at, 1984) that selects the most simple model that is within one standard
error of the numerically optimal results.
</p>
<p><code><a href="#topic+select_by_pct_loss">select_by_pct_loss()</a></code> selects the most simple model whose loss of
performance is within some acceptable limit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_best(x, ...)

## Default S3 method:
show_best(x, ...)

## S3 method for class 'tune_results'
show_best(
  x,
  ...,
  metric = NULL,
  eval_time = NULL,
  n = 5,
  call = rlang::current_env()
)

select_best(x, ...)

## Default S3 method:
select_best(x, ...)

## S3 method for class 'tune_results'
select_best(x, ..., metric = NULL, eval_time = NULL)

select_by_pct_loss(x, ...)

## Default S3 method:
select_by_pct_loss(x, ...)

## S3 method for class 'tune_results'
select_by_pct_loss(x, ..., metric = NULL, eval_time = NULL, limit = 2)

select_by_one_std_err(x, ...)

## Default S3 method:
select_by_one_std_err(x, ...)

## S3 method for class 'tune_results'
select_by_one_std_err(x, ..., metric = NULL, eval_time = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_best_+3A_x">x</code></td>
<td>
<p>The results of <code><a href="#topic+tune_grid">tune_grid()</a></code> or <code><a href="#topic+tune_bayes">tune_bayes()</a></code>.</p>
</td></tr>
<tr><td><code id="show_best_+3A_...">...</code></td>
<td>
<p>For <code><a href="#topic+select_by_one_std_err">select_by_one_std_err()</a></code> and <code><a href="#topic+select_by_pct_loss">select_by_pct_loss()</a></code>, this
argument is passed directly to <code><a href="dplyr.html#topic+arrange">dplyr::arrange()</a></code> so that the user can sort
the models from <em>most simple to most complex</em>. That is, for a parameter <code>p</code>,
pass the unquoted expression <code>p</code> if smaller values of <code>p</code> indicate a simpler
model, or <code>desc(p)</code> if larger values indicate a simpler model. At
least one term is required for these two functions. See the examples below.</p>
</td></tr>
<tr><td><code id="show_best_+3A_metric">metric</code></td>
<td>
<p>A character value for the metric that will be used to sort
the models. (See
<a href="https://yardstick.tidymodels.org/articles/metric-types.html">https://yardstick.tidymodels.org/articles/metric-types.html</a> for
more details). Not required if a single metric exists in <code>x</code>. If there are
multiple metric and none are given, the first in the metric set is used (and
a warning is issued).</p>
</td></tr>
<tr><td><code id="show_best_+3A_eval_time">eval_time</code></td>
<td>
<p>A single numeric time point where dynamic event time
metrics should be chosen (e.g., the time-dependent ROC curve, etc). The
values should be consistent with the values used to create <code>x</code>. The <code>NULL</code>
default will automatically use the first evaluation time used by <code>x</code>.</p>
</td></tr>
<tr><td><code id="show_best_+3A_n">n</code></td>
<td>
<p>An integer for the number of top results/rows to return.</p>
</td></tr>
<tr><td><code id="show_best_+3A_call">call</code></td>
<td>
<p>The call to be shown in errors and warnings.</p>
</td></tr>
<tr><td><code id="show_best_+3A_limit">limit</code></td>
<td>
<p>The limit of loss of performance that is acceptable (in percent
units). See details below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For percent loss, suppose the best model has an RMSE of 0.75 and a simpler
model has an RMSE of 1. The percent loss would be <code>(1.00 - 0.75)/1.00 * 100</code>,
or 25 percent. Note that loss will always be non-negative.
</p>


<h3>Value</h3>

<p>A tibble with columns for the parameters. <code><a href="#topic+show_best">show_best()</a></code> also
includes columns for performance metrics.
</p>


<h3>References</h3>

<p>Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984).
<em>Classification and Regression Trees.</em> Monterey, CA: Wadsworth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("example_ames_knn")

show_best(ames_iter_search, metric = "rmse")

select_best(ames_iter_search, metric = "rsq")

# To find the least complex model within one std error of the numerically
# optimal model, the number of nearest neighbors are sorted from the largest
# number of neighbors (the least complex class boundary) to the smallest
# (corresponding to the most complex model).

select_by_one_std_err(ames_grid_search, metric = "rmse", desc(K))

# Now find the least complex model that has no more than a 5% loss of RMSE:
select_by_pct_loss(
  ames_grid_search,
  metric = "rmse",
  limit = 5, desc(K)
)

</code></pre>

<hr>
<h2 id='show_notes'>Display distinct errors from tune objects</h2><span id='topic+show_notes'></span>

<h3>Description</h3>

<p>Display distinct errors from tune objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_notes(x, n = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_notes_+3A_x">x</code></td>
<td>
<p>An object of class <code>tune_results</code>.</p>
</td></tr>
<tr><td><code id="show_notes_+3A_n">n</code></td>
<td>
<p>An integer for how many unique notes to show.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, <code>x</code>. Function is called for side-effects and printing.
</p>

<hr>
<h2 id='tune_bayes'>Bayesian optimization of model parameters.</h2><span id='topic+tune_bayes'></span><span id='topic+tune_bayes.model_spec'></span><span id='topic+tune_bayes.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+tune_bayes">tune_bayes()</a></code> uses models to generate new candidate tuning parameter
combinations based on previous results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_bayes(object, ...)

## S3 method for class 'model_spec'
tune_bayes(
  object,
  preprocessor,
  resamples,
  ...,
  iter = 10,
  param_info = NULL,
  metrics = NULL,
  eval_time = NULL,
  objective = exp_improve(),
  initial = 5,
  control = control_bayes()
)

## S3 method for class 'workflow'
tune_bayes(
  object,
  resamples,
  ...,
  iter = 10,
  param_info = NULL,
  metrics = NULL,
  eval_time = NULL,
  objective = exp_improve(),
  initial = 5,
  control = control_bayes()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_bayes_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or an unfitted
<a href="workflows.html#topic+workflow">workflow()</a>. No tuning parameters are allowed; if arguments
have been marked with <a href="hardhat.html#topic+tune">tune()</a>, their values must be
<a href="#topic+finalize_model">finalized</a>.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="GPfit.html#topic+GP_fit">GPfit::GP_fit()</a></code> (mostly for the <code>corr</code> argument).</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset</code> resampling object created from an <code>rsample</code>
function, such as <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_iter">iter</code></td>
<td>
<p>The maximum number of search iterations.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_param_info">param_info</code></td>
<td>
<p>A <code><a href="dials.html#topic+parameters">dials::parameters()</a></code> object or <code>NULL</code>. If none is given,
a parameters set is derived from other arguments. Passing this argument can
be useful when parameter ranges need to be customized.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>, or <code>NULL</code> to compute a standard
set of metrics. The first metric in <code>metrics</code> is the one that will be optimized.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_objective">objective</code></td>
<td>
<p>A character string for what metric should be optimized or
an acquisition function object.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_initial">initial</code></td>
<td>
<p>An initial set of results in a tidy format (as would result
from <code><a href="#topic+tune_grid">tune_grid()</a></code>) or a positive integer. It is suggested that the number of
initial results be greater than the number of parameters being optimized.</p>
</td></tr>
<tr><td><code id="tune_bayes_+3A_control">control</code></td>
<td>
<p>A control object created by <code><a href="#topic+control_bayes">control_bayes()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optimization starts with a set of initial results, such as those
generated by <code><a href="#topic+tune_grid">tune_grid()</a></code>. If none exist, the function will create several
combinations and obtain their performance estimates.
</p>
<p>Using one of the performance estimates as the <em>model outcome</em>, a Gaussian
process (GP) model is created where the previous tuning parameter combinations
are used as the predictors.
</p>
<p>A large grid of potential hyperparameter combinations is predicted using
the model and scored using an <em>acquisition function</em>. These functions
usually combine the predicted mean and variance of the GP to decide the best
parameter combination to try next. For more information, see the
documentation for <code><a href="#topic+exp_improve">exp_improve()</a></code> and the corresponding package vignette.
</p>
<p>The best combination is evaluated using resampling and the process continues.
</p>


<h3>Value</h3>

<p>A tibble of results that mirror those generated by <code><a href="#topic+tune_grid">tune_grid()</a></code>.
However, these results contain an <code>.iter</code> column and replicate the <code>rset</code>
object multiple times over iterations (at limited additional memory costs).
</p>


<h3>Parallel Processing</h3>

<p>tune supports parallel processing with the <span class="pkg">future</span> package. To execute
the resampling iterations in parallel, specify a <a href="future.html#topic+plan">plan</a> with
future first. The <code>allow_par</code> argument can be used to avoid parallelism.
</p>
<p>For the most part, warnings generated during training are shown as they occur
and are associated with a specific resample when
<code>control_bayes(verbose = TRUE)</code>. They are (usually) not aggregated until the
end of processing.
</p>
<p>For Bayesian optimization, parallel processing is used to estimate the
resampled performance values once a new candidate set of values are estimated.
</p>


<h3>Initial Values</h3>

<p>The results of <code><a href="#topic+tune_grid">tune_grid()</a></code>, or a previous run of <code><a href="#topic+tune_bayes">tune_bayes()</a></code> can be used
in the <code>initial</code> argument. <code>initial</code> can also be a positive integer. In this
case, a space-filling design will be used to populate a preliminary set of
results. For good results, the number of initial values should be more than
the number of parameters being optimized.
</p>


<h3>Parameter Ranges and Values</h3>

<p>In some cases, the tuning parameter values depend on the dimensions of the
data (they are said to contain <a href="dials.html#topic+unknown">unknown</a> values). For
example, <code>mtry</code> in random forest models depends on the number of predictors.
In such cases, the unknowns in the tuning parameter object must be determined
beforehand and passed to the function via the <code>param_info</code> argument.
<code><a href="dials.html#topic+finalize">dials::finalize()</a></code> can be used to derive the data-dependent parameters.
Otherwise, a parameter set can be created via <code><a href="dials.html#topic+parameters">dials::parameters()</a></code>, and the
<code>dials</code> <code>update()</code> function can be used to specify the ranges or values.
</p>


<h3>Performance Metrics</h3>

<p>To use your own performance metrics, the <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> function
can be used to pick what should be measured for each model. If multiple
metrics are desired, they can be bundled. For example, to estimate the area
under the ROC curve as well as the sensitivity and specificity (under the
typical probability cutoff of 0.50), the <code>metrics</code> argument could be given:
</p>
<pre>
  metrics = metric_set(roc_auc, sens, spec)
</pre>
<p>Each metric is calculated for each candidate model.
</p>
<p>If no metric set is provided, one is created:
</p>

<ul>
<li><p> For regression models, the root mean squared error and coefficient
of determination are computed.
</p>
</li>
<li><p> For classification, the area under the ROC curve and overall accuracy
are computed.
</p>
</li></ul>

<p>Note that the metrics also determine what type of predictions are estimated
during tuning. For example, in a classification problem, if metrics are used
that are all associated with hard class predictions, the classification
probabilities are not created.
</p>
<p>The out-of-sample estimates of these metrics are contained in a list column
called <code>.metrics</code>. This tibble contains a row for each metric and columns
for the value, the estimator type, and so on.
</p>
<p><code><a href="#topic+collect_metrics">collect_metrics()</a></code> can be used for these objects to collapse the results
over the resampled (to obtain the final resampling estimates per tuning
parameter combination).
</p>


<h3>Obtaining Predictions</h3>

<p>When <code>control_bayes(save_pred = TRUE)</code>, the output tibble contains a list
column called <code>.predictions</code> that has the out-of-sample predictions for each
parameter combination in the grid and each fold (which can be very large).
</p>
<p>The elements of the tibble are tibbles with columns for the tuning
parameters, the row number from the original data object (<code>.row</code>), the
outcome data (with the same name(s) of the original data), and any columns
created by the predictions. For example, for simple regression problems, this
function generates a column called <code>.pred</code> and so on. As noted above, the
prediction columns that are returned are determined by the type of metric(s)
requested.
</p>
<p>This list column can be <code>unnested</code> using <code><a href="tidyr.html#topic+unnest">tidyr::unnest()</a></code> or using the
convenience function <code><a href="#topic+collect_predictions">collect_predictions()</a></code>.
</p>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code><a href="parsnip.html#topic+case_weights_allowed">parsnip::case_weights_allowed()</a></code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code><a href="hardhat.html#topic+importance_weights">hardhat:: importance_weights()</a></code> or
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.
</p>
<p>For functions such as <code><a href="#topic+fit_resamples">fit_resamples()</a></code> and the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, the
model must be contained inside of a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>. To declare that
case weights are used, invoke <code><a href="workflows.html#topic+add_case_weights">workflows::add_case_weights()</a></code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>Censored Regression Models</h3>

<p>Three types of metrics can be used to assess the quality of censored
regression models:
</p>

<ul>
<li><p> static: the prediction is independent of time.
</p>
</li>
<li><p> dynamic: the prediction is a time-specific probability (e.g., survival
probability) and is measured at one or more particular times.
</p>
</li>
<li><p> integrated: same as the dynamic metric but returns the integral of the
different metrics from each time point.
</p>
</li></ul>

<p>Which metrics are chosen by the user affects how many evaluation times
should be specified. For example:
</p>
<div class="sourceCode"><pre># Needs no `eval_time` value
metric_set(concordance_survival)

# Needs at least one `eval_time`
metric_set(brier_survival)
metric_set(brier_survival, concordance_survival)

# Needs at least two eval_time` values
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival, brier_survival)
</pre></div>
<p>Values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>


<h3>Optimizing Censored Regression Models</h3>

<p>With dynamic performance metrics (e.g. Brier or ROC curves), performance is
calculated for every value of <code>eval_time</code> but the <em>first</em> evaluation time
given by the user (e.g., <code>eval_time[1]</code>) is used to guide the optimization.
</p>


<h3>Extracting Information</h3>

<p>The <code>extract</code> control option will result in an additional function to be
returned called <code>.extracts</code>. This is a list column that has tibbles
containing the results of the user's function for each tuning parameter
combination. This can enable returning each model and/or recipe object that
is created during resampling. Note that this could result in a large return
object, depending on what is returned.
</p>
<p>The control function contains an option (<code>extract</code>) that can be used to
retain any model or recipe that was created within the resamples. This
argument should be a function with a single argument. The value of the
argument that is given to the function in each resample is a workflow
object (see <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> for more information). Several
helper functions can be used to easily pull out the preprocessing
and/or model information from the workflow, such as
<code><a href="workflows.html#topic+extract-workflow">extract_preprocessor()</a></code> and
<code><a href="workflows.html#topic+extract-workflow">extract_fit_parsnip()</a></code>.
</p>
<p>As an example, if there is interest in getting each parsnip model fit back,
one could use:
</p>
<pre>
  extract = function (x) extract_fit_parsnip(x)
</pre>
<p>Note that the function given to the <code>extract</code> argument is evaluated on
every model that is <em>fit</em> (as opposed to every model that is <em>evaluated</em>).
As noted above, in some cases, model predictions can be derived for
sub-models so that, in these cases, not every row in the tuning parameter
grid has a separate R object associated with it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+control_bayes">control_bayes()</a></code>, <code><a href="#topic+tune">tune()</a></code>, <code><a href="#topic+autoplot.tune_results">autoplot.tune_results()</a></code>,
<code><a href="#topic+show_best">show_best()</a></code>, <code><a href="#topic+select_best">select_best()</a></code>, <code><a href="#topic+collect_predictions">collect_predictions()</a></code>,
<code><a href="#topic+collect_metrics">collect_metrics()</a></code>, <code><a href="#topic+prob_improve">prob_improve()</a></code>, <code><a href="#topic+exp_improve">exp_improve()</a></code>, <code><a href="#topic+conf_bound">conf_bound()</a></code>,
<code><a href="#topic+fit_resamples">fit_resamples()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(rsample)
library(parsnip)

# define resamples and minimal recipe on mtcars
set.seed(6735)
folds &lt;- vfold_cv(mtcars, v = 5)

car_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_normalize(all_predictors())

# define an svm with parameters to tune
svm_mod &lt;-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%
  set_engine("kernlab") %&gt;%
  set_mode("regression")

# use a space-filling design with 6 points
set.seed(3254)
svm_grid &lt;- tune_grid(svm_mod, car_rec, folds, grid = 6)

show_best(svm_grid, metric = "rmse")

# use bayesian optimization to evaluate at 6 more points
set.seed(8241)
svm_bayes &lt;- tune_bayes(svm_mod, car_rec, folds, initial = svm_grid, iter = 6)

# note that bayesian optimization evaluated parameterizations
# similar to those that previously decreased rmse in svm_grid
show_best(svm_bayes, metric = "rmse")

# specifying `initial` as a numeric rather than previous tuning results
# will result in `tune_bayes` initially evaluating an space-filling
# grid using `tune_grid` with `grid = initial`
set.seed(0239)
svm_init &lt;- tune_bayes(svm_mod, car_rec, folds, initial = 6, iter = 6)

show_best(svm_init, metric = "rmse")

</code></pre>

<hr>
<h2 id='tune_grid'>Model tuning via grid search</h2><span id='topic+tune_grid'></span><span id='topic+tune_grid.model_spec'></span><span id='topic+tune_grid.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+tune_grid">tune_grid()</a></code> computes a set of performance metrics (e.g. accuracy or RMSE)
for a pre-defined set of tuning parameters that correspond to a model or
recipe across one or more resamples of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_grid(object, ...)

## S3 method for class 'model_spec'
tune_grid(
  object,
  preprocessor,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_grid()
)

## S3 method for class 'workflow'
tune_grid(
  object,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_grid()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_grid_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or an unfitted
<a href="workflows.html#topic+workflow">workflow()</a>. No tuning parameters are allowed; if arguments
have been marked with <a href="hardhat.html#topic+tune">tune()</a>, their values must be
<a href="#topic+finalize_model">finalized</a>.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset</code> resampling object created from an <code>rsample</code>
function, such as <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_param_info">param_info</code></td>
<td>
<p>A <code><a href="dials.html#topic+parameters">dials::parameters()</a></code> object or <code>NULL</code>. If none is given,
a parameters set is derived from other arguments. Passing this argument can
be useful when parameter ranges need to be customized.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_grid">grid</code></td>
<td>
<p>A data frame of tuning combinations or a positive integer. The
data frame should have columns for each parameter being tuned and rows for
tuning parameter candidates. An integer denotes the number of candidate
parameter sets to be created automatically.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>, or <code>NULL</code> to compute a standard
set of metrics.</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="tune_grid_+3A_control">control</code></td>
<td>
<p>An object used to modify the tuning process, likely created
by <code><a href="#topic+control_grid">control_grid()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose there are <em>m</em> tuning parameter combinations. <code><a href="#topic+tune_grid">tune_grid()</a></code> may not
require all <em>m</em> model/recipe fits across each resample. For example:
</p>

<ul>
<li><p> In cases where a single model fit can be used to make predictions
for different parameter values in the grid, only one fit is used.
For example, for some boosted trees, if 100 iterations of boosting
are requested, the model object for 100 iterations can be used to
make predictions on iterations less than 100 (if all other
parameters are equal).
</p>
</li>
<li><p> When the model is being tuned in conjunction with pre-processing
and/or post-processing parameters, the minimum number of fits are
used. For example, if the number of PCA components in a recipe step
are being tuned over three values (along with model tuning
parameters), only three recipes are trained. The alternative
would be to re-train the same recipe multiple times for each model
tuning parameter.
</p>
</li></ul>

<p>tune supports parallel processing with the <span class="pkg">future</span> package. To execute
the resampling iterations in parallel, specify a <a href="future.html#topic+plan">plan</a> with
future first. The <code>allow_par</code> argument can be used to avoid parallelism.
</p>
<p>For the most part, warnings generated during training are shown as they occur
and are associated with a specific resample when
<code>control_grid(verbose = TRUE)</code>. They are (usually) not aggregated until the
end of processing.
</p>


<h3>Value</h3>

<p>An updated version of <code>resamples</code> with extra list columns for <code>.metrics</code> and
<code>.notes</code> (optional columns are <code>.predictions</code> and <code>.extracts</code>). <code>.notes</code>
contains warnings and errors that occur during execution.
</p>


<h3>Parameter Grids</h3>

<p>If no tuning grid is provided, a semi-random grid (via
<code><a href="dials.html#topic+grid_max_entropy">dials::grid_latin_hypercube()</a></code>) is created with 10 candidate parameter
combinations.
</p>
<p>When provided, the grid should have column names for each parameter and
these should be named by the parameter name or <code>id</code>. For example, if a
parameter is marked for optimization using <code>penalty = tune()</code>, there should
be a column named <code>penalty</code>. If the optional identifier is used, such as
<code>penalty = tune(id = 'lambda')</code>, then the corresponding column name should
be <code>lambda</code>.
</p>
<p>In some cases, the tuning parameter values depend on the dimensions of the
data. For example, <code>mtry</code> in random forest models depends on the number of
predictors. In this case, the default tuning parameter object requires an
upper range. <code><a href="dials.html#topic+finalize">dials::finalize()</a></code> can be used to derive the data-dependent
parameters. Otherwise, a parameter set can be created (via
<code><a href="dials.html#topic+parameters">dials::parameters()</a></code>) and the <code>dials</code> <code>update()</code> function can be used to
change the values. This updated parameter set can be passed to the function
via the <code>param_info</code> argument.
</p>


<h3>Performance Metrics</h3>

<p>To use your own performance metrics, the <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> function
can be used to pick what should be measured for each model. If multiple
metrics are desired, they can be bundled. For example, to estimate the area
under the ROC curve as well as the sensitivity and specificity (under the
typical probability cutoff of 0.50), the <code>metrics</code> argument could be given:
</p>
<pre>
  metrics = metric_set(roc_auc, sens, spec)
</pre>
<p>Each metric is calculated for each candidate model.
</p>
<p>If no metric set is provided, one is created:
</p>

<ul>
<li><p> For regression models, the root mean squared error and coefficient
of determination are computed.
</p>
</li>
<li><p> For classification, the area under the ROC curve and overall accuracy
are computed.
</p>
</li></ul>

<p>Note that the metrics also determine what type of predictions are estimated
during tuning. For example, in a classification problem, if metrics are used
that are all associated with hard class predictions, the classification
probabilities are not created.
</p>
<p>The out-of-sample estimates of these metrics are contained in a list column
called <code>.metrics</code>. This tibble contains a row for each metric and columns
for the value, the estimator type, and so on.
</p>
<p><code><a href="#topic+collect_metrics">collect_metrics()</a></code> can be used for these objects to collapse the results
over the resampled (to obtain the final resampling estimates per tuning
parameter combination).
</p>


<h3>Obtaining Predictions</h3>

<p>When <code>control_grid(save_pred = TRUE)</code>, the output tibble contains a list
column called <code>.predictions</code> that has the out-of-sample predictions for each
parameter combination in the grid and each fold (which can be very large).
</p>
<p>The elements of the tibble are tibbles with columns for the tuning
parameters, the row number from the original data object (<code>.row</code>), the
outcome data (with the same name(s) of the original data), and any columns
created by the predictions. For example, for simple regression problems, this
function generates a column called <code>.pred</code> and so on. As noted above, the
prediction columns that are returned are determined by the type of metric(s)
requested.
</p>
<p>This list column can be <code>unnested</code> using <code><a href="tidyr.html#topic+unnest">tidyr::unnest()</a></code> or using the
convenience function <code><a href="#topic+collect_predictions">collect_predictions()</a></code>.
</p>


<h3>Extracting Information</h3>

<p>The <code>extract</code> control option will result in an additional function to be
returned called <code>.extracts</code>. This is a list column that has tibbles
containing the results of the user's function for each tuning parameter
combination. This can enable returning each model and/or recipe object that
is created during resampling. Note that this could result in a large return
object, depending on what is returned.
</p>
<p>The control function contains an option (<code>extract</code>) that can be used to
retain any model or recipe that was created within the resamples. This
argument should be a function with a single argument. The value of the
argument that is given to the function in each resample is a workflow
object (see <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> for more information). Several
helper functions can be used to easily pull out the preprocessing
and/or model information from the workflow, such as
<code><a href="workflows.html#topic+extract-workflow">extract_preprocessor()</a></code> and
<code><a href="workflows.html#topic+extract-workflow">extract_fit_parsnip()</a></code>.
</p>
<p>As an example, if there is interest in getting each parsnip model fit back,
one could use:
</p>
<pre>
  extract = function (x) extract_fit_parsnip(x)
</pre>
<p>Note that the function given to the <code>extract</code> argument is evaluated on
every model that is <em>fit</em> (as opposed to every model that is <em>evaluated</em>).
As noted above, in some cases, model predictions can be derived for
sub-models so that, in these cases, not every row in the tuning parameter
grid has a separate R object associated with it.
</p>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code><a href="parsnip.html#topic+case_weights_allowed">parsnip::case_weights_allowed()</a></code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code><a href="hardhat.html#topic+importance_weights">hardhat:: importance_weights()</a></code> or
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.
</p>
<p>For functions such as <code><a href="#topic+fit_resamples">fit_resamples()</a></code> and the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, the
model must be contained inside of a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>. To declare that
case weights are used, invoke <code><a href="workflows.html#topic+add_case_weights">workflows::add_case_weights()</a></code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>Censored Regression Models</h3>

<p>Three types of metrics can be used to assess the quality of censored
regression models:
</p>

<ul>
<li><p> static: the prediction is independent of time.
</p>
</li>
<li><p> dynamic: the prediction is a time-specific probability (e.g., survival
probability) and is measured at one or more particular times.
</p>
</li>
<li><p> integrated: same as the dynamic metric but returns the integral of the
different metrics from each time point.
</p>
</li></ul>

<p>Which metrics are chosen by the user affects how many evaluation times
should be specified. For example:
</p>
<div class="sourceCode"><pre># Needs no `eval_time` value
metric_set(concordance_survival)

# Needs at least one `eval_time`
metric_set(brier_survival)
metric_set(brier_survival, concordance_survival)

# Needs at least two eval_time` values
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival, brier_survival)
</pre></div>
<p>Values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+control_grid">control_grid()</a></code>, <code><a href="#topic+tune">tune()</a></code>, <code><a href="#topic+fit_resamples">fit_resamples()</a></code>,
<code><a href="#topic+autoplot.tune_results">autoplot.tune_results()</a></code>, <code><a href="#topic+show_best">show_best()</a></code>, <code><a href="#topic+select_best">select_best()</a></code>,
<code><a href="#topic+collect_predictions">collect_predictions()</a></code>, <code><a href="#topic+collect_metrics">collect_metrics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(ggplot2)

# ---------------------------------------------------------------------------

set.seed(6735)
folds &lt;- vfold_cv(mtcars, v = 5)

# ---------------------------------------------------------------------------

# tuning recipe parameters:

spline_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp, deg_free = tune("disp")) %&gt;%
  step_ns(wt, deg_free = tune("wt"))

lin_mod &lt;-
  linear_reg() %&gt;%
  set_engine("lm")

# manually create a grid
spline_grid &lt;- expand.grid(disp = 2:5, wt = 2:5)

# Warnings will occur from making spline terms on the holdout data that are
# extrapolations.
spline_res &lt;-
  tune_grid(lin_mod, spline_rec, resamples = folds, grid = spline_grid)
spline_res


show_best(spline_res, metric = "rmse")

# ---------------------------------------------------------------------------

# tune model parameters only (example requires the `kernlab` package)

car_rec &lt;-
  recipe(mpg ~ ., data = mtcars) %&gt;%
  step_normalize(all_predictors())

svm_mod &lt;-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%
  set_engine("kernlab") %&gt;%
  set_mode("regression")

# Use a space-filling design with 7 points
set.seed(3254)
svm_res &lt;- tune_grid(svm_mod, car_rec, resamples = folds, grid = 7)
svm_res

show_best(svm_res, metric = "rmse")

autoplot(svm_res, metric = "rmse") +
  scale_x_log10()

# ---------------------------------------------------------------------------

# Using a variables preprocessor with a workflow

# Rather than supplying a preprocessor (like a recipe) and a model directly
# to `tune_grid()`, you can also wrap them up in a workflow and pass
# that along instead (note that this doesn't do any preprocessing to
# the variables, it passes them along as-is).
wf &lt;- workflow() %&gt;%
  add_variables(outcomes = mpg, predictors = everything()) %&gt;%
  add_model(svm_mod)

set.seed(3254)
svm_res_wf &lt;- tune_grid(wf, resamples = folds, grid = 7)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
