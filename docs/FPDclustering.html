<!DOCTYPE html><html><head><title>Help for package FPDclustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FPDclustering}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ais'><p>Australian institute of sport data</p></a></li>
<li><a href='#asymmetric20'><p>Asymmetric data set shape 20</p></a></li>
<li><a href='#asymmetric3'><p>Asymmetric data set shape 3</p></a></li>
<li><a href='#Country_data'><p>Unsupervised Learning on Country Data</p></a></li>
<li><a href='#FPDC'><p>Factor probabilistic distance clustering</p></a></li>
<li><a href='#GPDC'><p>Gaussian PD-Clustering</p></a></li>
<li><a href='#outliers'><p>Data set with outliers</p></a></li>
<li><a href='#PDC'><p>Probabilistic Distance Clustering</p></a></li>
<li><a href='#PDQ'><p>Probabilistic Distance Clustering Adjusted for Cluster Size</p></a></li>
<li><a href='#plot.FPDclustering'>
<p>Plots for FPDclusteringt Objects</p></a></li>
<li><a href='#Silh'><p>Probabilistic silhouette plot</p></a></li>
<li><a href='#Star'><p>Star dataset to predict star types</p></a></li>
<li><a href='#Students'><p>Statistics 1 students</p></a></li>
<li><a href='#summary.FPDclustering'>
<p>Summary for FPDclusteringt Objects</p></a></li>
<li><a href='#TPDC'><p>Student-t PD-Clustering</p></a></li>
<li><a href='#TuckerFactors'><p>Choice of the number of Tucker 3 factors for FPDC</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>PD-Clustering and Related Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Cristina Tortora [aut, cre, cph], Noe Vidales [aut], Francesco Palumbo [aut], Tina Kalra [aut], and Paul D. McNicholas [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Cristina Tortora &lt;grikris1@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Probabilistic distance clustering (PD-clustering) is an iterative, distribution free, probabilistic clustering method. PD-clustering assigns units to a cluster according to their probability of membership, under the constraint that the product of the probability and the distance of each point to any cluster centre is a constant. PD-clustering is a flexible method that can be used with non-spherical clusters, outliers, or noisy data. PDQ is an extension of the algorithm for clusters of different size. GPDC and TPDC uses a dissimilarity measure based on densities. Factor PD-clustering (FPDC) is a factor clustering method that involves a linear transformation of variables and a cluster optimizing the PD-clustering criterion. It works on high dimensional data sets.</td>
</tr>
<tr>
<td>Depends:</td>
<td>ThreeWay ,mvtnorm,R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ExPosition,cluster,rootSolve, MASS, klaR, GGally, ggplot2,
ggeasy</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-29 23:41:44 UTC; 011543324</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-30 00:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='ais'>Australian institute of sport data
</h2><span id='topic+ais'></span>

<h3>Description</h3>

<p>Data obtained to study sex, sport and body-size dependency of hematology in highly trained athletes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ais)</code></pre>


<h3>Format</h3>

<p>A data frame with 202 observations and 13 variables.
</p>

<dl>
<dt>rcc</dt><dd><p> red blood cell count, in</p>
</dd>
<dt>wcc</dt><dd><p> while blood cell count, in per liter</p>
</dd>
<dt>hc</dt><dd><p> hematocrit, percent</p>
</dd>
<dt>hg</dt><dd><p> hemaglobin concentration, in g per decaliter</p>
</dd>
<dt>ferr</dt><dd><p> plasma ferritins, ng</p>
</dd>
<dt>bmi</dt><dd><p> Body mass index, kg</p>
</dd>
<dt>ssf</dt><dd><p> sum of skin folds</p>
</dd>
<dt>pcBfat</dt><dd><p> percent Body fat</p>
</dd>
<dt>lbm</dt><dd><p> lean body mass, kg</p>
</dd>
<dt>ht</dt><dd><p> height, cm</p>
</dd>
<dt>wt</dt><dd><p> weight, kg</p>
</dd>
<dt>sex</dt><dd><p> a factor with levels f m</p>
</dd>
<dt>sport</dt><dd><p> a factor with levels B_Ball Field Gym Netball Row Swim T_400m T_Sprnt Tennis W_Polo</p>
</dd>
</dl>


<h3>Source</h3>

<p>R package DAAG
</p>


<h3>References</h3>

<p>Telford, R.D. and Cunningham, R.B. 1991. Sex, sport and body-size dependency of hematology in highly trained athletes. Medicine and Science in Sports and Exercise 23: 788-794.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
pairs(ais[,1:11],col=ais$sex)
</code></pre>

<hr>
<h2 id='asymmetric20'>Asymmetric data set shape 20
</h2><span id='topic+asymmetric20'></span>

<h3>Description</h3>

<p>Each cluster has been generated according to a multivariate asymmetric Gaussian distribution, with shape 20, covariance matrix equal to the identity matrix and randomly generated centres.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(asymmetric20)</code></pre>


<h3>Format</h3>

<p>A data frame with 800 observations on the following 101 variables.
The first variable is the membership.
</p>


<h3>Source</h3>

<p>Generated with R using the package sn (The skew-normal and skew-t distributions), function rsn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(asymmetric20)
plot(asymmetric20[,2:3])
</code></pre>

<hr>
<h2 id='asymmetric3'>Asymmetric data set shape 3
</h2><span id='topic+asymmetric3'></span>

<h3>Description</h3>

<p>Each cluster has been generated according to a multivariate asymmetric Gaussian distribution, with shape 3, covariance matrix equal to the identity matrix and randomly generated centres.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(asymmetric3)</code></pre>


<h3>Format</h3>

<p>A data frame with 800 observations on 101 variables.
The first variable is the membership labels.
</p>


<h3>Source</h3>

<p>Generated with R using the package sn (The skew-normal and skew-t distributions), function rsn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(asymmetric3)
plot(asymmetric3[,2:3])
</code></pre>

<hr>
<h2 id='Country_data'>Unsupervised Learning on Country Data
</h2><span id='topic+Country_data'></span>

<h3>Description</h3>

<p>Ten vables recorded on 167 countries. The goal is to categorize the countries using socio-economic and health indicators that determine the country's overall development. The data set has been donated by the HELP International organization, an international humanitarian NGO that needs to
identify the countries that need aid and asked the analysts to categorize the countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Country_data)</code></pre>


<h3>Format</h3>

<p>A data frame with 167 observations and 10 variables.
</p>

<dl>
<dt>country</dt><dd><p>country name</p>
</dd>
<dt>child_mort</dt><dd><p>Death of children under 5 years of age per 1000 live births</p>
</dd>
<dt>exports</dt><dd><p> Exports of goods and services per capita. Given as %age of the GDP per capita</p>
</dd>
<dt>health</dt><dd><p> Total health spending per capita. Given as %age of GDP per capita</p>
</dd>
<dt>imports</dt><dd><p> Imports of goods and services per capita. Given as %age of the GDP per capita</p>
</dd>
<dt>income</dt><dd><p> Net income per person</p>
</dd>
<dt>inflation</dt><dd><p>The measurement of the annual growth rate of the Total GDP</p>
</dd>
<dt>life_expec</dt><dd><p>The average number of years a new born child would live if the current mortality patterns are to remain the same</p>
</dd>
<dt>total_fer</dt><dd><p> The number of children that would be born to each woman if the current age-fertility rates remain the same.</p>
</dd>
<dt>gdpp</dt><dd><p> The GDP per capita. Calculated as the Total GDP divided by the total population.</p>
</dd>
</dl>


<h3>Source</h3>

<p>https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data/metadata?resource=download
</p>


<h3>References</h3>

<p>R. Kokkula. Unsupervised 
learning on country data. kaggle, 2022. URL
https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data/metadata?resource=download
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Country_data)
pairs(Country_data[,2:10])
</code></pre>

<hr>
<h2 id='FPDC'>Factor probabilistic distance clustering
</h2><span id='topic+FPDC'></span>

<h3>Description</h3>

<p> An implementation of FPDC, a probabilistic factor clustering algorithm that involves a linear transformation of variables and a cluster optimizing the PD-clustering criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FPDC(data = NULL, k = 2, nf = 2, nu = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FPDC_+3A_data">data</code></td>
<td>
<p> A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="FPDC_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters
</p>
</td></tr>
<tr><td><code id="FPDC_+3A_nf">nf</code></td>
<td>
<p>A numerical parameter giving the number of factors for variables
</p>
</td></tr>
<tr><td><code id="FPDC_+3A_nu">nu</code></td>
<td>
<p>A numerical parameter giving the number of factors for units
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A class FPDclustering list with components
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>A vector of integers indicating the cluster membership for each unit</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix of probability of each point belonging to each cluster</p>
</td></tr>
<tr><td><code>JDF</code></td>
<td>
<p>The value of the Joint distance function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code>explained</code></td>
<td>
<p>The explained variability</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora and Paul D. McNicholas
</p>


<h3>References</h3>

<p>Tortora, C., M. Gettler Summa, M. Marino, and F. Palumbo. <em>Factor probabilistic distance clustering
(fpdc): a new clustering method for high dimensional data sets</em>. Advanced in Data Analysis and Classification,  10(4), 441-464, 2016. doi:10.1007/s11634-015-0219-5.
</p>
<p>Tortora C.,  Gettler Summa M., and  Palumbo F..
Factor pd-clustering. In Lausen et al., editor, <em>Algorithms from and for Nature and Life, Studies in Classification</em>, Data Analysis, and Knowledge Organization DOI 10.1007/978-3-319-00035-011, 115-123, 2013.
</p>
<p>Tortora C., <em>Non-hierarchical clustering methods on factorial subspaces</em>, 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PDC">PDC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Asymmetric data set clustering example (with shape 3).
data('asymmetric3')
x&lt;-asymmetric3[,-1]

#Clustering
fpdas3=FPDC(x,4,3,3)

#Results
table(asymmetric3[,1],fpdas3$label)
Silh(fpdas3$probability)
summary(fpdas3)
plot(fpdas3)

## End(Not run)

## Not run: 
# Asymmetric data set clustering example (with shape 20).
data('asymmetric20')
x&lt;-asymmetric20[,-1]

#Clustering
fpdas20=FPDC(x,4,3,3)

#Results
table(asymmetric20[,1],fpdas20$label)
Silh(fpdas20$probability)
summary(fpdas20)
plot(fpdas20)

## End(Not run)

## Not run: 
# Clustering example with outliers.
data('outliers')
x&lt;-outliers[,-1]

#Clustering
fpdout=FPDC(x,4,5,4)

#Results
table(outliers[,1],fpdout$label)
Silh(fpdout$probability)
summary(fpdout)
plot(fpdout)

## End(Not run)
</code></pre>

<hr>
<h2 id='GPDC'>Gaussian PD-Clustering
</h2><span id='topic+GPDC'></span>

<h3>Description</h3>

<p> An implementation of Gaussian PD-Clustering GPDC, an extention of PD-clustering adjusted for cluster size that uses a dissimilarity measure based on the Gaussian density. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GPDC(data=NULL,k=2,ini="kmedoids", nr=5,iter=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPDC_+3A_data">data</code></td>
<td>
<p> A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="GPDC_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters
</p>
</td></tr>
<tr><td><code id="GPDC_+3A_ini">ini</code></td>
<td>
<p>A parameter that selects center starts. Options available are random (&quot;random&quot;), kmedoid (&quot;kmedoid&quot;, by default), and PDC (&quot;PDclust&quot;).
</p>
</td></tr>
<tr><td><code id="GPDC_+3A_nr">nr</code></td>
<td>
<p>Number of random starts when ini set to &quot;random&quot;
</p>
</td></tr>
<tr><td><code id="GPDC_+3A_iter">iter</code></td>
<td>
<p>Maximum number of iterations
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A class FPDclustering list with components
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>A vector of integers indicating the cluster membership for each unit</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster means</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>A list of K elements, with the variance-covariance matrix per cluster</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix of probability of each point belonging to each cluster</p>
</td></tr>
<tr><td><code>JDF</code></td>
<td>
<p>The value of the Joint distance function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora and Francesco Palumbo
</p>


<h3>References</h3>

<p>Tortora C., McNicholas P.D.,  and Palumbo F. <em>A probabilistic distance clustering algorithm using Gaussian and Student-t multivariate density distributions.</em> SN Computer Science, 1:65, 2020.
</p>
<p>C. Rainey, C. Tortora and F.Palumbo.<em> A parametric version of probabilistic distance clustering</em>. In: Greselin F., Deldossi L., Bagnato L., Vichi M. (eds) Statistical Learning of Complex Data. CLADAG 2017. Studies in Classification, Data Analysis, and Knowledge Organization. Springer, Cham, 33-43 2019. doi.org/10.1007/978-3-030-21140-0_4
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PDC">PDC</a>,<a href="#topic+PDQ">PDQ</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Load the data
data(ais)
dataSEL=ais[,c(10,3,5,8)]

#Clustering
res=GPDC(dataSEL,k=2,ini = "kmedoids")

#Results
table(res$label,ais$sex)
plot(res)
summary(res)
</code></pre>

<hr>
<h2 id='outliers'>Data set with outliers
</h2><span id='topic+outliers'></span>

<h3>Description</h3>

<p>Each cluster has been generated according to a multivariate Gaussian distribution, with centers c randomly generated. For each cluster, 20% of uniform distributed outliers have been generated at a distance included in max(x-c) and max(x-c)+5 form the center.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(outliers)</code></pre>


<h3>Format</h3>

<p>A data frame with 960 observations on the following 101 variables.
The first variable corresponds to the membership
</p>


<h3>Source</h3>

<p>generated with R
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(outliers)
 plot(outliers[,2:3]) 
</code></pre>

<hr>
<h2 id='PDC'>Probabilistic Distance Clustering
</h2><span id='topic+PDC'></span>

<h3>Description</h3>

<p>Probabilistic distance clustering (PD-clustering) is an iterative, distribution free, probabilistic clustering method. PD clustering is based on the constraint that the product of the probability and the distance of each point to any cluster centre is a constant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PDC(data = NULL, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PDC_+3A_data">data</code></td>
<td>
<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="PDC_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A class FPDclustering list with components
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>A vector of integers indicating the cluster membership for each unit</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix of probability of each point belonging to each cluster</p>
</td></tr>
<tr><td><code>JDF</code></td>
<td>
<p>The value of the Joint distance function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora and Paul D. McNicholas
</p>


<h3>References</h3>

<p>Ben-Israel C. and  Iyigun C. Probabilistic D-Clustering.<em> Journal of Classification</em>, <b>25</b>(1), 5-26, 2008.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Normally generated clusters
c1 = c(+2,+2,2,2)
c2 = c(-2,-2,-2,-2)
c3 = c(-3,3,-3,3)
n=200
x1 = cbind(rnorm(n, c1[1]), rnorm(n, c1[2]), rnorm(n, c1[3]), rnorm(n, c1[4]) )
x2 = cbind(rnorm(n, c2[1]), rnorm(n, c2[2]),rnorm(n, c2[3]), rnorm(n, c2[4]) )
x3 = cbind(rnorm(n, c3[1]), rnorm(n, c3[2]),rnorm(n, c3[3]), rnorm(n, c3[4]) )
x = rbind(x1,x2,x3)

#Clustering
pdn=PDC(x,3)

#Results
plot(pdn)

</code></pre>

<hr>
<h2 id='PDQ'>Probabilistic Distance Clustering Adjusted for Cluster Size 
</h2><span id='topic+PDQ'></span>

<h3>Description</h3>

<p> An implementation of probabilistic distance clustering adjusted for cluster size (PDQ), a probabilistic distance clustering algorithm that involves optimizing the PD-clustering criterion. The algorithm can be used, on continous, count, or mixed type data setting Euclidean, Chi square, or Gower  as dissimilarity measurments. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PDQ(data=NULL,k=2,ini='kmd',dist='euc',cent=NULL,
ord=NULL,cat=NULL,bin=NULL,cont=NULL,w=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PDQ_+3A_data">data</code></td>
<td>
<p> A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters.
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_ini">ini</code></td>
<td>
<p>A parameter that selects center starts. Options available are random (&quot;random&quot;), kmedoid (&quot;kmd&quot;, by default&quot;), center (&quot;center&quot;, the user inputs the center), and kmode (&quot;kmode&quot;, for categoriacal data sets).
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_dist">dist</code></td>
<td>
<p>A parameter that selects the distance measure used. Options available are Eucledean (&quot;euc&quot;), Gower (&quot;gower&quot;) and chi square (&quot;chi&quot;).
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_cent">cent</code></td>
<td>
<p>User inputted centers if ini is set to &quot;center&quot;.
</p>
</td></tr>  
<tr><td><code id="PDQ_+3A_ord">ord</code></td>
<td>
<p>column indices of the x matrix indicating which columns are ordinal variables.
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_cat">cat</code></td>
<td>
<p>column indices of the x matrix indicating which columns are  categorical variables.
</p>
</td></tr>  
<tr><td><code id="PDQ_+3A_bin">bin</code></td>
<td>
<p>column indices of the x matrix indicating which columns are  binary variables.
</p>
</td></tr>  
<tr><td><code id="PDQ_+3A_cont">cont</code></td>
<td>
<p>column indices of the x matrix indicating which columns are  continuous variables.
</p>
</td></tr>
<tr><td><code id="PDQ_+3A_w">w</code></td>
<td>
<p>numerical vector same length as the columns of the data, containing the variable weights when using Gower distance, equal weights  by default.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A class FPDclustering list with components
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>A vector of integers indicating the cluster membership for each unit</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix of probability of each point belonging to each cluster</p>
</td></tr>
<tr><td><code>JDF</code></td>
<td>
<p>The value of the Joint distance function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code>jdfvector</code></td>
<td>
<p>collection of all jdf calculations at each iteration</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora and Noe Vidales
</p>


<h3>References</h3>

<p>Iyigun, Cem, and Adi Ben-Israel. <em>Probabilistic distance clustering adjusted for cluster size.</em> Probability in the Engineering and Informational Sciences 22.4 (2008): 603-621.
doi.org/10.1017/S0269964808000351.
</p>
<p>Tortora and Palumbo. <em>Clustering mixed-type data using a probabilistic distance algorithm.</em> submitted.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PDC">PDC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Mixed type data

sig=matrix(0.7,4,4)
diag(sig)=1###creat a correlation matrix
x1=rmvnorm(200,c(0,0,3,3))##  cluster 1
x2=rmvnorm(200,c(4,4,6,6),sigma=sig)##  cluster 2
x=rbind(x1,x2)# data set with 2 clusters
l=c(rep(1,200),rep(2,200))#creating the labels
x1=cbind(x1,rbinom(200,4,0.2),rbinom(200,4,0.2))#categorical variables
x2=cbind(x2,rbinom(200,4,0.7),rbinom(200,4,0.7))
x=rbind(x1,x2) ##Data set

#### Performing PDQ
pdq_class&lt;-PDQ(data=x,k=2, ini="random", dist="gower", cont= 1:4, cat = 5:6)

###Output
table(l,pdq_class$label)
plot(pdq_class)
summary(pdq_class)



###Continuous data example
# Gaussian Generated Data  no  overlap 
x&lt;-rmvnorm(100, mean=c(1,5,10), sigma=diag(1,3))
y&lt;-rmvnorm(100, mean=c(4,8,13), sigma=diag(1,3))
data&lt;-rbind(x,y)

#### Performing PDQ
pdq1=PDQ(data,2,ini="random",dist="euc")
table(rep(c(2,1),each=100),pdq1$label)
Silh(pdq1$probability)
plot(pdq1)
summary(pdq1)


# Gaussian Generated Data with  overlap 
x2&lt;-rmvnorm(100, mean=c(1,5,10), sigma=diag(1,3))
y2&lt;-rmvnorm(100, mean=c(2,6,11), sigma=diag(1,3))
data2&lt;-rbind(x2,y2)

#### Performing PDQ
pdq2=PDQ(data2,2,ini="random",dist="euc")
table(rep(c(1,2),each=100),pdq2$label)
plot(pdq2)
summary(pdq2)
</code></pre>

<hr>
<h2 id='plot.FPDclustering'>
Plots for FPDclusteringt Objects
</h2><span id='topic+plot.FPDclustering'></span>

<h3>Description</h3>

<p>Probability Silhouette plot, Scatterplot up to  MaxVar variables, and  parallel coordinate plot up to MaxVar variables, for objects of class FPDclustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FPDclustering'
plot(x, maxVar=30, ... ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.FPDclustering_+3A_x">x</code></td>
<td>

<p>an object of class FPDclustering
</p>
</td></tr>
<tr><td><code id="plot.FPDclustering_+3A_maxvar">maxVar</code></td>
<td>

<p>a scalar indicating the maximum number of variables to display on the parallel plot, 30 by default
</p>
</td></tr>
<tr><td><code id="plot.FPDclustering_+3A_...">...</code></td>
<td>
<p>Additional parameters for the function paris</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora
</p>

<hr>
<h2 id='Silh'>Probabilistic silhouette plot
</h2><span id='topic+Silh'></span>

<h3>Description</h3>

<p> Graphical tool to evaluate the clustering partition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Silh(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Silh_+3A_p">p</code></td>
<td>
<p> A matrix of probabilities such that rows correspond to observations and columns correspond to clusters.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probabilistic silhouettes are an adaptation of the ones proposed by Menardi(2011) according to the following formula:
</p>
<p style="text-align: center;"><code class="reqn">dbs_i =   (log(p_{im_k}/p_{im_1}))/max_i |log(p_{im_k}/p_{im_1})|</code>
</p>

<p>where <code class="reqn">m_k</code> is such that <code class="reqn">x_i</code> belongs to cluster <code class="reqn">k</code> and <code class="reqn">m_1</code> is such that <code class="reqn">p_{im_1}</code> is maximum for <code class="reqn">m</code> different from<code class="reqn">m_k</code>.
</p>


<h3>Value</h3>

<p> Probabilistic silhouette plot
</p>


<h3>Author(s)</h3>

<p> Cristina Tortora
</p>


<h3>References</h3>

<p> Menardi G. Density-based Silhouette diagnostics for clustering methods.<em>Statistics and Computing</em>, <b>21</b>, 295-308, 2011.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Asymmetric data set silhouette example (with shape=3).
data('asymmetric3')
x&lt;-asymmetric3[,-1]
fpdas3=FPDC(x,4,3,3)
Silh(fpdas3$probability)

## End(Not run)

## Not run: 
# Asymmetric data set shiluette example (with shape=20).
data('asymmetric20')
x&lt;-asymmetric20[,-1]
fpdas20=FPDC(x,4,3,3)
Silh(fpdas20$probability)

## End(Not run)

## Not run: 
# Shiluette example with outliers.
data('outliers')
x&lt;-outliers[,-1]
fpdout=FPDC(x,4,4,3)
Silh(fpdout$probability)

## End(Not run)
</code></pre>

<hr>
<h2 id='Star'>Star dataset to predict star types
</h2><span id='topic+Star'></span>

<h3>Description</h3>

<p>A 6 class star dataset for star classification with Deep Learned approaches
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ais)</code></pre>


<h3>Format</h3>

<p>A data frame with 202 observations and 13 variable.
</p>

<dl>
<dt>K</dt><dd><p> Absolute Temperature (in K)</p>
</dd>
<dt>Lum</dt><dd><p> Relative Luminosity (L/Lo)</p>
</dd>
<dt>Rad</dt><dd><p> Relative Radius (R/Ro)</p>
</dd>
<dt>Mag</dt><dd><p> Absolute Magnitude (Mv)</p>
</dd>
<dt>Col</dt><dd><p> Star Color (white,Red,Blue,Yellow,yellow-orange etc)</p>
</dd>
<dt>Spect</dt><dd><p> Spectral Class (O,B,A,F,G,K,,M)</p>
</dd>
<dt>Type</dt><dd><p> Star Type (Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , SuperGiants, HyperGiants)</p>
</dd>
</dl>



<h3>Source</h3>

<p> https://www.kaggle.com/deepu1109/star-dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Star)

</code></pre>

<hr>
<h2 id='Students'>Statistics 1 students 
</h2><span id='topic+Students'></span>

<h3>Description</h3>

<p>Data set collected in 2022 that contains 10 variables recorded on a convenience sample of 253 students enrolled in the first year at the University od Naples FedericoII and attending an introductory Statistics course.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Students)</code></pre>


<h3>Format</h3>

<p>A data frame with 253 observations and 10 variable.
</p>

<dl>
<dt>Sex</dt><dd><p> gender, binary</p>
</dd>
<dt>HS_qual</dt><dd><p> high school type, categorical</p>
</dd>
<dt>Stud_stat</dt><dd><p> prior knowladge of statistics, binary</p>
</dd>
<dt>Course_modality</dt><dd><p> course modality of attendance (in presence, online, mixed), categorical</p>
</dd>
<dt>HE_Parents</dt><dd><p>parents' education degree, categorical</p>
</dd>
<dt>PMP</dt><dd><p> mathematical prerequisits for psychometric, continuous</p>
</dd>
<dt>SAS</dt><dd><p> statistical anxiety sale, continuous</p>
</dd>
<dt>RAI</dt><dd><p> relative authonomy index, continuous</p>
</dd>
<dt>S_EFF</dt><dd><p> self-efficacy, continuous</p>
</dd>
<dt>COG</dt><dd><p> cognitive competence, continuous</p>
</dd>
</dl>



<h3>References</h3>

<p> R. Fabbricatore. Latent class analysis for proficiency assessment in higher education: integrating multidimensional latent traits and learning topics. Ph.D. thesis, University of Naples Federico II, 2023
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Students)

</code></pre>

<hr>
<h2 id='summary.FPDclustering'>
Summary for FPDclusteringt Objects
</h2><span id='topic+summary.FPDclustering'></span>

<h3>Description</h3>

<p>Number of elements per cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FPDclustering'
summary(object, ... ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.FPDclustering_+3A_object">object</code></td>
<td>

<p>an object of class FPDclustering
</p>
</td></tr>
<tr><td><code id="summary.FPDclustering_+3A_...">...</code></td>
<td>
<p>Additional parameters for the function paris</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora
</p>

<hr>
<h2 id='TPDC'>Student-t PD-Clustering
</h2><span id='topic+TPDC'></span>

<h3>Description</h3>

<p> An implementation of Student-t PD-Clustering TPDC, an extention of PD-clustering adjusted for cluster size that uses a dissimilarity measure based on the multivariate Student-t density. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TPDC(data=NULL,k=2,ini="kmedoids", nr=5,iter=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TPDC_+3A_data">data</code></td>
<td>
<p> A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="TPDC_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters
</p>
</td></tr>
<tr><td><code id="TPDC_+3A_ini">ini</code></td>
<td>
<p>A parameter that selects center starts. Options available are random (&quot;random&quot;), kmedoid (&quot;kmedoid&quot;, by default), and PDC (&quot;PDclust&quot;).
</p>
</td></tr>
<tr><td><code id="TPDC_+3A_nr">nr</code></td>
<td>
<p>Number of random starts if ini is &quot;random&quot;
</p>
</td></tr>
<tr><td><code id="TPDC_+3A_iter">iter</code></td>
<td>
<p>Maximum number of iterations
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A class FPDclustering list with components
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>A vector of integers indicating the cluster membership for each unit</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster means</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>A list of K elements, with the variance-covariance matrix per cluster</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>A vector of K degrees of freedom </p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix of probability of each point belonging to each cluster</p>
</td></tr>
<tr><td><code>JDF</code></td>
<td>
<p>The value of the Joint distance function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cristina Tortora and Francesco Palumbo
</p>


<h3>References</h3>

<p>Tortora C., McNicholas P.D.,  and Palumbo F. <em>A probabilistic distance clustering algorithm using Gaussian and Student-t multivariate density distributions.</em> SN Computer Science, 1:65, 2020.
</p>
<p>C. Rainey, C. Tortora and F.Palumbo.<em> A parametric version of probabilistic distance clustering</em>. In: Greselin F., Deldossi L., Bagnato L., Vichi M. (eds) Statistical Learning of Complex Data. CLADAG 2017. Studies in Classification, Data Analysis, and Knowledge Organization. Springer, Cham, 33-43 2019. doi.org/10.1007/978-3-030-21140-0_4
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PDC">PDC</a>,<a href="#topic+PDQ">PDQ</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Load the data
data(ais)
dataSEL=ais[,c(10,3,5,8)]

#Clustering
res=TPDC(dataSEL,k=2,ini = "kmedoids")

#Results
table(res$label,ais$sex)
summary(res)
plot(res)
</code></pre>

<hr>
<h2 id='TuckerFactors'>Choice of the number of Tucker 3 factors for FPDC
</h2><span id='topic+TuckerFactors'></span>

<h3>Description</h3>

<p> An empirical way of choosing the number of factors for FPDC. The function returns a graph and a table representing the explained variability varying the number of factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TuckerFactors(data = NULL, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TuckerFactors_+3A_data">data</code></td>
<td>
<p>A matrix or data frame such that rows correspond to observations and columns correspond to variables.
</p>
</td></tr>
<tr><td><code id="TuckerFactors_+3A_k">k</code></td>
<td>
<p>A numerical parameter giving the number of clusters
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A table containing the explained variability varying the number of factors for units (column) and for variables (row) and the corresponding plot
</p>


<h3>Author(s)</h3>

<p>Cristina Tortora
</p>


<h3>References</h3>

 
<p>Kiers H, Kinderen A. A fast method for choosing the numbers of components in
Tucker3 analysis.<em>British Journal of Mathematical and Statistical Psychology</em>, <b>56</b>(1), 119-125, 2003.
</p>
<p>Kroonenberg P. <em>Applied Multiway Data Analysis</em>. Ebooks Corporation, Hoboken, New Jersey, 2008.
</p>
<p>Tortora C.,  Gettler Summa M., and  Palumbo F..
Factor pd-clustering. In Lausen et al., editor, <em>Algorithms from and for Nature and Life, Studies in Classification</em>, Data Analysis, and Knowledge Organization DOI 10.1007/978-3-319-00035-011, 115-123, 2013.
</p>


<h3>See Also</h3>

<p><code><a href="ThreeWay.html#topic+T3">T3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Asymmetric data set example (with shape=3).
data('asymmetric3')
xp=TuckerFactors(asymmetric3[,-1], nc = 4)


## End(Not run)

## Not run: 
# Asymmetric data set example (with shape=20).
data('asymmetric20')
xp=TuckerFactors(asymmetric20[,-1], nc = 4)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
