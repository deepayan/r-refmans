<!DOCTYPE html><html><head><title>Help for package SGDinference</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SGDinference}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Census2000'><p>Census2000</p></a></li>
<li><a href='#sgd_lm'><p>Averaged SGD in Linear Mean Regression</p></a></li>
<li><a href='#sgd_qr'><p>Averaged S-subGD Estimator in Linear Quantile Regression</p></a></li>
<li><a href='#sgdi_lm'><p>Averaged SGD and its Inference via Random Scaling</p></a></li>
<li><a href='#sgdi_qr'><p>Averaged S-subGD and its Inference via Random Scaling in Linear Quantile Regression</p></a></li>
<li><a href='#SGDinference'><p>SGDinference</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Inference with Stochastic Gradient Descent</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation and inference methods for large-scale mean and quantile regression models via stochastic (sub-)gradient descent (S-subGD) algorithms. 
    The inference procedure handles cross-sectional data sequentially: 
    (i) updating the parameter estimate with each incoming "new observation", 
    (ii) aggregating it as a Polyak-Ruppert average, and 
    (iii) computing an asymptotically pivotal statistic for inference through random scaling. 
    The methodology used in the 'SGDinference' package is described in detail in the following papers: 
    (i) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2022) &lt;<a href="https://doi.org/10.1609%2Faaai.v36i7.20701">doi:10.1609/aaai.v36i7.20701</a>&gt; "Fast and robust online inference with stochastic gradient descent via random scaling".
    (ii) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2023) &lt;<a href="https://doi.org/10.48550/arXiv.2209.14502">doi:10.48550/arXiv.2209.14502</a>&gt; "Fast Inference for Quantile Regression with Tens of Millions of Observations". </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, Rcpp (&ge; 1.0.5)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0), lmtest (&ge; 0.9),
sandwich (&ge; 3.0), microbenchmark (&ge; 1.4), conquer (&ge; 1.3.3)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/SGDinference-Lab/SGDinference/">https://github.com/SGDinference-Lab/SGDinference/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/SGDinference-Lab/SGDinference/issues">https://github.com/SGDinference-Lab/SGDinference/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-16 15:30:15 UTC; yshin</td>
</tr>
<tr>
<td>Author:</td>
<td>Sokbae Lee [aut],
  Yuan Liao [aut],
  Myung Hwan Seo [aut],
  Youngki Shin [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Youngki Shin &lt;shiny11@mcmaster.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-16 20:43:54 UTC</td>
</tr>
</table>
<hr>
<h2 id='Census2000'>Census2000</h2><span id='topic+Census2000'></span>

<h3>Description</h3>

<p>The Census2000 dataset 
from Acemoglu and Autor (2011) consists of observations on 26,120 nonwhite, female workers. 
This small dataset is constructed from &quot;microwage2000_ext.dta&quot; at 
<a href="https://economics.mit.edu/people/faculty/david-h-autor/data-archive">https://economics.mit.edu/people/faculty/david-h-autor/data-archive</a>.
Specifically, observations are dropped if hourly wages are missing or 
years of education are smaller than 6.    
Then, a 5 percent random sample is drawn to make the dataset small.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Census2000
</code></pre>


<h3>Format</h3>

<p>A data frame with 26,120 rows and 3 variables:
</p>

<dl>
<dt>ln_hrwage</dt><dd><p>log hourly wages</p>
</dd>
<dt>edyrs</dt><dd><p>years of education</p>
</dd>
<dt>exp</dt><dd><p>years of potential experience</p>
</dd>
</dl>



<h3>Source</h3>

<p>The original dataset from Acemoglu and Autor (2011) is available 
at <a href="https://economics.mit.edu/people/faculty/david-h-autor/data-archive">https://economics.mit.edu/people/faculty/david-h-autor/data-archive</a>.
</p>


<h3>References</h3>

<p>Acemoglu, D. and Autor, D., 2011. 
Skills, tasks and technologies: Implications for employment and earnings. 
In Handbook of labor economics (Vol. 4, pp. 1043-1171). Elsevier.
</p>

<hr>
<h2 id='sgd_lm'>Averaged SGD in Linear Mean Regression</h2><span id='topic+sgd_lm'></span>

<h3>Description</h3>

<p>Compute the averaged SGD estimator for the coefficients in linear mean regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgd_lm(
  formula,
  data,
  gamma_0 = NULL,
  alpha = 0.501,
  burn = 1,
  bt_start = NULL,
  studentize = TRUE,
  no_studentize = 100L,
  intercept = TRUE,
  path = FALSE,
  path_index = c(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgd_lm_+3A_formula">formula</code></td>
<td>
<p>formula. The response is on the left of a ~ operator. The terms are on the right of a ~ operator, separated by a + operator.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing variables in the model.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_gamma_0">gamma_0</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is NULL and it is determined by the adaptive method: 1/sd(y).</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_alpha">alpha</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is 0.501.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_burn">burn</code></td>
<td>
<p>numeric. A tuning parameter for &quot;burn-in&quot; observations. 
We burn-in up to (burn-1) observations and use observations from (burn) for estimation. Default is 1, i.e. no burn-in.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_bt_start">bt_start</code></td>
<td>
<p>numeric. (p x 1) vector, excluding the intercept term. User-provided starting value. Default is NULL.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_studentize">studentize</code></td>
<td>
<p>logical. Studentize regressors. Default is TRUE.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_no_studentize">no_studentize</code></td>
<td>
<p>numeric. The number of observations to compute the mean and std error for studentization. Default is 100.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_intercept">intercept</code></td>
<td>
<p>logical. Use the intercept term for regressors. Default is TRUE. 
If this option is TRUE, the first element of the parameter vector is the intercept term.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_path">path</code></td>
<td>
<p>logical. The whole path of estimation results is out. Default is FALSE.</p>
</td></tr>
<tr><td><code id="sgd_lm_+3A_path_index">path_index</code></td>
<td>
<p>numeric. A vector of indices to print out the path. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"sgdi"</code>, which is a list containing the following
</p>

<dl>
<dt><code>coefficients</code></dt><dd><p>a vector of estimated parameter values</p>
</dd>
<dt><code>path_coefficients</code></dt><dd><p>The path of coefficients.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The dimension of <code>coefficients</code> is (p+1) if <code>intercept</code>=TRUE or p otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1e05
p = 5
bt0 = rep(5,p)
x = matrix(rnorm(n*(p-1)), n, (p-1))
y = cbind(1,x) %*% bt0 + rnorm(n)
my.dat = data.frame(y=y, x=x)
sgd.out = sgd_lm(y~., data=my.dat)
</code></pre>

<hr>
<h2 id='sgd_qr'>Averaged S-subGD Estimator in Linear Quantile Regression</h2><span id='topic+sgd_qr'></span>

<h3>Description</h3>

<p>Compute the averaged S-subGD (stochastic subgradient) estimator for the coefficients in linear quantile regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgd_qr(
  formula,
  data,
  gamma_0 = NULL,
  alpha = 0.501,
  burn = 1,
  bt_start = NULL,
  qt = 0.5,
  studentize = TRUE,
  no_studentize = 100L,
  intercept = TRUE,
  path = FALSE,
  path_index = c(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgd_qr_+3A_formula">formula</code></td>
<td>
<p>formula. The response is on the left of a ~ operator. The terms are on the right of a ~ operator, separated by a + operator.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_data">data</code></td>
<td>
<p>an optional data frame containing variables in the model.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_gamma_0">gamma_0</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is NULL and it is determined by the adaptive method in Lee et al. (2023).</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_alpha">alpha</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is 0.501.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_burn">burn</code></td>
<td>
<p>numeric. A tuning parameter for &quot;burn-in&quot; observations. 
We burn-in up to (burn-1) observations and use observations from (burn) for estimation. Default is 1, i.e. no burn-in.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_bt_start">bt_start</code></td>
<td>
<p>numeric. (p x 1) vector, excluding the intercept term. User-provided starting value. Default is NULL. Then, it is estimated by conquer.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_qt">qt</code></td>
<td>
<p>numeric. Quantile. Default is 0.5.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_studentize">studentize</code></td>
<td>
<p>logical. Studentize regressors. Default is TRUE.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_no_studentize">no_studentize</code></td>
<td>
<p>numeric. The number of observations to compute the mean and std error for studentization. Default is 100.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_intercept">intercept</code></td>
<td>
<p>logical. Use the intercept term for regressors. Default is TRUE. 
If this option is TRUE, the first element of the parameter vector is the intercept term.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_path">path</code></td>
<td>
<p>logical. The whole path of estimation results is out. Default is FALSE.</p>
</td></tr>
<tr><td><code id="sgd_qr_+3A_path_index">path_index</code></td>
<td>
<p>numeric. A vector of indices to print out the path. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"sgdi"</code>, which is a list containing the following
</p>

<dl>
<dt><code>coefficients</code></dt><dd><p>a vector of estimated parameter values</p>
</dd>
<dt><code>path_coefficients</code></dt><dd><p>The path of coefficients.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The dimension of <code>coefficients</code> is (p+1) if <code>intercept</code>=TRUE or p otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1e05
p = 5
bt0 = rep(5,p)
x = matrix(rnorm(n*(p-1)), n, (p-1))
y = cbind(1,x) %*% bt0 + rnorm(n)
my.dat = data.frame(y=y, x=x)
sgd.out = sgd_qr(y~., data=my.dat)
</code></pre>

<hr>
<h2 id='sgdi_lm'>Averaged SGD and its Inference via Random Scaling</h2><span id='topic+sgdi_lm'></span>

<h3>Description</h3>

<p>Compute the averaged SGD estimator and conduct inference via random scaling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgdi_lm(
  formula,
  data,
  gamma_0 = NULL,
  alpha = 0.501,
  burn = 1,
  inference = "rs",
  bt_start = NULL,
  studentize = TRUE,
  no_studentize = 100L,
  intercept = TRUE,
  rss_idx = c(1),
  level = 0.95,
  path = FALSE,
  path_index = c(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgdi_lm_+3A_formula">formula</code></td>
<td>
<p>formula. The response is on the left of a ~ operator. The terms are on the right of a ~ operator, separated by a + operator.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing variables in the model.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_gamma_0">gamma_0</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is NULL and it is determined by the adaptive method: 1/sd(y).</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_alpha">alpha</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is 0.501.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_burn">burn</code></td>
<td>
<p>numeric. A tuning parameter for &quot;burn-in&quot; observations. We burn-in up to (burn-1) observations and use observations from (burn) for estimation. Default is 1, i.e. no burn-in.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_inference">inference</code></td>
<td>
<p>character. Specifying the inference method. Default is &quot;rs&quot; (random scaling matrix for joint inference using all the parameters). 
&quot;rss&quot; is for ransom scaling subset inference. This option requires that &quot;rss_indx&quot; should be provided.
&quot;rsd&quot; is for the diagonal elements of the random scaling matrix, excluding one for the intercept term.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_bt_start">bt_start</code></td>
<td>
<p>numeric. (p x 1) vector. User-provided starting value Default is NULL.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_studentize">studentize</code></td>
<td>
<p>logical. Studentize regressors. Default is TRUE</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_no_studentize">no_studentize</code></td>
<td>
<p>numeric. The number of observations to compute the mean and std error for studentization. Default is 100.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_intercept">intercept</code></td>
<td>
<p>logical. Use the intercept term for regressors. Default is TRUE. 
If this option is TRUE, the first element of the parameter vector is the intercept term.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_rss_idx">rss_idx</code></td>
<td>
<p>numeric. Index of x for random scaling subset inference. Default is 1, the first regressor of x. 
For example, if we want to focus on the 1st and 3rd covariates of x, then set it to be c(1,3).</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_level">level</code></td>
<td>
<p>numeric. The confidence level required. Default is 0.95. Can choose 0.90 and 0.80.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_path">path</code></td>
<td>
<p>logical. The whole path of estimation results is out. Default is FALSE.</p>
</td></tr>
<tr><td><code id="sgdi_lm_+3A_path_index">path_index</code></td>
<td>
<p>numeric. A vector of indices to print out the path. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"sgdi"</code>, which is a list containing the following
</p>

<dl>
<dt><code>coefficient</code></dt><dd><p>A (p + 1)-vector of estimated parameter values including the intercept.</p>
</dd>
<dt><code>var</code></dt><dd><p>A (p+1)x (p+1) variance-covariance matrix of <code>coefficient</code></p>
</dd>
<dt><code>ci.lower</code></dt><dd><p>The lower part of the 95% confidence interval</p>
</dd>
<dt><code>ci.upper</code></dt><dd><p>The upper part of the 95% confidence interval</p>
</dd>
<dt><code>level</code></dt><dd><p>The confidence level required. Default is 0.95.</p>
</dd>
<dt><code>path_coefficients</code></dt><dd><p>The path of coefficients.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>n = 1e05
p = 5
bt0 = rep(5,p)
x = matrix(rnorm(n*(p-1)), n, (p-1))
y = cbind(1,x) %*% bt0 + rnorm(n)
my.dat = data.frame(y=y, x=x)
sgdi.out = sgdi_lm(y~., data=my.dat)
</code></pre>

<hr>
<h2 id='sgdi_qr'>Averaged S-subGD and its Inference via Random Scaling in Linear Quantile Regression</h2><span id='topic+sgdi_qr'></span>

<h3>Description</h3>

<p>Compute the averaged S-subGD (stochastic subgradient) estimator for the coefficients in linear quantile regression and conduct inference via random scaling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgdi_qr(
  formula,
  data,
  gamma_0 = NULL,
  alpha = 0.501,
  burn = 1,
  inference = "rs",
  bt_start = NULL,
  qt = 0.5,
  studentize = TRUE,
  no_studentize = 100L,
  intercept = TRUE,
  rss_idx = c(1),
  level = 0.95,
  path = FALSE,
  path_index = c(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgdi_qr_+3A_formula">formula</code></td>
<td>
<p>formula. The response is on the left of a ~ operator. The terms are on the right of a ~ operator, separated by a + operator.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_data">data</code></td>
<td>
<p>an optional data frame containing variables in the model.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_gamma_0">gamma_0</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is NULL and it is determined by the adaptive method in Lee et al. (2023).</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_alpha">alpha</code></td>
<td>
<p>numeric. A tuning parameter for the learning rate (gamma_0 x t ^ alpha). Default is 0.501.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_burn">burn</code></td>
<td>
<p>numeric. A tuning parameter for &quot;burn-in&quot; observations. 
We burn-in up to (burn-1) observations and use observations from (burn) for estimation. Default is 1, i.e. no burn-in.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_inference">inference</code></td>
<td>
<p>character. Specifying the inference method. Default is &quot;rs&quot; (random scaling matrix for joint inference using all the parameters). 
&quot;rss&quot; is for ransom scaling subset inference. This option requires that &quot;rss_indx&quot; should be provided.
&quot;rsd&quot; is for the diagonal elements of the random scaling matrix, excluding one for the intercept term.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_bt_start">bt_start</code></td>
<td>
<p>numeric. (p x 1) vector, excluding the intercept term. User-provided starting value. Default is NULL. Then, it is estimated by conquer.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_qt">qt</code></td>
<td>
<p>numeric. Quantile. Default is 0.5.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_studentize">studentize</code></td>
<td>
<p>logical. Studentize regressors. Default is TRUE.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_no_studentize">no_studentize</code></td>
<td>
<p>numeric. The number of observations to compute the mean and std error for studentization. Default is 100.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_intercept">intercept</code></td>
<td>
<p>logical. Use the intercept term for regressors. Default is TRUE. 
If this option is TRUE, the first element of the parameter vector is the intercept term.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_rss_idx">rss_idx</code></td>
<td>
<p>numeric. Index of x for random scaling subset inference. Default is 1, the first regressor of x. 
For example, if we want to focus on the 1st and 3rd covariates of x, then set it to be c(1,3).</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_level">level</code></td>
<td>
<p>numeric. The confidence level required. Default is 0.95. Can choose 0.90 and 0.80.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_path">path</code></td>
<td>
<p>logical. The whole path of estimation results is out. Default is FALSE.</p>
</td></tr>
<tr><td><code id="sgdi_qr_+3A_path_index">path_index</code></td>
<td>
<p>numeric. A vector of indices to print out the path. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"sgdi"</code>, which is a list containing the following
</p>

<dl>
<dt><code>coefficients</code></dt><dd><p>a vector of estimated parameter values</p>
</dd>
<dt><code>V</code></dt><dd><p>a random scaling matrix depending on the inference method</p>
</dd>
<dt><code>ci.lower</code></dt><dd><p>a vector of lower confidence limits</p>
</dd>
<dt><code>ci.upper</code></dt><dd><p>a vector of upper confidence limits</p>
</dd>
<dt><code>inference</code></dt><dd><p>character that specifies the inference method</p>
</dd>
<dt><code>level</code></dt><dd><p>The confidence level required. Default is 0.95.</p>
</dd>
<dt><code>path_coefficients</code></dt><dd><p>The path of coefficients.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The dimension of <code>coefficients</code> is (p+1) if <code>intercept</code>=TRUE or p otherwise.
The random scaling matrix <code>V</code> is a full matrix if &quot;rs&quot; is chosen;
it is a scalar or smaller matrix, depending on the specification of &quot;rss_indx&quot; if &quot;rss&quot; is selected;
it is a vector of diagonal elements of the full matrix if &quot;rsd&quot; is selected. 
In this case, the first element is missing if the intercept is included.
The confidence intervals may contain NA under &quot;rss&quot; and &quot;rsd&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1e05
p = 5
bt0 = rep(5,p)
x = matrix(rnorm(n*(p-1)), n, (p-1))
y = cbind(1,x) %*% bt0 + rnorm(n)
my.dat = data.frame(y=y, x=x)
sgdi.out = sgdi_qr(y~., data=my.dat)
</code></pre>

<hr>
<h2 id='SGDinference'>SGDinference</h2><span id='topic+SGDinference'></span>

<h3>Description</h3>

<p>The 'SGDinference' package provides estimation and inference methods for large-scale mean and quantile regression models via stochastic (sub-)gradient descent (S-subGD) algorithms. 
The inference procedure handles cross-sectional data sequentially: 
(i) updating the parameter estimate with each incoming &quot;new observation&quot;, 
(ii) aggregating it as a Polyak-Ruppert average, and 
(iii) computing an asymptotically pivotal statistic for inference through random scaling.
</p>


<h3>Author(s)</h3>

<p>Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
