<!DOCTYPE html><html lang="en"><head><title>Help for package npcs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {npcs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cv.npcs'><p>Compare the performance of the NPMC-CX, NPMC-ER, and vanilla models through cross-validation or bootstrapping methods</p></a></li>
<li><a href='#error_rate'><p>Calculate the error rates for each class.</p></a></li>
<li><a href='#gamma_smote'><p>Gamma-synthetic minority over-sampling technique (gamma-SMOTE).</p></a></li>
<li><a href='#generate_data'><p>Generate the data.</p></a></li>
<li><a href='#npcs'><p>Fit a multi-class Neyman-Pearson classifier with error controls via cost-sensitive learning.</p></a></li>
<li><a href='#predict.npcs'><p>Predict new labels from new data based on the fitted NPMC classifier.</p></a></li>
<li><a href='#print.cv.npcs'><p>Print the cv.npcs object.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Neyman-Pearson Classification via Cost-Sensitive Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>We connect the multi-class Neyman-Pearson classification (NP) problem to the cost-sensitive learning (CS) problem, and propose two algorithms (NPMC-CX and NPMC-ER) to solve the multi-class NP problem through cost-sensitive learning tools. Under certain conditions, the two algorithms are shown to satisfy multi-class NP properties. More details are available in the paper "Neyman-Pearson Multi-class Classification via Cost-sensitive Learning" (Ye Tian and Yang Feng, 2021).</td>
</tr>
<tr>
<td>Imports:</td>
<td>dfoptim, magrittr, smotefamily, foreach, caret, formatR,
dplyr, forcats, ggplot2, tidyr, nnet</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, gbm</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-26 15:18:10 UTC; tsung_macbook</td>
</tr>
<tr>
<td>Author:</td>
<td>Ye Tian [aut],
  Ching-Tsung Tsai [aut, cre],
  Yang Feng [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ching-Tsung Tsai &lt;tctsung@nyu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-27 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cv.npcs'>Compare the performance of the NPMC-CX, NPMC-ER, and vanilla models through cross-validation or bootstrapping methods</h2><span id='topic+cv.npcs'></span>

<h3>Description</h3>

<p>Compare the performance of the NPMC-CX, NPMC-ER, and vanilla models through cross-validation or bootstrapping methods. The function will return a summary of evaluation which includes various evaluation metrics, and visualize the class-specific error rates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.npcs(
  x,
  y,
  classifier,
  alpha,
  w,
  fold = 5,
  stratified = TRUE,
  partition_ratio = 0.7,
  resample = c("bootstrapping", "cv"),
  seed = 1,
  verbose = TRUE,
  plotit = TRUE,
  trControl = list(),
  tuneGrid = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.npcs_+3A_x">x</code></td>
<td>
<p>matrix; the predictor matrix of complete data</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_y">y</code></td>
<td>
<p>numeric/factor/string; the response vector of complete data.</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_classifier">classifier</code></td>
<td>
<p>string; Model to use for npcs function</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_alpha">alpha</code></td>
<td>
<p>the levels we want to control for error rates of each class. The length must be equal to the number of classes</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_w">w</code></td>
<td>
<p>the weights in objective function. Should be a vector of length K, where K is the number of classes.</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_fold">fold</code></td>
<td>
<p>integer; number of folds in CV or number of bootstrapping iterations, default=5</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_stratified">stratified</code></td>
<td>
<p>logical; if TRUE, sample will be split into groups based on the proportion of response vector</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_partition_ratio">partition_ratio</code></td>
<td>
<p>numeric; the proportion of data to be used for model construction when parameter resample==&quot;bootstrapping&quot;</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_resample">resample</code></td>
<td>
<p>string; the resampling method
</p>

<ul>
<li><p> bootstrapping: bootstrapping, which iteration number is set by parameter &quot;fold&quot;
</p>
</li>
<li><p> cv: cross validation, the number of folds is set by parameter &quot;fold&quot;
</p>
</li></ul>
</td></tr>
<tr><td><code id="cv.npcs_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, cv.npcs will print the progress. If FALSE, the model will remain silent</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_plotit">plotit</code></td>
<td>
<p>logical; if TRUE, the output list will return a box plot summarizing the error rates of vanilla model and NPMC model</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_trcontrol">trControl</code></td>
<td>
<p>list; resampling method within each fold</p>
</td></tr>
<tr><td><code id="cv.npcs_+3A_tunegrid">tuneGrid</code></td>
<td>
<p>list; for hyperparameters tuning or setting</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># data generation: case 1 in Tian, Y., &amp; Feng, Y. (2021) with n = 1000
set.seed(123, kind = "L'Ecuyer-CMRG")
train.set &lt;- generate_data(n = 1000, model.no = 1)
x &lt;- train.set$x
y &lt;- train.set$y
test.set &lt;- generate_data(n = 2000, model.no = 1)
x.test &lt;- test.set$x
y.test &lt;- test.set$y
alpha &lt;- c(0.05, NA, 0.01)
w &lt;- c(0, 1, 0)
# contruct the multi-class NP problem

cv.npcs.knn &lt;- cv.npcs(x, y, classifier = "knn", w = w, alpha = alpha)
# result summary and visualization
cv.npcs.knn$summaries
cv.npcs.knn$plot

</code></pre>

<hr>
<h2 id='error_rate'>Calculate the error rates for each class.</h2><span id='topic+error_rate'></span>

<h3>Description</h3>

<p>Calculate the error rate for each class given the predicted labels and true labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error_rate(y.pred, y, class.names = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="error_rate_+3A_y.pred">y.pred</code></td>
<td>
<p>the predicted labels.</p>
</td></tr>
<tr><td><code id="error_rate_+3A_y">y</code></td>
<td>
<p>the true labels.</p>
</td></tr>
<tr><td><code id="error_rate_+3A_class.names">class.names</code></td>
<td>
<p>the names of classes. Should be a string vector. Default = NULL, which will set the name as 1, ..., K, where K is the number of classes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the error rate for each class. The vector name is the same as <code>class.names</code>.
</p>


<h3>References</h3>

<p>Tian, Y., &amp; Feng, Y. (2021). Neyman-Pearson Multi-class Classification via Cost-sensitive Learning. Submitted. Available soon on arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npcs">npcs</a></code>, <code><a href="#topic+predict.npcs">predict.npcs</a></code>, <code><a href="#topic+generate_data">generate_data</a></code>, <code><a href="#topic+gamma_smote">gamma_smote</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data generation
set.seed(123, kind = "L'Ecuyer-CMRG")
train.set &lt;- generate_data(n = 1000, model.no = 1)
x &lt;- train.set$x
y &lt;- train.set$y

test.set &lt;- generate_data(n = 1000, model.no = 1)
x.test &lt;- test.set$x
y.test &lt;- test.set$y

library(nnet)
fit.vanilla &lt;- multinom(y~., data = data.frame(x = x, y = factor(y)), trace = FALSE)
y.pred.vanilla &lt;- predict(fit.vanilla, newdata = data.frame(x = x.test))
error_rate(y.pred.vanilla, y.test)
</code></pre>

<hr>
<h2 id='gamma_smote'>Gamma-synthetic minority over-sampling technique (gamma-SMOTE).</h2><span id='topic+gamma_smote'></span>

<h3>Description</h3>

<p>gamma-SMOTE with some gamma in [0,1], which is a variant of the original SMOTE proposed by Chawla, N. V. et. al (2002). This can be combined with the NPMC methods proposed in Tian, Y., &amp; Feng, Y. (2021). See Section 5.2.3 in Tian, Y., &amp; Feng, Y. (2021) for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamma_smote(x, y, dup_rate = 1, gamma = 0.5, k = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gamma_smote_+3A_x">x</code></td>
<td>
<p>the predictor matrix, where each row and column represents an observation and predictor, respectively.</p>
</td></tr>
<tr><td><code id="gamma_smote_+3A_y">y</code></td>
<td>
<p>the response vector. Must be integers from 1 to K for some K &gt;= 2. Can either be a numerical or factor vector.</p>
</td></tr>
<tr><td><code id="gamma_smote_+3A_dup_rate">dup_rate</code></td>
<td>
<p>duplicate rate of original data. Default = 1, which finally leads to a new data set with twice sample size.</p>
</td></tr>
<tr><td><code id="gamma_smote_+3A_gamma">gamma</code></td>
<td>
<p>the upper bound of uniform distribution used when generating synthetic data points in SMOTE. Can be any number between 0 and 1. Default = 0.5. When it equals to 1, gamma-SMOTE is equivalent to the original SMOTE (Chawla, N. V. et. al (2002)).</p>
</td></tr>
<tr><td><code id="gamma_smote_+3A_k">k</code></td>
<td>
<p>the number of nearest neighbors during sampling process in SMOTE. Default = 5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list consisting of merged original and synthetic data, with two components x and y. x is the predictor matrix and y is the label vector.
</p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., &amp; Kegelmeyer, W. P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357.
</p>
<p>Tian, Y., &amp; Feng, Y. (2021). Neyman-Pearson Multi-class Classification via Cost-sensitive Learning. Submitted. Available soon on arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npcs">npcs</a></code>, <code><a href="#topic+predict.npcs">predict.npcs</a></code>, <code><a href="#topic+error_rate">error_rate</a></code>, and <code><a href="#topic+generate_data">generate_data</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(123, kind = "L'Ecuyer-CMRG")
train.set &lt;- generate_data(n = 200, model.no = 1)
x &lt;- train.set$x
y &lt;- train.set$y

test.set &lt;- generate_data(n = 1000, model.no = 1)
x.test &lt;- test.set$x
y.test &lt;- test.set$y

# contruct the multi-class NP problem: case 1 in Tian, Y., &amp; Feng, Y. (2021)
alpha &lt;- c(0.05, NA, 0.01)
w &lt;- c(0, 1, 0)

## try NPMC-CX, NPMC-ER based on multinomial logistic regression, and vanilla multinomial
## logistic regression without SMOTE. NPMC-ER outputs the infeasibility error information.
fit.npmc.CX &lt;- try(npcs(x, y, algorithm = "CX", classifier = "multinom", w = w, alpha = alpha))
fit.npmc.ER &lt;- try(npcs(x, y, algorithm = "ER", classifier = "multinom", w = w, alpha = alpha,
refit = TRUE))
fit.vanilla &lt;- nnet::multinom(y~., data = data.frame(x = x, y = factor(y)), trace = FALSE)

# test error of NPMC-CX based on multinomial logistic regression without SMOTE
y.pred.CX &lt;- predict(fit.npmc.CX, x.test)
error_rate(y.pred.CX, y.test)

# test error of vanilla multinomial logistic regression without SMOTE
y.pred.vanilla &lt;- predict(fit.vanilla, newdata = data.frame(x = x.test))
error_rate(y.pred.vanilla, y.test)


## create synthetic data by 0.5-SMOTE
D.syn &lt;- gamma_smote(x, y, dup_rate = 1, gamma = 0.5, k = 5)
x &lt;- D.syn$x
y &lt;- D.syn$y

## try NPMC-CX, NPMC-ER based on multinomial logistic regression, and vanilla multinomial logistic
## regression with SMOTE. NPMC-ER can successfully find a solution after SMOTE.
fit.npmc.CX &lt;- try(npcs(x, y, algorithm = "CX", classifier = "multinom", w = w, alpha = alpha))
fit.npmc.ER &lt;- try(npcs(x, y, algorithm = "ER", classifier = "multinom", w = w, alpha = alpha,
refit = TRUE))
fit.vanilla &lt;- nnet::multinom(y~., data = data.frame(x = x, y = factor(y)), trace = FALSE)

# test error of NPMC-CX based on multinomial logistic regression with SMOTE
y.pred.CX &lt;- predict(fit.npmc.CX, x.test)
error_rate(y.pred.CX, y.test)

# test error of NPMC-ER based on multinomial logistic regression with SMOTE
y.pred.ER &lt;- predict(fit.npmc.ER, x.test)
error_rate(y.pred.ER, y.test)

# test error of vanilla multinomial logistic regression wit SMOTE
y.pred.vanilla &lt;- predict(fit.vanilla, newdata = data.frame(x = x.test))
error_rate(y.pred.vanilla, y.test)

## End(Not run)
</code></pre>

<hr>
<h2 id='generate_data'>Generate the data.</h2><span id='topic+generate_data'></span>

<h3>Description</h3>

<p>Generate the data from two simulation cases in Tian, Y., &amp; Feng, Y. (2021).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_data(n = 1000, model.no = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_data_+3A_n">n</code></td>
<td>
<p>the generated sample size. Default = 1000.</p>
</td></tr>
<tr><td><code id="generate_data_+3A_model.no">model.no</code></td>
<td>
<p>the model number in Tian, Y., &amp; Feng, Y. (2021). Can be 1 or 2. Default = 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two components x and y. x is the predictor matrix and y is the label vector.
</p>


<h3>References</h3>

<p>Tian, Y., &amp; Feng, Y. (2021). Neyman-Pearson Multi-class Classification via Cost-sensitive Learning. Submitted. Available soon on arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npcs">npcs</a></code>, <code><a href="#topic+predict.npcs">predict.npcs</a></code>, <code><a href="#topic+error_rate">error_rate</a></code>, and <code><a href="#topic+gamma_smote">gamma_smote</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123, kind = "L'Ecuyer-CMRG")
train.set &lt;- generate_data(n = 1000, model.no = 1)
x &lt;- train.set$x
y &lt;- train.set$y

</code></pre>

<hr>
<h2 id='npcs'>Fit a multi-class Neyman-Pearson classifier with error controls via cost-sensitive learning.</h2><span id='topic+npcs'></span>

<h3>Description</h3>

<p>Fit a multi-class Neyman-Pearson classifier with error controls via cost-sensitive learning. This function implements two algorithms proposed in Tian, Y. &amp; Feng, Y. (2021). The problem is minimize a linear combination of P(hat(Y)(X) != k| Y=k) for some classes k while controlling P(hat(Y)(X) != k| Y=k) for some classes k. See Tian, Y. &amp; Feng, Y. (2021) for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcs(
  x,
  y,
  algorithm = c("CX", "ER"),
  classifier,
  seed = 1,
  w,
  alpha,
  trControl = list(),
  tuneGrid = list(),
  split.ratio = 0.5,
  split.mode = c("by-class", "merged"),
  tol = 1e-06,
  refit = TRUE,
  protect = TRUE,
  opt.alg = c("Hooke-Jeeves", "Nelder-Mead")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="npcs_+3A_x">x</code></td>
<td>
<p>the predictor matrix of training data, where each row and column represents an observation and predictor, respectively.</p>
</td></tr>
<tr><td><code id="npcs_+3A_y">y</code></td>
<td>
<p>the response vector of training data. Must be integers from 1 to K for some K &gt;= 2. Can be either a numerical or factor vector.</p>
</td></tr>
<tr><td><code id="npcs_+3A_algorithm">algorithm</code></td>
<td>
<p>the NPMC algorithm to use. String only. Can be either &quot;CX&quot; or &quot;ER&quot;, which implements NPMC-CX or NPMC-ER in Tian, Y. &amp; Feng, Y. (2021).</p>
</td></tr>
<tr><td><code id="npcs_+3A_classifier">classifier</code></td>
<td>
<p>which model to use for estimating the posterior distribution P(Y|X = x). String only.</p>
</td></tr>
<tr><td><code id="npcs_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="npcs_+3A_w">w</code></td>
<td>
<p>the weights in objective function. Should be a vector of length K, where K is the number of classes.</p>
</td></tr>
<tr><td><code id="npcs_+3A_alpha">alpha</code></td>
<td>
<p>the levels we want to control for error rates of each class. Should be a vector of length K, where K is the number of classes. Use NA if if no error control is imposed for specific classes.</p>
</td></tr>
<tr><td><code id="npcs_+3A_trcontrol">trControl</code></td>
<td>
<p>list; resampling method</p>
</td></tr>
<tr><td><code id="npcs_+3A_tunegrid">tuneGrid</code></td>
<td>
<p>list; for hyperparameters tuning or setting</p>
</td></tr>
<tr><td><code id="npcs_+3A_split.ratio">split.ratio</code></td>
<td>
<p>the proportion of data to be used in searching lambda (cost parameters). Should be between 0 and 1. Default = 0.5. Only useful when <code>algorithm</code> = &quot;ER&quot;.</p>
</td></tr>
<tr><td><code id="npcs_+3A_split.mode">split.mode</code></td>
<td>
<p>two different modes to split the data for NPMC-ER. String only. Can be either &quot;per-class&quot; or &quot;merged&quot;. Default = &quot;per-class&quot;. Only useful when <code>algorithm</code> = &quot;ER&quot;.
</p>

<ul>
<li><p> per-class: split the data by class.
</p>
</li>
<li><p> merged: split the data as a whole.
</p>
</li></ul>
</td></tr>
<tr><td><code id="npcs_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance. Default = 1e-06. Used in the lambda-searching step. The optimization is terminated when the step length of the main loop becomes smaller than <code>tol</code>. See pages of <code><a href="dfoptim.html#topic+hjkb">hjkb</a></code> and <code><a href="dfoptim.html#topic+nmkb">nmkb</a></code> for more details.</p>
</td></tr>
<tr><td><code id="npcs_+3A_refit">refit</code></td>
<td>
<p>whether to refit the classifier using all data after finding lambda or not. Boolean value. Default = TRUE. Only useful when <code>algorithm</code> = &quot;ER&quot;.</p>
</td></tr>
<tr><td><code id="npcs_+3A_protect">protect</code></td>
<td>
<p>whether to threshold the close-zero lambda or not. Boolean value. Default = TRUE. This parameter is set to avoid extreme cases that some lambdas are set equal to zero due to computation accuracy limit. When <code>protect</code> = TRUE, all lambdas smaller than 1e-03 will be set equal to 1e-03.</p>
</td></tr>
<tr><td><code id="npcs_+3A_opt.alg">opt.alg</code></td>
<td>
<p>optimization method to use when searching lambdas. String only. Can be either &quot;Hooke-Jeeves&quot; or &quot;Nelder-Mead&quot;. Default = &quot;Hooke-Jeeves&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>"npcs"</code>.
</p>
<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>the estimated lambda vector, which consists of Lagrangian multipliers. It is related to the cost. See Section 2 of Tian, Y. &amp; Feng, Y. (2021) for details.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>the fitted classifier.</p>
</td></tr>
<tr><td><code>classifier</code></td>
<td>
<p>which classifier to use for estimating the posterior distribution P(Y|X = x).</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>the NPMC algorithm to use.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the levels we want to control for error rates of each class.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the weights in objective function.</p>
</td></tr>
<tr><td><code>pik</code></td>
<td>
<p>the estimated marginal probability for each class.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tian, Y., &amp; Feng, Y. (2021). Neyman-Pearson Multi-class Classification via Cost-sensitive Learning. Submitted. Available soon on arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.npcs">predict.npcs</a></code>, <code><a href="#topic+error_rate">error_rate</a></code>, <code><a href="#topic+generate_data">generate_data</a></code>, <code><a href="#topic+gamma_smote">gamma_smote</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data generation: case 1 in Tian, Y., &amp; Feng, Y. (2021) with n = 1000
set.seed(123, kind = "L'Ecuyer-CMRG")
train.set &lt;- generate_data(n = 1000, model.no = 1)
x &lt;- train.set$x
y &lt;- train.set$y

test.set &lt;- generate_data(n = 1000, model.no = 1)
x.test &lt;- test.set$x
y.test &lt;- test.set$y

# contruct the multi-class NP problem: case 1 in Tian, Y., &amp; Feng, Y. (2021)
alpha &lt;- c(0.05, NA, 0.01)
w &lt;- c(0, 1, 0)

# try NPMC-CX, NPMC-ER, and vanilla multinomial logistic regression
fit.vanilla &lt;- nnet::multinom(y~., data = data.frame(x = x, y = factor(y)), trace = FALSE)
fit.npmc.CX &lt;- try(npcs(x, y, algorithm = "CX", classifier = "multinom", 
w = w, alpha = alpha))
fit.npmc.ER &lt;- try(npcs(x, y, algorithm = "ER", classifier = "multinom", 
w = w, alpha = alpha, refit = TRUE))
# test error of vanilla multinomial logistic regression
y.pred.vanilla &lt;- predict(fit.vanilla, newdata = data.frame(x = x.test))
error_rate(y.pred.vanilla, y.test)
# test error of NPMC-CX
y.pred.CX &lt;- predict(fit.npmc.CX, x.test)
error_rate(y.pred.CX, y.test)
# test error of NPMC-ER
y.pred.ER &lt;- predict(fit.npmc.ER, x.test)
error_rate(y.pred.ER, y.test)

</code></pre>

<hr>
<h2 id='predict.npcs'>Predict new labels from new data based on the fitted NPMC classifier.</h2><span id='topic+predict.npcs'></span>

<h3>Description</h3>

<p>Predict new labels from new data based on the fitted NPMC classifier, which belongs to S3 class &quot;npcs&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npcs'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.npcs_+3A_object">object</code></td>
<td>
<p>the model object for prediction</p>
</td></tr>
<tr><td><code id="predict.npcs_+3A_newx">newx</code></td>
<td>
<p>input feature data</p>
</td></tr>
<tr><td><code id="predict.npcs_+3A_...">...</code></td>
<td>
<p>arguments to pass down</p>
</td></tr>
</table>

<hr>
<h2 id='print.cv.npcs'>Print the cv.npcs object.</h2><span id='topic+print.cv.npcs'></span>

<h3>Description</h3>

<p>Print the cv.npcs object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.npcs'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.cv.npcs_+3A_x">x</code></td>
<td>
<p>fitted cv.npcs object using <code>cv.npcs</code>.</p>
</td></tr>
<tr><td><code id="print.cv.npcs_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
