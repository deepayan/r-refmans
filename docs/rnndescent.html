<!DOCTYPE html><html><head><title>Help for package rnndescent</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rnndescent}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rnndescent-package'><p>rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors</p></a></li>
<li><a href='#brute_force_knn'><p>Find exact nearest neighbors by brute force</p></a></li>
<li><a href='#brute_force_knn_query'><p>Query exact nearest neighbors by brute force</p></a></li>
<li><a href='#graph_knn_query'><p>Query a search graph for nearest neighbors</p></a></li>
<li><a href='#k_occur'><p>Quantify hubness of a nearest neighbor graph</p></a></li>
<li><a href='#merge_knn'><p>Merge multiple approximate nearest neighbors graphs</p></a></li>
<li><a href='#neighbor_overlap'><p>Overlap between the indices of two nearest neighbor graphs</p></a></li>
<li><a href='#nnd_knn'><p>Find nearest neighbors using nearest neighbor descent</p></a></li>
<li><a href='#prepare_search_graph'><p>Convert a nearest neighbor graph into a search graph</p></a></li>
<li><a href='#random_knn'><p>Find nearest neighbors by random selection</p></a></li>
<li><a href='#random_knn_query'><p>Query nearest neighbors by random selection</p></a></li>
<li><a href='#rnnd_build'><p>Build approximate nearest neighbors index and neighbor graph</p></a></li>
<li><a href='#rnnd_knn'><p>Find approximate nearest neighbors</p></a></li>
<li><a href='#rnnd_query'><p>Query an index for approximate nearest neighbors</p></a></li>
<li><a href='#rpf_build'><p>Create a random projection forest nearest neighbor index</p></a></li>
<li><a href='#rpf_filter'><p>Keep the best trees in a random projection forest</p></a></li>
<li><a href='#rpf_knn'><p>Find nearest neighbors using a random projection forest</p></a></li>
<li><a href='#rpf_knn_query'><p>Query a random projection forest index for nearest neighbors</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nearest Neighbor Descent Method for Approximate Nearest
Neighbors</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.6</td>
</tr>
<tr>
<td>Description:</td>
<td>The Nearest Neighbor Descent method for finding approximate
    nearest neighbors by Dong and co-workers (2010)
    &lt;<a href="https://doi.org/10.1145%2F1963405.1963487">doi:10.1145/1963405.1963487</a>&gt;. Based on the 'Python' package
    'PyNNDescent' <a href="https://github.com/lmcinnes/pynndescent">https://github.com/lmcinnes/pynndescent</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://jlmelville.github.io/rnndescent/">https://jlmelville.github.io/rnndescent/</a>,
<a href="https://github.com/jlmelville/rnndescent">https://github.com/jlmelville/rnndescent</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jlmelville/rnndescent/issues">https://github.com/jlmelville/rnndescent/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>dqrng, Matrix (&ge; 1.3-0), methods, Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH, dqrng, Rcpp, sitmo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-14 05:59:48 UTC; jlmel</td>
</tr>
<tr>
<td>Author:</td>
<td>James Melville [aut, cre, cph],
  Vitalie Spinu [ctb],
  Ralf Stubner [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Melville &lt;jlmelville@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-14 06:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rnndescent-package'>rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors</h2><span id='topic+rnndescent'></span><span id='topic+rnndescent-package'></span>

<h3>Description</h3>

<p>The Nearest Neighbor Descent method for finding approximate nearest neighbors by Dong and co-workers (2010) <a href="https://doi.org/10.1145/1963405.1963487">doi:10.1145/1963405.1963487</a>. Based on the 'Python' package 'PyNNDescent' <a href="https://github.com/lmcinnes/pynndescent">https://github.com/lmcinnes/pynndescent</a>.
</p>


<h3>Details</h3>

<p>The rnndescent package provides functions to create approximate nearest
neighbors using the Nearest Neighbor Descent (Dong and co-workers, 2010) and
Random Partition Tree (Dasgupta and Freund, 2008) methods. In comparison to
other packages, it offers more metrics and can be used with sparse matrices.
For querying new data, it uses graph diversification methods (Harwood and
Drummond, 2016) and back-tracking (Iwasakai and Miyazaki, 2018) to improve
the search performance. The package also provides functions to diagnose
hubness in nearest neighbor results (Radovanovic and co-workers, 2010).
</p>
<p>This library is based heavily on the 'PyNNDescent' Python library.
</p>
<p>General resources:
</p>

<ul>
<li><p> Website for the 'rnndescent' package: <a href="https://github.com/jlmelville/rnndescent">https://github.com/jlmelville/rnndescent</a>
</p>
</li>
<li><p> Documentation for the 'rnndescent' package: <a href="https://jlmelville.github.io/rnndescent/">https://jlmelville.github.io/rnndescent/</a>
</p>
</li>
<li><p> Website of the 'PyNNDescent' package: <a href="https://github.com/lmcinnes/pynndescent">https://github.com/lmcinnes/pynndescent</a>
</p>
</li></ul>

<p>The following functions provide the main interface to the package, with
useful defaults:
</p>

<ul>
<li><p> Find the approximate nearest neighbors: <code><a href="#topic+rnnd_knn">rnnd_knn()</a></code>
</p>
</li>
<li><p> Create a search index and query new neighbors: <code><a href="#topic+rnnd_build">rnnd_build()</a></code>.
</p>
</li>
<li><p> Query new neighbors (or refine an existing knn graph): <code><a href="#topic+rnnd_query">rnnd_query()</a></code>.
</p>
</li></ul>

<p>Some diagnostic and helper functions to help explore the the structure of the
graphs and how well the approximation is working:
</p>

<ul>
<li><p> Find exact nearest neighbors: <code><a href="#topic+brute_force_knn">brute_force_knn()</a></code>, <code><a href="#topic+brute_force_knn_query">brute_force_knn_query()</a></code>.
</p>
</li>
<li><p> Merging graphs: <code><a href="#topic+merge_knn">merge_knn()</a></code>.
</p>
</li>
<li><p> Hubness: <code><a href="#topic+k_occur">k_occur()</a></code>.
</p>
</li>
<li><p> Overlap/accuracy of two neighbor graphs: <code><a href="#topic+neighbor_overlap">neighbor_overlap()</a></code>.
</p>
</li></ul>

<p>Some lower-level functions are also available if you want more control than
the <code style="white-space: pre;">&#8288;rnnd_*&#8288;</code> functions provide:
</p>

<ul>
<li><p> Find approximate nearest neighbors: <code><a href="#topic+rpf_knn">rpf_knn()</a></code>, <code><a href="#topic+nnd_knn">nnd_knn()</a></code>.
</p>
</li>
<li><p> Generating random neighbors: <code><a href="#topic+random_knn">random_knn()</a></code>, <code><a href="#topic+random_knn_query">random_knn_query()</a></code>.
</p>
</li>
<li><p> Building an index: <code><a href="#topic+rpf_build">rpf_build()</a></code>, <code><a href="#topic+rpf_filter">rpf_filter()</a></code>.
</p>
</li>
<li><p> Querying an index for new data: <code><a href="#topic+rpf_knn_query">rpf_knn_query()</a></code>, <code><a href="#topic+prepare_search_graph">prepare_search_graph()</a></code>,
<code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: James Melville <a href="mailto:jlmelville@gmail.com">jlmelville@gmail.com</a> [copyright holder]
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Vitalie Spinu [contributor]
</p>
</li>
<li><p> Ralf Stubner [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In <em>Proceedings of the 20th international conference on World Wide Web</em>
(pp. 577-586).
ACM.
<a href="https://doi.org/10.1145/1963405.1963487">doi:10.1145/1963405.1963487</a>.
</p>
<p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).
</p>
<p>Radovanovic, M., Nanopoulos, A., &amp; Ivanovic, M. (2010).
Hubs in space: Popular nearest neighbors in high-dimensional data.
<em>Journal of Machine Learning Research</em>, <em>11</em>, 2487-2531.
<a href="https://www.jmlr.org/papers/v11/radovanovic10a.html">https://www.jmlr.org/papers/v11/radovanovic10a.html</a>
</p>
<p>Iwasaki, M., &amp; Miyazaki, D. (2018).
Optimization of indexing based on k-nearest neighbor graph for proximity search in high-dimensional data.
<em>arXiv preprint</em> <em>arXiv:1810.07355</em>.
<a href="https://arxiv.org/abs/1810.07355">https://arxiv.org/abs/1810.07355</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://jlmelville.github.io/rnndescent/">https://jlmelville.github.io/rnndescent/</a>
</p>
</li>
<li> <p><a href="https://github.com/jlmelville/rnndescent">https://github.com/jlmelville/rnndescent</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/jlmelville/rnndescent/issues">https://github.com/jlmelville/rnndescent/issues</a>
</p>
</li></ul>


<hr>
<h2 id='brute_force_knn'>Find exact nearest neighbors by brute force</h2><span id='topic+brute_force_knn'></span>

<h3>Description</h3>

<p>Returns the exact nearest neighbors of a dataset. A brute force search is
carried out: all possible pairs of points are compared, and the nearest
neighbors are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brute_force_knn(
  data,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brute_force_knn_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="brute_force_knn_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method is accurate but scales poorly with dataset size, so use with
caution with larger datasets. Having the exact neighbors as a ground truth to
compare with approximate results is useful for benchmarking and determining
parameter settings of the approximate methods.
</p>


<h3>Value</h3>

<p>the nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Find the 4 nearest neighbors using Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn &lt;- brute_force_knn(iris, k = 4, metric = "euclidean")

# Manhattan (l1) distance
iris_nn &lt;- brute_force_knn(iris, k = 4, metric = "manhattan")

# Multi-threading: you can choose the number of threads to use: in real
# usage, you will want to set n_threads to at least 2
iris_nn &lt;- brute_force_knn(iris, k = 4, metric = "manhattan", n_threads = 1)

# Use verbose flag to see information about progress
iris_nn &lt;- brute_force_knn(iris, k = 4, metric = "euclidean", verbose = TRUE)
</code></pre>

<hr>
<h2 id='brute_force_knn_query'>Query exact nearest neighbors by brute force</h2><span id='topic+brute_force_knn_query'></span>

<h3>Description</h3>

<p>Returns the exact nearest neighbors of query data to the reference data. A
brute force search is carried out: all possible pairs of reference and query
points are compared, and the nearest neighbors are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brute_force_knn_query(
  query,
  reference,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brute_force_knn_query_+3A_query">query</code></td>
<td>
<p>Matrix of <code>n</code> query items, with observations in the rows and
features in the columns. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>reference</code> data must be passed in the same orientation as
<code>query</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_reference">reference</code></td>
<td>
<p>Matrix of <code>m</code> reference items, with observations in the rows
and features in the columns. The nearest neighbors to the queries are
calculated from this data. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>query</code> data must be passed in the same format and
orientation as <code>reference</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>,
<code><a href="base.html#topic+matrix">base::matrix()</a></code> or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in
<code>dgCMatrix</code> format.</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="brute_force_knn_query_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>query</code> and <code>reference</code>
orientation stores each observation as a column (the orientation must be
consistent). The default <code>"R"</code> means that observations are stored in each
row. Storing the data by row is usually more convenient, but internally
your data will be converted to column storage. Passing it already
column-oriented will save some memory and (a small amount of) CPU usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is accurate but scales poorly with dataset size, so use with caution
with larger datasets. Having the exact neighbors as a ground truth to compare
with approximate results is useful for benchmarking and determining
parameter settings of the approximate methods.
</p>


<h3>Value</h3>

<p>the nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices in
<code>reference</code>.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances to the
items in <code>reference</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># 100 reference iris items
iris_ref &lt;- iris[iris$Species %in% c("setosa", "versicolor"), ]

# 50 query items
iris_query &lt;- iris[iris$Species == "versicolor", ]

# For each item in iris_query find the 4 nearest neighbors in iris_ref
# If you pass a data frame, non-numeric columns are removed
# set verbose = TRUE to get details on the progress being made
iris_query_nn &lt;- brute_force_knn_query(iris_query,
  reference = iris_ref,
  k = 4, metric = "euclidean", verbose = TRUE
)

# Manhattan (l1) distance
iris_query_nn &lt;- brute_force_knn_query(iris_query,
  reference = iris_ref,
  k = 4, metric = "manhattan"
)
</code></pre>

<hr>
<h2 id='graph_knn_query'>Query a search graph for nearest neighbors</h2><span id='topic+graph_knn_query'></span>

<h3>Description</h3>

<p>Run queries against a search graph, to return nearest neighbors taken from
the reference data used to build that graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_knn_query(
  query,
  reference,
  reference_graph,
  k = NULL,
  metric = "euclidean",
  init = NULL,
  epsilon = 0.1,
  max_search_fraction = 1,
  use_alt_metric = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graph_knn_query_+3A_query">query</code></td>
<td>
<p>Matrix of <code>n</code> query items, with observations in the rows and
features in the columns. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>reference</code> data must be passed in the same orientation as
<code>query</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_reference">reference</code></td>
<td>
<p>Matrix of <code>m</code> reference items, with observations in the rows
and features in the columns. The nearest neighbors to the queries are
calculated from this data. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>query</code> data must be passed in the same format and
orientation as <code>reference</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>,
<code><a href="base.html#topic+matrix">base::matrix()</a></code> or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in
<code>dgCMatrix</code> format.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_reference_graph">reference_graph</code></td>
<td>
<p>Search graph of the <code>reference</code> data. A neighbor
graph, such as that output from <code><a href="#topic+nnd_knn">nnd_knn()</a></code> can be used, but
preferably a suitably prepared sparse search graph should be used, such as
that output by <code><a href="#topic+prepare_search_graph">prepare_search_graph()</a></code>.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return. Optional if <code>init</code> is
specified.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_init">init</code></td>
<td>
<p>Initial <code>query</code> neighbor graph to optimize. If not provided, <code>k</code>
random neighbors are created. If provided, the input format must be one of:
</p>

<ol>
<li><p> A list containing:
</p>

<ul>
<li> <p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> (optional) an <code>n</code> by <code>k</code> matrix containing the nearest neighbor
distances.
</p>
</li></ul>

<p>If <code>k</code> and <code>init</code> are specified as arguments to this function, and the
number of neighbors provided in <code>init</code> is not equal to <code>k</code> then:
</p>

<ul>
<li><p> if <code>k</code> is smaller, only the <code>k</code> closest values in <code>init</code> are retained.
</p>
</li>
<li><p> if <code>k</code> is larger, then random neighbors will be chosen to fill <code>init</code> to
the size of <code>k</code>. Note that there is no checking if any of the random
neighbors are duplicates of what is already in <code>init</code> so effectively fewer
than <code>k</code> neighbors may be chosen for some observations under these
circumstances.
</p>
</li></ul>

<p>If the input distances are omitted, they will be calculated for you.
</p>
</li>
<li><p> A random projection forest, such as that returned from <code><a href="#topic+rpf_build">rpf_build()</a></code> or
<code><a href="#topic+rpf_knn">rpf_knn()</a></code> with <code>ret_forest = TRUE</code>.
</p>
</li></ol>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_epsilon">epsilon</code></td>
<td>
<p>Controls trade-off between accuracy and search cost, as
described by Iwasaki and Miyazaki (2018), by specifying a distance
tolerance on whether to explore the neighbors of candidate points. The
larger the value, the more neighbors will be searched. A value of 0.1
allows query-candidate distances to be 10% larger than the current
most-distant neighbor of the query point, 0.2 means 20%, and so on.
Suggested values are between 0-0.5, although this value is highly dependent
on the distribution of distances in the dataset (higher dimensional data
should choose a smaller cutoff). Too large a value of <code>epsilon</code> will result
in the query search approaching brute force comparison. Use this parameter
in conjunction with <code>max_search_fraction</code> and <code><a href="#topic+prepare_search_graph">prepare_search_graph()</a></code> to
prevent excessive run time. Default is 0.1. If you set <code>verbose = TRUE</code>,
statistics of the number of distance calculations will be logged which
can help you tune <code>epsilon</code>.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_max_search_fraction">max_search_fraction</code></td>
<td>
<p>Maximum fraction of the reference data to search.
This is a value between 0 (search none of the reference data) and 1 (search
all of the data if necessary). This works in conjunction with <code>epsilon</code> and
will terminate the search early if the specified fraction of the reference
data has been searched. Default is 1.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably the
only reason to set this to <code>FALSE</code> is if you suspect that some sort of
numeric issue is occurring with your data in the alternative code path. If
a search forest is used for initialization via the <code>init</code> parameter, then
the metric is fetched from there and this setting is ignored.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="graph_knn_query_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>query</code> and <code>reference</code>
orientation stores each observation as a column (the orientation must be
consistent). The default <code>"R"</code> means that observations are stored in each
row. Storing the data by row is usually more convenient, but internally
your data will be converted to column storage. Passing it already
column-oriented will save some memory and (a small amount of) CPU usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A greedy beam search is used to query the graph, combining two search pruning
strategies. The first, due to Iwasaki and Miyazaki (2018), only considers
new candidates within a relative distance of the current furthest neighbor
in the query's graph. The second, due to Harwood and Drummond (2016), puts a
limit on the absolute number of distance calculations to carry out. See the
<code>epsilon</code> and <code>max_search_fraction</code> parameters respectively.
</p>


<h3>Value</h3>

<p>the approximate nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> a <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices
specifying the row of the neighbor in <code>reference</code>.
</p>
</li>
<li> <p><code>dist</code> a <code>n</code> by <code>k</code> matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>References</h3>

<p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).
</p>
<p>Iwasaki, M., &amp; Miyazaki, D. (2018).
Optimization of indexing based on k-nearest neighbor graph for proximity
search in high-dimensional data.
<em>arXiv preprint arXiv:1810.07355</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 100 reference iris items
iris_ref &lt;- iris[iris$Species %in% c("setosa", "versicolor"), ]

# 50 query items
iris_query &lt;- iris[iris$Species == "versicolor", ]

# First, find the approximate 4-nearest neighbor graph for the references:
iris_ref_graph &lt;- nnd_knn(iris_ref, k = 4)

# For each item in iris_query find the 4 nearest neighbors in iris_ref.
# You need to pass both the reference data and the reference graph.
# If you pass a data frame, non-numeric columns are removed.
# set verbose = TRUE to get details on the progress being made
iris_query_nn &lt;- graph_knn_query(iris_query, iris_ref, iris_ref_graph,
  k = 4, metric = "euclidean", verbose = TRUE
)

# A more complete example, converting the initial knn into a search graph
# and using a filtered random projection forest to initialize the search
# create initial knn and forest
iris_ref_graph &lt;- nnd_knn(iris_ref, k = 4, init = "tree", ret_forest = TRUE)
# keep the best tree in the forest
forest &lt;- rpf_filter(iris_ref_graph, n_trees = 1)
# expand the knn into a search graph
iris_ref_search_graph &lt;- prepare_search_graph(iris_ref, iris_ref_graph)
# run the query with the improved graph and initialization
iris_query_nn &lt;- graph_knn_query(iris_query, iris_ref, iris_ref_search_graph,
  init = forest, k = 4
)

</code></pre>

<hr>
<h2 id='k_occur'>Quantify hubness of a nearest neighbor graph</h2><span id='topic+k_occur'></span>

<h3>Description</h3>

<p><code>k_occur</code> returns a vector of the k-occurrences of a nearest neighbor graph
as defined by  Radovanovic and co-workers (2010). The k-occurrence of an
object is the number of times it occurs among the k-nearest neighbors of
objects in a dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_occur(idx, k = NULL, include_self = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_occur_+3A_idx">idx</code></td>
<td>
<p>integer matrix containing the nearest neighbor indices, integers
labeled starting at 1. Note that the integer labels do <em>not</em> have to
refer to the rows of <code>idx</code>, for example if the nearest neighbor result
is from querying one set of objects with respect to another (for instance
from running <code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>). You may also pass a nearest
neighbor graph object (e.g. the output of running <code><a href="#topic+nnd_knn">nnd_knn()</a></code>),
and the indices will be extracted from it, or a sparse matrix in the same
format as that returned by <code><a href="#topic+prepare_search_graph">prepare_search_graph()</a></code>.</p>
</td></tr>
<tr><td><code id="k_occur_+3A_k">k</code></td>
<td>
<p>The number of closest neighbors to use. Must be between 1 and the
number of columns in <code>idx</code>. By default, all columns of <code>idx</code> are
used. Ignored if <code>idx</code> is sparse.</p>
</td></tr>
<tr><td><code id="k_occur_+3A_include_self">include_self</code></td>
<td>
<p>logical indicating whether the label <code>i</code> in
<code>idx</code> is considered to be a valid neighbor when found in row <code>i</code>.
By default this is <code>TRUE</code>. This can be set to <code>FALSE</code> when the
the labels in <code>idx</code> refer to the row indices of <code>idx</code>, as in the
case of results from <code><a href="#topic+nnd_knn">nnd_knn()</a></code>. In this case you may not want
to consider the trivial case of an object being a neighbor of itself. In
all other cases leave this set to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The k-occurrence can take values between 0 and the size of the dataset. The
larger the k-occurrence for an object, the more &quot;popular&quot; it is. Very large
values of the k-occurrence (much larger than k) indicates that an object is a
&quot;hub&quot; and also implies the existence of &quot;anti-hubs&quot;: objects that never
appear as k-nearest neighbors of other objects.
</p>
<p>The presence of hubs can reduce the accuracy of nearest-neighbor descent and
other approximate nearest neighbor algorithms in terms of retrieving the
exact k-nearest neighbors. However the appearance of hubs can still be
detected in these approximate results, so calculating the k-occurrences for
the output of nearest neighbor descent is a useful diagnostic step.
</p>


<h3>Value</h3>

<p>a vector of length <code>max(idx)</code>, containing the number of times an
object in <code>idx</code> was found in the nearest neighbor list of the objects
represented by the row indices of <code>idx</code>.
</p>


<h3>References</h3>

<p>Radovanovic, M., Nanopoulos, A., &amp; Ivanovic, M. (2010).
Hubs in space: Popular nearest neighbors in high-dimensional data.
<em>Journal of Machine Learning Research</em>, <em>11</em>, 2487-2531.
<a href="https://www.jmlr.org/papers/v11/radovanovic10a.html">https://www.jmlr.org/papers/v11/radovanovic10a.html</a>
</p>
<p>Bratic, B., Houle, M. E., Kurbalija, V., Oria, V., &amp; Radovanovic, M. (2019).
The Influence of Hubness on NN-Descent.
<em>International Journal on Artificial Intelligence Tools</em>, <em>28</em>(06), 1960002.
<a href="https://doi.org/10.1142/S0218213019600029">doi:10.1142/S0218213019600029</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_nbrs &lt;- brute_force_knn(iris, k = 15)
iris_ko &lt;- k_occur(iris_nbrs$idx)
# items 42 and 107 are not in 15 nearest neighbors of any other members of
# iris
which(iris_ko == 1) # they are only their own nearest neighbor
max(iris_ko) # most "popular" item appears on 29 15-nearest neighbor lists
which(iris_ko == max(iris_ko)) # it's iris item 64
# with k = 15, a maximum k-occurrence = 29 ~= 1.9 * k, which is not a cause
# for concern
</code></pre>

<hr>
<h2 id='merge_knn'>Merge multiple approximate nearest neighbors graphs</h2><span id='topic+merge_knn'></span>

<h3>Description</h3>

<p><code>merge_knn</code> takes a list of nearest neighbor graphs and merges them into a
single graph, with the same number of neighbors as the first graph. This is
useful to combine the results of multiple different nearest neighbor
searches: the output will be at least as accurate as the most accurate of the
two input graphs, and ideally will be more accurate than either.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_knn(graphs, is_query = FALSE, n_threads = 0, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_knn_+3A_graphs">graphs</code></td>
<td>
<p>A list of nearest neighbor graphs to merge. Each item in the
list should consist of a sub-list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the k nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing k nearest neighbor distances.
The number of neighbors can differ between graphs, but the merged result
will have the same number of neighbors as the first graph in the list.
</p>
</li></ul>
</td></tr>
<tr><td><code id="merge_knn_+3A_is_query">is_query</code></td>
<td>
<p>If <code>TRUE</code> then the graphs are treated as the result of a knn
query, not a knn building process. Or: is the graph bipartite? This should
be set to <code>TRUE</code> if <code>nn_graphs</code> are the results of using e.g.
<code><a href="#topic+graph_knn_query">graph_knn_query()</a></code> or <code><a href="#topic+random_knn_query">random_knn_query()</a></code>, and set to <code>FALSE</code> if these
are the results of <code><a href="#topic+nnd_knn">nnd_knn()</a></code> or <code><a href="#topic+random_knn">random_knn()</a></code>. The difference is that if
<code>is_query = FALSE</code>, if an index <code>p</code> is found in <code>nn_graph1[i, ]</code>, i.e. <code>p</code>
is a neighbor of <code>i</code> with distance <code>d</code>, then it is assumed that <code>i</code> is a
neighbor of <code>p</code> with the same distance. If <code>is_query = TRUE</code>, then <code>i</code> and
<code>p</code> are indexes into two different datasets and the symmetry does not hold.
If you aren't sure what case applies to you, it's safe (but potentially
inefficient) to set <code>is_query = TRUE</code>.</p>
</td></tr>
<tr><td><code id="merge_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="merge_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the merged nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the merged nearest neighbor distances.
</p>
</li></ul>

<p>The size of <code>k</code> in the output graph is the same as that of the first
item in <code>nn_graphs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1337)
# Nearest neighbor descent with 15 neighbors for iris three times,
# starting from a different random initialization each time
iris_rnn1 &lt;- nnd_knn(iris, k = 15, n_iters = 1)
iris_rnn2 &lt;- nnd_knn(iris, k = 15, n_iters = 1)
iris_rnn3 &lt;- nnd_knn(iris, k = 15, n_iters = 1)

# Merged results should be an improvement over individual results
iris_mnn &lt;- merge_knn(list(iris_rnn1, iris_rnn2, iris_rnn3))
sum(iris_mnn$dist) &lt; sum(iris_rnn1$dist)
sum(iris_mnn$dist) &lt; sum(iris_rnn2$dist)
sum(iris_mnn$dist) &lt; sum(iris_rnn3$dist)
</code></pre>

<hr>
<h2 id='neighbor_overlap'>Overlap between the indices of two nearest neighbor graphs</h2><span id='topic+neighbor_overlap'></span>

<h3>Description</h3>

<p>Calculates the mean average number of neighbors in common between the two
graphs. The per-item overlap can also be returned. This function can be
useful as a measure of accuracy of approximation algorithms, if the
exact nearest neighbors are known, or as a measure of diversity of two
different approximate graphs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neighbor_overlap(idx1, idx2, k = NULL, ret_vec = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neighbor_overlap_+3A_idx1">idx1</code></td>
<td>
<p>Indices of a nearest neighbor graph, i.e. a matrix of nearest
neighbor indices. Can also be a list containing an <code>idx</code> element.</p>
</td></tr>
<tr><td><code id="neighbor_overlap_+3A_idx2">idx2</code></td>
<td>
<p>Indices of a nearest neighbor graph, i.e. a matrix of nearest
neighbor indices. Can also be a list containing an <code>idx</code> element. This is
considered to be the ground truth.</p>
</td></tr>
<tr><td><code id="neighbor_overlap_+3A_k">k</code></td>
<td>
<p>Number of neighbors to consider. If <code>NULL</code>, then the minimum of the
number of neighbors in <code>idx1</code> and <code>idx2</code> is used.</p>
</td></tr>
<tr><td><code id="neighbor_overlap_+3A_ret_vec">ret_vec</code></td>
<td>
<p>If <code>TRUE</code>, also return a vector containing the per-item overlap.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph format is the same as that returned by e.g. <code><a href="#topic+nnd_knn">nnd_knn()</a></code> and should
be of dimensions n by k, where n is the number of points and k is the number
of neighbors. If you pass a neighbor graph directly, the index matrix will be
extracted if present. If the two graphs have different numbers of neighbors,
then the smaller number of neighbors is used.
</p>


<h3>Value</h3>

<p>The mean overlap between <code>idx1</code> and <code>idx2</code>. If <code>ret_vec = TRUE</code>,
then a list containing the mean overlap and the overlap of each item in
is returned with names <code>mean</code> and <code>overlaps</code>, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1337)
# Generate two random neighbor graphs for iris
iris_rnn1 &lt;- random_knn(iris, k = 15)
iris_rnn2 &lt;- random_knn(iris, k = 15)

# Overlap between the two graphs
mean_overlap &lt;- neighbor_overlap(iris_rnn1, iris_rnn2)

# Also get a vector of per-item overlap
overlap_res &lt;- neighbor_overlap(iris_rnn1, iris_rnn2, ret_vec = TRUE)
summary(overlap_res$overlaps)
</code></pre>

<hr>
<h2 id='nnd_knn'>Find nearest neighbors using nearest neighbor descent</h2><span id='topic+nnd_knn'></span>

<h3>Description</h3>

<p>Uses the Nearest Neighbor Descent method due to Dong and co-workers (2011)
to optimize an approximate nearest neighbor graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnd_knn(
  data,
  k = NULL,
  metric = "euclidean",
  init = "rand",
  init_args = NULL,
  n_iters = NULL,
  max_candidates = NULL,
  delta = 0.001,
  low_memory = TRUE,
  weight_by_degree = FALSE,
  use_alt_metric = TRUE,
  n_threads = 0,
  verbose = FALSE,
  progress = "bar",
  obs = "R",
  ret_forest = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnd_knn_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return. Optional if <code>init</code> is
specified.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="nnd_knn_+3A_init">init</code></td>
<td>
<p>Name of the initialization strategy or initial <code>data</code> neighbor
graph to optimize. One of:
</p>

<ul>
<li> <p><code>"rand"</code> random initialization (the default).
</p>
</li>
<li> <p><code>"tree"</code> use the random projection tree method of Dasgupta and Freund
(2008).
</p>
</li>
<li><p> a pre-calculated neighbor graph. A list containing:
</p>

<ul>
<li> <p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> (optional) an <code>n</code> by <code>k</code> matrix containing the nearest
neighbor distances. If the input distances are omitted, they will be
calculated for you.'
</p>
</li></ul>

</li></ul>

<p>If <code>k</code> and <code>init</code> are specified as arguments to this function, and the
number of neighbors provided in <code>init</code> is not equal to <code>k</code> then:
</p>

<ul>
<li><p> if <code>k</code> is smaller, only the <code>k</code> closest values in <code>init</code> are retained.
</p>
</li>
<li><p> if <code>k</code> is larger, then random neighbors will be chosen to fill <code>init</code> to
the size of <code>k</code>. Note that there is no checking if any of the random
neighbors are duplicates of what is already in <code>init</code> so effectively fewer
than <code>k</code> neighbors may be chosen for some observations under these
circumstances.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nnd_knn_+3A_init_args">init_args</code></td>
<td>
<p>a list containing arguments to pass to the random partition
forest initialization. See <code><a href="#topic+rpf_knn">rpf_knn()</a></code> for possible arguments. To avoid
inconsistences with the tree calculation and subsequent nearest neighbor
descent optimization, if you attempt to provide a <code>metric</code> or
<code>use_alt_metric</code> option in this list it will be ignored.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_n_iters">n_iters</code></td>
<td>
<p>Number of iterations of nearest neighbor descent to carry out.
By default, this will be chosen based on the number of observations in
<code>data</code>.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of candidate neighbors to try for each
item in each iteration. Use relative to <code>k</code> to emulate the &quot;rho&quot;
sampling parameter in the nearest neighbor descent paper. By default, this
is set to <code>k</code> or <code>60</code>, whichever is smaller.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_delta">delta</code></td>
<td>
<p>The minimum relative change in the neighbor graph allowed before
early stopping. Should be a value between 0 and 1. The smaller the value,
the smaller the amount of progress between iterations is allowed. Default
value of <code>0.001</code> means that at least 0.1% of the neighbor graph must
be updated at each iteration.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_low_memory">low_memory</code></td>
<td>
<p>If <code>TRUE</code>, use a lower memory, but more
computationally expensive approach to index construction. If set to
<code>FALSE</code>, you should see a noticeable speed improvement, especially
when using a smaller number of threads, so this is worth trying if you have
the memory to spare.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_weight_by_degree">weight_by_degree</code></td>
<td>
<p>If <code>TRUE</code>, then candidates for the local join are
weighted according to their in-degree, so that if there are more than
<code>max_candidates</code> in a candidate list, candidates with a smaller degree are
favored for retention. This prevents items with large numbers of edges
crowding out other items and for high-dimensional data is likely to provide
a small improvement in accuracy. Because this incurs a small extra cost of
counting the degree of each node, and because it tends to delay early
convergence, by default this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_progress">progress</code></td>
<td>
<p>Determines the type of progress information logged if
<code>verbose = TRUE</code>. Options are:
</p>

<ul>
<li> <p><code>"bar"</code>: a simple text progress bar.
</p>
</li>
<li> <p><code>"dist"</code>: the sum of the distances in the approximate knn graph at the
end of each iteration.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nnd_knn_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
<tr><td><code id="nnd_knn_+3A_ret_forest">ret_forest</code></td>
<td>
<p>If <code>TRUE</code> and <code>init = "tree"</code> then the RP forest used to
initialize the nearest neighbors will be returned with the nearest neighbor
data. See the <code>Value</code> section for details. The returned forest can be used
as part of initializing the search for new data: see <code><a href="#topic+rpf_knn_query">rpf_knn_query()</a></code> and
<code><a href="#topic+rpf_filter">rpf_filter()</a></code> for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If no initial graph is provided, a random graph is generated, or you may also
specify the use of a graph generated from a forest of random projection
trees, using the method of Dasgupta and Freund (2008).
</p>


<h3>Value</h3>

<p>the approximate nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li>
<li> <p><code>forest</code> (if <code>init = "tree"</code> and <code>ret_forest = TRUE</code> only): the RP forest
used to initialize the neighbor data.
</p>
</li></ul>



<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In <em>Proceedings of the 20th international conference on World Wide Web</em>
(pp. 577-586).
ACM.
<a href="https://doi.org/10.1145/1963405.1963487">doi:10.1145/1963405.1963487</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Find 4 (approximate) nearest neighbors using Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean")

# Manhattan (l1) distance
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "manhattan")

# Multi-threading: you can choose the number of threads to use: in real
# usage, you will want to set n_threads to at least 2
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "manhattan", n_threads = 1)

# Use verbose flag to see information about progress
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", verbose = TRUE)

# Nearest neighbor descent uses random initialization, but you can pass any
# approximation using the init argument (as long as the metrics used to
# calculate the initialization are compatible with the metric options used
# by nnd_knn).
iris_nn &lt;- random_knn(iris, k = 4, metric = "euclidean")
iris_nn &lt;- nnd_knn(iris, init = iris_nn, metric = "euclidean", verbose = TRUE)

# Number of iterations controls how much optimization is attempted. A smaller
# value will run faster but give poorer results
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 2)

# You can also control the amount of work done within an iteration by
# setting max_candidates
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", max_candidates = 50)

# Optimization may also stop early if not much progress is being made. This
# convergence criterion can be controlled via delta. A larger value will
# stop progress earlier. The verbose flag will provide some information if
# convergence is occurring before all iterations are carried out.
set.seed(1337)
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 5, delta = 0.5)

# To ensure that descent only stops if no improvements are made, set delta = 0
set.seed(1337)
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 5, delta = 0)

# A faster version of the algorithm is available that avoids repeated
# distance calculations at the cost of using more RAM. Set low_memory to
# FALSE to try it.
set.seed(1337)
iris_nn &lt;- nnd_knn(iris, k = 4, metric = "euclidean", low_memory = FALSE)

# Using init = "tree" is usually more efficient than random initialization.
# arguments to the tree initialization method can be passed via the init_args
# list
set.seed(1337)
iris_nn &lt;- nnd_knn(iris, k = 4, init = "tree", init_args = list(n_trees = 5))
</code></pre>

<hr>
<h2 id='prepare_search_graph'>Convert a nearest neighbor graph into a search graph</h2><span id='topic+prepare_search_graph'></span>

<h3>Description</h3>

<p>Create a graph using existing nearest neighbor data to balance search
speed and accuracy using the occlusion pruning and truncation strategies
of Harwood and Drummond (2016). The resulting search graph should be more
efficient for querying new data than the original nearest neighbor graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_search_graph(
  data,
  graph,
  metric = "euclidean",
  use_alt_metric = TRUE,
  diversify_prob = 1,
  pruning_degree_multiplier = 1.5,
  prune_reverse = FALSE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_search_graph_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items, with observations in the rows and features
in the columns. Optionally, input can be passed with observations in the
columns, by setting <code>obs = "C"</code>, which should be more efficient. Possible
formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code> or
<code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code> format.
Dataframes will be converted to <code>numerical</code> matrix format internally, so if
your data columns are <code>logical</code> and intended to be used with the
specialized binary <code>metric</code>s, you should convert it to a logical matrix
first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_graph">graph</code></td>
<td>
<p>neighbor graph for <code>data</code>, a list containing:
</p>

<ul>
<li> <p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices of
the data in <code>data</code>.
</p>
</li>
<li> <p><code>dist</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor distances.
</p>
</li></ul>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_diversify_prob">diversify_prob</code></td>
<td>
<p>the degree of diversification of the search graph
by removing unnecessary edges through occlusion pruning. This should take a
value between <code>0</code> (no diversification) and <code>1</code> (remove as many edges as
possible) and is treated as the probability of a neighbor being removed if
it is found to be an &quot;occlusion&quot;. If item <code>p</code> and <code>q</code>, two members of the
neighbor list of item <code>i</code>, are closer to each other than they are to <code>i</code>,
then the nearer neighbor <code>p</code> is said to &quot;occlude&quot; <code>q</code>. It is likely that
<code>q</code> will be in the neighbor list of <code>p</code> so there is no need to retain it in
the neighbor list of <code>i</code>. You may also set this to <code>NULL</code> to skip any
occlusion pruning. Note that occlusion pruning is carried out twice, once
to the forward neighbors, and once to the reverse neighbors. Reducing this
value will result in a more dense graph. This is similar to increasing the
&quot;alpha&quot; parameter used by in the DiskAnn pruning method of Subramanya and
co-workers (2014).</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_pruning_degree_multiplier">pruning_degree_multiplier</code></td>
<td>
<p>How strongly to truncate the final neighbor
list for each item. The neighbor list of each item will be truncated to
retain only the closest <code>d</code> neighbors, where
<code>d = k * pruning_degree_multiplier</code>, and <code>k</code> is the
original number of neighbors per item in <code>graph</code>. Roughly, values
larger than <code>1</code> will keep all the nearest neighbors of an item, plus
the given fraction of reverse neighbors (if they exist). For example,
setting this to <code>1.5</code> will keep all the forward neighbors and then
half as many of the reverse neighbors, although exactly which neighbors are
retained is also dependent on any occlusion pruning that occurs. Set this
to <code>NULL</code> to skip this step.</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_prune_reverse">prune_reverse</code></td>
<td>
<p>If <code>TRUE</code>, prune the reverse neighbors of each item
before the reverse graph diversification step using
<code>pruning_degree_multiplier</code>. Because the number of reverse neighbors can be
much larger than the number of forward neighbors, this can help to avoid
excessive computation during the diversification step, with little overall
effect on the final search graph. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="prepare_search_graph_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An approximate nearest neighbor graph is not very useful for querying via
<code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>, especially if the query data is initialized randomly:
some items in the data set may not be in the nearest neighbor list of any
other item and can therefore never be returned as a neighbor, no matter how
close they are to the query. Even those which do appear in at least one
neighbor list may not be reachable by expanding an arbitrary starting list if
the neighbor graph contains disconnected components.
</p>
<p>Converting the directed graph represented by the neighbor graph to an
undirected graph by adding an edge from item <code>j</code> to <code>i</code> if
an edge exists from <code>i</code> to <code>j</code> (i.e. creating the mutual neighbor
graph) solves the problems above, but can result in inefficient searches.
Although the out-degree of each item is restricted to the number of neighbors
the in-degree has no such restrictions: a given item could be very &quot;popular&quot;
and in a large number of neighbors lists. Therefore mutualizing the neighbor
graph can result in some items with a large number of neighbors to search.
These usually have very similar neighborhoods so there is nothing to be
gained from searching all of them.
</p>
<p>To balance accuracy and search time, the following procedure is carried out:
</p>

<ol>
<li><p> The graph is &quot;diversified&quot; by occlusion pruning.
</p>
</li>
<li><p> The reverse graph is formed by reversing the direction of all edges in
the pruned graph.
</p>
</li>
<li><p> The reverse graph is diversified by occlusion pruning.
</p>
</li>
<li><p> The pruned forward and pruned reverse graph are merged.
</p>
</li>
<li><p> The outdegree of each node in the merged graph is truncated.
</p>
</li>
<li><p> The truncated merged graph is returned as the prepared search graph.
</p>
</li></ol>

<p>Explicit zero distances in the <code>graph</code> will be converted to a small positive
number to avoid being dropped in the sparse representation. The one exception
is the &quot;self&quot; distance, i.e. any edge in the <code>graph</code> which links a node to
itself (the diagonal of the sparse distance matrix). These trivial edges
aren't useful for search purposes and are always dropped.
</p>


<h3>Value</h3>

<p>a search graph for <code>data</code> based on <code>graph</code>, represented as a sparse
matrix, suitable for use with <code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>.
</p>


<h3>References</h3>

<p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).
</p>
<p>Jayaram Subramanya, S., Devvrit, F., Simhadri, H. V., Krishnawamy, R., &amp; Kadekodi, R. (2019).
Diskann: Fast accurate billion-point nearest neighbor search on a single node.
<em>Advances in Neural Information Processing Systems</em>, <em>32</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 100 reference iris items
iris_ref &lt;- iris[iris$Species %in% c("setosa", "versicolor"), ]

# 50 query items
iris_query &lt;- iris[iris$Species == "versicolor", ]

# First, find the approximate 4-nearest neighbor graph for the references:
ref_ann_graph &lt;- nnd_knn(iris_ref, k = 4)

# Create a graph for querying with
ref_search_graph &lt;- prepare_search_graph(iris_ref, ref_ann_graph)

# Using the search graph rather than the ref_ann_graph directly may give
# more accurate or faster results
iris_query_nn &lt;- graph_knn_query(
  query = iris_query, reference = iris_ref,
  reference_graph = ref_search_graph, k = 4, metric = "euclidean",
  verbose = TRUE
)
</code></pre>

<hr>
<h2 id='random_knn'>Find nearest neighbors by random selection</h2><span id='topic+random_knn'></span>

<h3>Description</h3>

<p>Create a neighbor graph by randomly selecting neighbors. This is not a useful
nearest neighbor method on its own, but can be used with other methods which
require initialization, such as <code><a href="#topic+nnd_knn">nnd_knn()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_knn(
  data,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  order_by_distance = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random_knn_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate random neighbors for, with
observations in the rows and features in the columns. Optionally, input can
be passed with observations in the columns, by setting <code>obs = "C"</code>, which
should be more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>,
<code><a href="base.html#topic+matrix">base::matrix()</a></code> or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in
<code>dgCMatrix</code> format. Dataframes will be converted to <code>numerical</code> matrix
format internally, so if your data columns are <code>logical</code> and intended to be
used with the specialized binary <code>metric</code>s, you should convert it to a
logical matrix first (otherwise you will get the slower dense numerical
version).</p>
</td></tr>
<tr><td><code id="random_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="random_knn_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="random_knn_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="random_knn_+3A_order_by_distance">order_by_distance</code></td>
<td>
<p>If <code>TRUE</code> (the default), then results for each
item are returned by increasing distance. If you don't need the results
sorted, e.g. you are going to pass the results as initialization to another
routine like <code><a href="#topic+nnd_knn">nnd_knn()</a></code>, set this to <code>FALSE</code> to save a small amount of
computational time.</p>
</td></tr>
<tr><td><code id="random_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="random_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="random_knn_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a random neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Find 4 random neighbors and calculate their Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn &lt;- random_knn(iris, k = 4, metric = "euclidean")

# Manhattan (l1) distance
iris_nn &lt;- random_knn(iris, k = 4, metric = "manhattan")

# Multi-threading: you can choose the number of threads to use: in real
# usage, you will want to set n_threads to at least 2
iris_nn &lt;- random_knn(iris, k = 4, metric = "manhattan", n_threads = 1)

# Use verbose flag to see information about progress
iris_nn &lt;- random_knn(iris, k = 4, metric = "euclidean", verbose = TRUE)

# These results can be improved by nearest neighbors descent. You don't need
# to specify k here because this is worked out from the initial input
iris_nn &lt;- nnd_knn(iris, init = iris_nn, metric = "euclidean", verbose = TRUE)
</code></pre>

<hr>
<h2 id='random_knn_query'>Query nearest neighbors by random selection</h2><span id='topic+random_knn_query'></span>

<h3>Description</h3>

<p>Run queries against reference data to return randomly selected neighbors.
This is not a useful query method on its own, but can be used with other
methods which require initialization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_knn_query(
  query,
  reference,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  order_by_distance = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random_knn_query_+3A_query">query</code></td>
<td>
<p>Matrix of <code>n</code> query items, with observations in the rows and
features in the columns. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>reference</code> data must be passed in the same orientation as
<code>query</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_reference">reference</code></td>
<td>
<p>Matrix of <code>m</code> reference items, with observations in the rows
and features in the columns. The nearest neighbors to the queries are
randomly selected from this data. Optionally, the data may be passed with
the observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. The <code>query</code> data must be passed in the same orientation
and format as <code>reference</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>,
<code><a href="base.html#topic+matrix">base::matrix()</a></code> or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in
<code>dgCMatrix</code> format.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="random_knn_query_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_order_by_distance">order_by_distance</code></td>
<td>
<p>If <code>TRUE</code> (the default), then results for each
item are returned by increasing distance. If you don't need the results
sorted, e.g. you are going to pass the results as initialization to another
routine like <code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>, set this to <code>FALSE</code> to save a
small amount of computational time.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="random_knn_query_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>query</code> and <code>reference</code>
orientation stores each observation as a column (the orientation must be
consistent). The default <code>"R"</code> means that observations are stored in each
row. Storing the data by row is usually more convenient, but internally
your data will be converted to column storage. Passing it already
column-oriented will save some memory and (a small amount of) CPU usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an approximate nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># 100 reference iris items
iris_ref &lt;- iris[iris$Species %in% c("setosa", "versicolor"), ]

# 50 query items
iris_query &lt;- iris[iris$Species == "versicolor", ]

# For each item in iris_query find 4 random neighbors in iris_ref
# If you pass a data frame, non-numeric columns are removed
# set verbose = TRUE to get details on the progress being made
iris_query_random_nbrs &lt;- random_knn_query(iris_query,
  reference = iris_ref,
  k = 4, metric = "euclidean", verbose = TRUE
)

# Manhattan (l1) distance
iris_query_random_nbrs &lt;- random_knn_query(iris_query,
  reference = iris_ref,
  k = 4, metric = "manhattan"
)
</code></pre>

<hr>
<h2 id='rnnd_build'>Build approximate nearest neighbors index and neighbor graph</h2><span id='topic+rnnd_build'></span>

<h3>Description</h3>

<p>This function builds an approximate nearest neighbors graph with convenient
defaults, then prepares the index for querying new data, for later use with
<code><a href="#topic+rnnd_query">rnnd_query()</a></code>. For more control over the process, please see the other
functions in the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnnd_build(
  data,
  k = 30,
  metric = "euclidean",
  use_alt_metric = TRUE,
  init = "tree",
  n_trees = NULL,
  leaf_size = NULL,
  max_tree_depth = 200,
  margin = "auto",
  n_iters = NULL,
  delta = 0.001,
  max_candidates = NULL,
  low_memory = TRUE,
  weight_by_degree = FALSE,
  n_search_trees = 1,
  pruning_degree_multiplier = 1.5,
  diversify_prob = 1,
  prune_reverse = FALSE,
  n_threads = 0,
  verbose = FALSE,
  progress = "bar",
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnnd_build_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to build the index for. You can specify
a different number when running <code>rnnd_query</code>, but the index is calibrated
using this value so it's recommended to set <code>k</code> according to the likely
number of neighbors you will want to retrieve. Optional if <code>init</code> is
specified, in which case <code>k</code> can be inferred from the <code>init</code> data. If you
do both, then the specified version of <code>k</code> will take precedence.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_build_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_init">init</code></td>
<td>
<p>Name of the initialization strategy or initial <code>data</code> neighbor
graph to optimize. One of:
</p>

<ul>
<li> <p><code>"rand"</code> random initialization (the default).
</p>
</li>
<li> <p><code>"tree"</code> use the random projection tree method of Dasgupta and Freund
(2008).
</p>
</li>
<li><p> a pre-calculated neighbor graph. A list containing:
</p>

<ul>
<li> <p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> (optional) an <code>n</code> by <code>k</code> matrix containing the nearest
neighbor distances. If the input distances are omitted, they will be
calculated for you.'
</p>
</li></ul>

</li></ul>

<p>If <code>k</code> and <code>init</code> are specified as arguments to this function, and the
number of neighbors provided in <code>init</code> is not equal to <code>k</code> then:
</p>

<ul>
<li><p> if <code>k</code> is smaller, only the <code>k</code> closest values in <code>init</code> are retained.
</p>
</li>
<li><p> if <code>k</code> is larger, then random neighbors will be chosen to fill <code>init</code> to
the size of <code>k</code>. Note that there is no checking if any of the random
neighbors are duplicates of what is already in <code>init</code> so effectively fewer
than <code>k</code> neighbors may be chosen for some observations under these
circumstances.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_build_+3A_n_trees">n_trees</code></td>
<td>
<p>The number of trees to use in the RP forest. A larger number
will give more accurate results at the cost of a longer computation time.
The default of <code>NULL</code> means that the number is chosen based on the number
of observations in <code>data</code>. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_leaf_size">leaf_size</code></td>
<td>
<p>The maximum number of items that can appear in a leaf. This
value should be chosen to match the expected number of neighbors you will
want to retrieve when running queries (e.g. if you want find 50 nearest
neighbors set <code>leaf_size = 50</code>) and should not be set to a value smaller
than <code>10</code>. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_max_tree_depth">max_tree_depth</code></td>
<td>
<p>The maximum depth of the tree to build (default = 200).
If the maximum tree depth is exceeded then the leaf size of a tree may
exceed <code>leaf_size</code> which can result in a large number of neighbor distances
being calculated. If <code>verbose = TRUE</code> a message will be logged to indicate
that the leaf size is large. However, increasing the <code>max_tree_depth</code> may
not help: it may be that there is something unusual about the distribution
of your data set under your chose <code>metric</code> that makes a tree-based
initialization inappropriate. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_margin">margin</code></td>
<td>
<p>A character string specifying the method used to  assign points
to one side of the hyperplane or the other. Possible values are:
</p>

<ul>
<li> <p><code>"explicit"</code> categorizes all distance metrics as either Euclidean or
Angular (Euclidean after normalization), explicitly calculates a hyperplane
and offset, and then calculates the margin based on the dot product with
the hyperplane.
</p>
</li>
<li> <p><code>"implicit"</code> calculates the distance from a point to each of the
points defining the normal vector. The margin is calculated by comparing the
two distances: the point is assigned to the side of the hyperplane that
the normal vector point with the closest distance belongs to.
</p>
</li>
<li> <p><code>"auto"</code> (the default) picks the margin method depending on whether a
binary-specific <code>metric</code> such as <code>"bhammming"</code> is chosen, in which case
<code>"implicit"</code> is used, and <code>"explicit"</code> otherwise: binary-specific metrics
involve storing the data in a way that isn't very efficient for the
<code>"explicit"</code> method and the binary-specific metric is usually a lot faster
than the generic equivalent such that the cost of two distance calculations
for the margin method is still faster.
</p>
</li></ul>

<p>Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_n_iters">n_iters</code></td>
<td>
<p>Number of iterations of nearest neighbor descent to carry out.
By default, this will be chosen based on the number of observations in
<code>data</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_delta">delta</code></td>
<td>
<p>The minimum relative change in the neighbor graph allowed before
early stopping. Should be a value between 0 and 1. The smaller the value,
the smaller the amount of progress between iterations is allowed. Default
value of <code>0.001</code> means that at least 0.1% of the neighbor graph must
be updated at each iteration.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of candidate neighbors to try for each
item in each iteration. Use relative to <code>k</code> to emulate the &quot;rho&quot;
sampling parameter in the nearest neighbor descent paper. By default, this
is set to <code>k</code> or <code>60</code>, whichever is smaller.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_low_memory">low_memory</code></td>
<td>
<p>If <code>TRUE</code>, use a lower memory, but more
computationally expensive approach to index construction. If set to
<code>FALSE</code>, you should see a noticeable speed improvement, especially when
using a smaller number of threads, so this is worth trying if you have the
memory to spare.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_weight_by_degree">weight_by_degree</code></td>
<td>
<p>If <code>TRUE</code>, then candidates for the local join are
weighted according to their in-degree, so that if there are more than
<code>max_candidates</code> in a candidate list, candidates with a smaller degree are
favored for retention. This prevents items with large numbers of edges
crowding out other items and for high-dimensional data is likely to provide
a small improvement in accuracy. Because this incurs a small extra cost of
counting the degree of each node, and because it tends to delay early
convergence, by default this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_n_search_trees">n_search_trees</code></td>
<td>
<p>the number of trees to keep in the search forest as
part of index preparation. The default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_pruning_degree_multiplier">pruning_degree_multiplier</code></td>
<td>
<p>How strongly to truncate the final neighbor
list for each item. The neighbor list of each item will be truncated to
retain only the closest <code>d</code> neighbors, where
<code>d = k * pruning_degree_multiplier</code>, and <code>k</code> is the
original number of neighbors per item in <code>graph</code>. Roughly, values
larger than <code>1</code> will keep all the nearest neighbors of an item, plus
the given fraction of reverse neighbors (if they exist). For example,
setting this to <code>1.5</code> will keep all the forward neighbors and then
half as many of the reverse neighbors, although exactly which neighbors are
retained is also dependent on any occlusion pruning that occurs. Set this
to <code>NULL</code> to skip this step.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_diversify_prob">diversify_prob</code></td>
<td>
<p>the degree of diversification of the search graph
by removing unnecessary edges through occlusion pruning. This should take a
value between <code>0</code> (no diversification) and <code>1</code> (remove as many edges as
possible) and is treated as the probability of a neighbor being removed if
it is found to be an &quot;occlusion&quot;. If item <code>p</code> and <code>q</code>, two members of the
neighbor list of item <code>i</code>, are closer to each other than they are to <code>i</code>,
then the nearer neighbor <code>p</code> is said to &quot;occlude&quot; <code>q</code>. It is likely that
<code>q</code> will be in the neighbor list of <code>p</code> so there is no need to retain it in
the neighbor list of <code>i</code>. You may also set this to <code>NULL</code> to skip any
occlusion pruning. Note that occlusion pruning is carried out twice, once
to the forward neighbors, and once to the reverse neighbors.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_prune_reverse">prune_reverse</code></td>
<td>
<p>If <code>TRUE</code>, prune the reverse neighbors of each item
before the reverse graph diversification step using
<code>pruning_degree_multiplier</code>. Because the number of reverse neighbors can be
much larger than the number of forward neighbors, this can help to avoid
excessive computation during the diversification step, with little overall
effect on the final search graph. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rnnd_build_+3A_progress">progress</code></td>
<td>
<p>Determines the type of progress information logged during the
nearest neighbor descent stage when <code>verbose = TRUE</code>. Options are:
</p>

<ul>
<li> <p><code>"bar"</code>: a simple text progress bar.
</p>
</li>
<li> <p><code>"dist"</code>: the sum of the distances in the approximate knn graph at the
end of each iteration.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_build_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The process of k-nearest neighbor graph construction using Random Projection
Forests (Dasgupta and Freund, 2008) for initialization and Nearest Neighbor
Descent (Dong and co-workers, 2011) for refinement. Index preparation, uses
the graph diversification method of Harwood and Drummond (2016).
</p>


<h3>Value</h3>

<p>the approximate nearest neighbor index, a list containing:
</p>

<ul>
<li> <p><code>graph</code> the k-nearest neighbor graph, a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>

</li>
<li><p> Other list items are intended only for internal use by other functions
such as <code><a href="#topic+rnnd_query">rnnd_query()</a></code>.
</p>
</li></ul>



<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In <em>Proceedings of the 20th international conference on World Wide Web</em>
(pp. 577-586).
ACM.
<a href="https://doi.org/10.1145/1963405.1963487">doi:10.1145/1963405.1963487</a>.
</p>
<p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rnnd_query">rnnd_query()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_even &lt;- iris[seq_len(nrow(iris)) %% 2 == 0, ]
iris_odd &lt;- iris[seq_len(nrow(iris)) %% 2 == 1, ]

# Find 4 (approximate) nearest neighbors using Euclidean distance
iris_even_index &lt;- rnnd_build(iris_even, k = 4)
iris_odd_nbrs &lt;- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)

</code></pre>

<hr>
<h2 id='rnnd_knn'>Find approximate nearest neighbors</h2><span id='topic+rnnd_knn'></span>

<h3>Description</h3>

<p>This function builds an approximate nearest neighbors graph of the provided
data using convenient defaults. It does not return an index for later
querying, to speed the graph construction and reduce the size and complexity
of the return value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnnd_knn(
  data,
  k = 30,
  metric = "euclidean",
  use_alt_metric = TRUE,
  init = "tree",
  n_trees = NULL,
  leaf_size = NULL,
  max_tree_depth = 200,
  margin = "auto",
  n_iters = NULL,
  delta = 0.001,
  max_candidates = NULL,
  weight_by_degree = FALSE,
  low_memory = TRUE,
  n_threads = 0,
  verbose = FALSE,
  progress = "bar",
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnnd_knn_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return. Optional if <code>init</code> is
specified.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_init">init</code></td>
<td>
<p>Name of the initialization strategy or initial <code>data</code> neighbor
graph to optimize. One of:
</p>

<ul>
<li> <p><code>"rand"</code> random initialization (the default).
</p>
</li>
<li> <p><code>"tree"</code> use the random projection tree method of Dasgupta and Freund
(2008).
</p>
</li>
<li><p> a pre-calculated neighbor graph. A list containing:
</p>

<ul>
<li> <p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> (optional) an <code>n</code> by <code>k</code> matrix containing the nearest
neighbor distances. If the input distances are omitted, they will be
calculated for you.'
</p>
</li></ul>

</li></ul>

<p>If <code>k</code> and <code>init</code> are specified as arguments to this function, and the
number of neighbors provided in <code>init</code> is not equal to <code>k</code> then:
</p>

<ul>
<li><p> if <code>k</code> is smaller, only the <code>k</code> closest values in <code>init</code> are retained.
</p>
</li>
<li><p> if <code>k</code> is larger, then random neighbors will be chosen to fill <code>init</code> to
the size of <code>k</code>. Note that there is no checking if any of the random
neighbors are duplicates of what is already in <code>init</code> so effectively fewer
than <code>k</code> neighbors may be chosen for some observations under these
circumstances.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_n_trees">n_trees</code></td>
<td>
<p>The number of trees to use in the RP forest. A larger number
will give more accurate results at the cost of a longer computation time.
The default of <code>NULL</code> means that the number is chosen based on the number
of observations in <code>data</code>. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_leaf_size">leaf_size</code></td>
<td>
<p>The maximum number of items that can appear in a leaf. This
value should be chosen to match the expected number of neighbors you will
want to retrieve when running queries (e.g. if you want find 50 nearest
neighbors set <code>leaf_size = 50</code>) and should not be set to a value smaller
than <code>10</code>. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_max_tree_depth">max_tree_depth</code></td>
<td>
<p>The maximum depth of the tree to build (default = 200).
If the maximum tree depth is exceeded then the leaf size of a tree may
exceed <code>leaf_size</code> which can result in a large number of neighbor distances
being calculated. If <code>verbose = TRUE</code> a message will be logged to indicate
that the leaf size is large. However, increasing the <code>max_tree_depth</code> may
not help: it may be that there is something unusual about the distribution
of your data set under your chose <code>metric</code> that makes a tree-based
initialization inappropriate. Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_margin">margin</code></td>
<td>
<p>A character string specifying the method used to  assign points
to one side of the hyperplane or the other. Possible values are:
</p>

<ul>
<li> <p><code>"explicit"</code> categorizes all distance metrics as either Euclidean or
Angular (Euclidean after normalization), explicitly calculates a hyperplane
and offset, and then calculates the margin based on the dot product with
the hyperplane.
</p>
</li>
<li> <p><code>"implicit"</code> calculates the distance from a point to each of the
points defining the normal vector. The margin is calculated by comparing the
two distances: the point is assigned to the side of the hyperplane that
the normal vector point with the closest distance belongs to.
</p>
</li>
<li> <p><code>"auto"</code> (the default) picks the margin method depending on whether a
binary-specific <code>metric</code> such as <code>"bhammming"</code> is chosen, in which case
<code>"implicit"</code> is used, and <code>"explicit"</code> otherwise: binary-specific metrics
involve storing the data in a way that isn't very efficient for the
<code>"explicit"</code> method and the binary-specific metric is usually a lot faster
than the generic equivalent such that the cost of two distance calculations
for the margin method is still faster.
</p>
</li></ul>

<p>Only used if <code>init = "tree"</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_n_iters">n_iters</code></td>
<td>
<p>Number of iterations of nearest neighbor descent to carry out.
By default, this will be chosen based on the number of observations in
<code>data</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_delta">delta</code></td>
<td>
<p>The minimum relative change in the neighbor graph allowed before
early stopping. Should be a value between 0 and 1. The smaller the value,
the smaller the amount of progress between iterations is allowed. Default
value of <code>0.001</code> means that at least 0.1% of the neighbor graph must
be updated at each iteration.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of candidate neighbors to try for each
item in each iteration. Use relative to <code>k</code> to emulate the &quot;rho&quot;
sampling parameter in the nearest neighbor descent paper. By default, this
is set to <code>k</code> or <code>60</code>, whichever is smaller.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_weight_by_degree">weight_by_degree</code></td>
<td>
<p>If <code>TRUE</code>, then candidates for the local join are
weighted according to their in-degree, so that if there are more than
<code>max_candidates</code> in a candidate list, candidates with a smaller degree are
favored for retention. This prevents items with large numbers of edges
crowding out other items and for high-dimensional data is likely to provide
a small improvement in accuracy. Because this incurs a small extra cost of
counting the degree of each node, and because it tends to delay early
convergence, by default this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_low_memory">low_memory</code></td>
<td>
<p>If <code>TRUE</code>, use a lower memory, but more
computationally expensive approach to index construction. If set to
<code>FALSE</code>, you should see a noticeable speed improvement, especially when
using a smaller number of threads, so this is worth trying if you have the
memory to spare.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_progress">progress</code></td>
<td>
<p>Determines the type of progress information logged during the
nearest neighbor descent stage when <code>verbose = TRUE</code>. Options are:
</p>

<ul>
<li> <p><code>"bar"</code>: a simple text progress bar.
</p>
</li>
<li> <p><code>"dist"</code>: the sum of the distances in the approximate knn graph at the
end of each iteration.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnnd_knn_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The process of k-nearest neighbor graph construction using Random Projection
Forests (Dasgupta and Freund, 2008) for initialization and Nearest Neighbor
Descent (Dong and co-workers, 2011) for refinement. If you are sure you will
not want to query new data then compared to <code><a href="#topic+rnnd_build">rnnd_build()</a></code> this function has
the advantage of not storing the index, which can be very large.
</p>


<h3>Value</h3>

<p>the approximate nearest neighbor index, a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In <em>Proceedings of the 20th international conference on World Wide Web</em>
(pp. 577-586).
ACM.
<a href="https://doi.org/10.1145/1963405.1963487">doi:10.1145/1963405.1963487</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rnnd_build">rnnd_build()</a></code>, <code><a href="#topic+rnnd_query">rnnd_query()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Find 4 (approximate) nearest neighbors using Euclidean distance
iris_knn &lt;- rnnd_knn(iris, k = 4)

</code></pre>

<hr>
<h2 id='rnnd_query'>Query an index for approximate nearest neighbors</h2><span id='topic+rnnd_query'></span>

<h3>Description</h3>

<p>Takes a nearest neighbor index produced by <code><a href="#topic+rnnd_build">rnnd_build()</a></code> and uses it to
find the nearest neighbors of a query set of observations, using a
back-tracking search with the search size determined by the method of
Iwasaki and Miyazaki (2018). For further control over the search effort, the
total number of distance calculations can also be bounded, similar to the
method of Harwood and Drummond (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnnd_query(
  index,
  query,
  k = 30,
  epsilon = 0.1,
  max_search_fraction = 1,
  init = NULL,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnnd_query_+3A_index">index</code></td>
<td>
<p>A nearest neighbor index produced by <code><a href="#topic+rnnd_build">rnnd_build()</a></code>.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_query">query</code></td>
<td>
<p>Matrix of <code>n</code> query items, with observations in the rows and
features in the columns. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).
Sparse and non-sparse data cannot be mixed, so if the data used to build
index was sparse, the <code>query</code> data must also be sparse. and vice versa.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_epsilon">epsilon</code></td>
<td>
<p>Controls trade-off between accuracy and search cost, as
described by Iwasaki and Miyazaki (2018). Setting <code>epsilon</code> to a positive
value specifies a distance tolerance on whether to explore the neighbors of
candidate points. The larger the value, the more neighbors will be
searched. A value of 0.1 allows query-candidate distances to be 10% larger
than the current most-distant neighbor of the query point, 0.2 means 20%,
and so on. Suggested values are between 0-0.5, although this value is
highly dependent on the distribution of distances in the dataset (higher
dimensional data should choose a smaller cutoff). Too large a value of
<code>epsilon</code> will result in the query search approaching brute force
comparison. Use this parameter in conjunction with <code>max_search_fraction</code> to
prevent excessive run time. Default is 0.1. If you set <code>verbose = TRUE</code>,
statistics of the number of distance calculations will be logged which can
help you tune <code>epsilon</code>.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_max_search_fraction">max_search_fraction</code></td>
<td>
<p>Maximum fraction of the reference data to search.
This is a value between 0 (search none of the reference data) and 1 (search
all of the data if necessary). This works in conjunction with <code>epsilon</code> and
will terminate the search early if the specified fraction of the reference
data has been searched. Default is 1.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_init">init</code></td>
<td>
<p>An optional matrix of <code>k</code> initial nearest neighbors for each
query point.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rnnd_query_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the approximate nearest neighbor index, a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>



<h3>References</h3>

<p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).
</p>
<p>Iwasaki, M., &amp; Miyazaki, D. (2018).
Optimization of indexing based on k-nearest neighbor graph for proximity search in high-dimensional data.
<em>arXiv preprint</em> <em>arXiv:1810.07355</em>.
<a href="https://arxiv.org/abs/1810.07355">https://arxiv.org/abs/1810.07355</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rnnd_query">rnnd_query()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_even &lt;- iris[seq_len(nrow(iris)) %% 2 == 0, ]
iris_odd &lt;- iris[seq_len(nrow(iris)) %% 2 == 1, ]

iris_even_index &lt;- rnnd_build(iris_even, k = 4)
iris_odd_nbrs &lt;- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)

</code></pre>

<hr>
<h2 id='rpf_build'>Create a random projection forest nearest neighbor index</h2><span id='topic+rpf_build'></span>

<h3>Description</h3>

<p>Builds a &quot;forest&quot; of Random Projection Trees (Dasgupta and Freund, 2008),
which can later be searched to find approximate nearest neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpf_build(
  data,
  metric = "euclidean",
  use_alt_metric = TRUE,
  n_trees = NULL,
  leaf_size = 10,
  max_tree_depth = 200,
  margin = "auto",
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpf_build_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate the index for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>Note that if <code>margin = "explicit"</code>, the metric is only used to determine
whether an &quot;angular&quot; or &quot;Euclidean&quot; distance is used to measure the
distance between split points in the tree.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>). Probably the only reason to set this to <code>FALSE</code> is
if you suspect that some sort of numeric issue is occurring with your data
in the alternative code path. Only applies if the implicit <code>margin</code> method
is used.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_n_trees">n_trees</code></td>
<td>
<p>The number of trees to use in the RP forest. A larger number
will give more accurate results at the cost of a longer computation time.
The default of <code>NULL</code> means that the number is chosen based on the number
of observations in <code>data</code>.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_leaf_size">leaf_size</code></td>
<td>
<p>The maximum number of items that can appear in a leaf. This
value should be chosen to match the expected number of neighbors you will
want to retrieve when running queries (e.g. if you want find 50 nearest
neighbors set <code>leaf_size = 50</code>) and should not be set to a value smaller
than <code>10</code>.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_max_tree_depth">max_tree_depth</code></td>
<td>
<p>The maximum depth of the tree to build (default = 200).
If the maximum tree depth is exceeded then the leaf size of a tree may
exceed <code>leaf_size</code> which can result in a large number of neighbor distances
being calculated. If <code>verbose = TRUE</code> a message will be logged to indicate
that the leaf size is large. However, increasing the <code>max_tree_depth</code> may
not help: it may be that there is something unusual about the distribution
of your data set under your chose <code>metric</code> that makes a tree-based
initialization inappropriate.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_margin">margin</code></td>
<td>
<p>A character string specifying the method used to  assign points
to one side of the hyperplane or the other. Possible values are:
</p>

<ul>
<li> <p><code>"explicit"</code> categorizes all distance metrics as either Euclidean or
Angular (Euclidean after normalization), explicitly calculates a hyperplane
and offset, and then calculates the margin based on the dot product with
the hyperplane.
</p>
</li>
<li> <p><code>"implicit"</code> calculates the distance from a point to each of the
points defining the normal vector. The margin is calculated by comparing the
two distances: the point is assigned to the side of the hyperplane that
the normal vector point with the closest distance belongs to.
</p>
</li>
<li> <p><code>"auto"</code> (the default) picks the margin method depending on whether a
binary-specific <code>metric</code> such as <code>"bhammming"</code> is chosen, in which case
<code>"implicit"</code> is used, and <code>"explicit"</code> otherwise: binary-specific metrics
involve storing the data in a way that isn't very efficient for the
<code>"explicit"</code> method and the binary-specific metric is usually a lot faster
than the generic equivalent such that the cost of two distance calculations
for the margin method is still faster.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rpf_build_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rpf_build_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a forest of random projection trees as a list. Each tree in the
forest is a further list, but is not intended to be examined or manipulated
by the user. As a normal R data type, it can be safely serialized and
deserialized with <code><a href="base.html#topic+readRDS">base::saveRDS()</a></code> and <code><a href="base.html#topic+readRDS">base::readRDS()</a></code>. To use it for
querying pass it as the <code>forest</code> parameter of <code><a href="#topic+rpf_knn_query">rpf_knn_query()</a></code>. The forest
does not store any of the <code>data</code> passed into build the tree, so if you
are going to search the forest, you will also need to store the <code>data</code> used
to build it and provide it during the search.
</p>


<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpf_knn_query">rpf_knn_query()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a forest of 10 trees from the odd rows
iris_odd &lt;- iris[seq_len(nrow(iris)) %% 2 == 1, ]
iris_odd_forest &lt;- rpf_build(iris_odd, n_trees = 10)

iris_even &lt;- iris[seq_len(nrow(iris)) %% 2 == 0, ]
iris_even_nn &lt;- rpf_knn_query(
  query = iris_even, reference = iris_odd,
  forest = iris_odd_forest, k = 15
)
</code></pre>

<hr>
<h2 id='rpf_filter'>Keep the best trees in a random projection forest</h2><span id='topic+rpf_filter'></span>

<h3>Description</h3>

<p>Reduce the size of a random projection forest, by scoring each tree against
a k-nearest neighbors graph. Only the top N trees will be retained which
allows for a faster querying.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpf_filter(nn, forest = NULL, n_trees = 1, n_threads = 0, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpf_filter_+3A_nn">nn</code></td>
<td>
<p>Nearest neighbor data in the dense list format. This should be
derived from the same data that was used to build the <code>forest</code>.</p>
</td></tr>
<tr><td><code id="rpf_filter_+3A_forest">forest</code></td>
<td>
<p>A random partition forest, e.g. created by <code><a href="#topic+rpf_build">rpf_build()</a></code>,
representing partitions of the same underlying data reflected in <code>nn</code>.
As a convenient, this parameter is ignored if the <code>nn</code> list contains a
<code>forest</code> entry, e.g. from running <code><a href="#topic+rpf_knn">rpf_knn()</a></code> or <code><a href="#topic+nnd_knn">nnd_knn()</a></code> with
<code>ret_forest = TRUE</code>, and the forest value will be extracted from <code>nn</code>.</p>
</td></tr>
<tr><td><code id="rpf_filter_+3A_n_trees">n_trees</code></td>
<td>
<p>The number of trees to retain. By default only the
best-scoring tree is retained.</p>
</td></tr>
<tr><td><code id="rpf_filter_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rpf_filter_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trees are scored based on how well each leaf reflects the neighbors as
specified in the nearest neighbor data. It's best to use as accurate nearest
neighbor data as you can and it does not need to come directly from
searching the <code>forest</code>: for example, the nearest neighbor data from running
<code><a href="#topic+nnd_knn">nnd_knn()</a></code> to optimize the neighbor data output from an RP Forest is a
good choice.
</p>
<p>Rather than rely on an RP Forest solely for approximate nearest neighbor
querying, it is probably more cost-effective to use a small number of trees
to initialize the neighbor list for use in a graph search via
<code><a href="#topic+graph_knn_query">graph_knn_query()</a></code>.
</p>


<h3>Value</h3>

<p>A forest with the best scoring <code>n_trees</code> trees.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpf_build">rpf_build()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a knn with a forest of 10 trees using the odd rows
iris_odd &lt;- iris[seq_len(nrow(iris)) %% 2 == 1, ]
# also return the forest with the knn
rfknn &lt;- rpf_knn(iris_odd, k = 15, n_trees = 10, ret_forest = TRUE)

# keep the best 2 trees:
iris_odd_filtered_forest &lt;- rpf_filter(rfknn)

# get some new data to search
iris_even &lt;- iris[seq_len(nrow(iris)) %% 2 == 0, ]

# search with the filtered forest
iris_even_nn &lt;- rpf_knn_query(
  query = iris_even, reference = iris_odd,
  forest = iris_odd_filtered_forest, k = 15
)
</code></pre>

<hr>
<h2 id='rpf_knn'>Find nearest neighbors using a random projection forest</h2><span id='topic+rpf_knn'></span>

<h3>Description</h3>

<p>Returns the approximate k-nearest neighbor graph of a dataset by searching
multiple random projection trees, a variant of k-d trees originated by
Dasgupta and Freund (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpf_knn(
  data,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  n_trees = NULL,
  leaf_size = NULL,
  max_tree_depth = 200,
  include_self = TRUE,
  ret_forest = FALSE,
  margin = "auto",
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpf_knn_+3A_data">data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting <code>obs = "C"</code>, which should be
more efficient. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return. Optional if <code>init</code> is
specified.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_metric">metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li></ul>

<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li></ul>

<p>Note that if <code>margin = "explicit"</code>, the metric is only used to determine
whether an &quot;angular&quot; or &quot;Euclidean&quot; distance is used to measure the
distance between split points in the tree.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_use_alt_metric">use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_n_trees">n_trees</code></td>
<td>
<p>The number of trees to use in the RP forest. A larger number
will give more accurate results at the cost of a longer computation time.
The default of <code>NULL</code> means that the number is chosen based on the number
of observations in <code>data</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_leaf_size">leaf_size</code></td>
<td>
<p>The maximum number of items that can appear in a leaf. The
default of <code>NULL</code> means that the number of leaves is chosen based on the
number of requested neighbors <code>k</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_max_tree_depth">max_tree_depth</code></td>
<td>
<p>The maximum depth of the tree to build (default = 200).
If the maximum tree depth is exceeded then the leaf size of a tree may
exceed <code>leaf_size</code> which can result in a large number of neighbor distances
being calculated. If <code>verbose = TRUE</code> a message will be logged to indicate
that the leaf size is large. However, increasing the <code>max_tree_depth</code> may
not help: it may be that there is something unusual about the distribution
of your data set under your chose <code>metric</code> that makes a tree-based
initialization inappropriate.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_include_self">include_self</code></td>
<td>
<p>If <code>TRUE</code> (the default) then an item is considered to
be a neighbor of itself. Hence the first nearest neighbor in the results
will be the item itself. This is a convention that many nearest neighbor
methods and software adopt, so if you want to use the resulting knn graph
from this function in downstream applications or compare with other
methods, you should probably keep this set to <code>TRUE</code>. However, if you are
planning on using the result of this as initialization to another nearest
neighbor method (e.g. <code><a href="#topic+nnd_knn">nnd_knn()</a></code>), then set this to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_ret_forest">ret_forest</code></td>
<td>
<p>If <code>TRUE</code> also return a search forest which can be used
for future querying (via <code><a href="#topic+rpf_knn_query">rpf_knn_query()</a></code>) and filtering
(via <code><a href="#topic+rpf_filter">rpf_filter()</a></code>). By default this is <code>FALSE</code>. Setting this to <code>TRUE</code>
will change the output list to be nested (see the <code>Value</code> section below).</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_margin">margin</code></td>
<td>
<p>A character string specifying the method used to  assign points
to one side of the hyperplane or the other. Possible values are:
</p>

<ul>
<li> <p><code>"explicit"</code> categorizes all distance metrics as either Euclidean or
Angular (Euclidean after normalization), explicitly calculates a hyperplane
and offset, and then calculates the margin based on the dot product with
the hyperplane.
</p>
</li>
<li> <p><code>"implicit"</code> calculates the distance from a point to each of the
points defining the normal vector. The margin is calculated by comparing the
two distances: the point is assigned to the side of the hyperplane that
the normal vector point with the closest distance belongs to.
</p>
</li>
<li> <p><code>"auto"</code> (the default) picks the margin method depending on whether a
binary-specific <code>metric</code> such as <code>"bhammming"</code> is chosen, in which case
<code>"implicit"</code> is used, and <code>"explicit"</code> otherwise: binary-specific metrics
involve storing the data in a way that isn't very efficient for the
<code>"explicit"</code> method and the binary-specific metric is usually a lot faster
than the generic equivalent such that the cost of two distance calculations
for the margin method is still faster.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rpf_knn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rpf_knn_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the approximate nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li>
<li> <p><code>forest</code> (if <code>ret_forest = TRUE</code>) the RP forest that generated the
neighbor graph, which can be used to query new data.
</p>
</li></ul>

<p><code>k</code> neighbors per observation are not guaranteed to be found. Missing data
is represented with an index of <code>0</code> and a distance of <code>NA</code>.
</p>


<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpf_filter">rpf_filter()</a></code>, <code><a href="#topic+nnd_knn">nnd_knn()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Find 4 (approximate) nearest neighbors using Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn &lt;- rpf_knn(iris, k = 4, metric = "euclidean", leaf_size = 3)

# If you want to initialize another method (e.g. nearest neighbor descent)
# with the result of the RP forest, then it's more efficient to skip
# evaluating whether an item is a neighbor of itself by setting
# `include_self = FALSE`:
iris_rp &lt;- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE)
# for future querying you may want to also return the RP forest:
iris_rpf &lt;- rpf_knn(iris,
  k = 4, n_trees = 3, include_self = FALSE,
  ret_forest = TRUE
)
</code></pre>

<hr>
<h2 id='rpf_knn_query'>Query a random projection forest index for nearest neighbors</h2><span id='topic+rpf_knn_query'></span>

<h3>Description</h3>

<p>Run queries against a &quot;forest&quot; of Random Projection Trees (Dasgupta and
Freund, 2008), to return nearest neighbors taken from the reference data used
to build the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpf_knn_query(
  query,
  reference,
  forest,
  k,
  cache = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpf_knn_query_+3A_query">query</code></td>
<td>
<p>Matrix of <code>n</code> query items, with observations in the rows and
features in the columns. Optionally, the data may be passed with the
observations in the columns, by setting <code>obs = "C"</code>, which should be more
efficient. The <code>reference</code> data must be passed in the same orientation as
<code>query</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code>
or <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code>
format. Dataframes will be converted to <code>numerical</code> matrix format
internally, so if your data columns are <code>logical</code> and intended to be used
with the specialized binary <code>metric</code>s, you should convert it to a logical
matrix first (otherwise you will get the slower dense numerical version).</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_reference">reference</code></td>
<td>
<p>Matrix of <code>m</code> reference items, with observations in the rows
and features in the columns. The nearest neighbors to the queries are
calculated from this data and should be the same data used to build the
<code>forest</code>. Optionally, the data may be passed with the observations in the
columns, by setting <code>obs = "C"</code>, which should be more efficient. The
<code>query</code> data must be passed in the same format and orientation as
<code>reference</code>. Possible formats are <code><a href="base.html#topic+data.frame">base::data.frame()</a></code>, <code><a href="base.html#topic+matrix">base::matrix()</a></code> or
<code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code> format.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_forest">forest</code></td>
<td>
<p>A random partition forest, created by <code><a href="#topic+rpf_build">rpf_build()</a></code>,
representing partitions of the data in <code>reference</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to return. You are unlikely to get good
results if you choose a value substantially larger than the value of
<code>leaf_size</code> used to build the <code>forest</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_cache">cache</code></td>
<td>
<p>if <code>TRUE</code> (the default) then candidate indices found in the
leaves of the forest are cached to avoid recalculating the same distance
repeatedly. This incurs an extra memory cost which scales with <code>n_threads</code>.
Set this to <code>FALSE</code> to disable distance caching.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use. Note that the parallelism in the
search is done over the observations in <code>query</code> not the trees in the
<code>forest</code>. Thus a single observation will not see any speed-up from
increasing <code>n_threads</code>.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td></tr>
<tr><td><code id="rpf_knn_query_+3A_obs">obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the approximate nearest neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li></ul>

<p><code>k</code> neighbors per observation are not guaranteed to be found. Missing data
is represented with an index of <code>0</code> and a distance of <code>NA</code>.
</p>


<h3>References</h3>

<p>Dasgupta, S., &amp; Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In <em>Proceedings of the fortieth annual ACM symposium on Theory of computing</em>
(pp. 537-546).
<a href="https://doi.org/10.1145/1374376.1374452">doi:10.1145/1374376.1374452</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpf_build">rpf_build()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a forest of 10 trees from the odd rows
iris_odd &lt;- iris[seq_len(nrow(iris)) %% 2 == 1, ]
iris_odd_forest &lt;- rpf_build(iris_odd, n_trees = 10)

iris_even &lt;- iris[seq_len(nrow(iris)) %% 2 == 0, ]
iris_even_nn &lt;- rpf_knn_query(
  query = iris_even, reference = iris_odd,
  forest = iris_odd_forest, k = 15
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
