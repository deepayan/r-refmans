<!DOCTYPE html><html><head><title>Help for package Biocomb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Biocomb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Biocomb-package'>
<p>Tools for Data Mining</p></a></li>
<li><a href='#CalcGene'>
<p>Calculate HUM value</p></a></li>
<li><a href='#CalcROC'>
<p>Calculate ROC points</p></a></li>
<li><a href='#Calculate3D'>
<p>Plot the 3D-ROC curve</p></a></li>
<li><a href='#CalculateHUM_Ex'>
<p>Calculate HUM value</p></a></li>
<li><a href='#CalculateHUM_Plot'>
<p>Plot 2D-ROC curve</p></a></li>
<li><a href='#CalculateHUM_ROC'>
<p>Compute the points for ROC curve</p></a></li>
<li><a href='#CalculateHUM_seq'>
<p>Calculate HUM value</p></a></li>
<li><a href='#chi2.algorithm'>
<p>Select the subset of features</p></a></li>
<li><a href='#classifier.loop'>
<p>Classification and classifier validation</p></a></li>
<li><a href='#compute.auc.permutation'>
<p>Calculates the p-values</p></a></li>
<li><a href='#compute.auc.random'>
<p>Calculates the p-values</p></a></li>
<li><a href='#compute.aucs'>
<p>Ranks the features</p></a></li>
<li><a href='#cost.curve'>
<p>Plots the RCC curve for two-class problem</p></a></li>
<li><a href='#data_test'>
<p>simulated data</p></a></li>
<li><a href='#datasetF6'>
<p>simulated data</p></a></li>
<li><a href='#generate.data.miss'>
<p>Generate the dataset with missing values</p></a></li>
<li><a href='#input_miss'>
<p>Process the dataset with missing values</p></a></li>
<li><a href='#leukemia_miss'>
<p>desease data</p></a></li>
<li><a href='#leukemia72'>
<p>desease data</p></a></li>
<li><a href='#leukemia72_2'>
<p>desease data</p></a></li>
<li><a href='#pauc'>
<p>Calculates the p-values</p></a></li>
<li><a href='#pauclog'>
<p>Calculates the p-values</p></a></li>
<li><a href='#plotClass.result'>
<p>Plots the results of classifier validation schemes</p></a></li>
<li><a href='#plotRoc.curves'>
<p>Plots the ROC curve for two-class problem</p></a></li>
<li><a href='#ProcessData'>
<p>Select the subset of features</p></a></li>
<li><a href='#select.cfs'>
<p>Select the subset of features</p></a></li>
<li><a href='#select.fast.filter'>
<p>Select the subset of features</p></a></li>
<li><a href='#select.forward.Corr'>
<p>Select the subset of features</p></a></li>
<li><a href='#select.forward.wrapper'>
<p>Select the subset of features</p></a></li>
<li><a href='#select.inf.chi2'>
<p>Ranks the features</p></a></li>
<li><a href='#select.inf.gain'>
<p>Ranks the features</p></a></li>
<li><a href='#select.inf.symm'>
<p>Ranks the features</p></a></li>
<li><a href='#select.process'>
<p>Feature ranking and feature selection</p></a></li>
<li><a href='#select.relief'>
<p>Ranks the features</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Feature Selection and Classification with the Embedded
Validation Procedures for Biomedical Data Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-05-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Natalia Novoselova,Junxi Wang,Frank Pessler,Frank Klawonn</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Natalia Novoselova &lt;novos65@mail.ru&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains functions for the data analysis with the emphasis on biological data, including several algorithms for feature ranking, feature selection, classification
 algorithms with the embedded validation procedures.
 The functions can deal with numerical as well as with nominal features. Includes also the functions for calculation
 of feature AUC (Area Under the ROC Curve) and HUM (hypervolume under manifold) values and construction 2D- and 3D- ROC curves.
 Provides the calculation of Area Above the RCC (AAC) values and construction of Relative Cost Curves
 (RCC) to estimate the classifier performance under unequal misclassification costs problem.
 There exists the special function to deal with missing values, including different imputing schemes.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>rgl, MASS, e1071, randomForest, pROC, ROCR, arules, pamr,
class, nnet, rpart, FSelector, RWeka, grDevices, graphics,
stats, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.13.0),gtools,Rcpp (&ge; 0.12.1)</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-18 18:13:37 UTC; novosel</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-18 18:33:55 UTC</td>
</tr>
</table>
<hr>
<h2 id='Biocomb-package'>
Tools for Data Mining
</h2><span id='topic+Biocomb-package'></span><span id='topic+Biocomb'></span>

<h3>Description</h3>

<p>Functions to make the data analysis with the emphasis on biological data.
They can deal with both numerical and nominal features. Biocomb includes functions for several feature ranking, feature selection algorithms. The feature ranking is based on several criteria: information gain, symmetrical uncertainty, chi-squared statistic etc. There are a number of features selection algorithms: Chi2 algorithm, based on chi-squared test,  fast correlation-based filter algorithm, feature weighting algorithm (RelieF), sequential forward search algorithm (CorrSF), Correlation-based feature selection algorithm (CFS). Package includes several classification algorithms with embedded feature selection and validation schemes. It includes also the functions for calculation of feature AUC (Area Under the ROC Curve) values with statistical significance analysis, calculation of Area Above the RCC (AAC) values. For two- and multi-class problems it is possible to use functions for HUM (hypervolume under manifold) calculation and construction 2D- and 3D- ROC curves. Relative Cost Curves (RCC) are provided to estimate the classifier performance under unequal misclassification costs.<br />
Biocomb has a special function to deal with missing values, including different imputing schemes.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> Biocomb</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2016-08-14</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 3)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Biocomb package presents the functions for two stages of data mining process: feature selection and classification. One of the main functions of Biocomb is the <code><a href="#topic+select.process">select.process</a></code> function. It presents the infrostructure to perform the feature ranking or feature selection for the data set with two or more class labels. Functions <code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+select.inf.gain">select.inf.gain</a></code>, <code><a href="#topic+select.inf.symm">select.inf.symm</a></code> and <code><a href="#topic+select.inf.chi2">select.inf.chi2</a></code> calculate the different criterion measure for each feature in the dataset. Function <code><a href="#topic+select.fast.filter">select.fast.filter</a></code> realizes the fast correlation-based filter method. Function <code><a href="#topic+chi2.algorithm">chi2.algorithm</a></code> performes Chi2 discretization algorithms with feature selection. Function <code><a href="#topic+select.forward.Corr">select.forward.Corr</a></code> is designed for the sequential forward features search according to the correlation measure. Function <code><a href="#topic+select.forward.wrapper">select.forward.wrapper</a></code> is the realization of the wrapper feature selection method with sequential forward search strategy.
The auxiliary function  <code><a href="#topic+ProcessData">ProcessData</a></code> performs the discretization of the numerical features and is called from the several functions for feature selection.
The second main function of the Biocomb is <code><a href="#topic+classifier.loop">classifier.loop</a></code> which presents the infrastructure for the classifier construction with the embedded feature selection and using the different schemes for the performance validation.
The functions <code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+compute.auc.permutation">compute.auc.permutation</a></code>, <code><a href="#topic+pauc">pauc</a></code>, <code><a href="#topic+pauclog">pauclog</a></code>, <code><a href="#topic+compute.auc.random">compute.auc.random</a></code> are the functions for calculation of feature AUC (Area Under  the ROC Curve) values with statistical significance analysis.
The functions <code><a href="#topic+plotRoc.curves">plotRoc.curves</a></code> is assigned for the construction of the ROC curve in 2D-space.
The functions <code><a href="#topic+cost.curve">cost.curve</a></code> plots the RCC and calculates the corresponding AAC to estimate the classifier performance under unequal misclassification costs problem.
The function <code><a href="#topic+input_miss">input_miss</a></code> deals with missing value problem and realizes the two methods of missing value imputing.
The function <code><a href="#topic+generate.data.miss">generate.data.miss</a></code> allows to generate the dataset with missing values from the input dataset in order to test the algorithms, which are designed to deal with missing values problem.
The functions <code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>, <code><a href="#topic+CalculateHUM_Plot">CalculateHUM_Plot</a></code> are for HUM calculation and construction 2D- and 3D- ROC curves.
</p>


<h3>Function</h3>


<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.process">select.process</a></code> </td><td style="text-align: left;"> Perform the features ranking or features selection</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+compute.aucs">compute.aucs</a></code> </td><td style="text-align: left;"> Calculate the AUC values </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.inf.gain">select.inf.gain</a></code> </td><td style="text-align: left;"> Calculate the Information Gain criterion </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.inf.symm">select.inf.symm</a></code> </td><td style="text-align: left;"> Calculate the Symmetrical uncertainty criterion</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.inf.chi2">select.inf.chi2</a></code> </td><td style="text-align: left;"> Calculate the chi-squared statistic</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.fast.filter">select.fast.filter</a></code> </td><td style="text-align: left;"> Select the feature subset with fast correlation-based filter method</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+chi2.algorithm">chi2.algorithm</a></code> </td><td style="text-align: left;"> Select the feature subset with Chi2 discretization algorithm.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.forward.Corr">select.forward.Corr</a></code> </td><td style="text-align: left;"> Select the feature subset with forward search strategy and correlation measure</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+select.forward.wrapper">select.forward.wrapper</a></code> </td><td style="text-align: left;"> Select the feature subset with a wrapper method</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+ProcessData">ProcessData</a></code> </td><td style="text-align: left;"> Perform the discretization of the numerical features</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+classifier.loop">classifier.loop</a></code> </td><td style="text-align: left;"> Perform the classification with the embedded feature selection</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+pauc">pauc</a></code> </td><td style="text-align: left;"> Calculate the p-values of the statistical significance of the two-class difference</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+pauclog">pauclog</a></code> </td><td style="text-align: left;"> Calculate the logarithm of p-values of the statistical significance</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+compute.auc.permutation">compute.auc.permutation</a></code> </td><td style="text-align: left;"> Compute the p-value of the significance of the AUC using the permutation test</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+compute.auc.random">compute.auc.random</a></code> </td><td style="text-align: left;"> Compute the p-value of the significance of the AUC using random sample generation</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+plotRoc.curves">plotRoc.curves</a></code> </td><td style="text-align: left;"> Plot the ROC curve in 2D-space </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code> </td><td style="text-align: left;"> Calculate a maximal HUM value and the corresponding permutation of class labels</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code> </td><td style="text-align: left;"> Calculate the HUM values with exaustive serach for specified number of class labels </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code> </td><td style="text-align: left;"> Function to construct and plot the 2D- or 3d-ROC curve </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalcGene">CalcGene</a></code> </td><td style="text-align: left;"> Compute the HUM value for one feature </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalcROC">CalcROC</a></code> </td><td style="text-align: left;"> Compute the point coordinates to plot the 2D- or 3D-ROC curve </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CalculateHUM_Plot">CalculateHUM_Plot</a></code> </td><td style="text-align: left;"> Plot the 2D-ROC curve </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Calculate3D">Calculate3D</a></code> </td><td style="text-align: left;"> Plot the 3D-ROC curve </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cost.curve">cost.curve</a></code> </td><td style="text-align: left;"> Plot the RCC and calculate the AAC for unequal misclassification costs</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+input_miss">input_miss</a></code> </td><td style="text-align: left;"> Perform the missing values imputation </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+generate.data.miss">generate.data.miss</a></code> </td><td style="text-align: left;"> Generate the dataset with missing values </td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Dataset</h3>

<p>This package comes with two simulated datasets and a real dataset of leukemia patients with 72 cases and 101 features. The last feature is the class (disease labels).
</p>


<h3>Installing and using</h3>

<p>To install this package, make sure you are connected to the internet and issue the following command in the R prompt:
</p>
<pre>
    install.packages("Biocomb")
  </pre>
<p>To load the package in R:
</p>
<pre>
    library(Biocomb)
  </pre>


<h3>Author(s)</h3>

<p>Natalia Novoselova, Junxi Wang,Frank Pessler,Frank Klawonn
</p>
<p>Maintainer: Natalia Novoselova &lt;novos65@mail.ru&gt;
</p>


<h3>References</h3>

<p>H. Liu and L. Yu. &quot;Toward Integrating Feature Selection Algorithms for Classification and Clustering&quot;, IEEE Trans. on Knowledge and Data Engineering, pdf, 17(4), 491-502, 2005.<br />
L. Yu and H. Liu. &quot;Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution&quot;. In Proceedings of The Twentieth International Conference on Machine Leaning (ICML-03), Washington, D.C. pp. 856-863. August 21-24, 2003.<br />
Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification?A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.<br />
Olga Montvida and Frank Klawonn Relative cost curves: An alternative to AUC and an extension to 3-class problems,Kybernetika 50 no. 5, 647-660, 2014
</p>


<h3>See Also</h3>

<p>CRAN packages <span class="pkg">arules</span> or <span class="pkg">discretization</span>
for feature discretization.
CRAN packages <span class="pkg">pROC</span> for ROC curves.
CRAN packages <span class="pkg">FSelector</span> for chi-squared test, forward search strategy.
CRAN packages <span class="pkg">pamr</span> for nearest shrunken centroid classifier,
CRAN packages <span class="pkg">MASS</span>, <span class="pkg">e1071</span>, <span class="pkg">randomForest</span>,<span class="pkg">class</span>,
<span class="pkg">nnet</span>, <span class="pkg">rpart</span> are used in this package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_test)
# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

# Perform the feature selection using the fast correlation-based filter algorithm
disc="MDL"
threshold=0.2
attrs.nominal=numeric()
out=select.fast.filter(data_test,disc.method=disc,threshold=threshold,
attrs.nominal=attrs.nominal)

# Perform the classification with cross-validation of results
out=classifier.loop(data_test,classifiers=c("svm","lda","rf"),
 feature.selection="auc", flag.feature=FALSE,method.cross="fold-crossval")

# Calculate the coordinates for 2D- or 3D- ROC curve and the optimal threshold point
## Not run: data(data_test)
xllim&lt;--4
xulim&lt;-4
yllim&lt;-30
yulim&lt;-110

attrs.no=1
pos.Class&lt;-levels(data_test[,ncol(data_test)])[1]
add.legend&lt;-TRUE

aacs&lt;-rep(0,length(attrs.no))
color&lt;-c(1:length(attrs.no))

out &lt;- cost.curve(data_test,attrs.no, pos.Class,col=color[1],add=F,
 xlim=c(xllim,xulim),ylim=c(yllim,yulim))

## End(Not run)
</code></pre>

<hr>
<h2 id='CalcGene'>
Calculate HUM value
</h2><span id='topic+CalcGene'></span>

<h3>Description</h3>

<p>This is the auxiliary function of the Biocomb package. It computes a HUM value for individual feature and returns a &ldquo;List&rdquo; object, consisting of HUM value and the best permutation of class labels in &ldquo;seq&rdquo; vector. This &ldquo;seq&rdquo; vector can be passed to the function <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcGene(s_data, seqAll, prodValue,thresholds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcGene_+3A_s_data">s_data</code></td>
<td>
<p>a list, which contains the vectors of sorted feature values for individual class labels.
</p>
</td></tr>
<tr><td><code id="CalcGene_+3A_seqall">seqAll</code></td>
<td>
<p>a numeric matrix of all the permutations of the class labels, where each row corresponds to individual permutation vector.
</p>
</td></tr>
<tr><td><code id="CalcGene_+3A_prodvalue">prodValue</code></td>
<td>
<p>a numeric value, which is the product of the sizes of feature vectors, corresponding to analized class labels.
</p>
</td></tr>
<tr><td><code id="CalcGene_+3A_thresholds">thresholds</code></td>
<td>
<p>a numeric vector, containing the values of thresholds for calculating ROC curve coordinates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to compute the maximal HUM value between the all possible permutations of class labels for individual feature, selected for analysis. See the &ldquo;Value&rdquo; section to this page for more details.
</p>


<h3>Value</h3>

<p>The data must be provided without missing values in order to process. A returned list consists of the following fields:
</p>
<table>
<tr><td><code>HUM</code></td>
<td>
<p>a list of HUM values for the specified number of analyzed features</p>
</td></tr>
<tr><td><code>seq</code></td>
<td>
<p>a list of vectors, each containing the sequence of class labels</p>
</td></tr>
</table>


<h3>Errors</h3>

<p>If there exists NA values for features or class labels no HUM value can be calculated and an error is triggered with
message &ldquo;Values are missing&rdquo;.
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72
indexF=3
indexClass=ncol(xdata)
label=levels(xdata[,indexClass])

indexLabel=label[1:2]

indexL=NULL
for(i in 1:length(indexLabel))
{
  indexL=c(indexL,which(label==indexLabel[i]))
}

indexEach=NULL
indexUnion=NULL
for(i in 1:length(label))
{
  vrem=which(xdata[,indexClass]==label[i])
  indexEach=c(indexEach,list(vrem))
  if(length(intersect(label[i],indexLabel))==1)
  indexUnion=union(indexUnion,vrem)
}

s_data=NULL
dataV=xdata[,indexF]
prodValue=1

for (j in 1:length(indexLabel))
{
  vrem=sort(dataV[indexEach[[indexL[j]]]])
  s_data=c(s_data,list(vrem))
  prodValue = prodValue*length(vrem)
}

len=length(indexLabel)
seq=permutations(len,len,1:len)

#claculate the threshold values
thresholds &lt;- sort(unique(dataV[indexUnion]))
thresholds=(c(-Inf, thresholds) + c(thresholds, +Inf))/2

out=CalcGene(s_data,seq,prodValue,thresholds)
</code></pre>

<hr>
<h2 id='CalcROC'>
Calculate ROC points
</h2><span id='topic+CalcROC'></span>

<h3>Description</h3>

<p>This is the auxiliary function of the Biocomb package. It computes a point coordinates for plotting ROC curve and returns a &ldquo;List&rdquo; object,  consisting of sensitivity and specificity values for 2D-ROC curve and 3D-points for 3D-ROC curve, the optimal threshold values with the corresponding feature values and the accuracy of the classifier (feature).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcROC(s_data, seq, thresholds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcROC_+3A_s_data">s_data</code></td>
<td>
<p>a list, which contains the vectors of sorted feature values for individual class labels.
</p>
</td></tr>
<tr><td><code id="CalcROC_+3A_seq">seq</code></td>
<td>
<p>a numeric vector, containing the particular permutation of class labels.
</p>
</td></tr>
<tr><td><code id="CalcROC_+3A_thresholds">thresholds</code></td>
<td>
<p>a numeric vector, containing the values of thresholds for calculating ROC curve coordinates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to compute the point coordinates to plot the 2D- or 3D-ROC curve, the optimal threshold values and the accuracy of classifier. See the &ldquo;Value&rdquo; section to this page for more details. The optimal threshold for two-class problem is the pair of sensitivity and specificity values for the selected feature. The optimal threshold for three-class problem is the 3D-point with the coordinates presenting the fraction of the correctly classified data objects for each class. The calculation of the optimal threshold is described in section &ldquo;Threshold&rdquo;.
</p>


<h3>Value</h3>

<p>The data must be provided without missing values in order to process. A returned list consists of the following fields:
</p>
<table>
<tr><td><code>Sn</code></td>
<td>
<p>a specificity values for 2D-ROC construction and the first coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>Sp</code></td>
<td>
<p>a sensitivity values for 2D-ROC construction and the second coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>S3</code></td>
<td>
<p>the third coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optSn</code></td>
<td>
<p>the optimal specificity value for 2D-ROC construction and the first coordinate of the op-timal threshold for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optSp</code></td>
<td>
<p>the optimal sensitivity value for 2D-ROC construction and the second coordinate of the optimal threshold for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optS3</code></td>
<td>
<p>the third coordinate of the optimal threshold for 3D-ROC construction
</p>
</td></tr>
<tr><td><code>optThre</code></td>
<td>
<p>the feature value according to the optimal threshold (optSn,optSp) for two-class problem</p>
</td></tr>
<tr><td><code>optThre1</code></td>
<td>
<p>the first feature value according to the optimal threshold (optSn,optSp,optS3) for three-class problem</p>
</td></tr>
<tr><td><code>optThre2</code></td>
<td>
<p>the second feature value according to the optimal threshold (optSn,optSp,optS3) for three-class problem</p>
</td></tr>
<tr><td><code>accuracy</code></td>
<td>
<p>the accuracy of classifier (feature) for the optimal threshold</p>
</td></tr>
</table>


<h3>Threshold</h3>

<p>The optimal threshold value is calculated for two-class problem as the pair &ldquo;(optSn, optSp)&rdquo; corresponding to the maximal value of &ldquo;Sn+Sp&rdquo;. According to &ldquo;(optSn, optSp)&rdquo; the corresponding feature threshold value &ldquo;optThre&rdquo; is calculated.
The optimal threshold value is calculated for three-class problem as the pair &ldquo;(optSn, optSp,optS3)&rdquo; corresponding to the maximal value of &ldquo;Sn+Sp+S3&rdquo;.According to &ldquo;(optSn, optSp,optS3)&rdquo; the corresponding feature threshold values &ldquo;optThre1,optThre2&rdquo; are calculated.
The accuracy of the classifier is the mean value of dQuote(optSn, optSp) for two-class problem and the mean value of &ldquo;(optSn, optSp,optS3)&rdquo; for three-class problem.
</p>


<h3>Errors</h3>

<p>If there exists NA values for features or class labels no HUM value can be calculated and an error is triggered with
message &ldquo;Values are missing&rdquo;.
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72_2)
# Basic example
# class label must be factor
leukemia72_2[,ncol(leukemia72_2)]&lt;-as.factor(leukemia72_2[,ncol(leukemia72_2)])

xdata=leukemia72_2
indexF=1:3
indexClass=ncol(xdata)

label=levels(xdata[,indexClass])
indexLabel=label

out=CalculateHUM_seq(xdata,indexF,indexClass,indexLabel)

HUM&lt;-out$HUM
seq&lt;-out$seq

indexL=NULL
for(i in 1:length(indexLabel))
{
  indexL=c(indexL,which(label==indexLabel[i]))
}

indexEach=NULL
indexUnion=NULL

for(i in 1:length(label))
{
  vrem=which(xdata[,indexClass]==label[i])
  indexEach=c(indexEach,list(vrem))
  if(length(intersect(label[i],indexLabel))==1)
    indexUnion=union(indexUnion,vrem)
}
s_data=NULL
dataV=xdata[,indexF[1]]  #single feature
prodValue=1
for (j in 1:length(indexLabel))
{
  vrem=sort(dataV[indexEach[[indexL[j]]]])

  s_data=c(s_data,list(vrem))
  prodValue = prodValue*length(vrem)
}
#calculate the threshold values for plot of 2D ROC and 3D ROC
thresholds &lt;- sort(unique(dataV[indexUnion]))
thresholds=(c(-Inf, thresholds) + c(thresholds, +Inf))/2

out=CalcROC(s_data,seq[,indexF[1]], thresholds)
</code></pre>

<hr>
<h2 id='Calculate3D'>
Plot the 3D-ROC curve
</h2><span id='topic+Calculate3D'></span>

<h3>Description</h3>

<p>This function plots the 3D-ROC curve using the point coordinates, computed by the function <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>. Optionally visualizes the optimal threshold point, which gives the maximal accuracy of the classifier(feature) (see <code><a href="#topic+CalcROC">CalcROC</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Calculate3D(sel,Sn,Sp,S3,optSn,optSp,optS3,thresholds,HUM,
name,print.optim=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Calculate3D_+3A_sel">sel</code></td>
<td>
<p>a character value, which is the name of the selected feature.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_sn">Sn</code></td>
<td>
<p>a numeric vector of the x-coordinates of the ROC curve..
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_sp">Sp</code></td>
<td>
<p>a numeric vector of the y-coordinates of the ROC curve.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_s3">S3</code></td>
<td>
<p>a numeric vector of the z-coordinates of the ROC curve.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_optsn">optSn</code></td>
<td>
<p>the first coordinate of the optimal threshold
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_optsp">optSp</code></td>
<td>
<p>the second coordinate of the optimal threshold
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_opts3">optS3</code></td>
<td>
<p>the third coordinate of the optimal threshold
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_thresholds">thresholds</code></td>
<td>
<p>a numeric vector with threshold values to calculate point coordinates.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_hum">HUM</code></td>
<td>
<p>a numeric vector of HUM values, calculated using function.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_name">name</code></td>
<td>
<p>a character vector of class labels.
</p>
</td></tr>
<tr><td><code id="Calculate3D_+3A_print.optim">print.optim</code></td>
<td>
<p>a boolean parameter to plot the optimal threshold point on the graph. The default value is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to plot the 3D-ROC curve according to the given point coordinates.
</p>


<h3>Value</h3>

<p>The function doesn't return any value.
</p>


<h3>Errors</h3>

<p>If there exists NA values for specificity or sensitivity values, or HUM values the plotting fails and an error is triggered with message &ldquo;Values are missing&rdquo;
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.<br />
Natalia Novoselova, Cristina Della Beffa, Junxi Wang, Jialiang Li, Frank Pessler, Frank Klawonn. HUM Calculator and HUM package for R: easy-to-use software tools for multicategory receiver operating characteristic analysis» / Bioinformatics. – 2014. – Vol. 30 (11): 1635-1636 doi:10.1093/ bioinformatics/btu086.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72

indexF=names(xdata)[10]

indexClass=ncol(xdata)
label=levels(xdata[,indexClass])
indexLabel=label

out=CalculateHUM_seq(xdata,indexF,indexClass,indexLabel)
HUM&lt;-out$HUM
seq&lt;-out$seq
out=CalculateHUM_ROC(xdata,indexF,indexClass,indexLabel,seq)
Calculate3D(indexF,out$Sn,out$Sp,out$S3,out$optSn,out$optSp,out$optS3,
out$thresholds,HUM,indexLabel[seq])
</code></pre>

<hr>
<h2 id='CalculateHUM_Ex'>
Calculate HUM value
</h2><span id='topic+CalculateHUM_Ex'></span>

<h3>Description</h3>

<p>This function calculates the HUM (hypervolume under manifold) feature values with the exhaustive search, i.e. for all combinations of the defined number of categories from the whole set of available categories. The function is used for ranking the features (in decreasing order of HUM values). HUM values are the extension of the AUC values for more than two classes.
It can handle only numerical values.
It computes a HUM value and returns a &ldquo;List&rdquo; object, consisting of HUM value and the best permutation of class labels in &ldquo;seq&rdquo; vector. This &ldquo;seq&rdquo; vector can be passed to the function <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code> for the calculating the coordinates of the 2D or 3D ROC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalculateHUM_Ex(data,indexF,indexClass,allLabel,amountL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalculateHUM_Ex_+3A_data">data</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the additional column with class labels is provided. Class labels could be numerical or character values. The maximal number of classes is ten. The <code>indexClass</code> determines the column with class labels.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Ex_+3A_indexf">indexF</code></td>
<td>
<p>a numeric or character vector, containing the column numbers or column names of the analyzed features.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Ex_+3A_indexclass">indexClass</code></td>
<td>
<p>a numeric or character value, containing the column number or column name of the class labels.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Ex_+3A_alllabel">allLabel</code></td>
<td>
<p>a character vector, containing the column names of the class labels, selected for the analysis.</p>
</td></tr>
<tr><td><code id="CalculateHUM_Ex_+3A_amountl">amountL</code></td>
<td>
<p>a number of the class categories to search for all the possible combinations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to compute the maximal HUM value between the all possible permutations of class labels, selected for analysis. See the
&ldquo;Value&rdquo; section to this page for more details. Before
returning, it will call the <code><a href="#topic+CalcGene">CalcGene</a></code> function to calculate the HUM value for each feature (object)..
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the separate column contains class labels. The maximal number of class labels equals 10. The computational efficiency of the function descrease in the case of more than 1000 cases with more than 6 class labels.
In order to use all the functions of the package  it is necessary to put the class label in the last column of the dataset. The class label features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned list consists of th the following fields:
</p>
<table>
<tr><td><code>HUM</code></td>
<td>
<p>a list of HUM values for the specified number of analyzed features</p>
</td></tr>
<tr><td><code>seq</code></td>
<td>
<p>a list of vectors, each containing the sequence of class labels</p>
</td></tr>
</table>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.<br />
Natalia Novoselova, Cristina Della Beffa, Junxi Wang, Jialiang Li, Frank Pessler, Frank Klawonn. HUM Calculator and HUM package for R: easy-to-use software tools for multicategory receiver operating characteristic analysis» / Bioinformatics. – 2014. – Vol. 30 (11): 1635-1636 doi:10.1093/ bioinformatics/btu086.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72
indexF=c(1:2)
indexClass=ncol(xdata)
allLabel=levels(xdata[,indexClass])

amountL=2
out=CalculateHUM_Ex(xdata,indexF,indexClass,allLabel,amountL)
</code></pre>

<hr>
<h2 id='CalculateHUM_Plot'>
Plot 2D-ROC curve
</h2><span id='topic+CalculateHUM_Plot'></span>

<h3>Description</h3>

<p>This function plots the 2D-ROC curve using the point coordinates, computed by the function <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>.Optionally visualizes the optimal threshold point, which gives the maximal accuracy of the classifier(feature) (see <code><a href="#topic+CalcROC">CalcROC</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalculateHUM_Plot(sel,Sn,Sp,optSn,optSp,HUM,print.optim=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalculateHUM_Plot_+3A_sel">sel</code></td>
<td>
<p>a character value, which is the name of the selected feature.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_sn">Sn</code></td>
<td>
<p>a numeric vector of the x-coordinates of the ROC curve, which is the specificity values of the standard ROC analysis..
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_sp">Sp</code></td>
<td>
<p>a numeric vector of the y-coordinates of the ROC curve, which is the sensitivity values of the standard ROC analysis..
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_optsn">optSn</code></td>
<td>
<p>the optimal specificity value for 2D-ROC construction
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_optsp">optSp</code></td>
<td>
<p>the optimal sensitivity value for 2D-ROC construction
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_hum">HUM</code></td>
<td>
<p>a numeric vector of HUM values, calculated using function <code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code> or <code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code>.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_Plot_+3A_print.optim">print.optim</code></td>
<td>
<p>a boolean parameter to plot the optimal threshold point on the graph. The default value is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to plot the 2D-ROC curve according to the given point coordinates.
</p>


<h3>Value</h3>

<p>The function doesn't return any value.
</p>


<h3>Errors</h3>

<p>If there exists NA values for specificity or sensitivity values, or HUM values the plotting fails and an error is triggered with message &ldquo;Values are missing&rdquo;.
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.<br />
Natalia Novoselova, Cristina Della Beffa, Junxi Wang, Jialiang Li, Frank Pessler, Frank Klawonn. HUM Calculator and HUM package for R: easy-to-use software tools for multicategory receiver operating characteristic analysis» / Bioinformatics. – 2014. – Vol. 30 (11): 1635-1636 doi:10.1093/ bioinformatics/btu086.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72

indexF=names(xdata)[3]

indexClass=ncol(xdata)
label=levels(xdata[,indexClass])
indexLabel=label[1:2]

out=CalculateHUM_seq(xdata,indexF,indexClass,indexLabel)
HUM&lt;-out$HUM
seq&lt;-out$seq
out=CalculateHUM_ROC(xdata,indexF,indexClass,indexLabel,seq)

CalculateHUM_Plot(indexF,out$Sn,out$Sp,out$optSn,out$optSp,HUM)
</code></pre>

<hr>
<h2 id='CalculateHUM_ROC'>
Compute the points for ROC curve
</h2><span id='topic+CalculateHUM_ROC'></span>

<h3>Description</h3>

<p>This is the  function for computing the points for ROC curve. It returns a &ldquo;List&rdquo; object, consisting of sensitivity and specificity values for 2D-ROC curve and 3D-points for 3D-ROC curve. Also the optimal threshold values are returned.
It can handle only numerical values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalculateHUM_ROC(data,indexF,indexClass,indexLabel,seq)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalculateHUM_ROC_+3A_data">data</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the additional column with class labels is provided. Class labels could be numerical or character values. The maximal number of classes is ten. The <code>indexClass</code> determines the column with class labels.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_ROC_+3A_indexf">indexF</code></td>
<td>
<p>a numeric or character vector, containing the column numbers or column names of the analyzed features.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_ROC_+3A_indexclass">indexClass</code></td>
<td>
<p>a numeric or character value, containing the column number or column name of the class labels.
</p>
</td></tr>
<tr><td><code id="CalculateHUM_ROC_+3A_indexlabel">indexLabel</code></td>
<td>
<p>a character vector, containing the column names of the class labels, selected for the analysis.</p>
</td></tr>
<tr><td><code id="CalculateHUM_ROC_+3A_seq">seq</code></td>
<td>
<p>a numeric matrix, containing the permutation of the class labels for all features.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to compute the point coordinates to plot the 2D- or 3D-ROC curve and the optimal threshold values. See the &ldquo;Value&rdquo; section to this page for more details. The function calls the <code><a href="#topic+CalcROC">CalcROC</a></code> to calculate the point coordinates, optimal thresholds and accuracy of classifier (feature) in the threshold.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the separate column contains class labels. The maximal number of class labels equals 10.
In order to use all the functions of the package  it is necessary to put the class label in the last column of the dataset. The class label features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data must be provided without missing values in order to process. A returned list consists of th the following fields:
</p>
<table>
<tr><td><code>Sn</code></td>
<td>
<p>a specificity values for 2D-ROC construction and the first coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>Sp</code></td>
<td>
<p>a sensitivity values for 2D-ROC construction and the second coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>S3</code></td>
<td>
<p>the third coordinate for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optSn</code></td>
<td>
<p>the optimal specificity value for 2D-ROC construction and the first coordinate of the op-timal threshold for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optSp</code></td>
<td>
<p>the optimal sensitivity value for 2D-ROC construction and the second coordinate of the optimal threshold for 3D-ROC construction</p>
</td></tr>
<tr><td><code>optS3</code></td>
<td>
<p>the third coordinate of the optimal threshold for 3D-ROC construction</p>
</td></tr>
</table>


<h3>Errors</h3>

<p>If there exists NA values for features or class labels no HUM value can be calculated and an error is triggered with
message &ldquo;Values are missing&rdquo;.
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.<br />
Natalia Novoselova, Cristina Della Beffa, Junxi Wang, Jialiang Li, Frank Pessler, Frank Klawonn. HUM Calculator and HUM package for R: easy-to-use software tools for multicategory receiver operating characteristic analysis» / Bioinformatics. – 2014. – Vol. 30 (11): 1635-1636 doi:10.1093/ bioinformatics/btu086.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code>, <code><a href="#topic+CalculateHUM_seq">CalculateHUM_seq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72
indexF=1:3
indexClass=ncol(xdata)
label=levels(xdata[,indexClass])
indexLabel=label[1:2]

out=CalculateHUM_seq(xdata,indexF,indexClass,indexLabel)
HUM&lt;-out$HUM
seq&lt;-out$seq
out=CalculateHUM_ROC(xdata,indexF,indexClass,indexLabel,seq)
</code></pre>

<hr>
<h2 id='CalculateHUM_seq'>
Calculate HUM value
</h2><span id='topic+CalculateHUM_seq'></span>

<h3>Description</h3>

<p>This function calculates the features weights using the HUM (hypervolume under manifold) values criterion measure and is used for ranking the features (in decreasing order of HUM values). HUM values are the extension of the AUC values for more than two classes.
It can handle only numerical values.
It computes a HUM value and returns a &ldquo;List&rdquo; object, consisting of HUM value and the best permutation of class labels in &ldquo;seq&rdquo; vector. This &ldquo;seq&rdquo; vector can be passed to the function <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code> for the calculating the coordinates of the 2D or 3D ROC.
This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;HUM&rdquo; for feature selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalculateHUM_seq(data,indexF,indexClass,indexLabel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalculateHUM_seq_+3A_data">data</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the additional column with class labels is provided. Class labels could be numerical or character values. The maximal number of classes is ten. The <code>indexClass</code> determines the column with class labels.</p>
</td></tr>
<tr><td><code id="CalculateHUM_seq_+3A_indexf">indexF</code></td>
<td>
<p>a numeric or character vector, containing the column numbers or column names of the analyzed features.</p>
</td></tr>
<tr><td><code id="CalculateHUM_seq_+3A_indexclass">indexClass</code></td>
<td>
<p>a numeric or character value, containing the column number or column name of the class labels.</p>
</td></tr>
<tr><td><code id="CalculateHUM_seq_+3A_indexlabel">indexLabel</code></td>
<td>
<p>a character vector, containing the column names of the class labels, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to compute the maximal HUM value between the all possible permutations of class labels, selected for analysis. See the
&ldquo;Value&rdquo; section to this page for more details. Before
returning, it will call the <code><a href="#topic+CalcGene">CalcGene</a></code> function to calculate the HUM value for each feature (object).
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the separate column contains class labels. The maximal number of class labels equals 10. The computational efficiency of the function descrease in the case of more than 1000 cases with more than 6 class labels.
In order to use all the functions of the package  it is necessary to put the class label in the last column of the dataset.The class label features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned list consists of th the following fields:
</p>
<table>
<tr><td><code>HUM</code></td>
<td>
<p>a list of HUM values for the specified number of analyzed features</p>
</td></tr>
<tr><td><code>seq</code></td>
<td>
<p>a list of vectors, each containing the sequence of class labels</p>
</td></tr>
</table>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC Analysis with Multiple Tests and Multiple Classes: methodology and its application in microarray studies.<em>Biostatistics</em>. 9 (3): 566-576.<br />
Natalia Novoselova, Cristina Della Beffa, Junxi Wang, Jialiang Li, Frank Pessler, Frank Klawonn. HUM Calculator and HUM package for R: easy-to-use software tools for multicategory receiver operating characteristic analysis» / Bioinformatics. – 2014. – Vol. 30 (11): 1635-1636 doi:10.1093/ bioinformatics/btu086.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CalculateHUM_Ex">CalculateHUM_Ex</a></code>, <code><a href="#topic+CalculateHUM_ROC">CalculateHUM_ROC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leukemia72)
# Basic example
# class label must be factor
leukemia72[,ncol(leukemia72)]&lt;-as.factor(leukemia72[,ncol(leukemia72)])

xdata=leukemia72
indexF=1:2
indexClass=ncol(xdata)
label=levels(xdata[,indexClass])
indexLabel=label[1:2]

out=CalculateHUM_seq(xdata,indexF,indexClass,indexLabel)

</code></pre>

<hr>
<h2 id='chi2.algorithm'>
Select the subset of features
</h2><span id='topic+chi2.algorithm'></span>

<h3>Description</h3>

<p>This function selects the subset of features on the basis of the Chi2 discretization algorithm. The algorithm provides the way to select numerical features while discretizing them. It is based on the <code class="reqn">\chi^2</code> statistic, and consists of two phases of discretization. According to the value of <code class="reqn">\chi^2</code> statistic for each pair of adjacent intervals the merging of the intervals continues until an inconsistency rate is exceeded. Chi2 algorithms automatically determines a proper <code class="reqn">\chi^2</code> threshold that keeps the fidelity of the original data. The nominal features must be determined as they didn't take part in the discretization process but in the process of inconsistency rate calculation. In the process of discretization the irrelevant features are removed. The results is in the form of &ldquo;list&rdquo;, consisting of two fields: the processed dataset without irrelevant features and the names of the selected features. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;Chi2-algorithm&rdquo; for feature selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chi2.algorithm(matrix,attrs.nominal,threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chi2.algorithm_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="chi2.algorithm_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
<tr><td><code id="chi2.algorithm_+3A_threshold">threshold</code></td>
<td>
<p>a numeric threshold value for the inconsistency rate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to select the subset of informative numerical features using the two phase process of feature values merging according to the <code class="reqn">\chi^2</code> statistic for the pairs of adjacent intervals. The stopping criterion of merging is the inconsistency rate of the processed dataset. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned data.frame consists of the the following fields:
</p>
<table>
<tr><td><code>data.out</code></td>
<td>
<p>the processed dataset without irrelevant features (features which have been<br /> merged into a single interval)</p>
</td></tr>
<tr><td><code>subset</code></td>
<td>
<p>a character vector of the selected feature names</p>
</td></tr>
</table>


<h3>References</h3>

<p>H. Liu and L. Yu. &quot;Toward Integrating Feature Selection Algorithms for Classification and Clustering&quot;, IEEE Trans. on Knowledge and Data Engineering, pdf, 17(4), 491-502, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
#p1=Sys.time()
data(data_test)
# not all features to select
xdata=data_test[,c(1:6,ncol(data_test))]
# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])
attrs.nominal=numeric()
threshold=0
out=chi2.algorithm(matrix=xdata,attrs.nominal=attrs.nominal,threshold=threshold)
#Sys.time()-p1
</code></pre>

<hr>
<h2 id='classifier.loop'>
Classification and classifier validation
</h2><span id='topic+classifier.loop'></span>

<h3>Description</h3>

<p>The main function for the classification and classifier validation. It performs the classification using different classification algorithms (classifiers) with the embedded feature selection and using the different schemes for the performance validation.<br />
It presents the infrostructure to perform classification of the data set with two or more class labels. The function calls several classification methods, including Nearest shrunken centroid (&quot;nsc&quot;), Naive Bayes classifier (&quot;nbc&quot;), Nearest Neighbour classifier (&quot;nn&quot;), Multinomial Logistic Regression (&quot;mlr&quot;), Support Vector Machines (&quot;svm&quot;), Linear Discriminant Analysis (&quot;lda&quot;), Random Forest (&quot;rf&quot;).
The function calls the <code><a href="#topic+select.process">select.process</a></code> in order to perform feature selection for classification, which helps to improve the quality of the classifier.
The classifier accuracy is estimated using the embedded validation procedures, including the Repeated random sub-sampling validation (&quot;sub-sampling&quot;), k-fold cross-validation (&quot;fold-crossval&quot;) and Leave-one-out cross-validation (&quot;leaveOneOut&quot;).
</p>
<p>The results is in the form of &ldquo;list&rdquo; with the data.frame of classification results for each selected classifier &ldquo;predictions&rdquo;, matrix with the statistics for the frequency each feature is selected &ldquo;no.selected&rdquo;, vector or matrix with the number of true classifications for each selected classifier &ldquo;true.classified&rdquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifier.loop(dattable,classifiers=c("svm","lda","rf","nsc"),
 feature.selection=c("auc","InformationGain"),
 disc.method="MDL",threshold=0.3, threshold.consis=0,
 attrs.nominal=numeric(), no.feat=20,flag.feature=TRUE,
 method.cross=c("leaveOneOut","sub-sampling","fold-crossval"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifier.loop_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_classifiers">classifiers</code></td>
<td>
<p>the names of the classifiers.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_feature.selection">feature.selection</code></td>
<td>
<p>a method of feature ranking or feature selection.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization. There are three options &quot;MDL&quot;,&quot;equal interval width&quot;,&quot;equal frequency&quot;. The discretization options &quot;MDL&quot; assigned to the minimal description length (MDL) discretization algorithm, which is a supervised algorithm. The last two options refer to the unsupervized discretization algorithms.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_threshold">threshold</code></td>
<td>
<p>a numeric threshold value for the correlation of feature with class to be included in the final subset. It is used by fast correlation-based filter method (FCBF)</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_threshold.consis">threshold.consis</code></td>
<td>
<p>a numeric threshold value for the inconsistency rate. It is used by Chi2 discretization algorithm.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_no.feat">no.feat</code></td>
<td>
<p>the maximal number of features to be selected.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_flag.feature">flag.feature</code></td>
<td>
<p>logical value; if TRUE the process of classifier construction and validation will be repeated for each subset of features, starting with one feature and upwards.</p>
</td></tr>
<tr><td><code id="classifier.loop_+3A_method.cross">method.cross</code></td>
<td>
<p>a character value with the names of the model validation technique for assessing how the classification results will generalize to an independent data set. It includes Repeated random sub-sampling validation, k-fold cross-validation and Leave-one-out cross-validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to perform classification with feature selection and the estimation of classification results with the model validation techniques. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned list consists of the the following fields:
</p>
<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data.frame of classification results for each selected classifier</p>
</td></tr>
<tr><td><code>no.selected</code></td>
<td>
<p>a matrix with the statistics for each feature selection frequency</p>
</td></tr>
<tr><td><code>true.classified</code></td>
<td>
<p>a vector or matrix with the number of true classifications for each selected classifier</p>
</td></tr>
</table>


<h3>References</h3>

<p>S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison of discrimination methods for the
classification of tumors using gene expression data. Journal of the American Statistical Association, 97(457):77–87, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+select.process">select.process</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(leukemia72_2)

# class label must be factor
leukemia72_2[,ncol(leukemia72_2)]&lt;-as.factor(leukemia72_2[,ncol(leukemia72_2)])

class.method="svm"
method="InformationGain"
disc&lt;-"MDL"
cross.method&lt;-"fold-crossval"

thr=0.1
thr.cons=0.05
attrs.nominal=numeric()
max.f=10

out=classifier.loop(leukemia72_2,classifiers=class.method,
feature.selection=method,disc.method=disc,
threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
no.feat=max.f,flag.feature=FALSE,method.cross=cross.method)


# example for dataset with missing values
## Not run: 
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}

class.method="svm"
method="InformationGain"
disc&lt;-"MDL"
cross.method&lt;-"fold-crossval"

thr=0.1
thr.cons=0.05
max.f=10

out=classifier.loop(xdata,classifiers=class.method,
feature.selection=method,disc.method=disc,
threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
no.feat=max.f,flag.feature=FALSE,method.cross=cross.method)

## End(Not run)
</code></pre>

<hr>
<h2 id='compute.auc.permutation'>
Calculates the p-values
</h2><span id='topic+compute.auc.permutation'></span>

<h3>Description</h3>

<p>This auxiliary function calculates the p-value of the significance of the AUC values using the permutation test (for each input feature). It takes as an input the results of the AUC value calculation using function <code><a href="#topic+compute.aucs">compute.aucs</a></code>.
</p>
<p>The results is in the form of &ldquo;numeric vector&rdquo; with p-values for each AUC value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.auc.permutation(aucs,dattable,repetitions=1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.auc.permutation_+3A_aucs">aucs</code></td>
<td>
<p>a numeric vector of AUC values.</p>
</td></tr>
<tr><td><code id="compute.auc.permutation_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values.</p>
</td></tr>
<tr><td><code id="compute.auc.permutation_+3A_repetitions">repetitions</code></td>
<td>
<p>the number of permutations of feature values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This auxiliary function's main job is to calculate the p-values of the statistical significance test of the AUC values for each input feature). See the   &ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data is the following:
</p>
<table>
<tr><td><code>p.values</code></td>
<td>
<p>a numeric vector with the p-values for each feature AUC value</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+pauclog">pauclog</a></code>, <code><a href="#topic+pauc">pauc</a></code>, <code><a href="#topic+compute.auc.random">compute.auc.random</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

auc.val=compute.aucs(dattable=data_test)
vauc&lt;-auc.val[,"AUC"]
rep.num&lt;-20

p.values=compute.auc.permutation(aucs=vauc,dattable=data_test,rep.num)
</code></pre>

<hr>
<h2 id='compute.auc.random'>
Calculates the p-values
</h2><span id='topic+compute.auc.random'></span>

<h3>Description</h3>

<p>This auxiliary function calculates the p-value of the significance of the AUC values using the comparison with random sample generation (for each input feature). It takes as an input the results of the AUC value calculation using function <code><a href="#topic+compute.aucs">compute.aucs</a></code>.
</p>
<p>The results is in the form of &ldquo;numeric vector&rdquo; with p-values for each AUC value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.auc.random(aucs,dattable,repetitions=10000,correction="none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.auc.random_+3A_aucs">aucs</code></td>
<td>
<p>a numeric vector of AUC values.</p>
</td></tr>
<tr><td><code id="compute.auc.random_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values.</p>
</td></tr>
<tr><td><code id="compute.auc.random_+3A_repetitions">repetitions</code></td>
<td>
<p>the number of repetitions of random sample' generation.</p>
</td></tr>
<tr><td><code id="compute.auc.random_+3A_correction">correction</code></td>
<td>
<p>the method of p-value correction for multiple testing, including Bonferroni-Holm, Bonferroni corrections or without correction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This auxiliary function's main job is to calculate the p-values of the statistical significance test of the AUC values for each input feature for two-class problem.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels (with two class labels).
</p>
<p>The correction methods include the Bonferroni correction (&quot;bonferroni&quot;) in which the p-values are multiplied by the number of comparisons and the less conservative corrections by Bonferroni-Holm method (&quot;bonferroniholm&quot;). A pass-through option (&quot;none&quot;) is also included.
</p>
<p>The correction methods are designed to give strong control of the family-wise error rate.
See the   &ldquo;Value&rdquo; section to this page for more details.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data is the following:
</p>
<table>
<tr><td><code>pvalues.raw</code></td>
<td>
<p>a numeric vector with the corrected p-values for each feature AUC value</p>
</td></tr>
</table>


<h3>References</h3>

<p>Benjamini, Y., and Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics 29, 1165–1188.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+pauclog">pauclog</a></code>, <code><a href="#topic+pauc">pauc</a></code>, <code><a href="#topic+compute.auc.permutation">compute.auc.permutation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example
data(datasetF6)

# class label must be factor
datasetF6[,ncol(datasetF6)]&lt;-as.factor(datasetF6[,ncol(datasetF6)])

auc.val=compute.aucs(dattable=datasetF6)
vauc&lt;-auc.val[,"AUC"]

cors&lt;-"none"
rep.num&lt;-100

pvalues.raw&lt;-compute.auc.random(aucs=vauc,dattable=datasetF6,
 repetitions=rep.num,correction=cors)
</code></pre>

<hr>
<h2 id='compute.aucs'>
Ranks the features
</h2><span id='topic+compute.aucs'></span>

<h3>Description</h3>

<p>This function calculates the features weights using the AUC (Area Under the ROC Curve) values.
It can handle only numerical values. This function performs two-class or multiclass AUC. A multiclass AUC is a mean of AUCs for all combinations of the two class labels.  This function measures the worth of a feature
by computing the AUC values with respect to the class.The results is in the form of &ldquo;data.frame&rdquo;. In the case of two-class problem it consists of the three fields: features (Biomarker) names, AUC values and level of the positive class. In the case of more than two classes it consists of two fields: features (Biomarker) names, AUC values. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;auc&rdquo; for feature selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.aucs(dattable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.aucs_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to calculate the weights of the features according to AUC values. See the   &ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data.frame consists of th the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>AUC</code></td>
<td>
<p>a numeric vector of AUC values for the features according to class</p>
</td></tr>
<tr><td><code>Positive class</code></td>
<td>
<p>a numeric vector of positive class levels for two-class problem</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

out=compute.aucs(dattable=data_test)

# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# the nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
xdata=xdata[,-attrs.nominal]
# the nominal features are not processed
out=compute.aucs(dattable=xdata)
</code></pre>

<hr>
<h2 id='cost.curve'>
Plots the RCC curve for two-class problem
</h2><span id='topic+cost.curve'></span>

<h3>Description</h3>

<p>This function plots the Relative Cost Curves (RCC) and calculates the corresponding Area Above the RCC (AAC) value to estimate the classifier performance under unequal misclassification costs.
It is intended for the two-class problem, but the extension to more than two classes will be produced later.
RCC is a graphical technique for visualising the performance of binary classifiers over the full range of possible relative misclassification costs. This curve provides helpful information to choose the best set of classifiers or to estimate misclassification costs if those are not known precisely. Area Above the RCC (AAC) is a scalar measure of classifier performance under unequal misclassification costs problem.
It can be reasonably used only for two-class problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cost.curve(data, attrs.no, pos.Class, AAC=TRUE, n=101, add=FALSE,
xlab="log2(c)",ylab="relative costs", main="RCC",lwd=2,col="black",
xlim=c(-4,4), ylim=(c(20,120)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cost.curve_+3A_data">data</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The function is provided for two classes.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_attrs.no">attrs.no</code></td>
<td>
<p>a numerical value, containing the column number of the features to construct the RCC.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_pos.class">pos.Class</code></td>
<td>
<p>a level of the class factor to be selected as the positive class for the construction of the RCC cost curve.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_aac">AAC</code></td>
<td>
<p>logical value; if TRUE the AAC value will be calculated.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_n">n</code></td>
<td>
<p>the  number of points for the construction of RCC curve (it corresponds to the number of cost values).</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_add">add</code></td>
<td>
<p>logical value; if TRUE the RCC curve can be added to the existent RCC plot.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_xlab">xlab</code></td>
<td>
<p>name of the X axis.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_ylab">ylab</code></td>
<td>
<p>name of the Y axis.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_main">main</code></td>
<td>
<p>name of the RCC plot.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_lwd">lwd</code></td>
<td>
<p>a positive number for line width.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_col">col</code></td>
<td>
<p>the color value for the RCC plot.</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_xlim">xlim</code></td>
<td>
<p>the vector with two numeric values for the X axis limits, it defines the values for log2(cost).</p>
</td></tr>
<tr><td><code id="cost.curve_+3A_ylim">ylim</code></td>
<td>
<p>the vector with two numeric values for the Y axis limits, it defines the values for the relative cost value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to plt the RCC curve and calculates the corresponding AAC value to estimate the classifier performance under unequal misclassification costs.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels with two class labels.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>


<h3>References</h3>

<p>Olga Montvida and Frank Klawonn Relative cost curves: An alternative to AUC and an extension to 3-class problems,Kybernetika 50 no. 5, 647-660, 2014
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+compute.auc.random">compute.auc.random</a></code>,
<code><a href="#topic+compute.auc.permutation">compute.auc.permutation</a></code>,<br /> <code><a href="#topic+plotRoc.curves">plotRoc.curves</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

xllim&lt;--4
xulim&lt;-4
yllim&lt;-30
yulim&lt;-110

attrs.no=c(1,9)
pos.Class&lt;-levels(data_test[,ncol(data_test)])[1]
add.legend&lt;-TRUE

aacs&lt;-rep(0,length(attrs.no))
color&lt;-c(1:length(attrs.no))

aacs[1] &lt;- cost.curve(data_test, attrs.no[1], pos.Class,col=color[1],add=FALSE,
 xlim=c(xllim,xulim),ylim=c(yllim,yulim))

if(length(attrs.no)&gt;1){
			for(i in 2:length(attrs.no)){
		      aacs[i]&lt;- cost.curve(data_test, attrs.no[i], pos.Class,
		      col=color[i],add=TRUE,xlim=c(xllim,xulim))
		    }
	    }

if(add.legend){
	       legt &lt;- colnames(data_test)[attrs.no]
		   for(i in 1:length(attrs.no)){
			   legt[i] &lt;- paste(legt[i],", AAC=",round(1000*aacs[i])/1000,sep="")
}
legend("bottomright",legend=legt,col=color,lwd=2)
}
</code></pre>

<hr>
<h2 id='data_test'>
simulated data
</h2><span id='topic+data_test'></span>

<h3>Description</h3>

<p>This data file consists of 300 objects with 10 features. The features x1-x5 are informative and define the cluster structure of the dataset. The clusters are generated in the two-dimensional space x1-x2. The values of the features x3-x5 are identically generated as for x2. Features values x1-x5 are normally distributed values with the same standard deviation and different mean values. Features x6-x10 are random variables uniformly distributed in the interval [0, 1] and present the uninformative features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_test</code></pre>


<h3>Format</h3>

<p>A data.frame containing 300 observations of 11 variables and class with three labels.</p>


<h3>Source</h3>

<p>Landgrebe T, Duin R (2006) A simplified extension of the Area under the ROC to the multiclass domain. In: Proceedings 17th Annual Symposium of the Pattern Recognition Association of South Africa. PRASA, pp. 241–245.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+leukemia72">leukemia72</a></code>, <code><a href="#topic+datasetF6">datasetF6</a></code>, <code><a href="#topic+leukemia72_2">leukemia72_2</a></code>, <code><a href="#topic+leukemia_miss">leukemia_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the dataset
data(data_test)
</code></pre>

<hr>
<h2 id='datasetF6'>
simulated data
</h2><span id='topic+datasetF6'></span>

<h3>Description</h3>

<p>This data file consists of six simulated predictors or variables with three class categories. For each class category the values are independently generated from the normal distribution with the mean µ1, µ2 and µ3 and the variances held at unity. The means are varied such that the problems range from near-separable problems, to near-random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>datasetF6</code></pre>


<h3>Format</h3>

<p>A data.frame containing 300 observations of six variables.</p>


<h3>Source</h3>

<p>Landgrebe T, Duin R (2006) A simplified extension of the Area under the ROC to the multiclass domain. In: Proceedings 17th Annual Symposium of the Pattern Recognition Association of South Africa. PRASA, pp. 241–245.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_test">data_test</a></code>, <code><a href="#topic+leukemia72">leukemia72</a></code>, <code><a href="#topic+leukemia72_2">leukemia72_2</a></code>, <code><a href="#topic+leukemia_miss">leukemia_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the dataset
data(datasetF6)
</code></pre>

<hr>
<h2 id='generate.data.miss'>
Generate the dataset with missing values
</h2><span id='topic+generate.data.miss'></span>

<h3>Description</h3>

<p>The function for the generation the dataset with missing values from the input dataset with all the values. It is mainly intended for the testing purposes.
The results is in the form of &ldquo;data.frame&rdquo; which corresponds to the input data.frame or matrix, where missing values are inserted. The percent of missing values is supplied as the input parameters. The processed dataset can be used in the algorithms for missing value imputation &ldquo;input_miss&rdquo; or for any other purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate.data.miss(data,percent=5,filename=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate.data.miss_+3A_data">data</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. This data set has not missing values</p>
</td></tr>
<tr><td><code id="generate.data.miss_+3A_percent">percent</code></td>
<td>
<p>a numerical value for the percent of the missing values to be inserted into the dataset.</p>
</td></tr>
<tr><td><code id="generate.data.miss_+3A_filename">filename</code></td>
<td>
<p>a character name of the output file to save the dataset with missing values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to generate the dataset with missing values from the input dataset with all the values. See the &ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>A returned data.frame corresponds to the input dataset with inserted missing values.
</p>


<h3>References</h3>

<p>McShane LM, Radmacher MD, Freidlin B, Yu R, Li MC, Simon R. Methods for assessing reproducibility of clustering patterns observed in analyses of microarray data. Bioinformatics. 2002 Nov;18(11):1462-9.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>,
<code><a href="#topic+classifier.loop">classifier.loop</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example

data(leukemia72_2)

percent =5
f.name=NULL #file name to include
out=generate.data.miss(data=leukemia72_2,percent=percent,filename=f.name)
</code></pre>

<hr>
<h2 id='input_miss'>
Process the dataset with missing values
</h2><span id='topic+input_miss'></span>

<h3>Description</h3>

<p>The main function for handling with missing values.
It performs the missing values imputation using two different approachs: imputation with mean values and using the nearest neighbour algorithm. It can handle both numerical and nominal values. The function also delete the features with the number of missing values more then specified threshold.
The results is in the form of &ldquo;list&rdquo; with the processed dataset and the logical value, which indicates the success or failure of processing. The processed dataset can be used in the algorithms for feature selection &ldquo;select.process&rdquo; and classification &ldquo;classifier.loop&rdquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>input_miss(matrix,method.subst="near.value",
attrs.nominal=numeric(),delThre=0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="input_miss_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="input_miss_+3A_method.subst">method.subst</code></td>
<td>
<p>a method of missing value processing.
There are two realized methods: substitution with mean value
('mean.value') and nearest neighbour algorithm<br />  ('near.value').</p>
</td></tr>
<tr><td><code id="input_miss_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
<tr><td><code id="input_miss_+3A_delthre">delThre</code></td>
<td>
<p>the minimal threshold for the deletion of features with missing values. It is in the interval [0,1], where for delThre=0 all features having at least one missing value will be deleted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to handle the missing values in the dataset. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data are provided with reasonable number of missing values that is preprocessed with one of the imputing methods.
</p>
<p>A returned list consists of the the following fields:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>a processed dataset</p>
</td></tr>
<tr><td><code>flag.miss</code></td>
<td>
<p>logical value; if TRUE the processing is successful, if FALSE the input dataset is returned without processing.</p>
</td></tr>
</table>


<h3>References</h3>

<p>McShane LM, Radmacher MD, Freidlin B, Yu R, Li MC, Simon R. Methods for assessing reproducibility of clustering patterns observed in analyses of microarray data. Bioinformatics. 2002 Nov;18(11):1462-9.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+select.process">select.process</a></code>, <code><a href="#topic+classifier.loop">classifier.loop</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
</code></pre>

<hr>
<h2 id='leukemia_miss'>
desease data
</h2><span id='topic+leukemia_miss'></span>

<h3>Description</h3>

<p>Leukemia dataset includes the bone marrow samples obtained from acute leukemia
patients at the time of diagnosis: 25 acute myeloid leukemia (AML) samples; 9
T-lineage acute lymphoblastic leukemia (ALL) samples; and 38 B-lineage ALL
samples. After preprocessing, the 100 genes with the largest variation across samples are selected. This dataset is the same as leukemia72 with the 5 percent of missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leukemia_miss</code></pre>


<h3>Format</h3>

<p>A data.frame containing 72 observations of 101 variables: 100 features and class with three diagnosis: 38 B-lineage ALL, 9 T-lineage ALL and 25 AML. It has 5 percent missing values.</p>


<h3>Source</h3>

<p>Handl J, Knowles J, Kell DB, Computational cluster validation in post-genomic data analysis, Bioinformatics 21:3201-3212, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_test">data_test</a></code>, <code><a href="#topic+datasetF6">datasetF6</a></code>, <code><a href="#topic+leukemia72">leukemia72</a></code>, <code><a href="#topic+leukemia72_2">leukemia72_2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the dataset
data(leukemia_miss)
# X95735_at
with(leukemia_miss, by(X95735_at,Class,mean,na.rm=TRUE))

# M27891_at
with(leukemia_miss,tapply(M27891_at, Class, FUN = mean,na.rm=TRUE))
</code></pre>

<hr>
<h2 id='leukemia72'>
desease data
</h2><span id='topic+leukemia72'></span>

<h3>Description</h3>

<p>Leukemia dataset includes the bone marrow samples obtained from acute leukemia
patients at the time of diagnosis: 25 acute myeloid leukemia (AML) samples; 9
T-lineage acute lymphoblastic leukemia (ALL) samples; and 38 B-lineage ALL
samples. After preprocessing, the 100 genes with the largest variation across samples are selected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leukemia72</code></pre>


<h3>Format</h3>

<p>A data.frame containing 72 observations of 101 variables: 100 features and class with three diagnosis: 38 B-lineage ALL, 9 T-lineage ALL and 25 AML.</p>


<h3>Source</h3>

<p>Handl J, Knowles J, Kell DB, Computational cluster validation in post-genomic data analysis, Bioinformatics 21:3201-3212, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_test">data_test</a></code>, <code><a href="#topic+datasetF6">datasetF6</a></code>, <code><a href="#topic+leukemia72_2">leukemia72_2</a></code>, <code><a href="#topic+leukemia_miss">leukemia_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the dataset
data(leukemia72)
# X95735_at
with(leukemia72, by(X95735_at,Class,mean))

# M27891_at
with(leukemia72,tapply(M27891_at, Class, FUN = mean))
with(leukemia72, table(M27891_at=ifelse(M27891_at&lt;=mean(M27891_at), "1", "2"), Class))
</code></pre>

<hr>
<h2 id='leukemia72_2'>
desease data
</h2><span id='topic+leukemia72_2'></span>

<h3>Description</h3>

<p>Leukemia dataset includes the bone marrow samples obtained from acute leukemia
patients at the time of diagnosis: 25 acute myeloid leukemia (AML) samples; 9
T-lineage acute lymphoblastic leukemia (ALL) samples; and 38 B-lineage ALL
samples. After preprocessing, the 100 genes with the largest variation across samples are selected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leukemia72_2</code></pre>


<h3>Format</h3>

<p>A data.frame containing 72 observations of 101 variables: 100 features and class with two diagnosis: 47 ALL and 25 AML).</p>


<h3>Source</h3>

<p>Handl J, Knowles J, Kell DB, Computational cluster validation in post-genomic data analysis, Bioinformatics 21:3201-3212, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_test">data_test</a></code>, <code><a href="#topic+datasetF6">datasetF6</a></code>, <code><a href="#topic+leukemia72">leukemia72</a></code>, <code><a href="#topic+leukemia_miss">leukemia_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the dataset
data(leukemia72_2)
# X95735_at
with(leukemia72_2, by(X95735_at,Class,mean))

# M27891_at
with(leukemia72_2,tapply(M27891_at, Class, FUN = mean))
with(leukemia72_2, table(M27891_at=ifelse(M27891_at&lt;=mean(M27891_at), "1", "2"), Class))
</code></pre>

<hr>
<h2 id='pauc'>
Calculates the p-values
</h2><span id='topic+pauc'></span>

<h3>Description</h3>

<p>This auxiliary function calculates the p-value of the statistical significance test of the difference of samples from two classes using AUC values (for each input feature). It takes as an input the results of the AUC value calculation using function <code><a href="#topic+compute.aucs">compute.aucs</a></code>.
It can be reasonably used only for two-class problem.
The results is in the form of &ldquo;numeric vector&rdquo; with p-values for each features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pauc(auc,n=100,n.plus=0.5,labels=numeric(),pos=numeric())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pauc_+3A_auc">auc</code></td>
<td>
<p>a numeric vector of AUC values.</p>
</td></tr>
<tr><td><code id="pauc_+3A_n">n</code></td>
<td>
<p> the whole number of observations for the test.</p>
</td></tr>
<tr><td><code id="pauc_+3A_n.plus">n.plus</code></td>
<td>
<p>the number of cases in the sample with the positive class.</p>
</td></tr>
<tr><td><code id="pauc_+3A_labels">labels</code></td>
<td>
<p>the factor with the class labels.</p>
</td></tr>
<tr><td><code id="pauc_+3A_pos">pos</code></td>
<td>
<p>the numeric vector with the level of the positive class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This auxiliary function's main job is to calculate the p-values of the statistical significance test of two samples, defined by negative and positive class labels, i.e. two-class problem. See the   &ldquo;Value&rdquo; section to this page for more details.
</p>


<h3>Value</h3>

<p>A returned data consists is the following:
</p>
<table>
<tr><td><code>pauc</code></td>
<td>
<p>a numeric vector with the p-values for each feature</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+pauclog">pauclog</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

auc.val=compute.aucs(dattable=data_test)
vauc&lt;-auc.val[,"AUC"]
val=levels(data_test[,ncol(data_test)])

if(length(val)==2)
{
	 pos=auc.val[,"Positive class"]
	 paucv&lt;-pauc(auc=vauc,labels=data_test[,ncol(data_test)],pos=pos)
}else{
	 num.size=100
	 num.prop=0.5
	 paucv&lt;-pauc(auc=vauc,n=num.size,n.plus=num.prop)
}
</code></pre>

<hr>
<h2 id='pauclog'>
Calculates the p-values
</h2><span id='topic+pauclog'></span>

<h3>Description</h3>

<p>This auxiliary function calculates the logarithm of p-values of the statistical significance test of the difference of samples from two classes using AUC values (for each input feature). It takes as an input the results of the AUC value calculation by the function <code><a href="#topic+compute.aucs">compute.aucs</a></code>.
It can be reasonably used only for two-class problem.
The results is in the form of &ldquo;numeric vector&rdquo; with the logarithms of the p-values for each features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pauclog(auc,n=100,n.plus=0.5,labels=numeric(),pos=numeric())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pauclog_+3A_auc">auc</code></td>
<td>
<p>a numeric vector of AUC values.</p>
</td></tr>
<tr><td><code id="pauclog_+3A_n">n</code></td>
<td>
<p> the whole number of observations for the test.</p>
</td></tr>
<tr><td><code id="pauclog_+3A_n.plus">n.plus</code></td>
<td>
<p>the number of cases in the sample with the positive class.</p>
</td></tr>
<tr><td><code id="pauclog_+3A_labels">labels</code></td>
<td>
<p>the factor with the class labels.</p>
</td></tr>
<tr><td><code id="pauclog_+3A_pos">pos</code></td>
<td>
<p>the numeric vector with the level of the positive class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This auxiliary function's main job is to calculate the logarithm of p-values of the statistical significance test of two samples, defined by negative and positive class labels, i.e. two-class problem. See the   &ldquo;Value&rdquo; section to this page for more details.
</p>


<h3>Value</h3>

<p>A returned data consists is the following:
</p>
<table>
<tr><td><code>pauclog</code></td>
<td>
<p>a numeric vector with the logarithm of p-value for each feature</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+pauc">pauc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example
data(datasetF6)

# class label must be factor
datasetF6[,ncol(datasetF6)]&lt;-as.factor(datasetF6[,ncol(datasetF6)])

auc.val=compute.aucs(dattable=datasetF6)
vauc&lt;-auc.val[,"AUC"]
val=levels(datasetF6[,ncol(datasetF6)])

if(length(val)==2)
{
	 pos=auc.val[,"Positive class"]
	 paucv&lt;-pauclog(auc=vauc,labels=datasetF6[,ncol(datasetF6)],pos=pos)
}else{
	 num.size=100
	 num.prop=0.5
	 paucv&lt;-pauclog(auc=vauc,n=num.size,n.plus=num.prop)
}
</code></pre>

<hr>
<h2 id='plotClass.result'>
Plots the results of classifier validation schemes
</h2><span id='topic+plotClass.result'></span>

<h3>Description</h3>

<p>This function plots the barplots and boxplots, which help in estimation of the results of classifiers' validation, performed by different validation models. It must be called after the performing the classification validation with function <code><a href="#topic+classifier.loop">classifier.loop</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotClass.result(true.classified, cross.method, class.method,
flag.feature, feat.num)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotClass.result_+3A_true.classified">true.classified</code></td>
<td>
<p>a vector or matrix of classification results for one or several classifiers and one or several feature sets. The matrix is the output value of the function <code><a href="#topic+classifier.loop">classifier.loop</a></code>.</p>
</td></tr>
<tr><td><code id="plotClass.result_+3A_cross.method">cross.method</code></td>
<td>
<p>a character value with the names of the model validation technique for assessing how the classification results will generalize to an independent data set. It includes Repeated random sub-sampling validation, k-fold cross-validation and Leave-one-out cross-validation.</p>
</td></tr>
<tr><td><code id="plotClass.result_+3A_class.method">class.method</code></td>
<td>
<p>the names of the classifiers.</p>
</td></tr>
<tr><td><code id="plotClass.result_+3A_flag.feature">flag.feature</code></td>
<td>
<p>logical value; if TRUE the process of classifier construction and validation will be repeated for each subset of features, starting with one feature and upwards.</p>
</td></tr>
<tr><td><code id="plotClass.result_+3A_feat.num">feat.num</code></td>
<td>
<p>the maximal number of features to be selected.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to plot the barplots and boxplots to visually estimate the results of classifiers' validation.
</p>


<h3>Value</h3>

<p>The results is visualization of the plot .
</p>


<h3>References</h3>

<p>S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison of discrimination methods for the
classification of tumors using gene expression data. Journal of the American Statistical Association, 97(457):77–87, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+select.process">select.process</a></code>, <code><a href="#topic+classifier.loop">classifier.loop</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values

data(leukemia72_2)

# class label must be factor
leukemia72_2[,ncol(leukemia72_2)]&lt;-as.factor(leukemia72_2[,ncol(leukemia72_2)])

class.method=c("svm","nn")
method="InformationGain"
disc&lt;-"MDL"
cross.method&lt;-"fold-crossval"

flag.feature=TRUE
thr=0.1
thr.cons=0.05
attrs.nominal=numeric()
max.f=10

out=classifier.loop(leukemia72_2,classifiers=class.method,
 feature.selection=method,disc.method=disc,
 threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
  no.feat=max.f,flag.feature=flag.feature,method.cross=cross.method)

plotClass.result(out$true.classified, cross.method, class.method, flag.feature, max.f)

# example for dataset with missing values
## Not run: 
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}

class.method=c("svm","nn")
method="InformationGain"
disc&lt;-"MDL"
cross.method&lt;-"fold-crossval"

flag.feature=TRUE
thr=0.1
thr.cons=0.05
max.f=10

out=classifier.loop(xdata,classifiers=class.method,
 feature.selection=method,disc.method=disc,
 threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
 no.feat=max.f,flag.feature=flag.feature,method.cross=cross.method)

plotClass.result(out$true.classified, cross.method, class.method, flag.feature, max.f)

## End(Not run)
</code></pre>

<hr>
<h2 id='plotRoc.curves'>
Plots the ROC curve for two-class problem
</h2><span id='topic+plotRoc.curves'></span>

<h3>Description</h3>

<p>This function plots the ROC curve for the two-class problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotRoc.curves(dattable,file.name=NULL,colours=NULL,ltys=NULL,
 add.legend=F,curve.names=NULL,include.auc=F,xaxis="",yaxis="",
 line.width=2,headline="",ispercent=F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotRoc.curves_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_file.name">file.name</code></td>
<td>
<p>the file name to save the plot.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_colours">colours</code></td>
<td>
<p>the color values for each plot if more than one feature or one color value in the case of one feature.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_ltys">ltys</code></td>
<td>
<p>the  line type values for each plot if more than one feature or one  line type in the case of one feature.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_add.legend">add.legend</code></td>
<td>
<p>logical value; if TRUE the legend will be plotted at the bottom righ.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_curve.names">curve.names</code></td>
<td>
<p>a character value or vector in the case of more than one feature with curve names to be used in the legend.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_include.auc">include.auc</code></td>
<td>
<p>logical value; if TRUE the AUC value will be included in the legend .</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_xaxis">xaxis</code></td>
<td>
<p>character value with the name of X axis.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_yaxis">yaxis</code></td>
<td>
<p>character value with the name of Y axis.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_line.width">line.width</code></td>
<td>
<p>a positive number for line width.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_headline">headline</code></td>
<td>
<p>the character value with the name of the plot.</p>
</td></tr>
<tr><td><code id="plotRoc.curves_+3A_ispercent">ispercent</code></td>
<td>
<p>logical value; if TRUE the true positive and false positive values of the plot are in the percents.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to plot the ROC curve for one or more features with the possibility to include the AUC values in the legend.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels with two class labels.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>


<h3>References</h3>

<p>David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute.aucs">compute.aucs</a></code>, <code><a href="#topic+pauclog">pauclog</a></code>, <code><a href="#topic+pauc">pauc</a></code>, <code><a href="#topic+compute.auc.permutation">compute.auc.permutation</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(leukemia72_2)

# class label must be factor
leukemia72_2[,ncol(leukemia72_2)]&lt;-as.factor(leukemia72_2[,ncol(leukemia72_2)])

add.legend&lt;-TRUE
include.auc&lt;-TRUE

attrs.no=c(1,2)
xdata=leukemia72_2[,c(attrs.no,ncol(leukemia72_2))]
plotRoc.curves(dattable=xdata,add.legend=add.legend,include.auc=include.auc)
</code></pre>

<hr>
<h2 id='ProcessData'>
Select the subset of features
</h2><span id='topic+ProcessData'></span>

<h3>Description</h3>

<p>The auxiliary function performs the discretization of the numerical features and is called from the several functions for feature selection. The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.
The results is in the form of &ldquo;list&rdquo;, consisting of two fields: the processed dataset and the column numbers of the features. When the value of the input parameter &ldquo;flag&rdquo;=TRUE the second field will include the column numbers of the features, which have more than single interval after discretization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProcessData(matrix,disc.method,attrs.nominal,flag=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProcessData_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="ProcessData_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="ProcessData_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
<tr><td><code id="ProcessData_+3A_flag">flag</code></td>
<td>
<p>a binary logical value. If equals TRUE the output list will contain the processed dataset with the features, having more than one interval after discretization together with their names. In the case of FALSE value the processed dataset with all the features will be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This auxiliary function's main job is to descritize the numerical features using the one of the discretization methods. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned list consists of the the following fields:
</p>
<table>
<tr><td><code>m3</code></td>
<td>
<p>a processed dataset</p>
</td></tr>
<tr><td><code>sel.feature</code></td>
<td>
<p>a numeric vector with the column numbers of the features, having more than one interval value (when &ldquo;flag&rdquo;=TRUE). If &ldquo;flag&rdquo;=FALSE it return all the column numbers of the dataset.</p>
</td></tr>
</table>


<h3>References</h3>

<p>H. Liu, F. Hussain, C. L. Tan, and M. Dash, &quot;Discretization: An enabling technique,&quot; Data Mining and Knowledge Discovery, Vol. 6, No. 4, 2002, pp. 393-423.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+select.inf.gain">select.inf.gain</a></code>, <code><a href="#topic+select.inf.symm">select.inf.symm</a></code>, <code><a href="#topic+select.inf.chi2">select.inf.chi2</a></code>, <br /> <code><a href="#topic+select.fast.filter">select.fast.filter</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

disc&lt;-"MDL"
attrs.nominal=numeric()
flag=FALSE
out=ProcessData(matrix=data_test,disc.method=disc,
attrs.nominal=attrs.nominal,flag=flag)
</code></pre>

<hr>
<h2 id='select.cfs'>
Select the subset of features
</h2><span id='topic+select.cfs'></span>

<h3>Description</h3>

<p>This function selects the subset of features using the best first search strategy on the basis of correlation measure (CFS). CFS evaluates a subset of features by considering the individual predictive ability of each feature along with the degree of redundancy between them. It can handle both numerical and nominal values. The results is in the form of &ldquo;data.frame&rdquo;, consisting of the following fields: features (Biomarker) names and the positions of the features in the dataset.
This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;CFS&rdquo; for feature selection. The variable &ldquo;Index&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.cfs(matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.cfs_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to select the subset of informative features according to best first search strategy using the correlation measure (informative theoretic measure). The measure consideres the individual predictive ability of each feature along with the degree of redundancy between them. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned list consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>Index</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
out=select.cfs(matrix=data_test)
</code></pre>

<hr>
<h2 id='select.fast.filter'>
Select the subset of features
</h2><span id='topic+select.fast.filter'></span>

<h3>Description</h3>

<p>This function selects the subset of features on the basis of the fast correlation-based filter method (FCBF). It can handle both numerical and nominal values. At first it performs the discretization of the numerical features values, according to several optional discretization methods using the function <code><a href="#topic+ProcessData">ProcessData</a></code>. A fast filter can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The overall complexity of FCBF is O(MN logN), where M - number of samples, N - number of features.The results is in the form of &ldquo;data.frame&rdquo;, consisting of the features (Biomarker) names, values of the information gain and the positions of the features in the dataset. The information gain value is the correlation between the features and the class. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;FastFilter&rdquo; for feature selection. The variable &ldquo;NumberFeature&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.fast.filter(matrix,disc.method,threshold,attrs.nominal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.fast.filter_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.fast.filter_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="select.fast.filter_+3A_threshold">threshold</code></td>
<td>
<p>a numeric threshold value for the correlation of feature with class to be included in the final subset.</p>
</td></tr>
<tr><td><code id="select.fast.filter_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to select the subset of informative features according to correlation between features and class, and between features themselves. See the
&ldquo;Value&rdquo; section to this page for more details. Before
starting it calls the <code><a href="#topic+ProcessData">ProcessData</a></code> function to make the discretization of numerical features.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned data.frame consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>Information.Gain</code></td>
<td>
<p>a numeric vector of information gain values for the features according to class</p>
</td></tr>
<tr><td><code>NumberFeature</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>L. Yu and H. Liu. &quot;Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution&quot;. In Proceedings of The Twentieth International Conference on Machine Leaning (ICML-03), Washington, D.C. pp. 856-863. August 21-24, 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProcessData">ProcessData</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
disc&lt;-"MDL"
threshold=0.2
attrs.nominal=numeric()
out=select.fast.filter(data_test, disc.method=disc, threshold=threshold,
attrs.nominal=attrs.nominal)

# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
disc&lt;-"MDL"
threshold=0.2
out=select.fast.filter(xdata, disc.method=disc, threshold=threshold,
attrs.nominal=attrs.nominal)
</code></pre>

<hr>
<h2 id='select.forward.Corr'>
Select the subset of features
</h2><span id='topic+select.forward.Corr'></span>

<h3>Description</h3>

<p>This function selects the subset of features using the forward search strategy on the basis of correlation measure (CFS algorithm with forward search). CFS evaluates a subset of features by considering the individual predictive ability of each feature along with the degree of redundancy between them. It can handle both numerical and nominal values. At the beginning the discretization of the numerical features values is performed using the function <code><a href="#topic+ProcessData">ProcessData</a></code>. At the first step of the method the one-feature subset is selected according to its informative score, which takes into account the average feature to class correlation and the average feature to feature correlation. In the following steps the subset is incrementally extended according to the forward search strategy until the stopping criterion is met.
The result is in the form of character vector with the names of the selected features. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.forward.Corr(matrix,disc.method,attrs.nominal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.forward.Corr_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.forward.Corr_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="select.forward.Corr_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to select the subset of informative features according to forward selection strategy using the correlation measure (informative theoretic measure). The measure consideres the individual predictive ability of each feature along with the degree of redundancy between them. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned value is
</p>
<table>
<tr><td><code>subset</code></td>
<td>
<p>a character vector of the names of selected features</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
disc&lt;-"MDL"
attrs.nominal=numeric()
out=select.forward.Corr(matrix=data_test,disc.method=disc,
attrs.nominal=attrs.nominal)
</code></pre>

<hr>
<h2 id='select.forward.wrapper'>
Select the subset of features
</h2><span id='topic+select.forward.wrapper'></span>

<h3>Description</h3>

<p>This function selects the subset of features using the wrapper method with decision tree algorithm and forward search strategy. It can handle both numerical and nominal values. The wrapper method makes use of the classification algorithm in order to estimate the quality measure of the feature subset. The method uses the built-in cross-validation procedure to estimate the accuracy of classification for the feature subset. At the first step of the method the one-feature subset is selected according to the quality measure. In the following steps the subset is incrementally extended according to the forward search strategy until the stopping criterion is met.
The result is in the form of character vector with the names of the selected features. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;CorrSF&rdquo; for feature selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.forward.wrapper(dattable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.forward.wrapper_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to select the subset of informative features according to forward selection strategy using the wrapper method. The decision tree is used as the classifier to estimate the quality of the feature subset. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned value is
</p>
<table>
<tr><td><code>subset</code></td>
<td>
<p>a character vector of the names of selected features</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

out=select.forward.wrapper(dattable=data_test)
</code></pre>

<hr>
<h2 id='select.inf.chi2'>
Ranks the features
</h2><span id='topic+select.inf.chi2'></span>

<h3>Description</h3>

<p>This function calculates the features weights using the chi-squared (<code class="reqn">\chi^2</code>) statistic and performs the ranking of the features. It can handle both numerical and nominal values. At first it performs the discretization of the numerical features values, according to several optional discretization methods using the function <code><a href="#topic+ProcessData">ProcessData</a></code>. This function measures the worth of a feature
by computing the value of the <code class="reqn">\chi^2</code> statistic with respect to the class.The results is in the form of &ldquo;data.frame&rdquo;, consisting of the following fields: features (Biomarker) names, values of the chi-squared statistic and the positions of the features in the dataset. The features in the data.frame are sorted according to the chi-squared statistic values. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;Chi-square&rdquo; for feature selection. The variable &ldquo;NumberFeature&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.inf.chi2(matrix,disc.method,attrs.nominal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.inf.chi2_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.inf.chi2_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="select.inf.chi2_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to rank the features according to chi-squared statistic. See the   &ldquo;Value&rdquo; section to this page for more details. Before
starting it calls the <code><a href="#topic+ProcessData">ProcessData</a></code> function to make the discretization of numerical features.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data.frame consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>ChiSquare</code></td>
<td>
<p>a numeric vector of chi-squared values for the features according to class</p>
</td></tr>
<tr><td><code>NumberFeature</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProcessData">ProcessData</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
disc&lt;-"equal interval width"
attrs.nominal=numeric()
out=select.inf.chi2(data_test,disc.method=disc,attrs.nominal=attrs.nominal)

# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
disc&lt;-"equal interval width"
out=select.inf.chi2(xdata,disc.method=disc,attrs.nominal=attrs.nominal)
</code></pre>

<hr>
<h2 id='select.inf.gain'>
Ranks the features
</h2><span id='topic+select.inf.gain'></span>

<h3>Description</h3>

<p>This function calculates the features weights using the Information Gain criterion measure and performs the ranking of the features (in decreasing order of Information Gain criteria). It can handle both numerical and nominal values. At first it performs the discretization of the numerical features values, according to several optional discretization methods using the function <code><a href="#topic+ProcessData">ProcessData</a></code>. This function measures the worth of a feature
by computing the Information Gain criterion measure with respect to the class.The results is in the form of &ldquo;data.frame&rdquo;, consisting of the following fields: features (Biomarker) names, values of the Information Gain criterion measure and the positions of the features in the dataset. The features in the data.frame are sorted according to the Information Gain uncertainty criterion values. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;InformationGain&rdquo; for feature selection. The variable &ldquo;NumberFeature&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.inf.gain(matrix,disc.method,attrs.nominal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.inf.gain_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.inf.gain_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="select.inf.gain_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to rank the features according to Information Gain criterion. See the   &ldquo;Value&rdquo; section to this page for more details. Before
starting it calls the <code><a href="#topic+ProcessData">ProcessData</a></code> function to make the discretization of numerical features.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned list consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>Information.Gain</code></td>
<td>
<p>a numeric vector of Information gain values for the features</p>
</td></tr>
<tr><td><code>NumberFeature</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProcessData">ProcessData</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
disc&lt;-"equal interval width"
attrs.nominal=numeric()
out=select.inf.gain(data_test,disc.method=disc,attrs.nominal=attrs.nominal)

# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
disc&lt;-"equal interval width"
out=select.inf.gain(xdata,disc.method=disc,attrs.nominal=attrs.nominal)
</code></pre>

<hr>
<h2 id='select.inf.symm'>
Ranks the features
</h2><span id='topic+select.inf.symm'></span>

<h3>Description</h3>

<p>This function calculates the features weights using the Symmetrical uncertainty criterion measure and performs the ranking of the features (in decreasing order of Symmetrical uncertainty criteria). It can handle both numerical and nominal values. At first it performs the discretization of the numerical features values, according to several optional discretization methods using the function <code><a href="#topic+ProcessData">ProcessData</a></code>. This function measures the worth of a feature
by computing the Symmetrical uncertainty criterion measure with respect to the class.The results is in the form of &ldquo;data.frame&rdquo;, consisting of the following fields: features (Biomarker) names, values of the Symmetrical uncertainty criterion measure and the positions of the features in the dataset. The features in the data.frame are sorted according to the Symmetrical uncertainty criterion values. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;symmetrical.uncertainty&rdquo; for feature selection. The variable &ldquo;NumberFeature&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.inf.symm(matrix,disc.method,attrs.nominal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.inf.symm_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.inf.symm_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization.The discretization options include minimal description length (MDL), equal frequency and equal interval width methods.</p>
</td></tr>
<tr><td><code id="select.inf.symm_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to rank the features according to Symmetrical uncertainty criterion. See the &ldquo;Value&rdquo; section to this page for more details. Before
starting it calls the <code><a href="#topic+ProcessData">ProcessData</a></code> function to make the discretization of numerical features.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data.frame consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>SymmetricalUncertainty</code></td>
<td>
<p>a numeric vector of Symmetrical uncertainty criterion values for the features according to class</p>
</td></tr>
<tr><td><code>NumberFeature</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProcessData">ProcessData</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])
disc&lt;-"equal interval width"
attrs.nominal=numeric()
out=select.inf.symm(data_test,disc.method=disc,attrs.nominal=attrs.nominal)

# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
disc&lt;-"equal interval width"
out=select.inf.symm(xdata,disc.method=disc,attrs.nominal=attrs.nominal)
</code></pre>

<hr>
<h2 id='select.process'>
Feature ranking and feature selection
</h2><span id='topic+select.process'></span>

<h3>Description</h3>

<p>The main function for the feature ranking or feature subset selection. It can handle both numerical and nominal values.
It presents the infrastructure to perform the feature ranking or feature selection for the data set with two or more class labels. The function calls several feature ranking methods with different quality measures, including AUC values (functions <code><a href="#topic+compute.aucs">compute.aucs</a></code>), information gain (function <code><a href="#topic+select.inf.gain">select.inf.gain</a></code>), symmetrical uncertainty (function <code><a href="#topic+select.inf.symm">select.inf.symm</a></code>), chi-squared (<code class="reqn">\chi^2</code>) statistic (function <code><a href="#topic+select.inf.chi2">select.inf.chi2</a></code>). It also calls the number of feature selection methods, including fast correlation-based filter method (FCBF) (function <code><a href="#topic+select.fast.filter">select.fast.filter</a></code>), Chi2 discretization algorithm (function <code><a href="#topic+chi2.algorithm">chi2.algorithm</a></code>), CFS algorithm with forward search (function <code><a href="#topic+select.forward.Corr">select.forward.Corr</a></code>), wrapper method with decision tree algorithm and forward search strategy (function <code><a href="#topic+select.forward.wrapper">select.forward.wrapper</a></code>).
The results is in the form of &ldquo;numeric vector&rdquo; with the column numbers of the selected features for features selection algorithms and ordered features' column numbers according to the criteria for feature ranking. The number of features can be limited to the &ldquo;max.no.features&rdquo; , which is the function input parameter. The output of the function is used in function &ldquo;classifier.loop&rdquo; in the process of classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.process(dattable,method="InformationGain",disc.method="MDL",
threshold=0.2,threshold.consis=0.05,attrs.nominal=numeric(),
max.no.features=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.process_+3A_dattable">dattable</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
<tr><td><code id="select.process_+3A_method">method</code></td>
<td>
<p>a method of feature ranking or feature selection. There are 6 methods for feature ranking (&quot;auc&quot;, &quot;HUM&quot;, &quot;Chi-square&quot;, &quot;InformationGain&quot;, &quot;symmetrical.uncertainty&quot;, &quot;Relief&quot;) and 4 methods for feature selection (&quot;FastFilter&quot;, &quot;CFS&quot;, &quot;CorrSF&quot;, &quot;Chi2-algorithm&quot;)</p>
</td></tr>
<tr><td><code id="select.process_+3A_disc.method">disc.method</code></td>
<td>
<p>a method used for feature discretization. There are three options &quot;MDL&quot;,&quot;equal interval width&quot;,&quot;equal frequency&quot;. The discretization options &quot;MDL&quot; assigned to the minimal description length (MDL) discretization algorithm, which is a supervised algorithm. The last two options refer to the unsupervized discretization algorithms.</p>
</td></tr>
<tr><td><code id="select.process_+3A_threshold">threshold</code></td>
<td>
<p>a numeric threshold value for the correlation of feature with class to be included in the final subset. It is used by fast correlation-based filter method (FCBF)</p>
</td></tr>
<tr><td><code id="select.process_+3A_threshold.consis">threshold.consis</code></td>
<td>
<p>a numeric threshold value for the inconsistency rate. It is used by Chi2 discretization algorithm.</p>
</td></tr>
<tr><td><code id="select.process_+3A_attrs.nominal">attrs.nominal</code></td>
<td>
<p>a numerical vector, containing the column numbers of the nominal features, selected for the analysis.</p>
</td></tr>
<tr><td><code id="select.process_+3A_max.no.features">max.no.features</code></td>
<td>
<p>the maximal number of features to be selected or ranked.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to present the infrostructure to perform the feature ranking or feature selection for the data set with two or more class labels. See the
&ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
</p>
<p>A returned value is
</p>
<table>
<tr><td><code>sel.feat</code></td>
<td>
<p>a vector of column numbers of the selected features for features selection algorithms and ordered features' column numbers according to the criteria for feature ranking</p>
</td></tr>
</table>


<h3>References</h3>

<p>H. Liu and L. Yu. &quot;Toward Integrating Feature Selection Algorithms for Classification and Clustering&quot;, IEEE Trans. on Knowledge and Data Engineering, pdf, 17(4), 491-502, 2005.<br />
L. Yu and H. Liu. &quot;Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution&quot;. In Proceedings of The Twentieth International Conference on Machine Leaning (ICML-03), Washington, D.C. pp. 856-863. August 21-24, 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+select.inf.gain">select.inf.gain</a></code>, <code><a href="#topic+select.inf.symm">select.inf.symm</a></code>, <code><a href="#topic+select.inf.chi2">select.inf.chi2</a></code>, <br /> <code><a href="#topic+select.fast.filter">select.fast.filter</a></code>, <code><a href="#topic+chi2.algorithm">chi2.algorithm</a></code>, <code><a href="#topic+select.forward.Corr">select.forward.Corr</a></code>, <br /> <code><a href="#topic+select.forward.wrapper">select.forward.wrapper</a></code>, <code><a href="#topic+input_miss">input_miss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

method="InformationGain"
disc&lt;-"MDL"
thr=0.1
thr.cons=0.05
attrs.nominal=numeric()
max.f=15

out=select.process(data_test,method=method,disc.method=disc,
threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
max.no.features=max.f)


# example for dataset with missing values
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}

method="InformationGain"
disc&lt;-"MDL"
thr=0.1
thr.cons=0.05
max.f=15

out=select.process(xdata,method=method,disc.method=disc,
threshold=thr, threshold.consis=thr.cons,attrs.nominal=attrs.nominal,
max.no.features=max.f)
</code></pre>

<hr>
<h2 id='select.relief'>
Ranks the features
</h2><span id='topic+select.relief'></span>

<h3>Description</h3>

<p>This function calculates the features weights basing on a distance between instances. It can handle only numeric. The results is in the form of &ldquo;data.frame&rdquo;, consisting of the following fields: features (Biomarker) names, weights and the positions of the features in the dataset. The features in the data.frame are sorted according to the weight values. This function is used internally to perform the classification with feature selection using the function &ldquo;classifier.loop&rdquo; with argument &ldquo;Chi-square&rdquo; for feature selection. The variable &ldquo;NumberFeature&rdquo; of the data.frame is passed to the classification function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.relief(matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.relief_+3A_matrix">matrix</code></td>
<td>
<p>a dataset, a matrix of feature values for several cases, the last column is for the class labels. Class labels could be numerical or character values. The maximal number of classes is ten.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function's main job is to rank the features according to weights. See the   &ldquo;Value&rdquo; section to this page for more details.
</p>
<p>Data can be provided in matrix form, where the rows correspond to cases with feature values and class label. The columns contain the values of individual features and the last column must contain class labels. The maximal number of class labels equals 10.
The class label features and all the nominal features must be defined as factors.
</p>


<h3>Value</h3>

<p>The data can be provided with reasonable number of missing values that must be at first preprocessed with one of the imputing methods in the function  <code><a href="#topic+input_miss">input_miss</a></code>.
A returned data.frame consists of the the following fields:
</p>
<table>
<tr><td><code>Biomarker</code></td>
<td>
<p>a character vector of feature names</p>
</td></tr>
<tr><td><code>Weights</code></td>
<td>
<p>a numeric vector of weight values for the features</p>
</td></tr>
<tr><td><code>NumberFeature</code></td>
<td>
<p>a numerical vector of the positions of the features in the dataset</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y. Wang, I.V. Tetko, M.A. Hall, E. Frank, A. Facius, K.F.X. Mayer, and H.W. Mewes, &quot;Gene Selection from Microarray Data for Cancer Classification—A Machine Learning Approach,&quot; Computational Biology and Chemistry, vol. 29, no. 1, pp. 37-46, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input_miss">input_miss</a></code>, <code><a href="#topic+select.process">select.process</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example for dataset without missing values
data(data_test)

# class label must be factor
data_test[,ncol(data_test)]&lt;-as.factor(data_test[,ncol(data_test)])

out=select.relief(data_test)

# example for dataset with missing values
## Not run: 
data(leukemia_miss)
xdata=leukemia_miss

# class label must be factor
xdata[,ncol(xdata)]&lt;-as.factor(xdata[,ncol(xdata)])

# nominal features must be factors
attrs.nominal=101
xdata[,attrs.nominal]&lt;-as.factor(xdata[,attrs.nominal])

delThre=0.2
out=input_miss(xdata,"mean.value",attrs.nominal,delThre)
if(out$flag.miss)
{
 xdata=out$data
}
out=select.relief(xdata)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
