<!DOCTYPE html><html><head><title>Help for package spls</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {spls}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ci.spls'><p>Calculate bootstrapped confidence intervals of SPLS coefficients</p></a></li>
<li><a href='#coefplot.spls'><p>Plot estimated coefficients of the SPLS object</p></a></li>
<li><a href='#correct.spls'><p>Correct the initial SPLS coefficient estimates</p>
based on bootstrapped confidence intervals</a></li>
<li><a href='#cv.sgpls'><p>Compute and plot the cross-validated error for SGPLS classification</p></a></li>
<li><a href='#cv.spls'><p>Compute and plot cross-validated mean squared prediction error for SPLS regression</p></a></li>
<li><a href='#cv.splsda'><p>Compute and plot cross-validated error for SPLSDA classification</p></a></li>
<li><a href='#lymphoma'><p>Lymphoma Gene Expression Dataset</p></a></li>
<li><a href='#mice'><p>Mice Dataset</p></a></li>
<li><a href='#plot.spls'><p>Plot the coefficient path of SPLS regression</p></a></li>
<li><a href='#predict.sgpls'>
<p>Make predictions or extract coefficients from a fitted SGPLS model</p></a></li>
<li><a href='#predict.spls'>
<p>Make predictions or extract coefficients from a fitted SPLS model</p></a></li>
<li><a href='#predict.splsda'>
<p>Make predictions or extract coefficients from a fitted SPLSDA model</p></a></li>
<li><a href='#print.sgpls'><p>Print function for a SGPLS object</p></a></li>
<li><a href='#print.spls'><p>Print function for a SPLS object</p></a></li>
<li><a href='#print.splsda'><p>Print function for a SPLSDA object</p></a></li>
<li><a href='#prostate'><p>Prostate Tumor Gene Expression Dataset</p></a></li>
<li><a href='#sgpls'><p>Fit SGPLS classification models</p></a></li>
<li><a href='#spls'><p>Fit SPLS regression models</p></a></li>
<li><a href='#spls-internal'><p>Internal SPLS functions</p></a></li>
<li><a href='#splsda'><p>Fit SPLSDA classification models</p></a></li>
<li><a href='#yeast'><p>Yeast Cell Cycle Dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.2-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-05-04</td>
</tr>
<tr>
<td>Title:</td>
<td>Sparse Partial Least Squares (SPLS) Regression and
Classification</td>
</tr>
<tr>
<td>Author:</td>
<td>Dongjun Chung &lt;chungdon@stat.wisc.edu&gt;, Hyonho Chun
        &lt;chun@stat.wisc.edu&gt;, Sunduz Keles &lt;keles@stat.wisc.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Valentin Todorov &lt;valentin.todorov@chello.at&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, nnet, parallel, pls</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for fitting a sparse
        partial least squares (SPLS) regression and classification
        (Chun and Keles (2010) &lt;<a href="https://doi.org/10.1111%2Fj.1467-9868.2009.00723.x">doi:10.1111/j.1467-9868.2009.00723.x</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-05-04 21:53:24 UTC; Share</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-05-04 23:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='ci.spls'>Calculate bootstrapped confidence intervals of SPLS coefficients</h2><span id='topic+ci.spls'></span>

<h3>Description</h3>

<p>Calculate bootstrapped confidence intervals of coefficients
of the selected predictors and generate confidence interval plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci.spls( object, coverage=0.95, B=1000,
        plot.it=FALSE, plot.fix="y",
        plot.var=NA, K=object$K, fit=object$fit )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci.spls_+3A_object">object</code></td>
<td>
<p> A fitted SPLS object. </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_coverage">coverage</code></td>
<td>
<p> Coverage of confidence intervals.
<code>coverage</code> should have a number between 0 and 1.
Default is 0.95 (95<code class="reqn">\%</code> confidence interval). </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_b">B</code></td>
<td>
<p> Number of bootstrap iterations. Default is 1000. </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_plot.it">plot.it</code></td>
<td>
<p> Plot confidence intervals of coefficients? </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_plot.fix">plot.fix</code></td>
<td>
<p> If <code>plot.fix="y"</code>, then plot confidence intervals
of the predictors for a given response.
If <code>plot.fix="x"</code>, then plot confidence intervals
of a given predictor across all the responses.
Relevant only when <code>plot.it=TRUE</code>. </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_plot.var">plot.var</code></td>
<td>
<p> Index vector of responses (if  <code>plot.fix="y"</code>)
or predictors (if  <code>plot.fix="x"</code>) to be fixed in <code>plot.fix</code>.
The indices of predictors are defined
among the set of the selected predictors.
Relevant only when <code>plot.it=TRUE</code>. </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_k">K</code></td>
<td>
<p> Number of hidden components.
Default is to use the same <code>K</code> as in the original SPLS fit. </p>
</td></tr>
<tr><td><code id="ci.spls_+3A_fit">fit</code></td>
<td>
<p> PLS algorithm for model fitting. Alternatives are
<code>"kernelpls"</code>, <code>"widekernelpls"</code>,
<code>"simpls"</code>, or <code>"oscorespls"</code>.
Default is to use the same PLS algorithm
as in the original SPLS fit.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> Invisibly returns a list with components:
</p>
<table>
<tr><td><code>cibeta</code></td>
<td>
<p> A list with as many matrix elements as the number of responses.
Each matrix element is p by 2, where i-th row of the matrix
lists the upper and lower bounds of the bootstrapped confidence
interval of the i-th predictor. </p>
</td></tr>
<tr><td><code>betahat</code></td>
<td>
<p> Matrix of original coefficients of the SPLS fit. </p>
</td></tr>
<tr><td><code>lbmat</code></td>
<td>
<p> Matrix of lower bounds of confidence intervals (for internal use). </p>
</td></tr>
<tr><td><code>ubmat</code></td>
<td>
<p> Matrix of upper bounds of confidence intervals (for internal use). </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+correct.spls">correct.spls</a></code> and <code><a href="#topic+spls">spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mice)
# SPLS with eta=0.6 &amp; 1 hidden components
f &lt;- spls( mice$x, mice$y, K=1, eta=0.6 )
# Calculate confidence intervals of coefficients
ci.f &lt;- ci.spls( f, plot.it=TRUE, plot.fix="x", plot.var=20 )
# Bootstrapped confidence intervals
cis &lt;- ci.f$cibeta
cis[[20]]   # equivalent, 'cis$1422478_a_at'
</code></pre>

<hr>
<h2 id='coefplot.spls'>Plot estimated coefficients of the SPLS object</h2><span id='topic+coefplot.spls'></span>

<h3>Description</h3>

<p>Plot estimated coefficients of the selected predictors in the SPLS object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coefplot.spls( object, nwin=c(2,2),
            xvar=c(1:length(object$A)), ylimit=NA )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coefplot.spls_+3A_object">object</code></td>
<td>
<p> A fitted SPLS object. </p>
</td></tr>
<tr><td><code id="coefplot.spls_+3A_nwin">nwin</code></td>
<td>
<p> Vector of the number of rows and columns in a plotting area.
Default is two rows and two columns, i.e., four plots.</p>
</td></tr>
<tr><td><code id="coefplot.spls_+3A_xvar">xvar</code></td>
<td>
<p> Index of variables to be plotted among the set of the selected predictors.
Default is to plot the coefficients of all the selected predictors.</p>
</td></tr>
<tr><td><code id="coefplot.spls_+3A_ylimit">ylimit</code></td>
<td>
<p> Range of the y axis (the coefficients) in the plot.
If <code>ylimit</code> is not specified, the y axis of the plot has the range
between the minimum and the maximum of all coefficient estimates. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plot is useful for visualizing coefficient estimates of a variable
for different responses. Hence, the function is applicable
only with multivariate response SPLS.
</p>


<h3>Value</h3>

<p>NULL.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ci.spls">ci.spls</a></code>, and <code><a href="#topic+correct.spls">correct.spls</a></code> and
<code><a href="#topic+plot.spls">plot.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
# SPLS with eta=0.7 &amp; 8 hidden components
f &lt;- spls( yeast$x, yeast$y, K=8, eta=0.7 )
# Draw estimated coefficient plot of the first four variables
# among the selected predictors
coefplot.spls( f, xvar=c(1:4), nwin=c(2,2) )
</code></pre>

<hr>
<h2 id='correct.spls'>Correct the initial SPLS coefficient estimates
based on bootstrapped confidence intervals</h2><span id='topic+correct.spls'></span>

<h3>Description</h3>

<p>Correct initial SPLS coefficient estimates of the selected predictors
based on bootstrapped confidence intervals and draw
heatmap of original and corrected coefficient estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct.spls( object, plot.it=TRUE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct.spls_+3A_object">object</code></td>
<td>
<p> An object obtained from the function <code>ci.spls</code>. </p>
</td></tr>
<tr><td><code id="correct.spls_+3A_plot.it">plot.it</code></td>
<td>
<p> Draw the heatmap of
original coefficient estimates
and corrected coefficient estimates? </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The set of the selected variables is updated by setting
the coefficients with zero-containing confidence intervals to zero.
</p>


<h3>Value</h3>

<p> Invisibly returns a matrix of corrected coefficient estimates.
</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ci.spls">ci.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mice)
# SPLS with eta=0.6 &amp; 1 latent components
f &lt;- spls( mice$x, mice$y, K=1, eta=0.6 )
# Calculate confidence intervals of coefficients
ci.f &lt;- ci.spls(f)
# Corrected coefficient estimates
cf &lt;- correct.spls( ci.f )
cf[20,1:5]
</code></pre>

<hr>
<h2 id='cv.sgpls'>Compute and plot the cross-validated error for SGPLS classification</h2><span id='topic+cv.sgpls'></span>

<h3>Description</h3>

<p>Draw heatmap of
v-fold cross-validated misclassification rates
and return optimal eta (thresholding parameter)
and K (number of hidden components).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.sgpls( x, y, fold=10, K, eta, scale.x=TRUE, plot.it=TRUE,
        br=TRUE, ftype='iden', n.core=8 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.sgpls_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_y">y</code></td>
<td>
<p> Vector of class indices. </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_fold">fold</code></td>
<td>
<p> Number of cross-validation folds. Default is 10-folds. </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_plot.it">plot.it</code></td>
<td>
<p> Draw the heatmap of cross-validated misclassification rates? </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_br">br</code></td>
<td>
<p> Apply Firth's bias reduction procedure? </p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_ftype">ftype</code></td>
<td>
<p> Type of Firth's bias reduction procedure.
Alternatives are <code>"iden"</code> (the approximated version)
or <code>"hat"</code> (the original version).
Default is <code>"iden"</code>.</p>
</td></tr>
<tr><td><code id="cv.sgpls_+3A_n.core">n.core</code></td>
<td>
<p> Number of CPUs to be used when parallel computing is utilized. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parallel computing can be utilized for faster computation.
Users can change the number of CPUs to be used
by changing the argument <code>n.core</code>.
</p>


<h3>Value</h3>

<p>Invisibly returns a list with components:
</p>
<table>
<tr><td><code>err.mat</code></td>
<td>
<p> Matrix of cross-validated misclassification rates.
Rows correspond to <code>eta</code> and
columns correspond to number of components (<code>K</code>). </p>
</td></tr>
<tr><td><code>eta.opt</code></td>
<td>
<p> Optimal <code>eta</code>. </p>
</td></tr>
<tr><td><code>K.opt</code></td>
<td>
<p> Optimal <code>K</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.sgpls">print.sgpls</a></code>, <code><a href="#topic+predict.sgpls">predict.sgpls</a></code>,
and <code><a href="#topic+coef.sgpls">coef.sgpls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(prostate)
    set.seed(1)

    # misclassification rate plot. eta is searched between 0.1 and 0.9 and
    # number of hidden components is searched between 1 and 5
    ## Not run: 
        cv &lt;- cv.sgpls(prostate$x, prostate$y, K = c(1:5), eta = seq(0.1,0.9,0.1),
            scale.x=FALSE, fold=5)
    
## End(Not run)
    
    

    (sgpls(prostate$x, prostate$y, eta=cv$eta.opt, K=cv$K.opt, scale.x=FALSE))
</code></pre>

<hr>
<h2 id='cv.spls'>Compute and plot cross-validated mean squared prediction error for SPLS regression</h2><span id='topic+cv.spls'></span>

<h3>Description</h3>

<p>Draw heatmap of
v-fold cross-validated mean squared prediction error
and return optimal eta (thresholding parameter)
and K (number of hidden components).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.spls( x, y, fold=10, K, eta, kappa=0.5,
        select="pls2", fit="simpls",
        scale.x=TRUE, scale.y=FALSE, plot.it=TRUE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.spls_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_y">y</code></td>
<td>
<p> Vector or matrix of responses. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_fold">fold</code></td>
<td>
<p> Number of cross-validation folds. Default is 10-folds. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_kappa">kappa</code></td>
<td>
<p> Parameter to control the effect of
the concavity of the objective function
and the closeness of original and surrogate direction vectors.
<code>kappa</code> is relevant only when responses are multivariate.
<code>kappa</code> should be between 0 and 0.5. Default is 0.5. </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_select">select</code></td>
<td>
<p> PLS algorithm for variable selection.
Alternatives are <code>"pls2"</code> or <code>"simpls"</code>.
Default is <code>"pls2"</code>.</p>
</td></tr>
<tr><td><code id="cv.spls_+3A_fit">fit</code></td>
<td>
<p> PLS algorithm for model fitting. Alternatives are
<code>"kernelpls"</code>, <code>"widekernelpls"</code>,
<code>"simpls"</code>, or <code>"oscorespls"</code>.
Default is <code>"simpls"</code>.</p>
</td></tr>
<tr><td><code id="cv.spls_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_scale.y">scale.y</code></td>
<td>
<p> Scale responses by dividing each response variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="cv.spls_+3A_plot.it">plot.it</code></td>
<td>
<p> Draw heatmap of cross-validated mean squared prediction error? </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns a list with components:
</p>
<table>
<tr><td><code>mspemat</code></td>
<td>
<p> Matrix of cross-validated mean squared prediction error.
Rows correspond to <code>eta</code> and
columns correspond to the number of components (<code>K</code>). </p>
</td></tr>
<tr><td><code>eta.opt</code></td>
<td>
<p> Optimal <code>eta</code>. </p>
</td></tr>
<tr><td><code>K.opt</code></td>
<td>
<p> Optimal <code>K</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.spls">print.spls</a></code>, <code><a href="#topic+plot.spls">plot.spls</a></code>, <code><a href="#topic+predict.spls">predict.spls</a></code>,
and <code><a href="#topic+coef.spls">coef.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(yeast)
    set.seed(1)

    # MSPE plot. eta is searched between 0.1 and 0.9 and
    # number of hidden components is searched between 1 and 10

    ## Not run: 
        cv &lt;- cv.spls(yeast$x, yeast$y, K = c(1:10), eta = seq(0.1,0.9,0.1))
        
        # Optimal eta and K
        cv$eta.opt
        cv$K.opt
        (spls(yeast$x, yeast$y, eta=cv$eta.opt, K=cv$K.opt))
    
## End(Not run)
</code></pre>

<hr>
<h2 id='cv.splsda'>Compute and plot cross-validated error for SPLSDA classification</h2><span id='topic+cv.splsda'></span>

<h3>Description</h3>

<p>Draw heatmap of
v-fold cross-validated misclassification rates
and return optimal eta (thresholding parameter)
and K (number of hidden components).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.splsda( x, y, fold=10, K, eta, kappa=0.5,
        classifier=c('lda','logistic'), scale.x=TRUE, plot.it=TRUE, n.core=8 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.splsda_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_y">y</code></td>
<td>
<p> Vector of class indices. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_fold">fold</code></td>
<td>
<p> Number of cross-validation folds. Default is 10-folds. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_kappa">kappa</code></td>
<td>
<p> Parameter to control the effect of
the concavity of the objective function
and the closeness of original and surrogate direction vectors.
<code>kappa</code> is relevant only for multicategory classification.
<code>kappa</code> should be between 0 and 0.5. Default is 0.5. </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_classifier">classifier</code></td>
<td>
<p> Classifier used in the second step of SPLSDA.
Alternatives are <code>"logistic"</code> or <code>"lda"</code>.
Default is <code>"lda"</code>.</p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_plot.it">plot.it</code></td>
<td>
<p> Draw the heatmap of the cross-validated misclassification rates? </p>
</td></tr>
<tr><td><code id="cv.splsda_+3A_n.core">n.core</code></td>
<td>
<p> Number of CPUs to be used when parallel computing is utilized. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parallel computing can be utilized for faster computation.
Users can change the number of CPUs to be used
by changing the argument <code>n.core</code>.
</p>


<h3>Value</h3>

<p>Invisibly returns a list with components:
</p>
<table>
<tr><td><code>err.mat</code></td>
<td>
<p> Matrix of cross-validated misclassification rates.
Rows correspond to <code>eta</code> and
columns correspond to number of components (<code>K</code>). </p>
</td></tr>
<tr><td><code>eta.opt</code></td>
<td>
<p> Optimal <code>eta</code>. </p>
</td></tr>
<tr><td><code>K.opt</code></td>
<td>
<p> Optimal <code>K</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.splsda">print.splsda</a></code>, <code><a href="#topic+predict.splsda">predict.splsda</a></code>,
and <code><a href="#topic+coef.splsda">coef.splsda</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
set.seed(1)
# misclassification rate plot. eta is searched between 0.1 and 0.9 and
# number of hidden components is searched between 1 and 5
## Not run:  cv &lt;- cv.splsda( prostate$x, prostate$y, K = c(1:5), eta = seq(0.1,0.9,0.1),
         scale.x=FALSE, fold=5 )
## End(Not run)

(splsda( prostate$x, prostate$y, eta=cv$eta.opt, K=cv$K.opt, scale.x=FALSE ))
</code></pre>

<hr>
<h2 id='lymphoma'>Lymphoma Gene Expression Dataset</h2><span id='topic+lymphoma'></span>

<h3>Description</h3>

<p>This is the Lymphoma Gene Expression dataset used in Chung and Keles (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> data(lymphoma) </code></pre>


<h3>Format</h3>

<p>A list with two components:
</p>

<dl>
<dt>x</dt><dd><p> Gene expression data. A matrix with 62 rows and 4026 columns.</p>
</dd>
<dt>y</dt><dd><p> Class index. A vector with 62 elements.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The lymphoma dataset consists of 42 samples of diffuse large B-cell lymphoma (DLBCL),
9 samples of follicular lymphoma (FL),
and 11 samples of chronic lymphocytic leukemia (CLL).  
DBLCL, FL, and CLL classes are coded in 0, 1, and 2, respectively, in <code>y</code> vector.
Matrix <code>x</code> is gene expression data and
arrays were normalized, imputed, log transformed, and standardized 
to zero mean and unit variance across genes as described
in Dettling (2004) and Dettling and Beuhlmann (2002).
See Chung and Keles (2010) for more details. 
</p>


<h3>Source</h3>

<p>Alizadeh A, Eisen MB, Davis RE, Ma C, Lossos IS, Rosenwald A, Boldrick JC,
Sabet H, Tran T, Yu X, Powell JI, Yang L, Marti GE, Moore T, Hudson J Jr, 
Lu L, Lewis DB, Tibshirani R, Sherlock G, Chan WC, Greiner TC, 
Weisenburger DD, Armitage JO, Warnke R, Levy R, Wilson W, Grever MR, Byrd JC,
Botstein D, Brown PO, and Staudt LM (2000), &quot;Distinct types of diffuse large
B-cell lymphoma identified by gene expression profiling&quot;, <em>Nature</em>, Vol. 403, pp. 503&ndash;511.
</p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Dettling M (2004), &quot;BagBoosting for tumor classification with gene expression data&quot;,
<em>Bioinformatics</em>, Vol. 20, pp. 3583&ndash;3593.
</p>
<p>Dettling M and Beuhlmann P (2002), &quot;Supervised clustering of genes&quot;, <em>Genome Biology</em>,
Vol. 3, pp. research0069.1&ndash;0069.15.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lymphoma)
lymphoma$x[1:5,1:5]
lymphoma$y
</code></pre>

<hr>
<h2 id='mice'>Mice Dataset</h2><span id='topic+mice'></span>

<h3>Description</h3>

<p>This is the Mice dataset used in Chun and Keles (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> data(mice) </code></pre>


<h3>Format</h3>

<p>A list with two components:
</p>

<dl>
<dt>x</dt><dd><p> Marker map data. A matrix with 60 rows and 145 columns.</p>
</dd>
<dt>y</dt><dd><p> Gene expression data. A matrix with 60 rows and 83 columns.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Mice dataset was published by Lan et al. (2006). Matrix <code>x</code> is
the marker map consisting of 145 microsatellite markers from 19 non-sex mouse chromosomes. 
Matrix <code>y</code> is gene expression measurements of the 83 transcripts
from liver tissues of 60 mice. This group of the 83 transcripts is one of the clusters
analyzed by Chun and Keles (2010). See Chun and Keles (2010) for more details.                                  
</p>


<h3>Source</h3>

<p>Lan H, Chen M, Flowers JB, Yandell BS, Stapleton DS, Mata CM, Mui E, Flowers MT, 
Schueler KL, Manly KF, Williams RW, Kendziorski C, and Attie AD (2006),
&quot;Combined expression trait correlations and expression quantitative trait locus mapping&quot;,
<em>PLoS Genetics</em>, Vol. 2, e6.</p>


<h3>References</h3>

<p>Chun H and Keles S (2009), &quot;Expression quantitative trait loci mapping 
with multivariate sparse partial least squares regression&quot;, <em>Genetics</em>, Vol. 182, pp. 79&ndash;90.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mice)
mice$x[1:5,1:5]
mice$y[1:5,1:5]
</code></pre>

<hr>
<h2 id='plot.spls'>Plot the coefficient path of SPLS regression</h2><span id='topic+plot.spls'></span>

<h3>Description</h3>

<p>Provide the coefficient path plot of SPLS regression as a function of
the number of hidden components (K) when eta is fixed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spls'
plot( x, yvar=c(1:ncol(x$y)), ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.spls_+3A_x">x</code></td>
<td>
<p> A fitted SPLS object. </p>
</td></tr>
<tr><td><code id="plot.spls_+3A_yvar">yvar</code></td>
<td>
<p> Index vector of responses to be plotted.</p>
</td></tr>
<tr><td><code id="plot.spls_+3A_...">...</code></td>
<td>
<p> Other parameters to be passed through to generic <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.spls</code> provides the coefficient path plot of SPLS fits.
The plot shows how estimated coefficients change
as a function of the number of hidden components (<code>K</code>),
when <code>eta</code> is fixed at the value used by the original SPLS fit.
</p>


<h3>Value</h3>

<p>NULL.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.spls">print.spls</a></code>, <code><a href="#topic+predict.spls">predict.spls</a></code>,
and <code><a href="#topic+coef.spls">coef.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
# SPLS with eta=0.7 &amp; 8 hidden components
f &lt;- spls( yeast$x, yeast$y, K=8, eta=0.7 )
# Draw coefficient path plots for the first two responses
plot( f, yvar=c(1:2) )
</code></pre>

<hr>
<h2 id='predict.sgpls'>
Make predictions or extract coefficients from a fitted SGPLS model
</h2><span id='topic+predict.sgpls'></span><span id='topic+coef.sgpls'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a fitted SGPLS object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgpls'
predict( object, newx, type = c("fit","coefficient"),
    fit.type = c("class","response"), ... )
## S3 method for class 'sgpls'
coef( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sgpls_+3A_object">object</code></td>
<td>
<p> A fitted SGPLS object.</p>
</td></tr>
<tr><td><code id="predict.sgpls_+3A_newx">newx</code></td>
<td>

<p>If <code>type="fit"</code>, then <code>newx</code> should be the predictor matrix of test dataset.
If newx is omitted, then prediction of training dataset is returned.
If <code>type="coefficient"</code>, then <code>newx</code> can be omitted.
</p>
</td></tr>
<tr><td><code id="predict.sgpls_+3A_type">type</code></td>
<td>

<p>If <code>type="fit"</code>, fitted values are returned.
If <code>type="coefficient"</code>,
coefficient estimates of SGPLS fits are returned.
</p>
</td></tr>
<tr><td><code id="predict.sgpls_+3A_fit.type">fit.type</code></td>
<td>

<p>If <code>fit.type="class"</code>, fitted classes are returned.
If <code>fit.type="response"</code>, fitted probabilities are returned.
Relevant only when <code>type="fit"</code>.
</p>
</td></tr>
<tr><td><code id="predict.sgpls_+3A_...">...</code></td>
<td>
<p> Any arguments for <code>predict.sgpls</code>
should work for <code>coef.sgpls</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can input either only selected variables or all variables for <code>newx</code>.
</p>


<h3>Value</h3>

<p>Matrix of coefficient estimates if <code>type="coefficient"</code>.
Matrix of predicted responses if <code>type="fit"</code>
(responses will be predicted classes if <code>fit.type="class"</code>
or predicted probabilities if <code>fit.type="response"</code>).</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.sgpls">print.sgpls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
# SGPLS with eta=0.55 &amp; 3 hidden components
f &lt;- sgpls( prostate$x, prostate$y, K=3, eta=0.55, scale.x=FALSE )
# Print out coefficients
coef.f &lt;- coef(f)
coef.f[ coef.f!=0, ]
# Prediction on the training dataset
(pred.f &lt;- predict( f, type="fit" ))
</code></pre>

<hr>
<h2 id='predict.spls'>
Make predictions or extract coefficients from a fitted SPLS model
</h2><span id='topic+predict.spls'></span><span id='topic+coef.spls'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a fitted SPLS object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spls'
predict( object, newx, type = c("fit","coefficient"), ... )
## S3 method for class 'spls'
coef( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.spls_+3A_object">object</code></td>
<td>
<p> A fitted SPLS object.</p>
</td></tr>
<tr><td><code id="predict.spls_+3A_newx">newx</code></td>
<td>

<p>If <code>type="fit"</code>, then <code>newx</code> should be the predictor matrix of test dataset.
If newx is omitted, then prediction of training dataset is returned.
If <code>type="coefficient"</code>, then <code>newx</code> can be omitted.
</p>
</td></tr>
<tr><td><code id="predict.spls_+3A_type">type</code></td>
<td>

<p>If <code>type="fit"</code>, fitted values are returned.
If <code>type="coefficient"</code>,
coefficient estimates of SPLS fits are returned.
</p>
</td></tr>
<tr><td><code id="predict.spls_+3A_...">...</code></td>
<td>
<p> Any arguments for <code>predict.spls</code>
should work for <code>coef.spls</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can input either only selected variables or all variables for <code>newx</code>.
</p>


<h3>Value</h3>

<p>Matrix of coefficient estimates if <code>type="coefficient"</code>.
Matrix of predicted responses if <code>type="fit"</code>.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+plot.spls">plot.spls</a></code> and <code><a href="#topic+print.spls">print.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
# SPLS with eta=0.7 &amp; 8 latent components
f &lt;- spls( yeast$x, yeast$y, K=8, eta=0.7 )
# Coefficient estimates of the SPLS fit
coef.f &lt;- coef(f)
coef.f[1:5,]
# Prediction on the training dataset
pred.f &lt;- predict( f, type="fit" )
pred.f[1:5,]
</code></pre>

<hr>
<h2 id='predict.splsda'>
Make predictions or extract coefficients from a fitted SPLSDA model
</h2><span id='topic+predict.splsda'></span><span id='topic+coef.splsda'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a fitted SPLSDA object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'splsda'
predict( object, newx, type = c("fit","coefficient"),
    fit.type = c("class","response"), ... )
## S3 method for class 'splsda'
coef( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.splsda_+3A_object">object</code></td>
<td>
<p> A fitted SPLSDA object.</p>
</td></tr>
<tr><td><code id="predict.splsda_+3A_newx">newx</code></td>
<td>

<p>If <code>type="fit"</code>, then <code>newx</code> should be the predictor matrix of test dataset.
If newx is omitted, then prediction of training dataset is returned.
If <code>type="coefficient"</code>, then <code>newx</code> can be omitted.
</p>
</td></tr>
<tr><td><code id="predict.splsda_+3A_type">type</code></td>
<td>

<p>If <code>type="fit"</code>, fitted values are returned.
If <code>type="coefficient"</code>,
coefficient estimates of SPLSDA fits are returned.
</p>
</td></tr>
<tr><td><code id="predict.splsda_+3A_fit.type">fit.type</code></td>
<td>

<p>If <code>fit.type="class"</code>, fitted classes are returned.
If <code>fit.type="response"</code>, fitted probabilities are returned.
Relevant only when <code>type="fit"</code>.
</p>
</td></tr>
<tr><td><code id="predict.splsda_+3A_...">...</code></td>
<td>
<p> Any arguments for <code>predict.splsda</code>
should work for <code>coef.splsda</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can input either only selected variables or all variables for <code>newx</code>.
</p>


<h3>Value</h3>

<p>Matrix of coefficient estimates if <code>type="coefficient"</code>.
Matrix of predicted responses if <code>type="fit"</code>
(responses will be predicted classes if <code>fit.type="class"</code>
or predicted probabilities if <code>fit.type="response"</code>).</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.splsda">print.splsda</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
# SPLSDA with eta=0.8 &amp; 3 hidden components
f &lt;- splsda( prostate$x, prostate$y, K=3, eta=0.8, scale.x=FALSE )
# Print out coefficients
coef.f &lt;- coef(f)
coef.f[ coef.f!=0, ]
# Prediction on the training dataset
(pred.f &lt;- predict( f, type="fit" ))
</code></pre>

<hr>
<h2 id='print.sgpls'>Print function for a SGPLS object</h2><span id='topic+print.sgpls'></span>

<h3>Description</h3>

<p>Print out SGPLS fit, the number and the list of selected predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgpls'
print( x, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.sgpls_+3A_x">x</code></td>
<td>
<p> A fitted SGPLS object.</p>
</td></tr>
<tr><td><code id="print.sgpls_+3A_...">...</code></td>
<td>
<p> Additonal arguments for generic <code>print</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.sgpls">predict.sgpls</a></code> and <code><a href="#topic+coef.sgpls">coef.sgpls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
# SGPLS with eta=0.55 &amp; 3 hidden components
f &lt;- sgpls( prostate$x, prostate$y, K=3, eta=0.55, scale.x=FALSE )
print(f)
</code></pre>

<hr>
<h2 id='print.spls'>Print function for a SPLS object</h2><span id='topic+print.spls'></span>

<h3>Description</h3>

<p>Print out SPLS fit, the number and the list of selected predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spls'
print( x, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.spls_+3A_x">x</code></td>
<td>
<p> A fitted SPLS object.</p>
</td></tr>
<tr><td><code id="print.spls_+3A_...">...</code></td>
<td>
<p> Additonal arguments for generic <code>print</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection,&quot;
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+plot.spls">plot.spls</a></code>, <code><a href="#topic+predict.spls">predict.spls</a></code>,
and <code><a href="#topic+coef.spls">coef.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
# SPLS with eta=0.7 &amp; 8 hidden components
f &lt;- spls( yeast$x, yeast$y, K=8, eta=0.7 )
print(f)
</code></pre>

<hr>
<h2 id='print.splsda'>Print function for a SPLSDA object</h2><span id='topic+print.splsda'></span>

<h3>Description</h3>

<p>Print out SPLSDA fits, the number and the list of selected predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'splsda'
print( x, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.splsda_+3A_x">x</code></td>
<td>
<p> A fitted SPLSDA object.</p>
</td></tr>
<tr><td><code id="print.splsda_+3A_...">...</code></td>
<td>
<p> Additonal arguments for generic <code>print</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.splsda">predict.splsda</a></code> and <code><a href="#topic+coef.splsda">coef.splsda</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
# SPLSDA with eta=0.8 &amp; 3 hidden components
f &lt;- splsda( prostate$x, prostate$y, K=3, eta=0.8, scale.x=FALSE )
print(f)
</code></pre>

<hr>
<h2 id='prostate'>Prostate Tumor Gene Expression Dataset</h2><span id='topic+prostate'></span>

<h3>Description</h3>

<p>This is the Prostate Tumor Gene Expression dataset used in Chung and Keles (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> data(prostate) </code></pre>


<h3>Format</h3>

<p>A list with two components:
</p>

<dl>
<dt>x</dt><dd><p> Gene expression data. A matrix with 102 rows and 6033 columns.</p>
</dd>
<dt>y</dt><dd><p> Class index. A vector with 102 elements.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The prostate dataset consists of 52 prostate tumor and 50 normal samples.
Normal and tumor classes are coded in 0 and 1, respectively, in <code>y</code> vector.
Matrix <code>x</code> is gene expression data and
arrays were normalized, log transformed, and standardized 
to zero mean and unit variance across genes as described
in Dettling (2004) and Dettling and Beuhlmann (2002).
See Chung and Keles (2010) for more details. 
</p>


<h3>Source</h3>

<p>Singh D, Febbo P, Ross K, Jackson D, Manola J, Ladd C, Tamayo P, Renshaw A,
DAmico A, Richie J, Lander E, Loda M, Kantoff P, Golub T, and Sellers W (2002),
&quot;Gene expression correlates of clinical prostate cancer behavior&quot;, <em>Cancer Cell</em>, 
Vol. 1, pp. 203&ndash;209.
</p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Dettling M (2004), &quot;BagBoosting for tumor classification with gene expression data&quot;,
<em>Bioinformatics</em>, Vol. 20, pp. 3583&ndash;3593.
</p>
<p>Dettling M and Beuhlmann P (2002), &quot;Supervised clustering of genes&quot;, <em>Genome Biology</em>,
Vol. 3, pp. research0069.1&ndash;0069.15.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
prostate$x[1:5,1:5]
prostate$y
</code></pre>

<hr>
<h2 id='sgpls'>Fit SGPLS classification models</h2><span id='topic+sgpls'></span>

<h3>Description</h3>

<p>Fit a SGPLS classification model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgpls( x, y, K, eta, scale.x=TRUE,
        eps=1e-5, denom.eps=1e-20, zero.eps=1e-5, maxstep=100,
        br=TRUE, ftype='iden' )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgpls_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_y">y</code></td>
<td>
<p> Vector of class indices. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="sgpls_+3A_eps">eps</code></td>
<td>
<p> An effective zero for change in estimates. Default is 1e-5. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_denom.eps">denom.eps</code></td>
<td>
<p> An effective zero for denominators. Default is 1e-20. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_zero.eps">zero.eps</code></td>
<td>
<p> An effective zero for success probabilities. Default is 1e-5. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_maxstep">maxstep</code></td>
<td>
<p> Maximum number of Newton-Raphson iterations.
Default is 100. </p>
</td></tr>
<tr><td><code id="sgpls_+3A_br">br</code></td>
<td>
<p> Apply Firth's bias reduction procedure? </p>
</td></tr>
<tr><td><code id="sgpls_+3A_ftype">ftype</code></td>
<td>
<p> Type of Firth's bias reduction procedure.
Alternatives are <code>"iden"</code> (the approximated version)
or <code>"hat"</code> (the original version).
Default is <code>"iden"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SGPLS method is described in detail in Chung and Keles (2010).
SGPLS provides PLS-based classification with variable selection,
by incorporating sparse partial least squares (SPLS) proposed in Chun and Keles (2010)
into a generalized linear model (GLM) framework.
<code>y</code> is assumed to have numerical values, 0, 1, ..., G,
where G is the number of classes subtracted by one.
</p>


<h3>Value</h3>

<p>A <code>sgpls</code> object is returned.
print, predict, coef methods use this object.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.sgpls">print.sgpls</a></code>, <code><a href="#topic+predict.sgpls">predict.sgpls</a></code>, and <code><a href="#topic+coef.sgpls">coef.sgpls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(prostate)

    # SGPLS with eta=0.6 &amp; 3 hidden components
    (f &lt;- sgpls(prostate$x, prostate$y, K=3, eta=0.6, scale.x=FALSE))

    # Print out coefficients
    coef.f &lt;- coef(f)
    coef.f[coef.f!=0, ]
</code></pre>

<hr>
<h2 id='spls'>Fit SPLS regression models</h2><span id='topic+spls'></span>

<h3>Description</h3>

<p>Fit a SPLS regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spls( x, y, K, eta, kappa=0.5, select="pls2", fit="simpls",
    scale.x=TRUE, scale.y=FALSE, eps=1e-4, maxstep=100, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spls_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="spls_+3A_y">y</code></td>
<td>
<p> Vector or matrix of responses. </p>
</td></tr>
<tr><td><code id="spls_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="spls_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="spls_+3A_kappa">kappa</code></td>
<td>
<p> Parameter to control the effect of
the concavity of the objective function
and the closeness of original and surrogate direction vectors.
<code>kappa</code> is relevant only when responses are multivariate.
<code>kappa</code> should be between 0 and 0.5. Default is 0.5. </p>
</td></tr>
<tr><td><code id="spls_+3A_select">select</code></td>
<td>
<p> PLS algorithm for variable selection.
Alternatives are <code>"pls2"</code> or <code>"simpls"</code>.
Default is <code>"pls2"</code>.</p>
</td></tr>
<tr><td><code id="spls_+3A_fit">fit</code></td>
<td>
<p> PLS algorithm for model fitting. Alternatives are
<code>"kernelpls"</code>, <code>"widekernelpls"</code>,
<code>"simpls"</code>, or <code>"oscorespls"</code>.
Default is <code>"simpls"</code>.</p>
</td></tr>
<tr><td><code id="spls_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="spls_+3A_scale.y">scale.y</code></td>
<td>
<p> Scale responses by dividing each response variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="spls_+3A_eps">eps</code></td>
<td>
<p> An effective zero. Default is 1e-4. </p>
</td></tr>
<tr><td><code id="spls_+3A_maxstep">maxstep</code></td>
<td>
<p> Maximum number of iterations when fitting direction vectors.
Default is 100. </p>
</td></tr>
<tr><td><code id="spls_+3A_trace">trace</code></td>
<td>
<p> Print out the progress of variable selection? </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SPLS method is described in detail in Chun and Keles (2010).
SPLS directly imposes sparsity on the dimension reduction step of PLS
in order to achieve accurate prediction and variable selection simultaneously.
The option <code>select</code> refers to the PLS algorithm for variable selection.
The option <code>fit</code> refers to the PLS algorithm for model fitting
and <code>spls</code> utilizes algorithms offered by the <span class="pkg">pls</span> package for this purpose.
See help files of the function <code>plsr</code> in the <span class="pkg">pls</span> package for more details.
The user should install the <span class="pkg">pls</span> package before using <span class="pkg">spls</span> functions.
The choices for <code>select</code> and <code>fit</code> are independent.
</p>


<h3>Value</h3>

<p>A <span class="pkg">spls</span> object is returned.
print, plot, predict, coef, ci.spls, coefplot.spls methods use this object.</p>


<h3>Author(s)</h3>

 
<p>Dongjun Chung, Hyonho Chun, and Sunduz Keles. 
</p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.spls">print.spls</a></code>, <code><a href="#topic+plot.spls">plot.spls</a></code>, <code><a href="#topic+predict.spls">predict.spls</a></code>,
<code><a href="#topic+coef.spls">coef.spls</a></code>, <code><a href="#topic+ci.spls">ci.spls</a></code>, and <code><a href="#topic+coefplot.spls">coefplot.spls</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(yeast)
    # SPLS with eta=0.7 &amp; 8 hidden components
    (f &lt;- spls(yeast$x, yeast$y, K=8, eta=0.7))

    # Print out coefficients
    coef.f &lt;- coef(f)
    coef.f[,1]

    # Coefficient path plot
    plot(f, yvar=1)
    dev.new()

    # Coefficient plot of selected variables
    coefplot.spls(f, xvar=c(1:4))
</code></pre>

<hr>
<h2 id='spls-internal'>Internal SPLS functions</h2><span id='topic+heatmap.spls'></span><span id='topic+spls.dv'></span><span id='topic+ust'></span><span id='topic+correctp'></span><span id='topic+cv.split'></span><span id='topic+wpls'></span><span id='topic+sgpls.binary'></span><span id='topic+sgpls.multi'></span><span id='topic+cv.sgpls.binary'></span><span id='topic+cv.sgpls.multi'></span>

<h3>Description</h3>

<p>Internal SPLS functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatmap.spls( mat, coln=16, as='n', ... )
spls.dv( Z, eta, kappa, eps, maxstep )
ust( b, eta )
correctp( x, y, eta, K, kappa, select, fit )
cv.split( y, fold )
wpls( x, y, V, K=ncol(x), type="pls1",
        center.x=TRUE, scale.x=FALSE )
sgpls.binary( x, y, K, eta, scale.x=TRUE,
        eps=1e-5, denom.eps=1e-20, zero.eps=1e-5, maxstep=100,
        br=TRUE, ftype='iden' )
sgpls.multi( x, y, K, eta, scale.x=TRUE,
        eps=1e-5, denom.eps=1e-20, zero.eps=1e-5, maxstep=100,
        br=TRUE, ftype='iden' )
cv.sgpls.binary( x, y, fold=10, K, eta, scale.x=TRUE, plot.it=TRUE,
    br=TRUE, ftype='iden', n.core=8 )
cv.sgpls.multi( x, y, fold=10, K, eta, scale.x=TRUE, plot.it=TRUE,
    br=TRUE, ftype='iden', n.core=8 )
</code></pre>


<h3>Details</h3>

<p>These are not to be called by the user.
</p>


<h3>Author(s)</h3>

<p> Dongjun Chung, Hyonho Chun, and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25. </p>

<hr>
<h2 id='splsda'>Fit SPLSDA classification models</h2><span id='topic+splsda'></span>

<h3>Description</h3>

<p>Fit a SPLSDA classification model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splsda( x, y, K, eta, kappa=0.5,
    classifier=c('lda','logistic'), scale.x=TRUE, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splsda_+3A_x">x</code></td>
<td>
<p> Matrix of predictors. </p>
</td></tr>
<tr><td><code id="splsda_+3A_y">y</code></td>
<td>
<p> Vector of class indices. </p>
</td></tr>
<tr><td><code id="splsda_+3A_k">K</code></td>
<td>
<p> Number of hidden components. </p>
</td></tr>
<tr><td><code id="splsda_+3A_eta">eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td></tr>
<tr><td><code id="splsda_+3A_kappa">kappa</code></td>
<td>
<p> Parameter to control the effect of
the concavity of the objective function
and the closeness of original and surrogate direction vectors.
<code>kappa</code> is relevant only for multicategory classification.
<code>kappa</code> should be between 0 and 0.5. Default is 0.5. </p>
</td></tr>
<tr><td><code id="splsda_+3A_classifier">classifier</code></td>
<td>
<p> Classifier used in the second step of SPLSDA.
Alternatives are <code>"logistic"</code> or <code>"lda"</code>.
Default is <code>"lda"</code>.</p>
</td></tr>
<tr><td><code id="splsda_+3A_scale.x">scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td></tr>
<tr><td><code id="splsda_+3A_...">...</code></td>
<td>
<p> Other parameters to be passed through to <code>spls</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SPLSDA method is described in detail in Chung and Keles (2010).
SPLSDA provides a two-stage approach for PLS-based classification with variable selection,
by directly imposing sparsity on the dimension reduction step of PLS
using sparse partial least squares (SPLS) proposed in Chun and Keles (2010).
<code>y</code> is assumed to have numerical values, 0, 1, ..., G,
where G is the number of classes subtracted by one.
The option <code>classifier</code> refers to the classifier used in the second step of SPLSDA
and <code>splsda</code> utilizes algorithms offered by <span class="pkg">MASS</span> and <span class="pkg">nnet</span> packages
for this purpose.
If <code>classifier="logistic"</code>, then either logistic regression or multinomial regression is used.
Linear discriminant analysis (LDA) is used if <code>classifier="lda"</code>.
<code>splsda</code> also utilizes algorithms offered by the <span class="pkg">pls</span> package for fitting <code>spls</code>.
The user should install <span class="pkg">pls</span>, <span class="pkg">MASS</span> and <span class="pkg">nnet</span> packages before using <code>splsda</code> functions.
</p>


<h3>Value</h3>

<p>A <code>splsda</code> object is returned.
print, predict, coef methods use this object.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
&quot;Sparse partial least squares classification for high dimensional data&quot;,
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+print.splsda">print.splsda</a></code>, <code><a href="#topic+predict.splsda">predict.splsda</a></code>, and <code><a href="#topic+coef.splsda">coef.splsda</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(prostate)
# SPLSDA with eta=0.8 &amp; 3 hidden components
f &lt;- splsda( prostate$x, prostate$y, K=3, eta=0.8, scale.x=FALSE )
print(f)
# Print out coefficients
coef.f &lt;- coef(f)
coef.f[ coef.f!=0, ]
</code></pre>

<hr>
<h2 id='yeast'>Yeast Cell Cycle Dataset</h2><span id='topic+yeast'></span>

<h3>Description</h3>

<p>This is the Yeast Cell Cycle dataset used in Chun and Keles (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> data(yeast) </code></pre>


<h3>Format</h3>

<p>A list with two components:
</p>

<dl>
<dt>x</dt><dd><p> ChIP-chip data. A matrix with 542 rows and 106 columns.</p>
</dd>
<dt>y</dt><dd><p> Cell cycle gene expression data.
A matrix with 542 rows and 18 columns.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Matrix <code>y</code> is cell cycle gene expression data (Spellman et al., 1998)
of 542 genes from an <code class="reqn">\alpha</code> factor based experiment.
Each column corresponds to mRNA levels
measured at every 7 minutes during 119 minutes (a total of 18 measurements).
Matrix <code>x</code> is the chromatin immunoprecipitation on chip (ChIP-chip) data of
Lee et al. (2002) and it contains the binding information for 106
transcription factors. See Chun and Keles (2010) for more details. 
</p>


<h3>Source</h3>

<p>Lee TI, Rinaldi NJ, Robert F, Odom DT, Bar-Joseph Z, Gerber GK, Hannett NM,
Harbison CT, Thomson CM, Simon I, Zeitlinger J, Jennings EG, Murray HL,
Gordon DB, Ren B, Wyrick JJ, Tagne JB, Volkert TL, Fraenkel E, Gifford DK,
and Young RA (2002), &quot;Transcriptional regulatory networks in <em>Saccharomyces cerevisiae</em>&quot;,
<em>Science</em>, Vol. 298, pp. 799&ndash;804.
</p>
<p>Spellman PT, Sherlock G, Zhang MQ, Iyer VR, Anders K, Eisen MB, Brown PO,
Botstein D, and Futcher B (1998), &quot;Comprehensive identification of cell cycle-regulated genes of
the yeast <em>Saccharomyces cerevisiae</em> by microarray hydrization&quot;,
<em>Molecular Biology of the Cell</em>, Vol. 9, pp. 3273&ndash;3279.
</p>


<h3>References</h3>

<p>Chun H and Keles S (2010), &quot;Sparse partial least squares
for simultaneous dimension reduction and variable selection&quot;,
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3&ndash;25. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
yeast$x[1:5,1:5]
yeast$y[1:5,1:5]
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
