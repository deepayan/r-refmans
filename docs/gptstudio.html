<!DOCTYPE html><html><head><title>Help for package gptstudio</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gptstudio}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gptstudio-package'><p>gptstudio: Use Large Language Models Directly in your Development Environment</p></a></li>
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#chat'><p>Chat Interface for gptstudio</p></a></li>
<li><a href='#chat_create_system_prompt'><p>Create system prompt</p></a></li>
<li><a href='#chat_history_append'><p>Append to chat history</p></a></li>
<li><a href='#chat_message_default'><p>Default chat message</p></a></li>
<li><a href='#check_api_connection_openai'><p>Check API Connection</p></a></li>
<li><a href='#create_chat_app_theme'><p>Chat App Theme</p></a></li>
<li><a href='#create_chat_cohere'><p>Create a chat with the Cohere Chat API</p></a></li>
<li><a href='#create_completion_anthropic'><p>Generate text completions using Anthropic's API</p></a></li>
<li><a href='#create_completion_azure_openai'><p>Generate text using Azure OpenAI's API</p></a></li>
<li><a href='#create_completion_google'><p>Generate text completions using Google AI Studio's API</p></a></li>
<li><a href='#create_completion_huggingface'><p>Generate text completions using HuggingFace's API</p></a></li>
<li><a href='#create_completion_perplexity'><p>Create a chat completion request to the Perplexity API</p></a></li>
<li><a href='#create_ide_matching_colors'><p>Chat message colors in RStudio</p></a></li>
<li><a href='#create_tmp_job_script'><p>Create a temporary job script</p></a></li>
<li><a href='#create_translator'><p>Internationalization for the ChatGPT addin</p></a></li>
<li><a href='#get_available_endpoints'><p>List supported endpoints</p></a></li>
<li><a href='#get_available_models'><p>List supported models</p></a></li>
<li><a href='#get_ide_theme_info'><p>Get IDE theme information.</p></a></li>
<li><a href='#gptstudio_chat'><p>Run Chat GPT</p>
Run the Chat GPT Shiny App as a background job and show it in the viewer pane</a></li>
<li><a href='#gptstudio_chat_in_source_addin'><p>ChatGPT in Source</p></a></li>
<li><a href='#gptstudio_comment_code'><p>Comment Code Addin</p></a></li>
<li><a href='#gptstudio_create_skeleton'><p>Create a Request Skeleton</p></a></li>
<li><a href='#gptstudio_request_perform'><p>Perform API Request</p></a></li>
<li><a href='#gptstudio_response_process'><p>Call API</p></a></li>
<li><a href='#gptstudio_sitrep'><p>Current Configuration for gptstudio</p></a></li>
<li><a href='#gptstudio_skeleton_build'><p>Construct a GPT Studio request skeleton.</p></a></li>
<li><a href='#gptstudio_spelling_grammar'><p>Spelling and Grammar Addin</p></a></li>
<li><a href='#mod_app_server'><p>App Server</p></a></li>
<li><a href='#mod_app_ui'><p>App UI</p></a></li>
<li><a href='#mod_chat_server'><p>Chat server</p></a></li>
<li><a href='#mod_chat_ui'><p>Chat UI</p></a></li>
<li><a href='#open_bg_shinyapp'><p>Open browser to local Shiny app</p></a></li>
<li><a href='#openai_create_chat_completion'><p>Generate text completions using OpenAI's API for Chat</p></a></li>
<li><a href='#OpenaiStreamParser'><p>Stream handler for chat completions</p></a></li>
<li><a href='#prepare_chat_history'><p>Prepare chat completion prompt</p></a></li>
<li><a href='#query_api_anthropic'><p>A function that sends a request to the Anthropic API and returns the</p>
response.</a></li>
<li><a href='#query_api_cohere'><p>Send a request to the Cohere Chat API and return the response</p></a></li>
<li><a href='#query_api_google'><p>A function that sends a request to the Google AI Studio API and returns the</p>
response.</a></li>
<li><a href='#query_api_huggingface'><p>A function that sends a request to the HuggingFace API and returns the</p>
response.</a></li>
<li><a href='#query_api_perplexity'><p>Send a request to the Perplexity API and return the response</p></a></li>
<li><a href='#query_openai_api'><p>A function that sends a request to the OpenAI API and returns the response.</p></a></li>
<li><a href='#random_port'><p>Generate a random safe port number</p></a></li>
<li><a href='#request_base'><p>Base for a request to the OPENAI API</p></a></li>
<li><a href='#request_base_anthropic'><p>Base for a request to the Anthropic API</p></a></li>
<li><a href='#request_base_cohere'><p>Base for a request to the Cohere Chat API</p></a></li>
<li><a href='#request_base_google'><p>Base for a request to the Google AI Studio API</p></a></li>
<li><a href='#request_base_huggingface'><p>Base for a request to the HuggingFace API</p></a></li>
<li><a href='#request_base_perplexity'><p>Base for a request to the Perplexity API</p></a></li>
<li><a href='#rgb_str_to_hex'><p>RGB str to hex</p></a></li>
<li><a href='#run_app_as_bg_job'><p>Run an R Shiny app in the background</p></a></li>
<li><a href='#run_chatgpt_app'><p>Run the ChatGPT app</p></a></li>
<li><a href='#stream_chat_completion'><p>Stream Chat Completion</p></a></li>
<li><a href='#streamingMessage'><p>Streaming message</p></a></li>
<li><a href='#streamingMessage-shiny'><p>Shiny bindings for streamingMessage</p></a></li>
<li><a href='#style_chat_history'><p>Style Chat History</p></a></li>
<li><a href='#style_chat_message'><p>Style chat message</p></a></li>
<li><a href='#text_area_input_wrapper'><p>Custom textAreaInput</p></a></li>
<li><a href='#welcomeMessage'><p>Welcome message</p></a></li>
<li><a href='#welcomeMessage-shiny'><p>Shiny bindings for welcomeMessage</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Use Large Language Models Directly in your Development
Environment</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Wade &lt;github@jameshwade.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Large language models are readily accessible via API. This
    package lowers the barrier to use the API inside of your development
    environment.  For more on the API, see
    <a href="https://platform.openai.com/docs/introduction">https://platform.openai.com/docs/introduction</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MichelNivard/gptstudio">https://github.com/MichelNivard/gptstudio</a>,
<a href="https://michelnivard.github.io/gptstudio/">https://michelnivard.github.io/gptstudio/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MichelNivard/gptstudio/issues">https://github.com/MichelNivard/gptstudio/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat, bslib (&ge; 0.6.0), cli, colorspace, curl,
fontawesome, glue, grDevices, htmltools, htmlwidgets, httr2,
ids, jsonlite, magrittr, purrr, R6, rlang, rstudioapi (&ge;
0.12), rvest, shiny, shiny.i18n, SSEparser, stringr (&ge; 1.5.0),
utils, waiter, yaml</td>
</tr>
<tr>
<td>Suggests:</td>
<td>AzureRMR, knitr, mockr, rmarkdown, shinytest2, spelling,
testthat (&ge; 3.0.0), withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-20 13:29:30 UTC; ua21849</td>
</tr>
<tr>
<td>Author:</td>
<td>Michel Nivard [aut, cph],
  James Wade <a href="https://orcid.org/0000-0002-9740-1905"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  Samuel Calderon <a href="https://orcid.org/0000-0001-6847-1210"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-21 11:21:21 UTC</td>
</tr>
</table>
<hr>
<h2 id='gptstudio-package'>gptstudio: Use Large Language Models Directly in your Development Environment</h2><span id='topic+gptstudio'></span><span id='topic+gptstudio-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Large language models are readily accessible via API. This package lowers the barrier to use the API inside of your development environment. For more on the API, see <a href="https://platform.openai.com/docs/introduction">https://platform.openai.com/docs/introduction</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: James Wade <a href="mailto:github@jameshwade.com">github@jameshwade.com</a> (<a href="https://orcid.org/0000-0002-9740-1905">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Michel Nivard <a href="mailto:m.g.nivard@vu.nl">m.g.nivard@vu.nl</a> [copyright holder]
</p>
</li>
<li><p> Samuel Calderon <a href="mailto:samuel.calderon@uarm.pe">samuel.calderon@uarm.pe</a> (<a href="https://orcid.org/0000-0001-6847-1210">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/MichelNivard/gptstudio">https://github.com/MichelNivard/gptstudio</a>
</p>
</li>
<li> <p><a href="https://michelnivard.github.io/gptstudio/">https://michelnivard.github.io/gptstudio/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/MichelNivard/gptstudio/issues">https://github.com/MichelNivard/gptstudio/issues</a>
</p>
</li></ul>


<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='chat'>Chat Interface for gptstudio</h2><span id='topic+chat'></span>

<h3>Description</h3>

<p>This function provides a high-level interface for communicating with various
services and models supported by gptstudio. It orchestrates the creation,
configuration, and execution of a request based on user inputs and options
set for gptstudio. The function supports a range of tasks from text
generation to code synthesis and can be customized according to skill level
and coding style preferences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat(
  prompt,
  service = getOption("gptstudio.service"),
  history = list(list(role = "system", content = "You are an R chat assistant")),
  stream = FALSE,
  model = getOption("gptstudio.model"),
  skill = getOption("gptstudio.skill"),
  style = getOption("gptstudio.code_style", "no preference"),
  task = getOption("gptstudio.task", "coding"),
  custom_prompt = NULL,
  process_response = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_+3A_prompt">prompt</code></td>
<td>
<p>A string containing the initial prompt or question to be sent
to the model. This is a required parameter.</p>
</td></tr>
<tr><td><code id="chat_+3A_service">service</code></td>
<td>
<p>The AI service to be used for the request. If not explicitly
provided, this defaults to the value set in
<code>getOption("gptstudio.service")</code>. If the option is not set, make sure to
provide this parameter to avoid errors.</p>
</td></tr>
<tr><td><code id="chat_+3A_history">history</code></td>
<td>
<p>An optional parameter that can be used to include previous
interactions or context for the current session. Defaults to a system
message indicating &quot;You are an R chat assistant&quot;.</p>
</td></tr>
<tr><td><code id="chat_+3A_stream">stream</code></td>
<td>
<p>A logical value indicating whether the interaction should be
treated as a stream for continuous interactions. If not explicitly
provided, this defaults to the value set in
<code>getOption("gptstudio.stream")</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_model">model</code></td>
<td>
<p>The specific model to use for the request. If not explicitly
provided, this defaults to the value set in <code>getOption("gptstudio.model")</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_skill">skill</code></td>
<td>
<p>A character string indicating the skill or capability level of
the user. This parameter allows for customizing the behavior of the model
to the user. If not explicitly provided, this defaults to the value set in
<code>getOption("gptstudio.skill")</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_style">style</code></td>
<td>
<p>The coding style preferred by the user for code generation
tasks. This parameter is particularly useful when the task involves
generating code snippets or scripts. If not explicitly provided, this
defaults to the value set in <code>getOption("gptstudio.code_style")</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_task">task</code></td>
<td>
<p>The specific type of task to be performed, ranging from text
generation to code synthesis, depending on the capabilities of the model.
If not explicitly provided, this defaults to the value set in
<code>getOption("gptstudio.task")</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>An optional parameter that provides a way to extend or
customize the initial prompt with additional instructions or context.</p>
</td></tr>
<tr><td><code id="chat_+3A_process_response">process_response</code></td>
<td>
<p>A logical indicating whether to process the model's
response. If <code>TRUE</code>, the response will be passed to
<code>gptstudio_response_process()</code> for further processing. Defaults to <code>FALSE</code>.
Refer to <code>gptstudio_response_process()</code> for more details.</p>
</td></tr>
<tr><td><code id="chat_+3A_...">...</code></td>
<td>
<p>Reserved for future use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the task and processing, the function returns the
response from the model, which could be text, code, or any other structured
output defined by the task and model capabilities. The precise format and
content of the output depend on the specified options and the capabilities
of the selected model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage with a text prompt:
result &lt;- chat("What is the weather like today?")

# Advanced usage with custom settings, assuming appropriate global options are set:
result &lt;- chat(
  prompt = "Write a simple function in R",
  skill = "advanced",
  style = "tidyverse",
  task = "coding"
)

# Usage with explicit service and model specification:
result &lt;- chat(
  prompt = "Explain the concept of tidy data in R",
  service = "openai",
  model = "gpt-4-turbo-preview",
  skill = "intermediate",
  task = "general"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat_create_system_prompt'>Create system prompt</h2><span id='topic+chat_create_system_prompt'></span>

<h3>Description</h3>

<p>This function creates a customizable system prompt based on user-defined
parameters such as coding style, skill level, and task. It supports
customization for specific use cases through a custom prompt option.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_create_system_prompt(
  style = getOption("gptstudio.code_style"),
  skill = getOption("gptstudio.skill"),
  task = getOption("gptstudio.task"),
  custom_prompt = getOption("gptstudio.custom_prompt"),
  in_source = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_create_system_prompt_+3A_style">style</code></td>
<td>
<p>A character string indicating the preferred coding style. Valid
values are &quot;tidyverse&quot;, &quot;base&quot;, &quot;no preference&quot;. Defaults to <code>getOption(gptstudio.code_style)</code>.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_skill">skill</code></td>
<td>
<p>The self-described skill level of the programmer. Valid values
are &quot;beginner&quot;, &quot;intermediate&quot;, &quot;advanced&quot;, &quot;genius&quot;. Defaults to <code>getOption(gptstudio.skill)</code>.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_task">task</code></td>
<td>
<p>The specific task to be performed: &quot;coding&quot;, &quot;general&quot;, &quot;advanced
developer&quot;, or &quot;custom&quot;. This influences the generated system prompt.
Defaults to &quot;coding&quot;.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>An optional custom prompt string to be utilized when
<code>task</code> is set to &quot;custom&quot;. Default is NULL.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_in_source">in_source</code></td>
<td>
<p>A logical indicating whether the instructions are intended
for use in a source script. This parameter is required and must be
explicitly set to TRUE or FALSE. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a character string that forms a system prompt tailored to the
specified parameters. The string provides guidance or instructions based on
the user's coding style, skill level, and task.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat_create_system_prompt(in_source = TRUE)
chat_create_system_prompt(
  style = "tidyverse",
  skill = "advanced",
  task = "coding",
  in_source = FALSE
)

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_history_append'>Append to chat history</h2><span id='topic+chat_history_append'></span>

<h3>Description</h3>

<p>This appends a new response to the chat history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_history_append(history, role, content, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_history_append_+3A_history">history</code></td>
<td>
<p>List containing previous responses.</p>
</td></tr>
<tr><td><code id="chat_history_append_+3A_role">role</code></td>
<td>
<p>Author of the message. One of <code>c("user", "assistant")</code></p>
</td></tr>
<tr><td><code id="chat_history_append_+3A_content">content</code></td>
<td>
<p>Content of the message. If it is from the user most probably
comes from an interactive input.</p>
</td></tr>
<tr><td><code id="chat_history_append_+3A_name">name</code></td>
<td>
<p>Name for the author of the message. Currently used to support rendering of help pages</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of chat messages
</p>

<hr>
<h2 id='chat_message_default'>Default chat message</h2><span id='topic+chat_message_default'></span>

<h3>Description</h3>

<p>Default chat message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_message_default(translator = create_translator())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_message_default_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A default chat message for welcoming users.
</p>

<hr>
<h2 id='check_api_connection_openai'>Check API Connection</h2><span id='topic+check_api_connection_openai'></span>

<h3>Description</h3>

<p>This generic function checks the API connection for a specified service
by dispatching to related methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_api_connection_openai(service, api_key)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_api_connection_openai_+3A_service">service</code></td>
<td>
<p>The name of the API service for which the connection is being checked.</p>
</td></tr>
<tr><td><code id="check_api_connection_openai_+3A_api_key">api_key</code></td>
<td>
<p>The API key used for authentication.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value indicating whether the connection was successful.
</p>

<hr>
<h2 id='create_chat_app_theme'>Chat App Theme</h2><span id='topic+create_chat_app_theme'></span>

<h3>Description</h3>

<p>Create a bslib theme that matches the user's RStudio IDE theme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_chat_app_theme(ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_chat_app_theme_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A bslib theme
</p>

<hr>
<h2 id='create_chat_cohere'>Create a chat with the Cohere Chat API</h2><span id='topic+create_chat_cohere'></span>

<h3>Description</h3>

<p>This function submits a user message to the Cohere Chat API,
potentially along with other parameters such as chat history or connectors,
and returns the API's response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_chat_cohere(
  prompt,
  chat_history = NULL,
  connectors = NULL,
  model = "command",
  api_key = Sys.getenv("COHERE_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_chat_cohere_+3A_prompt">prompt</code></td>
<td>
<p>A string containing the user message.</p>
</td></tr>
<tr><td><code id="create_chat_cohere_+3A_chat_history">chat_history</code></td>
<td>
<p>A list of previous messages for context, if any.</p>
</td></tr>
<tr><td><code id="create_chat_cohere_+3A_connectors">connectors</code></td>
<td>
<p>A list of connector objects, if any.</p>
</td></tr>
<tr><td><code id="create_chat_cohere_+3A_model">model</code></td>
<td>
<p>A string representing the Cohere model to be used, defaulting to &quot;command&quot;.
Other options include &quot;command-light&quot;, &quot;command-nightly&quot;, and &quot;command-light-nightly&quot;.</p>
</td></tr>
<tr><td><code id="create_chat_cohere_+3A_api_key">api_key</code></td>
<td>
<p>The API key for accessing the Cohere API, defaults to the
COHERE_API_KEY environment variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the Cohere Chat API containing the model's reply.
</p>

<hr>
<h2 id='create_completion_anthropic'>Generate text completions using Anthropic's API</h2><span id='topic+create_completion_anthropic'></span>

<h3>Description</h3>

<p>Generate text completions using Anthropic's API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_anthropic(
  prompt = list(list(role = "user", content = "Hello")),
  system = NULL,
  model = "claude-3-haiku-20240307",
  max_tokens = 1028,
  key = Sys.getenv("ANTHROPIC_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_anthropic_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_system">system</code></td>
<td>
<p>A system messages to instruct the model. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_model">model</code></td>
<td>
<p>The model to use for generating text. By default, the
function will try to use &quot;claude-2.1&quot;.</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_max_tokens">max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate. Defaults to 256.</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_key">key</code></td>
<td>
<p>The API key for accessing Anthropic's API. By default, the
function will try to use the <code>ANTHROPIC_API_KEY</code> environment variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_anthropic(
  prompt = "\n\nHuman: Hello, world!\n\nAssistant:",
  model = "claude-3-haiku-20240307",
  max_tokens = 1028
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_completion_azure_openai'>Generate text using Azure OpenAI's API</h2><span id='topic+create_completion_azure_openai'></span>

<h3>Description</h3>

<p>Use this function to generate text completions using OpenAI's
API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_azure_openai(
  prompt,
  task = Sys.getenv("AZURE_OPENAI_TASK"),
  base_url = Sys.getenv("AZURE_OPENAI_ENDPOINT"),
  deployment_name = Sys.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
  token = Sys.getenv("AZURE_OPENAI_KEY"),
  api_version = Sys.getenv("AZURE_OPENAI_API_VERSION")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_azure_openai_+3A_prompt">prompt</code></td>
<td>
<p>a list to use as the prompt for generating
completions</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_task">task</code></td>
<td>
<p>a character string for the API task (e.g. &quot;completions&quot;).
Defaults to the Azure OpenAI
task from environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_base_url">base_url</code></td>
<td>
<p>a character string for the base url. It defaults to the Azure
OpenAI endpoint from environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_deployment_name">deployment_name</code></td>
<td>
<p>a character string for the deployment name. It will
default to the Azure OpenAI deployment name from environment variables if
not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_token">token</code></td>
<td>
<p>a character string for the API key. It will default to the Azure
OpenAI API key from your environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_api_version">api_version</code></td>
<td>
<p>a character string for the API version. It will default to
the Azure OpenAI API version from your environment variables if not
specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the generated completions and other information returned
by the API
</p>

<hr>
<h2 id='create_completion_google'>Generate text completions using Google AI Studio's API</h2><span id='topic+create_completion_google'></span>

<h3>Description</h3>

<p>Generate text completions using Google AI Studio's API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_google(
  prompt,
  model = "gemini-pro",
  key = Sys.getenv("GOOGLE_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_google_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_google_+3A_model">model</code></td>
<td>
<p>The model to use for generating text. By default, the
function will try to use &quot;text-bison-001&quot;</p>
</td></tr>
<tr><td><code id="create_completion_google_+3A_key">key</code></td>
<td>
<p>The API key for accessing Google AI Studio's API. By default, the
function will try to use the <code>GOOGLE_API_KEY</code> environment variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_google(
  prompt = "Write a story about a magic backpack",
  temperature = 1.0,
  candidate_count = 3
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_completion_huggingface'>Generate text completions using HuggingFace's API</h2><span id='topic+create_completion_huggingface'></span>

<h3>Description</h3>

<p>Generate text completions using HuggingFace's API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_huggingface(
  prompt,
  history = NULL,
  model = "tiiuae/falcon-7b-instruct",
  token = Sys.getenv("HF_API_KEY"),
  max_new_tokens = 250
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_huggingface_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_history">history</code></td>
<td>
<p>A list of the previous chat responses</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_model">model</code></td>
<td>
<p>The model to use for generating text</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_token">token</code></td>
<td>
<p>The API key for accessing HuggingFace's API. By default, the
function will try to use the <code>HF_API_KEY</code> environment variable.</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_max_new_tokens">max_new_tokens</code></td>
<td>
<p>Maximum number of tokens to generate, defaults to 250</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_huggingface(
  model = "gpt2",
  prompt = "Hello world!"
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_completion_perplexity'>Create a chat completion request to the Perplexity API</h2><span id='topic+create_completion_perplexity'></span>

<h3>Description</h3>

<p>This function sends a series of messages alongside a chosen model to the Perplexity API
to generate a chat completion. It returns the API's generated responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_perplexity(
  prompt,
  model = "mistral-7b-instruct",
  api_key = Sys.getenv("PERPLEXITY_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_perplexity_+3A_prompt">prompt</code></td>
<td>
<p>A list containing prompts to be sent in the chat.</p>
</td></tr>
<tr><td><code id="create_completion_perplexity_+3A_model">model</code></td>
<td>
<p>A character string representing the Perplexity model to be used.
Defaults to &quot;mistral-7b-instruct&quot;.</p>
</td></tr>
<tr><td><code id="create_completion_perplexity_+3A_api_key">api_key</code></td>
<td>
<p>The API key for accessing the Perplexity API. Defaults to the
PERPLEXITY_API_KEY environment variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the Perplexity API containing the completion for the chat.
</p>

<hr>
<h2 id='create_ide_matching_colors'>Chat message colors in RStudio</h2><span id='topic+create_ide_matching_colors'></span>

<h3>Description</h3>

<p>This returns a list of color properties for a chat message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ide_matching_colors(role, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ide_matching_colors_+3A_role">role</code></td>
<td>
<p>The role of the message author</p>
</td></tr>
<tr><td><code id="create_ide_matching_colors_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list
</p>

<hr>
<h2 id='create_tmp_job_script'>Create a temporary job script</h2><span id='topic+create_tmp_job_script'></span>

<h3>Description</h3>

<p>This function creates a temporary R script file that runs the Shiny
application from the specified directory with the specified port and host.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tmp_job_script(appDir, port, host)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_tmp_job_script_+3A_appdir">appDir</code></td>
<td>
<p>The application to run. Should be one of the following:
</p>

<ul>
<li><p> A directory containing <code>server.R</code>, plus, either <code>ui.R</code> or
a <code>www</code> directory that contains the file <code>index.html</code>.
</p>
</li>
<li><p> A directory containing <code>app.R</code>.
</p>
</li>
<li><p> An <code>.R</code> file containing a Shiny application, ending with an
expression that produces a Shiny app object.
</p>
</li>
<li><p> A list with <code>ui</code> and <code>server</code> components.
</p>
</li>
<li><p> A Shiny app object created by <code><a href="shiny.html#topic+shinyApp">shinyApp()</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="create_tmp_job_script_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
<tr><td><code id="create_tmp_job_script_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string containing the path of a temporary job script
</p>

<hr>
<h2 id='create_translator'>Internationalization for the ChatGPT addin</h2><span id='topic+create_translator'></span>

<h3>Description</h3>

<p>The language can be set via <code>options("gptstudio.language" = "&lt;language&gt;")</code>
(defaults to &quot;en&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_translator(language = getOption("gptstudio.language"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_translator_+3A_language">language</code></td>
<td>
<p>The language to be found in the translation JSON file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Translator from <code>shiny.i18n::Translator</code>
</p>

<hr>
<h2 id='get_available_endpoints'>List supported endpoints</h2><span id='topic+get_available_endpoints'></span>

<h3>Description</h3>

<p>Get a list of the endpoints supported by gptstudio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_available_endpoints()
</code></pre>


<h3>Value</h3>

<p>A character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_available_endpoints()
</code></pre>

<hr>
<h2 id='get_available_models'>List supported models</h2><span id='topic+get_available_models'></span>

<h3>Description</h3>

<p>Get a list of the models supported by the OpenAI API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_available_models(service)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_available_models_+3A_service">service</code></td>
<td>
<p>The API service</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
get_available_models()

## End(Not run)
</code></pre>

<hr>
<h2 id='get_ide_theme_info'>Get IDE theme information.</h2><span id='topic+get_ide_theme_info'></span>

<h3>Description</h3>

<p>This function returns a list with the current IDE theme's information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ide_theme_info()
</code></pre>


<h3>Value</h3>

<p>A list with three components:
</p>
<table>
<tr><td><code>is_dark</code></td>
<td>
<p>A boolean indicating whether the current IDE theme is dark.</p>
</td></tr>
<tr><td><code>bg</code></td>
<td>
<p>The current IDE theme's background color.</p>
</td></tr>
<tr><td><code>fg</code></td>
<td>
<p>The current IDE theme's foreground color.</p>
</td></tr>
</table>

<hr>
<h2 id='gptstudio_chat'>Run Chat GPT
Run the Chat GPT Shiny App as a background job and show it in the viewer pane</h2><span id='topic+gptstudio_chat'></span>

<h3>Description</h3>

<p>Run Chat GPT
Run the Chat GPT Shiny App as a background job and show it in the viewer pane
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_chat(host = getOption("shiny.host", "127.0.0.1"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_chat_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Call the function as an RStudio addin
## Not run: 
gptstudio_chat()

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_chat_in_source_addin'>ChatGPT in Source</h2><span id='topic+gptstudio_chat_in_source_addin'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to improve spelling and
grammar of selected text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_chat_in_source_addin()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select some text in a source file
# Then call the function as an RStudio addin
## Not run: 
gptstudio_chat_in_source()

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_comment_code'>Comment Code Addin</h2><span id='topic+gptstudio_comment_code'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to add comments to your code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_comment_code()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Open a R file in Rstudio
# Then call the function as an RStudio addin
## Not run: 
gptstudio_comment_code()

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_create_skeleton'>Create a Request Skeleton</h2><span id='topic+gptstudio_create_skeleton'></span>

<h3>Description</h3>

<p>This function dynamically creates a request skeleton for different AI text
generation services.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_create_skeleton(
  service = "openai",
  prompt = "Name the top 5 packages in R.",
  history = list(list(role = "system", content = "You are an R chat assistant")),
  stream = TRUE,
  model = "gpt-3.5-turbo",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_create_skeleton_+3A_service">service</code></td>
<td>
<p>The text generation service to use. Currently supports
&quot;openai&quot;, &quot;huggingface&quot;, &quot;anthropic&quot;, &quot;google&quot;, &quot;azure_openai&quot;, &quot;ollama&quot;, and
&quot;perplexity&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_create_skeleton_+3A_prompt">prompt</code></td>
<td>
<p>The initial prompt or question to pass to the text generation
service.</p>
</td></tr>
<tr><td><code id="gptstudio_create_skeleton_+3A_history">history</code></td>
<td>
<p>A list indicating the conversation history, where each element
is a list with elements &quot;role&quot; (who is speaking; e.g., &quot;system&quot;, &quot;user&quot;)
and &quot;content&quot; (what was said).</p>
</td></tr>
<tr><td><code id="gptstudio_create_skeleton_+3A_stream">stream</code></td>
<td>
<p>Logical; indicates if streaming responses should be used.
Currently, this option is not supported across all services.</p>
</td></tr>
<tr><td><code id="gptstudio_create_skeleton_+3A_model">model</code></td>
<td>
<p>The specific model to use for generating responses. Defaults to
&quot;gpt-3.5-turbo&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_create_skeleton_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the service-specific skeleton
creation function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the selected service, returns a list that represents the
configured request ready to be passed to the corresponding API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
request_skeleton &lt;- gptstudio_create_skeleton(
  service = "openai",
  prompt = "Name the top 5 packages in R.",
  history = list(list(role = "system", content = "You are an R assistant")),
  stream = TRUE,
  model = "gpt-3.5-turbo"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='gptstudio_request_perform'>Perform API Request</h2><span id='topic+gptstudio_request_perform'></span>

<h3>Description</h3>

<p>This function provides a generic interface for calling different APIs
(e.g., OpenAI, HuggingFace, Google AI Studio). It dispatches the actual API
calls to the relevant method based on the <code>class</code> of the <code>skeleton</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_request_perform(skeleton, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_request_perform_+3A_skeleton">skeleton</code></td>
<td>
<p>A <code>gptstudio_request_skeleton</code> object</p>
</td></tr>
<tr><td><code id="gptstudio_request_perform_+3A_...">...</code></td>
<td>
<p>Extra arguments (e.g., <code>stream_handler</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>gptstudio_response_skeleton</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gptstudio_request_perform(gptstudio_skeleton)

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_response_process'>Call API</h2><span id='topic+gptstudio_response_process'></span>

<h3>Description</h3>

<p>This function provides a generic interface for calling different APIs
(e.g., OpenAI, HuggingFace, Google AI Studio). It dispatches the actual API
calls to the relevant method based on the <code>class</code> of the <code>skeleton</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_response_process(skeleton, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_response_process_+3A_skeleton">skeleton</code></td>
<td>
<p>A <code>gptstudio_response_skeleton</code> object</p>
</td></tr>
<tr><td><code id="gptstudio_response_process_+3A_...">...</code></td>
<td>
<p>Extra arguments, not currently used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>gptstudio_request_skeleton</code> with updated history and prompt removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gptstudio_response_process(gptstudio_skeleton)

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_sitrep'>Current Configuration for gptstudio</h2><span id='topic+gptstudio_sitrep'></span>

<h3>Description</h3>

<p>This function prints out the current configuration settings for gptstudio and
checks API connections if verbose is TRUE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_sitrep(verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_sitrep_+3A_verbose">verbose</code></td>
<td>
<p>Logical value indicating whether to output additional information,
such as API connection checks. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns NULL, as the primary purpose of this function is to
print to the console.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gptstudio_sitrep(verbose = FALSE) # Print basic settings, no API checks
gptstudio_sitrep() # Print settings and check API connections

</code></pre>

<hr>
<h2 id='gptstudio_skeleton_build'>Construct a GPT Studio request skeleton.</h2><span id='topic+gptstudio_skeleton_build'></span>

<h3>Description</h3>

<p>Construct a GPT Studio request skeleton.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_skeleton_build(skeleton, skill, style, task, custom_prompt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_skeleton_build_+3A_skeleton">skeleton</code></td>
<td>
<p>A GPT Studio request skeleton object.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_skill">skill</code></td>
<td>
<p>The skill level of the user for the chat conversation. This can
be set through the &quot;gptstudio.skill&quot; option. Default is the
&quot;gptstudio.skill&quot; option. Options are &quot;beginner&quot;, &quot;intermediate&quot;,
&quot;advanced&quot;, and &quot;genius&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_style">style</code></td>
<td>
<p>The style of code to use. Applicable styles can be retrieved
from the &quot;gptstudio.code_style&quot; option. Default is the
&quot;gptstudio.code_style&quot; option. Options are &quot;base&quot;, &quot;tidyverse&quot;, or &quot;no
preference&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_task">task</code></td>
<td>
<p>Specifies the task that the assistant will help with. Default is
&quot;coding&quot;. Others are &quot;general&quot;, &quot;advanced developer&quot;, and &quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>This is a custom prompt that may be used to guide the AI
in its responses. Default is NULL. It will be the only content provided to
the system prompt.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated GPT Studio request skeleton.
</p>

<hr>
<h2 id='gptstudio_spelling_grammar'>Spelling and Grammar Addin</h2><span id='topic+gptstudio_spelling_grammar'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to improve spelling and
grammar of selected text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_spelling_grammar()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select some text in Rstudio
# Then call the function as an RStudio addin
## Not run: 
gptstudio_spelling_grammar()

## End(Not run)
</code></pre>

<hr>
<h2 id='mod_app_server'>App Server</h2><span id='topic+mod_app_server'></span>

<h3>Description</h3>

<p>App Server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_app_server(id, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_app_server_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_app_server_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>

<hr>
<h2 id='mod_app_ui'>App UI</h2><span id='topic+mod_app_ui'></span>

<h3>Description</h3>

<p>App UI
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_app_ui(id, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_app_ui_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_app_ui_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>

<hr>
<h2 id='mod_chat_server'>Chat server</h2><span id='topic+mod_chat_server'></span>

<h3>Description</h3>

<p>Chat server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_chat_server(
  id,
  ide_colors = get_ide_theme_info(),
  translator = create_translator(),
  settings,
  history
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_chat_server_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_chat_server_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="mod_chat_server_+3A_translator">translator</code></td>
<td>
<p>Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
<tr><td><code id="mod_chat_server_+3A_settings">settings</code>, <code id="mod_chat_server_+3A_history">history</code></td>
<td>
<p>Reactive values from the settings and history module</p>
</td></tr>
</table>

<hr>
<h2 id='mod_chat_ui'>Chat UI</h2><span id='topic+mod_chat_ui'></span>

<h3>Description</h3>

<p>Chat UI
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_chat_ui(id, translator = create_translator())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_chat_ui_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_chat_ui_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
</table>

<hr>
<h2 id='open_bg_shinyapp'>Open browser to local Shiny app</h2><span id='topic+open_bg_shinyapp'></span>

<h3>Description</h3>

<p>This function takes in the host and port of a local Shiny app and opens the
app in the default browser.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_bg_shinyapp(host, port)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="open_bg_shinyapp_+3A_host">host</code></td>
<td>
<p>A character string representing the IP address or domain name of
the server where the Shiny app is hosted.</p>
</td></tr>
<tr><td><code id="open_bg_shinyapp_+3A_port">port</code></td>
<td>
<p>An integer representing the port number on which the Shiny app is
hosted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None (opens the Shiny app in the viewer pane or browser window)
</p>

<hr>
<h2 id='openai_create_chat_completion'>Generate text completions using OpenAI's API for Chat</h2><span id='topic+openai_create_chat_completion'></span>

<h3>Description</h3>

<p>Generate text completions using OpenAI's API for Chat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_create_chat_completion(
  prompt = "&lt;|endoftext|&gt;",
  model = getOption("gptstudio.model"),
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  task = "chat/completions"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="openai_create_chat_completion_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_model">model</code></td>
<td>
<p>The model to use for generating text</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>The API key for accessing OpenAI's API. By default, the
function will try to use the <code>OPENAI_API_KEY</code> environment variable.</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_task">task</code></td>
<td>
<p>The task that specifies the API url to use, defaults to
&quot;completions&quot; and &quot;chat/completions&quot; is required for ChatGPT model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
openai_create_completion(
  model = "text-davinci-002",
  prompt = "Hello world!"
)

## End(Not run)
</code></pre>

<hr>
<h2 id='OpenaiStreamParser'>Stream handler for chat completions</h2><span id='topic+OpenaiStreamParser'></span>

<h3>Description</h3>

<p>Stream handler for chat completions
</p>
<p>Stream handler for chat completions
</p>


<h3>Details</h3>

<p>R6 class that allows to handle chat completions chunk by chunk.
It also adds methods to retrieve relevant data. This class DOES NOT make the request.
</p>
<p>Because <code>curl::curl_fetch_stream</code> blocks the R console until the stream finishes,
this class can take a shiny session object to handle communication with JS
without recurring to a <code>shiny::observe</code> inside a module server.
</p>


<h3>Super class</h3>

<p><code><a href="SSEparser.html#topic+SSEparser">SSEparser::SSEparser</a></code> -&gt; <code>OpenaiStreamParser</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>shinySession</code></dt><dd><p>Holds the <code>session</code> provided at initialization</p>
</dd>
<dt><code>user_prompt</code></dt><dd><p>The <code>user_prompt</code> provided at initialization,
after being formatted with markdown.</p>
</dd>
<dt><code>value</code></dt><dd><p>The content of the stream. It updates constantly until the stream ends.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-OpenaiStreamParser-new"><code>OpenaiStreamParser$new()</code></a>
</p>
</li>
<li> <p><a href="#method-OpenaiStreamParser-append_parsed_sse"><code>OpenaiStreamParser$append_parsed_sse()</code></a>
</p>
</li>
<li> <p><a href="#method-OpenaiStreamParser-clone"><code>OpenaiStreamParser$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="SSEparser" data-topic="SSEparser" data-id="parse_sse"><a href='../../SSEparser/html/SSEparser.html#method-SSEparser-parse_sse'><code>SSEparser::SSEparser$parse_sse()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-OpenaiStreamParser-new"></a>



<h4>Method <code>new()</code></h4>

<p>Start a StreamHandler. Recommended to be assigned to the <code>stream_handler</code> name.
</p>


<h5>Usage</h5>

<div class="r"><pre>OpenaiStreamParser$new(session = NULL, user_prompt = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>session</code></dt><dd><p>The shiny session it will send the message to (optional).</p>
</dd>
<dt><code>user_prompt</code></dt><dd><p>The prompt for the chat completion.
Only to be displayed in an HTML tag containing the prompt. (Optional).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-OpenaiStreamParser-append_parsed_sse"></a>



<h4>Method <code>append_parsed_sse()</code></h4>

<p>Overwrites <code>SSEparser$append_parsed_sse()</code> to be able to send a custom message
to a shiny session, escaping shiny's reactivity.
</p>


<h5>Usage</h5>

<div class="r"><pre>OpenaiStreamParser$append_parsed_sse(parsed_event)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>parsed_event</code></dt><dd><p>An already parsed server-sent event to append to the events field.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-OpenaiStreamParser-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>OpenaiStreamParser$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='prepare_chat_history'>Prepare chat completion prompt</h2><span id='topic+prepare_chat_history'></span>

<h3>Description</h3>

<p>This function prepares the chat completion prompt to be sent to the OpenAI API.
It also generates a system message according to the given parameters and inserts
it at the beginning of the conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_chat_history(
  history = NULL,
  style = getOption("gptstudio.code_style"),
  skill = getOption("gptstudio.skill"),
  task = "coding",
  custom_prompt = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_chat_history_+3A_history">history</code></td>
<td>
<p>A list of previous messages in the conversation. This can include
roles such as 'system', 'user', or 'assistant'. System messages are discarded.
Default is NULL.i</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_style">style</code></td>
<td>
<p>The style of code to use. Applicable styles can be
retrieved from the &quot;gptstudio.code_style&quot; option. Default is the
&quot;gptstudio.code_style&quot; option. Options are &quot;base&quot;, &quot;tidyverse&quot;, or
&quot;no preference&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_skill">skill</code></td>
<td>
<p>The skill level of the user for the chat conversation. This
can be set through the &quot;gptstudio.skill&quot; option. Default is the
&quot;gptstudio.skill&quot; option. Options are &quot;beginner&quot;, &quot;intermediate&quot;,
&quot;advanced&quot;, and &quot;genius&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_task">task</code></td>
<td>
<p>Specifies the task that the assistant will help with. Default is
&quot;coding&quot;. Others are &quot;general&quot;, &quot;advanced developer&quot;, and &quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>This is a custom prompt that may be used to guide the AI in
its responses. Default is NULL. It will be the only content provided to the
system prompt.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first entry is an initial system message followed by any
non-system entries from the chat history.
</p>

<hr>
<h2 id='query_api_anthropic'>A function that sends a request to the Anthropic API and returns the
response.</h2><span id='topic+query_api_anthropic'></span>

<h3>Description</h3>

<p>A function that sends a request to the Anthropic API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_anthropic(request_body, key = Sys.getenv("ANTHROPIC_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_anthropic_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_anthropic_+3A_key">key</code></td>
<td>
<p>String containing an Anthropic API key. Defaults
to the ANTHROPIC_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_api_cohere'>Send a request to the Cohere Chat API and return the response</h2><span id='topic+query_api_cohere'></span>

<h3>Description</h3>

<p>This function sends a JSON post request to the Cohere Chat API,
retries on failure up to three times, and returns the response.
The function handles errors by providing a descriptive message and failing gracefully.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_cohere(request_body, api_key = Sys.getenv("COHERE_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_cohere_+3A_request_body">request_body</code></td>
<td>
<p>A list containing the body of the POST request.</p>
</td></tr>
<tr><td><code id="query_api_cohere_+3A_api_key">api_key</code></td>
<td>
<p>String containing a Cohere API key. Defaults to the
COHERE_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parsed JSON object as the API response.
</p>

<hr>
<h2 id='query_api_google'>A function that sends a request to the Google AI Studio API and returns the
response.</h2><span id='topic+query_api_google'></span>

<h3>Description</h3>

<p>A function that sends a request to the Google AI Studio API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_google(model, request_body, key = Sys.getenv("GOOGLE_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_google_+3A_model">model</code></td>
<td>
<p>A character string that specifies the model to send to the API.</p>
</td></tr>
<tr><td><code id="query_api_google_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_google_+3A_key">key</code></td>
<td>
<p>String containing a Google AI Studio API key. Defaults
to the GOOGLE_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_api_huggingface'>A function that sends a request to the HuggingFace API and returns the
response.</h2><span id='topic+query_api_huggingface'></span>

<h3>Description</h3>

<p>A function that sends a request to the HuggingFace API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_huggingface(task, request_body, token = Sys.getenv("HF_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_huggingface_+3A_task">task</code></td>
<td>
<p>A character string that specifies the task to send to the API.</p>
</td></tr>
<tr><td><code id="query_api_huggingface_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_huggingface_+3A_token">token</code></td>
<td>
<p>String containing a HuggingFace API key. Defaults
to the HF_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_api_perplexity'>Send a request to the Perplexity API and return the response</h2><span id='topic+query_api_perplexity'></span>

<h3>Description</h3>

<p>This function sends a JSON post request to the Perplexity API,
retries on failure up to three times, and returns the response.
The function handles errors by providing a descriptive message and failing gracefully.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_perplexity(request_body, api_key = Sys.getenv("PERPLEXITY_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_perplexity_+3A_request_body">request_body</code></td>
<td>
<p>A list containing the body of the POST request.</p>
</td></tr>
<tr><td><code id="query_api_perplexity_+3A_api_key">api_key</code></td>
<td>
<p>String containing a Perplexity API key. Defaults to the
PERPLEXITY_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A parsed JSON object as the API response.
</p>

<hr>
<h2 id='query_openai_api'>A function that sends a request to the OpenAI API and returns the response.</h2><span id='topic+query_openai_api'></span>

<h3>Description</h3>

<p>A function that sends a request to the OpenAI API and returns the response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_openai_api(
  task,
  request_body,
  openai_api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_openai_api_+3A_task">task</code></td>
<td>
<p>A character string that specifies the task to send to the API.</p>
</td></tr>
<tr><td><code id="query_openai_api_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_openai_api_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>String containing an OpenAI API key. Defaults to the OPENAI_API_KEY
environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='random_port'>Generate a random safe port number</h2><span id='topic+random_port'></span>

<h3>Description</h3>

<p>This function generates a random port allowed by shiny::runApp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_port()
</code></pre>


<h3>Value</h3>

<p>A single integer representing the randomly selected safe port number.
</p>

<hr>
<h2 id='request_base'>Base for a request to the OPENAI API</h2><span id='topic+request_base'></span>

<h3>Description</h3>

<p>This function sends a request to a specific OpenAI API <code>task</code> endpoint at
the base URL <code>https://api.openai.com/v1</code>, and authenticates with
an API key using a Bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base(task, token = Sys.getenv("OPENAI_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_+3A_task">task</code></td>
<td>
<p>character string specifying an OpenAI API endpoint task</p>
</td></tr>
<tr><td><code id="request_base_+3A_token">token</code></td>
<td>
<p>String containing an OpenAI API key. Defaults to the OPENAI_API_KEY
environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_anthropic'>Base for a request to the Anthropic API</h2><span id='topic+request_base_anthropic'></span>

<h3>Description</h3>

<p>This function sends a request to the Anthropic API endpoint and
authenticates with an API key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_anthropic(key = Sys.getenv("ANTHROPIC_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_anthropic_+3A_key">key</code></td>
<td>
<p>String containing an Anthropic API key. Defaults to the
ANTHROPIC_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_cohere'>Base for a request to the Cohere Chat API</h2><span id='topic+request_base_cohere'></span>

<h3>Description</h3>

<p>This function sets up a POST request to the Cohere Chat API's chat endpoint
and includes necessary headers such as 'accept', 'content-type', and 'Authorization'
with a bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_cohere(api_key = Sys.getenv("COHERE_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_cohere_+3A_api_key">api_key</code></td>
<td>
<p>String containing a Cohere API key. Defaults to the
COHERE_API_KEY environment variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>httr2</code> request object pre-configured with the API endpoint and required headers.
</p>

<hr>
<h2 id='request_base_google'>Base for a request to the Google AI Studio API</h2><span id='topic+request_base_google'></span>

<h3>Description</h3>

<p>This function sends a request to a specific Google AI Studio API endpoint and
authenticates with an API key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_google(model, key = Sys.getenv("GOOGLE_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_google_+3A_model">model</code></td>
<td>
<p>character string specifying a Google AI Studio API model</p>
</td></tr>
<tr><td><code id="request_base_google_+3A_key">key</code></td>
<td>
<p>String containing a Google AI Studio API key. Defaults to the
GOOGLE_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_huggingface'>Base for a request to the HuggingFace API</h2><span id='topic+request_base_huggingface'></span>

<h3>Description</h3>

<p>This function sends a request to a specific HuggingFace API endpoint and
authenticates with an API key using a Bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_huggingface(task, token = Sys.getenv("HF_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_huggingface_+3A_task">task</code></td>
<td>
<p>character string specifying a HuggingFace API endpoint task</p>
</td></tr>
<tr><td><code id="request_base_huggingface_+3A_token">token</code></td>
<td>
<p>String containing a HuggingFace API key. Defaults to the
HF_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_perplexity'>Base for a request to the Perplexity API</h2><span id='topic+request_base_perplexity'></span>

<h3>Description</h3>

<p>This function sets up a POST request to the Perplexity API's chat/completions endpoint
and includes necessary headers such as 'accept', 'content-type', and 'Authorization'
with a bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_perplexity(api_key = Sys.getenv("PERPLEXITY_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_perplexity_+3A_api_key">api_key</code></td>
<td>
<p>String containing a Perplexity API key. Defaults to the
PERPLEXITY_API_KEY environment variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>httr2</code> request object pre-configured with the API endpoint and required headers.
</p>

<hr>
<h2 id='rgb_str_to_hex'>RGB str to hex</h2><span id='topic+rgb_str_to_hex'></span>

<h3>Description</h3>

<p>RGB str to hex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rgb_str_to_hex(rgb_string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rgb_str_to_hex_+3A_rgb_string">rgb_string</code></td>
<td>
<p>The RGB string as returned by <code>rstudioapi::getThemeInfo()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>hex color
</p>

<hr>
<h2 id='run_app_as_bg_job'>Run an R Shiny app in the background</h2><span id='topic+run_app_as_bg_job'></span>

<h3>Description</h3>

<p>This function runs an R Shiny app as a background job using the specified
directory, name, host, and port.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_app_as_bg_job(appDir = ".", job_name, host, port)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_app_as_bg_job_+3A_appdir">appDir</code></td>
<td>
<p>The application to run. Should be one of the following:
</p>

<ul>
<li><p> A directory containing <code>server.R</code>, plus, either <code>ui.R</code> or
a <code>www</code> directory that contains the file <code>index.html</code>.
</p>
</li>
<li><p> A directory containing <code>app.R</code>.
</p>
</li>
<li><p> An <code>.R</code> file containing a Shiny application, ending with an
expression that produces a Shiny app object.
</p>
</li>
<li><p> A list with <code>ui</code> and <code>server</code> components.
</p>
</li>
<li><p> A Shiny app object created by <code><a href="shiny.html#topic+shinyApp">shinyApp()</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_job_name">job_name</code></td>
<td>
<p>The name of the background job to be created</p>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns nothing because is meant to run an app as a
side effect.
</p>

<hr>
<h2 id='run_chatgpt_app'>Run the ChatGPT app</h2><span id='topic+run_chatgpt_app'></span>

<h3>Description</h3>

<p>This starts the chatgpt app. It is exported to be able to run it from an R
script.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_chatgpt_app(
  ide_colors = get_ide_theme_info(),
  host = getOption("shiny.host", "127.0.0.1"),
  port = getOption("shiny.port")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_chatgpt_app_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="run_chatgpt_app_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
<tr><td><code id="run_chatgpt_app_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing.
</p>

<hr>
<h2 id='stream_chat_completion'>Stream Chat Completion</h2><span id='topic+stream_chat_completion'></span>

<h3>Description</h3>

<p><code>stream_chat_completion</code> sends the prepared chat completion request to the
OpenAI API and retrieves the streamed response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_chat_completion(
  messages = NULL,
  element_callback = cat,
  model = "gpt-3.5-turbo",
  openai_api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stream_chat_completion_+3A_messages">messages</code></td>
<td>
<p>A list of messages in the conversation,
including the current user prompt (optional).</p>
</td></tr>
<tr><td><code id="stream_chat_completion_+3A_element_callback">element_callback</code></td>
<td>
<p>A callback function to handle each element
of the streamed response (optional).</p>
</td></tr>
<tr><td><code id="stream_chat_completion_+3A_model">model</code></td>
<td>
<p>A character string specifying the model to use for chat completion.
The default model is &quot;gpt-3.5-turbo&quot;.</p>
</td></tr>
<tr><td><code id="stream_chat_completion_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>A character string of the OpenAI API key.
By default, it is fetched from the &quot;OPENAI_API_KEY&quot; environment variable.
Please note that the OpenAI API key is sensitive information and should be
treated accordingly.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same as <code>curl::curl_fetch_stream</code>
</p>

<hr>
<h2 id='streamingMessage'>Streaming message</h2><span id='topic+streamingMessage'></span>

<h3>Description</h3>

<p>Places an invisible empty chat message that will hold a streaming message.
It can be reset dynamically inside a shiny app
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamingMessage(
  ide_colors = get_ide_theme_info(),
  width = NULL,
  height = NULL,
  element_id = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="streamingMessage_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="streamingMessage_+3A_width">width</code>, <code id="streamingMessage_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="streamingMessage_+3A_element_id">element_id</code></td>
<td>
<p>The element's id</p>
</td></tr>
</table>

<hr>
<h2 id='streamingMessage-shiny'>Shiny bindings for streamingMessage</h2><span id='topic+streamingMessage-shiny'></span><span id='topic+streamingMessageOutput'></span><span id='topic+renderStreamingMessage'></span>

<h3>Description</h3>

<p>Output and render functions for using streamingMessage within Shiny
applications and interactive Rmd documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamingMessageOutput(outputId, width = "100%", height = NULL)

renderStreamingMessage(expr, env = parent.frame(), quoted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="streamingMessage-shiny_+3A_outputid">outputId</code></td>
<td>
<p>output variable to read from</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_width">width</code>, <code id="streamingMessage-shiny_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_expr">expr</code></td>
<td>
<p>An expression that generates a streamingMessage</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_env">env</code></td>
<td>
<p>The environment in which to evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_quoted">quoted</code></td>
<td>
<p>Is <code>expr</code> a quoted expression (with <code>quote()</code>)? This
is useful if you want to save an expression in a variable.</p>
</td></tr>
</table>

<hr>
<h2 id='style_chat_history'>Style Chat History</h2><span id='topic+style_chat_history'></span>

<h3>Description</h3>

<p>This function processes the chat history, filters out system messages, and
formats the remaining messages with appropriate styling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>style_chat_history(history, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="style_chat_history_+3A_history">history</code></td>
<td>
<p>A list of chat messages with elements containing 'role' and
'content'.</p>
</td></tr>
<tr><td><code id="style_chat_history_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of formatted chat messages with styling applied, excluding
system messages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chat_history_example &lt;- list(
  list(role = "user", content = "Hello, World!"),
  list(role = "system", content = "System message"),
  list(role = "assistant", content = "Hi, how can I help?")
)

## Not run: 
style_chat_history(chat_history_example)

## End(Not run)
</code></pre>

<hr>
<h2 id='style_chat_message'>Style chat message</h2><span id='topic+style_chat_message'></span>

<h3>Description</h3>

<p>Style a message based on the role of its author.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>style_chat_message(message, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="style_chat_message_+3A_message">message</code></td>
<td>
<p>A chat message.</p>
</td></tr>
<tr><td><code id="style_chat_message_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An HTML element.
</p>

<hr>
<h2 id='text_area_input_wrapper'>Custom textAreaInput</h2><span id='topic+text_area_input_wrapper'></span>

<h3>Description</h3>

<p>Modified version of <code>textAreaInput()</code> that removes the label container.
It's used in <code>mod_prompt_ui()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_area_input_wrapper(
  inputId,
  label,
  value = "",
  width = NULL,
  height = NULL,
  cols = NULL,
  rows = NULL,
  placeholder = NULL,
  resize = NULL,
  textarea_class = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_area_input_wrapper_+3A_inputid">inputId</code></td>
<td>
<p>The <code>input</code> slot that will be used to access the value.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_label">label</code></td>
<td>
<p>Display label for the control, or <code>NULL</code> for no label.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_value">value</code></td>
<td>
<p>Initial value.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_width">width</code></td>
<td>
<p>The width of the input, e.g. <code>'400px'</code>, or <code>'100%'</code>;
see <code><a href="shiny.html#topic+validateCssUnit">validateCssUnit()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_height">height</code></td>
<td>
<p>The height of the input, e.g. <code>'400px'</code>, or <code>'100%'</code>; see
<code><a href="shiny.html#topic+validateCssUnit">validateCssUnit()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_cols">cols</code></td>
<td>
<p>Value of the visible character columns of the input, e.g. <code>80</code>.
This argument will only take effect if there is not a CSS <code>width</code> rule
defined for this element; such a rule could come from the <code>width</code> argument
of this function or from a containing page layout such as
<code><a href="shiny.html#topic+fluidPage">fluidPage()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_rows">rows</code></td>
<td>
<p>The value of the visible character rows of the input, e.g. <code>6</code>.
If the <code>height</code> argument is specified, <code>height</code> will take precedence in the
browser's rendering.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_placeholder">placeholder</code></td>
<td>
<p>A character string giving the user a hint as to what can
be entered into the control. Internet Explorer 8 and 9 do not support this
option.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_resize">resize</code></td>
<td>
<p>Which directions the textarea box can be resized. Can be one of
<code>"both"</code>, <code>"none"</code>, <code>"vertical"</code>, and <code>"horizontal"</code>. The default, <code>NULL</code>,
will use the client browser's default setting for resizing textareas.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_textarea_class">textarea_class</code></td>
<td>
<p>Class to be applied to the textarea element</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified textAreaInput
</p>

<hr>
<h2 id='welcomeMessage'>Welcome message</h2><span id='topic+welcomeMessage'></span>

<h3>Description</h3>

<p>HTML widget for showing a welcome message in the chat app.
This has been created to be able to bind the message to a shiny event to trigger a new render.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>welcomeMessage(
  ide_colors = get_ide_theme_info(),
  translator = create_translator(),
  width = NULL,
  height = NULL,
  element_id = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="welcomeMessage_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_width">width</code>, <code id="welcomeMessage_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_element_id">element_id</code></td>
<td>
<p>The element's id</p>
</td></tr>
</table>

<hr>
<h2 id='welcomeMessage-shiny'>Shiny bindings for welcomeMessage</h2><span id='topic+welcomeMessage-shiny'></span><span id='topic+welcomeMessageOutput'></span><span id='topic+renderWelcomeMessage'></span>

<h3>Description</h3>

<p>Output and render functions for using welcomeMessage within Shiny
applications and interactive Rmd documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>welcomeMessageOutput(outputId, width = "100%", height = NULL)

renderWelcomeMessage(expr, env = parent.frame(), quoted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="welcomeMessage-shiny_+3A_outputid">outputId</code></td>
<td>
<p>output variable to read from</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_width">width</code>, <code id="welcomeMessage-shiny_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_expr">expr</code></td>
<td>
<p>An expression that generates a welcomeMessage</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_env">env</code></td>
<td>
<p>The environment in which to evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_quoted">quoted</code></td>
<td>
<p>Is <code>expr</code> a quoted expression (with <code>quote()</code>)? This
is useful if you want to save an expression in a variable.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
