<!DOCTYPE html><html><head><title>Help for package gptstudio</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gptstudio}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#addin_chatgpt'><p>Run Chat GPT</p>
Run the Chat GPT Shiny App as a background job and show it in the viewer pane</a></li>
<li><a href='#addin_chatgpt_in_source'><p>ChatGPT in Source</p></a></li>
<li><a href='#addin_comment_code'><p>Comment Code Addin</p></a></li>
<li><a href='#addin_spelling_grammar'><p>Spelling and Grammar Addin</p></a></li>
<li><a href='#chat_create_system_prompt'><p>Create system prompt</p></a></li>
<li><a href='#chat_history_append'><p>Append to chat history</p></a></li>
<li><a href='#chat_message_default'><p>Default chat message</p></a></li>
<li><a href='#check_api'><p>Check API setup</p></a></li>
<li><a href='#check_api_connection'><p>Check connection to OpenAI's API works</p></a></li>
<li><a href='#check_api_key'><p>Check API key</p></a></li>
<li><a href='#create_chat_app_theme'><p>Chat App Theme</p></a></li>
<li><a href='#create_completion_anthropic'><p>Generate text completions using Anthropic's API</p></a></li>
<li><a href='#create_completion_azure_openai'><p>Generate text using Azure OpenAI's API</p></a></li>
<li><a href='#create_completion_huggingface'><p>Generate text completions using HuggingFace's API</p></a></li>
<li><a href='#create_completion_palm'><p>Generate text completions using PALM (MakerSuite)'s API</p></a></li>
<li><a href='#create_ide_matching_colors'><p>Chat message colors in RStudio</p></a></li>
<li><a href='#create_tmp_job_script'><p>Create a temporary job script</p></a></li>
<li><a href='#create_translator'><p>Internationalization for the ChatGPT addin</p></a></li>
<li><a href='#get_available_endpoints'><p>List supported endpoints</p></a></li>
<li><a href='#get_available_models'><p>List supported models</p></a></li>
<li><a href='#get_ide_theme_info'><p>Get IDE theme information.</p></a></li>
<li><a href='#gpt_chat'><p>ChatGPT in RStudio</p></a></li>
<li><a href='#gpt_chat_in_source'><p>ChatGPT in Source</p></a></li>
<li><a href='#gptstudio_job'><p>Perform Job</p></a></li>
<li><a href='#gptstudio_request_perform'><p>Perform API Request</p></a></li>
<li><a href='#gptstudio_response_process'><p>Call API</p></a></li>
<li><a href='#gptstudio_skeleton_build'><p>Construct a GPT Studio request skeleton.</p></a></li>
<li><a href='#gptstudio-package'><p>gptstudio: Use Large Language Models Directly in your Development Environment</p></a></li>
<li><a href='#mod_app_server'><p>App Server</p></a></li>
<li><a href='#mod_app_ui'><p>App UI</p></a></li>
<li><a href='#mod_chat_server'><p>Chat server</p></a></li>
<li><a href='#mod_chat_ui'><p>Chat UI</p></a></li>
<li><a href='#open_bg_shinyapp'><p>Open browser to local Shiny app</p></a></li>
<li><a href='#openai_create_chat_completion'><p>Generate text completions using OpenAI's API for Chat</p></a></li>
<li><a href='#openai_stream_parse'><p>OpenAI Stream Parse</p></a></li>
<li><a href='#prepare_chat_history'><p>Prepare chat completion prompt</p></a></li>
<li><a href='#query_api_anthropic'><p>A function that sends a request to the Anthropic API and returns the</p>
response.</a></li>
<li><a href='#query_api_huggingface'><p>A function that sends a request to the HuggingFace API and returns the</p>
response.</a></li>
<li><a href='#query_api_palm'><p>A function that sends a request to the PALM (MakerSuite) API and returns the</p>
response.</a></li>
<li><a href='#query_openai_api'><p>A function that sends a request to the OpenAI API and returns the response.</p></a></li>
<li><a href='#random_port'><p>Generate a random safe port number</p></a></li>
<li><a href='#request_base'><p>Base for a request to the OPENAI API</p></a></li>
<li><a href='#request_base_anthropic'><p>Base for a request to the Anthropic API</p></a></li>
<li><a href='#request_base_huggingface'><p>Base for a request to the HuggingFace API</p></a></li>
<li><a href='#request_base_palm'><p>Base for a request to the PALM (MakerSuite) API</p></a></li>
<li><a href='#rgb_str_to_hex'><p>RGB str to hex</p></a></li>
<li><a href='#run_app_as_bg_job'><p>Run an R Shiny app in the background</p></a></li>
<li><a href='#run_chatgpt_app'><p>Run the ChatGPT app</p></a></li>
<li><a href='#stream_chat_completion'><p>Stream Chat Completion</p></a></li>
<li><a href='#streamingMessage'><p>Streaming message</p></a></li>
<li><a href='#streamingMessage-shiny'><p>Shiny bindings for streamingMessage</p></a></li>
<li><a href='#style_chat_history'><p>Style Chat History</p></a></li>
<li><a href='#style_chat_message'><p>Style chat message</p></a></li>
<li><a href='#text_area_input_wrapper'><p>Custom textAreaInput</p></a></li>
<li><a href='#welcomeMessage'><p>Welcome message</p></a></li>
<li><a href='#welcomeMessage-shiny'><p>Shiny bindings for welcomeMessage</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Use Large Language Models Directly in your Development
Environment</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Wade &lt;github@jameshwade.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Large language models are readily accessible via API. This
    package lowers the barrier to use the API inside of your development
    environment.  For more on the API, see
    <a href="https://platform.openai.com/docs/introduction">https://platform.openai.com/docs/introduction</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MichelNivard/gptstudio">https://github.com/MichelNivard/gptstudio</a>,
<a href="https://michelnivard.github.io/gptstudio/">https://michelnivard.github.io/gptstudio/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MichelNivard/gptstudio/issues">https://github.com/MichelNivard/gptstudio/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat, bslib (&ge; 0.4.2), callr, cli, colorspace, glue,
grDevices, htmltools, htmlwidgets, httr2, jsonlite, magrittr,
purrr, rlang, rstudioapi (&ge; 0.12), shiny, shiny.i18n, stringr
(&ge; 1.5.0), utils, waiter, yaml</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mockr, shinytest2, spelling, testthat (&ge; 3.0.0), uuid, withr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-11 00:28:26 UTC; james</td>
</tr>
<tr>
<td>Author:</td>
<td>Michel Nivard [aut, cph],
  James Wade <a href="https://orcid.org/0000-0002-9740-1905"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  Samuel Calderon <a href="https://orcid.org/0000-0001-6847-1210"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-11 12:00:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='addin_chatgpt'>Run Chat GPT
Run the Chat GPT Shiny App as a background job and show it in the viewer pane</h2><span id='topic+addin_chatgpt'></span>

<h3>Description</h3>

<p>Run Chat GPT
Run the Chat GPT Shiny App as a background job and show it in the viewer pane
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addin_chatgpt(host = getOption("shiny.host", "127.0.0.1"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addin_chatgpt_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Call the function as an RStudio addin
## Not run: 
addin_chatgpt()

## End(Not run)
</code></pre>

<hr>
<h2 id='addin_chatgpt_in_source'>ChatGPT in Source</h2><span id='topic+addin_chatgpt_in_source'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to improve spelling and
grammar of selected text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addin_chatgpt_in_source()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select some text in a source file
# Then call the function as an RStudio addin
## Not run: 
addin_chatgpt_in_source()

## End(Not run)
</code></pre>

<hr>
<h2 id='addin_comment_code'>Comment Code Addin</h2><span id='topic+addin_comment_code'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to add comments to your code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addin_comment_code()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Open a R file in Rstudio
# Then call the function as an RStudio addin
## Not run: 
addin_comment_code()

## End(Not run)
</code></pre>

<hr>
<h2 id='addin_spelling_grammar'>Spelling and Grammar Addin</h2><span id='topic+addin_spelling_grammar'></span>

<h3>Description</h3>

<p>Call this function as a Rstudio addin to ask GPT to improve spelling and
grammar of selected text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addin_spelling_grammar()
</code></pre>


<h3>Value</h3>

<p>This function has no return value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select some text in Rstudio
# Then call the function as an RStudio addin
## Not run: 
addin_spelling_grammar()

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_create_system_prompt'>Create system prompt</h2><span id='topic+chat_create_system_prompt'></span>

<h3>Description</h3>

<p>This creates a system prompt based on the user defined parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_create_system_prompt(
  style = c("tidyverse", "base", "no preference", NULL),
  skill = c("beginner", "intermediate", "advanced", "genius", NULL),
  task = c("coding", "general", "advanced developer", "custom"),
  custom_prompt = NULL,
  in_source
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_create_system_prompt_+3A_style">style</code></td>
<td>
<p>A character string indicating the preferred coding style, the
default is &quot;tidyverse&quot;.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_skill">skill</code></td>
<td>
<p>The self-described skill level of the programmer,
default is &quot;beginner&quot;</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_task">task</code></td>
<td>
<p>The task to be performed: &quot;coding&quot;, &quot;general&quot;, or &quot;advanced developer&quot;.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>An optional custom prompt to be displayed.</p>
</td></tr>
<tr><td><code id="chat_create_system_prompt_+3A_in_source">in_source</code></td>
<td>
<p>Whether to add instructions to act as in a source script.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string
</p>

<hr>
<h2 id='chat_history_append'>Append to chat history</h2><span id='topic+chat_history_append'></span>

<h3>Description</h3>

<p>This appends a new response to the chat history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_history_append(history, role, content)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_history_append_+3A_history">history</code></td>
<td>
<p>List containing previous responses.</p>
</td></tr>
<tr><td><code id="chat_history_append_+3A_role">role</code></td>
<td>
<p>Author of the message. One of <code>c("user", "assistant")</code></p>
</td></tr>
<tr><td><code id="chat_history_append_+3A_content">content</code></td>
<td>
<p>Content of the message. If it is from the user most probably
comes from an interactive input.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of chat messages
</p>

<hr>
<h2 id='chat_message_default'>Default chat message</h2><span id='topic+chat_message_default'></span>

<h3>Description</h3>

<p>Default chat message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_message_default(translator = create_translator())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_message_default_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A default chat message for welcoming users.
</p>

<hr>
<h2 id='check_api'>Check API setup</h2><span id='topic+check_api'></span>

<h3>Description</h3>

<p>This function checks whether the API key provided in the <code>OPENAI_API_KEY</code>
environment variable is valid. This function will not re-check an API if it
has already been validated in the current session.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_api()
</code></pre>


<h3>Value</h3>

<p>Nothing is returned. If the API key is valid, a success message is
printed. If the API key is invalid, an error message is printed and the
function aborts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Call the function to check the API key
## Not run: 
check_api()

## End(Not run)
</code></pre>

<hr>
<h2 id='check_api_connection'>Check connection to OpenAI's API works</h2><span id='topic+check_api_connection'></span>

<h3>Description</h3>

<p>This function checks whether the API key provided in the <code>OPENAI_API_KEY</code>
environment variable is valid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_api_connection(api_key, update_api = TRUE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_api_connection_+3A_api_key">api_key</code></td>
<td>
<p>An API key.</p>
</td></tr>
<tr><td><code id="check_api_connection_+3A_update_api">update_api</code></td>
<td>
<p>Whether to attempt to update api if invalid</p>
</td></tr>
<tr><td><code id="check_api_connection_+3A_verbose">verbose</code></td>
<td>
<p>Whether to provide information about the API connection</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing is returned. If the API key is valid, a success message is
printed. If the API key is invalid, an error message is printed and the
function is aborted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Call the function with an API key
## Not run: 
check_api_connection("my_api_key")

## End(Not run)
# Call the function with an API key and avoid updating the API key
## Not run: 
check_api_connection("my_api_key", update_api = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_api_key'>Check API key</h2><span id='topic+check_api_key'></span>

<h3>Description</h3>

<p>This function checks whether the API key provided as an argument is in the
correct format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_api_key(api_key, update_api = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_api_key_+3A_api_key">api_key</code></td>
<td>
<p>An API key.</p>
</td></tr>
<tr><td><code id="check_api_key_+3A_update_api">update_api</code></td>
<td>
<p>Whether to attempt to update api if invalid</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing is returned. If the API key is in the correct format, a
success message is printed. If the API key is not in the correct format,
an error message is printed and the function aborts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Call the function with an API key
## Not run: 
check_api_key("my_api_key")

## End(Not run)
# Call the function with an API key and avoid updating the API key
## Not run: 
check_api_key("my_api_key", update_api = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_chat_app_theme'>Chat App Theme</h2><span id='topic+create_chat_app_theme'></span>

<h3>Description</h3>

<p>Create a bslib theme that matches the user's RStudio IDE theme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_chat_app_theme(ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_chat_app_theme_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A bslib theme
</p>

<hr>
<h2 id='create_completion_anthropic'>Generate text completions using Anthropic's API</h2><span id='topic+create_completion_anthropic'></span>

<h3>Description</h3>

<p>Generate text completions using Anthropic's API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_anthropic(
  prompt,
  history = NULL,
  model = "claude-1",
  max_tokens_to_sample = 256,
  key = Sys.getenv("ANTHROPIC_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_anthropic_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_history">history</code></td>
<td>
<p>A list of the previous chat responses</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_model">model</code></td>
<td>
<p>The model to use for generating text. By default, the
function will try to use &quot;claude-1&quot;.</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_max_tokens_to_sample">max_tokens_to_sample</code></td>
<td>
<p>The maximum number of tokens to generate. Defaults to 256.</p>
</td></tr>
<tr><td><code id="create_completion_anthropic_+3A_key">key</code></td>
<td>
<p>The API key for accessing Anthropic's API. By default, the
function will try to use the <code>ANTHROPIC_API_KEY</code> environment variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_anthropic(
  prompt = "\n\nHuman: Hello, world!\n\nAssistant:",
  model = "claude-1",
  max_tokens_to_sample = 256
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_completion_azure_openai'>Generate text using Azure OpenAI's API</h2><span id='topic+create_completion_azure_openai'></span>

<h3>Description</h3>

<p>Use this function to generate text completions using OpenAI's
API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_azure_openai(
  prompt,
  task = Sys.getenv("AZURE_OPENAI_TASK"),
  base_url = Sys.getenv("AZURE_OPENAI_ENDPOINT"),
  deployment_name = Sys.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
  token = Sys.getenv("AZURE_OPENAI_KEY"),
  api_version = Sys.getenv("AZURE_OPENAI_API_VERSION")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_azure_openai_+3A_prompt">prompt</code></td>
<td>
<p>a list to use as the prompt for generating
completions</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_task">task</code></td>
<td>
<p>a character string for the API task. Defaults to the Azure OpenAI
task from environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_base_url">base_url</code></td>
<td>
<p>a character string for the base url. It defaults to the Azure
OpenAI endpoint from environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_deployment_name">deployment_name</code></td>
<td>
<p>a character string for the deployment name. It will
default to the Azure OpenAI deployment name from environment variables if
not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_token">token</code></td>
<td>
<p>a character string for the API key. It will default to the Azure
OpenAI API key from your environment variables if not specified.</p>
</td></tr>
<tr><td><code id="create_completion_azure_openai_+3A_api_version">api_version</code></td>
<td>
<p>a character string for the API version. It will default to
the Azure OpenAI API version from your environment variables if not
specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the generated completions and other information returned
by the API
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_azure_openai(
  prompt = list(list(role = "user", content = "Hello world!"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='create_completion_huggingface'>Generate text completions using HuggingFace's API</h2><span id='topic+create_completion_huggingface'></span>

<h3>Description</h3>

<p>Generate text completions using HuggingFace's API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_huggingface(
  prompt,
  history = NULL,
  model = "tiiuae/falcon-7b-instruct",
  token = Sys.getenv("HF_API_KEY"),
  max_new_tokens = 250
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_huggingface_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_history">history</code></td>
<td>
<p>A list of the previous chat responses</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_model">model</code></td>
<td>
<p>The model to use for generating text</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_token">token</code></td>
<td>
<p>The API key for accessing HuggingFace's API. By default, the
function will try to use the <code>HF_API_KEY</code> environment variable.</p>
</td></tr>
<tr><td><code id="create_completion_huggingface_+3A_max_new_tokens">max_new_tokens</code></td>
<td>
<p>Maximum number of tokens to generate, defaults to 250</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_huggingface(
  model = "gpt2",
  prompt = "Hello world!"
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_completion_palm'>Generate text completions using PALM (MakerSuite)'s API</h2><span id='topic+create_completion_palm'></span>

<h3>Description</h3>

<p>Generate text completions using PALM (MakerSuite)'s API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_completion_palm(
  prompt,
  model = "text-bison-001",
  key = Sys.getenv("PALM_API_KEY"),
  temperature = 0.5,
  candidate_count = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_completion_palm_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="create_completion_palm_+3A_model">model</code></td>
<td>
<p>The model to use for generating text. By default, the
function will try to use &quot;text-bison-001&quot;</p>
</td></tr>
<tr><td><code id="create_completion_palm_+3A_key">key</code></td>
<td>
<p>The API key for accessing PALM (MakerSuite)'s API. By default, the
function will try to use the <code>PALM_API_KEY</code> environment variable.</p>
</td></tr>
<tr><td><code id="create_completion_palm_+3A_temperature">temperature</code></td>
<td>
<p>The temperature to control the randomness of the model's output</p>
</td></tr>
<tr><td><code id="create_completion_palm_+3A_candidate_count">candidate_count</code></td>
<td>
<p>The number of completion candidates to generate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
create_completion_palm(
  prompt = list(text = "Write a story about a magic backpack"),
  temperature = 1.0,
  candidate_count = 3
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_ide_matching_colors'>Chat message colors in RStudio</h2><span id='topic+create_ide_matching_colors'></span>

<h3>Description</h3>

<p>This returns a list of color properties for a chat message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ide_matching_colors(role, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ide_matching_colors_+3A_role">role</code></td>
<td>
<p>The role of the message author</p>
</td></tr>
<tr><td><code id="create_ide_matching_colors_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list
</p>

<hr>
<h2 id='create_tmp_job_script'>Create a temporary job script</h2><span id='topic+create_tmp_job_script'></span>

<h3>Description</h3>

<p>This function creates a temporary R script file that runs the Shiny
application from the specified directory with the specified port and host.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tmp_job_script(appDir, port, host)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_tmp_job_script_+3A_appdir">appDir</code></td>
<td>
<p>The application to run. Should be one of the following:
</p>

<ul>
<li><p> A directory containing <code>server.R</code>, plus, either <code>ui.R</code> or
a <code>www</code> directory that contains the file <code>index.html</code>.
</p>
</li>
<li><p> A directory containing <code>app.R</code>.
</p>
</li>
<li><p> An <code>.R</code> file containing a Shiny application, ending with an
expression that produces a Shiny app object.
</p>
</li>
<li><p> A list with <code>ui</code> and <code>server</code> components.
</p>
</li>
<li><p> A Shiny app object created by <code><a href="shiny.html#topic+shinyApp">shinyApp()</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="create_tmp_job_script_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
<tr><td><code id="create_tmp_job_script_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string containing the path of a temporary job script
</p>

<hr>
<h2 id='create_translator'>Internationalization for the ChatGPT addin</h2><span id='topic+create_translator'></span>

<h3>Description</h3>

<p>The language can be set via <code>options("gptstudio.language" = "&lt;language&gt;")</code>
(defaults to &quot;en&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_translator(language = getOption("gptstudio.language"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_translator_+3A_language">language</code></td>
<td>
<p>The language to be found in the translation JSON file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Translator from <code>shiny.i18n::Translator</code>
</p>

<hr>
<h2 id='get_available_endpoints'>List supported endpoints</h2><span id='topic+get_available_endpoints'></span>

<h3>Description</h3>

<p>Get a list of the endpoints supported by gptstudio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_available_endpoints()
</code></pre>


<h3>Value</h3>

<p>A character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_available_endpoints()
</code></pre>

<hr>
<h2 id='get_available_models'>List supported models</h2><span id='topic+get_available_models'></span>

<h3>Description</h3>

<p>Get a list of the models supported by the OpenAI API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_available_models(service)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_available_models_+3A_service">service</code></td>
<td>
<p>The API service</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_available_endpoints()
</code></pre>

<hr>
<h2 id='get_ide_theme_info'>Get IDE theme information.</h2><span id='topic+get_ide_theme_info'></span>

<h3>Description</h3>

<p>This function returns a list with the current IDE theme's information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ide_theme_info()
</code></pre>


<h3>Value</h3>

<p>A list with three components:
</p>
<table>
<tr><td><code>is_dark</code></td>
<td>
<p>A boolean indicating whether the current IDE theme is dark.</p>
</td></tr>
<tr><td><code>bg</code></td>
<td>
<p>The current IDE theme's background color.</p>
</td></tr>
<tr><td><code>fg</code></td>
<td>
<p>The current IDE theme's foreground color.</p>
</td></tr>
</table>

<hr>
<h2 id='gpt_chat'>ChatGPT in RStudio</h2><span id='topic+gpt_chat'></span>

<h3>Description</h3>

<p>This function uses the ChatGPT API tailored to a user-provided style and
skill level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpt_chat(
  history,
  style = getOption("gptstudio.code_style"),
  skill = getOption("gptstudio.skill"),
  model = getOption("gptstudio.model")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpt_chat_+3A_history">history</code></td>
<td>
<p>A list of the previous chat responses</p>
</td></tr>
<tr><td><code id="gpt_chat_+3A_style">style</code></td>
<td>
<p>A character string indicating the preferred coding style, the
default is &quot;tidyverse&quot;.</p>
</td></tr>
<tr><td><code id="gpt_chat_+3A_skill">skill</code></td>
<td>
<p>The self-described skill level of the programmer,
default is &quot;beginner&quot;</p>
</td></tr>
<tr><td><code id="gpt_chat_+3A_model">model</code></td>
<td>
<p>The name of the GPT model to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the instructions for answering the question, the
context in which the question was asked, and the suggested answer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example 1: Get help with a tidyverse question
tidyverse_query &lt;- "How can I filter rows of a data frame?"
tidyverse_response &lt;- gpt_chat(
  query = tidyverse_query,
  style = "tidyverse",
  skill = "beginner"
)
print(tidyverse_response)

# Example 2: Get help with a base R question
base_r_query &lt;- "How can I merge two data frames?"
base_r_response &lt;- gpt_chat(
  query = base_r_query,
  style = "base",
  skill = "intermediate"
)
print(base_r_response)

# Example 3: No style preference
no_preference_query &lt;- "What is the best way to handle missing values in R?"
no_preference_response &lt;- gpt_chat(
  query = no_preference_query,
  style = "no preference",
  skill = "advanced"
)
print(no_preference_response)

## End(Not run)
</code></pre>

<hr>
<h2 id='gpt_chat_in_source'>ChatGPT in Source</h2><span id='topic+gpt_chat_in_source'></span>

<h3>Description</h3>

<p>Provides the same functionality as <code>gpt_chat()</code> with minor modifications to
give more useful output in a source (i.e., *.R) file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpt_chat_in_source(
  history = NULL,
  task = NULL,
  style = getOption("gptstudio.code_style"),
  skill = getOption("gptstudio.skill")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpt_chat_in_source_+3A_history">history</code></td>
<td>
<p>A list of the previous chat responses</p>
</td></tr>
<tr><td><code id="gpt_chat_in_source_+3A_task">task</code></td>
<td>
<p>Specific instructions to provide to the model as a system prompt</p>
</td></tr>
<tr><td><code id="gpt_chat_in_source_+3A_style">style</code></td>
<td>
<p>A character string indicating the preferred coding style, the
default is &quot;tidyverse&quot;.</p>
</td></tr>
<tr><td><code id="gpt_chat_in_source_+3A_skill">skill</code></td>
<td>
<p>The self-described skill level of the programmer,
default is &quot;beginner&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the instructions for answering the question, the
context in which the question was asked, and the suggested answer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example 1: Get help with a tidyverse question in a source file
# Select the following code comment in RStudio and run gpt_chat_in_source()
# How can I filter rows of a data frame?
tidyverse_response &lt;- gpt_chat_in_source(
  style = "tidyverse",
  skill = "beginner"
)

# Example 2: Get help with a base R question in a source file
# Select the following code comment in RStudio and run gpt_chat_in_source()
# How can I merge two data frames?
base_r_response &lt;- gpt_chat_in_source(style = "base", skill = "intermediate")

# Example 3: No style preference in a source file
# Select the following code comment in RStudio and run gpt_chat_in_source()
# What is the best way to handle missing values in R?
no_preference_response &lt;- gpt_chat_in_source(
  style = "no preference",
  skill = "advanced"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='gptstudio_job'>Perform Job</h2><span id='topic+gptstudio_job'></span>

<h3>Description</h3>

<p>Combined job to build the skeleton, perform the api request, and process
the response
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_job(
  skeleton = gptstudio_create_skeleton(),
  skill = getOption("gptstudio.skill"),
  style = getOption("gptstudio.code_style"),
  task = getOption("gptstudio.task"),
  custom_prompt = getOption("gptstudio.custom_prompt")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_job_+3A_skeleton">skeleton</code></td>
<td>
<p>A GPT Studio request skeleton object.</p>
</td></tr>
<tr><td><code id="gptstudio_job_+3A_skill">skill</code></td>
<td>
<p>The skill level of the user for the chat conversation. This can
be set through the &quot;gptstudio.skill&quot; option. Default is the
&quot;gptstudio.skill&quot; option. Options are &quot;beginner&quot;, &quot;intermediate&quot;,
&quot;advanced&quot;, and &quot;genius&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_job_+3A_style">style</code></td>
<td>
<p>The style of code to use. Applicable styles can be retrieved
from the &quot;gptstudio.code_style&quot; option. Default is the
&quot;gptstudio.code_style&quot; option. Options are &quot;base&quot;, &quot;tidyverse&quot;, or &quot;no
preference&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_job_+3A_task">task</code></td>
<td>
<p>Specifies the task that the assistant will help with. Default is
&quot;coding&quot;. Others are &quot;general&quot;, &quot;advanced developer&quot;, and &quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_job_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>This is a custom prompt that may be used to guide the AI
in its responses. Default is NULL. It will be the only content provided to
the system prompt.</p>
</td></tr>
</table>

<hr>
<h2 id='gptstudio_request_perform'>Perform API Request</h2><span id='topic+gptstudio_request_perform'></span>

<h3>Description</h3>

<p>This function provides a generic interface for calling different APIs
(e.g., OpenAI, HuggingFace, PALM (MakerSuite)). It dispatches the actual API
calls to the relevant method based on the <code>class</code> of the <code>skeleton</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_request_perform(skeleton, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_request_perform_+3A_skeleton">skeleton</code></td>
<td>
<p>A <code>gptstudio_request_skeleton</code> object</p>
</td></tr>
<tr><td><code id="gptstudio_request_perform_+3A_...">...</code></td>
<td>
<p>Extra arguments (e.g., <code>stream_handler</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>gptstudio_response_skeleton</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gptstudio_request_perform(gptstudio_skeleton)

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_response_process'>Call API</h2><span id='topic+gptstudio_response_process'></span>

<h3>Description</h3>

<p>This function provides a generic interface for calling different APIs
(e.g., OpenAI, HuggingFace, PALM (MakerSuite)). It dispatches the actual API
calls to the relevant method based on the <code>class</code> of the <code>skeleton</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_response_process(skeleton, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_response_process_+3A_skeleton">skeleton</code></td>
<td>
<p>A <code>gptstudio_response_skeleton</code> object</p>
</td></tr>
<tr><td><code id="gptstudio_response_process_+3A_...">...</code></td>
<td>
<p>Extra arguments, not currently used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>gptstudio_request_skeleton</code> with updated history and prompt removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gptstudio_response_process(gptstudio_skeleton)

## End(Not run)
</code></pre>

<hr>
<h2 id='gptstudio_skeleton_build'>Construct a GPT Studio request skeleton.</h2><span id='topic+gptstudio_skeleton_build'></span>

<h3>Description</h3>

<p>Construct a GPT Studio request skeleton.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gptstudio_skeleton_build(skeleton, skill, style, task, custom_prompt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gptstudio_skeleton_build_+3A_skeleton">skeleton</code></td>
<td>
<p>A GPT Studio request skeleton object.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_skill">skill</code></td>
<td>
<p>The skill level of the user for the chat conversation. This can
be set through the &quot;gptstudio.skill&quot; option. Default is the
&quot;gptstudio.skill&quot; option. Options are &quot;beginner&quot;, &quot;intermediate&quot;,
&quot;advanced&quot;, and &quot;genius&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_style">style</code></td>
<td>
<p>The style of code to use. Applicable styles can be retrieved
from the &quot;gptstudio.code_style&quot; option. Default is the
&quot;gptstudio.code_style&quot; option. Options are &quot;base&quot;, &quot;tidyverse&quot;, or &quot;no
preference&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_task">task</code></td>
<td>
<p>Specifies the task that the assistant will help with. Default is
&quot;coding&quot;. Others are &quot;general&quot;, &quot;advanced developer&quot;, and &quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>This is a custom prompt that may be used to guide the AI
in its responses. Default is NULL. It will be the only content provided to
the system prompt.</p>
</td></tr>
<tr><td><code id="gptstudio_skeleton_build_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated GPT Studio request skeleton.
</p>

<hr>
<h2 id='gptstudio-package'>gptstudio: Use Large Language Models Directly in your Development Environment</h2><span id='topic+gptstudio'></span><span id='topic+gptstudio-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Large language models are readily accessible via API. This package lowers the barrier to use the API inside of your development environment. For more on the API, see <a href="https://platform.openai.com/docs/introduction">https://platform.openai.com/docs/introduction</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: James Wade <a href="mailto:github@jameshwade.com">github@jameshwade.com</a> (<a href="https://orcid.org/0000-0002-9740-1905">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Michel Nivard <a href="mailto:m.g.nivard@vu.nl">m.g.nivard@vu.nl</a> [copyright holder]
</p>
</li>
<li><p> Samuel Calderon <a href="mailto:samuel.calderon@uarm.pe">samuel.calderon@uarm.pe</a> (<a href="https://orcid.org/0000-0001-6847-1210">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/MichelNivard/gptstudio">https://github.com/MichelNivard/gptstudio</a>
</p>
</li>
<li> <p><a href="https://michelnivard.github.io/gptstudio/">https://michelnivard.github.io/gptstudio/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/MichelNivard/gptstudio/issues">https://github.com/MichelNivard/gptstudio/issues</a>
</p>
</li></ul>


<hr>
<h2 id='mod_app_server'>App Server</h2><span id='topic+mod_app_server'></span>

<h3>Description</h3>

<p>App Server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_app_server(id, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_app_server_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_app_server_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>

<hr>
<h2 id='mod_app_ui'>App UI</h2><span id='topic+mod_app_ui'></span>

<h3>Description</h3>

<p>App UI
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_app_ui(id, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_app_ui_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_app_ui_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>

<hr>
<h2 id='mod_chat_server'>Chat server</h2><span id='topic+mod_chat_server'></span>

<h3>Description</h3>

<p>Chat server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_chat_server(
  id,
  ide_colors = get_ide_theme_info(),
  translator = create_translator()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_chat_server_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_chat_server_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="mod_chat_server_+3A_translator">translator</code></td>
<td>
<p>Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
</table>

<hr>
<h2 id='mod_chat_ui'>Chat UI</h2><span id='topic+mod_chat_ui'></span>

<h3>Description</h3>

<p>Chat UI
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mod_chat_ui(id, translator = create_translator())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mod_chat_ui_+3A_id">id</code></td>
<td>
<p>id of the module</p>
</td></tr>
<tr><td><code id="mod_chat_ui_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
</table>

<hr>
<h2 id='open_bg_shinyapp'>Open browser to local Shiny app</h2><span id='topic+open_bg_shinyapp'></span>

<h3>Description</h3>

<p>This function takes in the host and port of a local Shiny app and opens the
app in the default browser.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_bg_shinyapp(host, port)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="open_bg_shinyapp_+3A_host">host</code></td>
<td>
<p>A character string representing the IP address or domain name of
the server where the Shiny app is hosted.</p>
</td></tr>
<tr><td><code id="open_bg_shinyapp_+3A_port">port</code></td>
<td>
<p>An integer representing the port number on which the Shiny app is
hosted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None (opens the Shiny app in the viewer pane or browser window)
</p>

<hr>
<h2 id='openai_create_chat_completion'>Generate text completions using OpenAI's API for Chat</h2><span id='topic+openai_create_chat_completion'></span>

<h3>Description</h3>

<p>Generate text completions using OpenAI's API for Chat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_create_chat_completion(
  prompt = "&lt;|endoftext|&gt;",
  model = getOption("gptstudio.model"),
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  task = "chat/completions"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="openai_create_chat_completion_+3A_prompt">prompt</code></td>
<td>
<p>The prompt for generating completions</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_model">model</code></td>
<td>
<p>The model to use for generating text</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>The API key for accessing OpenAI's API. By default, the
function will try to use the <code>OPENAI_API_KEY</code> environment variable.</p>
</td></tr>
<tr><td><code id="openai_create_chat_completion_+3A_task">task</code></td>
<td>
<p>The task that specifies the API url to use, defaults to
&quot;completions&quot; and &quot;chat/completions&quot; is required for ChatGPT model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the generated completions and other information returned
by the API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
openai_create_completion(
  model = "text-davinci-002",
  prompt = "Hello world!"
)

## End(Not run)
</code></pre>

<hr>
<h2 id='openai_stream_parse'>OpenAI Stream Parse</h2><span id='topic+openai_stream_parse'></span>

<h3>Description</h3>

<p>This function handles the streaming data from the OpenAI API.
It concatenates the raw data chunks, attempts to parse JSON and
handles any error messages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_stream_parse(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="openai_stream_parse_+3A_x">x</code></td>
<td>
<p>A raw vector representing a chunk of data from the API stream.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function was inspired by the <code>{chattr}</code> R package
(https://github.com/mlverse/chattr).
</p>


<h3>Value</h3>

<p>If parsing is successful, a character string of the API response is
returned. In case of an error, an error message is returned instead.
</p>

<hr>
<h2 id='prepare_chat_history'>Prepare chat completion prompt</h2><span id='topic+prepare_chat_history'></span>

<h3>Description</h3>

<p>This function prepares the chat completion prompt to be sent to the OpenAI API.
It also generates a system message according to the given parameters and inserts
it at the beginning of the conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_chat_history(
  history = NULL,
  style = getOption("gptstudio.code_style"),
  skill = getOption("gptstudio.skill"),
  task = "coding",
  custom_prompt = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_chat_history_+3A_history">history</code></td>
<td>
<p>A list of previous messages in the conversation. This can include
roles such as 'system', 'user', or 'assistant'. System messages are discarded.
Default is NULL.i</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_style">style</code></td>
<td>
<p>The style of code to use. Applicable styles can be
retrieved from the &quot;gptstudio.code_style&quot; option. Default is the
&quot;gptstudio.code_style&quot; option. Options are &quot;base&quot;, &quot;tidyverse&quot;, or
&quot;no preference&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_skill">skill</code></td>
<td>
<p>The skill level of the user for the chat conversation. This
can be set through the &quot;gptstudio.skill&quot; option. Default is the
&quot;gptstudio.skill&quot; option. Options are &quot;beginner&quot;, &quot;intermediate&quot;,
&quot;advanced&quot;, and &quot;genius&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_task">task</code></td>
<td>
<p>Specifies the task that the assistant will help with. Default is
&quot;coding&quot;. Others are &quot;general&quot;, &quot;advanced developer&quot;, and &quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="prepare_chat_history_+3A_custom_prompt">custom_prompt</code></td>
<td>
<p>This is a custom prompt that may be used to guide the AI in
its responses. Default is NULL. It will be the only content provided to the
system prompt.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first entry is an initial system message followed by any
non-system entries from the chat history.
</p>

<hr>
<h2 id='query_api_anthropic'>A function that sends a request to the Anthropic API and returns the
response.</h2><span id='topic+query_api_anthropic'></span>

<h3>Description</h3>

<p>A function that sends a request to the Anthropic API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_anthropic(request_body, key = Sys.getenv("ANTHROPIC_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_anthropic_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_anthropic_+3A_key">key</code></td>
<td>
<p>String containing an Anthropic API key. Defaults
to the ANTHROPIC_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_api_huggingface'>A function that sends a request to the HuggingFace API and returns the
response.</h2><span id='topic+query_api_huggingface'></span>

<h3>Description</h3>

<p>A function that sends a request to the HuggingFace API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_huggingface(task, request_body, token = Sys.getenv("HF_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_huggingface_+3A_task">task</code></td>
<td>
<p>A character string that specifies the task to send to the API.</p>
</td></tr>
<tr><td><code id="query_api_huggingface_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_huggingface_+3A_token">token</code></td>
<td>
<p>String containing a HuggingFace API key. Defaults
to the HF_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_api_palm'>A function that sends a request to the PALM (MakerSuite) API and returns the
response.</h2><span id='topic+query_api_palm'></span>

<h3>Description</h3>

<p>A function that sends a request to the PALM (MakerSuite) API and returns the
response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_api_palm(model, request_body, key = Sys.getenv("PALM_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_api_palm_+3A_model">model</code></td>
<td>
<p>A character string that specifies the model to send to the API.</p>
</td></tr>
<tr><td><code id="query_api_palm_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_api_palm_+3A_key">key</code></td>
<td>
<p>String containing a PALM (MakerSuite) API key. Defaults
to the PALM_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='query_openai_api'>A function that sends a request to the OpenAI API and returns the response.</h2><span id='topic+query_openai_api'></span>

<h3>Description</h3>

<p>A function that sends a request to the OpenAI API and returns the response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query_openai_api(
  task,
  request_body,
  openai_api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_openai_api_+3A_task">task</code></td>
<td>
<p>A character string that specifies the task to send to the API.</p>
</td></tr>
<tr><td><code id="query_openai_api_+3A_request_body">request_body</code></td>
<td>
<p>A list that contains the parameters for the task.</p>
</td></tr>
<tr><td><code id="query_openai_api_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>String containing an OpenAI API key. Defaults to the OPENAI_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The response from the API.
</p>

<hr>
<h2 id='random_port'>Generate a random safe port number</h2><span id='topic+random_port'></span>

<h3>Description</h3>

<p>This function generates a random port allowed by shiny::runApp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_port()
</code></pre>


<h3>Value</h3>

<p>A single integer representing the randomly selected safe port number.
</p>

<hr>
<h2 id='request_base'>Base for a request to the OPENAI API</h2><span id='topic+request_base'></span>

<h3>Description</h3>

<p>This function sends a request to a specific OpenAI API <code>task</code> endpoint at the base URL <code>https://api.openai.com/v1</code>, and authenticates with an API key using a Bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base(task, token = Sys.getenv("OPENAI_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_+3A_task">task</code></td>
<td>
<p>character string specifying an OpenAI API endpoint task</p>
</td></tr>
<tr><td><code id="request_base_+3A_token">token</code></td>
<td>
<p>String containing an OpenAI API key. Defaults to the OPENAI_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_anthropic'>Base for a request to the Anthropic API</h2><span id='topic+request_base_anthropic'></span>

<h3>Description</h3>

<p>This function sends a request to the Anthropic API endpoint and
authenticates with an API key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_anthropic(key = Sys.getenv("ANTHROPIC_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_anthropic_+3A_key">key</code></td>
<td>
<p>String containing an Anthropic API key. Defaults to the
ANTHROPIC_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_huggingface'>Base for a request to the HuggingFace API</h2><span id='topic+request_base_huggingface'></span>

<h3>Description</h3>

<p>This function sends a request to a specific HuggingFace API endpoint and
authenticates with an API key using a Bearer token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_huggingface(task, token = Sys.getenv("HF_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_huggingface_+3A_task">task</code></td>
<td>
<p>character string specifying a HuggingFace API endpoint task</p>
</td></tr>
<tr><td><code id="request_base_huggingface_+3A_token">token</code></td>
<td>
<p>String containing a HuggingFace API key. Defaults to the
HF_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='request_base_palm'>Base for a request to the PALM (MakerSuite) API</h2><span id='topic+request_base_palm'></span>

<h3>Description</h3>

<p>This function sends a request to a specific PALM (MakerSuite) API endpoint and
authenticates with an API key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_base_palm(model, key = Sys.getenv("PALM_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_base_palm_+3A_model">model</code></td>
<td>
<p>character string specifying a PALM (MakerSuite) API model</p>
</td></tr>
<tr><td><code id="request_base_palm_+3A_key">key</code></td>
<td>
<p>String containing a PALM (MakerSuite) API key. Defaults to the
PALM_API_KEY environmental variable if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An httr2 request object
</p>

<hr>
<h2 id='rgb_str_to_hex'>RGB str to hex</h2><span id='topic+rgb_str_to_hex'></span>

<h3>Description</h3>

<p>RGB str to hex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rgb_str_to_hex(rgb_string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rgb_str_to_hex_+3A_rgb_string">rgb_string</code></td>
<td>
<p>The RGB string as returned by <code>rstudioapi::getThemeInfo()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>hex color
</p>

<hr>
<h2 id='run_app_as_bg_job'>Run an R Shiny app in the background</h2><span id='topic+run_app_as_bg_job'></span>

<h3>Description</h3>

<p>This function runs an R Shiny app as a background job using the specified
directory, name, host, and port.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_app_as_bg_job(appDir = ".", job_name, host, port)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_app_as_bg_job_+3A_appdir">appDir</code></td>
<td>
<p>The application to run. Should be one of the following:
</p>

<ul>
<li><p> A directory containing <code>server.R</code>, plus, either <code>ui.R</code> or
a <code>www</code> directory that contains the file <code>index.html</code>.
</p>
</li>
<li><p> A directory containing <code>app.R</code>.
</p>
</li>
<li><p> An <code>.R</code> file containing a Shiny application, ending with an
expression that produces a Shiny app object.
</p>
</li>
<li><p> A list with <code>ui</code> and <code>server</code> components.
</p>
</li>
<li><p> A Shiny app object created by <code><a href="shiny.html#topic+shinyApp">shinyApp()</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_job_name">job_name</code></td>
<td>
<p>The name of the background job to be created</p>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
<tr><td><code id="run_app_as_bg_job_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns nothing because is meant to run an app as a
side effect.
</p>

<hr>
<h2 id='run_chatgpt_app'>Run the ChatGPT app</h2><span id='topic+run_chatgpt_app'></span>

<h3>Description</h3>

<p>This starts the chatgpt app. It is exported to be able to run it from an R
script.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_chatgpt_app(
  ide_colors = get_ide_theme_info(),
  host = getOption("shiny.host", "127.0.0.1"),
  port = getOption("shiny.port")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_chatgpt_app_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="run_chatgpt_app_+3A_host">host</code></td>
<td>
<p>The IPv4 address that the application should listen on. Defaults
to the <code>shiny.host</code> option, if set, or <code>"127.0.0.1"</code> if not. See
Details.</p>
</td></tr>
<tr><td><code id="run_chatgpt_app_+3A_port">port</code></td>
<td>
<p>The TCP port that the application should listen on. If the
<code>port</code> is not specified, and the <code>shiny.port</code> option is set (with
<code>options(shiny.port = XX)</code>), then that port will be used. Otherwise,
use a random port between 3000:8000, excluding ports that are blocked
by Google Chrome for being considered unsafe: 3659, 4045, 5060,
5061, 6000, 6566, 6665:6669 and 6697. Up to twenty random
ports will be tried.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing.
</p>

<hr>
<h2 id='stream_chat_completion'>Stream Chat Completion</h2><span id='topic+stream_chat_completion'></span>

<h3>Description</h3>

<p><code>stream_chat_completion</code> sends the prepared chat completion request to the
OpenAI API and retrieves the streamed response. The results are then stored
in a temporary file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_chat_completion(
  prompt,
  model = "gpt-3.5-turbo",
  openai_api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stream_chat_completion_+3A_prompt">prompt</code></td>
<td>
<p>A list of messages. Each message is a list that includes a
&quot;role&quot; and &quot;content&quot;. The &quot;role&quot; can be &quot;system&quot;, &quot;user&quot;, or &quot;assistant&quot;.
The &quot;content&quot; is the text of the message from the role.</p>
</td></tr>
<tr><td><code id="stream_chat_completion_+3A_model">model</code></td>
<td>
<p>A character string specifying the model to use for chat completion.
The default model is &quot;gpt-3.5-turbo&quot;.</p>
</td></tr>
<tr><td><code id="stream_chat_completion_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>A character string of the OpenAI API key.
By default, it is fetched from the &quot;OPENAI_API_KEY&quot; environment variable.
Please note that the OpenAI API key is sensitive information and should be
treated accordingly.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string specifying the path to the tempfile that contains the
full response from the OpenAI API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Get API key from your environment variables
openai_api_key &lt;- Sys.getenv("OPENAI_API_KEY")

# Define the prompt
prompt &lt;- list(
  list(role = "system", content = "You are a helpful assistant."),
  list(role = "user", content = "Who won the world series in 2020?")
)

# Call the function
result &lt;- stream_chat_completion(prompt = prompt, openai_api_key = openai_api_key)

# Print the result
print(result)

## End(Not run)
</code></pre>

<hr>
<h2 id='streamingMessage'>Streaming message</h2><span id='topic+streamingMessage'></span>

<h3>Description</h3>

<p>Places an invisible empty chat message that will hold a streaming message.
It can be reset dynamically inside a shiny app
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamingMessage(
  ide_colors = get_ide_theme_info(),
  width = NULL,
  height = NULL,
  elementId = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="streamingMessage_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="streamingMessage_+3A_width">width</code>, <code id="streamingMessage_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="streamingMessage_+3A_elementid">elementId</code></td>
<td>
<p>The element's id</p>
</td></tr>
</table>

<hr>
<h2 id='streamingMessage-shiny'>Shiny bindings for streamingMessage</h2><span id='topic+streamingMessage-shiny'></span><span id='topic+streamingMessageOutput'></span><span id='topic+renderStreamingMessage'></span>

<h3>Description</h3>

<p>Output and render functions for using streamingMessage within Shiny
applications and interactive Rmd documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamingMessageOutput(outputId, width = "100%", height = NULL)

renderStreamingMessage(expr, env = parent.frame(), quoted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="streamingMessage-shiny_+3A_outputid">outputId</code></td>
<td>
<p>output variable to read from</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_width">width</code>, <code id="streamingMessage-shiny_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_expr">expr</code></td>
<td>
<p>An expression that generates a streamingMessage</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_env">env</code></td>
<td>
<p>The environment in which to evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="streamingMessage-shiny_+3A_quoted">quoted</code></td>
<td>
<p>Is <code>expr</code> a quoted expression (with <code>quote()</code>)? This
is useful if you want to save an expression in a variable.</p>
</td></tr>
</table>

<hr>
<h2 id='style_chat_history'>Style Chat History</h2><span id='topic+style_chat_history'></span>

<h3>Description</h3>

<p>This function processes the chat history, filters out system messages, and
formats the remaining messages with appropriate styling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>style_chat_history(history, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="style_chat_history_+3A_history">history</code></td>
<td>
<p>A list of chat messages with elements containing 'role' and
'content'.</p>
</td></tr>
<tr><td><code id="style_chat_history_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of formatted chat messages with styling applied, excluding
system messages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chat_history_example &lt;- list(
  list(role = "user", content = "Hello, World!"),
  list(role = "system", content = "System message"),
  list(role = "assistant", content = "Hi, how can I help?")
)

## Not run: 
style_chat_history(chat_history_example)

## End(Not run)
</code></pre>

<hr>
<h2 id='style_chat_message'>Style chat message</h2><span id='topic+style_chat_message'></span>

<h3>Description</h3>

<p>Style a message based on the role of its author.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>style_chat_message(message, ide_colors = get_ide_theme_info())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="style_chat_message_+3A_message">message</code></td>
<td>
<p>A chat message.</p>
</td></tr>
<tr><td><code id="style_chat_message_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An HTML element.
</p>

<hr>
<h2 id='text_area_input_wrapper'>Custom textAreaInput</h2><span id='topic+text_area_input_wrapper'></span>

<h3>Description</h3>

<p>Modified version of <code>textAreaInput()</code> that removes the label container.
It's used in <code>mod_prompt_ui()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_area_input_wrapper(
  inputId,
  label,
  value = "",
  width = NULL,
  height = NULL,
  cols = NULL,
  rows = NULL,
  placeholder = NULL,
  resize = NULL,
  textarea_class = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_area_input_wrapper_+3A_inputid">inputId</code></td>
<td>
<p>The <code>input</code> slot that will be used to access the value.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_label">label</code></td>
<td>
<p>Display label for the control, or <code>NULL</code> for no label.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_value">value</code></td>
<td>
<p>Initial value.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_width">width</code></td>
<td>
<p>The width of the input, e.g. <code>'400px'</code>, or <code>'100%'</code>;
see <code><a href="shiny.html#topic+validateCssUnit">validateCssUnit()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_height">height</code></td>
<td>
<p>The height of the input, e.g. <code>'400px'</code>, or <code>'100%'</code>; see
<code><a href="shiny.html#topic+validateCssUnit">validateCssUnit()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_cols">cols</code></td>
<td>
<p>Value of the visible character columns of the input, e.g. <code>80</code>.
This argument will only take effect if there is not a CSS <code>width</code> rule
defined for this element; such a rule could come from the <code>width</code> argument
of this function or from a containing page layout such as
<code><a href="shiny.html#topic+fluidPage">fluidPage()</a></code>.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_rows">rows</code></td>
<td>
<p>The value of the visible character rows of the input, e.g. <code>6</code>.
If the <code>height</code> argument is specified, <code>height</code> will take precedence in the
browser's rendering.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_placeholder">placeholder</code></td>
<td>
<p>A character string giving the user a hint as to what can
be entered into the control. Internet Explorer 8 and 9 do not support this
option.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_resize">resize</code></td>
<td>
<p>Which directions the textarea box can be resized. Can be one of
<code>"both"</code>, <code>"none"</code>, <code>"vertical"</code>, and <code>"horizontal"</code>. The default, <code>NULL</code>,
will use the client browser's default setting for resizing textareas.</p>
</td></tr>
<tr><td><code id="text_area_input_wrapper_+3A_textarea_class">textarea_class</code></td>
<td>
<p>Class to be applied to the textarea element</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified textAreaInput
</p>

<hr>
<h2 id='welcomeMessage'>Welcome message</h2><span id='topic+welcomeMessage'></span>

<h3>Description</h3>

<p>HTML widget for showing a welcome message in the chat app.
This has been created to be able to bind the message to a shiny event to trigger a new render.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>welcomeMessage(
  ide_colors = get_ide_theme_info(),
  translator = create_translator(),
  width = NULL,
  height = NULL,
  elementId = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="welcomeMessage_+3A_ide_colors">ide_colors</code></td>
<td>
<p>List containing the colors of the IDE theme.</p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_translator">translator</code></td>
<td>
<p>A Translator from <code>shiny.i18n::Translator</code></p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_width">width</code>, <code id="welcomeMessage_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="welcomeMessage_+3A_elementid">elementId</code></td>
<td>
<p>The element's id</p>
</td></tr>
</table>

<hr>
<h2 id='welcomeMessage-shiny'>Shiny bindings for welcomeMessage</h2><span id='topic+welcomeMessage-shiny'></span><span id='topic+welcomeMessageOutput'></span><span id='topic+renderWelcomeMessage'></span>

<h3>Description</h3>

<p>Output and render functions for using welcomeMessage within Shiny
applications and interactive Rmd documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>welcomeMessageOutput(outputId, width = "100%", height = NULL)

renderWelcomeMessage(expr, env = parent.frame(), quoted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="welcomeMessage-shiny_+3A_outputid">outputId</code></td>
<td>
<p>output variable to read from</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_width">width</code>, <code id="welcomeMessage-shiny_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit (like <code>'100%'</code>,
<code>'400px'</code>, <code>'auto'</code>) or a number, which will be coerced to a
string and have <code>'px'</code> appended.</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_expr">expr</code></td>
<td>
<p>An expression that generates a welcomeMessage</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_env">env</code></td>
<td>
<p>The environment in which to evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="welcomeMessage-shiny_+3A_quoted">quoted</code></td>
<td>
<p>Is <code>expr</code> a quoted expression (with <code>quote()</code>)? This
is useful if you want to save an expression in a variable.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
