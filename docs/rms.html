<!DOCTYPE html><html><head><title>Help for package rms</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rms}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anova.rms'><p>Analysis of Variance (Wald, LR, and F Statistics)</p></a></li>
<li><a href='#bj'>
<p>Buckley-James Multiple Regression Model</p></a></li>
<li><a href='#bootBCa'><p>BCa Bootstrap on Existing Bootstrap Replicates</p></a></li>
<li><a href='#bootcov'><p>Bootstrap Covariance and Distribution for Regression Coefficients</p></a></li>
<li><a href='#bplot'>
<p>3-D Plots Showing Effects of Two Continuous Predictors in a Regression</p>
Model Fit</a></li>
<li><a href='#calibrate'>
<p>Resampling Model Calibration</p></a></li>
<li><a href='#contrast.rms'><p>General Contrasts of Regression Coefficients</p></a></li>
<li><a href='#cph'><p>Cox Proportional Hazards Model and Extensions</p></a></li>
<li><a href='#cr.setup'><p>Continuation Ratio Ordinal Logistic Setup</p></a></li>
<li><a href='#datadist'>
<p>Distribution Summaries for Predictor Variables</p></a></li>
<li><a href='#ExProb'><p>Function Generator For Exceedance Probabilities</p></a></li>
<li><a href='#fastbw'><p>Fast Backward Variable Selection</p></a></li>
<li><a href='#Function'><p>Compose an S Function to Compute X beta from a Fit</p></a></li>
<li><a href='#gendata'><p>Generate Data Frame with Predictor Combinations</p></a></li>
<li><a href='#ggplot.Predict'><p>Plot Effects of Variables Estimated by a Regression Model Fit</p>
Using ggplot2</a></li>
<li><a href='#gIndex'><p>Calculate Total and Partial g-indexes for an rms Fit</p></a></li>
<li><a href='#Glm'><p>rms Version of glm</p></a></li>
<li><a href='#Gls'><p>Fit Linear Model Using Generalized Least Squares</p></a></li>
<li><a href='#groupkm'><p>Kaplan-Meier Estimates vs. a Continuous Variable</p></a></li>
<li><a href='#hazard.ratio.plot'><p>Hazard Ratio Plot</p></a></li>
<li><a href='#ie.setup'><p>Intervening Event Setup</p></a></li>
<li><a href='#impactPO'><p>Impact of Proportional Odds Assumpton</p></a></li>
<li><a href='#importedexported'><p>Exported Functions That Were Imported From Other Packages</p></a></li>
<li><a href='#latex.cph'><p>LaTeX Representation of a Fitted Cox Model</p></a></li>
<li><a href='#latexrms'><p>LaTeX Representation of a Fitted Model</p></a></li>
<li><a href='#lrm'><p>Logistic Regression Model</p></a></li>
<li><a href='#lrm.fit'><p>Logistic Model Fitter</p></a></li>
<li><a href='#lrm.fit.bare'><p>lrm.fit.bare</p></a></li>
<li><a href='#LRupdate'><p>LRupdate</p></a></li>
<li><a href='#matinv'>
<p>Total and Partial Matrix Inversion using Gauss-Jordan Sweep Operator</p></a></li>
<li><a href='#nomogram'><p>Draw a Nomogram Representing a Regression Fit</p></a></li>
<li><a href='#npsurv'><p>Nonparametric Survival Estimates for Censored Data</p></a></li>
<li><a href='#ols'><p>Linear Model Estimation Using Ordinary Least Squares</p></a></li>
<li><a href='#orm'><p>Ordinal Regression Model</p></a></li>
<li><a href='#orm.fit'><p>Ordinal Regression Model Fitter</p></a></li>
<li><a href='#pentrace'>
<p>Trace AIC and BIC vs. Penalty</p></a></li>
<li><a href='#plot.contrast.rms'><p>plot.contrast.rms</p></a></li>
<li><a href='#plot.Predict'><p>Plot Effects of Variables Estimated by a Regression Model Fit</p></a></li>
<li><a href='#plot.rexVar'><p>plot.rexVar</p></a></li>
<li><a href='#plot.xmean.ordinaly'>
<p>Plot Mean X vs. Ordinal Y</p></a></li>
<li><a href='#plotp.Predict'><p>Plot Effects of Variables Estimated by a Regression Model Fit</p>
Using plotly</a></li>
<li><a href='#poma'><p>Examine proportional odds and parallelism assumptions of 'orm' and 'lrm' model fits.</p></a></li>
<li><a href='#pphsm'><p>Parametric Proportional Hazards form of AFT Models</p></a></li>
<li><a href='#predab.resample'><p>Predictive Ability using Resampling</p></a></li>
<li><a href='#Predict'><p>Compute Predicted Values and Confidence Limits</p></a></li>
<li><a href='#predict.lrm'>
<p>Predicted Values for Binary and Ordinal Logistic Models</p></a></li>
<li><a href='#predictrms'><p>Predicted Values from Model Fit</p></a></li>
<li><a href='#print.cph'><p>Print cph Results</p></a></li>
<li><a href='#print.Glm'><p>print.glm</p></a></li>
<li><a href='#print.impactPO'><p>Print Result from impactPO</p></a></li>
<li><a href='#print.ols'><p>Print ols</p></a></li>
<li><a href='#print.rexVar'><p>print.rexVar</p></a></li>
<li><a href='#prmiInfo'><p>prmiInfo</p></a></li>
<li><a href='#processMI'><p>processMI</p></a></li>
<li><a href='#processMI.fit.mult.impute'><p>processMI.fit.mult.impute</p></a></li>
<li><a href='#psm'><p>Parametric Survival Model</p></a></li>
<li><a href='#residuals.cph'><p>Residuals for a cph Fit</p></a></li>
<li><a href='#residuals.Glm'><p>residuals.Glm</p></a></li>
<li><a href='#residuals.lrm'><p>Residuals from an <code>lrm</code> or <code>orm</code> Fit</p></a></li>
<li><a href='#residuals.ols'><p>Residuals for ols</p></a></li>
<li><a href='#rexVar'><p>rexVar</p></a></li>
<li><a href='#rms'><p>rms Methods and Generic Functions</p></a></li>
<li><a href='#rms-internal'><p>Internal rms functions</p></a></li>
<li><a href='#rms.trans'><p>rms Special Transformation Functions</p></a></li>
<li><a href='#rmsMisc'><p>Miscellaneous Design Attributes and Utility Functions</p></a></li>
<li><a href='#rmsOverview'><p>Overview of rms Package</p></a></li>
<li><a href='#robcov'><p>Robust Covariance Matrix Estimates</p></a></li>
<li><a href='#Rq'><p>rms Package Interface to quantreg Package</p></a></li>
<li><a href='#sensuc'><p>Sensitivity to Unmeasured Covariables</p></a></li>
<li><a href='#setPb'><p>Progress Bar for Simulations</p></a></li>
<li><a href='#specs.rms'><p>rms Specifications for Models</p></a></li>
<li><a href='#summary.rms'><p>Summary of Effects in Model</p></a></li>
<li><a href='#survest.cph'>
<p>Cox Survival Estimates</p></a></li>
<li><a href='#survest.psm'><p>Parametric Survival Estimates</p></a></li>
<li><a href='#survfit.cph'>
<p>Cox Predicted Survival</p></a></li>
<li><a href='#survplot'><p>Plot Survival Curves and Hazard Functions</p></a></li>
<li><a href='#val.prob'>
<p>Validate Predicted Probabilities</p></a></li>
<li><a href='#val.surv'>
<p>Validate Predicted Probabilities Against Observed Survival Times</p></a></li>
<li><a href='#validate'><p>Resampling Validation of a Fitted Model's Indexes of Fit</p></a></li>
<li><a href='#validate.cph'><p>Validation of a Fitted Cox or Parametric Survival Model's Indexes</p>
of Fit</a></li>
<li><a href='#validate.lrm'><p>Resampling Validation of a Logistic or Ordinal Regression Model</p></a></li>
<li><a href='#validate.ols'><p>Validation of an Ordinary Linear Model</p></a></li>
<li><a href='#validate.rpart'>
<p>Dxy and Mean Squared Error by Cross-validating a Tree Sequence</p></a></li>
<li><a href='#validate.Rq'><p>Validation of a Quantile Regression Model</p></a></li>
<li><a href='#vif'><p>Variance Inflation Factors</p></a></li>
<li><a href='#which.influence'>
<p>Which Observations are Influential</p></a></li>
<li><a href='#Xcontrast'><p>Xcontrast</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>6.8-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-11</td>
</tr>
<tr>
<td>Title:</td>
<td>Regression Modeling Strategies</td>
</tr>
<tr>
<td>Author:</td>
<td>Frank E Harrell Jr &lt;fh@fharrell.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Frank E Harrell Jr &lt;fh@fharrell.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0), Hmisc (&ge; 5.1-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, survival, quantreg, ggplot2, SparseM, rpart, nlme (&ge;
3.1-123), polspline, multcomp, htmlTable (&ge; 1.11.0),
htmltools, MASS, cluster, digest, colorspace, knitr, grDevices</td>
</tr>
<tr>
<td>Suggests:</td>
<td>boot, tcltk, plotly (&ge; 4.5.6), mice, rmsb, nnet, VGAM,
lattice, kableExtra</td>
</tr>
<tr>
<td>Description:</td>
<td>Regression modeling, testing, estimation, validation,
	graphics, prediction, and typesetting by storing enhanced model design
	attributes in the fit.  'rms' is a collection of functions that
	assist with and streamline modeling.  It also contains functions for
	binary and ordinal logistic regression models, ordinal models for
  continuous Y with a variety of distribution families, and the Buckley-James
	multiple regression model for right-censored responses, and implements
	penalized maximum likelihood estimation for logistic and ordinary
	linear models.  'rms' works with almost any regression model, but it
	was especially written to work with binary or ordinal regression
	models, Cox regression, accelerated failure time models,
	ordinary linear models,	the Buckley-James model, generalized least
	squares for serially or spatially correlated observations, generalized
	linear models, and quantile regression.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://hbiostat.org/R/rms/">https://hbiostat.org/R/rms/</a>, <a href="https://github.com/harrelfe/rms">https://github.com/harrelfe/rms</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-11 15:15:31 UTC; harrelfe</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-11 16:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='anova.rms'>Analysis of Variance (Wald, LR, and F Statistics)</h2><span id='topic+anova.rms'></span><span id='topic+print.anova.rms'></span><span id='topic+plot.anova.rms'></span><span id='topic+latex.anova.rms'></span><span id='topic+html.anova.rms'></span>

<h3>Description</h3>

<p>The <code>anova</code> function automatically tests most meaningful hypotheses
in a design. For example, suppose that age and cholesterol are
predictors, and that a general interaction is modeled using a restricted
spline surface. <code>anova</code> prints Wald statistics (<code class="reqn">F</code> statistics
for an <code>ols</code> fit) for testing linearity of age, linearity of
cholesterol, age effect (age + age by cholesterol interaction),
cholesterol effect (cholesterol + age by cholesterol interaction),
linearity of the age by cholesterol interaction (i.e., adequacy of the
simple age * cholesterol 1 d.f. product), linearity of the interaction
in age alone, and linearity of the interaction in cholesterol
alone. Joint tests of all interaction terms in the model and all
nonlinear terms in the model are also performed.  For any multiple
d.f. effects for continuous variables that were not modeled through
<code>rcs</code>, <code>pol</code>, <code>lsp</code>, etc., tests of linearity will be
omitted.  This applies to matrix predictors produced by e.g.
<code>poly</code> or <code>ns</code>.
</p>
<p>For <code>lrm, orm, cph, psm</code> and <code>Glm</code> fits, the better likelihood
ratio chi-square tests may be obtained by specifying <code>test='LR'</code>.
Fits must use <code>x=TRUE, y=TRUE</code> to run LR tests.  The tests are run
fairly efficiently by subsetting the design matrix rather than
recreating it.
</p>
<p><code>print.anova.rms</code> is the printing
method.  <code>plot.anova.rms</code> draws dot charts depicting the importance
of variables in the model, as measured by Wald or LR <code class="reqn">\chi^2</code>,
<code class="reqn">\chi^2</code> minus d.f., AIC, <code class="reqn">P</code>-values, partial
<code class="reqn">R^2</code>, <code class="reqn">R^2</code> for the whole model after deleting the effects in
question, or proportion of overall model <code class="reqn">R^2</code> that is due to each
predictor.  <code>latex.anova.rms</code> is the <code>latex</code> method.  It
substitutes Greek/math symbols in column headings, uses boldface for
<code>TOTAL</code> lines, and constructs a caption.  Then it passes the result
to <code>latex.default</code> for conversion to LaTeX.
</p>
<p>When the anova table was converted to account for missing data
imputation by <code>processMI</code>, a separate function <code>prmiInfo</code> can
be used to print information related to imputation adjustments.
</p>
<p>For Bayesian models such as <code>blrm</code>, <code>anova</code> computes relative
explained variation indexes (REV) based on approximate Wald statistics.
This uses the variance-covariance matrix of all of the posterior draws,
and the individual draws of betas, plus an overall summary from the
posterior mode/mean/median beta.  Wald chi-squares assuming multivariate
normality of betas are computed just as with frequentist models, and for
each draw (or for the summary) the ratio of the partial Wald chi-square
to the total Wald statistic for the model is computed as REV.
</p>
<p>The <code>print</code> method calls <code>latex</code> or <code>html</code> methods
depending on <code>options(prType=)</code>.  For
<code>latex</code> a <code>table</code> environment is not used and an ordinary
<code>tabular</code> is produced.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>
<p><code>html.anova.rms</code> just calls <code>latex.anova.rms</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rms'
anova(object, ..., main.effect=FALSE, tol=1e-9,
      test=c('F','Chisq','LR'), india=TRUE, indnl=TRUE, ss=TRUE,
      vnames=c('names','labels'),
      posterior.summary=c('mean', 'median', 'mode'), ns=500, cint=0.95)

## S3 method for class 'anova.rms'
print(x,
      which=c('none','subscripts','names','dots'),
      table.env=FALSE, ...)

## S3 method for class 'anova.rms'
plot(x,
     what=c("chisqminusdf","chisq","aic","P","partial R2","remaining R2",
            "proportion R2", "proportion chisq"),
     xlab=NULL, pch=16,
     rm.totals=TRUE, rm.ia=FALSE, rm.other=NULL, newnames,
     sort=c("descending","ascending","none"), margin=c('chisq','P'),
     pl=TRUE, trans=NULL, ntrans=40, height=NULL, width=NULL, ...)

## S3 method for class 'anova.rms'
latex(object, title, dec.chisq=2,
      dec.F=2, dec.ss=NA, dec.ms=NA, dec.P=4, dec.REV=3,
      table.env=TRUE,
      caption=NULL, fontsize=1, params, ...)

## S3 method for class 'anova.rms'
html(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.rms_+3A_object">object</code></td>
<td>

<p>a <code>rms</code> fit object.  <code>object</code> must
allow <code>vcov</code> to return the variance-covariance matrix.  For
<code>latex</code> is the result of <code>anova</code>.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_...">...</code></td>
<td>

<p>If omitted, all variables are tested, yielding tests for individual factors
and for pooled effects. Specify a subset of the variables to obtain tests
for only those factors, with a pooled tests for the combined effects
of all factors listed. Names may be abbreviated.  For example, specify
<code>anova(fit,age,cholesterol)</code> to get a Wald statistic for testing the joint
importance of age, cholesterol, and any factor interacting with them.
Add <code>test='LR'</code> to get a likelihood ratio chi-square test instead.
</p>
<p>Can be optional graphical parameters to send to
<code>dotchart2</code>, or other parameters to send to <code>latex.default</code>.
Ignored for <code>print</code>.
</p>
<p>For <code>html.anova.rms</code> the arguments are passed to <code>latex.anova.rms</code>.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_main.effect">main.effect</code></td>
<td>

<p>Set to <code>TRUE</code> to print the (usually meaningless) main effect tests even when
the factor is involved in an interaction. The default is <code>FALSE</code>, to print only
the effect of the main effect combined with all interactions involving that
factor.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_tol">tol</code></td>
<td>

<p>singularity criterion for use in matrix inversion
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_test">test</code></td>
<td>

<p>For an <code>ols</code> fit, set <code>test="Chisq"</code> to use Wald <code class="reqn">\chi^2</code>
tests rather than F-tests.  For <code>lrm, orm, cph, psm</code> and <code>Glm</code>
fits set <code>test='LR'</code> to get likelihood ratio <code class="reqn">\chi^2</code> tests.
This requires specifying <code>x=TRUE, y=TRUE</code> when fitting the model.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_india">india</code></td>
<td>
<p>set to <code>FALSE</code> to exclude individual tests of
interaction from the table</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_indnl">indnl</code></td>
<td>
<p>set to <code>FALSE</code> to exclude individual tests of
nonlinearity from the table</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_ss">ss</code></td>
<td>

<p>For an <code>ols</code> fit, set <code>ss=FALSE</code> to suppress printing partial
sums of squares, mean squares, and the Error SS and MS.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_vnames">vnames</code></td>
<td>
<p>set to <code>'labels'</code> to use variable labels rather than
variable names in the output</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>specifies whether the posterior mode/mean/median
beta are to be used as a measure of central tendence of the posterior
distribution, for use in relative explained variation from Bayesian
models</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_ns">ns</code></td>
<td>
<p>number of random samples from the posterior draws to use for
REV highest posterior density intervals</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_cint">cint</code></td>
<td>
<p>HPD interval probability</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_x">x</code></td>
<td>
<p>for <code>print,plot,text</code> is the result of <code>anova</code>.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_which">which</code></td>
<td>

<p>If <code>which</code> is not <code>"none"</code> (the default), <code>print.anova.rms</code> will
add to the rightmost column of the output the list of parameters being
tested by the hypothesis being tested in the current row.  Specifying
<code>which="subscripts"</code> causes the subscripts of the regression
coefficients being tested to be printed (with a subscript of one for
the first non-intercept term).  <code>which="names"</code> prints the names of
the terms being tested, and <code>which="dots"</code> prints dots for terms being
tested and blanks for those just being adjusted for.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_what">what</code></td>
<td>

<p>what type of statistic to plot.  The default is the <code class="reqn">\chi^2</code>
statistic for each factor (adding in the effect of higher-ordered
factors containing that factor) minus its degrees of freedom.  The
R2 choices for <code>what</code> only apply to <code>ols</code> models.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label, default is constructed according to <code>what</code>.
<code>plotmath</code> symbols are used for <span class="rlang"><b>R</b></span>, by default.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_pch">pch</code></td>
<td>

<p>character for plotting dots in dot charts.  Default is 16 (solid dot).
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_rm.totals">rm.totals</code></td>
<td>

<p>set to <code>FALSE</code> to keep total <code class="reqn">\chi^2</code>s (overall, nonlinear, interaction totals)
in the chart.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_rm.ia">rm.ia</code></td>
<td>

<p>set to <code>TRUE</code> to omit any effect that has <code>"*"</code> in its name
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_rm.other">rm.other</code></td>
<td>

<p>a list of other predictor names to omit from the chart
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_newnames">newnames</code></td>
<td>

<p>a list of substitute predictor names to use, after omitting any.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_sort">sort</code></td>
<td>
<p>default is to sort bars in descending order of the summary statistic. 
Available options: 'ascending', 'descending', 'none'. 
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_margin">margin</code></td>
<td>
<p>set to a vector of character strings to write text for
selected statistics in the right margin of the dot chart.  The
character strings can be any combination of <code>"chisq"</code>,
<code>"d.f."</code>, <code>"P"</code>, <code>"partial R2"</code>,
<code>"proportion R2"</code>, and <code>"proportion chisq"</code>.
Default is to not draw any statistics in the margin.  When
<code>plotly</code> is in effect, margin values are instead displayed as
hover text.</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_pl">pl</code></td>
<td>

<p>set to <code>FALSE</code> to suppress plotting.  This is useful when you only wish to
analyze the vector of statistics returned.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_trans">trans</code></td>
<td>

<p>set to a function to apply that transformation to the statistics
being plotted, and to truncate negative values at zero.  A good choice
is <code>trans=sqrt</code>.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_ntrans">ntrans</code></td>
<td>
<p><code>n</code> argument to <code><a href="base.html#topic+pretty">pretty</a></code>, specifying the
number of values for which to place tick marks.  This should be larger
than usual because of nonlinear scaling, to provide a sufficient
number of tick marks on the left (stretched) part of the chi-square
scale.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_height">height</code>, <code id="anova.rms_+3A_width">width</code></td>
<td>
<p>height and width of <code>plotly</code> plots drawn using
<code>dotchartp</code>, in pixels.  Ignored for ordinary plots.  Defaults to
minimum of 400 and 100 + 25 times the number of test statistics displayed.</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_title">title</code></td>
<td>

<p>title to pass to <code>latex</code>, default is name of fit object passed to
<code>anova</code> prefixed with <code>"anova."</code>.  For Windows, the default is
<code>"ano"</code> followed by the first 5 letters of the name of the fit
object.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.chisq">dec.chisq</code></td>
<td>

<p>number of places to the right of the decimal place for typesetting
<code class="reqn">\chi^2</code> values (default is <code>2</code>).  Use zero for integer, <code>NA</code> for
floating point.
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.f">dec.F</code></td>
<td>

<p>digits to the right for <code class="reqn">F</code> statistics (default is <code>2</code>)
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.ss">dec.ss</code></td>
<td>

<p>digits to the right for sums of squares (default is <code>NA</code>, indicating
floating point)
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.ms">dec.ms</code></td>
<td>

<p>digits to the right for mean squares (default is <code>NA</code>)
</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.p">dec.P</code></td>
<td>
<p>digits to the right for <code class="reqn">P</code>-values</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_dec.rev">dec.REV</code></td>
<td>
<p>digits to the right for REV</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_table.env">table.env</code></td>
<td>
<p>see <code><a href="Hmisc.html#topic+latex">latex</a></code></p>
</td></tr>
<tr><td><code id="anova.rms_+3A_caption">caption</code></td>
<td>
<p>caption for table if <code>table.env</code> is <code>TRUE</code>.
Default is constructed from the response variable.</p>
</td></tr>
<tr><td><code id="anova.rms_+3A_fontsize">fontsize</code></td>
<td>
<p>font size for html output; default is 1 for <code>1em</code></p>
</td></tr>
<tr><td><code id="anova.rms_+3A_params">params</code></td>
<td>
<p>used internally when called through print.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the statistics being plotted with <code>plot.anova.rms</code> are few in
number and one of them is negative or zero, <code>plot.anova.rms</code>
will quit because of an error in <code>dotchart2</code>.
</p>
<p>The <code>latex</code> method requires LaTeX packages <code>relsize</code> and
<code>needspace</code>.
</p>


<h3>Value</h3>

<p><code>anova.rms</code> returns a matrix of class <code>anova.rms</code> containing factors
as rows and <code class="reqn">\chi^2</code>, d.f., and <code class="reqn">P</code>-values as
columns (or d.f., partial <code class="reqn">SS, MS, F, P</code>).  An attribute
<code>vinfo</code> provides list of variables involved in each row and the
type of test done.
<code>plot.anova.rms</code> invisibly returns the vector of quantities
plotted.  This vector has a names attribute describing the terms for
which the statistics in the vector are calculated.
</p>


<h3>Side Effects</h3>

<p><code>print</code> prints, <code>latex</code> creates a
file with a name of the form <code>"title.tex"</code> (see the <code>title</code> argument above).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prmiInfo">prmiInfo</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+lrtest">lrtest</a></code>,
<code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+plot.Predict">plot.Predict</a></code>,
<code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>, <code><a href="Hmisc.html#topic+solvet">solvet</a></code>,
<code><a href="graphics.html#topic+locator">locator</a></code>,
<code><a href="Hmisc.html#topic+dotchart2">dotchart2</a></code>, <code><a href="Hmisc.html#topic+latex">latex</a></code>,
<code><a href="Hmisc.html#topic+xYplot">xYplot</a></code>, <code><a href="stats.html#topic+anova.lm">anova.lm</a></code>,
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+pantext">pantext</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
treat &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age &lt;- rnorm(n, 50, 10)
cholesterol &lt;- rnorm(n, 200, 25)
weight &lt;- rnorm(n, 150, 20)
sex &lt;- factor(sample(c('female','male'), n,TRUE))
label(age) &lt;- 'Age'      # label is in Hmisc
label(num.diseases) &lt;- 'Number of Comorbid Diseases'
label(cholesterol) &lt;- 'Total Cholesterol'
label(weight) &lt;- 'Weight, lbs.'
label(sex) &lt;- 'Sex'
units(cholesterol) &lt;- 'mg/dl'   # uses units.default in Hmisc


# Specify population model for log odds that Y=1
L &lt;- .1*(num.diseases-2) + .045*(age-50) +
     (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
     3.5*(treat=='b')+2*(treat=='c'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


fit &lt;- lrm(y ~ treat + scored(num.diseases) + rcs(age) +
               log(cholesterol+10) + treat:log(cholesterol+10),
           x=TRUE, y=TRUE)   # x, y needed for test='LR'
a &lt;- anova(fit)                       # Test all factors
b &lt;- anova(fit, treat, cholesterol)   # Test these 2 by themselves
                                      # to get their pooled effects
a
b
a2 &lt;- anova(fit, test='LR')
b2 &lt;- anova(fit, treat, cholesterol, test='LR')
a2
b2

# Add a new line to the plot with combined effects
s &lt;- rbind(a2, 'treat+cholesterol'=b2['TOTAL',])

class(s) &lt;- 'anova.rms'
plot(s, margin=c('chisq', 'proportion chisq'))

g &lt;- lrm(y ~ treat*rcs(age))
dd &lt;- datadist(treat, num.diseases, age, cholesterol)
options(datadist='dd')
p &lt;- Predict(g, age, treat="b")
s &lt;- anova(g)
tx &lt;- paste(capture.output(s), collapse='\n')
ggplot(p) + annotate('text', x=27, y=3.2, family='mono', label=tx,
                      hjust=0, vjust=1, size=1.5)

plot(s, margin=c('chisq', 'proportion chisq'))
# new plot - dot chart of chisq-d.f. with 2 other stats in right margin
# latex(s)                       # nice printout - creates anova.g.tex
options(datadist=NULL)


# Simulate data with from a given model, and display exactly which
# hypotheses are being tested


set.seed(123)
age &lt;- rnorm(500, 50, 15)
treat &lt;- factor(sample(c('a','b','c'), 500, TRUE))
bp  &lt;- rnorm(500, 120, 10)
y   &lt;- ifelse(treat=='a', (age-50)*.05, abs(age-50)*.08) + 3*(treat=='c') +
       pmax(bp, 100)*.09 + rnorm(500)
f   &lt;- ols(y ~ treat*lsp(age,50) + rcs(bp,4))
print(names(coef(f)), quote=FALSE)
specs(f)
anova(f)
an &lt;- anova(f)
options(digits=3)
print(an, 'subscripts')
print(an, 'dots')


an &lt;- anova(f, test='Chisq', ss=FALSE)
# plot(0:1)                        # make some plot
# tab &lt;- pantext(an, 1.2, .6, lattice=FALSE, fontfamily='Helvetica')
# create function to write table; usually omit fontfamily
# tab()                            # execute it; could do tab(cex=.65)
plot(an)                         # new plot - dot chart of chisq-d.f.
# Specify plot(an, trans=sqrt) to use a square root scale for this plot
# latex(an)                      # nice printout - creates anova.f.tex


## Example to save partial R^2 for all predictors, along with overall
## R^2, from two separate fits, and to combine them with ggplot2

require(ggplot2)
set.seed(1)
n &lt;- 100
x1 &lt;- runif(n)
x2 &lt;- runif(n)
y  &lt;- (x1-.5)^2 + x2 + runif(n)
group &lt;- c(rep('a', n/2), rep('b', n/2))
A &lt;- NULL
for(g in c('a','b')) {
    f &lt;- ols(y ~ pol(x1,2) + pol(x2,2) + pol(x1,2) %ia% pol(x2,2),
             subset=group==g)
    a &lt;- plot(anova(f),
              what='partial R2', pl=FALSE, rm.totals=FALSE, sort='none')
    a &lt;- a[-grep('NONLINEAR', names(a))]
    d &lt;- data.frame(group=g, Variable=factor(names(a), names(a)),
                    partialR2=unname(a))
    A &lt;- rbind(A, d)
  }
ggplot(A, aes(x=partialR2, y=Variable)) + geom_point() +
       facet_wrap(~ group) + xlab(ex &lt;- expression(partial~R^2)) +
       scale_y_discrete(limits=rev)
ggplot(A, aes(x=partialR2, y=Variable, color=group)) + geom_point() +
       xlab(ex &lt;- expression(partial~R^2)) +
       scale_y_discrete(limits=rev)

# Suppose that a researcher wants to make a big deal about a variable
# because it has the highest adjusted chi-square.  We use the
# bootstrap to derive 0.95 confidence intervals for the ranks of all
# the effects in the model.  We use the plot method for anova, with
# pl=FALSE to suppress actual plotting of chi-square - d.f. for each
# bootstrap repetition.
# It is important to tell plot.anova.rms not to sort the results, or
# every bootstrap replication would have ranks of 1,2,3,... for the stats.

n &lt;- 300
set.seed(1)
d &lt;- data.frame(x1=runif(n), x2=runif(n),  x3=runif(n),
   x4=runif(n), x5=runif(n), x6=runif(n),  x7=runif(n),
   x8=runif(n), x9=runif(n), x10=runif(n), x11=runif(n),
   x12=runif(n))
d$y &lt;- with(d, 1*x1 + 2*x2 + 3*x3 +  4*x4  + 5*x5 + 6*x6 +
               7*x7 + 8*x8 + 9*x9 + 10*x10 + 11*x11 +
              12*x12 + 9*rnorm(n))

f &lt;- ols(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12, data=d)
B &lt;- 20   # actually use B=1000
ranks &lt;- matrix(NA, nrow=B, ncol=12)
rankvars &lt;- function(fit)
  rank(plot(anova(fit), sort='none', pl=FALSE))
Rank &lt;- rankvars(f)
for(i in 1:B) {
  j &lt;- sample(1:n, n, TRUE)
  bootfit &lt;- update(f, data=d, subset=j)
  ranks[i,] &lt;- rankvars(bootfit)
  }
lim &lt;- t(apply(ranks, 2, quantile, probs=c(.025,.975)))
predictor &lt;- factor(names(Rank), names(Rank))
w &lt;- data.frame(predictor, Rank, lower=lim[,1], upper=lim[,2])
ggplot(w, aes(x=predictor, y=Rank)) + geom_point() + coord_flip() +
  scale_y_continuous(breaks=1:12) +
  geom_errorbar(aes(ymin=lim[,1], ymax=lim[,2]), width=0)
</code></pre>

<hr>
<h2 id='bj'>
Buckley-James Multiple Regression Model
</h2><span id='topic+bj'></span><span id='topic+bj.fit'></span><span id='topic+residuals.bj'></span><span id='topic+print.bj'></span><span id='topic+validate.bj'></span><span id='topic+bjplot'></span>

<h3>Description</h3>

<p><code>bj</code> fits the Buckley-James distribution-free least squares multiple
regression model to a possibly right-censored response variable.  
This model reduces to ordinary least squares if
there is no censoring.  By default, model fitting is done after
taking logs of the response variable.
<code>bj</code> uses the <code>rms</code> class
for automatic <code>anova</code>, <code>fastbw</code>, <code>validate</code>, <code>Function</code>, <code>nomogram</code>,
<code>summary</code>, <code>plot</code>, <code>bootcov</code>, and other functions.  The <code>bootcov</code>
function may be worth using with <code>bj</code> fits, as the properties of the
Buckley-James covariance matrix estimator are not fully known for
strange censoring patterns.
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>
<p>The <code>residuals.bj</code> function exists mainly to compute 
residuals and to censor them (i.e., return them as
<code>Surv</code> objects) just as the original
failure time variable was censored.  These residuals are useful for
checking to see if the model also satisfies certain distributional assumptions.
To get these residuals, the fit must have specified <code>y=TRUE</code>.
</p>
<p>The <code>bjplot</code> function is a special plotting function for objects
created by <code>bj</code> with <code>x=TRUE, y=TRUE</code> in effect.  It produces three
scatterplots for every covariate in the model: the first plots the
original situation, where censored data are distingushed from
non-censored data by a different plotting symbol. In the second plot,
called a renovated plot, vertical lines show how censored data were
changed by the procedure, and the third is equal to the second, but
without vertical lines.  Imputed data are again distinguished from the
non-censored by a different symbol.
</p>
<p>The <code>validate</code> method for <code>bj</code> validates the Somers' <code>Dxy</code> rank
correlation between predicted and observed responses, accounting for censoring.
</p>
<p>The primary fitting function for <code>bj</code> is <code>bj.fit</code>, which does not
allow missing data and expects a full design matrix as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bj(formula, data=environment(formula), subset, na.action=na.delete,
   link="log", control, method='fit', x=FALSE, y=FALSE, 
   time.inc)

## S3 method for class 'bj'
print(x, digits=4, long=FALSE, coefs=TRUE, 
title="Buckley-James Censored Data Regression", ...)

## S3 method for class 'bj'
residuals(object, type=c("censored","censored.normalized"),...)

bjplot(fit, which=1:dim(X)[[2]])

## S3 method for class 'bj'
validate(fit, method="boot", B=40,
         bw=FALSE,rule="aic",type="residual",sls=.05,aics=0,
         force=NULL, estimates=TRUE, pr=FALSE,
		 tol=1e-7, rel.tolerance=1e-3, maxiter=15, ...)

bj.fit(x, y, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bj_+3A_formula">formula</code></td>
<td>

<p>an S statistical model formula. Interactions up to third order are
supported. The left hand side must be a <code>Surv</code> object.
</p>
</td></tr>
<tr><td><code id="bj_+3A_data">data</code>, <code id="bj_+3A_subset">subset</code>, <code id="bj_+3A_na.action">na.action</code></td>
<td>
<p>the usual statistical model fitting arguments</p>
</td></tr>
<tr><td><code id="bj_+3A_fit">fit</code></td>
<td>

<p>a fit created by <code>bj</code>, required for all functions except <code>bj</code>.
</p>
</td></tr>
<tr><td><code id="bj_+3A_x">x</code></td>
<td>

<p>a design matrix with or without a first column of ones, to pass
to <code>bj.fit</code>.  All models will have an intercept.  For
<code>print.bj</code> is a result of <code>bj</code>.  For <code>bj</code>, set
<code>x=TRUE</code> to include the design matrix in the fit object. 
</p>
</td></tr>
<tr><td><code id="bj_+3A_y">y</code></td>
<td>

<p>a <code>Surv</code> object to pass to <code>bj.fit</code> as the two-column response 
variable.  Only right censoring is allowed, and there need not be any
censoring.  For <code>bj</code>, set <code>y</code> to <code>TRUE</code> to include the
two-column response matrix, with the 
event/censoring indicator in the second column.  The first column will
be transformed according to <code>link</code>, and depending on
<code>na.action</code>, rows with missing data in the predictors or the
response will be deleted.
</p>
</td></tr>
<tr><td><code id="bj_+3A_link">link</code></td>
<td>

<p>set to, for example, <code>"log"</code> (the default) to model the log of the
response, or <code>"identity"</code> to model the untransformed response.
</p>
</td></tr>
<tr><td><code id="bj_+3A_control">control</code></td>
<td>

<p>a list containing any or all of the following components: <code>iter.max</code>
(maximum number of iterations allowed, default is 20),
<code>eps</code> (convergence criterion: concergence is assumed when the ratio of
sum of squared errors from one iteration to the next is between
1-<code>eps</code> and 1+<code>eps</code>), <code>trace</code> (set to <code>TRUE</code> to monitor iterations), 
<code>tol</code> (matrix singularity criterion, default is 1e-7), and 'max.cycle' 
(in case of nonconvergence the program looks for a cycle that repeats itself, 
default is 30).  
</p>
</td></tr>
<tr><td><code id="bj_+3A_method">method</code></td>
<td>

<p>set to <code>"model.frame"</code> or <code>"model.matrix"</code> to return one of those
objects rather than the model fit.
</p>
</td></tr>
<tr><td><code id="bj_+3A_time.inc">time.inc</code></td>
<td>

<p>setting for default time spacing.
Default is 30 if time variable has <code>units="Day"</code>, 1 otherwise, unless
maximum follow-up time <code class="reqn">&lt; 1</code>. Then max time/10 is used as <code>time.inc</code>.
If <code>time.inc</code> is not given and max time/default <code>time.inc</code> is
<code class="reqn">&gt; 25</code>, <code>time.inc</code> is increased.
</p>
</td></tr>
<tr><td><code id="bj_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to print if not 4.
</p>
</td></tr>
<tr><td><code id="bj_+3A_long">long</code></td>
<td>

<p>set to <code>TRUE</code> to print the correlation matrix for parameter estimates
</p>
</td></tr>
<tr><td><code id="bj_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="bj_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="bj_+3A_object">object</code></td>
<td>
<p>the result of <code>bj</code></p>
</td></tr>
<tr><td><code id="bj_+3A_type">type</code></td>
<td>

<p>type of residual desired.  Default is censored unnormalized residuals,
defined as link(Y) - linear.predictors, where the
link function was usually the log function.  You can specify
<code>type="censored.normalized"</code> to divide the residuals by the estimate
of <code>sigma</code>.
</p>
</td></tr>
<tr><td><code id="bj_+3A_which">which</code></td>
<td>

<p>vector of integers or character strings naming elements of the design
matrix (the names of the original predictors if they entered the model
linearly) for which to have <code>bjplot</code> make plots of only the variables listed in <code>which</code> (names or numbers).
</p>
</td></tr>
<tr><td><code id="bj_+3A_b">B</code>, <code id="bj_+3A_bw">bw</code>, <code id="bj_+3A_rule">rule</code>, <code id="bj_+3A_sls">sls</code>, <code id="bj_+3A_aics">aics</code>, <code id="bj_+3A_force">force</code>, <code id="bj_+3A_estimates">estimates</code>, <code id="bj_+3A_pr">pr</code>, <code id="bj_+3A_tol">tol</code>, <code id="bj_+3A_rel.tolerance">rel.tolerance</code>, <code id="bj_+3A_maxiter">maxiter</code></td>
<td>
<p>see
<code><a href="#topic+predab.resample">predab.resample</a></code></p>
</td></tr> 
<tr><td><code id="bj_+3A_...">...</code></td>
<td>

<p>ignored for <code>print</code>; passed through to
<code>predab.resample</code> for <code>validate</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The program implements the algorithm as described in the original
article by Buckley &amp; James. Also, we have used the original Buckley &amp;
James prescription for computing variance/covariance estimator.  This
is based on non-censored observations only and does not have any
theoretical justification, but has been shown in simulation studies to
behave well. Our experience confirms this view.  Convergence is rather
slow with this method, so you may want to increase the number of
iterations.  Our experience shows that often, in particular with high
censoring, 100 iterations is not too many. Sometimes the method will
not converge, but will instead enter a loop of repeating values (this
is due to the discrete nature  
of Kaplan and Meier estimator and usually happens with small sample sizes).
The program will look for such a loop and return the average betas. It will also 
issue a warning message and give the size of the cycle (usually less than 6).
</p>


<h3>Value</h3>

<p><code>bj</code> returns a fit object with similar information to what <code>survreg</code>,
<code>psm</code>, <code>cph</code> would store as 
well as what <code>rms</code> stores and <code>units</code> and <code>time.inc</code>.
<code>residuals.bj</code> returns a <code>Surv</code> object.  One of the components of the
<code>fit</code> object produced by <code>bj</code> (and <code>bj.fit</code>) is a vector called
<code>stats</code> which contains the following names elements: 
<code>"Obs", "Events", "d.f.","error d.f.","sigma","g"</code>.  Here
<code>sigma</code> is the estimate of the residual standard deviation.
<code>g</code> is the <code class="reqn">g</code>-index.  If the link function is <code>"log"</code>,
the <code class="reqn">g</code>-index on the anti-log scale is also returned as <code>gr</code>.
</p>


<h3>Author(s)</h3>

<p>Janez Stare<br />
Department of Biomedical Informatics<br />
Ljubljana University<br />
Ljubljana, Slovenia<br />
<a href="mailto:janez.stare@mf.uni-lj.si">janez.stare@mf.uni-lj.si</a>
</p>
<p>Harald Heinzl<br />
Department of Medical Computer Sciences<br />
Vienna University<br />
Vienna, Austria<br />
<a href="mailto:harald.heinzl@akh-wien.ac.at">harald.heinzl@akh-wien.ac.at</a>
</p>
<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Buckley JJ, James IR. Linear regression with censored data. Biometrika 1979; 
66:429&ndash;36.
</p>
<p>Miller RG, Halpern J. Regression with censored data. Biometrika 1982; 69: 
521&ndash;31.
</p>
<p>James IR, Smith PJ. Consistency results for linear regression with censored 
data. Ann Statist 1984; 12: 590&ndash;600.
</p>
<p>Lai TL, Ying Z. Large sample theory of a modified Buckley-James estimator for 
regression analysis 
with censored data. Ann Statist 1991; 19: 1370&ndash;402.
</p>
<p>Hillis SL. Residual plots for the censored data linear regression model.  Stat in Med 1995; 14: 2023&ndash;2036.
</p>
<p>Jin Z, Lin DY, Ying Z. On least-squares regression with censored data.  Biometrika 2006; 93:147&ndash;161.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+psm">psm</a></code>, <code><a href="survival.html#topic+survreg">survreg</a></code>,
<code><a href="#topic+cph">cph</a></code>, <code><a href="survival.html#topic+Surv">Surv</a></code>, 
<code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>,
<code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>, <code><a href="#topic+datadist">datadist</a></code>,
<code><a href="Hmisc.html#topic+rcorr.cens">rcorr.cens</a></code>, <code><a href="Hmisc.html#topic+GiniMd">GiniMd</a></code>,
<code><a href="#topic+prModFit">prModFit</a></code>, <code><a href="#topic+dxy.cens">dxy.cens</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
suppressWarnings(RNGversion("3.5.0"))
set.seed(1)
ftime  &lt;- 10*rexp(200)
stroke &lt;- ifelse(ftime &gt; 10, 0, 1)
ftime  &lt;- pmin(ftime, 10)
units(ftime) &lt;- "Month"
age &lt;- rnorm(200, 70, 10)
hospital &lt;- factor(sample(c('a','b'),200,TRUE))
dd &lt;- datadist(age, hospital)
options(datadist="dd")

# Prior to rms 6.0 and R 4.0 the following worked with 5 knots
f &lt;- bj(Surv(ftime, stroke) ~ rcs(age,3) + hospital, x=TRUE, y=TRUE)
# add link="identity" to use a censored normal regression model instead
# of a lognormal one
anova(f)
fastbw(f)
validate(f, B=15)
plot(Predict(f, age, hospital))
# needs datadist since no explicit age,hosp.
coef(f)               # look at regression coefficients
coef(psm(Surv(ftime, stroke) ~ rcs(age,3) + hospital, dist='lognormal'))
                      # compare with coefficients from likelihood-based
                      # log-normal regression model
                      # use dist='gau' not under R 


r &lt;- resid(f, 'censored.normalized')
survplot(npsurv(r ~ 1), conf='none') 
                      # plot Kaplan-Meier estimate of 
                      # survival function of standardized residuals
survplot(npsurv(r ~ cut2(age, g=2)), conf='none')  
                      # may desire both strata to be n(0,1)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='bootBCa'>BCa Bootstrap on Existing Bootstrap Replicates</h2><span id='topic+bootBCa'></span>

<h3>Description</h3>

<p>This functions constructs an object resembling one produced by the
<code>boot</code> package's <code>boot</code> function, and runs that package's
<code>boot.ci</code> function to compute BCa and percentile confidence limits.
<code>bootBCa</code> can provide separate confidence limits for a vector of
statistics when <code>estimate</code> has length greater than 1.  In that
case, <code>estimates</code> must have the same number of columns as
<code>estimate</code> has values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootBCa(estimate, estimates, type=c('percentile','bca','basic'),
               n, seed, conf.int = 0.95)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootBCa_+3A_estimate">estimate</code></td>
<td>
<p>original whole-sample estimate</p>
</td></tr>
<tr><td><code id="bootBCa_+3A_estimates">estimates</code></td>
<td>
<p>vector of bootstrap estimates</p>
</td></tr>
<tr><td><code id="bootBCa_+3A_type">type</code></td>
<td>
<p>type of confidence interval, defaulting to nonparametric
percentile</p>
</td></tr>
<tr><td><code id="bootBCa_+3A_n">n</code></td>
<td>
<p>original number of observations</p>
</td></tr>
<tr><td><code id="bootBCa_+3A_seed">seed</code></td>
<td>
<p><code>.Random.seem</code> in effect before bootstrap estimates
were run</p>
</td></tr>
<tr><td><code id="bootBCa_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 2-vector if <code>estimate</code> is of length 1, otherwise a matrix
with 2 rows and number of columns equal to the length of
<code>estimate</code></p>


<h3>Note</h3>

<p>You can use <code>if(!exists('.Random.seed')) runif(1)</code> before running
your bootstrap to make sure that <code>.Random.seed</code> will be available
to <code>bootBCa</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="boot.html#topic+boot.ci">boot.ci</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x1 &lt;- runif(100); x2 &lt;- runif(100); y &lt;- sample(0:1, 100, TRUE)
f &lt;- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
seed &lt;- .Random.seed
b &lt;- bootcov(f)
# Get estimated log odds at x1=.4, x2=.6
X &lt;- cbind(c(1,1), x1=c(.4,2), x2=c(.6,3))
est &lt;- X 
ests &lt;- t(X 
bootBCa(est, ests, n=100, seed=seed)
bootBCa(est, ests, type='bca', n=100, seed=seed)
bootBCa(est, ests, type='basic', n=100, seed=seed)

## End(Not run)</code></pre>

<hr>
<h2 id='bootcov'>Bootstrap Covariance and Distribution for Regression Coefficients</h2><span id='topic+bootcov'></span><span id='topic+bootplot'></span><span id='topic+bootplot.bootcov'></span><span id='topic+confplot'></span><span id='topic+confplot.bootcov'></span><span id='topic+histdensity'></span>

<h3>Description</h3>

<p><code>bootcov</code> computes a bootstrap estimate of the covariance matrix for a set
of regression coefficients from <code>ols</code>, <code>lrm</code>, <code>cph</code>,
<code>psm</code>, <code>Rq</code>, and any
other fit where <code>x=TRUE, y=TRUE</code> was used to store the data used in making
the original regression fit and where an appropriate <code>fitter</code> function
is provided here.  The estimates obtained are not conditional on
the design matrix, but are instead unconditional estimates.  For
small sample sizes, this will make a difference as the unconditional
variance estimates are larger.  This function will also obtain
bootstrap estimates corrected for cluster sampling (intra-cluster
correlations) when a &quot;working independence&quot; model was used to fit
data which were correlated within clusters.  This is done by substituting
cluster sampling with replacement for the usual simple sampling with
replacement.  <code>bootcov</code> has an option (<code>coef.reps</code>) that causes all
of the regression coefficient estimates from all of the bootstrap
re-samples to be saved, facilitating computation of nonparametric
bootstrap confidence limits and plotting of the distributions of the
coefficient estimates (using histograms and kernel smoothing estimates).
</p>
<p>The <code>loglik</code> option facilitates the calculation of simultaneous
confidence regions from quantities of interest that are functions of
the regression coefficients, using the method of Tibshirani(1996).
With Tibshirani's method, one computes the objective criterion (-2 log
likelihood evaluated at the bootstrap estimate of <code class="reqn">\beta</code> but with
respect to the original design matrix and response vector) for the
original fit as well as for all of the bootstrap fits.  The confidence
set of the regression coefficients is the set of all coefficients that
are associated with objective function values that are less than or
equal to say the 0.95 quantile of the vector of <code>B + 1</code> objective
function values.  For the coefficients satisfying this condition,
predicted values are computed at a user-specified design matrix <code>X</code>,
and minima and maxima of these predicted values (over the qualifying
bootstrap repetitions) are computed to derive the final simultaneous
confidence band.
</p>
<p>The <code>bootplot</code> function takes the output of <code>bootcov</code> and 
either plots a histogram and kernel density
estimate of specified regression coefficients (or linear combinations
of them through the use of a specified design matrix <code>X</code>), or a
<code>qqnorm</code> plot of the quantities of interest to check for normality of
the maximum likelihood estimates.  <code>bootplot</code> draws vertical lines at
specified quantiles of the bootstrap distribution, and returns these
quantiles for possible printing by the user.  Bootstrap estimates may
optionally be transformed by a user-specified function <code>fun</code> before
plotting.
</p>
<p>The <code>confplot</code> function also uses the output of <code>bootcov</code> but to
compute and optionally plot nonparametric bootstrap pointwise confidence
limits or (by default) Tibshirani (1996) simultaneous confidence sets.
A design matrix must be specified to allow <code>confplot</code> to compute
quantities of interest such as predicted values across a range
of values or differences in predicted values (plots of effects of
changing one or more predictor variable values).
</p>
<p><code>bootplot</code> and <code>confplot</code> are actually generic functions, with
the particular functions <code>bootplot.bootcov</code> and <code>confplot.bootcov</code>
automatically invoked for <code>bootcov</code> objects.
</p>
<p>A service function called <code>histdensity</code> is also provided (for use with
<code>bootplot</code>).  It runs <code>hist</code> and <code>density</code> on the same plot, using
twice the number of classes than the default for <code>hist</code>, and 1.5 times the
<code>width</code> than the default used by <code>density</code>.
</p>
<p>A comprehensive example demonstrates the use of all of the functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootcov(fit, cluster, B=200, fitter, 
        coef.reps=TRUE, loglik=FALSE,
        pr=FALSE, maxit=15, eps=0.0001, group=NULL, stat=NULL,
        seed=sample(10000, 1))


bootplot(obj, which=1 : ncol(Coef), X,
         conf.int=c(.9,.95,.99),
         what=c('density', 'qqnorm', 'box'),
         fun=function(x) x, labels., ...)


confplot(obj, X, against, 
         method=c('simultaneous','pointwise'),
         conf.int=0.95, fun=function(x)x,
         add=FALSE, lty.conf=2, ...)


histdensity(y, xlab, nclass, width, mult.width=1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootcov_+3A_fit">fit</code></td>
<td>

<p>a fit object containing components <code>x</code> and <code>y</code>.  For fits from
<code>cph</code>, the <code>"strata"</code> attribute of the <code>x</code> component is used to
obtain the vector of stratum codes.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_obj">obj</code></td>
<td>

<p>an object created by <code>bootcov</code> with <code>coef.reps=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_x">X</code></td>
<td>

<p>a design matrix specified to <code>confplot</code>.  See <code>predict.rms</code> or
<code>contrast.rms</code>.  For <code>bootplot</code>, <code>X</code> is optional.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_y">y</code></td>
<td>

<p>a vector to pass to <code>histdensity</code>.  <code>NA</code>s are ignored.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_cluster">cluster</code></td>
<td>

<p>a variable indicating groupings. <code>cluster</code> may be any type of vector
(factor, character, integer).
Unique values of <code>cluster</code> indicate
possibly correlated groupings of observations. Note the data used in
the fit and stored in <code>fit$x</code> and <code>fit$y</code> may have had observations
containing missing values deleted.  It is assumed that if there were
any NAs, an <code>naresid</code> function exists for the class of <code>fit</code>. This
function restores NAs so that the rows of the design matrix
coincide with <code>cluster</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_b">B</code></td>
<td>

<p>number of bootstrap repetitions.  Default is 200.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_fitter">fitter</code></td>
<td>

<p>the name of a function with arguments <code>(x,y)</code> that will fit bootstrap
samples.  Default is taken from the class of <code>fit</code> if it is
<code>ols</code>, <code>lrm</code>, <code>cph</code>, <code>psm</code>, <code>Rq</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_coef.reps">coef.reps</code></td>
<td>

<p>set to <code>TRUE</code> if you want to store a matrix of all bootstrap regression
coefficient estimates in the returned component <code>boot.Coef</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_loglik">loglik</code></td>
<td>

<p>set to <code>TRUE</code> to store -2 log likelihoods for each bootstrap model,
evaluated against the original <code>x</code> and <code>y</code> data.  The default
is to do this when <code>coef.reps</code> is specified as <code>TRUE</code>.  The
use of <code>loglik=TRUE</code> assumes that an <code>oos.loglik</code> method
exists for the type of model being analyzed, 
to calculate out-of-sample -2 log likelihoods (see <code>rmsMisc</code>).
After the <code>B</code> -2 log likelihoods (stored in the element named
<code>boot.loglik</code> in the returned fit object), the <code>B+1</code> element is
the -2 log likelihood for the original model fit.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_pr">pr</code></td>
<td>

<p>set to <code>TRUE</code> to print the current sample number to monitor progress.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations, to pass to <code>fitter</code></p>
</td></tr>
<tr><td><code id="bootcov_+3A_eps">eps</code></td>
<td>
<p>argument to pass to various fitters</p>
</td></tr>
<tr><td><code id="bootcov_+3A_group">group</code></td>
<td>

<p>a grouping variable used to stratify the sample upon bootstrapping.
This allows one to handle k-sample problems, i.e., each bootstrap
sample will be forced to select the same number of observations from
each level of group as the number appearing in the original dataset.
You may specify both <code>group</code> and <code>cluster</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_stat">stat</code></td>
<td>

<p>a single character string specifying the name of a <code>stats</code>
element produced by the fitting function to save over the bootstrap
repetitions.  The vector of saved statistics will be in the
<code>boot.stats</code> part of the list returned by <code>bootcov</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_seed">seed</code></td>
<td>
<p>random number seed for <code>set.seed</code>, defaults to a random
integer between 1 and 10000; user should specify a constant for reproducibility</p>
</td></tr>
<tr><td><code id="bootcov_+3A_which">which</code></td>
<td>

<p>one or more integers specifying which regression coefficients to
plot for <code>bootplot</code>
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_conf.int">conf.int</code></td>
<td>

<p>a vector (for <code>bootplot</code>, default is <code>c(.9,.95,.99)</code>) or scalar 
(for <code>confplot</code>, default is <code>.95</code>) confidence level.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_what">what</code></td>
<td>

<p>for <code>bootplot</code>, specifies whether a density or a q-q plot is made,
a <code>ggplot2</code> is used to produce a box plot of all coefficients over
the bootstrap reps
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_fun">fun</code></td>
<td>

<p>for <code>bootplot</code> or <code>confplot</code> specifies a function used to translate
the quantities of interest before analysis.  A common choice is
<code>fun=exp</code> to compute anti-logs, e.g., odds ratios.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_labels.">labels.</code></td>
<td>

<p>a vector of labels for labeling the axes in plots produced by <code>bootplot</code>.
Default is row names of <code>X</code> if there are any, or sequential integers.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_...">...</code></td>
<td>

<p>For <code>bootplot</code> these are optional arguments passed to
<code>histdensity</code>.  Also may be optional arguments passed to
<code>plot</code> by <code>confplot</code> or optional arguments passed to
<code>hist</code> from <code>histdensity</code>, such as <code>xlim</code> and
<code>breaks</code>.  The argument <code>probability=TRUE</code> is always passed to
<code>hist</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_against">against</code></td>
<td>

<p>For <code>confplot</code>, specifying <code>against</code> causes a plot to be made (or added to).
The <code>against</code> variable is associated with rows of <code>X</code> and is used as the
x-coordinates.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_method">method</code></td>
<td>

<p>specifies whether <code>"pointwise"</code> or <code>"simultaneous"</code> confidence regions
are derived by <code>confplot</code>.  The default is simultaneous.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add to an existing plot, for <code>confplot</code>
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_lty.conf">lty.conf</code></td>
<td>

<p>line type for plotting confidence bands in <code>confplot</code>.  Default is
2 for dotted lines.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_xlab">xlab</code></td>
<td>

<p>label for x-axis for <code>histdensity</code>.  Default is <code>label</code> attribute or
argument name if there is no <code>label</code>.
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_nclass">nclass</code></td>
<td>

<p>passed to <code>hist</code> if present
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_width">width</code></td>
<td>

<p>passed to <code>density</code> if present
</p>
</td></tr>
<tr><td><code id="bootcov_+3A_mult.width">mult.width</code></td>
<td>

<p>multiplier by which to adjust the default <code>width</code> passed to <code>density</code>.
Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the fit has a scale parameter (e.g., a fit from <code>psm</code>), the log
of the individual bootstrap scale estimates are added to the vector
of parameter estimates and and column and row for the log scale are
added to the new covariance matrix (the old covariance matrix also
has this row and column).
</p>
<p>For <code>Rq</code> fits, the <code>tau</code>, <code>method</code>, and <code>hs</code>
arguments are taken from the original fit.
</p>


<h3>Value</h3>

<p>a new fit object with class of the original object and with the element
<code>orig.var</code> added. <code>orig.var</code> is
the covariance matrix of the original fit.  Also, the original <code>var</code>
component is replaced with the new bootstrap estimates.  The component
<code>boot.coef</code> is also added.  This contains the mean bootstrap estimates
of regression coefficients (with a log scale element added if
applicable).  <code>boot.Coef</code> is added if <code>coef.reps=TRUE</code>.
<code>boot.loglik</code> is added if <code>loglik=TRUE</code>.  If <code>stat</code> is
specified an additional vector <code>boot.stats</code> will be contained in
the returned object.  <code>B</code> contains the number of successfully fitted
bootstrap resamples.    A component
<code>clusterInfo</code> is added to contain elements <code>name</code> and <code>n</code>
holding the name of the <code>cluster</code> variable and the number of clusters.
</p>
<p><code>bootplot</code> returns a (possible matrix) of quantities of interest and
the requested quantiles of them.  <code>confplot</code> returns three vectors:
<code>fitted</code>, <code>lower</code>, and <code>upper</code>.
</p>


<h3>Side Effects</h3>

<p><code>bootcov</code> prints if <code>pr=TRUE</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a><br />
</p>
<p>Bill Pikounis<br />
Biometrics Research Department<br />
Merck Research Laboratories<br />
<a href="https://billpikounis.com/wpb/">https://billpikounis.com/wpb/</a>
</p>


<h3>References</h3>

<p>Feng Z, McLerran D, Grizzle J (1996): A comparison of statistical methods for
clustered data analysis with Gaussian error.  Stat in Med 15:1793&ndash;1806.
</p>
<p>Tibshirani R, Knight K (1996): Model search and inference by bootstrap 
&quot;bumping&quot;. Department of Statistics, University of Toronto.  Technical
report available from
<br />
http://www-stat.stanford.edu/~tibs/.
Presented at the Joint Statistical Meetings,
Chicago, August 1996.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+robcov">robcov</a></code>, <code><a href="base.html#topic+sample">sample</a></code>, <code><a href="#topic+rms">rms</a></code>,
<code><a href="stats.html#topic+lm.fit">lm.fit</a></code>, <code><a href="#topic+lrm.fit">lrm.fit</a></code>,
<code><a href="survival.html#topic+survival-internal">survival-internal</a></code>, 
<code><a href="#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>,
<code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+gendata">gendata</a></code>, 
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+setPb">setPb</a></code>,
<code>multiwayvcov::cluster.boot</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(191)
x &lt;- exp(rnorm(200))
logit &lt;- 1 + x/2
y &lt;- ifelse(runif(200) &lt;= plogis(logit), 1, 0)
f &lt;- lrm(y ~ pol(x,2), x=TRUE, y=TRUE)
g &lt;- bootcov(f, B=50, pr=TRUE, seed=3)
anova(g)    # using bootstrap covariance estimates
fastbw(g)   # using bootstrap covariance estimates
beta &lt;- g$boot.Coef[,1]
hist(beta, nclass=15)     #look at normality of parameter estimates
qqnorm(beta)
# bootplot would be better than these last two commands


# A dataset contains a variable number of observations per subject,
# and all observations are laid out in separate rows. The responses
# represent whether or not a given segment of the coronary arteries
# is occluded. Segments of arteries may not operate independently
# in the same patient.  We assume a "working independence model" to
# get estimates of the coefficients, i.e., that estimates assuming
# independence are reasonably efficient.  The job is then to get
# unbiased estimates of variances and covariances of these estimates.


set.seed(2)
n.subjects &lt;- 30
ages &lt;- rnorm(n.subjects, 50, 15)
sexes  &lt;- factor(sample(c('female','male'), n.subjects, TRUE))
logit &lt;- (ages-50)/5
prob &lt;- plogis(logit)  # true prob not related to sex
id &lt;- sample(1:n.subjects, 300, TRUE) # subjects sampled multiple times
table(table(id))  # frequencies of number of obs/subject
age &lt;- ages[id]
sex &lt;- sexes[id]
# In truth, observations within subject are independent:
y   &lt;- ifelse(runif(300) &lt;= prob[id], 1, 0)
f &lt;- lrm(y ~ lsp(age,50)*sex, x=TRUE, y=TRUE)
g &lt;- bootcov(f, id, B=50, seed=3)  # usually do B=200 or more
diag(g$var)/diag(f$var)
# add ,group=w to re-sample from within each level of w
anova(g)            # cluster-adjusted Wald statistics
# fastbw(g)         # cluster-adjusted backward elimination
plot(Predict(g, age=30:70, sex='female'))  # cluster-adjusted confidence bands


# Get design effects based on inflation of the variances when compared
# with bootstrap estimates which ignore clustering
g2 &lt;- bootcov(f, B=50, seed=3)
diag(g$var)/diag(g2$var)


# Get design effects based on pooled tests of factors in model
anova(g2)[,1] / anova(g)[,1]


# Simulate binary data where there is a strong 
# age x sex interaction with linear age effects 
# for both sexes, but where not knowing that
# we fit a quadratic model.  Use the bootstrap
# to get bootstrap distributions of various
# effects, and to get pointwise and simultaneous
# confidence limits


set.seed(71)
n   &lt;- 500
age &lt;- rnorm(n, 50, 10)
sex &lt;- factor(sample(c('female','male'), n, rep=TRUE))
L   &lt;- ifelse(sex=='male', 0, .1*(age-50))
y   &lt;- ifelse(runif(n)&lt;=plogis(L), 1, 0)


f &lt;- lrm(y ~ sex*pol(age,2), x=TRUE, y=TRUE)
b &lt;- bootcov(f, B=50, loglik=TRUE, pr=TRUE, seed=3)   # better: B=500


par(mfrow=c(2,3))
# Assess normality of regression estimates
bootplot(b, which=1:6, what='qq')
# They appear somewhat non-normal


# Plot histograms and estimated densities 
# for 6 coefficients
w &lt;- bootplot(b, which=1:6)
# Print bootstrap quantiles
w$quantiles

# Show box plots for bootstrap reps for all coefficients
bootplot(b, what='box')


# Estimate regression function for females
# for a sequence of ages
ages &lt;- seq(25, 75, length=100)
label(ages) &lt;- 'Age'


# Plot fitted function and pointwise normal-
# theory confidence bands
par(mfrow=c(1,1))
p &lt;- Predict(f, age=ages, sex='female')
plot(p)
# Save curve coordinates for later automatic
# labeling using labcurve in the Hmisc library
curves &lt;- vector('list',8)
curves[[1]] &lt;- with(p, list(x=age, y=lower))
curves[[2]] &lt;- with(p, list(x=age, y=upper))


# Add pointwise normal-distribution confidence 
# bands using unconditional variance-covariance
# matrix from the 500 bootstrap reps
p &lt;- Predict(b, age=ages, sex='female')
curves[[3]] &lt;- with(p, list(x=age, y=lower))
curves[[4]] &lt;- with(p, list(x=age, y=upper))


dframe &lt;- expand.grid(sex='female', age=ages)
X &lt;- predict(f, dframe, type='x')  # Full design matrix


# Add pointwise bootstrap nonparametric 
# confidence limits
p &lt;- confplot(b, X=X, against=ages, method='pointwise',
              add=TRUE, lty.conf=4)
curves[[5]] &lt;- list(x=ages, y=p$lower)
curves[[6]] &lt;- list(x=ages, y=p$upper)


# Add simultaneous bootstrap confidence band
p &lt;- confplot(b, X=X, against=ages, add=TRUE, lty.conf=5)
curves[[7]] &lt;- list(x=ages, y=p$lower)
curves[[8]] &lt;- list(x=ages, y=p$upper)
lab &lt;- c('a','a','b','b','c','c','d','d')
labcurve(curves, lab, pl=TRUE)


# Now get bootstrap simultaneous confidence set for
# female:male odds ratios for a variety of ages


dframe &lt;- expand.grid(age=ages, sex=c('female','male'))
X &lt;- predict(f, dframe, type='x')  # design matrix
f.minus.m &lt;- X[1:100,] - X[101:200,]
# First 100 rows are for females.  By subtracting
# design matrices are able to get Xf*Beta - Xm*Beta
# = (Xf - Xm)*Beta


confplot(b, X=f.minus.m, against=ages,
         method='pointwise', ylab='F:M Log Odds Ratio')
confplot(b, X=f.minus.m, against=ages,
         lty.conf=3, add=TRUE)


# contrast.rms makes it easier to compute the design matrix for use
# in bootstrapping contrasts:


f.minus.m &lt;- contrast(f, list(sex='female',age=ages),
                         list(sex='male',  age=ages))$X
confplot(b, X=f.minus.m)


# For a quadratic binary logistic regression model use bootstrap
# bumping to estimate coefficients under a monotonicity constraint
set.seed(177)
n &lt;- 400
x &lt;- runif(n)
logit &lt;- 3*(x^2-1)
y &lt;- rbinom(n, size=1, prob=plogis(logit))
f &lt;- lrm(y ~ pol(x,2), x=TRUE, y=TRUE)
k &lt;- coef(f)
k
vertex &lt;- -k[2]/(2*k[3])
vertex


# Outside [0,1] so fit satisfies monotonicity constraint within
# x in [0,1], i.e., original fit is the constrained MLE


g &lt;- bootcov(f, B=50, coef.reps=TRUE, loglik=TRUE, seed=3)
bootcoef &lt;- g$boot.Coef    # 100x3 matrix
vertex &lt;- -bootcoef[,2]/(2*bootcoef[,3])
table(cut2(vertex, c(0,1)))
mono &lt;- !(vertex &gt;= 0 &amp; vertex &lt;= 1)
mean(mono)    # estimate of Prob{monotonicity in [0,1]}


var(bootcoef)   # var-cov matrix for unconstrained estimates
var(bootcoef[mono,])   # for constrained estimates


# Find second-best vector of coefficient estimates, i.e., best
# from among bootstrap estimates
g$boot.Coef[order(g$boot.loglik[-length(g$boot.loglik)])[1],]
# Note closeness to MLE

## Not run: 
# Get the bootstrap distribution of the difference in two ROC areas for
# two binary logistic models fitted on the same dataset.  This analysis
# does not adjust for the bias ROC area (C-index) due to overfitting.
# The same random number seed is used in two runs to enforce pairing.

set.seed(17)
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
y &lt;- sample(0:1, 100, TRUE)
f &lt;- lrm(y ~ x1, x=TRUE, y=TRUE)
g &lt;- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
f &lt;- bootcov(f, stat='C', seed=4)
g &lt;- bootcov(g, stat='C', seed=4)
dif &lt;- g$boot.stats - f$boot.stats
hist(dif)
quantile(dif, c(.025,.25,.5,.75,.975))
# Compute a z-test statistic.  Note that comparing ROC areas is far less
# powerful than likelihood or Brier score-based methods
z &lt;- (g$stats['C'] - f$stats['C'])/sd(dif)
names(z) &lt;- NULL
c(z=z, P=2*pnorm(-abs(z)))

## End(Not run)
</code></pre>

<hr>
<h2 id='bplot'>
3-D Plots Showing Effects of Two Continuous Predictors in a Regression
Model Fit</h2><span id='topic+bplot'></span><span id='topic+perimeter'></span>

<h3>Description</h3>

<p>Uses lattice graphics and the output from <code>Predict</code> to plot image,
contour, or perspective plots showing the simultaneous effects of two
continuous predictor variables.  Unless <code>formula</code> is provided, the
<code class="reqn">x</code>-axis is constructed from the first variable listed in the call
to <code>Predict</code> and the <code class="reqn">y</code>-axis variable comes from the second.
</p>
<p>The <code>perimeter</code> function is used to generate the boundary of data
to plot when a 3-d plot is made.  It finds the area where there are
sufficient data to generate believable interaction fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bplot(x, formula, lfun=lattice::levelplot, xlab, ylab, zlab,
      adj.subtitle=!info$ref.zero, cex.adj=.75, cex.lab=1,
      perim, showperim=FALSE,
      zlim=range(yhat, na.rm=TRUE), scales=list(arrows=FALSE),
      xlabrot, ylabrot, zlabrot=90, ...)

perimeter(x, y, xinc=diff(range(x))/10, n=10, lowess.=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bplot_+3A_x">x</code></td>
<td>

<p>for <code>bplot</code>, an object created by <code>Predict</code> for which
two or more numeric predictors varied.
For <code>perim</code> is
the first variable of a pair of predictors forming a 3-d plot.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_formula">formula</code></td>
<td>

<p>a formula of the form <code>f(yhat) ~ x*y</code> optionally followed by |a*b*c
which are 1-3 paneling variables that were specified to <code>Predict</code>.
<code>f</code> can represent any R function of a vector that produces a
vector.  If the left hand side of the formula is omitted,
<code>yhat</code> will be inserted.  If <code>formula</code> is omitted, it will
be inferred from the first two variables that varied in the call to
<code>Predict</code>.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_lfun">lfun</code></td>
<td>

<p>a high-level lattice plotting function that takes formulas of the
form <code>z ~ x*y</code>.  The default is an image plot
(<code>levelplot</code>).  Other common choices are <code>wireframe</code> for
perspective plot or <code>contourplot</code> for a contour plot.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_xlab">xlab</code></td>
<td>

<p>Character string label for <code class="reqn">x</code>-axis. Default is given by <code>Predict</code>.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_ylab">ylab</code></td>
<td>

<p>Character string abel for <code class="reqn">y</code>-axis
</p>
</td></tr>
<tr><td><code id="bplot_+3A_zlab">zlab</code></td>
<td>

<p>Character string <code class="reqn">z</code>-axis label for perspective (wireframe) plots.
Default comes from <code>Predict</code>.  <code>zlab</code> will often be
specified if <code>fun</code> was specified to <code>Predict</code>.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_adj.subtitle">adj.subtitle</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress subtitling the graph with the list of
settings of non-graphed adjustment values. Default is <code>TRUE</code>
if there are non-plotted adjustment variables and <code>ref.zero</code>
was not used.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_cex.adj">cex.adj</code></td>
<td>

<p><code>cex</code> parameter for size of adjustment settings in subtitles.  Default is
0.75
</p>
</td></tr>
<tr><td><code id="bplot_+3A_cex.lab">cex.lab</code></td>
<td>

<p><code>cex</code> parameter for axis labels.  Default is 1.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_perim">perim</code></td>
<td>

<p>names a matrix created by <code>perimeter</code> when used for 3-d plots of
two continuous predictors.  When the combination of variables is outside
the range in <code>perim</code>, that section of the plot is suppressed.  If
<code>perim</code> 
is omitted, 3-d plotting will use the marginal distributions of the
two predictors to determine the plotting region, when the grid is
not specified explicitly in <code>variables</code>.  When instead a series of
curves is being plotted, <code>perim</code> specifies a function having two
arguments.  The first is the vector of values of the first variable that
is about to be plotted on the <code class="reqn">x</code>-axis.  The second argument
is the single 
value of the variable representing different curves, for the current
curve being plotted.  The function's returned value must be a logical
vector whose length is the same as that of the first argument, with
values <code>TRUE</code> if the corresponding point should be plotted for the
current curve, <code>FALSE</code> otherwise.  See one of the latter examples.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_showperim">showperim</code></td>
<td>

<p>set to <code>TRUE</code> if <code>perim</code> is specified and you want to
show the actual perimeter used.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_zlim">zlim</code></td>
<td>

<p>Controls the range for plotting in the <code class="reqn">z</code>-axis if there is
one. Computed by default.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_scales">scales</code></td>
<td>
<p>see <code><a href="lattice.html#topic+cloud">wireframe</a></code>
</p>
</td></tr>
<tr><td><code id="bplot_+3A_xlabrot">xlabrot</code></td>
<td>
<p>rotation angle for the x-axis.  Default is 30 for
<code>wireframe</code> and 0 otherwise.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_ylabrot">ylabrot</code></td>
<td>
<p>rotation angle for the y-axis.  Default is -40 for
<code>wireframe</code>, 90 for <code>contourplot</code> or <code>levelplot</code>,
and 0 otherwise.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_zlabrot">zlabrot</code></td>
<td>
<p>rotation angle for z-axis rotation for
<code>wireframe</code> plots
</p>
</td></tr>
<tr><td><code id="bplot_+3A_...">...</code></td>
<td>
<p>other arguments to pass to the lattice function
</p>
</td></tr>
<tr><td><code id="bplot_+3A_y">y</code></td>
<td>

<p>second variable of the pair for <code>perim</code>.  If omitted, <code>x</code> is
assumed to be a list with both <code>x</code> and <code>y</code> components.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_xinc">xinc</code></td>
<td>

<p>increment in <code>x</code> over which to examine the density of <code>y</code> in
<code>perimeter</code> 
</p>
</td></tr>
<tr><td><code id="bplot_+3A_n">n</code></td>
<td>

<p>within intervals of <code>x</code> for <code>perimeter</code>, takes the
informative range of <code>y</code> to be the <code class="reqn">n</code>th smallest to the
<code class="reqn">n</code>th largest values of <code>y</code>.  If there aren't 
at least 2<code class="reqn">n</code> <code>y</code> values in the <code>x</code> interval, no
<code>y</code> ranges are used for that interval.
</p>
</td></tr>
<tr><td><code id="bplot_+3A_lowess.">lowess.</code></td>
<td>

<p>set to <code>FALSE</code> to not have <code>lowess</code> smooth the data perimeters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>perimeter</code> is a kind of generalization of <code>datadist</code> for 2
continuous variables.  First, the <code>n</code> smallest and largest <code>x</code>
values are determined.  These form the lowest and highest possible
<code>x</code>s to display.  Then <code>x</code> is grouped into intervals bounded
by these two numbers, with the interval widths defined by <code>xinc</code>.
Within each interval, <code>y</code> is sorted and the <code class="reqn">n</code>th smallest and
largest <code>y</code> are taken as the interval containing sufficient data
density to plot interaction surfaces.  The interval is ignored when
there are insufficient <code>y</code> values.  When the data are being
readied for <code>persp</code>, <code>bplot</code> uses the <code>approx</code> function to do
linear interpolation of the <code>y</code>-boundaries as a function of the
<code>x</code> values actually used in forming the grid (the values of the
first variable specified to <code>Predict</code>).  To make the perimeter smooth,
specify <code>lowess.=TRUE</code> to <code>perimeter</code>.
</p>


<h3>Value</h3>

<p><code>perimeter</code> returns a matrix of class <code>perimeter</code>.  This
outline can be conveniently plotted by <code>lines.perimeter</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+Predict">Predict</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="lattice.html#topic+levelplot">levelplot</a></code>,
<code><a href="lattice.html#topic+contourplot">contourplot</a></code>, <code><a href="lattice.html#topic+cloud">wireframe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
               x=TRUE, y=TRUE)
p &lt;- Predict(fit, age, cholesterol, sex, np=50) # vary sex last
require(lattice)
bplot(p)                 # image plot for age, cholesterol with color
                         # coming from yhat; use default ranges for
                         # both continuous predictors; two panels (for sex)
bplot(p, lfun=wireframe) # same as bplot(p,,wireframe)
# View from different angle, change y label orientation accordingly
# Default is z=40, x=-60
bplot(p,, wireframe, screen=list(z=40, x=-75), ylabrot=-25)
bplot(p,, contourplot)   # contour plot
bounds  &lt;- perimeter(age, cholesterol, lowess=TRUE)
plot(age, cholesterol)     # show bivariate data density and perimeter
lines(bounds[,c('x','ymin')]); lines(bounds[,c('x','ymax')])
p &lt;- Predict(fit, age, cholesterol)  # use only one sex
bplot(p, perim=bounds)   # draws image() plot
                         # don't show estimates where data are sparse
                         # doesn't make sense here since vars don't interact
bplot(p, plogis(yhat) ~ age*cholesterol) # Probability scale
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='calibrate'>
Resampling Model Calibration
</h2><span id='topic+calibrate'></span><span id='topic+calibrate.default'></span><span id='topic+calibrate.cph'></span><span id='topic+calibrate.psm'></span><span id='topic+print.calibrate'></span><span id='topic+print.calibrate.default'></span><span id='topic+plot.calibrate'></span><span id='topic+plot.calibrate.default'></span>

<h3>Description</h3>

<p>Uses bootstrapping or cross-validation to get bias-corrected (overfitting-
corrected) estimates of predicted vs. observed values based on
subsetting predictions into intervals (for survival models) or on
nonparametric smoothers (for other models). There are calibration
functions for Cox (<code>cph</code>), parametric survival models (<code>psm</code>),
binary and ordinal logistic models (<code>lrm</code>) and ordinary least
squares (<code>ols</code>).  For survival models,
&quot;predicted&quot; means predicted survival probability at a single
time point, and &quot;observed&quot; refers to the corresponding Kaplan-Meier 
survival estimate, stratifying on intervals of predicted survival, or,
if the <code>polspline</code> package is installed, the predicted survival
probability as a function of transformed predicted survival probability
using the flexible hazard regression approach (see the <code>val.surv</code>
function for details).  For logistic and linear models, a nonparametric
calibration curve is estimated over a sequence of predicted values.  The
fit must have specified <code>x=TRUE, y=TRUE</code>.  The <code>print</code> and
<code>plot</code> methods for <code>lrm</code> and <code>ols</code> models (which use
<code>calibrate.default</code>) print the mean absolute error in predictions,
the mean squared error, and the 0.9 quantile of the absolute error.
Here, error refers to the difference between the predicted values and
the corresponding bias-corrected calibrated values.
</p>
<p>Below, the second, third, and fourth invocations of <code>calibrate</code>
are, respectively, for <code>ols</code> and <code>lrm</code>, <code>cph</code>, and
<code>psm</code>.  The first and second <code>plot</code> invocation are
respectively for <code>lrm</code> and <code>ols</code> fits or all other fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(fit, ...)
## Default S3 method:
calibrate(fit, predy, 
  method=c("boot","crossvalidation",".632","randomization"),
  B=40, bw=FALSE, rule=c("aic","p"),
  type=c("residual","individual"),
  sls=.05, aics=0, force=NULL, estimates=TRUE, pr=FALSE, kint,
  smoother="lowess", digits=NULL, ...) 
## S3 method for class 'cph'
calibrate(fit, cmethod=c('hare', 'KM'),
  method="boot", u, m=150, pred, cuts, B=40, 
  bw=FALSE, rule="aic", type="residual", sls=0.05, aics=0, force=NULL,
  estimates=TRUE,
  pr=FALSE, what="observed-predicted", tol=1e-12, maxdim=5, ...)
## S3 method for class 'psm'
calibrate(fit, cmethod=c('hare', 'KM'),
  method="boot", u, m=150, pred, cuts, B=40,
  bw=FALSE,rule="aic",
  type="residual", sls=.05, aics=0, force=NULL, estimates=TRUE,
  pr=FALSE, what="observed-predicted", tol=1e-12, maxiter=15, 
  rel.tolerance=1e-5, maxdim=5, ...)

## S3 method for class 'calibrate'
print(x, B=Inf, ...)
## S3 method for class 'calibrate.default'
print(x, B=Inf, ...)

## S3 method for class 'calibrate'
plot(x, xlab, ylab, subtitles=TRUE, conf.int=TRUE,
 cex.subtitles=.75, riskdist=TRUE, add=FALSE,
 scat1d.opts=list(nhistSpike=200), par.corrected=NULL, ...)

## S3 method for class 'calibrate.default'
plot(x, xlab, ylab, xlim, ylim,
  legend=TRUE, subtitles=TRUE, cex.subtitles=.75, riskdist=TRUE,
  scat1d.opts=list(nhistSpike=200), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_fit">fit</code></td>
<td>

<p>a fit from <code>ols</code>, <code>lrm</code>, <code>cph</code> or <code>psm</code>
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_x">x</code></td>
<td>
<p>an object created by <code>calibrate</code></p>
</td></tr>
<tr><td><code id="calibrate_+3A_method">method</code>, <code id="calibrate_+3A_b">B</code>, <code id="calibrate_+3A_bw">bw</code>, <code id="calibrate_+3A_rule">rule</code>, <code id="calibrate_+3A_type">type</code>, <code id="calibrate_+3A_sls">sls</code>, <code id="calibrate_+3A_aics">aics</code>, <code id="calibrate_+3A_force">force</code>, <code id="calibrate_+3A_estimates">estimates</code></td>
<td>
<p>see <code><a href="#topic+validate">validate</a></code>.
For <code>print.calibrate</code>, <code>B</code> is an
upper limit on the number of resamples for which 
information is printed about which variables were selected in each
model re-fit. Specify zero to suppress printing.  Default is to print
all re-samples.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_cmethod">cmethod</code></td>
<td>
<p>method for validating survival predictions using
right-censored data.  The default is <code>cmethod='hare'</code> to use the
<code>hare</code> function in the <code>polspline</code> package.  Specify
<code>cmethod='KM'</code> to use less precision stratified Kaplan-Meier
estimates.  If the <code>polspline</code> package is not available, the
procedure reverts to <code>cmethod='KM'</code>.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_u">u</code></td>
<td>

<p>the time point for which to validate predictions for survival
models. For <code>cph</code> fits, you must have specified <code>surv=TRUE,
  time.inc=u</code>, where <code>u</code> is the constant specifying the time to
predict.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_m">m</code></td>
<td>

<p>group predicted <code>u</code>-time units survival into intervals containing
<code>m</code> subjects on the average (for survival models only)
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_pred">pred</code></td>
<td>

<p>vector of predicted survival probabilities at which to evaluate the
calibration curve.  By default, the low and high prediction values
from <code>datadist</code> are used, which for large sample size is the 10th
smallest to the 10th largest predicted probability.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_cuts">cuts</code></td>
<td>

<p>actual cut points for predicted survival probabilities. You may
specify only one of <code>m</code> and <code>cuts</code> (for survival models only)
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_pr">pr</code></td>
<td>

<p>set to <code>TRUE</code> to print intermediate results for each re-sample
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_what">what</code></td>
<td>

<p>The default is <code>"observed-predicted"</code>, meaning to estimate optimism
in this difference. This is preferred as it accounts for skewed
distributions of predicted probabilities in outer intervals. You can
also specify <code>"observed"</code>.  This argument applies to survival models only.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_tol">tol</code></td>
<td>
<p>criterion for matrix singularity (default is <code>1e-12</code>)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_maxdim">maxdim</code></td>
<td>
<p>see <code><a href="polspline.html#topic+hare">hare</a></code></p>
</td></tr>
<tr><td><code id="calibrate_+3A_maxiter">maxiter</code></td>
<td>
<p>for <code>psm</code>, this is passed to
<code><a href="survival.html#topic+survreg.control">survreg.control</a></code> (default is 15 iterations)
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_rel.tolerance">rel.tolerance</code></td>
<td>
<p>parameter passed to
<code><a href="survival.html#topic+survreg.control">survreg.control</a></code> for <code>psm</code> (default is 1e-5).
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_predy">predy</code></td>
<td>

<p>a scalar or vector of predicted values to calibrate (for <code>lrm</code>,
<code>ols</code>).  Default is 50 equally spaced points between the 5th
smallest and the 5th largest  predicted values.  For <code>lrm</code> the
predicted values are probabilities (see <code>kint</code>).
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_kint">kint</code></td>
<td>

<p>For an ordinal logistic model the default predicted
probability that <code class="reqn">Y\geq</code> the middle level.  Specify <code>kint</code> to specify the
intercept to use, e.g., <code>kint=2</code> means to calibrate <code class="reqn">Prob(Y\geq
  b)</code>, where <code class="reqn">b</code> is the second level of <code class="reqn">Y</code>.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_smoother">smoother</code></td>
<td>

<p>a function in two variables which produces <code class="reqn">x</code>- and
<code class="reqn">y</code>-coordinates by smoothing the input <code>y</code>.  The default is to
use <code>lowess(x, y, iter=0)</code>. 
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_digits">digits</code></td>
<td>
<p>If specified, predicted values are rounded to
<code>digits</code> digits before passing to the smoother.  Occasionally,
large predicted values on the logit scale will lead to predicted
probabilities very near 1 that should be treated as 1, and the
<code>round</code> function will fix that.  Applies to <code>calibrate.default</code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>

<p>other arguments to pass to <code>predab.resample</code>, such as <code>group</code>,
<code>cluster</code>, and <code>subset</code>.
Also, other arguments for <code>plot</code>.
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_xlab">xlab</code></td>
<td>

<p>defaults to &quot;Predicted x-units Survival&quot; or to a suitable label for
other models
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_ylab">ylab</code></td>
<td>

<p>defaults to &quot;Fraction Surviving x-units&quot; or to a suitable label for
other models
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_xlim">xlim</code>, <code id="calibrate_+3A_ylim">ylim</code></td>
<td>
<p>2-vectors specifying x- and y-axis limits, if not using
defaults</p>
</td></tr> 
<tr><td><code id="calibrate_+3A_subtitles">subtitles</code></td>
<td>

<p>set to <code>FALSE</code> to suppress subtitles in plot describing method and for <code>lrm</code>
and <code>ols</code> the mean absolute error and original sample size
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_conf.int">conf.int</code></td>
<td>

<p>set to <code>FALSE</code> to suppress plotting 0.95 confidence intervals for
Kaplan-Meier estimates
</p>
</td></tr>
<tr><td><code id="calibrate_+3A_cex.subtitles">cex.subtitles</code></td>
<td>
<p>character size for plotting subtitles</p>
</td></tr>
<tr><td><code id="calibrate_+3A_riskdist">riskdist</code></td>
<td>
<p>set to <code>FALSE</code> to suppress the distribution of
predicted risks (survival probabilities) from being plotted</p>
</td></tr>
<tr><td><code id="calibrate_+3A_add">add</code></td>
<td>
<p>set to <code>TRUE</code> to add the calibration plot to an existing
plot</p>
</td></tr>
<tr><td><code id="calibrate_+3A_scat1d.opts">scat1d.opts</code></td>
<td>
<p>a list specifying options to send to <code>scat1d</code> if
<code>riskdist=TRUE</code>.  See <code><a href="Hmisc.html#topic+scat1d">scat1d</a></code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_par.corrected">par.corrected</code></td>
<td>
<p>a list specifying graphics parameters <code>col</code>,
<code>lty</code>, <code>lwd</code>, <code>pch</code> to be used in drawing
overfitting-corrected estimates.  Default is <code>col="blue"</code>,
<code>lty=1</code>, <code>lwd=1</code>, <code>pch=4</code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_legend">legend</code></td>
<td>

<p>set to <code>FALSE</code> to suppress legends (for <code>lrm</code>, <code>ols</code>
only) on the calibration plot, or specify a list with elements <code>x</code>
and <code>y</code> containing the coordinates of the upper left corner of the
legend.  By default, a legend will be drawn in the lower right 1/16th of
the plot.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the fit was created using penalized maximum likelihood estimation,
the same <code>penalty</code> and <code>penalty.scale</code> parameters are used during
validation.
</p>


<h3>Value</h3>

<p>matrix specifying mean predicted survival in each interval, the
corresponding estimated bias-corrected Kaplan-Meier estimates,
number of subjects, and other statistics.  For linear and logistic models,
the matrix instead has rows corresponding to the prediction points, and
the vector of predicted values being validated is returned as an attribute.
The returned object has class <code>"calibrate"</code> or
<code>"calibrate.default"</code>.
<code>plot.calibrate.default</code> invisibly returns the vector of estimated
prediction errors corresponding to the dataset used to fit the model.
</p>


<h3>Side Effects</h3>

<p>prints, and stores an object <code>pred.obs</code> or <code>.orig.cal</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>,
<code><a href="#topic+groupkm">groupkm</a></code>, <code><a href="Hmisc.html#topic+errbar">errbar</a></code>,
<code><a href="Hmisc.html#topic+scat1d">scat1d</a></code>, <code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+psm">psm</a></code>,
<code><a href="stats.html#topic+lowess">lowess</a></code>,<code><a href="Hmisc.html#topic+fit.mult.impute">fit.mult.impute</a></code>,
<code><a href="#topic+processMI">processMI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
set.seed(1)
n &lt;- 200
d.time &lt;- rexp(n)
x1 &lt;- runif(n)
x2 &lt;- factor(sample(c('a', 'b', 'c'), n, TRUE))
f &lt;- cph(Surv(d.time) ~ pol(x1,2) * x2, x=TRUE, y=TRUE, surv=TRUE, time.inc=1.5)
#or f &lt;- psm(S ~ \dots)
pa &lt;- requireNamespace('polspline')
if(pa) {
 cal &lt;- calibrate(f, u=1.5, B=20)  # cmethod='hare'
 plot(cal)
}
cal &lt;- calibrate(f, u=1.5, cmethod='KM', m=50, B=20)  # usually B=200 or 300
plot(cal, add=pa)

set.seed(1)
y &lt;- sample(0:2, n, TRUE)
x1 &lt;- runif(n)
x2 &lt;- runif(n)
x3 &lt;- runif(n)
x4 &lt;- runif(n)
f &lt;- lrm(y ~ x1 + x2 + x3 * x4, x=TRUE, y=TRUE)
cal &lt;- calibrate(f, kint=2, predy=seq(.2, .8, length=60), 
                 group=y)
# group= does k-sample validation: make resamples have same 
# numbers of subjects in each level of y as original sample

plot(cal)
#See the example for the validate function for a method of validating
#continuation ratio ordinal logistic models.  You can do the same
#thing for calibrate
</code></pre>

<hr>
<h2 id='contrast.rms'>General Contrasts of Regression Coefficients</h2><span id='topic+contrast'></span><span id='topic+contrast.rms'></span><span id='topic+print.contrast.rms'></span>

<h3>Description</h3>

<p>This function computes one or more contrasts of the estimated
regression coefficients in a fit from one of the functions in rms,
along with standard errors, confidence limits, t or Z statistics, P-values.
General contrasts are handled by obtaining the design matrix for two
sets of predictor settings (<code>a</code>, <code>b</code>) and subtracting the
corresponding rows of the two design matrics to obtain a new contrast
design matrix for testing the <code>a</code> - <code>b</code> differences.  This allows for
quite general contrasts (e.g., estimated differences in means between
a 30 year old female and a 40 year old male).
This can also be used
to obtain a series of contrasts in the presence of interactions (e.g.,
female:male log odds ratios for several ages when the model contains
age by sex interaction).  Another use of <code>contrast</code> is to obtain
center-weighted (Type III test) and subject-weighted (Type II test)
estimates in a model containing treatment by center interactions.  For
the latter case, you can specify <code>type="average"</code> and an optional
<code>weights</code> vector to average the within-center treatment contrasts.
The design contrast matrix computed by <code>contrast.rms</code> can be used
by other functions.
</p>
<p>When the model was fitted by a Bayesian function such as <code>blrm</code>,
highest posterior density intervals for contrasts are computed instead, along with the
posterior probability that the contrast is positive.
<code>posterior.summary</code> specifies whether posterior mean/median/mode is
to be used for contrast point estimates.
</p>
<p><code>contrast.rms</code> also allows one to specify four settings to
contrast, yielding contrasts that are double differences - the
difference between the first two settings (<code>a</code> - <code>b</code>) and the
last two (<code>a2</code> - <code>b2</code>).  This allows assessment of interactions.
</p>
<p>If <code>usebootcoef=TRUE</code>, the fit was run through <code>bootcov</code>, and
<code>conf.type="individual"</code>, the confidence intervals are bootstrap
nonparametric percentile confidence intervals, basic bootstrap, or BCa
intervals, obtained on contrasts evaluated on all bootstrap samples.
</p>
<p>By omitting the <code>b</code> argument, <code>contrast</code> can be used to obtain
an average or weighted average of a series of predicted values, along
with a confidence interval for this average.  This can be useful for
&quot;unconditioning&quot; on one of the predictors (see the next to last
example).
</p>
<p>Specifying <code>type="joint"</code>, and specifying at least as many contrasts
as needed to span the space of a complex test, one can make
multiple degree of freedom tests flexibly and simply.  Redundant
contrasts will be ignored in the joint test.  See the examples below.
These include an example of an &quot;incomplete interaction test&quot; involving
only two of three levels of a categorical variable (the test also tests
the main effect).
</p>
<p>When more than one contrast is computed, the list created by
<code>contrast.rms</code> is suitable for plotting (with error bars or bands)
with <code>xYplot</code> or <code>Dotplot</code> (see the last example before the
<code>type="joint"</code> examples).
</p>
<p>When <code>fit</code> is the result of a Bayesian model fit and <code>fun</code> is
specified, <code>contrast.rms</code> operates altogether differently.  <code>a</code>
and <code>b</code> must both be specified and <code>a2, b2</code> not specified.
<code>fun</code> is evaluated on the estimates
separately on <code>a</code> and <code>b</code> and the subtraction is deferred.  So
even in the absence of interactions, when <code>fun</code> is nonlinear, the
settings of factors (predictors) will not cancel out and estimates of
differences will be covariate-specific (unless there are no covariates
in the model besides the one being varied to get from <code>a</code> to <code>b</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrast(fit, ...)
## S3 method for class 'rms'
contrast(fit, a, b, a2, b2, ycut=NULL, cnames=NULL,
         fun=NULL, funint=TRUE,
         type=c("individual", "average", "joint"),
         conf.type=c("individual","simultaneous"), usebootcoef=TRUE,
         boot.type=c("percentile","bca","basic"),
         posterior.summary=c('mean', 'median', 'mode'),
         weights="equal", conf.int=0.95, tol=1e-7, expand=TRUE, ...)

## S3 method for class 'contrast.rms'
print(x, X=FALSE,
       fun=function(u)u, jointonly=FALSE, prob=0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrast.rms_+3A_fit">fit</code></td>
<td>

<p>a fit of class <code>"rms"</code>
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_a">a</code></td>
<td>

<p>a list containing settings for all predictors that you do not wish to
set to default (adjust-to) values.  Usually you will specify two
variables in this list, one set to a constant and one to a sequence of
values, to obtain contrasts for the sequence of values of an
interacting factor.  The <code>gendata</code> function will generate the
necessary combinations and default values for unspecified predictors,
depending on the <code>expand</code> argument.
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_b">b</code></td>
<td>

<p>another list that generates the same number of observations as <code>a</code>,
unless one of the two lists generates only one observation.  In that
case, the design matrix generated from the shorter list will have its
rows replicated so that the contrasts assess several differences
against the one set of predictor values.  This is useful for comparing
multiple treatments with control, for example.  If <code>b</code> is missing, the
design matrix generated from <code>a</code> is analyzed alone.
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_a2">a2</code></td>
<td>
<p>an optional third list of settings of predictors</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_b2">b2</code></td>
<td>
<p>an optional fourth list of settings of predictors.  Mandatory
if <code>a2</code> is given.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_ycut">ycut</code></td>
<td>
<p>used of the fit is a constrained partial proportional odds
model fit, to specify the single value or vector of values
(corresponding to the multiple contrasts) of the response
variable to use in forming contrasts.  When there is
non-proportional odds, odds ratios will vary over levels of the
response variable.  When there are multiple contrasts and only
one value is given for <code>ycut</code>, that value will be propagated to
all contrasts.  To show the effect of non-proportional odds, let
<code>ycut</code> vary.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_cnames">cnames</code></td>
<td>

<p>vector of character strings naming the contrasts when
<code>type!="average"</code>.  Usually <code>cnames</code> is not necessary as
<code>contrast.rms</code> tries to name the contrasts by examining which
predictors are varying consistently in the two lists.  <code>cnames</code> will
be needed when you contrast &quot;non-comparable&quot; settings, e.g., you
compare <code>list(treat="drug", age=c(20,30))</code> with
<code>list(treat="placebo"), age=c(40,50))</code>
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_fun">fun</code></td>
<td>
<p>a function to evaluate on the linear predictor for each of
<code>a</code> and <code>b</code>.  Applies to Bayesian model fits.  Also, 
a function to transform the contrast, SE, and lower and upper
confidence limits before printing.  For example, specify <code>fun=exp</code> to
anti-log them for logistic models.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_type">type</code></td>
<td>

<p>set <code>type="average"</code> to average the individual contrasts (e.g., to
obtain a Type II or III contrast).  Set <code>type="joint"</code> to jointly
test all non-redundant contrasts with a multiple degree of freedom test
and no averaging.
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_conf.type">conf.type</code></td>
<td>

<p>The default type of confidence interval computed for a given
individual (1 d.f.) contrast is a pointwise confidence interval.  Set
<code>conf.type="simultaneous"</code> to use the <code>multcomp</code> package's
<code>glht</code> and <code>confint</code> functions to compute confidence
intervals with simultaneous (family-wise) coverage, thus adjusting for
multiple comparisons.  Note that individual P-values are not adjusted
for multiplicity.
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_usebootcoef">usebootcoef</code></td>
<td>

<p>If <code>fit</code> was the result of <code>bootcov</code> but you want to use the
bootstrap covariance matrix instead of the nonparametric percentile,
basic, or BCa  method for confidence intervals (which uses all the bootstrap
coefficients), specify <code>usebootcoef=FALSE</code>.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_boot.type">boot.type</code></td>
<td>
<p>set to <code>'bca'</code> to compute BCa confidence
limits or <code>'basic'</code> to use the basic bootstrap.  The default is
to compute percentile intervals</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>By default the posterior mean is used.
Specify <code>posterior.summary='median'</code> to instead use the posterior
median and likewise <code>posterior.summary='mode'</code>.  Unlike other
functions, <code>contrast.rms</code> does not default to <code>'mode'</code> because
point estimates come from contrasts and not the original model
coefficients point estimates.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_weights">weights</code></td>
<td>

<p>a numeric vector, used when <code>type="average"</code>, to obtain weighted contrasts
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence level for confidence intervals for the contrasts (HPD
interval probability for Bayesian analyses)
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_tol">tol</code></td>
<td>
<p>tolerance for <code>qr</code> function for determining which
contrasts are redundant, and for inverting the covariance matrix
involved in a joint test</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_expand">expand</code></td>
<td>
<p>set to <code>FALSE</code> to have <code>gendata</code> not generate
all possible combinations of predictor settings.  This is useful when
getting contrasts over irregular predictor settings.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_...">...</code></td>
<td>
<p>passed to <code>print</code> for main output.  A useful thing to
pass is <code>digits=4</code>.</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_x">x</code></td>
<td>
<p>result of <code>contrast</code></p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_x">X</code></td>
<td>

<p>set <code>X=TRUE</code> to  print design matrix used in computing the contrasts (or
the average contrast)
</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_funint">funint</code></td>
<td>
<p>set to <code>FALSE</code> if <code>fun</code> is not a function such
as the result of <code>Mean</code>, <code>Quantile</code>, or <code>ExProb</code> that
contains an <code>intercepts</code> argument</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_jointonly">jointonly</code></td>
<td>
<p>set to <code>FALSE</code> to omit printing of individual
contrasts</p>
</td></tr>
<tr><td><code id="contrast.rms_+3A_prob">prob</code></td>
<td>
<p>highest posterior density interval probability when the fit
was Bayesian and <code>fun</code> was specified to <code>contrast.rms</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>"contrast.rms"</code> containing the elements
<code>Contrast</code>, <code>SE</code>, <code>Z</code>, <code>var</code>, <code>df.residual</code>
<code>Lower</code>, <code>Upper</code>, <code>Pvalue</code>, <code>X</code>, <code>cnames</code>, <code>redundant</code>, which denote the contrast
estimates, standard errors, Z or t-statistics, variance matrix,
residual degrees of freedom (this is <code>NULL</code> if the model was not
<code>ols</code>), lower and upper confidence limits, 2-sided P-value, design
matrix, contrast names (or <code>NULL</code>), and a logical vector denoting
which contrasts are redundant with the other contrasts.  If there are
any redundant contrasts, when the results of <code>contrast</code> are
printed, and asterisk is printed at the start of the corresponding
lines.  The object also contains <code>ctype</code> indicating what method was
used for compute confidence intervals.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University School of Medicine<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+gendata">gendata</a></code>, <code><a href="#topic+bootcov">bootcov</a></code>,
<code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
set.seed(1)
age &lt;- rnorm(200,40,12)
sex &lt;- factor(sample(c('female','male'),200,TRUE))
logit &lt;- (sex=='male') + (age-40)/5
y &lt;- ifelse(runif(200) &lt;= plogis(logit), 1, 0)
f &lt;- lrm(y ~ pol(age,2)*sex)
anova(f)
# Compare a 30 year old female to a 40 year old male
# (with or without age x sex interaction in the model)
contrast(f, list(sex='female', age=30), list(sex='male', age=40))
# Test for interaction between age and sex, duplicating anova
contrast(f, list(sex='female', age=30),
            list(sex='male',   age=30),
            list(sex='female', age=c(40,50)),
            list(sex='male',   age=c(40,50)), type='joint')
# Duplicate overall sex effect in anova with 3 d.f.
contrast(f, list(sex='female', age=c(30,40,50)),
            list(sex='male',   age=c(30,40,50)), type='joint')
# For females get an array of odds ratios against age=40
k &lt;- contrast(f, list(sex='female', age=30:50),
                 list(sex='female', age=40))
print(k, fun=exp)
# Plot odds ratios with pointwise 0.95 confidence bands using log scale
k &lt;- as.data.frame(k[c('Contrast','Lower','Upper')])
ggplot(k, aes(x=30:50, y=exp(Contrast))) + geom_line() +
   geom_ribbon(aes(ymin=exp(Lower), ymax=exp(Upper)),
               alpha=0.15, linetype=0) +
   scale_y_continuous(trans='log10', n.breaks=10,
               minor_breaks=c(seq(0.1, 1, by=.1), seq(1, 10, by=.5))) +
  xlab('Age') + ylab('OR against age 40')

# For a model containing two treatments, centers, and treatment
# x center interaction, get 0.95 confidence intervals separately
# by center
center &lt;- factor(sample(letters[1 : 8], 500, TRUE))
treat  &lt;- factor(sample(c('a','b'), 500, TRUE))
y      &lt;- 8*(treat == 'b') + rnorm(500, 100, 20)
f &lt;- ols(y ~ treat*center)


lc &lt;- levels(center)
contrast(f, list(treat='b', center=lc),
            list(treat='a', center=lc))


# Get 'Type III' contrast: average b - a treatment effect over
# centers, weighting centers equally (which is almost always
# an unreasonable thing to do)
contrast(f, list(treat='b', center=lc),
            list(treat='a', center=lc),
         type='average')


# Get 'Type II' contrast, weighting centers by the number of
# subjects per center.  Print the design contrast matrix used.
k &lt;- contrast(f, list(treat='b', center=lc),
                 list(treat='a', center=lc),
              type='average', weights=table(center))
print(k, X=TRUE)
# Note: If other variables had interacted with either treat 
# or center, we may want to list settings for these variables
# inside the list()'s, so as to not use default settings


# For a 4-treatment study, get all comparisons with treatment 'a'
treat  &lt;- factor(sample(c('a','b','c','d'),  500, TRUE))
y      &lt;- 8*(treat == 'b') + rnorm(500, 100, 20)
dd     &lt;- datadist(treat, center); options(datadist='dd')
f &lt;- ols(y ~ treat*center)
lt &lt;- levels(treat)
contrast(f, list(treat=lt[-1]),
            list(treat=lt[ 1]),
         cnames=paste(lt[-1], lt[1], sep=':'), conf.int=1 - .05 / 3)


# Compare each treatment with average of all others
for(i in 1 : length(lt)) {
  cat('Comparing with', lt[i], '\n\n')
  print(contrast(f, list(treat=lt[-i]),
                    list(treat=lt[ i]), type='average'))
}
options(datadist=NULL)

# Six ways to get the same thing, for a variable that
# appears linearly in a model and does not interact with
# any other variables.  We estimate the change in y per
# unit change in a predictor x1.  Methods 4, 5 also
# provide confidence limits.  Method 6 computes nonparametric
# bootstrap confidence limits.  Methods 2-6 can work
# for models that are nonlinear or non-additive in x1.
# For that case more care is needed in choice of settings
# for x1 and the variables that interact with x1.


## Not run: 
coef(fit)['x1']                            # method 1
diff(predict(fit, gendata(x1=c(0,1))))     # method 2
g &lt;- Function(fit)                         # method 3
g(x1=1) - g(x1=0)
summary(fit, x1=c(0,1))                    # method 4
k &lt;- contrast(fit, list(x1=1), list(x1=0)) # method 5
print(k, X=TRUE)
fit &lt;- update(fit, x=TRUE, y=TRUE)         # method 6
b &lt;- bootcov(fit, B=500)
contrast(fit, list(x1=1), list(x1=0))


# In a model containing age, race, and sex,
# compute an estimate of the mean response for a
# 50 year old male, averaged over the races using
# observed frequencies for the races as weights


f &lt;- ols(y ~ age + race + sex)
contrast(f, list(age=50, sex='male', race=levels(race)),
         type='average', weights=table(race))

# For a Bayesian model get the highest posterior interval for the
# difference in two nonlinear functions of predicted values
# Start with the mean from a proportional odds model
g &lt;- blrm(y ~ x)
M &lt;- Mean(g)
contrast(g, list(x=1), list(x=0), fun=M)

# For the median we have to make sure that contrast can pass the
# per-posterior-draw vector of intercepts through
qu &lt;- Quantile(g)
med &lt;- function(lp, intercepts) qu(0.5, lp, intercepts=intercepts)
contrast(g, list(x=1), list(x=0), fun=med)

## End(Not run)


# Plot the treatment effect (drug - placebo) as a function of age
# and sex in a model in which age nonlinearly interacts with treatment
# for females only

set.seed(1)
n &lt;- 800
treat &lt;- factor(sample(c('drug','placebo'), n,TRUE))
sex   &lt;- factor(sample(c('female','male'),  n,TRUE))
age   &lt;- rnorm(n, 50, 10)
y     &lt;- .05*age + (sex=='female')*(treat=='drug')*.05*abs(age-50) + rnorm(n)
f     &lt;- ols(y ~ rcs(age,4)*treat*sex)
d     &lt;- datadist(age, treat, sex); options(datadist='d')

# show separate estimates by treatment and sex

require(ggplot2)
ggplot(Predict(f, age, treat, sex='female'))
ggplot(Predict(f, age, treat, sex='male'))
ages  &lt;- seq(35,65,by=5); sexes &lt;- c('female','male')
w     &lt;- contrast(f, list(treat='drug',    age=ages, sex=sexes),
                     list(treat='placebo', age=ages, sex=sexes))
# add conf.type="simultaneous" to adjust for having done 14 contrasts
xYplot(Cbind(Contrast, Lower, Upper) ~ age | sex, data=w,
       ylab='Drug - Placebo')
w &lt;- as.data.frame(w[c('age','sex','Contrast','Lower','Upper')])
ggplot(w, aes(x=age, y=Contrast)) + geom_point() + facet_grid(sex ~ .) +
   geom_errorbar(aes(ymin=Lower, ymax=Upper), width=0)
ggplot(w, aes(x=age, y=Contrast)) + geom_line() + facet_grid(sex ~ .) +
   geom_ribbon(aes(ymin=Lower, ymax=Upper), width=0, alpha=0.15, linetype=0)
xYplot(Cbind(Contrast, Lower, Upper) ~ age, groups=sex, data=w,
       ylab='Drug - Placebo', method='alt bars')
options(datadist=NULL)


# Examples of type='joint' contrast tests

set.seed(1)
x1 &lt;- rnorm(100)
x2 &lt;- factor(sample(c('a','b','c'), 100, TRUE))
dd &lt;- datadist(x1, x2); options(datadist='dd')
y  &lt;- x1 + (x2=='b') + rnorm(100)

# First replicate a test statistic from anova()

f &lt;- ols(y ~ x2)
anova(f)
contrast(f, list(x2=c('b','c')), list(x2='a'), type='joint')

# Repeat with a redundancy; compare a vs b, a vs c, b vs c

contrast(f, list(x2=c('a','a','b')), list(x2=c('b','c','c')), type='joint')

# Get a test of association of a continuous predictor with y
# First assume linearity, then cubic

f &lt;- lrm(y&gt;0 ~ x1 + x2)
anova(f)
contrast(f, list(x1=1), list(x1=0), type='joint')  # a minimum set of contrasts
xs &lt;- seq(-2, 2, length=20)
contrast(f, list(x1=0), list(x1=xs), type='joint')

# All contrasts were redundant except for the first, because of
# linearity assumption

f &lt;- lrm(y&gt;0 ~ pol(x1,3) + x2)
anova(f)
contrast(f, list(x1=0), list(x1=xs), type='joint')
print(contrast(f, list(x1=0), list(x1=xs), type='joint'), jointonly=TRUE)

# All contrasts were redundant except for the first 3, because of
# cubic regression assumption

# Now do something that is difficult to do without cryptic contrast
# matrix operations: Allow each of the three x2 groups to have a different
# shape for the x1 effect where x1 is quadratic.  Test whether there is
# a difference in mean levels of y for x2='b' vs. 'c' or whether
# the shape or slope of x1 is different between x2='b' and x2='c' regardless
# of how they differ when x2='a'.  In other words, test whether the mean
# response differs between group b and c at any value of x1.
# This is a 3 d.f. test (intercept, linear, quadratic effects) and is
# a better approach than subsetting the data to remove x2='a' then
# fitting a simpler model, as it uses a better estimate of sigma from
# all the data.

f &lt;- ols(y ~ pol(x1,2) * x2)
anova(f)
contrast(f, list(x1=xs, x2='b'),
            list(x1=xs, x2='c'), type='joint')

# Note: If using a spline fit, there should be at least one value of
# x1 between any two knots and beyond the outer knots.
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='cph'>Cox Proportional Hazards Model and Extensions</h2><span id='topic+cph'></span><span id='topic+Survival.cph'></span><span id='topic+Quantile.cph'></span><span id='topic+Mean.cph'></span>

<h3>Description</h3>

<p>Modification of Therneau's <code>coxph</code> function to fit the Cox model and
its extension, the Andersen-Gill model. The latter allows for interval
time-dependent covariables, time-dependent strata, and repeated events.
The <code>Survival</code> method for an object created by <code>cph</code> returns an S
function for computing estimates of the survival function.
The <code>Quantile</code> method for <code>cph</code> returns an S function for computing
quantiles of survival time (median, by default).
The <code>Mean</code> method returns a function for computing the mean survival
time.  This function issues a warning if the last follow-up time is uncensored,
unless a restricted mean is explicitly requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cph(formula = formula(data), data=environment(formula),
    weights, subset, na.action=na.delete, 
    method=c("efron","breslow","exact","model.frame","model.matrix"), 
    singular.ok=FALSE, robust=FALSE,
    model=FALSE, x=FALSE, y=FALSE, se.fit=FALSE,
    linear.predictors=TRUE, residuals=TRUE, nonames=FALSE,
    eps=1e-4, init, iter.max=10, tol=1e-9, surv=FALSE, time.inc,
    type=NULL, vartype=NULL, debug=FALSE, ...)

## S3 method for class 'cph'
Survival(object, ...)
# Evaluate result as g(times, lp, stratum=1, type=c("step","polygon"))

## S3 method for class 'cph'
Quantile(object, ...)
# Evaluate like h(q, lp, stratum=1, type=c("step","polygon"))

## S3 method for class 'cph'
Mean(object, method=c("exact","approximate"), type=c("step","polygon"),
          n=75, tmax, ...)
# E.g. m(lp, stratum=1, type=c("step","polygon"), tmax, \dots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cph_+3A_formula">formula</code></td>
<td>

<p>an S formula object with a <code>Surv</code> object on the
left-hand side.  The <code>terms</code> can specify any S model
formula with up to third-order interactions.  The <code>strat</code> 
function may appear in the terms, as a main effect or an interacting
factor.  To stratify on both race and sex, you would include both
terms <code>strat(race)</code> and <code>strat(sex)</code>.  Stratification
factors may interact with non-stratification factors;
not all stratification terms need interact with the same modeled
factors.
</p>
</td></tr>
<tr><td><code id="cph_+3A_object">object</code></td>
<td>

<p>an object created by <code>cph</code> with <code>surv=TRUE</code>
</p>
</td></tr>
<tr><td><code id="cph_+3A_data">data</code></td>
<td>

<p>name of an S data frame containing all needed variables.  Omit this to use a
data frame already in the S &ldquo;search list&rdquo;.
</p>
</td></tr>
<tr><td><code id="cph_+3A_weights">weights</code></td>
<td>

<p>case weights
</p>
</td></tr>
<tr><td><code id="cph_+3A_subset">subset</code></td>
<td>

<p>an expression defining a subset of the observations to use in the fit.  The default
is to use all observations.  Specify for example <code>age&gt;50 &amp; sex="male"</code> or
<code>c(1:100,200:300)</code>
respectively to use the observations satisfying a logical expression or those having
row numbers in the given vector.
</p>
</td></tr>
<tr><td><code id="cph_+3A_na.action">na.action</code></td>
<td>

<p>specifies an S function to handle missing data.  The default is the function <code>na.delete</code>,
which causes observations with any variable missing to be deleted.  The main difference
between <code>na.delete</code> and the S-supplied function <code>na.omit</code> is that 
<code>na.delete</code> makes a list
of the number of observations that are missing on each variable in the model.
The <code>na.action</code> is usally specified by e.g. <code>options(na.action="na.delete")</code>.
</p>
</td></tr>
<tr><td><code id="cph_+3A_method">method</code></td>
<td>

<p>for <code>cph</code>, specifies a particular fitting method, <code>"model.frame"</code> instead to return the model frame
of the predictor and response variables satisfying any subset or missing value
checks, or <code>"model.matrix"</code> to return the expanded design matrix.
The default is <code>"efron"</code>, to use Efron's likelihood for fitting the
model.
</p>
<p>For <code>Mean.cph</code>, <code>method</code> is <code>"exact"</code> to use numerical
integration of the 
survival function at any linear predictor value to obtain a mean survival
time.  Specify <code>method="approximate"</code> to use an approximate method that is
slower when <code>Mean.cph</code> is executing but then is essentially instant
thereafter.  For the approximate method, the area is computed for <code>n</code>
points equally spaced between the min and max observed linear predictor
values.  This calculation is done separately for each stratum.  Then the
<code>n</code> pairs (X beta, area) are saved in the generated S function, and when
this function is evaluated, the <code>approx</code> function is used to evaluate
the mean for any given linear predictor values, using linear interpolation
over the <code>n</code> X beta values.
</p>
</td></tr>
<tr><td><code id="cph_+3A_singular.ok">singular.ok</code></td>
<td>

<p>If <code>TRUE</code>, the program will automatically skip over columns of the X matrix
that are linear combinations of earlier columns.  In this case the
coefficients for such columns will be NA, and the variance matrix will contain
zeros.  For ancillary calculations, such as the linear predictor, the missing
coefficients are treated as zeros.  The singularities will prevent many of
the features of the <code>rms</code> library from working.
</p>
</td></tr>
<tr><td><code id="cph_+3A_robust">robust</code></td>
<td>

<p>if <code>TRUE</code> a robust variance estimate is returned.  Default is <code>TRUE</code> if the
model includes a <code>cluster()</code> operative, <code>FALSE</code> otherwise.
</p>
</td></tr>
<tr><td><code id="cph_+3A_model">model</code></td>
<td>

<p>default is <code>FALSE</code>(false).  Set to <code>TRUE</code> to return the model frame as element 
<code>model</code> of the fit object.
</p>
</td></tr>
<tr><td><code id="cph_+3A_x">x</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to return the expanded design matrix as element <code>x</code>
(without intercept indicators) of the
returned fit object.
</p>
</td></tr>
<tr><td><code id="cph_+3A_y">y</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to return the vector of
response values (<code>Surv</code> object) as element <code>y</code> of the
fit.
</p>
</td></tr>
<tr><td><code id="cph_+3A_se.fit">se.fit</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to compute the estimated standard errors of
the estimate of X beta and store them in element <code>se.fit</code>
of the fit.  The predictors are first centered to their means
before computing the standard errors.
</p>
</td></tr>
<tr><td><code id="cph_+3A_linear.predictors">linear.predictors</code></td>
<td>
<p>set to <code>FALSE</code> to omit
<code>linear.predictors</code> vector from fit</p>
</td></tr>
<tr><td><code id="cph_+3A_residuals">residuals</code></td>
<td>
<p>set to <code>FALSE</code> to omit <code>residuals</code> vector
from fit</p>
</td></tr>
<tr><td><code id="cph_+3A_nonames">nonames</code></td>
<td>
<p>set to <code>TRUE</code> to not set <code>names</code> attribute
for <code>linear.predictors</code>, <code>residuals</code>, <code>se.fit</code>, and
rows of design matrix</p>
</td></tr>
<tr><td><code id="cph_+3A_eps">eps</code></td>
<td>

<p>convergence criterion - change in log likelihood.
</p>
</td></tr>
<tr><td><code id="cph_+3A_init">init</code></td>
<td>

<p>vector of initial parameter estimates.  Defaults to all zeros.
Special residuals can be obtained by setting some elements of <code>init</code>
to MLEs and others to zero and specifying <code>iter.max=1</code>.
</p>
</td></tr>
<tr><td><code id="cph_+3A_iter.max">iter.max</code></td>
<td>

<p>maximum number of iterations to allow.  Set to <code>0</code> to obtain certain
null-model residuals.
</p>
</td></tr>
<tr><td><code id="cph_+3A_tol">tol</code></td>
<td>

<p>tolerance for declaring singularity for matrix inversion (available
only when survival5 or later package is in effect)
</p>
</td></tr>
<tr><td><code id="cph_+3A_surv">surv</code></td>
<td>

<p>set to <code>TRUE</code> to compute underlying survival estimates for each
stratum, and to store these along with standard errors of log Lambda(t),
<code>maxtime</code> (maximum observed survival or censoring time),
and <code>surv.summary</code> in the returned object.  Set <code>surv="summary"</code>
to only compute and store <code>surv.summary</code>, not survival estimates
at each unique uncensored failure time. If you specify <code>x=TRUE</code>
and <code>y=TRUE</code>, 
you can obtain predicted survival later, with accurate confidence
intervals for any set of predictor values. The standard error information
stored as a result of <code>surv=TRUE</code> are only accurate at the mean of all
predictors. If the model has no covariables, these are of course OK.
The main reason for using <code>surv</code> is to greatly speed up the computation
of predicted survival probabilities as a function of the covariables,
when accurate confidence intervals are not needed.
</p>
</td></tr>
<tr><td><code id="cph_+3A_time.inc">time.inc</code></td>
<td>

<p>time increment used in deriving <code>surv.summary</code>.  Survival,
number at risk, and standard error will be stored for 
<code>t=0, time.inc, 2 time.inc, ..., maxtime</code>,
where <code>maxtime</code> is the maximum survival time over all strata.
<code>time.inc</code> is also used in constructing the time axis in the
<code>survplot</code> function (see below).  The default value for
<code>time.inc</code> is 30 if <code>units(ftime) = "Day"</code> or no <code>units</code>
attribute has been attached to the survival time variable.  If
<code>units(ftime)</code> is a word other than <code>"Day"</code>, the default
for <code>time.inc</code> is 1 when it is omitted, unless <code>maxtime&lt;1</code>, then
<code>maxtime/10</code> is used as <code>time.inc</code>.  If <code>time.inc</code> is not given and
<code>maxtime/ default time.inc</code> &gt; 25, <code>time.inc</code> is increased.
</p>
</td></tr>
<tr><td><code id="cph_+3A_type">type</code></td>
<td>

<p>(for <code>cph</code>) applies if <code>surv</code> is <code>TRUE</code> or <code>"summary"</code>. 
If <code>type</code> is omitted, the method consistent with <code>method</code> is used.
See <code>survfit.coxph</code> (under <code>survfit</code>) or <code>survfit.cph</code> for details and for the
definitions of values of <code>type</code>
</p>
<p>For <code>Survival, Quantile, Mean</code> set to <code>"polygon"</code> to use linear 
interpolation instead of the usual step function.  For <code>Mean</code>, the default
of <code>step</code> will yield the sample mean in the case of no censoring and no
covariables, if <code>type="kaplan-meier"</code> was specified to <code>cph</code>.
For <code>method="exact"</code>, the value of <code>type</code> is passed to the
generated function, and it can be overridden when that function is
actually invoked. For <code>method="approximate"</code>, <code>Mean.cph</code>
generates the function different ways according to <code>type</code>, and this
cannot be changed when the function is actually invoked.
</p>
</td></tr>
<tr><td><code id="cph_+3A_vartype">vartype</code></td>
<td>
<p>see <code>survfit.coxph</code></p>
</td></tr>
<tr><td><code id="cph_+3A_debug">debug</code></td>
<td>
<p>set to <code>TRUE</code> to print debugging information related
to model matrix construction.  You can also use <code>options(debug=TRUE)</code>.</p>
</td></tr>
<tr><td><code id="cph_+3A_...">...</code></td>
<td>

<p>other arguments passed to <code>coxph.fit</code> from <code>cph</code>.  Ignored by
other functions.
</p>
</td></tr>
<tr><td><code id="cph_+3A_times">times</code></td>
<td>

<p>a scalar or vector of times at which to evaluate the survival estimates
</p>
</td></tr>
<tr><td><code id="cph_+3A_lp">lp</code></td>
<td>

<p>a scalar or vector of linear predictors (including the centering constant)
at which to evaluate the survival estimates
</p>
</td></tr>
<tr><td><code id="cph_+3A_stratum">stratum</code></td>
<td>

<p>a scalar stratum number or name (e.g., <code>"sex=male"</code>) to use in getting
survival probabilities
</p>
</td></tr>
<tr><td><code id="cph_+3A_q">q</code></td>
<td>

<p>a scalar quantile or a vector of quantiles to compute
</p>
</td></tr>
<tr><td><code id="cph_+3A_n">n</code></td>
<td>

<p>the number of points at which to evaluate the mean survival time, for
<code>method="approximate"</code> in <code>Mean.cph</code>.
</p>
</td></tr>
<tr><td><code id="cph_+3A_tmax">tmax</code></td>
<td>

<p>For <code>Mean.cph</code>, the default is to compute the overall mean (and produce
a warning message if there is censoring at the end of follow-up).
To compute a restricted mean life length, specify the truncation point as <code>tmax</code>.
For <code>method="exact"</code>, <code>tmax</code> is passed to the generated function and it
may be overridden when that function is invoked.  For <code>method="approximate"</code>,
<code>tmax</code> must be specified at the time that <code>Mean.cph</code> is run.
</p>
</td></tr></table>


<h3>Details</h3>

<p>If there is any strata by covariable interaction in the model such that
the mean X beta varies greatly over strata, <code>method="approximate"</code> may
not yield very accurate estimates of the mean in <code>Mean.cph</code>.
</p>
<p>For <code>method="approximate"</code> if you ask for an estimate of the mean for
a linear predictor value that was outside the range of linear predictors
stored with the fit, the mean for that observation will be <code>NA</code>.
</p>


<h3>Value</h3>

<p>For <code>Survival</code>, <code>Quantile</code>, or <code>Mean</code>, an S function is returned.  Otherwise,
in addition to what is listed below, formula/design information and
the components 
<code>maxtime, time.inc, units, model, x, y, se.fit</code> are stored, the last 5 
depending on the settings of options by the same names.
The vectors or matrix stored if <code>y=TRUE</code> or <code>x=TRUE</code> have rows deleted according to <code>subset</code> and
to missing data, and have names or row names that come from the
data frame used as input data.
</p>
<table>
<tr><td><code>n</code></td>
<td>

<p>table with one row per stratum containing number of censored and uncensored observations
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>vector of regression coefficients
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>vector containing the named elements <code>Obs</code>, <code>Events</code>, <code>Model L.R.</code>, <code>d.f.</code>,
<code>P</code>, <code>Score</code>, <code>Score P</code>, <code>R2</code>, Somers'
<code>Dxy</code>, <code>g</code>-index, 
and <code>gr</code>, the <code>g</code>-index on the hazard ratio scale.
<code>R2</code> is the Nagelkerke R-squared, with division by the maximum
attainable R-squared.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>variance/covariance matrix of coefficients
</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>

<p>values of predicted X beta for observations used in fit, normalized
to have overall mean zero, then having any offsets added
</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>

<p>martingale residuals
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>log likelihood at initial and final parameter values
</p>
</td></tr>
<tr><td><code>score</code></td>
<td>

<p>value of score statistic at initial values of parameters
</p>
</td></tr>
<tr><td><code>times</code></td>
<td>

<p>lists of times (if <code>surv="T"</code>)
</p>
</td></tr>
<tr><td><code>surv</code></td>
<td>

<p>lists of underlying survival probability estimates
</p>
</td></tr>
<tr><td><code>std.err</code></td>
<td>

<p>lists of standard errors of estimate log-log survival
</p>
</td></tr>
<tr><td><code>surv.summary</code></td>
<td>

<p>a 3 dimensional array if <code>surv=TRUE</code>.  
The first dimension is time ranging from 0 to
<code>maxtime</code> by <code>time.inc</code>.  The second dimension refers to strata.
The third dimension contains the time-oriented matrix with
<code>Survival, n.risk</code> (number of subjects at risk), 
and <code>std.err</code> (standard error of log-log
survival). 
</p>
</td></tr>
<tr><td><code>center</code></td>
<td>

<p>centering constant, equal to overall mean of X beta.
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="survival.html#topic+survival-internal">survival-internal</a></code>,
<code><a href="survival.html#topic+Surv">Surv</a></code>, <code><a href="#topic+residuals.cph">residuals.cph</a></code>,
<code><a href="survival.html#topic+cox.zph">cox.zph</a></code>, <code><a href="#topic+survfit.cph">survfit.cph</a></code>,
<code><a href="#topic+survest.cph">survest.cph</a></code>, <code><a href="survival.html#topic+survfit.coxph">survfit.coxph</a></code>, 
<code><a href="#topic+survplot">survplot</a></code>, <code><a href="#topic+datadist">datadist</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+Predict">Predict</a></code>, 
<code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+plot.Predict">plot.Predict</a></code>, <code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code><a href="#topic+specs.rms">specs.rms</a></code>, <code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+which.influence">which.influence</a></code>,
<code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>,
<code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>,  <code><a href="#topic+print.cph">print.cph</a></code>,
<code><a href="#topic+latex.cph">latex.cph</a></code>, <code><a href="#topic+vif">vif</a></code>, <code><a href="#topic+ie.setup">ie.setup</a></code>,
<code><a href="Hmisc.html#topic+GiniMd">GiniMd</a></code>, <code><a href="#topic+dxy.cens">dxy.cens</a></code>,
<code><a href="survival.html#topic+survConcordance">survConcordance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from a population model in which the log hazard
# function is linear in age and there is no age x sex interaction

require(survival)
require(ggplot2)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, 
              rep=TRUE, prob=c(.6, .4)))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
dt &lt;- -log(runif(n))/h
label(dt) &lt;- 'Follow-up Time'
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
dd &lt;- datadist(age, sex)
options(datadist='dd')
S &lt;- Surv(dt,e)

f &lt;- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE)
cox.zph(f, "rank")             # tests of PH
anova(f)
ggplot(Predict(f, age, sex)) # plot age effect, 2 curves for 2 sexes
survplot(f, sex)             # time on x-axis, curves for x2
res &lt;- resid(f, "scaledsch")
time &lt;- as.numeric(dimnames(res)[[1]])
z &lt;- loess(res[,4] ~ time, span=0.50)   # residuals for sex
plot(time, fitted(z))
lines(supsmu(time, res[,4]),lty=2)
plot(cox.zph(f,"identity"))    #Easier approach for last few lines
# latex(f)


f &lt;- cph(S ~ age + strat(sex), surv=TRUE)
g &lt;- Survival(f)   # g is a function
g(seq(.1,1,by=.1), stratum="sex=Male", type="poly") #could use stratum=2
med &lt;- Quantile(f)
plot(Predict(f, age, fun=function(x) med(lp=x)))  #plot median survival

# Fit a model that is quadratic in age, interacting with sex as strata
# Compare standard errors of linear predictor values with those from
# coxph
# Use more stringent convergence criteria to match with coxph

f &lt;- cph(S ~ pol(age,2)*strat(sex), x=TRUE, eps=1e-9, iter.max=20)
coef(f)
se &lt;- predict(f, se.fit=TRUE)$se.fit
require(lattice)
xyplot(se ~ age | sex, main='From cph')
a &lt;- c(30,50,70)
comb &lt;- data.frame(age=rep(a, each=2),
                   sex=rep(levels(sex), 3))

p &lt;- predict(f, comb, se.fit=TRUE)
comb$yhat  &lt;- p$linear.predictors
comb$se    &lt;- p$se.fit
z &lt;- qnorm(.975)
comb$lower &lt;- p$linear.predictors - z*p$se.fit
comb$upper &lt;- p$linear.predictors + z*p$se.fit
comb

age2 &lt;- age^2
f2 &lt;- coxph(S ~ (age + age2)*strata(sex))
coef(f2)
se &lt;- predict(f2, se.fit=TRUE)$se.fit
xyplot(se ~ age | sex, main='From coxph')
comb &lt;- data.frame(age=rep(a, each=2), age2=rep(a, each=2)^2,
                   sex=rep(levels(sex), 3))
p &lt;- predict(f2, newdata=comb, se.fit=TRUE)
comb$yhat &lt;- p$fit
comb$se   &lt;- p$se.fit
comb$lower &lt;- p$fit - z*p$se.fit
comb$upper &lt;- p$fit + z*p$se.fit
comb


# g &lt;- cph(Surv(hospital.charges) ~ age, surv=TRUE)
# Cox model very useful for analyzing highly skewed data, censored or not
# m &lt;- Mean(g)
# m(0)                           # Predicted mean charge for reference age


#Fit a time-dependent covariable representing the instantaneous effect
#of an intervening non-fatal event
rm(age)
set.seed(121)
dframe &lt;- data.frame(failure.time=1:10, event=rep(0:1,5),
                     ie.time=c(NA,1.5,2.5,NA,3,4,NA,5,5,5), 
                     age=sample(40:80,10,rep=TRUE))
z &lt;- ie.setup(dframe$failure.time, dframe$event, dframe$ie.time)
S &lt;- z$S
ie.status &lt;- z$ie.status
attach(dframe[z$subs,])    # replicates all variables

f &lt;- cph(S ~ age + ie.status, x=TRUE, y=TRUE)  
#Must use x=TRUE,y=TRUE to get survival curves with time-dep. covariables


#Get estimated survival curve for a 50-year old who has an intervening
#non-fatal event at 5 days
new &lt;- data.frame(S=Surv(c(0,5), c(5,999), c(FALSE,FALSE)), age=rep(50,2),
                  ie.status=c(0,1))
g &lt;- survfit(f, new)
plot(c(0,g$time), c(1,g$surv[,2]), type='s', 
     xlab='Days', ylab='Survival Prob.')
# Not certain about what columns represent in g$surv for survival5
# but appears to be for different ie.status
#or:
#g &lt;- survest(f, new)
#plot(g$time, g$surv, type='s', xlab='Days', ylab='Survival Prob.')


#Compare with estimates when there is no intervening event
new2 &lt;- data.frame(S=Surv(c(0,5), c(5, 999), c(FALSE,FALSE)), age=rep(50,2),
                   ie.status=c(0,0))
g2 &lt;- survfit(f, new2)
lines(c(0,g2$time), c(1,g2$surv[,2]), type='s', lty=2)
#or:
#g2 &lt;- survest(f, new2)
#lines(g2$time, g2$surv, type='s', lty=2)
detach("dframe[z$subs, ]")
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='cr.setup'>Continuation Ratio Ordinal Logistic Setup</h2><span id='topic+cr.setup'></span>

<h3>Description</h3>

<p>Creates several new variables which help set up a dataset with an
ordinal response variable <code class="reqn">y</code> for use in fitting a forward continuation
ratio (CR) model.  The CR model can be fitted with binary logistic
regression if each input observation is replicated the proper
number of times according to the <code class="reqn">y</code> value, a new binary <code class="reqn">y</code>
is computed that has at most one <code class="reqn">y=1</code> per subject,
and if a <code>cohort</code> variable is used to define the current
qualifying condition for a cohort of subjects, e.g., <code class="reqn">y\geq 2</code>.
<code>cr.setup</code> creates the needed auxilliary variables. See
<code>predab.resample</code> and <code>validate.lrm</code> for information about
validating CR models (e.g., using the bootstrap to sample with
replacement from the original subjects instead of the records used in
the fit, validating the model separately for user-specified values of
<code>cohort</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cr.setup(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cr.setup_+3A_y">y</code></td>
<td>

<p>a character, numeric, <code>category</code>, or <code>factor</code> vector containing values of
the response variable.  For <code>category</code> or <code>factor</code> variables, the
<code>levels</code> of the variable are assumed to be listed in an ordinal way.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a list with components <code>y, cohort, subs, reps</code>.  <code>y</code> is a new binary
variable that is to be used in the binary logistic fit.  <code>cohort</code> is 
a <code>factor</code> vector specifying which cohort condition currently applies.
<code>subs</code> is a vector of subscripts that can be used to replicate other
variables the same way <code>y</code> was replicated.  <code>reps</code> specifies how many
times each original observation was replicated.  <code>y, cohort, subs</code> are
all the same length and are longer than the original <code>y</code> vector.
<code>reps</code> is the same length as the original <code>y</code> vector.
The <code>subs</code> vector is suitable for passing to <code>validate.lrm</code> or <code>calibrate</code>,
which pass this vector under the name <code>cluster</code> on to <code>predab.resample</code> so that bootstrapping can be
done by sampling with replacement from the original subjects rather than
from the individual records created by <code>cr.setup</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Berridge DM, Whitehead J: Analysis of failure time data with ordinal
categories of response.  Stat in Med 10:1703&ndash;1710, 1991.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(NA, 10, 21, 32, 32)
cr.setup(y)


set.seed(171)
y &lt;- sample(0:2, 100, rep=TRUE)
sex &lt;- sample(c("f","m"),100,rep=TRUE)
sex &lt;- factor(sex)
table(sex, y)
options(digits=5)
tapply(y==0, sex, mean)
tapply(y==1, sex, mean)
tapply(y==2, sex, mean)
cohort &lt;- y&gt;=1
tapply(y[cohort]==1, sex[cohort], mean)

u &lt;- cr.setup(y)
Y &lt;- u$y
cohort &lt;- u$cohort
sex &lt;- sex[u$subs]

lrm(Y ~ cohort + sex)
 
f &lt;- lrm(Y ~ cohort*sex)   # saturated model - has to fit all data cells
f

#Prob(y=0|female):
# plogis(-.50078)
#Prob(y=0|male):
# plogis(-.50078+.11301)
#Prob(y=1|y&gt;=1, female):
plogis(-.50078+.31845)
#Prob(y=1|y&gt;=1, male):
plogis(-.50078+.31845+.11301-.07379)

combinations &lt;- expand.grid(cohort=levels(cohort), sex=levels(sex))
combinations
p &lt;- predict(f, combinations, type="fitted")
p
p0 &lt;- p[c(1,3)]
p1 &lt;- p[c(2,4)]
p1.unconditional &lt;- (1 - p0) *p1
p1.unconditional
p2.unconditional &lt;- 1 - p0 - p1.unconditional
p2.unconditional


## Not run: 
dd &lt;- datadist(inputdata)   # do this on non-replicated data
options(datadist='dd')
pain.severity &lt;- inputdata$pain.severity
u &lt;- cr.setup(pain.severity)
# inputdata frame has age, sex with pain.severity
attach(inputdata[u$subs,])  # replicate age, sex
# If age, sex already available, could do age &lt;- age[u$subs] etc., or
# age &lt;- rep(age, u$reps), etc.
y      &lt;- u$y
cohort &lt;- u$cohort
dd     &lt;- datadist(dd, cohort)       # add to dd
f &lt;- lrm(y ~ cohort + age*sex)       # ordinary cont. ratio model
g &lt;- lrm(y ~ cohort*sex + age, x=TRUE,y=TRUE) # allow unequal slopes for
                                     # sex across cutoffs
cal &lt;- calibrate(g, cluster=u$subs, subset=cohort=='all')  
# subs makes bootstrap sample the correct units, subset causes
# Predicted Prob(pain.severity=0) to be checked for calibration

## End(Not run)
</code></pre>

<hr>
<h2 id='datadist'>
Distribution Summaries for Predictor Variables
</h2><span id='topic+datadist'></span><span id='topic+print.datadist'></span>

<h3>Description</h3>

<p>For a given set of variables or a data frame, determines summaries
of variables for effect and plotting ranges, values to adjust to,
and overall ranges
for <code>Predict</code>, <code>plot.Predict</code>, <code>ggplot.Predict</code>,
<code>summary.rms</code>, <code>survplot</code>, and <code>nomogram.rms</code>.
If <code>datadist</code> is called before
a model fit and the resulting object pointed to with <code>options(datadist="name")</code>,
the data characteristics will be stored with the fit by <code>Design()</code>, so
that later predictions and summaries of the fit will not need to access
the original data used in the fit.  Alternatively, you can specify the
values for each variable in the model when using these 3 functions, or
specify the values of some of them and let the functions look up the
remainder (of say adjustmemt levels) from an object created by <code>datadist</code>.
The best method is probably to run <code>datadist</code> once before any models are
fitted, storing the distribution summaries for all potential variables.
Adjustment values are <code>0</code> for binary variables, the most frequent
category (or optionally the first category level)
for categorical (<code>factor</code>) variables, the middle level for 
<code>ordered factor</code> variables, and medians for continuous variables.
See descriptions of <code>q.display</code> and <code>q.effect</code> for how display and
effect ranges are chosen for continuous variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>datadist(..., data, q.display, q.effect=c(0.25, 0.75),
         adjto.cat=c('mode','first'), n.unique=10)

## S3 method for class 'datadist'
print(x, ...)
# options(datadist="dd")
# used by summary, plot, survplot, sometimes predict
# For dd substitute the name of the result of datadist
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="datadist_+3A_...">...</code></td>
<td>

<p>a list of variable names, separated by commas, a single data frame, or
a fit with <code>Design</code> information.  The first element in this list may
also be an object created by an earlier call to <code>datadist</code>; then
the later variables are added to this <code>datadist</code> object.
For a fit object, the variables named
in the fit are retrieved from the active data frame or from the location
pointed to by <code>data=frame number</code> or <code>data="data frame name"</code>.
For <code>print</code>, is ignored.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_data">data</code></td>
<td>

<p>a data frame or a search position.  If <code>data</code> is a search position,
it is assumed that a data frame is attached in that position, and all
its variables are used.  If you specify both individual variables in
<code>...</code> and <code>data</code>, the two sets of variables are combined.  Unless the
first argument is a fit object, <code>data</code> must be an integer.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_q.display">q.display</code></td>
<td>

<p>set of two quantiles for computing the range of continuous variables
to use in displaying regression relationships.  Defaults are
<code class="reqn">q</code> and <code class="reqn">1-q</code>, where <code class="reqn">q=10/max(n,200)</code>, and <code class="reqn">n</code> is the
number of 
non-missing observations.  Thus for <code class="reqn">n&lt;200</code>, the .05 and .95 quantiles
are used.  For <code class="reqn">n\geq 200</code>, the <code class="reqn">10^{th}</code> smallest and
<code class="reqn">10^{th}</code> largest values are used.  If you specify <code>q.display</code>,
those quantiles are used whether or not <code class="reqn">n&lt;200</code>.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_q.effect">q.effect</code></td>
<td>

<p>set of two quantiles for computing the range of continuous variables
to use in estimating regression effects.  Defaults are c(.25,.75),
which yields inter-quartile-range odds ratios, etc.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_adjto.cat">adjto.cat</code></td>
<td>

<p>default is <code>"mode"</code>, indicating that the modal (most frequent) category
for categorical (factor) variables is the adjust-to setting.
Specify <code>"first"</code> to use the first level of factor variables as the
adjustment values.  In the case of many levels having the maximum
frequency, the first such level is used for <code>"mode"</code>.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_n.unique">n.unique</code></td>
<td>

<p>variables having <code>n.unique</code> or fewer unique values are considered
to be discrete variables in that their unique values are stored in the
<code>values</code> list.  This will affect how functions such as
<code>nomogram.Design</code> determine whether variables are discrete or not.
</p>
</td></tr>
<tr><td><code id="datadist_+3A_x">x</code></td>
<td>
<p>result of <code>datadist</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For categorical variables, the 7 limits are set to character strings
(factors) which correspond to
<code>c(NA,adjto.level,NA,1,k,1,k)</code>, where <code>k</code> is the number of levels.
For ordered variables with numeric levels, the limits are set to
<code>c(L,M,H,L,H,L,H)</code>, where <code>L</code> is the lowest level, <code>M</code> is the middle
level, and <code>H</code> is the highest level.
</p>


<h3>Value</h3>

<p>a list of class <code>"datadist"</code> with the following components
</p>
<table>
<tr><td><code>limits</code></td>
<td>

<p>a <code class="reqn">7 \times k</code> vector, where <code class="reqn">k</code> is the number of variables.
The 7 rows correspond to the low value for estimating the effect of
the variable, the value to adjust the variable to when examining
other variables, the high value for effect, low value for displaying
the variable, the high value for displaying it, and the overall lowest
and highest values.
</p>
</td></tr>
<tr><td><code>values</code></td>
<td>

<p>a named list, with one vector of unique values for each numeric
variable having no more than <code>n.unique</code> unique values
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="Hmisc.html#topic+describe">describe</a></code>, <code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
d &lt;- datadist(data=1)         # use all variables in search pos. 1
d &lt;- datadist(x1, x2, x3)
page(d)                       # if your options(pager) leaves up a pop-up
                              # window, this is a useful guide in analyses
d &lt;- datadist(data=2)         # all variables in search pos. 2
d &lt;- datadist(data=my.data.frame)
d &lt;- datadist(my.data.frame)  # same as previous.  Run for all potential vars.
d &lt;- datadist(x2, x3, data=my.data.frame)   # combine variables
d &lt;- datadist(x2, x3, q.effect=c(.1,.9), q.display=c(0,1))
# uses inter-decile range odds ratios,
# total range of variables for regression function plots
d &lt;- datadist(d, z)           # add a new variable to an existing datadist
options(datadist="d")         #often a good idea, to store info with fit
f &lt;- ols(y ~ x1*x2*x3)


options(datadist=NULL)        #default at start of session
f &lt;- ols(y ~ x1*x2)
d &lt;- datadist(f)              #info not stored in `f'
d$limits["Adjust to","x1"] &lt;- .5   #reset adjustment level to .5
options(datadist="d")


f &lt;- lrm(y ~ x1*x2, data=mydata)
d &lt;- datadist(f, data=mydata)
options(datadist="d")


f &lt;- lrm(y ~ x1*x2)           #datadist not used - specify all values for
summary(f, x1=c(200,500,800), x2=c(1,3,5))         # obtaining predictions
plot(Predict(f, x1=200:800, x2=3))  # or ggplot()


# Change reference value to get a relative odds plot for a logistic model
d$limits$age[2] &lt;- 30    # make 30 the reference value for age
# Could also do: d$limits["Adjust to","age"] &lt;- 30
fit &lt;- update(fit)   # make new reference value take effect
plot(Predict(fit, age, ref.zero=TRUE, fun=exp),
     ylab='Age=x:Age=30 Odds Ratio')   # or ggplot()

## End(Not run)
</code></pre>

<hr>
<h2 id='ExProb'>Function Generator For Exceedance Probabilities</h2><span id='topic+ExProb'></span><span id='topic+ExProb.orm'></span><span id='topic+plot.ExProb'></span>

<h3>Description</h3>

<p>For an <code>orm</code> object generates a function for computing the
estimates of the function Prob(Y&gt;=y) given one or more values of the
linear predictor using the reference (median) intercept.  This
function can optionally be evaluated at only a set of user-specified
<code>y</code> values, otherwise a right-step function is returned.  There
is a plot method for plotting the step functions, and if more than one
linear predictor was evaluated multiple step functions are drawn.
<code>ExProb</code> is especially useful for <code><a href="#topic+nomogram">nomogram</a></code>.
</p>
<p>Optionally a normal approximation for a confidence
interval for exceedance probabilities will be computed using the delta
method, if 
<code>conf.int &gt; 0</code> is specified to the function generated from calling
<code>ExProb</code>.  In that case, a <code>"lims"</code> attribute is included
in the result computed by the derived cumulative probability function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ExProb(object, ...)

## S3 method for class 'orm'
ExProb(object, codes = FALSE, ...)

## S3 method for class 'ExProb'
plot(x, ..., data=NULL,
                      xlim=NULL, xlab=x$yname, ylab=expression(Prob(Y&gt;=y)),
                      col=par('col'), col.vert='gray85', pch=20,
                      pch.data=21, lwd=par('lwd'), lwd.data=lwd,
                      lty.data=2, key=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ExProb_+3A_object">object</code></td>
<td>
<p>a fit object from <code>orm</code></p>
</td></tr>
<tr><td><code id="ExProb_+3A_codes">codes</code></td>
<td>
<p>if <code>TRUE</code>, <code>ExProb</code> use the integer codes
<code class="reqn">1,2,\ldots,k</code> for the <code class="reqn">k</code>-level response instead of its
original unique values</p>
</td></tr> 
<tr><td><code id="ExProb_+3A_...">...</code></td>
<td>
<p>ignored for <code>ExProb</code>.  Passed to <code>plot</code> for
<code>plot.ExProb</code></p>
</td></tr>
<tr><td><code id="ExProb_+3A_data">data</code></td>
<td>
<p>Specify <code>data</code> if you want to add stratified empirical
probabilities to the graph.  If <code>data</code> is a numeric vector, it
is assumed that no groups are present.  Otherwise <code>data</code> must
be a list or data frame where the first variable is the grouping
variable (corresponding to what made the linear predictor vary) and
the second variable is the data vector for the <code>y</code> variable.
The rows of data should be sorted to be in order of the linear
predictor argument.
</p>
</td></tr>
<tr><td><code id="ExProb_+3A_x">x</code></td>
<td>
<p>an object created by running the function created by <code>ExProb</code></p>
</td></tr>
<tr><td><code id="ExProb_+3A_xlim">xlim</code></td>
<td>
<p>limits for x-axis; default is range of observed <code>y</code></p>
</td></tr>
<tr><td><code id="ExProb_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="ExProb_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="ExProb_+3A_col">col</code></td>
<td>
<p>color for horizontal lines and points</p>
</td></tr>
<tr><td><code id="ExProb_+3A_col.vert">col.vert</code></td>
<td>
<p>color for vertical discontinuities</p>
</td></tr>
<tr><td><code id="ExProb_+3A_pch">pch</code></td>
<td>
<p>plotting symbol for predicted curves</p>
</td></tr>
<tr><td><code id="ExProb_+3A_lwd">lwd</code></td>
<td>
<p>line width for predicted curves</p>
</td></tr>
<tr><td><code id="ExProb_+3A_pch.data">pch.data</code>, <code id="ExProb_+3A_lwd.data">lwd.data</code>, <code id="ExProb_+3A_lty.data">lty.data</code></td>
<td>
<p>plotting parameters for data</p>
</td></tr>
<tr><td><code id="ExProb_+3A_key">key</code></td>
<td>
<p>set to <code>FALSE</code> to suppress key in plot if <code>data</code>
is given</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ExProb</code> returns an R function.  Running the function returns an
object of class <code>"ExProb"</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell and Shengxin Tu</p>


<h3>See Also</h3>

<p><code><a href="#topic+orm">orm</a></code>, <code><a href="#topic+Quantile.orm">Quantile.orm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- runif(200)
yvar &lt;- x1 + runif(200)
f &lt;- orm(yvar ~ x1)
d &lt;- ExProb(f)
lp &lt;- predict(f, newdata=data.frame(x1=c(.2,.8)))
w &lt;- d(lp)
s1 &lt;- abs(x1 - .2) &lt; .1
s2 &lt;- abs(x1 - .8) &lt; .1
plot(w, data=data.frame(x1=c(rep(.2, sum(s1)), rep(.8, sum(s2))),
                        yvar=c(yvar[s1], yvar[s2])))

qu &lt;- Quantile(f)
abline(h=c(.1,.5), col='gray80')
abline(v=qu(.5, lp), col='gray80')
abline(v=qu(.9, lp), col='green')
</code></pre>

<hr>
<h2 id='fastbw'>Fast Backward Variable Selection</h2><span id='topic+fastbw'></span><span id='topic+print.fastbw'></span>

<h3>Description</h3>

<p>Performs a slightly inefficient but numerically stable version of fast
backward elimination on factors, using a method based on Lawless and Singhal
(1978).
This method uses the fitted complete model and computes approximate Wald
statistics by computing conditional (restricted) maximum likelihood estimates
assuming multivariate normality of estimates.
<code>fastbw</code> deletes factors, not columns of the design matrix. Factors requiring multiple d.f. will be retained or dropped as a group.
The function prints the deletion statistics for each variable in
turn, and prints approximate parameter estimates for the model after
deleting variables.  The approximation is better when the number of
factors deleted is not large.  For <code>ols</code>, the approximation is exact for
regression coefficients, and standard errors are only off by a factor
equal to the ratio of the mean squared error estimate for the reduced
model to the original mean squared error estimate for the full model.
</p>
<p>If the fit was from <code>ols</code>, <code>fastbw</code> will compute the usual <code class="reqn">R^2</code>
statistic for each model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastbw(fit, rule=c("aic", "p"),
       type=c("residual", "individual", "total"),  sls=.05, aics=0, eps=1e-9,
       k.aic=2, force=NULL)

## S3 method for class 'fastbw'
print(x, digits=4, estimates=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastbw_+3A_fit">fit</code></td>
<td>

<p>fit object with <code>Varcov(fit)</code> defined (e.g., from <code>ols</code>, <code>lrm</code>, <code>cph</code>, <code>psm</code>, <code>glmD</code>)
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_rule">rule</code></td>
<td>

<p>Stopping rule. Defaults to <code>"aic"</code> for Akaike's information criterion. Use
<code>rule="p"</code> to use <code class="reqn">P</code>-values
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_type">type</code></td>
<td>

<p>Type of statistic on which to base the stopping rule. Default is
<code>"residual"</code> for
the pooled residual chi-square. Use <code>type="individual"</code> to use Wald
chi-square of individual factors.
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_sls">sls</code></td>
<td>

<p>Significance level for staying in a model if <code>rule="p"</code>.  Default is .05.
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_aics">aics</code></td>
<td>

<p>For <code>rule="aic"</code>,
variables are deleted until the chi-square - <code>k.aic</code> times d.f. would rise above <code>aics</code>.
Default <code>aics</code> is zero to use the ordinary AIC.  Set <code>aics</code> to say 10000
to see all variables deleted in order of descending importance.
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_eps">eps</code></td>
<td>

<p>Singularity criterion, default is <code>1E-9</code>.
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_k.aic">k.aic</code></td>
<td>

<p>multiplier to compute AIC, default is 2.  To use BIC, set <code>k.aic</code> equal
to <code class="reqn">\log(n)</code>, where <code class="reqn">n</code> is the effective sample size (number of events
for survival models).
</p>
</td></tr>
<tr><td><code id="fastbw_+3A_force">force</code></td>
<td>
<p>a vector of integers specifying parameters forced to be in
the model, not counting intercept(s)</p>
</td></tr>
<tr><td><code id="fastbw_+3A_x">x</code></td>
<td>
<p>result of <code>fastbw</code></p>
</td></tr>
<tr><td><code id="fastbw_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to print</p>
</td></tr>
<tr><td><code id="fastbw_+3A_estimates">estimates</code></td>
<td>
<p>set to <code>FALSE</code> to suppress printing table of
approximate coefficients, SEs, etc., after variable deletions</p>
</td></tr>
<tr><td><code id="fastbw_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with an attribute <code>kept</code> if <code>bw=TRUE</code>, and the
following components:
</p>
<table>
<tr><td><code>result</code></td>
<td>

<p>matrix of statistics with rows in order of deletion.
</p>
</td></tr>
<tr><td><code>names.kept</code></td>
<td>

<p>names of factors kept in final model.
</p>
</td></tr>
<tr><td><code>factors.kept</code></td>
<td>

<p>the subscripts of factors kept in the final model
</p>
</td></tr>
<tr><td><code>factors.deleted</code></td>
<td>

<p>opposite of <code>factors.kept</code>.
</p>
</td></tr>
<tr><td><code>parms.kept</code></td>
<td>

<p>column numbers in design matrix corresponding to parameters kept in
the final model.
</p>
</td></tr>
<tr><td><code>parms.deleted</code></td>
<td>

<p>opposite of <code>parms.kept</code>.
</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>

<p>vector of approximate coefficients of reduced model.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>approximate covariance matrix for reduced model.
</p>
</td></tr>
<tr><td><code>Coefficients</code></td>
<td>

<p>matrix of coefficients of all models.  Rows correspond to the
successive models examined and columns correspond to the coefficients
in the full model.  For variables not in a particular sub-model (row),
the coefficients are zero.
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Lawless, J. F. and Singhal, K. (1978): Efficient screening of nonnormal regression models.  Biometrics 34:318&ndash;327.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+ols">ols</a></code>, <code><a href="#topic+lrm">lrm</a></code>,
<code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+psm">psm</a></code>, <code><a href="#topic+validate">validate</a></code>,
<code><a href="Hmisc.html#topic+solvet">solvet</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
fastbw(fit, optional.arguments)     # print results
z &lt;- fastbw(fit, optional.args)     # typically used in simulations
lm.fit(X[,z$parms.kept], Y)         # least squares fit of reduced model

## End(Not run)
</code></pre>

<hr>
<h2 id='Function'>Compose an S Function to Compute X beta from a Fit</h2><span id='topic+Function.rms'></span><span id='topic+Function.cph'></span><span id='topic+sascode'></span><span id='topic+perlcode'></span>

<h3>Description</h3>

<p><code>Function</code> is a class of functions for creating other S functions.
<code>Function.rms</code> is the method for creating S functions to compute
X beta, based on a model fitted with <code>rms</code> in effect.  
Like <code>latexrms</code>, <code>Function.rms</code> simplifies restricted cubic
spline functions and factors out terms in second-order interactions.
<code>Function.rms</code> will not work for models that have third-order
interactions involving restricted cubic splines.
<code>Function.cph</code> is a particular method for handling fits from
<code>cph</code>, for which an intercept (the negative of the centering
constant) is added to  
the model.  <code>sascode</code> is a function that takes an S function such
as one created by <code>Function</code> and does most of the editing
to turn the function definition into
a fragment of SAS code for computing X beta from the fitted model, along
with assignment statements that initialize predictors to reference
values.
<code>perlcode</code> similarly creates Perl code to evaluate a fitted
regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rms'
Function(object, intercept=NULL,
 digits=max(8, .Options$digits), posterior.summary=c('mean', 'median', 'mode'), ...)
## S3 method for class 'cph'
Function(object, intercept=-object$center, ...)

# Use result as fun(predictor1=value1, predictor2=value2, \dots)

sascode(object, file='', append=FALSE)

perlcode(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Function_+3A_object">object</code></td>
<td>

<p>a fit created with <code>rms</code> in effect
</p>
</td></tr>
<tr><td><code id="Function_+3A_intercept">intercept</code></td>
<td>

<p>an intercept value to use (not allowed to be specified to <code>Function.cph</code>).
The intercept is usually retrieved from the regression coefficients
automatically.
</p>
</td></tr>
<tr><td><code id="Function_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to use for coefficients and knot locations</p>
</td></tr>
<tr><td><code id="Function_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>if using a Bayesian model fit such as from
<code>blrm</code>, specifies whether to use posterior mode/mean/median parameter estimates in generating the function</p>
</td></tr>
<tr><td><code id="Function_+3A_file">file</code></td>
<td>

<p>name of a file in which to write the SAS code.  Default is to write to
standard output.
</p>
</td></tr>
<tr><td><code id="Function_+3A_append">append</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>sascode</code> append code to an existing file named
<code>file</code>.
</p>
</td></tr>
<tr><td><code id="Function_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code>Function.rms</code> from
<code>Function.cph</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Function</code> returns an S-Plus function that can be invoked in any
usual context.  The function has one argument per predictor variable,
and the default values of the predictors are set to <code>adjust-to</code> values
(see <code>datadist</code>).  Multiple predicted X beta values may be calculated
by specifying vectors as arguments to the created function.
All non-scalar argument values must have the same length.
<code>perlcode</code> returns a character string with embedded newline characters.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell, Jeremy Stephens, and Thomas Dupont<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+latexrms">latexrms</a></code>, <code><a href="Hmisc.html#topic+transcan">transcan</a></code>,
<code><a href="#topic+predict.rms">predict.rms</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>suppressWarnings(RNGversion("3.5.0"))
set.seed(1331)
x1 &lt;- exp(rnorm(100))
x2 &lt;- factor(sample(c('a','b'),100,rep=TRUE))
dd &lt;- datadist(x1, x2)
options(datadist='dd')
y  &lt;- log(x1)^2+log(x1)*(x2=='b')+rnorm(100)/4
f  &lt;- ols(y ~ pol(log(x1),2)*x2)
f$coef
g  &lt;- Function(f, digits=5)
g
sascode(g)
cat(perlcode(g), '\n')
g()
g(x1=c(2,3), x2='b')   #could omit x2 since b is default category
predict(f, expand.grid(x1=c(2,3),x2='b'))
g8 &lt;- Function(f)   # default is 8 sig. digits
g8(x1=c(2,3), x2='b')
options(datadist=NULL)


## Not run: 
require(survival)
# Make self-contained functions for computing survival probabilities
# using a log-normal regression
f &lt;- psm(Surv(d.time, death) ~ rcs(age,4)*sex, dist='gaussian')
g &lt;- Function(f)
surv &lt;- Survival(f)
# Compute 2 and 5-year survival estimates for 50 year old male
surv(c(2,5), g(age=50, sex='male'))

## End(Not run)
</code></pre>

<hr>
<h2 id='gendata'>Generate Data Frame with Predictor Combinations</h2><span id='topic+gendata'></span>

<h3>Description</h3>

<p>If <code>nobs</code> is not specified, allows user to specify predictor settings
by e.g. <code>age=50, sex="male"</code>, and any omitted predictors are set to
reference values (default=median for continuous variables, first level
for categorical ones - see <code>datadist</code>).  If any predictor has more than one
value given, <code>expand.grid</code> is called to generate all possible combinations
of values, unless <code>expand=FALSE</code>.  If <code>nobs</code> is given, a data
frame is first generated which has 
<code>nobs</code> of adjust-to values duplicated.  Then an editor window is opened
which allows the user to subset the variable names down to ones which she
intends to vary (this streamlines the <code>data.ed</code> step).  Then, if any
predictors kept are discrete and <code>viewvals=TRUE</code>, a window (using <code>page</code>)
is opened defining the possible values of this subset, to facilitate
data editing.  Then the <code>data.ed</code> function is invoked to allow interactive
overriding of predictor settings in the <code>nobs</code> rows.  The subset of
variables are combined with the other predictors which were not
displayed with <code>data.ed</code>, and a final full data frame is returned.
<code>gendata</code> is most useful for creating a <code>newdata</code> data frame to pass
to <code>predict</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gendata(fit, ..., nobs, viewvals=FALSE, expand=TRUE, factors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gendata_+3A_fit">fit</code></td>
<td>

<p>a fit object created with <code>rms</code> in effect
</p>
</td></tr>
<tr><td><code id="gendata_+3A_...">...</code></td>
<td>

<p>predictor settings, if <code>nobs</code> is not given. 
</p>
</td></tr>
<tr><td><code id="gendata_+3A_nobs">nobs</code></td>
<td>

<p>number of observations to create if doing it interactively using X-windows
</p>
</td></tr>
<tr><td><code id="gendata_+3A_viewvals">viewvals</code></td>
<td>

<p>if <code>nobs</code> is given, set <code>viewvals=TRUE</code> to open a window
displaying the possible value of categorical predictors
</p>
</td></tr>
<tr><td><code id="gendata_+3A_expand">expand</code></td>
<td>

<p>set to <code>FALSE</code> to prevent <code>expand.grid</code> from being called,
and to instead just convert to a data frame.</p>
</td></tr>
<tr><td><code id="gendata_+3A_factors">factors</code></td>
<td>

<p>a list containing predictor settings with their names.  This is an
alternative to specifying the variables separately in ....  Unlike the
usage of ..., variables getting default ranges in <code>factors</code>
should have <code>NA</code> as their value. 
</p>
</td></tr></table>


<h3>Details</h3>

<p>if you have a variable in <code>...</code> that is named <code>n, no, nob,
  nob</code>, add <code>nobs=FALSE</code> to the invocation to prevent that variable
from being misrecognized as <code>nobs</code>
</p>


<h3>Value</h3>

<p>a data frame with all predictors, and an attribute <code>names.subset</code> if
<code>nobs</code> is specified.  This attribute contains the vector of variable
names for predictors which were passed to <code>de</code> and hence were
allowed to vary.  If neither <code>nobs</code> nor any predictor settings were
given, returns a data frame with adjust-to values.
</p>


<h3>Side Effects</h3>

<p>optionally writes to the terminal, opens X-windows, and generates a
temporary file using <code>sink</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.rms">predict.rms</a></code>, <code><a href="#topic+survest.cph">survest.cph</a></code>,
<code><a href="#topic+survest.psm">survest.psm</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>,
<code><a href="base.html#topic+expand.grid">expand.grid</a></code>, <code><a href="utils.html#topic+de">de</a></code>, <code><a href="utils.html#topic+page">page</a></code>, 
<code><a href="#topic+print.datadist">print.datadist</a></code>, <code><a href="#topic+Predict">Predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
age &lt;- rnorm(200, 50, 10)
sex &lt;- factor(sample(c('female','male'),200,TRUE))
race &lt;- factor(sample(c('a','b','c','d'),200,TRUE))
y &lt;- sample(0:1, 200, TRUE)
dd &lt;- datadist(age,sex,race)
options(datadist="dd")
f &lt;- lrm(y ~ age*sex + race)
gendata(f)
gendata(f, age=50)
d &lt;- gendata(f, age=50, sex="female")  # leave race=reference category
d &lt;- gendata(f, age=c(50,60), race=c("b","a"))  # 4 obs.
d$Predicted &lt;- predict(f, d, type="fitted")
d      # Predicted column prints at the far right
options(datadist=NULL)
## Not run: 
d &lt;- gendata(f, nobs=5, view=TRUE)        # 5 interactively defined obs.
d[,attr(d,"names.subset")]             # print variables which varied
predict(f, d)

## End(Not run)
</code></pre>

<hr>
<h2 id='ggplot.Predict'>Plot Effects of Variables Estimated by a Regression Model Fit
Using ggplot2</h2><span id='topic+ggplot.Predict'></span>

<h3>Description</h3>

<p>Uses <code>ggplot2</code> graphics to plot the effect of one or two predictors
on the linear predictor or X beta scale, or on some transformation of
that scale.  The first argument specifies the result of the
<code>Predict</code> function.  The predictor is always plotted in its
original coding.
</p>
<p>If <code>rdata</code> is given, a spike histogram is drawn showing
the location/density of data values for the <code class="reqn">x</code>-axis variable.  If
there is a <code>groups</code> (superposition) variable that generated separate
curves, the data density specific to each class of points is shown.
This assumes that the second variable was a factor variable.  The histograms
are drawn by <code>histSpikeg</code>.
</p>
<p>To plot effects instead of estimates (e.g., treatment differences as a
function of interacting factors) see <code>contrast.rms</code> and
<code>summary.rms</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Predict'
ggplot(data, mapping, formula=NULL, groups=NULL,
     aestype=c('color', 'linetype'),
     conf=c('fill', 'lines'),
     conflinetype=1,
     varypred=FALSE, sepdiscrete=c('no', 'list', 'vertical', 'horizontal'),
     subset, xlim., ylim., xlab, ylab, 
     colorscale=function(...) scale_color_manual(...,
       values=c("#000000", "#E69F00", "#56B4E9",
                "#009E73","#F0E442", "#0072B2", "#D55E00", "#CC79A7")),
     colfill='black',
     rdata=NULL, anova=NULL, pval=FALSE, size.anova=4,
     adj.subtitle, size.adj=2.5, perim=NULL, nlevels=3,
     flipxdiscrete=TRUE,
     legend.position='right', legend.label=NULL,
     vnames=c('labels','names'), abbrev=FALSE, minlength=6,
     layout=NULL, addlayer,
     histSpike.opts=list(frac=function(f) 0.01 + 
         0.02 * sqrt(f - 1)/sqrt(max(f, 2) - 1), side=1, nint=100),
     type=NULL, ggexpr=FALSE, height=NULL, width=NULL, ..., environment)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot.Predict_+3A_data">data</code></td>
<td>
<p>a data frame created by <code>Predict</code></p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_mapping">mapping</code></td>
<td>
<p>kept because of <code>ggplot</code> generic setup.  If
specified it will be assumed to be <code>formula</code>.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_formula">formula</code></td>
<td>

<p>a <code>ggplot</code> faceting formula of the form <code>vertical variables
		~ horizontal variables</code>, with variables separated by <code>*</code> if
there is more than one variable on a side.  If omitted, the formula
will be built using assumptions on the list of variables that varied
in the <code>Predict</code> call.  When plotting multiple panels (for
separate predictors), <code>formula</code> may be specified but by default
no formula is constructed.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_groups">groups</code></td>
<td>
<p>an optional character string containing the
name of one of the variables in <code>data</code> that
is to be used as a grouping (superpositioning) variable.
Set <code>groups=FALSE</code> to suppress superpositioning.  By default, the
second varying variable is used for superpositioning groups.  You can
also specify a length 2 string vector of variable names specifying two
dimensions of superpositioning, identified by different aesthetics
corresponding to the <code>aestype</code> argument. When plotting effects
of more than one predictor, <code>groups</code>
is a character string that specifies a single variable name in
<code>data</code> that can be used to form panels.  Only applies if using
<code>rbind</code> to combine several <code>Predict</code> results. If there is
more than one <code>groups</code> variable, confidence bands are suppressed
because <code>ggplot2:geom_ribbon</code> does not handle the aesthetics correctly.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_aestype">aestype</code></td>
<td>
<p>a string vector of aesthetic names corresponding to
variables in the <code>groups</code> vector.  Default is to use, in order,
<code>color</code>, and <code>linetype</code>.  Other permissible values are
<code>size</code>, <code>shape</code>.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_conf">conf</code></td>
<td>
<p>specify <code>conf="line"</code> to show confidence bands with
lines instead of filled ribbons, the default</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_conflinetype">conflinetype</code></td>
<td>
<p>specify an alternative <code>linetype</code> for confidence
intervals if <code>conf="line"</code></p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_varypred">varypred</code></td>
<td>
<p>set to <code>TRUE</code> if <code>data</code> is the result of
passing multiple <code>Predict</code> results, that represent different
predictors, to <code>rbind.Predict</code>.  This will cause the <code>.set.</code>
variable created by <code>rbind</code> to be copied to the
<code>.predictor.</code> variable.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_sepdiscrete">sepdiscrete</code></td>
<td>
<p>set to something other than <code>"no"</code> to create
separate graphics for continuous and discrete predictors.  For
discrete predictors, horizontal dot charts are produced.  This allows
use of the <code>ggplot2</code> <code>facet_wrap</code> function to make better
use of space.  If <code>sepdiscrete="list"</code>, a list of two <code>grid</code>
graphics objects is returned if both types of predictors are present
(otherwise one object for the type that existed in the model).  Set
<code>sepdiscrete="vertical"</code> to put the two types of plots into one
graphical object with continuous predictors on top and given a
fraction of space relative to the number of continuous vs. number of
discrete variables.  Set <code>sepdiscrete="horizontal"</code> to get a
horizontal arrangements with continuous variables on the left.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_subset">subset</code></td>
<td>
<p>a subsetting expression for restricting the rows of
<code>data</code> that are used in plotting.  For example, predictions may have
been requested for males and females but one wants to plot only females.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_xlim.">xlim.</code></td>
<td>

<p>This parameter is seldom used, as limits are usually controlled with
<code>Predict</code>.  Usually given as its legal abbreviation <code>xlim</code>.
One reason to use <code>xlim</code> is to plot a <code>factor</code> variable on
the x-axis that was created with the <code>cut2</code> function with the
<code>levels.mean</code> option, with <code>val.lev=TRUE</code> specified to
<code>plot.Predict</code>.  In this case you may want the axis to have the
range of the original variable values given to <code>cut2</code> rather than
the range of the means within quantile groups. 
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_ylim.">ylim.</code></td>
<td>

<p>Range for plotting on response variable axis. Computed by default.
Usually specified using its legal definition <code>ylim</code>.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_xlab">xlab</code></td>
<td>

<p>Label for <code>x</code>-axis. Default is one given to <code>asis, rcs</code>, etc.,
which may have been the <code>"label"</code> attribute of the variable.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_ylab">ylab</code></td>
<td>

<p>Label for <code>y</code>-axis.  If <code>fun</code> is not given,
default is <code>"log Odds"</code> for
<code>lrm</code>, <code>"log Relative Hazard"</code> for <code>cph</code>, name of the response
variable for <code>ols</code>, <code>TRUE</code> or <code>log(TRUE)</code> for <code>psm</code>,
or <code>"X * Beta"</code> otherwise.  Specify <code>ylab=NULL</code> to omit
<code>y</code>-axis labels.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_colorscale">colorscale</code></td>
<td>
<p>a <code>ggplot2</code> discrete scale function,
e.g. <code>function(...) scale_color_brewer(..., palette='Set1',
  type='qual')</code>.  The default is the colorblind-friendly palette
including black in <a href="http://www.cookbook-r.com/Graphs/Colors_(ggplot2)">http://www.cookbook-r.com/Graphs/Colors_(ggplot2)</a>.  If you get an error &quot;insufficient values in manual scale&quot;, which occurs when there are more than 8 groups, just specify <code>colorscale=function(...){}</code> to use default colors.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_colfill">colfill</code></td>
<td>
<p>a single character string or number specifying the fill color
to use for <code>geom_ribbon</code> for shaded confidence bands.  Alpha
transparency of 0.2 is applied to any color specified.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_rdata">rdata</code></td>
<td>
<p>a data frame containing the original raw data on which the
regression model were based, or at least containing the <code class="reqn">x</code>-axis
and grouping variable.  If <code>rdata</code> is present and contains the
needed variables, the original data are added to the graph in the form
of a spike histogram using <code>histSpikeg</code> in the Hmisc package.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_anova">anova</code></td>
<td>
<p>an object returned by <code><a href="#topic+anova.rms">anova.rms</a></code>.  If
<code>anova</code> is specified, the overall test of association for
predictor plotted is added as text to each panel, located at the spot
at which the panel is most empty unless there is significant empty
space at the top or bottom of the panel; these areas are given preference.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_pval">pval</code></td>
<td>
<p>specify <code>pval=TRUE</code> for <code>anova</code> to include not
only the test statistic but also the P-value</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_size.anova">size.anova</code></td>
<td>
<p>character size for the test statistic printed on the
panel, mm</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_adj.subtitle">adj.subtitle</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress subtitling the graph with the list of
settings of non-graphed adjustment values.  Subtitles appear as captions
with <code>ggplot2</code> using <code>labs(caption=)</code>.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_size.adj">size.adj</code></td>
<td>
<p>Size of adjustment settings in subtitles in mm.  Default is 2.5.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_perim">perim</code></td>
<td>

<p><code>perim</code> specifies a function having two
arguments.  The first is the vector of values of the first variable that
is about to be plotted on the x-axis.  The second argument is the single
value of the variable representing different curves, for the current
curve being plotted.  The function's returned value must be a logical
vector whose length is the same as that of the first argument, with
values <code>TRUE</code> if the corresponding point should be plotted for the
current curve, <code>FALSE</code> otherwise.  See one of the latter examples.
<code>perim</code> only applies if predictors were specified to <code>Predict</code>.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_nlevels">nlevels</code></td>
<td>

<p>when <code>groups</code> and <code>formula</code> are not specified, if any panel
variable has <code>nlevels</code> or fewer values, that variable is
converted to a <code>groups</code> (superpositioning) variable.  Set
<code>nlevels=0</code> to prevent this behavior.  For other situations, a
non-numeric x-axis variable with <code>nlevels</code> or fewer unique values
will cause a horizontal dot plot to be drawn instead of an x-y plot
unless <code>flipxdiscrete=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_flipxdiscrete">flipxdiscrete</code></td>
<td>
<p>see <code>nlevels</code></p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_legend.position">legend.position</code></td>
<td>
<p><code>"right"</code> (the default for single-panel
plots), <code>"left"</code>, <code>"bottom"</code>, <code>"top"</code>, a two-element
numeric vector, or <code>"none"</code> to suppress.  For multi-panel plots
the default is <code>"top"</code>, and a legend only appears for the first
(top left) panel.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_legend.label">legend.label</code></td>
<td>
<p>if omitted, group variable labels will be used for
label the legend.  Specify <code>legend.label=FALSE</code> to suppress using
a legend name, or a character string or expression to specify the
label.  Can be a vector is there is more than one grouping variable.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_vnames">vnames</code></td>
<td>
<p>applies to the case where multiple plots are produced
separately by predictor.  Set to <code>'names'</code> to use variable names
instead of labels for these small plots.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_abbrev">abbrev</code></td>
<td>
<p>set to true to abbreviate levels of predictors that are
categorical to a minimum length of <code>minlength</code></p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_minlength">minlength</code></td>
<td>
<p>see <code>abbrev</code></p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_layout">layout</code></td>
<td>
<p>for multi-panel plots a 2-vector specifying the number of
rows and number of columns.  If omitted will be computed from the
number of panels to make as square as possible.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_addlayer">addlayer</code></td>
<td>
<p>a <code>ggplot2</code> expression consisting of one or more
layers to add to the current plot</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_histspike.opts">histSpike.opts</code></td>
<td>
<p>a list containing named elements that specifies
parameters to <code><a href="Hmisc.html#topic+scat1d">histSpikeg</a></code> when <code>rdata</code> is given.  The
<code>col</code> parameter is usually derived from other plotting
information and not specified by the user.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_type">type</code></td>
<td>
<p>a value (<code>"l","p","b"</code>) to override default choices
related to showing or connecting points.  Especially  useful for
discrete x coordinate variables.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_ggexpr">ggexpr</code></td>
<td>
<p>set to <code>TRUE</code> to have the function return the
character string(s) constructed to invoke <code>ggplot</code> without
executing the commands</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_height">height</code>, <code id="ggplot.Predict_+3A_width">width</code></td>
<td>
<p>used if <code>plotly</code> is in effect, to specify the
<code>plotly</code> image in pixels.  Default is to let <code>plotly</code> size
the image.</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="ggplot.Predict_+3A_environment">environment</code></td>
<td>
<p>ignored; used to satisfy rules because of the generic ggplot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>"ggplot2"</code> ready for printing.  For the
case where predictors were not specified to <code>Predict</code>, 
<code>sepdiscrete=TRUE</code>, and there were both continuous and discrete
predictors in the model, a list of two graphics objects is returned.</p>


<h3>Note</h3>

<p>If plotting the effects of all predictors you can reorder the
panels using for example <code>p &lt;- Predict(fit); p$.predictor. &lt;-
	factor(p$.predictor., v)</code> where <code>v</code> is a vector of predictor
names specified in the desired order.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Fox J, Hong J (2009): Effect displays in R for multinomial and
proportional-odds logit models: Extensions to the effects package.  J
Stat Software 32 No. 1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+rbind.Predict">rbind.Predict</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+plot.Predict">plot.Predict</a></code>,
<code><a href="Hmisc.html#topic+labcurve">labcurve</a></code>, <code><a href="Hmisc.html#topic+scat1d">histSpikeg</a></code>,
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="Hmisc.html#topic+Overview">Overview</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
n &lt;- 350     # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
  .01 * (blood.pressure - 120)
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
               x=TRUE, y=TRUE)
an &lt;- anova(fit)
# Plot effects in two vertical sub-panels with continuous predictors on top
# ggplot(Predict(fit), sepdiscrete='vertical')
# Plot effects of all 4 predictors with test statistics from anova, and P
ggplot(Predict(fit), anova=an, pval=TRUE)
# ggplot(Predict(fit), rdata=llist(blood.pressure, age))
# spike histogram plot for two of the predictors

# p &lt;- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
# ggplot(p)

# p &lt;- Predict(fit, age=seq(20,80,length=100), sex, conf.int=FALSE)
#                        # Plot relationship between age and log
                         # odds, separate curve for each sex,
# ggplot(p, subset=sex=='female' | age &gt; 30)
# No confidence interval, suppress estimates for males &lt;= 30

# p &lt;- Predict(fit, age, sex)
# ggplot(p, rdata=llist(age,sex))
                         # rdata= allows rug plots (1-dimensional scatterplots)
                         # on each sex's curve, with sex-
                         # specific density of age
                         # If data were in data frame could have used that
# p &lt;- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
                         # works if datadist not used
# ggplot(p, ylab=expression(hat(P)))
                         # plot predicted probability in place of log odds
# per &lt;- function(x, y) x &gt;= 30
# ggplot(p, perim=per)       # suppress output for age &lt; 30 but leave scale alone

# Do ggplot2 faceting a few different ways
p &lt;- Predict(fit, age, sex, blood.pressure=c(120,140,160),
             cholesterol=c(180,200,215))
# ggplot(p)
ggplot(p, cholesterol ~ blood.pressure)
# ggplot(p, ~ cholesterol + blood.pressure)
# color for sex, line type for blood.pressure:
ggplot(p, groups=c('sex', 'blood.pressure'))
# Add legend.position='top' to allow wider plot
# Map blood.pressure to line thickness instead of line type:
# ggplot(p, groups=c('sex', 'blood.pressure'), aestype=c('color', 'size'))

# Plot the age effect as an odds ratio
# comparing the age shown on the x-axis to age=30 years

# ddist$limits$age[2] &lt;- 30    # make 30 the reference value for age
# Could also do: ddist$limits["Adjust to","age"] &lt;- 30
# fit &lt;- update(fit)   # make new reference value take effect
# p &lt;- Predict(fit, age, ref.zero=TRUE, fun=exp)
# ggplot(p, ylab='Age=x:Age=30 Odds Ratio',
#        addlayer=geom_hline(yintercept=1, col=gray(.8)) +
#                 geom_vline(xintercept=30, col=gray(.8)) +
#                 scale_y_continuous(trans='log',
#                       breaks=c(.5, 1, 2, 4, 8))))

# Compute predictions for three predictors, with superpositioning or
# conditioning on sex, combined into one graph

p1 &lt;- Predict(fit, age, sex)
p2 &lt;- Predict(fit, cholesterol, sex)
p3 &lt;- Predict(fit, blood.pressure, sex)
p &lt;- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
ggplot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE)
# ggplot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE, sepdiscrete='vert')

## Not run: 
# For males at the median blood pressure and cholesterol, plot 3 types
# of confidence intervals for the probability on one plot, for varying age
ages &lt;- seq(20, 80, length=100)
p1 &lt;- Predict(fit, age=ages, sex='male', fun=plogis)  # standard pointwise
p2 &lt;- Predict(fit, age=ages, sex='male', fun=plogis,
              conf.type='simultaneous')               # simultaneous
p3 &lt;- Predict(fit, age=c(60,65,70), sex='male', fun=plogis,
              conf.type='simultaneous')               # simultaneous 3 pts
# The previous only adjusts for a multiplicity of 3 points instead of 100
f &lt;- update(fit, x=TRUE, y=TRUE)
g &lt;- bootcov(f, B=500, coef.reps=TRUE)
p4 &lt;- Predict(g, age=ages, sex='male', fun=plogis)    # bootstrap percentile
p &lt;- rbind(Pointwise=p1, 'Simultaneous 100 ages'=p2,
           'Simultaneous     3 ages'=p3, 'Bootstrap nonparametric'=p4)
# as.data.frame so will call built-in ggplot
ggplot(as.data.frame(p), aes(x=age, y=yhat)) + geom_line() +
 geom_ribbon(data=p, aes(ymin=lower, ymax=upper), alpha=0.2, linetype=0)+
 facet_wrap(~ .set., ncol=2)

# Plots for a parametric survival model
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, 
              rep=TRUE, prob=c(.6, .4)))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
t &lt;- -log(runif(n))/h
label(t) &lt;- 'Follow-up Time'
e &lt;- ifelse(t&lt;=cens,1,0)
t &lt;- pmin(t, cens)
units(t) &lt;- "Year"
ddist &lt;- datadist(age, sex)
require(survival)
Srv &lt;- Surv(t,e)

# Fit log-normal survival model and plot median survival time vs. age
f &lt;- psm(Srv ~ rcs(age), dist='lognormal')
med &lt;- Quantile(f)       # Creates function to compute quantiles
                         # (median by default)
p &lt;- Predict(f, age, fun=function(x) med(lp=x))
ggplot(p, ylab="Median Survival Time")
# Note: confidence intervals from this method are approximate since
# they don't take into account estimation of scale parameter


# Fit an ols model to log(y) and plot the relationship between x1
# and the predicted mean(y) on the original scale without assuming
# normality of residuals; use the smearing estimator
# See help file for rbind.Predict for a method of showing two
# types of confidence intervals simultaneously.
# Add raw data scatterplot to graph
set.seed(1)
x1 &lt;- runif(300)
x2 &lt;- runif(300)
ddist &lt;- datadist(x1, x2); options(datadist='ddist')
y  &lt;- exp(x1 + x2 - 1 + rnorm(300))
f &lt;- ols(log(y) ~ pol(x1,2) + x2)
r &lt;- resid(f)
smean &lt;- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
formals(smean) &lt;- list(yhat=numeric(0), res=r[! is.na(r)])
#smean$res &lt;- r[! is.na(r)]   # define default res argument to function
ggplot(Predict(f, x1, fun=smean), ylab='Predicted Mean on y-scale', 
   addlayer=geom_point(aes(x=x1, y=y), data.frame(x1, y)))
# Had ggplot not added a subtitle (i.e., if x2 were not present), you
# could have done ggplot(Predict(), ylab=...) + geom_point(...) 

## End(Not run)

# Make an 'interaction plot', forcing the x-axis variable to be
# plotted at integer values but labeled with category levels
n &lt;- 100
set.seed(1)
gender &lt;- c(rep('male', n), rep('female',n))
m &lt;- sample(c('a','b'), 2*n, TRUE)
d &lt;-  datadist(gender, m); options(datadist='d')
anxiety &lt;- runif(2*n) + .2*(gender=='female') + .4*(gender=='female' &amp; m=='b')
tapply(anxiety, llist(gender,m), mean)
f &lt;- ols(anxiety ~ gender*m)
p &lt;- Predict(f, gender, m)
# ggplot(p)     # horizontal dot chart; usually preferred for categorical predictors
# ggplot(p, flipxdiscrete=FALSE)  # back to vertical
ggplot(p, groups='gender')
ggplot(p, ~ m, groups=FALSE, flipxdiscrete=FALSE)

options(datadist=NULL)

## Not run: 
# Example in which separate curves are shown for 4 income values
# For each curve the estimated percentage of voters voting for
# the democratic party is plotted against the percent of voters
# who graduated from college.  Data are county-level percents.

incomes &lt;- seq(22900, 32800, length=4)  
# equally spaced to outer quintiles
p &lt;- Predict(f, college, income=incomes, conf.int=FALSE)
ggplot(p, xlim=c(0,35), ylim=c(30,55))

# Erase end portions of each curve where there are fewer than 10 counties having
# percent of college graduates to the left of the x-coordinate being plotted,
# for the subset of counties having median family income with 1650
# of the target income for the curve

show.pts &lt;- function(college.pts, income.pt) {
  s &lt;- abs(income - income.pt) &lt; 1650  #assumes income known to top frame
  x &lt;- college[s]
  x &lt;- sort(x[!is.na(x)])
  n &lt;- length(x)
  low &lt;- x[10]; high &lt;- x[n-9]
  college.pts &gt;= low &amp; college.pts &lt;= high
}

ggplot(p, xlim=c(0,35), ylim=c(30,55), perim=show.pts)

# Rename variables for better plotting of a long list of predictors
f &lt;- ...
p &lt;- Predict(f)
re &lt;- c(trt='treatment', diabet='diabetes', sbp='systolic blood pressure')

for(n in names(re)) {
  names(p)[names(p)==n] &lt;- re[n]
  p$.predictor.[p$.predictor.==n] &lt;- re[n]
  }
ggplot(p)

## End(Not run)
</code></pre>

<hr>
<h2 id='gIndex'>Calculate Total and Partial g-indexes for an rms Fit</h2><span id='topic+gIndex'></span><span id='topic+print.gIndex'></span><span id='topic+plot.gIndex'></span>

<h3>Description</h3>

<p><code>gIndex</code> computes the total <code class="reqn">g</code>-index for a model based on
the vector of linear predictors, and the partial <code class="reqn">g</code>-index for
each predictor in a model.  The latter is computed by summing all the
terms involving each variable, weighted by their regression
coefficients, then computing Gini's mean difference on this sum.  For
example, a regression model having age and sex and age*sex on the
right hand side, with corresponding regression coefficients <code class="reqn">b_{1},
	b_{2}, b_{3}</code> will have the <code class="reqn">g</code>-index for age
computed from Gini's mean 
difference on the product of age <code class="reqn">\times (b_{1} + b_{3}w)</code> where
<code class="reqn">w</code> is an indicator set to one for observations with sex not equal
to the reference value.  When there are nonlinear terms associated
with a predictor, these terms will also be combined.
</p>
<p>A <code>print</code>
method is defined, and there is a <code>plot</code> method for displaying
<code class="reqn">g</code>-indexes using a dot chart.
</p>
<p>These functions use <code>Hmisc::GiniMd</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gIndex(object, partials=TRUE, type=c('ccterms', 'cterms', 'terms'),
           lplabel=if(length(object$scale) &amp;&amp; is.character(object$scale))
           object$scale[1] else 'X*Beta',
           fun, funlabel=if(missing(fun)) character(0) else
           deparse(substitute(fun)),
           postfun=if(length(object$scale)==2) exp else NULL,
           postlabel=if(length(postfun))
           ifelse(missing(postfun),
                  if((length(object$scale) &gt; 1) &amp;&amp;
                     is.character(object$scale)) object$scale[2] else
                     'Anti-log',
                     deparse(substitute(postfun))) else character(0),
           ...)

## S3 method for class 'gIndex'
print(x, digits=4, abbrev=FALSE,
 vnames=c("names","labels"), ...)

## S3 method for class 'gIndex'
plot(x, what=c('pre', 'post'),
 xlab=NULL, pch=16, rm.totals=FALSE,
sort=c('descending', 'ascending', 'none'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gIndex_+3A_object">object</code></td>
<td>
<p>result of an <code>rms</code> fitting function</p>
</td></tr>
<tr><td><code id="gIndex_+3A_partials">partials</code></td>
<td>
<p>set to <code>FALSE</code> to suppress computation of partial
<code class="reqn">g</code>s</p>
</td></tr>
<tr><td><code id="gIndex_+3A_type">type</code></td>
<td>
<p>defaults to <code>'ccterms'</code> which causes partial discrimination
indexes to be computed after maximally combining all related main
effects and interactions.  The is usually the only way that makes
sense when considering partial linear predictors.  Specify
<code>type='cterms'</code> to only combine a main effect 
with interactions containing it, not also with other main effects
connected through interactions.  Use <code>type='terms'</code> to separate
interactions into their own effects.</p>
</td></tr>
<tr><td><code id="gIndex_+3A_lplabel">lplabel</code></td>
<td>
<p>a replacement for default values such as
<code>"X*Beta"</code> or <code>"log odds"</code>/</p>
</td></tr>
<tr><td><code id="gIndex_+3A_fun">fun</code></td>
<td>
<p>an optional function to transform the linear predictors
before computing the total (only) <code class="reqn">g</code>.  When this is present, a
new component <code>gtrans</code> is added to the attributes of the object
resulting from <code>gIndex</code>.</p>
</td></tr>
<tr><td><code id="gIndex_+3A_funlabel">funlabel</code></td>
<td>
<p>a character string label for <code>fun</code>, otherwise
taken from the function name itself</p>
</td></tr>
<tr><td><code id="gIndex_+3A_postfun">postfun</code></td>
<td>
<p>a function to transform <code class="reqn">g</code> such as <code>exp</code>
(anti-log), which is the default for certain models such as the
logistic and Cox models</p>
</td></tr>
<tr><td><code id="gIndex_+3A_postlabel">postlabel</code></td>
<td>
<p>a label for <code>postfun</code></p>
</td></tr>
<tr><td><code id="gIndex_+3A_...">...</code></td>
<td>

<p>For <code>gIndex</code>, passed to <code>predict.rms</code>.
Ignored for <code>print</code>.  Passed to <code><a href="Hmisc.html#topic+dotchart2">dotchart2</a></code>
for <code>plot</code>.
</p>
</td></tr>
<tr><td><code id="gIndex_+3A_x">x</code></td>
<td>

<p>an object created by <code>gIndex</code> (for <code>print</code> or <code>plot</code>)
</p>
</td></tr>
<tr><td><code id="gIndex_+3A_digits">digits</code></td>
<td>
<p>causes rounding to the <code>digits</code> decimal place</p>
</td></tr>
<tr><td><code id="gIndex_+3A_abbrev">abbrev</code></td>
<td>
<p>set to <code>TRUE</code> to abbreviate labels if
<code>vname="labels"</code></p>
</td></tr>
<tr><td><code id="gIndex_+3A_vnames">vnames</code></td>
<td>
<p>set to <code>"labels"</code> to print predictor labels instead
of names</p>
</td></tr>
<tr><td><code id="gIndex_+3A_what">what</code></td>
<td>
<p>set to <code>"post"</code> to plot the transformed <code class="reqn">g</code>-index
if there is one (e.g., ratio scale)</p>
</td></tr>
<tr><td><code id="gIndex_+3A_xlab">xlab</code></td>
<td>
<p><code class="reqn">x</code>-axis label; constructed by default</p>
</td></tr>
<tr><td><code id="gIndex_+3A_pch">pch</code></td>
<td>
<p>plotting character for point</p>
</td></tr>
<tr><td><code id="gIndex_+3A_rm.totals">rm.totals</code></td>
<td>
<p>set to <code>TRUE</code> to remove the total <code class="reqn">g</code>-index
when plotting</p>
</td></tr>
<tr><td><code id="gIndex_+3A_sort">sort</code></td>
<td>
<p>specifies how to sort predictors by <code class="reqn">g</code>-index; default
is in descending order going down the dot chart</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For stratification factors in a Cox proportional hazards model, there is
no contribution of variation towards computing a partial <code class="reqn">g</code>
except from terms that interact with the stratification variable.
</p>


<h3>Value</h3>

<p><code>gIndex</code> returns a matrix of class <code>"gIndex"</code> with auxiliary
information stored as attributes, such as variable labels.
<code>GiniMd</code> returns a scalar.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>David HA (1968): Gini's mean difference rediscovered.  Biometrika 55:573&ndash;575.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.rms">predict.rms</a></code>,<code><a href="Hmisc.html#topic+GiniMd">GiniMd</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 40
x &lt;- 1:n
w &lt;- factor(sample(c('a','b'), n, TRUE))
u &lt;- factor(sample(c('A','B'), n, TRUE))
y &lt;- .01*x + .2*(w=='b') + .3*(u=='B') + .2*(w=='b' &amp; u=='B') + rnorm(n)/5
dd &lt;- datadist(x,w,u); options(datadist='dd')
f &lt;- ols(y ~ x*w*u, x=TRUE, y=TRUE)
f
anova(f)
z &lt;- list()
for(type in c('terms','cterms','ccterms'))
  {
    zc &lt;- predict(f, type=type)
    cat('type:', type, '\n')
    print(zc)
    z[[type]] &lt;- zc
  }

zc &lt;- z$cterms
GiniMd(zc[, 1])
GiniMd(zc[, 2])
GiniMd(zc[, 3])
GiniMd(f$linear.predictors)
g &lt;- gIndex(f)
g
g['Total',]
gIndex(f, partials=FALSE)
gIndex(f, type='cterms')
gIndex(f, type='terms')

y &lt;- y &gt; .8
f &lt;- lrm(y ~ x * w * u, x=TRUE, y=TRUE)
gIndex(f, fun=plogis, funlabel='Prob[y=1]')

# Manual calculation of combined main effect + interaction effort of
# sex in a 2x2 design with treatments A B, sexes F M,
# model -.1 + .3*(treat=='B') + .5*(sex=='M') + .4*(treat=='B' &amp; sex=='M')

set.seed(1)
X &lt;- expand.grid(treat=c('A','B'), sex=c('F', 'M'))
a &lt;- 3; b &lt;- 7; c &lt;- 13; d &lt;- 5
X &lt;- rbind(X[rep(1, a),], X[rep(2, b),], X[rep(3, c),], X[rep(4, d),])
y &lt;- with(X, -.1 + .3*(treat=='B') + .5*(sex=='M') + .4*(treat=='B' &amp; sex=='M')) 
f &lt;- ols(y ~ treat*sex, data=X, x=TRUE)
gIndex(f, type='cterms')
k &lt;- coef(f)
b1 &lt;- k[2]; b2 &lt;- k[3]; b3 &lt;- k[4]
n &lt;- nrow(X)
( (a+b)*c*abs(b2) + (a+b)*d*abs(b2+b3) + c*d*abs(b3))/(n*(n-1)/2 )

# Manual calculation for combined age effect in a model with sex,
# age, and age*sex interaction

a &lt;- 13; b &lt;- 7
sex &lt;- c(rep('female',a), rep('male',b))
agef &lt;- round(runif(a, 20, 30))
agem &lt;- round(runif(b, 20, 40))
age  &lt;- c(agef, agem)
y &lt;- (sex=='male') + age/10 - (sex=='male')*age/20
f &lt;- ols(y ~ sex*age, x=TRUE)
f
gIndex(f, type='cterms')
k &lt;- coef(f)
b1 &lt;- k[2]; b2 &lt;- k[3]; b3 &lt;- k[4]
n &lt;- a + b
sp &lt;- function(w, z=w) sum(outer(w, z, function(u, v) abs(u-v)))

( abs(b2)*sp(agef) + abs(b2+b3)*sp(agem) + 2*sp(b2*agef, (b2+b3)*agem) ) / (n*(n-1))

( abs(b2)*GiniMd(agef)*a*(a-1) + abs(b2+b3)*GiniMd(agem)*b*(b-1) +
  2*sp(b2*agef, (b2+b3)*agem) ) / (n*(n-1))

## Not run: 
# Compare partial and total g-indexes over many random fits
plot(NA, NA, xlim=c(0,3), ylim=c(0,3), xlab='Global',
     ylab='x1 (black)  x2 (red)  x3 (green)  x4 (blue)')
abline(a=0, b=1, col=gray(.9))
big &lt;- integer(3)
n &lt;- 50   # try with n=7 - see lots of exceptions esp. for interacting var
for(i in 1:100) {
   x1 &lt;- runif(n)
   x2 &lt;- runif(n)
   x3 &lt;- runif(n)
   x4 &lt;- runif(n)
   y  &lt;- x1 + x2 + x3 + x4 + 2*runif(n)
   f &lt;- ols(y ~ x1*x2+x3+x4, x=TRUE)
   # f &lt;- ols(y ~ x1+x2+x3+x4, x=TRUE)   # also try this
   w &lt;- gIndex(f)[,1]
   gt &lt;- w['Total']
   points(gt, w['x1, x2'])
   points(gt, w['x3'], col='green')
   points(gt, w['x4'], col='blue')
   big[1] &lt;- big[1] + (w['x1, x2'] &gt; gt)
   big[2] &lt;- big[2] + (w['x3'] &gt; gt)
   big[3] &lt;- big[3] + (w['x4'] &gt; gt)
   }
print(big)

## End(Not run)

options(datadist=NULL)
</code></pre>

<hr>
<h2 id='Glm'>rms Version of glm</h2><span id='topic+Glm'></span>

<h3>Description</h3>

<p>This function saves <code>rms</code> attributes with the fit object so that
<code>anova.rms</code>, <code>Predict</code>, etc. can be used just as with <code>ols</code>
and other fits.  No <code>validate</code> or <code>calibrate</code> methods exist for
<code>Glm</code> though.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Glm(
  formula,
  family = gaussian,
  data = environment(formula),
  weights,
  subset,
  na.action = na.delete,
  start = NULL,
  offset = NULL,
  control = glm.control(...),
  model = TRUE,
  method = "glm.fit",
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Glm_+3A_formula">formula</code>, <code id="Glm_+3A_family">family</code>, <code id="Glm_+3A_data">data</code>, <code id="Glm_+3A_weights">weights</code>, <code id="Glm_+3A_subset">subset</code>, <code id="Glm_+3A_na.action">na.action</code>, <code id="Glm_+3A_start">start</code>, <code id="Glm_+3A_offset">offset</code>, <code id="Glm_+3A_control">control</code>, <code id="Glm_+3A_model">model</code>, <code id="Glm_+3A_method">method</code>, <code id="Glm_+3A_x">x</code>, <code id="Glm_+3A_y">y</code>, <code id="Glm_+3A_contrasts">contrasts</code></td>
<td>
<p>see <code><a href="stats.html#topic+glm">stats::glm()</a></code>; for <code>print</code> <code>x</code> is the result of <code>Glm</code></p>
</td></tr>
<tr><td><code id="Glm_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the <code>print</code> method, format of output is controlled by the user
previously running <code>options(prType="lang")</code> where <code>lang</code> is
<code>"plain"</code> (the default), <code>"latex"</code>, or <code>"html"</code>.
</p>


<h3>Value</h3>

<p>a fit object like that produced by <code><a href="stats.html#topic+glm">stats::glm()</a></code> but with
<code>rms</code> attributes and a <code>class</code> of <code>"rms"</code>, <code>"Glm"</code>,
<code>"glm"</code>, and <code>"lm"</code>.  The <code>g</code> element of the fit object is
the <code class="reqn">g</code>-index.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code>,<code><a href="Hmisc.html#topic+GiniMd">Hmisc::GiniMd()</a></code>, <code><a href="#topic+prModFit">prModFit()</a></code>, <a href="stats.html#topic+glm.summaries">stats::residuals.glm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Dobson (1990) Page 93: Randomized Controlled Trial :
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
f &lt;- glm(counts ~ outcome + treatment, family=poisson())
f
anova(f)
summary(f)
f &lt;- Glm(counts ~ outcome + treatment, family=poisson())
# could have had rcs( ) etc. if there were continuous predictors
f
anova(f)
summary(f, outcome=c('1','2','3'), treatment=c('1','2','3'))

</code></pre>

<hr>
<h2 id='Gls'>Fit Linear Model Using Generalized Least Squares</h2><span id='topic+Gls'></span><span id='topic+print.Gls'></span>

<h3>Description</h3>

<p>This function fits a linear model using generalized least
squares. The errors are allowed to be correlated and/or have unequal
variances.  <code>Gls</code> is a slightly enhanced version of the
Pinheiro and Bates <code>gls</code> function in the <code>nlme</code> package to
make it easy to use with the rms package and to implement cluster
bootstrapping (primarily for nonparametric estimates of the
variance-covariance matrix of the parameter estimates and for
nonparametric confidence limits of correlation parameters).
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>. When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gls(model, data, correlation, weights, subset, method, na.action=na.omit,
    control, verbose, B=0, dupCluster=FALSE, pr=FALSE, x=FALSE)

## S3 method for class 'Gls'
print(x, digits=4, coefs=TRUE, title, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gls_+3A_model">model</code></td>
<td>
<p>a two-sided linear formula object describing the
model, with the response on the left of a <code>~</code> operator and the
terms, separated by <code>+</code> operators, on the right.</p>
</td></tr>
<tr><td><code id="Gls_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables named in
<code>model</code>, <code>correlation</code>, <code>weights</code>, and
<code>subset</code>. By default the variables are taken from the
environment from which <code>gls</code> is called.</p>
</td></tr>
<tr><td><code id="Gls_+3A_correlation">correlation</code></td>
<td>
<p>an optional <code>corStruct</code> object describing the
within-group correlation structure. See the documentation of
<code>corClasses</code> for a description of the available <code>corStruct</code>
classes. If a grouping variable is to be used, it must be specified in
the <code>form</code> argument to the <code>corStruct</code>
constructor. Defaults to <code>NULL</code>, corresponding to uncorrelated 
errors.</p>
</td></tr>  
<tr><td><code id="Gls_+3A_weights">weights</code></td>
<td>
<p>an optional <code>varFunc</code> object or one-sided formula
describing the within-group heteroscedasticity structure. If given as
a formula, it is used as the argument to <code>varFixed</code>,
corresponding to fixed variance weights. See the documentation on
<code>varClasses</code> for a description of the available <code>varFunc</code>
classes. Defaults to <code>NULL</code>, corresponding to homoscesdatic
errors.</p>
</td></tr> 
<tr><td><code id="Gls_+3A_subset">subset</code></td>
<td>
<p>an optional expression indicating which subset of the rows of
<code>data</code> should  be  used in the fit. This can be a logical
vector, or a numeric vector indicating which observation numbers are
to be included, or a  character  vector of the row names to be
included.  All observations are included by default.</p>
</td></tr>
<tr><td><code id="Gls_+3A_method">method</code></td>
<td>
<p>a character string.  If <code>"REML"</code> the model is fit by
maximizing the restricted log-likelihood.  If <code>"ML"</code> the
log-likelihood is maximized.  Defaults to <code>"REML"</code>.</p>
</td></tr>
<tr><td><code id="Gls_+3A_na.action">na.action</code></td>
<td>
<p>a function that indicates what should happen when the
data contain <code>NA</code>s.  The default action (<code>na.omit</code>) results
in deletion of observations having any of the variables of interest missing.</p>
</td></tr>
<tr><td><code id="Gls_+3A_control">control</code></td>
<td>
<p>a list of control values for the estimation algorithm to
replace the default values returned by the function <code>glsControl</code>.
Defaults to an empty list.</p>
</td></tr>
<tr><td><code id="Gls_+3A_verbose">verbose</code></td>
<td>
<p>an optional logical value. If <code>TRUE</code> information on
the evolution of the iterative algorithm is printed. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Gls_+3A_b">B</code></td>
<td>
<p>number of bootstrap resamples to fit and store, default is
none</p>
</td></tr>
<tr><td><code id="Gls_+3A_dupcluster">dupCluster</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>Gls</code> when
bootstrapping to consider multiply-sampled clusters as if they were
one large cluster when fitting using the <code>gls</code> algorithm</p>
</td></tr>
<tr><td><code id="Gls_+3A_pr">pr</code></td>
<td>
<p>set to <code>TRUE</code> to show progress of bootstrap resampling</p>
</td></tr>
<tr><td><code id="Gls_+3A_x">x</code></td>
<td>
<p>for <code>Gls</code> set to <code>TRUE</code> to store the design matrix
in the fit object; otherwise the result of <code>Gls</code></p>
</td></tr>
<tr><td><code id="Gls_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to print</p>
</td></tr>
<tr><td><code id="Gls_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="Gls_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="Gls_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="Hmisc.html#topic+na.delete">na.delete</a></code> function will not work with
<code>Gls</code> due to some nuance in the <code>model.frame.default</code>
function.  This probably relates to <code>na.delete</code> storing extra
information in the <code>"na.action"</code> attribute of the returned data
frame.
</p>


<h3>Value</h3>

<p>an object of classes <code>Gls</code>, <code>rms</code>, and <code>gls</code>
representing the linear model
fit. Generic functions such as <code>print</code>, <code>plot</code>,
<code>ggplot</code>, and <code>summary</code> have methods to show the results of
the fit. See 
<code>glsObject</code> for the components of the fit. The functions
<code>resid</code>, <code>coef</code>, and <code>fitted</code> can be used to extract
some of its components.  <code>Gls</code> returns the following components
not returned by <code>gls</code>: <code>Design</code>, <code>assign</code>,
<code>formula</code> (see arguments), <code>B</code> (see
arguments), <code>bootCoef</code> (matrix of <code>B</code> bootstrapped
coefficients), <code>boot.Corr</code> (vector of bootstrapped correlation
parameters), <code>Nboot</code> (vector of total sample size used in each
bootstrap (may vary if have unbalanced clusters), and <code>var</code>
(sample variance-covariance matrix of bootstrapped coefficients).  The
<code class="reqn">g</code>-index is also stored in the returned object under the name
<code>"g"</code>.
</p>


<h3>Author(s)</h3>

<p>Jose Pinheiro,
Douglas Bates <a href="mailto:bates@stat.wisc.edu">bates@stat.wisc.edu</a>,
Saikat DebRoy,
Deepayan Sarkar,
R-core <a href="mailto:R-core@R-project.org">R-core@R-project.org</a>,
Frank Harrell <a href="mailto:fh@fharrell.com">fh@fharrell.com</a>,
Patrick Aboyoun
</p>


<h3>References</h3>

<p>Pinheiro J, Bates D (2000): Mixed effects models in S and S-Plus.  New
York: Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="nlme.html#topic+gls">gls</a></code>
<code><a href="nlme.html#topic+glsControl">glsControl</a></code>, <code><a href="nlme.html#topic+glsObject">glsObject</a></code>,
<code><a href="nlme.html#topic+varFunc">varFunc</a></code>, <code><a href="nlme.html#topic+corClasses">corClasses</a></code>,
<code><a href="nlme.html#topic+varClasses">varClasses</a></code>, <code><a href="Hmisc.html#topic+GiniMd">GiniMd</a></code>,
<code><a href="#topic+prModFit">prModFit</a></code>, <code><a href="#topic+logLik.Gls">logLik.Gls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(ggplot2)
ns  &lt;- 20  # no. subjects
nt  &lt;- 10  # no. time points/subject
B   &lt;- 10  # no. bootstrap resamples
           # usually do 100 for variances, 1000 for nonparametric CLs
rho &lt;- .5  # AR(1) correlation parameter
V &lt;- matrix(0, nrow=nt, ncol=nt)
V &lt;- rho^abs(row(V)-col(V))   # per-subject correlation/covariance matrix

d &lt;- expand.grid(tim=1:nt, id=1:ns)
d$trt &lt;- factor(ifelse(d$id &lt;= ns/2, 'a', 'b'))
true.beta &lt;- c(Intercept=0,tim=.1,'tim^2'=0,'trt=b'=1)
d$ey  &lt;- true.beta['Intercept'] + true.beta['tim']*d$tim +
  true.beta['tim^2']*(d$tim^2) +  true.beta['trt=b']*(d$trt=='b')
set.seed(13)
library(MASS)   # needed for mvrnorm
d$y &lt;- d$ey + as.vector(t(mvrnorm(n=ns, mu=rep(0,nt), Sigma=V)))

dd &lt;- datadist(d); options(datadist='dd')
f &lt;- Gls(y ~ pol(tim,2) + trt, correlation=corCAR1(form= ~tim | id),
         data=d, B=B)
f
AIC(f)
f$var      # bootstrap variances
f$varBeta  # original variances
summary(f)
anova(f)
ggplot(Predict(f, tim, trt))
# v &lt;- Variogram(f, form=~tim|id, data=d)
nlme:::summary.gls(f)$tTable   # print matrix of estimates etc.

options(datadist=NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='groupkm'>Kaplan-Meier Estimates vs. a Continuous Variable</h2><span id='topic+groupkm'></span>

<h3>Description</h3>

<p>Function to divide <code>x</code> (e.g. age, or predicted survival at time
<code>u</code> created by <code>survest</code>) into <code>g</code> quantile groups, get
Kaplan-Meier estimates at time <code>u</code> (a scaler), and to return a
matrix with columns <code>x</code>=mean <code>x</code> in quantile, <code>n</code>=number
of subjects, <code>events</code>=no. events, and <code>KM</code>=K-M survival at
time <code>u</code>, <code>std.err</code> = s.e. of -log K-M.  Confidence intervals
are based on -log S(t).  Instead of supplying <code>g</code>, the user can
supply the minimum number of subjects to have in the quantile group
(<code>m</code>, default=50).  If <code>cuts</code> is given
(e.g. <code>cuts=c(0,.1,.2,...,.9,.1)</code>), it overrides <code>m</code> and
<code>g</code>.  Calls Therneau's <code>survfitKM</code> in the <code>survival</code>
package to get Kaplan-Meiers estimates and standard errors.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>groupkm(x, Srv, m=50, g, cuts, u, 
        pl=FALSE, loglog=FALSE, conf.int=.95, xlab, ylab,
        lty=1, add=FALSE, cex.subtitle=.7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="groupkm_+3A_x">x</code></td>
<td>
<p>variable to stratify</p>
</td></tr>
<tr><td><code id="groupkm_+3A_srv">Srv</code></td>
<td>

<p>a <code>Surv</code> object - n x 2 matrix containing survival
time and event/censoring 
1/0 indicator.  Units of measurement come from the &quot;units&quot; attribute
of the survival time variable.  &quot;Day&quot; is the default.
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_m">m</code></td>
<td>
<p>desired minimum number of observations in a group</p>
</td></tr>
<tr><td><code id="groupkm_+3A_g">g</code></td>
<td>
<p>number of quantile groups</p>
</td></tr>
<tr><td><code id="groupkm_+3A_cuts">cuts</code></td>
<td>
<p>actual cuts in <code>x</code>, e.g. <code>c(0,1,2)</code> to use [0,1), [1,2].
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_u">u</code></td>
<td>
<p>time for which to estimate survival</p>
</td></tr>
<tr><td><code id="groupkm_+3A_pl">pl</code></td>
<td>
<p>TRUE to plot results</p>
</td></tr>
<tr><td><code id="groupkm_+3A_loglog">loglog</code></td>
<td>

<p>set to <code>TRUE</code> to plot <code>log(-log(survival))</code> instead of survival
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_conf.int">conf.int</code></td>
<td>

<p>defaults to <code>.95</code> for 0.95 confidence bars.  Set to <code>FALSE</code> to suppress bars.
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_xlab">xlab</code></td>
<td>

<p>if <code>pl=TRUE</code>, is x-axis label.  Default is <code>label(x)</code> or name of calling argument
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_ylab">ylab</code></td>
<td>

<p>if <code>pl=TRUE</code>, is y-axis label.  Default is constructed from <code>u</code> and time <code>units</code>
attribute.
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_lty">lty</code></td>
<td>

<p>line time for primary line connecting estimates
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> if adding to an existing plot
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_cex.subtitle">cex.subtitle</code></td>
<td>

<p>character size for subtitle. Default is <code>.7</code>.  Use <code>FALSE</code> to
suppress subtitle. 
</p>
</td></tr>
<tr><td><code id="groupkm_+3A_...">...</code></td>
<td>
<p>plotting parameters to pass to the plot and errbar functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with columns named <code>x</code> (mean predictor value in interval), <code>n</code> (sample size
in interval), <code>events</code> (number of events in interval), <code>KM</code> (Kaplan-Meier
estimate), <code>std.err</code> (standard error of -log <code>KM</code>)
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+survfit">survfit</a></code>, <code><a href="Hmisc.html#topic+errbar">errbar</a></code>,
<code><a href="Hmisc.html#topic+cut2">cut2</a></code>, <code><a href="survival.html#topic+Surv">Surv</a></code>,
<code><a href="Hmisc.html#topic+units">units</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50))
d.time &lt;- -log(runif(n))/h
label(d.time) &lt;- 'Follow-up Time'
e &lt;- ifelse(d.time &lt;= cens,1,0)
d.time &lt;- pmin(d.time, cens)
units(d.time) &lt;- "Year"
groupkm(age, Surv(d.time, e), g=10, u=5, pl=TRUE)
#Plot 5-year K-M survival estimates and 0.95 confidence bars by 
#decile of age.  If omit g=10, will have &gt;= 50 obs./group.
</code></pre>

<hr>
<h2 id='hazard.ratio.plot'>Hazard Ratio Plot</h2><span id='topic+hazard.ratio.plot'></span>

<h3>Description</h3>

<p>The <code>hazard.ratio.plot</code> function repeatedly estimates Cox
regression coefficients and confidence limits within time intervals.
The log hazard ratios are plotted against the mean failure/censoring
time within the interval. Unless <code>times</code> is specified, the number of
time intervals will be <code class="reqn">\max(round(d/e),2)</code>, where <code class="reqn">d</code> is the
total number 
of events in the sample. Efron's likelihood is used for estimating
Cox regression coefficients (using <code>coxph.fit</code>).  In the case of
tied failure times, some intervals may have a point in common.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hazard.ratio.plot(x, Srv, which, times=, e=30, subset,
                  conf.int=.95, legendloc=NULL, smooth=TRUE, pr=FALSE, pl=TRUE,
                  add=FALSE, ylim, cex=.5, xlab="t", ylab, antilog=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hazard.ratio.plot_+3A_x">x</code></td>
<td>

<p>a vector or matrix of predictors
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_srv">Srv</code></td>
<td>
<p>a <code>Surv</code> object</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_which">which</code></td>
<td>

<p>a vector of column numbers of <code>x</code> for which to estimate hazard
ratios across time and make plots.
The default is to do so for all predictors.  Whenever
one predictor is displayed, all other predictors in the <code>x</code> matrix
are adjusted for (with a separate adjustment form for each time interval).
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_times">times</code></td>
<td>

<p>optional vector of time interval endpoints.
Example: <code>times=c(1,2,3)</code> uses intervals <code>[0,1), [1,2), [2,3), [3+)</code>.
If times is omitted, uses intervals containing <code>e</code> events
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_e">e</code></td>
<td>

<p>number of events per time interval if times not given
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_subset">subset</code></td>
<td>

<p>vector used for subsetting the entire analysis,
e.g. <code>subset=sex=="female"</code>
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence interval coverage
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_legendloc">legendloc</code></td>
<td>

<p>location for legend. Omit to use mouse, <code>"none"</code> for none,
<code>"ll"</code> for lower left of graph, or actual x and y coordinates (e.g.
<code>c(2,3)</code>)
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_smooth">smooth</code></td>
<td>

<p>also plot the super&ndash;smoothed version of the log hazard ratios
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_pr">pr</code></td>
<td>

<p>defaults to <code>FALSE</code> to suppress printing of individual Cox fits
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_pl">pl</code></td>
<td>

<p>defaults to <code>TRUE</code> to plot results
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_add">add</code></td>
<td>

<p>add this plot to an already existing plot
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_ylim">ylim</code></td>
<td>

<p>vector of <code>y</code>-axis limits. Default is computed to include confidence bands.
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_cex">cex</code></td>
<td>

<p>character size for legend information, default is 0.5
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_xlab">xlab</code></td>
<td>

<p>label for <code>x</code>-axis, default is <code>"t"</code>
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_ylab">ylab</code></td>
<td>

<p>label for <code>y</code>-axis, default is <code>"Log Hazard Ratio"</code> or <code>"Hazard Ratio"</code>,
depending on <code>antilog</code>.
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_antilog">antilog</code></td>
<td>

<p>default is <code>FALSE</code>. Set to <code>TRUE</code> to plot anti-log, i.e., hazard ratio.
</p>
</td></tr>
<tr><td><code id="hazard.ratio.plot_+3A_...">...</code></td>
<td>

<p>optional graphical parameters
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+cox.zph">cox.zph</a></code>, <code><a href="#topic+residuals.cph">residuals.cph</a></code>,
<code><a href="survival.html#topic+survival-internal">survival-internal</a></code>, <code><a href="#topic+cph">cph</a></code>,
<code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="survival.html#topic+Surv">Surv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
n &lt;- 500
set.seed(1)
age &lt;- 50 + 12*rnorm(n)
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50))
d.time &lt;- -log(runif(n))/h
label(d.time) &lt;- 'Follow-up Time'
e &lt;- ifelse(d.time &lt;= cens,1,0)
d.time &lt;- pmin(d.time, cens)
units(d.time) &lt;- "Year"
hazard.ratio.plot(age, Surv(d.time,e), e=20, legendloc='ll')
</code></pre>

<hr>
<h2 id='ie.setup'>Intervening Event Setup</h2><span id='topic+ie.setup'></span>

<h3>Description</h3>

<p>Creates several new variables which help set up a dataset for modeling
with <code>cph</code> or <code>coxph</code> when there is a single binary time-dependent
covariable which turns on at a given time, and stays on.  This is
typical when analyzing the impact of an intervening event.
<code>ie.setup</code> creates a <code>Surv</code> object using the start time, stop time
format.  It also creates a binary indicator for the intervening event,
and a variable called <code>subs</code> that is useful when <code>attach</code>-ing
a dataframe. 
<code>subs</code> has observation numbers duplicated for subjects having an
intervening event, so those subject's baseline covariables (that are
not time-dependent) can be duplicated correctly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ie.setup(failure.time, event, ie.time, break.ties=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ie.setup_+3A_failure.time">failure.time</code></td>
<td>

<p>a numeric variable containing the event or censoring times for the
terminating event
</p>
</td></tr>
<tr><td><code id="ie.setup_+3A_event">event</code></td>
<td>

<p>a binary (0/1) variable specifying whether observations had the
terminating event (event=1) or were censored (event=0)
</p>
</td></tr>
<tr><td><code id="ie.setup_+3A_ie.time">ie.time</code></td>
<td>

<p>intervening event times.  For subjects having no intervening events,
the corresponding values of ie.time must be NA.
</p>
</td></tr>
<tr><td><code id="ie.setup_+3A_break.ties">break.ties</code></td>
<td>

<p>Occasionally intervening events are recorded as happening at exactly
the same time as the termination of follow-up for some subjects.
The <code>Surv</code> and <code>Surv</code> functions will not allow this.  To
randomly break the ties 
by subtracting a random number from such tied intervening event times,
specify <code>break.ties=TRUE</code>.  The random number is uniform between zero and
the minimum difference between any two untied <code>failure.time</code>s.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a list with components <code>S, ie.status, subs, reps</code>.  <code>S</code> is a
<code>Surv</code>
object containing start and stop times for intervals of observation, 
along with event indicators.  <code>ie.status</code> is one if the intervening
event has occurred at the start of the interval, zero otherwise.
<code>subs</code> is a vector of subscripts that can be used to replicate other
variables the same way <code>S</code> was replicated.  <code>reps</code> specifies how many
times each original observation was replicated.  <code>S, ie.status, subs</code> are
all the same length (at least the number of rows for <code>S</code> is) and are longer than the original <code>failure.time</code> vector.
<code>reps</code> is the same length as the original <code>failure.time</code> vector.
The <code>subs</code> vector is suitable for passing to <code>validate.lrm</code> or <code>calibrate</code>,
which pass this vector under the name <code>cluster</code> on to <code>predab.resample</code> so that bootstrapping can be
done by sampling with replacement from the original subjects rather than
from the individual records created by <code>ie.setup</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cph">cph</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>,
<code><a href="survival.html#topic+Surv">Surv</a></code>, <code><a href="#topic+cr.setup">cr.setup</a></code>,
<code><a href="#topic+predab.resample">predab.resample</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>failure.time &lt;- c(1 ,   2,   3)
event        &lt;- c(1 ,   1,   0)
ie.time      &lt;- c(NA, 1.5, 2.5)

z &lt;- ie.setup(failure.time, event, ie.time)
S &lt;- z$S
S
ie.status &lt;- z$ie.status
ie.status
z$subs
z$reps
## Not run: 
attach(input.data.frame[z$subs,])   #replicates all variables
f &lt;- cph(S ~ age + sex + ie.status)
# Instead of duplicating rows of data frame, could do this:
attach(input.data.frame)
z &lt;- ie.setup(failure.time, event, ie.time)
s &lt;- z$subs
age &lt;- age[s]
sex &lt;- sex[s]
f &lt;- cph(S ~ age + sex + ie.status)

## End(Not run)
</code></pre>

<hr>
<h2 id='impactPO'>Impact of Proportional Odds Assumpton</h2><span id='topic+impactPO'></span>

<h3>Description</h3>

<p>Checks the impact of the proportional odds assumption by comparing predicted cell probabilities from a PO model with those from a multinomial or partial proportional odds logistic model that relax assumptions.  For a given model formula, fits the model with both <code>lrm</code> and either <code>nnet::multinom</code> or <code>VGAM::vglm</code> or both, and obtains predicted cell probabilities for the PO and relaxed models on the <code>newdata</code> data frame.  A <code>print</code> method formats the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impactPO(
  formula,
  relax = if (missing(nonpo)) "multinomial" else "both",
  nonpo,
  newdata,
  data = environment(formula),
  minfreq = 15,
  B = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impactPO_+3A_formula">formula</code></td>
<td>
<p>a model formula.  To work properly with <code>multinom</code> or <code>vglm</code> the terms should have completely specified knot locations if a spline function is being used.</p>
</td></tr>
<tr><td><code id="impactPO_+3A_relax">relax</code></td>
<td>
<p>defaults to <code>"both"</code> if <code>nonpo</code> is given, resulting in fitting two relaxed models.  Set <code>relax</code> to <code>"multinomial"</code> or <code>"ppo"</code> to fit only one relaxed model.  The multinomial model does not assume PO for any predictor.</p>
</td></tr>
<tr><td><code id="impactPO_+3A_nonpo">nonpo</code></td>
<td>
<p>a formula with no left hand side variable, specifying the variable or variables for which PO is not assumed.  Specifying <code>nonpo</code> results in a relaxed fit that is a partial PO model fitted with <code>VGAM::vglm</code>.</p>
</td></tr>
<tr><td><code id="impactPO_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or data table with one row per covariate setting for which predictions are to be made</p>
</td></tr>
<tr><td><code id="impactPO_+3A_data">data</code></td>
<td>
<p>data frame containing variables to fit; default is the frame in which <code>formula</code> is found</p>
</td></tr>
<tr><td><code id="impactPO_+3A_minfreq">minfreq</code></td>
<td>
<p>minimum sample size to allow for the least frequent category of the dependent variable.  If the observed minimum frequency is less than this, the <code><a href="Hmisc.html#topic+combine.levels">combine.levels()</a></code> function will be called to combine enough consecutive levels so that this minimum frequency is achieved.</p>
</td></tr>
<tr><td><code id="impactPO_+3A_b">B</code></td>
<td>
<p>number of bootstrap resamples to do to get confidence intervals for differences in predicted probabilities for relaxed methods vs. PO model fits.  Default is not to run the bootstrap.  When running the bootstrap make sure that all model variables are explicitly in <code style="white-space: pre;">&#8288;data=&#8288;</code> so that selection of random subsets of data will call along the correct rows for all predictors.</p>
</td></tr>
<tr><td><code id="impactPO_+3A_...">...</code></td>
<td>
<p>other parameters to pass to <code>lrm</code> and <code>multinom</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since partial proportional odds models and especially multinomial logistic models can have many parameters, it is not feasible to use this model comparison approach when the number of levels of the dependent variable Y is large.  By default, the function will use <code><a href="Hmisc.html#topic+combine.levels">combine.levels()</a></code> to combine consecutive levels if the lowest frequency category of Y has fewer than <code>minfreq</code> observations.
</p>


<h3>Value</h3>

<p>an <code>impactPO</code> object which is a list with elements <code>estimates</code>, <code>stats</code>, <code>mad</code>, <code>newdata</code>, <code>nboot</code>, and <code>boot</code>.  <code>estimates</code> is a data frame containing the variables and values in <code>newdata</code> in a tall and thin format with additional variable <code>method</code> (&quot;PO&quot;, &quot;Multinomial&quot;, &quot;PPO&quot;), <code>y</code> (current level of the dependent variable), and <code>Probability</code> (predicted cell probability for covariate values and value of <code>y</code> in the current row).  <code>stats</code> is a data frame containing <code>Deviance</code> the model deviance, <code>d.f.</code> the total number of parameters counting intercepts, <code>AIC</code>, <code>p</code> the number of regression coefficients, <code style="white-space: pre;">&#8288;LR chi^2&#8288;</code> the likelihood ratio chi-square statistic for testing the predictors, <code>LR - p</code> a chance-corrected LR chi-square, <code style="white-space: pre;">&#8288;LR chi^2 test for PO&#8288;</code> the likelihood ratio chi-square test statistic for testing the PO assumption (by comparing -2 log likelihood for a relaxed model to that of a fully PO model), <code>  d.f.</code> the degrees of freedom for this test, <code style="white-space: pre;">&#8288;  Pr(&gt;chi^2)&#8288;</code> the P-value for this test, <code style="white-space: pre;">&#8288;MCS R2&#8288;</code> the Maddala-Cox-Snell R2 using the actual sample size, <code style="white-space: pre;">&#8288;MCS R2 adj&#8288;</code> (<code style="white-space: pre;">&#8288;MCS R2&#8288;</code> adjusted for estimating <code>p</code> regression coefficients by subtracting <code>p</code> from <code>LR</code>), <code style="white-space: pre;">&#8288;McFadden R2&#8288;</code>, <code style="white-space: pre;">&#8288;McFadden R2 adj&#8288;</code> (an AIC-like adjustment proposed by McFadden without full justification), <code style="white-space: pre;">&#8288;Mean |difference} from PO&#8288;</code> the overall mean absolute difference between predicted probabilities over all categories of Y and over all covariate settings.  <code>mad</code> contains <code>newdata</code> and separately by rows in <code>newdata</code> the mean absolute difference (over Y categories) between estimated probabilities by the indicated relaxed model and those from the PO model.  <code>nboot</code> is the number of successful bootstrap repetitions, and <code>boot</code> is a 4-way array with dimensions represented by the <code>nboot</code> resamples, the number of rows in <code>newdata</code>, the number of outcome levels, and elements for <code>PPO</code> and <code>multinomial</code>.  For the modifications of the Maddala-Cox-Snell indexes see <code>Hmisc::R2Measures</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell <a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p><a href="https://hbiostat.org/bib/r2.html">Adjusted R-square note</a>
</p>


<h3>See Also</h3>

<p><code><a href="nnet.html#topic+multinom">nnet::multinom()</a></code>, <code><a href="VGAM.html#topic+vglm">VGAM::vglm()</a></code>, <code><a href="#topic+lrm">lrm()</a></code>, <code><a href="Hmisc.html#topic+popower">Hmisc::propsPO()</a></code>, <code><a href="Hmisc.html#topic+R2Measures">Hmisc::R2Measures()</a></code>, <code><a href="Hmisc.html#topic+combine.levels">Hmisc::combine.levels()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
set.seed(1)
age &lt;- rnorm(500, 50, 10)
sex &lt;- sample(c('female', 'male'), 500, TRUE)
y   &lt;- sample(0:4, 500, TRUE)
d   &lt;- expand.grid(age=50, sex=c('female', 'male'))
w   &lt;- impactPO(y ~ age + sex, nonpo = ~ sex, newdata=d)
w
# Note that PO model is a better model than multinomial (lower AIC)
# since multinomial model's improvement in fit is low in comparison
# with number of additional parameters estimated.  Same for PO model
# in comparison with partial PO model.

# Reverse levels of y so stacked bars have higher y located higher
revo &lt;- function(z) {
  z &lt;- as.factor(z)
  factor(z, levels=rev(levels(as.factor(z))))
}

require(ggplot2)
ggplot(w$estimates, aes(x=method, y=Probability, fill=revo(y))) +
  facet_wrap(~ sex) + geom_col() +
  xlab('') + guides(fill=guide_legend(title=''))

# Now vary 2 predictors

d &lt;- expand.grid(sex=c('female', 'male'), age=c(40, 60))
w &lt;- impactPO(y ~ age + sex, nonpo = ~ sex, newdata=d)
w
ggplot(w$estimates, aes(x=method, y=Probability, fill=revo(y))) +
  facet_grid(age ~ sex) + geom_col() +
 xlab('') + guides(fill=guide_legend(title=''))

## End(Not run)
</code></pre>

<hr>
<h2 id='importedexported'>Exported Functions That Were Imported From Other Packages</h2><span id='topic+Surv'></span><span id='topic+ggplot'></span>

<h3>Description</h3>

<p><code>Surv</code> and <code>ggplot</code> are imported from, respectively, the
<code>survival</code> and <code>ggplot2</code> packages and are exported from
<code>rms</code> so that the user does not have to attach these packages to do
simple things.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Surv(time, time2, event,
     type = c("right", "left", "interval", "counting", "interval2", "mstate"),
     origin = 0)

ggplot(data = NULL, mapping = aes(), ..., environment =  parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importedexported_+3A_time">time</code>, <code id="importedexported_+3A_time2">time2</code>, <code id="importedexported_+3A_event">event</code>, <code id="importedexported_+3A_type">type</code>, <code id="importedexported_+3A_origin">origin</code></td>
<td>
<p>see <code><a href="survival.html#topic+Surv">Surv</a></code></p>
</td></tr>
<tr><td><code id="importedexported_+3A_data">data</code>, <code id="importedexported_+3A_mapping">mapping</code>, <code id="importedexported_+3A_...">...</code>, <code id="importedexported_+3A_environment">environment</code></td>
<td>
<p>see <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>see documentation in the original packages</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+Surv">Surv</a></code>,
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
f &lt;- psm(Surv(dtime, death) ~ x1 + x2 + sex + race, dist='gau')
ggplot(Predict(f))

## End(Not run)
</code></pre>

<hr>
<h2 id='latex.cph'>LaTeX Representation of a Fitted Cox Model</h2><span id='topic+latex.cph'></span><span id='topic+latex.lrm'></span><span id='topic+latex.ols'></span><span id='topic+latex.orm'></span><span id='topic+latex.pphsm'></span><span id='topic+latex.psm'></span>

<h3>Description</h3>

<p>Creates a file containing a LaTeX representation of the fitted model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cph'
latex(object, title,
      file='', 
      append=FALSE, surv=TRUE, maxt=FALSE, which=NULL, varnames, columns=65, 
      inline=FALSE, before=if(inline)"" else "&amp; &amp;", after="", dec=3,
      pretrans=TRUE, caption, digits=.Options$digits, size="",
      ...) # for cph fit

## S3 method for class 'lrm'
latex(object, title, file, append, which, varnames,
columns, inline, before, after, pretrans, caption,
digits=.Options$digits, size="", ...) # for lrm fit

## S3 method for class 'ols'
latex(object, title, file, append, which, varnames,
columns, inline, before, after, pretrans, caption,
digits=.Options$digits, size="", ...) # ols fit

## S3 method for class 'orm'
latex(object, title, file, append, which, varnames,
columns, inline, before, after, pretrans, caption,
digits=.Options$digits, size="", intercepts=nrp &lt; 10, ...) # for orm fit

## S3 method for class 'pphsm'
latex(object, title, file, append, which=NULL, varnames,
columns, inline, before, after, pretrans, caption,
digits=.Options$digits, size="", ...) # pphsm fit

## S3 method for class 'psm'
latex(object, title, file, append, which=NULL, varnames,
columns, inline, before, after, pretrans, caption,
digits=.Options$digits, size="", ...) # psm fit
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latex.cph_+3A_object">object</code></td>
<td>

<p>a fit object created by a <code>rms</code> fitting function.
</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_title">title</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_file">file</code>, <code id="latex.cph_+3A_append">append</code></td>
<td>
<p>see <code><a href="Hmisc.html#topic+latex">latex.default</a></code>.  Defaults to
the console.  When using html/markdown, <code>file</code> is ignored.</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_surv">surv</code></td>
<td>

<p>if <code>surv=TRUE</code> was specified to <code>cph</code>, the underlying survival
probabilities from <code>object$surv.summary</code> will be placed in a table
unless <code>surv=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_maxt">maxt</code></td>
<td>

<p>if the maximum follow-up time in the data (<code>object$maxtime</code>) exceeds the
last entry in <code>object$surv.summary</code>, underlying survival estimates at
<code>object$maxtime</code> will be added to the table if <code>maxt=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_which">which</code>, <code id="latex.cph_+3A_varnames">varnames</code>, <code id="latex.cph_+3A_columns">columns</code>, <code id="latex.cph_+3A_inline">inline</code>, <code id="latex.cph_+3A_before">before</code>, <code id="latex.cph_+3A_dec">dec</code>, <code id="latex.cph_+3A_pretrans">pretrans</code></td>
<td>
<p>see
<code><a href="Hmisc.html#topic+latex.default">latex.default</a></code></p>
</td></tr>
<tr><td><code id="latex.cph_+3A_after">after</code></td>
<td>
<p>if not an empty string, added to end of markup if
<code>inline=TRUE</code></p>
</td></tr>
<tr><td><code id="latex.cph_+3A_caption">caption</code></td>
<td>
<p>a character string specifying a title for the equation to
be centered and typeset in bold face.   Default is no title.
</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_digits">digits</code></td>
<td>
<p>see <a href="#topic+latexrms">latexrms</a></p>
</td></tr>
<tr><td><code id="latex.cph_+3A_size">size</code></td>
<td>
<p>a LaTeX size to use, without the slash.  Default is the
prevailing size</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_intercepts">intercepts</code></td>
<td>
<p>for <code>orm</code> fits.  Default is to print intercepts
if they are fewer than 10 in number.  Set to <code>TRUE</code> or
<code>FALSE</code> to force.</p>
</td></tr>
<tr><td><code id="latex.cph_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the name of the created file, with class <code>c("latex","file")</code>.  This
object works with latex viewing and printing commands in Hmisc.  If
<code>file=''</code> and <code>options(prType=x</code> is in effect, where <code>x</code>
is <code>"html", "markdown"</code> or <code>"md"</code>, the result is run through
<code>knitr::asis_output</code> so that it will be rendered correctly no
matter which options are in effect in the chunk header.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+latexrms">latexrms</a></code>, <code><a href="Hmisc.html#topic+rcspline.restate">rcspline.restate</a></code>,
<code><a href="Hmisc.html#topic+latex">latex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(survival)
units(ftime) &lt;- "Day"
f &lt;- cph(Surv(ftime, death) ~ rcs(age)+sex, surv=TRUE, time.inc=60)
w &lt;- latex(f, file='f.tex')  #Interprets fitted model and makes table of S0(t)
               #for t=0,60,120,180,...
w              #displays image, if viewer installed and file given above
latex(f)   # send LaTeX code to the console for knitr
options(prType='html')
latex(f)       # for use with knitr and R Markdown/Quarto using MathJax

## End(Not run)
</code></pre>

<hr>
<h2 id='latexrms'>LaTeX Representation of a Fitted Model</h2><span id='topic+latexrms'></span><span id='topic+latex.bj'></span><span id='topic+latex.Glm'></span><span id='topic+latex.Gls'></span>

<h3>Description</h3>

<p>Creates a file containing a LaTeX representation of the fitted model.  For
model-specific typesetting there is <code>latex.lrm</code>, <code>latex.cph</code>,
<code>latex.psm</code> and <code>latex.ols</code>. <code>latex.cph</code> has some
arguments that are specific to <code>cph</code> models.
<code>latexrms</code> is the core function which is
called internally by <code>latexrms</code> (which is called by
<code>latex.cph</code>, <code>latex.ols</code>, etc.).  <code>html</code> and R
Markdown-compatible markup (using MathJax) are written if
<code>options(prType='html')</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latexrms(object,
 file='',
 append=FALSE, which=1:p, varnames, columns=65, prefix=NULL, inline=FALSE,
 before=if(inline)"" else "&amp; &amp;", after="", intercept, pretrans=TRUE,
 digits=.Options$digits, size="")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latexrms_+3A_object">object</code></td>
<td>

<p>a fit object created by a fitting function in the <code>rms</code> series
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_file">file</code></td>
<td>

<p>name of <code>.tex</code> file to create, default is to write to console.
<code>file</code> is ignored when <code>options(prType='html'</code>. 
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_append">append</code></td>
<td>
<p>whether or not to append to an existing file</p>
</td></tr>
<tr><td><code id="latexrms_+3A_which">which</code></td>
<td>

<p>a vector of subcripts (corresponding to <code>object$Design$name</code>)
specifying a submodel to print. Default is to describe the whole
model. 
<code>which</code> can also be a vector of character strings specifying the
factor names to print. Enough of each string is needed to ensure
a unique match. Names for interaction effects are of the form
<code>"age * sex"</code>. For any interaction effect for which you do not
request main effects, the main effects will be added to <code>which</code>.
When <code>which</code> is given, the model structural statement is not
included. In this case, intercepts are not included either.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_varnames">varnames</code></td>
<td>

<p>variable names to substitute for non-interactions. Order must correspond
to <code>object$Design$name</code> and interactions must be omitted.
Default is
<code>object$Design$name[object$Design$assume.code!=9]</code>. <code>varnames</code>
can contain any LaTeX commands such as subscripts and &quot;\\\\frac&quot;    
(all &quot;\&quot; must be quadrupled.)
Any &quot;/&quot; must be preceeded by &quot;\\&quot; (2, not 4 backslashes).
Elements of <code>varnames</code> for interactions are ignored; they can be
set to any value.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_columns">columns</code></td>
<td>

<p>maximum number of columns of printing characters to allow before
outputting a LaTeX newline command
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_prefix">prefix</code></td>
<td>

<p>if given, a LaTeX \lefteqn command of the form <code>\lefteqn{prefix =} \\</code>
will be inserted to print a left-hand-side of the equation.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_inline">inline</code></td>
<td>

<p>Set to <code>TRUE</code> to create text for insertion in an in-line equation. This
text contains only the expansion of X beta, and is not surrounded by
<code>"$"</code>.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_before">before</code></td>
<td>

<p>a character string to place before each line of output. Use the default
for a LaTeX <code>eqnarray</code> environment.  For <code>inline=TRUE</code>, the
<code>before</code> string, if not an empty string, will be placed once
before the entire markup.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_after">after</code></td>
<td>

<p>a character string to place after the output if <code>inline=TRUE</code>
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_intercept">intercept</code></td>
<td>

<p>a special intercept value to include that is not part of the standard
model parameters (e.g., centering constant in Cox model). Only allowed
in the <code>latexrms</code> rendition.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_pretrans">pretrans</code></td>
<td>

<p>if any spline or polynomial-expanded variables are themselves
transformed, a table of pre-transformations will be formed unless
<code>pretrans=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="latexrms_+3A_digits">digits</code></td>
<td>
<p>number of digits of precision to use in formatting
coefficients and other numbers</p>
</td></tr>
<tr><td><code id="latexrms_+3A_size">size</code></td>
<td>
<p>a LaTeX font size to use for the output, without the slash.
Default is current size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>latexrms</code> returns a character vector if <code>file=''</code>,
otherwise writes the output to <code>file</code>.  For particular model
fits, the <code>latex</code> method returns the result of running
<code>knitr::asis_output</code> on the LaTeX or HTML code if <code>file=''</code>,
<code>options(prType)</code> was set but not to <code>'plain'</code>, and if
<code>knitr</code> is currently running.  This causes correct output to be
rendered whether or not <code>results='asis'</code> appeared in the R
Markdown or Quarto chunk header.</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="Hmisc.html#topic+latex">latex</a></code>, <code><a href="Hmisc.html#topic+rcspline.restate">rcspline.restate</a></code>,
<code><a href="#topic+rms">rms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
f &lt;- lrm(death ~ rcs(age)+sex)
w &lt;- latex(f, file='f.tex')
w     # displays, using e.g. xdvi
latex(f)    # send LaTeX code to console, as for knitr
options(prType='html')
latex(f)    # emit html and latex for knitr html and html notebooks

## End(Not run)
</code></pre>

<hr>
<h2 id='lrm'>Logistic Regression Model</h2><span id='topic+lrm'></span><span id='topic+print.lrm'></span>

<h3>Description</h3>

<p>Fit binary and proportional odds ordinal
logistic regression models using maximum likelihood estimation or
penalized maximum likelihood estimation.  See <code>cr.setup</code> for how to
fit forward continuation ratio models with <code>lrm</code>.
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>.   When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lrm(formula, data=environment(formula),
    subset, na.action=na.delete, method="lrm.fit",
    model=FALSE, x=FALSE, y=FALSE, linear.predictors=TRUE, se.fit=FALSE, 
    penalty=0, penalty.matrix, tol=1e-7, 
    strata.penalty=0, var.penalty=c('simple','sandwich'),
    weights, normwt, scale=FALSE, ...)

## S3 method for class 'lrm'
print(x, digits=4, r2=c(0,2,4), strata.coefs=FALSE,
    coefs=TRUE, pg=FALSE, title='Logistic Regression Model', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lrm_+3A_formula">formula</code></td>
<td>

<p>a formula object. An <code>offset</code> term can be included. The offset causes
fitting of a model such as <code class="reqn">logit(Y=1) = X\beta + W</code>, where <code class="reqn">W</code> is the
offset variable having no estimated coefficient.
The response variable can be any data type; <code>lrm</code> converts it
in alphabetic or numeric order to an S factor variable and
recodes it 0,1,2,... internally. 
</p>
</td></tr>
<tr><td><code id="lrm_+3A_data">data</code></td>
<td>

<p>data frame to use. Default is the current frame.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_subset">subset</code></td>
<td>

<p>logical expression or vector of subscripts defining a subset of
observations to analyze
</p>
</td></tr>
<tr><td><code id="lrm_+3A_na.action">na.action</code></td>
<td>

<p>function to handle <code>NA</code>s in the data. Default is <code>na.delete</code>, which
deletes any observation having response or predictor missing, while
preserving the attributes of the predictors and maintaining frequencies
of deletions due to each variable in the model.  
This is usually specified using <code>options(na.action="na.delete")</code>.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_method">method</code></td>
<td>

<p>name of fitting function. Only allowable choice at present is <code>lrm.fit</code>.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_model">model</code></td>
<td>

<p>causes the model frame to be returned in the fit object
</p>
</td></tr>
<tr><td><code id="lrm_+3A_x">x</code></td>
<td>

<p>causes the expanded design matrix (with missings excluded)
to be returned under the name <code>x</code>.  For <code>print</code>, an object
created by <code>lrm</code>.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_y">y</code></td>
<td>

<p>causes the response variable (with missings excluded) to be returned
under the name <code>y</code>.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_linear.predictors">linear.predictors</code></td>
<td>

<p>causes the predicted X beta (with missings excluded) to be returned
under the name <code>linear.predictors</code>.  When the response variable has
more than two levels, the first intercept is used.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_se.fit">se.fit</code></td>
<td>

<p>causes the standard errors of the fitted values to be returned under
the name <code>se.fit</code>.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_penalty">penalty</code></td>
<td>

<p>The penalty factor subtracted from the log likelihood is
<code class="reqn">0.5 \beta' P \beta</code>, where <code class="reqn">\beta</code> is the vector of regression
coefficients other than intercept(s), and <code class="reqn">P</code> is
<code>penalty factors * penalty.matrix</code> and <code>penalty.matrix</code> is
defined below.  The default is <code>penalty=0</code> implying that ordinary 
unpenalized maximum likelihood estimation is used.
If <code>penalty</code> is a scalar, it is assumed to be a penalty factor that
applies 
to all non-intercept parameters in the model.  Alternatively, specify a
list to penalize different types of model terms by differing amounts.
The elements in this list are named <code>simple, nonlinear, interaction</code> and
<code>nonlinear.interaction</code>.  If you omit elements on the right of this
series, values are inherited from elements on the left.  Examples:
<code>penalty=list(simple=5, nonlinear=10)</code> uses a penalty factor of 10
for nonlinear or interaction terms.  
<code>penalty=list(simple=0, nonlinear=2, nonlinear.interaction=4)</code> does not
penalize linear main effects, uses a penalty factor of 2 for nonlinear or
interaction effects (that are not both), and 4 for nonlinear interaction
effects.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_penalty.matrix">penalty.matrix</code></td>
<td>

<p>specifies the symmetric penalty matrix for non-intercept terms.
The default matrix for continuous predictors has
the variance of the columns of the design matrix in its diagonal elements
so that the penalty to the log likelhood is unitless.  For main effects
for categorical predictors with <code class="reqn">c</code> categories, the rows and columns of
the matrix contain a <code class="reqn">c-1 \times c-1</code> sub-matrix that is used to
compute the 
sum of squares about the mean of the <code class="reqn">c</code> parameter values (setting the
parameter to zero for the reference cell) as the penalty component
for that predictor.  This makes the penalty independent of the choice of
the reference cell.  If you specify <code>penalty.matrix</code>, you may set
the rows and columns for certain parameters to zero so as to not
penalize those parameters.
Depending on <code>penalty</code>, some elements of <code>penalty.matrix</code> may
be overridden automatically by setting them to zero.
The penalty matrix that is used in the actual fit is 
<code class="reqn">penalty \times diag(pf) \times penalty.matrix \times diag(pf)</code>,
where <code class="reqn">pf</code> is the vector 
of square roots of penalty factors computed from <code>penalty</code> by
<code>Penalty.setup</code> in <code>rmsMisc</code>.  If you specify <code>penalty.matrix</code>
you must specify a nonzero value of <code>penalty</code> or no penalization will be
done.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_tol">tol</code></td>
<td>
<p>singularity criterion (see <code>lrm.fit</code>)</p>
</td></tr>
<tr><td><code id="lrm_+3A_strata.penalty">strata.penalty</code></td>
<td>
<p>scalar penalty factor for the stratification
factor, for the experimental <code>strat</code> variable</p>
</td></tr>
<tr><td><code id="lrm_+3A_var.penalty">var.penalty</code></td>
<td>

<p>the type of variance-covariance matrix to be stored in the <code>var</code>
component of the fit when penalization is used.  The default is the
inverse of the penalized information matrix.  Specify
<code>var.penalty="sandwich"</code> to use the sandwich estimator (see below
under <code>var</code>), which limited simulation studies have shown yields
variances estimates that are too low.
</p>
</td></tr>
<tr><td><code id="lrm_+3A_weights">weights</code></td>
<td>

<p>a vector (same length as <code>y</code>) of possibly fractional case weights
</p>
</td></tr>
<tr><td><code id="lrm_+3A_normwt">normwt</code></td>
<td>

<p>set to <code>TRUE</code> to scale <code>weights</code> so they sum to the length of
<code>y</code>; useful for sample surveys as opposed to the default of
frequency weighting 
</p>
</td></tr>
<tr><td><code id="lrm_+3A_scale">scale</code></td>
<td>
<p>set to <code>TRUE</code> to subtract means and divide by standard
deviations of columns of the design matrix
before fitting, and to back-solve for the un-normalized covariance
matrix and regression coefficients.  This can sometimes make the
model converge for very large 
sample sizes where for example spline or polynomial component
variables create scaling problems leading to loss of precision when
accumulating sums of squares and crossproducts.</p>
</td></tr>
<tr><td><code id="lrm_+3A_...">...</code></td>
<td>
<p>arguments that are passed to <code>lrm.fit</code>, or from
<code>print</code>, to <code><a href="#topic+prModFit">prModFit</a></code></p>
</td></tr>
<tr><td><code id="lrm_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to use</p>
</td></tr>
<tr><td><code id="lrm_+3A_r2">r2</code></td>
<td>
<p>vector of integers specifying which R^2 measures to print,
with 0 for Nagelkerke R^2 and 1:4 corresponding to the 4 measures
computed by <code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>.  Default is to print
Nagelkerke (labeled R2) and second and fourth <code>R2Measures</code>
which are the measures adjusted for the number of predictors, first
for the raw sample size then for the effective sample size, which
here is from the formula for the approximate variance of a log odds
ratio in a proportional odds model.</p>
</td></tr>
<tr><td><code id="lrm_+3A_strata.coefs">strata.coefs</code></td>
<td>
<p>set to <code>TRUE</code> to print the (experimental)
strata coefficients</p>
</td></tr>
<tr><td><code id="lrm_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="lrm_+3A_pg">pg</code></td>
<td>
<p>set to <code>TRUE</code> to print g-indexes</p>
</td></tr>
<tr><td><code id="lrm_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned fit object of <code>lrm</code> contains the following components
in addition to the ones mentioned under the optional arguments.
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>calling expression
</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>

<p>table of frequencies for <code>Y</code> in order of increasing <code>Y</code>
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>vector with the following elements: number of observations used in the
fit, maximum absolute value of first
derivative of log likelihood, model likelihood ratio
<code class="reqn">\chi^2</code>, d.f., 
<code class="reqn">P</code>-value, <code class="reqn">c</code> index (area under ROC curve), Somers' <code class="reqn">D_{xy}</code>,
Goodman-Kruskal <code class="reqn">\gamma</code>, Kendall's <code class="reqn">\tau_a</code> rank
correlations 
between predicted probabilities and observed response, the
Nagelkerke <code class="reqn">R^2</code> index, the Brier score computed with respect to
<code class="reqn">Y &gt;</code> its lowest level, the <code class="reqn">g</code>-index, <code class="reqn">gr</code> (the
<code class="reqn">g</code>-index on the odds ratio scale), and <code class="reqn">gp</code> (the <code class="reqn">g</code>-index
on the probability scale using the same cutoff used for the Brier
score).  Probabilities are rounded to the nearest 0.0002 
in the computations or rank correlation indexes.
In the case of penalized estimation, the <code>"Model L.R."</code> is computed
without the penalty factor, and <code>"d.f."</code> is the effective d.f. from
Gray's (1992) Equation 2.9.
The <code class="reqn">P</code>-value uses this corrected model
L.R. <code class="reqn">\chi^2</code> and corrected d.f. 
The score chi-square statistic uses first derivatives which contain
penalty components.
</p>
</td></tr>
<tr><td><code>fail</code></td>
<td>

<p>set to <code>TRUE</code> if convergence failed (and <code>maxiter&gt;1</code>)
</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>estimated parameters</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>estimated variance-covariance matrix (inverse of information matrix).
If <code>penalty&gt;0</code>, <code>var</code> is either the inverse of the penalized
information matrix (the default, if <code>var.penalty="simple"</code>) or the
sandwich-type variance - covariance
matrix estimate (Gray Eq. 2.6) if <code>var.penalty="sandwich"</code>.  For the
latter case the simple information-matrix - based variance
matrix is returned under the name <code>var.from.info.matrix</code>.
</p>
</td></tr>
<tr><td><code>effective.df.diagonal</code></td>
<td>

<p>is returned if <code>penalty&gt;0</code>.  It is the vector whose sum is the effective
d.f. of the model (counting intercept terms).
</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>vector of first derivatives of log-likelihood</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>

<p>-2 log likelihoods (counting penalty components)
When an offset variable is present, three
deviances are computed: for intercept(s) only, for
intercepts+offset, and for intercepts+offset+predictors.
When there is no offset variable, the vector contains deviances for
the intercept(s)-only model and the model with intercept(s) and predictors.
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>vector of column numbers of <code>X</code> fitted (intercepts are not counted)
</p>
</td></tr>
<tr><td><code>non.slopes</code></td>
<td>
<p>number of intercepts in model</p>
</td></tr>
<tr><td><code>penalty</code></td>
<td>
<p>see above</p>
</td></tr>
<tr><td><code>penalty.matrix</code></td>
<td>
<p>the penalty matrix actually used in the estimation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Le Cessie S, Van Houwelingen JC: Ridge estimators in logistic regression.
Applied Statistics 41:191&ndash;201, 1992.
</p>
<p>Verweij PJM, Van Houwelingen JC: Penalized likelihood in Cox regression.
Stat in Med 13:2427&ndash;2436, 1994.
</p>
<p>Gray RJ: Flexible methods for analyzing survival data using splines,
with applications to breast cancer prognosis.  JASA 87:942&ndash;951, 1992.
</p>
<p>Shao J: Linear model selection by cross-validation.  JASA 88:486&ndash;494, 1993.
</p>
<p>Verweij PJM, Van Houwelingen JC: Crossvalidation in survival analysis.
Stat in Med 12:2305&ndash;2314, 1993.
</p>
<p>Harrell FE: Model uncertainty, penalization, and parsimony.  ISCB
Presentation on UVa Web page, 1998.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm.fit">lrm.fit</a></code>, <code><a href="#topic+predict.lrm">predict.lrm</a></code>,
<code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="#topic+latex.lrm">latex.lrm</a></code>, 
<code><a href="#topic+residuals.lrm">residuals.lrm</a></code>, <code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>,
<code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>,
<code><a href="#topic+pentrace">pentrace</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+vif">vif</a></code>,
<code><a href="#topic+cr.setup">cr.setup</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>,
<code><a href="#topic+validate.lrm">validate.lrm</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+Mean.lrm">Mean.lrm</a></code>, <code><a href="#topic+gIndex">gIndex</a></code>, <code><a href="#topic+prModFit">prModFit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Fit a logistic model containing predictors age, blood.pressure, sex
#and cholesterol, with age fitted with a smooth 5-knot restricted cubic 
#spline function and a different shape of the age relationship for males 
#and females.  As an intermediate step, predict mean cholesterol from
#age using a proportional odds ordinal logistic model
#
require(ggplot2)
n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

#To use prop. odds model, avoid using a huge number of intercepts by
#grouping cholesterol into 40-tiles
ch &lt;- cut2(cholesterol, g=40, levels.mean=TRUE) # use mean values in intervals
table(ch)
f &lt;- lrm(ch ~ age)
options(prType='latex')
print(f, coefs=4)  # write latex code to console
m &lt;- Mean(f)    # see help file for Mean.lrm
d &lt;- data.frame(age=seq(0,90,by=10))
m(predict(f, d))
# Repeat using ols
f &lt;- ols(cholesterol ~ age)
predict(f, d)

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
     (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
cholesterol[1:3] &lt;- NA   # 3 missings, at random

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
               x=TRUE, y=TRUE)
#      x=TRUE, y=TRUE allows use of resid(), which.influence below
#      could define d &lt;- datadist(fit) after lrm(), but data distribution
#      summary would not be stored with fit, so later uses of Predict
#      or summary.rms would require access to the original dataset or
#      d or specifying all variable values to summary, Predict, nomogram
anova(fit)
p &lt;- Predict(fit, age, sex)
ggplot(p)   # or plot()
ggplot(Predict(fit, age=20:70, sex="male"))   # need if datadist not used
print(cbind(resid(fit,"dfbetas"), resid(fit,"dffits"))[1:20,])
which.influence(fit, .3)
# latex(fit)                       #print nice statement of fitted model
#
#Repeat this fit using penalized MLE, penalizing complex terms
#(for nonlinear or interaction effects)
#
fitp &lt;- update(fit, penalty=list(simple=0,nonlinear=10), x=TRUE, y=TRUE)
effective.df(fitp)
# or lrm(y ~ \dots, penalty=\dots)


#Get fits for a variety of penalties and assess predictive accuracy 
#in a new data set.  Program efficiently so that complex design 
#matrices are only created once.


set.seed(201)
x1 &lt;- rnorm(500)
x2 &lt;- rnorm(500)
x3 &lt;- sample(0:1,500,rep=TRUE)
L  &lt;- x1+abs(x2)+x3
y  &lt;- ifelse(runif(500)&lt;=plogis(L), 1, 0)
new.data &lt;- data.frame(x1,x2,x3,y)[301:500,]
#
for(penlty in seq(0,.15,by=.005)) {
  if(penlty==0) {
    f &lt;- lrm(y ~ rcs(x1,4)+rcs(x2,6)*x3, subset=1:300, x=TRUE, y=TRUE)
    # True model is linear in x1 and has no interaction
    X &lt;- f$x    # saves time for future runs - don't have to use rcs etc.
    Y &lt;- f$y    # this also deletes rows with NAs (if there were any)
    penalty.matrix &lt;- diag(diag(var(X)))
    Xnew &lt;- predict(f, new.data, type="x")  
    # expand design matrix for new data
    Ynew &lt;- new.data$y
  } else f &lt;- lrm.fit(X,Y, penalty.matrix=penlty*penalty.matrix)
#
  cat("\nPenalty :",penlty,"\n")
  pred.logit &lt;- f$coef[1] + (Xnew %*% f$coef[-1])
  pred &lt;- plogis(pred.logit)
  C.index &lt;- somers2(pred, Ynew)["C"]
  Brier   &lt;- mean((pred-Ynew)^2)
  Deviance&lt;- -2*sum( Ynew*log(pred) + (1-Ynew)*log(1-pred) )
  cat("ROC area:",format(C.index),"   Brier score:",format(Brier),
      "   -2 Log L:",format(Deviance),"\n")
}
#penalty=0.045 gave lowest -2 Log L, Brier, ROC in test sample for S+
#
#Use bootstrap validation to estimate predictive accuracy of
#logistic models with various penalties
#To see how noisy cross-validation estimates can be, change the
#validate(f, \dots) to validate(f, method="cross", B=10) for example.
#You will see tremendous variation in accuracy with minute changes in
#the penalty.  This comes from the error inherent in using 10-fold
#cross validation but also because we are not fixing the splits.  
#20-fold cross validation was even worse for some
#indexes because of the small test sample size.  Stability would be
#obtained by using the same sample splits for all penalty values 
#(see above), but then we wouldn't be sure that the choice of the 
#best penalty is not specific to how the sample was split.  This
#problem is addressed in the last example.
#
penalties &lt;- seq(0,.7,length=3)   # really use by=.02
index &lt;- matrix(NA, nrow=length(penalties), ncol=11,
	        dimnames=list(format(penalties),
          c("Dxy","R2","Intercept","Slope","Emax","D","U","Q","B","g","gp")))
i &lt;- 0
for(penlty in penalties)
{
  cat(penlty, "")
  i &lt;- i+1
  if(penlty==0)
    {
    f &lt;- lrm(y ~ rcs(x1,4)+rcs(x2,6)*x3, x=TRUE, y=TRUE)  # fit whole sample
    X &lt;- f$x
    Y &lt;- f$y
    penalty.matrix &lt;- diag(diag(var(X)))   # save time - only do once
    }
  else
   f &lt;- lrm(Y ~ X, penalty=penlty,
            penalty.matrix=penalty.matrix, x=TRUE,y=TRUE)
  val &lt;- validate(f, method="boot", B=20)  # use larger B in practice
  index[i,] &lt;- val[,"index.corrected"]
}
par(mfrow=c(3,3))
for(i in 1:9)
{
  plot(penalties, index[,i], 
       xlab="Penalty", ylab=dimnames(index)[[2]][i])
  lines(lowess(penalties, index[,i]))
}
options(datadist=NULL)

# Example of weighted analysis
x    &lt;- 1:5
y    &lt;- c(0,1,0,1,0)
reps &lt;- c(1,2,3,2,1)
lrm(y ~ x, weights=reps)
x &lt;- rep(x, reps)
y &lt;- rep(y, reps)
lrm(y ~ x)   # same as above

#
#Study performance of a modified AIC which uses the effective d.f.
#See Verweij and Van Houwelingen (1994) Eq. (6).  Here AIC=chisq-2*df.
#Also try as effective d.f. equation (4) of the previous reference.
#Also study performance of Shao's cross-validation technique (which was
#designed to pick the "right" set of variables, and uses a much smaller
#training sample than most methods).  Compare cross-validated deviance
#vs. penalty to the gold standard accuracy on a 7500 observation dataset.
#Note that if you only want to get AIC or Schwarz Bayesian information
#criterion, all you need is to invoke the pentrace function.
#NOTE: the effective.df( ) function is used in practice
#
## Not run: 
for(seed in c(339,777,22,111,3)){ 
# study performance for several datasets
  set.seed(seed)
  n &lt;- 175; p &lt;- 8
  X &lt;- matrix(rnorm(n*p), ncol=p) # p normal(0,1) predictors
  Coef &lt;- c(-.1,.2,-.3,.4,-.5,.6,-.65,.7)  # true population coefficients
  L &lt;- X %*% Coef                 # intercept is zero
  Y &lt;- ifelse(runif(n)&lt;=plogis(L), 1, 0)
  pm &lt;- diag(diag(var(X)))
  #Generate a large validation sample to use as a gold standard
  n.val &lt;- 7500
  X.val &lt;- matrix(rnorm(n.val*p), ncol=p)
  L.val &lt;- X.val %*% Coef
  Y.val &lt;- ifelse(runif(n.val)&lt;=plogis(L.val), 1, 0)
  #
  Penalty &lt;- seq(0,30,by=1)
  reps &lt;- length(Penalty)
  effective.df &lt;- effective.df2 &lt;- aic &lt;- aic2 &lt;- deviance.val &lt;- 
    Lpenalty &lt;- single(reps)
  n.t &lt;- round(n^.75)
  ncv &lt;- c(10,20,30,40)     # try various no. of reps in cross-val.
  deviance &lt;- matrix(NA,nrow=reps,ncol=length(ncv))
  #If model were complex, could have started things off by getting X, Y
  #penalty.matrix from an initial lrm fit to save time
  #
  for(i in 1:reps) {
    pen &lt;- Penalty[i]
    cat(format(pen),"")
    f.full &lt;- lrm.fit(X, Y, penalty.matrix=pen*pm)
    Lpenalty[i] &lt;- pen* t(f.full$coef[-1]) %*% pm %*% f.full$coef[-1]
    f.full.nopenalty &lt;- lrm.fit(X, Y, initial=f.full$coef, maxit=1)
    info.matrix.unpenalized &lt;- solve(f.full.nopenalty$var)
    effective.df[i] &lt;- sum(diag(info.matrix.unpenalized %*% f.full$var)) - 1
    lrchisq &lt;- f.full.nopenalty$stats["Model L.R."]
    # lrm does all this penalty adjustment automatically (for var, d.f.,
    # chi-square)
    aic[i] &lt;- lrchisq - 2*effective.df[i]
    #
    pred &lt;- plogis(f.full$linear.predictors)
    score.matrix &lt;- cbind(1,X) * (Y - pred)
    sum.u.uprime &lt;- t(score.matrix) %*% score.matrix
    effective.df2[i] &lt;- sum(diag(f.full$var %*% sum.u.uprime))
    aic2[i] &lt;- lrchisq - 2*effective.df2[i]
    #
    #Shao suggested averaging 2*n cross-validations, but let's do only 40
    #and stop along the way to see if fewer is OK
    dev &lt;- 0
    for(j in 1:max(ncv)) {
      s    &lt;- sample(1:n, n.t)
      cof  &lt;- lrm.fit(X[s,],Y[s], 
                      penalty.matrix=pen*pm)$coef
      pred &lt;- cof[1] + (X[-s,] %*% cof[-1])
      dev &lt;- dev -2*sum(Y[-s]*pred + log(1-plogis(pred)))
      for(k in 1:length(ncv)) if(j==ncv[k]) deviance[i,k] &lt;- dev/j
    }
    #
    pred.val &lt;- f.full$coef[1] + (X.val %*% f.full$coef[-1])
    prob.val &lt;- plogis(pred.val)
    deviance.val[i] &lt;- -2*sum(Y.val*pred.val + log(1-prob.val))
  }
  postscript(hor=TRUE)   # along with graphics.off() below, allow plots
  par(mfrow=c(2,4))   # to be printed as they are finished
  plot(Penalty, effective.df, type="l")
  lines(Penalty, effective.df2, lty=2)
  plot(Penalty, Lpenalty, type="l")
  title("Penalty on -2 log L")
  plot(Penalty, aic, type="l")
  lines(Penalty, aic2, lty=2)
  for(k in 1:length(ncv)) {
    plot(Penalty, deviance[,k], ylab="deviance")
    title(paste(ncv[k],"reps"))
    lines(supsmu(Penalty, deviance[,k]))
  }
  plot(Penalty, deviance.val, type="l")
  title("Gold Standard (n=7500)")
  title(sub=format(seed),adj=1,cex=.5)
  graphics.off()
}

## End(Not run)
#The results showed that to obtain a clear picture of the penalty-
#accuracy relationship one needs 30 or 40 reps in the cross-validation.
#For 4 of 5 samples, though, the super smoother was able to detect
#an accurate penalty giving the best (lowest) deviance using 10-fold
#cross-validation.  Cross-validation would have worked better had
#the same splits been used for all penalties.
#The AIC methods worked just as well and are much quicker to compute.
#The first AIC based on the effective d.f. in Gray's Eq. 2.9
#(Verweij and Van Houwelingen (1994) Eq. 5 (note typo)) worked best.
</code></pre>

<hr>
<h2 id='lrm.fit'>Logistic Model Fitter</h2><span id='topic+lrm.fit'></span>

<h3>Description</h3>

<p>Fits a binary or ordinal logistic model for a given design matrix and response
vector with no missing values in either.  Ordinary or penalized maximum
likelihood estimation is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lrm.fit(x, y, offset=0, initial, est, maxit=12, eps=.025,
        tol=1e-7, trace=FALSE, penalty.matrix=NULL, weights=NULL,
        normwt=FALSE, scale=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lrm.fit_+3A_x">x</code></td>
<td>

<p>design matrix with no column for an intercept
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_y">y</code></td>
<td>

<p>response vector, numeric, categorical, or character
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_offset">offset</code></td>
<td>
<p>optional numeric vector containing an offset on the logit scale</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_initial">initial</code></td>
<td>

<p>vector of initial parameter estimates, beginning with the
intercept
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_est">est</code></td>
<td>

<p>indexes of <code>x</code> to fit in the model (default is all columns of <code>x</code>).
Specifying <code>est=c(1,2,5)</code> causes columns 1,2, and 5 to have
parameters estimated. The score vector <code>u</code> and covariance matrix <code>var</code>
can be used to obtain score statistics for other columns
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_maxit">maxit</code></td>
<td>

<p>maximum no. iterations (default=<code>12</code>). Specifying <code>maxit=1</code>
causes logist to compute statistics at initial estimates.
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_eps">eps</code></td>
<td>

<p>difference in <code class="reqn">-2  log</code> likelihood for declaring convergence.
Default is <code>.025</code>.  If the <code class="reqn">-2 log</code> likelihood gets
worse by eps/10 while the maximum absolute first derivative of
<code class="reqn">-2 log</code> likelihood is below 1e-9, convergence is still
declared.  This handles the case where the initial estimates are MLEs,
to prevent endless step-halving.
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_tol">tol</code></td>
<td>

<p>Singularity criterion. Default is 1e-7
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_trace">trace</code></td>
<td>

<p>set to <code>TRUE</code> to print -2 log likelihood, step-halving
fraction, change in -2 log likelihood, maximum absolute value of first
derivative, and vector of first derivatives at each iteration.
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_penalty.matrix">penalty.matrix</code></td>
<td>

<p>a self-contained ready-to-use penalty matrix - see <code>lrm</code>
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_weights">weights</code></td>
<td>

<p>a vector (same length as <code>y</code>) of possibly fractional case weights
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_normwt">normwt</code></td>
<td>

<p>set to <code>TRUE</code> to scale <code>weights</code> so they sum to the length of
<code>y</code>; useful for sample surveys as opposed to the default of
frequency weighting 
</p>
</td></tr>
<tr><td><code id="lrm.fit_+3A_scale">scale</code></td>
<td>
<p>set to <code>TRUE</code> to subtract column means and divide by
column standard deviations of <code>x</code>
before fitting, and to back-solve for the un-normalized covariance
matrix and regresion coefficients.  This can sometimes make the model
converge for very large 
sample sizes where for example spline or polynomial component
variables create scaling problems leading to loss of precision when
accumulating sums of squares and crossproducts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>calling expression
</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>

<p>table of frequencies for <code>y</code> in order of increasing <code>y</code>
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>vector with the following elements: number of observations used in the
fit, maximum absolute value of first
derivative of log likelihood, model likelihood ratio chi-square, d.f.,
P-value,
<code class="reqn">c</code> index (area under ROC curve), Somers' <code class="reqn">D_{xy}</code>,
Goodman-Kruskal <code class="reqn">\gamma</code>, and Kendall's <code class="reqn">\tau_a</code>
rank correlations 
between predicted probabilities and observed response, the
Nagelkerke <code class="reqn">R^2</code> index, 4 indexes computed by
<code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>, the Brier probability score with
respect to computing the probability that <code class="reqn">y &gt;</code> the mid level less
one, the <code class="reqn">g</code>-index, <code class="reqn">gr</code> (the <code class="reqn">g</code>-index on the odds ratio
scale), and <code class="reqn">gp</code> (the <code class="reqn">g</code>-index on the probability scale using
the same cutoff used for the Brier score).
Probabilities are rounded to the nearest 0.002
in the computations or rank correlation indexes.
When <code>penalty.matrix</code> is present, the <code class="reqn">\chi^2</code>,
d.f., and P-value are not corrected for the effective d.f.
</p>
</td></tr>
<tr><td><code>fail</code></td>
<td>

<p>set to <code>TRUE</code> if convergence failed (and <code>maxit&gt;1</code>)
</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>

<p>estimated parameters
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>estimated variance-covariance matrix (inverse of information matrix).
Note that in the case of penalized estimation, <code>var</code> is not the
improved sandwich-type estimator (which <code>lrm</code> does compute).
</p>
</td></tr>
<tr><td><code>u</code></td>
<td>

<p>vector of first derivatives of log-likelihood
</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>

<p>-2 log likelihoods. 
When an offset variable is present, three
deviances are computed: for intercept(s) only, for
intercepts+offset, and for intercepts+offset+predictors.
When there is no offset variable, the vector contains deviances for
the intercept(s)-only model and the model with intercept(s) and predictors.
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>vector of column numbers of <code>X</code> fitted (intercepts are not counted)
</p>
</td></tr>
<tr><td><code>non.slopes</code></td>
<td>

<p>number of intercepts in model
</p>
</td></tr>
<tr><td><code>penalty.matrix</code></td>
<td>

<p>see above
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, <code><a href="#topic+matinv">matinv</a></code>,
<code><a href="Hmisc.html#topic+solvet">solvet</a></code>, <code><a href="#topic+cr.setup">cr.setup</a></code>, <code><a href="#topic+gIndex">gIndex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Fit an additive logistic model containing numeric predictors age, 
#blood.pressure, and sex, assumed to be already properly coded and 
#transformed
#
# fit &lt;- lrm.fit(cbind(age,blood.pressure,sex), death)
</code></pre>

<hr>
<h2 id='lrm.fit.bare'>lrm.fit.bare</h2><span id='topic+lrm.fit.bare'></span>

<h3>Description</h3>

<p>Bare Bones Logistic Regression Fit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lrm.fit.bare(x, y, maxit = 12, eps = 0.025, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lrm.fit.bare_+3A_x">x</code></td>
<td>
<p>a vector of matrix of covariate values</p>
</td></tr>
<tr><td><code id="lrm.fit.bare_+3A_y">y</code></td>
<td>
<p>a numeric or factor vector representing the dependent variable</p>
</td></tr>
<tr><td><code id="lrm.fit.bare_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iteractions</p>
</td></tr>
<tr><td><code id="lrm.fit.bare_+3A_eps">eps</code></td>
<td>
<p>stopping criterion (change in -2 log likelihood)</p>
</td></tr>
<tr><td><code id="lrm.fit.bare_+3A_tol">tol</code></td>
<td>
<p>matrix inversion tolerance for singularities</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a stripped down version of the <code>lrm.fit()</code> function that computes only the regression coefficients, variance-covariance-matrix, and log likelihood (for null and fitted model) and does not compute any model fit indexes etc.  This is for speed in simulations or with bootstrapping.  Missing data are not allowed.  The function handles binary and ordinal logistic regression (proportional odds model).
</p>


<h3>Value</h3>

<p>a list with elements <code>coefficients</code>, <code>var</code>, <code>fail</code>, <code>freq</code>, <code>deviance</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='LRupdate'>LRupdate</h2><span id='topic+LRupdate'></span>

<h3>Description</h3>

<p>Update Model LR Statistics After Multiple Imputation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LRupdate(fit, anova)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LRupdate_+3A_fit">fit</code></td>
<td>
<p>an <code>rms</code> fit object</p>
</td></tr>
<tr><td><code id="LRupdate_+3A_anova">anova</code></td>
<td>
<p>the result of <code>processMI(..., 'anova')</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For fits from <code style="white-space: pre;">&#8288;orm, lrm, orm, cph, psm&#8288;</code> that were created using <code>fit.mult.impute</code> with <code>lrt=TRUE</code> or equivalent options and for which <code>anova</code> was obtained using <code>processMI(fit, 'anova')</code> to compute imputation-adjusted LR statistics.  <code>LRupdate</code> uses the last line of the <code>anova</code> result (containing the overall model LR chi-square) to update <code style="white-space: pre;">&#8288;Model L.R.&#8288;</code> in the fit <code>stats</code> component, and to adjust any of the new R-square measures in <code>stats</code>.
</p>
<p>For models using Nagelkerke's R-squared, these are set to <code>NA</code> as they would need to be recomputed with a new intercept-only log-likelihood, which is not computed by <code>anova</code>.  For <code>ols</code> models, R-squared is left alone as it is sample-size-independent and <code>print.ols</code> prints the correct adjusted R-squared due to <code>fit.mult.impute</code> correcting the residual d.f. in stacked fits.
</p>


<h3>Value</h3>

<p>new fit object like <code>fit</code> but with the substitutions made
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+processMI.fit.mult.impute">processMI.fit.mult.impute()</a></code>, <code><a href="Hmisc.html#topic+R2Measures">Hmisc::R2Measures()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
a &lt;- aregImpute(~ y + x1 + x2, n.impute=30, data=d)
f &lt;- fit.mult.impute(y ~ x1 + x2, lrm, a, data=d, lrt=TRUE)
a &lt;- processMI(f, 'anova')
f &lt;- LRupdate(f, a)
print(f, r2=1:4)   # print all imputation-corrected R2 measures

## End(Not run)
</code></pre>

<hr>
<h2 id='matinv'>
Total and Partial Matrix Inversion using Gauss-Jordan Sweep Operator
</h2><span id='topic+matinv'></span>

<h3>Description</h3>

<p>This function inverts or partially inverts a matrix using pivoting
(the sweep operator).  It is useful for sequential model-building.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matinv(a, which, negate=TRUE, eps=1e-12)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matinv_+3A_a">a</code></td>
<td>

<p>square matrix to invert or partially invert.  May have been inverted or
partially inverted previously by matinv, in which case its &quot;swept&quot;
attribute is updated.  Will un-invert if already inverted.
</p>
</td></tr>
<tr><td><code id="matinv_+3A_which">which</code></td>
<td>

<p>vector of column/row numbers in a to invert.  Default is all, for total
inverse.
</p>
</td></tr>
<tr><td><code id="matinv_+3A_negate">negate</code></td>
<td>

<p>So that the algorithm can keep track of which pivots have been swept
as well as roundoff errors, it actually returns the negative of the
inverse or partial inverse.  By default, these elements are negated to
give the usual expected result.  Set negate=FALSE if you will be passing
the result right back into matinv, otherwise, negate the submatrix
before sending back to matinv.
</p>
</td></tr>
<tr><td><code id="matinv_+3A_eps">eps</code></td>
<td>

<p>singularity criterion
</p>
</td></tr></table>


<h3>Value</h3>

<p>a square matrix, with attributes &quot;rank&quot; and &quot;swept&quot;.
</p>


<h3>References</h3>

<p>Clarke MRB (1982).  Algorithm AS 178: The Gauss-Jordan sweep operator
with detection of collinearity.  Appl Statist 31:166&ndash;9.
</p>
<p>Ridout MS, Cobb JM (1986).  Algorithm AS R78 : A remark on algorithm AS 178:
The Gauss-Jordan
sweep operator with detection of collinearity.  Appl Statist 38:420&ndash;2.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="base.html#topic+solve">solve</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a      &lt;- diag(1:3)
a.inv1 &lt;- matinv(a, 1, negate=FALSE)	     #Invert with respect to a[1,1]
a.inv1
a.inv  &lt;- -matinv(a.inv1, 2:3, negate=FALSE) #Finish the job
a.inv
solve(a)
</code></pre>

<hr>
<h2 id='nomogram'>Draw a Nomogram Representing a Regression Fit</h2><span id='topic+nomogram'></span><span id='topic+print.nomogram'></span><span id='topic+plot.nomogram'></span><span id='topic+legend.nomabbrev'></span>

<h3>Description</h3>

<p>Draws a partial nomogram that can be used to manually obtain predicted
values from a regression model that was fitted with <code>rms</code>.
The nomogram does not have lines representing sums, but it has a reference
line for reading scoring points (default range 0&ndash;100).  Once the reader
manually totals the points, the predicted values can be read at the bottom.
Non-monotonic transformations of continuous variables are handled (scales
wrap around), as
are transformations which have flat sections (tick marks are labeled
with ranges).  If interactions are in the model, one variable
is picked as the &ldquo;axis variable&rdquo;, and separate axes are constructed for
each level of the interacting factors (preference is given automatically
to using any discrete factors to construct separate axes) and
levels of factors which are indirectly related to interacting
factors (see DETAILS).  Thus the nomogram is designed so that only
one axis is actually read for each variable, since the variable
combinations are disjoint.  For
categorical interacting factors, the default is to construct axes for
all levels.
The user may specify
coordinates of each predictor to label on its axis, or use default values.
If a factor interacts with other factors, settings for one or more of
the interacting factors may be specified separately (this is mandatory
for continuous variables).  Optional confidence intervals will be
drawn for individual scores as well as for the linear predictor.
If more than one confidence level is chosen, multiple levels may be
displayed using different colors or gray scales.  Functions of the
linear predictors may be added to the nomogram.
</p>
<p>The <code><a href="#topic+datadist">datadist</a></code> object that was in effect when the model
was fit is used to specify the limits of the axis for continuous
predictors when the user does not specify tick mark locations in the
<code>nomogram</code> call.
</p>
<p><code>print.nomogram</code> prints axis information stored in an object returned
by <code>nomogram</code>.  This is useful in producing tables of point assignments
by levels of predictors.  It also prints how many linear predictor
units there are per point and the number of points per unit change in
the linear predictor.
</p>
<p><code>legend.nomabbrev</code> draws legends describing abbreviations used for
labeling tick marks for levels of categorical predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nomogram(fit, ..., adj.to, lp=TRUE, lp.at=NULL,
         fun=NULL, fun.at=NULL, fun.lp.at=NULL, funlabel="Predicted Value",
         interact=NULL, kint=NULL,  conf.int=FALSE, 
         conf.lp=c("representative", "all", "none"),
         est.all=TRUE, posterior.summary=c('mean', 'median', 'mode'),
         abbrev=FALSE, minlength=4, maxscale=100, nint=10, 
         vnames=c("labels","names"),
         varname.label=TRUE, varname.label.sep="=",
         omit=NULL, verbose=FALSE)

## S3 method for class 'nomogram'
print(x, dec=0, ...)

## S3 method for class 'nomogram'
plot(x, lplabel="Linear Predictor", fun.side,
 col.conf=c(1, 0.3),
 conf.space=c(.08,.2), label.every=1, force.label=FALSE, 
 xfrac=.35, cex.axis=.85, cex.var=1, col.grid=NULL,
 varname.label=TRUE, varname.label.sep="=", ia.space=.7, 
 tck=NA, tcl=-0.25, lmgp=.4, naxes,
 points.label='Points', total.points.label='Total Points',
 total.sep.page=FALSE, total.fun, cap.labels=FALSE, ...)

legend.nomabbrev(object, which, x, y, ncol=3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nomogram_+3A_fit">fit</code></td>
<td>

<p>a regression model fit that was created with <code>rms</code>, and
(usually) with <code>options(datadist = "object.name")</code> in effect. 
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_...">...</code></td>
<td>

<p>settings of variables to use in constructing axes.  If <code>datadist</code>
was in effect, the default is to use <code>pretty(total range, nint)</code>
for continuous variables, and the class levels for discrete ones. 
For <code>legend.nomabbrev</code>, <code>...</code> specifies optional
parameters to pass 
to <code>legend</code>.  Common ones are <code>bty = "n"</code> to suppress drawing the
box.  You may want to specify a non-proportionally spaced font
(e.g., courier) number if abbreviations are more than one letter long.
This will make the abbreviation definitions line up (e.g., specify
<code>font = 2</code>, the default for courier).  Ignored for <code>print</code>
and <code>plot</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_adj.to">adj.to</code></td>
<td>

<p>If you didn't define <code>datadist</code> for all predictors, you will have to
define adjustment settings for the undefined ones, e.g.
<code>adj.to= list(age = 50, sex = "female")</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_lp">lp</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress creation of an axis for scoring
<code class="reqn">X\beta</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_lp.at">lp.at</code></td>
<td>

<p>If <code>lp=TRUE</code>, <code>lp.at</code> may specify a vector of settings of
<code class="reqn">X\beta</code>.
Default is to use <code>pretty(range of linear predictors, nint)</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_fun">fun</code></td>
<td>

<p>an optional function to transform the linear predictors, and to plot
on another axis.  If more than one transformation is plotted, put
them in a list, e.g. <code>list(function(x) x/2, function(x) 2*x)</code>.
Any function values equal to <code>NA</code> will be ignored.  
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_fun.at">fun.at</code></td>
<td>

<p>function values to label on axis.  Default <code>fun</code> evaluated
at <code>lp.at</code>.   If more than one <code>fun</code> was specified, using a vector
for <code>fun.at</code> will cause all functions to be evaluated at the same
argument values.  To use different values, specify a list of vectors for
<code>fun.at</code>, with elements corresponding to the different functions
(lists of vectors also applies to <code>fun.lp.at</code> and <code>fun.side</code>).
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_fun.lp.at">fun.lp.at</code></td>
<td>

<p>If you want to
evaluate one of the functions at a different set of linear predictor
values than may have been used in constructing the linear predictor axis,
specify a vector or list of vectors 
of linear predictor values at which to evaluate the function.  This is
especially useful for discrete functions.  The presence of this attribute
also does away with the need for <code>nomogram</code> to compute numerical approximations of 
the inverse of the function.  It also allows the user-supplied function
to return <code>factor</code> objects, which is useful when e.g. a single tick
mark position actually represents a range.
If the <code>fun.lp.at</code> parameter is present, the <code>fun.at</code>
vector for that function is ignored.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_funlabel">funlabel</code></td>
<td>

<p>label for <code>fun</code> axis.  If more than one function was given but
funlabel is of length one, it will be duplicated as needed.  If <code>fun</code> is
a list of functions for which you specified names (see the final example
below), these names will be used as labels.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_interact">interact</code></td>
<td>

<p>When a continuous variable interacts with a discrete one, axes are
constructed so that the continuous variable moves within the axis, and
separate axes represent levels of interacting factors.  For interactions
between two continuous variables, all but the axis variable must have
discrete levels defined in <code>interact</code>.  
For discrete interacting factors, you may specify levels to use in
constructing the multiple axes.  For continuous interacting factors,
you must do this.  Examples: <code>interact = list(age = seq(10,70,by=10),
      treat = c("A","B","D"))</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_kint">kint</code></td>
<td>

<p>for models such as the ordinal models with multiple intercepts,
specifies which one to use in evaluating the linear predictor.
Default is to use <code>fit$interceptRef</code> if it exists, or 1.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence levels to display for each scoring.  Default is <code>FALSE</code> to display
no confidence limits.  Setting <code>conf.int</code> to <code>TRUE</code> is the same as
setting it to <code>c(0.7, 0.9)</code>,
with the line segment between the 0.7 and 0.9 levels shaded using
gray scale.  
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_conf.lp">conf.lp</code></td>
<td>

<p>default is <code>"representative"</code> to group all linear predictors evaluated
into deciles, and to show, for the linear predictor confidence intervals,
only the mean linear predictor within the deciles along with the median
standard error within the deciles.  Set <code>conf.lp = "none"</code> to suppress
confidence limits for the linear predictors, and to <code>"all"</code> to show
all confidence limits.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_est.all">est.all</code></td>
<td>

<p>To plot axes for only the subset of variables named in <code>...</code>, set
<code>est.all = FALSE</code>.  Note: This option only works when zero has a special
meaning for the variables that are omitted from the graph.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>when operating on a Bayesian model such as a
result of <code>blrm</code> specifies whether to use posterior mean
(default) vs. posterior mode/median of parameter values in constructing
the nomogram</p>
</td></tr>
<tr><td><code id="nomogram_+3A_abbrev">abbrev</code></td>
<td>

<p>Set to <code>TRUE</code> to use the <code><a href="base.html#topic+abbreviate">abbreviate</a></code> function to abbreviate levels of
categorical factors, both for labeling tick marks and for axis titles.
If you only want to abbreviate certain predictor variables, set <code>abbrev</code>
to a vector of character strings containing their names.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_minlength">minlength</code></td>
<td>

<p>applies if <code>abbrev = TRUE</code>.  Is the minimum abbreviation length passed to the
<code><a href="base.html#topic+abbreviate">abbreviate</a></code> function.  If you set <code>minlength = 1</code>, the letters of the
alphabet are used to label tick marks for categorical predictors, and
all letters are drawn no matter how close together they are.  For
labeling axes (interaction settings), <code>minlength = 1</code> causes
<code>minlength = 4</code> to be used.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_maxscale">maxscale</code></td>
<td>

<p>default maximum point score is 100
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_nint">nint</code></td>
<td>

<p>number of intervals to label for axes representing continuous variables.
See <code><a href="base.html#topic+pretty">pretty</a></code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_vnames">vnames</code></td>
<td>

<p>By default, variable labels are used to label axes.  Set
<code>vnames = "names"</code>
to instead use variable names.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_omit">omit</code></td>
<td>

<p>vector of character strings containing names of variables for which to
suppress drawing axes.  Default is to show all variables.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_verbose">verbose</code></td>
<td>

<p>set to <code>TRUE</code> to get printed output detailing how tick marks are chosen
and labeled for function axes.  This is useful in seeing how certain
linear predictor values cannot be solved for using inverse linear
interpolation on the (requested linear predictor values, function values at 
these lp values).  When this happens you will see <code>NA</code>s in the verbose
output, and the corresponding tick marks will not appear in the nomogram.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_x">x</code></td>
<td>
<p>an object created by <code>nomogram</code>, or the x coordinate for
a legend</p>
</td></tr>
<tr><td><code id="nomogram_+3A_dec">dec</code></td>
<td>

<p>number of digits to the right of the decimal point, for rounding
point scores in <code>print.nomogram</code>.  Default is to round to the nearest
whole number of points.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_lplabel">lplabel</code></td>
<td>

<p>label for linear predictor axis.  Default is <code>"Linear Predictor"</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_fun.side">fun.side</code></td>
<td>

<p>a vector or list of vectors of <code>side</code> parameters for the <code>axis</code> function
for labeling function values.
Values may be 1 to position a tick mark label below the axis (the default),
or 3 for above the axis.  If for example an axis has 5 tick mark labels
and the second and third will run into each other, specify
<code>fun.side=c(1,1,3,1,1)</code> (assuming only one function is specified as <code>fun</code>).
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_col.conf">col.conf</code></td>
<td>

<p>colors corresponding to <code>conf.int</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_conf.space">conf.space</code></td>
<td>

<p>a 2-element vector with the vertical range within which to draw
confidence bars, in units of 1=spacing between main bars.  Four heights
are used within this range (8 for the linear predictor if more than
16 unique values were evaluated), cycling them among separate confidence
intervals to reduce overlapping.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_label.every">label.every</code></td>
<td>

<p>Specify <code>label.every = i</code> to label on every <code>i</code>th tick mark.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_force.label">force.label</code></td>
<td>

<p>set to <code>TRUE</code> to force every tick mark intended to be labeled to have
a label plotted (whether the labels run into each other or not)
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_xfrac">xfrac</code></td>
<td>

<p>fraction of horizontal plot to set aside for axis titles
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_cex.axis">cex.axis</code></td>
<td>

<p>character size for tick mark labels
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_cex.var">cex.var</code></td>
<td>

<p>character size for axis titles (variable names)
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_col.grid">col.grid</code></td>
<td>

<p>If left unspecified, no vertical reference lines are drawn.  Specify a
vector of length one (to use the same color for both minor and major
reference lines) or two (corresponding to the color for the major and
minor divisions, respectively) containing colors, to cause vertical reference
lines to the top points scale to be drawn.  For R, a good choice is
<code>col.grid = gray(c(0.8, 0.95))</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_varname.label">varname.label</code></td>
<td>

<p>In constructing axis titles for interactions, the default is to add
<code>(interacting.varname = level)</code> on the right.  Specify
<code>varname.label = FALSE</code>
to instead use <code>"(level)"</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_varname.label.sep">varname.label.sep</code></td>
<td>

<p>If <code>varname.label = TRUE</code>, you can change the separator to something other than
<code>=</code> by specifying this parameter.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_ia.space">ia.space</code></td>
<td>

<p>When multiple axes are draw for levels of interacting factors, the
default is to group combinations related to a main effect.  This is
done by spacing the axes for the second to last of these 
within a group only
0.7 (by default) of the way down as compared with normal space of 1 unit.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_tck">tck</code></td>
<td>

<p>see <code>tck</code> under <code><a href="graphics.html#topic+par">par</a></code>
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_tcl">tcl</code></td>
<td>
<p>length of tick marks in nomogram</p>
</td></tr>
<tr><td><code id="nomogram_+3A_lmgp">lmgp</code></td>
<td>

<p>spacing between numeric axis labels and axis (see <code><a href="graphics.html#topic+par">par</a></code>
for <code>mgp</code>)
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_naxes">naxes</code></td>
<td>

<p>maximum number of axes to allow on one plot.  If the nomogram requires more
than one &ldquo;page&rdquo;, the &ldquo;Points&rdquo; axis will be repeated at
the top of each page when necessary.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_points.label">points.label</code></td>
<td>

<p>a character string giving the axis label for the points scale
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_total.points.label">total.points.label</code></td>
<td>

<p>a character string giving the axis label for the total points scale
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_total.sep.page">total.sep.page</code></td>
<td>

<p>set to <code>TRUE</code> to force the total points and later axes to be placed on a
separate page
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_total.fun">total.fun</code></td>
<td>

<p>a user-provided function that will be executed before the total points
axis is drawn.  Default is not to execute a function.  This is useful e.g.
when <code>total.sep.page = TRUE</code> and you wish to use <code>locator</code> to find the
coordinates for positioning an abbreviation legend before it's too late
and a new page is started (i.e., <code>total.fun = function() print(locator(1))</code>).
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_cap.labels">cap.labels</code></td>
<td>
<p>logical: should the factor labels have their first
letter capitalized?</p>
</td></tr>
<tr><td><code id="nomogram_+3A_object">object</code></td>
<td>

<p>the result returned from <code>nomogram</code>
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_which">which</code></td>
<td>

<p>a character string giving the name of a variable for which to draw a
legend with abbreviations of factor levels
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_y">y</code></td>
<td>

<p>y-coordinate to pass to the <code>legend</code> function.  This is the upper left
corner of the legend box.  You can omit <code>y</code> if <code>x</code> is a list with
named elements <code>x</code> and <code>y</code>.  To use the mouse to locate the legend,
specify <code>locator(1)</code> for <code>x</code>.  For <code>print</code>, <code>x</code> is
the result of <code>nomogram</code>.
</p>
</td></tr>
<tr><td><code id="nomogram_+3A_ncol">ncol</code></td>
<td>

<p>the number of columns to form in drawing the legend.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A variable is considered to be discrete if it is categorical or ordered
or if <code><a href="#topic+datadist">datadist</a></code> stored <code>values</code> for it (meaning it
had <code>&lt;11</code> unique values).
A variable is said to be indirectly related to another variable if
the two are related by some interaction.  For example, if a model
has variables a, b, c, d, and the interactions are a:c and c:d,
variable d is indirectly related to variable a.  The complete list
of variables related to a is c, d.  If an axis is made for variable a,
several axes will actually be drawn, one for each combination of c
and d specified in <code>interact</code>.
</p>
<p>Note that with a caliper, it is easy to continually add point scores
for individual predictors, and then to place the caliper on the upper
&ldquo;Points&rdquo; axis (with extrapolation if needed).  Then transfer these
points to the
&ldquo;Total Points&rdquo; axis.  In this way, points can be added without
writing them down.
</p>
<p>Confidence limits for an individual predictor score are really confidence
limits for the entire linear predictor, with other predictors set to
adjustment values.  If <code>lp = TRUE</code>, all confidence bars for all linear
predictor values evaluated are drawn.  The extent to which multiple
confidence bars of differing widths appear at the same linear predictor
value means that precision depended on how the linear predictor was
arrived at (e.g., a certain value may be realized from a setting of
a certain predictor that was associated with a large standard error
on the regression coefficients for that predictor).
</p>
<p>On occasion, you may want to reverse the regression coefficients of a model
to make the &ldquo;points&rdquo; scales reverse direction.  For parametric survival
models, which are stated in terms of increasing regression effects meaning
longer survival (the opposite of a Cox model), just do something like
<code>fit$coefficients &lt;- -fit$coefficients</code> before invoking <code>nomogram</code>, 
and if you add function axes, negate the function
arguments.  For the Cox model, you also need to negate <code>fit$center</code>.
If you omit <code>lp.at</code>, also negate <code>fit$linear.predictors</code>.
</p>


<h3>Value</h3>

<p>a list of class <code>"nomogram"</code> that contains information used in plotting
the axes.  If you specified <code>abbrev = TRUE</code>, a list called <code>abbrev</code> is also
returned that gives the abbreviations used for tick mark labels, if any.  
This list is useful for
making legends and is used by <code>legend.nomabbrev</code> (see the last example).
The returned list also has components called <code>total.points</code>, <code>lp</code>,
and the function axis names.  These components have components
<code>x</code> (<code>at</code> argument vector given to <code>axis</code>), <code>y</code> (<code>pos</code> for <code>axis</code>),
and <code>x.real</code>, the x-coordinates appearing on tick mark labels.
An often useful result is stored in the list of data for each axis variable,
namely the exact number of points that correspond to each tick mark on
that variable's axis.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Banks J: Nomograms. Encylopedia of Statistical Sciences, Vol 6.
Editors: S Kotz and NL Johnson.  New York: Wiley; 1985.
</p>
<p>Lubsen J, Pool J, van der Does, E: A practical device for the application
of a diagnostic or prognostic function.  Meth. Inform. Med. 17:127&ndash;129;
1978.
</p>
<p>Wikipedia: Nomogram, <a href="https://en.wikipedia.org/wiki/Nomogram">https://en.wikipedia.org/wiki/Nomogram</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+plot.Predict">plot.Predict</a></code>,
<code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,	<code><a href="#topic+plot.summary.rms">plot.summary.rms</a></code>,
<code><a href="graphics.html#topic+axis">axis</a></code>, <code><a href="base.html#topic+pretty">pretty</a></code>, <code><a href="stats.html#topic+approx">approx</a></code>, 
<code><a href="#topic+latexrms">latexrms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
d &lt;- data.frame(age = rnorm(n, 50, 10),
                blood.pressure = rnorm(n, 120, 15),
                cholesterol = rnorm(n, 200, 25),
                sex = factor(sample(c('female','male'), n,TRUE)))

# Specify population model for log odds that Y=1
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
d &lt;- upData(d,
  L = .4*(sex=='male') + .045*(age-50) +
       (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')),
  y = ifelse(runif(n) &lt; plogis(L), 1, 0))

ddist &lt;- datadist(d); options(datadist='ddist')


f &lt;- lrm(y ~ lsp(age,50) + sex * rcs(cholesterol, 4) + blood.pressure,
         data=d)
nom &lt;- nomogram(f, fun=function(x)1/(1+exp(-x)),  # or fun=plogis
    fun.at=c(.001,.01,.05,seq(.1,.9,by=.1),.95,.99,.999),
    funlabel="Risk of Death")
#Instead of fun.at, could have specified fun.lp.at=logit of
#sequence above - faster and slightly more accurate
plot(nom, xfrac=.45)
print(nom)
nom &lt;- nomogram(f, age=seq(10,90,by=10))
plot(nom, xfrac=.45)
g &lt;- lrm(y ~ sex + rcs(age, 3) * rcs(cholesterol, 3), data=d)
nom &lt;- nomogram(g, interact=list(age=c(20,40,60)), 
                conf.int=c(.7,.9,.95))
plot(nom, col.conf=c(1,.5,.2), naxes=7)

require(survival)
w &lt;- upData(d,
            cens = 15 * runif(n),
            h = .02 * exp(.04 * (age - 50) + .8 * (sex == 'Female')),
            d.time = -log(runif(n)) / h,
            death = ifelse(d.time &lt;= cens, 1, 0),
            d.time = pmin(d.time, cens))


f &lt;- psm(Surv(d.time,death) ~ sex * age, data=w, dist='lognormal')
med  &lt;- Quantile(f)
surv &lt;- Survival(f)  # This would also work if f was from cph
plot(nomogram(f, fun=function(x) med(lp=x), funlabel="Median Survival Time"))
nom &lt;- nomogram(f, fun=list(function(x) surv(3, x),
                            function(x) surv(6, x)),
            funlabel=c("3-Month Survival Probability", 
                       "6-month Survival Probability"))
plot(nom, xfrac=.7)

## Not run: 
nom &lt;- nomogram(fit.with.categorical.predictors, abbrev=TRUE, minlength=1)
nom$x1$points   # print points assigned to each level of x1 for its axis
#Add legend for abbreviations for category levels
abb &lt;- attr(nom, 'info')$abbrev$treatment
legend(locator(1), abb$full, pch=paste(abb$abbrev,collapse=''), 
       ncol=2, bty='n')  # this only works for 1-letter abbreviations
#Or use the legend.nomabbrev function:
legend.nomabbrev(nom, 'treatment', locator(1), ncol=2, bty='n')

## End(Not run)


#Make a nomogram with axes predicting probabilities Y&gt;=j for all j=1-3
#in an ordinal logistic model, where Y=0,1,2,3
w &lt;- upData(w, Y = ifelse(y==0, 0, sample(1:3, length(y), TRUE)))
g &lt;- lrm(Y ~ age+rcs(cholesterol,4) * sex, data=w)
fun2 &lt;- function(x) plogis(x-g$coef[1]+g$coef[2])
fun3 &lt;- function(x) plogis(x-g$coef[1]+g$coef[3])
f &lt;- Newlabels(g, c(age='Age in Years'))  
#see Design.Misc, which also has Newlevels to change 
#labels for levels of categorical variables
g &lt;- nomogram(f, fun=list('Prob Y&gt;=1'=plogis, 'Prob Y&gt;=2'=fun2, 
                     'Prob Y=3'=fun3), 
         fun.at=c(.01,.05,seq(.1,.9,by=.1),.95,.99))
plot(g, lmgp=.2, cex.axis=.6)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='npsurv'>Nonparametric Survival Estimates for Censored Data</h2><span id='topic+npsurv'></span>

<h3>Description</h3>

<p>Computes an estimate of a survival curve for censored data
using either the Kaplan-Meier or the Fleming-Harrington method
or computes the predicted survivor function.
For competing risks data it computes the cumulative incidence curve.
This calls the <code>survival</code> package's <code>survfit.formula</code>
function.  Attributes of the event time variable are saved (label and
units of measurement).
</p>
<p>For competing risks the second argument for <code>Surv</code> should be the
event state variable, and it should be a factor variable with the first
factor level denoting right-censored observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npsurv(formula, data=environment(formula),
              subset, weights, na.action=na.delete, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npsurv_+3A_formula">formula</code></td>
<td>

<p>a formula object, which must have a <code>Surv</code> object as the
response on the left of the <code>~</code> operator and, if desired, terms
separated by + operators on the right.
One of the terms may be a <code>strata</code> object.
For a single survival curve the right hand side should be <code>~ 1</code>.
</p>
</td></tr>
<tr><td><code id="npsurv_+3A_data">data</code>, <code id="npsurv_+3A_subset">subset</code>, <code id="npsurv_+3A_weights">weights</code>, <code id="npsurv_+3A_na.action">na.action</code></td>
<td>
<p>see <code><a href="survival.html#topic+survfit.formula">survfit.formula</a></code></p>
</td></tr>
<tr><td><code id="npsurv_+3A_...">...</code></td>
<td>
<p>see <code><a href="survival.html#topic+survfit.formula">survfit.formula</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <code><a href="survival.html#topic+survfit.formula">survfit.formula</a></code> for details
</p>


<h3>Value</h3>

<p>an object of class <code>"npsurv"</code> and <code>"survfit"</code>.
See <code>survfit.object</code> for details. Methods defined for <code>survfit</code>
objects are <code>print</code>, <code>summary</code>, <code>plot</code>,<code>lines</code>, and
<code>points</code>. 
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley <a href="mailto:tlumley@u.washington.edu">tlumley@u.washington.edu</a> and Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.cph">survfit.cph</a></code> for survival curves from Cox models.
<code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+plot">plot</a></code>,
<code><a href="graphics.html#topic+lines">lines</a></code>,
<code><a href="survival.html#topic+coxph">coxph</a></code>,
<code><a href="survival.html#topic+strata">strata</a></code>,
<code><a href="#topic+survplot">survplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
# fit a Kaplan-Meier and plot it
fit &lt;- npsurv(Surv(time, status) ~ x, data = aml)
plot(fit, lty = 2:3)
legend(100, .8, c("Maintained", "Nonmaintained"), lty = 2:3)

# Here is the data set from Turnbull
#  There are no interval censored subjects, only left-censored (status=3),
#  right-censored (status 0) and observed events (status 1)
#
#                             Time
#                         1    2   3   4
# Type of observation
#           death        12    6   2   3
#          losses         3    2   0   3
#      late entry         2    4   2   5
#
tdata &lt;- data.frame(time   = c(1,1,1,2,2,2,3,3,3,4,4,4),
                    status = rep(c(1,0,2),4),
                    n      = c(12,3,2,6,2,4,2,0,2,3,3,5))
fit  &lt;- npsurv(Surv(time, time, status, type='interval') ~ 1,
               data=tdata, weights=n)

#
# Time to progression/death for patients with monoclonal gammopathy
# Competing risk curves (cumulative incidence)
# status variable must be a factor with first level denoting right censoring
m &lt;- upData(mgus1, stop = stop / 365.25, units=c(stop='years'),
            labels=c(stop='Follow-up Time'), subset=start == 0)
f &lt;- npsurv(Surv(stop, event) ~ 1, data=m)

# CI curves are always plotted from 0 upwards, rather than 1 down
plot(f, fun='event', xmax=20, mark.time=FALSE,
     col=2:3, xlab="Years post diagnosis of MGUS")
text(10, .4, "Competing Risk: death", col=3)
text(16, .15,"Competing Risk: progression", col=2)

# Use survplot for enhanced displays of cumulative incidence curves for
# competing risks

survplot(f, state='pcm', n.risk=TRUE, xlim=c(0, 20), ylim=c(0, .5), col=2)
survplot(f, state='death', add=TRUE, col=3)

f &lt;- npsurv(Surv(stop, event) ~ sex, data=m)
survplot(f, state='death', n.risk=TRUE, conf='diffbands')
</code></pre>

<hr>
<h2 id='ols'>Linear Model Estimation Using Ordinary Least Squares</h2><span id='topic+ols'></span>

<h3>Description</h3>

<p>Fits the usual weighted or unweighted linear regression model using the
same fitting routines used by <code>lm</code>, but also storing the variance-covariance
matrix <code>var</code> and using traditional dummy-variable coding for categorical
factors.  
Also fits unweighted models using penalized least squares, with the same
penalization options as in the <code>lrm</code> function.  For penalized estimation,
there is a fitter function call <code>lm.pfit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols(formula, data=environment(formula), weights, subset, na.action=na.delete, 
    method="qr", model=FALSE,
    x=FALSE, y=FALSE, se.fit=FALSE, linear.predictors=TRUE,
    penalty=0, penalty.matrix, tol=1e-7, sigma,
    var.penalty=c('simple','sandwich'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_+3A_formula">formula</code></td>
<td>

<p>an S formula object, e.g. 
<br />
Y ~ rcs(x1,5)*lsp(x2,c(10,20))
</p>
</td></tr>
<tr><td><code id="ols_+3A_data">data</code></td>
<td>

<p>name of an S data frame containing all needed variables.  Omit this to use a
data frame already in the S &ldquo;search list&rdquo;.
</p>
</td></tr>
<tr><td><code id="ols_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. If specified, weighted least squares is used with
weights <code>weights</code> (that is, minimizing <code class="reqn">sum(w*e^2)</code>);
otherwise ordinary least squares is used.</p>
</td></tr>
<tr><td><code id="ols_+3A_subset">subset</code></td>
<td>

<p>an expression defining a subset of the observations to use in the fit.  The default
is to use all observations.  Specify for example <code>age&gt;50 &amp; sex="male"</code> or
<code>c(1:100,200:300)</code>
respectively to use the observations satisfying a logical expression or those having
row numbers in the given vector.
</p>
</td></tr>
<tr><td><code id="ols_+3A_na.action">na.action</code></td>
<td>

<p>specifies an S function to handle missing data.  The default is the function <code>na.delete</code>,
which causes observations with any variable missing to be deleted.  The main difference
between <code>na.delete</code> and the S-supplied function <code>na.omit</code> is that 
<code>na.delete</code> makes a list
of the number of observations that are missing on each variable in the model.
The <code>na.action</code> is usally specified by e.g. <code>options(na.action="na.delete")</code>.
</p>
</td></tr>
<tr><td><code id="ols_+3A_method">method</code></td>
<td>

<p>specifies a particular fitting method, or <code>"model.frame"</code> instead to return the model frame
of the predictor and response variables satisfying any subset or missing value
checks.
</p>
</td></tr>
<tr><td><code id="ols_+3A_model">model</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to return the model frame
as element <code>model</code> of the fit object.
</p>
</td></tr>
<tr><td><code id="ols_+3A_x">x</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to return the expanded design matrix as element <code>x</code>
(without intercept indicators) of the
returned fit object.  Set both <code>x=TRUE</code> if you are going to use
the <code>residuals</code> function later to return anything other than ordinary residuals.
</p>
</td></tr>
<tr><td><code id="ols_+3A_y">y</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to return the vector of response values 
as element <code>y</code> of the fit.
</p>
</td></tr>
<tr><td><code id="ols_+3A_se.fit">se.fit</code></td>
<td>

<p>default is <code>FALSE</code>.  Set to <code>TRUE</code> to compute the estimated standard errors of
the estimate of <code class="reqn">X\beta</code> and store them in element <code>se.fit</code>
of the fit. 
</p>
</td></tr>
<tr><td><code id="ols_+3A_linear.predictors">linear.predictors</code></td>
<td>

<p>set to <code>FALSE</code> to cause predicted values not to be stored
</p>
</td></tr>
<tr><td><code id="ols_+3A_penalty">penalty</code></td>
<td>
<p>see <code>lrm</code></p>
</td></tr>
<tr><td><code id="ols_+3A_penalty.matrix">penalty.matrix</code></td>
<td>
<p>see <code>lrm</code></p>
</td></tr>
<tr><td><code id="ols_+3A_tol">tol</code></td>
<td>
<p>tolerance for information matrix singularity</p>
</td></tr>
<tr><td><code id="ols_+3A_sigma">sigma</code></td>
<td>

<p>If <code>sigma</code> is given, it is taken as the actual root mean squared error parameter for the model.  Otherwise <code>sigma</code> is estimated from the data using the usual formulas (except for penalized models).  It is often convenient to specify
<code>sigma=1</code> for models with no error, when using <code>fastbw</code> to find an
approximate model that predicts predicted values from the full model with
a given accuracy.
</p>
</td></tr>
<tr><td><code id="ols_+3A_var.penalty">var.penalty</code></td>
<td>

<p>the type of variance-covariance matrix to be stored in the <code>var</code>
component of the fit when penalization is used.  The default is the
inverse of the penalized information matrix.  Specify
<code>var.penalty="sandwich"</code> to use the sandwich estimator (see below
under <code>var</code>), which limited simulation studies have shown yields
variances estimates that are too low.
</p>
</td></tr>
<tr><td><code id="ols_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code><a href="stats.html#topic+lm.wfit">lm.wfit</a></code> or
<code><a href="stats.html#topic+lm.fit">lm.fit</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For penalized estimation, the penalty factor on the log likelihood is
<code class="reqn">-0.5 \beta' P \beta / \sigma^2</code>, where <code class="reqn">P</code> is defined above.
The penalized maximum likelihood estimate (penalized least squares
or ridge estimate) of <code class="reqn">\beta</code> is <code class="reqn">(X'X + P)^{-1} X'Y</code>.
The maximum likelihood estimate of <code class="reqn">\sigma^2</code> is <code class="reqn">(sse + \beta'
  P \beta) / n</code>, where
<code>sse</code> is the sum of squared errors (residuals).
The <code>effective.df.diagonal</code> vector is the
diagonal of the matrix <code class="reqn">X'X/(sse/n) \sigma^{2} (X'X + P)^{-1}</code>.
</p>


<h3>Value</h3>

<p>the same objects returned from <code>lm</code> (unless <code>penalty</code> or <code>penalty.matrix</code>
are given - then an
abbreviated list is returned since <code>lm.pfit</code> is used as a fitter)
plus the design attributes
(see <code>rms</code>).
Predicted values are always returned, in the element <code>linear.predictors</code>.
The vectors or matrix stored if <code>y=TRUE</code> or <code>x=TRUE</code> have rows deleted according to <code>subset</code> and
to missing data, and have names or row names that come from the
data frame used as input data.  If <code>penalty</code> or <code>penalty.matrix</code> is given, 
the <code>var</code> matrix
returned is an improved variance-covariance matrix
for the penalized regression coefficient estimates.  If
<code>var.penalty="sandwich"</code> (not the default, as limited simulation
studies have found it provides variance estimates that are too low) it
is defined as 
<code class="reqn">\sigma^{2} (X'X + P)^{-1} X'X (X'X + P)^{-1}</code>, where <code class="reqn">P</code> is 
<code>penalty factors * penalty.matrix</code>, with a column and row of zeros
added for the
intercept.  When <code>var.penalty="simple"</code> (the default), <code>var</code> is
<code class="reqn">\sigma^{2} (X'X + P)^{-1}</code>.
The returned list has a vector <code>stats</code> with named elements
<code>n, Model L.R., d.f., R2, g, Sigma</code>.  <code>Model L.R.</code> is the model
likelihood ratio <code class="reqn">\chi^2</code> statistic, and <code>R2</code> is
<code class="reqn">R^2</code>.  For penalized estimation, <code>d.f.</code> is the 
effective degrees of freedom, which is the sum of the elements of another
vector returned, <code>effective.df.diagonal</code>, minus one for the
intercept.
<code>g</code> is the <code class="reqn">g</code>-index.
<code>Sigma</code> is the penalized maximum likelihood estimate (see below).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+predict.rms">predict.rms</a></code>,
<code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+specs.rms">specs.rms</a></code>, <code><a href="#topic+cph">cph</a></code>,
<code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+which.influence">which.influence</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>,
<code><a href="stats.html#topic+summary.lm">summary.lm</a></code>, <code><a href="#topic+print.ols">print.ols</a></code>,
<code><a href="#topic+residuals.ols">residuals.ols</a></code>, <code><a href="#topic+latex.ols">latex.ols</a></code>,
<code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>, <code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+pentrace">pentrace</a></code>, <code><a href="#topic+vif">vif</a></code>,
<code><a href="Hmisc.html#topic+abs.error.pred">abs.error.pred</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- runif(200)
x2 &lt;- sample(0:3, 200, TRUE)
distance &lt;- (x1 + x2/3 + rnorm(200))^2
d &lt;- datadist(x1,x2)
options(datadist="d")   # No d -&gt; no summary, plot without giving all details


f &lt;- ols(sqrt(distance) ~ rcs(x1,4) + scored(x2), x=TRUE)
# could use d &lt;- datadist(f); options(datadist="d") at this point,
# but predictor summaries would not be stored in the fit object for
# use with Predict, summary.rms.  In that case, the original
# dataset or d would need to be accessed later, or all variable values
# would have to be specified to summary, plot
anova(f)
which.influence(f)
summary(f)
summary.lm(f)    # will only work if penalty and penalty.matrix not used


# Fit a complex model and approximate it with a simple one
x1 &lt;- runif(200)
x2 &lt;- runif(200)
x3 &lt;- runif(200)
x4 &lt;- runif(200)
y &lt;- x1 + x2 + rnorm(200)
f    &lt;- ols(y ~ rcs(x1,4) + x2 + x3 + x4)
pred &lt;- fitted(f)   # or predict(f) or f$linear.predictors
f2   &lt;- ols(pred ~ rcs(x1,4) + x2 + x3 + x4, sigma=1)
# sigma=1 prevents numerical problems resulting from R2=1
fastbw(f2, aics=100000)
# This will find the best 1-variable model, best 2-variable model, etc.
# in predicting the predicted values from the original model
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='orm'>Ordinal Regression Model</h2><span id='topic+orm'></span><span id='topic+print.orm'></span><span id='topic+Quantile.orm'></span>

<h3>Description</h3>

<p>Fits ordinal cumulative probability models for continuous or ordinal
response variables, efficiently allowing for a large number of
intercepts by capitalizing on the information matrix being sparse.
Five different distribution functions are implemented, with the
default being the logistic (i.e., the proportional odds
model).  The ordinal cumulative probability models are stated in terms
of exceedance probabilities (<code class="reqn">Prob[Y \ge y | X]</code>) so that as with
OLS larger predicted values are associated with larger <code>Y</code>.  This is
important to note for the asymmetric distributions given by the
log-log and complementary log-log families, for which negating the
linear predictor does not result in <code class="reqn">Prob[Y &lt; y | X]</code>.  The
<code>family</code> argument is defined in <code><a href="#topic+orm.fit">orm.fit</a></code>.  The model
assumes that the inverse of the assumed cumulative distribution
function, when applied to one minus the true cumulative distribution function
and plotted on the <code class="reqn">y</code>-axis (with the original <code class="reqn">y</code> on the
<code class="reqn">x</code>-axis) yields parallel curves (though not necessarily linear).
This can be checked by plotting the inverse cumulative probability
function of one minus the empirical distribution function, stratified
by <code>X</code>, and assessing parallelism.  Note that parametric
regression models make the much stronger assumption of linearity of
such inverse functions.
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>
<p><code>Quantile.orm</code> creates an R function that computes an estimate of
a given quantile for a given value of the linear predictor (which was
assumed to use thefirst intercept).  It uses a linear
interpolation method by default, but you can override that to use a
discrete method by specifying <code>method="discrete"</code> when calling
the function generated by <code>Quantile</code>.
Optionally a normal approximation for a confidence
interval for quantiles will be computed using the delta method, if
<code>conf.int &gt; 0</code> is specified to the function generated from calling
<code>Quantile</code> and you specify <code>X</code>.  In that case, a
<code>"lims"</code> attribute is included 
in the result computed by the derived quantile function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orm(formula, data=environment(formula),
    subset, na.action=na.delete, method="orm.fit",
    model=FALSE, x=FALSE, y=FALSE, linear.predictors=TRUE, se.fit=FALSE, 
    penalty=0, penalty.matrix, tol=1e-7, eps=0.005, 
    var.penalty=c('simple','sandwich'), scale=FALSE, ...)

## S3 method for class 'orm'
print(x, digits=4, r2=c(0,2,4), coefs=TRUE, pg=FALSE,
    intercepts=x$non.slopes &lt; 10, title, ...)

## S3 method for class 'orm'
Quantile(object, codes=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orm_+3A_formula">formula</code></td>
<td>

<p>a formula object. An <code>offset</code> term can be included. The offset causes
fitting of a model such as <code class="reqn">logit(Y=1) = X\beta + W</code>, where <code class="reqn">W</code> is the
offset variable having no estimated coefficient.
The response variable can be any data type; <code>orm</code> converts it
in alphabetic or numeric order to a factor variable and
recodes it 1,2,... internally. 
</p>
</td></tr>
<tr><td><code id="orm_+3A_data">data</code></td>
<td>

<p>data frame to use. Default is the current frame.
</p>
</td></tr>
<tr><td><code id="orm_+3A_subset">subset</code></td>
<td>

<p>logical expression or vector of subscripts defining a subset of
observations to analyze
</p>
</td></tr>
<tr><td><code id="orm_+3A_na.action">na.action</code></td>
<td>

<p>function to handle <code>NA</code>s in the data. Default is <code>na.delete</code>, which
deletes any observation having response or predictor missing, while
preserving the attributes of the predictors and maintaining frequencies
of deletions due to each variable in the model.  
This is usually specified using <code>options(na.action="na.delete")</code>.
</p>
</td></tr>
<tr><td><code id="orm_+3A_method">method</code></td>
<td>

<p>name of fitting function. Only allowable choice at present is <code>orm.fit</code>.
</p>
</td></tr>
<tr><td><code id="orm_+3A_model">model</code></td>
<td>

<p>causes the model frame to be returned in the fit object
</p>
</td></tr>
<tr><td><code id="orm_+3A_x">x</code></td>
<td>

<p>causes the expanded design matrix (with missings excluded)
to be returned under the name <code>x</code>.  For <code>print</code>, an object
created by <code>orm</code>.
</p>
</td></tr>
<tr><td><code id="orm_+3A_y">y</code></td>
<td>

<p>causes the response variable (with missings excluded) to be returned
under the name <code>y</code>.
</p>
</td></tr>
<tr><td><code id="orm_+3A_linear.predictors">linear.predictors</code></td>
<td>

<p>causes the predicted X beta (with missings excluded) to be returned
under the name <code>linear.predictors</code>.  The first intercept is used.
</p>
</td></tr>
<tr><td><code id="orm_+3A_se.fit">se.fit</code></td>
<td>

<p>causes the standard errors of the fitted values (on the linear predictor
scale) to be returned under the name <code>se.fit</code>.  The middle
intercept is used.
</p>
</td></tr>
<tr><td><code id="orm_+3A_penalty">penalty</code></td>
<td>
<p>see <code><a href="#topic+lrm">lrm</a></code></p>
</td></tr>
<tr><td><code id="orm_+3A_penalty.matrix">penalty.matrix</code></td>
<td>
<p>see <code><a href="#topic+lrm">lrm</a></code></p>
</td></tr>
<tr><td><code id="orm_+3A_tol">tol</code></td>
<td>
<p>singularity criterion (see <code>orm.fit</code>)</p>
</td></tr>
<tr><td><code id="orm_+3A_eps">eps</code></td>
<td>
<p>difference in <code class="reqn">-2 log</code> likelihood for declaring convergence</p>
</td></tr>
<tr><td><code id="orm_+3A_var.penalty">var.penalty</code></td>
<td>
<p>see <code><a href="#topic+lrm">lrm</a></code></p>
</td></tr>
<tr><td><code id="orm_+3A_scale">scale</code></td>
<td>
<p>set to <code>TRUE</code> to subtract column means and divide by
column standard deviations of the design matrix
before fitting, and to back-solve for the un-normalized covariance
matrix and regression coefficients.  This can sometimes make the model
converge for very large 
sample sizes where for example spline or polynomial component
variables create scaling problems leading to loss of precision when
accumulating sums of squares and crossproducts.</p>
</td></tr>
<tr><td><code id="orm_+3A_...">...</code></td>
<td>
<p>arguments that are passed to <code>orm.fit</code>, or from
<code>print</code>, to <code><a href="#topic+prModFit">prModFit</a></code>.  Ignored for
<code>Quantile</code>.  One of the most important arguments is <code>family</code>.</p>
</td></tr>
<tr><td><code id="orm_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to use</p>
</td></tr>
<tr><td><code id="orm_+3A_r2">r2</code></td>
<td>
<p>vector of integers specifying which R^2 measures to print,
with 0 for Nagelkerke R^2 and 1:4 corresponding to the 4 measures
computed by <code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>.  Default is to print
Nagelkerke (labeled R2) and second and fourth <code>R2Measures</code>
which are the measures adjusted for the number of predictors, first
for the raw sample size then for the effective sample size, which
here is from the formula for the approximate variance of a log odds
ratio in a proportional odds model.</p>
</td></tr>
<tr><td><code id="orm_+3A_pg">pg</code></td>
<td>
<p>set to <code>TRUE</code> to print g-indexes</p>
</td></tr>
<tr><td><code id="orm_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="orm_+3A_intercepts">intercepts</code></td>
<td>
<p>By default, intercepts are only printed if there are
fewer than 10 of them.  Otherwise this is controlled by specifying
<code>intercepts=FALSE</code> or <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="orm_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code>.
Default is constructed from the name of the distribution family.</p>
</td></tr>
<tr><td><code id="orm_+3A_object">object</code></td>
<td>
<p>an object created by <code>orm</code></p>
</td></tr>
<tr><td><code id="orm_+3A_codes">codes</code></td>
<td>
<p>if <code>TRUE</code>, uses the integer codes <code class="reqn">1,2,\ldots,k</code>
for the <code class="reqn">k</code>-level response in computing the predicted quantile</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned fit object of <code>orm</code> contains the following components
in addition to the ones mentioned under the optional arguments.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>calling expression</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>

<p>table of frequencies for <code>Y</code> in order of increasing <code>Y</code></p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>vector with the following elements: number of observations used in the
fit, number of unique <code>Y</code> values, median <code>Y</code> from among the
observations used int he fit, maximum absolute value of first
derivative of log likelihood, model likelihood ratio
<code class="reqn">\chi^2</code>, d.f.,  <code class="reqn">P</code>-value, score <code class="reqn">\chi^2</code>
statistic (if no initial values given), <code class="reqn">P</code>-value, Spearman's
<code class="reqn">\rho</code> rank correlation between the linear predictor and <code>Y</code>,
the Nagelkerke <code class="reqn">R^2</code> index, <code class="reqn">R^2</code> indexes computed by
<code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>, the <code class="reqn">g</code>-index, <code class="reqn">gr</code> (the
<code class="reqn">g</code>-index on the odds ratio scale), and <code class="reqn">pdm</code> (the mean absolute
difference between 0.5 and the predicted probability that <code class="reqn">Y\geq</code>
the marginal median).
In the case of penalized estimation, the <code>"Model L.R."</code> is computed
without the penalty factor, and <code>"d.f."</code> is the effective d.f. from
Gray's (1992) Equation 2.9.  The <code class="reqn">P</code>-value uses this corrected model
L.R. <code class="reqn">\chi^2</code> and corrected d.f. 
The score chi-square statistic uses first derivatives which contain
penalty components.
</p>
</td></tr>
<tr><td><code>fail</code></td>
<td>

<p>set to <code>TRUE</code> if convergence failed (and <code>maxiter&gt;1</code>) or if a
singular information matrix is encountered
</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>estimated parameters</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>estimated variance-covariance matrix (inverse of information matrix)
for the middle intercept and regression coefficients.  See
<code><a href="#topic+lrm">lrm</a></code> for details if penalization is used.
</p>
</td></tr>
<tr><td><code>effective.df.diagonal</code></td>
<td>
<p>see <code><a href="#topic+lrm">lrm</a></code></p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>the character string for <code>family</code>.  If <code>family</code>
was a user-customized list, it must have had an element named
<code>name</code>, which is taken as the return value for <code>family</code> here.</p>
</td></tr>
<tr><td><code>trans</code></td>
<td>
<p>a list of functions for the choice of <code>family</code>, with
elements <code>cumprob</code> (the cumulative probability distribution
function), <code>inverse</code> (inverse of <code>cumprob</code>), <code>deriv</code>
(first derivative of <code>cumprob</code>), and <code>deriv2</code> (second
derivative of <code>cumprob</code>)</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>

<p>-2 log likelihoods (counting penalty components)
When an offset variable is present, three
deviances are computed: for intercept(s) only, for
intercepts+offset, and for intercepts+offset+predictors.
When there is no offset variable, the vector contains deviances for
the intercept(s)-only model and the model with intercept(s) and predictors.
</p>
</td></tr>
<tr><td><code>non.slopes</code></td>
<td>
<p>number of intercepts in model</p>
</td></tr>
<tr><td><code>interceptRef</code></td>
<td>
<p>the index of the middle (median) intercept used in
computing the linear predictor and <code>var</code></p>
</td></tr>
<tr><td><code>penalty</code></td>
<td>
<p>see <code><a href="#topic+lrm">lrm</a></code></p>
</td></tr>
<tr><td><code>penalty.matrix</code></td>
<td>
<p>the penalty matrix actually used in the
estimation</p>
</td></tr>
<tr><td><code>info.matrix</code></td>
<td>
<p>a sparse matrix representation of type
<code>matrix.csr</code> from the <code>SparseM</code> package.  This allows the
full information matrix with all intercepts to be stored efficiently,
and matrix operations using the Cholesky decomposition to be fast.
<code>link{vcov.orm}</code> uses this information to compute the covariance
matrix for intercepts other than the middle one.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com<br />
For the <code>Quantile</code> function:<br />
Qi Liu and Shengxin Tu<br />
Department of Biostatistics, Vanderbilt University
</p>


<h3>References</h3>

<p>Sall J: A monotone regression smoother based on ordinal cumulative
logistic regression, 1991.
</p>
<p>Le Cessie S, Van Houwelingen JC: Ridge estimators in logistic regression.
Applied Statistics 41:191&ndash;201, 1992.
</p>
<p>Verweij PJM, Van Houwelingen JC: Penalized likelihood in Cox regression.
Stat in Med 13:2427&ndash;2436, 1994.
</p>
<p>Gray RJ: Flexible methods for analyzing survival data using splines,
with applications to breast cancer prognosis.  JASA 87:942&ndash;951, 1992.
</p>
<p>Shao J: Linear model selection by cross-validation.  JASA 88:486&ndash;494, 1993.
</p>
<p>Verweij PJM, Van Houwelingen JC: Crossvalidation in survival analysis.
Stat in Med 12:2305&ndash;2314, 1993.
</p>
<p>Harrell FE: Model uncertainty, penalization, and parsimony.  Available
from <a href="https://hbiostat.org/talks/iscb98.pdf">https://hbiostat.org/talks/iscb98.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+orm.fit">orm.fit</a></code>, <code><a href="#topic+predict.orm">predict.orm</a></code>, <code><a href="SparseM.html#topic+SparseM.solve">solve</a></code>,
<code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="MASS.html#topic+polr">polr</a></code>,
<code><a href="#topic+latex.orm">latex.orm</a></code>,  <code><a href="#topic+vcov.orm">vcov.orm</a></code>,
<code><a href="Hmisc.html#topic+num.intercepts">num.intercepts</a></code>,
<code><a href="#topic+residuals.orm">residuals.orm</a></code>, <code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>,
<code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>,
<code><a href="#topic+pentrace">pentrace</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+vif">vif</a></code>,
<code><a href="#topic+predab.resample">predab.resample</a></code>,
<code><a href="#topic+validate.orm">validate.orm</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+Mean.orm">Mean.orm</a></code>, <code><a href="#topic+gIndex">gIndex</a></code>, <code><a href="#topic+prModFit">prModFit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
set.seed(1)
n &lt;- 100
y &lt;- round(runif(n), 2)
x1 &lt;- sample(c(-1,0,1), n, TRUE)
x2 &lt;- sample(c(-1,0,1), n, TRUE)
f &lt;- lrm(y ~ x1 + x2, eps=1e-5)
g &lt;- orm(y ~ x1 + x2, eps=1e-5)
max(abs(coef(g) - coef(f)))
w &lt;- vcov(g, intercepts='all') / vcov(f) - 1
max(abs(w))

set.seed(1)
n &lt;- 300
x1 &lt;- c(rep(0,150), rep(1,150))
y &lt;- rnorm(n) + 3*x1
g &lt;- orm(y ~ x1)
g
k &lt;- coef(g)
i &lt;- num.intercepts(g)
h &lt;- orm(y ~ x1, family=probit)
ll &lt;- orm(y ~ x1, family=loglog)
cll &lt;- orm(y ~ x1, family=cloglog)
cau &lt;- orm(y ~ x1, family=cauchit)
x &lt;- 1:i
z &lt;- list(logistic=list(x=x, y=coef(g)[1:i]),
          probit  =list(x=x, y=coef(h)[1:i]),
          loglog  =list(x=x, y=coef(ll)[1:i]),
          cloglog =list(x=x, y=coef(cll)[1:i]))
labcurve(z, pl=TRUE, col=1:4, ylab='Intercept')

tapply(y, x1, mean)
m &lt;- Mean(g)
m(w &lt;- k[1] + k['x1']*c(0,1))
mh &lt;- Mean(h)
wh &lt;- coef(h)[1] + coef(h)['x1']*c(0,1)
mh(wh)

qu &lt;- Quantile(g)
# Compare model estimated and empirical quantiles
cq &lt;- function(y) {
   cat(qu(.1, w), tapply(y, x1, quantile, probs=.1), '\n')
   cat(qu(.5, w), tapply(y, x1, quantile, probs=.5), '\n')
   cat(qu(.9, w), tapply(y, x1, quantile, probs=.9), '\n')
   }
cq(y)

# Try on log-normal model
g &lt;- orm(exp(y) ~ x1)
g
k &lt;- coef(g)
plot(k[1:i])
m &lt;- Mean(g)
m(w &lt;- k[1] + k['x1']*c(0,1))
tapply(exp(y), x1, mean)

qu &lt;- Quantile(g)
cq(exp(y))

# Compare predicted mean with ols for a continuous x
set.seed(3)
n &lt;- 200
x1 &lt;- rnorm(n)
y &lt;- x1 + rnorm(n)
dd &lt;- datadist(x1); options(datadist='dd')
f &lt;- ols(y ~ x1)
g &lt;- orm(y ~ x1, family=probit)
h &lt;- orm(y ~ x1, family=logistic)
w &lt;- orm(y ~ x1, family=cloglog)
mg &lt;- Mean(g); mh &lt;- Mean(h); mw &lt;- Mean(w)
r &lt;- rbind(ols      = Predict(f, conf.int=FALSE),
           probit   = Predict(g, conf.int=FALSE, fun=mg),
           logistic = Predict(h, conf.int=FALSE, fun=mh),
           cloglog  = Predict(w, conf.int=FALSE, fun=mw))
plot(r, groups='.set.')

# Compare predicted 0.8 quantile with quantile regression
qu &lt;- Quantile(g)
qu80 &lt;- function(lp) qu(.8, lp)
f &lt;- Rq(y ~ x1, tau=.8)
r &lt;- rbind(probit   = Predict(g, conf.int=FALSE, fun=qu80),
           quantreg = Predict(f, conf.int=FALSE))
plot(r, groups='.set.')

# Verify transformation invariance of ordinal regression
ga &lt;- orm(exp(y) ~ x1, family=probit)
qua &lt;- Quantile(ga)
qua80 &lt;- function(lp) log(qua(.8, lp))
r &lt;- rbind(logprobit = Predict(ga, conf.int=FALSE, fun=qua80),
           probit    = Predict(g,  conf.int=FALSE, fun=qu80))
plot(r, groups='.set.')

# Try the same with quantile regression.  Need to transform x1
fa &lt;- Rq(exp(y) ~ rcs(x1,5), tau=.8)
r &lt;- rbind(qr    = Predict(f, conf.int=FALSE),
           logqr = Predict(fa, conf.int=FALSE, fun=log))
plot(r, groups='.set.')

# Make a plot of Pr(Y &gt;= y) vs. a continuous covariate for 3 levels
# of y and also against a binary covariate
set.seed(1)
n &lt;- 1000
age &lt;- rnorm(n, 50, 15)
sex &lt;- sample(c('m', 'f'), 1000, TRUE)
Y   &lt;- runif(n)
dd  &lt;- datadist(age, sex); options(datadist='dd')
f &lt;- orm(Y ~ age + sex)
# Use ExProb function to derive an R function to compute
# P(Y &gt;= y | X)
ex  &lt;- ExProb(f)
ex1 &lt;- function(x) ex(x, y=0.25)
ex2 &lt;- function(x) ex(x, y=0.5)
ex3 &lt;- function(x) ex(x, y=0.75)
p1  &lt;- Predict(f, age, sex, fun=ex1)
p2  &lt;- Predict(f, age, sex, fun=ex2)
p3  &lt;- Predict(f, age, sex, fun=ex3)
p   &lt;- rbind('P(Y &gt;= 0.25)' = p1,
             'P(Y &gt;= 0.5)'  = p2,
             'P(Y &gt;= 0.75)' = p3)
ggplot(p)

# Make plot with two curves (by sex) with y on the x-axis, and
# estimated P(Y &gt;= y | sex, age=median) on the y-axis
ys &lt;- seq(min(Y), max(Y), length=100)
g &lt;- function(sx) as.vector(ex(y=ys, Predict(f, sex=sx)$yhat)$prob)

d  &lt;- rbind(data.frame(sex='m', y=ys, p=g('m')),
            data.frame(sex='f', y=ys, p=g('f')))
ggplot(d, aes(x=y, y=p, color=sex)) + geom_line() +
  ylab(expression(P(Y &gt;= y))) +
  guides(color=guide_legend(title='Sex')) +
  theme(legend.position='bottom')

options(datadist=NULL)
## Not run: 
## Simulate power and type I error for orm logistic and probit regression
## for likelihood ratio, Wald, and score chi-square tests, and compare
## with t-test
require(rms)
set.seed(5)
nsim &lt;- 2000
r &lt;- NULL
for(beta in c(0, .4)) {
  for(n in c(10, 50, 300)) {
    cat('beta=', beta, '  n=', n, '\n\n')
    plogistic &lt;- pprobit &lt;- plogistics &lt;- pprobits &lt;- plogisticw &lt;-
      pprobitw &lt;- ptt &lt;- numeric(nsim)
    x &lt;- c(rep(0, n/2), rep(1, n/2))
    pb &lt;- setPb(nsim, every=25, label=paste('beta=', beta, '  n=', n))
    for(j in 1:nsim) {
      pb(j)
      y &lt;- beta*x + rnorm(n)
      tt &lt;- t.test(y ~ x)
      ptt[j] &lt;- tt$p.value
      f &lt;- orm(y ~ x)
      plogistic[j]  &lt;- f$stats['P']
      plogistics[j] &lt;- f$stats['Score P']
      plogisticw[j] &lt;- 1 - pchisq(coef(f)['x']^2 / vcov(f)[2,2], 1)
      f &lt;- orm(y ~ x, family=probit)
      pprobit[j]  &lt;- f$stats['P']
      pprobits[j] &lt;- f$stats['Score P']
      pprobitw[j] &lt;- 1 - pchisq(coef(f)['x']^2 / vcov(f)[2,2], 1)
    }
    if(beta == 0) plot(ecdf(plogistic))
    r &lt;- rbind(r, data.frame(beta         = beta, n=n,
                             ttest        = mean(ptt        &lt; 0.05),
                             logisticlr   = mean(plogistic  &lt; 0.05),
                             logisticscore= mean(plogistics &lt; 0.05),
                             logisticwald = mean(plogisticw &lt; 0.05),
                             probit       = mean(pprobit    &lt; 0.05),
                             probitscore  = mean(pprobits   &lt; 0.05),
                             probitwald   = mean(pprobitw   &lt; 0.05)))
  }
}
print(r)
#  beta   n  ttest logisticlr logisticscore logisticwald probit probitscore probitwald
#1  0.0  10 0.0435     0.1060        0.0655        0.043 0.0920      0.0920     0.0820
#2  0.0  50 0.0515     0.0635        0.0615        0.060 0.0620      0.0620     0.0620
#3  0.0 300 0.0595     0.0595        0.0590        0.059 0.0605      0.0605     0.0605
#4  0.4  10 0.0755     0.1595        0.1070        0.074 0.1430      0.1430     0.1285
#5  0.4  50 0.2950     0.2960        0.2935        0.288 0.3120      0.3120     0.3120
#6  0.4 300 0.9240     0.9215        0.9205        0.920 0.9230      0.9230     0.9230

## End(Not run)
</code></pre>

<hr>
<h2 id='orm.fit'>Ordinal Regression Model Fitter</h2><span id='topic+orm.fit'></span>

<h3>Description</h3>

<p>Fits ordinal cumulative probability models for continuous or ordinal
response variables, efficiently allowing for a large number of
intercepts by capitalizing on the information matrix being sparse.
Five different distribution functions are implemented, with the
default being the logistic (yielding the proportional odds
model).  Penalized estimation will be implemented in the future.
Weights are not implemented.
The optimization method is Newton-Raphson with step-halving.
Execution time is linear in the number of intercepts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orm.fit(x=NULL, y, family='logistic',
        offset=0., initial,  maxit=12L, eps=.005, tol=1e-7, trace=FALSE,
        penalty.matrix=NULL, scale=FALSE, y.precision = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orm.fit_+3A_x">x</code></td>
<td>

<p>design matrix with no column for an intercept
</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_y">y</code></td>
<td>

<p>response vector, numeric, factor, or character.  The ordering of levels
is assumed from <code>factor(y)</code>.
</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_family">family</code></td>
<td>
<p>the distribution family, corresponding to logistic (the
default), Gaussian, Cauchy, Gumbel maximum (<code class="reqn">exp(-exp(-x))</code>;
extreme value type I), and Gumbel minimum
(<code class="reqn">1-exp(-exp(x))</code>) distributions.  These are the cumulative
distribution functions assumed for <code class="reqn">Prob[Y \ge y | X]</code>.  The
<code>family</code> argument can be an unquoted or a quoted string,
e.g. <code>family=loglog</code> or <code>family="loglog"</code>.  To use
a built-in family, the string must be one of the following
corresponding to the previous list: <code>logistic, probit, loglog,
		cloglog, cauchit</code>.  The user can also provide her own customized
family by setting <code>family</code> to a list with elements <code>cumprob,
	inverse, deriv, deriv2</code>; see the body of <code>orm.fit</code> for examples.
An additional element, <code>name</code> must be given, which is a character
string used to name the family for <code>print</code> and <code>latex</code>.</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_offset">offset</code></td>
<td>
<p>optional numeric vector containing an offset on the logit scale</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_initial">initial</code></td>
<td>
<p>vector of initial parameter estimates, beginning with the
intercepts.  If <code>initial</code> is not specified, the function computes
the overall score <code class="reqn">\chi^2</code> test for the global null hypothesis of
no regression.</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_maxit">maxit</code></td>
<td>
<p>maximum no. iterations (default=<code>12</code>).</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_eps">eps</code></td>
<td>

<p>difference in <code class="reqn">-2 log</code> likelihood for declaring convergence.
Default is <code>.005</code>.  If the <code class="reqn">-2 log</code> likelihood gets
worse by eps/10 while the maximum absolute first derivative of
</p>
<pre>-2 log</pre><p> likelihood is below 1E-9, convergence is still
declared.  This handles the case where the initial estimates are MLEs,
to prevent endless step-halving.
</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_tol">tol</code></td>
<td>
<p>Singularity criterion. Default is 1e-7</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_trace">trace</code></td>
<td>

<p>set to <code>TRUE</code> to print -2 log likelihood, step-halving
fraction, change in -2 log likelihood, and maximum absolute value of first
derivative at each iteration.
</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_penalty.matrix">penalty.matrix</code></td>
<td>

<p>a self-contained ready-to-use penalty matrix - see<code>lrm</code> 
</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_scale">scale</code></td>
<td>
<p>set to <code>TRUE</code> to subtract column means and divide by
column standard deviations of <code>x</code>
before fitting, and to back-solve for the un-normalized covariance
matrix and regression coefficients.  This can sometimes make the model
converge for very large 
sample sizes where for example spline or polynomial component
variables create scaling problems leading to loss of precision when
accumulating sums of squares and crossproducts.</p>
</td></tr>
<tr><td><code id="orm.fit_+3A_y.precision">y.precision</code></td>
<td>
<p>When &lsquo;y&rsquo; is numeric, values may need to be rounded
to avoid unpredictable behavior with <code>unique()</code> with floating-point
numbers. Default is to 7 decimal places.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>calling expression
</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>

<p>table of frequencies for <code>y</code> in order of increasing <code>y</code>
</p>
</td></tr>
<tr><td><code>yunique</code></td>
<td>
<p>vector of sorted unique values of <code>y</code></p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>vector with the following elements: number of observations used in the
fit, number of unique <code>y</code> values, median <code>y</code> from among the
observations used in the fit, maximum absolute value of first
derivative of log likelihood, model likelihood ratio chi-square, d.f.,
P-value, score chi-square and its P-value, Spearman's <code class="reqn">\rho</code> rank
correlation between linear predictor and <code>y</code>, the
Nagelkerke <code class="reqn">R^2</code> index, the <code class="reqn">g</code>-index, <code class="reqn">gr</code> (the
<code class="reqn">g</code>-index on the ratio scale), and <code class="reqn">pdm</code> (the mean absolute
difference between 0.5 and the estimated probability that <code class="reqn">y\geq</code>
the marginal median).
When <code>penalty.matrix</code> is present, the <code class="reqn">\chi^2</code>,
d.f., and P-value are not corrected for the effective d.f.
</p>
</td></tr>
<tr><td><code>fail</code></td>
<td>

<p>set to <code>TRUE</code> if convergence failed (and <code>maxit&gt;1</code>)
</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>

<p>estimated parameters
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>estimated variance-covariance matrix (inverse of information matrix).
Note that in the case of penalized estimation, <code>var</code> is not the
improved sandwich-type estimator (which <code>lrm</code> does compute).  The
only intercept parameter included in the stored object is the middle
intercept.
</p>
</td></tr>
<tr><td><code>family</code>, <code>trans</code></td>
<td>
<p>see <code><a href="#topic+orm">orm</a></code></p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>

<p>-2 log likelihoods. 
When an offset variable is present, three
deviances are computed: for intercept(s) only, for
intercepts+offset, and for intercepts+offset+predictors.
When there is no offset variable, the vector contains deviances for
the intercept(s)-only model and the model with intercept(s) and predictors.
</p>
</td></tr>
<tr><td><code>non.slopes</code></td>
<td>
<p>number of intercepts in model</p>
</td></tr>
<tr><td><code>interceptRef</code></td>
<td>
<p>the index of the middle (median) intercept used in
computing the linear predictor and <code>var</code></p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>the linear predictor using the first intercept</p>
</td></tr>
<tr><td><code>penalty.matrix</code></td>
<td>
<p>see above</p>
</td></tr>
<tr><td><code>info.matrix</code></td>
<td>
<p>see <code><a href="#topic+orm">orm</a></code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+orm">orm</a></code>, <code><a href="#topic+lrm">lrm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="#topic+gIndex">gIndex</a></code>, <code><a href="SparseM.html#topic+SparseM.solve">solve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Fit an additive logistic model containing numeric predictors age, 
#blood.pressure, and sex, assumed to be already properly coded and 
#transformed
#
# fit &lt;- orm.fit(cbind(age,blood.pressure,sex), death)
</code></pre>

<hr>
<h2 id='pentrace'>
Trace AIC and BIC vs. Penalty
</h2><span id='topic+pentrace'></span><span id='topic+plot.pentrace'></span><span id='topic+print.pentrace'></span><span id='topic+effective.df'></span>

<h3>Description</h3>

<p>For an ordinary unpenalized fit from <code>lrm</code> or <code>ols</code> and for a vector or list of penalties, 
fits a series of logistic or linear models using penalized maximum likelihood
estimation, and saves the effective degrees of freedom, Akaike Information
Criterion (<code class="reqn">AIC</code>), Schwarz Bayesian Information Criterion (<code class="reqn">BIC</code>), and
Hurvich and Tsai's corrected <code class="reqn">AIC</code> (<code class="reqn">AIC_c</code>).  Optionally
<code>pentrace</code> can 
use the <code>nlminb</code> function to solve for the optimum penalty factor or
combination of factors penalizing different kinds of terms in the model.
The <code>effective.df</code> function prints the original and effective
degrees of freedom for a penalized fit or for an unpenalized fit and
the best penalization determined from a previous invocation of
<code>pentrace</code> if <code>method="grid"</code> (the default).
The effective d.f. is computed separately for each class of terms in
the model (e.g., interaction, nonlinear).
A <code>plot</code> method exists to plot the results, and a <code>print</code> method exists
to print the most pertinent components.  Both <code class="reqn">AIC</code> and <code class="reqn">BIC</code>
may be plotted if 
there is only one penalty factor type specified in <code>penalty</code>.  Otherwise,
the first two types of penalty factors are plotted, showing only the <code class="reqn">AIC</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pentrace(fit, penalty, penalty.matrix, 
         method=c('grid','optimize'),
         which=c('aic.c','aic','bic'), target.df=NULL,
         fitter, pr=FALSE, tol=1e-7,
         keep.coef=FALSE, complex.more=TRUE, verbose=FALSE, maxit=12,
         subset, noaddzero=FALSE)

effective.df(fit, object)

## S3 method for class 'pentrace'
print(x, ...)

## S3 method for class 'pentrace'
plot(x, method=c('points','image'), 
     which=c('effective.df','aic','aic.c','bic'), pch=2, add=FALSE, 
     ylim, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pentrace_+3A_fit">fit</code></td>
<td>

<p>a result from <code>lrm</code> or <code>ols</code> with <code>x=TRUE, y=TRUE</code> and without using <code>penalty</code> or
<code>penalty.matrix</code>
(or optionally using penalization in the case of <code>effective.df</code>)
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_penalty">penalty</code></td>
<td>

<p>can be a vector or a list.  If it is a vector, all types of terms in
the model will be penalized by the same amount, specified by elements in
<code>penalty</code>, with a penalty of zero automatically added.  <code>penalty</code> can
also be a list in the format documented in the <code>lrm</code> function, except that
elements of the list can be vectors.  The <code>expand.grid</code> function is
invoked by <code>pentrace</code> to generate all possible combinations of
penalties.  For example, specifying 
<code>penalty=list(simple=1:2, nonlinear=1:3)</code> will generate 6 combinations
to try, so that the analyst can attempt to determine whether penalizing
more complex terms in the model more than the linear or categorical
variable terms will be beneficial.  If <code>complex.more=TRUE</code>, it is assumed
that the variables given in <code>penalty</code> are listed in order from less
complex to more complex.  With <code>method="optimize"</code> <code>penalty</code> specifies
an initial guess for the penalty or penalties.  If all term types are
to be equally penalized, <code>penalty</code> should be a single number,
otherwise it should be a list containing single numbers as elements,
e.g., <code>penalty=list(simple=1, nonlinear=2)</code>.  Experience has shown that the optimization algorithm is more likely to find a reasonable solution when the starting value specified in <code>penalty</code> is too large rather than too small.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_object">object</code></td>
<td>

<p>an object returned by <code>pentrace</code>.  For <code>effective.df</code>, <code>object</code> can be
omitted if the <code>fit</code> was penalized.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_penalty.matrix">penalty.matrix</code></td>
<td>

<p>see <code>lrm</code>
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_method">method</code></td>
<td>

<p>The default is <code>method="grid"</code> to print various indexes for all
combinations of penalty parameters given by the user.  Specify
<code>method="optimize"</code> to have <code>pentrace</code> use <code>nlminb</code> to solve for the
combination of penalty parameters that gives the maximum value of the
objective named in <code>which</code>, or, if <code>target.df</code> is given, to find the
combination that yields <code>target.df</code> effective total degrees of freedom
for the model.  When <code>target.df</code> is specified, <code>method</code> is set to
<code>"optimize"</code> automatically.
For <code>plot.pentrace</code> this parameter applies only if more than one
penalty term-type was used.  The default is to use open triangles
whose sizes are proportional to the ranks of the AICs, plotting the
first two penalty factors respectively on the x and y  axes.  Use
<code>method="image"</code> to plot an image plot. 
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_which">which</code></td>
<td>

<p>the objective to maximize for either <code>method</code>.  Default is <code>"aic.c"</code> (corrected
AIC).
For <code>plot.pentrace</code>, <code>which</code> is a vector of names of criteria to show;
default is to plot all 4 types, with effective d.f. in its own separate plot
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_target.df">target.df</code></td>
<td>

<p>applies only to <code>method="optimize"</code>.  See <code>method</code>.  <code>target.df</code> makes
sense mainly when a single type of penalty factor is specified.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_fitter">fitter</code></td>
<td>

<p>a fitting function.  Default is <code>lrm.fit</code> (<code>lm.pfit</code> is always used for <code>ols</code>).
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_pr">pr</code></td>
<td>

<p>set to <code>TRUE</code> to print intermediate results
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_tol">tol</code></td>
<td>

<p>tolerance for declaring a matrix singular (see <code>lrm.fit, solvet</code>)
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_keep.coef">keep.coef</code></td>
<td>

<p>set to <code>TRUE</code> to store matrix of regression  coefficients for all the fits (corresponding
to increasing values of <code>penalty</code>) in object <code>Coefficients</code> in the
returned list.  Rows correspond to penalties, columns to regression
parameters.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_complex.more">complex.more</code></td>
<td>

<p>By default if <code>penalty</code> is a list, combinations of penalties for which
complex terms are penalized less than less complex terms will be
dropped after <code>expand.grid</code> is invoked.  Set <code>complex.more=FALSE</code> to
allow more complex terms to be penalized less.  Currently this option
is ignored for <code>method="optimize"</code>.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_verbose">verbose</code></td>
<td>
<p>set to <code>TRUE</code> to print number of intercepts and sum
of effective degrees of freedom</p>
</td></tr>
<tr><td><code id="pentrace_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations to allow in a model fit (default=12).
This is passed to the appropriate fitter function with the correct
argument name.  Increase <code>maxit</code> if you had to when fitting the
original unpenalized model.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_subset">subset</code></td>
<td>

<p>a logical or integer vector specifying rows of the design and response
matrices to subset in fitting models.  This is most useful for
bootstrapping <code>pentrace</code> to see if the best penalty can be estimated
with little error so that variation due to selecting the optimal
penalty can be safely ignored when bootstrapping standard errors of regression
coefficients and measures of predictive accuracy.  See an example below.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_noaddzero">noaddzero</code></td>
<td>
<p>set to <code>TRUE</code> to not add an unpenalized model to
the list of models to fit</p>
</td></tr>
<tr><td><code id="pentrace_+3A_x">x</code></td>
<td>
<p>a result from <code>pentrace</code></p>
</td></tr>
<tr><td><code id="pentrace_+3A_pch">pch</code></td>
<td>
<p>used for <code>method="points"</code></p>
</td></tr>
<tr><td><code id="pentrace_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add to an existing plot.  In that case, the effective
d.f. plot is not re-drawn, but the AIC/BIC plot is added to.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_ylim">ylim</code></td>
<td>

<p>2-vector of y-axis limits for plots other than effective d.f.
</p>
</td></tr>
<tr><td><code id="pentrace_+3A_...">...</code></td>
<td>

<p>other arguments passed to <code>plot</code>, <code>lines</code>, or <code>image</code>
</p>
</td></tr></table>


<h3>Value</h3>

<p>a list of class <code>"pentrace"</code>
with elements <code>penalty, df, objective, fit, var.adj, diag, results.all</code>, and
optionally <code>Coefficients</code>.
The first 6 elements correspond to the fit that had the best objective
as named in the <code>which</code> argument, from the sequence of fits tried.
Here <code>fit</code> is the fit object from <code>fitter</code> which was a penalized fit,
<code>diag</code> is the diagonal of the matrix used to compute the effective
d.f., and <code>var.adj</code> is Gray (1992) Equation 2.9, which is an improved
covariance matrix for the penalized beta. <code>results.all</code> is a data
frame whose first few variables are the components of <code>penalty</code> and
whose other columns are <code>df, aic, bic, aic.c</code>.  <code>results.all</code> thus
contains a summary of results for all fits attempted.  When
<code>method="optimize"</code>, only two components are returned: <code>penalty</code> and
<code>objective</code>, and the object does not have a class.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Gray RJ: Flexible methods for analyzing survival data using splines,
with applications to breast cancer prognosis.  JASA 87:942&ndash;951, 1992.
</p>
<p>Hurvich CM, Tsai, CL: Regression and time series model selection in small
samples.  Biometrika 76:297&ndash;307, 1989.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+ols">ols</a></code>, <code><a href="Hmisc.html#topic+solvet">solvet</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="graphics.html#topic+image">image</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


f &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
         x=TRUE, y=TRUE)
p &lt;- pentrace(f, seq(.2,1,by=.05))
plot(p)
p$diag      # may learn something about fractional effective d.f. 
            # for each original parameter
pentrace(f, list(simple=c(0,.2,.4), nonlinear=c(0,.2,.4,.8,1)))


# Bootstrap pentrace 5 times, making a plot of corrected AIC plot with 5 reps
n &lt;- nrow(f$x)
plot(pentrace(f, seq(.2,1,by=.05)), which='aic.c', 
     col=1, ylim=c(30,120)) #original in black
for(j in 1:5)
  plot(pentrace(f, seq(.2,1,by=.05), subset=sample(n,n,TRUE)), 
       which='aic.c', col=j+1, add=TRUE)


# Find penalty giving optimum corrected AIC.  Initial guess is 1.0
# Not implemented yet
# pentrace(f, 1, method='optimize')


# Find penalty reducing total regression d.f. effectively to 5
# pentrace(f, 1, target.df=5)


# Re-fit with penalty giving best aic.c without differential penalization
f &lt;- update(f, penalty=p$penalty)
effective.df(f)
</code></pre>

<hr>
<h2 id='plot.contrast.rms'>plot.contrast.rms</h2><span id='topic+plot.contrast.rms'></span>

<h3>Description</h3>

<p>Plot Bayesian Contrast Posterior Densities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'contrast.rms'
plot(
  x,
  bivar = FALSE,
  bivarmethod = c("ellipse", "kernel"),
  prob = 0.95,
  which = c("both", "diff", "ind"),
  nrow = NULL,
  ncol = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.contrast.rms_+3A_x">x</code></td>
<td>
<p>the result of <code>contrast.rms</code></p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_bivar">bivar</code></td>
<td>
<p>set to <code>TRUE</code> to plot 2-d posterior density contour</p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_bivarmethod">bivarmethod</code></td>
<td>
<p>see <code><a href="rmsb.html#topic+pdensityContour">rmsb::pdensityContour()</a></code></p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_prob">prob</code></td>
<td>
<p>posterior coverage probability for HPD interval or 2-d contour</p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_which">which</code></td>
<td>
<p>applies when plotting the result of <code>contrast(..., fun=)</code>, defaulting to showing the posterior density of both estimates plus their difference.  Set to <code>"ind"</code> to only show the two individual densities or <code>"diff"</code> to only show the posterior density for the differences.</p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_nrow">nrow</code></td>
<td>
<p>for <code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code></p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_ncol">ncol</code></td>
<td>
<p>likewise</p>
</td></tr>
<tr><td><code id="plot.contrast.rms_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If there are exactly two contrasts and <code>bivar=TRUE</code> plots an elliptical or kernal (based on <code>bivarmethod</code> posterior density contour with probability <code>prob</code>).  Otherwise plots a series of posterior densities of contrasts along with HPD intervals, posterior means, and medians.  When the result being plotted comes from <code>contrast</code> with <code style="white-space: pre;">&#8288;fun=&#8288;</code> specified, both the two individual estimates and their difference are plotted.
</p>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='plot.Predict'>Plot Effects of Variables Estimated by a Regression Model Fit</h2><span id='topic+plot.Predict'></span><span id='topic+pantext'></span>

<h3>Description</h3>

<p>Uses <code>lattice</code> graphics to plot the effect of one or two predictors
on the linear predictor or X beta scale, or on some transformation of
that scale.  The first argument specifies the result of the
<code>Predict</code> function.  The predictor is always plotted in its
original coding.  <code>plot.Predict</code> uses the
<code>xYplot</code> function unless <code>formula</code> is omitted and the x-axis
variable is a factor, in which case it reverses the x- and y-axes and
uses the <code>Dotplot</code> function.
</p>
<p>If <code>data</code> is given, a rug plot is drawn showing
the location/density of data values for the <code class="reqn">x</code>-axis variable.  If
there is a <code>groups</code> (superposition) variable that generated separate
curves, the data density specific to each class of points is shown.
This assumes that the second variable was a factor variable.  The rug plots
are drawn by <code>scat1d</code>.  When the same predictor is used on all
<code class="reqn">x</code>-axes, and multiple panels are drawn, you can use
<code>subdata</code> to specify an expression to subset according to other
criteria in addition.
</p>
<p>To plot effects instead of estimates (e.g., treatment differences as a
function of interacting factors) see <code>contrast.rms</code> and
<code>summary.rms</code>.
</p>
<p><code>pantext</code> creates a <code>lattice</code> panel function for including
text such as that produced by <code>print.anova.rms</code> inside a panel or
in a base graphic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Predict'
plot(x, formula, groups=NULL,
     cond=NULL, varypred=FALSE, subset,
     xlim, ylim, xlab, ylab, 
     data=NULL, subdata, anova=NULL, pval=FALSE, cex.anova=.85,
     col.fill=gray(seq(.825, .55, length=5)),
     adj.subtitle, cex.adj, cex.axis, perim=NULL, digits=4, nlevels=3,
     nlines=FALSE, addpanel, scat1d.opts=list(frac=0.025, lwd=0.3),
     type=NULL, yscale=NULL, scaletrans=function(z) z, ...)

pantext(object, x, y, cex=.5, adj=0, fontfamily="Courier", lattice=TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.Predict_+3A_x">x</code></td>
<td>
<p>a data frame created by <code>Predict</code>, or for <code>pantext</code>
the x-coordinate for text</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_formula">formula</code></td>
<td>

<p>the right hand side of a <code>lattice</code> formula reference variables in
data frame <code>x</code>.  You may not specify <code>formula</code> if you varied
multiple predictors separately when calling <code>Predict</code>.
Otherwise, when <code>formula</code> is not given, <code>plot.Predict</code>
constructs one from information in <code>x</code>.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_groups">groups</code></td>
<td>
<p>an optional name of one of the variables in <code>x</code> that
is to be used as a grouping (superpositioning) variable.  Note that
<code>groups</code> does not contain the groups data as is customary in
<code>lattice</code>; it is only a single character string specifying the
name of the grouping variable.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_cond">cond</code></td>
<td>
<p>when plotting effects of different predictors, <code>cond</code>
is a character string that specifies a single variable name in
<code>x</code> that can be used to form panels.  Only applies if using
<code>rbind</code> to combine several <code>Predict</code> results.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_varypred">varypred</code></td>
<td>
<p>set to <code>TRUE</code> if <code>x</code> is the result of
passing multiple <code>Predict</code> results, that represent different
predictors, to <code>rbind.Predict</code>.  This will cause the <code>.set.</code>
variable created by <code>rbind</code> to be copied to the
<code>.predictor.</code> variable.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_subset">subset</code></td>
<td>
<p>a subsetting expression for restricting the rows of
<code>x</code> that are used in plotting.  For example, predictions may have
been requested for males and females but one wants to plot only females.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_xlim">xlim</code></td>
<td>

<p>This parameter is seldom used, as limits are usually controlled with
<code>Predict</code>.  One reason to use <code>xlim</code> is to plot a
<code>factor</code> variable on the x-axis that was created with the <code>cut2</code> function
with the <code>levels.mean</code> option, with <code>val.lev=TRUE</code> specified to <code>plot.Predict</code>. 
In this case you may want the axis to
have the range of the original variable values given to <code>cut2</code> rather
than the range of the means within quantile groups.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_ylim">ylim</code></td>
<td>

<p>Range for plotting on response variable axis. Computed by default.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_xlab">xlab</code></td>
<td>

<p>Label for <code>x</code>-axis. Default is one given to <code>asis, rcs</code>, etc.,
which may have been the <code>"label"</code> attribute of the variable.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_ylab">ylab</code></td>
<td>

<p>Label for <code>y</code>-axis.  If <code>fun</code> is not given,
default is <code>"log Odds"</code> for
<code>lrm</code>, <code>"log Relative Hazard"</code> for <code>cph</code>, name of the response
variable for <code>ols</code>, <code>TRUE</code> or <code>log(TRUE)</code> for <code>psm</code>, or <code>"X * Beta"</code> otherwise.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_data">data</code></td>
<td>
<p>a data frame containing the original raw data on which the
regression model were based, or at least containing the <code class="reqn">x</code>-axis
and grouping variable.  If <code>data</code> is present and contains the
needed variables, the original data are added to the graph in the form
of a rug plot using <code>scat1d</code>.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_subdata">subdata</code></td>
<td>
<p>if <code>data</code> is specified, an expression to be
evaluated in the <code>data</code> environment that evaluates to a logical
vector specifying which observations in <code>data</code> to keep.  This
will be intersected with the criterion for the <code>groups</code>
variable.  Example: if conditioning on two paneling variables using
<code>|a*b</code> you can specify
<code>subdata=b==levels(b)[which.packet()[2]]</code>, where the <code>2</code>
comes from the fact that <code>b</code> was listed second after the
vertical bar (this assumes <code>b</code> is a <code>factor</code> in
<code>data</code>.  Another example:
<code>subdata=sex==c('male','female')[current.row()]</code>.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_anova">anova</code></td>
<td>
<p>an object returned by <code><a href="#topic+anova.rms">anova.rms</a></code>.  If
<code>anova</code> is specified, the overall test of association for
predictor plotted is added as text to each panel, located at the spot
at which the panel is most empty unless there is significant empty
space at the top or bottom of the panel; these areas are given preference.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_pval">pval</code></td>
<td>
<p>specify <code>pval=TRUE</code> for <code>anova</code> to include not
only the test statistic but also the P-value</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_cex.anova">cex.anova</code></td>
<td>
<p>character size for the test statistic printed on the panel</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_col.fill">col.fill</code></td>
<td>

<p>a vector of colors used to fill confidence bands for successive
superposed groups.  Default is inceasingly dark gray scale.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_adj.subtitle">adj.subtitle</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress subtitling the graph with the list of
settings of non-graphed adjustment values.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_cex.adj">cex.adj</code></td>
<td>

<p><code>cex</code> parameter for size of adjustment settings in subtitles.  Default is
0.75 times <code>par("cex")</code>.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_cex.axis">cex.axis</code></td>
<td>

<p><code>cex</code> parameter for x-axis tick labels
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_perim">perim</code></td>
<td>

<p><code>perim</code> specifies a function having two
arguments.  The first is the vector of values of the first variable that
is about to be plotted on the x-axis.  The second argument is the single
value of the variable representing different curves, for the current
curve being plotted.  The function's returned value must be a logical
vector whose length is the same as that of the first argument, with
values <code>TRUE</code> if the corresponding point should be plotted for the
current curve, <code>FALSE</code> otherwise.  See one of the latter examples.
If a predictor is not specified to <code>plot</code>, <code>NULL</code> is passed as
the second argument to <code>perim</code>, although it makes little sense to
use <code>perim</code> when the same <code>perim</code> is used for multiple predictors.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_digits">digits</code></td>
<td>

<p>Controls how numeric variables used for panel labels are formatted. The
default is 4 significant digits.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_nlevels">nlevels</code></td>
<td>

<p>when <code>groups</code> and <code>formula</code> are not specified, if any panel
variable has <code>nlevels</code> or fewer values, that variable is
converted to a <code>groups</code> (superpositioning) variable.  Set
<code>nlevels=0</code> to prevent this behavior.  For other situations, a
numeric x-axis variable with <code>nlevels</code> or fewer unique values
will cause a dot plot to be drawn instead of an x-y plot.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_nlines">nlines</code></td>
<td>
<p>If <code>formula</code> is given, you can set <code>nlines</code> to
<code>TRUE</code> to convert the x-axis variable to a factor and then to an
integer.  Points are plotted at integer values on the x-axis but
labeled with category levels.  Points are connected by lines.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_addpanel">addpanel</code></td>
<td>
<p>an additional panel function to call along with panel
functions used for <code>xYplot</code> and <code>Dotplot</code> displays</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_scat1d.opts">scat1d.opts</code></td>
<td>
<p>a list containing named elements that specifies
parameters to <code><a href="Hmisc.html#topic+scat1d">scat1d</a></code> when <code>data</code> is given.  The
<code>col</code> parameter is usually derived from other plotting
information and not specified by the user.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_type">type</code></td>
<td>
<p>a value (<code>"l","p","b"</code>) to override default choices
related to showing or connecting points.  Especially  useful for
discrete x coordinate variables.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_yscale">yscale</code></td>
<td>
<p>a <code>lattice</code> scale <code>list</code> for the <code>y</code>-axis
to be added to what is automatically generated for the <code>x</code>-axis.
Example:
<code>yscale=list(at=c(.005,.01,.05),labels=format(c(.005,.01,.05)))</code>.
See <a href="lattice.html#topic+xyplot">xyplot</a></p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_scaletrans">scaletrans</code></td>
<td>
<p>a function that operates on the <code>scale</code> object
created by <code>plot.Predict</code> to produce a modified <code>scale</code>
object that is passed to the lattice graphics function.  This is
useful for adding other <code>scales</code> options or for changing the
<code>x</code>-axis limits for one predictor.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_...">...</code></td>
<td>

<p>extra arguments to pass to <code>xYplot</code> or <code>Dotplot</code>.  Some
useful ones are <code>label.curves</code> and <code>abline</code>.
Set <code>label.curves</code> to <code>FALSE</code> to suppress labeling of
separate curves. Default is <code>TRUE</code>, which
causes <code>labcurve</code> to be invoked to place labels at positions where the
curves are most separated, labeling each curve with the full curve label.
Set <code>label.curves</code> to a <code>list</code> to specify options to
<code>labcurve</code>, e.g., <code>label.curves=</code> <code>list(method="arrow",
	cex=.8)</code>. 
These option names may be abbreviated in the usual way arguments
are abbreviated.  Use for example <code>label.curves=list(keys=letters[1:5])</code>
to draw single lower case letters on 5 curves where they are most
separated, and automatically position a legend
in the most empty part of the plot.  The <code>col</code>, <code>lty</code>, and
<code>lwd</code> parameters are passed automatically to <code>labcurve</code>
although they may be overridden here.
It is also useful to use ... to pass <code>lattice</code> graphics parameters, e.g.
<code>par.settings=list(axis.text=list(cex=1.2), par.ylab.text=list(col='blue',cex=.9),par.xlab.text=list(cex=1))</code>.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_object">object</code></td>
<td>
<p>an object having a <code>print</code> method</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_y">y</code></td>
<td>
<p>y-coordinate for placing text in a <code>lattice</code> panel
or on a base graphics plot</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_cex">cex</code></td>
<td>
<p>character expansion size for <code>pantext</code></p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_adj">adj</code></td>
<td>
<p>text justification.  Default is left justified.</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_fontfamily">fontfamily</code></td>
<td>

<p>font family for <code>pantext</code>.  Default is <code>"Courier"</code> which
will line up columns of a table.
</p>
</td></tr>
<tr><td><code id="plot.Predict_+3A_lattice">lattice</code></td>
<td>
<p>set to <code>FALSE</code> to use <code>text</code> instead of
<code>ltext</code> in the function generated by <code>pantext</code>, to use base
graphics</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When a <code>groups</code> (superpositioning) variable was used, you can issue
the command <code>Key(...)</code> after printing the result of
<code>plot.Predict</code>, to draw a key for the groups.
</p>


<h3>Value</h3>

<p>a <code>lattice</code> object ready to <code>print</code> for rendering.
</p>


<h3>Note</h3>

<p>If plotting the effects of all predictors you can reorder the
panels using for example <code>p &lt;- Predict(fit); p$.predictor. &lt;-
	factor(p$.predictor., v)</code> where <code>v</code> is a vector of predictor
names specified in the desired order.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Fox J, Hong J (2009): Effect displays in R for multinomial and
proportional-odds logit models: Extensions to the effects package.  J
Stat Software 32 No. 1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code>link{plotp.Predict}</code>, <code><a href="#topic+rbind.Predict">rbind.Predict</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, 
<code><a href="Hmisc.html#topic+labcurve">labcurve</a></code>, <code><a href="Hmisc.html#topic+scat1d">scat1d</a></code>,
<code><a href="Hmisc.html#topic+xYplot">xYplot</a></code>, <code><a href="Hmisc.html#topic+Overview">Overview</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
               x=TRUE, y=TRUE)
an &lt;- anova(fit)
# Plot effects of all 4 predictors with test statistics from anova, and P
plot(Predict(fit), anova=an, pval=TRUE)
plot(Predict(fit), data=llist(blood.pressure,age))
                         # rug plot for two of the predictors

p &lt;- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
plot(p)

p &lt;- Predict(fit, age=seq(20,80,length=100), sex, conf.int=FALSE)
                         # Plot relationship between age and log
                         # odds, separate curve for each sex,
plot(p, subset=sex=='female' | age &gt; 30)
# No confidence interval, suppress estimates for males &lt;= 30

p &lt;- Predict(fit, age, sex)
plot(p, label.curves=FALSE, data=llist(age,sex))
                         # use label.curves=list(keys=c('a','b'))'
                         # to use 1-letter abbreviations
                         # data= allows rug plots (1-dimensional scatterplots)
                         # on each sex's curve, with sex-
                         # specific density of age
                         # If data were in data frame could have used that
p &lt;- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
                         # works if datadist not used
plot(p, ylab=expression(hat(P)))
                         # plot predicted probability in place of log odds

per &lt;- function(x, y) x &gt;= 30
plot(p, perim=per)       # suppress output for age &lt; 30 but leave scale alone

# Take charge of the plot setup by specifying a lattice formula
p &lt;- Predict(fit, age, blood.pressure=c(120,140,160),
             cholesterol=c(180,200,215), sex)
plot(p, ~ age | blood.pressure*cholesterol, subset=sex=='male')
# plot(p, ~ age | cholesterol*blood.pressure, subset=sex=='female')
# plot(p, ~ blood.pressure|cholesterol*round(age,-1), subset=sex=='male')
plot(p)

# Plot the age effect as an odds ratio
# comparing the age shown on the x-axis to age=30 years

ddist$limits$age[2] &lt;- 30    # make 30 the reference value for age
# Could also do: ddist$limits["Adjust to","age"] &lt;- 30
fit &lt;- update(fit)   # make new reference value take effect
p &lt;- Predict(fit, age, ref.zero=TRUE, fun=exp)
plot(p, ylab='Age=x:Age=30 Odds Ratio',
     abline=list(list(h=1, lty=2, col=2), list(v=30, lty=2, col=2)))

# Compute predictions for three predictors, with superpositioning or
# conditioning on sex, combined into one graph

p1 &lt;- Predict(fit, age, sex)
p2 &lt;- Predict(fit, cholesterol, sex)
p3 &lt;- Predict(fit, blood.pressure, sex)
p &lt;- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
plot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE)
plot(p, cond='sex', varypred=TRUE, adj.subtitle=FALSE)

## Not run: 
# For males at the median blood pressure and cholesterol, plot 3 types
# of confidence intervals for the probability on one plot, for varying age
ages &lt;- seq(20, 80, length=100)
p1 &lt;- Predict(fit, age=ages, sex='male', fun=plogis)  # standard pointwise
p2 &lt;- Predict(fit, age=ages, sex='male', fun=plogis,
              conf.type='simultaneous')               # simultaneous
p3 &lt;- Predict(fit, age=c(60,65,70), sex='male', fun=plogis,
              conf.type='simultaneous')               # simultaneous 3 pts
# The previous only adjusts for a multiplicity of 3 points instead of 100
f &lt;- update(fit, x=TRUE, y=TRUE)
g &lt;- bootcov(f, B=500, coef.reps=TRUE)
p4 &lt;- Predict(g, age=ages, sex='male', fun=plogis)    # bootstrap percentile
p &lt;- rbind(Pointwise=p1, 'Simultaneous 100 ages'=p2,
           'Simultaneous     3 ages'=p3, 'Bootstrap nonparametric'=p4)
xYplot(Cbind(yhat, lower, upper) ~ age, groups=.set.,
       data=p, type='l', method='bands', label.curve=list(keys='lines'))

## End(Not run)

# Plots for a parametric survival model
require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, 
              rep=TRUE, prob=c(.6, .4)))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
t &lt;- -log(runif(n))/h
label(t) &lt;- 'Follow-up Time'
e &lt;- ifelse(t&lt;=cens,1,0)
t &lt;- pmin(t, cens)
units(t) &lt;- "Year"
ddist &lt;- datadist(age, sex)
Srv &lt;- Surv(t,e)


# Fit log-normal survival model and plot median survival time vs. age
f &lt;- psm(Srv ~ rcs(age), dist='lognormal')
med &lt;- Quantile(f)       # Creates function to compute quantiles
                         # (median by default)
p &lt;- Predict(f, age, fun=function(x) med(lp=x))
plot(p, ylab="Median Survival Time")
# Note: confidence intervals from this method are approximate since
# they don't take into account estimation of scale parameter


# Fit an ols model to log(y) and plot the relationship between x1
# and the predicted mean(y) on the original scale without assuming
# normality of residuals; use the smearing estimator
# See help file for rbind.Predict for a method of showing two
# types of confidence intervals simultaneously.
set.seed(1)
x1 &lt;- runif(300)
x2 &lt;- runif(300)
ddist &lt;- datadist(x1,x2)
y  &lt;- exp(x1+x2-1+rnorm(300))
f &lt;- ols(log(y) ~ pol(x1,2)+x2)
r &lt;- resid(f)
smean &lt;- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
formals(smean) &lt;- list(yhat=numeric(0), res=r[!is.na(r)])
#smean$res &lt;- r[!is.na(r)]   # define default res argument to function
plot(Predict(f, x1, fun=smean), ylab='Predicted Mean on y-scale')

# Make an 'interaction plot', forcing the x-axis variable to be
# plotted at integer values but labeled with category levels
n &lt;- 100
set.seed(1)
gender &lt;- c(rep('male', n), rep('female',n))
m &lt;- sample(c('a','b'), 2*n, TRUE)
d &lt;-  datadist(gender, m); options(datadist='d')
anxiety &lt;- runif(2*n) + .2*(gender=='female') + .4*(gender=='female' &amp; m=='b')
tapply(anxiety, llist(gender,m), mean)
f &lt;- ols(anxiety ~ gender*m)
p &lt;- Predict(f, gender, m)
plot(p)     # horizontal dot chart; usually preferred for categorical predictors
Key(.5, .5)
plot(p, ~gender, groups='m', nlines=TRUE)
plot(p, ~m, groups='gender', nlines=TRUE)
plot(p, ~gender|m, nlines=TRUE)

options(datadist=NULL)

## Not run: 
# Example in which separate curves are shown for 4 income values
# For each curve the estimated percentage of voters voting for
# the democratic party is plotted against the percent of voters
# who graduated from college.  Data are county-level percents.

incomes &lt;- seq(22900, 32800, length=4)  
# equally spaced to outer quintiles
p &lt;- Predict(f, college, income=incomes, conf.int=FALSE)
plot(p, xlim=c(0,35), ylim=c(30,55))

# Erase end portions of each curve where there are fewer than 10 counties having
# percent of college graduates to the left of the x-coordinate being plotted,
# for the subset of counties having median family income with 1650
# of the target income for the curve

show.pts &lt;- function(college.pts, income.pt) {
  s &lt;- abs(income - income.pt) &lt; 1650  #assumes income known to top frame
  x &lt;- college[s]
  x &lt;- sort(x[!is.na(x)])
  n &lt;- length(x)
  low &lt;- x[10]; high &lt;- x[n-9]
  college.pts &gt;= low &amp; college.pts &lt;= high
}

plot(p, xlim=c(0,35), ylim=c(30,55), perim=show.pts)

# Rename variables for better plotting of a long list of predictors
f &lt;- ...
p &lt;- Predict(f)
re &lt;- c(trt='treatment', diabet='diabetes', sbp='systolic blood pressure')

for(n in names(re)) {
  names(p)[names(p)==n] &lt;- re[n]
  p$.predictor.[p$.predictor.==n] &lt;- re[n]
  }
plot(p)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.rexVar'>plot.rexVar</h2><span id='topic+plot.rexVar'></span>

<h3>Description</h3>

<p>Plot rexVar Result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rexVar'
plot(
  x,
  xlab = "Relative Explained Variation",
  xlim = NULL,
  pch = 16,
  sort = c("descending", "ascending", "none"),
  margin = FALSE,
  height = NULL,
  width = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.rexVar_+3A_x">x</code></td>
<td>
<p>a vector or matrix created by <code>rexVar</code></p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits; defaults to range of all values (limits and point estimates)</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_pch">pch</code></td>
<td>
<p>plotting symbol for dot</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_sort">sort</code></td>
<td>
<p>defaults to sorted predictors in descending order of relative explained variable.  Can set to <code>ascending</code> or <code>none</code>.</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_margin">margin</code></td>
<td>
<p>set to <code>TRUE</code> to show the REV values in the right margin if using base graphics</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_height">height</code></td>
<td>
<p>optional height in pixels for <code>plotly</code> graph</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_width">width</code></td>
<td>
<p>likewise optional width</p>
</td></tr>
<tr><td><code id="plot.rexVar_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>dotchart2</code> or <code>dotchartpl</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Makes a dot chart displaying the results of <code>rexVar</code>.  Base graphics are used unless <code>options(grType='plotly')</code> is in effect, in which case a <code>plotly</code> graphic is produced with hovertext
</p>


<h3>Value</h3>

<p><code>plotly</code> graphics object if using <code>plotly</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='plot.xmean.ordinaly'>
Plot Mean X vs. Ordinal Y
</h2><span id='topic+plot.xmean.ordinaly'></span>

<h3>Description</h3>

<p>Separately for each predictor variable <code class="reqn">X</code> in a formula, plots the mean of
<code class="reqn">X</code> vs. levels of <code class="reqn">Y</code>.  Then under the proportional odds assumption,
the expected value of the predictor for each <code class="reqn">Y</code> value is also plotted (as
a dotted line).  This plot is useful for assessing the ordinality assumption 
for <code class="reqn">Y</code> separately for each <code class="reqn">X</code>, and for assessing the proportional odds
assumption in a simple univariable way.  If several predictors do not
distinguish adjacent categories of <code class="reqn">Y</code>, those levels may need to be 
pooled.  This display assumes
that each predictor is linearly related to the log odds of each event in
the proportional odds model.  There is also an option to plot the
expected means assuming a forward continuation ratio model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xmean.ordinaly'
plot(x, data, subset, na.action, subn=TRUE,
                    cr=FALSE, topcats=1, cex.points=.75, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.xmean.ordinaly_+3A_x">x</code></td>
<td>

<p>an S formula.  Response variable is treated as ordinal.  For categorical
predictors, a binary version of the variable is substituted, specifying
whether or not the variable equals the modal category.  Interactions or
non-linear effects are not allowed.
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_data">data</code></td>
<td>

<p>a data frame or frame number
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_subset">subset</code></td>
<td>

<p>vector of subscripts or logical vector describing subset of data to
analyze
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_na.action">na.action</code></td>
<td>

<p>defaults to <code>na.keep</code> so all NAs are initially retained.  Then NAs
are deleted only for each predictor currently being plotted.
Specify <code>na.action=na.delete</code> to remove observations that are missing
on any of the predictors (or the response).
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_subn">subn</code></td>
<td>

<p>set to <code>FALSE</code> to suppress a left bottom subtitle specifying the sample size
used in constructing each plot
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_cr">cr</code></td>
<td>

<p>set to <code>TRUE</code> to plot expected values by levels of the response,
assuming a forward continuation ratio model holds.  The function is fairly slow
when this option is specified.
</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_topcats">topcats</code></td>
<td>
<p>When a predictor is categorical, by default only the
proportion of observations in the overall most frequent category will
be plotted against response variable strata.  Specify a higher value
of <code>topcats</code> to make separate plots for the proportion in the
<code>k</code> most frequent predictor categories, where <code>k</code> is
<code>min(ncat-1, topcats)</code> and <code>ncat</code> is the number of unique
values of the predictor.</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_cex.points">cex.points</code></td>
<td>
<p>if <code>cr</code> is <code>TRUE</code>, specifies the size of the
<code>"C"</code> that is plotted.  Default is 0.75.</p>
</td></tr>
<tr><td><code id="plot.xmean.ordinaly_+3A_...">...</code></td>
<td>

<p>other arguments passed to <code>plot</code> and <code>lines</code>
</p>
</td></tr></table>


<h3>Side Effects</h3>

<p>plots
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Harrell FE et al. (1998): Development of a clinical prediction model for
an ordinal outcome. Stat in Med 17:909&ndash;44.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+residuals.lrm">residuals.lrm</a></code>, <code><a href="#topic+cr.setup">cr.setup</a></code>,
<code><a href="Hmisc.html#topic+summary.formula">summary.formula</a></code>, <code><a href="Hmisc.html#topic+biVar">biVar</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from a population proportional odds model
set.seed(1)
n &lt;- 400
age &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
region &lt;- factor(sample(c('north','south','east','west'), n, replace=TRUE))
L &lt;- .2*(age-50) + .1*(blood.pressure-120)
p12 &lt;- plogis(L)    # Pr(Y&gt;=1)
p2  &lt;- plogis(L-1)  # Pr(Y=2)
p   &lt;- cbind(1-p12, p12-p2, p2)   # individual class probabilites
# Cumulative probabilities:
cp  &lt;- matrix(cumsum(t(p)) - rep(0:(n-1), rep(3,n)), byrow=TRUE, ncol=3)
y   &lt;- (cp &lt; runif(n)) %*% rep(1,3)
# Thanks to Dave Krantz &lt;dhk@paradox.psych.columbia.edu&gt; for this trick

par(mfrow=c(2,2))
plot.xmean.ordinaly(y ~ age + blood.pressure + region, cr=TRUE, topcats=2)
par(mfrow=c(1,1))
# Note that for unimportant predictors we don't care very much about the
# shapes of these plots.  Use the Hmisc chiSquare function to compute
# Pearson chi-square statistics to rank the variables by unadjusted
# importance without assuming any ordering of the response:
chiSquare(y ~ age + blood.pressure + region, g=3)
chiSquare(y ~ age + blood.pressure + region, g=5)
</code></pre>

<hr>
<h2 id='plotp.Predict'>Plot Effects of Variables Estimated by a Regression Model Fit
Using plotly</h2><span id='topic+plotp.Predict'></span>

<h3>Description</h3>

<p>Uses <code>plotly</code> graphics (without using ggplot2) to plot the effect
of one or two predictors 
on the linear predictor or X beta scale, or on some transformation of
that scale.  The first argument specifies the result of the
<code>Predict</code> function.  The predictor is always plotted in its
original coding.  Hover text shows point estimates, confidence
intervals, and on the leftmost x-point, adjustment variable settings.
</p>
<p>If <code>Predict</code> was run with no variable settings, so that each
predictor is varied one at a time, the result of <code>plotp.Predict</code>
is a list with two elements.  The first, named <code>Continuous</code>, is a
<code>plotly</code> object containing a single graphic with all the
continuous predictors varying.  The second, named <code>Categorical</code>,
is a <code>plotly</code> object containing a single graphic with all the
categorical predictors varying.  If there are no categorical
predictors, the value returned by by <code>plotp.Predict</code> is a single
<code>plotly</code> object and not a list of objects.
</p>
<p>If <code>rdata</code> is given, a spike histogram is drawn showing
the location/density of data values for the <code class="reqn">x</code>-axis variable.  If
there is a superposition variable that generated separate
curves, the data density specific to each class of points is shown.
The histograms are drawn by <code>histSpikeg</code>.
</p>
<p>To plot effects instead of estimates (e.g., treatment differences as a
function of interacting factors) see <code>contrast.rms</code> and
<code>summary.rms</code>.
</p>
<p>Unlike <code>ggplot.Predict</code>, <code>plotp.Predict</code> does not handle
<code>groups</code>, <code>anova</code>, or <code>perim</code> arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Predict'
plotp(data, subset, xlim, ylim, xlab, ylab, 
     rdata=NULL, nlevels=3, vnames=c('labels','names'),
     histSpike.opts=list(frac=function(f) 0.01 + 
         0.02 * sqrt(f - 1)/sqrt(max(f, 2) - 1), side=1, nint=100),
     ncols=3, width=800, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotp.Predict_+3A_data">data</code></td>
<td>
<p>a data frame created by <code>Predict</code></p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_subset">subset</code></td>
<td>
<p>a subsetting expression for restricting the rows of
<code>data</code> that are used in plotting.  For example, predictions may have
been requested for males and females but one wants to plot only females.</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_xlim">xlim</code></td>
<td>
<p>ignored unless predictors were specified to <code>Predict</code>.
Specifies the x-axis limits of the single plot produced.</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_ylim">ylim</code></td>
<td>

<p>Range for plotting on response variable axis. Computed by default and
includes the confidence limits.
</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_xlab">xlab</code></td>
<td>

<p>Label for <code>x</code>-axis when a single plot is made, i.e., when a
predictor is specified to <code>Predict</code>.  Default is one given to
<code>asis, rcs</code>, etc., which may have been the <code>"label"</code> attribute
of the variable. 
</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_ylab">ylab</code></td>
<td>

<p>Label for <code>y</code>-axis.  If <code>fun</code> is not given,
default is <code>"log Odds"</code> for
<code>lrm</code>, <code>"log Relative Hazard"</code> for <code>cph</code>, name of the response
variable for <code>ols</code>, <code>TRUE</code> or <code>log(TRUE)</code> for <code>psm</code>,
or <code>"X * Beta"</code> otherwise.  Specify <code>ylab=NULL</code> to omit
<code>y</code>-axis labels.
</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_rdata">rdata</code></td>
<td>
<p>a data frame containing the original raw data on which the
regression model were based, or at least containing the <code class="reqn">x</code>-axis
and grouping variable.  If <code>rdata</code> is present and contains the
needed variables, the original data are added to the graph in the form
of a spike histogram using <code>histSpikeg</code> in the Hmisc package.
</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_nlevels">nlevels</code></td>
<td>

<p>A non-numeric x-axis variable with <code>nlevels</code> or fewer unique values
will cause a horizontal dot plot to be drawn instead of an x-y plot.
</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_vnames">vnames</code></td>
<td>
<p>applies to the case where multiple plots are produced
separately by predictor.  Set to <code>'names'</code> to use variable names
instead of labels for these small plots.</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_histspike.opts">histSpike.opts</code></td>
<td>
<p>a list containing named elements that specifies
parameters to <code><a href="Hmisc.html#topic+scat1d">histSpikeg</a></code> when <code>rdata</code> is given.  The
<code>col</code> parameter is usually derived from other plotting
information and not specified by the user.</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_ncols">ncols</code></td>
<td>
<p>number of columns of plots to use when plotting multiple
continuous predictors</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_width">width</code></td>
<td>
<p>width in pixels for <code>plotly</code> graphics</p>
</td></tr>
<tr><td><code id="plotp.Predict_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>plotly</code> object or a list containing two elements, each
one a <code>plotly</code> object</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Fox J, Hong J (2009): Effect displays in R for multinomial and
proportional-odds logit models: Extensions to the effects package.  J
Stat Software 32 No. 1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+rbind.Predict">rbind.Predict</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>,
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+plot.Predict">plot.Predict</a></code>,
<code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code><a href="Hmisc.html#topic+histSpikeg">histSpikeg</a></code>,
<code><a href="Hmisc.html#topic+Overview">Overview</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 350     # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
    (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
    .01 * (blood.pressure - 120)
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
           x=TRUE, y=TRUE)

p &lt;- plotp(Predict(fit))
p$Continuous
p$Categorical
# When using Rmarkdown html notebook, best to use
# prList(p) to render the two objects
plotp(Predict(fit), rdata=llist(blood.pressure, age))$Continuous
# spike histogram plot for two of the predictors

p &lt;- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
plotp(p)

p &lt;- Predict(fit, age, sex)
plotp(p, rdata=llist(age,sex))
# rdata= allows rug plots (1-dimensional scatterplots)
# on each sex's curve, with sex-
# specific density of age
# If data were in data frame could have used that
p &lt;- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
# works if datadist not used
plotp(p, ylab='P')
# plot predicted probability in place of log odds

# Compute predictions for three predictors, with superpositioning or
# conditioning on sex, combined into one graph

p1 &lt;- Predict(fit, age, sex)
p2 &lt;- Predict(fit, cholesterol, sex)
p3 &lt;- Predict(fit, blood.pressure, sex)
p &lt;- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
plotp(p, ncols=2, rdata=llist(age, cholesterol, sex))

## End(Not run)
</code></pre>

<hr>
<h2 id='poma'>Examine proportional odds and parallelism assumptions of 'orm' and 'lrm' model fits.</h2><span id='topic+poma'></span>

<h3>Description</h3>

<p>Based on codes and strategies from Frank Harrell's canonical 'Regression Modeling Strategies' text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poma(mod.orm, cutval, minfreq = 15, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poma_+3A_mod.orm">mod.orm</code></td>
<td>
<p>Model fit of class 'orm' or 'lrm'. For 'fit.mult.impute' objects, 'poma' will refit model on a singly-imputed data-set</p>
</td></tr>
<tr><td><code id="poma_+3A_cutval">cutval</code></td>
<td>
<p>Numeric vector; sequence of observed values to cut outcome</p>
</td></tr>
<tr><td><code id="poma_+3A_minfreq">minfreq</code></td>
<td>
<p>Numeric vector; an 'impactPO' argument which specifies the minimum sample size to allow for the least frequent category of the dependent variable.</p>
</td></tr>
<tr><td><code id="poma_+3A_...">...</code></td>
<td>
<p>parameters to pass to 'impactPO' function such as 'newdata', 'nonpo', and 'B'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Strategy 1: Compare PO model fit with models that relax the PO assumption (for discrete response variable) <br />
Strategy 2: Apply different link functions to Prob of Binary Ys (defined by cutval). Regress transformed outcome on combined X and assess constancy of slopes (betas) across cut-points <br />
Strategy 3: Generate score residual plot for each predictor (for response variable with &lt;10 unique levels) <br />
Strategy 4: Assess parallelism of link function transformed inverse CDFs curves for different XBeta levels (for response variables with &gt;=10 unique levels)
</p>


<h3>Author(s)</h3>

<p>Yong Hao Pua &lt;puayonghao@gmail.com&gt;
</p>


<h3>See Also</h3>

<p>Harrell FE. *Regression Modeling Strategies: with applications to linear models,
logistic and ordinal regression, and survival analysis.* New York: Springer Science, LLC, 2015. <br />
Harrell FE. Statistical Thinking - Assessing the Proportional Odds Assumption and Its Impact. https://www.fharrell.com/post/impactpo/. Published March 9, 2022. Accessed January 13, 2023.
[rms::impactPO()] <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## orm model (response variable has fewer than 10 unique levels)
mod.orm &lt;- orm(carb ~ cyl + hp , x = TRUE, y = TRUE, data = mtcars)
poma(mod.orm)


## runs rms::impactPO when its args are supplied
## More examples: (https://yhpua.github.io/poma/)
d &lt;- expand.grid(hp = c(90, 180), vs = c(0, 1))
mod.orm &lt;- orm(cyl ~ vs + hp , x = TRUE, y = TRUE, data = mtcars)
poma(mod.orm, newdata = d)


## orm model (response variable has &gt;=10 unique levels)
mod.orm &lt;- orm(mpg ~ cyl + hp , x=TRUE, y=TRUE, data = mtcars)
poma(mod.orm)


## orm model using imputation
dat &lt;- mtcars
## introduce NAs
dat[sample(rownames(dat), 10), "cyl"] &lt;- NA
im &lt;- aregImpute(~ cyl + wt + mpg + am, data = dat)
aa &lt;- fit.mult.impute(mpg ~ cyl + wt , xtrans = im, data = dat, fitter = orm)
poma(aa)

## End(Not run)
</code></pre>

<hr>
<h2 id='pphsm'>Parametric Proportional Hazards form of AFT Models</h2><span id='topic+pphsm'></span><span id='topic+print.pphsm'></span><span id='topic+vcov.pphsm'></span>

<h3>Description</h3>

<p>Translates an accelerated failure time (AFT) model fitted by
<code>psm</code> to proportional hazards form, if the fitted model was
a Weibull or exponential model (extreme value distribution with
&quot;log&quot; link).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pphsm(fit)
## S3 method for class 'pphsm'
print(x, digits=max(options()$digits - 4, 3),
correlation=TRUE, ...)
## S3 method for class 'pphsm'
vcov(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pphsm_+3A_fit">fit</code></td>
<td>
<p>fit object created by <code>psm</code></p>
</td></tr>
<tr><td><code id="pphsm_+3A_x">x</code></td>
<td>
<p>result of <code>psm</code></p>
</td></tr>
<tr><td><code id="pphsm_+3A_digits">digits</code></td>
<td>
<p>how many significant digits are to be used for the
returned value</p>
</td></tr>
<tr><td><code id="pphsm_+3A_correlation">correlation</code></td>
<td>
<p>set to <code>FALSE</code> to suppress printing of
correlation matrix of parameter estimates</p>
</td></tr>
<tr><td><code id="pphsm_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="pphsm_+3A_object">object</code></td>
<td>
<p>a pphsm object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a new fit object with transformed parameter estimates
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psm">psm</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+print.pphsm">print.pphsm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
set.seed(1)
S &lt;- Surv(runif(100))
x &lt;- runif(100)
dd &lt;- datadist(x); options(datadist='dd')
f &lt;- psm(S ~ x, dist="exponential")
summary(f)        # effects on log(T) scale
f.ph &lt;- pphsm(f)
## Not run: summary(f.ph)     # effects on hazard ratio scale
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='predab.resample'>Predictive Ability using Resampling</h2><span id='topic+predab.resample'></span>

<h3>Description</h3>

<p><code>predab.resample</code> is a general-purpose
function that is used by functions for specific models.
It computes estimates of optimism of, and bias-corrected estimates of a vector
of indexes of predictive accuracy, for a model with a specified
design matrix, with or without fast backward step-down of predictors. If <code>bw=TRUE</code>, the design
matrix <code>x</code> must have been created by <code>ols</code>, <code>lrm</code>, or <code>cph</code>.
If <code>bw=TRUE</code>, <code>predab.resample</code> stores as the <code>kept</code>
attribute a logical matrix encoding which
factors were selected at each repetition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predab.resample(fit.orig, fit, measure, 
                method=c("boot","crossvalidation",".632","randomization"),
                bw=FALSE, B=50, pr=FALSE, prmodsel=TRUE,
                rule="aic", type="residual", sls=.05, aics=0,
                tol=1e-12, force=NULL, estimates=TRUE,
                non.slopes.in.x=TRUE, kint=1,
                cluster, subset, group=NULL,
                allow.varying.intercepts=FALSE, debug=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predab.resample_+3A_fit.orig">fit.orig</code></td>
<td>

<p>object containing the original full-sample fit, with the <code>x=TRUE</code> and
<code>y=TRUE</code> options specified to the model fitting function.  This model
should be the FULL model including all candidate variables ever excluded
because of poor associations with the response.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_fit">fit</code></td>
<td>

<p>a function to fit the model, either the original model fit, or a fit in a
sample.  fit has as arguments <code>x</code>,<code>y</code>, <code>iter</code>, <code>penalty</code>, <code>penalty.matrix</code>,
<code>xcol</code>, and other arguments passed to <code>predab.resample</code>. 
If you don't want <code>iter</code>
as an argument inside the definition of <code>fit</code>, add ... to the end of its
argument list. <code>iter</code> is passed to <code>fit</code> to inform the function of the
sampling repetition number (0=original sample).  If <code>bw=TRUE</code>, <code>fit</code> should
allow for the possibility of selecting no predictors, i.e., it should fit an
intercept-only model if the model has intercept(s). <code>fit</code> must return
objects <code>coef</code> and <code>fail</code> (<code>fail=TRUE</code> if <code>fit</code> failed due to singularity or
non-convergence - these cases are excluded from summary statistics). <code>fit</code>
must add design attributes to the returned object if <code>bw=TRUE</code>.  
The <code>penalty.matrix</code> parameter is not used if <code>penalty=0</code>.  The <code>xcol</code>
vector is a vector of columns of <code>X</code> to be used in the current model fit.
For <code>ols</code> and <code>psm</code> it includes a <code>1</code> for the intercept position.
<code>xcol</code> is not defined if <code>iter=0</code> unless the initial fit had been from
a backward step-down.  <code>xcol</code> is used to select the correct rows and columns
of <code>penalty.matrix</code> for the current variables selected, for example.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_measure">measure</code></td>
<td>

<p>a function to compute a vector of indexes of predictive accuracy for a given fit.
For <code>method=".632"</code> or <code>method="crossval"</code>, it will make the most sense for
measure to compute only indexes that are independent of sample size. The
measure function should take the following arguments or use ...: <code>xbeta</code> 
(X beta for
current fit), <code>y</code>, <code>evalfit</code>, <code>fit</code>, <code>iter</code>, and <code>fit.orig</code>. <code>iter</code> is as in <code>fit</code>.
<code>evalfit</code> is set to <code>TRUE</code>
by <code>predab.resample</code> if the fit is being evaluated on the sample used to make the
fit, <code>FALSE</code> otherwise; <code>fit.orig</code> is the fit object returned by the original fit on the whole
sample. Using <code>evalfit</code> will sometimes save computations. For example, in
bootstrapping the area under an ROC curve for a logistic regression model,
<code>lrm</code> already computes the area if the fit is on the training sample. 
<code>fit.orig</code>
is used to pass computed configuration parameters from the original fit such as
quantiles of predicted probabilities that are used as cut points in other samples.
The vector created by measure should have <code>names()</code> associated with it.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_method">method</code></td>
<td>

<p>The default is <code>"boot"</code> for ordinary bootstrapping (Efron, 1983,
Eq. 2.10).   Use <code>".632"</code> for Efron's <code>.632</code> method (Efron,
1983, Section 6 and Eq. 6.10), <code>"crossvalidation"</code> for grouped
cross&ndash;validation, <code>"randomization"</code> for the randomization
method. May be abbreviated down to any level, e.g. <code>"b"</code>,
<code>"."</code>, <code>"cross"</code>, <code>"rand"</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_bw">bw</code></td>
<td>

<p>Set to <code>TRUE</code> to do fast backward step-down for each training
sample. Default is <code>FALSE</code>. 
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_b">B</code></td>
<td>

<p>Number of repetitions, default=50. For <code>method="crossvalidation"</code>,
this is also the number of groups the original sample is split into.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_pr">pr</code></td>
<td>

<p><code>TRUE</code> to print results for each sample. Default is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_prmodsel">prmodsel</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing of model selection output such
as that from <code><a href="#topic+fastbw">fastbw</a></code>.</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_rule">rule</code></td>
<td>

<p>Stopping rule for fastbw, <code>"aic"</code> or <code>"p"</code>. Default is
<code>"aic"</code> to use Akaike's information criterion.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_type">type</code></td>
<td>

<p>Type of statistic to use in stopping rule for fastbw, <code>"residual"</code>
(the default) or <code>"individual"</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_sls">sls</code></td>
<td>

<p>Significance level for stopping in fastbw if <code>rule="p"</code>. Default is
<code>.05</code>. 
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_aics">aics</code></td>
<td>

<p>Stopping criteria for <code>rule="aic"</code>. Stops deleting factors when
chi-square - 2 times d.f. falls below <code>aics</code>. Default is <code>0</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_tol">tol</code></td>
<td>

<p>Tolerance for singularity checking.  Is passed to <code>fit</code> and <code>fastbw</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_force">force</code></td>
<td>
<p>see <code><a href="#topic+fastbw">fastbw</a></code></p>
</td></tr>
<tr><td><code id="predab.resample_+3A_estimates">estimates</code></td>
<td>
<p>see <code><a href="#topic+print.fastbw">print.fastbw</a></code></p>
</td></tr>
<tr><td><code id="predab.resample_+3A_non.slopes.in.x">non.slopes.in.x</code></td>
<td>
<p>set to <code>FALSE</code> if the design matrix <code>x</code>
does not have columns for intercepts and these columns are needed</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_kint">kint</code></td>
<td>

<p>For multiple intercept models such as the ordinal logistic model, you may
specify which intercept to use as <code>kint</code>.  This affects the linear
predictor that is passed to <code>measure</code>.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_cluster">cluster</code></td>
<td>

<p>Vector containing cluster identifiers.  This can be specified only if
<code>method="boot"</code>.  If it is present, the bootstrap is done using sampling
with replacement from the clusters rather than from the original records.
If this vector is not the same length as the number of rows in the data
matrix used in the fit, an attempt will be made to use <code>naresid</code> on 
<code>fit.orig</code> to conform <code>cluster</code> to the data.  
See <code>bootcov</code> for more about this.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_subset">subset</code></td>
<td>

<p>specify a vector of positive or negative integers or a logical vector when
you want to have the <code>measure</code> function compute measures of accuracy on
a subset of the data.  The whole dataset is still used for all model development.
For example, you may want to <code>validate</code> or <code>calibrate</code> a model by
assessing the predictions on females when the fit was based on males and
females.  When you use <code>cr.setup</code> to build extra observations for fitting the
continuation ratio ordinal logistic model, you can use <code>subset</code> to specify
which <code>cohort</code> or observations to use for deriving indexes of predictive
accuracy.  For example, specify <code>subset=cohort=="all"</code> to validate the
model for the first layer of the continuation ratio model (Prob(Y=0)).
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_group">group</code></td>
<td>

<p>a grouping variable used to stratify the sample upon bootstrapping.
This allows one to handle k-sample problems, i.e., each bootstrap
sample will be forced to selected the same number of observations from
each level of group as the number appearing in the original dataset.
</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_allow.varying.intercepts">allow.varying.intercepts</code></td>
<td>
<p>set to <code>TRUE</code> to not throw an error
if the number of intercepts varies from fit to fit</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_debug">debug</code></td>
<td>
<p>set to <code>TRUE</code> to print subscripts of all training and
test samples</p>
</td></tr>
<tr><td><code id="predab.resample_+3A_...">...</code></td>
<td>

<p>The user may add other arguments here that are passed to <code>fit</code> and
<code>measure</code>.
</p>
</td></tr></table>


<h3>Details</h3>

<p>For <code>method=".632"</code>, the program stops with an error if every observation
is not omitted at least once from a bootstrap sample.  Efron's &quot;.632&quot; method
was developed for measures that are formulated in terms on per-observation
contributions.  In general, error measures (e.g., ROC areas) cannot be
written in this way, so this function uses a heuristic extension to
Efron's formulation in which it is assumed that the average error measure
omitting the <code>i</code>th observation is the same as the average error measure
omitting any other observation.  Then weights are derived
for each bootstrap repetition and weighted averages over the <code>B</code> repetitions
can easily be computed.
</p>


<h3>Value</h3>

<p>a matrix of class <code>"validate"</code> with rows corresponding
to indexes computed by <code>measure</code>, and the following columns:
</p>
<table>
<tr><td><code>index.orig</code></td>
<td>

<p>indexes in original overall fit
</p>
</td></tr>
<tr><td><code>training</code></td>
<td>

<p>average indexes in training samples
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>average indexes in test samples
</p>
</td></tr>
<tr><td><code>optimism</code></td>
<td>

<p>average <code>training-test</code> except for <code>method=".632"</code> - is .632 times
<code>(index.orig - test)</code>
</p>
</td></tr>
<tr><td><code>index.corrected</code></td>
<td>

<p><code>index.orig-optimism</code>
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>number of successful repetitions with the given index non-missing
</p>
</td></tr></table>
<p>.
Also contains an attribute <code>keepinfo</code> if <code>measure</code> returned
such an attribute when run on the original fit.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Efron B, Tibshirani R (1997). Improvements on cross-validation: The .632+ bootstrap method.  JASA 92:548&ndash;560.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>,
<code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+ols">ols</a></code>, <code><a href="#topic+cph">cph</a></code>,
<code><a href="#topic+bootcov">bootcov</a></code>, <code><a href="#topic+setPb">setPb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See the code for validate.ols for an example of the use of
# predab.resample
</code></pre>

<hr>
<h2 id='Predict'>Compute Predicted Values and Confidence Limits</h2><span id='topic+Predict'></span><span id='topic+print.Predict'></span><span id='topic+rbind.Predict'></span>

<h3>Description</h3>

<p><code>Predict</code> allows the user to easily specify which predictors are to
vary.  When the vector of values over which a predictor should vary is
not specified, the
range will be all levels of a categorical predictor or equally-spaced
points between the <code><a href="#topic+datadist">datadist</a></code> <code>"Low:prediction"</code> and
<code>"High:prediction"</code> values for the variable (<code>datadist</code> by
default uses the 10th smallest and 10th largest predictor values in the
dataset).  Predicted values are 
the linear predictor (X beta), a user-specified transformation of that
scale, or estimated probability of surviving past a fixed single time
point given the linear predictor.  <code>Predict</code> is usually used for
plotting predicted values but there is also a <code>print</code> method.
</p>
<p>When the first argument to <code>Predict</code> is a fit object created by
<code>bootcov</code> with <code>coef.reps=TRUE</code>, confidence limits come from
the stored matrix of bootstrap repetitions of coefficients, using 
bootstrap percentile nonparametric confidence limits, basic bootstrap,
or BCa limits.  Such confidence 
intervals do not make distributional assumptions.  You can force
<code>Predict</code> to instead use the bootstrap covariance matrix by setting
<code>usebootcoef=FALSE</code>.  If <code>coef.reps</code> was <code>FALSE</code>,
<code>usebootcoef=FALSE</code> is the default.
</p>
<p>There are <code>ggplot</code>, <code>plotp</code>, and <code>plot</code> methods for
<code>Predict</code> objects that makes it easy to show predicted values and
confidence bands. 
</p>
<p>The <code>rbind</code> method for <code>Predict</code> objects allows you to create
separate sets of predictions under different situations and to combine
them into one set for feeding to <code>plot.Predict</code>, 
<code>ggplot.Predict</code>, or <code>plotp.Predict</code>.  For example you
might want to plot confidence intervals for means and for individuals
using <code>ols</code>, and have the two types of confidence bands be
superposed onto one plot or placed into two panels.  Another use for
<code>rbind</code> is to combine predictions from quantile regression models
that predicted three different quantiles.
</p>
<p>If <code>conf.type="simultaneous"</code>, simultaneous (over all requested
predictions) confidence limits are computed.  See the
<code><a href="#topic+predictrms">predictrms</a></code> function for details.
</p>
<p>If <code>fun</code> is given, <code>conf.int</code> &gt; 0,  the model is not a
Bayesian model, and the bootstrap was not used, <code>fun</code> may return
<code>limits</code> attribute when <code>fun</code> computed its own confidence
limits.  These confidence limits will be functions of the design matrix,
not just the linear predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Predict(object, ..., fun=NULL, funint=TRUE,
        type = c("predictions", "model.frame", "x"),
        np = 200, conf.int = 0.95,
        conf.type = c("mean", "individual","simultaneous"),
        usebootcoef=TRUE, boot.type=c("percentile", "bca", "basic"),
        posterior.summary=c('mean', 'median', 'mode'),
        adj.zero = FALSE, ref.zero = FALSE,
        kint=NULL, ycut=NULL, time = NULL, loglog = FALSE, digits=4, name,
        factors=NULL, offset=NULL)

## S3 method for class 'Predict'
print(x, ...)

## S3 method for class 'Predict'
rbind(..., rename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predict_+3A_object">object</code></td>
<td>

<p>an <code>rms</code> fit object, or for <code>print</code> the result of <code>Predict</code>.
<code>options(datadist="d")</code> must have been specified (where
<code>d</code> was created by <code>datadist</code>), or 
it must have been in effect when the the model was fitted.</p>
</td></tr>
<tr><td><code id="Predict_+3A_...">...</code></td>
<td>

<p>One or more variables to vary, or single-valued adjustment values.
Specify a variable name without an equal sign to use the default
display range, or any range 
you choose (e.g. <code>seq(0,100,by=2),c(2,3,7,14)</code>). 
The default list of values for which predictions are made
is taken as the list of unique values of the variable if they number fewer
than 11. For variables with <code class="reqn">&gt;10</code> unique values, <code>np</code>
equally spaced values in the range are used for plotting if the
range is not specified.  Variables not specified are set to the default
adjustment value <code>limits[2]</code>, i.e. the median for continuous
variables and a reference category for 	non-continuous ones.
Later variables define adjustment settings.
For categorical variables, specify the class labels in quotes when
specifying variable values.  If the levels of a categorical variable
are numeric, you may omit the quotes.  For variables not described
using <code>datadist</code>, you must specify explicit ranges and
adjustment settings for predictors  that were in the model.
If no variables are specified in ..., predictions will be made by
separately varying all predictors in the model over their default
range, holding the other predictors at their adjustment values.
This has the same effect as specifying <code>name</code> as a vector
containing all the predictors.  For <code>rbind</code>, ... represents a
series of results from <code>Predict</code>.  If you name the results,
these names will be taken as the values of the new <code>.set.</code>
variable added to the concatenated data frames.  See an example below.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_fun">fun</code></td>
<td>
<p>an optional transformation of the linear predictor.
Specify <code>fun='mean'</code> if the fit is a proportional odds model
fit and you ran <code>bootcov</code> with <code>coef.reps=TRUE</code>.  This
will let the mean function be re-estimated for each bootstrap rep to
properly account for all sources of uncertainty in estimating the
mean response.  <code>fun</code> can be a general function and can compute
confidence limits (stored as a list in the <code>limits</code> attribute) of
the transformed parameters such as means.</p>
</td></tr>
<tr><td><code id="Predict_+3A_funint">funint</code></td>
<td>
<p>set to <code>FALSE</code> if <code>fun</code> is not a function such
as the result of <code>Mean</code>, <code>Quantile</code>, or <code>ExProb</code> that
contains an <code>intercepts</code> argument</p>
</td></tr>
<tr><td><code id="Predict_+3A_type">type</code></td>
<td>

<p>defaults to providing predictions.  Set to <code>"model.frame"</code> to
return a data frame of predictor settings used.  Set to <code>"x"</code>
to return the corresponding design matrix constructed from the
predictor settings.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_np">np</code></td>
<td>

<p>the number of equally-spaced points computed for continuous
predictors that vary, i.e., when the specified value is <code>.</code>
or <code>NA</code>
</p>
</td></tr>
<tr><td><code id="Predict_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence level (highest posterior density interval probability for
Bayesian models).  Default is 0.95.  Specify <code>FALSE</code> to suppress.</p>
</td></tr>
<tr><td><code id="Predict_+3A_conf.type">conf.type</code></td>
<td>

<p>type of confidence interval.  Default is <code>"mean"</code> which applies
to all models.  For models containing a residual variance (e.g,
<code>ols</code>), you can specify <code>conf.type="individual"</code> instead,
to obtain limits on the predicted value for an individual subject.
Specify <code>conf.type="simultaneous"</code> to obtain simultaneous
confidence bands for mean predictions with family-wise coverage of
<code>conf.int</code>.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_usebootcoef">usebootcoef</code></td>
<td>
<p>set to <code>FALSE</code> to force the use of the bootstrap
covariance matrix estimator even when bootstrap coefficient reps are
present</p>
</td></tr>
<tr><td><code id="Predict_+3A_boot.type">boot.type</code></td>
<td>
<p>set to <code>'bca'</code> to compute BCa confidence
limits or <code>'basic'</code> to use the basic bootstrap.  The default is
to compute percentile intervals</p>
</td></tr>
<tr><td><code id="Predict_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>defaults to using the posterior mean of the
regression coefficients.  Specify <code>'mode'</code> or <code>'median'</code>
to instead use the other summaries.</p>
</td></tr>
<tr><td><code id="Predict_+3A_adj.zero">adj.zero</code></td>
<td>

<p>Set to <code>TRUE</code> to adjust all non-plotted variables to 0 (or
reference cell for categorical variables) and to omit intercept(s)
from consideration. Default is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_ref.zero">ref.zero</code></td>
<td>

<p>Set to <code>TRUE</code> to subtract a constant from <code class="reqn">X\beta</code>
before plotting so that the reference value of the <code>x</code>-variable
yields <code>y=0</code>.  This is done before applying function <code>fun</code>.
This is especially useful for Cox models to make the hazard ratio be
1.0 at reference values, and the confidence interval have width zero.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_kint">kint</code></td>
<td>

<p>This is only useful in a multiple intercept model such as the ordinal
logistic model. There to use to second of three intercepts, for example,
specify <code>kint=2</code>. The default is 1 for <code>lrm</code> and the middle
intercept corresponding to the median <code>y</code> for <code>orm</code> or
<code>blrm</code>.  You can specify <code>ycut</code> instead, and the intercept
corresponding to Y &gt;= ycut will be used for <code>kint</code>.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_ycut">ycut</code></td>
<td>
<p>for an ordinal model specifies the Y cutoff to use in
evaluating departures from proportional odds, when the constrained
partial proportional odds model is used.  When omitted, <code>ycut</code>
is implied by <code>kint</code>.  The only time it is absolutely mandatory
to specify <code>ycut</code> is when computed an effect (e.g., odds ratio)
at a level of the response variable that did not occur in the data.
This would only occur when the <code>cppo</code> function given to
<code>blrm</code> is a continuous function.</p>
</td></tr>
<tr><td><code id="Predict_+3A_time">time</code></td>
<td>

<p>Specify a single time <code>u</code> to cause function <code>survest</code> to
be invoked to plot the probability of surviving until time <code>u</code>
when the fit is from <code>cph</code> or <code>psm</code>.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_loglog">loglog</code></td>
<td>

<p>Specify <code>loglog=TRUE</code> to plot <code>log[-log(survival)]</code>
instead of survival, when <code>time</code> is given.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_digits">digits</code></td>
<td>

<p>Controls how &ldquo;adjust-to&rdquo; values are plotted.  The default is 4
significant digits.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_name">name</code></td>
<td>

<p>Instead of specifying the variables to vary in the
<code>variables</code> (...) list, you can specify one or more variables
by specifying a vector of character string variable names in the
<code>name</code> argument.  Using this mode you cannot specify a list of
variable values to use; prediction is done as if you had said e.g.
<code>age</code> without the equal sign.  Also, interacting factors can
only be set to their reference values using this notation.
</p>
</td></tr>
<tr><td><code id="Predict_+3A_factors">factors</code></td>
<td>

<p>an alternate way of specifying ..., mainly for use by
<code>survplot</code> or <code>gendata</code>.  This must be a list with one or
more values for each variable listed, with <code>NA</code> values for
default ranges.</p>
</td></tr>
<tr><td><code id="Predict_+3A_offset">offset</code></td>
<td>
<p>a list containing one value for one variable, which is
mandatory if the model included an offset term.  The variable name
must match the innermost variable name in the offset term.  The
single offset is added to all predicted values.</p>
</td></tr>
<tr><td><code id="Predict_+3A_x">x</code></td>
<td>
<p>an object created by <code>Predict</code></p>
</td></tr>
<tr><td><code id="Predict_+3A_rename">rename</code></td>
<td>

<p>If you are concatenating predictor sets using <code>rbind</code> and one
or more of the variables were renamed for one or more of the sets,
but these new names represent different versions of the same
predictors (e.g., using or not using imputation), you can specify a
named character vector to rename predictors to a central name.  For
example, specify <code>rename=c(age.imputed='age',
	  corrected.bp='bp')</code> to rename from old names <code>age.imputed,
	  corrected.bp</code> to <code>age, bp</code>.  This happens before
concatenation of rows.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When there are no intercepts in the fitted model, plot subtracts
adjustment values from each factor while computing variances for
confidence limits. 
</p>
<p>Specifying <code>time</code> will not work for Cox models with time-dependent
covariables.  Use <code>survest</code> or <code>survfit</code> for that purpose.
</p>


<h3>Value</h3>

<p>a data frame containing all model predictors and the computed values
<code>yhat</code>, <code>lower</code>, <code>upper</code>, the latter two if confidence
intervals were requested.  The data frame has an additional
<code>class</code> <code>"Predict"</code>.  If <code>name</code> is specified or no
predictors are specified in ..., the resulting data frame has an
additional variable called <code>.predictor.</code> specifying which
predictor is currently being varied.   <code>.predictor.</code> is handy for
use as a paneling variable in <code>lattice</code> or <code>ggplot2</code> graphics.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.Predict">plot.Predict</a></code>, <code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code><a href="#topic+plotp.Predict">plotp.Predict</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>,
<code><a href="#topic+contrast.rms">contrast.rms</a></code>, <code><a href="#topic+summary.rms">summary.rms</a></code>,  
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+survest">survest</a></code>,
<code><a href="#topic+survplot">survplot</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>,
<code><a href="Hmisc.html#topic+transace">transace</a></code>, <code>rbind</code>, <code><a href="#topic+bootcov">bootcov</a></code>,
<code><a href="#topic+bootBCa">bootBCa</a></code>, <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'

# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)

ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')

fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)))
Predict(fit, age, cholesterol, np=4)
Predict(fit, age=seq(20,80,by=10), sex, conf.int=FALSE)
Predict(fit, age=seq(20,80,by=10), sex='male')  # works if datadist not used
# Get simultaneous confidence limits accounting for making 7 estimates
# Predict(fit, age=seq(20,80,by=10), sex='male', conf.type='simult')
# (this needs the multcomp package)

ddist$limits$age[2] &lt;- 30    # make 30 the reference value for age
# Could also do: ddist$limits["Adjust to","age"] &lt;- 30
fit &lt;- update(fit)   # make new reference value take effect
Predict(fit, age, ref.zero=TRUE, fun=exp)

# Make two curves, and plot the predicted curves as two trellis panels
w &lt;- Predict(fit, age, sex)
require(lattice)
xyplot(yhat ~ age | sex, data=w, type='l')
# To add confidence bands we need to use the Hmisc xYplot function in
# place of xyplot
xYplot(Cbind(yhat,lower,upper) ~ age | sex, data=w, 
       method='filled bands', type='l', col.fill=gray(.95))
# If non-displayed variables were in the model, add a subtitle to show
# their settings using title(sub=paste('Adjusted to',attr(w,'info')$adjust),adj=0)
# Easier: feed w into plot.Predict, ggplot.Predict, plotp.Predict
## Not run: 
# Predictions form a parametric survival model
require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, 
              rep=TRUE, prob=c(.6, .4)))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
t &lt;- -log(runif(n))/h
label(t) &lt;- 'Follow-up Time'
e &lt;- ifelse(t&lt;=cens,1,0)
t &lt;- pmin(t, cens)
units(t) &lt;- "Year"
ddist &lt;- datadist(age, sex)
Srv &lt;- Surv(t,e)

# Fit log-normal survival model and plot median survival time vs. age
f &lt;- psm(Srv ~ rcs(age), dist='lognormal')
med &lt;- Quantile(f)       # Creates function to compute quantiles
                         # (median by default)
Predict(f, age, fun=function(x)med(lp=x))
# Note: This works because med() expects the linear predictor (X*beta)
#       as an argument.  Would not work if use 
#       ref.zero=TRUE or adj.zero=TRUE.
# Also, confidence intervals from this method are approximate since
# they don't take into account estimation of scale parameter

# Fit an ols model to log(y) and plot the relationship between x1
# and the predicted mean(y) on the original scale without assuming
# normality of residuals; use the smearing estimator.  Before doing
# that, show confidence intervals for mean and individual log(y),
# and for the latter, also show bootstrap percentile nonparametric
# pointwise confidence limits
set.seed(1)
x1 &lt;- runif(300)
x2 &lt;- runif(300)
ddist &lt;- datadist(x1,x2); options(datadist='ddist')
y  &lt;- exp(x1+ x2 - 1 + rnorm(300))
f  &lt;- ols(log(y) ~ pol(x1,2) + x2, x=TRUE, y=TRUE)  # x y for bootcov
fb &lt;- bootcov(f, B=100)
pb &lt;- Predict(fb, x1, x2=c(.25,.75))
p1 &lt;- Predict(f,  x1, x2=c(.25,.75))
p &lt;- rbind(normal=p1, boot=pb)
plot(p)

p1 &lt;- Predict(f, x1, conf.type='mean')
p2 &lt;- Predict(f, x1, conf.type='individual')
p  &lt;- rbind(mean=p1, individual=p2)
plot(p, label.curve=FALSE)   # uses superposition
plot(p, ~x1 | .set.)         # 2 panels

r &lt;- resid(f)
smean &lt;- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
formals(smean) &lt;- list(yhat=numeric(0), res=r[!is.na(r)])
#smean$res &lt;- r[!is.na(r)]   # define default res argument to function
Predict(f, x1, fun=smean)

## Example using offset
g &lt;- Glm(Y ~ offset(log(N)) + x1 + x2, family=poisson)
Predict(g, offset=list(N=100))

## End(Not run)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='predict.lrm'>
Predicted Values for Binary and Ordinal Logistic Models
</h2><span id='topic+predict.lrm'></span><span id='topic+predict.orm'></span><span id='topic+Mean.lrm'></span><span id='topic+Mean.orm'></span>

<h3>Description</h3>

<p>Computes a variety of types of predicted values for fits from
<code>lrm</code> and <code>orm</code>, either from the original dataset or for new
observations.  The <code>Mean.lrm</code> and <code>Mean.orm</code> functions produce
an R function to compute the predicted mean of a numeric ordered
response variable given the linear predictor, which is assumed to use
the first intercept when it was computed.  The returned function has two
optional arguments if confidence intervals are desired: <code>conf.int</code>
and the design matrix <code>X</code>.  When this derived function is called
with nonzero <code>conf.int</code>, an attribute named <code>limits</code> is attached
to the estimated mean.  This is a list with elements <code>lower</code> and
<code>upper</code> containing normal approximations for confidence limits
using the delta method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lrm'
predict(object, ..., type=c("lp", "fitted",
            "fitted.ind", "mean", "x", "data.frame",
            "terms", "cterms", "ccterms", "adjto","adjto.data.frame", 
            "model.frame"), se.fit=FALSE, codes=FALSE)
## S3 method for class 'orm'
predict(object, ..., type=c("lp", "fitted",
            "fitted.ind", "mean", "x", "data.frame",
            "terms", "cterms", "ccterms", "adjto","adjto.data.frame", 
            "model.frame"), se.fit=FALSE, codes=FALSE)

## S3 method for class 'lrm'
Mean(object, codes=FALSE, ...)
## S3 method for class 'orm'
Mean(object, codes=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lrm_+3A_object">object</code></td>
<td>
<p>a object created by <code>lrm</code> or <code>orm</code></p>
</td></tr>
<tr><td><code id="predict.lrm_+3A_...">...</code></td>
<td>

<p>arguments passed to <code>predictrms</code>, such as <code>kint</code> and <code>newdata</code>
(which is used if you are predicting <code>out of data</code>).  See
<code>predictrms</code> to see how NAs are handled.  Ignored for other functions.
</p>
</td></tr>
<tr><td><code id="predict.lrm_+3A_type">type</code></td>
<td>

<p>See <code>predict.rms</code> for <code>"x", "data.frame", "terms", "cterms",
  "ccterms", "adjto", "adjto.data.frame"</code> and
<code>"model.frame"</code>. <code>type="lp"</code> is used to get 
linear predictors (using the first intercept by default; specify
<code>kint</code> to use others). <code>type="fitted"</code>
is used to get all the probabilities <code class="reqn">Y\geq	j</code>.
<code>type="fitted.ind"</code> gets all the individual probabilities 
<code class="reqn">Y=j</code> (not recommended for <code>orm</code> fits). For an ordinal response
variable, <code>type="mean"</code> computes 
the estimated mean <code class="reqn">Y</code> by summing values of <code class="reqn">Y</code> 
multiplied by the estimated <code class="reqn">Prob(Y=j)</code>. If <code class="reqn">Y</code> was a character or
<code>factor</code> object, the levels are the character values or factor levels,
so these must be translatable to numeric, unless <code>codes=TRUE</code>.
See the Hannah and Quigley reference below for the method of estimating
(and presenting) the mean score.  If you specify
<code>type="fitted","fitted.ind","mean"</code> you may not specify <code>kint</code>.
</p>
</td></tr>
<tr><td><code id="predict.lrm_+3A_se.fit">se.fit</code></td>
<td>

<p>applies only to <code>type="lp"</code>, to get standard errors.
</p>
</td></tr>
<tr><td><code id="predict.lrm_+3A_codes">codes</code></td>
<td>

<p>if <code>TRUE</code>, <code>type="mean"</code>, <code>Mean.lrm</code>, and <code>Mean.orm</code>
use the integer codes <code class="reqn">1,2,\ldots,k</code> for the <code class="reqn">k</code>-level response
in computing the predicted mean response.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector (<code>type="lp"</code> with <code>se.fit=FALSE</code>, or
<code>type="mean"</code> or only one 
observation being predicted), a list (with elements <code>linear.predictors</code>
and <code>se.fit</code> if <code>se.fit=TRUE</code>), a matrix (<code>type="fitted"</code>
or <code>type="fitted.ind"</code>), a data frame, or a design matrix.  For
<code>Mean.lrm</code> and <code>Mean.orm</code>, the result is an R function.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com<br />
For the <code>Quantile</code> function:<br />
Qi Liu and Shengxin Tu<br />
Department of Biostatistics, Vanderbilt University
</p>


<h3>References</h3>

<p>Hannah M, Quigley P: Presentation of ordinal regression analysis on the
original scale.  Biometrics 52:771&ndash;5; 1996.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+orm">orm</a></code>, <code><a href="#topic+predict.rms">predict.rms</a></code>,
<code><a href="stats.html#topic+naresid">naresid</a></code>, <code><a href="#topic+contrast.rms">contrast.rms</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See help for predict.rms for several binary logistic
# regression examples


# Examples of predictions from ordinal models
set.seed(1)
y &lt;- factor(sample(1:3, 400, TRUE), 1:3, c('good','better','best'))
x1 &lt;- runif(400)
x2 &lt;- runif(400)
f &lt;- lrm(y ~ rcs(x1,4)*x2, x=TRUE)     #x=TRUE needed for se.fit
# Get 0.95 confidence limits for Prob[better or best]
L &lt;- predict(f, se.fit=TRUE)           #omitted kint= so use 1st intercept
plogis(with(L, linear.predictors + 1.96*cbind(-se.fit,se.fit)))
predict(f, type="fitted.ind")[1:10,]   #gets Prob(better) and all others
d &lt;- data.frame(x1=c(.1,.5),x2=c(.5,.15))
predict(f, d, type="fitted")        # Prob(Y&gt;=j) for new observation
predict(f, d, type="fitted.ind")    # Prob(Y=j)
predict(f, d, type='mean', codes=TRUE) # predicts mean(y) using codes 1,2,3
m &lt;- Mean(f, codes=TRUE)
lp &lt;- predict(f, d)
m(lp)
# Can use function m as an argument to Predict or nomogram to
# get predicted means instead of log odds or probabilities
dd &lt;- datadist(x1,x2); options(datadist='dd')
m
plot(Predict(f, x1, fun=m), ylab='Predicted Mean')
# Note: Run f through bootcov with coef.reps=TRUE to get proper confidence
# limits for predicted means from the prop. odds model
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='predictrms'>Predicted Values from Model Fit</h2><span id='topic+predictrms'></span><span id='topic+predict.rms'></span><span id='topic+predict.bj'></span><span id='topic+predict.cph'></span><span id='topic+predict.Glm'></span><span id='topic+predict.Gls'></span><span id='topic+predict.ols'></span><span id='topic+predict.psm'></span>

<h3>Description</h3>

<p>The <code>predict</code> function is used to obtain a variety of values or
predicted values from either the data used to fit the model (if
<code>type="adjto"</code> or <code>"adjto.data.frame"</code> or if <code>x=TRUE</code> or
<code>linear.predictors=TRUE</code> were specified to the modeling function), or from
a new dataset. Parameters such as knots and factor levels used in creating 
the design matrix in the original fit are &quot;remembered&quot;.
See the <code>Function</code> function for another method for computing the
linear predictors.  <code>predictrms</code> is an internal utility function
that is for the other functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictrms(fit, newdata=NULL,
           type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
             "adjto", "adjto.data.frame", "model.frame"),
           se.fit=FALSE, conf.int=FALSE,
           conf.type=c('mean', 'individual', 'simultaneous'),
           kint=NULL, na.action=na.keep, expand.na=TRUE,
           center.terms=type=="terms", ref.zero=FALSE,
           posterior.summary=c('mean', 'median', 'mode'),
           second=FALSE, ...)
## S3 method for class 'bj'
predict(object, newdata,
        type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
               "adjto", "adjto.data.frame", "model.frame"), 
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1,
        na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # for bj

## S3 method for class 'cph'
predict(object, newdata=NULL,
        type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
               "adjto", "adjto.data.frame", "model.frame"),
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1, na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # cph

## S3 method for class 'Glm'
predict(object, newdata,
        type= c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
                "adjto", "adjto.data.frame", "model.frame"),
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1, na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # Glm

## S3 method for class 'Gls'
predict(object, newdata,
        type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
               "adjto", "adjto.data.frame", "model.frame"),
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1, na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # Gls

## S3 method for class 'ols'
predict(object, newdata,
        type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
               "adjto", "adjto.data.frame", "model.frame"),
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1, na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # ols

## S3 method for class 'psm'
predict(object, newdata,
        type=c("lp", "x", "data.frame", "terms", "cterms", "ccterms",
               "adjto", "adjto.data.frame", "model.frame"),
        se.fit=FALSE, conf.int=FALSE,
        conf.type=c('mean','individual','simultaneous'),
        kint=1, na.action=na.keep, expand.na=TRUE,
        center.terms=type=="terms", ...) # psm
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictrms_+3A_object">object</code>, <code id="predictrms_+3A_fit">fit</code></td>
<td>
<p>a fit object with an <code>rms</code> fitting function</p>
</td></tr>
<tr><td><code id="predictrms_+3A_newdata">newdata</code></td>
<td>

<p>An S data frame, list or a matrix specifying new data for which predictions
are desired.  If <code>newdata</code> is a list, it is converted to a matrix first.
A matrix is converted to a data frame.  For the matrix form, categorical
variables (<code>catg</code> or <code>strat</code>) must be coded as integer category
numbers corresponding to the order in which value labels were stored.
For list or matrix forms, <code>matrx</code> factors must be given a single
value.  If this single value is the S missing value <code>NA</code>, the adjustment
values of matrx (the column medians) will later replace this value.
If the single value is not <code>NA</code>, it is propagated throughout the columns
of the <code>matrx</code> factor.  For <code>factor</code> variables having numeric levels,
you can specify the numeric values in <code>newdata</code> without first converting
the variables to factors.  These numeric values are checked to make sure
they match a level, then the variable is converted internally to a <code>factor</code>.
It is most typical to use a data frame
for newdata, and the S function <code>expand.grid</code> is very handy here.
For example, one may specify 
<br />
<code>newdata=expand.grid(age=c(10,20,30),</code>
<br />
<code>race=c("black","white","other"),</code>
<br />
<code>chol=seq(100,300,by=25))</code>.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_type">type</code></td>
<td>

<p>Type of output desired.  The default is <code>"lp"</code> to get the linear predictors -
predicted <code class="reqn">X\beta</code>.  For Cox models, these predictions are centered.
You may specify <code>"x"</code> to get an expanded design matrix
at the desired combinations of values, <code>"data.frame"</code> to get an
S data frame of the combinations, <code>"model.frame"</code> to get a data frame
of the transformed predictors, <code>"terms"</code> to get a matrix with
each column being the linear combination of variables making up
a factor (with separate terms for interactions), <code>"cterms"</code>
(&quot;combined terms&quot;) to not create separate terms for interactions 
but to add all interaction terms involving each predictor to the
main terms for each predictor, <code>"ccterms"</code> to combine all related
terms (related through interactions) and their interactions into a
single column, <code>"adjto"</code> to return a vector of 
<code>limits[2]</code> (see <code>datadist</code>) in coded 
form, and <code>"adjto.data.frame"</code> to return a data frame version of these
central adjustment values.  Use of <code>type="cterms"</code> does not make
sense for a <code>strat</code> variable that does not interact with
another variable.  If <code>newdata</code> is not given, <code>predict</code>
will attempt to return information stored with the fit object if the
appropriate options were used with the modeling function (e.g., <code>x, y, linear.predictors, se.fit</code>).
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_se.fit">se.fit</code></td>
<td>

<p>Defaults to <code>FALSE</code>.  If <code>type="linear.predictors"</code>, set
<code>se.fit=TRUE</code> to return a list with components
<code>linear.predictors</code> and <code>se.fit</code> instead of just a vector of
fitted values.   For Cox model fits, standard errors of linear
predictors are computed after subtracting the original column means from
the new design matrix.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_conf.int">conf.int</code></td>
<td>

<p>Specify <code>conf.int</code> as a positive fraction to obtain upper and lower
confidence intervals (e.g., <code>conf.int=0.95</code>).  The <code class="reqn">t</code>-distribution is
used in the calculation for <code>ols</code> fits.  Otherwise, the normal
critical value is used.  For Bayesian models <code>conf.int</code> is the
highest posterior density interval probability.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_conf.type">conf.type</code></td>
<td>

<p>specifies the type of confidence interval.  Default is for the mean.
For <code>ols</code> fits there is the option of obtaining confidence limits for
individual predicted values by specifying <code>conf.type="individual"</code>.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>when making predictions from a Bayesian model,
specifies whether you want the linear predictor to be computed
from the posterior mean of parameters (default) or the posterior
mode or median median</p>
</td></tr>
<tr><td><code id="predictrms_+3A_second">second</code></td>
<td>
<p>set to <code>TRUE</code> to use the model's second formula.  At
present this pertains only to a partial proportional odds model
fitted using the <code>blrm</code> function.  When <code>second=TRUE</code>
and <code>type='x'</code> the Z design matrix is returned (that goes
with the tau parameters in the partial PO model).  When
<code>type='lp'</code> is specified Z*tau is computed.  In neither case
is the result is multiplied by the by the <code>cppo</code> function.</p>
</td></tr>
<tr><td><code id="predictrms_+3A_kint">kint</code></td>
<td>
<p>a single integer specifying the number of the intercept to use in
multiple-intercept models.  The default is 1 for <code>lrm</code> and the reference median intercept for <code>orm</code> and <code>blrm</code>.  For a partial PO model, <code>kint</code> should correspond to the response variable value that will be used when dealing with <code>second=TRUE</code>.</p>
</td></tr>
<tr><td><code id="predictrms_+3A_na.action">na.action</code></td>
<td>

<p>Function to handle missing values in <code>newdata</code>.  For predictions
&quot;in data&quot;, the same <code>na.action</code> that was used during model fitting is
used to define an <code>naresid</code> function to possibly restore rows of the data matrix
that were deleted due to NAs.  For predictions &quot;out of data&quot;, the default
<code>na.action</code> is <code>na.keep</code>, resulting in NA predictions when a row of
<code>newdata</code> has an NA.  Whatever <code>na.action</code> is in effect at the time
for &quot;out of data&quot; predictions, the corresponding <code>naresid</code> is used also.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_expand.na">expand.na</code></td>
<td>

<p>set to <code>FALSE</code> to keep the <code>naresid</code> from having any effect, i.e., to keep
from adding back observations removed because of NAs in the returned object.
If <code>expand.na=FALSE</code>, the <code>na.action</code> attribute will be added to the returned
object.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_center.terms">center.terms</code></td>
<td>

<p>set to <code>FALSE</code> to suppress subtracting adjust-to values from
columns of the design matrix before computing terms with <code>type="terms"</code>.
</p>
</td></tr>
<tr><td><code id="predictrms_+3A_ref.zero">ref.zero</code></td>
<td>
<p>Set to <code>TRUE</code> to subtract a constant from <code class="reqn">X\beta</code>
before plotting so that the reference value of the <code>x</code>-variable
yields <code>y=0</code>.  This is done before applying function <code>fun</code>.
This is especially useful for Cox models to make the hazard ratio be
1.0 at reference values, and the confidence interval have width zero.</p>
</td></tr>
<tr><td><code id="predictrms_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>datadist</code> and <code>options(datadist=)</code> should be run before <code>predictrms</code>
if using <code>type="adjto"</code>, <code>type="adjto.data.frame"</code>, or <code>type="terms"</code>,
or if the fit is a Cox model fit and you are requesting <code>se.fit=TRUE</code>.
For these cases, the adjustment values are needed (either for the
returned result or for the correct covariance matrix computation).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.Predict">plot.Predict</a></code>, <code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code><a href="#topic+summary.rms">summary.rms</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+predict.lrm">predict.lrm</a></code>,
<code><a href="#topic+predict.orm">predict.orm</a></code>,
<code><a href="#topic+residuals.cph">residuals.cph</a></code>, <code><a href="#topic+datadist">datadist</a></code>,
<code><a href="#topic+gendata">gendata</a></code>, <code><a href="#topic+gIndex">gIndex</a></code>,
<code><a href="#topic+Function.rms">Function.rms</a></code>, <code><a href="Hmisc.html#topic+reShape">reShape</a></code>, 
<code><a href="Hmisc.html#topic+xYplot">xYplot</a></code>, <code><a href="#topic+contrast.rms">contrast.rms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
treat          &lt;- factor(sample(c('a','b','c'), n,TRUE))


# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
  .3*sqrt(blood.pressure-60)-2.3 + 1*(treat=='b')
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


ddist &lt;- datadist(age, blood.pressure, cholesterol, sex, treat)
options(datadist='ddist')


fit &lt;- lrm(y ~ rcs(blood.pressure,4) + 
           sex * (age + rcs(cholesterol,4)) + sex*treat*age)


# Use xYplot to display predictions in 9 panels, with error bars,
# with superposition of two treatments


dat &lt;- expand.grid(treat=levels(treat),sex=levels(sex),
                   age=c(20,40,60),blood.pressure=120,
                   cholesterol=seq(100,300,length=10))
# Add variables linear.predictors and se.fit to dat
dat &lt;- cbind(dat, predict(fit, dat, se.fit=TRUE))
# This is much easier with Predict
# xYplot in Hmisc extends xyplot to allow error bars

xYplot(Cbind(linear.predictors,linear.predictors-1.96*se.fit,
             linear.predictors+1.96*se.fit) ~ cholesterol | sex*age,
       groups=treat, data=dat, type='b')




# Since blood.pressure doesn't interact with anything, we can quickly and
# interactively try various transformations of blood.pressure, taking
# the fitted spline function as the gold standard. We are seeking a
# linearizing transformation even though this may lead to falsely
# narrow confidence intervals if we use this data-dredging-based transformation


bp &lt;- 70:160
logit &lt;- predict(fit, expand.grid(treat="a", sex='male', age=median(age),
                 cholesterol=median(cholesterol),
                 blood.pressure=bp), type="terms")[,"blood.pressure"]
#Note: if age interacted with anything, this would be the age
#      "main effect" ignoring interaction terms
#Could also use Predict(f, age=ag)$yhat
#which allows evaluation of the shape for any level of interacting
#factors.  When age does not interact with anything, the result from
#predict(f, \dots, type="terms") would equal the result from
#plot if all other terms were ignored


plot(bp^.5, logit)               # try square root vs. spline transform.
plot(bp^1.5, logit)              # try 1.5 power
plot(sqrt(bp-60), logit)


#Some approaches to making a plot showing how predicted values
#vary with a continuous predictor on the x-axis, with two other
#predictors varying


combos &lt;- gendata(fit, age=seq(10,100,by=10), cholesterol=c(170,200,230),
                  blood.pressure=c(80,120,160))
#treat, sex not specified -&gt; set to mode
#can also used expand.grid

require(lattice)
combos$pred &lt;- predict(fit, combos)
xyplot(pred ~ age | cholesterol*blood.pressure, data=combos, type='l')
xYplot(pred ~ age | cholesterol, groups=blood.pressure, data=combos, type='l')
Key()   # Key created by xYplot
xYplot(pred ~ age, groups=interaction(cholesterol,blood.pressure),
       data=combos, type='l', lty=1:9)
Key()


# Add upper and lower 0.95 confidence limits for individuals
combos &lt;- cbind(combos, predict(fit, combos, conf.int=.95))
xYplot(Cbind(linear.predictors, lower, upper) ~ age | cholesterol,
       groups=blood.pressure, data=combos, type='b')
Key()


# Plot effects of treatments (all pairwise comparisons) vs.
# levels of interacting factors (age, sex)


d &lt;- gendata(fit, treat=levels(treat), sex=levels(sex), age=seq(30,80,by=10))
x &lt;- predict(fit, d, type="x")
betas &lt;- fit$coef
cov   &lt;- vcov(fit, intercepts='none')


i &lt;- d$treat=="a"; xa &lt;- x[i,]; Sex &lt;- d$sex[i]; Age &lt;- d$age[i]
i &lt;- d$treat=="b"; xb &lt;- x[i,]
i &lt;- d$treat=="c"; xc &lt;- x[i,]


doit &lt;- function(xd, lab) {
  xb &lt;- matxv(xd, betas)
  se &lt;- apply((xd %*% cov) * xd, 1, sum)^.5
  q &lt;- qnorm(1-.01/2)   # 0.99 confidence limits
  lower &lt;- xb - q * se; upper &lt;- xb + q * se
  #Get odds ratios instead of linear effects
  xb &lt;- exp(xb); lower &lt;- exp(lower); upper &lt;- exp(upper)
  #First elements of these agree with 
  #summary(fit, age=30, sex='female',conf.int=.99))
  for(sx in levels(Sex)) {
    j &lt;- Sex==sx
    errbar(Age[j], xb[j], upper[j], lower[j], xlab="Age", 
           ylab=paste(lab, "Odds Ratio"), ylim=c(.1, 20), log='y')
    title(paste("Sex:", sx))
    abline(h=1, lty=2)
  }
}


par(mfrow=c(3,2), oma=c(3,0,3,0))
doit(xb - xa, "b:a")
doit(xc - xa, "c:a")
doit(xb - xa, "c:b")

# NOTE: This is much easier to do using contrast.rms

# Demonstrate type="terms", "cterms", "ccterms"
set.seed(1)
n &lt;- 40
x &lt;- 1:n
w &lt;- factor(sample(c('a', 'b'), n, TRUE))
u &lt;- factor(sample(c('A', 'B'), n, TRUE))
y &lt;- .01*x + .2*(w=='b') + .3*(u=='B') + .2*(w=='b' &amp; u=='B') + rnorm(n)/5
ddist &lt;- datadist(x, w, u)
f &lt;- ols(y ~ x*w*u, x=TRUE, y=TRUE)
f
anova(f)
z &lt;- predict(f, type='terms', center.terms=FALSE)
z[1:5,]
k &lt;- coef(f)
## Manually compute combined terms
wb &lt;- w=='b'
uB &lt;- u=='B'
h  &lt;- k['x * w=b * u=B']*x*wb*uB
tx &lt;- k['x']  *x  + k['x * w=b']*x*wb + k['x * u=B']  *x*uB  + h
tw &lt;- k['w=b']*wb + k['x * w=b']*x*wb + k['w=b * u=B']*wb*uB + h
tu &lt;- k['u=B']*uB + k['x * u=B']*x*uB + k['w=b * u=B']*wb*uB + h
h   &lt;- z[,'x * w * u'] # highest order term is present in all cterms
tx2 &lt;- z[,'x']+z[,'x * w']+z[,'x * u']+h
tw2 &lt;- z[,'w']+z[,'x * w']+z[,'w * u']+h
tu2 &lt;- z[,'u']+z[,'x * u']+z[,'w * u']+h
ae &lt;- function(a, b) all.equal(a, b, check.attributes=FALSE)
ae(tx, tx2)
ae(tw, tw2)
ae(tu, tu2)

zc &lt;- predict(f, type='cterms')
zc[1:5,]
ae(tx, zc[,'x'])
ae(tw, zc[,'w'])
ae(tu, zc[,'u'])

zc &lt;- predict(f, type='ccterms')
# As all factors are indirectly related, ccterms gives overall linear
# predictor except for the intercept
zc[1:5,]
ae(as.vector(zc + coef(f)[1]), f$linear.predictors)

## Not run: 
#A variable state.code has levels "1", "5","13"
#Get predictions with or without converting variable in newdata to factor
predict(fit, data.frame(state.code=c(5,13)))
predict(fit, data.frame(state.code=factor(c(5,13))))


#Use gendata function (gendata.rms) for interactive specification of
#predictor variable settings (for 10 observations)
df &lt;- gendata(fit, nobs=10, viewvals=TRUE)
df$predicted &lt;- predict(fit, df)  # add variable to data frame
df


df &lt;- gendata(fit, age=c(10,20,30))  # leave other variables at ref. vals.
predict(fit, df, type="fitted")


# See reShape (in Hmisc) for an example where predictions corresponding to 
# values of one of the varying predictors are reformatted into multiple
# columns of a matrix

## End(Not run)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='print.cph'>Print cph Results</h2><span id='topic+print.cph'></span>

<h3>Description</h3>

<p>Formatted printing of an object of class <code>cph</code>. Prints strata
frequencies, parameter estimates, standard errors, z-statistics, numbers
of missing values, etc.
Format of output is controlled by the user previously running
<code>options(prType="lang")</code> where <code>lang</code> is <code>"plain"</code> (the default),
<code>"latex"</code>, or <code>"html"</code>.  This does not require <code>results='asis'</code>
in <code>knitr</code> chunk headers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cph'
print(x, digits=4, r2=c(0,2,4), table=TRUE, conf.int=FALSE, 
coefs=TRUE, pg=FALSE, title='Cox Proportional Hazards Model', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cph_+3A_x">x</code></td>
<td>
<p>fit object</p>
</td></tr>
<tr><td><code id="print.cph_+3A_digits">digits</code></td>
<td>
<p>number of digits to right of decimal place to print</p>
</td></tr>
<tr><td><code id="print.cph_+3A_r2">r2</code></td>
<td>
<p>vector of integers specifying which R^2 measures to print,
with 0 for Nagelkerke R^2 and 1:4 corresponding to the 4 measures
computed by <code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>.  Default is to print
Nagelkerke (labeled R2) and second and fourth <code>R2Measures</code>
which are the measures adjusted for the number of predictors, first
for the raw sample size then for the effective sample size, which
here is the number of non-censored observations.</p>
</td></tr>
<tr><td><code id="print.cph_+3A_conf.int">conf.int</code></td>
<td>

<p>set to e.g. .95 to print 0.95 confidence intervals on simple hazard
ratios (which are usually meaningless as one-unit changes are seldom
relevant and most models contain multiple terms per predictor)
</p>
</td></tr>
<tr><td><code id="print.cph_+3A_table">table</code></td>
<td>

<p>set to <code>FALSE</code> to suppress event frequency statistics
</p>
</td></tr>
<tr><td><code id="print.cph_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="print.cph_+3A_pg">pg</code></td>
<td>
<p>set to <code>TRUE</code> to print g-indexes</p>
</td></tr>
<tr><td><code id="print.cph_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="print.cph_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>prModFit</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="#topic+prModFit">prModFit</a></code>
</p>

<hr>
<h2 id='print.Glm'>print.glm</h2><span id='topic+print.Glm'></span>

<h3>Description</h3>

<p>Print a 'Glm' Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Glm'
print(x, digits = 4, coefs = TRUE, title = "General Linear Model", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.Glm_+3A_x">x</code></td>
<td>
<p>'Glm' object</p>
</td></tr>
<tr><td><code id="print.Glm_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to print</p>
</td></tr>
<tr><td><code id="print.Glm_+3A_coefs">coefs</code></td>
<td>
<p>specify 'coefs=FALSE' to suppress printing the table of
model coefficients, standard errors, etc.  Specify 'coefs=n' to print
only the first 'n' regression coefficients in the model.</p>
</td></tr>
<tr><td><code id="print.Glm_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to 'prModFit'</p>
</td></tr>
<tr><td><code id="print.Glm_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prints a 'Glm' object, optionally in LaTeX or html
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='print.impactPO'>Print Result from impactPO</h2><span id='topic+print.impactPO'></span>

<h3>Description</h3>

<p>Prints statistical summaries and optionally predicted values computed by <code>impactPO</code>, transposing statistical summaries for easy reading
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'impactPO'
print(x, estimates = nrow(x$estimates) &lt; 16, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.impactPO_+3A_x">x</code></td>
<td>
<p>an object created by <code>impactPO</code></p>
</td></tr>
<tr><td><code id="print.impactPO_+3A_estimates">estimates</code></td>
<td>
<p>set to <code>FALSE</code> to suppess printing estimated category probabilities.  Defaults to <code>TRUE</code> when the number of rows &lt; 16.</p>
</td></tr>
<tr><td><code id="print.impactPO_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='print.ols'>Print ols</h2><span id='topic+print.ols'></span>

<h3>Description</h3>

<p>Formatted printing of an object of class <code>ols</code> using methods taken
from <code>print.lm</code> and <code>summary.lm</code>. Prints R-squared, adjusted
R-squared, parameter estimates, standard errors, and t-statistics (Z
statistics if penalized estimation was used).  For penalized estimation,
prints the maximum penalized likelihood estimate of the residual
standard deviation (<code>Sigma</code>) instead of the usual root mean squared
error.
Format of output is controlled by the user previously running
<code>options(prType="lang")</code> where <code>lang</code> is <code>"plain"</code> (the default),
<code>"latex"</code>, or <code>"html"</code>.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ols'
print(x, digits=4, long=FALSE, coefs=TRUE, 
 title="Linear Regression Model", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ols_+3A_x">x</code></td>
<td>
<p>fit object</p>
</td></tr>
<tr><td><code id="print.ols_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to print</p>
</td></tr>
<tr><td><code id="print.ols_+3A_long">long</code></td>
<td>
<p>set to <code>TRUE</code> to print the correlation matrix of
parameter estimates</p>
</td></tr>
<tr><td><code id="print.ols_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="print.ols_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="print.ols_+3A_...">...</code></td>
<td>
<p>other parameters to pass to <code>print</code> or <code>format</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ols">ols</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>,<code><a href="#topic+prModFit">prModFit</a></code>
</p>

<hr>
<h2 id='print.rexVar'>print.rexVar</h2><span id='topic+print.rexVar'></span>

<h3>Description</h3>

<p>Print rexVar Result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rexVar'
print(x, title = "Relative Explained Variation", digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.rexVar_+3A_x">x</code></td>
<td>
<p>a vector or matrix created by <code>rexVar</code></p>
</td></tr>
<tr><td><code id="print.rexVar_+3A_title">title</code></td>
<td>
<p>character string which can be set to <code>NULL</code> or <code>''</code> to suppress</p>
</td></tr>
<tr><td><code id="print.rexVar_+3A_digits">digits</code></td>
<td>
<p>passed to <code><a href="base.html#topic+round">round()</a></code></p>
</td></tr>
<tr><td><code id="print.rexVar_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prints the results of an <code>rexVar</code> call
</p>


<h3>Value</h3>

<p>invisible
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='prmiInfo'>prmiInfo</h2><span id='topic+prmiInfo'></span>

<h3>Description</h3>

<p>Print Information About Impact of Imputation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prmiInfo(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prmiInfo_+3A_x">x</code></td>
<td>
<p>an object created by <code>processMI(..., 'anova')</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the results of <code>processMI.fit.mult.impute</code> prints or writes html (the latter if <code>options(prType='html')</code> is in effect) summarizing various correction factors related to missing data multiple imputation.
</p>


<h3>Value</h3>

<p>nothing
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
a &lt;- aregImpute(...)
f &lt;- fit.mult.impute(...)
v &lt;- processMI(f, 'anova')
prmiInfo(v)

## End(Not run)
</code></pre>

<hr>
<h2 id='processMI'>processMI</h2><span id='topic+processMI'></span>

<h3>Description</h3>

<p>Process Special Multiple Imputation Output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>processMI(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="processMI_+3A_object">object</code></td>
<td>
<p>a fit object created by <code><a href="Hmisc.html#topic+transcan">Hmisc::fit.mult.impute()</a></code></p>
</td></tr>
<tr><td><code id="processMI_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Processes lists that have one element per imputation
</p>


<h3>Value</h3>

<p>an object that resembles something created by a single fit without multiple imputation
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+processMI.fit.mult.impute">processMI.fit.mult.impute()</a></code>
</p>

<hr>
<h2 id='processMI.fit.mult.impute'>processMI.fit.mult.impute</h2><span id='topic+processMI.fit.mult.impute'></span>

<h3>Description</h3>

<p>Process Special Multiple Imputation Output From <code>fit.mult.impute</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fit.mult.impute'
processMI(
  object,
  which = c("validate", "calibrate", "anova"),
  plotall = TRUE,
  nind = 0,
  prmi = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="processMI.fit.mult.impute_+3A_object">object</code></td>
<td>
<p>a fit object created by <code>fit.mult.impute</code></p>
</td></tr>
<tr><td><code id="processMI.fit.mult.impute_+3A_which">which</code></td>
<td>
<p>specifies which component of the extra output should be processed</p>
</td></tr>
<tr><td><code id="processMI.fit.mult.impute_+3A_plotall">plotall</code></td>
<td>
<p>set to <code>FALSE</code> when <code>which='calibrate'</code> to suppress having <code>ggplot</code> render a graph showing calibration curves produced separately for all the imputations</p>
</td></tr>
<tr><td><code id="processMI.fit.mult.impute_+3A_nind">nind</code></td>
<td>
<p>set to a positive integer to use base graphics to plot a matrix of graphs, one each for the first <code>nind</code> imputations, and the overall average calibration curve at the end</p>
</td></tr>
<tr><td><code id="processMI.fit.mult.impute_+3A_prmi">prmi</code></td>
<td>
<p>set to <code>FALSE</code> to not print imputation corrections for <code>anova</code></p>
</td></tr>
<tr><td><code id="processMI.fit.mult.impute_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Processes a <code>funresults</code> object stored in a fit object created by <code>fit.mult.impute</code> when its <code>fun</code> argument was used.  These objects are typically named <code>validate</code> or <code>calibrate</code> and represent bootstrap or cross-validations run separately for each imputation.  See <a href="https://hbiostat.org/rmsc/validate.html#sec-val-mival">this</a> for a case study.
</p>
<p>For <code>which='anova'</code> assumes that the <code>fun</code> given to <code>fit.mult.impute</code> runs <code>anova(fit, test='LR')</code> to get likelihood ratio tests, and that <code>method='stack'</code> was specified also so that a final <code>anova</code> was run on the stacked combination of all completed datasets.  The method of <a href="https://hbiostat.org/rmsc/missing.html#sec-missing-lrt">Chan and Meng (2022)</a> is used to obtain overall likelihood ratio tests, with each line of the <code>anova</code> table getting a customized adjustment based on the amount of missing information pertaining to the variables tested in that line.  The resulting statistics are chi-square and not $F$ statistics as used by Chan and Meng.  This will matter when the estimated denominator degrees of freedom for a variable is small (e.g., less than 50).  These d.f. are reported so that user can take appropriate cautions such as increasing <code>n.impute</code> for <code>aregImpute</code>.
</p>


<h3>Value</h3>

<p>an object like a <code>validate</code>, <code>calibrate</code>, or <code>anova</code> result obtained when no multiple imputation was done.  This object is suitable for <code>print</code> and <code>plot</code> methods for these kinds of objects.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="Hmisc.html#topic+transcan">Hmisc::fit.mult.impute()</a></code>
</p>

<hr>
<h2 id='psm'>Parametric Survival Model</h2><span id='topic+psm'></span><span id='topic+print.psm'></span><span id='topic+Hazard'></span><span id='topic+Survival'></span><span id='topic+Hazard.psm'></span><span id='topic+Mean.psm'></span><span id='topic+Quantile.psm'></span><span id='topic+Survival.psm'></span><span id='topic+residuals.psm'></span><span id='topic+lines.residuals.psm.censored.normalized'></span><span id='topic+survplot.residuals.psm.censored.normalized'></span>

<h3>Description</h3>

<p><code>psm</code> is a modification of Therneau's <code>survreg</code> function for
fitting the accelerated failure time family of parametric survival
models.  <code>psm</code> uses the <code>rms</code> class for automatic
<code>anova</code>, <code>fastbw</code>, <code>calibrate</code>, <code>validate</code>, and
other functions.  <code>Hazard.psm</code>, <code>Survival.psm</code>,
<code>Quantile.psm</code>, and <code>Mean.psm</code> create S functions that
evaluate the hazard, survival, quantile, and mean (expected value)
functions analytically, as functions of time or probabilities and the
linear predictor values.  The Nagelkerke R^2 and and adjusted
Maddala-Cox-Snell R^2 are computed.  For the latter the notation is
R2(p,m) where p is the number of regression coefficients being
adjusted for and m is the effective sample size (number of uncensored
observations).  See <code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code> for more information.
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>. 
</p>
<p>The <code>residuals.psm</code> function exists mainly to compute normalized
(standardized) residuals and to censor them (i.e., return them as
<code>Surv</code> objects) just as the original failure time variable was
censored.  These residuals are useful for checking the underlying
distributional assumption (see the examples).  To get these residuals,
the fit must have specified <code>y=TRUE</code>.  A <code>lines</code> method for these
residuals automatically draws a curve with the assumed standardized
survival distribution.  A <code>survplot</code> method runs the standardized
censored residuals through <code>npsurv</code> to get Kaplan-Meier estimates,
with optional stratification (automatically grouping a continuous
variable into quantiles) and then through <code>survplot.npsurv</code> to plot
them.  Then <code>lines</code> is invoked to show the theoretical curve.  Other
types of residuals are computed by <code>residuals</code> using
<code>residuals.survreg</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psm(formula,
    data=environment(formula), weights,
    subset, na.action=na.delete, dist="weibull",
    init=NULL, scale=0, 
    control=survreg.control(),
    parms=NULL, 
    model=FALSE, x=FALSE, y=TRUE, time.inc, ...)

## S3 method for class 'psm'
print(x, correlation=FALSE, digits=4, r2=c(0,2,4), coefs=TRUE,
pg=FALSE, title, ...) 

Hazard(object, ...)
## S3 method for class 'psm'
Hazard(object, ...)   # for psm fit
# E.g. lambda &lt;- Hazard(fit)

Survival(object, ...)
## S3 method for class 'psm'
Survival(object, ...) # for psm
# E.g. survival &lt;- Survival(fit)

## S3 method for class 'psm'
Quantile(object, ...) # for psm
# E.g. quantsurv &lt;- Quantile(fit)

## S3 method for class 'psm'
Mean(object, ...)     # for psm
# E.g. meant   &lt;- Mean(fit)

# lambda(times, lp)   # get hazard function at t=times, xbeta=lp
# survival(times, lp) # survival function at t=times, lp
# quantsurv(q, lp)    # quantiles of survival time
# meant(lp)           # mean survival time

## S3 method for class 'psm'
residuals(object, type=c("censored.normalized",
"response", "deviance", "dfbeta",
"dfbetas", "working", "ldcase", "ldresp", "ldshape", "matrix", "score"), ...)

## S3 method for class 'residuals.psm.censored.normalized'
survplot(fit, x, g=4, col, main, ...)

## S3 method for class 'residuals.psm.censored.normalized'
lines(x, n=100, lty=1, xlim,
lwd=3, ...)
# for type="censored.normalized"
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psm_+3A_formula">formula</code></td>
<td>

<p>an S statistical model formula. Interactions up to third order are
supported. The left hand side must be a <code>Surv</code> object.
</p>
</td></tr>
<tr><td><code id="psm_+3A_object">object</code></td>
<td>
<p>a fit created by <code>psm</code>.  For <code>survplot</code> with
residuals from <code>psm</code>, <code>object</code> is the result of
<code>residuals.psm</code>.
</p>
</td></tr>
<tr><td><code id="psm_+3A_fit">fit</code></td>
<td>
<p>a fit created by <code>psm</code></p>
</td></tr>
<tr><td><code id="psm_+3A_data">data</code>, <code id="psm_+3A_subset">subset</code>, <code id="psm_+3A_weights">weights</code>, <code id="psm_+3A_dist">dist</code>, <code id="psm_+3A_scale">scale</code>, <code id="psm_+3A_init">init</code>, <code id="psm_+3A_na.action">na.action</code>, <code id="psm_+3A_control">control</code></td>
<td>
<p>see <code>survreg</code>.</p>
</td></tr>
<tr><td><code id="psm_+3A_parms">parms</code></td>
<td>
<p>a list of fixed parameters.  For the <code class="reqn">t</code>-distribution
this is the degrees of freedom; most of the distributions have no
parameters.</p>
</td></tr>
<tr><td><code id="psm_+3A_model">model</code></td>
<td>

<p>set to <code>TRUE</code> to include the model frame in the returned object
</p>
</td></tr>
<tr><td><code id="psm_+3A_x">x</code></td>
<td>

<p>set to <code>TRUE</code> to include the design matrix in the object produced
by <code>psm</code>.  For the <code>survplot</code> method, <code>x</code> is an optional
stratification variable (character, numeric, or categorical).  For
<code>lines.residuals.psm.censored.normalized</code>, <code>x</code> is the result
of <code>residuals.psm</code>.  For <code>print</code> it is the result of <code>psm</code>.
</p>
</td></tr>
<tr><td><code id="psm_+3A_y">y</code></td>
<td>

<p>set to <code>TRUE</code> to include the <code>Surv()</code> matrix
</p>
</td></tr>
<tr><td><code id="psm_+3A_time.inc">time.inc</code></td>
<td>

<p>setting for default time spacing. Used in constructing time axis
in <code>survplot</code>, and also in make confidence bars. Default is 30
if time variable has <code>units="Day"</code>, 1 otherwise, unless
maximum follow-up time <code class="reqn">&lt; 1</code>. Then max time/10 is used as <code>time.inc</code>.
If <code>time.inc</code> is not given and max time/default <code>time.inc</code> is
<code class="reqn">&gt; 25</code>, <code>time.inc</code> is increased.
</p>
</td></tr>
<tr><td><code id="psm_+3A_correlation">correlation</code></td>
<td>
<p>set to <code>TRUE</code> to print the correlation matrix
for parameter estimates</p>
</td></tr>
<tr><td><code id="psm_+3A_digits">digits</code></td>
<td>
<p>number of places to print to the right of the decimal
point</p>
</td></tr>
<tr><td><code id="psm_+3A_r2">r2</code></td>
<td>
<p>vector of integers specifying which R^2 measures to print,
with 0 for Nagelkerke R^2 and 1:4 corresponding to the 4 measures
computed by <code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>.  Default is to print
Nagelkerke (labeled R2) and second and fourth <code>R2Measures</code>
which are the measures adjusted for the number of predictors, first
for the raw sample size then for the effective sample size, which
here is the number of uncensored observations.</p>
</td></tr>
<tr><td><code id="psm_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="psm_+3A_pg">pg</code></td>
<td>
<p>set to <code>TRUE</code> to print g-indexes</p>
</td></tr>
<tr><td><code id="psm_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="psm_+3A_...">...</code></td>
<td>

<p>other arguments to fitting routines, or to pass to <code>survplot</code> from
<br />
<code>survplot.residuals.psm.censored.normalized</code>.  Passed to the
generic <code>lines</code> function for <code>lines</code>.</p>
</td></tr>
<tr><td><code id="psm_+3A_times">times</code></td>
<td>

<p>a scalar or vector of times for which to evaluate survival probability
or hazard
</p>
</td></tr>
<tr><td><code id="psm_+3A_lp">lp</code></td>
<td>

<p>a scalar or vector of linear predictor values at which to evaluate
survival probability or hazard.  If both <code>times</code> and <code>lp</code> are
vectors, they must be of the same length.
</p>
</td></tr>
<tr><td><code id="psm_+3A_q">q</code></td>
<td>

<p>a scalar or vector of probabilities.  The default is .5, so just the
median survival time is returned.  If <code>q</code> and <code>lp</code> are both vectors,
a matrix of quantiles is returned, with rows corresponding to <code>lp</code>
and columns to <code>q</code>.
</p>
</td></tr>
<tr><td><code id="psm_+3A_type">type</code></td>
<td>

<p>type of residual desired.  Default is censored normalized residuals,
defined as (link(Y) - linear.predictors)/scale parameter, where the
link function was usually the log function.  See <code>survreg</code> for other
types.  <code>type="score"</code> returns the score residual matrix.
</p>
</td></tr>
<tr><td><code id="psm_+3A_n">n</code></td>
<td>

<p>number of points to evaluate theoretical standardized survival
function for 
<br />
<code>lines.residuals.psm.censored.normalized</code>
</p>
</td></tr>
<tr><td><code id="psm_+3A_lty">lty</code></td>
<td>

<p>line type for <code>lines</code>, default is 1
</p>
</td></tr>
<tr><td><code id="psm_+3A_xlim">xlim</code></td>
<td>

<p>range of times (or transformed times) for which to evaluate the standardized
survival function.  Default is range in normalized residuals.
</p>
</td></tr>
<tr><td><code id="psm_+3A_lwd">lwd</code></td>
<td>

<p>line width for theoretical distribution, default is 3
</p>
</td></tr>
<tr><td><code id="psm_+3A_g">g</code></td>
<td>

<p>number of quantile groups to use for stratifying continuous variables
having more than 5 levels
</p>
</td></tr>
<tr><td><code id="psm_+3A_col">col</code></td>
<td>

<p>vector of colors for <code>survplot</code> method, corresponding to levels of <code>x</code>
(must be a scalar if there is no <code>x</code>)
</p>
</td></tr>
<tr><td><code id="psm_+3A_main">main</code></td>
<td>

<p>main plot title for <code>survplot</code>.  If omitted, is the name or label of
<code>x</code> if <code>x</code> is given.  Use <code>main=""</code> to suppress a title when you
specify <code>x</code>.
</p>
</td></tr></table>


<h3>Details</h3>

<p>The object <code>survreg.distributions</code> contains definitions of properties
of the various survival distributions. 
<br />
<code>psm</code> does not trap singularity errors due to the way <code>survreg.fit</code>
does matrix inversion.  It will trap non-convergence (thus returning
<code>fit$fail=TRUE</code>) if you give the argument <code>failure=2</code> inside the
<code>control</code> list which is passed to <code>survreg.fit</code>.  For example, use
<code>f &lt;- psm(S ~ x, control=list(failure=2, maxiter=20))</code> to allow up to
20 iterations and to set <code>f$fail=TRUE</code> in case of non-convergence.
This is especially useful in simulation work.
</p>


<h3>Value</h3>

<p><code>psm</code> returns a fit object with all the information <code>survreg</code> would store as 
well as what <code>rms</code> stores and <code>units</code> and <code>time.inc</code>.
<code>Hazard</code>, <code>Survival</code>, and <code>Quantile</code> return S-functions.
<code>residuals.psm</code> with <code>type="censored.normalized"</code> returns a
<code>Surv</code> object which has a special attribute <code>"theoretical"</code>
which is used by the <code>lines</code> 
routine.  This is the assumed standardized survival function as a function
of time or transformed time.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="survival.html#topic+survreg">survreg</a></code>,
<code><a href="survival.html#topic+residuals.survreg">residuals.survreg</a></code>, <code><a href="survival.html#topic+survreg.object">survreg.object</a></code>,  
<code><a href="survival.html#topic+survreg.distributions">survreg.distributions</a></code>,
<code><a href="#topic+pphsm">pphsm</a></code>, <code><a href="#topic+survplot">survplot</a></code>, <code><a href="#topic+survest">survest</a></code>,
<code><a href="survival.html#topic+Surv">Surv</a></code>, 
<code><a href="Hmisc.html#topic+na.delete">na.delete</a></code>,
<code><a href="Hmisc.html#topic+na.detail.response">na.detail.response</a></code>, <code><a href="#topic+datadist">datadist</a></code>,
<code><a href="#topic+latex.psm">latex.psm</a></code>, <code><a href="Hmisc.html#topic+GiniMd">GiniMd</a></code>, <code><a href="#topic+prModFit">prModFit</a></code>,
<code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>, <code><a href="#topic+plot.Predict">plot.Predict</a></code>,
<code><a href="Hmisc.html#topic+R2Measures">R2Measures</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
n &lt;- 400
set.seed(1)
age &lt;- rnorm(n, 50, 12)
sex &lt;- factor(sample(c('Female','Male'),n,TRUE))
dd &lt;- datadist(age,sex)
options(datadist='dd')
# Population hazard function:
h &lt;- .02*exp(.06*(age-50)+.8*(sex=='Female'))
d.time &lt;- -log(runif(n))/h
cens &lt;- 15*runif(n)
death &lt;- ifelse(d.time &lt;= cens,1,0)
d.time &lt;- pmin(d.time, cens)

f &lt;- psm(Surv(d.time,death) ~ sex*pol(age,2), 
         dist='lognormal')
# Log-normal model is a bad fit for proportional hazards data
print(f, r2=0:4, pg=TRUE)

anova(f)
fastbw(f)  # if deletes sex while keeping age*sex ignore the result
f &lt;- update(f, x=TRUE,y=TRUE)       # so can validate, compute certain resids
validate(f, B=10)      # ordinarily use B=300 or more
plot(Predict(f, age, sex))   # needs datadist since no explicit age, hosp.
# Could have used ggplot(Predict(...))
survplot(f, age=c(20,60))     # needs datadist since hospital not set here
# latex(f)


S &lt;- Survival(f)
plot(f$linear.predictors, S(6, f$linear.predictors),
     xlab=expression(X*hat(beta)),
     ylab=expression(S(6,X*hat(beta))))
# plots 6-month survival as a function of linear predictor (X*Beta hat)


times &lt;- seq(0,24,by=.25)
plot(times, S(times,0), type='l')   # plots survival curve at X*Beta hat=0
lam &lt;- Hazard(f)
plot(times, lam(times,0), type='l') # similarly for hazard function


med &lt;- Quantile(f)        # new function defaults to computing median only
lp &lt;- seq(-3, 5, by=.1)
plot(lp, med(lp=lp), ylab="Median Survival Time")
med(c(.25,.5), f$linear.predictors)
                          # prints matrix with 2 columns


# fit a model with no predictors
f &lt;- psm(Surv(d.time,death) ~ 1, dist="weibull")
f
pphsm(f)          # print proportional hazards form
g &lt;- survest(f)
plot(g$time, g$surv, xlab='Time', type='l',
     ylab=expression(S(t)))


f &lt;- psm(Surv(d.time,death) ~ age, 
         dist="loglogistic", y=TRUE)
r &lt;- resid(f, 'cens') # note abbreviation
survplot(npsurv(r ~ 1), conf='none') 
                      # plot Kaplan-Meier estimate of 
                      # survival function of standardized residuals
survplot(npsurv(r ~ cut2(age, g=2)), conf='none')  
                      # both strata should be n(0,1)
lines(r)              # add theoretical survival function
#More simply:
survplot(r, age, g=2)

options(datadist=NULL)
</code></pre>

<hr>
<h2 id='residuals.cph'>Residuals for a cph Fit</h2><span id='topic+residuals.cph'></span>

<h3>Description</h3>

<p>Calculates martingale, deviance, score or Schoenfeld residuals 
(scaled or unscaled) or influence statistics for a
Cox proportional hazards model. This is a slightly modified version
of Therneau's <code>residuals.coxph</code> function. It assumes that <code>x=TRUE</code> and
<code>y=TRUE</code> were specified to <code>cph</code>, except for martingale
residuals, which are stored with the fit by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cph'
residuals(object,
      type=c("martingale", "deviance", "score", "schoenfeld", 
             "dfbeta", "dfbetas", "scaledsch", "partial"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.cph_+3A_object">object</code></td>
<td>
<p>a <code>cph</code> object</p>
</td></tr>
<tr><td><code id="residuals.cph_+3A_type">type</code></td>
<td>

<p>character string indicating the type of residual desired;
the default is martingale.
Only enough of the string to determine a unique match is required.
Instead of the usual residuals, <code>type="dfbeta"</code> may be specified
to obtain approximate leave-out-one <code class="reqn">\Delta \beta</code>s.  Use
<code>type="dfbetas"</code> to normalize the <code class="reqn">\Delta \beta</code>s for
the standard errors of the regression coefficient estimates.
Scaled Schoenfeld residuals (<code>type="scaledsch"</code>, Grambsch and
Therneau, 1993) better 
reflect the log hazard ratio function than ordinary Schoenfeld
residuals, and they are on the regression coefficient scale.  
The weights use Grambsch and Therneau's &quot;average variance&quot; method.
</p>
</td></tr>
<tr><td><code id="residuals.cph_+3A_...">...</code></td>
<td>
<p>see <code><a href="survival.html#topic+residuals.coxph">residuals.coxph</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned will be a vector for martingale and deviance 
residuals and matrices for score and schoenfeld residuals, dfbeta, or dfbetas.
There will
be one row of residuals for each row in the input data (without <code>collapse</code>).
One column of score and Schoenfeld
residuals will be returned for each column in the model.matrix.
The scaled Schoenfeld residuals are used in the <code>cox.zph</code> function.
</p>
<p>The score residuals are each individual's contribution to the score
vector.  Two transformations of this are often more useful: <code>dfbeta</code> is
the approximate change in the coefficient vector if that observation
were dropped, and <code>dfbetas</code> is the approximate change in the coefficients,
scaled by the standard error for the coefficients.
</p>


<h3>References</h3>

<p>T. Therneau, P. Grambsch, and T.Fleming. &quot;Martingale based residuals
for survival models&quot;, Biometrika, March 1990.
</p>
<p>P. Grambsch, T. Therneau. &quot;Proportional hazards tests and diagnostics
based on weighted residuals&quot;, unpublished manuscript, Feb 1993.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cph">cph</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="survival.html#topic+residuals.coxph">residuals.coxph</a></code>, <code><a href="survival.html#topic+cox.zph">cox.zph</a></code>, <code><a href="stats.html#topic+naresid">naresid</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># fit &lt;- cph(Surv(start, stop, event) ~ (age + surgery)* transplant, 
#            data=jasa1)
# mresid &lt;- resid(fit, collapse=jasa1$id)


# Get unadjusted relationships for several variables
# Pick one variable that's not missing too much, for fit

require(survival)
n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
cens   &lt;- 15*runif(n)
h      &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
d.time &lt;- -log(runif(n))/h
death  &lt;- ifelse(d.time &lt;= cens,1,0)
d.time &lt;- pmin(d.time, cens)


f &lt;- cph(Surv(d.time, death) ~ age + blood.pressure + cholesterol, iter.max=0)
res &lt;- resid(f) # This re-inserts rows for NAs, unlike f$resid
yl &lt;- quantile(res, c(10/length(res),1-10/length(res)), na.rm=TRUE)
# Scale all plots from 10th smallest to 10th largest residual
par(mfrow=c(2,2), oma=c(3,0,3,0))
p &lt;- function(x) {
  s &lt;- !is.na(x+res)
  plot(lowess(x[s], res[s], iter=0), xlab=label(x), ylab="Residual",
       ylim=yl, type="l")
}
p(age); p(blood.pressure); p(cholesterol)
mtext("Smoothed Martingale Residuals", outer=TRUE)


# Assess PH by estimating log relative hazard over time
f &lt;- cph(Surv(d.time,death) ~ age + sex + blood.pressure, x=TRUE, y=TRUE)
r &lt;- resid(f, "scaledsch")
tt &lt;- as.numeric(dimnames(r)[[1]])
par(mfrow=c(3,2))
for(i in 1:3) {
  g &lt;- areg.boot(I(r[,i]) ~ tt, B=20)
  plot(g, boot=FALSE)  # shows bootstrap CIs
}                  # Focus on 3 graphs on right
# Easier approach:
plot(cox.zph(f))    # invokes plot.cox.zph
par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='residuals.Glm'>residuals.Glm</h2><span id='topic+residuals.Glm'></span>

<h3>Description</h3>

<p>Residuals for 'Glm'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Glm'
residuals(object, type, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.Glm_+3A_object">object</code></td>
<td>
<p>a fit object produced by 'Glm'</p>
</td></tr>
<tr><td><code id="residuals.Glm_+3A_type">type</code></td>
<td>
<p>either ''score'' or a 'type' accepted by 'residuals.glm'</p>
</td></tr>
<tr><td><code id="residuals.Glm_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function mainly passes through to &lsquo;residuals.glm' but for 'type=&rsquo;score'' computes the matrix of score residuals using code modified from 'sandwich::estfun.glm'.
</p>


<h3>Value</h3>

<p>a vector or matrix
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='residuals.lrm'>Residuals from an <code>lrm</code> or <code>orm</code> Fit</h2><span id='topic+residuals.lrm'></span><span id='topic+residuals.orm'></span><span id='topic+plot.lrm.partial'></span>

<h3>Description</h3>

<p>For a binary logistic model fit, computes the following residuals, letting
<code class="reqn">P</code> denote the predicted probability of the higher category of <code class="reqn">Y</code>,
<code class="reqn">X</code> denote the design matrix (with a column of 1s for the intercept), and
<code class="reqn">L</code> denote the logit or linear predictors: ordinary or Li-Shepherd
(<code class="reqn">Y-P</code>), score (<code class="reqn">X (Y-P)</code>), pearson (<code class="reqn">(Y-P)/\sqrt{P(1-P)}</code>),
deviance (for <code class="reqn">Y=0</code> is <code class="reqn">-\sqrt{2|\log(1-P)|}</code>, for <code class="reqn">Y=1</code> is
<code class="reqn">\sqrt{2|\log(P)|}</code>, pseudo dependent variable used in influence
statistics  (<code class="reqn">L + (Y-P)/(P(1-P))</code>), and partial (<code class="reqn">X_{i}\beta_{i}
	+ (Y-P)/(P(1-P))</code>). 
</p>
<p>Will compute all these residuals for an ordinal logistic model, using
as temporary binary responses dichotomizations of <code class="reqn">Y</code>, along with
the corresponding <code class="reqn">P</code>, the probability that <code class="reqn">Y \geq</code> cutoff.  For
<code>type="partial"</code>, all 
possible dichotomizations are used, and for <code>type="score"</code>, the actual
components of the first derivative of the log likelihood are used for
an ordinal model.  For <code>type="li.shepherd"</code> the residual is
<code class="reqn">Pr(W &lt; Y) - Pr(W &gt; Y)</code> where Y is the observed response and W is a
random variable from the fitted distribution.
Alternatively, specify <code>type="score.binary"</code>
to use binary model score residuals but for all cutpoints of <code class="reqn">Y</code>
(plotted only, not returned). The <code>score.binary</code>, 
<code>partial</code>, and perhaps <code>score</code> residuals are useful for
checking the proportional odds assumption. 
If the option <code>pl=TRUE</code> is used to plot the <code>score</code> or
<code>score.binary</code> residuals, a score residual plot is made for each
column of the design (predictor) matrix, with <code>Y</code> cutoffs on the 
x-axis and the mean +- 1.96 standard errors of the score residuals on
the y-axis.  You can instead use a box plot to display these residuals,
for both <code>score.binary</code> and <code>score</code>.
Proportional odds dictates a horizontal <code>score.binary</code> plot.  Partial
residual plots use smooth nonparametric estimates, separately for each
cutoff of <code class="reqn">Y</code>.  One examines that plot for parallelism of the curves
to check the proportional odds assumption, as well as to see if the
predictor behaves linearly. 
</p>
<p>Also computes a variety of influence statistics and the 
le Cessie - van Houwelingen - Copas - Hosmer unweighted sum of squares test
for global goodness of fit, done separately for each cutoff of <code class="reqn">Y</code> in the
case of an ordinal model.
</p>
<p>The <code>plot.lrm.partial</code> function computes partial residuals for a series
of binary logistic model fits that all used the same predictors and that
specified <code>x=TRUE, y=TRUE</code>.  It then computes smoothed partial residual
relationships (using <code>lowess</code> with <code>iter=0</code>) and plots them separately
for each predictor, with residual plots from all model fits shown on the
same plot for that predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lrm'
residuals(object, type=c("li.shepherd","ordinary",
 "score", "score.binary", "pearson", "deviance", "pseudo.dep",
 "partial", "dfbeta", "dfbetas", "dffit", "dffits", "hat", "gof", "lp1"),
           pl=FALSE, xlim, ylim, kint, label.curves=TRUE, which, ...)
## S3 method for class 'orm'
residuals(object, type=c("li.shepherd","ordinary",
 "score", "score.binary", "pearson", "deviance", "pseudo.dep",
 "partial", "dfbeta", "dfbetas", "dffit", "dffits", "hat", "gof", "lp1"),
           pl=FALSE, xlim, ylim, kint, label.curves=TRUE, which, ...)

## S3 method for class 'lrm.partial'
plot(..., labels, center=FALSE, ylim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.lrm_+3A_object">object</code></td>
<td>
<p>object created by <code>lrm</code> or <code>orm</code></p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_...">...</code></td>
<td>

<p>for <code>residuals</code>, applies to <code>type="partial"</code>  when <code>pl</code>
is not <code>FALSE</code>.  These are extra arguments passed to the smoothing
function.  Can also be used to pass extra arguments to <code>boxplot</code>
for <code>type="score"</code> or <code>"score.binary"</code>. 
For <code>plot.lrm.partial</code> this specifies a series of binary model fit
objects.
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_type">type</code></td>
<td>

<p>type of residual desired.  Use <code>type="lp1"</code> to get approximate leave-out-1
linear predictors, derived by subtracting the <code>dffit</code> from the original
linear predictor values.
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_pl">pl</code></td>
<td>

<p>applies only to <code>type="partial"</code>, <code>"score"</code>, and
<code>"score.binary"</code>.  For score residuals in an ordinal model, set
<code>pl=TRUE</code> to get means and approximate 0.95 confidence bars
vs. <code class="reqn">Y</code>, separately for each <code class="reqn">X</code>.  Alternatively, specify
<code>pl="boxplot"</code> to use <code>boxplot</code> to draw the plot, with notches
and with width proportional to the square root of the cell sizes.  For
partial residuals, set <code>pl=TRUE</code> (which uses <code>lowess</code>) or
<code>pl="supsmu"</code> to get smoothed partial residual plots for all
columns of <code class="reqn">X</code> using <code>supsmu</code>. Use <code>pl="loess"</code> to use
<code>loess</code> and get confidence bands (<code>"loess"</code> is not implemented
for ordinal responses).  Under R, <code>pl="loess"</code> uses <code>lowess</code>
and does not provide confidence bands.  If there is more than one <code class="reqn">X</code>,
you should probably use <code>par(mfrow=c( , ))</code> before calling <code>resid</code>.
Note that <code>pl="loess"</code> results in <code>plot.loess</code> being called, which
requires a large memory allocation.
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_xlim">xlim</code></td>
<td>

<p>plotting range for x-axis (default = whole range of predictor)
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_ylim">ylim</code></td>
<td>

<p>plotting range for y-axis (default = whole range of residuals, range of
all confidence intervals for <code>score</code> or <code>score.binary</code> or
range of all smoothed curves for <code>partial</code> if <code>pl=TRUE</code>, or
0.1 and 0.9 quantiles of the residuals for <code>pl="boxplot"</code>.) 
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_kint">kint</code></td>
<td>

<p>for an ordinal model for residuals other than <code>li.shepherd</code>,
<code>partial</code>, <code>score</code>, or <code>score.binary</code>, specifies
the intercept (and the cutoff of <code class="reqn">Y</code>) to use for the calculations.
Specifying <code>kint=2</code>, for example, means to use <code class="reqn">Y \geq</code> 3rd level.
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_label.curves">label.curves</code></td>
<td>

<p>set to <code>FALSE</code> to suppress curve labels when <code>type="partial"</code>.
The default, <code>TRUE</code>, causes <code>labcurve</code> to be invoked to label
curves where they are most separated.  <code>label.curves</code> can be a list
containing the <code>opts</code> parameter for <code>labcurve</code>, to send
options to <code>labcurve</code>, such as <code>tilt</code>.  The default for
<code>tilt</code> here is <code>TRUE</code>. 
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_which">which</code></td>
<td>

<p>a vector of integers specifying column numbers of the design matrix for
which to compute or plot residuals, for
<code>type="partial","score","score.binary"</code>.  
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_labels">labels</code></td>
<td>

<p>for <code>plot.lrm.partial</code> this specifies a vector of character strings 
providing labels for the list of binary fits.  By default, the names of
the fit objects are used as labels.  The <code>labcurve</code> function is used
to label the curve with the <code>labels</code>.
</p>
</td></tr>
<tr><td><code id="residuals.lrm_+3A_center">center</code></td>
<td>

<p>for <code>plot.lrm.partial</code> this causes partial residuals for every
model to have a mean of zero before smoothing and plotting 
</p>
</td></tr></table>


<h3>Details</h3>

<p>For the goodness-of-fit test, the le Cessie-van Houwelingen normal test
statistic for the unweighted sum of squared errors (Brier score times <code class="reqn">n</code>)
is used.  For an ordinal response variable, the test 
for predicting the probability that <code class="reqn">Y\geq j</code> is done separately for
all <code class="reqn">j</code> (except the first).  Note that the test statistic can have
strange behavior  (i.e., it is far too large) if the model has no
predictive value. 
</p>
<p>For most of the values of <code>type</code>, you must have specified
<code>x=TRUE, y=TRUE</code> to <code>lrm</code> or <code>orm</code>.
</p>
<p>There is yet no literature on interpreting score residual plots for the
ordinal model.  Simulations when proportional odds is satisfied have
still shown a U-shaped residual plot.  The series of binary model score
residuals for all cutoffs of <code class="reqn">Y</code> seems to better check the assumptions.
See the examples.
</p>
<p>The li.shepherd residual is a single value per observation on the
probability scale and can be useful for examining linearity, checking
for outliers, and measuring residual correlation.
</p>


<h3>Value</h3>

<p>a matrix (<code>type="partial","dfbeta","dfbetas","score"</code>), 
test statistic (<code>type="gof"</code>), or a vector otherwise.  
For partial residuals from an ordinal
model, the returned object is a 3-way array (rows of <code class="reqn">X</code> by columns
of <code class="reqn">X</code> by cutoffs of <code class="reqn">Y</code>), and NAs deleted during the fit
are not re-inserted into the residuals.  For <code>score.binary</code>, nothing
is returned.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Landwehr, Pregibon, Shoemaker. JASA 79:61&ndash;83, 1984.
</p>
<p>le Cessie S, van Houwelingen JC. Biometrics 47:1267&ndash;1282, 1991.
</p>
<p>Hosmer DW, Hosmer T, Lemeshow S, le Cessie S, Lemeshow S.  A
comparison of goodness-of-fit tests for the logistic regression model.
Stat in Med 16:965&ndash;980, 1997.
</p>
<p>Copas JB.  Applied Statistics 38:71&ndash;80, 1989.
</p>
<p>Li C, Shepherd BE.  Biometrika 99:473-480, 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+orm">orm</a></code>,
<code><a href="stats.html#topic+naresid">naresid</a></code>, <code><a href="#topic+which.influence">which.influence</a></code>,
<code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="stats.html#topic+supsmu">supsmu</a></code>, <code><a href="stats.html#topic+lowess">lowess</a></code>,
<code><a href="graphics.html#topic+boxplot">boxplot</a></code>, <code><a href="Hmisc.html#topic+labcurve">labcurve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- runif(200, -1, 1)
x2 &lt;- runif(200, -1, 1)
L  &lt;- x1^2 - .5 + x2
y  &lt;- ifelse(runif(200) &lt;= plogis(L), 1, 0)
f &lt;- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
resid(f)            #add rows for NAs back to data
resid(f, "score")   #also adds back rows
r &lt;- resid(f, "partial")  #for checking transformations of X's
par(mfrow=c(1,2))
for(i in 1:2) {
  xx &lt;- if(i==1)x1 else x2
  plot(xx, r[,i], xlab=c('x1','x2')[i])
  lines(lowess(xx,r[,i]))
}
resid(f, "partial", pl="loess")  #same as last 3 lines
resid(f, "partial", pl=TRUE) #plots for all columns of X using supsmu
resid(f, "gof")           #global test of goodness of fit
lp1 &lt;- resid(f, "lp1")    #approx. leave-out-1 linear predictors
-2*sum(y*lp1 + log(1-plogis(lp1)))  #approx leave-out-1 deviance
                                    #formula assumes y is binary


# Simulate data from a population proportional odds model
set.seed(1)
n   &lt;- 400
age &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
L &lt;- .05*(age-50) + .03*(blood.pressure-120)
p12 &lt;- plogis(L)    # Pr(Y&gt;=1)
p2  &lt;- plogis(L-1)  # Pr(Y=2)
p   &lt;- cbind(1-p12, p12-p2, p2)   # individual class probabilites
# Cumulative probabilities:
cp  &lt;- matrix(cumsum(t(p)) - rep(0:(n-1), rep(3,n)), byrow=TRUE, ncol=3)
# simulate multinomial with varying probs:
y &lt;- (cp &lt; runif(n)) %*% rep(1,3)
y &lt;- as.vector(y)
# Thanks to Dave Krantz for this trick
f &lt;- lrm(y ~ age + blood.pressure, x=TRUE, y=TRUE)
par(mfrow=c(2,2))
resid(f, 'score.binary',   pl=TRUE)              #plot score residuals
resid(f, 'partial', pl=TRUE)                     #plot partial residuals
resid(f, 'gof')           #test GOF for each level separately


# Show use of Li-Shepherd residuals
f.wrong &lt;- lrm(y ~ blood.pressure, x=TRUE, y=TRUE)
par(mfrow=c(2,1))
# li.shepherd residuals from model without age
plot(age, resid(f.wrong, type="li.shepherd"),
     ylab="li.shepherd residual")
lines(lowess(age, resid(f.wrong, type="li.shepherd")))
# li.shepherd residuals from model including age
plot(age, resid(f, type="li.shepherd"),
     ylab="li.shepherd residual")
lines(lowess(age, resid(f, type="li.shepherd")))


# Make a series of binary fits and draw 2 partial residual plots
#
f1 &lt;- lrm(y&gt;=1 ~ age + blood.pressure, x=TRUE, y=TRUE)
f2  &lt;- update(f1, y==2 ~.)
par(mfrow=c(2,1))
plot.lrm.partial(f1, f2)


# Simulate data from both a proportional odds and a non-proportional
# odds population model.  Check how 3 kinds of residuals detect
# non-prop. odds
set.seed(71)
n &lt;- 400
x &lt;- rnorm(n)

par(mfrow=c(2,3))
for(j in 1:2) {     # 1: prop.odds   2: non-prop. odds
  if(j==1) 
    L &lt;- matrix(c(1.4,.4,-.1,-.5,-.9),
                nrow=n, ncol=5, byrow=TRUE) + x / 2
    else {
	  # Slopes and intercepts for cutoffs of 1:5 :
	  slopes &lt;- c(.7,.5,.3,.3,0)
	  ints   &lt;- c(2.5,1.2,0,-1.2,-2.5)
      L &lt;- matrix(ints,   nrow=n, ncol=5, byrow=TRUE) +
           matrix(slopes, nrow=n, ncol=5, byrow=TRUE) * x
    }
  p &lt;- plogis(L)
  # Cell probabilities
  p &lt;- cbind(1-p[,1],p[,1]-p[,2],p[,2]-p[,3],p[,3]-p[,4],p[,4]-p[,5],p[,5])
  # Cumulative probabilities from left to right
  cp  &lt;- matrix(cumsum(t(p)) - rep(0:(n-1), rep(6,n)), byrow=TRUE, ncol=6)
  y   &lt;- (cp &lt; runif(n)) %*% rep(1,6)


  f &lt;- lrm(y ~ x, x=TRUE, y=TRUE)
  for(cutoff in 1:5) print(lrm(y &gt;= cutoff ~ x)$coef)


  print(resid(f,'gof'))
  resid(f, 'score', pl=TRUE)
  # Note that full ordinal model score residuals exhibit a
  # U-shaped pattern even under prop. odds
  ti &lt;- if(j==2) 'Non-Proportional Odds\nSlopes=.7 .5 .3 .3 0' else
    'True Proportional Odds\nOrdinal Model Score Residuals'
  title(ti)
  resid(f, 'score.binary', pl=TRUE)
  if(j==1) ti &lt;- 'True Proportional Odds\nBinary Score Residuals'
  title(ti)
  resid(f, 'partial', pl=TRUE)
  if(j==1) ti &lt;- 'True Proportional Odds\nPartial Residuals'
  title(ti)
}
par(mfrow=c(1,1))

# Shepherd-Li residuals from orm.  Thanks: Qi Liu

set.seed(3)
n  &lt;- 100
x1 &lt;- rnorm(n)
y  &lt;- x1 + rnorm(n)
g &lt;- orm(y ~ x1, family=probit, x=TRUE, y=TRUE)
g.resid &lt;- resid(g)
plot(x1, g.resid, cex=0.4); lines(lowess(x1, g.resid)); abline(h=0, col=2,lty=2)

set.seed(3)
n &lt;- 100
x1 &lt;- rnorm(n)
y &lt;- x1 + x1^2 +rnorm(n)
# model misspecification, the square term is left out in the model
g &lt;- orm(y ~ x1, family=probit, x=TRUE, y=TRUE)
g.resid &lt;- resid(g)
plot(x1, g.resid, cex=0.4); lines(lowess(x1, g.resid)); abline(h=0, col=2,lty=2)


## Not run: 
# Get data used in Hosmer et al. paper and reproduce their calculations
v &lt;- Cs(id, low, age, lwt, race, smoke, ptl, ht, ui, ftv, bwt)
d &lt;- read.table("http://www.umass.edu/statdata/statdata/data/lowbwt.dat",
                skip=6, col.names=v)
d &lt;- upData(d, race=factor(race,1:3,c('white','black','other')))
f &lt;- lrm(low ~ age + lwt + race + smoke, data=d, x=TRUE,y=TRUE)
f
resid(f, 'gof')
# Their Table 7 Line 2 found sum of squared errors=36.91, expected
# value under H0=36.45, variance=.065, P=.071
# We got 36.90, 36.45, SD=.26055 (var=.068), P=.085
# Note that two logistic regression coefficients differed a bit
# from their Table 1

## End(Not run)
</code></pre>

<hr>
<h2 id='residuals.ols'>Residuals for ols</h2><span id='topic+residuals.ols'></span>

<h3>Description</h3>

<p>Computes various residuals and measures of influence for a
fit from <code>ols</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ols'
residuals(object, 
      type=c("ordinary", "score", "dfbeta", "dfbetas", 
             "dffit", "dffits", "hat", "hscore", "influence.measures",
             "studentized"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.ols_+3A_object">object</code></td>
<td>

<p>object created by <code>ols</code>.  Depending on <code>type</code>, you may have had to
specify <code>x=TRUE</code> to <code>ols</code>.
</p>
</td></tr>
<tr><td><code id="residuals.ols_+3A_type">type</code></td>
<td>

<p>type of residual desired.  <code>"ordinary"</code> refers to the usual residual.
<code>"score"</code> is the matrix of score residuals (contributions to first
derivative of log likelihood).
<code>dfbeta</code> and <code>dfbetas</code> mean respectively the raw and normalized matrix of changes in regression coefficients after
deleting in turn each observation.  The coefficients are normalized by their
standard errors.  <code>hat</code> contains the leverages &mdash; diagonals of the &ldquo;hat&rdquo; matrix.
<code>dffit</code> and <code>dffits</code> contain respectively the difference and normalized
difference in predicted values when each observation is omitted. 
The S <code>lm.influence</code> function is used.  When <code>type="hscore"</code>, the
ordinary residuals are divided by one minus the corresponding hat
matrix diagonal element to make residuals have equal variance.  When
<code>type="influence.measures"</code> the model is converted to an
<code>lm</code> model and <code>influence.measures(object)$infmat</code> is
returned.  This is a matrix with dfbetas for all predictors, dffit,
cov.r, Cook's d, and hat.  For <code>type="studentized"</code> studentized leave-out-one residuals are computed.
See the help file for <code>influence.measures</code> for more details.
</p>
</td></tr>
<tr><td><code id="residuals.ols_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix or vector, with places for observations that were originally
deleted by <code>ols</code> held by <code>NA</code>s 
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm.influence">lm.influence</a></code>, <code><a href="#topic+ols">ols</a></code>,
<code><a href="#topic+which.influence">which.influence</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
x1[1] &lt;- 100
y &lt;- x1 + x2 + rnorm(100)
f &lt;- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
resid(f, "dfbetas")
which.influence(f)
i &lt;- resid(f, 'influence.measures') # dfbeta, dffit, etc.
</code></pre>

<hr>
<h2 id='rexVar'>rexVar</h2><span id='topic+rexVar'></span>

<h3>Description</h3>

<p>Relative Explained Variation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rexVar(object, data, ns = 500, cint = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rexVar_+3A_object">object</code></td>
<td>
<p>a fit from <code>rms</code> or <code>rmsb</code></p>
</td></tr>
<tr><td><code id="rexVar_+3A_data">data</code></td>
<td>
<p>a data frame, data table, or list providing the predictors used in the original fit</p>
</td></tr>
<tr><td><code id="rexVar_+3A_ns">ns</code></td>
<td>
<p>maximum number of bootstrap repetitions or posterior draws to use</p>
</td></tr>
<tr><td><code id="rexVar_+3A_cint">cint</code></td>
<td>
<p>confidence interval coverage probability for nonparametric bootstrap percentile intervals, or probability for a Bayesian highest posterior density interval for the relative explained variations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes measures of relative explained variation for each predictor in an <code>rms</code> or <code>rmsb</code> model fit <code>object</code>.  This is similar to <code>plot(anova(fit), what='proportion R2')</code>.  For an <code>ols</code> model the result is exactly that.  Uncertainty intervals are computed if the model fit is from <code>rmsb</code> or was run through <code><a href="#topic+bootcov">bootcov()</a></code> with <code>coef.reps=TRUE</code>.  The results may be printed, and there is also a <code>plot</code> method.
</p>
<p>When <code>object</code> is not an <code>ols</code> fit, the linear predictor from the fit in <code>object</code> is predicted from the original predictors, resulting in a linear model with <code class="reqn">R^{2}=1.0</code>.  The partial <code class="reqn">R^2</code> for each predictor from a new <code>ols</code> fit is the relative explained variation.  The process is repeated when bootstrap coefficients repetitions or posterior draws are present, to get uncertainty intervals.  So relative explained variation is the proportion of variation in the initial model's predicted values (on the linear predictor scale) that is due to each predictor.
</p>
<p>Nonlinear and interaction terms are pooled with main linear effect of predictors, so relative explained variation for a predictor measures its total impact on predicted values, either as main effects or effect modifiers (interaction components).
</p>


<h3>Value</h3>

<p>a vector (if bootstrapping or Bayesian posterior sampling was not done) or a matrix otherwise, with rows corresponding to predictors and colums <code>REV</code>, <code>Lower</code>, <code>Upper</code>.  The returned object is of class <code>rexVar</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
x3 &lt;- rnorm(n)
y  &lt;- x1 + x2 + rnorm(n) / 2.
d &lt;- data.frame(x1, x2, x3, y)
dd &lt;- datadist(d); options(datadist='dd')
f  &lt;- ols(y ~ pol(x1, 2) * pol(x2, 2) + x3,
          data=d, x=TRUE, y=TRUE)
plot(anova(f), what='proportion R2', pl=FALSE)
rexVar(f)
g &lt;- bootcov(f, B=20, coef.reps=TRUE)
rexVar(g, data=d)
f &lt;- orm(y ~ pol(x1,2) * pol(x2, 2) + x3,
         data=d, x=TRUE, y=TRUE)
rexVar(f, data=d)
g &lt;- bootcov(f, B=20, coef.reps=TRUE)
rexVar(g, data=d)
## Not run: 
require(rmsb)
h &lt;- blrm(y ~ pol(x1,2) * pol(x2, 2) + x3, data=d)
rexVar(h, data=d)

## End(Not run)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='rms'>rms Methods and Generic Functions</h2><span id='topic+rms'></span><span id='topic+Design'></span><span id='topic+modelData'></span>

<h3>Description</h3>

<p>This is a series of special transformation functions (<code>asis</code>,
<code>pol</code>, <code>lsp</code>, <code>rcs</code>, <code>catg</code>, <code>scored</code>,
<code>strat</code>, <code>matrx</code>), fitting functions (e.g.,
<code>lrm</code>,<code>cph</code>, <code>psm</code>, or <code>ols</code>), and generic
analysis functions (<code>anova.rms</code>, <code>summary.rms</code>,
<code>Predict</code>,  <code>plot.Predict</code>, <code>ggplot.Predict</code>, <code>survplot</code>,
<code>fastbw</code>, <code>validate</code>, <code>calibrate</code>, <code>specs.rms</code>,
<code>which.influence</code>, <code>latexrms</code>, <code>nomogram</code>,
<code>datadist</code>, <code>gendata</code>)
that help automate many
analysis steps, e.g. fitting restricted interactions and multiple
stratification variables, analysis of variance (with tests of linearity
of each factor and pooled tests), plotting effects of variables in the
model, estimating and graphing effects of variables that appear non-linearly in the
model using e.g. inter-quartile-range hazard ratios, bootstrapping
model fits, and constructing nomograms for obtaining predictions manually. 
Behind the scene is the <code>Design</code> function which
stores extra attributes. <code>Design()</code> is not intended to be
called by users.  
<code>Design</code> causes detailed design attributes
and descriptions of the distribution of predictors to be stored 
in an attribute of the <code>terms</code> component called <code>Design</code>.
</p>
<p><code>modelData</code> is a replacement for <code>model.frame.default</code> that is
much streamlined and prepares data for <code>Design()</code>.  If a second
formula is present, <code>modelData</code> ensures that missing data deletions
are the same for both formulas, and produces a second model frame for
<code>formula2</code> as the <code>data2</code> attribute of the main returned data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelData(data=environment(formula), formula, formula2=NULL,
          weights, subset, na.action=na.delete, dotexpand=TRUE,
          callenv=parent.frame(n=2))

Design(mf, formula=NULL, specials=NULL, allow.offset=TRUE, intercept=1)
# not to be called by the user; called by fitting routines
# dist &lt;- datadist(x1,x2,sex,age,race,bp)   
# or dist &lt;- datadist(my.data.frame)
# Can omit call to datadist if not using summary.rms, Predict,
# survplot.rms, or if all variable settings are given to them
# options(datadist="dist")
# f &lt;- fitting.function(formula = y ~ rcs(x1,4) + rcs(x2,5) + x1%ia%x2 +
#                       rcs(x1,4)%ia%rcs(x2,5) +
#                       strat(sex)*age + strat(race)*bp)
# See rms.trans for rcs, strat, etc.
# %ia% is restricted interaction - not doubly nonlinear
# for x1 by x2 this uses the simple product only, but pools x1*x2
# effect with nonlinear function for overall tests
# specs(f)
# anova(f)
# summary(f)
# fastbw(f)
# pred &lt;- predict(f, newdata=expand.grid(x1=1:10,x2=3,sex="male",
#                 age=50,race="black"))
# pred &lt;- predict(f, newdata=gendata(f, x1=1:10, x2=3, sex="male"))
# This leaves unspecified variables set to reference values from datadist
# pred.combos &lt;- gendata(f, nobs=10)   # Use X-windows to edit predictor settings
# predict(f, newdata=pred.combos)
# plot(Predict(f, x1))  # or ggplot(...)
# latex(f)
# nomogram(f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rms_+3A_data">data</code></td>
<td>
<p>a data frame or calling environment</p>
</td></tr>
<tr><td><code id="rms_+3A_formula">formula</code></td>
<td>
<p>model formula</p>
</td></tr>
<tr><td><code id="rms_+3A_formula2">formula2</code></td>
<td>
<p>an optional second model formula (see for example
<code>ppo</code> in <code>blrm</code>)</p>
</td></tr>
<tr><td><code id="rms_+3A_weights">weights</code></td>
<td>
<p>a weight variable or expression</p>
</td></tr>
<tr><td><code id="rms_+3A_subset">subset</code></td>
<td>
<p>a subsetting expression evaluated in the calling frame
or <code>data</code></p>
</td></tr>
<tr><td><code id="rms_+3A_na.action">na.action</code></td>
<td>
<p>NA handling function, ideally one such as
<code>na.delete</code> that stores extra information about data omissions</p>
</td></tr>
<tr><td><code id="rms_+3A_specials">specials</code></td>
<td>
<p>a character vector specifying which function
evaluations appearing in <code>formula</code> are &quot;special&quot; in the
<code>model.frame</code> sense</p>
</td></tr>
<tr><td><code id="rms_+3A_dotexpand">dotexpand</code></td>
<td>
<p>set to <code>FALSE</code> to prevent . on right hand side
of model formula from expanding into all variables in <code>data</code>;
used for <code>cph</code></p>
</td></tr>
<tr><td><code id="rms_+3A_callenv">callenv</code></td>
<td>
<p>the parent frame that called the fitting function</p>
</td></tr>
<tr><td><code id="rms_+3A_mf">mf</code></td>
<td>
<p>a model frame</p>
</td></tr>
<tr><td><code id="rms_+3A_allow.offset">allow.offset</code></td>
<td>
<p>set to <code>TRUE</code> if model fitter allows an
offset term</p>
</td></tr>
<tr><td><code id="rms_+3A_intercept">intercept</code></td>
<td>
<p>1 if an ordinary intercept is present, 0 otherwise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame augmented with additional information about the
predictors and model formulation
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+rmsMisc">rmsMisc</a></code>, <code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+ols">ols</a></code>, <code><a href="#topic+specs.rms">specs.rms</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="#topic+summary.rms">summary.rms</a></code>, <code><a href="#topic+Predict">Predict</a></code>, <code><a href="#topic+gendata">gendata</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>.
<code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>, <code><a href="#topic+which.influence">which.influence</a></code>,
<code><a href="Hmisc.html#topic+latex">latex</a></code>, <code><a href="#topic+latexrms">latexrms</a></code>, <code><a href="stats.html#topic+model.frame.default">model.frame.default</a></code>, <code><a href="#topic+datadist">datadist</a></code>, <code><a href="Hmisc.html#topic+describe">describe</a></code>,
<code><a href="#topic+nomogram">nomogram</a></code>, <code><a href="#topic+vif">vif</a></code>, <code><a href="Hmisc.html#topic+dataRep">dataRep</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(rms)
require(ggplot2)
require(survival)
dist &lt;- datadist(data=2)     # can omit if not using summary, (gg)plot, survplot,
                             # or if specify all variable values to them. Can
                             # also  defer.  data=2: get distribution summaries
                             # for all variables in search position 2
                             # run datadist once, for all candidate variables
dist &lt;- datadist(age,race,bp,sex,height)   # alternative
options(datadist="dist")
f &lt;- cph(Surv(d.time, death) ~ rcs(age,4)*strat(race) +
         bp*strat(sex)+lsp(height,60),x=TRUE,y=TRUE)
anova(f)
anova(f,age,height)          # Joint test of 2 vars
fastbw(f)
summary(f, sex="female")     # Adjust sex to "female" when testing
                             # interacting factor bp
bplot(Predict(f, age, height))   # 3-D plot
ggplot(Predict(f, age=10:70, height=60))
latex(f)                     # LaTeX representation of fit


f &lt;- lm(y ~ x)               # Can use with any fitting function that
                             # calls model.frame.default, e.g. lm, glm
specs.rms(f)                 # Use .rms since class(f)="lm"
anova(f)                     # Works since Varcov(f) (=Varcov.lm(f)) works
fastbw(f)
options(datadist=NULL)
f &lt;- ols(y ~ x1*x2)          # Saves enough information to do fastbw, anova
anova(f)                     # Will not do Predict since distributions
fastbw(f)                    # of predictors not saved
plot(f, x1=seq(100,300,by=.5), x2=.5) 
                             # all values defined - don't need datadist
dist &lt;- datadist(x1,x2)      # Equivalent to datadist(f)
options(datadist="dist")
plot(f, x1, x2=.5)        # Now you can do plot, summary
plot(nomogram(f, interact=list(x2=c(.2,.7))))

## End(Not run)
</code></pre>

<hr>
<h2 id='rms-internal'>Internal rms functions</h2><span id='topic+annotateAnova'></span><span id='topic+coxphFit'></span><span id='topic+lm.pfit'></span><span id='topic+ols.influence'></span><span id='topic+plotmathAnova'></span><span id='topic+probabilityFamilies'></span><span id='topic+prType'></span><span id='topic+as.data.frame.rms'></span><span id='topic+survreg.auxinfo'></span><span id='topic+val.probg'></span>

<h3>Description</h3>

<p>Internal rms functions</p>


<h3>Details</h3>

<p>These are not to be called by the user or are undocumented.</p>

<hr>
<h2 id='rms.trans'>rms Special Transformation Functions</h2><span id='topic+rms.trans'></span><span id='topic+asis'></span><span id='topic+pol'></span><span id='topic+lsp'></span><span id='topic+rcs'></span><span id='topic+catg'></span><span id='topic+scored'></span><span id='topic+strat'></span><span id='topic+matrx'></span><span id='topic+gTrans'></span><span id='topic++25ia+25'></span><span id='topic+makepredictcall.rms'></span>

<h3>Description</h3>

<p>This is a series of functions (<code>asis</code>, <code>pol</code>, <code>lsp</code>,
<code>rcs</code>, <code>catg</code>, <code>scored</code>, <code>strat</code>, <code>matrx</code>,
<code>gTrans</code>, and
<code>%ia%</code>) that set up special attributes  (such as
knots and nonlinear term indicators) that are carried through to fits
(using for example <code>lrm</code>,<code>cph</code>, <code>ols</code>,
<code>psm</code>). <code>anova.rms</code>, <code>summary.rms</code>, <code>Predict</code>,
<code>survplot</code>, <code>fastbw</code>, <code>validate</code>, <code>specs</code>,
<code>which.influence</code>, <code>nomogram</code> and <code>latex.rms</code> use these 
attributes to automate certain analyses (e.g., automatic tests of linearity
for each predictor are done by <code>anova.rms</code>).  Many of the functions
are called implicitly.  Some S functions such as <code>ns</code> derive data-dependent
transformations that are not always &quot;remembered&quot; when predicted values are
later computed, so the predictions may be incorrect. The functions listed
here solve that problem when used in the <code>rms</code> context.
</p>
<p><code>asis</code> is the identity transformation, <code>pol</code> is an ordinary
(non-orthogonal) polynomial, <code>rcs</code> is a linear tail-restricted
cubic spline function (natural spline, for which the
<code>rcspline.eval</code> function generates the design matrix, the
presence of system option <code>rcspc</code> causes <code>rcspline.eval</code> to be
invoked with <code>pc=TRUE</code>, and the presence of system option <code>fractied</code>
causes this value to be passed to <code>rcspline.eval</code> as the <code>fractied</code>
argument), <code>catg</code> is for a categorical variable,
<code>scored</code> is for an ordered categorical variable, <code>strat</code> is
for a stratification factor in a Cox model, <code>matrx</code> is for a matrix
predictor, and <code>%ia%</code> represents restricted interactions in which
products involving nonlinear effects on both variables are not included
in the model.  <code>asis, catg, scored, matrx</code> are seldom invoked
explicitly by the user (only to specify <code>label</code> or <code>name</code>,
usually).
</p>
<p><code>gTrans</code> is a general multiple-parameter transformation function.
It can be used to specify new polynomial bases, smooth relationships
with a discontinuity at one or more values of <code>x</code>, grouped
categorical variables, e.g., a categorical variable with 5 levels where
you want to combine two of the levels to spend only 3 degrees of freedom in
all but see plots of predicted values where the two combined categories
are kept separate but will have equal effect estimates.  The first
argument to <code>gTrans</code> is a regular numeric, character, or factor
variable.  The next argument is a function that transforms a vector into
a matrix.  If the basis functions are to include a linear term it is up
too the user to include the original <code>x</code> as one of the columns.
Column names are assigned automaticall, but any column names specified
by the user will override the default name.  If you want to signal which
terms correspond to linear and which correspond to nonlinear effects for
the purpose of running <code>anova.rms</code>, add an integer vector attribute
<code>nonlinear</code> to the resulting matrix.  This vector specifies the
column numbers corresponding to nonlinear effects.  The default is to assume a column
is a linear effect.  The <code>parms</code> attribute stored with a
<code>gTrans</code> result a character vector version of the function, so as
to not waste space carrying along any environment information.  If you
will be using the <code>latex</code> method for typesetting the fitted model,
you must include a <code>tex</code> attribute also in the produced matrix.
This must be a function of a single character string argument (that will
ultimately contain the name of the predictor in LaTeX notation) and must
produce a vector of LaTeX character strings.  See
<a href="https://hbiostat.org/R/examples/gTrans/gTrans.html">https://hbiostat.org/R/examples/gTrans/gTrans.html</a> for several examples of the
use of <code>gTrans</code> including the use of <code>nonlinear</code> and
<code>tex</code>.
</p>
<p>A <code>makepredictcall</code> method is defined so that usage of the
transformation functions outside of <code>rms</code> fitting functions will
work for getting predicted values.  Thanks to Therry Therneau for the code.
</p>
<p>In the list below, functions <code>asis</code> through <code>gTrans</code> can have
arguments <code>x, parms, label, name</code> except that <code>parms</code> does not
apply to <code>asis, matrx, strat</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asis(...)
matrx(...)
pol(...)
lsp(...)
rcs(...)
catg(...)
scored(...)
strat(...)
gTrans(...)
x1 %ia% x2
## S3 method for class 'rms'
makepredictcall(var, call)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rms.trans_+3A_...">...</code></td>
<td>

<p>The arguments ... above contain the following.
</p>

<dl>
<dt><code>x</code></dt><dd><p>a predictor variable (or a function of one).  If
you specify e.g. <code>pol(pmin(age,10),3)</code>, a cubic polynomial
will be fitted in <code>pmin(age,10)</code> (<code>pmin</code> is the S vector
element&ndash;by&ndash;element function). The predictor will be labeled
<code>age</code> in the output, and plots with have <code>age</code> in its
original units on the axes. If you use a function such as
<code>pmin</code>, the predictor is taken as the first argument, and
other arguments must be defined in the frame in effect when
predicted values, etc., are computed.</p>
</dd>
<dt><code>parms</code></dt><dd><p>parameters of transformation (e.g. number or
location of knots). For <code>pol</code> the argument is the order of
the polynomial, e.g. <code>2</code> for quadratic (the usual
default). For <code>lsp</code> it is a vector of knot locations
(<code>lsp</code> will not estimate knot locations).  For <code>rcs</code> it
is the number of knots (if scalar), or vector of knot locations
(if <code>&gt;2</code> elements).  The default number is the <code>nknots</code>
system option if <code>parms</code> is not given.  If the number of
knots is given, locations are computed for that number of knots.
If system option <code>rcspc</code> is <code>TRUE</code> the <code>parms</code>
vector has an attribute defining the principal components
transformation parameters.  For <code>catg</code>, <code>parms</code> is the
category labels (not needed if variable is an S category or factor
variable). If omitted, <code>catg</code> will use <code>unique(x)</code>, or
<code>levels(x)</code> if <code>x</code> is a <code>category</code> or a
<code>factor</code>.  For <code>scored</code>, <code>parms</code> is a vector of
unique values of variable (uses <code>unique(x)</code> by default).
This is not needed if <code>x</code> is an S <code>ordered</code> variable.
For <code>strat</code>, <code>parms</code> is the category labels (not needed
if variable is an S category variable). If omitted, will use
<code>unique(x)</code>, or <code>levels(x)</code> if <code>x</code> is
<code>category</code> or <code>factor</code>. <code>parms</code> is not used for
<code>matrix</code>.</p>
</dd>
<dt><code>label</code></dt><dd><p>label of predictor for plotting (default =
<code>"label"</code> attribute or variable name)</p>
</dd>
<dt><code>name</code></dt><dd><p>Name to use for predictor in model. Default is
name of argument to function.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="rms.trans_+3A_x1">x1</code>, <code id="rms.trans_+3A_x2">x2</code></td>
<td>
<p>two continuous variables for which to form a
non-doubly-nonlinear interaction</p>
</td></tr>
<tr><td><code id="rms.trans_+3A_var">var</code></td>
<td>
<p>a model term passed from a (usually non-<code>rms</code>) function</p>
</td></tr>
<tr><td><code id="rms.trans_+3A_call">call</code></td>
<td>
<p>call object for a model term</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="Hmisc.html#topic+rcspline.eval">rcspline.eval</a></code>,
<code><a href="Hmisc.html#topic+rcspline.restate">rcspline.restate</a></code>, <code><a href="#topic+rms">rms</a></code>,
<code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+ols">ols</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="stats.html#topic+makepredictcall">makepredictcall</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
options(knots=4, poly.degree=2)
# To get the old behavior of rcspline.eval knot placement (which didnt' handle
# clumping at the lowest or highest value of the predictor very well):
# options(fractied = 1.0)   # see rcspline.eval for details
country &lt;- factor(country.codes)
blood.pressure &lt;- cbind(sbp=systolic.bp, dbp=diastolic.bp)
fit &lt;- lrm(Y ~ sqrt(x1)*rcs(x2) + rcs(x3,c(5,10,15)) + 
       lsp(x4,c(10,20)) + country + blood.pressure + poly(age,2))
# sqrt(x1) is an implicit asis variable, but limits of x1, not sqrt(x1)
#       are used for later plotting and effect estimation
# x2 fitted with restricted cubic spline with 4 default knots
# x3 fitted with r.c.s. with 3 specified knots
# x4 fitted with linear spline with 2 specified knots
# country is an implied catg variable
# blood.pressure is an implied matrx variable
# since poly is not an rms function (pol is), it creates a
#       matrx type variable with no automatic linearity testing
#       or plotting
f1 &lt;- lrm(y ~ rcs(x1) + rcs(x2) + rcs(x1) %ia% rcs(x2))
# %ia% restricts interactions. Here it removes terms nonlinear in
# both x1 and x2
f2 &lt;- lrm(y ~ rcs(x1) + rcs(x2) + x1 %ia% rcs(x2))
# interaction linear in x1
f3 &lt;- lrm(y ~ rcs(x1) + rcs(x2) + x1 %ia% x2)
# simple product interaction (doubly linear)
# Use x1 %ia% x2 instead of x1:x2 because x1 %ia% x2 triggers
# anova to pool x1*x2 term into x1 terms to test total effect
# of x1
#
# Examples of gTrans
#
# Linear relationship with a discontinuity at zero:
ldisc &lt;- function(x) {z &lt;- cbind(x == 0, x); attr(z, 'nonlinear') &lt;- 1; z}
gTrans(x, ldisc)
# Duplicate pol(x, 2):
pol2 &lt;- function(x) {z &lt;- cbind(x, x^2); attr(z, 'nonlinear') &lt;- 2; z}
gTrans(x, pol2)
# Linear spline with a knot at x=10 with the new slope taking effect
# until x=20 and the spline turning flat at that point but with a
# discontinuous vertical shift
# tex is only needed if you will be using latex(fit)
dspl &lt;- function(x) {
  z &lt;- cbind(x, pmax(pmin(x, 20) - 10, 0), x &gt; 20)
  attr(z, 'nonlinear') &lt;- 2:3
  attr(z, 'tex') &lt;- function(x) sprintf(c('%s', '(\min(%s, 20) - 10)_{+}',
                                          '[%s &gt; 20]'), x)
  z }
gTrans(x, dspl)

## End(Not run)
</code></pre>

<hr>
<h2 id='rmsMisc'>Miscellaneous Design Attributes and Utility Functions</h2><span id='topic+rmsMisc'></span><span id='topic+calibrate.rms'></span><span id='topic+DesignAssign'></span><span id='topic+vcov.rms'></span><span id='topic+vcov.cph'></span><span id='topic+vcov.Glm'></span><span id='topic+vcov.Gls'></span><span id='topic+vcov.lrm'></span><span id='topic+vcov.ols'></span><span id='topic+vcov.orm'></span><span id='topic+vcov.psm'></span><span id='topic+oos.loglik'></span><span id='topic+oos.loglik.ols'></span><span id='topic+oos.loglik.lrm'></span><span id='topic+oos.loglik.cph'></span><span id='topic+oos.loglik.psm'></span><span id='topic+oos.loglik.Glm'></span><span id='topic+Getlim'></span><span id='topic+Getlimi'></span><span id='topic+related.predictors'></span><span id='topic+interactions.containing'></span><span id='topic+combineRelatedPredictors'></span><span id='topic+param.order'></span><span id='topic+Penalty.matrix'></span><span id='topic+Penalty.setup'></span><span id='topic+logLik.Gls'></span><span id='topic+logLik.ols'></span><span id='topic+logLik.rms'></span><span id='topic+AIC.rms'></span><span id='topic+nobs.rms'></span><span id='topic+lrtest'></span><span id='topic+univarLR'></span><span id='topic+Newlabels'></span><span id='topic+Newlevels'></span><span id='topic+Newlabels.rms'></span><span id='topic+Newlevels.rms'></span><span id='topic+rmsArgs'></span><span id='topic+print.rms'></span><span id='topic+print.lrtest'></span><span id='topic+survest.rms'></span><span id='topic+prModFit'></span><span id='topic+prStats'></span><span id='topic+reListclean'></span><span id='topic+formatNP'></span><span id='topic+latex.naprint.delete'></span><span id='topic+html.naprint.delete'></span><span id='topic+removeFormulaTerms'></span>

<h3>Description</h3>

<p>These functions are used internally to <code>anova.rms</code>,
<code>fastbw</code>, etc., to retrieve various attributes of a design.  These
functions allow some fitting functions not in the <code>rms</code> series
(e.g,, <code>lm</code>, <code>glm</code>) to be used with <code>rms.Design</code>,
<code>fastbw</code>, and similar functions. 
</p>
<p>For <code>vcov</code>, there are several functions.  The method for <code>orm</code>
fits is a bit different because the covariance matrix stored in the fit
object only deals with the middle intercept.  See the <code>intercepts</code>
argument for more options.  There is a method for <code>lrm</code> that also
allows non-default intercept(s) to be selected (default is first).
</p>
<p>The <code>oos.loglik</code> function for
each type of model implemented computes the -2 log likelihood for
out-of-sample data (i.e., data not necessarily used to fit the model)
evaluated at the parameter estimates from a model fit.  Vectors for the
model's linear predictors and response variable must be given.
<code>oos.loglik</code> is used primarily by <code>bootcov</code>.
</p>
<p>The <code>Getlim</code> function retrieves distribution summaries
from the fit or from a <code>datadist</code> object.  It handles getting summaries
from both sources to fill in characteristics for variables that were not
defined during the model fit.  <code>Getlimi</code> returns the summary
for an individual model variable.  
</p>
<p><code>Mean</code> is a generic function that creates an R function that
calculates the expected value of the response variable given a fit from
<code>rms</code> or <code>rmsb</code>.
</p>
<p>The <code>related.predictors</code> function
returns a list containing variable numbers that are directly or
indirectly related to each predictor.  The <code>interactions.containing</code>
function returns indexes of interaction effects containing a given
predictor.  The <code>param.order</code> function returns a vector of logical
indicators for whether parameters are associated with certain types of
effects (nonlinear, interaction, nonlinear interaction).
<code>combineRelatedPredictors</code> creates of list of inter-connected main
effects and interations for use with <code>predictrms</code> with
<code>type='ccterms'</code> (useful for <code>gIndex</code>).
</p>
<p>The <code>Penalty.matrix</code> function builds a default penalty matrix for
non-intercept term(s) for use in penalized maximum likelihood
estimation.  The <code>Penalty.setup</code> function takes a constant or list
describing penalty factors for each type of term in the model and
generates the proper vector of penalty multipliers for the current model.
</p>
<p><code>logLik.rms</code> returns the maximized log likelihood for the model,
whereas <code>AIC.rms</code> returns the AIC.  The latter function has an
optional argument for computing AIC on a &quot;chi-square&quot; scale (model
likelihood ratio chi-square minus twice the regression degrees of
freedom.  <code>logLik.ols</code> handles the case for <code>ols</code>, just by
invoking <code>logLik.lm</code> in the <code>stats</code> package.
<code>logLik.Gls</code> is also defined.
</p>
<p><code>nobs.rms</code> returns the number of observations used in the fit.
</p>
<p>The <code>lrtest</code> function does likelihood ratio tests for
two nested models, from fits that have <code>stats</code> components with
<code>"Model L.R."</code> values.  For models such as <code>psm, survreg, ols, lm</code> which have
scale parameters, it is assumed that scale parameter for the smaller model
is fixed at the estimate from the larger model (see the example).
</p>
<p><code>univarLR</code> takes a multivariable model fit object from
<code>rms</code> and re-fits a sequence of models containing one predictor
at a time.  It prints a table of likelihood ratio <code class="reqn">chi^2</code> statistics
from these fits.
</p>
<p>The <code>Newlabels</code> function is used to override the variable labels in a
fit object.  Likewise, <code>Newlevels</code> can be used to create a new fit object
with levels of categorical predictors changed.  These two functions are
especially useful when constructing nomograms.
</p>
<p><code>rmsArgs</code> handles ... arguments to functions such as
<code>Predict</code>, <code>summary.rms</code>, <code>nomogram</code> so that variables to
vary may be specified without values (after an equals sign).
</p>
<p><code>prModFit</code> is the workhorse for the <code>print</code> methods for
highest-level <code>rms</code> model fitting functions, handling both regular,
html, and LaTeX printing, the latter two resulting in html or LaTeX code
written to the console, automatically ready for <code>knitr</code>.  The work
of printing 
summary statistics is done by <code>prStats</code>, which uses the Hmisc
<code>print.char.matrix</code> function to print overall model statistics if
<code>options(prType=)</code> was not set to <code>"latex"</code> or <code>"html"</code>.
Otherwise it generates customized LaTeX or html
code.  The LaTeX longtable and epic packages must be in effect to use LaTeX.
</p>
<p><code>reListclean</code> allows one to rename a subset of a named list,
ignoring the previous names and not concatenating them as <span class="rlang"><b>R</b></span> does.  It
also removes <code>NULL</code> elements and (by default) elements that are
<code>NA</code>, as when an 
optional named element is fetched that doesn't exist.  It has an
argument <code>dec</code> whose elements are correspondingly removed, then
<code>dec</code> is appended to the result vector.
</p>
<p><code>formatNP</code> is a function to format a vector of numerics.  If
<code>digits</code> is specified, <code>formatNP</code> will make sure that the
formatted representation has <code>digits</code> positions to the right of the
decimal place.  If <code>lang="latex"</code> it will translate any scientific
notation to LaTeX math form.  If <code>lang="html"</code> will convert to html.
If <code>pvalue=TRUE</code>, it will replace
formatted values with &quot;&lt; 0.0001&quot; (if <code>digits=4</code>).
</p>
<p><code>latex.naprint.delete</code> will, if appropriate, use LaTeX to draw a
dot chart of frequency of variable <code>NA</code>s related to model fits.
<code>html.naprint.delete</code> does the same thing in the RStudio R markdown
context, using <code>Hmisc:dotchartp</code> (which uses <code>plotly</code>) for
drawing any needed dot chart.
</p>
<p><code>removeFormulaTerms</code> removes one or more terms from a model
formula, using strictly character manipulation.  This handles problems
such as <code>[.terms</code> removing <code>offset()</code> if you subset on
anything.  The function can also be used to remove the dependent
variable(s) from the formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rms'
vcov(object, regcoef.only=TRUE, intercepts='all', ...)
## S3 method for class 'cph'
vcov(object, regcoef.only=TRUE, ...)
## S3 method for class 'Glm'
vcov(object, regcoef.only=TRUE, intercepts='all', ...)
## S3 method for class 'Gls'
vcov(object, intercepts='all', ...)
## S3 method for class 'lrm'
vcov(object, regcoef.only=TRUE, intercepts='all', ...)
## S3 method for class 'ols'
vcov(object, regcoef.only=TRUE, ...)
## S3 method for class 'orm'
vcov(object, regcoef.only=TRUE, intercepts='mid', ...)
## S3 method for class 'psm'
vcov(object, regcoef.only=TRUE, ...)

# Given Design attributes and number of intercepts creates R
# format assign list.  atr non.slopes Terms
DesignAssign(atr, non.slopes, Terms)

oos.loglik(fit, ...)

## S3 method for class 'ols'
oos.loglik(fit, lp, y, ...)
## S3 method for class 'lrm'
oos.loglik(fit, lp, y, ...)
## S3 method for class 'cph'
oos.loglik(fit, lp, y, ...)
## S3 method for class 'psm'
oos.loglik(fit, lp, y, ...)
## S3 method for class 'Glm'
oos.loglik(fit, lp, y, ...)

Getlim(at, allow.null=FALSE, need.all=TRUE)
Getlimi(name, Limval, need.all=TRUE)

related.predictors(at, type=c("all","direct"))
interactions.containing(at, pred)
combineRelatedPredictors(at)
param.order(at, term.order)

Penalty.matrix(at, X)
Penalty.setup(at, penalty)

## S3 method for class 'Gls'
logLik(object, ...)
## S3 method for class 'ols'
logLik(object, ...)
## S3 method for class 'rms'
logLik(object, ...)
## S3 method for class 'rms'
AIC(object, ..., k=2, type=c('loglik', 'chisq'))
## S3 method for class 'rms'
nobs(object, ...)

lrtest(fit1, fit2)
## S3 method for class 'lrtest'
print(x, ...)

univarLR(fit)

Newlabels(fit, ...)
Newlevels(fit, ...)
## S3 method for class 'rms'
Newlabels(fit, labels, ...)
## S3 method for class 'rms'
Newlevels(fit, levels, ...)

prModFit(x, title, w, digits=4, coefs=TRUE, footer=NULL,
         lines.page=40, long=TRUE, needspace, subtitle=NULL, ...)

prStats(labels, w, lang=c("plain", "latex", "html"))

reListclean(..., dec=NULL, na.rm=TRUE)

formatNP(x, digits=NULL, pvalue=FALSE,
         lang=c("plain", "latex", "html"))

## S3 method for class 'naprint.delete'
latex(object, file="", append=TRUE, ...)

## S3 method for class 'naprint.delete'
html(object, ...)

removeFormulaTerms(form, which=NULL, delete.response=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmsMisc_+3A_fit">fit</code></td>
<td>
<p>result of a fitting function</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_object">object</code></td>
<td>
<p>result of a fitting function</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_regcoef.only">regcoef.only</code></td>
<td>
<p>For fits such as parametric survival models
which have a final row and column of the covariance matrix for a
non-regression parameter such as a log(scale) parameter, setting
<code>regcoef.only=TRUE</code> causes only the first 
<code>p</code> rows and columns of the covariance matrix to be returned,
where <code>p</code> is the length of <code>object$coef</code>.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_intercepts">intercepts</code></td>
<td>
<p>set to <code>"none"</code> to omit any rows and columns
related to intercepts.  Set to an integer scalar
or vector to include particular intercept elements.  Set to
<code>'all'</code> to include all intercepts, or for <code>orm</code> to
<code>"mid"</code> to use the default for <code>orm</code>.  The default is to use the
first for <code>lrm</code> and the median intercept for <code>orm</code>.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_at">at</code></td>
<td>

<p><code>Design</code> element of a fit
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_pred">pred</code></td>
<td>

<p>index of a predictor variable (main effect)
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_fit1">fit1</code>, <code id="rmsMisc_+3A_fit2">fit2</code></td>
<td>

<p>fit objects from <code>lrm,ols,psm,cph</code> etc.  It doesn't matter which
fit object is the sub-model.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_lp">lp</code></td>
<td>

<p>linear predictor vector for <code>oos.loglik</code>.  For proportional odds
ordinal logistic models, this should have used the first intercept
only.  If <code>lp</code> and <code>y</code> are omitted, the -2 log likelihood for the
original fit are returned.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_y">y</code></td>
<td>

<p>values of a new vector of responses passed to <code>oos.loglik</code>.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_name">name</code></td>
<td>

<p>the name of a variable in the model
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_limval">Limval</code></td>
<td>

<p>an object returned by <code>Getlim</code>
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_allow.null">allow.null</code></td>
<td>

<p>prevents <code>Getlim</code> from issuing an error message if no limits are found
in the fit or in the object pointed to by <code>options(datadist=)</code>
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_need.all">need.all</code></td>
<td>

<p>set to <code>FALSE</code> to prevent <code>Getlim</code> or <code>Getlimi</code> from issuing an error message
if data for a variable are not found
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_type">type</code></td>
<td>

<p>For <code>related.predictors</code>, set to <code>"direct"</code> to return lists of
indexes of directly related factors only (those in interactions with the
predictor).  For <code>AIC.rms</code>, <code>type</code> specifies the basis on
which to return AIC.  The default is minus twice the maximized log
likelihood plus <code>k</code> times the degrees of freedom counting
intercept(s).  Specify <code>type='chisq'</code> to get a penalized model
likelihood ratio chi-square instead.  
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_term.order">term.order</code></td>
<td>

<p>1 for all parameters, 2 for all parameters associated with either nonlinear
or interaction effects, 3 for nonlinear effects (main or interaction),
4 for interaction effects, 5 for nonlinear interaction effects.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_x">X</code></td>
<td>

<p>a design matrix, not including columns for intercepts
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_penalty">penalty</code></td>
<td>

<p>a vector or list specifying penalty multipliers for types of model terms
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_k">k</code></td>
<td>
<p>the multiplier of the degrees of freedom to be used in
computing AIC.  The default is 2.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_x">x</code></td>
<td>
<p>a result of <code>lrtest</code>, or the result of a high-level model
fitting function (for <code>prModFit</code>)</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_labels">labels</code></td>
<td>

<p>a character vector specifying new labels for variables in a fit.
To give new labels for all variables, you can specify <code>labels</code> of the
form <code>labels=c("Age in Years","Cholesterol")</code>, where the list of new labels is
assumed to be the length of all main effect-type variables in the fit and
in their original order in the model formula.  You may specify a named
vector to give new labels in random order or for a subset of the 
variables, e.g., <code>labels=c(age="Age in Years",chol="Cholesterol")</code>.
For <code>prStats</code>, is a list with major column headings, which can
themselves be vectors that are then stacked vertically.
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_levels">levels</code></td>
<td>

<p>a list of named vectors specifying new level labels for categorical
predictors.  This will override <code>parms</code> as well as <code>datadist</code> information
(if available) that were stored with the fit.  
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_title">title</code></td>
<td>
<p>a single character string used to specify an overall title
for the regression fit, which is printed first by <code>prModFit</code>.
Set to <code>""</code> to suppress the title.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_w">w</code></td>
<td>
<p>For <code>prModFit</code>, a special list of lists, which each list
element specifying information about a block of information to include
in the <code>print.</code> output for a fit.  For <code>prStats</code>, <code>w</code>
is a list of statistics to print, elements of which can be vectors
that are stacked vertically.  Unnamed elements specify number of
digits to the right of the decimal place to which to round (<code>NA</code>
means use <code>format</code> without rounding, as with integers and
floating point values).  Negative values of <code>digits</code> indicate
that the value is a P-value to be formatted with <code>formatNP</code>.
Digits are recycled as needed. 
</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_digits">digits</code></td>
<td>
<p>number of digits to the right of the decimal point, for
formatting numeric values in printed output</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_footer">footer</code></td>
<td>
<p>a character string to appear at the bottom of the
regression model output</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_file">file</code></td>
<td>
<p>name of file to which to write model output</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_append">append</code></td>
<td>
<p>specify <code>append=FALSE</code> when using <code>file</code> and you
want to start over instead of adding to an existing file.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_lang">lang</code></td>
<td>
<p>specifies the typesetting language: plain text, LaTeX, or html</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_lines.page">lines.page</code></td>
<td>
<p>see <code><a href="Hmisc.html#topic+latex">latex</a></code></p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_long">long</code></td>
<td>
<p>set to <code>FALSE</code> to suppress printing of formula and
certain other model output</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_needspace">needspace</code></td>
<td>
<p>optional character string to insert inside a LaTeX
needspace macro call before the statistics table and before the
coefficient matrix, to avoid bad page splits.  This assumes the LaTeX
needspace style is available.  Example:
<code>needspace='6\baselineskip'</code> or <code>needspace='1.5in'</code>.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_subtitle">subtitle</code></td>
<td>
<p>optional vector of character strings containing
subtitles that will appear under <code>title</code> but not bolded</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_dec">dec</code></td>
<td>
<p>vector of decimal places used for rounding</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_na.rm">na.rm</code></td>
<td>
<p>set to <code>FALSE</code> to keep <code>NA</code>s in the vector
created by <code>reListclean</code></p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_pvalue">pvalue</code></td>
<td>
<p>set to <code>TRUE</code> if you want values below 10 to the
minus <code>digits</code> to be formatted to be less than that value</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_form">form</code></td>
<td>
<p>a formula object</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_which">which</code></td>
<td>
<p>a vector of one or more character strings specifying the
names of functions that are called from a formula, e.g.,
<code>"cluster"</code>.  By default no right-hand-side terms are removed.</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_delete.response">delete.response</code></td>
<td>
<p>set to <code>TRUE</code> to remove the dependent
variable(s) from the formula</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_atr">atr</code>, <code id="rmsMisc_+3A_non.slopes">non.slopes</code>, <code id="rmsMisc_+3A_terms">Terms</code></td>
<td>
<p><code>Design</code> function attributes, number
of intercepts, and <code>terms</code> object</p>
</td></tr>
<tr><td><code id="rmsMisc_+3A_...">...</code></td>
<td>
<p>other arguments.  For <code>reListclean</code> this contains the
elements being extracted.  For <code>prModFit</code> this information is
passed to the <code>Hmisc latexTabular</code> function when a block of
output is a vector to be formatted in LaTeX.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>vcov</code> returns a variance-covariance matrix
<code>oos.loglik</code> returns a scalar -2 log likelihood value.
<code>Getlim</code> returns a list with components <code>limits</code> and <code>values</code>, either
stored in <code>fit</code> or retrieved from the object created by <code>datadist</code> and
pointed to in <code>options(datadist=)</code>.
<code>related.predictors</code> and <code>combineRelatedPredictors</code> return a
list of vectors, and <code>interactions.containing</code> 
returns a vector.  <code>param.order</code> returns a logical vector corresponding
to non-strata terms in the model.
<code>Penalty.matrix</code> returns a symmetric matrix with dimension equal to the
number of slopes in the model.  For all but categorical predictor main
effect elements, the matrix is diagonal with values equal to the variances
of the columns of <code>X</code>.  For segments corresponding to <code>c-1</code> dummy variables
for <code>c</code>-category predictors,  puts a <code>c-1</code> x <code>c-1</code> sub-matrix in
<code>Penalty.matrix</code> that is constructed so that a quadratic form with 
<code>Penalty.matrix</code> in the middle computes the sum of squared differences
in parameter values about the mean, including a portion for the reference
cell in which the parameter is by definition zero.
<code>Newlabels</code> returns a new fit object with the labels adjusted.
</p>
<p><code>reListclean</code> returns a vector of named (by its arguments) elements.
<code>formatNP</code> returns a character vector.
</p>
<p><code>removeFormulaTerms</code> returns a formula object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+anova.rms">anova.rms</a></code>,
<code><a href="stats.html#topic+summary.lm">summary.lm</a></code>, <code><a href="stats.html#topic+summary.glm">summary.glm</a></code>,
<code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+vif">vif</a></code>, <code><a href="#topic+bootcov">bootcov</a></code>,
<code><a href="Hmisc.html#topic+latex">latex</a></code>, <code><a href="Hmisc.html#topic+latexTabular">latexTabular</a></code>,
<code><a href="Hmisc.html#topic+latex">latexSN</a></code>,
<code><a href="Hmisc.html#topic+print.char.matrix">print.char.matrix</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
f &lt;- psm(S ~ x1 + x2 + sex + race, dist='gau')
g &lt;- psm(S ~ x1 + sex + race, dist='gau', 
         fixed=list(scale=exp(f$parms)))
lrtest(f, g)


g &lt;- Newlabels(f, c(x2='Label for x2'))
g &lt;- Newlevels(g, list(sex=c('Male','Female'),race=c('B','W')))
nomogram(g)

## End(Not run)
</code></pre>

<hr>
<h2 id='rmsOverview'>Overview of rms Package</h2><span id='topic+rmsOverview'></span><span id='topic+rms.Overview'></span>

<h3>Description</h3>

<p>rms is the package that goes along with the book Regression Modeling
Strategies.  rms does regression modeling, testing, estimation,
validation, graphics, prediction, and typesetting by storing enhanced
model design attributes in the fit.  rms is a re-written version of the
Design package that has improved graphics and duplicates very little
code in the survival package.
</p>
<p>The package is a collection of about 180 functions that assist and
streamline modeling, especially for biostatistical and epidemiologic
applications.  It also contains functions for binary and ordinal
logistic regression models and the Buckley-James multiple regression
model for right-censored responses, and implements penalized maximum
likelihood estimation for logistic and ordinary linear models.  rms
works with almost any regression model, but it was especially written
to work with logistic regression, Cox regression, accelerated failure
time models, ordinary linear models, the Buckley-James model,
generalized lease squares for longitudinal data (using the nlme
package), generalized linear models, and quantile regression (using
the quantreg package).  rms requires the Hmisc package to
be installed.  Note that Hmisc has several functions useful for data
analysis (especially data reduction and imputation).
</p>
<p>Older references below pertaining to the Design package are relevant to rms.
</p>


<h3>Details</h3>

<p>To make use of automatic typesetting features you must
have LaTeX or one of its variants installed.<br />
</p>
<p>Some aspects of rms (e.g., <code>latex</code>) will not work correctly if
<code>options(contrasts=)</code> other than <code>c("contr.treatment",
    "contr.poly")</code> are used.
</p>
<p>rms relies on a wealth of survival analysis
functions written by Terry Therneau of Mayo Clinic.
Front-ends have been written for several of
Therneau's functions, and other functions have been
slightly modified.
</p>


<h3>Statistical Methods Implemented</h3>


<ul>
<li><p> Ordinary linear regression models
</p>
</li>
<li><p> Binary and ordinal logistic models (proportional odds
and continuation ratio models, probit, log-log, complementary
log-log including ordinal cumulative probability models for continuous
Y, efficiently handling thousands of distinct Y values using full
likelihood methods)
</p>
</li>
<li><p> Bayesian binary and ordinal regression models, partial
proportional odds model, and random effects
</p>
</li>
<li><p> Cox model
</p>
</li>
<li><p> Parametric survival models in the accelerated failure
time class
</p>
</li>
<li><p> Buckley-James least-squares linear regression model
with possibly right-censored responses
</p>
</li>
<li><p> Generalized linear model
</p>
</li>
<li><p> Quantile regression
</p>
</li>
<li><p> Generalized least squares
</p>
</li>
<li><p> Bootstrap model validation to obtain unbiased
estimates of model performance without requiring a
separate validation sample
</p>
</li>
<li><p> Automatic Wald tests of all effects in the model that
are not parameterization-dependent (e.g., tests of
nonlinearity of main effects when the variable does
not interact with other variables, tests of
nonlinearity of interaction effects, tests for
whether a predictor is important, either as a main
effect or as an effect modifier)
</p>
</li>
<li><p> Graphical depictions of model estimates (effect
plots, odds/hazard ratio plots, nomograms that
allow model predictions to be obtained manually even
when there are nonlinear effects and interactions
in the model)
</p>
</li>
<li><p> Various smoothed residual plots, including some new
residual plots for verifying ordinal logistic model
assumptions
</p>
</li>
<li><p> Composing S functions to evaluate the linear
predictor (<code class="reqn">X\hat{beta}</code>), hazard function, survival
function, quantile functions analytically from the
fitted model
</p>
</li>
<li><p> Typesetting of fitted model using LaTeX
</p>
</li>
<li><p> Robust covariance matrix estimation (Huber or
bootstrap)
</p>
</li>
<li><p> Cubic regression splines with linear tail restrictions (natural splines)
</p>
</li>
<li><p> Tensor splines
</p>
</li>
<li><p> Interactions restricted to not be doubly nonlinear
</p>
</li>
<li><p> Penalized maximum likelihood estimation for ordinary
linear regression and logistic regression models.
Different parts of the model may be penalized by
different amounts, e.g., you may want to penalize
interaction or nonlinear effects more than main
effects or linear effects
</p>
</li>
<li><p> Estimation of hazard or odds ratios in presence of
nolinearity and interaction
</p>
</li>
<li><p> Sensitivity analysis for an unmeasured binary confounder in a
binary logistic model
</p>
</li></ul>



<h3>Motivation</h3>

<p>rms was motivated by the following needs:
</p>

<ul>
<li><p> need to automatically print interesting Wald tests that can be
constructed from the design
</p>

<ul>
<li><p> tests of linearity with respect to each predictor
</p>
</li>
<li><p> tests of linearity of interactions
</p>
</li>
<li><p> pooled interaction tests (e.g., all interactions involving race)
</p>
</li>
<li><p> pooled tests of effects with higher order effects
</p>

<ul>
<li><p> test of main effect not meaningful when effect in interaction
</p>
</li>
<li><p> pooled test of main effect + interaction effect is meaningful
</p>
</li>
<li><p> test of 2nd-order interaction + any 3rd-order interaction containing
those factors is meaningful
</p>
</li></ul>

</li></ul>

</li>
<li><p> need to store transformation parameters with the fit
</p>

<ul>
<li><p> example: knot locations for spline functions
</p>
</li>
<li><p> these are &quot;remembered&quot; when getting predictions, unlike standard
S or <span class="rlang"><b>R</b></span>
</p>
</li>
<li><p>  for categorical predictors, save levels so that same dummy variables
will be generated for predictions; check that all levels in out-of-data
predictions were present when model was fitted
</p>
</li></ul>

</li>
<li><p> need for uniform re-insertion of observations deleted because of NAs
when using <code>predict</code> without <code>newdata</code> or when using
<code>resid</code>
</p>
</li>
<li><p> need to easily plot the regression effect of any predictor
</p>

<ul>
<li><p> example: age is represented by a linear spline with knots at 40 and 60y
plot effect of age on log odds of disease, adjusting
interacting factors to easily specified constants
</p>
</li>
<li><p>  vary 2 predictors: plot x1 on x-axis, separate curves for discrete
x2 or 3d perspective plot for continuous x2
</p>
</li>
<li><p> if predictor is represented as a function in the model, plots
should be with respect to the original variable:<br />
<code>f &lt;- lrm(y ~ log(cholesterol)+age)</code> <br />
<code>plot(Predict(f, cholesterol))   # cholesterol on x-axis, default range</code> <br />
<code>ggplot(Predict(f, cholesterol)) # same using ggplot2</code>
<code>plotp(Predict(f, cholesterol))  # same directly using plotly</code>
</p>
</li></ul>

</li>
<li><p> need to store summary of distribution of predictors with the fit
</p>

<ul>
<li><p> plotting limits (default: 10th smallest, 10th largest values or %-tiles)
</p>
</li>
<li><p> effect limits   (default: .25 and .75 quantiles for continuous vars.)
</p>
</li>
<li><p>  adjustment values for other predictors (default: median for continuous
predictors, most frequent level for categorical ones)
</p>
</li>
<li><p> discrete numeric predictors: list of possible values
example: x=0,1,2,3,5 -&gt; by default don't plot prediction at x=4
</p>
</li>
<li><p> values are on the inner-most variable, e.g. cholesterol, not log(chol.)
</p>
</li>
<li><p> allows estimation/plotting long after original dataset has been deleted
</p>
</li>
<li><p>  for Cox models, underlying survival also stored with fit, so original
data not needed to obtain predicted survival curves
</p>
</li></ul>

</li>
<li><p> need to automatically print estimates of effects in presence of non-
linearity and interaction
</p>

<ul>
<li><p> example: age is quadratic, interacting with sex
default effect is inter-quartile-range hazard ratio (for
Cox model), for sex=reference level
</p>
</li>
<li><p> user-controlled effects: <code>summary(fit, age=c(30,50),
        sex="female")</code> -&gt; odds ratios for logistic model, relative survival time
for accelerated failure time survival models
</p>
</li>
<li><p> effects for all variables (e.g. odds ratios) may be plotted with
multiple-confidence-level bars
</p>
</li></ul>

</li>
<li><p> need for prettier and more concise effect names in printouts,
especially for expanded nonlinear terms and interaction terms
</p>

<ul>
<li><p> use inner-most variable name to identify predictors
</p>
</li>
<li><p> e.g. for <code>pmin(x^2-3,10)</code> refer to factor with legal S-name
<code>x</code>
</p>
</li></ul>

</li>
<li><p> need to recognize that an intercept is not always a simple
concept
</p>
  
<ul>
<li><p> some models (e.g., Cox) have no intercept
</p>
</li>
<li><p> some models (e.g., ordinal logistic) have multiple intercepts
</p>
</li></ul>

</li>
<li><p> need for automatic high-quality printing of fitted mathematical
model (with dummy variables defined, regression spline terms
simplified, interactions &quot;factored&quot;).  Focus is on regression splines
instead of nonparametric smoothers or smoothing splines, so that
explicit formulas for fit may be obtained for use outside S.
rms can also compose S functions to evaluate <code class="reqn">X\beta</code> from
the fitted model analytically, as well as compose SAS code to
do this.
</p>
</li>
<li><p> need for automatic drawing of nomogram to represent the fitted model
</p>
</li>
<li><p> need for automatic bootstrap validation of a fitted model, with
only one S command (with respect to calibration and discrimination)
</p>
</li>
<li><p> need for robust (Huber sandwich) estimator of covariance matrix,
and be able to do all other analysis (e.g., plots, C.L.) using the
adjusted covariances
</p>
</li>
<li><p> need for robust (bootstrap) estimator of covariance matrix, easily
used in other analyses without change
</p>
</li>
<li><p> need for Huber sandwich and bootstrap covariance matrices adjusted
for cluster sampling
</p>
</li>
<li><p> need for routine reporting of how many observations were deleted
by missing values on each predictor (see <code>na.delete</code> in Hmisc)
</p>
</li>
<li><p> need for optional reporting of descriptive statistics for Y stratified
by missing status of each X (see na.detail.response)
</p>
</li>
<li><p> need for pretty, annotated survival curves, using the same commands
for parametric and Cox models
</p>
</li>
<li><p> need for ordinal logistic model (proportional odds model, continuation
ratio model)
</p>
</li>
<li><p> need for estimating and testing general contrasts without having to
be conscious of variable coding or parameter order
</p>
</li></ul>



<h3>Fitting Functions Compatible with rms</h3>

<p>rms will work with a wide variety of fitting
functions, but it is meant especially for the
following:
</p>

<table>
<tr>
 <td style="text-align: left;">
    <b>Function</b> </td><td style="text-align: left;"> <b>Purpose</b> </td><td style="text-align: left;">  <b>Related S</b></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;">                </td><td style="text-align: left;">  <b>Functions</b></td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>ols</code></b>         </td><td style="text-align: left;"> Ordinary least squares linear model     </td><td style="text-align: left;"> <code>lm</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>lrm</code></b>         </td><td style="text-align: left;"> Binary and ordinal logistic regression  </td><td style="text-align: left;"> <code>glm</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> model                                   </td><td style="text-align: left;"> <code>cr.setup</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>orm</code></b>         </td><td style="text-align: left;"> Ordinal regression model
  </td><td style="text-align: left;"> <code>lrm</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	<b><code>blrm</code></b>        </td><td style="text-align: left;"> Bayesian binary and ordinal regression </td><td style="text-align: left;">\ </td>
</tr>
<tr>
 <td style="text-align: left;">
  <b><code>psm</code></b>         </td><td style="text-align: left;"> Accelerated failure time parametric     </td><td style="text-align: left;"> <code>survreg</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> survival model                          </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>cph</code></b>         </td><td style="text-align: left;"> Cox proportional hazards regression     </td><td style="text-align: left;"> <code>coxph</code></td>
</tr>
<tr>
 <td style="text-align: left;">
		<b><code>npsurv</code></b>      </td><td style="text-align: left;"> Nonparametric survival estimates </td><td style="text-align: left;">
<code>survfit.formula</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>bj</code></b>          </td><td style="text-align: left;"> Buckley-James censored least squares    </td><td style="text-align: left;"> <code>survreg</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> linear model                            </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Glm</code></b>        </td><td style="text-align: left;"> Version of <code>glm</code> for use with rms </td><td style="text-align: left;"> <code>glm</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Gls</code></b>        </td><td style="text-align: left;"> Version of <code>gls</code> for use with rms </td><td style="text-align: left;"> <code>gls</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	<b><code>Rq</code></b>         </td><td style="text-align: left;"> Version of <code>rq</code> for use with rms  </td><td style="text-align: left;"> <code>rq</code></td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Methods in rms</h3>

<p>The following generic functions work with fits with rms in effect:
</p>

<table>
<tr>
 <td style="text-align: left;">
    <b>Function</b>           </td><td style="text-align: left;">  <b>Purpose</b> </td><td style="text-align: left;"> <b>Related</b></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;">                 </td><td style="text-align: left;"> <b>Functions</b></td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>print</code></b>       </td><td style="text-align: left;"> Print parameters and statistics of fit </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>coef</code></b>        </td><td style="text-align: left;"> Fitted regression coefficients  </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>formula</code></b>     </td><td style="text-align: left;"> Formula used in the fit </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>specs</code></b>       </td><td style="text-align: left;"> Detailed specifications of fit </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>robcov</code></b>      </td><td style="text-align: left;"> Robust covariance matrix estimates </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>bootcov</code></b>     </td><td style="text-align: left;"> Bootstrap covariance matrix estimates </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>summary</code></b>     </td><td style="text-align: left;"> Summary of effects of predictors </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>plot.summary</code></b> </td><td style="text-align: left;"> Plot continuously shaded confidence </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> bars for results of summary  </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>anova</code></b>       </td><td style="text-align: left;"> Wald tests of most meaningful hypotheses </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>contrast</code></b>    </td><td style="text-align: left;"> General contrasts, C.L., tests           </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>plot.anova</code></b>  </td><td style="text-align: left;"> Depict results of anova graphically      </td><td style="text-align: left;"> <code>dotchart</code>     </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Predict</code></b>     </td><td style="text-align: left;"> Partial predictor effects </td><td style="text-align: left;">  <code>predict</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	<b><code>plot.Predict</code></b></td><td style="text-align: left;"> Plot predictor effects using lattice graphics </td><td style="text-align: left;"> <code>predict</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
  <b><code>ggplot</code></b>      </td><td style="text-align: left;"> Similar to above but using ggplot2 </td>
</tr>
<tr>
 <td style="text-align: left;">
	<b><code>plotp</code></b>       </td><td style="text-align: left;"> Similar to above but using plotly </td>
</tr>
<tr>
 <td style="text-align: left;">
	<b><code>bplot</code></b>       </td><td style="text-align: left;"> 3-D plot of effects of varying two </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
                              </td><td style="text-align: left;"> continuous predictors </td><td style="text-align: left;"> <code>image, persp, contour</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>gendata</code></b>     </td><td style="text-align: left;"> Generate data frame with predictor       </td><td style="text-align: left;"> <code>expand.grid</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> combinations (optionally interactively) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>predict</code></b>     </td><td style="text-align: left;"> Obtain predicted values or design matrix </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>fastbw</code></b>      </td><td style="text-align: left;"> Fast backward step-down variable            </td><td style="text-align: left;"> <code>step</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> selection </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>residuals</code></b>   </td><td style="text-align: left;"> Residuals, influence statistics from fit </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    (or <b><code>resid</code></b>)  </td><td style="text-align: left;">                         </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>which.influence</code></b> 
    </td><td style="text-align: left;"> Which observations are overly               </td><td style="text-align: left;"> <code>residuals</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> influential </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>sensuc</code></b>      </td><td style="text-align: left;"> Sensitivity of one binary predictor in </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> lrm and cph models to an unmeasured </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> binary confounder </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>latex</code></b>       </td><td style="text-align: left;"> LaTeX representation of fitted              </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> model or <code>anova</code> or <code>summary</code> table </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Function</code></b>    </td><td style="text-align: left;"> S function analytic representation          </td><td style="text-align: left;"> <code>Function.transcan</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> of a fitted regression model (<code class="reqn">X\beta</code>) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>hazard</code></b>      </td><td style="text-align: left;"> S function analytic representation          </td><td style="text-align: left;"> <code>rcspline.restate</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> of a fitted hazard function (for <code>psm</code>) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Survival</code></b>    </td><td style="text-align: left;"> S function analytic representation of </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> fitted survival function (for <code>psm,cph</code>) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>Quantile</code></b>    </td><td style="text-align: left;"> S function analytic representation of </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> fitted function for quantiles of </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> survival time (for <code>psm, cph</code>) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>nomogram</code></b>    </td><td style="text-align: left;"> Draws a nomogram for the fitted model
  </td><td style="text-align: left;"> <code>latex, plot, ggplot, plotp</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>survest</code></b>     </td><td style="text-align: left;"> Estimate survival probabilities             </td><td style="text-align: left;"> <code>survfit</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;">  (for <code>psm, cph</code>) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>survplot</code></b>    </td><td style="text-align: left;"> Plot survival curves (psm, cph, npsurv)             </td><td style="text-align: left;"> plot.survfit </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>validate</code></b>    </td><td style="text-align: left;"> Validate indexes of model fit using         </td><td style="text-align: left;"> val.prob </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> resampling </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>calibrate</code></b>   </td><td style="text-align: left;"> Estimate calibration curve for model </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> using resampling </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>vif</code></b>         </td><td style="text-align: left;"> Variance inflation factors for a fit </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>naresid</code></b>     </td><td style="text-align: left;"> Bring elements corresponding to missing  </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> data back into predictions and residuals </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>naprint</code></b>     </td><td style="text-align: left;"> Print summary of missing values </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>pentrace</code></b>    </td><td style="text-align: left;"> Find optimum penality for penalized MLE </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>effective.df</code></b>
    </td><td style="text-align: left;"> Print effective d.f. for each type of  </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> variable in model, for penalized fit or  </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> pentrace result </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>rm.impute</code></b>   </td><td style="text-align: left;"> Impute repeated measures data with     </td><td style="text-align: left;"> <code>transcan</code>, </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> non-random dropout </td><td style="text-align: left;"> <code>fit.mult.impute</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> <em>experimental, non-functional</em> </td><td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Background for Examples</h3>

<p>The following programs demonstrate how the pieces of
the rms package work together.  A (usually)
one-time call to the function <code>datadist</code> requires a
pass at the entire data frame to store distribution
summaries for potential predictor variables.  These
summaries contain (by default) the .25 and .75
quantiles of continuous variables (for estimating
effects such as odds ratios), the 10th smallest and
10th largest values (or .1 and .9 quantiles for small
<code class="reqn">n</code>) for plotting ranges for estimated curves, and the
total range.  For discrete numeric variables (those
having <code class="reqn">\leq 10</code> unique values), the list of unique values
is also stored.  Such summaries are used by the
<code>summary.rms, Predict</code>, and <code>nomogram.rms</code>
functions.  You may save time and defer running
<code>datadist</code>.  In that case, the distribution summary
is not stored with the fit object, but it can be
gathered before running <code>summary</code>, <code>plot</code>, <code>ggplot</code>, or
<code>plotp</code>.
</p>
<p><code>d &lt;- datadist(my.data.frame) # or datadist(x1,x2)</code><br />
<code>options(datadist="d")        # omit this or use options(datadist=NULL)</code><br />
<code>                             # if not run datadist yet</code><br />
<code>cf &lt;- ols(y ~ x1 * x2)</code><br />
<code>anova(f)</code><br />
<code>fastbw(f)</code><br />
<code>Predict(f, x2)</code>
<code>predict(f, newdata)</code>
</p>
<p>In the <b>Examples</b> section there are three detailed examples using a
fitting function 
designed to be used with rms, <code>lrm</code> (logistic
regression model).  In <b>Detailed Example 1</b> we
create 3 predictor variables and a two binary response
on 500 subjects.  For the first binary response, <code>dz</code>,
the true model involves only <code>sex</code> and <code>age</code>, and there is
a nonlinear interaction between the two because the log
odds is a truncated linear relationship in <code>age</code> for
females and a quadratic function for males.  For the
second binary outcome, <code>dz.bp</code>, the true population model
also involves systolic blood pressure (<code>sys.bp</code>) through
a truncated linear relationship.  First, nonparametric
estimation of relationships is done using the Hmisc
package's <code>plsmo</code> function which uses <code>lowess</code> with outlier
detection turned off for binary responses.  Then
parametric modeling is done using restricted cubic
splines.  This modeling does not assume that we know
the true transformations for <code>age</code> or <code>sys.bp</code> but that
these transformations are smooth (which is not actually
the case in the population).
</p>
<p>For <b>Detailed Example 2</b>, suppose that a
categorical variable treat has values <code>"a", "b"</code>, and
<code>"c"</code>, an ordinal variable <code>num.diseases</code> has values
0,1,2,3,4, and that there are two continuous variables,
<code>age</code> and <code>cholesterol</code>.  <code>age</code> is fitted with a restricted
cubic spline, while <code>cholesterol</code> is transformed using
the transformation <code>log(cholesterol - 10)</code>.  Cholesterol
is missing on three subjects, and we impute these using
the overall median cholesterol.  We wish to allow for
interaction between <code>treat</code> and <code>cholesterol</code>.  The
following S program will fit a logistic model,
test all effects in the design, estimate effects, and
plot estimated transformations. The fit for
<code>num.diseases</code> really considers the variable to be a
5-level categorical variable. The only difference is
that a 3 d.f. test of linearity is done to assess
whether the variable can be re-modeled &quot;asis&quot;.  Here
we also show statements to attach the rms package
and store predictor characteristics from datadist.
</p>
<p><b>Detailed Example 3</b> shows some of the survival
analysis capabilities of rms related to the Cox
proportional hazards model.  We simulate data for 2000
subjects with 2 predictors, <code>age</code> and <code>sex</code>.  In the true
population model, the log hazard function is linear in
<code>age</code> and there is no <code>age</code> <code class="reqn">\times</code> <code>sex</code>
interaction.  In the  
analysis below we do not make use of the linearity in
age.  rms makes use of many of Terry Therneau's
survival functions that are builtin to S.
</p>
<p>The following is a typical sequence of steps that
would be used with rms in conjunction with the Hmisc
<code>transcan</code> function to do single imputation of all NAs in the
predictors (multiple imputation would be better but would be
harder to do in the context of bootstrap model validation),
fit a model, do backward stepdown to reduce the number of
predictors in the model (with all the severe problems this can
entail), and use the bootstrap to validate this stepwise model,
repeating the variable selection for each re-sample.  Here we
take a short cut as the imputation is not repeated within the
bootstrap.
</p>
<p>In what follows we (atypically) have only 3
candidate predictors.  In practice be sure to have the
validate and calibrate functions operate on a model fit that
contains all predictors that were involved in previous analyses
that used the response variable.  Here the imputation
is necessary because backward stepdown would otherwise delete
observations missing on any candidate variable.
</p>
<p>Note that you would have to define <code>x1, x2, x3, y</code> to run
the following code.
</p>
<p><code>xt &lt;- transcan(~ x1 + x2 + x3, imputed=TRUE)</code><br />
<code>impute(xt)  # imputes any NAs in x1, x2, x3</code><br />
<code># Now fit original full model on filled-in data</code><br />
<code>f &lt;- lrm(y ~ x1 + rcs(x2,4) + x3, x=TRUE, y=TRUE) #x,y allow boot.</code><br />
<code>fastbw(f)</code><br />
<code># derives stepdown model (using default stopping rule)</code><br />
<code>validate(f, B=100, bw=TRUE) # repeats fastbw 100 times</code><br />
<code>cal &lt;- calibrate(f, B=100, bw=TRUE)  # also repeats fastbw</code><br />
<code>plot(cal)</code>
</p>


<h3>Common Problems to Avoid</h3>


<ol>
<li><p> Don't have a formula like <code>y ~ age + age^2</code>.
In S you need to connect related variables using
a function which produces a matrix, such as <code>pol</code> or
<code>rcs</code>.
This allows effect estimates (e.g., hazard ratios)
to be computed as well as multiple d.f. tests of
association.
</p>
</li>
<li><p> Don't use <code>poly</code> or <code>strata</code> inside formulas used in
rms.  Use <code>pol</code> and <code>strat</code> instead.
</p>
</li>
<li><p> Almost never code your own dummy variables or
interaction variables in S.  Let S do this
automatically.  Otherwise, <code>anova</code> can't do its
job.
</p>
</li>
<li><p> Almost never transform predictors outside of
the model formula, as then plots of predicted
values vs. predictor values, and other displays,
would not be made on the original scale.  Use
instead something like <code>y ~ log(cell.count+1)</code>,
which will allow <code>cell.count</code> to appear on
<code class="reqn">x</code>-axes.  You can get fancier, e.g.,
<code>y ~ rcs(log(cell.count+1),4)</code> to fit a restricted
cubic spline with 4 knots in <code>log(cell.count+1)</code>.
For more complex transformations do something
like 
<code>f &lt;- function(x) {</code><br />
<code>... various 'if' statements, etc.</code><br />
<code>log(pmin(x,50000)+1)</code><br />
<code>}</code><br />
<code>fit1 &lt;- lrm(death ~ f(cell.count))</code><br />
<code>fit2 &lt;- lrm(death ~ rcs(f(cell.count),4))</code><br />
<code>}</code>
</p>
</li>
<li><p> Don't put <code>$</code> inside variable names used in formulas.
Either attach data frames or use <code>data=</code>.
</p>
</li>
<li><p> Don't forget to use <code>datadist</code>.  Try to use it
at the top of your program so that all model fits
can automatically take advantage if its
distributional summaries for the predictors.
</p>
</li>
<li><p> Don't <code>validate</code> or <code>calibrate</code> models which were
reduced by dropping &quot;insignificant&quot; predictors.
Proper bootstrap or cross-validation must repeat
any variable selection steps for each re-sample.
Therefore, <code>validate</code> or <code>calibrate</code> models
which contain all candidate predictors, and if
you must reduce models, specify the option
<code>bw=TRUE</code> to <code>validate</code> or <code>calibrate</code>.
</p>
</li>
<li><p> Dropping of &quot;insignificant&quot; predictors ruins much
of the usual statistical inference for
regression models (confidence limits, standard
errors, <code class="reqn">P</code>-values, <code class="reqn">\chi^2</code>, ordinary indexes of
model performance) and it also results in models
which will have worse predictive discrimination.
</p>
</li></ol>



<h3>Accessing the Package</h3>

<p>Use <code>require(rms)</code>.
</p>


<h3>Published Applications of rms and Regression Splines</h3>


<ul>
<li><p> Spline fits
</p>

<ol>
<li><p> Spanos A, Harrell FE, Durack DT (1989): Differential
diagnosis of acute meningitis: An analysis of the
predictive value of initial observations.  <em>JAMA</em>
2700-2707.
</p>
</li>
<li><p> Ohman EM, Armstrong PW, Christenson RH, <em>et al</em>. (1996):
Cardiac troponin T levels for risk stratification in
acute myocardial ischemia.  <em>New Eng J Med</em> 335:1333-1341.
</p>
</li></ol>

</li>
<li><p> Bootstrap calibration curve for a parametric survival
model:
</p>

<ol>
<li><p> Knaus WA, Harrell FE, Fisher CJ, Wagner DP, <em>et al</em>.
(1993):  The clinical evaluation of new drugs for
sepsis: A prospective study design based on survival
analysis.  <em>JAMA</em> 270:1233-1241.
</p>
</li></ol>

</li>
<li><p> Splines, interactions with splines, algebraic form of
fitted model from <code>latex.rms</code>
</p>

<ol>
<li><p> Knaus WA, Harrell FE, Lynn J, et al. (1995): The
SUPPORT prognostic model: Objective estimates of
survival for seriously ill hospitalized adults.  <em>Annals
of Internal Medicine</em> 122:191-203.
</p>
</li></ol>

</li>
<li><p> Splines, odds ratio chart from fitted model with
nonlinear and interaction terms, use of <code>transcan</code> for
imputation
</p>

<ol>
<li><p> Lee KL, Woodlief LH, Topol EJ, Weaver WD, Betriu A.
Col J, Simoons M, Aylward P, Van de Werf F, Califf RM.
Predictors of 30-day mortality in the era of
reperfusion for acute myocardial infarction: results
from an international trial of 41,021 patients.
<em>Circulation</em> 1995;91:1659-1668.
</p>
</li></ol>

</li>
<li><p> Splines, external validation of logistic models,
prediction rules using point tables
</p>

<ol>
<li><p> Steyerberg EW, Hargrove YV, <em>et al</em> (2001): Residual mass
histology in testicular cancer: development and
validation of a clinical prediction rule.  <em>Stat in Med</em>
2001;20:3847-3859.
</p>
</li>
<li><p> van Gorp MJ, Steyerberg EW, <em>et al</em> (2003): Clinical
prediction rule for 30-day mortality in Bjork-Shiley convexo-concave
valve replacement.  <em>J Clinical Epidemiology</em> 2003;56:1006-1012.
</p>
</li></ol>

</li>
<li><p> Model fitting, bootstrap validation, missing value
imputation
</p>

<ol>
<li><p> Krijnen P, van Jaarsveld BC, Steyerberg EW, Man in 't
Veld AJ, Schalekamp, MADH, Habbema JDF (1998): A
clinical prediction  rule for renal artery stenosis.
<em>Annals of Internal Medicine</em> 129:705-711.
</p>
</li></ol>

</li>
<li><p> Model fitting, splines, bootstrap validation, nomograms
</p>

<ol>
<li><p> Kattan MW, Eastham JA, Stapleton AMF, Wheeler TM,
Scardino PT.  A preoperative nomogram for disease
recurrence following radical prostatectomy for
prostate cancer.  <em>J Natl Ca Inst</em> 1998;
90(10):766-771.
</p>
</li>
<li><p> Kattan, MW, Wheeler TM, Scardino PT.  A
postoperative nomogram for disease recurrence
following radical prostatectomy for prostate
cancer. <em>J Clin Oncol</em> 1999; 17(5):1499-1507
</p>
</li>
<li><p> Kattan MW, Zelefsky MJ, Kupelian PA, Scardino PT, 
Fuks Z, Leibel SA.  A pretreatment nomogram for
predicting the outcome of three-dimensional
conformal radiotherapy in prostate cancer.  
<em>J Clin Oncol</em> 2000; 18(19):3252-3259.
</p>
</li>
<li><p> Eastham JA, May R, Robertson JL, Sartor O, Kattan
MW.  Development of a nomogram which predicts the
probability of a positive prostate biopsy in men
with an abnormal digital rectal examination and a
prostate specific antigen between 0 and 4
ng/ml. <em>Urology</em>. (In press).
</p>
</li>
<li><p> Kattan MW, Heller G, Brennan MF.  A competing-risk
nomogram fir sarcoma-specific death following local recurrence.
<em>Stat in Med</em> 2003; 22; 3515-3525.
</p>
</li></ol>

</li>
<li><p> Penalized maximum likelihood estimation, regression splines, web
site to get predicted values
</p>

<ol>
<li><p> Smits M, Dippel DWJ, Steyerberg EW, et al.  Predicting intracranial
traumatic findings on computed tomography in patients with minor head
injury: The CHIP prediction rule.  <em>Ann Int Med</em> 2007; 146:397-405.
</p>
</li></ol>

</li>
<li><p> Nomogram with 2- and 5-year survival probability and median survival
time (but watch out for the use of univariable screening)
</p>

<ol>
<li><p> Clark TG, Stewart ME, Altman DG, Smyth JF.  A prognostic
model for ovarian cancer.  <em>Br J Cancer</em> 2001; 85:944-52.
</p>
</li></ol>

</li>
<li><p> Comprehensive example of parametric survival modeling
with an extensive nomogram, time ratio chart, anova
chart, survival curves generated using survplot,
bootstrap calibration curve
</p>

<ol>
<li><p> Teno JM, Harrell FE, Knaus WA, et al.  Prediction of
survival for older hospitalized patients: The HELP
survival model.  <em>J Am Geriatrics Soc</em> 2000;
48: S16-S24.
</p>
</li></ol>

</li>
<li><p> Model fitting, imputation, and several nomograms
expressed in tabular form
</p>

<ol>
<li><p> Hasdai D, Holmes DR, et al.  Cardiogenic shock complicating
acute myocardial infarction: Predictors of death.
<em>Am Heart J</em> 1999; 138:21-31.
</p>
</li></ol>

</li>
<li><p> Ordinal logistic model with bootstrap calibration plot
</p>

<ol>
<li><p> Wu AW, Yasui U, Alzola CF <em>et al</em>.  Predicting functional
status outcomes in hospitalized patients aged 80 years and
older.  <em>J Am Geriatric Society</em> 2000; 48:S6-S15.
</p>
</li></ol>

</li>
<li><p> Propensity modeling in evaluating medical diagnosis, anova
dot chart
</p>

<ol>
<li><p> Weiss JP, Gruver C, et al.  Ordering an echocardiogram 
for evaluation of left ventricular function: Level
of expertise necessary for efficient use. <em>J Am Soc 
Echocardiography</em> 2000; 13:124-130.
</p>
</li></ol>

</li>
<li><p> Simulations using rms to study the properties
of various modeling strategies
</p>

<ol>
<li><p> Steyerberg EW, Eijkemans MJC, Habbema JDF.  Stepwise selection
in small data sets: A simulation study of bias in logistic
regression analysis.  <em>J Clin Epi</em> 1999; 52:935-942.
</p>
</li>
<li><p> Steyerberg WE, Eijekans MJC, Harrell FE, Habbema JDF.
Prognostic modeling with logistic regression analysis: In
search of a sensible strategy in small data sets.  <em>Med
Decision Making</em> 2001; 21:45-56.
</p>
</li></ol>

</li>
<li><p> Statistical methods and
references related to rms, along with case studies
which includes the rms code which produced the
analyses
</p>

<ol>
<li><p> Harrell FE, Lee KL, Mark DB (1996): Multivariable
prognostic models: Issues in developing models,
evaluating assumptions and adequacy, and measuring and
reducing errors.  <em>Stat in Med</em> 15:361-387.
</p>
</li>
<li><p> Harrell FE, Margolis PA, Gove S, Mason KE, Mulholland
EK et al. (1998): Development of a clinical prediction
model for an ordinal outcome: The World Health
Organization ARI Multicentre Study of clinical signs
and etiologic agents of pneumonia, sepsis, and
meningitis in young infants. <em>Stat in Med</em> 17:909-944.
</p>
</li>
<li><p> Bender R, Benner, A (2000): Calculating ordinal regression
models in SAS and S-Plus.  <em>Biometrical J</em> 42:677-699.
</p>
</li></ol>

</li></ul>



<h3>Bug Reports</h3>

<p>The author is willing to help with problems.  Send
E-mail to <a href="mailto:fh@fharrell.com">fh@fharrell.com</a>.  To report bugs,
please do the following:
</p>

<ol>
<li><p> If the bug occurs when running a function on a fit
object (e.g., <code>anova</code>), attach a <code>dump</code>'d text
version of the fit object to your note.  If you
used <code>datadist</code> but not until after the fit was
created, also send the object created by
<code>datadist</code>.  Example: <code>save(myfit,"/tmp/myfit.rda")</code> will create
an R binary save file that can be attached to the E-mail.  
</p>
</li>
<li><p> If the bug occurs during a model fit (e.g., with
<code>lrm, ols, psm, cph</code>), send the statement causing
the error with a <code>save</code>'d version of the data
frame used in the fit.  If this data frame is very
large, reduce it to a small subset which still
causes the error.
</p>
</li></ol>



<h3>Copyright Notice</h3>

<p>GENERAL DISCLAIMER  This program is free software;
you can redistribute it and/or modify it under the
terms of the GNU General Public License as
published by the Free Software Foundation; either
version 2, or (at your option) any later version.
</p>
<p>This program is
distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied
warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.  See the GNU General Public
License for more details.   In short: you may
use this code any way you like, as long as you don't charge
money for it, remove this notice, or hold anyone
liable for its results.  Also, please acknowledge
the source and communicate changes to the author.
</p>
<p>If this software is used is work presented for
publication, kindly reference it using for example:
Harrell FE (2009): rms: S functions for
biostatistical/epidemiologic modeling, testing,
estimation, validation, graphics, and prediction.
Programs available from <a href="https://hbiostat.org/R/rms/">https://hbiostat.org/R/rms/</a>.
Be sure to reference other packages used as well as 
<span class="rlang"><b>R</b></span> itself.
</p>


<h3>Author(s)</h3>

<p>Frank E Harrell Jr<br />
Professor of Biostatistics<br />
Vanderbilt University School of Medicine<br />
Nashville, Tennessee<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>The primary resource for the rms package is
<em>Regression Modeling Strategies, second edition</em> by
FE Harrell (Springer-Verlag, 2015) and the web page
<a href="https://hbiostat.org/R/rms/">https://hbiostat.org/R/rms/</a>.  See also
the Statistics in Medicine articles by Harrell <em>et al</em> listed
below for case studies of modeling and model validation using rms.
</p>
<p>Several datasets useful for multivariable modeling with
rms are found at
<a href="https://hbiostat.org/data/">https://hbiostat.org/data/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## To run several comprehensive examples, run the following command
## Not run: 
demo(all, 'rms')

## End(Not run)
</code></pre>

<hr>
<h2 id='robcov'>Robust Covariance Matrix Estimates</h2><span id='topic+robcov'></span>

<h3>Description</h3>

<p>Uses the Huber-White method to adjust the variance-covariance matrix of
a fit from maximum likelihood or least squares, to correct for
heteroscedasticity and for correlated responses from cluster samples.
The method uses the ordinary estimates of regression coefficients and
other parameters of the model, but involves correcting the covariance
matrix for model misspecification and sampling design. 
Models currently implemented are models that have a 
<code>residuals(fit,type="score")</code> function implemented, such as <code>lrm</code>, 
<code>cph</code>, <code>coxph</code>, and ordinary linear models (<code>ols</code>).
The fit must have specified the <code>x=TRUE</code> and <code>y=TRUE</code> options for certain models.
Observations in different clusters are assumed to be independent.
For the special case where every cluster contains one observation, the
corrected covariance matrix returned is the &quot;sandwich&quot; estimator
(see Lin and Wei). This is a consistent estimate of the covariance matrix
even if the model is misspecified (e.g. heteroscedasticity, underdispersion,
wrong covariate form).
</p>
<p>For the special case of ols fits, <code>robcov</code> can compute the improved
(especially for small samples) Efron estimator that adjusts for
natural heterogeneity of residuals (see Long and Ervin (2000)
estimator HC3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robcov(fit, cluster, method=c('huber','efron'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robcov_+3A_fit">fit</code></td>
<td>

<p>a fit object from the <code>rms</code> series
</p>
</td></tr>
<tr><td><code id="robcov_+3A_cluster">cluster</code></td>
<td>

<p>a variable indicating groupings. <code>cluster</code> may be any type of vector
(factor, character, integer).  NAs are not allowed.
Unique values of <code>cluster</code> indicate
possibly correlated groupings of observations. Note the data used in
the fit and stored in <code>fit$x</code> and <code>fit$y</code> may have had observations
containing missing values deleted. It is assumed that if any NAs were
removed during the original model fitting, an <code>naresid</code> function
exists to restore NAs so that the rows of the score matrix coincide
with <code>cluster</code>.
If <code>cluster</code> is omitted,
it defaults to the integers 1,2,...,n to obtain the &quot;sandwich&quot; robust
covariance matrix estimate.
</p>
</td></tr>
<tr><td><code id="robcov_+3A_method">method</code></td>
<td>

<p>can set to <code>"efron"</code> for ols fits (only).  Default is Huber-White
estimator of the covariance matrix.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a new fit object with the same class as the original fit,
and with the element <code>orig.var</code> added. <code>orig.var</code> is
the covariance matrix of the original fit.  Also, the original <code>var</code>
component is replaced with the new Huberized estimates.  A component
<code>clusterInfo</code> is added to contain elements <code>name</code> and <code>n</code>
holding the name of the <code>cluster</code> variable and the number of clusters.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Huber, PJ. Proc Fifth Berkeley Symposium Math Stat 1:221&ndash;33, 1967.
</p>
<p>White, H. Econometrica 50:1&ndash;25, 1982.
</p>
<p>Lin, DY, Wei, LJ. JASA 84:1074&ndash;8, 1989.
</p>
<p>Rogers, W.  Stata Technical Bulletin STB-8, p. 15&ndash;17, 1992.
</p>
<p>Rogers, W.  Stata Release 3 Manual, <code>deff</code>, <code>loneway</code>, <code>huber</code>, <code>hreg</code>, <code>hlogit</code>
functions.
</p>
<p>Long, JS, Ervin, LH.  The American Statistician 54:217&ndash;224, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bootcov">bootcov</a></code>, <code><a href="stats.html#topic+naresid">naresid</a></code>,
<code><a href="#topic+residuals.cph">residuals.cph</a></code>, <code>http://gforge.se/gmisc</code> interfaces
<code>rms</code> to the <code>sandwich</code> package
</p>


<h3>Examples</h3>

<pre><code class='language-R'># In OLS test against more manual approach
set.seed(1)
n &lt;- 15
x1 &lt;- 1:n
x2 &lt;- sample(1:n)
y &lt;- round(x1 + x2 + 8*rnorm(n))
f &lt;- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
vcov(f)
vcov(robcov(f))
X &lt;- f$x
G &lt;- diag(resid(f)^2)
solve(t(X) %*% X) %*% (t(X) %*% G %*% X) %*% solve(t(X) %*% X)

# Duplicate data and adjust for intra-cluster correlation to see that
# the cluster sandwich estimator completely ignored the duplicates
x1 &lt;- c(x1,x1)
x2 &lt;- c(x2,x2)
y  &lt;- c(y, y)
g &lt;- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
vcov(robcov(g, c(1:n, 1:n)))

# A dataset contains a variable number of observations per subject,
# and all observations are laid out in separate rows. The responses
# represent whether or not a given segment of the coronary arteries
# is occluded. Segments of arteries may not operate independently
# in the same patient.  We assume a "working independence model" to
# get estimates of the coefficients, i.e., that estimates assuming
# independence are reasonably efficient.  The job is then to get
# unbiased estimates of variances and covariances of these estimates.

n.subjects &lt;- 30
ages &lt;- rnorm(n.subjects, 50, 15)
sexes  &lt;- factor(sample(c('female','male'), n.subjects, TRUE))
logit &lt;- (ages-50)/5
prob &lt;- plogis(logit)  # true prob not related to sex
id &lt;- sample(1:n.subjects, 300, TRUE) # subjects sampled multiple times
table(table(id))  # frequencies of number of obs/subject
age &lt;- ages[id]
sex &lt;- sexes[id]
# In truth, observations within subject are independent:
y   &lt;- ifelse(runif(300) &lt;= prob[id], 1, 0)
f &lt;- lrm(y ~ lsp(age,50)*sex, x=TRUE, y=TRUE)
g &lt;- robcov(f, id)
diag(g$var)/diag(f$var)
# add ,group=w to re-sample from within each level of w
anova(g)            # cluster-adjusted Wald statistics
# fastbw(g)         # cluster-adjusted backward elimination
plot(Predict(g, age=30:70, sex='female'))  # cluster-adjusted confidence bands
# or use ggplot(...)

# Get design effects based on inflation of the variances when compared
# with bootstrap estimates which ignore clustering
g2 &lt;- robcov(f)
diag(g$var)/diag(g2$var)


# Get design effects based on pooled tests of factors in model
anova(g2)[,1] / anova(g)[,1]




# A dataset contains one observation per subject, but there may be
# heteroscedasticity or other model misspecification. Obtain
# the robust sandwich estimator of the covariance matrix.


# f &lt;- ols(y ~ pol(age,3), x=TRUE, y=TRUE)
# f.adj &lt;- robcov(f)
</code></pre>

<hr>
<h2 id='Rq'>rms Package Interface to quantreg Package</h2><span id='topic+Rq'></span><span id='topic+RqFit'></span><span id='topic+print.Rq'></span><span id='topic+latex.Rq'></span><span id='topic+predict.Rq'></span>

<h3>Description</h3>

<p>The <code>Rq</code> function is the <code>rms</code> front-end to the
<code>quantreg</code> package's <code>rq</code> function.  <code>print</code> and
<code>latex</code> methods are also provided, and a fitting function
<code>RqFit</code> is defined for use in bootstrapping, etc.  Its result is a
function definition.
</p>
<p>For the <code>print</code> method, format of output is controlled by the
user previously running <code>options(prType="lang")</code> where
<code>lang</code> is <code>"plain"</code> (the default), <code>"latex"</code>, or
<code>"html"</code>.  For the <code>latex</code> method, <code>html</code> will actually
be used of <code>options(prType='html')</code>.  When using html with Quarto
or RMarkdown, <code>results='asis'</code> need not be written in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rq(formula, tau = 0.5, data=environment(formula),
   subset, weights, na.action=na.delete,
   method = "br", model = FALSE, contrasts = NULL,
   se = "nid", hs = TRUE, x = FALSE, y = FALSE, ...)

## S3 method for class 'Rq'
print(x, digits=4, coefs=TRUE, title, ...)

## S3 method for class 'Rq'
latex(object,
           file = '', append=FALSE,
           which, varnames, columns=65, inline=FALSE, caption=NULL, ...)

## S3 method for class 'Rq'
predict(object, ..., kint=1, se.fit=FALSE)

RqFit(fit, wallow=TRUE, passdots=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rq_+3A_formula">formula</code></td>
<td>
<p>model formula</p>
</td></tr>
<tr><td><code id="Rq_+3A_tau">tau</code></td>
<td>

<p>the single quantile to estimate.  Unlike <code>rq</code> you cannot estimate
more than one quantile at one model fitting.
</p>
</td></tr>
<tr><td><code id="Rq_+3A_data">data</code>, <code id="Rq_+3A_subset">subset</code>, <code id="Rq_+3A_weights">weights</code>, <code id="Rq_+3A_na.action">na.action</code>, <code id="Rq_+3A_method">method</code>, <code id="Rq_+3A_model">model</code>, <code id="Rq_+3A_contrasts">contrasts</code>, <code id="Rq_+3A_se">se</code>, <code id="Rq_+3A_hs">hs</code></td>
<td>
<p>see
<code><a href="quantreg.html#topic+rq">rq</a></code></p>
</td></tr>
<tr><td><code id="Rq_+3A_x">x</code></td>
<td>
<p>set to <code>TRUE</code> to store the design matrix with the fit.
For <code>print</code> is an <code>Rq</code> object.</p>
</td></tr>
<tr><td><code id="Rq_+3A_y">y</code></td>
<td>
<p>set to <code>TRUE</code> to store the response vector with the fit</p>
</td></tr>
<tr><td><code id="Rq_+3A_...">...</code></td>
<td>

<p>other arguments passed to one of the <code>rq</code> fitting routines.
For <code>latex.Rq</code> these are optional arguments passed to
<code>latexrms</code>.  Ignored for <code>print.Rq</code>.  For
<code>predict.Rq</code> this is usually just a <code>newdata</code> argument.
</p>
</td></tr>
<tr><td><code id="Rq_+3A_digits">digits</code></td>
<td>

<p>number of significant digits used in formatting results in
<code>print.Rq</code>.
</p>
</td></tr>
<tr><td><code id="Rq_+3A_coefs">coefs</code></td>
<td>
<p>specify <code>coefs=FALSE</code> to suppress printing the table
of model coefficients, standard errors, etc.  Specify <code>coefs=n</code>
to print only the first <code>n</code> regression coefficients in the
model.</p>
</td></tr>
<tr><td><code id="Rq_+3A_title">title</code></td>
<td>
<p>a character string title to be passed to <code>prModFit</code></p>
</td></tr>
<tr><td><code id="Rq_+3A_object">object</code></td>
<td>
<p>an object created by <code>Rq</code></p>
</td></tr>
<tr><td><code id="Rq_+3A_file">file</code>, <code id="Rq_+3A_append">append</code>, <code id="Rq_+3A_which">which</code>, <code id="Rq_+3A_varnames">varnames</code>, <code id="Rq_+3A_columns">columns</code>, <code id="Rq_+3A_inline">inline</code>, <code id="Rq_+3A_caption">caption</code></td>
<td>
<p>see
<code><a href="#topic+latexrms">latexrms</a></code></p>
</td></tr>
<tr><td><code id="Rq_+3A_kint">kint</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="Rq_+3A_se.fit">se.fit</code></td>
<td>
<p>set to <code>TRUE</code> to obtain standard errors of
predicted quantiles</p>
</td></tr>
<tr><td><code id="Rq_+3A_fit">fit</code></td>
<td>
<p>an object created by <code>Rq</code></p>
</td></tr>
<tr><td><code id="Rq_+3A_wallow">wallow</code></td>
<td>

<p>set to <code>TRUE</code> if <code>weights</code> are allowed in the
current context.
</p>
</td></tr>
<tr><td><code id="Rq_+3A_passdots">passdots</code></td>
<td>

<p>set to <code>TRUE</code> if ... may be passed to the fitter</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Rq</code> returns a list of class <code>"rms", "lassorq"</code> or <code>"scadrq",
	"Rq"</code>, and <code>"rq"</code>.  <code>RqFit</code> returns a function
definition.  <code>latex.Rq</code> returns an object of class <code>"latex"</code>.
</p>


<h3>Note</h3>

<p>The author and developer of methodology in the <code>quantreg</code> package
is Roger Koenker.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="quantreg.html#topic+rq">rq</a></code>, <code><a href="#topic+prModFit">prModFit</a></code>, <code><a href="#topic+orm">orm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1)
n &lt;- 100
x1 &lt;- rnorm(n)
y &lt;- exp(x1 + rnorm(n)/4)
dd &lt;- datadist(x1); options(datadist='dd')
fq2 &lt;- Rq(y ~ pol(x1,2))
anova(fq2)
fq3 &lt;- Rq(y ~ pol(x1,2), tau=.75)
anova(fq3)
pq2 &lt;- Predict(fq2, x1)
pq3 &lt;- Predict(fq3, x1)
p &lt;- rbind(Median=pq2, Q3=pq3)
plot(p, ~ x1 | .set.)
# For superpositioning, with true curves superimposed
a &lt;- function(x, y, ...) {
 x &lt;- unique(x)
 col &lt;- trellis.par.get('superpose.line')$col
 llines(x, exp(x), col=col[1], lty=2)
 llines(x, exp(x + qnorm(.75)/4), col=col[2], lty=2)
}
plot(p, addpanel=a)

## End(Not run)
</code></pre>

<hr>
<h2 id='sensuc'>Sensitivity to Unmeasured Covariables</h2><span id='topic+sensuc'></span><span id='topic+plot.sensuc'></span>

<h3>Description</h3>

<p>Performs an analysis of the sensitivity of a binary treatment (<code class="reqn">X</code>)
effect to an unmeasured binary confounder (<code class="reqn">U</code>) for a fitted binary
logistic or an unstratified non-time-dependent Cox survival model (the
function works well for the former, not so well for the latter).  This
is done by fitting a sequence of models with separately created <code class="reqn">U</code>
variables added to the original model.  The sequence of models is formed
by simultaneously varying <code class="reqn">a</code> and <code class="reqn">b</code>, where <code class="reqn">a</code> measures
the association between <code class="reqn">U</code> and <code class="reqn">X</code> and <code class="reqn">b</code> measures the
association between <code class="reqn">U</code> and <code class="reqn">Y</code>, where <code class="reqn">Y</code> is the outcome of
interest.  For Cox models, an approximate solution is used by letting
<code class="reqn">Y</code> represent some binary classification of the event/censoring time
and the event indicator.  For example, <code class="reqn">Y</code> could be just be the
event indicator, ignoring time of the event or censoring, or it could be
<code class="reqn">1</code> if a subject failed before one year and <code class="reqn">0</code> otherwise.  When
for each combination of <code class="reqn">a</code> and <code class="reqn">b</code> the vector of binary values
<code class="reqn">U</code> is generated, one of two methods is used to constrain the
properties of <code class="reqn">U</code>.  With either method, the overall prevalence of
<code class="reqn">U</code> is constrained to be <code>prev.u</code>.  With the default method
(<code>or.method="x:u y:u"</code>), <code class="reqn">U</code> is sampled so that the <code class="reqn">X:U</code>
odds ratio is <code class="reqn">a</code> and the <code class="reqn">Y:U</code> odds ratio is <code class="reqn">b</code>.  With the
second method, <code class="reqn">U</code> is sampled according to the model <code class="reqn">logit(U=1
| X, Y) = \alpha + \beta*Y + \gamma*X</code>, where <code class="reqn">\beta=\log(b)</code> and
<code class="reqn">\gamma=\log(a)</code> and <code class="reqn">\alpha</code> is determined so that the
prevalence of <code class="reqn">U=1</code> is <code>prev.u</code>.  This second method results in
the adjusted odds ratio for <code class="reqn">Y:U</code> given <code class="reqn">X</code> being <code class="reqn">b</code>
whereas the default method forces the unconditional (marginal) <code class="reqn">Y:U</code>
odds ratio to be <code class="reqn">b</code>.  Rosenbaum uses the default method.
</p>
<p>There is a <code>plot</code> method for plotting objects created by
<code>sensuc</code>.  Values of <code class="reqn">a</code> are placed on the x-axis and observed
marginal odds or hazards ratios for <code class="reqn">U</code> (unadjusted ratios) appear
on the y-axis.  For Cox models, the hazard ratios will not agree exactly
with <code class="reqn">X</code>:event indicator odds ratios but they sometimes be made
close through judicious choice of the <code>event</code> function.  The
default plot uses four symbols which differentiate whether for the
<code class="reqn">a,b</code> combination the effect of <code class="reqn">X</code> adjusted for <code class="reqn">U</code> (and
for any other covariables that were in the original model fit) is
positive (usually meaning an effect ratio greater than 1) and
&quot;significant&quot;, merely positive, not positive and non significant, or not
positive but significant.  There is also an option to draw the numeric
value of the <code class="reqn">X</code> effect ratio at the <code class="reqn">a</code>,<code class="reqn">b</code> combination
along with its <code class="reqn">Z</code> statistic underneath in smaller letters, and an
option to draw the effect ratio in one of four colors depending on the
significance of the <code class="reqn">Z</code> statistic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- lrm(formula=y ~ x + other.predictors, x=TRUE, y=TRUE)  #or
# fit &lt;- cph(formula=Surv(event.time,event.indicator) ~ x + other.predictors,
#            x=TRUE, y=TRUE)

sensuc(fit,  
       or.xu=seq(1, 6, by = 0.5), or.u=or.xu, 
       prev.u=0.5, constrain.binary.sample=TRUE, 
       or.method=c("x:u y:u","u|x,y"),
       event=function(y) if(is.matrix(y))y[,ncol(y)] else 1*y)

## S3 method for class 'sensuc'
plot(x,  ylim=c((1+trunc(min(x$effect.u)-.01))/
                   ifelse(type=='numbers',2,1),
                   1+trunc(max(x$effect.u)-.01)),
     xlab='Odds Ratio for X:U',
     ylab=if(x$type=='lrm')'Odds Ratio for Y:U' else
          'Hazard Ratio for Y:U',
     digits=2, cex.effect=.75, cex.z=.6*cex.effect,
     delta=diff(par('usr')[3:4])/40, 
     type=c('symbols','numbers','colors'),
     pch=c(15,18,5,0), col=c(2,3,1,4), alpha=.05,
     impressive.effect=function(x)x &gt; 1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sensuc_+3A_fit">fit</code></td>
<td>

<p>result of <code>lrm</code> or <code>cph</code> with <code>x=TRUE, y=TRUE</code>.  The
first variable in the right hand side of the model formula must have
been the binary <code class="reqn">X</code> variable, and it may not interact with other
predictors.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_x">x</code></td>
<td>

<p>result of <code>sensuc</code>
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_or.xu">or.xu</code></td>
<td>

<p>vector of possible odds ratios measuring the <code class="reqn">X:U</code> association.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_or.u">or.u</code></td>
<td>

<p>vector of possible odds ratios measuring the <code class="reqn">Y:U</code> association.
Default is <code>or.xu</code>.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_prev.u">prev.u</code></td>
<td>

<p>desired prevalence of <code class="reqn">U=1</code>.  Default is 0.5, which is usually a
&quot;worst case&quot; for sensitivity analyses.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_constrain.binary.sample">constrain.binary.sample</code></td>
<td>

<p>By default, the binary <code class="reqn">U</code> values are sampled from the appropriate
distributions conditional on <code class="reqn">Y</code> and <code class="reqn">X</code> so that the proportions of
<code class="reqn">U=1</code> in each sample are exactly the desired probabilities, to within
the closeness of <code class="reqn">n\times</code>probability to an integer.  Specify
<code>constrain.binary.sample=FALSE</code> to sample from ordinary Bernoulli
distributions, to allow proportions of <code class="reqn">U=1</code> to reflect sampling fluctuations.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_or.method">or.method</code></td>
<td>

<p>see above
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_event">event</code></td>
<td>

<p>a function classifying the response variable into a binary event for the
purposes of constraining the association between <code class="reqn">U</code> and <code class="reqn">Y</code>.
For binary logistic models, <code>event</code> is left at its default value, which
is the identify function, i.e, the original <code class="reqn">Y</code> values are taken as the
events (no other choice makes any sense here).  For Cox models, the 
default <code>event</code> function takes the last column of the <code>Surv</code> object
stored with the fit.  For rare events (high proportion of censored
observations), odds ratios approximate hazard ratios, so the default is OK.  
For other cases, the survival times should be considered (probably in
conjunction with the event indicators), although it may not be possible
to get a high enough hazard ratio between <code class="reqn">U</code> and <code class="reqn">Y</code> by sampling <code class="reqn">U</code> by
temporarily making <code class="reqn">Y</code> binary.  See the last example which is
for a 2-column <code>Surv</code> object (first column of
response variable=event time,  second=event indicator).  When
dichotomizing survival time at a given point, it is advantageous to choose
the cutpoint so that not many censored survival times preceed the cutpoint.
Note that in fitting Cox models to examine sensitivity to <code class="reqn">U</code>, the original
non-dichotomized failure times are used.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_ylim">ylim</code></td>
<td>

<p>y-axis limits for <code>plot</code>
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_digits">digits</code></td>
<td>

<p>number of digits to the right of the decimal point for drawing numbers
on the plot, for
<code>type="numbers"</code> or <code>type="colors"</code>.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_cex.effect">cex.effect</code></td>
<td>

<p>character size for drawing effect ratios
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_cex.z">cex.z</code></td>
<td>

<p>character size for drawing <code class="reqn">Z</code> statistics
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_delta">delta</code></td>
<td>

<p>decrement in <code class="reqn">y</code> value used to draw <code class="reqn">Z</code> values below effect ratios
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_type">type</code></td>
<td>

<p>specify <code>"symbols"</code> (the default), <code>"numbers"</code>, or <code>"colors"</code> (see above)
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_pch">pch</code></td>
<td>

<p>4 plotting characters corresponding to positive and significant
effects for <code class="reqn">X</code>, positive and non-significant effects, not positive and
not significant, not positive but significant
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_col">col</code></td>
<td>

<p>4 colors as for <code>pch</code>
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_alpha">alpha</code></td>
<td>

<p>significance level
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_impressive.effect">impressive.effect</code></td>
<td>

<p>a function of the odds or hazard ratio for <code class="reqn">X</code> returning <code>TRUE</code> for a
positive effect.  By default, a positive effect is taken to mean a
ratio exceeding one.
</p>
</td></tr>
<tr><td><code id="sensuc_+3A_...">...</code></td>
<td>

<p>optional arguments passed to <code>plot</code>
</p>
</td></tr></table>


<h3>Value</h3>

<p><code>sensuc</code> returns an object of class <code>"sensuc"</code> with the following elements: <code>OR.xu</code>
(vector of desired <code class="reqn">X:U</code> odds ratios or <code class="reqn">a</code> values), <code>OOR.xu</code>
(observed marginal <code class="reqn">X:U</code> odds ratios), <code>OR.u</code> (desired <code class="reqn">Y:U</code> odds
ratios or <code class="reqn">b</code> values), <code>effect.x</code> (adjusted odds or hazards ratio for
<code class="reqn">X</code> in a model adjusted for <code class="reqn">U</code> and all of the other predictors),
<code>effect.u</code> (unadjusted <code class="reqn">Y:U</code> odds or hazards ratios), <code>effect.u.adj</code>
(adjusted <code class="reqn">Y:U</code> odds or hazards ratios), <code class="reqn">Z</code> (Z-statistics), <code>prev.u</code>
(input to <code>sensuc</code>), <code>cond.prev.u</code> (matrix with one row per <code class="reqn">a</code>,<code class="reqn">b</code>
combination, specifying prevalences of <code class="reqn">U</code> conditional on <code class="reqn">Y</code> and <code class="reqn">X</code>
combinations), and <code>type</code> (<code>"lrm"</code> or <code>"cph"</code>).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Mark Conaway<br />
Department of Biostatistics<br />
Vanderbilt University School of Medicine<br />
fh@fharrell.com, mconaway@virginia.edu
</p>


<h3>References</h3>

<p>Rosenbaum, Paul R (1995): Observational Studies.  New York: Springer-Verlag.
</p>
<p>Rosenbaum P, Rubin D (1983): Assessing sensitivity to an unobserved binary
covariate in an observational study with binary outcome.  J Roy Statist Soc
B 45:212&ndash;218.
</p>
<p>Lee WC (2011): Bounding the bias of unmeasured factors with confounding
and effect-modifying potentials.  Stat in Med 30:1007-1017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lrm">lrm</a></code>, <code><a href="#topic+cph">cph</a></code>, <code><a href="base.html#topic+sample">sample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(17)
x &lt;- sample(0:1, 500,TRUE)
y &lt;- sample(0:1, 500,TRUE)
y[1:100] &lt;- x[1:100]  # induce an association between x and y
x2 &lt;- rnorm(500)


f &lt;- lrm(y ~ x + x2, x=TRUE, y=TRUE)


#Note: in absence of U odds ratio for x is exp(2nd coefficient)


g &lt;- sensuc(f, c(1,3))


# Note: If the generated sample of U was typical, the odds ratio for
# x dropped had U been known, where U had an odds ratio
# with x of 3 and an odds ratio with y of 3


plot(g)


# Fit a Cox model and check sensitivity to an unmeasured confounder

# require(survival)
# f &lt;- cph(Surv(d.time,death) ~ treatment + pol(age,2)*sex, x=TRUE, y=TRUE)
# sensuc(f, event=function(y) y[,2] &amp; y[,1] &lt; 365.25 )
# Event = failed, with event time before 1 year
# Note: Analysis uses f$y which is a 2-column Surv object
</code></pre>

<hr>
<h2 id='setPb'>Progress Bar for Simulations</h2><span id='topic+setPb'></span>

<h3>Description</h3>

<p>Depending on prevailing <code>options(showprogress=)</code> and availability
of the <code>tcltk</code> package, sets up a progress bar and creates a
function for simple updating of the bar as iterations progress.  Setting
<code>options(showprogressbar=FALSE)</code> or
<code>options(showprogressbar='none')</code> results in no progress being
shown.  Setting the option to something other than <code>"tk"</code> or
<code>"none"</code> results in the console being used to show the current
iteration number and intended number of iterations, the same as if
<code>tcltk</code> is not installed.  It is not recommended that the
<code>"tk"</code> be used for simulations requiring fewer than 10 seconds for
more than 100 iterations, as the time required to update the pop-up
window will be more than the time required to do the simulations.  This
problem can be solved by specifying, for example, <code>every=10</code> to
<code>setPb</code> or to the function created by <code>setPb</code>, or by using
<code>options(showevery=10)</code> before <code>setPb</code> is called.  If
<code>options(showprogress=)</code> is not specified, progress is shown in the
console with an iteration counter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setPb(n, type = c("Monte Carlo Simulation", "Bootstrap",
                  "Cross-Validation"),
         label, usetk = TRUE, onlytk=FALSE, every=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setPb_+3A_n">n</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="setPb_+3A_type">type</code></td>
<td>
<p>type of simulation.  Used for the progress bar title if
<code>tcltk</code> is being used.</p>
</td></tr>
<tr><td><code id="setPb_+3A_label">label</code></td>
<td>
<p>used to customize the bar label if present, overriding <code>type</code></p>
</td></tr>
<tr><td><code id="setPb_+3A_usetk">usetk</code></td>
<td>
<p>set to <code>FALSE</code> to override, acting as though the
<code>tcltk</code> package were not installed</p>
</td></tr>
<tr><td><code id="setPb_+3A_onlytk">onlytk</code></td>
<td>
<p>set to <code>TRUE</code> to not write to the console even if
<code>tcltk</code> is unavailable and <code>showprogressbar</code> is not
<code>FALSE</code> or <code>"none"</code></p>
</td></tr>
<tr><td><code id="setPb_+3A_every">every</code></td>
<td>
<p>print a message for every <code>every</code> iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a function that should be called by the user once per iteration,
specifying the iteration number as the sole argument</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="tcltk.html#topic+tkProgressBar">tkProgressBar</a></code>, <code><a href="tcltk.html#topic+tkProgressBar">setTkProgressBar</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
options(showprogress=TRUE)   # same as ='tk'
pb &lt;- setPb(1000)
for(i in 1:1000) {
   pb(i)   # pb(i, every=10) to only show for multiples of 10
   # your calculations
  }
# Force rms functions to do simulations to not report progress
options(showprogress='none')
# For functions that do simulations to use the console instead of pop-up
# Even with tcltk is installed
options(showprogress='console')
pb &lt;- setPb(1000, label='Random Sampling')

## End(Not run)
</code></pre>

<hr>
<h2 id='specs.rms'>rms Specifications for Models</h2><span id='topic+specs.rms'></span><span id='topic+specs'></span><span id='topic+print.specs.rms'></span>

<h3>Description</h3>

<p>Prints the design specifications, e.g., number of parameters for each
factor, levels of categorical factors, knot locations in splines,
pre-transformations, etc. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specs(fit, ...)
## S3 method for class 'rms'
specs(fit, long=FALSE, ...)

## S3 method for class 'specs.rms'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="specs.rms_+3A_fit">fit</code></td>
<td>

<p>a fit object created with the <code>rms</code> library in effect
</p>
</td></tr>
<tr><td><code id="specs.rms_+3A_x">x</code></td>
<td>

<p>an object returned by <code>specs</code>
</p>
</td></tr>
<tr><td><code id="specs.rms_+3A_long">long</code></td>
<td>

<p>if  <code>TRUE</code>, causes the plotting and estimation limits to be printed for
each factor
</p>
</td></tr>
<tr><td><code id="specs.rms_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing information about the fit and the predictors as elements
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+latexrms">latexrms</a></code>, <code><a href="#topic+datadist">datadist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
blood.pressure &lt;- rnorm(200, 120, 15)
dd &lt;- datadist(blood.pressure)
options(datadist='dd')
L &lt;- .03*(blood.pressure-120)
sick &lt;- ifelse(runif(200) &lt;= plogis(L), 1, 0)
f &lt;- lrm(sick ~ rcs(blood.pressure,5))
specs(f)    # find out where 5 knots are placed
g &lt;- Glm(sick ~ rcs(blood.pressure,5), family=binomial)
specs(g,long=TRUE)
options(datadist=NULL)
</code></pre>

<hr>
<h2 id='summary.rms'>Summary of Effects in Model</h2><span id='topic+summary.rms'></span><span id='topic+print.summary.rms'></span><span id='topic+latex.summary.rms'></span><span id='topic+html.summary.rms'></span><span id='topic+plot.summary.rms'></span>

<h3>Description</h3>

<p><code>summary.rms</code> forms a summary of the effects of each
factor.  When <code>summary</code> is used to estimate odds or hazard ratios for
continuous variables, it allows the levels of interacting factors to be
easily set, as well as allowing the user to choose the interval for the
effect. This method of estimating effects allows for nonlinearity in
the predictor.  Factors requiring multiple parameters are handled, as
<code>summary</code> obtains predicted values at the needed points and takes
differences.  By default, inter-quartile range effects (odds ratios,
hazards ratios, etc.) are printed for continuous factors, and all
comparisons with the reference level are made for categorical factors.
<code>print.summary.rms</code> prints the results, <code>latex.summary.rms</code>
and <code>html.summary.rms</code> typeset the results, and <code>plot.summary.rms</code>
plots shaded confidence bars to display the results graphically.
The longest confidence bar on each page is labeled with confidence levels
(unless this bar has been ignored due to <code>clip</code>).  By default, the
following confidence levels are all shown: .9, .95, and .99, using 
blue of different transparencies.  The <code>plot</code> method currently
ignores bootstrap and Bayesian highest posterior density intervals but approximates
intervals based on standard errors.  The <code>html</code> method is for use
with R Markdown using html.
</p>
<p>The <code>print</code> method will call the <code>latex</code> or <code>html</code> method
if <code>options(prType=)</code> is set to <code>"latex"</code> or <code>"html"</code>.
For <code>"latex"</code> printing through <code>print()</code>, the LaTeX table
environment is turned off.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>
<p>If <code>usebootcoef=TRUE</code> and the fit was run through <code>bootcov</code>,
the confidence intervals are bootstrap nonparametric percentile
confidence intervals, basic bootstrap, or BCa intervals, obtained on contrasts
evaluated on all bootstrap samples.
</p>
<p>If <code>options(grType='plotly')</code> is in effect and the <code>plotly</code>
package is installed, <code>plot</code> is used instead of base graphics to
draw the point estimates and confidence limits when the <code>plot</code>
method for <code>summary</code> is called.  Colors and other graphical
arguments to <code>plot.summary</code> are ignored in this case.  Various
special effects are implemented such as only drawing 0.95 confidence
limits by default but including a legend that allows the other CLs to be
activated.  Hovering over point estimates shows adjustment values if
there are any.  <code>nbar</code> is not implemented for <code>plotly</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rms'
summary(object, ..., ycut=NULL, est.all=TRUE, antilog,
conf.int=.95, abbrev=FALSE, vnames=c("names","labels"),
conf.type=c('individual','simultaneous'),
usebootcoef=TRUE, boot.type=c("percentile","bca","basic"),
posterior.summary=c('mean', 'median', 'mode'), verbose=FALSE)

## S3 method for class 'summary.rms'
print(x, ..., table.env=FALSE)

## S3 method for class 'summary.rms'
latex(object, title, table.env=TRUE, ...)

## S3 method for class 'summary.rms'
html(object, digits=4, dec=NULL, ...)

## S3 method for class 'summary.rms'
plot(x, at, log=FALSE,
    q=c(0.9, 0.95, 0.99), xlim, nbar, cex=1, nint=10,
    cex.main=1, clip=c(-1e30,1e30), main,
    col=rgb(red=.1,green=.1,blue=.8,alpha=c(.1,.4,.7)),
    col.points=rgb(red=.1,green=.1,blue=.8,alpha=1), pch=17,
    lwd=if(length(q) == 1) 3 else 2 : (length(q) + 1), digits=4,
    declim=4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.rms_+3A_object">object</code></td>
<td>

<p>a <code>rms</code> fit object.  Either <code>options(datadist)</code> should have
been set before the fit, or <code>datadist()</code> and
<code>options(datadist)</code> run before <code>summary</code>.  For <code>latex</code> is
the result of <code>summary</code>.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_...">...</code></td>
<td>

<p>For <code>summary</code>, omit list of variables to estimate effects for all
predictors. Use a list 
of variables of the form <code>age, sex</code> to estimate using default
ranges. Specify <code>age=50</code> for example to adjust age to 50 when testing
other factors (this will only matter for factors that interact with age).
Specify e.g. <code>age=c(40,60)</code> to estimate the effect of increasing age from
40 to 60. Specify <code>age=c(40,50,60)</code> to let age range from 40 to 60 and
be adjusted to 50 when testing other interacting factors. For category
factors, a single value specifies the reference cell and the adjustment
value. For example, if <code>treat</code> has levels <code>"a", "b"</code> and
<code>"c"</code> and <code>treat="b"</code> is given to <code>summary</code>,
treatment <code>a</code> will be compared to <code>b</code> and <code>c</code> will be
compared to <code>b</code>. Treatment <code>b</code> will be used when
estimating the effect of other factors. Category variables can have
category labels listed (in quotes), or an unquoted number that is a
legal level, if all levels  are numeric.  You need only use the
first few letters of each variable name - enough for unique
identification. For variables not defined with <code>datadist</code>, you
must specify 3 values, none of which are <code>NA</code>.
</p>
<p>Also represents other arguments to pass to <code>latex</code>, is ignored for
<code>print</code> and <code>plot</code>.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_ycut">ycut</code></td>
<td>
<p>must be specified if the fit is a partial proportional odds
model.  Specifies the single value of the response variable used to
estimate ycut-specific regression effects, e.g., odds ratios</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_est.all">est.all</code></td>
<td>

<p>Set to <code>FALSE</code> to only estimate effects of variables listed. Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_antilog">antilog</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress printing of anti-logged effects. Default
is <code>TRUE</code> if the model was fitted by <code>lrm</code> or <code>cph</code>.
Antilogged effects will be odds ratios for logistic models and hazard ratios
for proportional hazards models.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_conf.int">conf.int</code></td>
<td>

<p>Defaults to <code>.95</code> for <code>95%</code> confidence intervals of effects.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_abbrev">abbrev</code></td>
<td>

<p>Set to <code>TRUE</code> to use the <code>abbreviate</code> function to shorten
factor levels for categorical variables in the model.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_vnames">vnames</code></td>
<td>

<p>Set to <code>"labels"</code> to use variable labels to label effects.
Default is <code>"names"</code> to use variable names.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_conf.type">conf.type</code></td>
<td>

<p>The default type of confidence interval computed for a given
individual (1 d.f.) contrast is a pointwise confidence interval.  Set
<code>conf.type="simultaneous"</code> to use the <code>multcomp</code> package's
<code>glht</code> and <code>confint</code> functions to compute confidence
intervals with simultaneous (family-wise) coverage, thus adjusting for
multiple comparisons.  Contrasts are simultaneous only over groups of
intervals computed together.  
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_usebootcoef">usebootcoef</code></td>
<td>

<p>If <code>fit</code> was the result of <code>bootcov</code> but you want to use the
bootstrap covariance matrix instead of the nonparametric percentile,
basic, or BCa methods for confidence intervals (which uses all the bootstrap
coefficients), specify <code>usebootcoef=FALSE</code>.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_boot.type">boot.type</code></td>
<td>
<p>set to <code>'bca'</code> to compute BCa confidence
limits or to <code>'basic'</code> to use the basic bootstrap.  The default
is to compute percentile intervals.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_posterior.summary">posterior.summary</code></td>
<td>
<p>set to <code>'mode'</code> or <code>'median'</code> to use the posterior
mean/median instead of the mean for point estimates of contrasts</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_verbose">verbose</code></td>
<td>
<p>set to <code>TRUE</code> when <code>conf.type='simultaneous'</code>
to get output describing scope of simultaneous adjustments</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_x">x</code></td>
<td>
<p>result of <code>summary</code></p>
</td></tr>
<tr><td><code id="summary.rms_+3A_title">title</code></td>
<td>

<p><code>title</code> to pass to <code>latex</code>.  Default is name of fit object passed to
<code>summary</code> prefixed with <code>"summary"</code>.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_table.env">table.env</code></td>
<td>
<p>see <code><a href="Hmisc.html#topic+latex">latex</a></code></p>
</td></tr>
<tr><td><code id="summary.rms_+3A_digits">digits</code>, <code id="summary.rms_+3A_dec">dec</code></td>
<td>
<p>for <code>html.summary.rms</code>; <code>digits</code> is the
number of significant digits for printing for effects, standard
errors, and confidence limits.  It is ignored if <code>dec</code> is
given. The statistics are rounded to <code>dec</code> digits to the right of
the decimal point of <code>dec</code> is given.  <code>digits</code> is also the
number of significant digits to format numeric hover text and labels
for <code>plotly</code>.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_declim">declim</code></td>
<td>
<p>number of digits to the right of the decimal point to
which to round confidence limits for labeling axes</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_at">at</code></td>
<td>

<p>vector of coordinates at which to put tick mark labels on the main axis.  If
<code>log=TRUE</code>, <code>at</code> should be in anti-log units.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_log">log</code></td>
<td>

<p>Set to <code>TRUE</code> to plot on <code class="reqn">X\beta</code> scale but labeled with
anti-logs. 
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_q">q</code></td>
<td>
<p>scalar or vector of confidence coefficients to depict</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_xlim">xlim</code></td>
<td>

<p>X-axis limits for <code>plot</code> in units of the linear predictors (log scale
if <code>log=TRUE</code>).  If <code>at</code> is specified and <code>xlim</code> is
omitted, <code>xlim</code> is derived from the range of <code>at</code>.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_nbar">nbar</code></td>
<td>

<p>Sets up plot to leave room for <code>nbar</code> horizontal bars.  Default is the
number of non-interaction factors in the model.  Set <code>nbar</code> to a larger
value to keep too much surrounding space from appearing around horizontal
bars.  If <code>nbar</code> is smaller than the number of bars, the plot is divided
into multiple pages with up to <code>nbar</code> bars on each page.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_cex">cex</code></td>
<td>
<p><code>cex</code> parameter for factor labels.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_nint">nint</code></td>
<td>
<p>Number of tick mark numbers for <code>pretty</code>.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_cex.main">cex.main</code></td>
<td>
<p><code>cex</code> parameter for main title.  Set to <code>0</code> to
suppress the title.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_clip">clip</code></td>
<td>

<p>confidence limits outside the interval <code>c(clip[1], clip[2])</code> will be
ignored, and <code>clip</code> also be respected when computing <code>xlim</code>
when <code>xlim</code> is not specified.  <code>clip</code> should be in the units of
<code>fun(x)</code>.  If <code>log=TRUE</code>, <code>clip</code> should be in <code class="reqn">X\beta</code> units. 
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_main">main</code></td>
<td>

<p>main title.  Default is inferred from the model and value of <code>log</code>,
e.g., <code>"log Odds Ratio"</code>.
</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_col">col</code></td>
<td>
<p>vector of colors, one per value of <code>q</code></p>
</td></tr>
<tr><td><code id="summary.rms_+3A_col.points">col.points</code></td>
<td>
<p>color for points estimates</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_pch">pch</code></td>
<td>
<p>symbol for point estimates.  Default is solid triangle.</p>
</td></tr>
<tr><td><code id="summary.rms_+3A_lwd">lwd</code></td>
<td>
<p>line width for confidence intervals, corresponding to
<code>q</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>summary.rms</code>, a matrix of class <code>summary.rms</code> 
with rows corresponding to factors in
the model and columns containing the low and high values for the effects,
the range for the effects, the effect point estimates (difference in
predicted values for high and low factor values), the standard error
of this effect estimate, and the lower and upper confidence limits.
If <code>fit$scale.pred</code> has a second level, two rows appear for each factor,
the second corresponding to anti&ndash;logged effects. Non&ndash;categorical factors
are stored first, and effects for any categorical factors are stored at
the end of the returned matrix.  <code>scale.pred</code> and <code>adjust</code>.  <code>adjust</code>
is a character string containing levels of adjustment variables, if
there are any interactions.  Otherwise it is &quot;&quot;.
<code>latex.summary.rms</code> returns an object of class <code>c("latex","file")</code>.
It requires the <code>latex</code> function in Hmisc.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Hui Nian<br />  
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>,
<code><a href="#topic+rmsMisc">rmsMisc</a></code>, 
<code><a href="Hmisc.html#topic+Misc">Misc</a></code>, <code><a href="base.html#topic+pretty">pretty</a></code>, <code><a href="#topic+contrast.rms">contrast.rms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
set.seed(17) # so can reproduce the results
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))
label(age)            &lt;- 'Age'      # label is in Hmisc
label(cholesterol)    &lt;- 'Total Cholesterol'
label(blood.pressure) &lt;- 'Systolic Blood Pressure'
label(sex)            &lt;- 'Sex'
units(cholesterol)    &lt;- 'mg/dl'   # uses units.default in Hmisc
units(blood.pressure) &lt;- 'mmHg'


# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


ddist &lt;- datadist(age, blood.pressure, cholesterol, sex)
options(datadist='ddist')


fit &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)))


s &lt;- summary(fit)                # Estimate effects using default ranges
                                 # Gets odds ratio for age=3rd quartile
                                 # compared to 1st quartile
## Not run: 
latex(s)                         # Use LaTeX to print nice version
latex(s, file="")                # Just write LaTeX code to console
html(s)                          # html/LaTeX to console for knitr
# Or:
options(prType='latex')
summary(fit)                     # prints with LaTeX, table.env=FALSE
options(prType='html')
summary(fit)                     # prints with html

## End(Not run)
summary(fit, sex='male', age=60) # Specify ref. cell and adjustment val
summary(fit, age=c(50,70))       # Estimate effect of increasing age from
                                 # 50 to 70
s &lt;- summary(fit, age=c(50,60,70)) 
                                 # Increase age from 50 to 70, adjust to
                                 # 60 when estimating effects of other factors
#Could have omitted datadist if specified 3 values for all non-categorical
#variables (1 value for categorical ones - adjustment level)
plot(s, log=TRUE, at=c(.1,.5,1,1.5,2,4,8))


options(datadist=NULL)
</code></pre>

<hr>
<h2 id='survest.cph'>
Cox Survival Estimates
</h2><span id='topic+survest'></span><span id='topic+survest.cph'></span>

<h3>Description</h3>

<p>Compute survival probabilities and optional confidence limits for
Cox survival models.  If <code>x=TRUE, y=TRUE</code> were specified to <code>cph</code>,
confidence limits use the correct formula for any combination of
predictors. Otherwise, if <code>surv=TRUE</code> was specified to <code>cph</code>,
confidence limits are based only on standard errors of <code>log(S(t))</code>
at the mean value of <code class="reqn">X\beta</code>. If the model 
contained only stratification factors, or if predictions are being
requested near the mean of each covariable, this approximation will be
accurate. Unless <code>times</code> is given, at most one observation may be
predicted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survest(fit, ...)
## S3 method for class 'cph'
survest(fit, newdata, linear.predictors, x, times, 
        fun, loglog=FALSE, conf.int=0.95, type, vartype,
        conf.type=c("log", "log-log", "plain", "none"), se.fit=TRUE,
        what=c('survival','parallel'),
        individual=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survest.cph_+3A_fit">fit</code></td>
<td>

<p>a model fit from <code>cph</code>
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_newdata">newdata</code></td>
<td>

<p>a data frame containing predictor variable combinations for which
predictions are desired
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_linear.predictors">linear.predictors</code></td>
<td>

<p>a vector of linear predictor values (centered) for which predictions
are desired. If the model is stratified, the &quot;strata&quot; attribute
must be attached to this vector (see example).
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_x">x</code></td>
<td>

<p>a design matrix at which to compute estimates, with any strata attached
as a &quot;strata&quot; attribute. Only one of <code>newdata</code>,
<code>linear.predictors</code>, or <code>x</code> may be specified.  If none is
specified, but <code>times</code> is specified, you will get survival
predictions at all subjects' linear predictor and strata values.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_times">times</code></td>
<td>

<p>a vector of times at which to get predictions. If omitted, predictions
are made at all unique failure times in the original input data.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_loglog">loglog</code></td>
<td>

<p>set to <code>TRUE</code> to make the <code>log-log</code> transformation of survival
estimates and confidence limits.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_fun">fun</code></td>
<td>

<p>any function to transform the estimates and confidence limits (<code>loglog</code>
is a special case)
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_conf.int">conf.int</code></td>
<td>

<p>set to <code>FALSE</code> or <code>0</code> to suppress confidence limits, or e.g. <code>.95</code> to 
cause 0.95 confidence limits to be computed
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_type">type</code></td>
<td>

<p>see <code>survfit.coxph</code>
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_vartype">vartype</code></td>
<td>

<p>see <code>survfit.coxph</code>
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_conf.type">conf.type</code></td>
<td>

<p>specifies the basis for computing confidence limits. <code>"log"</code> is the
default as in the <code>survival</code> package.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_se.fit">se.fit</code></td>
<td>

<p>set to <code>TRUE</code> to get standard errors of log predicted survival
(no matter what <code>conf.type</code> is).
If <code>FALSE</code>, confidence limits are suppressed.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_individual">individual</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>survfit</code> interpret <code>newdata</code> as
specifying a covariable path for a single individual (represented by
multiple records).
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_what">what</code></td>
<td>

<p>Normally use <code>what="survival"</code> to estimate survival probabilities at
times that may not correspond to the subjects' own times.
<code>what="parallel"</code> assumes that the length of <code>times</code> is the number of
subjects (or one), and causes <code>survest</code> to estimate the ith subject's survival
probability at the ith value of <code>times</code> (or at the scalar value of <code>times</code>).
<code>what="parallel"</code> is used by <code>val.surv</code> for example.
</p>
</td></tr>
<tr><td><code id="survest.cph_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result is passed through <code>naresid</code> if <code>newdata</code>,
<code>linear.predictors</code>, and <code>x</code> are not specified, to restore
placeholders for <code>NA</code>s.
</p>


<h3>Value</h3>

<p>If <code>times</code> is omitted, returns a list with the elements
<code>time</code>, <code>n.risk</code>, <code>n.event</code>, <code>surv</code>, <code>call</code>
(calling statement), and optionally <code>std.err</code>, <code>upper</code>,
<code>lower</code>, <code>conf.type</code>, <code>conf.int</code>. The estimates in this
case correspond to one subject. If <code>times</code> is specified, the
returned list has possible components <code>time</code>, <code>surv</code>,
<code>std.err</code>, <code>lower</code>, and <code>upper</code>. These will be matrices
(except for <code>time</code>) if more than one subject is being predicted,
with rows representing subjects and columns representing <code>times</code>.
If <code>times</code> has only one time, these are reduced to vectors with
the number of elements equal to the number of subjects.  </p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+survfit.cph">survfit.cph</a></code>, <code><a href="survival.html#topic+survfit.coxph">survfit.coxph</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>, <code><a href="#topic+survplot">survplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from a population model in which the log hazard
# function is linear in age and there is no age x sex interaction
# Proportional hazards holds for both variables but we
# unnecessarily stratify on sex to see what happens
require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, TRUE))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
dt &lt;- -log(runif(n))/h
label(dt) &lt;- 'Follow-up Time'
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
dd &lt;- datadist(age, sex)
options(datadist='dd')
Srv &lt;- Surv(dt,e)


f &lt;- cph(Srv ~ age*strat(sex), x=TRUE, y=TRUE) #or surv=T
survest(f, expand.grid(age=c(20,40,60),sex=c("Male","Female")),
	    times=c(2,4,6), conf.int=.9)
f &lt;- update(f, surv=TRUE)
lp &lt;- c(0, .5, 1)
f$strata   # check strata names
attr(lp,'strata') &lt;- rep(1,3)  # or rep('sex=Female',3)
survest(f, linear.predictors=lp, times=c(2,4,6))

# Test survest by comparing to survfit.coxph for a more complex model
f &lt;- cph(Srv ~ pol(age,2)*strat(sex), x=TRUE, y=TRUE)
survest(f, data.frame(age=median(age), sex=levels(sex)), times=6)

age2 &lt;- age^2
f2 &lt;- coxph(Srv ~ (age + age2)*strata(sex))
new &lt;- data.frame(age=median(age), age2=median(age)^2, sex='Male')
summary(survfit(f2, new), times=6)
new$sex &lt;- 'Female'
summary(survfit(f2, new), times=6)

options(datadist=NULL)
</code></pre>

<hr>
<h2 id='survest.psm'>Parametric Survival Estimates</h2><span id='topic+survest.psm'></span><span id='topic+print.survest.psm'></span>

<h3>Description</h3>

<p>Computes predicted survival probabilities or hazards and optionally confidence
limits (for survival only) for parametric survival models fitted with
<code>psm</code>. 
If getting predictions for more than one observation, <code>times</code> must
be specified. For a model without predictors, no input data are
specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psm'
survest(fit, newdata, linear.predictors, x, times, fun,
        loglog=FALSE, conf.int=0.95,
        what=c("survival","hazard","parallel"), ...)

## S3 method for class 'survest.psm'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survest.psm_+3A_fit">fit</code></td>
<td>

<p>fit from <code>psm</code>
</p>
</td></tr>
<tr><td><code id="survest.psm_+3A_newdata">newdata</code>, <code id="survest.psm_+3A_linear.predictors">linear.predictors</code>, <code id="survest.psm_+3A_x">x</code>, <code id="survest.psm_+3A_times">times</code>, <code id="survest.psm_+3A_conf.int">conf.int</code></td>
<td>

<p>see <code>survest.cph</code>. One of <code>newdata</code>, <code>linear.predictors</code>, <code>x</code> must be given.
<code>linear.predictors</code> includes the intercept.
If <code>times</code> is omitted, predictions are made at 200 equally spaced points
between 0 and the maximum failure/censoring time used to fit the model.
</p>
<p><code>x</code> can also be a result from <code>survest.psm</code>.
</p>
</td></tr>
<tr><td><code id="survest.psm_+3A_what">what</code></td>
<td>

<p>The default is to compute survival probabilities.  Set <code>what="hazard"</code> or
some abbreviation of <code>"hazard"</code> to compute hazard rates.
<code>what="parallel"</code> assumes that the length of <code>times</code> is the number of
subjects (or one), and causes <code>survest</code> to estimate the
<code class="reqn">i^{th}</code> subject's survival probability at the <code class="reqn">i^{th}</code> value of
<code>times</code> (or at the scalar value of <code>times</code>). 
<code>what="parallel"</code> is used by <code>val.surv</code> for example.
</p>
</td></tr>
<tr><td><code id="survest.psm_+3A_loglog">loglog</code></td>
<td>

<p>set to <code>TRUE</code> to transform survival estimates and confidence limits using
log-log
</p>
</td></tr>
<tr><td><code id="survest.psm_+3A_fun">fun</code></td>
<td>

<p>a function to transform estimates and optional confidence intervals
</p>
</td></tr>
<tr><td><code id="survest.psm_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals are based on asymptotic normality of the linear
predictors.
The intervals account for the fact that a scale parameter may have been
estimated jointly with beta.
</p>


<h3>Value</h3>

<p>see <code>survest.cph</code>. If the model has no predictors, predictions are
made with respect to varying time only, and the returned object
is of class <code>"npsurv"</code> so the survival curve can be plotted
with <code>survplot.npsurv</code>. If <code>times</code> is omitted, the
entire survival curve or hazard from <code>t=0,...,fit$maxtime</code> is estimated, with
increments computed to yield 200 points where <code>fit$maxtime</code> is the
maximum survival time in the data used in model fitting. Otherwise,
the <code>times</code> vector controls the time points used.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psm">psm</a></code>, <code><a href="survival.html#topic+survreg">survreg</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="survival.html#topic+survfit">survfit</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>, <code><a href="#topic+survplot">survplot</a></code>,
<code><a href="survival.html#topic+survreg.distributions">survreg.distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from a proportional hazards population model
require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50))
dt &lt;- -log(runif(n))/h
label(dt) &lt;- 'Follow-up Time'
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
S &lt;- Surv(dt,e)

f &lt;- psm(S ~ lsp(age,c(40,70)))
survest(f, data.frame(age=seq(20,80,by=5)), times=2)

#Get predicted survival curve for 40 year old
survest(f, data.frame(age=40))

#Get hazard function for 40 year old
survest(f, data.frame(age=40), what="hazard")$surv #still called surv
</code></pre>

<hr>
<h2 id='survfit.cph'>
Cox Predicted Survival
</h2><span id='topic+survfit.cph'></span>

<h3>Description</h3>

<p>This is a slightly modified version of Therneau's <code>survfit.coxph</code>
function.  The difference is that <code>survfit.cph</code> assumes that
<code>x=TRUE,y=TRUE</code> were specified to the fit. This assures that the
environment in effect at the time of the fit (e.g., automatic knot
estimation for spline functions) is the same one used for basing predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cph'
survfit(formula, newdata, se.fit=TRUE, conf.int=0.95, 
        individual=FALSE, type=NULL, vartype=NULL,
        conf.type=c('log', "log-log", "plain", "none"), id, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit.cph_+3A_formula">formula</code></td>
<td>

<p>a fit object from <code>cph</code> or <code>coxph</code>
see <code><a href="survival.html#topic+survfit.coxph">survfit.coxph</a></code>
</p>
</td></tr>
<tr><td><code id="survfit.cph_+3A_newdata">newdata</code>, <code id="survfit.cph_+3A_se.fit">se.fit</code>, <code id="survfit.cph_+3A_conf.int">conf.int</code>, <code id="survfit.cph_+3A_individual">individual</code>, <code id="survfit.cph_+3A_type">type</code>, <code id="survfit.cph_+3A_vartype">vartype</code>, <code id="survfit.cph_+3A_conf.type">conf.type</code>, <code id="survfit.cph_+3A_id">id</code></td>
<td>
<p>see
<code><a href="survival.html#topic+survfit">survfit</a></code>.  If <code>individual</code> is <code>TRUE</code>,
there must be exactly one <code>Surv</code> object in <code>newdata</code>.  This 
object is used to specify time intervals for time-dependent covariate
paths.  To get predictions for multiple subjects with time-dependent
covariates, specify a vector <code>id</code> which specifies unique
hypothetical subjects.  The length of <code>id</code> should equal the
number of rows in <code>newdata</code>.</p>
</td></tr>
<tr><td><code id="survfit.cph_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code>survfit.coxph</code></p>


<h3>See Also</h3>

<p><code><a href="#topic+survest.cph">survest.cph</a></code></p>

<hr>
<h2 id='survplot'>Plot Survival Curves and Hazard Functions</h2><span id='topic+survplot'></span><span id='topic+survplotp'></span><span id='topic+survplot.rms'></span><span id='topic+survplot.npsurv'></span><span id='topic+survplotp.npsurv'></span><span id='topic+survdiffplot'></span>

<h3>Description</h3>

<p>Plot estimated survival curves, and for parametric survival models, plot
hazard functions.  There is an option to print the number of subjects
at risk at the start of each time interval.  Curves are automatically
labeled at the points of maximum separation (using the <code>labcurve</code>
function), and there are many other options for labeling that can be
specified with the <code>label.curves</code> parameter.  For example, different
plotting symbols can be placed at constant x-increments and a legend
linking the symbols with category labels can automatically positioned on
the most empty portion of the plot.
</p>
<p>For the case of a two stratum analysis by <code>npsurv</code>,
<code>survdiffplot</code> plots the difference in two Kaplan-Meier estimates
along with approximate confidence bands for the differences, with a
reference line at zero.  The number of subjects at risk is optionally
plotted.  This number is taken as the minimum of the number of subjects
at risk over the two strata.  When <code>conf='diffbands'</code>,
<code>survdiffplot</code> instead does not make a new plot but adds a shaded
polygon to an existing plot, showing the midpoint of two survival
estimates plus or minus 1/2 the width of the confidence interval for the
difference of two Kaplan-Meier estimates.
</p>
<p><code>survplotp</code> creates an interactive <code>plotly</code> graphic with
shaded confidence bands.  In the two strata case, it draws the 1/2
confidence bands for the difference in two probabilities centered at the
midpoint of the probability estimates, so that where the two curves
touch this band there is no significant difference (no multiplicity
adjustment is made).  For the two strata case, the two individual
confidence bands have entries in the legend but are not displayed until
the user clicks on the legend.
</p>
<p>When <code>code</code> was from running <code>npsurv</code> on a
multi-state/competing risk <code>Surv</code> object, <code>survplot</code> plots
cumulative incidence curves properly accounting for competing risks.
You must specify exactly one state/event cause to plot using the
<code>state</code> argument.  <code>survplot</code> will not plot multiple states on
one graph.  This can be accomplished using multiple calls with different
values of <code>state</code> and specifying <code>add=TRUE</code> for all but the
first call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survplot(fit, ...)
survplotp(fit, ...)
## S3 method for class 'rms'
survplot(fit, ..., xlim,
         ylim=if(loglog) c(-5, 1.5) else if
                 (what == "survival" &amp; missing(fun)) c(0, 1),
         xlab, ylab, time.inc,
         what=c("survival","hazard"),
         type=c("tsiatis","kaplan-meier"),
         conf.type=c("log","log-log","plain","none"),
         conf.int=FALSE, conf=c("bands","bars"), mylim=NULL,
         add=FALSE, label.curves=TRUE,
         abbrev.label=FALSE, levels.only=FALSE,
         lty, lwd=par("lwd"),
         col=1, col.fill=gray(seq(.95, .75, length=5)),
         adj.subtitle=TRUE, loglog=FALSE, fun,
         n.risk=FALSE, logt=FALSE, dots=FALSE, dotsize=.003,
         grid=NULL, srt.n.risk=0, sep.n.risk=0.056, adj.n.risk=1, 
         y.n.risk, cex.n.risk=.6, cex.xlab=par('cex.lab'),
         cex.ylab=cex.xlab, pr=FALSE)
## S3 method for class 'npsurv'
survplot(fit, xlim, 
         ylim, xlab, ylab, time.inc, state=NULL,
         conf=c("bands","bars","diffbands","none"), mylim=NULL,
         add=FALSE, label.curves=TRUE, abbrev.label=FALSE,
         levels.only=FALSE, lty,lwd=par('lwd'),
         col=1, col.fill=gray(seq(.95, .75, length=5)),
         loglog=FALSE, fun, n.risk=FALSE, aehaz=FALSE, times=NULL,
         logt=FALSE, dots=FALSE, dotsize=.003, grid=NULL,
         srt.n.risk=0, sep.n.risk=.056, adj.n.risk=1,
         y.n.risk, cex.n.risk=.6, cex.xlab=par('cex.lab'), cex.ylab=cex.xlab,
         pr=FALSE, ...)
## S3 method for class 'npsurv'
survplotp(fit, xlim, ylim, xlab, ylab, time.inc, state=NULL,
         conf=c("bands", "none"), mylim=NULL, abbrev.label=FALSE,
         col=colorspace::rainbow_hcl,  levels.only=TRUE,
         loglog=FALSE, fun=function(y) y, aehaz=FALSE, times=NULL,
         logt=FALSE, pr=FALSE, ...)
survdiffplot(fit, order=1:2, fun=function(y) y,
           xlim, ylim, xlab, ylab="Difference in Survival Probability",
           time.inc, conf.int, conf=c("shaded", "bands","diffbands","none"),
           add=FALSE, lty=1, lwd=par('lwd'), col=1,
           n.risk=FALSE, grid=NULL,
           srt.n.risk=0, adj.n.risk=1,
           y.n.risk, cex.n.risk=.6, cex.xlab=par('cex.lab'),
           cex.ylab=cex.xlab, convert=function(f) f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survplot_+3A_fit">fit</code></td>
<td>

<p>result of fit (<code>cph</code>, <code>psm</code>, <code>npsurv</code>,
<code>survest.psm</code>).  For <code>survdiffplot</code>, <code>fit</code> must be the
result of <code>npsurv</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_...">...</code></td>
<td>

<p>list of factors with names used in model. For fits from <code>npsurv</code>
these arguments do not appear - all strata are plotted. Otherwise the
first factor listed is the factor used to determine different survival
curves.  Any other factors are used to specify single constants to be
adjusted to, when defaults given to fitting routine (through
<code>limits</code>) are not used.  The value given to factors is the original
coding of data given to fit, except that for categorical or strata
factors the text string levels may be specified.  The form of values
given to the first factor are none (omit the equal sign to use default range
or list of all values if variable is discrete), <code>"text"</code> if factor
is categorical, <code>c(value1, value2, ...)</code>, or a function which
returns a vector, such as <code>seq(low,high,by=increment)</code>.  Only the
first factor may have the values omitted.  In this case the <code>Low
effect</code>, <code>Adjust to</code>, and <code>High effect</code> values will be used
from <code>datadist</code> if the variable is continuous.  For variables not
defined to <code>datadist</code>, you must specify non-missing constant
settings (or a vector of settings for the one displayed variable).  Note
that since <code>npsurv</code> objects do not use the variable list in 
<code>...</code>, you can specify any extra arguments to <code>labcurve</code> by
adding them at the end of the list of arguments.  For <code>survplotp</code>
... (e.g., <code>height</code>, <code>width</code>) is passed to <code>plotly::plot_ly</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_xlim">xlim</code></td>
<td>

<p>a vector of two numbers specifiying the x-axis range for follow-up time.
Default is <code>(0,maxtime)</code> where <code>maxtime</code> was the <code>pretty()</code>d version
of the maximum follow-up time
in any stratum, stored in <code>fit$maxtime</code>.  If <code>logt=TRUE</code>,
default is <code>(1, log(maxtime))</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_ylim">ylim</code></td>
<td>

<p>y-axis limits.  Default is <code>c(0,1)</code> for survival, and
<code>c(-5,1.5)</code> if <code>loglog=TRUE</code>. If <code>fun</code> or
<code>loglog=TRUE</code> are given and <code>ylim</code> is not, the limits will be
computed from the data.  For <code>what="hazard"</code>, default 
limits are computed from the first hazard function plotted.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label.  Default is <code>units</code> attribute of failure time
variable given to <code>Surv</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label.  Default is <code>"Survival Probability"</code> or 
<code>"log(-log Survival Probability)"</code>. If <code>fun</code> is given, the default
is <code>""</code>.  For <code>what="hazard"</code>, the default is
<code>"Hazard Function"</code>.  For a multi-state/competing risk application
the default is <code>"Cumulative Incidence"</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_time.inc">time.inc</code></td>
<td>

<p>time increment for labeling the x-axis and printing numbers at risk. 
If not specified, the value
of <code>time.inc</code> stored with the model fit will be used.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_state">state</code></td>
<td>
<p>the state/event cause to use in plotting if the fit was for
a multi-state/competing risk <code>Surv</code> object</p>
</td></tr>
<tr><td><code id="survplot_+3A_type">type</code></td>
<td>

<p>specifies type of estimates, <code>"tsiatis"</code> (the default) or <code>"kaplan-meier"</code>.
<code>"tsiatis"</code> here corresponds to the Breslow
estimator. This is ignored if survival estimates stored with <code>surv=TRUE</code> are
being used. For fits from <code>npsurv</code>, this argument
is also ignored, since it is specified as an argument to <code>npsurv</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_conf.type">conf.type</code></td>
<td>

<p>specifies the basis for confidence limits. This argument is
ignored for fits from <code>npsurv</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_conf.int">conf.int</code></td>
<td>

<p>Default is <code>FALSE</code>.  Specify e.g. <code>.95</code> to plot 0.95 confidence bands.
For fits from parametric survival models, or Cox models with
<code>x=TRUE</code> and <code>y=TRUE</code> specified to the fit, the exact
asymptotic formulas will be used to compute standard errors, and
confidence limits are based on <code>log(-log S(t))</code> if <code>loglog=TRUE</code>. 
If <code>x=TRUE</code> and <code>y=TRUE</code> were not specified to <code>cph</code> but
<code>surv=TRUE</code> was, the standard errors stored for the underlying
survival curve(s) will be used. These agree with the former if
predictions are requested at the mean value of X beta or if there are
only stratification factors in the model. This argument is ignored for
fits from <code>npsurv</code>, which must have previously specified
confidence interval specifications.  For <code>survdiffplot</code> if
<code>conf.int</code> is not specified, the level used in the call to
<code>npsurv</code> will be used.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_conf">conf</code></td>
<td>

<p><code>"bars"</code> for confidence bars at each <code>time.inc</code> time point. If the fit
was from <code>cph(..., surv=TRUE)</code>, the <code>time.inc</code> used will be
that stored with the fit. Use <code>conf="bands"</code> (the default) for
bands using standard errors at each failure time. For <code>npsurv</code>
objects only, <code>conf</code> may also be <code>"none"</code>, indicating that 
confidence interval information stored with the <code>npsurv</code> result
should be ignored.  For <code>npsurv</code> and <code>survdiffplot</code>,
<code>conf</code> may be <code>"diffbands"</code> whereby a shaded region is drawn
for comparing two curves.  The polygon is centered at the midpoint of
the two survival estimates and the height of the polygon is 1/2 the
width of the approximate <code>conf.int</code> pointwise confidence region.
Survival curves not overlapping the shaded area are approximately
significantly different at the <code>1 - conf.int</code> level.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_mylim">mylim</code></td>
<td>
<p>used to curtail computed <code>ylim</code>.  When <code>ylim</code> is
not given by the user, the computed limits are expanded to force
inclusion of the values specified in <code>mylim</code>.</p>
</td></tr>
<tr><td><code id="survplot_+3A_what">what</code></td>
<td>

<p>defaults to <code>"survival"</code> to plot survival estimates.  Set to
<code>"hazard"</code> or an abbreviation to plot the hazard function (for
<code>psm</code> fits only). Confidence intervals are not available for
<code>what="hazard"</code>. 
</p>
</td></tr>
<tr><td><code id="survplot_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add curves to an existing plot.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_label.curves">label.curves</code></td>
<td>

<p>default is <code>TRUE</code> to use <code>labcurve</code> to label curves where they are farthest
apart.  Set <code>label.curves</code> to a <code>list</code> to specify options to
<code>labcurve</code>, e.g., <code>label.curves=list(method="arrow", cex=.8)</code>.
These option names may be abbreviated in the usual way arguments
are abbreviated.  Use for example <code>label.curves=list(keys=1:5)</code>
to draw symbols (as in <code>pch=1:5</code> - see <code>points</code>)
on the curves and automatically position a legend
in the most empty part of the plot.  Set <code>label.curves=FALSE</code> to
suppress drawing curve labels.  The <code>col</code>, <code>lty</code>, <code>lwd</code>, and <code>type</code>
parameters are automatically passed to <code>labcurve</code>, although you
can override them here.  To distinguish curves by line types and
still have <code>labcurve</code> construct a legend, use for example
<code>label.curves=list(keys="lines")</code>.  The negative value for the
plotting symbol will suppress a plotting symbol from being drawn
either on the curves or in the legend.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_abbrev.label">abbrev.label</code></td>
<td>

<p>set to <code>TRUE</code> to <code>abbreviate()</code> curve labels that are plotted
</p>
</td></tr>
<tr><td><code id="survplot_+3A_levels.only">levels.only</code></td>
<td>

<p>set to <code>TRUE</code> to remove <code>variablename=</code> from the start of
curve labels.</p>
</td></tr>
<tr><td><code id="survplot_+3A_lty">lty</code></td>
<td>

<p>vector of line types to use for different factor levels.  Default is
<code>c(1,3,4,5,6,7,...)</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_lwd">lwd</code></td>
<td>

<p>vector of line widths to use for different factor levels.  Default is
current <code>par</code> setting for <code>lwd</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_col">col</code></td>
<td>

<p>color for curve, default is <code>1</code>.  Specify a vector to assign different
colors to different curves.  For <code>survplotp</code>, <code>col</code> is a
vector of colors corresponding to strata, or a function that will be
called to generate such colors.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_col.fill">col.fill</code></td>
<td>
<p>a vector of colors to used in filling confidence bands</p>
</td></tr>
<tr><td><code id="survplot_+3A_adj.subtitle">adj.subtitle</code></td>
<td>

<p>set to <code>FALSE</code> to suppress plotting subtitle with levels of adjustment factors
not plotted. Defaults to <code>TRUE</code>.
This argument is ignored for <code>npsurv</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_loglog">loglog</code></td>
<td>

<p>set to <code>TRUE</code> to plot <code>log(-log Survival)</code> instead of <code>Survival</code>
</p>
</td></tr>
<tr><td><code id="survplot_+3A_fun">fun</code></td>
<td>

<p>specifies any function to translate estimates and confidence limits
before plotting.  If the fit is a multi-state object the default for
<code>fun</code> is <code>function(y) 1 - y</code> to draw cumulative incidence curves.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_logt">logt</code></td>
<td>

<p>set to <code>TRUE</code> to plot <code>log(t)</code> instead of <code>t</code> on the x-axis
</p>
</td></tr>
<tr><td><code id="survplot_+3A_n.risk">n.risk</code></td>
<td>

<p>set to <code>TRUE</code> to add number of subjects at risk for each curve, using the
<code>surv.summary</code> created by <code>cph</code> or using the failure times used in
fitting the model if <code>y=TRUE</code> was specified to the fit or if the fit
was from <code>npsurv</code>.
The numbers are placed at the bottom
of the graph unless <code>y.n.risk</code> is given. 
If the fit is from <code>survest.psm</code>, <code>n.risk</code> does not apply.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_srt.n.risk">srt.n.risk</code></td>
<td>

<p>angle of rotation for leftmost number of subjects at risk (since this number
may run into the second or into the y-axis).  Default is <code>0</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_adj.n.risk">adj.n.risk</code></td>
<td>

<p>justification for leftmost number at risk. Default is <code>1</code> for right 
justification.
Use <code>0</code> for left justification, <code>.5</code> for centered.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_sep.n.risk">sep.n.risk</code></td>
<td>

<p>multiple of upper y limit - lower y limit for separating lines of text
containing number of subjects at risk.  Default is <code>.056*(ylim[2]-ylim[1])</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_y.n.risk">y.n.risk</code></td>
<td>

<p>When <code>n.risk=TRUE</code>, the default is to place numbers of patients at
risk above the x-axis.  You can specify a y-coordinate for the bottom
line of the numbers using <code>y.n.risk</code>.  Specify
<code>y.n.risk='auto'</code> to place the numbers below the x-axis at a
distance of 1/3 of the range of <code>ylim</code>.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_cex.n.risk">cex.n.risk</code></td>
<td>
 
<p>character size for number of subjects at risk (when <code>n.risk</code> is
<code>TRUE</code>) 
</p>
</td></tr>
<tr><td><code id="survplot_+3A_cex.xlab">cex.xlab</code></td>
<td>
<p><code>cex</code> for x-axis label</p>
</td></tr>
<tr><td><code id="survplot_+3A_cex.ylab">cex.ylab</code></td>
<td>
<p><code>cex</code> for y-axis label</p>
</td></tr>
<tr><td><code id="survplot_+3A_dots">dots</code></td>
<td>

<p>set to <code>TRUE</code> to plot a grid of dots.  Will be plotted at every
<code>time.inc</code> (see <code>cph</code>) and at survival increments of .1 (if
<code>d&gt;.4</code>), .05 (if <code>.2 &lt; d &lt;= .4</code>), or .025 (if <code>d &lt;= .2</code>),
where <code>d</code> is the range of survival displayed. 
</p>
</td></tr>
<tr><td><code id="survplot_+3A_dotsize">dotsize</code></td>
<td>
<p>size of dots in inches</p>
</td></tr>
<tr><td><code id="survplot_+3A_grid">grid</code></td>
<td>

<p>defaults to <code>NULL</code> (not drawing grid lines). Set to <code>TRUE</code> to
plot <code>gray(.8)</code> grid lines, or specify any color.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_pr">pr</code></td>
<td>

<p>set to <code>TRUE</code> to print survival curve coordinates used in the plots
</p>
</td></tr>
<tr><td><code id="survplot_+3A_aehaz">aehaz</code></td>
<td>
<p>set to <code>TRUE</code> to add number of events and exponential
distribution hazard rate estimates in curve labels.  For
competing risk data the number of events is for the cause of interest,
and the hazard rate is the number of events divided by the sum of all
failure and censoring times.</p>
</td></tr>
<tr><td><code id="survplot_+3A_times">times</code></td>
<td>
<p>a numeric vector of times at which to compute cumulative
incidence probability estimates to add to curve labels</p>
</td></tr>
<tr><td><code id="survplot_+3A_order">order</code></td>
<td>

<p>an integer vector of length two specifying the order of groups when
computing survival differences.  The default of <code>1:2</code> indicates
that the second group is subtracted from the first.  Specify
<code>order=2:1</code> to instead subtract the first from the second.  A
subtitle indicates what was done.
</p>
</td></tr>
<tr><td><code id="survplot_+3A_convert">convert</code></td>
<td>
<p>a function to convert the output of
<code>summary.survfitms</code> to pick off the data needed for a single state</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>survplot</code> will not work for Cox models with time-dependent covariables.
Use <code>survest</code> or <code>survfit</code> for that purpose.
</p>
<p>There is a set a system option <code><a href="Hmisc.html#topic+mgp.axis">mgp.axis.labels</a></code> to allow x
and y-axes to have differing <code>mgp</code> graphical parameters (see <code>par</code>).
This is important when labels for y-axis tick marks are to be written
horizontally (<code>par(las=1)</code>), as a larger gap between the labels and
the tick marks are needed.  You can set the axis-specific 2nd
component of <code>mgp</code> using <code>mgp.axis.labels(c(xvalue,yvalue))</code>.
</p>


<h3>Value</h3>

<p>list with components adjust (text string specifying adjustment levels)
and <code>curve.labels</code> (vector of text strings corresponding to levels
of factor used to distinguish curves). For <code>npsurv</code>, the returned
value is the vector of strata labels, or NULL if there are no strata.
</p>


<h3>Side Effects</h3>

<p>plots. If <code>par()$mar[4] &lt; 4</code>, issues <code>par(mar=)</code> to increment <code>mar[4]</code> by 2
if <code>n.risk=TRUE</code> and <code>add=FALSE</code>. The user may want to reset <code>par(mar)</code> in
this case to not leave such a wide right margin for plots. You usually
would issue <code>par(mar=c(5,4,4,2)+.1)</code>.
</p>


<h3>References</h3>

<p>Boers M (2004): Null bar and null zone are better than the error bar to
compare group means in graphs.  J Clin Epi 57:712-715.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+datadist">datadist</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+cph">cph</a></code>,
<code><a href="#topic+psm">psm</a></code>, <code><a href="#topic+survest">survest</a></code>, <code><a href="#topic+predictrms">predictrms</a></code>,
<code><a href="#topic+plot.Predict">plot.Predict</a></code>, <code><a href="#topic+ggplot.Predict">ggplot.Predict</a></code>,
<code><a href="Hmisc.html#topic+units">units</a></code>, <code><a href="Hmisc.html#topic+errbar">errbar</a></code>,  
<code><a href="survival.html#topic+survfit">survfit</a></code>, <code><a href="survival.html#topic+survreg.distributions">survreg.distributions</a></code>,
<code><a href="Hmisc.html#topic+labcurve">labcurve</a></code>,
<code><a href="Hmisc.html#topic+mgp.axis">mgp.axis</a></code>, <code><a href="graphics.html#topic+par">par</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from a population model in which the log hazard
# function is linear in age and there is no age x sex interaction
require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('male','female'), n, TRUE))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='female'))
dt &lt;- -log(runif(n))/h
label(dt) &lt;- 'Follow-up Time'
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
dd &lt;- datadist(age, sex)
options(datadist='dd')
S &lt;- Surv(dt,e)

# When age is in the model by itself and we predict at the mean age,
# approximate confidence intervals are ok

f &lt;- cph(S ~ age, surv=TRUE)
survplot(f, age=mean(age), conf.int=.95)
g &lt;- cph(S ~ age, x=TRUE, y=TRUE)
survplot(g, age=mean(age), conf.int=.95, add=TRUE, col='red', conf='bars')

# Repeat for an age far from the mean; not ok
survplot(f, age=75, conf.int=.95)
survplot(g, age=75, conf.int=.95, add=TRUE, col='red', conf='bars')


#Plot stratified survival curves by sex, adj for quadratic age effect
# with age x sex interaction (2 d.f. interaction)

f &lt;- cph(S ~ pol(age,2)*strat(sex), x=TRUE, y=TRUE)
#or f &lt;- psm(S ~ pol(age,2)*sex)
Predict(f, sex, age=c(30,50,70))
survplot(f, sex, n.risk=TRUE, levels.only=TRUE)   #Adjust age to median
survplot(f, sex, logt=TRUE, loglog=TRUE)   #Check for Weibull-ness (linearity)
survplot(f, sex=c("male","female"), age=50)
                                        #Would have worked without datadist
                                        #or with an incomplete datadist
survplot(f, sex, label.curves=list(keys=c(2,0), point.inc=2))
                                        #Identify curves with symbols


survplot(f, sex, label.curves=list(keys=c('m','f')))
                                        #Identify curves with single letters


#Plots by quintiles of age, adjusting sex to male
options(digits=3)
survplot(f, age=quantile(age,(1:4)/5), sex="male")


#Plot survival Kaplan-Meier survival estimates for males
f &lt;- npsurv(S ~ 1, subset=sex=="male")
survplot(f)


#Plot survival for both sexes and show exponential hazard estimates
f &lt;- npsurv(S ~ sex)
survplot(f, aehaz=TRUE)
#Check for log-normal and log-logistic fits
survplot(f, fun=qnorm, ylab="Inverse Normal Transform")
survplot(f, fun=function(y)log(y/(1-y)), ylab="Logit S(t)")

#Plot the difference between sexes
survdiffplot(f)

#Similar but show half-width of confidence intervals centered
#at average of two survival estimates
#See Boers (2004)
survplot(f, conf='diffbands')

options(datadist=NULL)
## Not run: 
#
# Time to progression/death for patients with monoclonal gammopathy
# Competing risk curves (cumulative incidence)
# status variable must be a factor with first level denoting right censoring
m &lt;- upData(mgus1, stop = stop / 365.25, units=c(stop='years'),
            labels=c(stop='Follow-up Time'), subset=start == 0)
f &lt;- npsurv(Surv(stop, event) ~ 1, data=m)

# Use survplot for enhanced displays of cumulative incidence curves for
# competing risks

survplot(f, state='pcm', n.risk=TRUE, xlim=c(0, 20), ylim=c(0, .5), col=2)
survplot(f, state='death', aehaz=TRUE, col=3,
         label.curves=list(keys='lines'))
f &lt;- npsurv(Surv(stop, event) ~ sex, data=m)
survplot(f, state='death', aehaz=TRUE, n.risk=TRUE, conf='diffbands',
         label.curves=list(keys='lines'))

## End(Not run)
</code></pre>

<hr>
<h2 id='val.prob'>
Validate Predicted Probabilities
</h2><span id='topic+val.prob'></span><span id='topic+print.val.prob'></span><span id='topic+plot.val.prob'></span>

<h3>Description</h3>

<p>The <code>val.prob</code> function is useful for validating
predicted probabilities against binary events.
</p>
<p>Given a set of predicted probabilities <code>p</code> or predicted log odds
<code>logit</code>, and a vector of binary outcomes <code>y</code> that were not
used in developing the predictions <code>p</code> or <code>logit</code>,
<code>val.prob</code> computes the following indexes and statistics: Somers'
<code class="reqn">D_{xy}</code> rank correlation between <code>p</code> and <code>y</code>
[<code class="reqn">2(C-.5)</code>, <code class="reqn">C</code>=ROC area], Nagelkerke-Cox-Snell-Maddala-Magee
R-squared index, Discrimination index <code>D</code> [ (Logistic model
L.R. <code class="reqn">\chi^2</code> - 1)/n], L.R. <code class="reqn">\chi^2</code>,
its <code class="reqn">P</code>-value, Unreliability index <code class="reqn">U</code>, <code class="reqn">\chi^2</code>
with 2 d.f.  for testing unreliability (H0: intercept=0, slope=1), its
<code class="reqn">P</code>-value, the quality index <code class="reqn">Q</code>, <code>Brier</code> score (average
squared difference in <code>p</code> and <code>y</code>), <code>Intercept</code>, and
<code>Slope</code>, <code class="reqn">E_{max}</code>=maximum absolute difference in predicted
and loess-calibrated probabilities, <code>Eavg</code>, the average in same,
<code>E90</code>, the 0.9 quantile of same, the Spiegelhalter <code class="reqn">Z</code>-test for 
calibration accuracy, and its two-tailed <code class="reqn">P</code>-value.  If
<code>pl=TRUE</code>, plots fitted logistic 
calibration curve and optionally a smooth nonparametric fit using
<code>lowess(p,y,iter=0)</code> and grouped proportions vs.  mean predicted
probability in group.  If the predicted probabilities or logits are
constant, the statistics are returned and no plot is made.
<code>Eavg, Emax, E90</code> were from linear logistic calibration before
rms 4.5-1.
</p>
<p>When <code>group</code> is present, different statistics are computed,
different graphs are made, and the object returned by <code>val.prob</code> is
different.  <code>group</code> specifies a stratification variable.
Validations are done separately by levels of group and overall.  A
<code>print</code> method prints summary statistics and several quantiles of
predicted probabilities, and a <code>plot</code> method plots calibration
curves with summary statistics superimposed, along with selected
quantiles of the predicted probabilities (shown as tick marks on
calibration curves).  Only the <code>lowess</code> calibration curve is
estimated.  The statistics computed are the average predicted
probability, the observed proportion of events, a 1 d.f. chi-square
statistic for testing for overall mis-calibration (i.e., a test of the
observed vs. the overall average predicted probability of the event)
(<code>ChiSq</code>), and a 2 d.f. chi-square statistic for testing
simultaneously that the intercept of a linear logistic calibration curve
is zero and the slope is one (<code>ChiSq2</code>), average absolute
calibration error (average absolute difference between the
<code>lowess</code>-estimated calibration curve and the line of identity,
labeled <code>Eavg</code>), <code>Eavg</code> divided by the difference between the
0.95 and 0.05 quantiles of predictive probabilities (<code>Eavg/P90</code>), a
&quot;median odds ratio&quot;, i.e., the anti-log of the median absolute
difference between predicted and calibrated predicted log odds of the
event (<code>Med OR</code>), the C-index (ROC area), the Brier quadratic error
score (<code>B</code>), a chi-square test of goodness of fit based on the
Brier score (<code>B ChiSq</code>), and the Brier score computed on calibrated rather than raw
predicted probabilities (<code>B cal</code>).  The first chi-square test is a
test of overall calibration accuracy (&quot;calibration in the large&quot;), and
the second will also detect errors such as slope shrinkage caused by
overfitting or regression to the mean.  See Cox (1970) for both of these
score tests.  The goodness of fit test based on the (uncalibrated) Brier
score is due to Hilden, Habbema, and Bjerregaard (1978) and is discussed
in Spiegelhalter (1986).  When <code>group</code> is present you can also
specify sampling <code>weights</code> (usually frequencies), to obtained
weighted calibration curves.
</p>
<p>To get the behavior that results from a grouping variable being present
without having a grouping variable, use <code>group=TRUE</code>.  In the
<code>plot</code> method, calibration curves are drawn and labeled by default
where they are maximally separated using the <code>labcurve</code> function.
The following parameters do not apply when <code>group</code> is present:
<code>pl</code>, <code>smooth</code>, <code>logistic.cal</code>, <code>m</code>, <code>g</code>,
<code>cuts</code>, <code>emax.lim</code>, <code>legendloc</code>, <code>riskdist</code>,
<code>mkh</code>, <code>connect.group</code>, <code>connect.smooth</code>.  The following
parameters apply to the <code>plot</code> method but not to <code>val.prob</code>:
<code>xlab</code>, <code>ylab</code>, <code>lim</code>, <code>statloc</code>, <code>cex</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>val.prob(p, y, logit, group, weights=rep(1,length(y)), normwt=FALSE, 
         pl=TRUE, smooth=TRUE, logistic.cal=TRUE,
         xlab="Predicted Probability", ylab="Actual Probability",
         lim=c(0, 1), m, g, cuts, emax.lim=c(0,1),
         legendloc=lim[1] + c(0.55 * diff(lim), 0.27 * diff(lim)), 
         statloc=c(0,0.99), riskdist=c("predicted", "calibrated"),
         cex=.7, mkh=.02,
         connect.group=FALSE, connect.smooth=TRUE, g.group=4, 
         evaluate=100, nmin=0)

## S3 method for class 'val.prob'
print(x, ...)

## S3 method for class 'val.prob'
plot(x, xlab="Predicted Probability", 
     ylab="Actual Probability",
     lim=c(0,1), statloc=lim, stats=1:12, cex=.5, 
     lwd.overall=4, quantiles=c(.05,.95), flag, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="val.prob_+3A_p">p</code></td>
<td>

<p>predicted probability
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_y">y</code></td>
<td>

<p>vector of binary outcomes
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_logit">logit</code></td>
<td>

<p>predicted log odds of outcome.  Specify either <code>p</code> or <code>logit</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_group">group</code></td>
<td>

<p>a grouping variable.  If numeric this variable is grouped into
<code>g.group</code> quantile groups (default is quartiles).  Set <code>group=TRUE</code> to
use the <code>group</code> algorithm but with a single stratum for <code>val.prob</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_weights">weights</code></td>
<td>

<p>an optional numeric vector of per-observation weights (usually frequencies),
used only if <code>group</code> is given.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_normwt">normwt</code></td>
<td>

<p>set to <code>TRUE</code> to make <code>weights</code> sum to the number of non-missing observations.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_pl">pl</code></td>
<td>

<p>TRUE to plot calibration curves and optionally statistics
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_smooth">smooth</code></td>
<td>

<p>plot smooth fit to <code>(p,y)</code> using <code>lowess(p,y,iter=0)</code>
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_logistic.cal">logistic.cal</code></td>
<td>

<p>plot linear logistic calibration fit to <code>(p,y)</code>
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label, default is <code>"Predicted Probability"</code> for <code>val.prob</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label, default is <code>"Actual Probability"</code> for
<code>val.prob</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_lim">lim</code></td>
<td>

<p>limits for both x and y axes  
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_m">m</code></td>
<td>

<p>If grouped proportions are desired, average no. observations per group
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_g">g</code></td>
<td>

<p>If grouped proportions are desired, number of quantile groups
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_cuts">cuts</code></td>
<td>

<p>If grouped proportions are desired, actual cut points for constructing
intervals, e.g. <code>c(0,.1,.8,.9,1)</code> or <code>seq(0,1,by=.2)</code>
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_emax.lim">emax.lim</code></td>
<td>

<p>Vector containing lowest and highest predicted probability over which to
compute <code>Emax</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_legendloc">legendloc</code></td>
<td>

<p>If <code>pl=TRUE</code>, list with components <code>x,y</code> or vector <code>c(x,y)</code> for upper left corner
of legend for curves and points.  Default is <code>c(.55, .27)</code> scaled to
<code>lim</code>.  Use <code>locator(1)</code> to use the mouse, <code>FALSE</code> to suppress legend.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_statloc">statloc</code></td>
<td>

<p><code class="reqn">D_{xy}</code>, <code class="reqn">C</code>, <code class="reqn">R^2</code>, <code class="reqn">D</code>, <code class="reqn">U</code>, <code class="reqn">Q</code>, <code>Brier</code> score, <code>Intercept</code>, <code>Slope</code>, and <code class="reqn">E_{max}</code>
will be added to plot, using
<code>statloc</code> as the upper left corner of a box (default is <code>c(0,.9)</code>).
You can specify a list or a vector.  Use <code>locator(1)</code>
for the mouse, <code>FALSE</code> to suppress statistics.  
This is plotted after the curve legends.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_riskdist">riskdist</code></td>
<td>

<p>Use <code>"calibrated"</code> to plot the relative frequency distribution of
calibrated probabilities after dividing into 101 bins from <code>lim[1]</code> to
<code>lim[2]</code>.
Set to <code>"predicted"</code> (the default as of rms 4.5-1) to use raw assigned risk, <code>FALSE</code> to omit risk distribution.
Values are scaled so that highest bar is <code>0.15*(lim[2]-lim[1])</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_cex">cex</code></td>
<td>

<p>Character size for legend or for table of statistics when <code>group</code> is given
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_mkh">mkh</code></td>
<td>

<p>Size of symbols for legend.   Default is 0.02 (see <code>par()</code>).
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_connect.group">connect.group</code></td>
<td>

<p>Defaults to <code>FALSE</code> to only represent group fractions as triangles.
Set to <code>TRUE</code> to also connect with a solid line.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_connect.smooth">connect.smooth</code></td>
<td>

<p>Defaults to <code>TRUE</code> to draw smoothed estimates using a dashed line.
Set to <code>FALSE</code> to instead use dots at individual estimates.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_g.group">g.group</code></td>
<td>

<p>number of quantile groups to use when <code>group</code> is given and variable is
numeric.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_evaluate">evaluate</code></td>
<td>

<p>number of points at which to store the <code>lowess</code>-calibration curve.
Default is 100.  If there are more than <code>evaluate</code> unique predicted
probabilities, <code>evaluate</code> equally-spaced quantiles of the unique
predicted probabilities, with linearly interpolated calibrated values,
are retained for plotting (and stored in the object returned by
<code>val.prob</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_nmin">nmin</code></td>
<td>

<p>applies when <code>group</code> is given.  When <code>nmin</code> <code class="reqn">&gt; 0</code>, <code>val.prob</code> will not
store coordinates of smoothed calibration curves in the outer tails,
where there are fewer than <code>nmin</code> raw observations represented in
those tails.  If for example <code>nmin</code>=50, the <code>plot</code> function will only
plot the estimated calibration curve from <code class="reqn">a</code> to <code class="reqn">b</code>, where there are
50 subjects with predicted probabilities <code class="reqn">&lt; a</code> and <code class="reqn">&gt; b</code>.
<code>nmin</code> is ignored when computing accuracy statistics.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_x">x</code></td>
<td>
<p>result of <code>val.prob</code> (with <code>group</code> in effect)</p>
</td></tr>
<tr><td><code id="val.prob_+3A_...">...</code></td>
<td>

<p>optional arguments for <code>labcurve</code> (through <code>plot</code>).  Commonly used
options are <code>col</code> (vector of colors for the strata plus overall) and
<code>lty</code>.  Ignored for <code>print</code>.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_stats">stats</code></td>
<td>

<p>vector of column numbers of statistical indexes to write on plot
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_lwd.overall">lwd.overall</code></td>
<td>

<p>line width for plotting the overall calibration curve
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_quantiles">quantiles</code></td>
<td>

<p>a vector listing which quantiles should be indicated on each
calibration curve using tick marks.  The values in <code>quantiles</code> can be
any number of values from the following: .01, .025, .05, .1, .25, .5, .75, .9, .95, .975, .99.
By default the 0.05 and 0.95 quantiles are indicated.
</p>
</td></tr>
<tr><td><code id="val.prob_+3A_flag">flag</code></td>
<td>

<p>a function of the matrix of statistics (rows representing groups)
returning a vector of character strings (one value for each group, including
&quot;Overall&quot;).  <code>plot.val.prob</code> will print this vector of character
values to the left of the statistics.  The <code>flag</code> function 
can refer to columns of the matrix used as input to the function by
their names given in the description above.  The default function
returns <code>"*"</code> if either <code>ChiSq2</code> or <code>B ChiSq</code> is
significant at the 0.01 level and <code>" "</code> otherwise.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 2 d.f. <code class="reqn">\chi^2</code> test and <code>Med OR</code> exclude predicted or
calibrated predicted probabilities <code class="reqn">\leq 0</code> to zero or <code class="reqn">\geq 1</code>,
adjusting the sample size as needed.
</p>


<h3>Value</h3>

<p><code>val.prob</code> without <code>group</code> returns a vector with the following named
elements: <code>Dxy</code>, <code>R2</code>, <code>D</code>, <code>D:Chi-sq</code>, <code>D:p</code>,
<code>U</code>, <code>U:Chi-sq</code>, <code>U:p</code>, <code>Q</code>, <code>Brier</code>,
<code>Intercept</code>, <code>Slope</code>, <code>S:z</code>, <code>S:p</code>, <code>Emax</code>.
When <code>group</code> is present <code>val.prob</code> returns an object of class
<code>val.prob</code> containing a list with summary statistics and calibration
curves for all the strata plus <code>"Overall"</code>.  
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Harrell FE, Lee KL, Mark DB (1996): Multivariable prognostic models:
Issues in developing models, evaluating assumptions and adequacy, and
measuring and reducing errors.  Stat in Med 15:361&ndash;387.
</p>
<p>Harrell FE, Lee KL (1987):  Using logistic calibration to assess the
accuracy of probability predictions (Technical Report).
</p>
<p>Miller ME, Hui SL, Tierney WM (1991): Validation techniques for
logistic regression models.  Stat in Med 10:1213&ndash;1226.
</p>
<p>Stallard N (2009): Simple tests for the external validation of mortality
prediction scores.  Stat in Med 28:377&ndash;388.
</p>
<p>Harrell FE, Lee KL (1985):  A comparison of the <em>discrimination</em>
of discriminant analysis and logistic regression under multivariate
normality.  In Biostatistics: Statistics in Biomedical, Public Health,
and Environmental Sciences.  The Bernard G. Greenberg Volume, ed. PK
Sen. New York: North-Holland, p. 333&ndash;343.
</p>
<p>Cox DR (1970): The Analysis of Binary Data, 1st edition, section 4.4.
London: Methuen.
</p>
<p>Spiegelhalter DJ (1986):Probabilistic prediction in patient management.
Stat in Med 5:421&ndash;433.
</p>
<p>Rufibach K (2010):Use of Brier score to assess binary predictions.  J
Clin Epi 63:938-939
</p>
<p>Tjur T (2009):Coefficients of determination in logistic regression
models-A new proposal:The coefficient of discrimination.  Am Statist
63:366&ndash;372.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+validate.lrm">validate.lrm</a></code>, <code><a href="#topic+lrm.fit">lrm.fit</a></code>, <code><a href="#topic+lrm">lrm</a></code>,
<code><a href="Hmisc.html#topic+labcurve">labcurve</a></code>,
<code><a href="Hmisc.html#topic+wtd.stats">wtd.stats</a></code>, <code><a href="Hmisc.html#topic+scat1d">scat1d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit logistic model on 100 observations simulated from the actual 
# model given by Prob(Y=1 given X1, X2, X3) = 1/(1+exp[-(-1 + 2X1)]),
# where X1 is a random uniform [0,1] variable.  Hence X2 and X3 are 
# irrelevant.  After fitting a linear additive model in X1, X2,
# and X3, the coefficients are used to predict Prob(Y=1) on a
# separate sample of 100 observations.  Note that data splitting is
# an inefficient validation method unless n &gt; 20,000.


set.seed(1)
n &lt;- 200
x1 &lt;- runif(n)
x2 &lt;- runif(n)
x3 &lt;- runif(n)
logit &lt;- 2*(x1-.5)
P &lt;- 1/(1+exp(-logit))
y &lt;- ifelse(runif(n)&lt;=P, 1, 0)
d &lt;- data.frame(x1,x2,x3,y)
f &lt;- lrm(y ~ x1 + x2 + x3, subset=1:100)
pred.logit &lt;- predict(f, d[101:200,])
phat &lt;- 1/(1+exp(-pred.logit))
val.prob(phat, y[101:200], m=20, cex=.5)  # subgroups of 20 obs.


# Validate predictions more stringently by stratifying on whether
# x1 is above or below the median


v &lt;- val.prob(phat, y[101:200], group=x1[101:200], g.group=2)
v
plot(v)
plot(v, flag=function(stats) ifelse(
  stats[,'ChiSq2'] &gt; qchisq(.95,2) |
  stats[,'B ChiSq'] &gt; qchisq(.95,1), '*', ' ') )
# Stars rows of statistics in plot corresponding to significant
# mis-calibration at the 0.05 level instead of the default, 0.01


plot(val.prob(phat, y[101:200], group=x1[101:200], g.group=2), 
              col=1:3) # 3 colors (1 for overall)


# Weighted calibration curves
# plot(val.prob(pred, y, group=age, weights=freqs))
</code></pre>

<hr>
<h2 id='val.surv'>
Validate Predicted Probabilities Against Observed Survival Times
</h2><span id='topic+val.surv'></span><span id='topic+plot.val.surv'></span><span id='topic+plot.val.survh'></span><span id='topic+print.val.survh'></span>

<h3>Description</h3>

<p>The <code>val.surv</code> function is useful for validating predicted survival
probabilities against right-censored failure times.  If <code>u</code> is
specified, the hazard regression function <code>hare</code> in the
<code>polspline</code> package is used to relate predicted survival
probability at time <code>u</code> to observed survival times (and censoring
indicators) to estimate the actual survival probability at time
<code>u</code> as a function of the estimated survival probability at that
time, <code>est.surv</code>.  If <code>est.surv</code> is not given, <code>fit</code> must
be specified and the <code>survest</code> function is used to obtain the
predicted values (using <code>newdata</code> if it is given, or using the
stored linear predictor values if not).  <code>hare</code> is given the sole
predictor <code>fun(est.surv)</code> where <code>fun</code> is given by the user or
is inferred from <code>fit</code>.  <code>fun</code> is the function of predicted
survival probabilities that one expects to create a linear relationship
with the linear predictors.
</p>
<p><code>hare</code> uses an adaptive procedure to find a linear spline of
<code>fun(est.surv)</code> in a model where the log hazard is a linear spline
in time <code class="reqn">t</code>, and cross-products between the two splines are allowed so as to
not assume proportional hazards.  Thus <code>hare</code> assumes that the
covariate and time functions are smooth but not much else, if the number
of events in the dataset is large enough for obtaining a reliable
flexible fit.  There are special <code>print</code> and <code>plot</code> methods
when <code>u</code> is given.  In this case, <code>val.surv</code> returns an object
of class <code>"val.survh"</code>, otherwise it returns an object of class
<code>"val.surv"</code>.
</p>
<p>If <code>u</code> is not specified, <code>val.surv</code> uses Cox-Snell (1968)
residuals on the cumulative 
probability scale to check on the calibration of a survival model
against right-censored failure time data.  If the predicted survival
probability at time <code class="reqn">t</code> for a subject having predictors <code class="reqn">X</code> is
<code class="reqn">S(t|X)</code>, this method is based on the fact that the predicted
probability of failure before time <code class="reqn">t</code>, <code class="reqn">1 - S(t|X)</code>, when
evaluated at the subject's actual survival time <code class="reqn">T</code>, has a uniform
(0,1) distribution.  The quantity <code class="reqn">1 - S(T|X)</code> is right-censored
when <code class="reqn">T</code> is.  By getting one minus the Kaplan-Meier estimate of the
distribution of <code class="reqn">1 - S(T|X)</code> and plotting against the 45 degree line
we can check for calibration accuracy.  A more stringent assessment can
be obtained by stratifying this analysis by an important predictor
variable.  The theoretical uniform distribution is only an approximation
when the survival probabilities are estimates and not population values.
</p>
<p>When <code>censor</code> is specified to <code>val.surv</code>, a different
validation is done that is more stringent but that only uses the
uncensored failure times.  This method is used for type I censoring when
the theoretical censoring times are known for subjects having uncensored
failure times.  Let <code class="reqn">T</code>, <code class="reqn">C</code>, and <code class="reqn">F</code> denote respectively
the failure time, censoring time, and cumulative failure time
distribution (<code class="reqn">1 - S</code>).  The expected value of <code class="reqn">F(T | X)</code> is 0.5
when <code class="reqn">T</code> represents the subject's actual failure time.  The expected
value for an uncensored time is the expected value of <code class="reqn">F(T | T \leq
C, X) = 0.5 F(C | X)</code>.  A smooth plot of <code class="reqn">F(T|X) - 0.5 F(C|X)</code> for
uncensored <code class="reqn">T</code> should be a flat line through <code class="reqn">y=0</code> if the model
is well calibrated.  A smooth plot of <code class="reqn">2F(T|X)/F(C|X)</code> for
uncensored <code class="reqn">T</code> should be a flat line through <code class="reqn">y=1.0</code>. The smooth
plot is obtained by smoothing the (linear predictor, difference or
ratio) pairs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>val.surv(fit, newdata, S, est.surv, censor,
         u, fun, lim, evaluate=100, pred, maxdim=5, ...)

## S3 method for class 'val.survh'
print(x, ...)

## S3 method for class 'val.survh'
plot(x, lim, xlab, ylab,
                         riskdist=TRUE, add=FALSE,
                         scat1d.opts=list(nhistSpike=200), ...)

## S3 method for class 'val.surv'
plot(x, group, g.group=4,
     what=c('difference','ratio'),
     type=c('l','b','p'),
     xlab, ylab, xlim, ylim, datadensity=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="val.surv_+3A_fit">fit</code></td>
<td>
<p>a fit object created by <code>cph</code> or <code>psm</code></p>
</td></tr>
<tr><td><code id="val.surv_+3A_newdata">newdata</code></td>
<td>

<p>a data frame for which <code>val.surv</code> should obtain predicted survival
probabilities.  If omitted, survival estimates are made for all of the
subjects used in <code>fit</code>.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_s">S</code></td>
<td>
<p>an <code><a href="survival.html#topic+Surv">Surv</a></code> object</p>
</td></tr>
<tr><td><code id="val.surv_+3A_est.surv">est.surv</code></td>
<td>

<p>a vector of estimated survival probabilities corresponding to times in
the first column of <code>S</code>.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_censor">censor</code></td>
<td>

<p>a vector of censoring times.  Only the censoring times for uncensored
observations are used.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_u">u</code></td>
<td>
<p>a single numeric follow-up time</p>
</td></tr>
<tr><td><code id="val.surv_+3A_fun">fun</code></td>
<td>
<p>a function that transforms survival probabilities into the
scale of the linear predictor.  If <code>fit</code> is given, and
represents either a Cox, Weibull, or exponential fit, <code>fun</code> is
automatically set to log(-log(p)).</p>
</td></tr>
<tr><td><code id="val.surv_+3A_lim">lim</code></td>
<td>
<p>a 2-vector specifying limits of predicted survival
probabilities for obtaining estimated actual probabilities at time
<code>u</code>.  Default for
<code>val.surv</code> is the limits for predictions from <code>datadist</code>,
which for large <code class="reqn">n</code> is the 10th smallest and 10th largest
predicted survival probability.  For <code>plot.val.survh</code>, the
default for <code>lim</code> is the range of the combination of predicted
probabilities and calibrated actual probabilities.  <code>lim</code> is
used for both axes of the calibration plot.</p>
</td></tr>
<tr><td><code id="val.surv_+3A_evaluate">evaluate</code></td>
<td>
<p>the number of evenly spaced points over the range of
predicted probabilities.  This defines the points at which
calibrated predictions are obtained for plotting.</p>
</td></tr>
<tr><td><code id="val.surv_+3A_pred">pred</code></td>
<td>
<p>a vector of points at which to evaluate predicted
probabilities, overriding <code>lim</code></p>
</td></tr>
<tr><td><code id="val.surv_+3A_maxdim">maxdim</code></td>
<td>
<p>see <code><a href="polspline.html#topic+hare">hare</a></code></p>
</td></tr>
<tr><td><code id="val.surv_+3A_x">x</code></td>
<td>
<p>result of <code>val.surv</code></p>
</td></tr>
<tr><td><code id="val.surv_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.  For <code>plot.survh</code>, defaults for
<code>xlab</code> and <code>ylab</code> come from <code>u</code> and the units of
measurement for the raw survival times.</p>
</td></tr>
<tr><td><code id="val.surv_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="val.surv_+3A_riskdist">riskdist</code></td>
<td>
<p>set to <code>FALSE</code> to not call <code>scat1d</code> to draw the
distribution of predicted (uncalibrated) probabilities</p>
</td></tr>
<tr><td><code id="val.surv_+3A_add">add</code></td>
<td>
<p>set to <code>TRUE</code> if adding to an existing plot</p>
</td></tr>
<tr><td><code id="val.surv_+3A_scat1d.opts">scat1d.opts</code></td>
<td>
<p>a <code>list</code> of options to pass to <code>scat1d</code>.
By default, the option <code>nhistSpike=200</code> is passed so that a spike
histogram is used if the sample size exceeds 200.</p>
</td></tr>
<tr><td><code id="val.surv_+3A_...">...</code></td>
<td>
<p>When <code>u</code> is given to <code>val.surv</code>, ... represents
optional arguments to <code>hare</code>.  It can represent arguments to
pass to <code>plot</code> or <code>lines</code> for 
<code>plot.val.survh</code>.  Otherwise, ... contains optional
arguments for <code>plsmo</code> or <code>plot</code>.   For
<code>print.val.survh</code>, ... is ignored.</p>
</td></tr> 
<tr><td><code id="val.surv_+3A_group">group</code></td>
<td>

<p>a grouping variable.  If numeric this variable is grouped into
<code>g.group</code> quantile groups (default is quartiles).  <code>group</code>,
<code>g.group</code>, <code>what</code>, and <code>type</code> apply when
<code>u</code> is not given.</p>
</td></tr>
<tr><td><code id="val.surv_+3A_g.group">g.group</code></td>
<td>

<p>number of quantile groups to use when <code>group</code> is given and variable
is numeric.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_what">what</code></td>
<td>

<p>the quantity to plot when <code>censor</code> was in effect.  The default is to
show the difference between cumulative probabilities and their
expectation given the censoring time.  Set <code>what="ratio"</code> to show the
ratio instead.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_type">type</code></td>
<td>

<p>Set to the default (<code>"l"</code>) to plot the trend line only, <code>"b"</code>
to plot both individual subjects ratios and trend lines, or
<code>"p"</code> to plot only points. 
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_xlim">xlim</code>, <code id="val.surv_+3A_ylim">ylim</code></td>
<td>

<p>axis limits for <code>plot.val.surv</code> when the <code>censor</code> variable was used.
</p>
</td></tr>
<tr><td><code id="val.surv_+3A_datadensity">datadensity</code></td>
<td>

<p>By default, <code>plot.val.surv</code> will show the data density on each curve
that is created as a result of <code>censor</code> being present.  Set
<code>datadensity=FALSE</code> to suppress these tick marks drawn by <code>scat1d</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>"val.surv"</code> or <code>"val.survh"</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Cox DR, Snell EJ (1968):A general definition of residuals (with
discussion).  JRSSB 30:248&ndash;275.
</p>
<p>Kooperberg C, Stone C, Truong Y (1995): Hazard regression.  JASA 90:78&ndash;94.
</p>
<p>May M, Royston P, Egger M, Justice AC, Sterne JAC (2004):Development and
validation of a prognostic model for survival time data: application to
prognosis of HIV positive patients treated with antiretroviral therapy.
Stat in Med 23:2375&ndash;2398.
</p>
<p>Stallard N (2009): Simple tests for th external validation of mortality
prediction scores.  Stat in Med 28:377&ndash;388.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>, <code><a href="polspline.html#topic+hare">hare</a></code>,
<code><a href="Hmisc.html#topic+scat1d">scat1d</a></code>, <code><a href="#topic+cph">cph</a></code>, <code><a href="#topic+psm">psm</a></code>,
<code><a href="#topic+groupkm">groupkm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate failure times from an exponential distribution
require(survival)
set.seed(123)              # so can reproduce results
n &lt;- 1000
age &lt;- 50 + 12*rnorm(n)
sex &lt;- factor(sample(c('Male','Female'), n, rep=TRUE, prob=c(.6, .4)))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
t &lt;- -log(runif(n))/h
units(t) &lt;- 'Year'
label(t) &lt;- 'Time to Event'
ev &lt;- ifelse(t &lt;= cens, 1, 0)
t &lt;- pmin(t, cens)
S &lt;- Surv(t, ev)

# First validate true model used to generate data

# If hare is available, make a smooth calibration plot for 1-year
# survival probability where we predict 1-year survival using the
# known true population survival probability
# In addition, use groupkm to show that grouping predictions into
# intervals and computing Kaplan-Meier estimates is not as accurate.

if(requireNamespace('polspline')) {
  s1 &lt;- exp(-h*1)
  w &lt;- val.surv(est.surv=s1, S=S, u=1,
                fun=function(p)log(-log(p)))
  plot(w, lim=c(.85,1), scat1d.opts=list(nhistSpike=200, side=1))
  groupkm(s1, S, m=100, u=1, pl=TRUE, add=TRUE)
}

# Now validate the true model using residuals

w &lt;- val.surv(est.surv=exp(-h*t), S=S)
plot(w)
plot(w, group=sex)  # stratify by sex


# Now fit an exponential model and validate
# Note this is not really a validation as we're using the
# training data here
f &lt;- psm(S ~ age + sex, dist='exponential', y=TRUE)
w &lt;- val.surv(f)
plot(w, group=sex)


# We know the censoring time on every subject, so we can
# compare the predicted Pr[T &lt;= observed T | T&gt;c, X] to
# its expectation 0.5 Pr[T &lt;= C | X] where C = censoring time
# We plot a ratio that should equal one
w &lt;- val.surv(f, censor=cens)
plot(w)
plot(w, group=age, g=3)   # stratify by tertile of age
</code></pre>

<hr>
<h2 id='validate'>Resampling Validation of a Fitted Model's Indexes of Fit</h2><span id='topic+validate'></span><span id='topic+print.validate'></span><span id='topic+latex.validate'></span><span id='topic+html.validate'></span>

<h3>Description</h3>

<p>The <code>validate</code> function when used on an object created by one of the
<code>rms</code> series does resampling validation of a 
regression model, with or without backward step-down variable
deletion.
The <code>print</code> method will call the <code>latex</code> or <code>html</code> method
if <code>options(prType=)</code> is set to <code>"latex"</code> or <code>"html"</code>.
For <code>"latex"</code> printing through <code>print()</code>, the LaTeX table
environment is turned off.  When using html with Quarto or RMarkdown,
<code>results='asis'</code> need not be written in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- fitting.function(formula=response ~ terms, x=TRUE, y=TRUE)
validate(fit, method="boot", B=40,
         bw=FALSE, rule="aic", type="residual", sls=0.05, aics=0, 
         force=NULL, estimates=TRUE, pr=FALSE, ...)
## S3 method for class 'validate'
print(x, digits=4, B=Inf, ...)
## S3 method for class 'validate'
latex(object, digits=4, B=Inf, file='', append=FALSE,
                         title=first.word(deparse(substitute(x))),
                         caption=NULL, table.env=FALSE,
                         size='normalsize', extracolsize=size, ...)
## S3 method for class 'validate'
html(object, digits=4, B=Inf, caption=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_+3A_fit">fit</code></td>
<td>

<p>a fit derived by e.g. <code>lrm</code>, <code>cph</code>, <code>psm</code>,
<code>ols</code>. The options <code>x=TRUE</code> and <code>y=TRUE</code> 
must have been specified.
</p>
</td></tr>
<tr><td><code id="validate_+3A_method">method</code></td>
<td>

<p>may be <code>"crossvalidation"</code>, <code>"boot"</code> (the default),
<code>".632"</code>, or <code>"randomization"</code>.
See <code>predab.resample</code> for details.  Can abbreviate, e.g.
<code>"cross", "b", ".6"</code>.
</p>
</td></tr>
<tr><td><code id="validate_+3A_b">B</code></td>
<td>

<p>number of repetitions.  For <code>method="crossvalidation"</code>, is the
number of groups of omitted observations.  For <code>print.validate</code>,
<code>latex.validate</code>, and <code>html.validate</code>, <code>B</code> is an upper
limit on the number 
of resamples for which information is printed about which variables
were selected in each model re-fit.  Specify zero to suppress
printing.  Default is to print all re-samples.
</p>
</td></tr>
<tr><td><code id="validate_+3A_bw">bw</code></td>
<td>

<p><code>TRUE</code> to do fast step-down using the <code>fastbw</code> function,
for both the overall model and for each repetition. <code>fastbw</code>
keeps parameters together that represent the same factor.
</p>
</td></tr>
<tr><td><code id="validate_+3A_rule">rule</code></td>
<td>

<p>Applies if <code>bw=TRUE</code>.  <code>"aic"</code> to use Akaike's information criterion as a
stopping rule (i.e., a factor is deleted if the <code class="reqn">\chi^2</code> falls below
twice its degrees of freedom), or <code>"p"</code> to use <code class="reqn">P</code>-values.
</p>
</td></tr>
<tr><td><code id="validate_+3A_type">type</code></td>
<td>

<p><code>"residual"</code> or <code>"individual"</code> - stopping rule is for individual factors or
for the residual <code class="reqn">\chi^2</code> for all variables deleted
</p>
</td></tr>
<tr><td><code id="validate_+3A_sls">sls</code></td>
<td>

<p>significance level for a factor to be kept in a model, or for judging the
residual <code class="reqn">\chi^2</code>.
</p>
</td></tr>
<tr><td><code id="validate_+3A_aics">aics</code></td>
<td>

<p>cutoff on AIC when <code>rule="aic"</code>.
</p>
</td></tr>
<tr><td><code id="validate_+3A_force">force</code></td>
<td>
<p>see <code><a href="#topic+fastbw">fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate_+3A_estimates">estimates</code></td>
<td>
<p>see <code><a href="#topic+print.fastbw">print.fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate_+3A_pr">pr</code></td>
<td>

<p><code>TRUE</code> to print results of each repetition
</p>
</td></tr>
<tr><td><code id="validate_+3A_...">...</code></td>
<td>

<p>parameters for each specific validate function, and parameters to
pass to <code>predab.resample</code> (note especially the <code>group</code>,
<code>cluster</code>, amd <code>subset</code> parameters).  For <code>latex</code>,
optional arguments to <code><a href="Hmisc.html#topic+latex">latex.default</a></code>.  Ignored for
<code>html.validate</code>.
</p>
<p>For <code>psm</code>, you can pass the <code>maxiter</code> parameter here (passed to 
<code>survreg.control</code>, default is 15 iterations) as well as a <code>tol</code> parameter 
for judging matrix singularity in <code>solvet</code> (default is 1e-12)
and a <code>rel.tolerance</code> parameter that is passed to
<code>survreg.control</code> (default is 1e-5).
</p>
<p>For <code>print.validate</code> ... is ignored.
</p>
</td></tr>
<tr><td><code id="validate_+3A_x">x</code>, <code id="validate_+3A_object">object</code></td>
<td>
<p>an object produced by one of the <code>validate</code> functions</p>
</td></tr>
<tr><td><code id="validate_+3A_digits">digits</code></td>
<td>
<p>number of decimal places to print</p>
</td></tr>
<tr><td><code id="validate_+3A_file">file</code></td>
<td>
<p>file to write LaTeX output.  Default is standard output.</p>
</td></tr>
<tr><td><code id="validate_+3A_append">append</code></td>
<td>
<p>set to <code>TRUE</code> to append LaTeX output to an existing
file</p>
</td></tr>
<tr><td><code id="validate_+3A_title">title</code>, <code id="validate_+3A_caption">caption</code>, <code id="validate_+3A_table.env">table.env</code>, <code id="validate_+3A_extracolsize">extracolsize</code></td>
<td>
<p>see
<code><a href="Hmisc.html#topic+latex.default">latex.default</a></code>.  If <code>table.env</code> is
<code>FALSE</code> and <code>caption</code> is given, the character string
contained in <code>caption</code> will be placed before the table,
centered.</p>
</td></tr>
<tr><td><code id="validate_+3A_size">size</code></td>
<td>
<p>size of LaTeX output.  Default is <code>'normalsize'</code>.  Must
be a defined LaTeX size when prepended by double slash.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It provides bias-corrected indexes that are specific to each type
of model. For <code>validate.cph</code> and <code>validate.psm</code>, see <code>validate.lrm</code>,
which is similar. <br />
For <code>validate.cph</code> and <code>validate.psm</code>, there is
an extra argument <code>dxy</code>, which if <code>TRUE</code> causes the <code>dxy.cens</code>
function to be invoked to compute the Somers' <code class="reqn">D_{xy}</code> rank correlation
to be computed at each resample. The values corresponding to the row
<code class="reqn">D_{xy}</code> are equal to <code class="reqn">2 * (C - 0.5)</code> where C is the
C-index or concordance probability. <br />
</p>
<p>For <code>validate.cph</code> with <code>dxy=TRUE</code>,
you must specify an argument <code>u</code> if the model is stratified, since
survival curves can then cross and <code class="reqn">X\beta</code> is not 1-1 with
predicted survival. <br />
There is also <code>validate</code> method for
<code>tree</code>, which only does cross-validation and which has a different
list of arguments.   
</p>


<h3>Value</h3>

<p>a matrix with rows corresponding to the statistical indexes and
columns for columns for the original index, resample estimates, 
indexes applied to
the whole or omitted sample using the model derived from the resample,
average optimism, corrected index, and number of successful re-samples.
</p>


<h3>Side Effects</h3>

<p>prints a summary, and optionally statistics for each re-fit
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+validate.ols">validate.ols</a></code>, <code><a href="#topic+validate.cph">validate.cph</a></code>,
<code><a href="#topic+validate.lrm">validate.lrm</a></code>, <code><a href="#topic+validate.rpart">validate.rpart</a></code>, 
<code><a href="#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+rms">rms</a></code>,
<code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+dxy.cens">dxy.cens</a></code>, <code><a href="survival.html#topic+survConcordance">survConcordance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples for validate.cph, validate.lrm, validate.ols
# Example of validating a parametric survival model:

require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, TRUE))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
dt &lt;- -log(runif(n))/h
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
S &lt;- Surv(dt,e)


f &lt;- psm(S ~ age*sex, x=TRUE, y=TRUE)  # Weibull model
# Validate full model fit
validate(f, B=10)                # usually B=150


# Validate stepwise model with typical (not so good) stopping rule
# bw=TRUE does not preserve hierarchy of terms at present
validate(f, B=10, bw=TRUE, rule="p", sls=.1, type="individual")
</code></pre>

<hr>
<h2 id='validate.cph'>Validation of a Fitted Cox or Parametric Survival Model's Indexes
of Fit</h2><span id='topic+validate.cph'></span><span id='topic+validate.psm'></span><span id='topic+dxy.cens'></span>

<h3>Description</h3>

<p>This is the version of the <code>validate</code> function specific to models
fitted with <code>cph</code> or <code>psm</code>.  Also included is a small
function <code>dxy.cens</code> that retrieves <code class="reqn">D_{xy}</code> and its
standard error from the <code>survival</code> package's
<code>survConcordance.fit</code> function.  This allows for incredibly fast
computation of <code class="reqn">D_{xy}</code> or the c-index even for hundreds of
thousands of observations.  <code>dxy.cens</code> negates <code class="reqn">D_{xy}</code>
if log relative hazard is being predicted.  If <code>y</code> is a
left-censored <code>Surv</code> object, times are negated and a
right-censored object is created, then <code class="reqn">D_{xy}</code> is negated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- cph(formula=Surv(ftime,event) ~ terms, x=TRUE, y=TRUE, \dots)
## S3 method for class 'cph'
validate(fit, method="boot", B=40, bw=FALSE, rule="aic",
type="residual", sls=.05, aics=0, force=NULL, estimates=TRUE,
pr=FALSE, dxy=TRUE, u, tol=1e-9, ...)

## S3 method for class 'psm'
validate(fit, method="boot",B=40,
        bw=FALSE, rule="aic", type="residual", sls=.05, aics=0,
        force=NULL, estimates=TRUE, pr=FALSE,
        dxy=TRUE, tol=1e-12, rel.tolerance=1e-5, maxiter=15, ...)

dxy.cens(x, y, type=c('time','hazard'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate.cph_+3A_fit">fit</code></td>
<td>

<p>a fit derived <code>cph</code>. The options <code>x=TRUE</code> and <code>y=TRUE</code>
must have been specified. If the model contains any stratification factors
and dxy=TRUE,
the options <code>surv=TRUE</code> and <code>time.inc=u</code> must also have been given,
where <code>u</code> is the same value of <code>u</code> given to <code>validate</code>.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_method">method</code></td>
<td>
<p>see <code><a href="#topic+validate">validate</a></code></p>
</td></tr>
<tr><td><code id="validate.cph_+3A_b">B</code></td>
<td>

<p>number of repetitions.  For <code>method="crossvalidation"</code>, is the
number of groups of omitted observations.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_rel.tolerance">rel.tolerance</code>, <code id="validate.cph_+3A_maxiter">maxiter</code>, <code id="validate.cph_+3A_bw">bw</code></td>
<td>

<p><code>TRUE</code> to do fast step-down using the <code>fastbw</code> function,
for both the overall model and for each repetition. <code>fastbw</code>
keeps parameters together that represent the same factor.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_rule">rule</code></td>
<td>

<p>Applies if <code>bw=TRUE</code>.  <code>"aic"</code> to use Akaike's information criterion as a
stopping rule (i.e., a factor is deleted if the <code class="reqn">\chi^2</code> falls below
twice its degrees of freedom), or <code>"p"</code> to use <code class="reqn">P</code>-values.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_type">type</code></td>
<td>

<p><code>"residual"</code> or <code>"individual"</code> - stopping rule is for
individual factors or for the residual <code class="reqn">\chi^2</code> for
all variables deleted.  For <code>dxy.cens</code>, specify
<code>type="hazard"</code> if <code>x</code> is on the hazard or cumulative
hazard (or their logs) scale, causing negation of the correlation index.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_sls">sls</code></td>
<td>

<p>significance level for a factor to be kept in a model, or for judging the
residual <code class="reqn">\chi^2</code>.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_aics">aics</code></td>
<td>

<p>cutoff on AIC when <code>rule="aic"</code>.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_force">force</code></td>
<td>
<p>see <code><a href="#topic+fastbw">fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate.cph_+3A_estimates">estimates</code></td>
<td>
<p>see <code><a href="#topic+print.fastbw">print.fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate.cph_+3A_pr">pr</code></td>
<td>
<p><code>TRUE</code> to print results of each repetition</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_tol">tol</code>, <code id="validate.cph_+3A_...">...</code></td>
<td>
<p>see <code><a href="#topic+validate">validate</a></code> or <code><a href="#topic+predab.resample">predab.resample</a></code></p>
</td></tr>
<tr><td><code id="validate.cph_+3A_dxy">dxy</code></td>
<td>

<p>set to <code>TRUE</code> to validate Somers' <code class="reqn">D_{xy}</code>  using
<code>dxy.cens</code>, which is fast until n &gt; 500,000.  Uses the
<code>survival</code> package's <code>survConcordance.fit</code> service
function for <code>survConcordance</code>.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_u">u</code></td>
<td>

<p>must be specified if the model has any stratification factors and
<code>dxy=TRUE</code>. 
In that case, strata are not included in <code class="reqn">X\beta</code> and the
survival curves may cross.  Predictions at time <code>t=u</code> are
correlated with observed survival times.  Does not apply to
<code>validate.psm</code>.
</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="validate.cph_+3A_y">y</code></td>
<td>
<p>a <code>Surv</code> object that may be uncensored or
right-censored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Statistics validated include the Nagelkerke <code class="reqn">R^2</code>, 
<code class="reqn">D_{xy}</code>, slope shrinkage,  the discrimination index <code class="reqn">D</code>
[(model L.R. <code class="reqn">\chi^2</code> - 1)/L], the unreliability index
<code class="reqn">U</code> = (difference in -2 log likelihood between uncalibrated
<code class="reqn">X\beta</code> and  
<code class="reqn">X\beta</code> with overall slope calibrated to test sample) / L,
and the overall quality index <code class="reqn">Q = D - U</code>.  <code class="reqn">g</code> is the
<code class="reqn">g</code>-index on the log relative hazard (linear predictor) scale.
L is -2 log likelihood with beta=0.  The &quot;corrected&quot; slope
can be thought of as shrinkage factor that takes into account overfitting.
See <code>predab.resample</code> for the list of resampling methods.
</p>


<h3>Value</h3>

<p>matrix with rows corresponding to <code class="reqn">D_{xy}</code>, Slope, <code class="reqn">D</code>,
<code class="reqn">U</code>, and <code class="reqn">Q</code>, and columns for the original index, resample estimates, 
indexes applied to whole or omitted sample using model derived from
resample, average optimism, corrected index, and number of successful
resamples.<br />
</p>
<p>The values corresponding to the row <code class="reqn">D_{xy}</code> are equal to <code class="reqn">2 *
    (C - 0.5)</code> where C is the C-index or concordance probability.
If the user is correlating the linear predictor (predicted log hazard)
with survival time, <code class="reqn">D_{xy}</code> is automatically negated. 
</p>


<h3>Side Effects</h3>

<p>prints a summary, and optionally statistics for each re-fit (if
<code>pr=TRUE</code>) 
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+validate">validate</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>,
<code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>,
<code><a href="#topic+calibrate">calibrate</a></code>, <code><a href="Hmisc.html#topic+rcorr.cens">rcorr.cens</a></code>,
<code><a href="#topic+cph">cph</a></code>, <code><a href="survival.html#topic+survival-internal">survival-internal</a></code>,
<code><a href="#topic+gIndex">gIndex</a></code>, <code><a href="survival.html#topic+survConcordance">survConcordance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
n &lt;- 1000
set.seed(731)
age &lt;- 50 + 12*rnorm(n)
label(age) &lt;- "Age"
sex &lt;- factor(sample(c('Male','Female'), n, TRUE))
cens &lt;- 15*runif(n)
h &lt;- .02*exp(.04*(age-50)+.8*(sex=='Female'))
dt &lt;- -log(runif(n))/h
e &lt;- ifelse(dt &lt;= cens,1,0)
dt &lt;- pmin(dt, cens)
units(dt) &lt;- "Year"
S &lt;- Surv(dt,e)

f &lt;- cph(S ~ age*sex, x=TRUE, y=TRUE)
# Validate full model fit
validate(f, B=10)               # normally B=150

# Validate a model with stratification.  Dxy is the only
# discrimination measure for such models, by Dxy requires
# one to choose a single time at which to predict S(t|X)
f &lt;- cph(S ~ rcs(age)*strat(sex), 
         x=TRUE, y=TRUE, surv=TRUE, time.inc=2)
validate(f, u=2, B=10)   # normally B=150
# Note u=time.inc
</code></pre>

<hr>
<h2 id='validate.lrm'>Resampling Validation of a Logistic or Ordinal Regression Model</h2><span id='topic+validate.lrm'></span><span id='topic+validate.orm'></span>

<h3>Description</h3>

<p>The <code>validate</code> function when used on an object created by
<code>lrm</code> or <code>orm</code> does resampling validation of a logistic
regression model, 
with or without backward step-down variable deletion.  It provides
bias-corrected Somers' <code class="reqn">D_{xy}</code> rank correlation, R-squared index,
the intercept and slope of an overall logistic calibration equation, the
maximum absolute difference in predicted and calibrated probabilities
<code class="reqn">E_{max}</code>, the discrimination index <code class="reqn">D</code> (model L.R. <code class="reqn">(\chi^2
- 1)/n</code>), the unreliability index <code class="reqn">U</code> =
difference in -2 log likelihood between un-calibrated <code class="reqn">X\beta</code> and <code class="reqn">X\beta</code> with overall intercept and slope
calibrated to test sample / n, the overall quality index (logarithmic
probability score) <code class="reqn">Q = D - U</code>, and the Brier or quadratic
probability score, <code class="reqn">B</code> (the last 3 are not computed for ordinal
models), the <code class="reqn">g</code>-index, and <code>gp</code>, the <code class="reqn">g</code>-index on the
probability scale.  The corrected slope can be thought of as shrinkage
factor that takes into account overfitting.  For <code>orm</code> fits, a
subset of the above indexes is provided, Spearman's <code class="reqn">\rho</code> is
substituted for <code class="reqn">D_{xy}</code>, and a new index is reported: <code>pdm</code>, the mean
absolute difference between 0.5 and the predicted probability that
<code class="reqn">Y\geq</code> the marginal median of <code class="reqn">Y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- lrm(formula=response ~ terms, x=TRUE, y=TRUE) or orm
## S3 method for class 'lrm'
validate(fit, method="boot", B=40,
         bw=FALSE, rule="aic", type="residual", sls=0.05, aics=0,
         force=NULL, estimates=TRUE,
         pr=FALSE,  kint, Dxy.method=if(k==1) 'somers2' else 'lrm',
         emax.lim=c(0,1), ...)
## S3 method for class 'orm'
validate(fit, method="boot", B=40, bw=FALSE, rule="aic",
         type="residual",	sls=.05, aics=0, force=NULL, estimates=TRUE,
         pr=FALSE,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate.lrm_+3A_fit">fit</code></td>
<td>

<p>a fit derived by <code>lrm</code> or <code>orm</code>. The options <code>x=TRUE</code> and
<code>y=TRUE</code> must have been specified.
</p>
</td></tr>
<tr><td><code id="validate.lrm_+3A_method">method</code>, <code id="validate.lrm_+3A_b">B</code>, <code id="validate.lrm_+3A_bw">bw</code>, <code id="validate.lrm_+3A_rule">rule</code>, <code id="validate.lrm_+3A_type">type</code>, <code id="validate.lrm_+3A_sls">sls</code>, <code id="validate.lrm_+3A_aics">aics</code>, <code id="validate.lrm_+3A_force">force</code>, <code id="validate.lrm_+3A_estimates">estimates</code>, <code id="validate.lrm_+3A_pr">pr</code></td>
<td>
<p>see <code><a href="#topic+validate">validate</a></code> and <code><a href="#topic+predab.resample">predab.resample</a></code></p>
</td></tr>
<tr><td><code id="validate.lrm_+3A_kint">kint</code></td>
<td>

<p>In the case of an ordinal model, specify which intercept to validate.
Default is the middle intercept.  For <code>validate.orm</code>,
intercept-specific quantities are not validated so this does not matter.
</p>
</td></tr>
<tr><td><code id="validate.lrm_+3A_dxy.method">Dxy.method</code></td>
<td>

<p><code>"lrm"</code> to use <code>lrm</code>s computation of <code class="reqn">D_{xy}</code> correlation,
which rounds predicted probabilities to nearest .002.  Use
<code>Dxy.method="somers2"</code> (the default) to instead use the more
accurate but slower <code>somers2</code> function.  This will matter most when
the model is extremely predictive. The default is <code>"lrm"</code> for
ordinal models, since <code>somers2</code> only handles binary response
variables. 
</p>
</td></tr>
<tr><td><code id="validate.lrm_+3A_emax.lim">emax.lim</code></td>
<td>

<p>range of predicted probabilities over which to compute the maximum
error.  Default is entire range. 
</p>
</td></tr>
<tr><td><code id="validate.lrm_+3A_...">...</code></td>
<td>

<p>other arguments to pass to <code>lrm.fit</code> (now only <code>maxit</code> and
<code>tol</code> are allowed) and to <code>predab.resample</code> (note especially
the <code>group</code>, <code>cluster</code>, and <code>subset</code> parameters)
</p>
</td></tr></table>


<h3>Details</h3>

<p>If the original fit was created using penalized maximum likelihood estimation,
the same <code>penalty.matrix</code> used with the original
fit are used during validation.
</p>


<h3>Value</h3>

<p>a matrix with rows corresponding to <code class="reqn">D_{xy}</code>,
<code class="reqn">R^2</code>, <code>Intercept</code>, <code>Slope</code>, <code class="reqn">E_{max}</code>, <code class="reqn">D</code>,
<code class="reqn">U</code>, <code class="reqn">Q</code>, <code class="reqn">B</code>, <code class="reqn">g</code>, <code class="reqn">gp</code>, and
columns for the original index, resample estimates, indexes applied to
the whole or omitted sample using the model derived from the resample,
average optimism, corrected index, and number of successful re-samples.
For <code>validate.orm</code> not all columns are provided, Spearman's rho
is returned instead of <code class="reqn">D_{xy}</code>, and <code>pdm</code> is reported.
</p>


<h3>Side Effects</h3>

<p>prints a summary, and optionally statistics for each re-fit
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Miller ME, Hui SL, Tierney WM (1991): Validation techniques for
logistic regression models.  Stat in Med 10:1213&ndash;1226.
</p>
<p>Harrell FE, Lee KL (1985):  A comparison of the
<em>discrimination</em>
of discriminant analysis and logistic regression under multivariate
normality.  In Biostatistics: Statistics in Biomedical, Public Health,
and Environmental Sciences.  The Bernard G. Greenberg Volume, ed. PK
Sen. New York: North-Holland, p. 333&ndash;343.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>, <code><a href="#topic+lrm">lrm</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>, 
<code><a href="Hmisc.html#topic+somers2">somers2</a></code>, <code><a href="#topic+cr.setup">cr.setup</a></code>,
<code><a href="#topic+gIndex">gIndex</a></code>, <code><a href="#topic+orm">orm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000    # define sample size
age            &lt;- rnorm(n, 50, 10)
blood.pressure &lt;- rnorm(n, 120, 15)
cholesterol    &lt;- rnorm(n, 200, 25)
sex            &lt;- factor(sample(c('female','male'), n,TRUE))


# Specify population model for log odds that Y=1
L &lt;- .4*(sex=='male') + .045*(age-50) +
  (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


f &lt;- lrm(y ~ sex*rcs(cholesterol)+pol(age,2)+blood.pressure, x=TRUE, y=TRUE)
#Validate full model fit
validate(f, B=10)              # normally B=300
validate(f, B=10, group=y)  
# two-sample validation: make resamples have same numbers of
# successes and failures as original sample


#Validate stepwise model with typical (not so good) stopping rule
validate(f, B=10, bw=TRUE, rule="p", sls=.1, type="individual")


## Not run: 
#Fit a continuation ratio model and validate it for the predicted
#probability that y=0
u &lt;- cr.setup(y)
Y &lt;- u$y
cohort &lt;- u$cohort
attach(mydataframe[u$subs,])
f &lt;- lrm(Y ~ cohort+rcs(age,4)*sex, penalty=list(interaction=2))
validate(f, cluster=u$subs, subset=cohort=='all') 
#see predab.resample for cluster and subset

## End(Not run)
</code></pre>

<hr>
<h2 id='validate.ols'>Validation of an Ordinary Linear Model</h2><span id='topic+validate.ols'></span>

<h3>Description</h3>

<p>The <code>validate</code> function when used on an object created by
<code>ols</code> does resampling validation of a multiple linear regression
model, with or without backward step-down variable deletion.  Uses
resampling to estimate the optimism in various measures of predictive
accuracy which include <code class="reqn">R^2</code>, <code class="reqn">MSE</code> (mean squared error with a 
denominator of <code class="reqn">n</code>), the <code class="reqn">g</code>-index, and the intercept and slope
of an overall 
calibration <code class="reqn">a + b\hat{y}</code>.  The &quot;corrected&quot;
slope can be thought of as shrinkage factor that takes into account
overfitting.  <code>validate.ols</code> can also be used when a model for a
continuous response is going to be applied to a binary response. A
Somers' <code class="reqn">D_{xy}</code> for this case is computed for each resample by
dichotomizing <code>y</code>. This can be used to obtain an ordinary receiver
operating characteristic curve area using the formula <code class="reqn">0.5(D_{xy} +
1)</code>. The Nagelkerke-Maddala <code class="reqn">R^2</code> index for the dichotomized
<code>y</code> is also given.  See <code>predab.resample</code> for the list of
resampling methods.
</p>
<p>The LaTeX needspace package must be in effect to use the <code>latex</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- fitting.function(formula=response ~ terms, x=TRUE, y=TRUE)
## S3 method for class 'ols'
validate(fit, method="boot", B=40,
         bw=FALSE, rule="aic", type="residual", sls=0.05, aics=0, 
         force=NULL, estimates=TRUE, pr=FALSE, u=NULL, rel="&gt;",
         tolerance=1e-7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate.ols_+3A_fit">fit</code></td>
<td>

<p>a fit derived by <code>ols</code>. The options <code>x=TRUE</code> and <code>y=TRUE</code>
must have been specified.  See <code>validate</code> for a description of
arguments <code>method</code> - <code>pr</code>.
</p>
</td></tr>
<tr><td><code id="validate.ols_+3A_method">method</code>, <code id="validate.ols_+3A_b">B</code>, <code id="validate.ols_+3A_bw">bw</code>, <code id="validate.ols_+3A_rule">rule</code>, <code id="validate.ols_+3A_type">type</code>, <code id="validate.ols_+3A_sls">sls</code>, <code id="validate.ols_+3A_aics">aics</code>, <code id="validate.ols_+3A_force">force</code>, <code id="validate.ols_+3A_estimates">estimates</code>, <code id="validate.ols_+3A_pr">pr</code></td>
<td>
<p>see
<code><a href="#topic+validate">validate</a></code> and <code><a href="#topic+predab.resample">predab.resample</a></code> and
<code><a href="#topic+fastbw">fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate.ols_+3A_u">u</code></td>
<td>

<p>If specifed, <code>y</code> is also dichotomized at the cutoff <code>u</code> for
the purpose of getting a bias-corrected estimate of <code class="reqn">D_{xy}</code>.
</p>
</td></tr>
<tr><td><code id="validate.ols_+3A_rel">rel</code></td>
<td>

<p>relationship for dichotomizing predicted <code>y</code>. Defaults to
<code>"&gt;"</code> to use <code>y&gt;u</code>. <code>rel</code> can also be <code>"&lt;"</code>,
<code>"&gt;="</code>, and <code>"&lt;="</code>. 
</p>
</td></tr>
<tr><td><code id="validate.ols_+3A_tolerance">tolerance</code></td>
<td>

<p>tolerance for singularity; passed to <code>lm.fit.qr</code>.
</p>
</td></tr>
<tr><td><code id="validate.ols_+3A_...">...</code></td>
<td>

<p>other arguments to pass to <code>predab.resample</code>, such as <code>group</code>, <code>cluster</code>, and <code>subset</code>
</p>
</td></tr></table>


<h3>Value</h3>

<p>matrix with rows corresponding to R-square, MSE, g, intercept, slope, and 
optionally <code class="reqn">D_{xy}</code> and <code class="reqn">R^2</code>, and
columns for the original index, resample estimates, 
indexes applied to whole or omitted sample using model derived from
resample, average optimism, corrected index, and number of successful resamples.
</p>


<h3>Side Effects</h3>

<p>prints a summary, and optionally statistics for each re-fit
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ols">ols</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>, <code><a href="#topic+calibrate">calibrate</a></code>,
<code><a href="#topic+gIndex">gIndex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- runif(200)
x2 &lt;- sample(0:3, 200, TRUE)
x3 &lt;- rnorm(200)
distance &lt;- (x1 + x2/3 + rnorm(200))^2

f &lt;- ols(sqrt(distance) ~ rcs(x1,4) + scored(x2) + x3, x=TRUE, y=TRUE)

#Validate full model fit (from all observations) but for x1 &lt; .75
validate(f, B=20, subset=x1 &lt; .75)   # normally B=300

#Validate stepwise model with typical (not so good) stopping rule
validate(f, B=20, bw=TRUE, rule="p", sls=.1, type="individual")
</code></pre>

<hr>
<h2 id='validate.rpart'>
Dxy and Mean Squared Error by Cross-validating a Tree Sequence
</h2><span id='topic+validate.rpart'></span><span id='topic+print.validate.rpart'></span><span id='topic+plot.validate.rpart'></span>

<h3>Description</h3>

<p>Uses <code>xval</code>-fold cross-validation of a sequence of trees to derive
estimates of the mean squared error and Somers' <code>Dxy</code> rank correlation
between predicted and observed responses.  In the case of a binary response
variable, the mean squared error is the Brier accuracy score.  For
survival trees, <code>Dxy</code> is negated so that larger is better.
There are <code>print</code> and <code>plot</code> methods for
objects created by <code>validate.rpart</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># f &lt;- rpart(formula=y ~ x1 + x2 + \dots) # or rpart
## S3 method for class 'rpart'
validate(fit, method, B, bw, rule, type, sls, aics,
    force, estimates, pr=TRUE,
    k, rand, xval=10, FUN, ...)
## S3 method for class 'validate.rpart'
print(x, ...)
## S3 method for class 'validate.rpart'
plot(x, what=c("mse","dxy"), legendloc=locator, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate.rpart_+3A_fit">fit</code></td>
<td>

<p>an object created by <code>rpart</code>.  You must have specified the
<code>model=TRUE</code> argument to <code>rpart</code>. 
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_method">method</code>, <code id="validate.rpart_+3A_b">B</code>, <code id="validate.rpart_+3A_bw">bw</code>, <code id="validate.rpart_+3A_rule">rule</code>, <code id="validate.rpart_+3A_type">type</code>, <code id="validate.rpart_+3A_sls">sls</code>, <code id="validate.rpart_+3A_aics">aics</code>, <code id="validate.rpart_+3A_force">force</code>, <code id="validate.rpart_+3A_estimates">estimates</code></td>
<td>

<p>are there only for consistency with the generic <code>validate</code>
function; these are ignored
</p>
</td></tr> 
<tr><td><code id="validate.rpart_+3A_x">x</code></td>
<td>
<p>the result of <code>validate.rpart</code></p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_k">k</code></td>
<td>

<p>a sequence of cost/complexity values.  By default these are obtained
from calling <code>FUN</code> with no optional arguments or
from the <code>rpart</code> <code>cptable</code> object in the original fit object.
You may also specify a scalar or vector.
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_rand">rand</code></td>
<td>
<p>a random sample (usually omitted)</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_xval">xval</code></td>
<td>
<p>number of splits</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_fun">FUN</code></td>
<td>

<p>the name of a function which produces a sequence of trees, such
<code>prune</code>.
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_...">...</code></td>
<td>

<p>additional arguments to <code>FUN</code> (ignored by <code>print,plot</code>).
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to prevent intermediate results for each <code>k</code>
to be printed 
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_what">what</code></td>
<td>

<p>a vector of things to plot.  By default, 2 plots will be done, one for
<code>mse</code> and one for <code>Dxy</code>.
</p>
</td></tr>
<tr><td><code id="validate.rpart_+3A_legendloc">legendloc</code></td>
<td>

<p>a function that is evaluated with a single argument equal to <code>1</code> to
generate a list with components <code>x, y</code> specifying coordinates of the
upper left corner of a legend, or a 2-vector.  For the latter,
<code>legendloc</code> specifies the relative fraction of the plot at which to
center the legend.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>"validate.rpart"</code> with components named <code>k, size, dxy.app</code>,
<code>dxy.val, mse.app, mse.val, binary, xval</code>.  <code>size</code> is the number of nodes,
<code>dxy</code> refers to Somers' <code>D</code>, <code>mse</code> refers to mean squared error of prediction,
<code>app</code> means apparent accuracy on training samples, <code>val</code> means validated
accuracy on test samples, <code>binary</code> is a logical variable indicating whether
or not the response variable was binary (a logical or 0/1 variable is
binary).  <code>size</code> will not be present if the user specifies <code>k</code>.
</p>


<h3>Side Effects</h3>

<p>prints if <code>pr=TRUE</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="Hmisc.html#topic+somers2">somers2</a></code>,
<code><a href="#topic+dxy.cens">dxy.cens</a></code>, <code><a href="graphics.html#topic+locator">locator</a></code>,
<code><a href="graphics.html#topic+legend">legend</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 100
set.seed(1)
x1 &lt;- runif(n)
x2 &lt;- runif(n)
x3 &lt;- runif(n)
y  &lt;- 1*(x1+x2+rnorm(n) &gt; 1)
table(y)
require(rpart)
f &lt;- rpart(y ~ x1 + x2 + x3, model=TRUE)
v &lt;- validate(f)
v    # note the poor validation
par(mfrow=c(1,2))
plot(v, legendloc=c(.2,.5))
par(mfrow=c(1,1))

## End(Not run)
</code></pre>

<hr>
<h2 id='validate.Rq'>Validation of a Quantile Regression Model</h2><span id='topic+validate.Rq'></span>

<h3>Description</h3>

<p>The <code>validate</code> function when used on an object created by
<code>Rq</code> does resampling validation of a quantile regression
model, with or without backward step-down variable deletion.  Uses
resampling to estimate the optimism in various measures of predictive
accuracy which include mean absolute prediction error (MAD), Spearman
rho, the <code class="reqn">g</code>-index, and the intercept and slope 
of an overall 
calibration <code class="reqn">a + b\hat{y}</code>.  The &quot;corrected&quot;
slope can be thought of as shrinkage factor that takes into account
overfitting.  <code>validate.Rq</code> can also be used when a model for a
continuous response is going to be applied to a binary response. A
Somers' <code class="reqn">D_{xy}</code> for this case is computed for each resample by
dichotomizing <code>y</code>. This can be used to obtain an ordinary receiver
operating characteristic curve area using the formula <code class="reqn">0.5(D_{xy} +
1)</code>. See <code>predab.resample</code> for the list of
resampling methods.
</p>
<p>The LaTeX <code>needspace</code> package must be in effect to use the
<code>latex</code> method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'># fit &lt;- fitting.function(formula=response ~ terms, x=TRUE, y=TRUE)
## S3 method for class 'Rq'
validate(fit, method="boot", B=40,
         bw=FALSE, rule="aic", type="residual", sls=0.05, aics=0, 
         force=NULL, estimates=TRUE, pr=FALSE, u=NULL, rel="&gt;",
         tolerance=1e-7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate.Rq_+3A_fit">fit</code></td>
<td>

<p>a fit derived by <code>Rq</code>. The options <code>x=TRUE</code> and <code>y=TRUE</code>
must have been specified.  See <code>validate</code> for a description of
arguments <code>method</code> - <code>pr</code>.
</p>
</td></tr>
<tr><td><code id="validate.Rq_+3A_method">method</code>, <code id="validate.Rq_+3A_b">B</code>, <code id="validate.Rq_+3A_bw">bw</code>, <code id="validate.Rq_+3A_rule">rule</code>, <code id="validate.Rq_+3A_type">type</code>, <code id="validate.Rq_+3A_sls">sls</code>, <code id="validate.Rq_+3A_aics">aics</code>, <code id="validate.Rq_+3A_force">force</code>, <code id="validate.Rq_+3A_estimates">estimates</code>, <code id="validate.Rq_+3A_pr">pr</code></td>
<td>
<p>see
<code><a href="#topic+validate">validate</a></code> and <code><a href="#topic+predab.resample">predab.resample</a></code> and
<code><a href="#topic+fastbw">fastbw</a></code></p>
</td></tr>
<tr><td><code id="validate.Rq_+3A_u">u</code></td>
<td>

<p>If specifed, <code>y</code> is also dichotomized at the cutoff <code>u</code> for
the purpose of getting a bias-corrected estimate of <code class="reqn">D_{xy}</code>.
</p>
</td></tr>
<tr><td><code id="validate.Rq_+3A_rel">rel</code></td>
<td>

<p>relationship for dichotomizing predicted <code>y</code>. Defaults to
<code>"&gt;"</code> to use <code>y&gt;u</code>. <code>rel</code> can also be <code>"&lt;"</code>,
<code>"&gt;="</code>, and <code>"&lt;="</code>. 
</p>
</td></tr>
<tr><td><code id="validate.Rq_+3A_tolerance">tolerance</code></td>
<td>

<p>ignored
</p>
</td></tr>
<tr><td><code id="validate.Rq_+3A_...">...</code></td>
<td>

<p>other arguments to pass to <code>predab.resample</code>, such as <code>group</code>, <code>cluster</code>, and <code>subset</code>
</p>
</td></tr></table>


<h3>Value</h3>

<p>matrix with rows corresponding to various indexes, and 
optionally <code class="reqn">D_{xy}</code>, and
columns for the original index, resample estimates, 
indexes applied to whole or omitted sample using model derived from
resample, average optimism, corrected index, and number of successful resamples.
</p>


<h3>Side Effects</h3>

<p>prints a summary, and optionally statistics for each re-fit
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Rq">Rq</a></code>, <code><a href="#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+fastbw">fastbw</a></code>,
<code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+rms.trans">rms.trans</a></code>,
<code><a href="#topic+gIndex">gIndex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- runif(200)
x2 &lt;- sample(0:3, 200, TRUE)
x3 &lt;- rnorm(200)
distance &lt;- (x1 + x2/3 + rnorm(200))^2

f &lt;- Rq(sqrt(distance) ~ rcs(x1,4) + scored(x2) + x3, x=TRUE, y=TRUE)

#Validate full model fit (from all observations) but for x1 &lt; .75
validate(f, B=20, subset=x1 &lt; .75)   # normally B=300

#Validate stepwise model with typical (not so good) stopping rule
validate(f, B=20, bw=TRUE, rule="p", sls=.1, type="individual")
</code></pre>

<hr>
<h2 id='vif'>Variance Inflation Factors</h2><span id='topic+vif'></span>

<h3>Description</h3>

<p>Computes variance inflation factors from the covariance matrix of
parameter estimates, using the method of Davis et al. (1986), which
is based on the correlation matrix from the information matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vif(fit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vif_+3A_fit">fit</code></td>
<td>

<p>an object created by <code>lrm</code>, <code>ols</code>, <code>psm</code>, <code>cph</code>,
<code>Rq</code>, <code>Glm</code>, <code>glm</code> 
</p>
</td></tr></table>


<h3>Value</h3>

<p>vector of vifs</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
fh@fharrell.com
</p>


<h3>References</h3>

<p>Davis CE, Hyde JE, Bangdiwala SI, Nelson JJ: An example of dependencies 
among variables in a conditional logistic regression. In Modern
Statistical Methods in Chronic Disease Epidemiology, Eds SH Moolgavkar and
RL Prentice, pp. 140&ndash;147.  New York: Wiley; 1986.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rmsMisc">rmsMisc</a></code> (for <code><a href="Hmisc.html#topic+num.intercepts">num.intercepts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- rnorm(100)
x2 &lt;- x1+.1*rnorm(100)
y  &lt;- sample(0:1, 100, TRUE)
f  &lt;- lrm(y ~ x1 + x2)
vif(f)
</code></pre>

<hr>
<h2 id='which.influence'>
Which Observations are Influential
</h2><span id='topic+which.influence'></span><span id='topic+show.influence'></span>

<h3>Description</h3>

<p>Creates a list with a component for each factor in the model.  The
names of the components are the factor names.  Each component contains
the observation identifiers of all observations that are &quot;overly
influential&quot; with respect to that factor, meaning that <code class="reqn">|dfbetas| &gt;
u</code> for at least one <code class="reqn">\beta_i</code> associated with that factor,
for a given <code>cutoff</code>.  The default <code>cutoff</code> is <code>.2</code>.  The
fit must come from a function that has <code>resid(fit, type="dfbetas")</code>
defined.
</p>
<p><code>show.influence</code>, written by Jens Oehlschlaegel-Akiyoshi, applies the
result of <code>which.influence</code> to a data frame, usually the one used to
fit the model, to report the results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which.influence(fit, cutoff=.2)

show.influence(object, dframe, report=NULL, sig=NULL, id=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="which.influence_+3A_fit">fit</code></td>
<td>

<p>fit object
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_object">object</code></td>
<td>

<p>the result of <code>which.influence</code>
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_dframe">dframe</code></td>
<td>

<p>data frame containing observations pertinent to the model fit
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_cutoff">cutoff</code></td>
<td>

<p>cutoff value
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_report">report</code></td>
<td>

<p>other columns of the data frame to report besides those corresponding
to predictors that are influential for some observations
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_sig">sig</code></td>
<td>

<p>runs results through <code>signif</code> with <code>sig</code> digits if <code>sig</code> is given
</p>
</td></tr>
<tr><td><code id="which.influence_+3A_id">id</code></td>
<td>

<p>a character vector that labels rows of <code>dframe</code> if <code>row.names</code> were
not used
</p>
</td></tr></table>


<h3>Value</h3>

<p><code>show.influence</code> returns a marked dataframe with the first column being
a count of influence values
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics, Vanderbilt University<br />
fh@fharrell.com
<br />
</p>
<p>Jens Oehlschlaegel-Akiyoshi<br />
Center for Psychotherapy Research<br />
Christian-Belser-Strasse 79a<br />
D-70597 Stuttgart Germany<br />
oehl@psyres-stuttgart.de
</p>


<h3>See Also</h3>

<p><code><a href="#topic+residuals.lrm">residuals.lrm</a></code>, <code><a href="#topic+residuals.cph">residuals.cph</a></code>,
<code><a href="#topic+residuals.ols">residuals.ols</a></code>, <code><a href="#topic+rms">rms</a></code>, <code><a href="#topic+lrm">lrm</a></code>,
<code><a href="#topic+ols">ols</a></code>, <code><a href="#topic+cph">cph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#print observations in data frame that are influential,
#separately for each factor in the model
x1 &lt;- 1:20
x2 &lt;- abs(x1-10)
x3 &lt;- factor(rep(0:2,length.out=20))
y  &lt;- c(rep(0:1,8),1,1,1,1)
f  &lt;- lrm(y ~ rcs(x1,3) + x2 + x3, x=TRUE,y=TRUE)
w &lt;- which.influence(f, .55)
nam &lt;- names(w)
d   &lt;- data.frame(x1,x2,x3,y)
for(i in 1:length(nam)) {
 print(paste("Influential observations for effect of ",nam[i]),quote=FALSE)
 print(d[w[[i]],])
}

show.influence(w, d)  # better way to show results
</code></pre>

<hr>
<h2 id='Xcontrast'>Xcontrast</h2><span id='topic+Xcontrast'></span>

<h3>Description</h3>

<p>Produce Design Matrices for Contrasts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Xcontrast(
  fit,
  a,
  b = NULL,
  a2 = NULL,
  b2 = NULL,
  ycut = NULL,
  weights = "equal",
  expand = TRUE,
  Zmatrix = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Xcontrast_+3A_fit">fit</code></td>
<td>
<p>an 'rms' or 'rmsb' fit object, not necessarily complete</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_a">a</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_b">b</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_a2">a2</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_b2">b2</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_ycut">ycut</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_weights">weights</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_expand">expand</code></td>
<td>
<p>see [rms::contrast.rms()]</p>
</td></tr>
<tr><td><code id="Xcontrast_+3A_zmatrix">Zmatrix</code></td>
<td>
<p>set to 'FALSE' for a partial PO model in which you do not want to include the Z matrix in the returned contrast matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simpler version of 'contrast.rms' that creates design matrices or differences of them and does not require the fit object to be complete (i.e., to have coefficients).  This is used for the 'pcontrast' option in [rmsb::blrm()].
</p>


<h3>Value</h3>

<p>numeric matrix
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
