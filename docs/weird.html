<!DOCTYPE html><html><head><title>Help for package weird</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {weird}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#weird-package'><p>weird: Functions and Data Sets for &quot;That's Weird: Anomaly Detection Using R&quot; by Rob J Hyndman</p></a></li>
<li><a href='#as_kde'><p>Convert data frame or matrix object to kde class</p></a></li>
<li><a href='#autoplot.kde'><p>Produce ggplot of densities in 1 or 2 dimensions</p></a></li>
<li><a href='#cricket_batting'><p>Cricket batting data for international test players</p></a></li>
<li><a href='#density_scores'><p>Density scores</p></a></li>
<li><a href='#fetch_wine_reviews'><p>Wine prices and points</p></a></li>
<li><a href='#gg_bagplot'><p>Bagplot</p></a></li>
<li><a href='#gg_hdrboxplot'><p>HDR plot</p></a></li>
<li><a href='#glosh_scores'><p>GLOSH scores</p></a></li>
<li><a href='#grubbs_anomalies'><p>Statistical tests for anomalies using Grubbs' test and Dixon's test</p></a></li>
<li><a href='#hdr_palette'><p>Color palette designed for plotting Highest Density Regions</p></a></li>
<li><a href='#hdr_table'><p>Table of Highest Density Regions</p></a></li>
<li><a href='#kde_bandwidth'><p>Robust bandwidth estimation for kernel density estimation</p></a></li>
<li><a href='#lof_scores'><p>Local outlier factors</p></a></li>
<li><a href='#lookout'><p>Lookout probabilities</p></a></li>
<li><a href='#mvscale'><p>Compute robust multivariate scaled data</p></a></li>
<li><a href='#n01'><p>Multivariate standard normal data</p></a></li>
<li><a href='#oldfaithful'><p>Old faithful eruption data</p></a></li>
<li><a href='#peirce_anomalies'><p>Anomalies according to Peirce's and Chauvenet's criteria</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#stray_anomalies'><p>Stray anomalies</p></a></li>
<li><a href='#stray_scores'><p>Stray scores</p></a></li>
<li><a href='#weird_conflicts'><p>Conflicts between weird packages and other packages</p></a></li>
<li><a href='#weird_packages'><p>List all packages loaded by weird</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Functions and Data Sets for "That's Weird: Anomaly Detection
Using R" by Rob J Hyndman</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>All functions and data sets required for the examples in the book
    Hyndman (2024) "That's Weird: Anomaly Detection Using R" <a href="https://OTexts.com/weird/">https://OTexts.com/weird/</a>.  
    All packages needed to run the examples are also loaded.</td>
</tr>
<tr>
<td>Imports:</td>
<td>aplpack, broom, cli (&ge; 1.0.0), crayon (&ge; 1.3.4), dbscan,
dplyr (&ge; 0.7.4), evd, ggplot2 (&ge; 3.1.1), grDevices,
interpolation, ks, purrr (&ge; 0.2.4), rlang, robustbase,
rstudioapi (&ge; 0.7), stray, tibble (&ge; 1.4.2)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mgcv, outliers, testthat (&ge; 3.0.0), tidyr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://pkg.robjhyndman.com/weird-package/">https://pkg.robjhyndman.com/weird-package/</a>,
<a href="https://github.com/robjhyndman/weird-package">https://github.com/robjhyndman/weird-package</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/robjhyndman/weird-package/issues">https://github.com/robjhyndman/weird-package/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-24 06:27:01 UTC; hyndman</td>
</tr>
<tr>
<td>Author:</td>
<td>Rob Hyndman <a href="https://orcid.org/0000-0002-2140-5352"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  RStudio [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rob Hyndman &lt;Rob.Hyndman@monash.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-24 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='weird-package'>weird: Functions and Data Sets for &quot;That's Weird: Anomaly Detection Using R&quot; by Rob J Hyndman</h2><span id='topic+weird'></span><span id='topic+weird-package'></span>

<h3>Description</h3>

<p>All functions and data sets required for the examples in the book Hyndman (2024) &quot;That's Weird: Anomaly Detection Using R&quot; <a href="https://OTexts.com/weird/">https://OTexts.com/weird/</a>. All packages needed to run the examples are also loaded.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Rob Hyndman <a href="mailto:Rob.Hyndman@monash.edu">Rob.Hyndman@monash.edu</a> (<a href="https://orcid.org/0000-0002-2140-5352">ORCID</a>) [copyright holder]
</p>
<p>Other contributors:
</p>

<ul>
<li><p> RStudio [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://pkg.robjhyndman.com/weird-package/">https://pkg.robjhyndman.com/weird-package/</a>
</p>
</li>
<li> <p><a href="https://github.com/robjhyndman/weird-package">https://github.com/robjhyndman/weird-package</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/robjhyndman/weird-package/issues">https://github.com/robjhyndman/weird-package/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as_kde'>Convert data frame or matrix object to kde class</h2><span id='topic+as_kde'></span>

<h3>Description</h3>

<p>A density specified as a data frame or matrix can be converted to a kde object.
This is useful for plotting the density using <code><a href="#topic+autoplot.kde">autoplot.kde</a></code>.
As kde objects are defined on a grid, the density values are interpolated
based on the points in the data frame or matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_kde(object, density_column, ngrid, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_kde_+3A_object">object</code></td>
<td>
<p>Data frame or matrix with numerical columns, where one column
(specified by <code>density_column</code>) contains the density values, and the
remaining columns define the points at which the density is evaluated.</p>
</td></tr>
<tr><td><code id="as_kde_+3A_density_column">density_column</code></td>
<td>
<p>Name of the column containing the density values, specified
as a bare expression. If missing, the last column is used.</p>
</td></tr>
<tr><td><code id="as_kde_+3A_ngrid">ngrid</code></td>
<td>
<p>Number of points to use for the grid in each dimension. Default is
10001 for univariate densities and 101 for multivariate densities.</p>
</td></tr>
<tr><td><code id="as_kde_+3A_...">...</code></td>
<td>
<p>Additional arguments are ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;kde&quot;
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tibble(y = seq(-4, 4, by = 0.01), density = dnorm(y)) |&gt;
  as_kde()
</code></pre>

<hr>
<h2 id='autoplot.kde'>Produce ggplot of densities in 1 or 2 dimensions</h2><span id='topic+autoplot.kde'></span>

<h3>Description</h3>

<p>Produce ggplot of densities in 1 or 2 dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kde'
autoplot(
  object,
  prob = seq(9)/10,
  fill = FALSE,
  show_hdr = FALSE,
  show_points = FALSE,
  show_mode = FALSE,
  show_lookout = FALSE,
  color = "#00659e",
  palette = hdr_palette,
  alpha = ifelse(fill, 1, min(1, 1000/NROW(object$x))),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.kde_+3A_object">object</code></td>
<td>
<p>Probability density function as estimated by <code>ks::kde()</code>.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_prob">prob</code></td>
<td>
<p>Probability of the HDR contours to be drawn (for a bivariate plot only).</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_fill">fill</code></td>
<td>
<p>If <code>TRUE</code>, and the density is bivariate, the bivariate contours
are shown as filled regions rather than lines.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_show_hdr">show_hdr</code></td>
<td>
<p>If <code>TRUE</code>, and the density is univariate, then the HDR regions
specified by <code>prob</code> are shown as a ribbon below the density.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_show_points">show_points</code></td>
<td>
<p>If <code>TRUE</code>, then individual points are plotted.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_show_mode">show_mode</code></td>
<td>
<p>If <code>TRUE</code>, then the mode of the distribution is shown.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_show_lookout">show_lookout</code></td>
<td>
<p>If <code>TRUE</code>, then the observations with lookout probabilities less than 0.05 are shown in red.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_color">color</code></td>
<td>
<p>Color used for mode and HDR contours. If <code>palette = hdr_palette</code>,
it is also used as the basis for HDR regions.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_palette">palette</code></td>
<td>
<p>Color palette function to use for HDR filled regions
(if <code>fill</code> is <code>TRUE</code> or <code>show_hdr</code> is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_alpha">alpha</code></td>
<td>
<p>Transparency of points. When <code>fill</code> is <code>FALSE</code>, defaults to
min(1, 1000/n), where n is the number of observations. Otherwise, set to 1.</p>
</td></tr>
<tr><td><code id="autoplot.kde_+3A_...">...</code></td>
<td>
<p>Additional arguments are currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces a ggplot of the density estimate produced by <code>ks::kde()</code>.
For univariate densities, it produces a line plot of the density function, with
an optional ribbon showing some highest density regions (HDRs) and/or the observations.
For bivariate densities, it produces a contour plot of the density function, with
the observations optionally shown as points.
The mode can also be drawn as a point with the HDRs.
For bivariate densities, the combination of <code>fill = TRUE</code>, <code>show_points = TRUE</code>,
<code>show_mode = TRUE</code>, and <code>prob = c(0.5, 0.99)</code> is equivalent to an HDR boxplot.
For univariate densities,  the combination of <code>show_hdr = TRUE</code>, <code>show_points = TRUE</code>,
<code>show_mode = TRUE</code>, and <code>prob = c(0.5, 0.99)</code> is equivalent to an HDR boxplot.
</p>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate density
c(rnorm(500), rnorm(500, 4, 1.5)) |&gt;
  kde() |&gt;
  autoplot(show_hdr = TRUE, prob= c(0.5, 0.95), color = "#c14b14")
ymat &lt;- tibble(y1 = rnorm(5000), y2 = y1 + rnorm(5000))
ymat |&gt;
  kde(H = kde_bandwidth(ymat)) |&gt;
  autoplot(show_points = TRUE, alpha = 0.1, fill = TRUE)
</code></pre>

<hr>
<h2 id='cricket_batting'>Cricket batting data for international test players</h2><span id='topic+cricket_batting'></span>

<h3>Description</h3>

<p>A dataset containing career batting statistics for all international test
players (men and women) up to 6 October 2021.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cricket_batting
</code></pre>


<h3>Format</h3>

<p>A data frame with 3754 rows and 15 variables:
</p>

<dl>
<dt>Player</dt><dd><p>Player name in form of &quot;initials surname&quot;</p>
</dd>
<dt>Country</dt><dd><p>Country played for</p>
</dd>
<dt>Start</dt><dd><p>First year of test playing career</p>
</dd>
<dt>End</dt><dd><p>Last year of test playing career</p>
</dd>
<dt>Matches</dt><dd><p>Number of matches played</p>
</dd>
<dt>Innings</dt><dd><p>Number of innings batted</p>
</dd>
<dt>NotOuts</dt><dd><p>Number of times not out</p>
</dd>
<dt>Runs</dt><dd><p>Total runs scored</p>
</dd>
<dt>HighScore</dt><dd><p>Highest score in an innings</p>
</dd>
<dt>HighScoreNotOut</dt><dd><p>Was highest score not out?</p>
</dd>
<dt>Average</dt><dd><p>Batting average at end of career</p>
</dd>
<dt>Hundreds</dt><dd><p>Total number of 100s scored</p>
</dd>
<dt>Fifties</dt><dd><p>Total number of 50s scored</p>
</dd>
<dt>Ducks</dt><dd><p>Total number of 0s scored</p>
</dd>
<dt>Gender</dt><dd><p>&quot;Men&quot; or &quot;Women&quot;</p>
</dd>
</dl>



<h3>Value</h3>

<p>Data frame
</p>


<h3>Source</h3>

<p><a href="https://www.espncricinfo.com">https://www.espncricinfo.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cricket_batting |&gt;
  filter(Innings &gt; 20) |&gt;
  select(Player, Country, Matches, Runs, Average, Hundreds, Fifties, Ducks) |&gt;
  arrange(desc(Average))
</code></pre>

<hr>
<h2 id='density_scores'>Density scores</h2><span id='topic+density_scores'></span><span id='topic+density_scores.default'></span><span id='topic+density_scores.kde'></span><span id='topic+density_scores.lm'></span><span id='topic+density_scores.gam'></span>

<h3>Description</h3>

<p>Compute density scores or leave-one-out density scores from a
model or a kernel density estimate of a data set.
The density scores are defined as minus the log of the conditional density,
or kernel density estimate, at each observation.
The leave-one-out density scores (or LOO density scores) are obtained by
estimating the conditional density or kernel density estimate using all
other observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>density_scores(object, loo = FALSE, ...)

## Default S3 method:
density_scores(
  object,
  loo = FALSE,
  h = kde_bandwidth(object, method = "double"),
  H = kde_bandwidth(object, method = "double"),
  ...
)

## S3 method for class 'kde'
density_scores(object, loo = FALSE, ...)

## S3 method for class 'lm'
density_scores(object, loo = FALSE, ...)

## S3 method for class 'gam'
density_scores(object, loo = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="density_scores_+3A_object">object</code></td>
<td>
<p>A model object or a numerical data set.</p>
</td></tr>
<tr><td><code id="density_scores_+3A_loo">loo</code></td>
<td>
<p>Should leave-one-out density scores be computed?</p>
</td></tr>
<tr><td><code id="density_scores_+3A_...">...</code></td>
<td>
<p>Other arguments are ignored.</p>
</td></tr>
<tr><td><code id="density_scores_+3A_h">h</code></td>
<td>
<p>Bandwidth for univariate kernel density estimate. Default is <code><a href="#topic+kde_bandwidth">kde_bandwidth</a></code>.</p>
</td></tr>
<tr><td><code id="density_scores_+3A_h">H</code></td>
<td>
<p>Bandwidth for multivariate kernel density estimate. Default is <code><a href="#topic+kde_bandwidth">kde_bandwidth</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the first argument is a numerical vector or matrix, then
a kernel density estimate is computed, using a Gaussian kernel,
with default bandwidth given by a robust normal reference rule.
Otherwise the model is used to compute the conditional
density function at each observation, from which the density scores (or
possibly the LOO density scores) are obtained.
</p>


<h3>Value</h3>

<p>A numerical vector containing either the density scores, or the LOO
density scores.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kde_bandwidth">kde_bandwidth</a></code>
<code><a href="ks.html#topic+kde">kde</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Density scores computed from bivariate data set
of &lt;- oldfaithful |&gt;
  filter(duration &lt; 7000, waiting &lt; 7000) |&gt;
  mutate(
    fscores = density_scores(cbind(duration, waiting)),
    loo_fscores = density_scores(cbind(duration, waiting), loo = TRUE),
    lookout_prob = lookout(density_scores = fscores, loo_scores = loo_fscores)
  )
of |&gt;
  ggplot(aes(x = duration, y = waiting, color = lookout_prob &lt; 0.01)) +
  geom_point()
# Density scores computed from bivariate KDE
f_kde &lt;- kde(of[, 2:3], H = kde_bandwidth(of[, 2:3]))
of |&gt;
  mutate(
    fscores = density_scores(f_kde),
    loo_fscores = density_scores(f_kde, loo = TRUE)
  )
# Density scores computed from linear model
of &lt;- oldfaithful |&gt;
  filter(duration &lt; 7200, waiting &lt; 7200)
lm_of &lt;- lm(waiting ~ duration, data = of)
of |&gt;
  mutate(
    fscore = density_scores(lm_of),
    loo_fscore = density_scores(lm_of, loo = TRUE),
    lookout_prob = lookout(density_scores = fscore, loo_scores = loo_fscore)
  ) |&gt;
  ggplot(aes(x = duration, y = waiting, color = lookout_prob &lt; 0.02)) +
  geom_point()
# Density scores computed from GAM
of &lt;- oldfaithful |&gt;
  filter(duration &gt; 1, duration &lt; 7200, waiting &lt; 7200)
gam_of &lt;- mgcv::gam(waiting ~ s(duration), data = of)
of |&gt;
  mutate(
    fscore = density_scores(gam_of),
    lookout_prob = lookout(density_scores = fscore)
  ) |&gt;
  filter(lookout_prob &lt; 0.02)
</code></pre>

<hr>
<h2 id='fetch_wine_reviews'>Wine prices and points</h2><span id='topic+fetch_wine_reviews'></span><span id='topic+wine_reviews'></span>

<h3>Description</h3>

<p>A data set containing data on wines from 44 countries, taken from <em>Wine Enthusiast Magazine</em>
during the week of 15 June 2017. The data are downloaded and returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_wine_reviews()
</code></pre>


<h3>Format</h3>

<p>A data frame with 110,203 rows and 8 columns:
</p>

<dl>
<dt>country</dt><dd><p>Country of origin</p>
</dd>
<dt>state</dt><dd><p>State or province of origin</p>
</dd>
<dt>region</dt><dd><p>Region of origin</p>
</dd>
<dt>winery</dt><dd><p>Name of vineyard that made the wine</p>
</dd>
<dt>variety</dt><dd><p>Variety of grape</p>
</dd>
<dt>points</dt><dd><p>Points allocated by WineEnthusiast reviewer on a scale of 0-100</p>
</dd>
<dt>price</dt><dd><p>Price of a bottle of wine in $US</p>
</dd>
<dt>year</dt><dd><p>Year of wine extracted from <code>title</code></p>
</dd>
</dl>



<h3>Value</h3>

<p>Data frame
</p>


<h3>Source</h3>

<p><a href="https://kaggle.com">https://kaggle.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
wine_reviews &lt;- fetch_wine_reviews()
wine_reviews |&gt;
 ggplot(aes(x = points, y = price)) +
 geom_jitter(height = 0, width = 0.2, alpha = 0.1) +
 scale_y_log10()

## End(Not run)
</code></pre>

<hr>
<h2 id='gg_bagplot'>Bagplot</h2><span id='topic+gg_bagplot'></span>

<h3>Description</h3>

<p>Produces a bivariate bagplot. A bagplot is analagous to a
univariate boxplot, except it is in two dimensions. Like a boxplot, it
shows the median, a region containing 50% of the observations, a region
showing the remaining observations other than outliers, and any outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gg_bagplot(
  data,
  var1,
  var2,
  col = c(hdr_palette(color = "#00659e", prob = c(0.5, 0.99)), "#000000"),
  scatterplot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gg_bagplot_+3A_data">data</code></td>
<td>
<p>A data frame or matrix containing the data.</p>
</td></tr>
<tr><td><code id="gg_bagplot_+3A_var1">var1</code></td>
<td>
<p>The name of the first variable to plot (a bare expression).</p>
</td></tr>
<tr><td><code id="gg_bagplot_+3A_var2">var2</code></td>
<td>
<p>The name of the second variable to plot (a bare expression).</p>
</td></tr>
<tr><td><code id="gg_bagplot_+3A_col">col</code></td>
<td>
<p>The colors to use in the order: median, bag, loop and outliers.</p>
</td></tr>
<tr><td><code id="gg_bagplot_+3A_scatterplot">scatterplot</code></td>
<td>
<p>A logical argument indicating if a regular bagplot is required
(<code>FALSE</code>), or if a scatterplot in the same colors is required (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="gg_bagplot_+3A_...">...</code></td>
<td>
<p>Other arguments are passed to the <code><a href="aplpack.html#topic+compute.bagplot">compute.bagplot</a></code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object showing a bagplot or scatterplot of the data.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Rousseeuw, P. J., Ruts, I., &amp; Tukey, J. W. (1999).
The bagplot: A bivariate boxplot. <em>The American Statistician</em>, <b>52</b>(4), 382–387.
</p>


<h3>See Also</h3>

<p><code><a href="aplpack.html#topic+bagplot">bagplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gg_bagplot(n01, v1, v2)
gg_bagplot(n01, v1, v2, scatterplot = TRUE)
</code></pre>

<hr>
<h2 id='gg_hdrboxplot'>HDR plot</h2><span id='topic+gg_hdrboxplot'></span>

<h3>Description</h3>

<p>Produces a 1d or 2d box plot of HDR regions. The darker regions
contain observations with higher probability, while the lighter regions contain
points with lower probability. Points outside the largest HDR are shown as
individual points. Points with lookout probabilities
less than 0.05 are optionally shown in red.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gg_hdrboxplot(
  data,
  var1,
  var2 = NULL,
  prob = c(0.5, 0.99),
  color = "#00659e",
  scatterplot = FALSE,
  show_lookout = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gg_hdrboxplot_+3A_data">data</code></td>
<td>
<p>A data frame or matrix containing the data.</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_var1">var1</code></td>
<td>
<p>The name of the first variable to plot (a bare expression).</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_var2">var2</code></td>
<td>
<p>Optionally, the name of the second variable to plot (a bare expression).</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_prob">prob</code></td>
<td>
<p>A numeric vector specifying the coverage probabilities for the HDRs.</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_color">color</code></td>
<td>
<p>The base color to use for the mode. Colors for the HDRs are generated
by whitening this color.</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_scatterplot">scatterplot</code></td>
<td>
<p>A logical argument indicating if a regular HDR plot is required
(<code>FALSE</code>), or if a scatterplot in the same colors is required (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_show_lookout">show_lookout</code></td>
<td>
<p>A logical argument indicating if the plot should highlight observations with &quot;lookout&quot;
probabilities less than 0.05.</p>
</td></tr>
<tr><td><code id="gg_hdrboxplot_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="ks.html#topic+kde">kde</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original HDR boxplot proposed by Hyndman (1996), R can be produced with
all arguments set to their defaults other than <code>lookout</code>.
</p>


<h3>Value</h3>

<p>A ggplot object showing an HDR plot or scatterplot of the data.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Hyndman, R J (1996) Computing and Graphing Highest Density Regions,
<em>The American Statistician</em>, <strong>50</strong>(2), 120–126. <a href="https://robjhyndman.com/publications/hdr/">https://robjhyndman.com/publications/hdr/</a>
Kandanaarachchi, S &amp; Hyndman, R J (2022) &quot;Leave-one-out kernel density estimates for outlier detection&quot;,
<em>J Computational &amp; Graphical Statistics</em>, <strong>31</strong>(2), 586-599. <a href="https://robjhyndman.com/publications/lookout/">https://robjhyndman.com/publications/lookout/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- data.frame(x = c(rnorm(1000), rnorm(1000, 5, 1)))
df$y &lt;- df$x + rnorm(200, sd=2)
gg_hdrboxplot(df, x)
gg_hdrboxplot(df, x, y, scatterplot = TRUE)
oldfaithful |&gt;
  filter(duration &lt; 7000, waiting &lt; 7000) |&gt;
  gg_hdrboxplot(duration, waiting, scatterplot = TRUE)
cricket_batting |&gt;
  filter(Innings &gt; 20) |&gt;
  gg_hdrboxplot(Average)

</code></pre>

<hr>
<h2 id='glosh_scores'>GLOSH scores</h2><span id='topic+glosh_scores'></span>

<h3>Description</h3>

<p>Compute Global-Local Outlier Score from Hierarchies. This is based
on hierarchical clustering where the minimum cluster size is k. The resulting
outlier score is a measure of how anomalous each observation is.
The function uses <code>dbscan::<a href="dbscan.html#topic+hdbscan">hdbscan</a></code> to do the calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glosh_scores(y, k = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glosh_scores_+3A_y">y</code></td>
<td>
<p>Numerical matrix or vector of data</p>
</td></tr>
<tr><td><code id="glosh_scores_+3A_k">k</code></td>
<td>
<p>Minimum cluster size. Default: 5.</p>
</td></tr>
<tr><td><code id="glosh_scores_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>dbscan::<a href="dbscan.html#topic+hdbscan">hdbscan</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector containing GLOSH values
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>See Also</h3>

<p><code>dbscan::<a href="dbscan.html#topic+glosh">glosh</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rnorm(49), 5)
glosh_scores(y)
</code></pre>

<hr>
<h2 id='grubbs_anomalies'>Statistical tests for anomalies using Grubbs' test and Dixon's test</h2><span id='topic+grubbs_anomalies'></span><span id='topic+dixon_anomalies'></span>

<h3>Description</h3>

<p>Grubbs' test (proposed in 1950) identifies possible anomalies in univariate
data using z-scores assuming the data come from a normal distribution.
Dixon's test (also from 1950) compares the difference in the largest two values
to the range of the data. Critical values for Dixon's test have been
computed using simulation with interpolation using a quadratic model on
logit(alpha) and log(log(n)).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grubbs_anomalies(y, alpha = 0.05)

dixon_anomalies(y, alpha = 0.05, two_sided = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grubbs_anomalies_+3A_y">y</code></td>
<td>
<p>numerical vector of observations</p>
</td></tr>
<tr><td><code id="grubbs_anomalies_+3A_alpha">alpha</code></td>
<td>
<p>size of the test.</p>
</td></tr>
<tr><td><code id="grubbs_anomalies_+3A_two_sided">two_sided</code></td>
<td>
<p>If <code>TRUE</code>, both minimum and maximums will be considered. Otherwise
only the maximum will be used. (Take negative values to consider only the minimum with
<code>two_sided=FALSE</code>.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Grubbs' test is based on z-scores, and a point is identified as an
anomaly when the associated absolute z-score is greater than a threshold value.
A vector of logical values is returned, where <code>TRUE</code> indicates an anomaly.
This version of Grubbs' test looks for outliers anywhere in the sample.
Grubbs' original test came in several variations which looked for one outlier,
or two outliers in one tail, or two outliers on opposite tails. These variations
are implemented in the <code><a href="outliers.html#topic+grubbs.test">grubbs.test</a></code> function.
Dixon's test only considers the maximum (and possibly the minimum) as potential outliers.
</p>


<h3>Value</h3>

<p>A logical vector
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Grubbs, F. E. (1950). Sample criteria for testing outlying observations.
<em>Annals of Mathematical Statistics</em>, 21(1), 27–58.
Dixon, W. J. (1950). Analysis of extreme values.
<em>Annals of Mathematical Statistics</em>, 21(4), 488–506.
</p>


<h3>See Also</h3>

<p><code><a href="outliers.html#topic+grubbs.test">grubbs.test</a></code>, <code><a href="outliers.html#topic+dixon.test">dixon.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(rnorm(1000), 5:10)
tibble(x = x) |&gt; filter(grubbs_anomalies(x))
tibble(x = x) |&gt; filter(dixon_anomalies(x))
y &lt;- c(rnorm(1000), 5)
tibble(y = y) |&gt; filter(grubbs_anomalies(y))
tibble(y = y) |&gt; filter(dixon_anomalies(y))
</code></pre>

<hr>
<h2 id='hdr_palette'>Color palette designed for plotting Highest Density Regions</h2><span id='topic+hdr_palette'></span>

<h3>Description</h3>

<p>A sequential color palette is returned, with the first color being <code>color</code>,
and the rest of the colors being a mix of <code>color</code> with increasing amounts of white.
If <code>prob</code> is provided, then the mixing proportions are determined by <code>prob</code> (and
n is ignored). Otherwise the mixing proportions are equally spaced between 0 and 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdr_palette(n, color = "#00659e", prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdr_palette_+3A_n">n</code></td>
<td>
<p>Number of colors in palette.</p>
</td></tr>
<tr><td><code id="hdr_palette_+3A_color">color</code></td>
<td>
<p>First color of vector.</p>
</td></tr>
<tr><td><code id="hdr_palette_+3A_prob">prob</code></td>
<td>
<p>Vector of probabilities between 0 and 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function that returns a vector of colors of length <code>length(prob) + 1</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hdr_palette(prob = c(0.5, 0.99))
</code></pre>

<hr>
<h2 id='hdr_table'>Table of Highest Density Regions</h2><span id='topic+hdr_table'></span>

<h3>Description</h3>

<p>Compute the highest density regions (HDR) for a kernel density estimate. The HDRs
are returned as a tibble with one row per interval and columns:
<code>prob</code> (giving the probability coverage),
<code>density</code> (the value of the density at the boundary of the HDR),
For one dimensional density functions, the tibble also has columns
<code>lower</code> (the lower ends of the intervals),
<code>upper</code> (the upper ends of the interval),
<code>mode</code> (the point at which the density is maximized within each interval).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdr_table(
  y = NULL,
  density = NULL,
  prob = c(0.5, 0.99),
  h = kde_bandwidth(y, method = "double"),
  H = kde_bandwidth(y, method = "double"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdr_table_+3A_y">y</code></td>
<td>
<p>Numerical vector or matrix of data</p>
</td></tr>
<tr><td><code id="hdr_table_+3A_density">density</code></td>
<td>
<p>Probability density function, either estimated by <code>ks::kde()</code> or
a data frame or matrix with numerical columns that can be passed to <code>as_kde()</code>.</p>
</td></tr>
<tr><td><code id="hdr_table_+3A_prob">prob</code></td>
<td>
<p>Probability of the HDR</p>
</td></tr>
<tr><td><code id="hdr_table_+3A_h">h</code></td>
<td>
<p>Bandwidth for univariate kernel density estimate. Default is <code><a href="#topic+kde_bandwidth">kde_bandwidth</a></code>.</p>
</td></tr>
<tr><td><code id="hdr_table_+3A_h">H</code></td>
<td>
<p>Bandwidth for multivariate kernel density estimate. Default is <code><a href="#topic+kde_bandwidth">kde_bandwidth</a></code>.</p>
</td></tr>
<tr><td><code id="hdr_table_+3A_...">...</code></td>
<td>
<p>If <code>y</code> is supplied, other arguments are passed to <code><a href="ks.html#topic+kde">kde</a></code>.
Otherwise, additional arguments are passed to <code><a href="#topic+as_kde">as_kde</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Hyndman, R J. (1996) Computing and Graphing Highest Density Regions,
<em>The American Statistician</em>, <b>50</b>(2), 120–126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate HDRs
y &lt;- c(rnorm(100), rnorm(100, 3, 1))
hdr_table(y = y)
hdr_table(density = ks::kde(y))
x &lt;- seq(-4, 4, by = 0.01)
hdr_table(density = data.frame(y = x, density = dnorm(x)), prob = 0.95)
# Bivariate HDRs
y &lt;- cbind(rnorm(100), rnorm(100))
hdr_table(y = y)
grid &lt;- seq(-4, 4, by=0.1)
density &lt;- expand.grid(grid, grid) |&gt;
  mutate(density = dnorm(Var1) * dnorm(Var2))
hdr_table(density = density)
</code></pre>

<hr>
<h2 id='kde_bandwidth'>Robust bandwidth estimation for kernel density estimation</h2><span id='topic+kde_bandwidth'></span>

<h3>Description</h3>

<p>Robust bandwidth estimation for kernel density estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kde_bandwidth(
  data,
  method = c("robust_normal", "double", "lookout"),
  max.iter = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kde_bandwidth_+3A_data">data</code></td>
<td>
<p>A numeric matrix or data frame.</p>
</td></tr>
<tr><td><code id="kde_bandwidth_+3A_method">method</code></td>
<td>
<p>Method to use for selecting the bandwidth.
<code>robust_normal</code> uses a robust version of the normal reference rule.
<code>lookout</code> uses the topological data analysis approach that is part of the lookout algorithm.</p>
</td></tr>
<tr><td><code id="kde_bandwidth_+3A_max.iter">max.iter</code></td>
<td>
<p>How many times should the <code>lookout</code> method be iterated. That is, outliers
(probability &lt; 0.05) are removed and the bandwidth is re-computed from the
remaining observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of bandwidths (or scalar in the case of univariate data).
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate bandwidth calculation
kde_bandwidth(oldfaithful$duration)
# Bivariate bandwidth calculation
kde_bandwidth(oldfaithful[,2:3])
</code></pre>

<hr>
<h2 id='lof_scores'>Local outlier factors</h2><span id='topic+lof_scores'></span>

<h3>Description</h3>

<p>Compute local outlier factors using k nearest neighbours. A local
outlier factor is a measure of how anomalous each observation is based on
the density of neighbouring points.
The function uses <code>dbscan::<a href="dbscan.html#topic+lof">lof</a></code> to do the calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lof_scores(y, k = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lof_scores_+3A_y">y</code></td>
<td>
<p>Numerical matrix or vector of data</p>
</td></tr>
<tr><td><code id="lof_scores_+3A_k">k</code></td>
<td>
<p>Number of neighbours to include. Default: 5.</p>
</td></tr>
<tr><td><code id="lof_scores_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>dbscan::<a href="dbscan.html#topic+lof">lof</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector containing LOF values
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>See Also</h3>

<p><code>dbscan::<a href="dbscan.html#topic+lof">lof</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rnorm(49), 5)
lof_scores(y)
</code></pre>

<hr>
<h2 id='lookout'>Lookout probabilities</h2><span id='topic+lookout'></span>

<h3>Description</h3>

<p>Compute leave-one-out log score probabilities using a
Generalized Pareto distribution. These give the probability of each observation
being an anomaly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lookout(
  object = NULL,
  density_scores = NULL,
  loo_scores = density_scores,
  threshold_probability = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lookout_+3A_object">object</code></td>
<td>
<p>A model object or a numerical data set.</p>
</td></tr>
<tr><td><code id="lookout_+3A_density_scores">density_scores</code></td>
<td>
<p>Numerical vector of log scores</p>
</td></tr>
<tr><td><code id="lookout_+3A_loo_scores">loo_scores</code></td>
<td>
<p>Optional numerical vector of leave-one-out log scores</p>
</td></tr>
<tr><td><code id="lookout_+3A_threshold_probability">threshold_probability</code></td>
<td>
<p>Probability threshold when computing the POT model for the log scores.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can work with several object types.
If <code>object</code> is not <code>NULL</code>, then the object is passed to <code><a href="#topic+density_scores">density_scores</a></code>
to compute density scores (and possibly LOO density scores). Otherwise,
the density scores are taken from the <code>density_scores</code> argument, and the
LOO density scores are taken from the <code>loo_scores</code> argument. Then the Generalized
Pareto distribution is fitted to the scores, to obtain the probability of each observation.
</p>


<h3>Value</h3>

<p>A numerical vector containing the lookout probabilities
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Sevvandi Kandanaarachchi &amp; Rob J Hyndman (2022) &quot;Leave-one-out
kernel density estimates for outlier detection&quot;, <em>J Computational &amp; Graphical
Statistics</em>, <strong>31</strong>(2), 586-599. <a href="https://robjhyndman.com/publications/lookout/">https://robjhyndman.com/publications/lookout/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate data
tibble(
  y = c(5, rnorm(49)),
  lookout = lookout(y)
)
# Bivariate data with score calculation done outside the function
tibble(
  x = rnorm(50),
  y = c(5, rnorm(49)),
  fscores = density_scores(y),
  loo_fscores = density_scores(y, loo = TRUE),
  lookout = lookout(density_scores = fscores, loo_scores = loo_fscores)
)
# Using a regression model
of &lt;- oldfaithful |&gt; filter(duration &lt; 7200, waiting &lt; 7200)
fit_of &lt;- lm(waiting ~ duration, data = of)
of |&gt;
  mutate(lookout_prob = lookout(fit_of)) |&gt;
  arrange(lookout_prob)
</code></pre>

<hr>
<h2 id='mvscale'>Compute robust multivariate scaled data</h2><span id='topic+mvscale'></span>

<h3>Description</h3>

<p>A multivariate version of <code>base::scale()</code>, that takes account
of the covariance matrix of the data, and uses robust estimates
of center, scale and covariance by default. The centers are removed using medians, the
scale function is the IQR, and the covariance matrix is estimated using a
robust OGK estimate. The data are scaled using the Cholesky decomposition of
the inverse covariance. Then the scaled data are returned. This is useful for
computing pairwise Mahalanobis distances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvscale(
  object,
  center = stats::median,
  scale = robustbase::s_IQR,
  cov = robustbase::covOGK,
  warning = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvscale_+3A_object">object</code></td>
<td>
<p>A vector, matrix, or data frame containing some numerical data.</p>
</td></tr>
<tr><td><code id="mvscale_+3A_center">center</code></td>
<td>
<p>A function to compute the center of each numerical variable. Set
to NULL if no centering is required.</p>
</td></tr>
<tr><td><code id="mvscale_+3A_scale">scale</code></td>
<td>
<p>A function to scale each numerical variable. When
<code>cov = robustbase::covOGK</code>, it is passed as the <code>sigmamu</code> argument.</p>
</td></tr>
<tr><td><code id="mvscale_+3A_cov">cov</code></td>
<td>
<p>A function to compute the covariance matrix. Set to NULL if no rotation required.</p>
</td></tr>
<tr><td><code id="mvscale_+3A_warning">warning</code></td>
<td>
<p>Should a warning be issued if non-numeric columns are ignored?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optionally, the centering and scaling can be done for each variable
separately, so there is no rotation of the data, by setting <code>cov = NULL</code>.
Also optionally, non-robust methods can be used by specifying <code>center = mean</code>,
scale = <code>stats::sd</code>, and <code>cov = stats::cov</code>. Any non-numeric columns are retained
with a warning.
</p>


<h3>Value</h3>

<p>A vector, matrix or data frame of the same size and class as <code>object</code>,
but with numerical variables replaced by scaled versions.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate z-scores (no rotation)
mvscale(oldfaithful, center = mean, scale = sd, cov = NULL, warning = FALSE)
# Non-robust scaling with rotation
mvscale(oldfaithful, center = mean, cov = stats::cov, warning = FALSE)
mvscale(oldfaithful, warning = FALSE)
# Robust Mahalanobis distances
oldfaithful |&gt;
  select(-time) |&gt;
  mvscale() |&gt;
  head(5) |&gt;
  dist()
</code></pre>

<hr>
<h2 id='n01'>Multivariate standard normal data</h2><span id='topic+n01'></span>

<h3>Description</h3>

<p>A synthetic data set containing 1000 observations on 10 variables generated
from independent standard normal distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n01
</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 rows and 10 columns.
</p>


<h3>Value</h3>

<p>Data frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n01
</code></pre>

<hr>
<h2 id='oldfaithful'>Old faithful eruption data</h2><span id='topic+oldfaithful'></span>

<h3>Description</h3>

<p>A data set containing data on recorded eruptions of the Old Faithful Geyser
in Yellowstone National Park, Wyoming, USA, from
1 January 2015 to 1 October 2021.
Recordings are incomplete, especially during the winter months when observers
may not be present.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oldfaithful
</code></pre>


<h3>Format</h3>

<p>A data frame with 2261 rows and 3 columns:
</p>

<dl>
<dt>time</dt><dd><p>Time eruption started</p>
</dd>
<dt>duration</dt><dd><p>Duration of eruption in seconds</p>
</dd>
<dt>waiting</dt><dd><p>Time to the following eruption</p>
</dd>
</dl>



<h3>Value</h3>

<p>Data frame
</p>


<h3>Source</h3>

<p><a href="https://geysertimes.org">https://geysertimes.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldfaithful |&gt;
 filter(duration &lt; 7000, waiting &lt; 7000) |&gt;
 ggplot(aes(x = duration, y = waiting)) +
 geom_point()
</code></pre>

<hr>
<h2 id='peirce_anomalies'>Anomalies according to Peirce's and Chauvenet's criteria</h2><span id='topic+peirce_anomalies'></span><span id='topic+chauvenet_anomalies'></span>

<h3>Description</h3>

<p>Peirce's criterion and Chauvenet's criterion were both proposed in the 1800s
as a way of determining what observations should be rejected in a univariate sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peirce_anomalies(y)

chauvenet_anomalies(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peirce_anomalies_+3A_y">y</code></td>
<td>
<p>numerical vector of observations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions take a univariate sample <code>y</code> and return a logical
vector indicating which observations should be considered anomalies according
to either Peirce's criterion or Chauvenet's criterion.
</p>


<h3>Value</h3>

<p>A logical vector
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Peirce, B. (1852). Criterion for the rejection of doubtful observations.
<em>The Astronomical Journal</em>, 2(21), 161–163.
</p>
<p>Chauvenet, W. (1863). 'Method of least squares'. Appendix to
<em>Manual of Spherical and Practical Astronomy</em>, Vol.2, Lippincott, Philadelphia, pp.469-566.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(1000)
tibble(y = y) |&gt; filter(peirce_anomalies(y))
tibble(y = y) |&gt; filter(chauvenet_anomalies(y))
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+autoplot'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
</dl>

<hr>
<h2 id='stray_anomalies'>Stray anomalies</h2><span id='topic+stray_anomalies'></span>

<h3>Description</h3>

<p>Test if observations are anomalies according to the stray algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stray_anomalies(y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stray_anomalies_+3A_y">y</code></td>
<td>
<p>A vector, matrix, or data frame consisting of numerical variables.</p>
</td></tr>
<tr><td><code id="stray_anomalies_+3A_...">...</code></td>
<td>
<p>Other arguments are passed to <code><a href="stray.html#topic+find_HDoutliers">find_HDoutliers</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector containing logical values indicating if the
observation is identified as an anomaly using the stray algorithm.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate data
y &lt;- c(6, rnorm(49))
stray_anomalies(y)
# Bivariate data
y &lt;- cbind(rnorm(50), c(5, rnorm(49)))
stray_anomalies(y)
</code></pre>

<hr>
<h2 id='stray_scores'>Stray scores</h2><span id='topic+stray_scores'></span>

<h3>Description</h3>

<p>Compute stray scores indicating how anomalous each observation is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stray_scores(y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stray_scores_+3A_y">y</code></td>
<td>
<p>A vector, matrix, or data frame consisting of numerical variables.</p>
</td></tr>
<tr><td><code id="stray_scores_+3A_...">...</code></td>
<td>
<p>Other arguments are passed to <code><a href="stray.html#topic+find_HDoutliers">find_HDoutliers</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector containing stray scores.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate data
y &lt;- c(6, rnorm(49))
scores &lt;- stray_scores(y)
threshold &lt;- stray::find_threshold(scores, alpha = 0.01, outtail = "max", p = 0.5, tn = 50)
which(scores &gt; threshold)
</code></pre>

<hr>
<h2 id='weird_conflicts'>Conflicts between weird packages and other packages</h2><span id='topic+weird_conflicts'></span>

<h3>Description</h3>

<p>This function lists all the conflicts between packages in the weird collection
and other packages that you have loaded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weird_conflicts()
</code></pre>


<h3>Details</h3>

<p>Some conflicts are deliberately ignored: <code>intersect</code>, <code>union</code>,
<code>setequal</code>, and <code>setdiff</code> from dplyr; and <code>intersect</code>,
<code>union</code>, <code>setdiff</code>, and <code>as.difftime</code> from lubridate.
These functions make the base equivalents generic, so shouldn't negatively affect any
existing code.
</p>


<h3>Value</h3>

<p>A list object of class <code>weird_conflicts</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>weird_conflicts()
</code></pre>

<hr>
<h2 id='weird_packages'>List all packages loaded by weird</h2><span id='topic+weird_packages'></span>

<h3>Description</h3>

<p>List all packages loaded by weird
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weird_packages(include_self = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weird_packages_+3A_include_self">include_self</code></td>
<td>
<p>Include weird in the list?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of package names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>weird_packages()
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
