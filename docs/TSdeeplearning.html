<!DOCTYPE html><html><head><title>Help for package TSdeeplearning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TSdeeplearning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Data_Maize'>
<p>Monthly International Maize Price Data</p></a></li>
<li><a href='#GRU_ts'>
<p>Gated Recurrent Unit Model</p></a></li>
<li><a href='#LST_ts'>
<p>Long- Short Term Memory Model</p></a></li>
<li><a href='#RNN_ts'>
<p>Recurrent neural network Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Learning Model for Time Series Forecasting</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ronit Jaiswal &lt;ronitjaiswal2912@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>RNNs are preferred for sequential data like time series, speech, text, etc. but when dealing with long range dependencies, vanishing gradient problems account for their poor performance. LSTM and GRU are effective solutions which are nothing but RNN networks with the abilities of learning both short-term and long-term dependencies. Their structural makeup enables them to remember information for a long period without any difficulty. LSTM consists of one cell state and three gates, namely, forget gate, input gate and output gate whereas GRU comprises only two gates, namely, reset gate and update gate. This package consists of three different functions for the application of RNN, LSTM and GRU to any time series data for its forecasting. For method details see Jaiswal, R. et al. (2022). &lt;<a href="https://doi.org/10.1007%2Fs00521-021-06621-3">doi:10.1007/s00521-021-06621-3</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>keras, tensorflow, reticulate, tsutils, BiocGenerics, utils,
graphics, magrittr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-08 11:50:23 UTC; kapil</td>
</tr>
<tr>
<td>Author:</td>
<td>Ronit Jaiswal [aut, cre],
  Girish Kumar Jha [aut, ths, ctb],
  Rajeev Ranjan Kumar [aut, ctb],
  Kapil Choudhary [aut, ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-09 07:33:00 UTC</td>
</tr>
</table>
<hr>
<h2 id='Data_Maize'>
Monthly International Maize Price Data
</h2><span id='topic+Data_Maize'></span>

<h3>Description</h3>

<p>Monthly international Maize price (Dollor per million ton) from January 2010 to June 2020.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Data_Maize")</code></pre>


<h3>Format</h3>

<p>A time series data with 126 observations.
</p>

<dl>
<dt><code>price</code></dt><dd><p>a time series</p>
</dd>
</dl>



<h3>Details</h3>

<p>Dataset contains 126 observations of monthly international Maize price (Dollor per million ton). It is obtained from World Bank &quot;Pink sheet&quot;.
</p>


<h3>Source</h3>

<p>https://www.worldbank.org/en/research/commodity-markets
</p>


<h3>References</h3>

<p>https://www.worldbank.org/en/research/commodity-markets
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Data_Maize)
</code></pre>

<hr>
<h2 id='GRU_ts'>
Gated Recurrent Unit Model
</h2><span id='topic+GRU_ts'></span>

<h3>Description</h3>

<p>The GRU function computes forecasted value with different forecasting evaluation criteria for gated recurrent unit  model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GRU_ts(xt, xtlag = 4, uGRU = 2, Drate = 0, nEpochs = 10,
Loss = "mse", AccMetrics = "mae",ActFn = "tanh",
Split = 0.8, Valid = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GRU_ts_+3A_xt">xt</code></td>
<td>

<p>Input univariate time series (ts) data.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_xtlag">xtlag</code></td>
<td>

<p>Lag of time series data.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_ugru">uGRU</code></td>
<td>

<p>Number of unit in GRU layer.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_drate">Drate</code></td>
<td>

<p>Dropout rate.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_nepochs">nEpochs</code></td>
<td>

<p>Number of epochs.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_loss">Loss</code></td>
<td>

<p>Loss function.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_accmetrics">AccMetrics</code></td>
<td>

<p>Metrics.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_actfn">ActFn</code></td>
<td>

<p>Activation function.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_split">Split</code></td>
<td>

<p>Index of the split point and separates the data into the training and testing datasets.
</p>
</td></tr>
<tr><td><code id="GRU_ts_+3A_valid">Valid</code></td>
<td>

<p>Validation set.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gated recurrent unit (GRU) was introduced by Cho et al.(2014). A GRU is part of a specific model of recurrent neural network that intends to use connections through a sequence of nodes to perform machine learning tasks associated with memory and clustering. Its internal structure is simpler and, therefore, it is also easier to train, as less calculation is required to upgrade the internal states. The update port controls the extent to which the state information from the previous moment is retained in the current state, while the reset port determines whether the current state should be combined with the previous information. Gated recurrent units help to adjust neural network input weights to solve the vanishing gradient problem that is a common issue with recurrent neural networks.
</p>


<h3>Value</h3>

<table>
<tr><td><code>TrainFittedValue</code></td>
<td>
<p>Training Fitted value for given time series data.</p>
</td></tr>
<tr><td><code>TestPredictedValue</code></td>
<td>
<p>Final forecasted value of the GRU model.</p>
</td></tr>
<tr><td><code>fcast_criteria</code></td>
<td>
<p>Different Forecasting evaluation criteria for GRU model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cho, K., Van Merriënboer, B., Bahdanau, D. and Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
</p>


<h3>See Also</h3>

<p>LSTM, RNN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Data_Maize")
GRU_ts(Data_Maize)

</code></pre>

<hr>
<h2 id='LST_ts'>
Long- Short Term Memory Model
</h2><span id='topic+LSTM_ts'></span>

<h3>Description</h3>

<p>The LSTM function computes forecasted value with different forecasting evaluation criteria for long- short term memory  model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LSTM_ts(xt, xtlag = 4, uLSTM = 2, Drate = 0, nEpochs = 10,
Loss = "mse", AccMetrics = "mae",ActFn = "tanh",
Split = 0.8, Valid = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LST_ts_+3A_xt">xt</code></td>
<td>

<p>Input univariate time series (ts) data.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_xtlag">xtlag</code></td>
<td>

<p>Lag of time series data.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_ulstm">uLSTM</code></td>
<td>

<p>Number of unit in LSTM layer.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_drate">Drate</code></td>
<td>

<p>Dropout rate.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_nepochs">nEpochs</code></td>
<td>

<p>Number of epochs.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_loss">Loss</code></td>
<td>

<p>Loss function.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_accmetrics">AccMetrics</code></td>
<td>

<p>Metrics.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_actfn">ActFn</code></td>
<td>

<p>Activation function.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_split">Split</code></td>
<td>

<p>Index of the split point and separates the data into the training and testing datasets.
</p>
</td></tr>
<tr><td><code id="LST_ts_+3A_valid">Valid</code></td>
<td>

<p>Validation set.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) based RNN is designed to overcome the vanishing gradients problem while dealing with long term dependencies. In contrast to standard RNN, LSTM has this peculiar and unique inbuilt ability by maintaining a memory cell to determine which unimportant features should be forgotten and which important features should be remembered during the learning process (Jaiswal et al., 2022). An LSTM model analyses and captures both short-term and long-term temporal dependencies of a complex time series effectively due to its architecture of recurrent neural network and the memory function used in the hidden nodes.
</p>


<h3>Value</h3>

<table>
<tr><td><code>TrainFittedValue</code></td>
<td>
<p>Training Fitted value for given time series data.</p>
</td></tr>
<tr><td><code>TestPredictedValue</code></td>
<td>
<p>Final forecasted value of the LSTM model.</p>
</td></tr>
<tr><td><code>fcast_criteria</code></td>
<td>
<p>Different Forecasting evaluation criteria for LSTM model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cho, K., Van Merriënboer, B., Bahdanau, D. and Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
</p>


<h3>See Also</h3>

<p>GRU, RNN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Data_Maize")
LSTM_ts(Data_Maize)

</code></pre>

<hr>
<h2 id='RNN_ts'>
Recurrent neural network Model
</h2><span id='topic+RNN_ts'></span>

<h3>Description</h3>

<p>The RNN function computes forecasted value with different forecasting evaluation criteria for recurrent neural network model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RNN_ts(xt, xtlag = 4, uRNN = 2, Drate = 0, nEpochs = 10,
Loss = "mse", AccMetrics = "mae",ActFn = "tanh",
Split = 0.8, Valid = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RNN_ts_+3A_xt">xt</code></td>
<td>

<p>Input univariate time series (ts) data.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_xtlag">xtlag</code></td>
<td>

<p>Lag of time series data.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_urnn">uRNN</code></td>
<td>

<p>Number of unit in RNN layer.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_drate">Drate</code></td>
<td>

<p>Dropout rate.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_nepochs">nEpochs</code></td>
<td>

<p>Number of epochs.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_loss">Loss</code></td>
<td>

<p>Loss function.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_accmetrics">AccMetrics</code></td>
<td>

<p>Metrics.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_actfn">ActFn</code></td>
<td>

<p>Activation function.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_split">Split</code></td>
<td>

<p>Index of the split point and separates the data into the training and testing datasets.
</p>
</td></tr>
<tr><td><code id="RNN_ts_+3A_valid">Valid</code></td>
<td>

<p>Validation set.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recurrent neural networks (RNNs) (Rumelhart 1986) add the explicit handling of order between observations when learning a mapping function from inputs to outputs. RNNs actually process single elements of any input sequence at a particular time, and maintain a ‘state vector’ in their hidden units. Nevertheless, when the interval of data dependencies increases, the standard RNNs tend to suffer increasingly heavily from the problem of either vanishing gradient or exploding gradient (Bengio et al. 1994; Lin et al. 1996).
</p>


<h3>Value</h3>

<table>
<tr><td><code>TrainFittedValue</code></td>
<td>
<p>Training Fitted value for given time series data.</p>
</td></tr>
<tr><td><code>TestPredictedValue</code></td>
<td>
<p>Final forecasted value of the RNN model.</p>
</td></tr>
<tr><td><code>fcast_criteria</code></td>
<td>
<p>Different Forecasting evaluation criteria for RNN model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bengio et al. 1994; Lin Sagheer A, Kotb M (2019) Time series forecasting of petroleum production using deep LSTM recurrent networks. Neurocomputing 323: 203&ndash;213.
</p>
<p>Rumelhart DE (1986) Learning internal representations by error propagation. In: Parallel distributed processing: Explorations in the microstructure of cognition. pp 318&ndash;362.
</p>
<p>Jha, G. K. and Sinha, K. (2014). Time-delay neural networks for time series prediction: An application to the monthly wholesale price of oilseeds in India. Neural Computing and Applications, 24(3&ndash;4), 563&ndash;571.
Jaiswal, R., Jha, G. K., Kumar, R. R. and Choudhary, K. (2022). Deep long short-term memory based model for agricultural price forecasting. Neural Computing and Applications, 34(6), 4661&ndash;4676.
</p>


<h3>See Also</h3>

<p>LSTM, GRU
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Data_Maize")
RNN_ts(Data_Maize)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
