<!DOCTYPE html><html><head><title>Help for package FDboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FDboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FDboost-package'><p>FDboost: Boosting Functional Regression Models</p></a></li>
<li><a href='#[.hmatrix'><p>Extract or replace parts of a hmatrix-object</p></a></li>
<li><a href='#%Xc%'><p>Constrained row tensor product</p></a></li>
<li><a href='#anisotropic_Kronecker'><p>Kronecker product or row tensor product of two base-learners with anisotropic penalty</p></a></li>
<li><a href='#applyFolds'><p>Cross-Validation and Bootstrapping over Curves</p></a></li>
<li><a href='#bbsc'><p>Constrained Base-learners for Scalar Covariates</p></a></li>
<li><a href='#bhistx'><p>Base-learners for Functional Covariates</p></a></li>
<li><a href='#birthDistribution'><p>Densities of live births in Germany</p></a></li>
<li><a href='#bootstrapCI'><p>Function to compute bootstrap confidence intervals</p></a></li>
<li><a href='#bsignal'><p>Base-learners for Functional Covariates</p></a></li>
<li><a href='#clr'><p>Clr and inverse clr transformation</p></a></li>
<li><a href='#coef.FDboost'><p>Coefficients of boosted functional regression model</p></a></li>
<li><a href='#cvrisk.FDboostLSS'><p>Cross-validation for FDboostLSS</p></a></li>
<li><a href='#emotion'><p>EEG and EMG recordings in a computerised gambling study</p></a></li>
<li><a href='#extract.blg'><p>Extract information of a base-learner</p></a></li>
<li><a href='#factorize'><p>Factorize tensor product model</p></a></li>
<li><a href='#FDboost'><p>Model-based Gradient Boosting for Functional Response</p></a></li>
<li><a href='#FDboost_fac-class'><p>'FDboost_fac' S3 class for factorized FDboost model components</p></a></li>
<li><a href='#FDboostLSS'><p>Model-based Gradient Boosting for Functional GAMLSS</p></a></li>
<li><a href='#fitted.FDboost'><p>Fitted values of a boosted functional regression model</p></a></li>
<li><a href='#fuelSubset'><p>Spectral data of fossil fuels</p></a></li>
<li><a href='#funMRD'><p>Functional MRD</p></a></li>
<li><a href='#funMSE'><p>Functional MSE</p></a></li>
<li><a href='#funplot'><p>Plot functional data with linear interpolation of missing values</p></a></li>
<li><a href='#funRsquared'><p>Functional R-squared</p></a></li>
<li><a href='#getTime'><p>Generic functions to asses attributes of functional data objects</p></a></li>
<li><a href='#getTime.hmatrix'><p>Extract attributes of hmatrix</p></a></li>
<li><a href='#hmatrix'><p>A S3 class for univariate functional data on a common grid</p></a></li>
<li><a href='#integrationWeights'><p>Functions to compute integration weights</p></a></li>
<li><a href='#is.hmatrix'><p>Test to class of hmatrix</p></a></li>
<li><a href='#mstop.validateFDboost'><p>Methods for objects of class validateFDboost</p></a></li>
<li><a href='#o_control'><p>Function to control estimation of smooth offset</p></a></li>
<li><a href='#plot.bootstrapCI'><p>Methods for objects of class bootstrapCI</p></a></li>
<li><a href='#plot.FDboost'><p>Plot the fit or the coefficients of a boosted functional regression model</p></a></li>
<li><a href='#predict.FDboost'><p>Prediction for boosted functional regression model</p></a></li>
<li><a href='#predict.FDboost_fac'><p>Prediction and plotting for factorized FDboost model components</p></a></li>
<li><a href='#residuals.FDboost'><p>Residual values of a boosted functional regression model</p></a></li>
<li><a href='#reweightData'><p>Function to Reweight Data</p></a></li>
<li><a href='#stabsel.FDboost'><p>Stability Selection</p></a></li>
<li><a href='#subset_hmatrix'><p>Subsets hmatrix according to an index</p></a></li>
<li><a href='#summary.FDboost'><p>Print and summary of a boosted functional regression model</p></a></li>
<li><a href='#truncateTime'><p>Function to truncate time in functional data</p></a></li>
<li><a href='#update.FDboost'><p>Function to update FDboost objects</p></a></li>
<li><a href='#validateFDboost'><p>Cross-Validation and Bootstrapping over Curves</p></a></li>
<li><a href='#viscosity'><p> Viscosity of resin over time</p></a></li>
<li><a href='#wide2long'><p>Transform id and time of wide format into long format</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Boosting Functional Regression Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-12</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Ruegamer &lt;david.ruegamer@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Regression models for functional data, i.e., scalar-on-function,
    function-on-scalar and function-on-function regression models, are fitted
    by a component-wise gradient boosting algorithm. 
	For a manual on how to use 'FDboost', see Brockhaus, Ruegamer, Greven (2017) &lt;<a href="https://doi.org/10.18637%2Fjss.v094.i10">doi:10.18637/jss.v094.i10</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), mboost (&ge; 2.9-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, graphics, grDevices, utils, stats, Matrix,
gamboostLSS (&ge; 2.0-0), stabs, mgcv, MASS, zoo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>fda, fields, ggplot2, maps, mapdata, knitr, refund, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-12 14:51:20 UTC; david</td>
</tr>
<tr>
<td>Collate:</td>
<td>'aaa.R' 'FDboost-package.R' 'FDboost.R' 'baselearners.R'
'baselearnersX.R' 'bootstrapCIs.R' 'clr_functions.R'
'constrainedX.R' 'crossvalidation.R' 'factorize.R'
'FDboostLSS.R' 'hmatrix.R' 'methods.R' 'stabsel.R'
'utilityFunctions.R'</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/boost-R/FDboost/issues">https://github.com/boost-R/FDboost/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/boost-R/FDboost">https://github.com/boost-R/FDboost</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Sarah Brockhaus [aut],
  David Ruegamer [aut, cre],
  Almond Stoecker [aut],
  Torsten Hothorn [ctb],
  with contributions by many others (see inst/CONTRIBUTIONS) [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-12 15:20:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='FDboost-package'>FDboost: Boosting Functional Regression Models</h2><span id='topic+FDboost-package'></span><span id='topic+_PACKAGE'></span><span id='topic+FDboost_package'></span><span id='topic+package-FDboost'></span>

<h3>Description</h3>

<p>Regression models for functional data, i.e., scalar-on-function, 
function-on-scalar and function-on-function regression models, are fitted 
by a component-wise gradient boosting algorithm.
</p>


<h3>Details</h3>

<p>This package is intended to fit regression models with functional variables. 
It is possible to fit models with functional response and/or functional covariates, 
resulting in scalar-on-function, function-on-scalar and function-on-function regression. 
Furthermore, the package can be used to fit density-on-scalar regression models.
Details on the functional regression models that can be fitted with <span class="pkg">FDboost</span>
can be found in Brockhaus et al. (2015, 2017, 2018) and Ruegamer et al. (2018).  
A hands-on tutorial for the package can be found 
in Brockhaus, Ruegamer and Greven (2020), see &lt;doi:10.18637/jss.v094.i10&gt;.
For density-on-scalar regression models see Maier et al. (2021).
</p>
<p>Using component-wise gradient boosting as fitting procedure, <span class="pkg">FDboost</span> relies on 
the R package <span class="pkg">mboost</span> (Hothorn et al., 2017). 
A comprehensive tutorial to <span class="pkg">mboost</span> is given in Hofner et al. (2014). 
</p>
<p>The main fitting function is <code><a href="#topic+FDboost">FDboost</a></code>. 
The model complexity is controlled by the number of boosting iterations (mstop).   
Like the fitting procedures in <span class="pkg">mboost</span>, the function <code>FDboost</code> DOES NOT 
select an appropriate stopping iteration. This must be chosen by the user.  
The user can determine an adequate stopping iteration by resampling methods like 
cross-validation or bootstrap. 
This can be done using the function <code><a href="#topic+applyFolds">applyFolds</a></code>. 
</p>
<p>Aside from common effect surface plots, tensor product factorization via the 
function <code><a href="#topic+factorize">factorize</a></code> presents an alternative tool for visualization 
of estimated effects for non-linear function-on-scalar models 
(Stoecker, Steyer and Greven (2022), <a href="https://arxiv.org/abs/2109.02624">https://arxiv.org/abs/2109.02624</a>). 
After factorization, effects are decomposed multiple scalar effects into 
functional main effect directions, which can be separately plotted allowing to 
visualize more complex effect structures.
</p>


<h3>Author(s)</h3>

<p>Sarah Brockhaus, David Ruegamer and Almond Stoecker
</p>


<h3>References</h3>

<p>Brockhaus, S., Ruegamer, D. and Greven, S. (2020):
Boosting Functional Regression Models with FDboost. 
Journal of Statistical Software, 94(10), 1–50.
&lt;doi:10.18637/jss.v094.i10&gt;
</p>
<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300. 
</p>
<p>Brockhaus, S., Melcher, M., Leisch, F. and Greven, S. (2017): 
Boosting flexible functional regression models with a high number of functional historical effects,  
Statistics and Computing, 27(4), 913-926.   
</p>
<p>Brockhaus, S., Fuest, A., Mayr, A. and Greven, S. (2018): 
Signal regression models for location, scale and shape with an application to stock returns. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 665-686.
</p>
<p>Hothorn T., Buehlmann P., Kneib T., Schmid M., and Hofner B. (2017). mboost: Model-Based Boosting, 
R package version 2.8-1, <a href="https://cran.r-project.org/package=mboost">https://cran.r-project.org/package=mboost</a>
</p>
<p>Hofner, B., Mayr, A., Robinzonov, N., Schmid, M. (2014). Model-based Boosting in R: 
A Hands-on Tutorial Using the R Package mboost. Computational Statistics, 29, 3-35. 
<a href="https://cran.r-project.org/package=mboost/vignettes/mboost_tutorial.pdf">https://cran.r-project.org/package=mboost/vignettes/mboost_tutorial.pdf</a>
</p>
<p>Maier, E.-M., Stoecker, A., Fitzenberger, B., Greven, S. (2021):
Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics.
arXiv preprint arXiv:2110.11771.
</p>
<p>Ruegamer D., Brockhaus, S., Gentsch K., Scherer, K., Greven, S. (2018). 
Boosting factor-specific functional historical models for the detection of synchronization in bioelectrical signals. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 621-642.
</p>
<p>Stoecker A., Steyer L., Greven S. (2022):
Functional Additive Models on Manifolds of Planar Shapes and Forms.
arXiv preprint arXiv:2109.02624.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the main fitting function and 
<code><a href="#topic+applyFolds">applyFolds</a></code> for model tuning via resampling methods.
</p>

<hr>
<h2 id='+5B.hmatrix'>Extract or replace parts of a hmatrix-object</h2><span id='topic++5B.hmatrix'></span>

<h3>Description</h3>

<p>Operator acting on hmatrix preserving the attributes when rows are extracted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmatrix'
x[i, j, ..., drop = FALSE]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B.hmatrix_+3A_x">x</code></td>
<td>
<p>object from which to extract element(s) or in which to replace element(s).</p>
</td></tr>
<tr><td><code id="+2B5B.hmatrix_+3A_i">i</code>, <code id="+2B5B.hmatrix_+3A_j">j</code></td>
<td>
<p>indices specifying elements to extract or replace. Indices are numeric 
vectors or empty (missing) or NULL. Numeric values are coerced to integer as by as.integer 
(and hence truncated towards zero).</p>
</td></tr>
<tr><td><code id="+2B5B.hmatrix_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="+2B5B.hmatrix_+3A_drop">drop</code></td>
<td>
<p>If <code>TRUE</code> the result is coerced to the lowest possible dimension 
(or just a matrix). This only works for extracting elements, not for the 
replacement, defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If used on columns or rows/columns a matrix is returned. 
If used on rows only, i.e. x[i,] an object of class hmatrix is returned. 
The id is changed so that it runs from 1, ..., nNew, where nNew is the number of different 
id values in the new hmatrix-object. 
From the functional covariate <code>x</code> rows are selected accordingly.
</p>


<h3>Value</h3>

<p>a <code>"hmatrix"</code> object
</p>


<h3>See Also</h3>

<p>?&quot;[&quot;
</p>

<hr>
<h2 id='+25Xc+25'>Constrained row tensor product</h2><span id='topic++25Xc+25'></span>

<h3>Description</h3>

<p>Combining single base-learners to form new, more complex base-learners, with
an identifiability constraint to center the interaction around the intercept and
around the two main effects. Suitable for functional response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bl1 %Xc% bl2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25Xc+2B25_+3A_bl1">bl1</code></td>
<td>
<p>base-learner 1, e.g. <code>bols(x1)</code></p>
</td></tr>
<tr><td><code id="+2B25Xc+2B25_+3A_bl2">bl2</code></td>
<td>
<p>base-learner 2, e.g. <code>bols(x2)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to <code>%X%</code> in package <code>mboost</code>, see 
<code><a href="mboost.html#topic+baselearners">%X%</a></code>, 
a row tensor product of linear base-learners is returned by <code>%Xc%</code>. 
<code>%Xc%</code> applies a sum-to-zero constraint to the design matrix suitable for
functional response if an interaction of two scalar covariates is specified 
in the case that the model contains a global intercept and both main effects, 
as the interaction is centered around the intercept and centered around the two main effects. 
See Web Appendix A of Brockhaus et al. (2015) for details on how to enforce the constraint 
for the functional intercept.   
Use, e.g., in a model call to <code>FDboost</code>, following the scheme, 
<code>y ~ 1 + bolsc(x1) + bolsc(x2) + bols(x1) %Xc% bols(x2)</code>, 
where <code>1</code> induces a global intercept and <code>x1</code>, <code>x2</code> are factor variables, 
see Ruegamer et al. (2018).
</p>


<h3>Value</h3>

<p>An object of class <code>blg</code> (base-learner generator) with a <code>dpp</code> function 
as for other <code><a href="mboost.html#topic+baselearners">baselearners</a></code>.
</p>


<h3>Author(s)</h3>

<p>Sarah Brockhaus, David Ruegamer
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300.
</p>
<p>Ruegamer D., Brockhaus, S., Gentsch K., Scherer, K., Greven, S. (2018). 
Boosting factor-specific functional historical models for the detection of synchronization in bioelectrical signals. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 621-642.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
######## Example for function-on-scalar-regression with interaction effect of two scalar covariates 
data("viscosity", package = "FDboost") 
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## fit model with interaction that is centered around the intercept 
## and the two main effects 
mod1 &lt;- FDboost(vis ~ 1 + bolsc(T_C, df=1) + bolsc(T_A, df=1) + 
                bols(T_C, df=1) %Xc% bols(T_A, df=1),
                timeformula = ~bbs(time, df=6),
                numInt = "equal", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = viscosity, control=boost_control(mstop = 100, nu = 0.4))
                
## check centering around intercept
colMeans(predict(mod1, which = 4))

## check centering around main effects
colMeans(predict(mod1, which = 4)[viscosity$T_A == "low", ])
colMeans(predict(mod1, which = 4)[viscosity$T_A == "high", ])
colMeans(predict(mod1, which = 4)[viscosity$T_C == "low", ])
colMeans(predict(mod1, which = 4)[viscosity$T_C == "low", ])

## find optimal mstop using cvrsik() or validateFDboost()
## ... 

## look at interaction effect in one plot
# funplot(mod1$yind, predict(mod1, which=4))

</code></pre>

<hr>
<h2 id='anisotropic_Kronecker'>Kronecker product or row tensor product of two base-learners with anisotropic penalty</h2><span id='topic+anisotropic_Kronecker'></span><span id='topic++25A+25'></span><span id='topic++A0+'></span><span id='topic++25Xa0+25'></span>

<h3>Description</h3>

<p>Kronecker product or row tensor product of two base-learners allowing for anisotropic penalties. 
For the Kronecker product, <code>%A%</code> works in the general case, <code>%A0%</code> for the special case where 
the penalty is zero in one direction. 
For the row tensor product, <code>%Xa0%</code> works for the special case where 
the penalty is zero in one direction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bl1 %A% bl2

bl1 %A0% bl2

bl1 %Xa0% bl2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anisotropic_Kronecker_+3A_bl1">bl1</code></td>
<td>
<p>base-learner 1, e.g. <code>bbs(x1)</code></p>
</td></tr>
<tr><td><code id="anisotropic_Kronecker_+3A_bl2">bl2</code></td>
<td>
<p>base-learner 2, e.g. <code>bbs(x2)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>%O%</code> is called with a specification of <code>df</code> in both base-learners, 
e.g. <code>bbs(x1, df = df1) %O% bbs(t, df = df2)</code>, the global <code>df</code> for the 
Kroneckered base-learner is computed as <code>df = df1 * df2</code>. 
And thus the penalty has only one smoothness parameter lambda resulting in an isotropic penalty, 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(P1 o I) + (I o P2)],</code>
</p>
 
<p>with overall penalty <code class="reqn">P</code>, Kronecker product <code class="reqn">o</code>, 
marginal penalty matrices <code class="reqn">P1, P2</code> and identity matrices <code class="reqn">I</code>.  
(Currie et al. (2006) introduced the generalized linear array model, which has a design matrix that 
is composed of the Kronecker product of two marginal design matrices, which was implemented in mboost 
as <code>%O%</code>.  
See Brockhaus et al. (2015) for the application of array models to functional data.)  
</p>
<p>In contrast, a Kronecker product with anisotropic penalty is obtained by <code>%A%</code>, 
which allows for a different amount of smoothness in the two directions. 
For example <code>bbs(x1, df = df1) %A% bbs(t, df = df2)</code> results in computing two
different values for lambda for the two marginal design matrices and a global value of 
lambda to adjust for the global <code>df</code>, i.e. 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(lambda1 * P1 o I) +  (I o lambda2 * P2)],</code>
</p>
 
<p>with Kronecker product <code class="reqn">o</code>, 
where <code class="reqn">lambda1</code> is computed individually for <code class="reqn">df1</code> and <code class="reqn">P1</code>, 
<code class="reqn">lambda2</code> is computed individually for <code class="reqn">df2</code>  and <code class="reqn">P2</code>, 
and <code class="reqn">lambda</code> is computed such that the global <code class="reqn">df</code> hold <code class="reqn">df = df1 * df2</code>. 
For the computation of <code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> weights specified in the model 
call can only be used when the weights, are such that they are specified on the level 
of rows and columns of the response matrix Y, e.g. resampling weights on the level of 
rows of Y and integration weights on the columns of Y are possible. 
If this the weights cannot be separated to blg1 and blg2 all
weights are set to 1 for the computation of <code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> which implies that 
<code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> are equal over 
folds of <code>cvrisk</code>. The computation of the global <code class="reqn">lambda</code> considers the 
specified <code>weights</code>, such the global <code class="reqn">df</code> are correct.    
</p>
<p>The operator <code>%A0%</code> treats the important special case where <code class="reqn">lambda1 = 0</code> or 
<code class="reqn">lambda2 = 0</code>. In this case it suffices to compute the global lambda and computation gets
faster and arbitrary weights can be specified. Consider <code class="reqn">lambda1 = 0</code> then the penalty becomes 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(1 * P1 o I) +  (I o lambda2 * P2)] = lambda * lambda2 * (I o P2),</code>
</p>
  
<p>and only one global <code class="reqn">lambda</code> is computed which is then  <code class="reqn">lambda * lambda2</code>.  
</p>
<p>If the <code>formula</code> in <code>FDboost</code> contains base-learners connected by 
<code>%O%</code>, <code>%A%</code> or <code>%A0%</code>, 
those effects are not expanded with <code>timeformula</code>, allowing for model specifications 
with different effects in time-direction.  
</p>
<p><code>%Xa0%</code> computes like <code>%X%</code> the row tensor product of two base-learners, 
with the difference that it sets the penalty for one direction to zero. 
Thus, <code>%Xa0%</code> behaves to <code>%X%</code> analogously like <code>%A0%</code> to <code>%O%</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>blg</code> (base-learner generator) with a <code>dpp</code> function 
as for other <code><a href="mboost.html#topic+baselearners">baselearners</a></code>.
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300. 
</p>
<p>Currie, I.D., Durban, M. and Eilers P.H.C. (2006):  
Generalized linear array models with applications to multidimensional smoothing. 
Journal of the Royal Statistical Society, Series B-Statistical Methodology, 68(2), 259-280.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
######## Example for anisotropic penalty  
data("viscosity", package = "FDboost") 
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## isotropic penalty, as timeformula is kroneckered to each effect using %O% 
## only for the smooth intercept %A0% is used, as 1-direction should not be penalized 
mod1 &lt;- FDboost(vis ~ 1 + 
                bolsc(T_C, df = 1) + 
                bolsc(T_A, df = 1) + 
                bols(T_C, df = 1) %Xc% bols(T_A, df = 1),
                timeformula = ~ bbs(time, df = 3),
                numInt = "equal", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = viscosity, control=boost_control(mstop = 100, nu = 0.4))
## cf. the formula that is passed to mboost
mod1$formulaMboost

## anisotropic effects using %A0%, as lambda1 = 0 for all base-learners
## in this case using %A% gives the same model, but three lambdas are computed explicitly 
mod1a &lt;- FDboost(vis ~ 1 + 
                bolsc(T_C, df = 1) %A0% bbs(time, df = 3) + 
                bolsc(T_A, df = 1) %A0% bbs(time, df = 3) + 
                bols(T_C, df = 1) %Xc% bols(T_A, df = 1) %A0% bbs(time, df = 3),
                timeformula = ~ bbs(time, df = 3),
                numInt = "equal", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = viscosity, control=boost_control(mstop = 100, nu = 0.4)) 
## cf. the formula that is passed to mboost
mod1a$formulaMboost

## alternative model specification by using a 0-matrix as penalty 
## only works for bolsc() as in bols() one cannot specify K 
## -&gt; model without interaction term 
K0 &lt;- matrix(0, ncol = 2, nrow = 2)
mod1k0 &lt;- FDboost(vis ~ 1 + 
                 bolsc(T_C, df = 1, K = K0) +
                 bolsc(T_A, df = 1, K = K0), 
                 timeformula = ~ bbs(time, df = 3), 
                 numInt = "equal", family = QuantReg(), 
                 offset = NULL, offset_control = o_control(k_min = 9), 
                 data = viscosity, control=boost_control(mstop = 100, nu = 0.4))
## cf. the formula that is passed to mboost
mod1k0$formulaMboost
                
## optimize mstop for mod1, mod1a and mod1k0
## ...
                
## compare estimated coefficients

oldpar &lt;- par(mfrow=c(4, 2))
plot(mod1, which = 1)
plot(mod1a, which = 1)
plot(mod1, which = 2)
plot(mod1a, which = 2)
plot(mod1, which = 3)
plot(mod1a, which = 3)
funplot(mod1$yind, predict(mod1, which=4))
funplot(mod1$yind, predict(mod1a, which=4))
par(oldpar)


</code></pre>

<hr>
<h2 id='applyFolds'>Cross-Validation and Bootstrapping over Curves</h2><span id='topic+applyFolds'></span><span id='topic+cvMa'></span><span id='topic+cvLong'></span><span id='topic+cvrisk.FDboost'></span>

<h3>Description</h3>

<p>Cross-validation and bootstrapping over curves to compute the empirical risk for 
hyper-parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>applyFolds(
  object,
  folds = cv(rep(1, length(unique(object$id))), type = "bootstrap"),
  grid = 1:mstop(object),
  fun = NULL,
  riskFun = NULL,
  numInt = object$numInt,
  papply = mclapply,
  mc.preschedule = FALSE,
  showProgress = TRUE,
  compress = FALSE,
  ...
)

## S3 method for class 'FDboost'
cvrisk(
  object,
  folds = cvLong(id = object$id, weights = model.weights(object)),
  grid = 1:mstop(object),
  papply = mclapply,
  fun = NULL,
  mc.preschedule = FALSE,
  ...
)

cvLong(
  id,
  weights = rep(1, l = length(id)),
  type = c("bootstrap", "kfold", "subsampling", "curves"),
  B = ifelse(type == "kfold", 10, 25),
  prob = 0.5,
  strata = NULL
)

cvMa(
  ydim,
  weights = rep(1, l = ydim[1] * ydim[2]),
  type = c("bootstrap", "kfold", "subsampling", "curves"),
  B = ifelse(type == "kfold", 10, 25),
  prob = 0.5,
  strata = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="applyFolds_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_folds">folds</code></td>
<td>
<p>a weight matrix with number of rows equal to the number of observed trajectories.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_grid">grid</code></td>
<td>
<p>the grid over which the optimal number of boosting iterations (mstop) is searched.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_fun">fun</code></td>
<td>
<p>if <code>fun</code> is <code>NULL</code>, the out-of-bag risk is returned. 
<code>fun</code>, as a function of <code>object</code>, 
may extract any other characteristic of the cross-validated models. These are returned as is.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_riskfun">riskFun</code></td>
<td>
<p>only exists in <code>applyFolds</code>; allows to compute other risk functions than the risk 
of the family that was specified in object. 
Must be specified as function of arguments <code>(y, f, w = 1)</code>, where <code>y</code> is the 
observed response, <code>f</code> is the prediction from the model and <code>w</code> is the weight. 
The risk function must return a scalar numeric value for vector valued input.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_numint">numInt</code></td>
<td>
<p>only exists in <code>applyFolds</code>; the scheme for numerical integration, 
see <code>numInt</code> in <code><a href="#topic+FDboost">FDboost</a></code>.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_papply">papply</code></td>
<td>
<p>(parallel) apply function, defaults to <code><a href="parallel.html#topic+mclapply">mclapply</a></code> from 
R package <code>parallel</code>, see <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> for details.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_mc.preschedule">mc.preschedule</code></td>
<td>
<p>Defaults to <code>FALSE</code>. Preschedule tasks if they are parallelized using <code>mclapply</code>.
For details see <code><a href="parallel.html#topic+mclapply">mclapply</a></code>.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_showprogress">showProgress</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_compress">compress</code></td>
<td>
<p>logical, defaults to <code>FALSE</code>. Only used to force a meaningful
behaviour of <code>applyFolds</code> with hmatrix objects when using nested resampling.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_...">...</code></td>
<td>
<p>further arguments passed to the (parallel) apply function.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_id">id</code></td>
<td>
<p>the id-vector as integers 1, 2, ... specifying which observations belong to the same curve, 
deprecated in <code>cvMa()</code>.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_weights">weights</code></td>
<td>
<p>a numeric vector of (integration) weights, defaults to 1.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_type">type</code></td>
<td>
<p>character argument for specifying the cross-validation 
method. Currently (stratified) bootstrap, k-fold cross-validation, subsampling and 
leaving-one-curve-out cross validation (i.e. jack knife on curves) are implemented.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_b">B</code></td>
<td>
<p>number of folds, per default 25 for <code>bootstrap</code> and
<code>subsampling</code> and 10 for <code>kfold</code>.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_prob">prob</code></td>
<td>
<p>percentage of observations to be included in the learning samples 
for subsampling.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_strata">strata</code></td>
<td>
<p>a factor of the same length as <code>weights</code> for stratification.</p>
</td></tr>
<tr><td><code id="applyFolds_+3A_ydim">ydim</code></td>
<td>
<p>dimensions of response-matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of boosting iterations is an important hyper-parameter of boosting.   
It be chosen using the functions <code>applyFolds</code> or <code>cvrisk.FDboost</code>. Those functions 
compute honest, i.e., out-of-bag, estimates of the empirical risk for different 
numbers of boosting iterations. 
The weights (zero weights correspond to test cases) are defined via the folds matrix, 
see <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> in package mboost. 
</p>
<p>In case of functional response, we recommend to use <code>applyFolds</code>. 
It recomputes the model in each fold using <code>FDboost</code>. Thus, all parameters are recomputed, 
including the smooth offset (if present) and the identifiability constraints (if present, only 
relevant for <code>bolsc</code>, <code>brandomc</code> and <code>bbsc</code>).  
Note, that the function <code>applyFolds</code> expects folds that give weights
per curve without considering integration weights.  
</p>
<p>The function <code>cvrisk.FDboost</code> is a wrapper for <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> in package mboost. 
It overrides the default for the folds, so that the folds are sampled on the level of curves 
(not on the level of single observations, which does not make sense for functional response).  
Note that the smooth offset and the computation of the identifiability constraints
are not part of the refitting if <code>cvrisk</code> is used. 
Per default the integration weights of the model fit are used to compute the prediction errors 
(as the integration weights are part of the default folds). 
Note that in <code>cvrisk</code> the weights are rescaled to sum up to one. 
</p>
<p>The functions <code>cvMa</code> and <code>cvLong</code> can be used to build an appropriate 
weight matrix for functional response to be used with <code>cvrisk</code> as sampling 
is done on the level of curves. The probability for each 
curve to enter a fold is equal over all curves.     
The function <code>cvMa</code> takes the dimensions of the response matrix as input argument and thus
can only be used for regularly observed response. 
The function <code>cvLong</code> takes the id variable and the weights as arguments and thus can be used
for responses in long format that are potentially observed irregularly. 
</p>
<p>If <code>strata</code> is defined 
sampling is performed in each stratum separately thus preserving 
the distribution of the <code>strata</code> variable in each fold.
</p>


<h3>Value</h3>

<p><code>cvMa</code> and <code>cvLong</code> return a matrix of sampling weights to be used in <code>cvrisk</code>. 
</p>
<p>The functions <code>applyFolds</code> and <code>cvrisk.FDboost</code> return a <code>cvrisk</code>-object, 
which is a matrix of the computed out-of-bag risk. The matrix has the folds in rows and the 
number of boosting iteratins in columns. Furhtermore, the matrix has attributes including: 
</p>
<table>
<tr><td><code>risk</code></td>
<td>
<p>name of the applied risk function</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>model call of the model object</p>
</td></tr>
<tr><td><code>mstop</code></td>
<td>
<p>gird of stopping iterations that is used</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>name for the type of folds</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Use argument <code>mc.cores = 1L</code> to set the numbers of cores that is used in 
parallel computation. On Windows only 1 core is possible, <code>mc.cores = 1</code>, which is the default.
</p>


<h3>See Also</h3>

<p><code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> to perform cross-validation with scalar response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Ytest &lt;- matrix(rnorm(15), ncol = 3) # 5 trajectories, each with 3 observations 
Ylong &lt;- as.vector(Ytest)
## 4-folds for bootstrap for the response in long format without integration weights
cvMa(ydim = c(5,3), type = "bootstrap", B = 4)  
cvLong(id = rep(1:5, times = 3), type = "bootstrap", B = 4)

if(require(fda)){
 ## load the data
 data("CanadianWeather", package = "fda")
 
 ## use data on a daily basis 
 canada &lt;- with(CanadianWeather, 
                list(temp = t(dailyAv[ , , "Temperature.C"]),
                     l10precip = t(dailyAv[ , , "log10precip"]),
                     l10precip_mean = log(colMeans(dailyAv[ , , "Precipitation.mm"]), base = 10),
                     lat = coordinates[ , "N.latitude"],
                     lon = coordinates[ , "W.longitude"],
                     region = factor(region),
                     place = factor(place),
                     day = 1:365,  ## corresponds to t: evaluation points of the fun. response 
                     day_s = 1:365))  ## corresponds to s: evaluation points of the fun. covariate
 
## center temperature curves per day 
canada$tempRaw &lt;- canada$temp
canada$temp &lt;- scale(canada$temp, scale = FALSE) 
rownames(canada$temp) &lt;- NULL ## delete row-names 
  
## fit the model  
mod &lt;- FDboost(l10precip ~ 1 + bolsc(region, df = 4) + 
                 bsignal(temp, s = day_s, cyclic = TRUE, boundary.knots = c(0.5, 365.5)), 
               timeformula = ~ bbs(day, cyclic = TRUE, boundary.knots = c(0.5, 365.5)), 
               data = canada)
mod &lt;- mod[75]


  #### create folds for 3-fold bootstrap: one weight for each curve
  set.seed(123)
  folds_bs &lt;- cv(weights = rep(1, mod$ydim[1]), type = "bootstrap", B = 3)

  ## compute out-of-bag risk on the 3 folds for 1 to 75 boosting iterations  
  cvr &lt;- applyFolds(mod, folds = folds_bs, grid = 1:75)

  ## weights per observation point  
  folds_bs_long &lt;- folds_bs[rep(1:nrow(folds_bs), times = mod$ydim[2]), ]
  attr(folds_bs_long, "type") &lt;- "3-fold bootstrap"
  ## compute out-of-bag risk on the 3 folds for 1 to 75 boosting iterations  
  cvr3 &lt;- cvrisk(mod, folds = folds_bs_long, grid = 1:75)



  ## plot the out-of-bag risk
  oldpar &lt;- par(mfrow = c(1,3))
  plot(cvr); legend("topright", lty=2, paste(mstop(cvr)))
  plot(cvr3); legend("topright", lty=2, paste(mstop(cvr3)))
  par(oldpar)


}

</code></pre>

<hr>
<h2 id='bbsc'>Constrained Base-learners for Scalar Covariates</h2><span id='topic+bbsc'></span><span id='topic+brandomc'></span><span id='topic+bolsc'></span>

<h3>Description</h3>

<p>Constrained base-learners for fitting effects of scalar covariates in models 
with functional response
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bbsc(
  ...,
  by = NULL,
  index = NULL,
  knots = 10,
  boundary.knots = NULL,
  degree = 3,
  differences = 2,
  df = 4,
  lambda = NULL,
  center = FALSE,
  cyclic = FALSE
)

bolsc(
  ...,
  by = NULL,
  index = NULL,
  intercept = TRUE,
  df = NULL,
  lambda = 0,
  K = NULL,
  weights = NULL,
  contrasts.arg = "contr.treatment"
)

brandomc(..., contrasts.arg = "contr.dummy", df = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bbsc_+3A_...">...</code></td>
<td>
<p>one or more predictor variables or one matrix or data 
frame of predictor variables.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_by">by</code></td>
<td>
<p>an optional variable defining varying coefficients, 
either a factor or numeric variable.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_index">index</code></td>
<td>
<p>a vector of integers for expanding the variables in <code>...</code>.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_knots">knots</code></td>
<td>
<p>either the number of knots or a vector of the positions 
of the interior knots (for more details see <code><a href="mboost.html#topic+baselearners">bbs</a></code>).</p>
</td></tr>
<tr><td><code id="bbsc_+3A_boundary.knots">boundary.knots</code></td>
<td>
<p>boundary points at which to anchor the B-spline basis 
(default the range of the data). A vector (of length 2) 
for the lower and the upper boundary knot can be specified.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_degree">degree</code></td>
<td>
<p>degree of the regression spline.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_differences">differences</code></td>
<td>
<p>a non-negative integer, typically 1, 2 or 3. 
If <code>differences</code> = <em>k</em>, <em>k</em>-th-order differences are used as 
a penalty (<em>0</em>-th order differences specify a ridge penalty).</p>
</td></tr>
<tr><td><code id="bbsc_+3A_df">df</code></td>
<td>
<p>trace of the hat matrix for the base-learner defining the 
base-learner complexity. Low values of <code>df</code> correspond to a 
large amount of smoothing and thus to &quot;weaker&quot; base-learners.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_lambda">lambda</code></td>
<td>
<p>smoothing parameter of the penalty, computed from <code>df</code> when 
<code>df</code> is specified.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_center">center</code></td>
<td>
<p>See <code><a href="mboost.html#topic+baselearners">bbs</a></code>.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_cyclic">cyclic</code></td>
<td>
<p>if <code>cyclic = TRUE</code> the fitted values coincide at 
the boundaries (useful for cyclic covariates such as day time etc.).</p>
</td></tr>
<tr><td><code id="bbsc_+3A_intercept">intercept</code></td>
<td>
<p>if <code>intercept = TRUE</code> an intercept is added to the design matrix 
of a linear base-learner.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_k">K</code></td>
<td>
<p>in <code>bolsc</code> it is possible to specify the penalty matrix K</p>
</td></tr>
<tr><td><code id="bbsc_+3A_weights">weights</code></td>
<td>
<p>experiemtnal! weights that are used for the computation of the transformation matrix Z.</p>
</td></tr>
<tr><td><code id="bbsc_+3A_contrasts.arg">contrasts.arg</code></td>
<td>
<p>Note that a special <code>contrasts.arg</code> exists in 
package <code>mboost</code>, namely &quot;contr.dummy&quot;. This contrast is used per default 
in <code>brandomc</code>. It leads to a 
dummy coding as returned by <code>model.matrix(~ x - 1)</code> were the 
intercept is implicitly included but each factor level gets a 
separate effect estimate (for more details see <code><a href="mboost.html#topic+baselearners">brandom</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The base-learners <code>bbsc</code>, <code>bolsc</code> and <code>brandomc</code> are 
the base-learners <code><a href="mboost.html#topic+baselearners">bbs</a></code>, <code><a href="mboost.html#topic+baselearners">bols</a></code> and 
<code><a href="mboost.html#topic+baselearners">brandom</a></code> with additional identifiability constraints. 
The constraints enforce that 
<code class="reqn">\sum_{i} \hat h(x_i, t) = 0</code> for all <code class="reqn">t</code>, so that 
effects varying over <code class="reqn">t</code> can be interpreted as deviations 
from the global functional intercept, see Web Appendix A of 
Scheipl et al. (2015). 
The constraint is enforced by a basis transformation of the design and penalty matrix. 
In particular, it is sufficient to apply the constraint on the covariate-part of the design 
and penalty matrix and thus, it is not necessary to change the basis in $t$-direction.  
See Appendix A of Brockhaus et al. (2015) for technical details on how to enforce this sum-to-zero constraint.   
</p>
<p>Cannot deal with any missing values in the covariates.
</p>


<h3>Value</h3>

<p>Equally to the base-learners of package <code>mboost</code>: 
</p>
<p>An object of class <code>blg</code> (base-learner generator) with a 
<code>dpp</code> function (data pre-processing) and other functions. 
</p>
<p>The call to <code>dpp</code> returns an object of class 
<code>bl</code> (base-learner) with a <code>fit</code> function. The call to 
<code>fit</code> finally returns an object of class <code>bm</code> (base-model).
</p>


<h3>Author(s)</h3>

<p>Sarah Brockhaus, Almond Stoecker
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300.
</p>
<p>Scheipl, F., Staicu, A.-M. and Greven, S. (2015):  
Functional Additive Mixed Models, Journal of Computational and Graphical Statistics, 24(2), 477-501.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit. 
<code><a href="mboost.html#topic+baselearners">bbs</a></code>, <code><a href="mboost.html#topic+baselearners">bols</a></code> 
and <code><a href="mboost.html#topic+baselearners">brandom</a></code> for the 
corresponding base-learners in <code>mboost</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### simulate data with functional response and scalar covariate (functional ANOVA)
n &lt;- 60   ## number of cases
Gy &lt;- 27  ## number of observation poionts per response curve 
dat &lt;- list()
dat$t &lt;- (1:Gy-1)^2/(Gy-1)^2
set.seed(123)
dat$z1 &lt;- rep(c(-1, 1), length = n)
dat$z1_fac &lt;- factor(dat$z1, levels = c(-1, 1), labels = c("1", "2"))
# dat$z1 &lt;- runif(n)
# dat$z1 &lt;- dat$z1 - mean(dat$z1)

# mean and standard deviation for the functional response 
mut &lt;- matrix(2*sin(pi*dat$t), ncol = Gy, nrow = n, byrow = TRUE) + 
        outer(dat$z1, dat$t, function(z1, t) z1*cos(pi*t) ) # true linear predictor
sigma &lt;- 0.1

# draw respone y_i(t) ~ N(mu_i(t), sigma)
dat$y &lt;- apply(mut, 2, function(x) rnorm(mean = x, sd = sigma, n = n)) 

## fit function-on-scalar model with a linear effect of z1
m1 &lt;- FDboost(y ~ 1 + bolsc(z1_fac, df = 1), timeformula = ~ bbs(t, df = 6), data = dat)

# look for optimal mSTOP using cvrisk() or validateFDboost()
 
cvm &lt;- cvrisk(m1, grid = 1:500)
m1[mstop(cvm)]

m1[200] # use 200 boosting iterations 

# plot true and estimated coefficients 
plot(dat$t, 2*sin(pi*dat$t), col = 2, type = "l", main = "intercept")
plot(m1, which = 1, lty = 2, add = TRUE)

plot(dat$t, 1*cos(pi*dat$t), col = 2, type = "l", main = "effect of z1")
lines(dat$t, -1*cos(pi*dat$t), col = 2, type = "l")
plot(m1, which = 2, lty = 2, col = 1, add = TRUE)


</code></pre>

<hr>
<h2 id='bhistx'>Base-learners for Functional Covariates</h2><span id='topic+bhistx'></span>

<h3>Description</h3>

<p>Base-learners that fit historical functional effects that can be used with the 
tensor product, as, e.g., <code>hbistx(...) %X% bolsc(...)</code>, to form interaction 
effects (Ruegamer et al., 2018).  
For expert use only! May show unexpected behavior  
compared to other base-learners for functional data!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhistx(
  x,
  limits = "s&lt;=t",
  standard = c("no", "time", "length"),
  intFun = integrationWeightsLeft,
  inS = c("smooth", "linear", "constant"),
  inTime = c("smooth", "linear", "constant"),
  knots = 10,
  boundary.knots = NULL,
  degree = 3,
  differences = 1,
  df = 4,
  lambda = NULL,
  penalty = c("ps", "pss"),
  check.ident = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bhistx_+3A_x">x</code></td>
<td>
<p>object of type <code>hmatrix</code> containing time, index and functional covariate; 
note that <code>timeLab</code> in the <code>hmatrix</code>-object must be equal to 
the name of the time-variable in <code>timeformula</code> in the <code>FDboost</code>-call</p>
</td></tr>
<tr><td><code id="bhistx_+3A_limits">limits</code></td>
<td>
<p>defaults to <code>"s&lt;=t"</code> for an historical effect with s&lt;=t;
either one of <code>"s&lt;t"</code> or <code>"s&lt;=t"</code> for [l(t), u(t)] = [T1, t]; 
otherwise specify limits as a function for integration limits [l(t), u(t)]: 
function that takes <code class="reqn">s</code> as the first and <code>t</code> as the second argument and returns 
<code>TRUE</code> for combinations of values (s,t) if <code class="reqn">s</code> falls into the integration range for 
the given <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_standard">standard</code></td>
<td>
<p>the historical effect can be standardized with a factor. 
&quot;no&quot; means no standardization, &quot;time&quot; standardizes with the current value of time and 
&quot;lenght&quot; standardizes with the lenght of the integral</p>
</td></tr>
<tr><td><code id="bhistx_+3A_intfun">intFun</code></td>
<td>
<p>specify the function that is used to compute integration weights in <code>s</code> 
over the functional covariate <code class="reqn">x(s)</code></p>
</td></tr>
<tr><td><code id="bhistx_+3A_ins">inS</code></td>
<td>
<p>historical effect can be smooth, linear or constant in s, 
which is the index of the functional covariates x(s).</p>
</td></tr>
<tr><td><code id="bhistx_+3A_intime">inTime</code></td>
<td>
<p>historical effect can be smooth, linear or constant in time, 
which is the index of the functional response y(time).</p>
</td></tr>
<tr><td><code id="bhistx_+3A_knots">knots</code></td>
<td>
<p>either the number of knots or a vector of the positions 
of the interior knots (for more details see <code><a href="mboost.html#topic+baselearners">bbs</a>)</code>.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_boundary.knots">boundary.knots</code></td>
<td>
<p>boundary points at which to anchor the B-spline basis 
(default the range of the data). A vector (of length 2) 
for the lower and the upper boundary knot can be specified.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_degree">degree</code></td>
<td>
<p>degree of the regression spline.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_differences">differences</code></td>
<td>
<p>a non-negative integer, typically 1, 2 or 3. Defaults to 1.
If <code>differences</code> = <em>k</em>, <em>k</em>-th-order differences are used as 
a penalty (<em>0</em>-th order differences specify a ridge penalty).</p>
</td></tr>
<tr><td><code id="bhistx_+3A_df">df</code></td>
<td>
<p>trace of the hat matrix for the base-learner defining the 
base-learner complexity. Low values of <code>df</code> correspond to a 
large amount of smoothing and thus to &quot;weaker&quot; base-learners.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_lambda">lambda</code></td>
<td>
<p>smoothing parameter of the penalty, computed from <code>df</code> when <code>df</code> is specified.</p>
</td></tr>
<tr><td><code id="bhistx_+3A_penalty">penalty</code></td>
<td>
<p>by default, <code>penalty="ps"</code>, the difference penalty for P-splines is used, 
for <code>penalty="pss"</code> the penalty matrix is transformed to have full rank, 
so called shrinkage approach by Marra and Wood (2011)</p>
</td></tr>
<tr><td><code id="bhistx_+3A_check.ident">check.ident</code></td>
<td>
<p>use checks for identifiability of the effect, based on Scheipl and Greven (2016); 
see Brockhaus et al. (2017) for identifiability checks that take into account the integration limits</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bhistx</code> implements a base-learner for functional covariates with 
flexible integration limits <code>l(t)</code>, <code>r(t)</code> and the possibility to
standardize the effect by <code>1/t</code> or the length of the integration interval. 
The effect is <code>stand * int_{l(t)}^{r_{t}} x(s)beta(t,s) ds</code>. 
The base-learner defaults to a historical effect of the form 
<code class="reqn">\int_{T1}^{t} x_i(s)beta(t,s) ds</code>, 
where <code class="reqn">T1</code> is the minimal index of <code class="reqn">t</code> of the response <code class="reqn">Y(t)</code>. 
<code>bhistx</code> can only be used if <code class="reqn">Y(t)</code> and <code class="reqn">x(s)</code> are observd over
the same domain <code class="reqn">s,t \in [T1, T2]</code>. 
The base-learner <code>bhistx</code> can be used to set up complex interaction effects 
like factor-specific historical  effects as discussed in Ruegamer et al. (2018). 
</p>
<p>Note that the data has to be supplied as a <code>hmatrix</code> object for 
model fit and predictions.
</p>


<h3>Value</h3>

<p>Equally to the base-learners of package mboost: 
</p>
<p>An object of class <code>blg</code> (base-learner generator) with a 
<code>dpp</code> function (dpp, data pre-processing). 
</p>
<p>The call of <code>dpp</code> returns an object of class 
<code>bl</code> (base-learner) with a <code>fit</code> function. The call to 
<code>fit</code> finally returns an object of class <code>bm</code> (base-model).
</p>


<h3>References</h3>

<p>Brockhaus, S., Melcher, M., Leisch, F. and Greven, S. (2017): 
Boosting flexible functional regression models with a high number of functional historical effects,  
Statistics and Computing, 27(4), 913-926. 
</p>
<p>Marra, G. and Wood, S.N. (2011): Practical variable selection for generalized additive models. 
Computational Statistics &amp; Data Analysis, 55, 2372-2387.
</p>
<p>Ruegamer D., Brockhaus, S., Gentsch K., Scherer, K., Greven, S. (2018). 
Boosting factor-specific functional historical models for the detection of synchronization in bioelectrical signals. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 621-642.
</p>
<p>Scheipl, F., Staicu, A.-M. and Greven, S. (2015): 
Functional Additive Mixed Models, Journal of Computational and Graphical Statistics, 24(2), 477-501.
<a href="https://arxiv.org/abs/1207.5947">https://arxiv.org/abs/1207.5947</a> 
</p>
<p>Scheipl, F. and Greven, S. (2016): Identifiability in penalized function-on-function regression models. 
Electronic Journal of Statistics, 10(1), 495-526.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit and <code><a href="#topic+bhist">bhist</a></code> 
for simple hisotorical effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(refund)){
## simulate some data from a historical model
## the interaction effect is in this case not necessary
n &lt;- 100
nygrid &lt;- 35
data1 &lt;- pffrSim(scenario = c("int", "ff"), limits = function(s,t){ s &lt;= t }, 
                n = n, nygrid = nygrid)
data1$X1 &lt;- scale(data1$X1, scale = FALSE) ## center functional covariate                  
dataList &lt;- as.list(data1)
dataList$tvals &lt;- attr(data1, "yindex")

## create the hmatrix-object
X1h &lt;- with(dataList, hmatrix(time = rep(tvals, each = n), id = rep(1:n, nygrid), 
                             x = X1, argvals = attr(data1, "xindex"), 
                             timeLab = "tvals", idLab = "wideIndex", 
                             xLab = "myX", argvalsLab = "svals"))
dataList$X1h &lt;- I(X1h)   
dataList$svals &lt;- attr(data1, "xindex")
## add a factor variable 
dataList$zlong &lt;- factor(gl(n = 2, k = n/2, length = n*nygrid), levels = 1:2)  
dataList$z &lt;- factor(gl(n = 2, k = n/2, length = n), levels = 1:2)

## do the model fit with main effect of bhistx() and interaction of bhistx() and bolsc()
mod &lt;- FDboost(Y ~ 1 + bhistx(x = X1h, df = 5, knots = 5) + 
               bhistx(x = X1h, df = 5, knots = 5) %X% bolsc(zlong), 
              timeformula = ~ bbs(tvals, knots = 10), data = dataList)
              
## alternative parameterization: interaction of bhistx() and bols()
mod &lt;- FDboost(Y ~ 1 + bhistx(x = X1h, df = 5, knots = 5) %X% bols(zlong), 
              timeformula = ~ bbs(tvals, knots = 10), data = dataList)


  # find the optimal mstop over 5-fold bootstrap (small example to reduce run time)
  cv &lt;- cvrisk(mod, folds = cv(model.weights(mod), B = 5))
  mstop(cv)
  mod[mstop(cv)]
  
  appl1 &lt;- applyFolds(mod, folds = cv(rep(1, length(unique(mod$id))), type = "bootstrap", B = 5))

 # plot(mod)

}

</code></pre>

<hr>
<h2 id='birthDistribution'>Densities of live births in Germany</h2><span id='topic+birthDistribution'></span>

<h3>Description</h3>

<p><code>birthDistribution</code> contains densities of live births in Germany over the
months per year (1950 to 2019) and sex (male and female), resulting in 140
densities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(birthDistribution, package = "FDboost")
</code></pre>


<h3>Format</h3>

<p>A list in the correct format to be passed to <code><a href="#topic+FDboost">FDboost</a></code> for 
density-on-scalar regression:
</p>

<dl>
<dt><code>birth_densities</code></dt><dd><p>A 140 x 12 matrix containing the birth densities
in its rows. The first 70 rows correspond to male newborns, the second 70 rows 
to female ones. Within both of these, the years are ordered increasingly 
(1950-2019), see also <code>sex</code> and <code>year</code>.</p>
</dd>
<dt><code>birth_densities_clr</code></dt><dd><p>A 140 x 12 matrix containing the clr
transformed densities in its rows. Same structure as <code>birth_densities</code>.</p>
</dd>
<dt><code>sex</code></dt><dd><p>A factor vector of length 140 with levels <code>"m"</code> (male)
and <code>"f"</code> (female), corresponding to the sex of the newborns for the rows of
<code>birth_densities</code> and <code>birth_densities_clr</code>. The first 70 elements 
are <code>"m"</code>, the second 70 <code>"f"</code>.</p>
</dd>
<dt><code>year</code></dt><dd><p>A vector of length 140 containing the integers from 1950 
to 2019 two times (<code>c(1950:2019, 1950:2019)</code>), corresponding to the years
for the rows of <code>birth_densities</code> and <code>birth_densities_clr</code>.</p>
</dd>
<dt><code>month</code></dt><dd><p>A vector containing the integers from 1 to 12, corresponding
to the months for the columns of <code>birth_densities</code> and <code>birth_densities_clr</code>
(domain <code class="reqn">\mathcal{T}</code> of the (clr-)densities).</p>
</dd>
</dl>

<p>Note that for estimating a density-on-scalar model with <code>FDboost</code>, the
clr transformed densities (<code>birth_densities_clr</code>) serve as response, see
also the vignette &quot;FDboost_density-on-scalar_births&quot;.
The original densities (<code>birth_densities</code>) are not needed for estimation,
but still included for the sake of completeness.
</p>


<h3>Details</h3>

<p>To compensate for the different lengths of the months, the average 
number of births per day for each month (by sex and year) was used to compute
the birth shares from the absolute birth counts. The 12 shares corresponding
to one year and sex form one density in the Bayes Hilbert space 
<code class="reqn">B^2(\delta) = B^2\left( \mathcal{T}, \mathcal{A}, \delta\right)</code>,
where <code class="reqn">\mathcal{T} = \{1, \ldots, 12\}</code> corresponds
to the set of the 12 months, <code class="reqn">\mathcal{A} := \mathcal{P}(\mathcal{T})</code>
corresponds to the power set of <code class="reqn">\mathcal{T}</code>, and the reference measure
<code class="reqn">\delta := \sum_{t = 1}^{12} \delta_t</code> corresponds to the sum of dirac 
measures at <code class="reqn">t \in \mathcal{T}</code>.
</p>


<h3>Source</h3>

<p>Statistisches Bundesamt (Destatis), Genesis-Online, data set 
<a href="https://www-genesis.destatis.de/genesis//online?operation=table&amp;code=12612-0002&amp;bypass=true&amp;levelindex=0&amp;levelid=1610983595176#abreadcrumb">12612-0002</a> 
(01/18/2021); <a href="https://www.govdata.de/dl-de/by-2-0">dl-de/by-2-0</a>; 
processed by Eva-Maria Maier
</p>


<h3>References</h3>

<p>Maier, E.-M., Stoecker, A., Fitzenberger, B., Greven, S. (2021):
Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics.
arXiv preprint arXiv:2110.11771.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clr">clr</a></code> for the (inverse) clr transformation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("birthDistribution", package = "FDboost")

# Plot densities
year_col &lt;- rainbow(70, start = 0.5, end = 1)
year_lty &lt;- c(1, 2, 4, 5)
oldpar &lt;- par(mfrow = c(1, 2))
funplot(1:12, birthDistribution$birth_densities[1:70, ], ylab = "densities", xlab = "month", 
        xaxp = c(1, 12, 11), pch = 20, col = year_col, lty = year_lty, main = "Male")
funplot(1:12, birthDistribution$birth_densities[71:140, ], ylab = "densities", xlab = "month", 
        xaxp = c(1, 12, 11), pch = 20, col = year_col, lty = year_lty, main = "Female")
par(mfrow = c(1, 1))

# fit density-on-scalar model with effects for sex and year
model &lt;- FDboost(birth_densities_clr ~ 1 + bolsc(sex, df = 1) + 
                   bbsc(year, df = 1, differences = 1),
                 # use bbsc() in timeformula to ensure integrate-to-zero constraint
                 timeformula = ~bbsc(month, df = 4, 
                                     # December is followed by January of subsequent year
                                     cyclic = TRUE, 
                                     # knots = {1, ..., 12} with additional boundary knot
                                     # 0 (coinciding with 12) due to cyclic = TRUE
                                     knots = 1:11, boundary.knots = c(0, 12), 
                                     # degree = 1 with these knots yields identity matrix 
                                     # as design matrix
                                     degree = 1),
                 data = birthDistribution, offset = 0, 
                 control = boost_control(mstop = 1000))

# Plotting 'model' yields the clr-transformed effects
par(mfrow = c(1, 3))
plot(model, n1 = 12, n2 = 12)

# Use inverse clr transformation to get effects in Bayes Hilbert space, e.g. for intercept
intercept_clr &lt;- predict(model, which = 1)[1, ]
intercept &lt;- clr(intercept_clr, w = 1, inverse = TRUE)
funplot(1:12, intercept, xlab = "month", xaxp = c(1, 12, 11), pch = 20,
        main = "Intercept", ylab = expression(hat(beta)[0]), id = rep(1, 12))

# Same with predictions
predictions_clr &lt;- predict(model)
predictions &lt;- t(apply(predictions_clr, 1, clr, inverse = TRUE))
pred_ylim &lt;- range(birthDistribution$birth_densities)
par(mfrow = c(1, 2))
funplot(1:12, predictions[1:70, ], ylab = "predictions", xlab = "month", ylim = pred_ylim,
        xaxp = c(1, 12, 11), pch = 20, col = year_col, lty = year_lty, main = "Male")
funplot(1:12, predictions[71:140, ], ylab = "predictions", xlab = "month", ylim = pred_ylim,
        xaxp = c(1, 12, 11), pch = 20, col = year_col, lty = year_lty, main = "Female")
par(oldpar)
</code></pre>

<hr>
<h2 id='bootstrapCI'>Function to compute bootstrap confidence intervals</h2><span id='topic+bootstrapCI'></span>

<h3>Description</h3>

<p>The model is fitted on bootstrapped samples of the data to compute bootstrapped 
coefficient estimates. To determine the optimal stopping iteration an inner bootstrap 
is run within each bootstrap fold. 
As estimation by boosting shrinks the coefficient estimates towards zero, 
to bootstrap confidence intervals are biased towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrapCI(
  object,
  which = NULL,
  resampling_fun_outer = NULL,
  resampling_fun_inner = NULL,
  B_outer = 100,
  B_inner = 25,
  type_inner = c("bootstrap", "kfold", "subsampling"),
  levels = c(0.05, 0.95),
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrapCI_+3A_object">object</code></td>
<td>
<p>a fitted model object of class <code>FDboost</code>, 
for which the confidence intervals should be computed.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_which">which</code></td>
<td>
<p>a subset of base-learners to take into account for 
computing confidence intervals.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_resampling_fun_outer">resampling_fun_outer</code></td>
<td>
<p>function for the outer resampling procedure.
<code>resampling_fun_outer</code> must be a function with arguments <code>object</code>
and <code>fun</code>, where <code>object</code> corresponds to the fitted 
<code>FDboost</code> object and <code>fun</code> is passed to the <code>fun</code>
argument of the resampling function (see examples).
If <code>NULL</code>, <code><a href="#topic+applyFolds">applyFolds</a></code> is used with 100-fold boostrap.
Further arguments to <code><a href="#topic+applyFolds">applyFolds</a></code> can be passed via <code>...</code>.
Although the function can be defined very flexible, it is recommended 
to use <code>applyFolds</code> and, in particular, not <code>cvrisk</code>, 
as in this case, weights of the inner and outer 
fold will interact, probably causing the inner 
resampling to crash. For bootstrapped confidence intervals
the outer function should usually be a bootstrap type of resampling.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_resampling_fun_inner">resampling_fun_inner</code></td>
<td>
<p>function for the inner resampling procudure,
which determines the optimal stopping iteration in each fold of the
outer resampling procedure. Should be a function with one argument
<code>object</code> for the fitted <code>FDboost</code> object. 
If <code>NULL</code>, <code>cvrisk</code> is used with 25-fold bootstrap.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_b_outer">B_outer</code></td>
<td>
<p>Number of resampling folds in the outer loop.
Argument is overwritten, when a custom <code>resampling_fun_outer</code>
is supplied.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_b_inner">B_inner</code></td>
<td>
<p>Number of resampling folds in the inner loop.
Argument is overwritten, when a custom <code>resampling_fun_inner</code>
is supplied.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_type_inner">type_inner</code></td>
<td>
<p>character argument for specifying the cross-validation method for
the inner resampling level. Default is <code>"bootstrap"</code>. Currently  
bootstrap, k-fold cross-validation and subsampling are implemented.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_levels">levels</code></td>
<td>
<p>the confidence levels required. If NULL, the 
raw results are returned.</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, information will be printed in the console</p>
</td></tr>
<tr><td><code id="bootstrapCI_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code><a href="#topic+applyFolds">applyFolds</a></code> if
the default for <code>resampling_fun_outer</code> is used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the elements <code>raw_results</code>, the 
<code>quantiles</code> and <code>mstops</code>. 
In <code>raw_results</code> and <code>quantiles</code>, each baselearner
selected with <code>which</code> in turn corresponds to a list
element. The quantiles are given as vector, matrix or list of
matrices depending on the nature of the effect. In case of functional
effects the list element in<code>quantiles</code> is a <code>length(levels)</code> times
<code>length(effect)</code> matrix, i.e. the rows correspond to the quantiles.
In case of coefficient surfaces, <code>quantiles</code> comprises a list of matrices,
where each list element corresponds to a quantile.
</p>


<h3>Note</h3>

<p>Note that parallelization can be achieved by defining
the <code>resampling_fun_outer</code> or <code>_inner</code> accordingly.
See, e.g., <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> on how to parallelize resampling
functions or the examples below. Also note that by defining
a custum inner or outer resampling function the respective
argument <code>B_inner</code> or <code>B_outer</code> is ignored.
For models with complex baselearners, e.g., created by combining
several baselearners with the Kronecker or row-wise tensor product,
it is also recommended to use <code>levels = NULL</code> in order to
let the function return the raw results and then manually compute
confidence intervals.
If a baselearner is not selected in any fold, the function
treats its effect as constantly zero.
</p>


<h3>Author(s)</h3>

<p>David Ruegamer, Sarah Brockhaus
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(refund)){
#########
# model with linear functional effect, use bsignal()
# Y(t) = f(t) + \int X1(s)\beta(s,t)ds + eps
set.seed(2121)
data1 &lt;- pffrSim(scenario = "ff", n = 40)
data1$X1 &lt;- scale(data1$X1, scale = FALSE)
dat_list &lt;- as.list(data1)
dat_list$t &lt;- attr(data1, "yindex")
dat_list$s &lt;- attr(data1, "xindex")

## model fit by FDboost 
m1 &lt;- FDboost(Y ~ 1 + bsignal(x = X1, s = s, knots = 8, df = 3), 
              timeformula = ~ bbs(t, knots = 8), data = dat_list)

}
              
             
# a short toy example with to few folds  
# and up to 200 boosting iterations 
bootCIs &lt;- bootstrapCI(m1[200], B_inner = 2, B_outer = 5) 

# look at stopping iterations
bootCIs$mstops

# plot bootstrapped coefficient estimates
plot(bootCIs, ask = FALSE)


my_inner_fun &lt;- function(object){ 
cvrisk(object, folds = cvLong(id = object$id, weights = 
model.weights(object), B = 2) # 10-fold for inner resampling
) 
}


bootCIs &lt;- bootstrapCI(m1, resampling_fun_inner = my_inner_fun, 
                       B_outer = 5) # small B_outer to speed up


## We can also use the ... argument to parallelize the applyFolds
## function in the outer resampling 


bootCIs &lt;- bootstrapCI(m1, B_inner = 5, B_outer = 3)


## Now let's parallelize the outer resampling and use 
## crossvalidation instead of bootstrap for the inner resampling

my_inner_fun &lt;- function(object){ 
cvrisk(object, folds = cvLong(id = object$id, weights = 
model.weights(object), type = "kfold", # use CV
B = 5, # 5-fold for inner resampling
)) # use five cores
}

# use applyFolds for outer function to avoid messing up weights
my_outer_fun &lt;- function(object, fun){
applyFolds(object = object,
folds = cv(rep(1, length(unique(object$id))), 
type = "bootstrap", B = 10), fun = fun) # parallelize on 10 cores
}


bootCIs &lt;- bootstrapCI(m1, resampling_fun_inner = my_inner_fun,
                       resampling_fun_outer = my_outer_fun,
                       B_inner = 5, B_outer = 10) 


######## Example for scalar-on-function-regression with bsignal() 
data("fuelSubset", package = "FDboost")

## center the functional covariates per observed wavelength
fuelSubset$UVVIS &lt;- scale(fuelSubset$UVVIS, scale = FALSE)
fuelSubset$NIR &lt;- scale(fuelSubset$NIR, scale = FALSE)

## to make mboost:::df2lambda() happy (all design matrix entries &lt; 10)
## reduce range of argvals to [0,1] to get smaller integration weights
fuelSubset$uvvis.lambda &lt;- with(fuelSubset, (uvvis.lambda - min(uvvis.lambda)) /
(max(uvvis.lambda) - min(uvvis.lambda) ))
fuelSubset$nir.lambda &lt;- with(fuelSubset, (nir.lambda - min(nir.lambda)) /
(max(nir.lambda) - min(nir.lambda) ))

## model fit with scalar response and two functional linear effects 
## include no intercept as all base-learners are centered around 0    

mod2 &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bsignal(NIR, nir.lambda, knots = 40, df=4, check.ident = FALSE), 
               timeformula = NULL, data = fuelSubset) 



# takes some time, because of defaults: B_outer = 100, B_inner = 25
bootCIs &lt;- bootstrapCI(mod2, B_outer = 10, B_inner = 5)
           # in practice, rather set B_outer = 1000



</code></pre>

<hr>
<h2 id='bsignal'>Base-learners for Functional Covariates</h2><span id='topic+bsignal'></span><span id='topic+bconcurrent'></span><span id='topic+bhist'></span><span id='topic+bfpc'></span>

<h3>Description</h3>

<p>Base-learners that fit effects of functional covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsignal(
  x,
  s,
  index = NULL,
  inS = c("smooth", "linear", "constant"),
  knots = 10,
  boundary.knots = NULL,
  degree = 3,
  differences = 1,
  df = 4,
  lambda = NULL,
  center = FALSE,
  cyclic = FALSE,
  Z = NULL,
  penalty = c("ps", "pss"),
  check.ident = FALSE
)

bconcurrent(
  x,
  s,
  time,
  index = NULL,
  knots = 10,
  boundary.knots = NULL,
  degree = 3,
  differences = 1,
  df = 4,
  lambda = NULL,
  cyclic = FALSE
)

bhist(
  x,
  s,
  time,
  index = NULL,
  limits = "s&lt;=t",
  standard = c("no", "time", "length"),
  intFun = integrationWeightsLeft,
  inS = c("smooth", "linear", "constant"),
  inTime = c("smooth", "linear", "constant"),
  knots = 10,
  boundary.knots = NULL,
  degree = 3,
  differences = 1,
  df = 4,
  lambda = NULL,
  penalty = c("ps", "pss"),
  check.ident = FALSE
)

bfpc(
  x,
  s,
  index = NULL,
  df = 4,
  lambda = NULL,
  penalty = c("identity", "inverse", "no"),
  pve = 0.99,
  npc = NULL,
  npc.max = 15,
  getEigen = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bsignal_+3A_x">x</code></td>
<td>
<p>matrix of functional variable x(s). The functional covariate has to be 
supplied as n by &lt;no. of evaluations&gt; matrix, i.e., each row is one functional observation.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_s">s</code></td>
<td>
<p>vector for the index of the functional variable x(s) giving the 
measurement points of the functional covariate.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_index">index</code></td>
<td>
<p>a vector of integers for expanding the covariate in <code>x</code> 
For example, <code>bsignal(X, s, index = index)</code> is equal to <code>bsignal(X[index,], s)</code>, 
where index is an integer of length greater or equal to <code>NROW(x)</code>.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_ins">inS</code></td>
<td>
<p>the functional effect can be smooth, linear or constant in s, 
which is the index of the functional covariates x(s).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_knots">knots</code></td>
<td>
<p>either the number of knots or a vector of the positions 
of the interior knots (for more details see <code><a href="mboost.html#topic+baselearners">bbs</a></code>).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_boundary.knots">boundary.knots</code></td>
<td>
<p>boundary points at which to anchor the B-spline basis 
(default the range of the data). A vector (of length 2) 
for the lower and the upper boundary knot can be specified.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_degree">degree</code></td>
<td>
<p>degree of the regression spline.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_differences">differences</code></td>
<td>
<p>a non-negative integer, typically 1, 2 or 3. Defaults to 1.
If <code>differences</code> = <em>k</em>, <em>k</em>-th-order differences are used as 
a penalty (<em>0</em>-th order differences specify a ridge penalty).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_df">df</code></td>
<td>
<p>trace of the hat matrix for the base-learner defining the 
base-learner complexity. Low values of <code>df</code> correspond to a 
large amount of smoothing and thus to &quot;weaker&quot; base-learners.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_lambda">lambda</code></td>
<td>
<p>smoothing parameter of the penalty, computed from <code>df</code> when <code>df</code> is specified.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_center">center</code></td>
<td>
<p>See <code><a href="mboost.html#topic+baselearners">bbs</a></code>. 
The effect is re-parameterized such that the unpenalized part of the fit is subtracted and only 
the penalized effect is fitted, using a spectral decomposition of the penalty matrix.  
The unpenalized, parametric part has then to be included in separate 
base-learners using <code>bsignal(..., inS = 'constant')</code> or <code>bsignal(..., inS = 'linear')</code> 
for first (<code>difference = 1</code>) and second (<code>difference = 2</code>) order difference penalty respectively. 
See the help on the argument <code>center</code> of <code><a href="mboost.html#topic+baselearners">bbs</a></code>.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_cyclic">cyclic</code></td>
<td>
<p>if <code>cyclic = TRUE</code> the fitted coefficient function coincides at the boundaries 
(useful for cyclic covariates such as day time etc.).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_z">Z</code></td>
<td>
<p>a transformation matrix for the design-matrix over the index of the covariate.
<code>Z</code> can be calculated as the transformation matrix for a sum-to-zero constraint in the case
that all trajectories have the same mean 
(then a shift in the coefficient function is not identifiable).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_penalty">penalty</code></td>
<td>
<p>for <code>bsignal</code>, by default, <code>penalty = "ps"</code>, the difference penalty for P-splines is used, 
for <code>penalty = "pss"</code> the penalty matrix is transformed to have full rank, 
so called shrinkage approach by Marra and Wood (2011). 
For <code>bfpc</code> the penalty can be either <code>"identity"</code> for a ridge penalty 
(the default) or <code>"inverse"</code> to use the matrix with the inverse eigenvalues 
on the diagonal as penalty matrix or <code>"no"</code> for no penalty.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_check.ident">check.ident</code></td>
<td>
<p>use checks for identifiability of the effect, based on Scheipl and Greven (2016) 
for linear functional effect using <code>bsignal</code> and 
based on Brockhaus et al. (2017) for historical effects using <code>bhist</code></p>
</td></tr>
<tr><td><code id="bsignal_+3A_time">time</code></td>
<td>
<p>vector for the index of the functional response y(time) 
giving the measurement points of the functional response.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_limits">limits</code></td>
<td>
<p>defaults to <code>"s&lt;=t"</code> for an historical effect with s&lt;=t;
either one of <code>"s&lt;t"</code> or <code>"s&lt;=t"</code> for [l(t), u(t)] = [T1, t]; 
otherwise specify limits as a function for integration limits [l(t), u(t)]: 
function that takes <code class="reqn">s</code> as the first and <code>t</code> as the second argument and returns 
<code>TRUE</code> for combinations of values (s,t) if <code class="reqn">s</code> falls into the integration range for 
the given <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_standard">standard</code></td>
<td>
<p>the historical effect can be standardized with a factor. 
&quot;no&quot; means no standardization, &quot;time&quot; standardizes with the current value of time and 
&quot;length&quot; standardizes with the length of the integral</p>
</td></tr>
<tr><td><code id="bsignal_+3A_intfun">intFun</code></td>
<td>
<p>specify the function that is used to compute integration weights in <code>s</code> 
over the functional covariate <code class="reqn">x(s)</code></p>
</td></tr>
<tr><td><code id="bsignal_+3A_intime">inTime</code></td>
<td>
<p>the historical effect can be smooth, linear or constant in time, 
which is the index of the functional response y(time).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_pve">pve</code></td>
<td>
<p>proportion of variance explained by the first K functional principal components (FPCs): 
used to choose the number of functional principal components (FPCs).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_npc">npc</code></td>
<td>
<p>prespecified value for the number K of FPCs (if given, this overrides <code>pve</code>).</p>
</td></tr>
<tr><td><code id="bsignal_+3A_npc.max">npc.max</code></td>
<td>
<p>maximal number K of FPCs to use; defaults to 15.</p>
</td></tr>
<tr><td><code id="bsignal_+3A_geteigen">getEigen</code></td>
<td>
<p>save the eigenvalues and eigenvectors, defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bsignal()</code> implements a base-learner for functional covariates to  
estimate an effect of the form <code class="reqn">\int x_i(s)\beta(s)ds</code>. Defaults to a cubic  
B-spline basis with first difference penalties for <code class="reqn">\beta(s)</code> and numerical 
integration over the entire range by using trapezoidal Riemann weights. 
If <code>bsignal()</code> is used within <code>FDboost()</code>, the base-learner of 
<code>timeformula</code> is attached, resulting in an effect varying over the index
of the response <code class="reqn">\int x_i(s)\beta(s, t)ds</code> if <code>timeformula = bbs(t)</code>. 
The functional variable must be observed on one common grid <code>s</code>.  
</p>
<p><code>bconcurrent()</code> implements a concurrent effect for a functional covariate
on a functional response, i.e., an effect of the form <code class="reqn">x_i(t)\beta(t)</code> for
a functional response <code class="reqn">Y_i(t)</code> and concurrently observed covariate <code class="reqn">x_i(t)</code>. 
<code>bconcurrent()</code> can only be used if <code class="reqn">Y(t)</code> and <code class="reqn">x(s)</code> are observed over
the same domain <code class="reqn">s,t \in [T1, T2]</code>.  
</p>
<p><code>bhist()</code> implements a base-learner for functional covariates with 
flexible integration limits <code>l(t)</code>, <code>r(t)</code> and the possibility to
standardize the effect by <code>1/t</code> or the length of the integration interval. 
The effect is <code class="reqn">stand * \int_{l(t)}^{r_{t}} x(s)\beta(t,s)ds</code>, where <code class="reqn">stand</code> is 
the chosen standardization which defaults to 1. 
The base-learner defaults to a historical effect of the form 
<code class="reqn">\int_{T1}^{t} x_i(s)\beta(t,s)ds</code>, 
where <code class="reqn">T1</code> is the minimal index of <code class="reqn">t</code> of the response <code class="reqn">Y(t)</code>. 
The functional covariate must be observed on one common grid <code>s</code>.  
See Brockhaus et al. (2017) for details on historical effects.   
</p>
<p><code>bfpc()</code> is a base-learner for a linear effect of functional covariates based on 
functional principal component analysis (FPCA). 
For the functional linear effect <code class="reqn">\int x_i(s)\beta(s)ds</code> the functional covariate 
and the coefficient function are both represented by a FPC basis. 
The functional covariate
<code class="reqn">x(s)</code> is decomposed into <code class="reqn">x(s) \approx \sum_{k=1}^K \xi_{ik} \Phi_k(s)</code> using 
<code><a href="refund.html#topic+fpca.sc">fpca.sc</a></code> for the truncated Karhunen-Loeve decomposition. 
Then <code class="reqn">\beta(s)</code> is represented in the function
space spanned by <code class="reqn">\Phi_k(s)</code>, k=1,...,K, see Scheipl et al. (2015) for details. 
As penalty matrix, the identity matrix is used. 
The implementation is similar to <code><a href="refund.html#topic+ffpc">ffpc</a></code>.  
</p>
<p>It is recommended to use centered functional covariates with 
<code class="reqn">\sum_i x_i(s) = 0</code> for all <code class="reqn">s</code> in <code>bsignal()</code>-, 
<code>bhist()</code>- and <code>bconcurrent()</code>-terms. 
For centered covariates, the effects are centered per time-point of the response. 
If all effects are centered, the functional intercept 
can be interpreted as the global mean function. 
</p>
<p>The base-learners for functional covariates cannot deal with any missing 
values in the covariates.
</p>


<h3>Value</h3>

<p>Equally to the base-learners of package <code>mboost</code>: 
</p>
<p>An object of class <code>blg</code> (base-learner generator) with a 
<code>dpp()</code> function (dpp, data pre-processing). 
</p>
<p>The call of <code>dpp()</code> returns an object of class 
<code>bl</code> (base-learner) with a <code>fit()</code> function. The call to 
<code>fit()</code> finally returns an object of class <code>bm</code> (base-model).
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300.
</p>
<p>Brockhaus, S., Melcher, M., Leisch, F. and Greven, S. (2017): 
Boosting flexible functional regression models with a high number of functional historical effects,  
Statistics and Computing, 27(4), 913-926.   
</p>
<p>Marra, G. and Wood, S.N. (2011): Practical variable selection for generalized additive models. 
Computational Statistics &amp; Data Analysis, 55, 2372-2387.
</p>
<p>Scheipl, F., Staicu, A.-M. and Greven, S. (2015): 
Functional Additive Mixed Models, Journal of Computational and Graphical Statistics, 24(2), 477-501. 
</p>
<p>Scheipl, F. and Greven, S. (2016): Identifiability in penalized function-on-function regression models. 
Electronic Journal of Statistics, 10(1), 495-526.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>######## Example for scalar-on-function-regression with bsignal()  
data("fuelSubset", package = "FDboost")

## center the functional covariates per observed wavelength
fuelSubset$UVVIS &lt;- scale(fuelSubset$UVVIS, scale = FALSE)
fuelSubset$NIR &lt;- scale(fuelSubset$NIR, scale = FALSE)

## to make mboost:::df2lambda() happy (all design matrix entries &lt; 10)
## reduce range of argvals to [0,1] to get smaller integration weights
fuelSubset$uvvis.lambda &lt;- with(fuelSubset, (uvvis.lambda - min(uvvis.lambda)) /
                                  (max(uvvis.lambda) - min(uvvis.lambda) ))
fuelSubset$nir.lambda &lt;- with(fuelSubset, (nir.lambda - min(nir.lambda)) /
                                (max(nir.lambda) - min(nir.lambda) ))

## model fit with scalar response and two functional linear effects 
## include no intercept 
## as all base-learners are centered around 0 
mod2 &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bsignal(NIR, nir.lambda, knots = 40, df=4, check.ident = FALSE), 
               timeformula = NULL, data = fuelSubset) 
summary(mod2) 

 
###############################################
### data simulation like in manual of pffr::ff

if(require(refund)){

#########
# model with linear functional effect, use bsignal()
# Y(t) = f(t) + \int X1(s)\beta(s,t)ds + eps
set.seed(2121)
data1 &lt;- pffrSim(scenario = "ff", n = 40)
data1$X1 &lt;- scale(data1$X1, scale = FALSE)
dat_list &lt;- as.list(data1)
dat_list$t &lt;- attr(data1, "yindex")
dat_list$s &lt;- attr(data1, "xindex")

## model fit by FDboost 
m1 &lt;- FDboost(Y ~ 1 + bsignal(x = X1, s = s, knots = 5), 
              timeformula = ~ bbs(t, knots = 5), data = dat_list, 
              control = boost_control(mstop = 21))

## search optimal mSTOP

  set.seed(123)
  cv &lt;- validateFDboost(m1, grid = 1:100) # 21 iterations


## model fit by pffr
t &lt;- attr(data1, "yindex")
s &lt;- attr(data1, "xindex")
m1_pffr &lt;- pffr(Y ~ ff(X1, xind = s), yind = t, data = data1)


  oldpar &lt;- par(mfrow = c(2, 2))
  plot(m1, which = 1); plot(m1, which = 2) 
  plot(m1_pffr, select = 1, shift = m1_pffr$coefficients["(Intercept)"]) 
  plot(m1_pffr, select = 2)
  par(oldpar)



############################################
# model with functional historical effect, use bhist() 
# Y(t) = f(t)  + \int_0^t X1(s)\beta(s,t)ds + eps
set.seed(2121)
mylimits &lt;- function(s, t){
  (s &lt; t) | (s == t)
}
data2 &lt;- pffrSim(scenario = "ff", n = 40, limits = mylimits)
data2$X1 &lt;- scale(data2$X1, scale = FALSE)
dat2_list &lt;- as.list(data2)
dat2_list$t &lt;- attr(data2, "yindex")
dat2_list$s &lt;- attr(data2, "xindex")

## model fit by FDboost 
m2 &lt;- FDboost(Y ~ 1 + bhist(x = X1, s = s, time = t, knots = 5), 
              timeformula = ~ bbs(t, knots = 5), data = dat2_list, 
              control = boost_control(mstop = 40))
              
## search optimal mSTOP

  set.seed(123)
  cv2 &lt;- validateFDboost(m2, grid = 1:100) # 40 iterations
               

## model fit by pffr
t &lt;- attr(data2, "yindex")
s &lt;- attr(data2, "xindex")
m2_pffr &lt;- pffr(Y ~ ff(X1, xind = s, limits = "s&lt;=t"), yind = t, data = data2)


oldpar &lt;- par(mfrow = c(2, 2))
plot(m2, which = 1); plot(m2, which = 2)
## plot of smooth intercept does not contain m1_pffr$coefficients["(Intercept)"]
plot(m2_pffr, select = 1, shift = m2_pffr$coefficients["(Intercept)"]) 
plot(m2_pffr, select = 2) 
par(oldpar)



}


</code></pre>

<hr>
<h2 id='clr'>Clr and inverse clr transformation</h2><span id='topic+clr'></span>

<h3>Description</h3>

<p><code>clr</code> computes the clr or inverse clr transformation of a vector <code>f</code>
with respect to integration weights <code>w</code>, corresponding to a Bayes Hilbert space
<code class="reqn">B^2(\mu) = B^2(\mathcal{T}, \mathcal{A}, \mu)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clr(f, w = 1, inverse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clr_+3A_f">f</code></td>
<td>
<p>a vector containing the function values (evaluated on a grid) of the 
function <code class="reqn">f</code> to transform. If <code>inverse = TRUE</code>, <code>f</code> must be a density,
i.e., all entries must be positive and usually <code>f</code> integrates to one. 
If <code>inverse = FALSE</code>, <code>f</code> should integrate to zero, see Details.</p>
</td></tr>
<tr><td><code id="clr_+3A_w">w</code></td>
<td>
<p>a vector of length one or of the same length as <code>f</code> containing 
positive integration weights. If <code>w</code> has length one, this
weight is used for all function values. The integral of <code class="reqn">f</code> is approximated
via <code class="reqn">\int_{\mathcal{T}} f \, \mathrm{d}\mu \approx 
\sum_{j=1}^m</code> <code>w</code><code class="reqn">_j</code> <code>f</code><code class="reqn">_j</code>,
where <code class="reqn">m</code> equals the length of <code>f</code>.</p>
</td></tr>
<tr><td><code id="clr_+3A_inverse">inverse</code></td>
<td>
<p>if <code>TRUE</code>, the inverse clr transformation is computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The clr transformation maps a density <code class="reqn">f</code> from <code class="reqn">B^2(\mu)</code> to
<code class="reqn">L^2_0(\mu) := \{ f \in L^2(\mu) ~|~ \int_{\mathcal{T}} f \, \mathrm{d}\mu = 0\}</code>
via
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{clr}(f) := \log f - \frac{1}{\mu (\mathcal{T})} \int_{\mathcal{T}} \log f \, \mathrm{d}\mu.</code>
</p>

<p>The inverse clr transformation maps a function <code class="reqn">f</code> from
<code class="reqn">L^2_0(\mu)</code> to <code class="reqn">B^2(\mu)</code> via
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{clr}^{-1}(f) := \frac{\exp f}{\int_{\mathcal{T}} \exp f \, \mathrm{d}\mu}.</code>
</p>

<p>Note that in contrast to Maier et al. (2021), this definition of the inverse
clr transformation includes normalization, yielding the respective probability 
density function (representative of the equivalence class of proportional
functions in <code class="reqn">B^2(\mu)</code>). 
</p>
<p>The (inverse) clr transformation depends not only on <code class="reqn">f</code>, but also on the
underlying measure space <code class="reqn">\left( \mathcal{T}, \mathcal{A}, \mu\right)</code>, 
which determines the integral. In <code>clr</code> this is specified via the 
integration weights <code>w</code>. E.g., for a discrete set <code class="reqn">\mathcal{T}</code>
with <code class="reqn">\mathcal{A} = \mathcal{P}(\mathcal{T})</code> the power set of 
<code class="reqn">\mathcal{T}</code> and <code class="reqn">\mu = \sum_{t \in T} \delta_t</code> the sum of dirac
measures at <code class="reqn">t \in \mathcal{T}</code>, the default <code>w = 1</code> is
the correct choice. In this case, integrals are indeed computed exactly, not
only approximately. 
For an interval <code class="reqn">\mathcal{T} = [a, b]</code>
with <code class="reqn">\mathcal{A} = \mathcal{B}</code> the Borel <code class="reqn">\sigma</code>-algebra 
restricted to <code class="reqn">\mathcal{T}</code> and <code class="reqn">\mu = \lambda</code> the Lebesgue measure,
the choice of <code>w</code> depends on the grid on which the function was evaluated:
<code>w</code><code class="reqn">_j</code> must correspond to the length of the subinterval of <code class="reqn">[a, b]</code>, which 
<code>f</code><code class="reqn">_j</code> represents.
E.g., for a grid with equidistant distance <code class="reqn">d</code>, where the boundary grid 
values are <code class="reqn">a + \frac{d}{2}</code> and <code class="reqn">b - \frac{d}{2}</code>
(i.e., the grid points are centers of intervals of size <code class="reqn">d</code>),
equal weights <code class="reqn">d</code> should be chosen for <code>w</code>. 
</p>
<p>The clr transformation is crucial for density-on-scalar regression 
since estimating the clr transformed model in <code class="reqn">L^2_0(\mu)</code> is equivalent
to estimating the original model in <code class="reqn">B^2(\mu)</code> (as the clr transformation
is an isometric isomorphism), see also the vignette &quot;FDboost_density-on-scalar_births&quot;
and Maier et al. (2021).
</p>


<h3>Value</h3>

<p>A vector of the same length as <code>f</code> containing the (inverse) clr 
transformation of <code>f</code>.
</p>


<h3>Author(s)</h3>

<p>Eva-Maria Maier
</p>


<h3>References</h3>

<p>Maier, E.-M., Stoecker, A., Fitzenberger, B., Greven, S. (2021):
Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics.
arXiv preprint arXiv:2110.11771.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Continuous case (T = [0, 1] with Lebesgue measure):
# evaluate density of a Beta distribution on an equidistant grid
g &lt;- seq(from = 0.005, to = 0.995, by = 0.01)
f &lt;- dbeta(g, 2, 5)
# compute clr transformation with distance of two grid points as integration weight
f_clr &lt;- clr(f, w = 0.01)
# visualize result
plot(g, f_clr , type = "l")
abline(h = 0, col = "grey")
# compute inverse clr transformation (w as above)
f_clr_inv &lt;- clr(f_clr, w = 0.01, inverse = TRUE)
# visualize result
plot(g, f, type = "l")
lines(g, f_clr_inv, lty = 2, col = "red")

### Discrete case (T = {1, ..., 12} with sum of dirac measures at t in T):
data("birthDistribution", package = "FDboost")
# fit density-on-scalar model with effects for sex and year
model &lt;- FDboost(birth_densities_clr ~ 1 + bolsc(sex, df = 1) + 
                   bbsc(year, df = 1, differences = 1),
                 # use bbsc() in timeformula to ensure integrate-to-zero constraint
                 timeformula = ~bbsc(month, df = 4, 
                                     # December is followed by January of subsequent year
                                     cyclic = TRUE, 
                                     # knots = {1, ..., 12} with additional boundary knot
                                     # 0 (coinciding with 12) due to cyclic = TRUE
                                     knots = 1:11, boundary.knots = c(0, 12), 
                                     # degree = 1 with these knots yields identity matrix 
                                     # as design matrix
                                     degree = 1),
                 data = birthDistribution, offset = 0, 
                 control = boost_control(mstop = 1000))
# Extract predictions (clr-transformed!) and transform them to Bayes Hilbert space
predictions_clr &lt;- predict(model)
predictions &lt;- t(apply(predictions_clr, 1, clr, inverse = TRUE))

</code></pre>

<hr>
<h2 id='coef.FDboost'>Coefficients of boosted functional regression model</h2><span id='topic+coef.FDboost'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object produced by <code><a href="#topic+FDboost">FDboost</a>()</code> and 
returns estimated coefficient functions/surfaces <code class="reqn">\beta(t), \beta(s,t)</code> and 
estimated smooth effects <code class="reqn">f(z), f(x,z)</code> or <code class="reqn">f(x, z, t)</code>. 
Not implemented for smooths in more than 3 dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
coef(
  object,
  raw = FALSE,
  which = NULL,
  computeCoef = TRUE,
  returnData = FALSE,
  n1 = 40,
  n2 = 40,
  n3 = 20,
  n4 = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.FDboost_+3A_object">object</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_raw">raw</code></td>
<td>
<p>logical defaults to <code>FALSE</code>.
If <code>raw = FALSE</code> for each effect the estimated function/surface is calculated. 
If <code>raw = TRUE</code> the coefficients of the model are returned.</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_which">which</code></td>
<td>
<p>a subset of base-learners for which the coefficients
should be computed (numeric vector), 
defaults to NULL which is the same as <code>which=1:length(object$baselearner)</code>.
In the special case of <code>which=0</code>, only the coefficients of the offset are returned.</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_computecoef">computeCoef</code></td>
<td>
<p>defaults to <code>TRUE</code>, if <code>FALSE</code> only the names of the terms are returned</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_returndata">returnData</code></td>
<td>
<p>return the dataset which is used to get the coefficient estimates as 
predictions, see Details.</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_n1">n1</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_n2">n2</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_n3">n3</code></td>
<td>
<p>n1, n2, n3 give the number of grid-points for 1-/2-/3-dimensional 
smooth terms used in the marginal equidistant grids over the range of the 
covariates at which the estimated effects are evaluated.</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_n4">n4</code></td>
<td>
<p>gives the number of points for the third dimension in a 3-dimensional smooth term</p>
</td></tr>
<tr><td><code id="coef.FDboost_+3A_...">...</code></td>
<td>
<p>other arguments, not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>raw = FALSE</code> the function <code>coef.FDboost</code> generates adequate dummy data 
and uses the function <code>predict.FDboost</code> to 
compute the estimated coefficient functions.
</p>


<h3>Value</h3>

<p>If <code>raw = FALSE</code>, a list containing 
</p>

<ul>
<li> <p><code>offset</code> a list with plot information for the offset.
</p>
</li>
<li> <p><code>smterms</code> a named list with one entry for each smooth term in the model. 
Each entry contains
</p>

<ul>
<li> <p><code>x, y, z</code> the unique grid-points used to evaluate the smooth/coefficient function/coefficient surface
</p>
</li>
<li> <p><code>xlim, ylim, zlim</code> the extent of the x/y/z-axes
</p>
</li>
<li> <p><code>xlab, ylab, zlab</code> the names of the covariates for the x/y/z-axes
</p>
</li>
<li> <p><code>value</code> a vector/matrix/list of matrices containing the coefficient values 
</p>
</li>
<li> <p><code>dim</code> the dimensionality of the effect
</p>
</li>
<li> <p><code>main</code> the label of the smooth term (a short label)
</p>
</li></ul>
</li></ul>
 
<p>If <code>raw = TRUE</code>, a list containing the estimated spline coefficients.
</p>

<hr>
<h2 id='cvrisk.FDboostLSS'>Cross-validation for FDboostLSS</h2><span id='topic+cvrisk.FDboostLSS'></span>

<h3>Description</h3>

<p>Multidimensional cross-validated estimation of the empirical risk for hyper-parameter selection, 
for an object of class <code>FDboostLSS</code> setting the folds per default to resampling curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboostLSS'
cvrisk(
  object,
  folds = cvLong(id = object[[1]]$id, weights = model.weights(object[[1]])),
  grid = NULL,
  papply = mclapply,
  trace = TRUE,
  fun = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvrisk.FDboostLSS_+3A_object">object</code></td>
<td>
<p>an object of class <code>FDboostLSS</code>.</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_folds">folds</code></td>
<td>
<p>a weight matrix a weight matrix with number of rows equal to the number of observations. 
The number of columns corresponds to the number of cross-validation runs, 
defaults to 25 bootstrap samples, resampling whole curves</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_grid">grid</code></td>
<td>
<p>defaults to a grid up to the current number of boosting iterations. 
The default generates the grid according to the defaults of 
<code><a href="gamboostLSS.html#topic+cvrisk.mboostLSS">cvrisk.mboostLSS</a></code> which are different for models with cyclic or noncyclic fitting.</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_papply">papply</code></td>
<td>
<p>(parallel) apply function, defaults to <code><a href="parallel.html#topic+mclapply">mclapply</a></code>, 
see <code><a href="gamboostLSS.html#topic+cvrisk.mboostLSS">cvrisk.mboostLSS</a></code> for details.</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_trace">trace</code></td>
<td>
<p>print status information during cross-validation? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_fun">fun</code></td>
<td>
<p>if <code>fun</code> is <code>NULL</code>, the out-of-sample risk is returned. 
<code>fun</code>, as a function of <code>object</code>, 
may extract any other characteristic of the cross-validated models. These are returned as is.</p>
</td></tr>
<tr><td><code id="cvrisk.FDboostLSS_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="parallel.html#topic+mclapply">mclapply</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>cvrisk.FDboostLSS</code> is a wrapper for 
<code>cvrisk.mboostLSS</code> in package <code>gamboostLSS</code>.  
It overrides the default for the folds, so that the folds are sampled on the level of curves 
(not on the level of single observations, which does not make sense for functional response).
</p>


<h3>Value</h3>

<p>An object of class <code>cvriskLSS</code> (when <code>fun</code> was not specified), 
basically a matrix containing estimates of the empirical risk for a varying number 
of bootstrap iterations. <code>plot</code> and <code>print</code> methods are available as well as an 
<code>mstop</code> method, see <code><a href="gamboostLSS.html#topic+cvrisk.mboostLSS">cvrisk.mboostLSS</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="gamboostLSS.html#topic+cvrisk.mboostLSS">cvrisk.mboostLSS</a></code> in 
package <code>gamboostLSS</code>.
</p>

<hr>
<h2 id='emotion'>EEG and EMG recordings in a computerised gambling study</h2><span id='topic+emotion'></span>

<h3>Description</h3>

<p>To analyse the functional relationship between electroencephalography (EEG) and facial electromyography (EMG), Gentsch et al. (2014) simultaneously recorded EEG and EMG signals from 24 participants while they were playing a computerised gambling task. The given subset contains aggregated observations of 23 participants. Curves were averaged over each subject and each of the 8 study settings, resulting in 23 times 8 curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("emotion")</code></pre>


<h3>Format</h3>

<p>A list with the following 10 variables.
</p>

<dl>
<dt><code>power</code></dt><dd><p>factor variable with levels <em>high</em> and <em>low</em></p>
</dd>
<dt><code>game_outcome</code></dt><dd><p>factor variable with levels <em>gain</em> and <em>loss</em></p>
</dd>
<dt><code>control</code></dt><dd><p>factor variable with levels <em>high</em> and <em>low</em></p>
</dd>
<dt><code>subject</code></dt><dd><p>factor variable with 23 levels</p>
</dd>
<dt><code>EEG</code></dt><dd><p>matrix; EEG signal in wide format</p>
</dd>
<dt><code>EMG</code></dt><dd><p>matrix; EMG signal in wide format</p>
</dd>
<dt><code>s</code></dt><dd><p>time points for the functional covariate</p>
</dd>
<dt><code>t</code></dt><dd><p>time points for the functional response</p>
</dd>
</dl>



<h3>Details</h3>

<p>The aim is to explain potentials in the EMG signal by study settings as well as
the EEG signal (see Ruegamer et al., 2018).
</p>


<h3>Source</h3>

<p>Gentsch,  K.,  Grandjean,  D.  and  Scherer,  K.  R.  (2014)  Coherence  explored  between  emotion components:  Evidence from event-related potentials and facial electromyography. Biological Psychology, 98, 70-81.
</p>
<p>Ruegamer D., Brockhaus, S., Gentsch K., Scherer, K., Greven, S. (2018). Boosting factor-specific functional historical models for the detection of synchronization in bioelectrical signals. Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 621-642. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("emotion", package = "FDboost")

# fit function-on-scalar model with random effect and power effect
fos_random_power &lt;- FDboost(EMG ~ 1 + brandomc(subject, df = 2)
                            + bolsc(power, df = 2),
                            timeformula = ~ bbs(t, df = 3),
                            data = emotion)
## Not run:                             
# fit function-on-function model with intercept and historical EEG effect
# where limits specifies the used lag between EMG and EEG signal
fof_historical &lt;- FDboost(EMG ~ 1 + bhist(EEG, s = s, time = t,
                          limits = function(s,t) s &lt; t - 3),
                          timeformula = ~ bbs(t, df = 3), data = emotion,
                          control = boost_control(mstop = 200))                            

## End(Not run)
</code></pre>

<hr>
<h2 id='extract.blg'>Extract information of a base-learner</h2><span id='topic+extract.blg'></span>

<h3>Description</h3>

<p>Takes a base-learner and extracts information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blg'
extract(
  object,
  what = c("design", "penalty", "index"),
  asmatrix = FALSE,
  expand = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract.blg_+3A_object">object</code></td>
<td>
<p>a base-learner</p>
</td></tr>
<tr><td><code id="extract.blg_+3A_what">what</code></td>
<td>
<p>a character specifying the quantities to extract.
This can be a subset of &quot;design&quot; (default; design matrix), 
&quot;penalty&quot; (penalty matrix) and &quot;index&quot; (index of ties used to expand 
the design matrix)</p>
</td></tr>
<tr><td><code id="extract.blg_+3A_asmatrix">asmatrix</code></td>
<td>
<p>a logical indicating whether the the returned matrix should be 
coerced to a matrix (default) or if the returned object stays as it is 
(i.e., potentially a sparse matrix). This option is only applicable if <code>extract</code> 
returns matrices, i.e., <code>what = "design"</code> or <code>what = "penalty"</code>.</p>
</td></tr>
<tr><td><code id="extract.blg_+3A_expand">expand</code></td>
<td>
<p>a logical indicating whether the design matrix should be expanded 
(default: <code>FALSE</code>). This is useful if ties were taken into account either manually 
(via argument <code>index</code> in a base-learner) or automatically for data sets with many 
observations. <code>expand = TRUE</code> is equivalent to <code>extract(B)[extract(B, what = "index"),]</code> 
for a base-learner <code>B</code>.</p>
</td></tr>
<tr><td><code id="extract.blg_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="mboost.html#topic+methods">extract</a></code> for the <code>extract</code> function 
of the package <code>mboost</code>.
</p>

<hr>
<h2 id='factorize'>Factorize tensor product model</h2><span id='topic+factorize'></span><span id='topic+factorise'></span><span id='topic+factorize.FDboost'></span>

<h3>Description</h3>

<p>Factorize an FDboost tensor product model into the response and covariate parts 
</p>
<p style="text-align: center;"><code class="reqn">h_j(x, t) = \sum_{k} v_j^{(k)}(t) h_j^{(k)}(x), j = 1, ..., J,</code>
</p>

<p>for effect visualization as proposed in Stoecker, Steyer and Greven (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factorize(x, ...)

## S3 method for class 'FDboost'
factorize(x, newdata = NULL, newweights = 1, blwise = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factorize_+3A_x">x</code></td>
<td>
<p>a model object of class FDboost.</p>
</td></tr>
<tr><td><code id="factorize_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods.</p>
</td></tr>
<tr><td><code id="factorize_+3A_newdata">newdata</code></td>
<td>
<p>new data the factorization is based on. 
By default (<code>NULL</code>), the factorization is carried out on the data used for fitting.</p>
</td></tr>
<tr><td><code id="factorize_+3A_newweights">newweights</code></td>
<td>
<p>vector of the length of the data or length one, 
containing new weights used for factorization.</p>
</td></tr>
<tr><td><code id="factorize_+3A_blwise">blwise</code></td>
<td>
<p>logical, should the factorization be carried out base-learner-wise (<code>TRUE</code>, default)
or for the whole model simultaneously.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mboost infrastructure is used for handling the orthogonal response 
directions <code class="reqn">v_j^{(k)}(t)</code> in one <code>mboost</code>-object 
(with <code class="reqn">k</code> running over iteration indices) and the effects into the respective 
directions <code class="reqn">h_j^{(k)}(t)</code> in another <code>mboost</code>-object, 
both of subclass <code>FDboost_fac</code>. 
The number of boosting iterations of <code>FDboost_fac</code>-objects cannot be 
further increased as in regular <code>mboost</code>-objects.
</p>


<h3>Value</h3>

<p>a list of two mboost models of class <code>FDboost_fac</code> containing basis functions
for response and covariates, respectively, as base-learners.
</p>
<p>A factorized model
</p>


<h3>References</h3>

<p>Stoecker, A., Steyer L. and Greven, S. (2022):
Functional additive models on manifolds of planar shapes and forms
&lt;arXiv:2109.02624&gt;
</p>


<h3>See Also</h3>

<p>[FDboost_fac-class]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FDboost)

# generate irregular toy data -------------------------------------------------------

n &lt;- 100
m &lt;- 40
# covariates
x &lt;- seq(0,2,len = n)
# time &amp; id
set.seed(90384)
t &lt;- runif(n = n*m, -pi,pi)
id &lt;- sample(1:n, size = n*m, replace = TRUE)

# generate components
fx &lt;- ft &lt;- list()
fx[[1]] &lt;- exp(x)
d &lt;- numeric(2)
d[1] &lt;- sqrt(c(crossprod(fx[[1]])))
fx[[1]] &lt;- fx[[1]] / d[1]
fx[[2]] &lt;- -5*x^2
fx[[2]] &lt;- fx[[2]] - fx[[1]] * c(crossprod(fx[[1]], fx[[2]])) # orthogonalize fx[[2]]
d[2] &lt;- sqrt(c(crossprod(fx[[2]])))
fx[[2]] &lt;- fx[[2]] / d[2]
ft[[1]] &lt;- sin(t)
ft[[2]] &lt;- cos(t)
ft[[1]] &lt;- ft[[1]] / sqrt(sum(ft[[1]]^2))
ft[[2]] &lt;- ft[[2]] / sqrt(sum(ft[[2]]^2))

mu1 &lt;- d[1] * fx[[1]][id] * ft[[1]]
mu2 &lt;- d[2] * fx[[2]][id] * ft[[2]]
# add linear covariate
ft[[3]] &lt;- t^2 * sin(4*t) 
ft[[3]] &lt;- ft[[3]] - ft[[1]] * c(crossprod(ft[[1]], ft[[3]]))
ft[[3]] &lt;- ft[[3]] - ft[[2]] * c(crossprod(ft[[2]], ft[[3]]))
ft[[3]] &lt;- ft[[3]] / sqrt(sum(ft[[3]]^2))
set.seed(9234)
fx[[3]] &lt;- runif(0,3, n = length(x))
fx[[3]] &lt;- fx[[3]] - fx[[1]] * c(crossprod(fx[[1]], fx[[3]]))
fx[[3]] &lt;- fx[[3]] - fx[[2]] * c(crossprod(fx[[2]], fx[[3]]))
d[3] &lt;- sqrt(sum(fx[[3]]^2))
fx[[3]] &lt;- fx[[3]] / d[3]

mu3 &lt;- d[3] * fx[[3]][id] * ft[[3]]

mu &lt;- mu1 + mu2 + mu3
# add some noise
y &lt;- mu + rnorm(length(mu), 0, .01)
# and noise covariate
z &lt;- rnorm(n)

# fit FDboost model -------------------------------------------------------

dat &lt;- list(y = y, x = x, t = t, x_lin = fx[[3]], id = id)
m &lt;- FDboost(y ~ bbs(x, knots = 5, df = 2, differences = 0) + 
               # bbs(z, knots = 2, df = 2, differences = 0) + 
               bols(x_lin, intercept = FALSE, df = 2)
               , ~ bbs(t),
             id = ~ id, 
             offset = 0, #numInt = "Riemann", 
             control = boost_control(nu = 1), 
             data = dat)
MU &lt;- split(mu, id)
PRED &lt;- split(predict(m), id)
Ti &lt;- split(t, id)
t0 &lt;- seq(-pi, pi, length.out = 40)
MU &lt;- do.call(cbind, Map(function(mu, t) approx(t, mu, t0)$y,
          MU, Ti)) 
PRED &lt;- do.call(cbind, Map(function(mu, t) approx(t, mu, t0)$y,
            PRED, Ti))

opar &lt;- par(mfrow = c(2,2))
image(t0, x, MU)
contour(t0, x, MU, add = TRUE)
image(t0, x, PRED)
contour(t0, x, PRED, add = TRUE)
persp(t0, x, MU, zlim = range(c(MU, PRED), na.rm = TRUE))
persp(t0, x, PRED, zlim = range(c(MU, PRED), na.rm = TRUE))
par(opar)

# factorize model ---------------------------------------------------------

fac &lt;- factorize(m)

vi &lt;- as.data.frame(varimp(fac$cov))
# if(require(lattice))
#   barchart(variable ~ reduction, group = blearner, vi, stack = TRUE)

cbind(d^2, sort(vi$reduction, decreasing = TRUE)[1:3])


x_plot &lt;- list(x, x, fx[[3]])

cols &lt;- c("cornflowerblue", "darkseagreen", "darkred")
opar &lt;- par(mfrow = c(3,2))
wch &lt;- c(1,2,10)
for(w in 1:length(wch)) {
  plot.mboost(fac$resp, which = wch[w], col = "darkgrey", ask = FALSE,
       main = names(fac$resp$baselearner[wch[w]]))
  lines(sort(t), ft[[w]][order(t)]*max(d), col = cols[w], lty = 2)
  plot(fac$cov, which = wch[w], 
       main = names(fac$cov$baselearner[wch[w]]))
  points(x_plot[[w]], d[w] * fx[[w]] / max(d), col = cols[w], pch = 3)
}
par(opar)

# re-compose predictions
preds &lt;- lapply(fac, predict)
predf &lt;- rowSums(preds$resp * preds$cov[id, ])
PREDf &lt;- split(predf, id)
PREDf &lt;- do.call(cbind, Map(function(mu, t) approx(t, mu, t0)$y,
                            PREDf, Ti))
opar &lt;- par(mfrow = c(1,2))
image(t0,x, PRED, main = "original prediction")
contour(t0,x, PRED, add = TRUE)
image(t0,x,PREDf, main = "recomposed")
contour(t0,x, PREDf, add = TRUE)
par(opar)

stopifnot(all.equal(PRED, PREDf))

# check out other methods
set.seed(8399)
newdata_resp &lt;- list(t = sort(runif(60, min(t), max(t))))
a &lt;- predict(fac$resp, newdata = newdata_resp, which = 1:5)
plot(newdata_resp$t, a[, 1])
# coef method
cf &lt;- coef(fac$resp, which = 1)


# check factorization on a new dataset ------------------------------------

t_grid &lt;- seq(-pi,pi,len = 30)
x_grid &lt;- seq(0,2,len = 30)
x_lin_grid &lt;- seq(min(dat$x_lin), max(dat$x_lin), len = 30)

# use grid data for factorization
griddata &lt;- expand.grid(
  # time 
  t = t_grid,
  # covariates
  x = x_grid,
  x_lin = 0
)

griddata_lin &lt;- expand.grid(
  t = seq(-pi, pi, len = 30),
  x = 0,
  x_lin = x_lin_grid
)

griddata &lt;- rbind(griddata, griddata_lin)

griddata$id &lt;- as.numeric(factor(paste(griddata$x, griddata$x_lin, sep = ":")))

fac2 &lt;- factorize(m, newdata = griddata)

ratio &lt;- -max(abs(predict(fac$resp, which = 1))) / max(abs(predict(fac2$resp, which = 1)))

opar &lt;- par(mfrow = c(3,2))
wch &lt;- c(1,2,10)
for(w in 1:length(wch)) {
  plot.mboost(fac$resp, which = wch[w], col = "darkgrey", ask = FALSE,
                                main = names(fac$resp$baselearner[wch[w]]))
  
  lines(sort(griddata$t), 
        ratio*predict(fac2$resp, which = wch[w])[order(griddata$t)], 
        col = cols[w], lty = 2)
  plot(fac$cov, which = wch[w], 
       main = names(fac$cov$baselearner[wch[w]]))
  this_x &lt;- fac2$cov$model.frame(which = wch[w])[[1]][[1]]
    lines(sort(this_x), 1/ratio*predict(fac2$cov, which = wch[w])[order(this_x)], 
          col = cols[w], lty = 1)
}
par(opar)

# check predictions
p &lt;- predict(fac2$resp, which = 1)
library(FDboost)

# generate regular toy data --------------------------------------------------

n &lt;- 100
m &lt;- 40
# covariates
x &lt;- seq(0,2,len = n)
# time 
t &lt;- seq(-pi,pi,len = m)
# generate components
fx &lt;- ft &lt;- list()
fx[[1]] &lt;- exp(x)
d &lt;- numeric(2)
d[1] &lt;- sqrt(c(crossprod(fx[[1]])))
fx[[1]] &lt;- fx[[1]] / d[1]
fx[[2]] &lt;- -5*x^2
fx[[2]] &lt;- fx[[2]] - fx[[1]] * c(crossprod(fx[[1]], fx[[2]])) # orthogonalize fx[[2]]
d[2] &lt;- sqrt(c(crossprod(fx[[2]])))
fx[[2]] &lt;- fx[[2]] / d[2]
ft[[1]] &lt;- sin(t)
ft[[2]] &lt;- cos(t)
ft[[1]] &lt;- ft[[1]] / sqrt(sum(ft[[1]]^2))
ft[[2]] &lt;- ft[[2]] / sqrt(sum(ft[[2]]^2))
mu1 &lt;- d[1] * fx[[1]] %*% t(ft[[1]])
mu2 &lt;- d[2] * fx[[2]] %*% t(ft[[2]])
# add linear covariate
ft[[3]] &lt;- t^2 * sin(4*t) 
ft[[3]] &lt;- ft[[3]] - ft[[1]] * c(crossprod(ft[[1]], ft[[3]]))
ft[[3]] &lt;- ft[[3]] - ft[[2]] * c(crossprod(ft[[2]], ft[[3]]))
ft[[3]] &lt;- ft[[3]] / sqrt(sum(ft[[3]]^2))
set.seed(9234)
fx[[3]] &lt;- runif(0,3, n = length(x))
fx[[3]] &lt;- fx[[3]] - fx[[1]] * c(crossprod(fx[[1]], fx[[3]]))
fx[[3]] &lt;- fx[[3]] - fx[[2]] * c(crossprod(fx[[2]], fx[[3]]))
d[3] &lt;- sqrt(sum(fx[[3]]^2))
fx[[3]] &lt;- fx[[3]] / d[3]
mu3 &lt;- d[3] * fx[[3]] %*% t(ft[[3]])

mu &lt;- mu1 + mu2 + mu3
# add some noise
y &lt;- mu + rnorm(length(mu), 0, .01)
# and noise covariate
z &lt;- rnorm(n)

# fit FDboost model -------------------------------------------------------

dat &lt;- list(y = y, x = x, t = t, x_lin = fx[[3]])
m &lt;- FDboost(y ~ bbs(x, knots = 5, df = 2, differences = 0) + 
               # bbs(z, knots = 2, df = 2, differences = 0) + 
               bols(x_lin, intercept = FALSE, df = 2)
               , ~ bbs(t), offset = 0, 
             control = boost_control(nu = 1), 
             data = dat)

opar &lt;- par(mfrow = c(1,2))
image(t, x, t(mu))
contour(t, x, t(mu), add = TRUE)
image(t, x, t(predict(m)))
contour(t, x, t(predict(m)), add = TRUE)
par(opar)

# factorize model ---------------------------------------------------------

fac &lt;- factorize(m)

vi &lt;- as.data.frame(varimp(fac$cov))
# if(require(lattice))
#   barchart(variable ~ reduction, group = blearner, vi, stack = TRUE)

cbind(d^2, vi$reduction[c(1:2, 10)])


x_plot &lt;- list(x, x, fx[[3]])

cols &lt;- c("cornflowerblue", "darkseagreen", "darkred")
opar &lt;- par(mfrow = c(3,2))
wch &lt;- c(1,2,10)
for(w in 1:length(wch)) {
  plot.mboost(fac$resp, which = wch[w], col = "darkgrey", ask = FALSE,
       main = names(fac$resp$baselearner[wch[w]]))
  lines(t, ft[[w]]*max(d), col = cols[w], lty = 2)
  plot(fac$cov, which = wch[w], 
       main = names(fac$cov$baselearner[wch[w]]))
  points(x_plot[[w]], d[w] * fx[[w]] / max(d), col = cols[w], pch = 3)
}
par(opar)

# re-compose prediction
preds &lt;- lapply(fac, predict)
PREDSf &lt;- array(0, dim = c(nrow(preds$resp),nrow(preds$cov)))
for(i in 1:ncol(preds$resp))
  PREDSf &lt;- PREDSf + preds$resp[,i] %*% t(preds$cov[,i])

opar &lt;- par(mfrow = c(1,2))
image(t,x, t(predict(m)), main = "original prediction")
contour(t,x, t(predict(m)), add = TRUE)
image(t,x,PREDSf, main = "recomposed")
contour(t,x, PREDSf, add = TRUE)
par(opar)
# =&gt; matches
stopifnot(all.equal(as.numeric(t(predict(m))), as.numeric(PREDSf)))

# check out other methods
set.seed(8399)
newdata_resp &lt;- list(t = sort(runif(60, min(t), max(t))))
a &lt;- predict(fac$resp, newdata = newdata_resp, which = 1:5)
plot(newdata_resp$t, a[, 1])
# coef method
cf &lt;- coef(fac$resp, which = 1)

</code></pre>

<hr>
<h2 id='FDboost'>Model-based Gradient Boosting for Functional Response</h2><span id='topic+FDboost'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions, where component-wise models 
are utilized as base-learners in the case of functional responses. 
Scalar responses are treated as the special case where each functional response has 
only one observation. 
This function is a wrapper for <code>mboost</code>'s <code><a href="mboost.html#topic+mboost">mboost</a></code> and its 
siblings to fit models of the general form 
</p>
<p style="text-align: center;"><code class="reqn">\xi(Y_i(t) | X_i = x_i) = \sum_{j} h_j(x_i, t), i = 1, ..., N,</code>
</p>
 
<p>with a functional (but not necessarily continuous) response <code class="reqn">Y(t)</code>, 
transformation function <code class="reqn">\xi</code>, e.g., the expectation, the median or some quantile, 
and partial effects <code class="reqn">h_j(x_i, t)</code> depending on covariates <code class="reqn">x_i</code>  
and the current index of the response <code class="reqn">t</code>. The index of the response can 
be for example time.  
Possible effects are, e.g., a smooth intercept <code class="reqn">\beta_0(t)</code>, 
a linear functional effect <code class="reqn">\int x_i(s)\beta(s,t)ds</code>, 
potentially with integration limits depending on <code class="reqn">t</code>, 
smooth and linear effects of scalar covariates <code class="reqn">f(z_i,t)</code> or <code class="reqn">z_i \beta(t)</code>. 
A hands-on tutorial for the package can be found at &lt;doi:10.18637/jss.v094.i10&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FDboost(
  formula,
  timeformula,
  id = NULL,
  numInt = "equal",
  data,
  weights = NULL,
  offset = NULL,
  offset_control = o_control(),
  check0 = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FDboost_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. 
Per default no intercept is added, only a smooth offset, see argument <code>offset</code>. 
To add a smooth intercept, use 1, e.g., <code>y ~ 1</code> for a pure intercept model.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_timeformula">timeformula</code></td>
<td>
<p>one-sided formula for the specification of the effect over the index of the response. 
For functional response <code class="reqn">Y_i(t)</code> typically use <code>~ bbs(t)</code> to obtain smooth 
effects over <code class="reqn">t</code>. 
In the limiting case of <code class="reqn">Y_i</code> being a scalar response, 
use <code>~ bols(1)</code>, which sets up a base-learner for the scalar 1. 
Or use <code>timeformula = NULL</code>, then the scalar response is treated as scalar.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_id">id</code></td>
<td>
<p>defaults to NULL which means that all response trajectories are observed
on a common grid allowing to represent the response as a matrix. 
If the response is given in long format for observation-specific grids, <code>id</code> 
contains the information which observations belong to the same trajectory and must 
be supplied as a formula, <code>~ nameid</code>, where the variable <code>nameid</code> should 
contain integers 1, 2, 3, ..., N.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_numint">numInt</code></td>
<td>
<p>integration scheme for the integration of the loss function.
One of <code>c("equal", "Riemann")</code> meaning equal weights of 1 or 
trapezoidal Riemann weights.
Alternatively a vector of length <code>ncol(response)</code> containing  
positive weights can be specified.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_data">data</code></td>
<td>
<p>a data frame or list containing the variables in the model.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_weights">weights</code></td>
<td>
<p>only for internal use to specify resampling weights;
per default all weights are equal to 1.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_offset">offset</code></td>
<td>
<p>a numeric vector to be used as offset over the index of the response (optional).
If no offset is specified, per default <code>offset = NULL</code> which means that a 
smooth time-specific offset is computed and used before the model fit to center the data. 
If you do not want to use a time-specific offset, set <code>offset = "scalar"</code> to get an overall scalar offset, 
like in <code>mboost</code>.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_offset_control">offset_control</code></td>
<td>
<p>parameters for the estimation of the offset, 
defaults to <code>o_control()</code>, see <code><a href="#topic+o_control">o_control</a></code>.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_check0">check0</code></td>
<td>
<p>logical, for response in matrix form, i.e. response that is observed on a common grid, 
check the fitted effects for the sum-to-zero constraint 
<code class="reqn">h_j(x_i)(t) = 0</code> for all <code class="reqn">t</code> and give a warning if it is not fulfilled. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FDboost_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="mboost.html#topic+mboost">mboost</a></code>, 
including, <code>family</code> and <code>control</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In matrix representation of functional response and covariates each row 
represents one functional observation, e.g., <code>Y[i,t_g]</code> corresponds to <code class="reqn">Y_i(t_g)</code>, 
giving a &lt;number of curves&gt; by &lt;number of evaluations&gt; matrix. 
For the model fit, the matrix of the functional
response evaluations <code class="reqn">Y_i(t_g)</code> are stacked internally into one long vector. 
</p>
<p>If it is possible to represent the model as a generalized linear array model 
(Currie et al., 2006), the array structure is used for an efficient implementation, 
see <code><a href="mboost.html#topic+mboost">mboost</a></code>. This is only possible if the design 
matrix can be written as the Kronecker product of two marginal design 
matrices yielding a functional linear array model (FLAM), 
see Brockhaus et al. (2015) for details. 
The Kronecker product of two marginal bases is implemented in R-package mboost 
in the function <code>%O%</code>, see <code><a href="mboost.html#topic+baselearners">%O%</a></code>. 
</p>
<p>When <code>%O%</code> is called with a specification of <code>df</code> in both base-learners, 
e.g., <code>bbs(x1, df = df1) %O% bbs(t, df = df2)</code>, the global <code>df</code> for the 
Kroneckered base-learner is computed as <code>df = df1 * df2</code>. 
And thus the penalty has only one smoothness parameter lambda resulting in an isotropic penalty. 
A Kronecker product with anisotropic penalty is <code>%A%</code>, allowing for different 
amount of smoothness in the two directions, see <code><a href="#topic++25A+25">%A%</a></code>. 
If the formula contains base-learners connected by <code>%O%</code>, <code>%A%</code> or <code>%A0%</code>, 
those effects are not expanded with <code>timeformula</code>, allowing for model specifications 
with different effects in time-direction.   
</p>
<p>If the response is observed on curve-specific grids it must be supplied  
as a vector in long format and the argument <code>id</code> has  
to be specified (as formula!) to define which observations belong to which curve.  
In this case the base-learners are built as row tensor-products of marginal base-learners, 
see Scheipl et al. (2015) and Brockhaus et al. (2017), for details on how to set up the effects. 
The row tensor product of two marginal bases is implemented in R-package mboost 
in the function <code>%X%</code>, see <code><a href="mboost.html#topic+baselearners">%X%</a></code>. 
</p>
<p>A scalar response can be seen as special case of a functional response with only
one time-point, and thus it can be represented as FLAM with basis 1 in 
time-direction, use <code>timeformula = ~bols(1)</code>. In this case, a penalty in the 
time-direction is used, see Brockhaus et al. (2015) for details.  
Alternatively, the scalar response is fitted as scalar response, like in the function
<code><a href="mboost.html#topic+mboost">mboost</a></code> in package mboost. 
The advantage of using <code>FDboost</code> in that case 
is that methods for the functional base-learners are available, e.g., <code>plot</code>. 
</p>
<p>The desired regression type is specified by the <code>family</code>-argument, 
see the help-page of <code><a href="mboost.html#topic+mboost">mboost</a></code>. For example a mean regression model is obtained by  
<code>family = Gaussian()</code> which is the default or median regression 
by <code>family = QuantReg()</code>; 
see <code><a href="mboost.html#topic+Family">Family</a></code> for a list of implemented families. 
</p>
<p>With <code>FDboost</code> the following covariate effects can be estimated by specifying 
the following effects in the <code>formula</code>
(similar to function <code><a href="refund.html#topic+pffr">pffr</a></code> 
in R-package refund. 
The <code>timeformula</code> is used to expand the effects in <code>t</code>-direction. 
</p>

<ul>
<li><p> Linear functional effect of scalar (numeric or factor) covariate <code class="reqn">z</code> that varies 
smoothly over <code class="reqn">t</code>, i.e. <code class="reqn">z_i \beta(t)</code>, specified as
<code>bolsc(z)</code>, see <code><a href="#topic+bolsc">bolsc</a></code>, 
or for a group effect with mean zero use <code>brandomc(z)</code>.  
</p>
</li>
<li><p> Nonlinear effects  of a scalar covariate that vary smoothly over <code class="reqn">t</code>, 
i.e. <code class="reqn">f(z_i, t)</code>, specified as <code>bbsc(z)</code>, 
see <code><a href="#topic+bbsc">bbsc</a></code>. 
</p>
</li>
<li><p> (Nonlinear) effects of scalar covariates that are constant 
over <code class="reqn">t</code>, e.g., <code class="reqn">f(z_i)</code>, specified as <code>c(bbs(z))</code>, 
or <code class="reqn">\beta z_i</code>, specified as <code>c(bols(z))</code>.
</p>
</li>
<li><p> Interaction terms between two scalar covariates, e.g., <code class="reqn">z_i1 zi2 \beta(t)</code>, 
are specified as <code>bols(z1) %Xc% bols(z2)</code> and  
an interaction <code class="reqn">z_i1 f(zi2, t)</code> as <code>bols(z1) %Xc% bbs(z2)</code>, as 
<code>%Xc%</code> applies the sum-to-zero constraint to the desgin matrix of the tensor product 
built by <code>%Xc%</code>, see <code><a href="#topic++25Xc+25">%Xc%</a></code>.
</p>
</li>
<li><p> Function-on-function regression terms of functional covariates <code>x</code>, 
e.g., <code class="reqn">\int x_i(s)\beta(s,t)ds</code>, specified as <code>bsignal(x, s = s)</code>, 
using P-splines, see <code><a href="#topic+bsignal">bsignal</a></code>. 
Terms given by <code><a href="#topic+bfpc">bfpc</a></code> provide FPC-based effects of functional 
covariates, see <code><a href="#topic+bfpc">bfpc</a></code>. 
</p>
</li>
<li><p> Function-on-function regression terms of functional covariates <code>x</code> 
with integration limits <code class="reqn">[l(t), u(t)]</code> depending on <code class="reqn">t</code>,  
e.g., <code class="reqn">\int_[l(t), u(t)] x_i(s)\beta(s,t)ds</code>, specified as 
<code>bhist(x, s = s, time = t, limits)</code>. The <code>limits</code> argument defaults to
<code>"s&lt;=t"</code> which yields a historical effect with limits <code class="reqn">[min(t),t]</code>, 
see <code><a href="#topic+bhist">bhist</a></code>.
</p>
</li>
<li><p> Concurrent effects of functional covariates <code>x</code>
measured on the same grid as the response, i.e., <code class="reqn">x_i(s)\beta(t)</code>, 
are specified as <code>bconcurrent(x, s = s, time = t)</code>, 
see <code><a href="#topic+bconcurrent">bconcurrent</a></code>. 
</p>
</li>
<li><p> Interaction effects can be estimated as tensor product smooth, e.g., 
<code class="reqn"> z \int x_i(s)\beta(s,t)ds</code> as <code>bsignal(x, s = s) %X% bolsc(z)</code>
</p>
</li>
<li><p> For interaction effects with historical functional effects, e.g., 
<code class="reqn"> z_i \int_[l(t),u(t)] x_i(s)\beta(s,t)ds</code> the base-learner 
<code>bhistx</code> should be used instead of <code>bhist</code>, 
e.g., <code>bhistx(x, limits) %X% bolsc(z)</code>, see <code><a href="#topic+bhistx">bhistx</a></code>.
</p>
</li>
<li><p> Generally, the <code>c()</code>-notation can be used to get effects that are 
constant over the index of the functional response. 
</p>
</li>
<li><p> If the <code>formula</code> in <code>FDboost</code> contains base-learners connected by 
<code>%O%</code>, <code>%A%</code> or <code>%A0%</code>, those effects are not expanded with <code>timeformula</code>, 
allowing for model specifications with different effects in time-direction.  
</p>
</li></ul>
 
<p>In order to obtain a fair selection of base-learners, the same degrees of freedom (df) 
should be specified for all baselearners. If the number of df differs among the base-learners, 
the selection is biased towards more flexible base-learners with higher df as they are more 
likely to yield larger improvements of the fit. It is recommended to use 
a rather small number of df for all base-learners. 
It is not possible to specify df larger than the rank of the design matrix.
For base-learners with rank-deficient penalty, it is not possible to specify df smaller than the 
rank of the null space of the penalty (e.g., in <code>bbs</code> unpenalized part of P-splines). 
The df of the base-learners in an FDboost-object can be checked using <code>extract(object, "df")</code>, 
see <code><a href="mboost.html#topic+methods">extract</a></code>.  
</p>
<p>The most important tuning parameter of component-wise gradient boosting 
is the number of boosting iterations. It is recommended to use the number of 
boosting iterations as only tuning parameter, 
fixing the step-length at a small value (e.g., nu = 0.1). 
Note that the default number of boosting iterations is 100 which is arbitrary and in most 
cases not adequate (the optimal number of boosting iterations can considerably exceed 100). 
The optimal stopping iteration can be determined by resampling methods like
cross-validation or bootstrapping, see the function <code><a href="#topic+cvrisk.FDboost">cvrisk.FDboost</a></code> which searches 
the optimal stopping iteration on a grid, which in many cases has to be extended.
</p>


<h3>Value</h3>

<p>An object of class <code>FDboost</code> that inherits from <code>mboost</code>.
Special <code><a href="#topic+predict.FDboost">predict.FDboost</a></code>, <code><a href="#topic+coef.FDboost">coef.FDboost</a></code> and 
<code><a href="#topic+plot.FDboost">plot.FDboost</a></code> methods are available. 
The methods of <code><a href="mboost.html#topic+mboost">mboost</a></code> are available as well, 
e.g., <code><a href="mboost.html#topic+methods">extract</a></code>. 
The <code>FDboost</code>-object is a named list containing: 
</p>
<table>
<tr><td><code>...</code></td>
<td>
<p>all elements of an <code>mboost</code>-object</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>the name of the response</p>
</td></tr>
<tr><td><code>ydim</code></td>
<td>
<p>dimension of the response matrix, if the response is represented as such</p>
</td></tr>
<tr><td><code>yind</code></td>
<td>
<p>the observation (time-)points of the response, i.e. the evaluation points, 
with its name as attribute</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the data that was used for the model fit</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>the id variable of the response</p>
</td></tr>
<tr><td><code>predictOffset</code></td>
<td>
<p>the function to predict the smooth offset</p>
</td></tr>
<tr><td><code>offsetFDboost</code></td>
<td>
<p>offset as specified in call to FDboost</p>
</td></tr> 
<tr><td><code>offsetMboost</code></td>
<td>
<p>offset as given to mboost</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call to <code>FDboost</code></p>
</td></tr>
<tr><td><code>callEval</code></td>
<td>
<p>the evaluated function call to <code>FDboost</code> without data</p>
</td></tr>
<tr><td><code>numInt</code></td>
<td>
<p>value of argument <code>numInt</code> determining the numerical integration scheme</p>
</td></tr>
<tr><td><code>timeformula</code></td>
<td>
<p>the time-formula</p>
</td></tr>
<tr><td><code>formulaFDboost</code></td>
<td>
<p>the formula with which <code>FDboost</code> was called</p>
</td></tr>
<tr><td><code>formulaMboost</code></td>
<td>
<p>the formula with which <code>mboost</code> was called within <code>FDboost</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sarah Brockhaus, Torsten Hothorn
</p>


<h3>References</h3>

<p>Brockhaus, S., Ruegamer, D. and Greven, S. (2017):
Boosting Functional Regression Models with FDboost.
&lt;doi:10.18637/jss.v094.i10&gt;
</p>
<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300. 
</p>
<p>Brockhaus, S., Melcher, M., Leisch, F. and Greven, S. (2017): 
Boosting flexible functional regression models with a high number of functional historical effects,  
Statistics and Computing, 27(4), 913-926.   
</p>
<p>Currie, I.D., Durban, M. and Eilers P.H.C. (2006):  
Generalized linear array models with applications to multidimensional smoothing. 
Journal of the Royal Statistical Society, Series B-Statistical Methodology, 68(2), 259-280.
</p>
<p>Scheipl, F., Staicu, A.-M. and Greven, S. (2015):  
Functional additive mixed models, Journal of Computational and Graphical Statistics, 24(2), 477-501.
</p>


<h3>See Also</h3>

<p>Note that <a href="#topic+FDboost">FDboost</a> calls <code><a href="mboost.html#topic+mboost">mboost</a></code> directly.  
See, e.g., <code><a href="#topic+bsignal">bsignal</a></code> and <code><a href="#topic+bbsc">bbsc</a></code> 
for possible base-learners.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>######## Example for function-on-scalar-regression 
data("viscosity", package = "FDboost") 
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## fit median regression model with 100 boosting iterations,
## step-length 0.4 and smooth time-specific offset
## the factors are coded such that the effects are zero for each timepoint t
## no integration weights are used!
mod1 &lt;- FDboost(vis ~ 1 + bolsc(T_C, df = 2) + bolsc(T_A, df = 2),
               timeformula = ~ bbs(time, df = 4),
               numInt = "equal", family = QuantReg(),
               offset = NULL, offset_control = o_control(k_min = 9),
               data = viscosity, control=boost_control(mstop = 100, nu = 0.4))


  #### find optimal mstop over 5-fold bootstrap, small number of folds for example
  #### do the resampling on the level of curves
  
  ## possibility 1: smooth offset and transformation matrices are refitted 
  set.seed(123)
  appl1 &lt;- applyFolds(mod1, folds = cv(rep(1, length(unique(mod1$id))), B = 5), 
                      grid = 1:500)
  ## plot(appl1)
  mstop(appl1)
  mod1[mstop(appl1)]
  
  ## possibility 2: smooth offset is refitted, 
  ## computes oob-risk and the estimated coefficients on the folds
  set.seed(123)
  val1 &lt;- validateFDboost(mod1, folds = cv(rep(1, length(unique(mod1$id))), B = 5), 
                        grid = 1:500)
  ## plot(val1)
  mstop(val1)
  mod1[mstop(val1)]

  ## possibility 3: very efficient 
  ## using the function cvrisk; be careful to do the resampling on the level of curves
  folds1 &lt;- cvLong(id = mod1$id, weights = model.weights(mod1), B = 5)
  cvm1 &lt;- cvrisk(mod1, folds = folds1, grid = 1:500)
  ## plot(cvm1)
  mstop(cvm1)
  
## look at the model
summary(mod1)
coef(mod1)
plot(mod1)
plotPredicted(mod1, lwdPred = 2)


######## Example for scalar-on-function-regression 
data("fuelSubset", package = "FDboost")

## center the functional covariates per observed wavelength
fuelSubset$UVVIS &lt;- scale(fuelSubset$UVVIS, scale = FALSE)
fuelSubset$NIR &lt;- scale(fuelSubset$NIR, scale = FALSE)

## to make mboost:::df2lambda() happy (all design matrix entries &lt; 10)
## reduce range of argvals to [0,1] to get smaller integration weights
fuelSubset$uvvis.lambda &lt;- with(fuelSubset, (uvvis.lambda - min(uvvis.lambda)) / 
                                          (max(uvvis.lambda) - min(uvvis.lambda) ))
fuelSubset$nir.lambda &lt;- with(fuelSubset, (nir.lambda - min(nir.lambda)) / 
                                          (max(nir.lambda) - min(nir.lambda) )) 

## model fit with scalar response 
## include no intercept as all base-learners are centered around 0
mod2 &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bsignal(NIR, nir.lambda, knots = 40, df = 4, check.ident = FALSE), 
               timeformula = NULL, data = fuelSubset, control = boost_control(mstop = 200)) 
               
## additionally include a non-linear effect of the scalar variable h2o 
mod2s &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bsignal(NIR, nir.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bbs(h2o, df = 4), 
               timeformula = NULL, data = fuelSubset, control = boost_control(mstop = 200)) 
               
## alternative model fit as FLAM model with scalar response; as timeformula = ~ bols(1)  
## adds a penalty over the index of the response, i.e., here a ridge penalty
## thus, mod2f and mod2 have different penalties 
mod2f &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots = 40, df = 4, check.ident = FALSE) 
               + bsignal(NIR, nir.lambda, knots = 40, df = 4, check.ident = FALSE), 
               timeformula = ~ bols(1), data = fuelSubset, control = boost_control(mstop = 200))
               
   
  ## bootstrap to find optimal mstop takes some time
  set.seed(123)      
  folds2 &lt;- cv(weights = model.weights(mod2), B = 10)     
  cvm2 &lt;- cvrisk(mod2, folds = folds2, grid = 1:1000)
  mstop(cvm2) ## mod2[327]
  summary(mod2) 
  ## plot(mod2)


## Example for function-on-function-regression 
if(require(fda)){

  data("CanadianWeather", package = "fda")
  CanadianWeather$l10precip &lt;- t(log(CanadianWeather$monthlyPrecip))
  CanadianWeather$temp &lt;- t(CanadianWeather$monthlyTemp)
  CanadianWeather$region &lt;- factor(CanadianWeather$region)
  CanadianWeather$month.s &lt;- CanadianWeather$month.t &lt;- 1:12
  
  ## center the temperature curves per time-point
  CanadianWeather$temp &lt;- scale(CanadianWeather$temp, scale = FALSE)
  rownames(CanadianWeather$temp) &lt;- NULL ## delete row-names
  
  ## fit model with cyclic splines over the year
  mod3 &lt;- FDboost(l10precip ~ bols(region, df = 2.5, contrasts.arg = "contr.dummy") 
                   + bsignal(temp, month.s, knots = 11, cyclic = TRUE, 
                             df = 2.5, boundary.knots = c(0.5,12.5), check.ident = FALSE), 
                  timeformula = ~ bbs(month.t, knots = 11, cyclic = TRUE, 
                                      df = 3, boundary.knots = c(0.5, 12.5)), 
                  offset = "scalar", offset_control = o_control(k_min = 5), 
                  control = boost_control(mstop = 60), 
                  data = CanadianWeather) 
 
                   
   #### find the optimal mstop over 5-fold bootstrap 
   ## using the function applyFolds 
   set.seed(123)
   folds3 &lt;- cv(rep(1, length(unique(mod3$id))), B = 5)
   appl3 &lt;- applyFolds(mod3, folds = folds3, grid = 1:200)
 
   ## use function cvrisk; be careful to do the resampling on the level of curves
   set.seed(123)
   folds3long &lt;- cvLong(id = mod3$id, weights = model.weights(mod3), B = 5)
   cvm3 &lt;- cvrisk(mod3, folds = folds3long, grid = 1:200)
   mstop(cvm3) ## mod3[64]
   
   summary(mod3)
   ## plot(mod3, pers = TRUE)
 
}

######## Example for functional response observed on irregular grid
######## Delete part of observations in viscosity data-set
data("viscosity", package = "FDboost")
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## only keep one eighth of the observation points
set.seed(123)
selectObs &lt;- sort(sample(x = 1:(64*46), size = 64*46/4, replace = FALSE))
dataIrregular &lt;- with(viscosity, list(vis = c(vis)[selectObs], 
                                      T_A = T_A, T_C = T_C,  
                                      time = rep(time, each = 64)[selectObs], 
                                      id = rep(1:64, 46)[selectObs]))

## fit median regression model with 50 boosting iterations,
## step-length 0.4 and smooth time-specific offset
## the factors are in effect coding -1, 1 for the levels
## no integration weights are used!
mod4 &lt;- FDboost(vis ~ 1 + bols(T_C, contrasts.arg = "contr.sum", intercept = FALSE)
                + bols(T_A, contrasts.arg = "contr.sum", intercept=FALSE),
                timeformula = ~ bbs(time, lambda = 100), id = ~id, 
                numInt = "Riemann", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = dataIrregular, control = boost_control(mstop = 50, nu = 0.4))
## summary(mod4)
## plot(mod4)
## plotPredicted(mod4, lwdPred = 2)


  ## Find optimal mstop, small grid/low B for a fast example
  set.seed(123)
  folds4 &lt;- cv(rep(1, length(unique(mod4$id))), B = 3)
  appl4 &lt;- applyFolds(mod4, folds = folds4, grid = 1:50)
  ## val4 &lt;- validateFDboost(mod4, folds = folds4, grid = 1:50)

  set.seed(123)
  folds4long &lt;- cvLong(id = mod4$id, weights = model.weights(mod4), B = 3)
  cvm4 &lt;- cvrisk(mod4, folds = folds4long, grid = 1:50)
  mstop(cvm4)


## Be careful if you want to predict newdata with irregular response,  
## as the argument index is not considered in the prediction of newdata. 
## Thus, all covariates have to be repeated according to the number of observations 
## in each response trajectroy. 
## Predict four response curves with full time-observations 
## for the four combinations of T_A and T_C. 
newd &lt;- list(T_A = factor(c(1,1,2,2), levels = 1:2, 
                        labels = c("low", "high"))[rep(1:4, length(viscosity$time))], 
             T_C = factor(c(1,2,1,2), levels = 1:2, 
                        labels = c("low", "high"))[rep(1:4, length(viscosity$time))], 
             time = rep(viscosity$time, 4))
             
pred &lt;- predict(mod4, newdata = newd)
## funplot(x = rep(viscosity$time, 4), y = pred, id = rep(1:4, length(viscosity$time)))
                  
                
</code></pre>

<hr>
<h2 id='FDboost_fac-class'>'FDboost_fac' S3 class for factorized FDboost model components</h2><span id='topic+FDboost_fac-class'></span>

<h3>Description</h3>

<p>Model factorization with 'factorize()' decomposes an
'FDboost' model into two objects of class 'FDboost_fac' - one for the
response and one for the covariate predictor. The first is essentially
an 'FDboost' object and the second an 'mboost' object, however,
in a 'read-only' mode and slightly adjusted methods (method defaults).
</p>


<h3>See Also</h3>

<p>[factorize(), factorize.FDboost()]
</p>

<hr>
<h2 id='FDboostLSS'>Model-based Gradient Boosting for Functional GAMLSS</h2><span id='topic+FDboostLSS'></span>

<h3>Description</h3>

<p>Function for fitting generalized additive models for location, scale and shape (GAMLSS)  
with functional data using component-wise gradient boosting, for details see 
Brockhaus et al. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FDboostLSS(
  formula,
  timeformula,
  data = list(),
  families = GaussianLSS(),
  control = boost_control(),
  weights = NULL,
  method = c("cyclic", "noncyclic"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FDboostLSS_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. 
If <code>formula</code> is a single formula, the same formula is used for all distribution parameters. 
<code>formula</code> can also be a (named) list, where each list element corresponds to one distribution 
parameter of the GAMLSS distribution. The names must be the same as in the <code>families</code>.</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_timeformula">timeformula</code></td>
<td>
<p>one-sided formula for the expansion over the index of the response. 
For a functional response <code class="reqn">Y_i(t)</code> typically <code>~bbs(t)</code> to obtain a smooth 
expansion of the effects along <code>t</code>. In the limiting case that <code class="reqn">Y_i</code> is a scalar response
use <code>~bols(1)</code>, which sets up a base-learner for the scalar 1. 
Or you can use <code>timeformula=NULL</code>, then the scalar response is treated as scalar. 
Analogously to <code>formula</code>, <code>timeformula</code> can either be a one-sided formula or 
a named list of one-sided formulas.</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_data">data</code></td>
<td>
<p>a data frame or list containing the variables in the model.</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_families">families</code></td>
<td>
<p>an object of class <code>families</code>. It can be either one of the pre-defined distributions 
that come along with the package <code>gamboostLSS</code> or a new distribution specified by the user 
(see <code><a href="gamboostLSS.html#topic+Families">Families</a></code> for details). 
Per default, the two-parametric <code><a href="gamboostLSS.html#topic+GaussianLSS">GaussianLSS</a></code> family is used.</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_control">control</code></td>
<td>
<p>a list of parameters controlling the algorithm. 
For more details see <code><a href="mboost.html#topic+boost_control">boost_control</a></code>.</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_weights">weights</code></td>
<td>
<p>does not work!</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_method">method</code></td>
<td>
<p>fitting method, currently two methods are supported: 
<code>"cyclic"</code> (see Mayr et al., 2012) and <code>"noncyclic"</code> 
(algorithm with inner loss of Thomas et al., 2018).</p>
</td></tr>
<tr><td><code id="FDboostLSS_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+FDboost">FDboost</a></code>, 
including, <code>family</code> and <code>control</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the theory of GAMLSS, see Rigby and Stasinopoulos (2005). 
<code>FDboostLSS</code> calls <code>FDboost</code> to fit the distribution parameters of a GAMLSS - 
a functional boosting model is fitted for each parameter of the response distribution.  
In <code><a href="gamboostLSS.html#topic+mboostLSS">mboostLSS</a></code>, details on boosting of GAMLSS based on 
Mayr et al. (2012) and Thomas et al. (2018) are given.   
In <code><a href="#topic+FDboost">FDboost</a></code>, details on boosting regression models with functional variables 
are given (Brockhaus et al., 2015, Brockhaus et al., 2017).
</p>


<h3>Value</h3>

<p>An object of class <code>FDboostLSS</code> that inherits from <code>mboostLSS</code>. 
The <code>FDboostLSS</code>-object is a named list containing one list entry per distribution parameter
and some attributes. The list is named like the parameters, e.g. mu and sigma, 
if the parameters mu and sigma are modeled. Each list-element is an object of class <code>FDboost</code>.
</p>


<h3>Author(s)</h3>

<p>Sarah Brockhaus
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015). 
The functional linear array model. Statistical Modelling, 15(3), 279-300.
</p>
<p>Brockhaus, S., Melcher, M., Leisch, F. and Greven, S. (2017): 
Boosting flexible functional regression models with a high number of functional historical effects,  
Statistics and Computing, 27(4), 913-926.
</p>
<p>Brockhaus, S., Fuest, A., Mayr, A. and Greven, S. (2018): 
Signal regression models for location, scale and shape with an application to stock returns. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 67, 665-686. 
</p>
<p>Mayr, A., Fenske, N., Hofner, B., Kneib, T. and Schmid, M. (2012): 
Generalized additive models for location, scale and shape for high-dimensional 
data - a flexible approach based on boosting. 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 61(3), 403-427. 
</p>
<p>Rigby, R. A. and D. M. Stasinopoulos (2005):  
Generalized additive models for location, scale and shape (with discussion). 
Journal of the Royal Statistical Society: Series C (Applied Statistics), 54(3), 507-554. 
</p>
<p>Thomas, J., Mayr, A., Bischl, B., Schmid, M., Smith, A., and Hofner, B. (2018), 
Gradient boosting for distributional regression - faster tuning and improved 
variable selection via noncyclical updates. 
Statistics and Computing, 28, 673-687. 
</p>
<p>Stoecker, A., Brockhaus, S., Schaffer, S., von Bronk, B., Opitz, M., and Greven, S. (2019): 
Boosting Functional Response Models for Location, Scale and Shape with an Application to Bacterial Competition. 
<a href="https://arxiv.org/abs/1809.09881">https://arxiv.org/abs/1809.09881</a>
</p>


<h3>See Also</h3>

<p>Note that <code>FDboostLSS</code> calls <code><a href="#topic+FDboost">FDboost</a></code> directly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########### simulate Gaussian scalar-on-function data
n &lt;- 500 ## number of observations
G &lt;- 120 ## number of observations per functional covariate
set.seed(123) ## ensure reproducibility
z &lt;- runif(n) ## scalar covariate
z &lt;- z - mean(z)
s &lt;- seq(0, 1, l=G) ## index of functional covariate
## generate functional covariate
if(require(splines)){
   x &lt;- t(replicate(n, drop(bs(s, df = 5, int = TRUE) %*% runif(5, min = -1, max = 1))))
}else{
  x &lt;- matrix(rnorm(n*G), ncol = G, nrow = n)
}
x &lt;- scale(x, center = TRUE, scale = FALSE) ## center x per observation point

mu &lt;- 2 + 0.5*z + (1/G*x) %*% sin(s*pi)*5 ## true functions for expectation
sigma &lt;- exp(0.5*z - (1/G*x) %*% cos(s*pi)*2) ## for standard deviation

y &lt;- rnorm(mean = mu, sd = sigma, n = n) ## draw respone y_i ~ N(mu_i, sigma_i)

## save data as list containing s as well 
dat_list &lt;- list(y = y, z = z, x = I(x), s = s)

## model fit with noncyclic algorithm assuming Gaussian location scale model 
m_boost &lt;- FDboostLSS(list(mu = y ~ bols(z, df = 2) + bsignal(x, s, df = 2, knots = 16), 
                           sigma = y ~ bols(z, df = 2) + bsignal(x, s, df = 2, knots = 16)), 
                           timeformula = NULL, data = dat_list, method = "noncyclic")
summary(m_boost)


 if(require(gamboostLSS)){
  ## find optimal number of boosting iterations on a grid in 1:1000
  ## using 5-fold bootstrap
  ## takes some time, easy to parallelize on Linux
  set.seed(123) 
  cvr &lt;- cvrisk(m_boost, folds = cv(model.weights(m_boost[[1]]), B = 5),
                grid = 1:1000, trace = FALSE)
  ## use model at optimal stopping iterations 
  m_boost &lt;- m_boost[mstop(cvr)] ## 832
   
  ## plot smooth effects of functional covariates for mu and sigma
  oldpar &lt;- par(mfrow = c(1,2))
  plot(m_boost$mu, which = 2, ylim = c(0,5))
  lines(s, sin(s*pi)*5, col = 3, lwd = 2)
  plot(m_boost$sigma, which = 2, ylim = c(-2.5,2.5))
  lines(s, -cos(s*pi)*2, col = 3, lwd = 2)
  par(oldpar)
 }

</code></pre>

<hr>
<h2 id='fitted.FDboost'>Fitted values of a boosted functional regression model</h2><span id='topic+fitted.FDboost'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object and computes the fitted values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
fitted(object, toFDboost = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted.FDboost_+3A_object">object</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="fitted.FDboost_+3A_tofdboost">toFDboost</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. In case of regular response in wide format 
(i.e., response is supplied as matrix): should the predictions be returned as matrix, or list 
of matrices instead of vectors</p>
</td></tr>
<tr><td><code id="fitted.FDboost_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to <code><a href="#topic+predict.FDboost">predict.FDboost</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix or vector of fitted values
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit.
</p>

<hr>
<h2 id='fuelSubset'>Spectral data of fossil fuels</h2><span id='topic+fuelSubset'></span>

<h3>Description</h3>

<p>For 129 laboratory samples of fossil fuels the heat value and the humidity were
determined together with two spectra.
One spectrum is ultraviolet-visible (UV-VIS), measured at 1335 wavelengths in
the range of 250.4 to 878.4 nanometer (nm), the other a near infrared  spectrum
(NIR) measured at 2307 wavelengths in the range of 800.4 to 2779.0 nm.
<code>fuelSubset</code> is a subset of the original dataset containing only 10% of
the original measures of the spectra, resulting in 231 measures of the
NIR spectrum and 134 measures of the UVVIS spectrum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("fuelSubset")</code></pre>


<h3>Format</h3>

<p>A data list with 129 observations on the following 7 variables.
</p>

<dl>
<dt><code>heatan</code></dt><dd><p>heat value in mega joule (mJ)</p>
</dd>
<dt><code>h2o</code></dt><dd><p>humidity in percent</p>
</dd>
<dt><code>NIR</code></dt><dd><p>near infrared spectrum (NIR) </p>
</dd>
<dt><code>UVVIS</code></dt><dd><p>ultraviolet-visible spectrum (UV-VIS)</p>
</dd>
<dt><code>nir.lambda</code></dt><dd><p>wavelength of NIR spectrum in nm</p>
</dd>
<dt><code>uvvis.lambda</code></dt><dd><p>wavelength of UV-VIS spectrum in nm</p>
</dd>
<dt><code>h2o.fit</code></dt><dd><p>predicted values of humidity</p>
</dd>
</dl>



<h3>Details</h3>

<p>The aim is to predict the heat value using the spectral data. The variable
<code>h2o.fit</code> was generated by a functional linear regression model, using
both spectra and their derivatives as predictors.
</p>


<h3>Source</h3>

<p>Siemens AG
</p>
<p>Fuchs, K., Scheipl, F. &amp; Greven, S. (2015), Penalized scalar-on-functions
regression with interaction term. Computational Statistics and Data Analysis. 81, 38-51. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    data("fuelSubset", package = "FDboost")
    
    ## center the functional covariates per observed wavelength
    fuelSubset$UVVIS &lt;- scale(fuelSubset$UVVIS, scale = FALSE)
    fuelSubset$NIR &lt;- scale(fuelSubset$NIR, scale = FALSE)

    ## to make mboost::df2lambda() happy (all design matrix entries &lt; 10)
    ## reduce range of argvals to [0,1] to get smaller integration weights
    fuelSubset$uvvis.lambda &lt;- with(fuelSubset, (uvvis.lambda - min(uvvis.lambda)) /
                                          (max(uvvis.lambda) - min(uvvis.lambda) ))
    fuelSubset$nir.lambda &lt;- with(fuelSubset, (nir.lambda - min(nir.lambda)) /
                                          (max(nir.lambda) - min(nir.lambda) ))


    ### fit mean regression model with 100 boosting iterations,
    ### step-length 0.1 and
    mod &lt;- FDboost(heatan ~ bsignal(UVVIS, uvvis.lambda, knots=40, df=4, check.ident=FALSE)
                   + bsignal(NIR, nir.lambda, knots=40, df=4, check.ident=FALSE),
                   timeformula = NULL, data = fuelSubset)
    summary(mod)
    ## plot(mod)
</code></pre>

<hr>
<h2 id='funMRD'>Functional MRD</h2><span id='topic+funMRD'></span>

<h3>Description</h3>

<p>Calculates the functional MRD for a fitted FDboost-object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funMRD(object, overTime = TRUE, breaks = object$yind, global = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funMRD_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object with regular response</p>
</td></tr>
<tr><td><code id="funMRD_+3A_overtime">overTime</code></td>
<td>
<p>per default the functional MRD is calculated over time
if <code>overTime=FALSE</code>, the MRD is calculated per curve</p>
</td></tr>
<tr><td><code id="funMRD_+3A_breaks">breaks</code></td>
<td>
<p>an optional vector or number giving the time-points at which the model is evaluated.
Can be specified as number of equidistant time-points or as vector of time-points.
Defaults to the index of the response in the model.</p>
</td></tr>
<tr><td><code id="funMRD_+3A_global">global</code></td>
<td>
<p>logical. defaults to <code>FALSE</code>, 
if TRUE the global MRD like in a normal linear model is calculated</p>
</td></tr>
<tr><td><code id="funMRD_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formula to calculate MRD over time, <code>overTime=TRUE</code>: <br />
<code class="reqn"> MRD(t) = n^{-1} \sum_i |Y_i(t) - \hat{Y}_i(t)| / |Y_i(t)| </code> 
</p>
<p>Formula to calculate MRD over subjects, <code>overTime=FALSE</code>: <br />
<code class="reqn"> MRD_{i} = \int |Y_i(t) - \hat{Y}_i(t)| / |Y_i(t)| dt  \approx G^{-1} \sum_g |Y_i(t_g) - \hat{Y}_i(t_g)| / |Y_i(t)|</code>
</p>


<h3>Value</h3>

<p>Returns a vector with the calculated MRD and some extra information in attributes.
</p>


<h3>Note</h3>

<p><code>breaks</code> cannot be changed in the case the <code>bsignal()</code> 
is used over the same domain
as the response! In that case you would have to rename the index of the response or that 
of the covariates.
</p>

<hr>
<h2 id='funMSE'>Functional MSE</h2><span id='topic+funMSE'></span>

<h3>Description</h3>

<p>Calculates the functional MSE for a fitted FDboost-object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funMSE(
  object,
  overTime = TRUE,
  breaks = object$yind,
  global = FALSE,
  relative = FALSE,
  root = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funMSE_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="funMSE_+3A_overtime">overTime</code></td>
<td>
<p>per default the functional R-squared is calculated over time
if <code>overTime=FALSE</code>, the R-squared is calculated per curve</p>
</td></tr>
<tr><td><code id="funMSE_+3A_breaks">breaks</code></td>
<td>
<p>an optional vector or number giving the time-points at which the model is evaluated.
Can be specified as number of equidistant time-points or as vector of time-points.
Defaults to the index of the response in the model.</p>
</td></tr>
<tr><td><code id="funMSE_+3A_global">global</code></td>
<td>
<p>logical. defaults to <code>FALSE</code>, 
if TRUE the global R-squared like in a normal linear model is calculated</p>
</td></tr>
<tr><td><code id="funMSE_+3A_relative">relative</code></td>
<td>
<p>logical. defaults to <code>FALSE</code>. If <code>TRUE</code> the MSE is standardized
by the global variance of the response <br />
<code class="reqn"> n^{-1} \int  \sum_i (Y_i(t) - \bar{Y})^2 dt \approx  G^{-1} n^{-1} \sum_g \sum_i (Y_i(t_g) - \bar{Y})^2 </code></p>
</td></tr>
<tr><td><code id="funMSE_+3A_root">root</code></td>
<td>
<p>take the square root of the MSE</p>
</td></tr>
<tr><td><code id="funMSE_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formula to calculate MSE over time, <code>overTime=TRUE</code>: <br />
<code class="reqn"> MSE(t) = n^{-1} \sum_i (Y_i(t) - \hat{Y}_i(t))^2 </code> 
</p>
<p>Formula to calculate MSE over subjects, <code>overTime=FALSE</code>: <br />
<code class="reqn"> MSE_i = \int (Y_i(t) - \hat{Y}_i(t))^2 dt  \approx G^{-1} \sum_g (Y_i(t_g) - \hat{Y}_i(t_g))^2</code>
</p>


<h3>Value</h3>

<p>Returns a vector with the calculated MSE and some extra information in attributes.
</p>


<h3>Note</h3>

<p><code>breaks</code> cannot be changed in the case the <code>bsignal()</code> 
is used over the same domain
as the response! In that case you would have to rename the index of the response or that 
of the covariates.
</p>

<hr>
<h2 id='funplot'>Plot functional data with linear interpolation of missing values</h2><span id='topic+funplot'></span>

<h3>Description</h3>

<p>Plot functional data with linear interpolation of missing values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funplot(x, y, id = NULL, rug = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funplot_+3A_x">x</code></td>
<td>
<p>optional, time-vector for plotting</p>
</td></tr>
<tr><td><code id="funplot_+3A_y">y</code></td>
<td>
<p>matrix of functional data with functions in rows and measured times in columns; 
or vector or functional observations, in this case id has to be specified</p>
</td></tr>
<tr><td><code id="funplot_+3A_id">id</code></td>
<td>
<p>defaults to NULL for y matrix, is id-variables for y in long format</p>
</td></tr>
<tr><td><code id="funplot_+3A_rug">rug</code></td>
<td>
<p>logical. Should rugs be plotted? Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="funplot_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code><a href="graphics.html#topic+matplot">matplot</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All observations are marked by a small cross (<code>pch=3</code>).
Missing values are imputed by linear interpolation. Parts that are
interpolated are plotted by dotted lines, parts with non-missing values as solid lines.
</p>


<h3>Value</h3>

<p>see <code><a href="graphics.html#topic+matplot">matplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### examples for regular data in wide format
data(viscosity)
with(viscosity, funplot(timeAll, visAll, pch=20))
if(require(fda)){
  with(fda::growth, funplot(age, t(hgtm)))
}

</code></pre>

<hr>
<h2 id='funRsquared'>Functional R-squared</h2><span id='topic+funRsquared'></span>

<h3>Description</h3>

<p>Calculates the functional R-squared for a fitted FDboost-object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funRsquared(object, overTime = TRUE, breaks = object$yind, global = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funRsquared_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="funRsquared_+3A_overtime">overTime</code></td>
<td>
<p>per default the functional R-squared is calculated over time
if <code>overTime=FALSE</code>, the R-squared is calculated per curve</p>
</td></tr>
<tr><td><code id="funRsquared_+3A_breaks">breaks</code></td>
<td>
<p>an optional vector or number giving the time-points at which the model is evaluated.
Can be specified as number of equidistant time-points or as vector of time-points.
Defaults to the index of the response in the model.</p>
</td></tr>
<tr><td><code id="funRsquared_+3A_global">global</code></td>
<td>
<p>logical. defaults to <code>FALSE</code>, 
if TRUE the global R-squared like in a normal linear model is calculated</p>
</td></tr>
<tr><td><code id="funRsquared_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>breaks</code> should be set to some grid, if there are many
missing values or time-points with very few observations in the dataset.
Otherwise at these points of t the variance will be almost 0 
(or even 0 if there is only one observation at a time-point),
and then the prediction by the local means <code class="reqn">\mu(t)</code> is locally very good.
The observations are interpolated linearly if necessary.
</p>
<p>Formula to calculate R-squared over time, <code>overTime=TRUE</code>: <br />
<code class="reqn">R^2(t) = 1 - \sum_{i}( Y_i(t) - \hat{Y}_i(t))^2 /  \sum_{i}( Y_i(t) - \bar{Y}(t) )^2 </code> 
</p>
<p>Formula to calculate R-squared over subjects, <code>overTime=FALSE</code>: <br />
<code class="reqn">R^2_i = 1 - \int (Y_i(t) - \hat{Y}_i(t))^2 dt /  \int (Y_i(t) - \bar{Y}_i )^2 dt </code>
</p>


<h3>Value</h3>

<p>Returns a vector with the calculated R-squared and some extra information in attributes.
</p>


<h3>Note</h3>

<p><code>breaks</code> cannot be changed in the case the <code>bsignal()</code> 
is used over the same domain
as the response! In that case you would have to rename the index of the response or that 
of the covariates.
</p>


<h3>References</h3>

<p>Ramsay, J., Silverman, B. (2006). Functional data analysis. 
Wiley Online Library. chapter 16.3
</p>

<hr>
<h2 id='getTime'>Generic functions to asses attributes of functional data objects</h2><span id='topic+getTime'></span><span id='topic+getId'></span><span id='topic+getX'></span><span id='topic+getArgvals'></span><span id='topic+getTimeLab'></span><span id='topic+getIdLab'></span><span id='topic+getXLab'></span><span id='topic+getArgvalsLab'></span>

<h3>Description</h3>

<p>Extract attributes of an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTime(object)

getId(object)

getX(object)

getArgvals(object)

getTimeLab(object)

getIdLab(object)

getXLab(object)

getArgvalsLab(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTime_+3A_object">object</code></td>
<td>
<p>an R-object, currently implemented for hmatrix and fmatrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extract the time variable <code>getTime</code>, the id<code>getId</code>, 
the functional covariate <code>getX</code>, its argument values <code>getArgvals</code>. 
Or the names of the different variables <code>getTimeLab</code>, 
<code>getIdLab</code>, <code>getXLab</code>, <code>getArgvalsLab</code>.
</p>


<h3>Value</h3>

<p>properties of a hmatrix or fmatrix
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmatrix">hmatrix</a></code> for the h.atrix class.
</p>

<hr>
<h2 id='getTime.hmatrix'>Extract attributes of hmatrix</h2><span id='topic+getTime.hmatrix'></span><span id='topic+getId.hmatrix'></span><span id='topic+getX.hmatrix'></span><span id='topic+getArgvals.hmatrix'></span><span id='topic+getTimeLab.hmatrix'></span><span id='topic+getXLab.hmatrix'></span><span id='topic+getArgvalsLab.hmatrix'></span><span id='topic+getIdLab.hmatrix'></span>

<h3>Description</h3>

<p>Extract attributes of an object of class <code>hmatrix</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmatrix'
getTime(object)

## S3 method for class 'hmatrix'
getId(object)

## S3 method for class 'hmatrix'
getX(object)

## S3 method for class 'hmatrix'
getArgvals(object)

## S3 method for class 'hmatrix'
getTimeLab(object)

## S3 method for class 'hmatrix'
getIdLab(object)

## S3 method for class 'hmatrix'
getXLab(object)

## S3 method for class 'hmatrix'
getArgvalsLab(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTime.hmatrix_+3A_object">object</code></td>
<td>
<p>object of class hmatrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extract the time variable <code>getTime</code>, the id<code>getId</code>, 
the functional covariate <code>getX</code>, its argument values <code>getArgvals</code>. 
Or the names of the different variables <code>getTimeLab</code>, 
<code>getIdLab</code>, <code>getXLab</code>, <code>getArgvalsLab</code> for an object of class <code>hmatrix</code>.
</p>


<h3>Value</h3>

<p>properties of a hmatrix
</p>

<hr>
<h2 id='hmatrix'>A S3 class for univariate functional data on a common grid</h2><span id='topic+hmatrix'></span>

<h3>Description</h3>

<p>The hmatrix class represents data for a functional historical effect. 
The class is basically a matrix containing the time and the id for the observations of the 
functional response. The functional covariate is contained as attribute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmatrix(
  time,
  id,
  x,
  argvals = 1:ncol(x),
  timeLab = "t",
  idLab = "wideIndex",
  xLab = "x",
  argvalsLab = "s"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmatrix_+3A_time">time</code></td>
<td>
<p>set of argument values of the response in long format, 
i.e. at which <code>t</code> the response curve is observed</p>
</td></tr>
<tr><td><code id="hmatrix_+3A_id">id</code></td>
<td>
<p>specify to which curve the point belongs to, id from 1, 2, ..., n.</p>
</td></tr>
<tr><td><code id="hmatrix_+3A_x">x</code></td>
<td>
<p>matrix of functional covariate, each trajectory is in one row</p>
</td></tr>
<tr><td><code id="hmatrix_+3A_argvals">argvals</code></td>
<td>
<p>set of argument values, i.e., the common gird at which the functional covariate 
is observed, by default <code>1:ncol(x)</code></p>
</td></tr>
<tr><td><code id="hmatrix_+3A_timelab">timeLab</code></td>
<td>
<p>name of the time axis, by default <code>t</code></p>
</td></tr>
<tr><td><code id="hmatrix_+3A_idlab">idLab</code></td>
<td>
<p>name of the id variable, by default <code>wideIndex</code></p>
</td></tr>
<tr><td><code id="hmatrix_+3A_xlab">xLab</code></td>
<td>
<p>name of the functional variable, by default NULL</p>
</td></tr>
<tr><td><code id="hmatrix_+3A_argvalslab">argvalsLab</code></td>
<td>
<p>name of the argument for the covariate by default <code>s</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the hmatrix class the id has to run from i=1, 2, ..., n including all integers from 1 to n. 
The rows of the functional covariate x correspond to those observations.
</p>


<h3>Value</h3>

<p>An matrix object of type <code>"hmatrix"</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTime.hmatrix">getTime.hmatrix</a></code> to extract attributes, 
and ?&quot;[.hmatrix&quot; for the extract method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for a hmatrix object
t1 &lt;- rep((1:5)/2, each = 3)
id1 &lt;- rep(1:3, 5)
x1 &lt;- matrix(1:15, ncol = 5) 
s1 &lt;- (1:5)/2 
myhmatrix &lt;- hmatrix(time = t1, id = id1, x = x1, argvals = s1, 
                     timeLab = "t1", argvalsLab = "s1", xLab = "test")

# extract with [ keeps attributes 
# select observations of subjects 2 and 3
myhmatrixSub &lt;- myhmatrix[id1 %in% c(2, 3), ]  
str(myhmatrixSub)
getX(myhmatrixSub)
getX(myhmatrix)

# get time
myhmatrix[ , 1] # as column matrix as drop = FALSE
getTime(myhmatrix) # as vector

# get id
myhmatrix[ , 2] # as column matrix as drop = FALSE
getId(myhmatrix) # as vector

# subset hmatrix on the basis of an index, which is defined on the curve level
reweightData(data = list(hmat = myhmatrix), vars = "hmat", index = c(1, 1, 2))
# this keeps only the unique x values in attr(,'x') but multiplies the corresponding
# ids and times in the time id matrix 
# for bhistx baselearner, there may be an additional id variable for the tensor product
newdat &lt;- reweightData(data = list(hmat = myhmatrix, 
  repIDx = rep(1:nrow(attr(myhmatrix,'x')), length(attr(myhmatrix,"argvals")))), 
  vars = "hmat", index = c(1,1,2), idvars="repIDx")
length(newdat$repIDx) 

## use hmatrix within a data.frame
mydat &lt;- data.frame(I(myhmatrix), z=rnorm(3)[id1])
str(mydat)
str(mydat[id1 %in% c(2, 3), ])
str(myhmatrix[id1 %in% c(2, 3), ])

</code></pre>

<hr>
<h2 id='integrationWeights'>Functions to compute integration weights</h2><span id='topic+integrationWeights'></span><span id='topic+integrationWeightsLeft'></span>

<h3>Description</h3>

<p>Computes trapezoidal integration weights (Riemann sums) for a functional variable 
<code>X1</code> that has evaluation points <code>xind</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>integrationWeights(X1, xind, id = NULL)

integrationWeightsLeft(X1, xind, leftWeight = c("first", "mean", "zero"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="integrationWeights_+3A_x1">X1</code></td>
<td>
<p>for functional data that is observed on one common grid, 
a matrix containing the observations of the functional variable. 
For a functional variable that is observed on curve specific grids, a long vector.</p>
</td></tr>
<tr><td><code id="integrationWeights_+3A_xind">xind</code></td>
<td>
<p>evaluation points (index) of functional variable</p>
</td></tr>
<tr><td><code id="integrationWeights_+3A_id">id</code></td>
<td>
<p>defaults to <code>NULL</code>. Only necessary for response in long format. 
In this case <code>id</code> specifies which curves belong together.</p>
</td></tr>
<tr><td><code id="integrationWeights_+3A_leftweight">leftWeight</code></td>
<td>
<p>one of <code>c("mean", "first", "zero")</code>. With left Riemann sums 
different assumptions for the weight of the first observation are possible. 
The default is to use the mean over all integration weights, <code>"mean"</code>. 
Alternatively one can use the first integration weight, <code>"first"</code>, or 
use the distance to zero, <code>"zero"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>integrationWeights()</code> computes trapezoidal integration weights, 
that are symmetric. Per default those weights are used in the <code><a href="#topic+bsignal">bsignal</a></code>-base-learner. 
In the special case of evaluation points (<code>xind</code>) with equal distances, 
all integration weights are equal.
</p>
<p>The function <code>integrationWeightsLeft()</code> computes weights,
that take into account only the distance to the prior observation point. 
Thus one has to decide what to do with the first observation. 
The left weights are adequate for historical effects like in <code><a href="#topic+bhist">bhist</a></code>.
</p>


<h3>Value</h3>

<p>Matrix with integration
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bsignal">bsignal</a></code> and <code><a href="#topic+bhist">bhist</a></code> for the base-learners.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for trapezoidal integration weights
xind0 &lt;- seq(0,1,l = 5)
xind &lt;- c(0, 0.1, 0.3, 0.7, 1)
X1 &lt;- matrix(xind^2, ncol = length(xind0), nrow = 2)

# Regualar observation points
integrationWeights(X1, xind0)
# Irregular observation points
integrationWeights(X1, xind)

# with missing value
X1[1,2] &lt;- NA
integrationWeights(X1, xind0)
integrationWeights(X1, xind)

## Example for left integration weights
xind0 &lt;- seq(0,1,l = 5)
xind &lt;- c(0, 0.1, 0.3, 0.7, 1)
X1 &lt;- matrix(xind^2, ncol = length(xind0), nrow = 2)

# Regular observation points
integrationWeightsLeft(X1, xind0, leftWeight = "mean") 
integrationWeightsLeft(X1, xind0, leftWeight = "first") 
integrationWeightsLeft(X1, xind0, leftWeight = "zero")

# Irregular observation points
integrationWeightsLeft(X1, xind, leftWeight = "mean") 
integrationWeightsLeft(X1, xind, leftWeight = "first") 
integrationWeightsLeft(X1, xind, leftWeight = "zero")

# obervation points that do not start with 0
xind2 &lt;- xind + 0.5
integrationWeightsLeft(X1, xind2, leftWeight = "zero")
 
</code></pre>

<hr>
<h2 id='is.hmatrix'>Test to class of hmatrix</h2><span id='topic+is.hmatrix'></span>

<h3>Description</h3>

<p>is.hmatrix tests if its argument is an object of class hmatrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.hmatrix(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.hmatrix_+3A_object">object</code></td>
<td>
<p>object of class hmatrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical value
</p>

<hr>
<h2 id='mstop.validateFDboost'>Methods for objects of class validateFDboost</h2><span id='topic+mstop.validateFDboost'></span><span id='topic+print.validateFDboost'></span><span id='topic+plot.validateFDboost'></span><span id='topic+plotPredCoef'></span>

<h3>Description</h3>

<p>Methods for objects that are fitted to determine the optimal mstop and the 
prediction error of a model fitted by FDboost.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'validateFDboost'
mstop(object, riskopt = c("mean", "median"), ...)

## S3 method for class 'validateFDboost'
print(x, ...)

## S3 method for class 'validateFDboost'
plot(
  x,
  riskopt = c("mean", "median"),
  ylab = attr(x, "risk"),
  xlab = "Number of boosting iterations",
  ylim = range(x$oobrisk),
  which = 1,
  modObject = NULL,
  predictNA = FALSE,
  names.arg = NULL,
  ask = TRUE,
  ...
)

plotPredCoef(
  x,
  which = NULL,
  pers = TRUE,
  commonRange = TRUE,
  showNumbers = FALSE,
  showQuantiles = TRUE,
  ask = TRUE,
  terms = TRUE,
  probs = c(0.25, 0.5, 0.75),
  ylim = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mstop.validateFDboost_+3A_object">object</code></td>
<td>
<p>object of class <code>validateFDboost</code></p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_riskopt">riskopt</code></td>
<td>
<p>how the risk is minimized to obtain the optimal stopping iteration; 
defaults to the mean, can be changed to the median.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_...">...</code></td>
<td>
<p>additional arguments passed to callies.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_x">x</code></td>
<td>
<p>an object of class <code>validateFDboost</code>.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_xlab">xlab</code></td>
<td>
<p>label for x-axis</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_ylim">ylim</code></td>
<td>
<p>values for limits of y-axis</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_which">which</code></td>
<td>
<p>In the case of <code>plotPredCoef()</code> the subset of base-learners to take into account for plotting. 
In the case of <code>plot.validateFDboost()</code> the diagnostic plots that are given 
(1: empirical risk per fold as a funciton of the boosting iterations, 
2: empirical risk per fold, 3: MRD per fold, 
4: observed and predicted values, 5: residuals; 
2-5 for the model with the optimal number of boosting iterations).</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_modobject">modObject</code></td>
<td>
<p>if the original model object of class <code>FDboost</code> is given 
predicted values of the whole model can be compared to the predictions of the cross-validated models</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_predictna">predictNA</code></td>
<td>
<p>should missing values in the response be predicted? Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_names.arg">names.arg</code></td>
<td>
<p>names of the observed curves</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_ask">ask</code></td>
<td>
<p>defaults to <code>TRUE</code>, ask for next plot using <code>par(ask = ask)</code>  ?</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_pers">pers</code></td>
<td>
<p>plot coefficient surfaces as persp-plots? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_commonrange">commonRange</code></td>
<td>
<p>plot predicted coefficients on a common range, defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_shownumbers">showNumbers</code></td>
<td>
<p>show number of curve in plot of predicted coefficients, defaults to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_showquantiles">showQuantiles</code></td>
<td>
<p>plot the 0.05 and the 0.95 Quantile of coefficients in 1-dim effects.</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_terms">terms</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>; plot the added terms (default) or the coefficients?</p>
</td></tr>
<tr><td><code id="mstop.validateFDboost_+3A_probs">probs</code></td>
<td>
<p>vector of quantiles to be used in the plotting of 2-dimensional coefficients surfaces,
defaults to <code>probs = c(0.25, 0.5, 0.75)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>mstop.validateFDboost</code> extracts the optimal mstop by minimizing the 
mean (or the median) risk. 
<code>plot.validateFDboost</code> plots cross-validated risk, RMSE, MRD, measured and predicted values 
and residuals as determined by <code>validateFDboost</code>. The function <code>plotPredCoef</code> plots the 
coefficients that were estimated in the folds - only possible if the argument getCoefCV is <code>TRUE</code> in 
the call to <code>validateFDboost</code>.
</p>


<h3>Value</h3>

<p>No return value (plot method) or the object itself (print method)
</p>

<hr>
<h2 id='o_control'>Function to control estimation of smooth offset</h2><span id='topic+o_control'></span>

<h3>Description</h3>

<p>Function to control estimation of smooth offset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>o_control(k_min = 20, rule = 2, silent = TRUE, cyclic = FALSE, knots = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="o_control_+3A_k_min">k_min</code></td>
<td>
<p>maximal number of k in s()</p>
</td></tr>
<tr><td><code id="o_control_+3A_rule">rule</code></td>
<td>
<p>which rule to use in approx() of the response before calculating the 
global mean, rule=1 means no extrapolation, rule=2 means to extrapolate the 
closest non-missing value, see <code><a href="stats.html#topic+approxfun">approx</a></code></p>
</td></tr>
<tr><td><code id="o_control_+3A_silent">silent</code></td>
<td>
<p>print error messages of model fit?</p>
</td></tr>
<tr><td><code id="o_control_+3A_cyclic">cyclic</code></td>
<td>
<p>defaults to FALSE, if TRUE cyclic splines are used</p>
</td></tr>
<tr><td><code id="o_control_+3A_knots">knots</code></td>
<td>
<p>arguments knots passed to <code><a href="mgcv.html#topic+gam">gam</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with controls
</p>

<hr>
<h2 id='plot.bootstrapCI'>Methods for objects of class bootstrapCI</h2><span id='topic+plot.bootstrapCI'></span><span id='topic+print.bootstrapCI'></span>

<h3>Description</h3>

<p>Methods for objects that are fitted to compute bootstrap confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bootstrapCI'
plot(
  x,
  which = NULL,
  pers = TRUE,
  commonRange = TRUE,
  showNumbers = FALSE,
  showQuantiles = TRUE,
  ask = TRUE,
  probs = c(0.25, 0.5, 0.75),
  ylim = NULL,
  ...
)

## S3 method for class 'bootstrapCI'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bootstrapCI_+3A_x">x</code></td>
<td>
<p>an object of class <code>bootstrapCI</code>.</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_which">which</code></td>
<td>
<p>base-learners that are plotted</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_pers">pers</code></td>
<td>
<p>plot coefficient surfaces as persp-plots? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_commonrange">commonRange</code></td>
<td>
<p>plot predicted coefficients on a common range, defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_shownumbers">showNumbers</code></td>
<td>
<p>show number of curve in plot of predicted coefficients, defaults to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_showquantiles">showQuantiles</code></td>
<td>
<p>plot the 0.05 and the 0.95 Quantile of coefficients in 1-dim effects.</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_ask">ask</code></td>
<td>
<p>defaults to <code>TRUE</code>, ask for next plot using <code>par(ask = ask)</code>?</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_probs">probs</code></td>
<td>
<p>vector of quantiles to be used in the plotting of 2-dimensional coefficients surfaces,
defaults to <code>probs = c(0.25, 0.5, 0.75)</code></p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_ylim">ylim</code></td>
<td>
<p>values for limits of y-axis</p>
</td></tr>
<tr><td><code id="plot.bootstrapCI_+3A_...">...</code></td>
<td>
<p>additional arguments passed to callies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.bootstrapCI</code> plots the bootstrapped coefficients.
</p>


<h3>Value</h3>

<p>No return value (plot method) or <code>x</code> itself (print method)
</p>

<hr>
<h2 id='plot.FDboost'>Plot the fit or the coefficients of a boosted functional regression model</h2><span id='topic+plot.FDboost'></span><span id='topic+plotPredicted'></span><span id='topic+plotResiduals'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object produced by <code><a href="#topic+FDboost">FDboost</a>()</code> and 
plots the fitted effects or the coefficient-functions/surfaces.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
plot(
  x,
  raw = FALSE,
  rug = TRUE,
  which = NULL,
  includeOffset = TRUE,
  ask = TRUE,
  n1 = 40,
  n2 = 40,
  n3 = 20,
  n4 = 11,
  onlySelected = TRUE,
  pers = FALSE,
  commonRange = FALSE,
  ...
)

plotPredicted(
  x,
  subset = NULL,
  posLegend = "topleft",
  lwdObs = 1,
  lwdPred = 1,
  ...
)

plotResiduals(x, subset = NULL, posLegend = "topleft", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.FDboost_+3A_x">x</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_raw">raw</code></td>
<td>
<p>logical defaults to <code>FALSE</code>.
If <code>raw = FALSE</code> for each effect the estimated function/surface is calculated. 
If <code>raw = TRUE</code> the coefficients of the model are returned.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_rug">rug</code></td>
<td>
<p>when <code>TRUE</code> (default) then the covariate to which the plot applies is 
displayed as a rug plot at the foot of each plot of a 1-d smooth, 
and the locations of the covariates are plotted as points on the contour plot 
representing a 2-d smooth.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_which">which</code></td>
<td>
<p>a subset of base-learners to take into account for plotting.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_includeoffset">includeOffset</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. Should the offset be included in 
the plot of the intercept (default) or should it be plotted separately.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_ask">ask</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>, if several effects are plotted the user
has to hit Return to see next plot.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_n1">n1</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_n2">n2</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_n3">n3</code></td>
<td>
<p>n1, n2, n3 give the number of grid-points for 1-/2-/3-dimensional 
smooth terms used in the marginal equidistant grids over the range of the 
covariates at which the estimated effects are evaluated.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_n4">n4</code></td>
<td>
<p>gives the number of points for the third dimension in a 3-dimensional smooth term</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_onlyselected">onlySelected</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. Only plot effects that where 
selected in at least one boosting iteration.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_pers">pers</code></td>
<td>
<p>logical, defaults to <code>FALSE</code>, 
If <code>TRUE</code>, perspective plots (<code><a href="graphics.html#topic+persp">persp</a></code>) for 
2- and 3-dimensional effects are drawn.
If <code>FALSE</code>, image/contour-plots (<code><a href="graphics.html#topic+image">image</a></code>, 
<code><a href="graphics.html#topic+contour">contour</a></code>) are drawn for 2- and 3-dimensional effects.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_commonrange">commonRange</code></td>
<td>
<p>logical, defaults to <code>FALSE</code>, 
if <code>TRUE</code> the range over all effects is the same 
(does not affect perspecitve or image plots).</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_...">...</code></td>
<td>
<p>other arguments, passed to <code>funplot</code> (only used in plotPredicted)</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_subset">subset</code></td>
<td>
<p>subset of the observed response curves and their predictions that is plotted. 
Per default all observations are plotted.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_poslegend">posLegend</code></td>
<td>
<p>location of the legend, if a legend is drawn automatically 
(only used in plotPredicted). The default is &quot;topleft&quot;.</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_lwdobs">lwdObs</code></td>
<td>
<p>lwd of observed curves (only used in plotPredicted)</p>
</td></tr>
<tr><td><code id="plot.FDboost_+3A_lwdpred">lwdPred</code></td>
<td>
<p>lwd of predicted curves (only used in plotPredicted)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>no return value (plot method)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit and 
<code><a href="#topic+coef.FDboost">coef.FDboost</a></code> for the calculation of the coefficient functions.
</p>

<hr>
<h2 id='predict.FDboost'>Prediction for boosted functional regression model</h2><span id='topic+predict.FDboost'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object produced by <code><a href="#topic+FDboost">FDboost</a>()</code> and produces 
predictions given a new set of values for the model covariates or the original 
values used for the model fit. This is a wrapper
function for <code><a href="mboost.html#topic+methods">predict.mboost</a>()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
predict(object, newdata = NULL, which = NULL, toFDboost = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.FDboost_+3A_object">object</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="predict.FDboost_+3A_newdata">newdata</code></td>
<td>
<p>a named list or a data frame containing the values of the model 
covariates at which predictions are required.
If this is not provided then predictions corresponding to the original data are returned. 
If <code>newdata</code> is provided then it should contain all the variables needed for 
prediction, in the format supplied to <code>FDboost</code>, i.e., 
functional predictors must be supplied as matrices with each row corresponding to 
one observed function.</p>
</td></tr>
<tr><td><code id="predict.FDboost_+3A_which">which</code></td>
<td>
<p>a subset of base-learners to take into account for computing predictions 
or coefficients. If which is given (as an integer vector corresponding to base-learners) 
a list is returned.</p>
</td></tr>
<tr><td><code id="predict.FDboost_+3A_tofdboost">toFDboost</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. In case of regular response in wide format 
(i.e. response is supplied as matrix): should the predictions be returned as matrix, or list 
of matrices instead of vectors</p>
</td></tr>
<tr><td><code id="predict.FDboost_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to <code><a href="mboost.html#topic+methods">predict.mboost</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix or list of predictions depending on values of unlist and which
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit 
and <code><a href="#topic+plotPredicted">plotPredicted</a></code> for a plot of the observed values and their predictions.
</p>

<hr>
<h2 id='predict.FDboost_fac'>Prediction and plotting for factorized FDboost model components</h2><span id='topic+predict.FDboost_fac'></span><span id='topic+plot.FDboost_fac'></span>

<h3>Description</h3>

<p>Prediction and plotting for factorized FDboost model components
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost_fac'
predict(object, newdata = NULL, which = NULL, ...)

## S3 method for class 'FDboost_fac'
plot(x, which = NULL, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.FDboost_fac_+3A_object">object</code>, <code id="predict.FDboost_fac_+3A_x">x</code></td>
<td>
<p>a model-factor given as a <code>FDboost_fac</code> object</p>
</td></tr>
<tr><td><code id="predict.FDboost_fac_+3A_newdata">newdata</code></td>
<td>
<p>optionally, a data frame or list
in which to look for variables with which to predict.
See <code><a href="mboost.html#topic+predict.mboost">predict.mboost</a></code>.</p>
</td></tr>
<tr><td><code id="predict.FDboost_fac_+3A_which">which</code></td>
<td>
<p>a subset of base-learner components to take into
account for computing predictions or coefficients. Different
components are never aggregated to a joint prediction, but always
returned as a matrix or list. Select the k-th component
by name in the format <code>bl(x, ...)[k]</code> or all components of a base-learner
by dropping the index or all base-learners of a variable by using
the variable name.</p>
</td></tr>
<tr><td><code id="predict.FDboost_fac_+3A_...">...</code></td>
<td>
<p>additional arguments passed to underlying methods.</p>
</td></tr>
<tr><td><code id="predict.FDboost_fac_+3A_main">main</code></td>
<td>
<p>the plot title. By default, base-learner names are used with 
component numbers <code>[k]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of predictions (for predict method) or no 
return value (plot method)
</p>


<h3>See Also</h3>

<p>[factorize(), factorize.FDboost()]
</p>

<hr>
<h2 id='residuals.FDboost'>Residual values of a boosted functional regression model</h2><span id='topic+residuals.FDboost'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object and computes the residuals, 
more precisely the current value of the negative gradient is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
residuals(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.FDboost_+3A_object">object</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="residuals.FDboost_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The residual is missing if the corresponding value of the response was missing.
</p>


<h3>Value</h3>

<p>matrix of residual values
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit.
</p>

<hr>
<h2 id='reweightData'>Function to Reweight Data</h2><span id='topic+reweightData'></span>

<h3>Description</h3>

<p>Function to Reweight Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reweightData(
  data,
  argvals,
  vars,
  longvars = NULL,
  weights,
  index,
  idvars = NULL,
  compress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reweightData_+3A_data">data</code></td>
<td>
<p>a named list or data.frame.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_argvals">argvals</code></td>
<td>
<p>character (vector); name(s) for entries in data giving 
the index for observed grid points; must be supplied if <code>vars</code> is not supplied.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_vars">vars</code></td>
<td>
<p>character (vector); name(s) for entries in data, which
are subsetted according to weights or index. Must be supplied if <code>argvals</code> is not supplied.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_longvars">longvars</code></td>
<td>
<p>variables in long format, e.g., a response that is observed at curve specific grids.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_weights">weights</code></td>
<td>
<p>vector of weights for observations. Must be supplied if <code>index</code> is not supplied.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_index">index</code></td>
<td>
<p>vector of indices for observations. Must be supplied if <code>weights</code> is not supplied.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_idvars">idvars</code></td>
<td>
<p>character (vector); index, which is needed to expand <code>vars</code> to be conform
with the <code>hmatrix</code> structure when using <code>bhistx</code>-base-learners or to be conform with 
variables in long format specified in <code>longvars</code>.</p>
</td></tr>
<tr><td><code id="reweightData_+3A_compress">compress</code></td>
<td>
<p>logical; whether <code>hmatrix</code> objects are saved in compressed form or not. Default is <code>TRUE</code>.
Should be set to <code>FALSE</code> when using <code>reweightData</code> for nested resampling.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>reweightData</code> indexes the rows of matrices and / or positions of vectors by using
either the <code>index</code> or the <code>weights</code>-argument. To prevent the function from indexing
the list entry / entries, which serve as time index for observed grid points of each trajectory of
functional observations, the <code>argvals</code> argument (vector of character names for these list entries) 
can be supplied. If <code>argvals</code> is not supplied, <code>vars</code> must be supplied and it is assumed that 
<code>argvals</code> is equal to <code>names(data)[!names(data) %in% vars]</code>.
</p>
<p>When using <code>weights</code>, a weight vector of length N must be supplied, where N is the number of observations.
When using <code>index</code>, the vector must contain the index of each row as many times as it shall be included in the
new data set.
</p>


<h3>Value</h3>

<p>A list with the reweighted or subsetted data.
</p>


<h3>Author(s)</h3>

<p>David Ruegamer, Sarah Brockhaus
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load data
data("viscosity", package = "FDboost")
interval &lt;- "101"
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[ , 1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]

## what does data look like
str(viscosity)

## do some reweighting
# correct weights
str(reweightData(viscosity, vars=c("vis", "T_C", "T_A", "rspeed", "mflow"), 
    argvals = "time", weights = c(0, 32, 32, rep(0, 61))))

str(visNew &lt;- reweightData(viscosity, vars=c("vis", "T_C", "T_A", "rspeed", "mflow"), 
    argvals = "time", weights = c(0, 32, 32, rep(0, 61))))
# check the result
# visNew$vis[1:5, 1:5] ## image(visNew$vis)

# incorrect weights
str(reweightData(viscosity, vars=c("vis", "T_C", "T_A", "rspeed", "mflow"), 
    argvals = "time", weights = sample(1:64, replace = TRUE)), 1)

# supply meaningful index
str(visNew &lt;- reweightData(viscosity, vars = c("vis", "T_C", "T_A", "rspeed", "mflow"), 
              argvals = "time", index = rep(1:32, each = 2)))
# check the result
# visNew$vis[1:5, 1:5]

# errors
if(FALSE){
   reweightData(viscosity, argvals = "")
   reweightData(viscosity, argvals = "covThatDoesntExist", index = rep(1,64))
   }
   
</code></pre>

<hr>
<h2 id='stabsel.FDboost'>Stability Selection</h2><span id='topic+stabsel.FDboost'></span>

<h3>Description</h3>

<p>Function for stability selection with functional response. Per default the sampling is done
on the level of curves and if the model contains a smooth functional intercept, this intercept 
is refittedn in each sampling fold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
stabsel(
  x,
  refitSmoothOffset = TRUE,
  cutoff,
  q,
  PFER,
  folds = cvLong(x$id, weights = rep(1, l = length(x$id)), type = "subsampling", B = B),
  B = ifelse(sampling.type == "MB", 100, 50),
  assumption = c("unimodal", "r-concave", "none"),
  sampling.type = c("SS", "MB"),
  papply = mclapply,
  verbose = TRUE,
  eval = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stabsel.FDboost_+3A_x">x</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_refitsmoothoffset">refitSmoothOffset</code></td>
<td>
<p>logical, should the offset be refitted in each learning sample? 
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_cutoff">cutoff</code></td>
<td>
<p>cutoff between 0.5 and 1. Preferably a value between 0.6 and 0.9 should be used.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_q">q</code></td>
<td>
<p>number of (unique) selected variables (or groups of variables depending on the model) 
that are selected on each subsample.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_pfer">PFER</code></td>
<td>
<p>upper bound for the per-family error rate. This specifies the amount of falsely 
selected base-learners, which is tolerated. See details of <code><a href="mboost.html#topic+stabsel">stabsel</a></code>.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_folds">folds</code></td>
<td>
<p>a weight matrix with number of rows equal to the number of observations, 
see <code>{cvLong}</code>. Usually one should not change the default here as subsampling 
with a fraction of 1/2 is needed for the error bounds to hold. One usage scenario where 
specifying the folds by hand might be the case when one has dependent data (e.g. clusters) and 
thus wants to draw clusters (i.e., multiple rows together) not individuals.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_b">B</code></td>
<td>
<p>number of subsampling replicates. Per default, we use 50 complementary pairs for the error 
bounds of Shah &amp; Samworth (2013) and 100 for the error bound derived in Meinshausen &amp; Buehlmann (2010). 
As we use <code>B</code> complementary pairs in the former case this leads to <code>2B</code> subsamples.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_assumption">assumption</code></td>
<td>
<p>Defines the type of assumptions on the distributions of the selection probabilities 
and simultaneous selection probabilities. Only applicable for <code>sampling.type = "SS"</code>. 
For <code>sampling.type = "MB"</code> we always use <code>"none"</code>.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_sampling.type">sampling.type</code></td>
<td>
<p>use sampling scheme of of Shah &amp; Samworth (2013), i.e., with complementary pairs 
(<code>sampling.type = "SS"</code>), or the original sampling scheme of Meinshausen &amp; Buehlmann (2010).</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_papply">papply</code></td>
<td>
<p>(parallel) apply function, defaults to mclapply. Alternatively, parLapply can be used. 
In the latter case, usually more setup is needed (see example of cvrisk for some details).</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_verbose">verbose</code></td>
<td>
<p>logical (default: TRUE) that determines wether warnings should be issued.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_eval">eval</code></td>
<td>
<p>logical. Determines whether stability selection is evaluated (<code>eval = TRUE</code>; default) 
or if only the parameter combination is returned.</p>
</td></tr>
<tr><td><code id="stabsel.FDboost_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> or <code><a href="#topic+validateFDboost">validateFDboost</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of boosting iterations is an important hyper-parameter of the boosting algorithms 
and can be chosen using the functions <code>cvrisk.FDboost</code> and <code>validateFDboost</code> as they compute
honest, i.e. out-of-bag, estimates of the empirical risk for different numbers of boosting iterations. 
The weights (zero weights correspond to test cases) are defined via the folds matrix, 
see <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> in package mboost. 
See Hofner et al. (2015) for the combination of stability selection and component-wise boosting.
</p>


<h3>Value</h3>

<p>An object of class <code>stabsel</code> with a special print method. 
For the elements of the object, see <code><a href="mboost.html#topic+stabsel">stabsel</a></code>
</p>


<h3>References</h3>

<p>B. Hofner, L. Boccuto and M. Goeker (2015), Controlling false discoveries in 
high-dimensional situations: boosting with stability selection. 
BMC Bioinformatics, 16, 1-17.
</p>
<p>N. Meinshausen and P. Buehlmann (2010), Stability selection. 
Journal of the Royal Statistical Society, Series B, 72, 417-473.
</p>
<p>R.D. Shah and R.J. Samworth (2013), Variable selection with error control: 
another look at stability selection. Journal of the Royal Statistical Society, Series B, 75, 55-80.
</p>


<h3>See Also</h3>

<p><code><a href="mboost.html#topic+stabsel">stabsel</a></code> to perform stability selection for a mboost-object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>######## Example for function-on-scalar-regression
data("viscosity", package = "FDboost")
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## fit a model cotaining all main effects 
modAll &lt;- FDboost(vis ~ 1 
          + bolsc(T_C, df=1) %A0% bbs(time, df=5) 
          + bolsc(T_A, df=1) %A0% bbs(time, df=5)
          + bolsc(T_B, df=1) %A0% bbs(time, df=5)
          + bolsc(rspeed, df=1) %A0% bbs(time, df=5)
          + bolsc(mflow, df=1) %A0% bbs(time, df=5), 
       timeformula = ~bbs(time, df=5), 
       numInt = "Riemann", family = QuantReg(), 
       offset = NULL, offset_control = o_control(k_min = 10),
       data = viscosity, 
       control = boost_control(mstop = 100, nu = 0.2))


## create folds for stability selection  
## only 5 folds for a fast example, usually use 50 folds 
set.seed(1911)
folds &lt;- cvLong(modAll$id, weights = rep(1, l = length(modAll$id)), 
                type = "subsampling", B = 5) 
    
        
## stability selection with refit of the smooth intercept 
stabsel_parameters(q = 3, PFER = 1, p = 6, sampling.type = "SS")
sel1 &lt;- stabsel(modAll, q = 3, PFER = 1, folds = folds, grid = 1:200, sampling.type = "SS")
sel1

## stability selection without refit of the smooth intercept 
sel2 &lt;- stabsel(modAll, refitSmoothOffset = FALSE, q = 3, PFER = 1, 
                folds = folds, grid = 1:200, sampling.type = "SS")
sel2


</code></pre>

<hr>
<h2 id='subset_hmatrix'>Subsets hmatrix according to an index</h2><span id='topic+subset_hmatrix'></span>

<h3>Description</h3>

<p>Subsets hmatrix according to an index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subset_hmatrix(x, index, compress = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset_hmatrix_+3A_x">x</code></td>
<td>
<p>hmatix object that should be subsetted</p>
</td></tr>
<tr><td><code id="subset_hmatrix_+3A_index">index</code></td>
<td>
<p>integer vector with (possibly duplicated) indices
for each curve to select</p>
</td></tr>
<tr><td><code id="subset_hmatrix_+3A_compress">compress</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. Only used to force a meaningful
behaviour of <code>applyFolds</code> with hmatrix objects when using nested resampling.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This methods is primary useful when subsetting repeatedly.
</p>


<h3>Value</h3>

<p>a <code>hmatrix</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>t1 &lt;- rep((1:5)/2, each = 3)
id1 &lt;- rep(1:3, 5)
x1 &lt;- matrix(1:15, ncol = 5) 
s1 &lt;- (1:5)/2 
hmat &lt;- hmatrix(time = t1, id = id1, x = x1, argvals = s1, timeLab = "t1", 
                argvalsLab = "s1", xLab = "test")

index1 &lt;- c(1, 1, 3)
index2 &lt;- c(2, 3, 3)
resMat &lt;- subset_hmatrix(hmat, index = index1)
try(resMat2 &lt;- subset_hmatrix(resMat, index = index2))
resMat &lt;- subset_hmatrix(hmat, index = index1, compress = FALSE)
try(resMat2 &lt;- subset_hmatrix(resMat, index = index2))

</code></pre>

<hr>
<h2 id='summary.FDboost'>Print and summary of a boosted functional regression model</h2><span id='topic+summary.FDboost'></span><span id='topic+print.FDboost'></span>

<h3>Description</h3>

<p>Takes a fitted <code>FDboost</code>-object and produces a print 
to the console or a summary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
summary(object, ...)

## S3 method for class 'FDboost'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.FDboost_+3A_object">object</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
<tr><td><code id="summary.FDboost_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
<tr><td><code id="summary.FDboost_+3A_x">x</code></td>
<td>
<p>a fitted <code>FDboost</code>-object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with information on the model / a list with summary information
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FDboost">FDboost</a></code> for the model fit.
</p>

<hr>
<h2 id='truncateTime'>Function to truncate time in functional data</h2><span id='topic+truncateTime'></span>

<h3>Description</h3>

<p>Function to truncate time in functional data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncateTime(funVar, time, newtime, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncateTime_+3A_funvar">funVar</code></td>
<td>
<p>names of functional variables that should be truncated</p>
</td></tr>
<tr><td><code id="truncateTime_+3A_time">time</code></td>
<td>
<p>name of time variable</p>
</td></tr>
<tr><td><code id="truncateTime_+3A_newtime">newtime</code></td>
<td>
<p>new time vector that should be used. Must be part of the old time-line.</p>
</td></tr>
<tr><td><code id="truncateTime_+3A_data">data</code></td>
<td>
<p>list containing all the data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the data containing all variables of the original dataset
with the variables of <code>funVar</code> truncated according to <code>newtime</code>.
</p>


<h3>Note</h3>

<p>All variables that are not part if <code>funVar</code>, or <code>time</code>
are simply copied into the new data list
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(fda)){
  dat &lt;- fda::growth
  dat$hgtm &lt;- t(dat$hgtm[,1:10])
  dat$hgtf &lt;- t(dat$hgtf[,1:10])
  
  ## only use time-points 1:16 of variable age
  datTr &lt;- truncateTime(funVar=c("hgtm","hgtf"), time="age", newtime=1:16, data=dat)
  
  
  oldpar &lt;- par(mfrow=c(1,2))
  with(dat, funplot(age, hgtm, main="Original data"))
  with(datTr, funplot(age, hgtm, main="Yearly data"))
  par(mfrow=c(1,1))   
  par(oldpar)
  
}
</code></pre>

<hr>
<h2 id='update.FDboost'>Function to update FDboost objects</h2><span id='topic+update.FDboost'></span>

<h3>Description</h3>

<p>Function to update FDboost objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FDboost'
update(
  object,
  weights = NULL,
  oobweights = NULL,
  risk = NULL,
  trace = NULL,
  ...,
  evaluate = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.FDboost_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="update.FDboost_+3A_weights">weights</code>, <code id="update.FDboost_+3A_oobweights">oobweights</code>, <code id="update.FDboost_+3A_risk">risk</code>, <code id="update.FDboost_+3A_trace">trace</code></td>
<td>
<p>see <code>?FDboost</code></p>
</td></tr>
<tr><td><code id="update.FDboost_+3A_...">...</code></td>
<td>
<p>Additional arguments to the call, or arguments with changed values.</p>
</td></tr>
<tr><td><code id="update.FDboost_+3A_evaluate">evaluate</code></td>
<td>
<p>If true evaluate the new call else return the call.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the call of (<code>evaluate = FALSE</code>) or the updated (<code>evaluate = TRUE</code>) FDboost model
</p>


<h3>Author(s)</h3>

<p>David Ruegamer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>######## Example from \code{?FDboost}
data("viscosity", package = "FDboost") 
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

mod1 &lt;- FDboost(vis ~ 1 + bolsc(T_C, df = 2) + bolsc(T_A, df = 2),
               timeformula = ~ bbs(time, df = 4),
               numInt = "equal", family = QuantReg(),
               offset = NULL, offset_control = o_control(k_min = 9),
               data = viscosity, control=boost_control(mstop = 10, nu = 0.4))
               
# update nu
mod2 &lt;- update(mod1, control=boost_control(nu = 1)) # mstop will stay the same
# update mstop
mod3 &lt;- update(mod2, control=boost_control(mstop = 100)) # nu=1 does not get changed
mod4 &lt;- update(mod1, formula = vis ~ 1 + bolsc(T_C, df = 2)) # drop one term
</code></pre>

<hr>
<h2 id='validateFDboost'>Cross-Validation and Bootstrapping over Curves</h2><span id='topic+validateFDboost'></span>

<h3>Description</h3>

<p>DEPRECATED! 
The function <code>validateFDboost()</code> is deprecated,  
use <code><a href="#topic+applyFolds">applyFolds</a></code> and <code><a href="#topic+bootstrapCI">bootstrapCI</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validateFDboost(
  object,
  response = NULL,
  folds = cv(rep(1, length(unique(object$id))), type = "bootstrap"),
  grid = 1:mstop(object),
  fun = NULL,
  getCoefCV = TRUE,
  riskopt = c("mean", "median"),
  mrdDelete = 0,
  refitSmoothOffset = TRUE,
  showProgress = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validateFDboost_+3A_object">object</code></td>
<td>
<p>fitted FDboost-object</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_response">response</code></td>
<td>
<p>optional, specify a response vector for the computation of the prediction errors.
Defaults to <code>NULL</code> which means that the response of the fitted model is used.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_folds">folds</code></td>
<td>
<p>a weight matrix with number of rows equal to the number of observed trajectories.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_grid">grid</code></td>
<td>
<p>the grid over which the optimal number of boosting iterations (mstop) is searched.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_fun">fun</code></td>
<td>
<p>if <code>fun</code> is <code>NULL</code>, the out-of-bag risk is returned. 
<code>fun</code>, as a function of <code>object</code>, 
may extract any other characteristic of the cross-validated models. These are returned as is.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_getcoefcv">getCoefCV</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. Should the coefficients and predictions
be computed for all the models on the sampled data?</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_riskopt">riskopt</code></td>
<td>
<p>how is the optimal stopping iteration determined. Defaults to the mean, 
but median is possible as well.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_mrddelete">mrdDelete</code></td>
<td>
<p>Delete values that are <code>mrdDelete</code> percent smaller than the mean
of the response. Defaults to 0 which means that only response values being 0 
are not used in the calculation of the MRD (= mean relative deviation).</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_refitsmoothoffset">refitSmoothOffset</code></td>
<td>
<p>logical, should the offset be refitted in each learning sample? 
Defaults to <code>TRUE</code>. In <code><a href="mboost.html#topic+cvrisk">cvrisk</a></code> the offset of the original model fit in  
<code>object</code> is used in all folds.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_showprogress">showProgress</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="validateFDboost_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code><a href="parallel.html#topic+mclapply">mclapply</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of boosting iterations is an important hyper-parameter of boosting  
and can be chosen using the function <code>validateFDboost</code> as they compute
honest, i.e., out-of-bag, estimates of the empirical risk for different numbers of boosting iterations. 
</p>
<p>The function <code>validateFDboost</code> is especially suited to models with functional response. 
Using the option <code>refitSmoothOffset</code> the offset is refitted on each fold. 
Note, that the function <code>validateFDboost</code> expects folds that give weights
per curve without considering integration weights. The integration weights of 
<code>object</code> are used to compute the empirical risk as integral. The argument <code>response</code> 
can be useful in simulation studies where the true value of the response is known but for 
the model fit the response is used with noise.
</p>


<h3>Value</h3>

<p>The function <code>validateFDboost</code> returns a <code>validateFDboost</code>-object, 
which is a named list containing: 
</p>
<table>
<tr><td><code>response</code></td>
<td>
<p>the response</p>
</td></tr>
<tr><td><code>yind</code></td>
<td>
<p>the observation points of the response</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>the id variable of the response</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>
<p>folds that were used</p>
</td></tr>
<tr><td><code>grid</code></td>
<td>
<p>grid of possible numbers of boosting iterations</p>
</td></tr>
<tr><td><code>coefCV</code></td>
<td>
<p>if <code>getCoefCV</code> is <code>TRUE</code> the estimated coefficient functions in the folds</p>
</td></tr>
<tr><td><code>predCV</code></td>
<td>
<p>if <code>getCoefCV</code> is <code>TRUE</code> the out-of-bag predicted values of the response</p>
</td></tr>
<tr><td><code>oobpreds</code></td>
<td>
<p>if the type of folds is curves the out-of-bag predictions for each trajectory</p>
</td></tr>
<tr><td><code>oobrisk</code></td>
<td>
<p>the out-of-bag risk</p>
</td></tr>
<tr><td><code>oobriskMean</code></td>
<td>
<p>the out-of-bag risk at the minimal mean risk</p>
</td></tr>
<tr><td><code>oobmse</code></td>
<td>
<p>the out-of-bag mean squared error (MSE)</p>
</td></tr>
<tr><td><code>oobrelMSE</code></td>
<td>
<p>the out-of-bag relative mean squared error (relMSE)</p>
</td></tr>
<tr><td><code>oobmrd</code></td>
<td>
<p>the out-of-bag mean relative deviation (MRD)</p>
</td></tr>
<tr><td><code>oobrisk0</code></td>
<td>
<p>the out-of-bag risk without consideration of integration weights</p>
</td></tr>
<tr><td><code>oobmse0</code></td>
<td>
<p>the out-of-bag mean squared error (MSE) without consideration of integration weights</p>
</td></tr>
<tr><td><code>oobmrd0</code></td>
<td>
<p>the out-of-bag mean relative deviation (MRD) without consideration of integration weights</p>
</td></tr>
<tr><td><code>format</code></td>
<td>
<p>one of &quot;FDboostLong&quot; or &quot;FDboost&quot; depending on the class of the object</p>
</td></tr>
<tr><td><code>fun_ret</code></td>
<td>
<p>list of what fun returns if fun was specified</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
if(require(fda)){
 ## load the data
 data("CanadianWeather", package = "fda")
 
 ## use data on a daily basis 
 canada &lt;- with(CanadianWeather, 
                list(temp = t(dailyAv[ , , "Temperature.C"]),
                     l10precip = t(dailyAv[ , , "log10precip"]),
                     l10precip_mean = log(colMeans(dailyAv[ , , "Precipitation.mm"]), base = 10),
                     lat = coordinates[ , "N.latitude"],
                     lon = coordinates[ , "W.longitude"],
                     region = factor(region),
                     place = factor(place),
                     day = 1:365,  ## corresponds to t: evaluation points of the fun. response 
                     day_s = 1:365))  ## corresponds to s: evaluation points of the fun. covariate
 
## center temperature curves per day 
canada$tempRaw &lt;- canada$temp
canada$temp &lt;- scale(canada$temp, scale = FALSE) 
rownames(canada$temp) &lt;- NULL ## delete row-names 
  
## fit the model  
mod &lt;- FDboost(l10precip ~ 1 + bolsc(region, df = 4) + 
                 bsignal(temp, s = day_s, cyclic = TRUE, boundary.knots = c(0.5, 365.5)), 
               timeformula = ~ bbs(day, cyclic = TRUE, boundary.knots = c(0.5, 365.5)), 
               data = canada)
mod &lt;- mod[75]

  #### create folds for 3-fold bootstrap: one weight for each curve
  set.seed(124)
  folds_bs &lt;- cv(weights = rep(1, mod$ydim[1]), type = "bootstrap", B = 3)

  ## compute out-of-bag risk on the 3 folds for 1 to 75 boosting iterations  
  cvr &lt;- applyFolds(mod, folds = folds_bs, grid = 1:75)

  ## compute out-of-bag risk and coefficient estimates on folds  
  cvr2 &lt;- validateFDboost(mod, folds = folds_bs, grid = 1:75)

  ## weights per observation point  
  folds_bs_long &lt;- folds_bs[rep(1:nrow(folds_bs), times = mod$ydim[2]), ]
  attr(folds_bs_long, "type") &lt;- "3-fold bootstrap"
  ## compute out-of-bag risk on the 3 folds for 1 to 75 boosting iterations  
  cvr3 &lt;- cvrisk(mod, folds = folds_bs_long, grid = 1:75)

  ## plot the out-of-bag risk
  oldpar &lt;- par(mfrow = c(1,3))
  plot(cvr); legend("topright", lty=2, paste(mstop(cvr)))
  plot(cvr2)
  plot(cvr3); legend("topright", lty=2, paste(mstop(cvr3)))

  ## plot the estimated coefficients per fold
  ## more meaningful for higher number of folds, e.g., B = 100 
  par(mfrow = c(2,2))
  plotPredCoef(cvr2, terms = FALSE, which = 1)
  plotPredCoef(cvr2, terms = FALSE, which = 3)
  
  ## compute out-of-bag risk and predictions for leaving-one-curve-out cross-validation
  cvr_jackknife &lt;- validateFDboost(mod, folds = cvLong(unique(mod$id), 
                                   type = "curves"), grid = 1:75)
  plot(cvr_jackknife)
  ## plot oob predictions per fold for 3rd effect 
  plotPredCoef(cvr_jackknife, which = 3) 
  ## plot coefficients per fold for 2nd effect
  plotPredCoef(cvr_jackknife, which = 2, terms = FALSE)
  
  par(oldpar)

}


</code></pre>

<hr>
<h2 id='viscosity'> Viscosity of resin over time</h2><span id='topic+viscosity'></span>

<h3>Description</h3>

<p>In an experimental setting the viscosity of resin was measured over time
to asses the curing process depending on 5 binary factors (low-high).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("viscosity")</code></pre>


<h3>Format</h3>

<p>A data list with 64 observations on the following 7 variables.
</p>

<dl>
<dt><code>visAll</code></dt><dd><p>viscosity measures over all available time points</p>
</dd>
<dt><code>timeAll</code></dt><dd><p>time points of viscosity measures</p>
</dd>
<dt><code>T_C</code></dt><dd><p> temperature of tools</p>
</dd>
<dt><code>T_A</code></dt><dd><p>temperature of resin</p>
</dd>
<dt><code>T_B</code></dt><dd><p>temperature of curing agent</p>
</dd>
<dt><code>rspeed</code></dt><dd><p>rotational speed</p>
</dd>
<dt><code>mflow</code></dt><dd><p>mass flow</p>
</dd>
</dl>



<h3>Details</h3>

<p>The aim is to determine factors that affect the curing process in the mold.
The desired viscosity-curve has low values in the beginning followed
by a sharp increase.
Due to technical reasons the measuring method of the rheometer has to be
changed in a certain range of viscosity. The first observations are measured
by rotation of a blade giving observations every two seconds,
the later observations are measured through oscillation of a blade giving
observations every ten seconds. In the later observations the resin is quite
hard so the measurements should be interpreted as a qualitative measure of hardening.
</p>


<h3>Source</h3>

<p>Wolfgang Raffelt, Technical University of Munich, Institute for Carbon Composites
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 data("viscosity", package = "FDboost")
 ## set time-interval that should be modeled
 interval &lt;- "101"

 ## model time until "interval" and take log() of viscosity
 end &lt;- which(viscosity$timeAll==as.numeric(interval))
 viscosity$vis &lt;- log(viscosity$visAll[,1:end])
 viscosity$time &lt;- viscosity$timeAll[1:end]
 
 ## fit median regression model with 100 boosting iterations,
 ## step-length 0.4 and smooth time-specific offset
 ## the factors are in effect coding -1, 1 for the levels
 mod &lt;- FDboost(vis ~ 1 + bols(T_C, contrasts.arg = "contr.sum", intercept=FALSE)
                + bols(T_A, contrasts.arg = "contr.sum", intercept=FALSE),
                timeformula=~bbs(time, lambda=100),
                numInt="equal", family=QuantReg(),
                offset=NULL, offset_control = o_control(k_min = 9),
                data=viscosity, control=boost_control(mstop = 100, nu = 0.4))
 summary(mod)

</code></pre>

<hr>
<h2 id='wide2long'>Transform id and time of wide format into long format</h2><span id='topic+wide2long'></span>

<h3>Description</h3>

<p>Transform id and time from wide format into long format, i.e., time and id are 
repeated accordingly so that two vectors of the same length are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wide2long(time, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wide2long_+3A_time">time</code></td>
<td>
<p>the observation points</p>
</td></tr>
<tr><td><code id="wide2long_+3A_id">id</code></td>
<td>
<p>the id for the curve</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with <code>time</code> and <code>id</code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
