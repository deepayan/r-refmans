<!DOCTYPE html><html><head><title>Help for package metafor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {metafor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#metafor-package'><p>metafor: A Meta-Analysis Package for R <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script></p></a></li>
<li><a href='#addpoly'><p>Add Polygons to Forest Plots</p></a></li>
<li><a href='#addpoly.default'><p>Add Polygons to Forest Plots (Default Method)</p></a></li>
<li><a href='#addpoly.predict.rma'><p>Add Polygons to Forest Plots (Method for 'predict.rma' Objects)</p></a></li>
<li><a href='#addpoly.rma'><p>Add Polygons to Forest Plots (Method for 'rma' Objects)</p></a></li>
<li><a href='#aggregate.escalc'><p>Aggregate Multiple Effect Sizes or Outcomes Within Studies</p></a></li>
<li><a href='#anova.rma'><p>Likelihood Ratio and Wald-Type Tests for 'rma' Objects</p></a></li>
<li><a href='#baujat'><p>Baujat Plots for 'rma' Objects</p></a></li>
<li><a href='#bldiag'><p>Construct Block Diagonal Matrix</p></a></li>
<li><a href='#blsplit'><p>Split Block Diagonal Matrix</p></a></li>
<li><a href='#blup'><p>Best Linear Unbiased Predictions for 'rma.uni' Objects</p></a></li>
<li><a href='#coef.matreg'><p>Extract the Model Coefficients and Variance-Covariance Matrix from 'matreg' Objects</p></a></li>
<li><a href='#coef.permutest.rma.uni'><p>Extract the Model Coefficient Table from 'permutest.rma.uni' Objects</p></a></li>
<li><a href='#coef.rma'><p>Extract the Model Coefficients and Coefficient Table from 'rma' and 'summary.rma' Objects</p></a></li>
<li><a href='#confint.rma'><p>Confidence Intervals for 'rma' Objects</p></a></li>
<li><a href='#contrmat'><p>Construct Contrast Matrix for Two-Group Comparisons</p></a></li>
<li><a href='#conv.2x2'><p>Reconstruct Cell Frequencies of \(2 \times 2\) Tables</p></a></li>
<li><a href='#conv.delta'><p>Transform Observed Effect Sizes or Outcomes and their Sampling Variances using the Delta Method</p></a></li>
<li><a href='#conv.fivenum'><p>Estimate Means and Standard Deviations from Five-Number Summary Values</p></a></li>
<li><a href='#conv.wald'><p>Convert Wald-Type Confidence Intervals and Tests to Sampling Variances</p></a></li>
<li><a href='#cumul'><p>Cumulative Meta-Analysis for 'rma' Objects</p></a></li>
<li><a href='#dfround'><p>Round Variables in a Data Frame</p></a></li>
<li><a href='#emmprep'><p>Create a Reference Grid for the 'emmeans' Function</p></a></li>
<li><a href='#escalc'><p>Calculate Effect Sizes and Outcome Measures</p></a></li>
<li><a href='#fitstats'><p>Fit Statistics and Information Criteria for 'rma' Objects</p></a></li>
<li><a href='#fitted.rma'><p>Fitted Values for 'rma' Objects</p></a></li>
<li><a href='#forest'><p>Forest Plots</p></a></li>
<li><a href='#forest.cumul.rma'><p>Forest Plots (Method for 'cumul.rma' Objects)</p></a></li>
<li><a href='#forest.default'><p>Forest Plots (Default Method)</p></a></li>
<li><a href='#forest.rma'><p>Forest Plots (Method for 'rma' Objects)</p></a></li>
<li><a href='#formatters'><p>Formatter Functions</p></a></li>
<li><a href='#formula.rma'><p>Extract the Model Formula from 'rma' Objects</p></a></li>
<li><a href='#fsn'><p>Fail-Safe N Analysis (File Drawer Analysis)</p></a></li>
<li><a href='#funnel'><p>Funnel Plots</p></a></li>
<li><a href='#gosh'><p>GOSH Plots for 'rma' Objects</p></a></li>
<li><a href='#hc'><p>Meta-Analysis based on the Method by Henmi and Copas (2010)</p></a></li>
<li><a href='#influence.rma.mv'><p>Model Diagnostics for 'rma.mv' Objects</p></a></li>
<li><a href='#influence.rma.uni'><p>Model Diagnostics for 'rma.uni' Objects</p></a></li>
<li><a href='#labbe'><p>L'Abbe Plots for 'rma' Objects</p></a></li>
<li><a href='#leave1out'><p>Leave-One-Out Diagnostics for 'rma' Objects</p></a></li>
<li><a href='#llplot'><p>Plot of Likelihoods for Individual Studies</p></a></li>
<li><a href='#matreg'><p>Fit Regression Models based on Correlation and Covariance Matrices</p></a></li>
<li><a href='#metafor.news'><p>Read News File of the Metafor Package</p></a></li>
<li><a href='#methods.anova.rma'><p>Methods for 'anova.rma' Objects</p></a></li>
<li><a href='#methods.confint.rma'><p>Methods for 'confint.rma' Objects</p></a></li>
<li><a href='#methods.escalc'><p>Methods for 'escalc' Objects</p></a></li>
<li><a href='#methods.list.rma'><p>Methods for 'list.rma' Objects</p></a></li>
<li><a href='#methods.vif.rma'><p>Methods for 'vif.rma' Objects</p></a></li>
<li><a href='#mfopt'><p>Getting and Setting Package Options</p></a></li>
<li><a href='#misc-models'><p>Fixed-Effects and Random-Effects Models in Meta-Analysis <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script></p></a></li>
<li><a href='#misc-options'><p>Miscellaneous Options and Features</p></a></li>
<li><a href='#misc-recs'><p>Some Recommended Practices</p></a></li>
<li><a href='#model.matrix.rma'><p>Extract the Model Matrix from 'rma' Objects</p></a></li>
<li><a href='#permutest'><p>Permutation Tests for 'rma.uni' Objects</p></a></li>
<li><a href='#plot.cumul.rma'><p>Plot Method for 'cumul.rma' Objects</p></a></li>
<li><a href='#plot.gosh.rma'><p>Plot Method for 'gosh.rma' Objects</p></a></li>
<li><a href='#plot.infl.rma.uni'><p>Plot Method for 'infl.rma.uni' Objects</p></a></li>
<li><a href='#plot.permutest.rma.uni'><p>Plot Method for 'permutest.rma.uni' Objects</p></a></li>
<li><a href='#plot.rma'><p>Plot Method for 'rma' Objects</p></a></li>
<li><a href='#plot.rma.uni.selmodel'><p>Plot Method for 'plot.rma.uni.selmodel' Objects</p></a></li>
<li><a href='#plot.vif.rma'><p>Plot Method for 'vif.rma' Objects</p></a></li>
<li><a href='#predict.rma'><p>Predicted Values for 'rma' Objects</p></a></li>
<li><a href='#print.anova.rma'><p>Print Methods for 'anova.rma' and 'list.anova.rma' Objects</p></a></li>
<li><a href='#print.confint.rma'><p>Print Methods for 'confint.rma' and 'list.confint.rma' Objects</p></a></li>
<li><a href='#print.escalc'><p>Print and Summary Methods for 'escalc' Objects</p></a></li>
<li><a href='#print.fsn'><p>Print Method for 'fsn' Objects</p></a></li>
<li><a href='#print.gosh.rma'><p>Print Method for 'gosh.rma' Objects</p></a></li>
<li><a href='#print.hc.rma.uni'><p>Print Method for 'hc.rma.uni' Objects</p></a></li>
<li><a href='#print.list.rma'><p>Print Method for 'list.rma' Objects</p></a></li>
<li><a href='#print.matreg'><p>Print and Summary Methods for 'matreg' Objects</p></a></li>
<li><a href='#print.permutest.rma.uni'><p>Print Method for 'permutest.rma.uni' Objects</p></a></li>
<li><a href='#print.ranktest'><p>Print Method for 'ranktest' Objects</p></a></li>
<li><a href='#print.regtest'><p>Print Method for 'regtest' Objects</p></a></li>
<li><a href='#print.rma'><p>Print and Summary Methods for 'rma' Objects</p></a></li>
<li><a href='#profile.rma'><p>Profile Likelihood Plots for 'rma' Objects</p></a></li>
<li><a href='#qqnorm.rma'><p>Normal QQ Plots for 'rma' Objects</p></a></li>
<li><a href='#radial'><p>Radial (Galbraith) Plots for 'rma' Objects</p></a></li>
<li><a href='#ranef'><p>Best Linear Unbiased Predictions for 'rma.uni' and 'rma.mv' Objects</p></a></li>
<li><a href='#ranktest'><p>Rank Correlation Test for Funnel Plot Asymmetry</p></a></li>
<li><a href='#rcalc'><p>Calculate the Variance-Covariance of Dependent Correlation Coefficients</p></a></li>
<li><a href='#regplot'><p>Scatter Plots / Bubble Plots</p></a></li>
<li><a href='#regtest'><p>Regression Test for Funnel Plot Asymmetry</p></a></li>
<li><a href='#replmiss'><p>Replace Missing Values in a Vector</p></a></li>
<li><a href='#reporter'><p>Dynamically Generated Analysis Reports for 'rma.uni' Objects</p></a></li>
<li><a href='#residuals.rma'><p>Residual Values based on 'rma' Objects</p></a></li>
<li><a href='#rma.glmm'><p>Meta-Analysis via Generalized Linear (Mixed-Effects) Models</p></a></li>
<li><a href='#rma.mh'><p>Meta-Analysis via the Mantel-Haenszel Method</p></a></li>
<li><a href='#rma.mv'><p>Meta-Analysis via Multivariate/Multilevel Linear (Mixed-Effects) Models</p></a></li>
<li><a href='#rma.peto'><p>Meta-Analysis via Peto's Method</p></a></li>
<li><a href='#rma.uni'><p>Meta-Analysis via Linear (Mixed-Effects) Models</p></a></li>
<li><a href='#robust'><p>Cluster-Robust Tests and Confidence Intervals for 'rma' Objects</p></a></li>
<li><a href='#selmodel'><p>Selection Models</p></a></li>
<li><a href='#simulate.rma'><p>Simulate Method for 'rma' Objects</p></a></li>
<li><a href='#tes'><p>Test of Excess Significance</p></a></li>
<li><a href='#to.long'><p>Convert Data from Vector to Long Format</p></a></li>
<li><a href='#to.table'><p>Convert Data from Vector to Table Format</p></a></li>
<li><a href='#to.wide'><p>Convert Data from a Long to a Wide Format</p></a></li>
<li><a href='#transf'><p>Transformation Functions</p></a></li>
<li><a href='#trimfill'><p>Trim and Fill Analysis for 'rma.uni' Objects</p></a></li>
<li><a href='#update.rma'><p>Model Updating for 'rma' Objects</p></a></li>
<li><a href='#vcalc'><p>Construct or Approximate the Variance-Covariance Matrix of Dependent Effect Sizes or Outcomes</p></a></li>
<li><a href='#vcov.rma'><p>Extract Various Types of Variance-Covariance Matrices from 'rma' Objects</p></a></li>
<li><a href='#vec2mat'><p>Convert a Vector into a Square Matrix</p></a></li>
<li><a href='#vif'><p>Variance Inflation Factors for 'rma' Objects</p></a></li>
<li><a href='#weights.rma'><p>Compute Weights for 'rma' Objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>4.6-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-28</td>
</tr>
<tr>
<td>Title:</td>
<td>Meta-Analysis Package for R</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), methods, Matrix, metadat, numDeriv</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, utils, graphics, grDevices, nlme, mathjaxr, pbapply</td>
</tr>
<tr>
<td>Suggests:</td>
<td>lme4, pracma, minqa, nloptr, dfoptim, ucminf, lbfgsb3c,
subplex, BB, Rsolnp, alabama, optimParallel, CompQuadForm,
mvtnorm, BiasedUrn, Epi, survival, GLMMadaptive, glmmTMB,
multcomp, gsl, sp, ape, boot, clubSandwich, crayon, R.rsp,
testthat, rmarkdown, wildmeta, emmeans, estmeansd, metaBLUE,
rstudioapi</td>
</tr>
<tr>
<td>Description:</td>
<td>A comprehensive collection of functions for conducting meta-analyses in R. The package includes functions to calculate various effect sizes or outcome measures, fit equal-, fixed-, random-, and mixed-effects models to such data, carry out moderator and meta-regression analyses, and create various types of meta-analytical plots (e.g., forest, funnel, radial, L'Abbe, Baujat, bubble, and GOSH plots). For meta-analyses of binomial and person-time data, the package also provides functions that implement specialized methods, including the Mantel-Haenszel method, Peto's method, and a variety of suitable generalized linear (mixed-effects) models (i.e., mixed-effects logistic and Poisson regression models). Finally, the package provides functionality for fitting meta-analytic multivariate/multilevel models that account for non-independent sampling errors and/or true effects (e.g., due to the inclusion of multiple treatment studies, multiple endpoints, or other forms of clustering). Network meta-analyses and meta-analyses accounting for known correlation structures (e.g., due to phylogenetic relatedness) can also be conducted. An introduction to the package can be found in Viechtbauer (2010) &lt;<a href="https://doi.org/10.18637%2Fjss.v036.i03">doi:10.18637/jss.v036.i03</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>mathjaxr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
<a href="https://github.com/wviechtb/metafor">https://github.com/wviechtb/metafor</a>
<a href="https://wviechtb.github.io/metafor/">https://wviechtb.github.io/metafor/</a> <a href="https://www.wvbauer.com">https://www.wvbauer.com</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/wviechtb/metafor/issues">https://github.com/wviechtb/metafor/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-28 11:02:23 UTC; wviechtb</td>
</tr>
<tr>
<td>Author:</td>
<td>Wolfgang Viechtbauer
    <a href="https://orcid.org/0000-0003-3463-4063"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wolfgang Viechtbauer &lt;wvb@metafor-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-28 16:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='metafor-package'>metafor: A Meta-Analysis Package for R <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script></h2><span id='topic+metafor-package'></span><span id='topic+metafor'></span>

<h3>Description</h3>

<p>The <span class="pkg">metafor</span> package provides a comprehensive collection of functions for conducting meta-analyses in <span class="rlang"><b>R</b></span>. The package can be used to calculate various effect sizes or outcome measures and then allows the user to fit equal-, fixed-, and random-effects models to these data. By including study-level variables (&lsquo;moderators&rsquo;) as predictors in these models, (mixed-effects) meta-regression models can also be fitted. For meta-analyses of \(2 \times 2\) tables, proportions, incidence rates, and incidence rate ratios, the package also provides functions that implement specialized methods, including the Mantel-Haenszel method, Peto's method, and a variety of suitable generalized linear mixed-effects models (i.e., mixed-effects logistic and Poisson regression models). For non-independent effects/outcomes (e.g., due to correlated sampling errors, correlated true effects or outcomes, or other forms of clustering), the package also provides a function for fitting multilevel and multivariate models to meta-analytic data.
</p>
<p>Various methods are available to assess model fit, to identify outliers and/or influential studies, and for conducting sensitivity analyses (e.g., standardized residuals, Cook's distances, leave-one-out analyses). Advanced techniques for hypothesis testing and obtaining confidence intervals (e.g., for the average effect or outcome or for the model coefficients in a meta-regression model) have also been implemented (e.g., the Knapp and Hartung method, permutation tests, cluster-robust inference methods / robust variance estimation).
</p>
<p>The package also provides functions for creating forest, funnel, radial (Galbraith), normal quantile-quantile, L'Abb√©, Baujat, bubble, and GOSH plots. The presence of publication bias (or more precisely, funnel plot asymmetry or &lsquo;small-study effects&rsquo;) and its potential impact on the results can be examined via the rank correlation and Egger's regression test, the trim and fill method, the test of excess significance, and by applying a variety of selection models.
</p>


<h3>The escalc Function</h3>

<p>[<code><a href="#topic+escalc">escalc</a></code>] Before a meta-analysis can be conducted, the relevant results from each study must be quantified in such a way that the resulting values can be further aggregated and compared. The <code><a href="#topic+escalc">escalc</a></code> function can be used to compute a wide variety of effect sizes or &lsquo;outcome measures&rsquo; (and the corresponding sampling variances) that are often used in meta-analyses (e.g., risk ratios, odds ratios, risk differences, mean differences, standardized mean differences, response ratios / ratios of means, raw or r-to-z transformed correlation coefficients). Measures for quantifying some characteristic of individual groups (e.g., in terms of means, proportions, or incidence rates and transformations thereof), measures of change (e.g., raw and standardized mean changes), and measures of variability (e.g., variability ratios and coefficient of variation ratios) are also available.
</p>


<h3>The rma.uni Function</h3>

<p>[<code><a href="#topic+rma.uni">rma.uni</a></code>] The various meta-analytic models that are typically used in practice are special cases of the general linear (mixed-effects) model. The <code><a href="#topic+rma.uni">rma.uni</a></code> function (with alias <code><a href="#topic+rma">rma</a></code>) provides a general framework for fitting such models. The function can be used in combination with any of the effect sizes or outcome measures computed with the <code><a href="#topic+escalc">escalc</a></code> function or, more generally, any set of estimates (with corresponding sampling variances or standard errors) one would like to analyze. The notation and models underlying the <code><a href="#topic+rma.uni">rma.uni</a></code> function are explained below.
</p>
<p>For a set of \(i = 1, \ldots, k\) independent studies, let \(y_i\) denote the observed value of the effect size or outcome measure in the \(i\textrm{th}\) study. Let \(\theta_i\) denote the corresponding (unknown) true effect/outcome, such that \[y_i \mid \theta_i \sim N(\theta_i, v_i).\] In other words, the observed effect sizes or outcomes are assumed to be unbiased and normally distributed estimates of the corresponding true effects/outcomes with sampling variances equal to \(v_i\) (where \(v_i\) is just the square of the standard errors of the estimates). The \(v_i\) values are assumed to be known. Depending on the outcome measure used, a bias correction, normalizing, and/or variance stabilizing transformation may be necessary to ensure that these assumptions are (at least approximately) true (e.g., the log transformation for odds/risk ratios, the bias correction for standardized mean differences, Fisher's r-to-z transformation for correlations; see <code><a href="#topic+escalc">escalc</a></code> for more details).
</p>
<p>According to the <b>random-effects model</b>, we further assume that \(\theta_i \sim N(\mu, \tau^2)\), that is, the true effects/outcomes are normally distributed with \(\mu\) denoting the average true effect/outcome and \(\tau^2\) the variance in the true effects/outcomes (\(\tau^2\) is therefore often referred to as the amount of &lsquo;heterogeneity&rsquo; in the true effects/outcomes). The random-effects model can also be written as \[y_i = \mu + u_i + \varepsilon_i,\] where \(u_i \sim N(0, \tau^2)\) and \(\varepsilon_i \sim N(0, v_i)\). The fitted model provides estimates of \(\mu\) and \(\tau^2\), that is, \[\hat{\mu} = \frac{\sum_{i=1}^k w_i y_i}{\sum_{i=1}^k w_i},\] where \(w_i = 1/(\hat{\tau}^2 + v_i)\) and \(\hat{\tau}^2\) denotes an estimate of \(\tau^2\) obtained with one of the many estimators that have described in the literature for this purpose (this is the standard &lsquo;inverse-variance&rsquo; method for random-effects models).
</p>
<p>A special case of the model above is the <b>equal-effects model</b> (also sometimes called the common-effects model) which arises when \(\tau^2 = 0\). In this case, the true effects/outcomes are homogeneous (i.e., \(\theta_1 = \theta_2 = \ldots = \theta_k \equiv \theta\)) and hence we can write the model as \[y_i = \theta + \varepsilon_i,\] where \(\theta\) denotes <em>the</em> true effect/outcome in the studies, which is estimated with \[\hat{\theta} = \frac{\sum_{i=1}^k w_i y_i}{\sum_{i=1}^k w_i},\] where \(w_i = 1/v_i\) (again, this is the standard &lsquo;inverse-variance&rsquo; method as described in the meta-analytic literature). Note that the commonly-used term &lsquo;fixed-effects model&rsquo; is not used here - for an explanation, see <a href="#topic+misc-models">here</a>.
</p>
<p>Study-level variables (often referred to as &lsquo;moderators&rsquo;) can also be included as predictors in meta-analytic models, leading to so-called &lsquo;meta-regression&rsquo; analyses (to examine whether the effects/outcomes tend to be larger/smaller under certain conditions or circumstances). When including moderator variables in a random-effects model, we obtain a <b>mixed-effects meta-regression model</b>. This model can be written as \[y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_{p'} x_{ip'} + u_i + \varepsilon_i,\] where \(u_i \sim N(0, \tau^2)\) and \(\varepsilon_i \sim N(0, v_i)\) as before and \(x_{ij}\) denotes the value of the \(j\textrm{th}\) moderator variable for the \(i\textrm{th}\) study (letting \(p = p' + 1\) denote the total number of coefficients in the model including the model intercept). Therefore, \(\beta_j\) denotes how the average true effect/outcome changes for a one-unit increase in \(x_{ij}\) and the model intercept \(\beta_0\) denotes the average true effect/outcome when the values of all moderator variables are equal to zero. The value of \(\tau^2\) in the mixed-effects model denotes the amount of &lsquo;residual heterogeneity&rsquo; in the true effects/outcomes (i.e., the amount of variability in the true effects/outcomes that is not accounted for by the moderators included in the model).
</p>


<h3>The rma.mh Function</h3>

<p>[<code><a href="#topic+rma.mh">rma.mh</a></code>] The Mantel-Haenszel method provides an alternative approach for fitting equal-effects models when dealing with studies providing data in the form of \(2 \times 2\) tables or in the form of event counts (i.e., person-time data) for two groups (Mantel &amp; Haenszel, 1959). The method is particularly advantageous when aggregating a large number of studies with small sample sizes (the so-called sparse data or increasing strata case). The Mantel-Haenszel method is implemented in the <code><a href="#topic+rma.mh">rma.mh</a></code> function. It can be used in combination with risk ratios, odds ratios, risk differences, incidence rate ratios, and incidence rate differences.
</p>


<h3>The rma.peto Function</h3>

<p>[<code><a href="#topic+rma.peto">rma.peto</a></code>] Yet another method that can be used in the context of a meta-analysis of \(2 \times 2\) table data is Peto's method (see Yusuf et al., 1985), implemented in the <code><a href="#topic+rma.peto">rma.peto</a></code> function. The method provides an estimate of the (log) odds ratio under an equal-effects model. The method is particularly advantageous when the event of interest is rare, but see the documentation of the function for some caveats.
</p>


<h3>The rma.glmm Function</h3>

<p>[<code><a href="#topic+rma.glmm">rma.glmm</a></code>] Dichotomous response variables and event counts (based on which one can calculate outcome measures such as odds ratios, incidence rate ratios, proportions, and incidence rates) are often assumed to arise from binomial and Poisson distributed data. Meta-analytic models that are directly based on such distributions are implemented in the <code><a href="#topic+rma.glmm">rma.glmm</a></code> function. These models are essentially special cases of generalized linear mixed-effects models (i.e., mixed-effects logistic and Poisson regression models). For \(2 \times 2\) table data, a mixed-effects conditional logistic model (based on the non-central hypergeometric distribution) is also available. Random/mixed-effects models with dichotomous data are often referred to as &lsquo;binomial-normal&rsquo; models in the meta-analytic literature. Analogously, for event count data, such models could be referred to as &lsquo;Poisson-normal&rsquo; models.
</p>


<h3>The rma.mv Function</h3>

<p>[<code><a href="#topic+rma.mv">rma.mv</a></code>] Standard meta-analytic models assume independence between the observed effect sizes or outcomes obtained from a set of studies. This assumption is often violated in practice. Dependencies can arise for a variety of reasons. For example, the sampling errors and/or true effects/outcomes may be correlated in multiple treatment studies (e.g., when multiple treatment groups are compared with a common control/reference group, such that the data from the control/reference group is used multiple times to compute the observed effect sizes or outcomes) or in multiple endpoint studies (e.g., when more than one effect size estimate or outcome is calculated based on the same sample of subjects due to the use of multiple endpoints or response variables). Correlations in the true effects/outcomes can also arise due to other forms of clustering (e.g., when multiple effects/outcomes derived from the same author, lab, or research group may be more similar to each other than effects/outcomes derived from different authors, labs, or research groups). In ecology and related fields, the shared phylogenetic history among the organisms studied (e.g., plants, fungi, animals) can also induce correlations among the effects/outcomes. The <code><a href="#topic+rma.mv">rma.mv</a></code> function can be used to fit suitable meta-analytic multivariate/multilevel models to such data, so that the non-independence in the effects/outcomes is accounted for. Network meta-analyses (also called multiple/mixed treatment comparisons) can also be carried out with this function.
</p>


<h3>Future Plans and Updates</h3>

<p>The <span class="pkg">metafor</span> package is a work in progress and is updated on a regular basis with new functions and options. With <code>metafor.news()</code>, you can read the &lsquo;<span class="file">NEWS</span>&rsquo; file of the package after installation. Comments, feedback, and suggestions for improvements are always welcome.
</p>


<h3>Citing the Package</h3>

<p>To cite the package, please use the following reference:
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1-48. <a href="https://doi.org/10.18637/jss.v036.i03">doi:10.18637/jss.v036.i03</a>
</p>


<h3>Getting Started with the Package</h3>

<p>The paper mentioned above is a good starting place for those interested in using the package. The purpose of the article is to provide a general overview of the package and its capabilities (as of version 1.4-0). Not all of the functions and options are described in the paper, but it should provide a useful introduction to the package. The paper can be freely downloaded from the URL given above or can be directly loaded with the command <code>vignette("metafor")</code>.
</p>
<p>In addition to reading the paper, carefully read this page and then the help pages for the <code><a href="#topic+escalc">escalc</a></code> and the <code><a href="#topic+rma.uni">rma.uni</a></code> functions (or the <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and/or <code><a href="#topic+rma.mv">rma.mv</a></code> functions if you intend to use these methods). The help pages for these functions provide links to many additional functions, which can be used after fitting a model. You can also read the entire documentation online at <a href="https://wviechtb.github.io/metafor/">https://wviechtb.github.io/metafor/</a> (where it is nicely formatted and the output from all examples is provided).
</p>
<p>A (pdf) diagram showing the various functions in the metafor package (and how they are related to each other) can be opened with the command <code>vignette("diagram")</code>.
</p>
<p>Finally, additional information about the package, several detailed analysis examples, examples of plots and figures provided by the package (with the corresponding code), some additional tips and notes, and a FAQ can be found on the package website at <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <br />
package website: <a href="https://www.metafor-project.org">https://www.metafor-project.org</a> <br />
author homepage: <code style="white-space: pre;">&#8288;https://www.wvbauer.com&#8288;</code> <br />
</p>
<p>Suggestions on how to obtain help with using the package can found on the package website at: <a href="https://www.metafor-project.org/doku.php/help">https://www.metafor-project.org/doku.php/help</a>
</p>


<h3>References</h3>

<p>Cooper, H., Hedges, L. V., &amp; Valentine, J. C. (Eds.) (2009). <em>The handbook of research synthesis and meta-analysis</em> (2nd ed.). New York: Russell Sage Foundation.
</p>
<p>Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical methods for meta-analysis</em>. San Diego, CA: Academic Press.
</p>
<p>Mantel, N., &amp; Haenszel, W. (1959). Statistical aspects of the analysis of data from retrospective studies of disease. <em>Journal of the National Cancer Institute</em>, <b>22</b>(4), 719&ndash;748. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/jnci/22.4.719&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Yusuf, S., Peto, R., Lewis, J., Collins, R., &amp; Sleight, P. (1985). Beta blockade during and after myocardial infarction: An overview of the randomized trials. <em>Progress in Cardiovascular Disease</em>, <b>27</b>(5), 335&ndash;371. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/s0033-0620(85)80003-7&#8288;</code>
</p>

<hr>
<h2 id='addpoly'>Add Polygons to Forest Plots</h2><span id='topic+addpoly'></span>

<h3>Description</h3>

<p>Function to add polygons (sometimes called &lsquo;diamonds&rsquo;) to a forest plot, for example to indicate summary estimates for subgroups of studies or to indicate fitted/predicted values based on models involving moderators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addpoly(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addpoly_+3A_x">x</code></td>
<td>
<p>either an object of class <code>"rma"</code>, an object of class <code>"predict.rma"</code>, or the values at which polygons should be drawn. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="addpoly_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, methods exist for three types of situations.
</p>
<p>In the first case, object <code>x</code> is a fitted model coming from the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, or <code><a href="#topic+rma.mv">rma.mv</a></code> functions. The model must either be an equal- or a random-effects model, that is, the model should not contain any moderators. The corresponding method is <code><a href="#topic+addpoly.rma">addpoly.rma</a></code>. It can be used to add a polygon to an existing forest plot (usually at the bottom), showing the summary estimate (with its confidence interval) based on the fitted model.
</p>
<p>Alternatively, <code>x</code> can be an object of class <code>"predict.rma"</code> obtained with the <code><a href="#topic+predict.rma">predict</a></code> function. In this case, polygons based on the predicted values are drawn. The corresponding method is <code><a href="#topic+addpoly.predict.rma">addpoly.predict.rma</a></code>.
</p>
<p>Alternatively, object <code>x</code> can be a vector with values at which one or more polygons should be drawn. The corresponding method is <code><a href="#topic+addpoly.default">addpoly.default</a></code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+addpoly.rma">addpoly.rma</a></code>, <code><a href="#topic+addpoly.predict.rma">addpoly.predict.rma</a></code>, and <code><a href="#topic+addpoly.default">addpoly.default</a></code> for the specific method functions.
</p>
<p><code><a href="#topic+forest">forest</a></code> for functions to draw forest plots to which polygons can be added.
</p>

<hr>
<h2 id='addpoly.default'>Add Polygons to Forest Plots (Default Method)</h2><span id='topic+addpoly.default'></span>

<h3>Description</h3>

<p>Function to add one or more polygons to a forest plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
addpoly(x, vi, sei, ci.lb, ci.ub, pi.lb, pi.ub,
        rows=-1, level, annotate, digits, width, mlab,
        transf, atransf, targs, efac, col, border, lty, fonts, cex,
        constarea=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addpoly.default_+3A_x">x</code></td>
<td>
<p>vector with the values at which the polygons should be drawn.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding variances.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_ci.lb">ci.lb</code></td>
<td>
<p>vector with the corresponding lower confidence interval bounds. Not needed if <code>vi</code> or <code>sei</code> is specified. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_ci.ub">ci.ub</code></td>
<td>
<p>vector with the corresponding upper confidence interval bounds. Not needed if <code>vi</code> or <code>sei</code> is specified. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_pi.lb">pi.lb</code></td>
<td>
<p>optional vector with the corresponding lower prediction interval bounds.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_pi.ub">pi.ub</code></td>
<td>
<p>optional vector with the corresponding upper prediction interval bounds.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_rows">rows</code></td>
<td>
<p>vector to specify the rows (or more generally, the horizontal positions) for plotting the polygons (defaults is <code>-1</code>). Can also be a single value to specify the row (horizontal position) of the first polygon (the remaining polygons are then plotted below this starting row).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_level">level</code></td>
<td>
<p>optional numeric value between 0 and 100 to specify the confidence interval level (see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_annotate">annotate</code></td>
<td>
<p>optional logical to specify whether annotations should be added to the plot for the polygons that are drawn.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the annotations should be rounded.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_mlab">mlab</code></td>
<td>
<p>optional character vector with the same length as <code>x</code> giving labels for the polygons that are drawn.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the <code>x</code> values and confidence interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_efac">efac</code></td>
<td>
<p>optional vertical expansion factor for the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_border">border</code></td>
<td>
<p>optional character string to specify the border color of the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the prediction interval.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the labels and annotations.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_cex">cex</code></td>
<td>
<p>optional symbol expansion factor.</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_constarea">constarea</code></td>
<td>
<p>logical to specify whether the height of the polygons (when adding multiple) should be adjusted so that the area of the polygons is constant (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="addpoly.default_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to add one or more polygons to an existing forest plot created with the <code><a href="#topic+forest">forest</a></code> function. For example, summary estimates based on a model involving moderators can be added to the plot this way (see &lsquo;Examples&rsquo;).
</p>
<p>To use the function, one should specify the values at which the polygons should be drawn (via the <code>x</code> argument) together with the corresponding variances (via the <code>vi</code> argument) or with the corresponding standard errors (via the <code>sei</code> argument). Alternatively, one can specify the values at which the polygons should be drawn together with the corresponding confidence interval bounds (via the <code>ci.lb</code> and <code>ci.ub</code> arguments). Optionally, one can also specify the bounds of the corresponding prediction interval bounds via the <code>pi.lb</code> and <code>pi.ub</code> arguments.
</p>
<p>If unspecified, arguments <code>level</code>, <code>annotate</code>, <code>digits</code>, <code>width</code>, <code>transf</code>, <code>atransf</code>, <code>targs</code>, <code>efac</code> (only if the forest plot was created with <code><a href="#topic+forest.rma">forest.rma</a></code>), <code>fonts</code>, <code>cex</code>, <code>annosym</code>, and <code>textpos</code> are automatically set equal to the same values that were used when creating the forest plot.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for functions to draw forest plots to which polygons can be added.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude as a moderator
res &lt;- rma(yi, vi, mods = ~ ablat, slab=paste(author, year, sep=", "), data=dat)

### forest plot of the observed risk ratios
forest(res, addfit=FALSE, atransf=exp, xlim=c(-9,5), ylim=c(-4.5,15), cex=0.9,
       order=ablat, ilab=ablat, ilab.xpos=-4.5,
       header="Author(s) and Year", top=2)

### predicted average log risk ratios for 10, 30, and 50 degrees absolute latitude
x &lt;- predict(res, newmods=c(10, 30, 50))

### add predicted average risk ratios to the forest plot
addpoly(x$pred, sei=x$se, rows=-2, mlab=c("- at 10 Degrees", "- at 30 Degrees", "- at 50 Degrees"))
abline(h=0)
text(-9, -1, "Model-Based Estimates:", pos=4, cex=0.9)
text(-4.5, res$k+2, "Latitude", cex=0.9, font=2)
</code></pre>

<hr>
<h2 id='addpoly.predict.rma'>Add Polygons to Forest Plots (Method for 'predict.rma' Objects)</h2><span id='topic+addpoly.predict.rma'></span>

<h3>Description</h3>

<p>Function to add one or more polygons to a forest plot based on an object of class <code>"predict.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'predict.rma'
addpoly(x, rows=-2, annotate,
        addpred=FALSE, digits, width, mlab, transf, atransf, targs,
        efac, col, border, lty, fonts, cex, constarea=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addpoly.predict.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"predict.rma"</code>.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_rows">rows</code></td>
<td>
<p>vector to specify the rows (or more generally, the horizontal positions) for plotting the polygons (defaults is <code>-2</code>). Can also be a single value to specify the row (horizontal position) of the first polygon (the remaining polygons are then plotted below this starting row).</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_annotate">annotate</code></td>
<td>
<p>optional logical to specify whether annotations should be added to the plot for the polygons that are drawn.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_addpred">addpred</code></td>
<td>
<p>logical to specify whether the bounds of the prediction interval should be added to the plot (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the annotations should be rounded.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_mlab">mlab</code></td>
<td>
<p>optional character vector with the same length as <code>x</code> giving labels for the polygons that are drawn.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the <code>x</code> values and confidence interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_efac">efac</code></td>
<td>
<p>optional vertical expansion factor for the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_border">border</code></td>
<td>
<p>optional character string to specify the border color of the polygons.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the prediction interval.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the labels and annotations.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_cex">cex</code></td>
<td>
<p>optional symbol expansion factor.</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_constarea">constarea</code></td>
<td>
<p>logical to specify whether the height of the polygons (when adding multiple) should be adjusted so that the area of the polygons is constant (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="addpoly.predict.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to add one or more polygons to an existing forest plot created with the <code><a href="#topic+forest">forest</a></code> function. For example, summary estimates based on a model involving moderators can be added to the plot this way (see &lsquo;Examples&rsquo;).
</p>
<p>To use the function, one should specify the values at which the polygons should be drawn (via the <code>x</code> argument) together with the corresponding variances (via the <code>vi</code> argument) or with the corresponding standard errors (via the <code>sei</code> argument). Alternatively, one can specify the values at which the polygons should be drawn together with the corresponding confidence interval bounds (via the <code>ci.lb</code> and <code>ci.ub</code> arguments). Optionally, one can also specify the bounds of the corresponding prediction interval bounds via the <code>pi.lb</code> and <code>pi.ub</code> arguments.
</p>
<p>If unspecified, arguments <code>annotate</code>, <code>digits</code>, <code>width</code>, <code>transf</code>, <code>atransf</code>, <code>targs</code>, <code>efac</code> (only if the forest plot was created with <code><a href="#topic+forest.rma">forest.rma</a></code>), <code>fonts</code>, <code>cex</code>, <code>annosym</code>, and <code>textpos</code> are automatically set equal to the same values that were used when creating the forest plot.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for functions to draw forest plots to which polygons can be added.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
              data=dat.bcg, slab=paste(author, year, sep=", "))

### forest plot of the observed risk ratios
with(dat, forest(yi, vi, atransf=exp, xlim=c(-9,5), ylim=c(-4.5,15),
                 at=log(c(0.05, 0.25, 1, 4)), cex=0.9, order=alloc,
                 ilab=alloc, ilab.xpos=-4.5,
                 header="Author(s) and Year", top=2))

### fit mixed-effects model with allocation method as a moderator
res &lt;- rma(yi, vi, mods = ~ 0 + alloc, data=dat)

### predicted log risk ratios for the different allocation methods
x &lt;- predict(res, newmods=diag(3))

### add predicted average risk ratios to the forest plot
addpoly(x, efac=1.3, col="gray", addpred=TRUE,
        mlab=c("Alternate Allocation", "Random Allocation", "Systematic Allocation"))
abline(h=0)
text(-9, -1, "Model-Based Estimates:", pos=4, cex=0.9, font=2)
text(-4.5, res$k+2, "Allocation", cex=0.9, font=2)
</code></pre>

<hr>
<h2 id='addpoly.rma'>Add Polygons to Forest Plots (Method for 'rma' Objects)</h2><span id='topic+addpoly.rma'></span>

<h3>Description</h3>

<p>Function to add a polygon to a forest plot showing the summary estimate with corresponding confidence interval based on an object of class <code>"rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
addpoly(x, row=-2, level=x$level, annotate,
        addpred=FALSE, digits, width, mlab, transf, atransf, targs,
        efac, col, border, lty, fonts, cex, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addpoly.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_row">row</code></td>
<td>
<p>numeric value to specify the row (or more generally, the horizontal position) for plotting the polygon (the default is <code>-2</code>).</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (see <a href="#topic+misc-options">here</a> for details). The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_annotate">annotate</code></td>
<td>
<p>optional logical to specify whether annotations for the summary estimate should be added to the plot.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_addpred">addpred</code></td>
<td>
<p>logical to specify whether the bounds of the prediction interval should be added to the plot (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the annotations should be rounded.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_mlab">mlab</code></td>
<td>
<p>optional character string giving a label for the summary estimate polygon. If unspecified, the function sets a default label.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the summary estimate and confidence interval bound (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>).</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_efac">efac</code></td>
<td>
<p>optional vertical expansion factor for the polygon.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the polygon.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_border">border</code></td>
<td>
<p>optional character string to specify the border color of the polygon.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the prediction interval.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the label and annotations.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_cex">cex</code></td>
<td>
<p>optional symbol expansion factor.</p>
</td></tr>
<tr><td><code id="addpoly.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to add a four-sided polygon, sometimes called a summary &lsquo;diamond&rsquo;, to an existing forest plot created with the <code><a href="#topic+forest">forest</a></code> function. The polygon shows the summary estimate (with its confidence interval bounds) based on an equal- or a random-effects model. Using this function, summary estimates based on different types of models can be shown in the same plot. Also, summary estimates based on a subgrouping of the studies can be added to the plot this way. See &lsquo;Examples&rsquo;.
</p>
<p>If unspecified, arguments <code>annotate</code>, <code>digits</code>, <code>width</code>, <code>transf</code>, <code>atransf</code>, <code>targs</code>, <code>efac</code> (only if the forest plot was created with <code><a href="#topic+forest.rma">forest.rma</a></code>), <code>fonts</code>, <code>cex</code>, <code>annosym</code>, and <code>textpos</code> are automatically set equal to the same values that were used when creating the forest plot.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for functions to draw forest plots to which polygons can be added.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### meta-analysis of the log risk ratios using the Mantel-Haenszel method
res &lt;- rma.mh(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
              slab=paste(author, year, sep=", "))

### forest plot of the observed risk ratios with summary estimate
forest(res, atransf=exp, xlim=c(-8,6), ylim=c(-2.5,15), header=TRUE, top=2)

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### add the summary estimate from the random-effects model to the forest plot
addpoly(res)

### forest plot with subgrouping of studies and summaries per subgroup
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           slab=paste(author, year, sep=", "))
tmp &lt;- forest(res, xlim=c(-16, 4.6), at=log(c(0.05, 0.25, 1, 4)), atransf=exp,
              ilab=cbind(tpos, tneg, cpos, cneg), ilab.xpos=c(-9.5,-8,-6,-4.5),
              cex=0.75, ylim=c(-1, 27), order=alloc, rows=c(3:4,9:15,20:23),
              mlab="RE Model for All Studies", header="Author(s) and Year")
op &lt;- par(cex=0.75, font=2)
text(c(-9.5,-8,-6,-4.5), tmp$ylim[2]-1, c("TB+", "TB-", "TB+", "TB-"))
text(c(-8.75,-5.25),     tmp$ylim[2],   c("Vaccinated", "Control"))
par(font=4)
text(-16, c(24,16,5), c("Systematic Allocation", "Random Allocation",
                        "Alternate Allocation"), pos=4)
par(op)
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           subset=(alloc=="systematic"))
addpoly(res, row=18.5, mlab="RE Model for Subgroup")
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           subset=(alloc=="random"))
addpoly(res, row=7.5, mlab="RE Model for Subgroup")
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           subset=(alloc=="alternate"))
addpoly(res, row=1.5, mlab="RE Model for Subgroup")
</code></pre>

<hr>
<h2 id='aggregate.escalc'>Aggregate Multiple Effect Sizes or Outcomes Within Studies</h2><span id='topic+aggregate'></span><span id='topic+aggregate.escalc'></span>

<h3>Description</h3>

<p>Function to aggregate multiple effect sizes or outcomes belonging to the same study (or to the same level of some other clustering variable) into a single combined effect size or outcome. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'escalc'
aggregate(x, cluster, time, obs, V, struct="CS", rho, phi,
          weighted=TRUE, checkpd=TRUE, fun, na.rm=TRUE,
          addk=FALSE, subset, select, digits, var.names, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate.escalc_+3A_x">x</code></td>
<td>
<p>an object of class <code>"escalc"</code>.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_cluster">cluster</code></td>
<td>
<p>vector to specify the clustering variable (e.g., study).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_time">time</code></td>
<td>
<p>optional vector to specify the time points (only relevant when <code>struct="CAR"</code>, <code>"CS+CAR"</code>, or <code>"CS*CAR"</code>).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_obs">obs</code></td>
<td>
<p>optional vector to distinguish different observed effect sizes or outcomes measured at the same time point (only relevant when <code>struct="CS*CAR"</code>).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_v">V</code></td>
<td>
<p>optional argument to specify the variance-covariance matrix of the sampling errors. If unspecified, argument <code>struct</code> is used to specify the variance-covariance structure.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_struct">struct</code></td>
<td>
<p>character string to specify the variance-covariance structure of the sampling errors within the same cluster (either <code>"ID"</code>, <code>"CS"</code>, <code>"CAR"</code>, <code>"CS+CAR"</code>, or <code>"CS*CAR"</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_rho">rho</code></td>
<td>
<p>value of the correlation of the sampling errors within clusters (when <code>struct="CS"</code>, <code>"CS+CAR"</code>, or <code>"CS*CAR"</code>). Can also be a vector with the value of the correlation for each cluster.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_phi">phi</code></td>
<td>
<p>value of the autocorrelation of the sampling errors within clusters (when <code>struct="CAR"</code>, <code>"CS+CAR"</code>, or <code>"CS*CAR"</code>). Can also be a vector with the value of the autocorrelation for each cluster.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_weighted">weighted</code></td>
<td>
<p>logical to specify whether estimates within clusters should be aggregated using inverse-variance weighting (the default is <code>TRUE</code>). If set to <code>FALSE</code>, unweighted averages are computed.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_checkpd">checkpd</code></td>
<td>
<p>logical to specify whether to check that the variance-covariance matrices of the sampling errors within clusters are positive definite (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_fun">fun</code></td>
<td>
<p>optional list with three functions for aggregating other variables besides the effect sizes or outcomes within clusters (for numeric/integer variables, for logicals, and for all other types, respectively).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_na.rm">na.rm</code></td>
<td>
<p>logical to specify whether <code>NA</code> values should be removed before aggregating values within clusters (the default is <code>TRUE</code>). Can also be a vector with two logicals (the first pertaining to the effect sizes or outcomes, the second to all other variables).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_addk">addk</code></td>
<td>
<p>logical to specify whether to add the cluster size as a new variable (called <code>ki</code>) to the dataset (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of rows to include when aggregating the effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_select">select</code></td>
<td>
<p>optional vector to specify the names of the variables to include in the aggregated dataset.</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_var.names">var.names</code></td>
<td>
<p>optional character vector with two elements to specify the name of the variable that contains the observed effect sizes or outcomes and the name of the variable with the corresponding sampling variances (when unspecified, the function attempts to set these automatically based on the object).</p>
</td></tr>
<tr><td><code id="aggregate.escalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In many meta-analyses, multiple effect sizes or outcomes can be extracted from the same study. Ideally, such structures should be analyzed using an appropriate multilevel/multivariate model as can be fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function. However, there may occasionally be reasons for aggregating multiple effect sizes or outcomes belonging to the same study (or to the same level of some other clustering variable) into a single combined effect size or outcome. The present function can be used for this purpose.
</p>
<p>The input must be an object of class <code>"escalc"</code>. The error &lsquo;<code>Error in match.fun(FUN): argument "FUN" is missing, with no default</code>&rsquo; indicates that a regular data frame was passed to the function, but this does not work. One can turn a regular data frame (containing the effect sizes or outcomes and the corresponding sampling variances) into an <code>"escalc"</code> object with the <code><a href="#topic+escalc">escalc</a></code> function. See the &lsquo;Examples&rsquo; below for an illustration of this.
</p>
<p>The <code>cluster</code> variable is used to specify which estimates/outcomes belong to the same study/cluster.
</p>
<p>In the simplest case, the estimates/outcomes within clusters (or, to be precise, their sampling errors) are assumed to be independent. This is usually a safe assumption as long as each study participant (or whatever the study units are) only contributes data to a single estimate/outcome. For example, if a study provides effect size estimates for male and female subjects separately, then the sampling errors can usually be assumed to be independent. In this case, one can set <code>struct="ID"</code> and multiple estimates/outcomes within the same cluster are combined using standard inverse-variance weighting (i.e., using weighted least squares) under the assumption of independence.
</p>
<p>In other cases, the estimates/outcomes within clusters cannot be assumed to be independent. For example, if multiple effect size estimates are computed for the same group of subjects (e.g., based on different scales to measure some construct of interest), then the estimates are likely to be correlated. If the actual correlation between the estimates is unknown, one can often still make an educated guess and set argument <code>rho</code> to this value, which is then assumed to be the same for all pairs of estimates within clusters when <code>struct="CS"</code> (for a compound symmetric structure). Multiple estimates/outcomes within the same cluster are then combined using inverse-variance weighting taking their correlation into consideration (i.e., using generalized least squares). One can also specify a different value of <code>rho</code> for each cluster by passing a vector (of the same length as the number of clusters) to this argument.
</p>
<p>If multiple effect size estimates are computed for the same group of subjects at different time points, then it may be more sensible to assume that the correlation between estimates decreases as a function of the distance between the time points. If so, one can specify <code>struct="CAR"</code> (for a continuous-time autoregressive structure), set <code>phi</code> to the autocorrelation (for two estimates one time-unit apart), and use argument <code>time</code> to specify the actual time points corresponding to the estimates. The correlation between two estimates, \(y_{it}\) and \(y_{it'}\), in the \(i\textrm{th}\) cluster, with time points \(\textrm{time}_{it}\) and \(\textrm{time}_{it'}\), is then given by \(\phi^{|\textrm{time}_{it} - \textrm{time}_{it'}|}\). One can also specify a different value of <code>phi</code> for each cluster by passing a vector (of the same length as the number of clusters) to this argument.
</p>
<p>One can also combine the compound symmetric and autoregressive structures if there are multiple time points and multiple observed effect sizes or outcomes at these time points. One option is <code>struct="CS+CAR"</code>. In this case, one must specify the <code>time</code> argument and both <code>rho</code> and <code>phi</code>. The correlation between two estimates, \(y_{it}\) and \(y_{it'}\), in the \(i\textrm{th}\) cluster, with time points \(\textrm{time}_{it}\) and \(\textrm{time}_{it'}\), is then given by \(\rho + (1 - \rho) \phi^{|\textrm{time}_{it} - \textrm{time}_{it'}|}\).
</p>
<p>Alternatively, one can specify <code>struct="CS*CAR"</code>. In this case, one must specify both the <code>time</code> and <code>obs</code> arguments and both <code>rho</code> and <code>phi</code>. The correlation between two estimates, \(y_{ijt}\) and \(y_{ijt'}\), with the same value for <code>obs</code> but different values for <code>time</code>, is then given by \(\phi^{|\textrm{time}_{ijt} - \textrm{time}_{ijt'}|}\), the correlation between two estimates, \(y_{ijt}\) and \(y_{ij't}\), with different values for <code>obs</code> but the same value for <code>time</code>, is then given by \(\rho\), and the correlation between two estimates, \(y_{ijt}\) and \(y_{ij't'}\), with different values for <code>obs</code> and different values for <code>time</code>, is then given by \(\rho \times \phi^{|\textrm{time}_{ijt} - \textrm{time}_{ijt'}|}\).
</p>
<p>Finally, if one actually knows the correlation (and hence the covariance) between each pair of estimates (or has an approximation thereof), one can also specify the entire variance-covariance matrix of the estimates (or more precisely, their sampling errors) via the <code>V</code> argument (in this case, arguments <code>struct</code>, <code>time</code>, <code>obs</code>, <code>rho</code>, and <code>phi</code> are ignored). Note that the <code><a href="#topic+vcalc">vcalc</a></code> function can be used to construct such a <code>V</code> matrix and provides even more flexibility for specifying various types of dependencies. See the &lsquo;Examples&rsquo; below for an illustration of this.
</p>
<p>Instead of using inverse-variance weighting (i.e., weighted/generalized least squares) to combine the estimates within clusters, one can set <code>weighted=FALSE</code> in which case the estimates are averaged within clusters without any weighting (although the correlations between estimates as specified are still taken into consideration).
</p>
<p>Other variables (besides the estimates) will also be aggregated to the cluster level. By default, numeric/integer type variables are averaged, logicals are also averaged (yielding the proportion of <code>TRUE</code> values), and for all other types of variables (e.g., character variables or factors) the most frequent category/level is returned. One can also specify a list of three functions via the <code>fun</code> argument for aggregating variables belonging to these three types.
</p>
<p>Argument <code>na.rm</code> controls how missing values should be handled. By default, any missing estimates are first removed before aggregating the non-missing values within each cluster. The same applies when aggregating the other variables. One can also specify a vector with two logicals for the <code>na.rm</code> argument to control how missing values should be handled when aggregating the estimates and when aggregating all other variables.
</p>


<h3>Value</h3>

<p>An object of class <code>c("escalc","data.frame")</code> that contains the (selected) variables aggregated to the cluster level.
</p>
<p>The object is formatted and printed with the <code><a href="#topic+print.escalc">print</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to create <code>escalc</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat' and examine data
dat &lt;- dat.konstantopoulos2011
head(dat, 11)

### aggregate estimates to the district level, assuming independent sampling
### errors for multiples studies/schools within the same district
agg &lt;- aggregate(dat, cluster=district, struct="ID", addk=TRUE)
agg

### copy data into 'dat' and examine data
dat &lt;- dat.assink2016
head(dat, 19)

### note: 'dat' is an 'escalc' object
class(dat)

### turn 'dat' into a regular data frame
dat &lt;- as.data.frame(dat)
class(dat)

### turn data frame into an 'escalc' object
dat &lt;- escalc(measure="SMD", yi=yi, vi=vi, data=dat)
class(dat)

### aggregate the estimates to the study level, assuming a CS structure for
### the sampling errors within studies with a correlation of 0.6
agg &lt;- aggregate(dat, cluster=study, rho=0.6)
agg

### use vcalc() and then the V argument
V &lt;- vcalc(vi, cluster=study, obs=esid, data=dat, rho=0.6)
agg &lt;- aggregate(dat, cluster=study, V=V)
agg

### use a correlation of 0.7 for effect sizes corresponding to the same type of
### delinquent behavior and a correlation of 0.5 for effect sizes corresponding
### to different types of delinquent behavior
V &lt;- vcalc(vi, cluster=study, type=deltype, obs=esid, data=dat, rho=c(0.7, 0.5))
agg &lt;- aggregate(dat, cluster=study, V=V)
agg

### reshape 'dat.ishak2007' into long format
dat &lt;- dat.ishak2007
dat &lt;- reshape(dat.ishak2007, direction="long", idvar="study", v.names=c("yi","vi"),
               varying=list(c(2,4,6,8), c(3,5,7,9)))
dat &lt;- dat[order(study, time),]
dat &lt;- dat[!is.na(yi),]
rownames(dat) &lt;- NULL
head(dat, 8)

### aggregate the estimates to the study level, assuming a CAR structure for
### the sampling errors within studies with an autocorrelation of 0.9
agg &lt;- aggregate(dat, cluster=study, struct="CAR", time=time, phi=0.9)
head(agg, 5)
</code></pre>

<hr>
<h2 id='anova.rma'>Likelihood Ratio and Wald-Type Tests for 'rma' Objects</h2><span id='topic+anova'></span><span id='topic+anova.rma'></span>

<h3>Description</h3>

<p>For two (nested) models of class <code>"rma.uni"</code> or <code>"rma.mv"</code>, the function provides a full versus reduced model comparison in terms of model fit statistics and a likelihood ratio test. When a single model is specified, a Wald-type test of one or more model coefficients or linear combinations thereof is carried out. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
anova(object, object2, btt, X, att, Z, rhs, digits, refit=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma.uni"</code> or <code>"rma.mv"</code>.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_object2">object2</code></td>
<td>
<p>an (optional) object of class <code>"rma.uni"</code> or <code>"rma.mv"</code>. Only relevant when conducting a model comparison and likelihood ratio test. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_btt">btt</code></td>
<td>
<p>optional vector of indices (or list thereof) to specify which coefficients should be included in the Wald-type test. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_x">X</code></td>
<td>
<p>optional numeric vector or matrix to specify one or more linear combinations of the coefficients in the model that should be tested. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_att">att</code></td>
<td>
<p>optional vector of indices (or list thereof) to specify which scale coefficients should be included in the Wald-type test. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. See &lsquo;Details&rsquo;. Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>).</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_z">Z</code></td>
<td>
<p>optional numeric vector or matrix to specify one or more linear combinations of the scale coefficients in the model that should be tested. See &lsquo;Details&rsquo;. Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>).</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_rhs">rhs</code></td>
<td>
<p>optional scalar or vector of values for the right-hand side of the null hypothesis when testing a set of coefficients (via <code>btt</code> or <code>att</code>) or linear combinations thereof (via <code>X</code> or <code>Z</code>). If unspecified, this defaults to a vector of zeros of the appropriate length. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_refit">refit</code></td>
<td>
<p>logical to indicate whether models fitted with REML estimation and differing in their fixed effects should be refitted with ML estimation when conducting a likelihood ratio test (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="anova.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used in three different ways:
</p>

<ol>
<li><p> When a single model is specified (via argument <code>object</code>), the function provides a Wald-type test of one or more model coefficients, that is, \[\mbox{H}_0{:}\; \beta_{j \in \texttt{btt}} = 0,\] where \(\beta_{j \in \texttt{btt}}\) is the set of coefficients to be tested (by default whether the set of coefficients is significantly different from zero, but one can specify a different value under the null hypothesis via argument <code>rhs</code>).
</p>
<p>In particular, for equal- or random-effects models (i.e., models without moderators), this is just the test of the single coefficient of the model (i.e., \(\mbox{H}_0{:}\; \theta = 0\) or \(\mbox{H}_0{:}\; \mu = 0\)). For models including moderators, an omnibus test of all model coefficients is conducted that excludes the intercept (the first coefficient) if it is included in the model. If no intercept is included in the model, then the omnibus test includes all coefficients in the model including the first.
</p>
<p>Alternatively, one can manually specify the indices of the coefficients to test via the <code>btt</code> (&lsquo;betas to test&rsquo;) argument. For example, with <code>btt=c(3,4)</code>, only the third and fourth coefficients from the model are included in the test (if an intercept is included in the model, then it corresponds to the first coefficient in the model). Instead of specifying the coefficient numbers, one can specify a string for <code>btt</code>. In that case, <code><a href="base.html#topic+grep">grep</a></code> will be used to search for all coefficient names that match the string (and hence, one can use regular expressions to fine-tune the search for matching strings). Using the <code>btt</code> argument, one can for example select all coefficients corresponding to a particular factor to test if the factor as a whole is significant. One can also specify a list of indices/strings, in which case tests of all list elements will be conducted. See &lsquo;Examples&rsquo;.
</p>
<p>For location-scale models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, one can use the <code>att</code> argument in an analogous manner to specify the indices of the scale coefficients to test (i.e., \(\mbox{H}_0{:}\; \alpha_{j \in \texttt{att}} = 0\), where \(\alpha_{j \in \texttt{att}}\) is the set of coefficients to be tested).
</p>
</li>
<li><p> When a single model is specified (via argument <code>object</code>), one can use the <code>X</code> argument\(^1\) to specify a linear combination of the coefficients in the model that should be tested using a Wald-type test, that is, \[\mbox{H}_0{:}\; X \beta = 0,\] where <code>X</code> is a (row) vector of the same length as there are coefficients in the model (by default whether the linear combination is significantly different from zero, but one can specify a different value under the null hypothesis via argument <code>rhs</code>). If a matrix of linear combinations is specified, each row defines a particular linear combination to be tested (if <code>rhs</code> is used, then it should either be a scalar or of the same length as the number of combinations to be tested). If the matrix is of full rank, an omnibus Wald-type test of all linear combinations is also provided. Linear combinations can also be obtained with the <code><a href="#topic+predict.rma">predict</a></code> function, which provides corresponding confidence intervals.
</p>
<p>For location-scale models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, one can use the <code>Z</code> argument in an analogous manner to specify one or multiple linear combinations of the scale coefficients in the model that should be tested (i.e., \(\mbox{H}_0{:}\; Z \alpha = 0\)).
</p>
</li>
<li><p> When specifying two models for comparison (via arguments <code>object</code> and <code>object2</code>), the function provides a likelihood ratio test (LRT) comparing the two models. The two models must be based on the same set of data, must be of the same class, and should be nested for the LRT to make sense. Also, LRTs are not meaningful when using REML estimation and the two models differ in terms of their fixed effects (setting <code>refit=TRUE</code> automatically refits the two models using ML estimation). Also, the theory underlying LRTs is only really applicable when comparing models that were fitted with ML/REML estimation, so if some other estimation was used to fit the two models, the results should be treated with caution.
</p>
</li></ol>

<p>&mdash;&mdash;&mdash;
</p>
<p>\(^1\) This argument used to be called <code>L</code>, but was renamed to <code>X</code> (but using <code>L</code> in place of <code>X</code> still works).
</p>


<h3>Value</h3>

<p>An object of class <code>"anova.rma"</code>. When a single model is specified (without any further arguments or together with the <code>btt</code> or <code>att</code> argument), the object is a list containing the following components:
</p>
<table>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the Wald-type test of the model coefficients.</p>
</td></tr>
<tr><td><code>QMdf</code></td>
<td>
<p>corresponding degrees of freedom.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>btt</code></td>
<td>
<p>indices of the coefficients tested by the Wald-type test.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of outcomes included in the analysis.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>number of coefficients in the model (including the intercept).</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of coefficients included in the Wald-type test.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>When <code>btt</code> or <code>att</code> was a list, then the object is a list of class <code>"list.anova.rma"</code>, where each element is an <code>"anova.rma"</code> object as described above.
</p>
<p>When argument <code>X</code> is used, the object is a list containing the following components:
</p>
<table>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the omnibus Wald-type test of all linear combinations.</p>
</td></tr>
<tr><td><code>QMdf</code></td>
<td>
<p>corresponding degrees of freedom.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>hyp</code></td>
<td>
<p>description of the linear combinations tested.</p>
</td></tr>
<tr><td><code>Xb</code></td>
<td>
<p>values of the linear combinations.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard errors of the linear combinations.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the linear combinations.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
</table>
<p>When two models are specified, the object is a list containing the following components:
</p>
<table>
<tr><td><code>fit.stats.f</code></td>
<td>
<p>log-likelihood, deviance, AIC, BIC, and AICc for the full model.</p>
</td></tr>
<tr><td><code>fit.stats.r</code></td>
<td>
<p>log-likelihood, deviance, AIC, BIC, and AICc for the reduced model.</p>
</td></tr>
<tr><td><code>parms.f</code></td>
<td>
<p>number of parameters in the full model.</p>
</td></tr>
<tr><td><code>parms.r</code></td>
<td>
<p>number of parameters in the reduced model.</p>
</td></tr>
<tr><td><code>LRT</code></td>
<td>
<p>likelihood ratio test statistic.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>QE.f</code></td>
<td>
<p>test statistic of the test for (residual) heterogeneity from the full model.</p>
</td></tr>
<tr><td><code>QE.r</code></td>
<td>
<p>test statistic of the test for (residual) heterogeneity from the reduced model.</p>
</td></tr>
<tr><td><code>tau2.f</code></td>
<td>
<p>estimated \(\tau^2\) value from the full model. <code>NA</code> for <code>"rma.mv"</code> objects.</p>
</td></tr>
<tr><td><code>tau2.r</code></td>
<td>
<p>estimated \(\tau^2\) value from the reduced model. <code>NA</code> for <code>"rma.mv"</code> objects.</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>amount (in percent) of the heterogeneity in the reduced model that is accounted for in the full model (<code>NA</code> for <code>"rma.mv"</code> objects). This can be regarded as a pseudo \(R^2\) statistic (Raudenbush, 2009). Note that the value may not be very accurate unless \(k\) is large (Lopez-Lopez et al., 2014).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.anova.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.anova.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>The function can also be used to conduct a likelihood ratio test (LRT) for the amount of (residual) heterogeneity in random- and mixed-effects models. The full model should then be fitted with either <code>method="ML"</code> or <code>method="REML"</code> and the reduced model with <code>method="EE"</code> (or with <code>tau2=0</code>). The p-value for the test is based on a chi-square distribution with 1 degree of freedom, but actually needs to be adjusted for the fact that the parameter (i.e., \(\tau^2\)) falls on the boundary of the parameter space under the null hypothesis (see Viechtbauer, 2007, for more details).
</p>
<p>LRTs for variance components in more complex models (as fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function) can also be conducted in this manner (see &lsquo;Examples&rsquo;).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Hardy, R. J., &amp; Thompson, S. G. (1996). A likelihood approach to meta-analysis with random effects. <em>Statistics in Medicine</em>, <b>15</b>(6), 619&ndash;629. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(sici)1097-0258(19960330)15:6%3C619::aid-sim188%3E3.0.co;2-a&#8288;</code>
</p>
<p>Huizenga, H. M., Visser, I., &amp; Dolan, C. V. (2011). Testing overall and moderator effects in random effects meta-regression. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>64</b>(1), 1&ndash;19. <code style="white-space: pre;">&#8288;https://doi.org/10.1348/000711010X522687&#8288;</code>
</p>
<p>L√≥pez-L√≥pez, J. A., Mar√≠n-Mart√≠nez, F., S√°nchez-Meca, J., Van den Noortgate, W., &amp; Viechtbauer, W. (2014). Estimation of the predictive power of the model in mixed-effects meta-regression: A simulation study. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>67</b>(1), 30&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/bmsp.12002&#8288;</code>
</p>
<p>Raudenbush, S. W. (2009). Analyzing effect sizes: Random effects models. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 295&ndash;315). New York: Russell Sage Foundation.
</p>
<p>Viechtbauer, W. (2007). Hypothesis tests for population heterogeneity in meta-analysis. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>60</b>(1), 29&ndash;60. <code style="white-space: pre;">&#8288;https://doi.org/10.1348/000711005X64042&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which likelihood ratio and Wald-type tests can be conducted.
</p>
<p><code><a href="#topic+print.anova.rma">print</a></code> for the print method and <code><a href="#topic+as.data.frame.anova.rma">as.data.frame</a></code> for the method to format the results as a data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res1 &lt;- rma(yi, vi, data=dat, method="ML")
res1

### fit mixed-effects model with two moderators (absolute latitude and publication year)
res2 &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat, method="ML")
res2

### Wald-type test of the two moderators
anova(res2)

### alternative way of specifying the same test
anova(res2, X=rbind(c(0,1,0), c(0,0,1)))

### corresponding likelihood ratio test
anova(res1, res2)

### Wald-type test of a linear combination
anova(res2, X=c(1,35,1970))

### use predict() to obtain the same linear combination (with its CI)
predict(res2, newmods=c(35,1970))

### mixed-effects model with three moderators
res3 &lt;- rma(yi, vi, mods = ~ ablat + year + alloc, data=dat, method="ML")
res3

### Wald-type test of the 'alloc' factor
anova(res3, btt=4:5)

### instead of specifying the coefficient numbers, grep for "alloc"
anova(res3, btt="alloc")

### specify a list for the 'btt' argument
anova(res3, btt=list(2,3,4:5))

############################################################################

### an example of doing LRTs of variance components in more complex models
dat &lt;- dat.konstantopoulos2011
res &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat)

### likelihood ratio test of the district-level variance component
res0 &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat, sigma2=c(0,NA))
anova(res, res0)

### likelihood ratio test of the school-level variance component
res0 &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat, sigma2=c(NA,0))
anova(res, res0)

### likelihood ratio test of both variance components simultaneously
res0 &lt;- rma.mv(yi, vi, data=dat)
anova(res, res0)

############################################################################

### an example illustrating a workflow involving cluster-robust inference
dat &lt;- dat.assink2016

### assume that the effect sizes within studies are correlated with rho=0.6
V &lt;- vcalc(vi, cluster=study, obs=esid, data=dat, rho=0.6)

### fit multilevel model using this approximate V matrix
res &lt;- rma.mv(yi, V, random = ~ 1 | study/esid, data=dat)
res

### likelihood ratio tests of the two variance components
res0 &lt;- rma.mv(yi, V, random = ~ 1 | study/esid, data=dat, sigma2=c(0,NA))
anova(res, res0)
res0 &lt;- rma.mv(yi, V, random = ~ 1 | study/esid, data=dat, sigma2=c(NA,0))
anova(res, res0)

### use cluster-robust methods for inferences about the fixed effects
sav &lt;- robust(res, cluster=study, clubSandwich=TRUE)
sav

### examine if 'deltype' is a potential moderator
res &lt;- rma.mv(yi, V, mods = ~ deltype, random = ~ 1 | study/esid, data=dat)
sav &lt;- robust(res, cluster=study, clubSandwich=TRUE)
sav

### note: the (denominator) dfs for the omnibus F-test are very low, so the results
### of this test may not be trustworthy; consider using cluster wild bootstrapping
## Not run: 
library(wildmeta)
Wald_test_cwb(res, constraints=constrain_zero(2:3), R=1000, seed=1234)

## End(Not run)
</code></pre>

<hr>
<h2 id='baujat'>Baujat Plots for 'rma' Objects</h2><span id='topic+baujat'></span><span id='topic+baujat.rma'></span>

<h3>Description</h3>

<p>Function to create Baujat plots for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>baujat(x, ...)

## S3 method for class 'rma'
baujat(x, xlim, ylim, xlab, ylab, cex, symbol="ids", grid=TRUE, progbar=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baujat_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="baujat_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="baujat_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="baujat_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="baujat_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="baujat_+3A_cex">cex</code></td>
<td>
<p>symbol/character expansion factor.</p>
</td></tr>
<tr><td><code id="baujat_+3A_symbol">symbol</code></td>
<td>
<p>either an integer to specify the <code>pch</code> value (i.e., plotting symbol), or <code>"slab"</code> to plot the study labels, or <code>"ids"</code> (the default) to plot the study id numbers.</p>
</td></tr>
<tr><td><code id="baujat_+3A_grid">grid</code></td>
<td>
<p>logical to specify whether a grid should be added to the plot. Can also be a color name.</p>
</td></tr>
<tr><td><code id="baujat_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="baujat_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model specified via <code>x</code> must be a model fitted with either the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, or <code><a href="#topic+rma.peto">rma.peto</a></code> functions.
</p>
<p>Baujat et al. (2002) proposed a diagnostic plot to detect sources of heterogeneity in meta-analytic data. The plot shows the contribution of each study to the overall \(Q\)-test statistic for heterogeneity on the x-axis versus the influence of each study (defined as the standardized squared difference between the overall estimate based on an equal-effects model with and without the study included in the model fitting) on the y-axis. The same type of plot can be produced by first fitting an equal-effects model with either the <code><a href="#topic+rma.uni">rma.uni</a></code> (using <code>method="EE"</code>), <code><a href="#topic+rma.mh">rma.mh</a></code>, or <code><a href="#topic+rma.peto">rma.peto</a></code> functions and then passing the fitted model object to the <code>baujat</code> function.
</p>
<p>For models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function (which may be random-effects or mixed-effects meta-regressions models), the idea underlying this type of plot can be generalized as described by Viechtbauer (2021): The x-axis then corresponds to the squared Pearson residual of a study, while the y-axis corresponds to the standardized squared difference between the predicted/fitted value for the study with and without the study included in the model fitting.
</p>
<p>By default, the points plotted are the study id numbers, but one can also plot the study labels by setting <code>symbol="slab"</code> (if study labels are available within the model object) or one can specify a plotting symbol via the <code>symbol</code> argument that gets passed to <code>pch</code> (see <code><a href="graphics.html#topic+points">points</a></code> for possible options).
</p>


<h3>Value</h3>

<p>A data frame with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>ids</code></td>
<td>
<p>the study id numbers.</p>
</td></tr>
<tr><td><code>slab</code></td>
<td>
<p>the study labels.</p>
</td></tr>
</table>
<p>Note that the data frame is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Baujat, B., Mahe, C., Pignon, J.-P., &amp; Hill, C. (2002). A graphical method for exploring heterogeneity in meta-analyses: Application to a meta-analysis of 65 trials. <em>Statistics in Medicine</em>, <b>21</b>(18), 2641&ndash;2652. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1221&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code> and <code><a href="#topic+rma.peto">rma.peto</a></code> for functions to fit models for which Baujat plots can be created.
</p>
<p><code><a href="#topic+influence.rma.uni">influence</a></code> for other model diagnostics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data from Pignon et al. (2000) into 'dat'
dat &lt;- dat.pignon2000

### calculate estimated log hazard ratios and sampling variances
dat$yi &lt;- with(dat, OmE/V)
dat$vi &lt;- with(dat, 1/V)

### meta-analysis based on all 65 trials
res &lt;- rma(yi, vi, data=dat, method="EE", slab=trial)

### create Baujat plot
baujat(res)

### some variations of the plotting symbol
baujat(res, symbol=19)
baujat(res, symbol="slab")

### label only a selection of the more 'extreme' points
sav &lt;- baujat(res, symbol=19, xlim=c(0,20))
sav &lt;- sav[sav$x &gt;= 10 | sav$y &gt;= 0.10,]
text(sav$x, sav$y, sav$slab, pos=1, cex=0.8)
</code></pre>

<hr>
<h2 id='bldiag'>Construct Block Diagonal Matrix</h2><span id='topic+bldiag'></span>

<h3>Description</h3>

<p>Function to construct a block diagonal matrix from (a list of) matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bldiag(..., order)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bldiag_+3A_...">...</code></td>
<td>
<p>individual matrices or a list of matrices.</p>
</td></tr>
<tr><td><code id="bldiag_+3A_order">order</code></td>
<td>
<p>optional argument to specify a variable based on which a square block diagonal matrix should be ordered.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Posted to R-help by Berton Gunter (2 Sep 2005) with some further adjustments by Wolfgang Viechtbauer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.mv">rma.mv</a></code> for the model fitting function that can take such a block diagonal matrix as input (for the <code>V</code> argument).
</p>
<p><code><a href="#topic+blsplit">blsplit</a></code> for a function that can split a block diagonal matrix into a list of sub-matrices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat'
dat &lt;- dat.berkey1998
dat

### construct list with the variance-covariance matrices of the observed outcomes for the studies
V &lt;- lapply(split(dat[c("v1i","v2i")], dat$trial), as.matrix)
V

### construct block diagonal matrix
V &lt;- bldiag(V)
V

### if we split based on 'author', the list elements in V are in a different order than tha data
V &lt;- lapply(split(dat[c("v1i","v2i")], dat$author), as.matrix)
V

### can use 'order' argument to reorder the block-diagonal matrix into the correct order
V &lt;- bldiag(V, order=dat$author)
V
</code></pre>

<hr>
<h2 id='blsplit'>Split Block Diagonal Matrix</h2><span id='topic+blsplit'></span>

<h3>Description</h3>

<p>Function to split a block diagonal matrix into a list of sub-matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blsplit(x, cluster, fun, args, sort=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blsplit_+3A_x">x</code></td>
<td>
<p>a block diagonal matrix.</p>
</td></tr>
<tr><td><code id="blsplit_+3A_cluster">cluster</code></td>
<td>
<p>vector to specify the clustering variable to use for splitting.</p>
</td></tr>
<tr><td><code id="blsplit_+3A_fun">fun</code></td>
<td>
<p>optional argument to specify a function to apply to each sub-matrix.</p>
</td></tr>
<tr><td><code id="blsplit_+3A_args">args</code></td>
<td>
<p>optional argument to specify any additional argument(s) for the function specified via <code>fun</code>.</p>
</td></tr>
<tr><td><code id="blsplit_+3A_sort">sort</code></td>
<td>
<p>logical to indicate whether to sort the list by the unique cluster values (the default is <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of one or more sub-matrices.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bldiag">bldiag</a></code> for a function to create a block diagonal matrix based on sub-matrices.
</p>
<p><code><a href="#topic+vcalc">vcalc</a></code> for a function to construct a variance-covariance matrix of dependent effect sizes or outcomes, which often has a block diagonal structure.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat'
dat &lt;- dat.assink2016

### assume that the effect sizes within studies are correlated with rho=0.6
V &lt;- vcalc(vi, cluster=study, obs=esid, data=dat, rho=0.6)

### split V matrix into list of sub-matrices
Vs &lt;- blsplit(V, cluster=dat$study)
Vs[1:2]
lapply(Vs[1:2], cov2cor)

### illustrate the use of the fun and args arguments
blsplit(V, cluster=dat$study, cov2cor)[1:2]
blsplit(V, cluster=dat$study, round, 3)[1:2]
</code></pre>

<hr>
<h2 id='blup'>Best Linear Unbiased Predictions for 'rma.uni' Objects</h2><span id='topic+blup'></span><span id='topic+blup.rma.uni'></span>

<h3>Description</h3>

<p>Function to compute best linear unbiased predictions (BLUPs) of the study-specific true effect sizes or outcomes (by combining the fitted values based on the fixed effects and the estimated contributions of the random effects) for objects of class <code>"rma.uni"</code>. Corresponding standard errors and prediction interval bounds are also provided. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blup(x, ...)

## S3 method for class 'rma.uni'
blup(x, level, digits, transf, targs, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blup_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="blup_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the prediction interval level (see <a href="#topic+misc-options">here</a> for details). If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="blup_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="blup_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the predicted values and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="blup_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="blup_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"list.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted values.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>pi.lb</code></td>
<td>
<p>lower bound of the prediction intervals.</p>
</td></tr>
<tr><td><code>pi.ub</code></td>
<td>
<p>upper bound of the prediction intervals.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The object is formatted and printed with the <code><a href="#topic+print.list.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>For best linear unbiased predictions of only the random effects, see <code><a href="#topic+ranef">ranef</a></code>.
</p>
<p>For predicted/fitted values that are based only on the fixed effects of the model, see <code><a href="#topic+fitted.rma">fitted</a></code> and <code><a href="#topic+predict.rma">predict</a></code>.
</p>
<p>For conditional residuals (the deviations of the observed effect sizes or outcomes from the BLUPs), see <code>rstandard.rma.uni</code> with <code>type="conditional"</code>.
</p>
<p>Equal-effects models do not contain random study effects. The BLUPs for these models will therefore be equal to the fitted values, that is, those obtained with <code><a href="#topic+fitted.rma">fitted</a></code> and <code><a href="#topic+predict.rma">predict</a></code>.
</p>
<p>When using the <code>transf</code> argument, the transformation is applied to the predicted values and the corresponding interval bounds. The standard errors are then set equal to <code>NA</code> and are omitted from the printed output.
</p>
<p>By default, a standard normal distribution is used to construct the prediction intervals. When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then a t-distribution with \(k-p\) degrees of freedom is used.
</p>
<p>To be precise, it should be noted that the function actually computes empirical BLUPs (eBLUPs), since the predicted values are a function of the estimated value of \(\tau^2\).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Kackar, R. N., &amp; Harville, D. A. (1981). Unbiasedness of two-stage estimation and prediction procedures for mixed linear models. Communications in Statistics, Theory and Methods, <b>10</b>(13), 1249&ndash;1261. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/03610928108828108&#8288;</code>
</p>
<p>Raudenbush, S. W., &amp; Bryk, A. S. (1985). Empirical Bayes meta-analysis. <em>Journal of Educational Statistics</em>, <b>10</b>(2), 75&ndash;98. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986010002075&#8288;</code>
</p>
<p>Robinson, G. K. (1991). That BLUP is a good thing: The estimation of random effects. <em>Statistical Science</em>, <b>6</b>(1), 15&ndash;32. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/ss/1177011926&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit models for which BLUPs can be extracted.
</p>
<p><code><a href="#topic+predict.rma">predict</a></code> and <code><a href="#topic+fitted.rma">fitted</a></code> for functions to compute the predicted/fitted values based only on the fixed effects and <code><a href="#topic+ranef">ranef</a></code> for a function to compute the BLUPs based only on the random effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(yi, vi, data=dat)

### BLUPs of the true risk ratios for each study
blup(res, transf=exp)

### illustrate shrinkage of BLUPs towards the (estimated) population average
res &lt;- rma(yi, vi, data=dat)
blups &lt;- blup(res)$pred
plot(NA, NA, xlim=c(.8,2.4), ylim=c(-2,0.5), pch=19,
     xaxt="n", bty="n", xlab="", ylab="Log Risk Ratio")
segments(rep(1,13), dat$yi, rep(2,13), blups, col="darkgray")
points(rep(1,13), dat$yi, pch=19)
points(rep(2,13), blups, pch=19)
axis(side=1, at=c(1,2), labels=c("Observed\nValues", "BLUPs"), lwd=0)
segments(0, res$beta, 2.15, res$beta, lty="dotted")
text(2.3, res$beta, substitute(hat(mu)==muhat, list(muhat=round(res$beta[[1]], 2))), cex=1)
</code></pre>

<hr>
<h2 id='coef.matreg'>Extract the Model Coefficients and Variance-Covariance Matrix from 'matreg' Objects</h2><span id='topic+coef.matreg'></span><span id='topic+vcov.matreg'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"matreg"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'matreg'
coef(object, ...)
## S3 method for class 'matreg'
vcov(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.matreg_+3A_object">object</code></td>
<td>
<p>an object of class <code>"matreg"</code>.</p>
</td></tr>
<tr><td><code id="coef.matreg_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>coef</code> function extracts the estimated model coefficients from objects of class <code>"matreg"</code>. The <code>vcov</code> function extracts the corresponding variance-covariance matrix.
</p>


<h3>Value</h3>

<p>Either a vector with the estimated model coefficients or a variance-covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matreg">matreg</a></code> for the function to create <code>matreg</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### fit a regression model with lm() to the 'mtcars' dataset
res &lt;- lm(mpg ~ hp + wt + am, data=mtcars)
coef(res)
vcov(res)

### covariance matrix of the dataset
S &lt;- cov(mtcars)

### fit the same regression model using matreg()
res &lt;- matreg(y="mpg", x=c("hp","wt","am"), R=S, cov=TRUE,
              means=colMeans(mtcars), n=nrow(mtcars))
coef(res)
vcov(res)
</code></pre>

<hr>
<h2 id='coef.permutest.rma.uni'>Extract the Model Coefficient Table from 'permutest.rma.uni' Objects</h2><span id='topic+coef.permutest.rma.uni'></span>

<h3>Description</h3>

<p>Function to extract the estimated model coefficients, corresponding standard errors, test statistics, p-values (based on the permutation tests), and confidence interval bounds from objects of class <code>"permutest.rma.uni"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'permutest.rma.uni'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.permutest.rma.uni_+3A_object">object</code></td>
<td>
<p>an object of class <code>"permutest.rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="coef.permutest.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the following elements:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>estimated model coefficient(s).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard error(s).</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>corresponding test statistic(s).</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>p-value(s) based on the permutation test(s).</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the (permutation-based) confidence interval(s).</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the (permutation-based) confidence interval(s).</p>
</td></tr>
</table>
<p>When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then <code>zval</code> is called <code>tval</code> in the data frame that is returned by the function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permutest.rma.uni">permutest</a></code> for the function to conduct permutation tests and <code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit models for which permutation tests can be conducted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### carry out permutation test
## Not run: 
set.seed(1234) # for reproducibility
sav &lt;- permutest(res)
coef(sav)

## End(Not run)
</code></pre>

<hr>
<h2 id='coef.rma'>Extract the Model Coefficients and Coefficient Table from 'rma' and 'summary.rma' Objects</h2><span id='topic+coef'></span><span id='topic+coef.rma'></span><span id='topic+coef.summary.rma'></span>

<h3>Description</h3>

<p>Function to extract the estimated model coefficients from objects of class <code>"rma"</code>. For objects of class <code>"summary.rma"</code>, the model coefficients, corresponding standard errors, test statistics, p-values, and confidence interval bounds are extracted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
coef(object, ...)
## S3 method for class 'summary.rma'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code> or <code>"summary.rma"</code>.</p>
</td></tr>
<tr><td><code id="coef.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector with the estimated model coefficient(s) or a data frame with the following elements:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>estimated model coefficient(s).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard error(s).</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>corresponding test statistic(s).</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-value(s).</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>corresponding lower bound of the confidence interval(s).</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>corresponding upper bound of the confidence interval(s).</p>
</td></tr>
</table>
<p>When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then <code>zval</code> is called <code>tval</code> in the data frame that is returned by the function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which model coefficients/tables can be extracted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### extract model coefficients
coef(res)

### extract model coefficient table
coef(summary(res))
</code></pre>

<hr>
<h2 id='confint.rma'>Confidence Intervals for 'rma' Objects</h2><span id='topic+confint'></span><span id='topic+confint.rma'></span><span id='topic+confint.rma.uni'></span><span id='topic+confint.rma.mh'></span><span id='topic+confint.rma.peto'></span><span id='topic+confint.rma.glmm'></span><span id='topic+confint.rma.mv'></span><span id='topic+confint.rma.uni.selmodel'></span><span id='topic+confint.rma.ls'></span>

<h3>Description</h3>

<p>Functions to compute confidence intervals for the model coefficients, variance components, and other parameters in meta-analytic models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
confint(object, parm, level, fixed=FALSE, random=TRUE, type,
        digits, transf, targs, verbose=FALSE, control, ...)

## S3 method for class 'rma.mh'
confint(object, parm, level, digits, transf, targs, ...)

## S3 method for class 'rma.peto'
confint(object, parm, level, digits, transf, targs, ...)

## S3 method for class 'rma.glmm'
confint(object, parm, level, digits, transf, targs, ...)

## S3 method for class 'rma.mv'
confint(object, parm, level, fixed=FALSE, sigma2, tau2, rho, gamma2, phi,
        digits, transf, targs, verbose=FALSE, control, ...)

## S3 method for class 'rma.uni.selmodel'
confint(object, parm, level, fixed=FALSE, tau2, delta,
        digits, transf, targs, verbose=FALSE, control, ...)

## S3 method for class 'rma.ls'
confint(object, parm, level, fixed=FALSE, alpha,
        digits, transf, targs, verbose=FALSE, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, <code>"rma.mv"</code>, <code>"rma.uni.selmodel"</code>, or <code>"rma.ls"</code>. The method is not yet implemented for objects of class <code>"rma.glmm"</code>.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_parm">parm</code></td>
<td>
<p>this argument is here for compatibility with the generic function <code><a href="#topic+confint">confint</a></code>, but is (currently) ignored.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_fixed">fixed</code></td>
<td>
<p>logical to specify whether confidence intervals for the model coefficients should be returned.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_random">random</code></td>
<td>
<p>logical to specify whether a confidence interval for the amount of (residual) heterogeneity should be returned.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_type">type</code></td>
<td>
<p>optional character string to specify the method for computing the confidence interval for the amount of (residual) heterogeneity (either <code>"QP"</code>, <code>"GENQ"</code>, <code>"PL"</code>, or <code>"HT"</code>).</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_sigma2">sigma2</code></td>
<td>
<p>integer to specify for which \(\sigma^2\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_tau2">tau2</code></td>
<td>
<p>integer to specify for which \(\tau^2\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_rho">rho</code></td>
<td>
<p>integer to specify for which \(\rho\) parameter the confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_gamma2">gamma2</code></td>
<td>
<p>integer to specify for which \(\gamma^2\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_phi">phi</code></td>
<td>
<p>integer to specify for which \(\phi\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_delta">delta</code></td>
<td>
<p>integer to specify for which \(\delta\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_alpha">alpha</code></td>
<td>
<p>integer to specify for which \(\alpha\) parameter a confidence interval should be obtained.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (see <a href="#topic+misc-options">here</a> for details). If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the model coefficients and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the iterative algorithms used to obtain the confidence intervals (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_control">control</code></td>
<td>
<p>list of control values for the iterative algorithms. If unspecified, default values are used. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="confint.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals for the model coefficients can be obtained by setting <code>fixed=TRUE</code> and are simply the usual Wald-type intervals (which are also shown when printing the fitted object).
</p>
<p>Other parameter(s) for which confidence intervals can be obtained depend on the model object:
</p>

<ul>
<li><p> For objects of class <code>"rma.uni"</code> obtained with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, a confidence interval for the amount of (residual) heterogeneity (i.e., \(\tau^2\)) can be obtained by setting <code>random=TRUE</code> (which is the default). The interval is obtained iteratively either via the Q-profile method or via the generalized Q-statistic method (Hartung and Knapp, 2005; Viechtbauer, 2007; Jackson, 2013; Jackson et al., 2014). The latter is automatically used when the model was fitted with <code>method="GENQ"</code> or <code>method="GENQM"</code>, the former is used in all other cases. Either method provides an exact confidence interval for \(\tau^2\) in random- and mixed-effects models. The square root of the interval bounds is also returned for easier interpretation. Confidence intervals for \(I^2\) and \(H^2\) are also provided (Higgins &amp; Thompson, 2002). Since \(I^2\) and \(H^2\) are just monotonic transformations of \(\tau^2\) (for details, see <code><a href="#topic+print.rma.uni">print</a></code>), the confidence intervals for \(I^2\) and \(H^2\) are also exact. One can also set <code>type="PL"</code> to obtain a profile likelihood confidence interval for \(\tau^2\) (and corresponding CIs for \(I^2\) and \(H^2\)), which would be more consistent with the use of ML/REML estimation, but is not exact (see &lsquo;Note&rsquo;). For models without moderators (i.e., random-effects models), one can also set <code>type="HT"</code>, in which case the &lsquo;test-based method&rsquo; (method III in Higgins &amp; Thompson, 2002) is used to construct confidence intervals for \(\tau^2\), \(I^2\), and \(H^2\) (see also Borenstein et al., 2009, chapter 16). However, note that this method tends to yield confidence intervals that are too narrow when the amount of heterogeneity is large.
</p>
</li>
<li><p> For objects of class <code>"rma.mv"</code> obtained with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, confidence intervals are obtained by default for all variance and correlation components of the model. Alternatively, one can use the <code>sigma2</code>, <code>tau2</code>, <code>rho</code>, <code>gamma2</code>, or <code>phi</code> arguments to specify for which variance/correlation parameter a confidence interval should be obtained. Only one of these arguments can be used at a time. A single integer is used to specify the number of the parameter. The function provides profile likelihood confidence intervals for these parameters. It is a good idea to examine the corresponding profile likelihood plots (via the <code><a href="#topic+profile.rma.mv">profile</a></code> function) to make sure that the bounds obtained are sensible.
</p>
</li>
<li><p> For selection model objects of class <code>"rma.uni.selmodel"</code> obtained with the <code><a href="#topic+selmodel">selmodel</a></code> function, confidence intervals are obtained by default for \(\tau^2\) (for models where this is an estimated parameter) and all selection model parameters. Alternatively, one can choose to obtain a confidence interval only for \(\tau^2\) by setting <code>tau2=TRUE</code> or for one of the selection model parameters by specifying its number via the <code>delta</code> argument. The function provides profile likelihood confidence intervals for these parameters. It is a good idea to examine the corresponding profile likelihood plots (via the <code><a href="#topic+profile.rma.uni.selmodel">profile</a></code> function) to make sure that the bounds obtained are sensible.
</p>
</li>
<li><p> For location-scale model objects of class <code>"rma.ls"</code> obtained with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, confidence intervals are obtained by default for all scale parameters. Alternatively, one can choose to obtain a confidence interval for one of the scale parameters by specifying its number via the <code>alpha</code> argument. The function provides profile likelihood confidence intervals for these parameters. It is a good idea to examine the corresponding profile likelihood plots (via the <code><a href="#topic+profile.rma.ls">profile</a></code> function) to make sure that the bounds obtained are sensible.
</p>
</li></ul>

<p>The methods used to find confidence intervals for these parameters are iterative and require the use of the <code><a href="stats.html#topic+uniroot">uniroot</a></code> function. By default, the desired accuracy (<code>tol</code>) is set equal to <code>.Machine$double.eps^0.25</code> and the maximum number of iterations (<code>maxiter</code>) to <code>1000</code>. These values can be adjusted with <code>control=list(tol=value, maxiter=value)</code>, but the defaults should be adequate for most purposes. If <code>verbose=TRUE</code>, output is generated on the progress of the iterative algorithms. This is especially useful when model fitting is slow, in which case finding the confidence interval bounds can also take considerable amounts of time.
</p>
<p>When using the <code><a href="stats.html#topic+uniroot">uniroot</a></code> function, one must also set appropriate end points of the interval to be searched for the confidence interval bounds. The function sets some sensible defaults for the end points, but it may happen that the function is only able to determine that a bound is below/above a certain limit (this is indicated in the output accordingly with <code>&lt;</code> or <code>&gt;</code> signs). It can also happen that the model cannot be fitted or does not converge especially at the extremes of the interval to be searched. This will result in missing (<code>NA</code>) bounds and corresponding warnings. It may then be necessary to adjust the end points manually (see &lsquo;Note&rsquo;).
</p>
<p>Finally, it is also possible that the lower and upper confidence interval bounds for a variance component both fall below zero. Since both bounds then fall outside of the parameter space, the confidence interval then consists of the null/empty set. Alternatively, one could interpret this as a confidence interval with bounds \([0,0]\) or as indicating &lsquo;highly/overly homogeneous&rsquo; data.
</p>


<h3>Value</h3>

<p>An object of class <code>"confint.rma"</code>. The object is a list with either one or two elements (named <code>fixed</code> and <code>random</code>) with the following elements:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>estimate of the model coefficient, variance/correlation component, or selection model parameter.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence interval.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence interval.</p>
</td></tr>
</table>
<p>When obtaining confidence intervals for multiple components, the object is a list of class <code>"list.confint.rma"</code>, where each element is a <code>"confint.rma"</code> object as described above.
</p>
<p>The results are formatted and printed with the <code><a href="#topic+print.confint.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.confint.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>When computing a CI for \(\tau^2\) for objects of class <code>"rma.uni"</code>, the estimate of \(\tau^2\) will usually fall within the CI bounds provided by the Q-profile method. However, this is not guaranteed. Depending on the method used to estimate \(\tau^2\) and the width of the CI, it can happen that the CI does not actually contain the estimate. Using the empirical Bayes or Paule-Mandel estimator of \(\tau^2\) when fitting the model (i.e., using <code>method="EB"</code> or <code>method="PM"</code>) usually ensures that the estimate of \(\tau^2\) falls within the CI (for <code>method="PMM"</code>, this is guaranteed). When <code>method="GENQ"</code> was used to fit the model, the corresponding CI obtained via the generalized Q-statistic method also usually contains the estimate \(\tau^2\) (for <code>method="GENQM"</code>, this is guaranteed). When using ML/REML estimation, the profile likelihood CI (obtained when setting <code>type="PL"</code>) is guaranteed to contain the estimate of \(\tau^2\).
</p>
<p>When computing a CI for \(\tau^2\) for objects of class <code>"rma.uni"</code>, the end points of the interval to be searched for the CI bounds are \([0,100]\) (or, for the upper bound, ten times the estimate of \(\tau^2\), whichever is greater). The upper bound should be large enough for most cases, but can be adjusted with <code>control=list(tau2.max=value)</code>. One can also adjust the lower end point with <code>control=list(tau2.min=value)</code>. You should only play around with this value if you know what you are doing.
</p>
<p>For objects of class <code>"rma.mv"</code>, the function provides profile likelihood CIs for the variance/correlation parameters in the model. For variance components, the lower end point of the interval to be searched is set to 0 and the upper end point to the larger of 10 and 100 times the value of the component. For correlations, the function sets the lower end point to a sensible default depending on the type of variance structure chosen, while the upper end point is set to 1. One can adjust the lower and/or upper end points with <code>control=list(vc.min=value, vc.max=value)</code>. Also, the function adjusts the lower/upper end points when the model does not converge at these extremes (the end points are then moved closer to the estimated value of the component). The total number of tries for setting/adjusting the end points in this manner is determined via <code>control=list(eptries=value)</code>, with the default being 10 tries.
</p>
<p>For objects of class <code>"rma.uni.selmodel"</code> or <code>"rma.ls"</code>, the function also sets some sensible defaults for the end points of the interval to be searched for the CI bounds (of the \(\tau^2\), \(\delta\), and \(\alpha\) parameter(s)). One can again adjust the end points and the number of retries (as described above) with <code>control=list(vc.min=value, vc.max=value, eptries=value)</code>.
</p>
<p>The Q-profile and generalized Q-statistic methods are both exact under the assumptions of the random- and mixed-effects models (i.e., normally distributed observed and true effect sizes or outcomes and known sampling variances). In practice, these assumptions are usually only approximately true, turning CIs for \(\tau^2\) also into approximations. Profile likelihood CIs are not exact by construction and rely on the asymptotic behavior of the likelihood ratio statistic, so they may be inaccurate in small samples, but they are inherently consistent with the use of ML/REML estimation.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. (2009). <em>Introduction to meta-analysis</em>. Chichester, UK: Wiley.
</p>
<p>Hardy, R. J., &amp; Thompson, S. G. (1996). A likelihood approach to meta-analysis with random effects. <em>Statistics in Medicine</em>, <b>15</b>(6), 619&ndash;629. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(sici)1097-0258(19960330)15:6%3C619::aid-sim188%3E3.0.co;2-a&#8288;</code>
</p>
<p>Hartung, J., &amp; Knapp, G. (2005). On confidence intervals for the among-group variance in the one-way random effects model with unequal error variances. <em>Journal of Statistical Planning and Inference</em>, <b>127</b>(1-2), 157&ndash;177. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.jspi.2003.09.032&#8288;</code>
</p>
<p>Higgins, J. P. T., &amp; Thompson, S. G. (2002). Quantifying heterogeneity in a meta-analysis. <em>Statistics in Medicine</em>, <b>21</b>(11), 1539&ndash;1558. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1186&#8288;</code>
</p>
<p>Jackson, D. (2013). Confidence intervals for the between-study variance in random effects meta-analysis using generalised Cochran heterogeneity statistics. <em>Research Synthesis Methods</em>, <b>4</b>(3), 220&ndash;229. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/s12874-016-0219-y&#8288;</code>
</p>
<p>Jackson, D., Turner, R., Rhodes, K., &amp; Viechtbauer, W. (2014). Methods for calculating confidence and credible intervals for the residual between-study variance in random effects meta-regression models. <em>BMC Medical Research Methodology</em>, <b>14</b>, 103. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/1471-2288-14-103&#8288;</code>
</p>
<p>Viechtbauer, W. (2007). Confidence intervals for the amount of heterogeneity in meta-analysis. <em>Statistics in Medicine</em>, <b>26</b>(1), 37&ndash;52. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2514&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, <code><a href="#topic+rma.mv">rma.mv</a></code>, and <code><a href="#topic+selmodel.rma.uni">selmodel</a></code> for functions to fit models for which confidence intervals can be computed.
</p>
<p><code><a href="#topic+profile.rma">profile</a></code> for functions to create profile likelihood plots corresponding to profile likelihood confidence intervals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(yi, vi, data=dat, method="REML")

### confidence interval for the total amount of heterogeneity
confint(res)

### mixed-effects model with absolute latitude in the model
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)

### confidence interval for the residual amount of heterogeneity
confint(res)

### multilevel random-effects model
res &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat.konstantopoulos2011)

### profile plots and confidence intervals for the variance components
## Not run: 
par(mfrow=c(2,1))
profile(res, sigma2=1, steps=40, cline=TRUE)
sav &lt;- confint(res, sigma2=1)
sav
abline(v=sav$random[1,2:3], lty="dotted")
profile(res, sigma2=2, steps=40, cline=TRUE)
sav &lt;- confint(res, sigma2=2)
sav
abline(v=sav$random[1,2:3], lty="dotted")

## End(Not run)

### multivariate parameterization of the model
res &lt;- rma.mv(yi, vi, random = ~ school | district, data=dat.konstantopoulos2011)

### profile plots and confidence intervals for the variance component and correlation
## Not run: 
par(mfrow=c(2,1))
profile(res, tau2=1, steps=40, cline=TRUE)
sav &lt;- confint(res, tau2=1)
sav
abline(v=sav$random[1,2:3], lty="dotted")
profile(res, rho=1, steps=40, cline=TRUE)
sav &lt;- confint(res, rho=1)
sav
abline(v=sav$random[1,2:3], lty="dotted")

## End(Not run)
</code></pre>

<hr>
<h2 id='contrmat'>Construct Contrast Matrix for Two-Group Comparisons</h2><span id='topic+contrmat'></span>

<h3>Description</h3>

<p>Function to construct a matrix that indicates which two groups have been contrasted against each other in each row of a dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrmat(data, grp1, grp2, last, shorten=FALSE, minlen=2, check=TRUE, append=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrmat_+3A_data">data</code></td>
<td>
<p>a data frame in wide format.</p>
</td></tr>
<tr><td><code id="contrmat_+3A_grp1">grp1</code></td>
<td>
<p>either the name (given as a character string) or the position (given as a single number) of the first group variable in the data frame.</p>
</td></tr>
<tr><td><code id="contrmat_+3A_grp2">grp2</code></td>
<td>
<p>either the name (given as a character string) or the position (given as a single number) of the second group variable in the data frame.</p>
</td></tr>
<tr><td><code id="contrmat_+3A_last">last</code></td>
<td>
<p>optional character string to specify which group will be placed in the last column of the matrix (must be one of the groups in the group variables). If not given, the most frequently occurring second group is placed last.</p>
</td></tr>
<tr><td><code id="contrmat_+3A_shorten">shorten</code></td>
<td>
<p>logical to specify whether the variable names corresponding to the group names should be shortened (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="contrmat_+3A_minlen">minlen</code></td>
<td>
<p>integer to specify the minimum length of the shortened variable names (the default is 2).</p>
</td></tr>
<tr><td><code id="contrmat_+3A_check">check</code></td>
<td>
<p>logical to specify whether the variables names should be checked to ensure that they are syntactically valid variable names and if not, they are adjusted (by <code><a href="base.html#topic+make.names">make.names</a></code>) so that they are (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="contrmat_+3A_append">append</code></td>
<td>
<p>logical to specify whether the contrast matrix should be appended to the data frame specified via the <code>data</code> argument (the default is <code>TRUE</code>). If <code>append=FALSE</code>, only the contrast matrix is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to construct a matrix that indicates which two groups have been contrasted against each other in each row of a data frame (with <code>1</code> for the first group, <code>-1</code> for the second group, and <code>0</code> otherwise).
</p>
<p>The <code>grp1</code> and <code>grp2</code> arguments are used to specify the group variables in the dataset (either as character strings or as numbers indicating the column positions of these variables in the dataset). Optional argument <code>last</code> is used to specify which group will be placed in the last column of the matrix.
</p>
<p>If <code>shorten=TRUE</code>, the variable names corresponding to the group names are shortened (to at least <code>minlen</code>; the actual length might be longer to ensure uniqueness of the variable names).
</p>
<p>The examples below illustrate the use of this function.
</p>


<h3>Value</h3>

<p>A matrix with as many variables as there are groups.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.wide">to.wide</a></code> for a function to create &lsquo;wide&rsquo; format datasets.
</p>
<p><code><a href="metadat.html#topic+dat.senn2013">dat.senn2013</a></code>, <code><a href="metadat.html#topic+dat.hasselblad1998">dat.hasselblad1998</a></code>, <code><a href="metadat.html#topic+dat.lopez2019">dat.lopez2019</a></code> for illustrative examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### restructure to wide format
dat &lt;- dat.senn2013
dat &lt;- dat[c(1,4,3,2,5,6)]
dat &lt;- to.wide(dat, study="study", grp="treatment", ref="placebo", grpvars=4:6)
dat

### add contrast matrix
dat &lt;- contrmat(dat, grp1="treatment.1", grp2="treatment.2")
dat

### data in long format
dat &lt;- dat.hasselblad1998
dat

### restructure to wide format
dat &lt;- to.wide(dat, study="study", grp="trt", ref="no_contact", grpvars=6:7)
dat

### add contrast matrix
dat &lt;- contrmat(dat, grp1="trt.1", grp2="trt.2", shorten=TRUE, minlen=3)
dat
</code></pre>

<hr>
<h2 id='conv.2x2'>Reconstruct Cell Frequencies of \(2 \times 2\) Tables</h2><span id='topic+conv.2x2'></span>

<h3>Description</h3>

<p>Function to reconstruct the cell frequencies of \(2 \times 2\) tables based on other summary statistics. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conv.2x2(ori, ri, x2i, ni, n1i, n2i, correct=TRUE, data, include,
         var.names=c("ai","bi","ci","di"), append=TRUE, replace="ifna")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conv.2x2_+3A_ori">ori</code></td>
<td>
<p>optional vector with the odds ratios corresponding to the tables.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_ri">ri</code></td>
<td>
<p>optional vector with the phi coefficients corresponding to the tables.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_x2i">x2i</code></td>
<td>
<p>optional vector with the (signed) chi-square statistics corresponding to the tables.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_ni">ni</code></td>
<td>
<p>vector with the total sample sizes.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_n1i">n1i</code></td>
<td>
<p>vector with the marginal counts for the outcome of interest on the first variable.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_n2i">n2i</code></td>
<td>
<p>vector with the marginal counts for the outcome of interest on the second variable.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_correct">correct</code></td>
<td>
<p>optional logical (or vector thereof) to indicate whether chi-square statistics were computed using Yates's correction for continuity (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_include">include</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies for which the cell frequencies should be reconstructed.</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_var.names">var.names</code></td>
<td>
<p>character vector with four elements to specify the names of the variables for the reconstructed cell frequencies (the default is <code>c("ai","bi","ci","di")</code>).</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame provided via the <code>data</code> argument should be returned together with the reconstructed values (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.2x2_+3A_replace">replace</code></td>
<td>
<p>character string or logical to specify how values in <code>var.names</code> should be replaced (only relevant when using the <code>data</code> argument and if variables in <code>var.names</code> already exist in the data frame). See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For meta-analyses based on \(2 \times 2\) table data, the problem often arises that some studies do not directly report the cell frequencies. The present function allows the reconstruction of such tables based on other summary statistics.
</p>
<p>In particular, assume that the data of interest for a particular study are of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
                         </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> variable 2, outcome + </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> variable 2, outcome - </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
   variable 1, outcome + </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   variable 1, outcome - </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;">            </td>
</tr>
<tr>
 <td style="text-align: left;">
   total                 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code>            </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;">                       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ni</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies (i.e., the number of individuals falling into a particular category), <code>n1i</code> (i.e., <code>ai+bi</code>) and <code>n2i</code> (i.e., <code>ai+ci</code>) are the marginal totals for the outcome of interest on the first and second variable, respectively, and <code>ni</code> is the total sample size (i.e., <code>ai+bi+ci+di</code>) of the study.
</p>
<p>For example, if variable 1 denotes two different groups (e.g., treated versus control) and variable 2 indicates whether a particular outcome of interest has occurred or not (e.g., death, complications, failure to improve under the treatment), then <code>n1i</code> denotes the number of individuals in the treatment group, but <code>n2i</code> is <em>not</em> the number of individuals in the control group, but the total number of individuals who experienced the outcome of interest on variable 2. <b>Note that the meaning of <code>n2i</code> is therefore different here compared to the <code><a href="#topic+escalc">escalc</a></code> function (where <code>n2i</code> denotes <code>ci+di</code>)</b>.
</p>
<p>If a study does not report the cell frequencies, but it reports the total sample size (which can be specified via the <code>ni</code> argument), the two marginal counts (which can be specified via the <code>n1i</code> and <code>n2i</code> arguments), and some other statistic corresponding to the table, then it may be possible to reconstruct the cell frequencies. The present function currently allows this for three different cases:
</p>

<ol>
<li><p> If the odds ratio \[OR = \frac{a_i d_i}{b_i c_i}\] is known, then the cell frequencies can be reconstructed (Bonett, 2007). Odds ratios can be specified via the <code>ori</code> argument.
</p>
</li>
<li><p> If the phi coefficient \[\phi = \frac{a_i d_i - b_i c_i}{\sqrt{n_{1i}(n_i-n_{1i})n_{2i}(n_i-n_{2i})}}\] is known, then the cell frequencies can again be reconstructed (own derivation). Phi coefficients can be specified via the <code>ri</code> argument.
</p>
</li>
<li><p> If the chi-square statistic from Pearson's chi-square test of independence is known (which can be specified via the <code>x2i</code> argument), then it can be used to recalculate the phi coefficient and hence again the cell frequencies can be reconstructed. However, the chi-square statistic does not carry information about the sign of the phi coefficient. Therefore, values specified via the <code>x2i</code> argument can be positive or negative, which allows the specification of the correct sign. Also, when using a chi-square statistic as input, it is assumed that it was computed using Yates's correction for continuity (unless <code>correct=FALSE</code>). If the chi-square statistic is not known, but its p-value, one can first back-calculate the chi-square statistic using <code>qchisq(&lt;p-value&gt;, df=1, lower.tail=FALSE)</code>.
</p>
</li></ol>

<p>Typically, the odds ratio, phi coefficient, or chi-square statistic (or its p-value) that can be extracted from a study will be rounded to a certain degree. The calculations underlying the function are exact only for unrounded values. Rounding can therefore introduce some discrepancies.
</p>
<p>If a marginal total is unknown, then external information needs to be used to &lsquo;guestimate&rsquo; the number of individuals that experienced the outcome of interest on this variable. Depending on the accuracy of such an estimate, the reconstructed cell frequencies will be more or less accurate and need to be treated with due caution.
</p>
<p>The true marginal counts also put constraints on the possible values for the odds ratio, phi coefficient, and chi-square statistic. If a marginal count is replaced by a guestimate which is not compatible with the given statistic, one or more reconstructed cell frequencies may be negative. The function issues a warning if this happens and sets the cell frequencies to <code>NA</code> for such a study.
</p>
<p>If only one of the two marginal counts is unknown but a 95% CI for the odds ratio is also available, then the <a href="https://cran.r-project.org/package=estimraw">estimraw</a> package can also be used to reconstruct the corresponding cell frequencies (Di Pietrantonj, 2006; but see Veroniki et al., 2013, for some cautions).
</p>


<h3>Value</h3>

<p>If the <code>data</code> argument was not specified or <code>append=FALSE</code>, a data frame with four variables called <code>var.names</code> with the reconstructed cell frequencies.
</p>
<p>If <code>data</code> was specified and <code>append=TRUE</code>, then the original data frame is returned. If <code>var.names[j]</code> (for \(\textrm{j} \in \{1, \ldots, 4\}\)) is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the estimated frequencies (where possible) and otherwise a new variable called <code>var.names[j]</code> is added to the data frame.
</p>
<p>If <code>replace="all"</code> (or <code>replace=TRUE</code>), then all values in <code>var.names[j]</code> where a reconstructed cell frequency can be computed are replaced, even for cases where the value in <code>var.names[j]</code> is not missing.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Bonett, D. G. (2007). Transforming odds ratios into correlations for meta-analytic research. <em>American Psychologist</em>, <b>62</b>(3), 254&ndash;255. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0003-066x.62.3.254&#8288;</code>
</p>
<p>Di Pietrantonj, C. (2006). Four-fold table cell frequencies imputation in meta analysis. <em>Statistics in Medicine</em>, <b>25</b>(13), 2299&ndash;2322. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2287&#8288;</code>
</p>
<p>Veroniki, A. A., Pavlides, M., Patsopoulos, N. A., &amp; Salanti, G. (2013). Reconstructing 2 x 2 contingency tables from odds ratios using the Di Pietrantonj method: Difficulties, constraints and impact in meta-analysis results. <em>Research Synthesis Methods</em>, <b>4</b>(1), 78&ndash;94. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1061&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute various effect size measures based on \(2 \times 2\) table data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### demonstration that the reconstruction of the 2x2 table works
### (note: the values in rows 2, 3, and 4 correspond to those in row 1)
dat &lt;- data.frame(ai=c(36,NA,NA,NA), bi=c(86,NA,NA,NA), ci=c(20,NA,NA,NA), di=c(98,NA,NA,NA),
                  oddsratio=NA, phi=NA, chisq=NA, ni=NA, n1i=NA, n2i=NA)
dat$oddsratio[2] &lt;- round(exp(escalc(measure="OR", ai=ai, bi=bi, ci=ci, di=di, data=dat)$yi[1]), 2)
dat$phi[3] &lt;- round(escalc(measure="PHI", ai=ai, bi=bi, ci=ci, di=di, data=dat)$yi[1], 2)
dat$chisq[4] &lt;- round(chisq.test(matrix(c(t(dat[1,1:4])), nrow=2, byrow=TRUE))$statistic, 2)
dat$ni[2:4]  &lt;- with(dat, ai[1] + bi[1] + ci[1] + di[1])
dat$n1i[2:4] &lt;- with(dat, ai[1] + bi[1])
dat$n2i[2:4] &lt;- with(dat, ai[1] + ci[1])
dat

### reconstruct cell frequencies for rows 2, 3, and 4
dat &lt;- conv.2x2(ri=phi, ori=oddsratio, x2i=chisq, ni=ni, n1i=n1i, n2i=n2i, data=dat)
dat

### same example but with cell frequencies that are 10 times as large
dat &lt;- data.frame(ai=c(360,NA,NA,NA), bi=c(860,NA,NA,NA), ci=c(200,NA,NA,NA), di=c(980,NA,NA,NA),
                  oddsratio=NA, phi=NA, chisq=NA, ni=NA, n1i=NA, n2i=NA)
dat$oddsratio[2] &lt;- round(exp(escalc(measure="OR", ai=ai, bi=bi, ci=ci, di=di, data=dat)$yi[1]), 2)
dat$phi[3] &lt;- round(escalc(measure="PHI", ai=ai, bi=bi, ci=ci, di=di, data=dat)$yi[1], 2)
dat$chisq[4] &lt;- round(chisq.test(matrix(c(t(dat[1,1:4])), nrow=2, byrow=TRUE))$statistic, 2)
dat$ni[2:4]  &lt;- with(dat, ai[1] + bi[1] + ci[1] + di[1])
dat$n1i[2:4] &lt;- with(dat, ai[1] + bi[1])
dat$n2i[2:4] &lt;- with(dat, ai[1] + ci[1])
dat &lt;- conv.2x2(ri=phi, ori=oddsratio, x2i=chisq, ni=ni, n1i=n1i, n2i=n2i, data=dat)
dat # slight inaccuracy in row 3 due to rounding

### demonstrate what happens when a true marginal count is guestimated
escalc(measure="PHI", ai=176, bi=24, ci=72, di=128)
conv.2x2(ri=0.54, ni=400, n1i=200, n2i=248) # using the true marginal counts
conv.2x2(ri=0.54, ni=400, n1i=200, n2i=200) # marginal count for variable 2 is guestimated
conv.2x2(ri=0.54, ni=400, n1i=200, n2i=50)  # marginal count for variable 2 is incompatible

### demonstrate that using the correct sign for the chi-square statistic is important
chisq &lt;- round(chisq.test(matrix(c(40,60,60,40), nrow=2, byrow=TRUE))$statistic, 2)
conv.2x2(x2i=-chisq, ni=200, n1i=100, n2i=100) # correct reconstruction
conv.2x2(x2i=chisq, ni=200, n1i=100, n2i=100) # incorrect reconstruction

### demonstrate use of the 'correct' argument
tab &lt;- matrix(c(28,14,12,18), nrow=2, byrow=TRUE)
chisq &lt;- round(chisq.test(tab)$statistic, 2) # chi-square test with Yates' continuity correction
conv.2x2(x2i=chisq, ni=72, n1i=42, n2i=40) # correct reconstruction
chisq &lt;- round(chisq.test(tab, correct=FALSE)$statistic, 2) # without Yates' continuity correction
conv.2x2(x2i=chisq, ni=72, n1i=42, n2i=40) # incorrect reconstruction
conv.2x2(x2i=chisq, ni=72, n1i=42, n2i=40, correct=FALSE) # correct reconstruction

### recalculate chi-square statistic based on p-value
pval &lt;- round(chisq.test(tab)$p.value, 2)
chisq &lt;- qchisq(pval, df=1, lower.tail=FALSE)
conv.2x2(x2i=chisq, ni=72, n1i=42, n2i=40)
</code></pre>

<hr>
<h2 id='conv.delta'>Transform Observed Effect Sizes or Outcomes and their Sampling Variances using the Delta Method</h2><span id='topic+conv.delta'></span>

<h3>Description</h3>

<p>Function to transform observed effect sizes or outcomes and their sampling variances using the delta method. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conv.delta(yi, vi, ni, data, include, transf, var.names, append=TRUE, replace="ifna", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conv.delta_+3A_yi">yi</code></td>
<td>
<p>vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_ni">ni</code></td>
<td>
<p>vector with the total sample sizes of the studies.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_include">include</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies for which the transformation should be carried out.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_transf">transf</code></td>
<td>
<p>a function which should be used for the transformation.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_var.names">var.names</code></td>
<td>
<p>character vector with two elements to specify the name of the variable for the transformed effect sizes or outcomes and the name of the variable for the corresponding sampling variances (if <code>data</code> is an object of class <code>"escalc"</code>, the <code>var.names</code> are taken from the object; otherwise the defaults are <code>"yi"</code> and <code>"vi"</code>).</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame provided via the <code>data</code> argument should be returned together with the estimated values (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_replace">replace</code></td>
<td>
<p>character string or logical to specify how values in <code>var.names</code> should be replaced (only relevant when using the <code>data</code> argument and if variables in <code>var.names</code> already exist in the data frame). See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
<tr><td><code id="conv.delta_+3A_...">...</code></td>
<td>
<p>other arguments for the transformation function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+escalc">escalc</a></code> function can be used to compute a wide variety of effect sizes or &lsquo;outcome measures&rsquo;. In some cases, it may be necessary to transform one type of measure to another. The present function provides a general method for doing so via the <a href="https://en.wikipedia.org/wiki/Delta_method">delta method</a>, which briefly works as follows.
</p>
<p>Let \(y_i\) denote the observed effect size or outcome for a particular study and \(v_i\) the corresponding sampling variance. Then \(f(y_i)\) will be the transformed effect size or outcome, where \(f(\cdot)\) is the function specified via the <code>transf</code> argument. The sampling variance of the transformed effect size or outcome is then computed with \(v_i \times f'(y_i)^2\), where \(f'(y_i)\) denotes the derivative of \(f(\cdot)\) evaluated at \(y_i\). The present function computes the derivative numerically using the <code><a href="numDeriv.html#topic+grad">grad</a></code> function from the <code>numDeriv</code> package.
</p>
<p>The value of the observed effect size or outcome should be the first argument of the function specified via <code>transf</code>. The function can have additional arguments, which can be specified via the ... argument. However, due to the manner in which these additional arguments are evaluated, they cannot have names that match one of the arguments of the <code><a href="numDeriv.html#topic+grad">grad</a></code> function (an error will be issued if such a naming clash is detected).
</p>
<p>Optionally, one can use the <code>ni</code> argument to supply the total sample sizes of the studies. This has no relevance for the calculations done by the present function, but some other functions may use this information (e.g., when drawing a funnel plot with the <code><a href="#topic+funnel">funnel</a></code> function and one adjusts the <code>yaxis</code> argument to one of the options that puts the sample sizes or some transformation thereof on the y-axis).
</p>


<h3>Value</h3>

<p>If the <code>data</code> argument was not specified or <code>append=FALSE</code>, a data frame of class <code>c("escalc","data.frame")</code> with two variables called <code>var.names[1]</code> (by default <code>"yi"</code>) and <code>var.names[2]</code> (by default <code>"vi"</code>) with the transformed observed effect sizes or outcomes and the corresponding sampling variances (computed as described above).
</p>
<p>If <code>data</code> was specified and <code>append=TRUE</code>, then the original data frame is returned. If <code>var.names[1]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the transformed observed effect sizes or outcomes (where possible) and otherwise a new variable called <code>var.names[1]</code> is added to the data frame. Similarly, if <code>var.names[2]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the sampling variances calculated as described above (where possible) and otherwise a new variable called <code>var.names[2]</code> is added to the data frame.
</p>
<p>If <code>replace="all"</code> (or <code>replace=TRUE</code>), then all values in <code>var.names[1]</code> and <code>var.names[2]</code> are replaced, even for cases where the value in <code>var.names[1]</code> and <code>var.names[2]</code> is not missing.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute various effect size measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### the following examples illustrate that the use of the delta method (with numeric derivatives)
### yields essentially identical results as the analytic calculations that are done by escalc()

### compute logit transformed proportions and corresponding sampling variances for two studies
escalc(measure="PLO", xi=c(5,12), ni=c(40,80))

### compute raw proportions and corresponding sampling variances for the two studies
dat &lt;- escalc(measure="PR", xi=c(5,12), ni=c(40,80))
dat

### apply the logit transformation (note: this yields the same values as above with measure="PLO")
conv.delta(dat$yi, dat$vi, transf=transf.logit)

### using the 'data' argument
conv.delta(yi, vi, data=dat, transf=transf.logit, var.names=c("yi.t","vi.t"))

### or replace the existing 'yi' and 'vi' values
conv.delta(yi, vi, data=dat, transf=transf.logit, replace="all")

######################################

### use escalc() with measure D2ORN which transforms standardized mean differences (computed
### from means and standard deviations) into the corresponding log odds ratios
escalc(measure="D2ORN", m1i=m1i, sd1i=sd1i, n1i=n1i,
                        m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999)

### use escalc() to compute standardized mean differences (without the usual bias correction) and
### then apply the same transformation to the standardized mean differences
dat &lt;- escalc(measure="SMD", m1i=m1i, sd1i=sd1i, n1i=n1i,
                             m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999, correct=FALSE)
conv.delta(yi, vi, data=dat, transf=transf.dtolnor.norm, replace="all")

######################################

### an example where the transformation function takes additional arguments

### use escalc() with measure RPB which transforms standardized mean differences (computed
### from means and standard deviations) into the corresponding point-biserial correlations
escalc(measure="RPB", m1i=m1i, sd1i=sd1i, n1i=n1i,
                      m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999)

### use escalc() to compute standardized mean differences (without the usual bias correction) and
### then apply the same transformation to the standardized mean differences
dat &lt;- escalc(measure="SMD", m1i=m1i, sd1i=sd1i, n1i=n1i,
                             m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999, correct=FALSE)
conv.delta(yi, vi, data=dat, transf=transf.dtorpb, n1i=n1i, n2i=n2i, replace="all")

############################################################################

### a more elaborate example showing how this function could be used in the data
### preparation steps for a meta-analysis of standardized mean differences (SMDs)

dat &lt;- data.frame(study=1:6,
         m1i=c(2.03,NA,NA,NA,NA,NA), sd1i=c(0.95,NA,NA,NA,NA,NA), n1i=c(32,95,145,NA,NA,NA),
         m2i=c(1.25,NA,NA,NA,NA,NA), sd2i=c(1.04,NA,NA,NA,NA,NA), n2i=c(30,99,155,NA,NA,NA),
         tval=c(NA,2.12,NA,NA,NA,NA), dval=c(NA,NA,0.37,NA,NA,NA),
         ai=c(NA,NA,NA,26,NA,NA), bi=c(NA,NA,NA,58,NA,NA),
         ci=c(NA,NA,NA,11,NA,NA), di=c(NA,NA,NA,74,NA,NA),
         or=c(NA,NA,NA,NA,2.56,NA), lower=c(NA,NA,NA,NA,1.23,NA), upper=c(NA,NA,NA,NA,5.30,NA),
         corr=c(NA,NA,NA,NA,NA,.32), ntot=c(NA,NA,NA,NA,NA,86))
dat

### study types:
### 1) reports means and SDs so that the SMD can be directly calculated
### 2) reports the t-statistic from an independent samples t-test (and group sizes)
### 3) reports the standardized mean difference directly (and group sizes)
### 4) dichotomized the continuous dependent variable and reports the resulting 2x2 table
### 5) dichotomized the continuous dependent variable and reports an odds ratio with 95% CI
### 6) treated the group variable continuously and reports a Pearson product-moment correlation

### use escalc() to directly compute the SMD and its variance for studies 1, 2, and 3
dat &lt;- escalc(measure="SMD", m1i=m1i, sd1i=sd1i, n1i=n1i,
                             m2i=m2i, sd2i=sd2i, n2i=n2i, ti=tval, di=dval, data=dat)
dat

### use escalc() with measure OR2DN to compute the SMD value for study 4
dat &lt;- escalc(measure="OR2DN", ai=ai, bi=bi, ci=ci, di=di, data=dat, replace=FALSE)
dat

### use conv.wald() to convert the OR and CI into the log odds ratio and its variance for study 5
dat &lt;- conv.wald(out=or, ci.lb=lower, ci.ub=upper, data=dat,
                 transf=log, var.names=c("lnor","vlnor"))
dat

### use conv.delta() to transform the log odds ratio into the SMD value for study 5
dat &lt;- conv.delta(lnor, vlnor, data=dat,
                  transf=transf.lnortod.norm, var.names=c("yi","vi"))
dat

### remove the lnor and vlnor variables (no longer needed)
dat$lnor  &lt;- NULL
dat$vlnor &lt;- NULL

### use escalc() with measure COR to compute the sampling variance of ri for study 6
dat &lt;- escalc(measure="COR", ri=corr, ni=ntot, data=dat, var.names=c("ri","vri"))
dat

### use conv.delta() to transform the correlation into the SMD value for study 6
dat &lt;- conv.delta(ri, vri, data=dat, transf=transf.rtod, var.names=c("yi","vi"))
dat

### remove the ri and vri variables (no longer needed)
dat$ri  &lt;- NULL
dat$vri &lt;- NULL

### now variable 'yi' is complete with the SMD values for all studies
dat

### fit an equal-effects model to the SMD values
rma(yi, vi, data=dat, method="EE")

############################################################################

### a more elaborate example showing how this function could be used in the data
### preparation steps for a meta-analysis of correlation coefficients

dat &lt;- data.frame(study=1:6,
         ri=c(.42,NA,NA,NA,NA,NA),
         tval=c(NA,2.85,NA,NA,NA,NA),
         phi=c(NA,NA,NA,0.27,NA,NA),
         ni=c(93,182,NA,112,NA,NA),
         ai=c(NA,NA,NA,NA,61,NA), bi=c(NA,NA,NA,NA,36,NA),
         ci=c(NA,NA,NA,NA,39,NA), di=c(NA,NA,NA,NA,57,NA),
         or=c(NA,NA,NA,NA,NA,1.86), lower=c(NA,NA,NA,NA,NA,1.12), upper=c(NA,NA,NA,NA,NA,3.10),
         m1i=c(NA,NA,54.1,NA,NA,NA), sd1i=c(NA,NA,5.79,NA,NA,NA), n1i=c(NA,NA,66,75,NA,NA),
         m2i=c(NA,NA,51.7,NA,NA,NA), sd2i=c(NA,NA,6.23,NA,NA,NA), n2i=c(NA,NA,65,88,NA,NA))
dat

### study types:
### 1) reports the correlation coefficient directly
### 2) reports the t-statistic from a t-test of H0: rho = 0
### 3) dichotomized one variable and reports means and SDs for the two corresponding groups
### 4) reports the phi coefficient, marginal counts, and total sample size
### 5) dichotomized both variables and reports the resulting 2x2 table
### 6) dichotomized both variables and reports an odds ratio with 95% CI

### use escalc() to directly compute the correlation and its variance for studies 1 and 2
dat &lt;- escalc(measure="COR", ri=ri, ni=ni, ti=tval, data=dat)
dat

### use escalc() with measure RBIS to compute the biserial correlation for study 3
dat &lt;- escalc(measure="RBIS", m1i=m1i, sd1i=sd1i, n1i=n1i,
                              m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat, replace=FALSE)
dat

### use conv.2x2() to reconstruct the 2x2 table for study 4
dat &lt;- conv.2x2(ri=phi, ni=ni, n1i=n1i, n2i=n2i, data=dat)
dat

### use escalc() with measure RTET to compute the tetrachoric correlation for studies 4 and 5
dat &lt;- escalc(measure="RTET", ai=ai, bi=bi, ci=ci, di=di, data=dat, replace=FALSE)
dat

### use conv.wald() to convert the OR and CI into the log odds ratio and its variance for study 6
dat &lt;- conv.wald(out=or, ci.lb=lower, ci.ub=upper, data=dat,
                 transf=log, var.names=c("lnor","vlnor"))
dat

### use conv.delta() to estimate the tetrachoric correlation from the log odds ratio for study 6
dat &lt;- conv.delta(lnor, vlnor, data=dat,
                  transf=transf.lnortortet.pearson, var.names=c("yi","vi"))
dat

### remove the lnor and vlnor variables (no longer needed)
dat$lnor  &lt;- NULL
dat$vlnor &lt;- NULL

### now variable 'yi' is complete with the correlations for all studies
dat

### fit an equal-effects model to the correlations
rma(yi, vi, data=dat, method="EE")

############################################################################
</code></pre>

<hr>
<h2 id='conv.fivenum'>Estimate Means and Standard Deviations from Five-Number Summary Values</h2><span id='topic+conv.fivenum'></span>

<h3>Description</h3>

<p>Function to estimate means and standard deviations from five-number summary values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conv.fivenum(min, q1, median, q3, max, n, data, include,
             method="default", dist="norm", transf=TRUE, test=TRUE,
             var.names=c("mean","sd"), append=TRUE, replace="ifna", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conv.fivenum_+3A_min">min</code></td>
<td>
<p>vector with the minimum values.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_q1">q1</code></td>
<td>
<p>vector with the lower/first quartile values.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_median">median</code></td>
<td>
<p>vector with the median values.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_q3">q3</code></td>
<td>
<p>vector with the upper/third quartile values.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_max">max</code></td>
<td>
<p>vector with the maximum values.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_n">n</code></td>
<td>
<p>vector with the sample sizes.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_include">include</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies for which means and standard deviations should be estimated.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_method">method</code></td>
<td>
<p>character string indicating the method to use. Either <code>"default"</code> (same as <code>"luo/wan/shi"</code> which is the current default), <code>"qe"</code>, <code>"bc"</code>, <code>"mln"</code>, or <code>"blue"</code>. Can be abbreviated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_dist">dist</code></td>
<td>
<p>character string indicating the distribution assumed for the underlying data (either <code>"norm"</code> for a normal distribution or <code>"lnorm"</code> for a log-normal distribution). Can also be a string vector if different distributions are assumed for different studies. Only relevant when <code>method="default"</code>.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_transf">transf</code></td>
<td>
<p>logical to specify whether the estimated means and standard deviations of the log-transformed data should be back-transformed as described by Shi et al. (2020b) (the default is <code>TRUE</code>). Only relevant when <code>dist="lnorm"</code> and when <code>method="default"</code>.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_test">test</code></td>
<td>
<p>logical to specify whether a study should be excluded from the estimation if the test for skewness is significant (the default is <code>TRUE</code>, but whether this is applicable depends on the method; see &lsquo;Details&rsquo;).</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_var.names">var.names</code></td>
<td>
<p>character vector with two elements to specify the name of the variable for the estimated means and the name of the variable for the estimated standard deviations (the defaults are <code>"mean"</code> and <code>"sd"</code>).</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame provided via the <code>data</code> argument should be returned together with the estimated values (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_replace">replace</code></td>
<td>
<p>character string or logical to specify how values in <code>var.names</code> should be replaced (only relevant when using the <code>data</code> argument and if variables in <code>var.names</code> already exist in the data frame). See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
<tr><td><code id="conv.fivenum_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Various effect size measures require means and standard deviations (SDs) as input (e.g., raw or standardized mean differences, ratios of means / response ratios; see <code><a href="#topic+escalc">escalc</a></code> for further details). For some studies, authors may not report means and SDs, but other statistics, such as the so-called &lsquo;five-number summary&rsquo;, consisting of the minimum, lower/first quartile, median, upper/third quartile, and the maximum of the sample values (plus the sample sizes). Occasionally, only a subset of these values are reported.
</p>
<p>The present function can be used to estimate means and standard deviations from five-number summary values based on various methods described in the literature (Bland, 2015; Cai et al. 2021; Hozo et al., 2005; Luo et al., 2016; McGrath et al., 2020; Shi et al., 2020a; Walter &amp; Yao, 2007; Wan et al., 2014; Yang et al., 2022).
</p>
<p>When <code>method="default"</code> (which is the same as <code>"luo/wan/shi"</code>), the following methods are used:
</p>


<h4>Case 1: Min, Median, Max</h4>

<p>In case only the minimum, median, and maximum is available for a study (plus the sample size), then the function uses the method by Luo et al. (2016), equation (7), to estimate the mean and the method by Wan et al. (2014), equation (9), to estimate the SD.
</p>



<h4>Case 2: Q1, Median, Q3</h4>

<p>In case only the lower/first quartile, median, and upper/third quartile is available for a study (plus the sample size), then the function uses the method by Luo et al. (2016), equation (11), to estimate the mean and the method by Wan et al. (2014), equation (16), to estimate the SD.
</p>



<h4>Case 3: Min, Q1, Median, Q3, Max</h4>

<p>In case the full five-number summary is available for a study (plus the sample size), then the function uses the method by Luo et al. (2016), equation (15), to estimate the mean and the method by Shi et al. (2020a), equation (10), to estimate the SD.
</p>

<p>&mdash;&mdash;&mdash;
</p>
<p>The median is not actually needed in the methods by Wan et al. (2014) and Shi et al. (2020a) and hence it is possible to estimate the SD even if the median is unavailable (this can be useful if a study reports the mean directly, but instead of the SD, it reports the minimum/maximum and/or first/third quartile values).
</p>
<p>Note that the sample size must be at least 5 to apply these methods. Studies where the sample size is smaller are not included in the estimation. The function also checks that <code>min &lt;= q1 &lt;= median &lt;= q3 &lt;= max</code> and throws an error if any studies are found where this is not the case.
</p>


<h4>Test for Skewness</h4>

<p>The methods described above were derived under the assumption that the data are normally distributed. Testing this assumption would require access to the raw data, but based on the three cases above, Shi et al. (2023) derived tests for skewness that only require the reported quantile values and the sample sizes. These tests are automatically carried out. When <code>test=TRUE</code> (which is the default), a study is automatically excluded from the estimation if the test is significant. If all studies should be included, set <code>test=FALSE</code>, but note that the accuracy of the methods will tend to be poorer when the data come from an apparently skewed (and hence non-normal) distribution.
</p>



<h4>Log-Normal Distribution</h4>

<p>When setting <code>dist="lnorm"</code>, the raw data are assumed to follow a log-normal distribution. In this case, the methods as described by Shi et al. (2020b) are used to estimate the mean and SD of the log transformed data for the three cases above. When <code>transf=TRUE</code> (the default), the estimated mean and SD of the log transformed data are back-transformed to the estimated mean and SD of the raw data (using the bias-corrected back-transformation as described by Shi et al., 2020b). Note that the test for skewness is also carried out when <code>dist="lnorm"</code>, but now testing if the log transformed data exhibit skewness.
</p>



<h4>Alternative Methods</h4>

<p>As an alternative to the methods above, one can make use of the methods implemented in the <a href="https://cran.r-project.org/package=estmeansd">estmeansd</a> package to estimate means and SDs based on the three cases above. Available are the quantile estimation method (<code>method="qe"</code>; using the <code><a href="estmeansd.html#topic+qe.mean.sd">qe.mean.sd</a></code> function; McGrath et al., 2020), the Box-Cox method (<code>method="bc"</code>; using the <code><a href="estmeansd.html#topic+bc.mean.sd">bc.mean.sd</a></code> function; McGrath et al., 2020), and the method for unknown non-normal distributions (<code>method="mln"</code>; using the <code><a href="estmeansd.html#topic+mln.mean.sd">mln.mean.sd</a></code> function; Cai et al. 2021). The advantage of these methods is that they do not assume that the data underlying the reported values are normally distributed (and hence the <code>test</code> argument is ignored), but they can only be used when the values are positive (except for the quantile estimation method, which can also be used when one or more of the values are negative, but in this case the method does assume that the data are normally distributed and hence the test for skewness is applied when <code>test=TRUE</code>). Note that all of these methods may struggle to provide sensible estimates when some of the values are equal to each other (which can happen when the data include a lot of ties and/or the reported values are rounded). Also, the Box-Cox method and the method for unknown non-normal distributions involve simulated data and hence results will slightly change on repeated runs. Setting the seed of the random number generator (with <code><a href="base.html#topic+set.seed">set.seed</a></code>) ensures reproducibility.
</p>
<p>Finally, by setting <code>method="blue"</code>, one can make use of the <code><a href="metaBLUE.html#topic+BLUE_s">BLUE_s</a></code> function from the <a href="https://cran.r-project.org/package=metaBLUE">metaBLUE</a> package to estimate means and SDs based on the three cases above (Yang et al., 2022). The method assumes that the underlying data are normally distributed (and hence the test for skewness is applied when <code>test=TRUE</code>).
</p>



<h3>Value</h3>

<p>If the <code>data</code> argument was not specified or <code>append=FALSE</code>, a data frame with two variables called <code>var.names[1]</code> (by default <code>"mean"</code>) and <code>var.names[2]</code> (by default <code>"sd"</code>) with the estimated means and SDs.
</p>
<p>If <code>data</code> was specified and <code>append=TRUE</code>, then the original data frame is returned. If <code>var.names[1]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the estimated means (where possible) and otherwise a new variable called <code>var.names[1]</code> is added to the data frame. Similarly, if <code>var.names[2]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the estimated SDs (where possible) and otherwise a new variable called <code>var.names[2]</code> is added to the data frame.
</p>
<p>If <code>replace="all"</code> (or <code>replace=TRUE</code>), then all values in <code>var.names[1]</code> and <code>var.names[2]</code> where an estimated mean and SD can be computed are replaced, even for cases where the value in <code>var.names[1]</code> and <code>var.names[2]</code> is not missing.
</p>
<p>When missing values in <code>var.names[1]</code> are replaced, an attribute called <code>"est"</code> is added to the variable, which is a logical vector that is <code>TRUE</code> for values that were estimated. The same is done when missing values in <code>var.names[2]</code> are replaced.
</p>
<p>Attributes called <code>"tval"</code>, <code>"crit"</code>, <code>"sig"</code>, and <code>"dist"</code> are also added to <code>var.names[1]</code> corresponding to the test statistic and critical value for the test for skewness, whether the test was significant, and the assumed distribution (for the quantile estimation method, this is the distribution that provides the best fit to the given values).
</p>


<h3>Note</h3>

<p><b>A word of caution:</b> Under the given distributional assumptions, the estimated means and SDs are approximately unbiased and hence so are any effect size measures computed based on them (assuming a measure is unbiased to begin with when computed with directly reported means and SDs). However, the estimated means and SDs are less precise (i.e., are more variable) than directly reported means and SDs (especially under case 1) and hence computing the sampling variance of a measure with equations that assume that directly reported means and SDs are available will tend to underestimate the actual sampling variance of the measure, giving too much weight to estimates computed based on estimated means and SDs (see also McGrath et al., 2023). It would therefore be prudent to treat effect size estimates computed from estimated means and SDs with caution (e.g., by examining in a moderator analysis whether there are systematic differences between studies directly reporting means and SDs and those where the means and SDs needed to be estimated and/or as part of a sensitivity analysis). McGrath et al. (2023) also suggest to use bootstrapping to estimate the sampling variance of effect size measures computed based on estimated means and SDs. See also the <a href="https://cran.r-project.org/package=metamedian">metamedian</a> package for this purpose.
</p>
<p>Also note that the development of methods for estimating means and SDs based on five-number summary values is an active area of research. Currently, when <code>method="default"</code>, then this is identical to <code>method="luo/wan/shi"</code>, but this might change in the future. For reproducibility, it is therefore recommended to explicitly set <code>method="luo/wan/shi"</code> (or one of the other methods) when running this function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Bland, M. (2015). Estimating mean and standard deviation from the sample size, three quartiles, minimum, and maximum. <em>International Journal of Statistics in Medical Research</em>, <b>4</b>(1), 57&ndash;64. <code style="white-space: pre;">&#8288;https://doi.org/10.6000/1929-6029.2015.04.01.6&#8288;</code>
</p>
<p>Cai, S., Zhou, J., &amp; Pan, J. (2021). Estimating the sample mean and standard deviation from order statistics and sample size in meta-analysis. <em>Statistical Methods in Medical Research</em>, <b>30</b>(12), 2701&ndash;2719. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/09622802211047348&#8288;</code>
</p>
<p>Hozo, S. P., Djulbegovic, B. &amp; Hozo, I. (2005). Estimating the mean and variance from the median, range, and the size of a sample. <em>BMC Medical Research Methodology</em>, <b>5</b>, 13. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/1471-2288-5-13&#8288;</code>
</p>
<p>Luo, D., Wan, X., Liu, J. &amp; Tong, T. (2016). Optimally estimating the sample mean from the sample size, median, mid-range, and/or mid-quartile range. <em>Statistical Methods in Medical Research</em>, <b>27</b>(6), 1785&ndash;1805. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/0962280216669183&#8288;</code>
</p>
<p>McGrath, S., Zhao, X., Steele, R., Thombs, B. D., Benedetti, A., &amp; the DEPRESsion Screening Data (DEPRESSD) Collaboration (2020). Estimating the sample mean and standard deviation from commonly reported quantiles in meta-analysis. <em>Statistical Methods in Medical Research</em>, <b>29</b>(9), 2520&ndash;2537. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/0962280219889080&#8288;</code>
</p>
<p>McGrath, S., Katzenschlager, S., Zimmer, A. J., Seitel, A., Steele, R., &amp; Benedetti, A. (2023). Standard error estimation in meta-analysis of studies reporting medians. <em>Statistical Methods in Medical Research</em>, <b>32</b>(2), 373&ndash;388. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/09622802221139233&#8288;</code>
</p>
<p>Shi, J., Luo, D., Weng, H., Zeng, X.-T., Lin, L., Chu, H. &amp; Tong, T. (2020a). Optimally estimating the sample standard deviation from the five-number summary. <em>Research Synthesis Methods</em>, <b>11</b>(5), 641&ndash;654. <code style="white-space: pre;">&#8288;https://doi.org/https://doi.org/10.1002/jrsm.1429&#8288;</code>
</p>
<p>Shi, J., Tong, T., Wang, Y. &amp; Genton, M. G. (2020b). Estimating the mean and variance from the five-number summary of a log-normal distribution. <em>Statistics and Its Interface</em>, <b>13</b>(4), 519&ndash;531. https://doi.org/10.4310/sii.2020.v13.n4.a9
</p>
<p>Shi, J., Luo, D., Wan, X., Liu, Y., Liu, J., Bian, Z. &amp; Tong, T. (2023). Detecting the skewness of data from the five-number summary and its application in meta-analysis. <em>Statistical Methods in Medical Research</em>, <b>32</b>(7), 1338&ndash;1360. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/09622802231172043&#8288;</code>
</p>
<p>Walter, S. D. &amp; Yao, X. (2007). Effect sizes can be calculated for studies reporting ranges for outcome variables in systematic reviews. <em>Journal of Clinical Epidemiology</em>, <b>60</b>(8), 849-852. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.jclinepi.2006.11.003&#8288;</code>
</p>
<p>Wan, X., Wang, W., Liu, J. &amp; Tong, T. (2014). Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range. <em>BMC Medical Research Methodology</em>, <b>14</b>, 135. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/1471-2288-14-135&#8288;</code>
</p>
<p>Yang, X., Hutson, A. D., &amp; Wang, D. (2022). A generalized BLUE approach for combining location and scale information in a meta-analysis. <em>Journal of Applied Statistics</em>, <b>49</b>(15), 3846&ndash;3867. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/02664763.2021.1967890&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute various effect size measures based on means and standard deviations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example data frame
dat &lt;- data.frame(case=c(1:3,NA), min=c(2,NA,2,NA), q1=c(NA,4,4,NA),
                  median=c(6,6,6,NA), q3=c(NA,10,10,NA), max=c(14,NA,14,NA),
                  mean=c(NA,NA,NA,7.0), sd=c(NA,NA,NA,4.2), n=c(20,20,20,20))
dat

# note that study 4 provides the mean and SD directly, while studies 1-3 provide five-number
# summary values or a subset thereof (corresponding to cases 1-3 above)

# estimate means/SDs (note: existing values in 'mean' and 'sd' are not touched)
dat &lt;- conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat)
dat

# check attributes (none of the tests are significant, so means/SDs are estimated for studies 1-3)
dfround(data.frame(attributes(dat$mean)), digits=3)

# calculate the log transformed coefficient of variation and corresponding sampling variance
dat &lt;- escalc(measure="CVLN", mi=mean, sdi=sd, ni=n, data=dat)
dat

# fit equal-effects model to the estimates
res &lt;- rma(yi, vi, data=dat, method="EE")
res

# estimated coefficient of variation (with 95% CI)
predict(res, transf=exp, digits=2)

############################################################################

# example data frame
dat &lt;- data.frame(case=c(1:3,NA), min=c(2,NA,2,NA), q1=c(NA,4,4,NA),
                  median=c(6,6,6,NA), q3=c(NA,10,10,NA), max=c(14,NA,14,NA),
                  mean=c(NA,NA,NA,7.0), sd=c(NA,NA,NA,4.2), n=c(20,20,20,20))
dat

# try out different methods
conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat)
set.seed(1234)
conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat, method="qe")
conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat, method="bc")
conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat, method="mln")
conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat, method="blue")

############################################################################

# example data frame
dat &lt;- data.frame(case=c(1:3,NA), min=c(2,NA,2,NA), q1=c(NA,4,4,NA),
                  median=c(6,6,6,NA), q3=c(NA,10,14,NA), max=c(14,NA,20,NA),
                  mean=c(NA,NA,NA,7.0), sd=c(NA,NA,NA,4.2), n=c(20,20,20,20))
dat

# for study 3, the third quartile and maximum value suggest that the data have
# a right skewed distribution (they are much further away from the median than
# the minimum and first quartile)

# estimate means/SDs
dat &lt;- conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat)
dat

# note that the mean and SD are not estimated for study 3; this is because the
# test for skewness is significant for this study
dfround(data.frame(attributes(dat$mean)), digits=3)

# estimate means/SDs, but assume that the data for study 3 come from a log-normal distribution
# and back-transform the estimated mean/SD of the log-transformed data back to the raw data
dat &lt;- conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=dat,
                    dist=c("norm","norm","lnorm","norm"), replace="all")
dat

# this works now because the test for skewness of the log-transformed data is not significant
dfround(data.frame(attributes(dat$mean)), digits=3)
</code></pre>

<hr>
<h2 id='conv.wald'>Convert Wald-Type Confidence Intervals and Tests to Sampling Variances</h2><span id='topic+conv.wald'></span>

<h3>Description</h3>

<p>Function to convert Wald-type confidence intervals (CIs) and test statistics (or the corresponding p-values) to sampling variances. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conv.wald(out, ci.lb, ci.ub, zval, pval, n, data, include,
          level=95, transf, check=TRUE, var.names, append=TRUE, replace="ifna", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conv.wald_+3A_out">out</code></td>
<td>
<p>vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_ci.lb">ci.lb</code></td>
<td>
<p>vector with the lower bounds of the corresponding Wald-type CIs.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_ci.ub">ci.ub</code></td>
<td>
<p>vector with the upper bounds of the corresponding Wald-type CIs.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_zval">zval</code></td>
<td>
<p>vector with the Wald-type test statistics.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_pval">pval</code></td>
<td>
<p>vector with the p-values of the Wald-type tests.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_n">n</code></td>
<td>
<p>vector with the total sample sizes of the studies.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_include">include</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies for which the conversion should be carried out.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_level">level</code></td>
<td>
<p>numeric value (or vector) to specify the confidence interval level(s) (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform <code>out</code>, <code>ci.lb</code>, and <code>ci.ub</code> (e.g., <code>transf=log</code>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_check">check</code></td>
<td>
<p>logical to specify whether the function should carry out a check to examine if the point estimates fall (approximately) halfway between the CI bounds (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_var.names">var.names</code></td>
<td>
<p>character vector with two elements to specify the name of the variable for the observed effect sizes or outcomes and the name of the variable for the corresponding sampling variances (if <code>data</code> is an object of class <code>"escalc"</code>, the <code>var.names</code> are taken from the object; otherwise the defaults are <code>"yi"</code> and <code>"vi"</code>).</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame provided via the <code>data</code> argument should be returned together with the estimated values (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_replace">replace</code></td>
<td>
<p>character string or logical to specify how values in <code>var.names</code> should be replaced (only relevant when using the <code>data</code> argument and if variables in <code>var.names</code> already exist in the data frame). See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
<tr><td><code id="conv.wald_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+escalc">escalc</a></code> function can be used to compute a wide variety of effect sizes or &lsquo;outcome measures&rsquo;. However, the inputs required to compute certain measures with this function may not be reported for all of the studies. Under certain circumstances, other information (such as point estimates and corresponding confidence intervals and/or test statistics) may be available that can be converted into the appropriate format needed for a meta-analysis. The purpose of the present function is to facilitate this process.
</p>
<p>The function typically takes a data frame created with the <code><a href="#topic+escalc">escalc</a></code> function as input via the <code>data</code> argument. This object should contain variables <code>yi</code> and <code>vi</code> (unless argument <code>var.names</code> was used to adjust these variable names when the <code>"escalc"</code> object was created) for the observed effect sizes or outcomes and the corresponding sampling variances, respectively. For some studies, the values for these variables may be missing.
</p>


<h4>Converting Point Estimates and Confidence Intervals</h4>

<p>In some studies, the effect size estimate or observed outcome may already be reported. If so, such values can be supplied via the <code>out</code> argument and are then substituted for missing <code>yi</code> values. At times, it may be necessary to transform the reported values (e.g., reported odds ratios to log odds ratios). Via argument <code>transf</code>, an appropriate transformation function can be specified (e.g., <code>transf=log</code>), in which case \(y_i = f(\textrm{out})\) where \(f(\cdot)\) is the function specified via <code>transf</code>.
</p>
<p>Moreover, a confidence interval (CI) may have been reported together with the estimate. The bounds of the CI can be supplied via arguments <code>ci.lb</code> and <code>ci.ub</code>, which are also transformed if a function is specified via <code>transf</code>. Assume that the bounds were obtained from a Wald-type CI of the form \(y_i \pm z_{crit} \sqrt{v_i}\) (on the transformed scale if <code>transf</code> is specified), where \(v_i\) is the sampling variance corresponding to the effect size estimate or observed outcome (so that \(\sqrt{v_i}\) is the corresponding standard error) and \(z_{crit}\) is the appropriate critical value from a standard normal distribution (e.g., \(1.96\) for a 95% CI). Then \[v_i = \left(\frac{\textrm{ci.ub} - \textrm{ci.lb}}{2 \times z_{crit}}\right)^2\] is used to back-calculate the sampling variances of the (transformed) effect size estimates or observed outcomes and these values are then substituted for missing <code>vi</code> values in the dataset.
</p>
<p>For example, consider the following dataset of three RCTs used as input for a meta-analysis of log odds ratios:
</p>
<pre>
dat &lt;- data.frame(study = 1:3,
                  cases.trt = c(23, NA, 4), n.trt = c(194, 183, 46),
                  cases.plc = c(38, NA, 7), n.plc = c(201, 188, 44),
                  oddsratio = c(NA, 0.64, NA), lower = c(NA, 0.33, NA), upper = c(NA, 1.22, NA))
dat &lt;- escalc(measure="OR", ai=cases.trt, n1i=n.trt, ci=cases.plc, n2i=n.plc, data=dat)
dat

#   study cases.trt n.trt cases.plc n.plc oddsratio lower upper      yi     vi
# 1     1        23   194        38   201        NA    NA    NA -0.5500 0.0818
# 2     2        NA   183        NA   188      0.64  0.33  1.22      NA     NA
# 3     3         4    46         7    44        NA    NA    NA -0.6864 0.4437</pre>
<p>where variable <code>yi</code> contains the log odds ratios and <code>vi</code> the corresponding sampling variances as computed from the counts and group sizes by <code>escalc()</code>.
</p>
<p>Study 2 does not report the counts (or sufficient information to reconstruct them), but the odds ratio and a corresponding 95% confidence interval (CI) directly, as given by variables <code>oddsratio</code>, <code>lower</code>, and <code>upper</code>. The CI is a standard Wald-type CI that was computed on the log scale (and whose bounds were then exponentiated). Then the present function can be used as follows:
</p>
<pre>
dat &lt;- conv.wald(out=oddsratio, ci.lb=lower, ci.ub=upper, data=dat, transf=log)
dat

#   study cases.trt n.trt cases.plc n.plc oddsratio lower upper      yi     vi
# 1     1        23   194        38   201        NA    NA    NA -0.5500 0.0818
# 2     2        NA   183        NA   188      0.64  0.33  1.22 -0.4463 0.1113
# 3     3         4    46         7    44        NA    NA    NA -0.6864 0.4437</pre>
<p>Now variables <code>yi</code> and <code>vi</code> in the dataset are complete.
</p>
<p>If the CI was not a 95% CI, then one can specify the appropriate level via the <code>level</code> argument. This can also be an entire vector in case different studies used different levels.
</p>
<p>By default (i.e., when <code>check=TRUE</code>), the function carries out a rough check to examine if the point estimate falls (approximately) halfway between the CI bounds (on the transformed scale) for each study for which the conversion was carried out. A warning is issued if there are studies where this is not the case. This may indicate that a particular CI was not a Wald-type CI or was computed on a different scale (in which case the back-calculation above would be inappropriate), but can also arise due to rounding of the reported values (in which case the back-calculation would still be appropriate, albeit possibly a bit inaccurate). Care should be taken when using such back-calculated values in a meta-analysis.
</p>



<h4>Converting Test Statistics and P-Values</h4>

<p>Similarly, study authors may report the test statistic and/or p-value from a Wald-type test of the form \(\textrm{zval} = y_i / \sqrt{v_i}\) (on the transformed scale if <code>transf</code> is specified), with the corresponding two-sided p-value given by \(\textrm{pval} = 2(1 - \Phi(\textrm{|zval|}))\), where \(\Phi(\cdot)\) denotes the cumulative distribution function of a standard normal distribution (i.e., <code><a href="stats.html#topic+pnorm">pnorm</a></code>). Test statistics and/or corresponding p-values of this form can be supplied via arguments <code>zval</code> and <code>pval</code>.
</p>
<p>A given p-value can be back-transformed into the corresponding test statistic (if it is not already available) with \(\textrm{zval} = \Phi^{-1}(1 - \textrm{pval}/2)\), where \(\Phi^{-1}(\cdot)\) denotes the quantile function (i.e., the inverse of the cumulative distribution function) of a standard normal distribution (i.e., <code><a href="stats.html#topic+qnorm">qnorm</a></code>). Then \[v_i = \left(\frac{y_i}{\textrm{zval}}\right)^2\] is used to back-calculate a missing <code>vi</code> value in the dataset.
</p>
<p>Note that the conversion of a p-value to the corresponding test statistic (which is then converted into sampling variance) as shown above assumes that the exact p-value is reported. If authors only report that the p-value fell below a certain threshold (e.g., \(p < .01\) or if authors only state that the test was significant &ndash; which typically implies \(p < .05\)), then a common approach is to use the value of the cutoff reported (e.g., if \(p < .01\) is reported, then assume \(p = .01\)), which is conservative (since the actual p-value was below that assumed value by some unknown amount). The conversion will therefore tend to be much less accurate.
</p>
<p>Using the earlier example, suppose that only the odds ratio and the corresponding two-sided p-value from a Wald-type test (whether the log odds ratio differs significantly from zero) is reported for study 2.
</p>
<pre>
dat &lt;- data.frame(study = 1:3,
                  cases.trt = c(23, NA, 4), n.trt = c(194, 183, 46),
                  cases.plc = c(38, NA, 7), n.plc = c(201, 188, 44),
                  oddsratio = c(NA, 0.64, NA), pval = c(NA, 0.17, NA))
dat &lt;- escalc(measure="OR", ai=cases.trt, n1i=n.trt, ci=cases.plc, n2i=n.plc, data=dat)
dat

  study cases.trt n.trt cases.plc n.plc oddsratio pval      yi     vi
1     1        23   194        38   201        NA   NA -0.5500 0.0818
2     2        NA   183        NA   188      0.64 0.17      NA     NA
3     3         4    46         7    44        NA   NA -0.6864 0.4437</pre>
<p>Then the function can be used as follows:
</p>
<pre>
dat &lt;- conv.wald(out=oddsratio, pval=pval, data=dat, transf=log)
dat

#   study cases.trt n.trt cases.plc n.plc oddsratio pval      yi     vi
# 1     1        23   194        38   201        NA   NA -0.5500 0.0818
# 2     2        NA   183        NA   188      0.64 0.17 -0.4463 0.1058
# 3     3         4    46         7    44        NA   NA -0.6864 0.4437</pre>
<p>Note that the back-calculated sampling variance for study 2 is not identical in these two examples, because the CI bounds and p-value are rounded to two decimal places, which introduces some inaccuracies. Also, if both (<code>ci.lb</code>, <code>ci.ub</code>) and either <code>zval</code> or <code>pval</code> is available for a study, then the back-calculation of \(v_i\) via the confidence interval is preferred.
</p>

<p>Optionally, one can use the <code>n</code> argument to supply the total sample sizes of the studies. This has no relevance for the calculations done by the present function, but some other functions may use this information (e.g., when drawing a funnel plot with the <code><a href="#topic+funnel">funnel</a></code> function and one adjusts the <code>yaxis</code> argument to one of the options that puts the sample sizes or some transformation thereof on the y-axis).
</p>


<h3>Value</h3>

<p>If the <code>data</code> argument was not specified or <code>append=FALSE</code>, a data frame of class <code>c("escalc","data.frame")</code> with two variables called <code>var.names[1]</code> (by default <code>"yi"</code>) and <code>var.names[2]</code> (by default <code>"vi"</code>) with the (transformed) observed effect sizes or outcomes and the corresponding sampling variances (computed as described above).
</p>
<p>If <code>data</code> was specified and <code>append=TRUE</code>, then the original data frame is returned. If <code>var.names[1]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the (possibly transformed) observed effect sizes or outcomes from <code>out</code> (where possible) and otherwise a new variable called <code>var.names[1]</code> is added to the data frame. Similarly, if <code>var.names[2]</code> is a variable in <code>data</code> and <code>replace="ifna"</code> (or <code>replace=FALSE</code>), then only missing values in this variable are replaced with the sampling variances back-calculated as described above (where possible) and otherwise a new variable called <code>var.names[2]</code> is added to the data frame.
</p>
<p>If <code>replace="all"</code> (or <code>replace=TRUE</code>), then all values in <code>var.names[1]</code> and <code>var.names[2]</code> are replaced, even for cases where the value in <code>var.names[1]</code> and <code>var.names[2]</code> is not missing.
</p>


<h3>Note</h3>

<p><b>A word of caution</b>: Except for the check on the CI bounds, there is no possibility to determine if the back-calculations done by the function are appropriate in a given context. They are only appropriate when the CI bounds and tests statistics (or p-values) arose from Wald-type CIs / tests as described above. Using the same back-calculations for other purposes is likely to yield nonsensical values.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute various effect size measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### a very simple example
dat &lt;- data.frame(or=c(1.37,1.89), or.lb=c(1.03,1.60), or.ub=c(1.82,2.23))
dat

### convert the odds ratios and CIs into log odds ratios with corresponding sampling variances
dat &lt;- conv.wald(out=or, ci.lb=or.lb, ci.ub=or.ub, data=dat, transf=log)
dat

############################################################################

### a more elaborate example based on the BCG vaccine dataset
dat &lt;- dat.bcg[,c(2:7)]
dat

### with complete data, we can use escalc() in the usual way
dat1 &lt;- escalc(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)
dat1

### random-effects model fitted to these data
res1 &lt;- rma(yi, vi, data=dat1)
res1

### now suppose that the 2x2 table data are not reported in all studies, but that the
### following dataset could be assembled based on information reported in the studies
dat2 &lt;- data.frame(summary(dat1))
dat2[c("yi", "ci.lb", "ci.ub")] &lt;- data.frame(summary(dat1, transf=exp))[c("yi", "ci.lb", "ci.ub")]
names(dat2)[which(names(dat2) == "yi")] &lt;- "or"
dat2[,c("or","ci.lb","ci.ub","pval")] &lt;- round(dat2[,c("or","ci.lb","ci.ub","pval")], digits=2)
dat2$vi &lt;- dat2$sei &lt;- dat2$zi &lt;- NULL
dat2$ntot &lt;- with(dat2, tpos + tneg + cpos + cneg)
dat2[c(1,12),c(3:6,9:10)] &lt;- NA
dat2[c(4,9), c(3:6,8)] &lt;- NA
dat2[c(2:3,5:8,10:11,13),c(7:10)] &lt;- NA
dat2$ntot[!is.na(dat2$tpos)] &lt;- NA
dat2

### in studies 1 and 12, authors reported only the odds ratio and the corresponding p-value
### in studies 4 and 9, authors reported only the odds ratio and the corresponding 95% CI

### use escalc() first
dat2 &lt;- escalc(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat2)
dat2

### fill in the missing log odds ratios and sampling variances
dat2 &lt;- conv.wald(out=or, ci.lb=ci.lb, ci.ub=ci.ub, pval=pval, n=ntot, data=dat2, transf=log)
dat2

### random-effects model fitted to these data
res2 &lt;- rma(yi, vi, data=dat2)
res2

### any differences between res1 and res2 are a result of or, ci.lb, ci.ub, and pval being
### rounded in dat2 to two decimal places; without rounding, the results would be identical
</code></pre>

<hr>
<h2 id='cumul'>Cumulative Meta-Analysis for 'rma' Objects</h2><span id='topic+cumul'></span><span id='topic+cumul.rma.uni'></span><span id='topic+cumul.rma.mh'></span><span id='topic+cumul.rma.peto'></span>

<h3>Description</h3>

<p>Function to carry out a &lsquo;cumulative meta-analysis&rsquo;, by repeatedly fitting the specified model adding one study at a time. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cumul(x, ...)

## S3 method for class 'rma.uni'
cumul(x, order, digits, transf, targs, progbar=FALSE, ...)
## S3 method for class 'rma.mh'
cumul(x, order, digits, transf, targs, progbar=FALSE, ...)
## S3 method for class 'rma.peto'
cumul(x, order, digits, transf, targs, progbar=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cumul_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, or <code>"rma.peto"</code>.</p>
</td></tr>
<tr><td><code id="cumul_+3A_order">order</code></td>
<td>
<p>optional argument to specify a variable based on which the studies will be ordered for the cumulative meta-analysis.</p>
</td></tr>
<tr><td><code id="cumul_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="cumul_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the model coefficients and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="cumul_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="cumul_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="cumul_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>"rma.uni"</code> objects, the model specified via <code>x</code> must be a model without moderators (i.e., either an equal- or a random-effects model).
</p>
<p>If argument <code>order</code> is not specified, the studies are added according to their order in the original dataset.
</p>
<p>When a variable is specified for <code>order</code>, the variable is assumed to be of the same length as the original dataset that was used in the model fitting (and if the <code>data</code> argument was used in the original model fit, then the variable will be searched for within this data frame first). Any subsetting and removal of studies with missing values that was applied during the model fitting is also automatically applied to the variable specified via the <code>order</code> argument. See &lsquo;Examples&rsquo;.
</p>


<h3>Value</h3>

<p>An object of class <code>c("list.rma","cumul.rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>estimated (average) outcomes.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>corresponding test statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bounds of the confidence intervals.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bounds of the confidence intervals.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>test statistics for the test of heterogeneity.</p>
</td></tr>
<tr><td><code>Qp</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated amount of heterogeneity (only for random-effects models).</p>
</td></tr>
<tr><td><code>I2</code></td>
<td>
<p>values of \(I^2\).</p>
</td></tr>
<tr><td><code>H2</code></td>
<td>
<p>values of \(H^2\).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>
<p>When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then <code>zval</code> is called <code>tval</code> in the object that is returned by the function.
</p>
<p>The object is formatted and printed with the <code><a href="#topic+print.list.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function. A forest plot showing the results from the cumulative meta-analysis can be obtained with <code><a href="#topic+forest.cumul.rma">forest</a></code>. Alternatively, <code><a href="#topic+plot.cumul.rma">plot</a></code> can also be used to visualize the results.
</p>


<h3>Note</h3>

<p>When using the <code>transf</code> option, the transformation is applied to the estimated coefficients and the corresponding interval bounds. The standard errors are then set equal to <code>NA</code> and are omitted from the printed output.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Chalmers, T. C., &amp; Lau, J. (1993). Meta-analytic stimulus for changes in clinical trials. <em>Statistical Methods in Medical Research</em>, <b>2</b>(2), 161&ndash;172. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/096228029300200204&#8288;</code>
</p>
<p>Lau, J., Schmid, C. H., &amp; Chalmers, T. C. (1995). Cumulative meta-analysis of clinical trials builds evidence for exemplary medical care. <em>Journal of Clinical Epidemiology</em>, <b>48</b>(1), 45&ndash;57. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/0895-4356(94)00106-z&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest.cumul.rma">forest</a></code> for a function to draw cumulative forest plots and <code><a href="#topic+plot.cumul.rma">plot</a></code> for a different visualization of the cumulative results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### cumulative meta-analysis (in the order of publication year)
cumul(res, transf=exp, order=year)

### meta-analysis of the (log) risk ratios using the Mantel-Haenszel method
res &lt;- rma.mh(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### cumulative meta-analysis
cumul(res, order=year)
cumul(res, order=year, transf=TRUE)

### meta-analysis of the (log) odds ratios using Peto's method
res &lt;- rma.peto(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### cumulative meta-analysis
cumul(res, order=year)
cumul(res, order=year, transf=TRUE)

### make first log risk ratio missing and fit model without study 2; then the
### variable specified via 'order' should still be of the same length as the
### original dataset; subsetting and removal of studies with missing values is
### automatically done by the cumul() function
dat$yi[1] &lt;- NA
res &lt;- rma(yi, vi, data=dat, subset=-2)
cumul(res, transf=exp, order=year)
</code></pre>

<hr>
<h2 id='dfround'>Round Variables in a Data Frame</h2><span id='topic+dfround'></span>

<h3>Description</h3>

<p>Function to round the numeric variables in a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfround(x, digits, drop0=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfround_+3A_x">x</code></td>
<td>
<p>a data frame.</p>
</td></tr>
<tr><td><code id="dfround_+3A_digits">digits</code></td>
<td>
<p>either a single integer or a numeric vector of the same length as there are columns in <code>x</code>.</p>
</td></tr>
<tr><td><code id="dfround_+3A_drop0">drop0</code></td>
<td>
<p>logical (or a vector thereof) indicating if trailing zeros after the decimal mark should be removed (the default is <code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple convenience function to round the numeric variables in a data frame, possibly to different numbers of digits. Hence, <code>digits</code> can either be a single integer (which will then be used to round all numeric variables to the specified number of digits) or a numeric vector (of the same length as there are columns in <code>x</code>) to specify the number of digits to which each variable should be rounded.
</p>
<p>Non-numeric variables are skipped. If <code>digits</code> is a vector, some arbitrary value (or <code>NA</code>) can be specified for those variables.
</p>
<p>Note: When <code>drop0=FALSE</code>, then <code><a href="base.html#topic+formatC">formatC</a></code> is used to format the numbers, which turns them into character variables.
</p>


<h3>Value</h3>

<p>Returns the data frame with variables rounded as specified.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- dat.bcg
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)
coef(summary(res))
dfround(coef(summary(res)), digits=c(2,3,2,3,2,2))
</code></pre>

<hr>
<h2 id='emmprep'>Create a Reference Grid for the 'emmeans' Function</h2><span id='topic+emmprep'></span>

<h3>Description</h3>

<p>Function to create a reference grid for use with the <code><a href="emmeans.html#topic+emmeans">emmeans</a></code> function from the package of the same name. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emmprep(x, verbose=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emmprep_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="emmprep_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether information on some (extracted) settings should be printed when creating the reference grid (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="emmprep_+3A_...">...</code></td>
<td>
<p>other arguments that will be passed on to the <code><a href="emmeans.html#topic+qdrg">qdrg</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="https://cran.r-project.org/package=emmeans">emmeans</a> package is a popular package that facilitates the computation of 'estimated marginal means'. The function is a wrapper around the <code><a href="emmeans.html#topic+qdrg">qdrg</a></code> function from the <code>emmeans</code> package to make <code>"rma"</code> objects compatible with the latter. Unless one needs to pass additional arguments to the <code><a href="emmeans.html#topic+qdrg">qdrg</a></code> function, one simply applies this function to the <code>"rma"</code> object and then the <code><a href="emmeans.html#topic+emmeans">emmeans</a></code> function (or one of the other functions that can be applied to <code>"emmGrid"</code> objects) to the resulting object to obtain the desired estimated marginal means.
</p>


<h3>Value</h3>

<p>An <code>"emmGrid"</code> object as created by the <code><a href="emmeans.html#topic+qdrg">qdrg</a></code> function from the <code>emmeans</code> package.
</p>
<p>The resulting object will typically be used in combination with the <code><a href="emmeans.html#topic+emmeans">emmeans</a></code> function.
</p>


<h3>Note</h3>

<p>When creating the reference grid, the function extracts the degrees of freedom for tests/confidence intervals from the model object (if the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>; otherwise the degrees of freedom are infinity). In some cases, there is not just a single value for the degrees of freedom, but an entire vector (e.g., for models fitted with <code><a href="#topic+rma.mv">rma.mv</a></code>). In this case, the smallest value will be used (as a conservative option). One can set a different/custom value for the degrees of freedom with <code>emmprep(..., df=value)</code>.
</p>
<p>When the model object contains information about the outcome measure used in the analysis (which should be the case if the observed outcomes were computed with <code><a href="#topic+escalc">escalc</a></code> or if the <code>measure</code> argument was set when fitting the model), then information about the appropriate back-transformation (if available) is stored as part of the returned object. If so, the back-transformation is automatically applied when calling <code><a href="emmeans.html#topic+emmeans">emmeans</a></code> with <code>type="response"</code>.
</p>
<p>The function also tries to extract the estimated value of \(\tau^2\) (or more precisely, its square root) from the model object (when the model is a random/mixed-effects model). This value is only needed when computing prediction intervals (i.e., when <code>interval="predict"</code> in <code><a href="emmeans.html#topic+predict.emmGrid">predict.emmGrid</a></code>) or when applying the bias adjustment in the back-transformation (i.e., when <code>bias.adjust=TRUE</code> in <code><a href="emmeans.html#topic+summary.emmGrid">summary.emmGrid</a></code>). For some models (e.g., those fitted with <code><a href="#topic+rma.mv">rma.mv</a></code>), it is not possible to automatically extract the estimate. In this case, one can manually set the value with <code>emmprep(..., sigma=value)</code> (note: the argument is called <code>sigma</code>, following the conventions of <code><a href="emmeans.html#topic+summary.emmGrid">summary.emmGrid</a></code> and one must supply the square root of the \(\tau^2\) estimate).
</p>
<p>By default, the reference grid is created based on the data used for fitting the original model (which is typically the sensible thing to do). One can specify a different dataset with <code>emmprep(..., data=obj)</code>, where <code>obj</code> must be a data frame that contains the same variables as used in the original model fitted.
</p>
<p>If the original model fitted involved redundant predictors that were dropped from the model (due to &lsquo;rank deficiencies&rsquo;), then the function cannot be used. In this case, one should remove any redundancies in the original model fitted before using this function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit meta-regression model with absolute latitude as predictor
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)
res

### create reference grid
sav &lt;- emmprep(res, verbose=TRUE)

### estimated marginal mean (back-transformed to the risk ratio scale)
if (require(emmeans))
   emmeans(sav, specs="1", type="response")

### same as the predicted effect at the mean absolute latitude
predict(res, newmods = mean(model.matrix(res, asdf=TRUE)$ablat), transf=exp, digits=3)

### fit meta-regression model with allocation factor
res &lt;- rma(yi, vi, mods = ~ alloc, data=dat)
res

### create reference grid
sav &lt;- emmprep(res)

### estimated marginal mean using proportional cell weighting
if (require(emmeans))
   emmeans(sav, specs="1", type="response", weights="proportional")

### estimated marginal mean using equal cell weighting (this is actually the default)
if (require(emmeans))
   emmeans(sav, specs="1", type="response", weights="equal")

### same as the predicted effect using cell proportions as observed in the data
### or using equal proportions for the three groups
predict(res, newmods = colMeans(model.matrix(res))[-1], transf=exp, digits=3)
predict(res, newmods = c(1/3,1/3), transf=exp, digits=3)

### fit meta-regression model with absolute latitude and allocation as predictors
res &lt;- rma(yi, vi, mods = ~ ablat + alloc, data=dat)
res

### create reference grid
sav &lt;- emmprep(res)

### estimated marginal mean using equal cell weighting
if (require(emmeans))
   emmeans(sav, specs="1", type="response")

### same as the predicted effect at the mean absolute latitude and using equal proportions
### for the allocation factor
predict(res, newmods = c(mean(model.matrix(res, asdf=TRUE)$ablat),1/3,1/3), transf=exp, digits=3)

### create reference grid with ablat set equal to 10, 30, and 50 degrees
sav &lt;- emmprep(res, at=list(ablat=c(10,30,50)))

### estimated marginal means at the three ablat values
if (require(emmeans))
   emmeans(sav, specs="1", by="ablat", type="response")

### same as the predicted effect at the chosen absolute latitude values and using equal
### proportions for the allocation factor
predict(res, newmods = cbind(c(10,30,50),1/3,1/3), transf=exp, digits=3)

############################################################################

### copy data into 'dat' and examine data
dat &lt;- dat.mcdaniel1994
head(dat)

### calculate r-to-z transformed correlations and corresponding sampling variances
dat &lt;- escalc(measure="ZCOR", ri=ri, ni=ni, data=dat)

### mixed-effects model with interview type as factor
res &lt;- rma(yi, vi, mods = ~ factor(type), data=dat, test="knha")
res

### create reference grid
sav &lt;- emmprep(res, verbose=TRUE)

### estimated marginal mean (back-transformed to the correlation scale)
if (require(emmeans))
   emmeans(sav, specs="1", type="response")

### same as the predicted correlation using equal cell proportions
predict(res, newmods = c(1/3,1/3), transf=transf.ztor, digits=3)

### estimated marginal means for the three interview types
if (require(emmeans))
   emmeans(sav, specs="type", type="response")

### same as the predicted correlations
predict(res, newmods = rbind(c(0,0), c(1,0), c(0,1)), transf=transf.ztor, digits=3)

### illustrate use of the 'df' and 'sigma' arguments
res &lt;- rma.mv(yi, vi, mods = ~ factor(type), random = ~ 1 | study,
              data=dat, test="t", dfs="contain")
res

### create reference grid
sav &lt;- emmprep(res, verbose=TRUE, df=154, sigma=0.1681)

### estimated marginal mean (back-transformed to the correlation scale)
if (require(emmeans))
   emmeans(sav, specs="1", type="response")
</code></pre>

<hr>
<h2 id='escalc'>Calculate Effect Sizes and Outcome Measures</h2><span id='topic+escalc'></span>

<h3>Description</h3>

<p>Function to calculate various effect sizes or outcome measures (and the corresponding sampling variances) that are commonly used in meta-analyses. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>escalc(measure, ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
       m1i, m2i, sd1i, sd2i, xi, mi, ri, ti, fi, pi, sdi, r2i, ni, yi, vi, sei,
       data, slab, subset, include,
       add=1/2, to="only0", drop00=FALSE, vtype="LS",
       var.names=c("yi","vi"), add.measure=FALSE,
       append=TRUE, replace=TRUE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="escalc_+3A_measure">measure</code></td>
<td>
<p>a character string to specify which effect size or outcome measure should be calculated. See &lsquo;Details&rsquo; for possible options and how the data needed to compute the selected effect size or outcome measure should then be specified (i.e., which of the following arguments need to be used).</p>
</td></tr>
<tr><td><code id="escalc_+3A_ai">ai</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper left cell).</p>
</td></tr>
<tr><td><code id="escalc_+3A_bi">bi</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper right cell).</p>
</td></tr>
<tr><td><code id="escalc_+3A_ci">ci</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower left cell).</p>
</td></tr>
<tr><td><code id="escalc_+3A_di">di</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower right cell).</p>
</td></tr>
<tr><td><code id="escalc_+3A_n1i">n1i</code></td>
<td>
<p>vector with the group sizes or row totals (first group/row).</p>
</td></tr>
<tr><td><code id="escalc_+3A_n2i">n2i</code></td>
<td>
<p>vector with the group sizes or row totals (second group/row).</p>
</td></tr>
<tr><td><code id="escalc_+3A_x1i">x1i</code></td>
<td>
<p>vector with the number of events (first group).</p>
</td></tr>
<tr><td><code id="escalc_+3A_x2i">x2i</code></td>
<td>
<p>vector with the number of events (second group).</p>
</td></tr>
<tr><td><code id="escalc_+3A_t1i">t1i</code></td>
<td>
<p>vector with the total person-times (first group).</p>
</td></tr>
<tr><td><code id="escalc_+3A_t2i">t2i</code></td>
<td>
<p>vector with the total person-times (second group).</p>
</td></tr>
<tr><td><code id="escalc_+3A_m1i">m1i</code></td>
<td>
<p>vector with the means (first group or time point).</p>
</td></tr>
<tr><td><code id="escalc_+3A_m2i">m2i</code></td>
<td>
<p>vector with the means (second group or time point).</p>
</td></tr>
<tr><td><code id="escalc_+3A_sd1i">sd1i</code></td>
<td>
<p>vector with the standard deviations (first group or time point).</p>
</td></tr>
<tr><td><code id="escalc_+3A_sd2i">sd2i</code></td>
<td>
<p>vector with the standard deviations (second group or time point).</p>
</td></tr>
<tr><td><code id="escalc_+3A_xi">xi</code></td>
<td>
<p>vector with the frequencies of the event of interest.</p>
</td></tr>
<tr><td><code id="escalc_+3A_mi">mi</code></td>
<td>
<p>vector with the frequencies of the complement of the event of interest or the group means.</p>
</td></tr>
<tr><td><code id="escalc_+3A_ri">ri</code></td>
<td>
<p>vector with the raw correlation coefficients.</p>
</td></tr>
<tr><td><code id="escalc_+3A_ti">ti</code></td>
<td>
<p>vector with the total person-times or t-test statistics.</p>
</td></tr>
<tr><td><code id="escalc_+3A_fi">fi</code></td>
<td>
<p>vector with the F-test statistics.</p>
</td></tr>
<tr><td><code id="escalc_+3A_pi">pi</code></td>
<td>
<p>vector with the (signed) p-values.</p>
</td></tr>
<tr><td><code id="escalc_+3A_sdi">sdi</code></td>
<td>
<p>vector with the standard deviations.</p>
</td></tr>
<tr><td><code id="escalc_+3A_r2i">r2i</code></td>
<td>
<p>vector with the \(R^2\) values.</p>
</td></tr>
<tr><td><code id="escalc_+3A_ni">ni</code></td>
<td>
<p>vector with the sample/group sizes.</p>
</td></tr>
<tr><td><code id="escalc_+3A_yi">yi</code></td>
<td>
<p>vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="escalc_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances.</p>
</td></tr>
<tr><td><code id="escalc_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors.</p>
</td></tr>
<tr><td><code id="escalc_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="escalc_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the studies.</p>
</td></tr>
<tr><td><code id="escalc_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that will be included in the data frame returned by the function.</p>
</td></tr>
<tr><td><code id="escalc_+3A_include">include</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies for which the measure should be calculated. See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
<tr><td><code id="escalc_+3A_add">add</code></td>
<td>
<p>a non-negative number to specify the amount to add to zero cells, counts, or frequencies. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="escalc_+3A_to">to</code></td>
<td>
<p>a character string to specify when the values under <code>add</code> should be added (either <code>"all"</code>, <code>"only0"</code>, <code>"if0all"</code>, or <code>"none"</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="escalc_+3A_drop00">drop00</code></td>
<td>
<p>logical to specify whether studies with no cases/events (or only cases) in both groups should be dropped when calculating the observed effect sizes or outcomes. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="escalc_+3A_vtype">vtype</code></td>
<td>
<p>a character string to specify the type of sampling variances to calculate. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="escalc_+3A_var.names">var.names</code></td>
<td>
<p>character vector with two elements to specify the name of the variable for the observed effect sizes or outcomes and the name of the variable for the corresponding sampling variances (the defaults are <code>"yi"</code> and <code>"vi"</code>).</p>
</td></tr>
<tr><td><code id="escalc_+3A_add.measure">add.measure</code></td>
<td>
<p>logical to specify whether a variable should be added to the data frame (with default name <code>"measure"</code>) that indicates the type of outcome measure computed. When using this option, <code>var.names</code> can have a third element to change this variable name.</p>
</td></tr>
<tr><td><code id="escalc_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame provided via the <code>data</code> argument should be returned together with the observed effect sizes or outcomes and corresponding sampling variances (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="escalc_+3A_replace">replace</code></td>
<td>
<p>logical to specify whether existing values for <code>yi</code> and <code>vi</code> in the data frame should be replaced. Only relevant when <code>append=TRUE</code> and the data frame already contains the <code>yi</code> and <code>vi</code> variables. If <code>replace=TRUE</code> (the default), all of the existing values will be overwritten. If <code>replace=FALSE</code>, only <code>NA</code> values will be replaced. See the &lsquo;Value&rsquo; section for more details.</p>
</td></tr>
<tr><td><code id="escalc_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. Note that the values are stored without rounding in the returned object. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="escalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Before a meta-analysis can be conducted, the relevant results from each study must be quantified in such a way that the resulting values can be further aggregated and compared. Depending on (a) the goals of the meta-analysis, (b) the design and types of studies included, and (c) the information provided therein, one of the various effect sizes or outcome measures described below may be appropriate for the meta-analysis and can be computed with the <code>escalc</code> function.
</p>
<p>The <code>measure</code> argument is a character string to specify the outcome measure that should be calculated (see below for the various options), arguments <code>ai</code> through <code>ni</code> are then used to specify the information needed to calculate the various measures (depending on the chosen outcome measure, different arguments need to be specified), and <code>data</code> can be used to specify a data frame containing the variables given to the previous arguments. The <code>add</code>, <code>to</code>, and <code>drop00</code> arguments may be needed when dealing with frequency or count data that may need special handling when some of the frequencies or counts are equal to zero (see below for details). Finally, the <code>vtype</code> argument is used to specify how the sampling variances should be estimated (again, see below for details).
</p>
<p>To provide a structure to the various effect sizes or outcome measures that can be calculated with the <code>escalc</code> function, we can distinguish between measures that are used to:
</p>

<table>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (1) </td><td style="text-align: left;"> contrast two independent (either experimentally created or naturally occurring) groups, </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (2) </td><td style="text-align: left;"> describe the direction and strength of the association between two variables, </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (3) </td><td style="text-align: left;"> summarize some characteristic or attribute of individual groups, or </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (4) </td><td style="text-align: left;"> quantify change within a single group or the difference between two matched/paired samples.</td>
</tr>

</table>

<p>Furthermore, where appropriate, we can further distinguish between measures that are applicable when the characteristic, response, or dependent variable assessed within the individual studies is:
</p>

<table>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (a) </td><td style="text-align: left;"> a quantitative variable (e.g., amount of depression as assessed by a rating scale), </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (b) </td><td style="text-align: left;"> a dichotomous (binary) variable (e.g., remission versus no remission), </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (c) </td><td style="text-align: left;"> a count of events per time unit (e.g., number of migraines per year), or </td>
</tr>
<tr>
 <td style="text-align: left;">
   &emsp; </td><td style="text-align: left;"> (d) </td><td style="text-align: left;"> a mix of the types above.</td>
</tr>

</table>

<p>Below, these number and letter codes are used (also in combination) to make it easier to quickly find a measure suitable for a particular meta-analysis (e.g., search for <code>(1b)</code> to find measures that describe the difference between two groups with respect to a dichotomous variable or <code>(2a)</code> for measures that quantify the association between two quantitative variables).
</p>


<h4>(1) Outcome Measures for Two-Group Comparisons</h4>

<p>In many meta-analyses, the goal is to synthesize the results from studies that compare or contrast two groups. The groups may be experimentally defined (e.g., a treatment and a control group created via random assignment) or may occur naturally (e.g., men and women, employees working under high- versus low-stress conditions, people/animals/plants exposed to some environmental risk factor versus those not exposed).
</p>


<h5>(1a) Measures for Quantitative Variables</h5>

<p>When the response or dependent variable assessed within the individual studies is measured on a quantitative scale, it is customary to report certain summary statistics, such as the mean and standard deviation of the observations within the two groups (in case medians, min/max values, and quartiles are reported, see <code><a href="#topic+conv.fivenum">conv.fivenum</a></code> for a function that can be used to estimate means and standard deviations from such statistics). The data layout for a study comparing two groups with respect to such a variable is then of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
                 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> mean       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> standard deviation </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> group size </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>m1i</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>sd1i</code>        </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>m2i</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>sd2i</code>        </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>m1i</code> and <code>m2i</code> are the observed means of the two groups, <code>sd1i</code> and <code>sd2i</code> are the observed standard deviations, and <code>n1i</code> and <code>n2i</code> denote the number of individuals in each group. The raw mean difference, the standardized mean difference, and the (log transformed) ratio of means (also called the log &lsquo;response ratio&rsquo;) are useful outcome measures when meta-analyzing studies of this type.
</p>
<p>The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"MD"</code> for the <em>raw mean difference</em> (e.g., Borenstein, 2009),
</p>
</li>
<li> <p><code>"SMD"</code> for the <em>standardized mean difference</em> (Hedges, 1981),
</p>
</li>
<li> <p><code>"SMDH"</code> for the <em>standardized mean difference</em> with heteroscedastic population variances in the two groups (Bonett, 2008, 2009),
</p>
</li>
<li> <p><code>"SMD1"</code> for the <em>standardized mean difference</em> where the mean difference is divided by the standard deviation of the second group (and <code>"SMD1H"</code> for the same but with heteroscedastic population variances),
</p>
</li>
<li> <p><code>"ROM"</code> for the <em>log transformed ratio of means</em> (Hedges et al., 1999; Lajeunesse, 2011).
</p>
</li></ul>

<p>The raw mean difference is simply \((\textrm{m1i}-\textrm{m2i})\), while the standardized mean difference is given by \((\textrm{m1i}-\textrm{m2i})/\textrm{sdi}\). For <code>measure="SMD"</code>, \(\textrm{sdi} = \sqrt{\frac{(\textrm{n1i}-1)\textrm{sd1i}^2 + (\textrm{n2i}-1)\textrm{sd2i}^2}{\textrm{n1i}+\textrm{n2i}-2}}\) is the pooled standard deviation of the two groups (assuming homoscedasticity of the population variances). For <code>measure="SMDH"</code>, \(\textrm{sdi} = \sqrt{\frac{\textrm{sd1i}^2 + \textrm{sd2i}^2}{2}}\) is the square root of the average variance (allowing for heteroscedastic population variances). Finally, for <code>measure="SMD1"</code> and <code>measure="SMD1H"</code>, \(\textrm{sdi} = \textrm{sd2i}\) (note: for <code>measure="SMD1"</code>, only <code>sd2i</code> needs to be specified and <code>sd1i</code> is ignored).
</p>
<p>For <code>measure="SMD"</code>, the positive bias in the standardized mean difference (i.e., in a Cohen's d value) is automatically corrected for within the function, yielding Hedges' g (Hedges, 1981). Similarly, the analogous bias correction is applied for <code>measure="SMDH"</code> (Bonett, 2009), <code>measure="SMD1"</code> (Hedges, 1981), and <code>measure="SMD1H"</code>.
</p>
<p>For <code>measure="ROM"</code>, the log is taken of the ratio of means (i.e., \(\log(\textrm{m1i}/\textrm{m2i})\)), which makes this outcome measure symmetric around 0 and results in a sampling distribution that is closer to normality. Hence, this measure cannot be computed when <code>m1i</code> and <code>m2i</code> have opposite signs (in fact, this measure is only meant to be used for ratio scale measurements, where both means should be positive anyway).
</p>
<p>For <code>measure="SMD"</code>, if the means and standard deviations are unknown for some studies, but the standardized mean differences (Cohen's d values) are directly available (e.g., if they are reported in those studies), then these can be specified via argument <code>di</code>. Also, if the t-statistics from an independent samples (Student's) t-test are available for some studies, one can specify those values via argument <code>ti</code>, which are then transformed into the corresponding standardized mean differences within the function (the sign of the t-statistics is then taken to be the sign of the standardized mean differences). If only the (two-sided) p-values corresponding to the t-tests are known, one can specify those values via argument <code>pi</code> (which are then transformed into the t-statistics and then further into the standardized mean differences). However, since a two-sided p-value does not carry information about the sign of the test statistic (and hence neither about the standardized mean difference), the sign of the p-values (which can be negative) is used as the sign of the standardized mean differences (e.g., <code>escalc(measure="SMD", pi=-0.018, n1i=20, n2i=20)</code> yields a negative standardized mean difference of <code>-0.7664</code>). See <a href="https://www.metafor-project.org/doku.php/tips:assembling_data_smd">here</a> for a more detailed illustration of using the <code>ti</code> and <code>pi</code> arguments.
</p>
<p>For <code>measure="MD"</code>, one can choose between <code>vtype="LS"</code> (the default) and <code>vtype="HO"</code>. The former computes the sampling variances without assuming homoscedasticity (i.e., that the true variances of the measurements are the same in group 1 and group 2 within each study), while the latter assumes homoscedasticity (equations 12.5 and 12.3 in Borenstein, 2009, respectively). For <code>measure="SMD"</code>, one can choose between <code>vtype="LS"</code> (the default) for the usual large-sample approximation to compute the sampling variances (equation 8 in Hedges, 1982), <code>vtype="LS2"</code> to compute the sampling variances as described in Borenstein (2009; equation 12.17), <code>vtype="UB"</code> to compute unbiased estimates of the sampling variances (equation 9 in Hedges, 1983), and <code>vtype="AV"</code> to compute the sampling variances with the usual large-sample approximation but plugging the sample-size weighted average of the Hedges' g values into the equation. The same choices also apply to <code>measure="SMD1"</code>. For <code>measure="ROM"</code>, one can choose between <code>vtype="LS"</code> (the default) for the usual large-sample approximation to compute the sampling variances (equation 1 in Hedges et al., 1999), <code>vtype="HO"</code> to compute the sampling variances assuming homoscedasticity (the unnumbered equation after equation 1 in Hedges et al., 1999), <code>vtype="AV"</code> to compute the sampling variances assuming homoscedasticity of the coefficient of variation within each group across studies, and <code>vtype="AVHO"</code> to compute the sampling variances assuming homoscedasticity of the coefficient of variation for both groups across studies (see Nakagawa et al., 2023, for details on the latter two options and why they are interesting).
</p>
<p>Datasets corresponding to data of this type are provided in <code><a href="metadat.html#topic+dat.normand1999">dat.normand1999</a></code>, <code><a href="metadat.html#topic+dat.curtis1998">dat.curtis1998</a></code>, and <code><a href="metadat.html#topic+dat.gibson2002">dat.gibson2002</a></code>.
</p>
<p>Interest may also be focused on differences between the two groups with respect to their variability. Here, the (log transformed) ratio of the coefficient of variation of the two groups (also called the coefficient of variation ratio) can be a useful measure (Nakagawa et al., 2015). If focus is solely on the variability of the measurements within the two groups, then the (log transformed) ratio of the standard deviations (also called the variability ratio) can be used (Nakagawa et al., 2015). For the latter, one only needs to specify <code>sd1i</code>, <code>sd2i</code>, <code>n1i</code>, and <code>n2i</code>. The options for the <code>measure</code> argument are:
</p>

<ul>
<li> <p><code>"CVR"</code> for the <em>log transformed coefficient of variation ratio</em>,
</p>
</li>
<li> <p><code>"VR"</code> for the <em>log transformed variability ratio</em>.
</p>
</li></ul>

<p>Measure <code>"CVR"</code> is computed with \(\log\mathopen{}\left(\left(\textrm{sd1i}/\textrm{m1i}\right) \middle/ \left(\textrm{sd2i}/\textrm{m2i}\right) \right)\mathclose{}\), while <code>"VR"</code> is simply \(\log(\textrm{sd1i}/\textrm{sd2i})\), but note that a slight bias correction is applied for both of these measures (Nakagawa et al., 2015). Also, the sampling variance for <code>measure="CVR"</code> is computed as given by equation 12 in Nakagawa et al. (2015), but without the &lsquo;\(-2 \rho \ldots\)&rsquo; terms, since for normally distributed data (which we assume here) the mean and variance (and transformations thereof) are independent.
</p>



<h5>(1b) Measures for Dichotomous Variables</h5>

<p>In various fields of research (such as the health and medical sciences), the response variable measured within the individual studies is often dichotomous (binary), so that the data from a study comparing two different groups can be expressed in terms of a \(2 \times 2\) table, such as:
</p>

<table>
<tr>
 <td style="text-align: left;">
                 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies (i.e., the number of individuals falling into a particular category) and <code>n1i</code> and <code>n2i</code> are the row totals (i.e., the group sizes).
</p>
<p>For example, in a set of randomized clinical trials, group 1 and group 2 may refer to the treatment and placebo/control group, respectively, with outcome 1 denoting some event of interest (e.g., death, complications, failure to improve under the treatment) and outcome 2 its complement. Similarly, in a set of cohort studies, group 1 and group 2 may denote those who engage in and those who do not engage in a potentially harmful behavior (e.g., smoking), with outcome 1 denoting the development of a particular disease (e.g., lung cancer) during the follow-up period. Finally, in a set of case-control studies, group 1 and group 2 may refer to those with the disease (i.e., cases) and those free of the disease (i.e., controls), with outcome 1 denoting, for example, exposure to some environmental risk factor in the past and outcome 2 non-exposure. Note that in all of these examples, the stratified sampling scheme fixes the row totals (i.e., the group sizes) by design.
</p>
<p>A meta-analysis of studies reporting results in terms of \(2 \times 2\) tables can be based on one of several different outcome measures, including the risk ratio (also called the relative risk), the odds ratio, the risk difference, and the arcsine square root transformed risk difference (e.g., Fleiss &amp; Berlin, 2009, R√ºcker et al., 2009). For any of these outcome measures, one needs to specify the cell frequencies via the <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> arguments (or alternatively, one can use the <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code> arguments).
</p>
<p>The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"RR"</code> for the <em>log risk ratio</em>,
</p>
</li>
<li> <p><code>"OR"</code> for the <em>log odds ratio</em>,
</p>
</li>
<li> <p><code>"RD"</code> for the <em>risk difference</em>,
</p>
</li>
<li> <p><code>"AS"</code> for the <em>arcsine square root transformed risk difference</em> (R√ºcker et al., 2009),
</p>
</li>
<li> <p><code>"PETO"</code> for the <em>log odds ratio</em> estimated with Peto's method (Yusuf et al., 1985).
</p>
</li></ul>

<p>Let \(\textrm{p1i} = \textrm{ai}/\textrm{n1i}\) and \(\textrm{p2i} = \textrm{ci}/\textrm{n2i}\) denote the proportion of individuals with outcome 1 in group 1 and group 2, respectively. Then the log risk ratio is computed with \(\log(\textrm{p1i}/\textrm{p2i})\), the log odds ratio with \(\log\mathopen{}\left(\left(\frac{\textrm{p1i}}{1-\textrm{p1i}}\right) \middle/ \left(\frac{\textrm{p2i}}{1-\textrm{p2i}}\right) \right)\mathclose{}\), the risk difference with \(\textrm{p1i}-\textrm{p2i}\), and the arcsine square root transformed risk difference with \(\textrm{asin}(\sqrt{\textrm{p1i}})-\textrm{asin}(\sqrt{\textrm{p2i}})\). See Yusuf et al. (1985) for the computation of the log odds ratio when <code>measure="PETO"</code>. Note that the log is taken of the risk ratio and the odds ratio, which makes these outcome measures symmetric around 0 and results in corresponding sampling distributions that are closer to normality. Also, when multiplied by 2, the arcsine square root transformed risk difference is identical to Cohen's h (Cohen, 1988). For all of these measures, a positive value indicates that the proportion of individuals with outcome 1 is larger in group 1 compared to group 2.
</p>
<p>Cell entries with a zero count can be problematic, especially for the risk ratio and the odds ratio. Adding a small constant to the cells of the \(2 \times 2\) tables is a common solution to this problem. When <code>to="only0"</code> (the default), the value of <code>add</code> (the default is <code>1/2</code>; but see &lsquo;Note&rsquo;) is added to each cell of those \(2 \times 2\) tables with at least one cell equal to 0. When <code>to="all"</code>, the value of <code>add</code> is added to each cell of all \(2 \times 2\) tables. When <code>to="if0all"</code>, the value of <code>add</code> is added to each cell of all \(2 \times 2\) tables, but only when there is at least one \(2 \times 2\) table with a zero cell. Setting <code>to="none"</code> or <code>add=0</code> has the same effect: No adjustment to the observed table frequencies is made. Depending on the outcome measure and the data, this may lead to division by zero (when this occurs, the resulting value is recoded to <code>NA</code>). Also, studies where <code>ai=ci=0</code> or <code>bi=di=0</code> may be considered to be uninformative about the size of the effect and dropping such studies has sometimes been recommended (Higgins et al., 2019). This can be done by setting <code>drop00=TRUE</code>. The values for such studies will then be set to <code>NA</code> (i.e., missing).
</p>
<p>Datasets corresponding to data of this type are provided in <code><a href="metadat.html#topic+dat.bcg">dat.bcg</a></code>, <code><a href="metadat.html#topic+dat.collins1985a">dat.collins1985a</a></code>, <code><a href="metadat.html#topic+dat.collins1985b">dat.collins1985b</a></code>, <code><a href="metadat.html#topic+dat.egger2001">dat.egger2001</a></code>, <code><a href="metadat.html#topic+dat.hine1989">dat.hine1989</a></code>, <code><a href="metadat.html#topic+dat.laopaiboon2015">dat.laopaiboon2015</a></code>, <code><a href="metadat.html#topic+dat.lee2004">dat.lee2004</a></code>, <code><a href="metadat.html#topic+dat.li2007">dat.li2007</a></code>, <code><a href="metadat.html#topic+dat.linde2005">dat.linde2005</a></code>, <code><a href="metadat.html#topic+dat.nielweise2007">dat.nielweise2007</a></code>, and <code><a href="metadat.html#topic+dat.yusuf1985">dat.yusuf1985</a></code>.
</p>
<p>If the \(2 \times 2\) table is not available (or cannot be reconstructed, for example with the <code><a href="#topic+conv.2x2">conv.2x2</a></code> function) for a study, but the odds ratio and the corresponding confidence interval is reported, one can easily transform these values into the corresponding log odds ratio and sampling variance (and combine such a study with those that do report \(2 \times 2\) table data). See the <code><a href="#topic+conv.wald">conv.wald</a></code> function and <a href="https://www.metafor-project.org/doku.php/tips:assembling_data_or">here</a> for an illustration/discussion of this.
</p>



<h5>(1c) Measures for Event Counts</h5>

<p>In medical and epidemiological studies comparing two different groups (e.g., treated versus untreated patients, exposed versus unexposed individuals), results are sometimes reported in terms of event counts (i.e., the number of events, such as strokes or myocardial infarctions) over a certain period of time. Data of this type are also referred to as &lsquo;person-time data&rsquo;. Assume that the studies report data in the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
                 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> number of events </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total person-time </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>x1i</code>       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>t1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>x2i</code>       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>t2i</code></td>
</tr>

</table>

<p>where <code>x1i</code> and <code>x2i</code> denote the number of events in the first and the second group, respectively, and <code>t1i</code> and <code>t2i</code> the corresponding total person-times at risk. Often, the person-time is measured in years, so that <code>t1i</code> and <code>t2i</code> denote the total number of follow-up years in the two groups.
</p>
<p>This form of data is fundamentally different from what was described in the previous section, since the total follow-up time may differ even for groups of the same size and the individuals studied may experience the event of interest multiple times. Hence, different outcome measures than the ones described in the previous section need to be considered when data are reported in this format. These include the incidence rate ratio, the incidence rate difference, and the square root transformed incidence rate difference (Bagos &amp; Nikolopoulos, 2009; Rothman et al., 2008). For any of these outcome measures, one needs to specify the total number of events via the <code>x1i</code> and <code>x2i</code> arguments and the corresponding total person-time values via the <code>t1i</code> and <code>t2i</code> arguments.
</p>
<p>The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"IRR"</code> for the <em>log incidence rate ratio</em>,
</p>
</li>
<li> <p><code>"IRD"</code> for the <em>incidence rate difference</em>,
</p>
</li>
<li> <p><code>"IRSD"</code> for the <em>square root transformed incidence rate difference</em>.
</p>
</li></ul>

<p>Let \(\textrm{ir1i} = \textrm{x1i}/\textrm{t1i}\) and \(\textrm{ir2i} = \textrm{x2i}/\textrm{t2i}\) denote the observed incidence rates in each group. Then the log incidence rate ratio is computed with \(\log(\textrm{ir1i}/\textrm{ir2i})\), the incidence rate difference with \(\textrm{ir1i}-\textrm{ir2i}\), and the square root transformed incidence rate difference with \(\sqrt{\textrm{ir1i}}-\sqrt{\textrm{ir2i}}\). Note that the log is taken of the incidence rate ratio, which makes this outcome measure symmetric around 0 and results in a sampling distribution that is closer to normality.
</p>
<p>Studies with zero events in one or both groups can be problematic, especially for the incidence rate ratio. Adding a small constant to the number of events is a common solution to this problem. When <code>to="only0"</code> (the default), the value of <code>add</code> (the default is <code>1/2</code>; but see &lsquo;Note&rsquo;) is added to <code>x1i</code> and <code>x2i</code> only in the studies that have zero events in one or both groups. When <code>to="all"</code>, the value of <code>add</code> is added to <code>x1i</code> and <code>x2i</code> in all studies. When <code>to="if0all"</code>, the value of <code>add</code> is added to <code>x1i</code> and <code>x2i</code> in all studies, but only when there is at least one study with zero events in one or both groups. Setting <code>to="none"</code> or <code>add=0</code> has the same effect: No adjustment to the observed number of events is made. Depending on the outcome measure and the data, this may lead to division by zero (when this occurs, the resulting value is recoded to <code>NA</code>). Like for \(2 \times 2\) table data, studies where <code>x1i=x2i=0</code> may be considered to be uninformative about the size of the effect and dropping such studies has sometimes been recommended. This can be done by setting <code>drop00=TRUE</code>. The values for such studies will then be set to <code>NA</code>.
</p>
<p>Datasets corresponding to data of this type are provided in <code><a href="metadat.html#topic+dat.hart1999">dat.hart1999</a></code> and <code><a href="metadat.html#topic+dat.nielweise2008">dat.nielweise2008</a></code>.
</p>



<h5>(1d) Transforming SMDs to ORs and Vice-Versa</h5>

<p>In some meta-analyses, one may encounter studies that contrast two groups with respect to a quantitative response variable (case 1a above) and other studies that contrast the same two groups with respect to a dichotomous variable (case 2b above). If both types of studies are to be combined in the same analysis, one needs to compute the same outcome measure across all studies.
</p>
<p>For this, one may need to transform standardized mean differences into log odds ratios (e.g., Cox &amp; Snell, 1989; Chinn, 2000; Hasselblad &amp; Hedges, 1995; S√°nchez-Meca et al., 2003). Here, the data need to be specified as described under (1a) and the options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"D2ORN"</code> for the <em>transformed standardized mean difference</em> assuming normal distributions,
</p>
</li>
<li> <p><code>"D2ORL"</code> for the <em>transformed standardized mean difference</em> assuming logistic distributions.
</p>
</li></ul>

<p>Both of these transformations provide an estimate of the log odds ratio, the first assuming that the responses within the two groups are normally distributed, while the second assumes that the responses follow logistic distributions.
</p>
<p>Alternatively, assuming that the dichotomous outcome in a \(2 \times 2\) table is actually a dichotomized version of the responses on an underlying quantitative scale, it is also possible to estimate the standardized mean difference based on \(2 \times 2\) table data, using either the probit transformed risk difference or a transformation of the odds ratio (e.g., Cox &amp; Snell, 1989; Chinn, 2000; Hasselblad &amp; Hedges, 1995; S√°nchez-Meca et al., 2003). Here, the data need to be specified as described under (1b) and the options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"PBIT"</code> for the <em>probit transformed risk difference</em>,
</p>
</li>
<li> <p><code>"OR2DN"</code> for the <em>transformed odds ratio</em> assuming normal distributions,
</p>
</li>
<li> <p><code>"OR2DL"</code> for the <em>transformed odds ratio</em> assuming logistic distributions.
</p>
</li></ul>

<p>All of these transformations provide an estimate of the standardized mean difference, the first two assuming that the responses on the underlying quantitative scale are normally distributed, while the third assumes that the responses follow logistic distributions.
</p>
<p>A dataset illustrating the combined analysis of standardized mean differences and probit transformed risk differences is provided in <code><a href="metadat.html#topic+dat.gibson2002">dat.gibson2002</a></code>.
</p>




<h4>(2) Outcome Measures for Variable Association</h4>

<p>Meta-analyses are often used to synthesize studies that examine the direction and strength of the association between two variables measured concurrently and/or without manipulation by experimenters. In this section, a variety of outcome measures will be discussed that may be suitable for a meta-analysis with this purpose. We can distinguish between measures that are applicable when both variables are measured on quantitative scales, when both variables measured are dichotomous, and when the two variables are of mixed types.
</p>


<h5>(2a) Measures for Two Quantitative Variables</h5>

<p>The (Pearson or product-moment) correlation coefficient quantifies the direction and strength of the (linear) relationship between two quantitative variables and is therefore frequently used as the outcome measure for meta-analyses. Two alternative measures are a bias-corrected version of the correlation coefficient and Fisher's r-to-z transformed correlation coefficient.
</p>
<p>For these measures, one needs to specify <code>ri</code>, the vector with the raw correlation coefficients, and <code>ni</code>, the corresponding sample sizes. The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"COR"</code> for the <em>raw correlation coefficient</em>,
</p>
</li>
<li> <p><code>"UCOR"</code> for the <em>raw correlation coefficient</em> corrected for its slight negative bias (based on equation 2.3 in Olkin &amp; Pratt, 1958),
</p>
</li>
<li> <p><code>"ZCOR"</code> for <em>Fisher's r-to-z transformed correlation coefficient</em> (Fisher, 1921).
</p>
</li></ul>

<p>If the correlation coefficient is unknown for some studies, but the t-statistics (i.e., \(t_i = r_i \sqrt{n_i - 2} / \sqrt{1 - r_i^2}\)) are available for those studies (for the standard test of \(\mbox{H}_0{:}\; \rho_i = 0\)), one can specify those values via argument <code>ti</code>, which are then transformed into the corresponding correlation coefficients within the function (the sign of the t-statistics is then taken to be the sign of the correlations). If only the (two-sided) p-values corresponding to the t-tests are known, one can specify those values via argument <code>pi</code>. However, since a two-sided p-value does not carry information about the sign of the test statistic (and hence neither about the correlation), the sign of the p-values (which can be negative) is used as the sign of the correlation coefficients (e.g., <code>escalc(measure="COR", pi=-0.07, ni=30)</code> yields a negative correlation of <code>-0.3354</code>).
</p>
<p>For <code>measure="COR"</code> and <code>measure="UCOR"</code>, one can choose between <code>vtype="LS"</code> (the default) for the usual large-sample approximation to compute the sampling variances (i.e., plugging the (biased-corrected) correlation coefficients into equation 12.27 in Borenstein, 2009) and <code>vtype="AV"</code> to compute the sampling variances with the usual large-sample approximation but plugging the sample-size weighted average of the (bias-corrected) correlation coefficients into the equation. For <code>measure="UCOR"</code>, one can also choose <code>vtype="UB"</code> to compute unbiased estimates of the sampling variances (see Hedges, 1989, but using the exact equation instead of the approximation).
</p>
<p>Datasets corresponding to data of this type are provided in <code><a href="metadat.html#topic+dat.mcdaniel1994">dat.mcdaniel1994</a></code> and <code><a href="metadat.html#topic+dat.molloy2014">dat.molloy2014</a></code>.
</p>
<p>For meta-analyses involving multiple (dependent) correlation coefficients extracted from the same sample, see also the <code><a href="#topic+rcalc">rcalc</a></code> function.
</p>



<h5>(2b) Measures for Two Dichotomous Variables</h5>

<p>When the goal of a meta-analysis is to examine the relationship between two dichotomous variables, the data for each study can again be presented in the form of a \(2 \times 2\) table, except that there may not be a clear distinction between the grouping variable and the outcome variable. Moreover, the table may be a result of cross-sectional (i.e., multinomial) sampling, where none of the table margins (except the total sample size) are fixed by the study design.
</p>
<p>In particular, assume that the data of interest for a particular study are of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
                               </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> variable 2, outcome + </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> variable 2, outcome - </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
         variable 1, outcome + </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         variable 1, outcome - </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code>             </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies (i.e., the number of individuals falling into a particular category) and <code>n1i</code> and <code>n2i</code> are the row totals.
</p>
<p>The phi coefficient and the odds ratio are commonly used measures of association for \(2 \times 2\) table data (e.g., Fleiss &amp; Berlin, 2009). The latter is particularly advantageous, as it is directly comparable to values obtained from stratified sampling (as described earlier). Yule's Q and Yule's Y (Yule, 1912) are additional measures of association for \(2 \times 2\) table data (although they are not typically used in meta-analyses). Finally, assuming that the two dichotomous variables are actually dichotomized versions of the responses on two underlying quantitative scales (and assuming that the two variables follow a bivariate normal distribution), it is also possible to estimate the correlation between the two quantitative variables using the tetrachoric correlation coefficient (Pearson, 1900; Kirk, 1973).
</p>
<p>For any of these outcome measures, one needs to specify the cell frequencies via the <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> arguments (or alternatively, one can use the <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code> arguments). The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"OR"</code> for the <em>log odds ratio</em>,
</p>
</li>
<li> <p><code>"PHI"</code> for the <em>phi coefficient</em>,
</p>
</li>
<li> <p><code>"YUQ"</code> for <em>Yule's Q</em> (Yule, 1912),
</p>
</li>
<li> <p><code>"YUY"</code> for <em>Yule's Y</em> (Yule, 1912),
</p>
</li>
<li> <p><code>"RTET"</code> for the <em>tetrachoric correlation coefficient</em>.
</p>
</li></ul>

<p>There are also measures <code>"ZPHI"</code> and <code>"ZTET"</code> for applying Fisher's r-to-z transformation to these measures. This may be useful when combining these with other types of correlation coefficients that were r-to-z transformed. However, note that the r-to-z transformation is <em>not</em> a variance-stabilizing transformation for these measures.
</p>
<p>Tables with one or more zero counts are handled as described earlier. For <code>measure="PHI"</code>, one must indicate via <code>vtype="ST"</code> or <code>vtype="CS"</code> whether the data for the studies were obtained using stratified or cross-sectional (i.e., multinomial) sampling, respectively (it is also possible to specify an entire vector for the <code>vtype</code> argument in case the sampling scheme differed for the various studies).
</p>
<p>A dataset corresponding to data of this type is provided in <code><a href="metadat.html#topic+dat.bourassa1996">dat.bourassa1996</a></code>.
</p>



<h5>(2d) Measures for Mixed Variable Types</h5>

<p>We can also consider outcome measures that can be used to describe the relationship between two variables, where one variable is dichotomous and the other variable measures some quantitative characteristic. In that case, it is likely that study authors again report summary statistics, such as the mean and standard deviation of the measurements within the two groups (defined by the dichotomous variable). Based on this information, one can compute the point-biserial correlation coefficient (Tate, 1954) as a measure of association between the two variables. If the dichotomous variable is actually a dichotomized version of the responses on an underlying quantitative scale (and assuming that the two variables follow a bivariate normal distribution), it is also possible to estimate the correlation between the two variables using the biserial correlation coefficient (Pearson, 1909; Soper, 1914; Jacobs &amp; Viechtbauer, 2017).
</p>
<p>Here, one again needs to specify <code>m1i</code> and <code>m2i</code> for the observed means of the two groups, <code>sd1i</code> and <code>sd2i</code> for the observed standard deviations, and <code>n1i</code> and <code>n2i</code> for the number of individuals in each group. The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"RPB"</code> for the <em>point-biserial correlation coefficient</em>,
</p>
</li>
<li> <p><code>"RBIS"</code> for the <em>biserial correlation coefficient</em>.
</p>
</li></ul>

<p>There are also measures <code>"ZPB"</code> and <code>"ZBIS"</code> for applying Fisher's r-to-z transformation to these measures. This may be useful when combining these with other types of correlation coefficients that were r-to-z transformed. However, note that the r-to-z transformation is <em>not</em> a variance-stabilizing transformation for these measures.
</p>
<p>If the means and standard deviations are unknown for some studies, one can also use arguments <code>di</code>, <code>ti</code>, or <code>pi</code> to specify standardized mean differences (Cohen's d values), t-statistics from an independent samples t-test, or (signed) p-values for the t-test, respectively, as described earlier under (1a) (together with the group sizes, these are sufficient statistics for computing the (point-)biserial correlation coefficients).
</p>
<p>For <code>measure="RPB"</code>, one must indicate via <code>vtype="ST"</code> or <code>vtype="CS"</code> whether the data for the studies were obtained using stratified or cross-sectional (i.e., multinomial) sampling, respectively (it is also possible to specify an entire vector for the <code>vtype</code> argument in case the sampling scheme differed for the various studies).
</p>




<h4>(3) Outcome Measures for Individual Groups</h4>

<p>In this section, outcome measures will be described which may be useful when the goal of a meta-analysis is to synthesize studies that characterize some property of individual groups. We will again distinguish between measures that are applicable when the characteristic assessed is a quantitative variable, a dichotomous variable, or when the characteristic represents an event count.
</p>


<h5>(3a) Measures for Quantitative Variables</h5>

<p>The goal of a meta-analysis may be to characterize individual groups, where the response, characteristic, or dependent variable assessed in the individual studies is measured on some quantitative scale. In the simplest case, the raw mean for the quantitative variable is reported for each group, which then becomes the observed outcome for the meta-analysis. Here, one needs to specify <code>mi</code>, <code>sdi</code>, and <code>ni</code> for the observed means, the observed standard deviations, and the sample sizes, respectively. One can also compute the &lsquo;single-group standardized mean&rsquo;, where the mean is divided by the standard deviation (when first subtracting some fixed constant from each mean, then this is the &lsquo;single-group standardized mean difference&rsquo;). For ratio scale measurements, the log transformed mean or the log transformed coefficient of variation (with bias correction) may also be of interest (Nakagawa et al., 2015). If focus is solely on the variability of the measurements, then the log transformed standard deviation (with bias correction) is a useful measure (Nakagawa et al., 2015; Raudenbush &amp; Bryk, 1987). For the latter, one only needs to specify <code>sdi</code> and <code>ni</code>.
</p>
<p>The options for the <code>measure</code> argument are:
</p>

<ul>
<li> <p><code>"MN"</code> for the <em>raw mean</em>,
</p>
</li>
<li> <p><code>"SMN"</code> for the <em>single-group standardized mean</em>,
</p>
</li>
<li> <p><code>"MNLN"</code> for the <em>log transformed mean</em>,
</p>
</li>
<li> <p><code>"CVLN"</code> for the <em>log transformed coefficient of variation</em>,
</p>
</li>
<li> <p><code>"SDLN"</code> for the <em>log transformed standard deviation</em>.
</p>
</li></ul>

<p>Note that <code>sdi</code> is used to specify the standard deviations of the observed values of the response, characteristic, or dependent variable and not the standard errors of the means. Also, the sampling variance for <code>measure="CVLN"</code> is computed as given by equation 27 in Nakagawa et al. (2015), but without the &lsquo;\(-2 \rho \ldots\)&rsquo; term, since for normally distributed data (which we assume here) the mean and variance (and transformations thereof) are independent.
</p>



<h5>(3b) Measures for Dichotomous Variables</h5>

<p>A meta-analysis may also be conducted to aggregate studies that provide data about individual groups with respect to a dichotomous dependent variable. Here, one needs to specify <code>xi</code> and <code>ni</code>, denoting the number of individuals experiencing the event of interest and the total number of individuals within each study, respectively. Instead of specifying <code>ni</code>, one can use <code>mi</code> to specify the number of individuals that do not experience the event of interest (i.e., <code>mi=ni-xi</code>). The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"PR"</code> for the <em>raw proportion</em>,
</p>
</li>
<li> <p><code>"PLN"</code> for the <em>log transformed proportion</em>,
</p>
</li>
<li> <p><code>"PLO"</code> for the <em>logit transformed proportion</em> (i.e., log odds),
</p>
</li>
<li> <p><code>"PAS"</code> for the <em>arcsine square root transformed proportion</em> (i.e., the angular transformation),
</p>
</li>
<li> <p><code>"PFT"</code> for the <em>Freeman-Tukey double arcsine transformed proportion</em> (Freeman &amp; Tukey, 1950).
</p>
</li></ul>

<p>Zero cell entries can be problematic for certain outcome measures. When <code>to="only0"</code> (the default), the value of <code>add</code> (the default is <code>1/2</code>; but see &lsquo;Note&rsquo;) is added to <code>xi</code> and <code>mi</code> only for studies where <code>xi</code> or <code>mi</code> is equal to 0. When <code>to="all"</code>, the value of <code>add</code> is added to <code>xi</code> and <code>mi</code> in all studies. When <code>to="if0all"</code>, the value of <code>add</code> is added in all studies, but only when there is at least one study with a zero value for <code>xi</code> or <code>mi</code>. Setting <code>to="none"</code> or <code>add=0</code> has the same effect: No adjustment to the observed values is made. Depending on the outcome measure and the data, this may lead to division by zero (when this occurs, the resulting value is recoded to <code>NA</code>).
</p>
<p>Datasets corresponding to data of this type are provided in <code><a href="metadat.html#topic+dat.pritz1997">dat.pritz1997</a></code> and <code><a href="metadat.html#topic+dat.debruin2009">dat.debruin2009</a></code>.
</p>



<h5>(3c) Measures for Event Counts</h5>

<p>Various measures can be used to characterize individual groups when the dependent variable assessed is an event count. Here, one needs to specify <code>xi</code> and <code>ti</code>, denoting the number of events that occurred and the total person-times at risk, respectively. The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"IR"</code> for the <em>raw incidence rate</em>,
</p>
</li>
<li> <p><code>"IRLN"</code> for the <em>log transformed incidence rate</em>,
</p>
</li>
<li> <p><code>"IRS"</code> for the <em>square root transformed incidence rate</em>,
</p>
</li>
<li> <p><code>"IRFT"</code> for the <em>Freeman-Tukey transformed incidence rate</em> (Freeman &amp; Tukey, 1950).
</p>
</li></ul>

<p>Measures <code>"IR"</code> and <code>"IRLN"</code> can also be used when meta-analyzing standardized incidence ratios (SIRs), where the observed number of events is divided by the expected number of events. In this case, arguments <code>xi</code> and <code>ti</code> are used to specify the observed and expected number of events in the studies. Since SIRs are not symmetric around 1, it is usually more appropriate to meta-analyze the log transformed SIRs (i.e., using measure <code>"IRLN"</code>), which are symmetric around 0.
</p>
<p>Studies with zero events can be problematic, especially for the log transformed incidence rate. Adding a small constant to the number of events is a common solution to this problem. When <code>to="only0"</code> (the default), the value of <code>add</code> (the default is <code>1/2</code>; but see &lsquo;Note&rsquo;) is added to <code>xi</code> only in the studies that have zero events. When <code>to="all"</code>, the value of <code>add</code> is added to <code>xi</code> in all studies. When <code>to="if0all"</code>, the value of <code>add</code> is added to <code>xi</code> in all studies, but only when there is at least one study with zero events. Setting <code>to="none"</code> or <code>add=0</code> has the same effect: No adjustment to the observed number of events is made. Depending on the outcome measure and the data, this may lead to division by zero (when this occurs, the resulting value is recoded to <code>NA</code>).
</p>




<h4>(4) Outcome Measures for Change or Matched Pairs</h4>

<p>The purpose of a meta-analysis may be to assess the amount of change within individual groups (e.g., before and after a treatment or under two different treatments) or when dealing with matched pairs designs.
</p>


<h5>(4a) Measures for Quantitative Variables</h5>

<p>When the response or dependent variable assessed in the individual studies is measured on some quantitative scale, the raw mean change, standardized versions thereof, or the (log transformed) ratio of means (log response ratio) can be used as outcome measures (Becker, 1988; Gibbons et al., 1993; Lajeunesse, 2011; Morris, 2000). Here, one needs to specify <code>m1i</code> and <code>m2i</code>, the observed means at the two measurement occasions, <code>sd1i</code> and <code>sd2i</code> for the corresponding observed standard deviations, <code>ri</code> for the correlation between the measurements at the two measurement occasions, and <code>ni</code> for the sample size. The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"MC"</code> for the <em>raw mean change</em>,
</p>
</li>
<li> <p><code>"SMCC"</code> for the <em>standardized mean change</em> using change score standardization (Gibbons et al., 1993),
</p>
</li>
<li> <p><code>"SMCR"</code> for the <em>standardized mean change</em> using raw score standardization (Becker, 1988),
</p>
</li>
<li> <p><code>"SMCRH"</code> for the <em>standardized mean change</em> using raw score standardization with heteroscedastic population variances at the two measurement occasions (Bonett, 2008),
</p>
</li>
<li> <p><code>"SMCRP"</code> for the <em>standardized mean change</em> using raw score standardization with pooled standard deviations (Cousineau, 2020),
</p>
</li>
<li> <p><code>"SMCRPH"</code> for the <em>standardized mean change</em> using raw score standardization with pooled standard deviations and heteroscedastic population variances at the two measurement occasions (Bonett, 2008),
</p>
</li>
<li> <p><code>"ROMC"</code> for the <em>log transformed ratio of means</em> (Lajeunesse, 2011).
</p>
</li></ul>

<p>The raw mean change is simply \(\textrm{m1i}-\textrm{m2i}\), while the standardized mean change is given by \((\textrm{m1i}-\textrm{m2i})/\textrm{sdi}\). For <code>measure="SMCC"</code>, \(\textrm{sdi} = \sqrt{\textrm{sd1i}^2 + \textrm{sd2i}^2 - 2\times\textrm{ri}\times\textrm{sd1i}\times\textrm{sd2i}}\) is the standard deviation of the change scores, for <code>measure="SMCR"</code> and <code>measure="SMCRH"</code>, \(\textrm{sdi} = \textrm{sd1i}\), and for <code>measure="SMCRP"</code> and <code>measure="SMCRPH"</code>, \(\textrm{sdi} = \sqrt{\frac{\textrm{sd1i}^2 + \textrm{sd2i}^2}{2}}\) is the square root of the average variance. See also Morris and DeShon (2002) for a thorough discussion of the difference between the <code>"SMCC"</code> and <code>"SMCR"</code> change score measures. All of these measures are also applicable for matched pairs designs (subscripts 1 and 2 then simply denote the first and second group that are formed by the matching).
</p>
<p>In practice, one often has a mix of information available from the individual studies to compute these measures. In particular, if <code>m1i</code> and <code>m2i</code> are unknown, but the raw mean change is directly reported in a particular study, then one can set <code>m1i</code> to that value and <code>m2i</code> to 0 (making sure that the raw mean change was computed as <code>m1i-m2i</code> within that study and not the other way around). Also, for measures <code>"MC"</code> and <code>"SMCC"</code>, if <code>sd1i</code>, <code>sd2i</code>, and <code>ri</code> are unknown, but the standard deviation of the change scores is directly reported, then one can set <code>sd1i</code> to that value and both <code>sd2i</code> and <code>ri</code> to 0. For measure <code>"SMCR"</code>, argument <code>sd2i</code> is actually not needed, as the standardization is only based on <code>sd1i</code> (Becker, 1988; Morris, 2000), which is usually the pre-test standard deviation (if the post-test standard deviation should be used, then set <code>sd1i</code> to that). Finally, for <code>measure="SMCC"</code>, one can also directly specify standardized mean change values via argument <code>di</code> or the t-statistics from a paired samples t-test or the corresponding (two-sided) p-values via argument <code>ti</code> or <code>pi</code>, respectively (which are then transformed into the corresponding standardized mean change values within the function). The sign of the p-values (which can be negative) is used as the sign of the standardized mean change values (e.g., <code>escalc(measure="SMCC", pi=-0.018, ni=50)</code> yields a negative standardized mean change value of <code>-0.3408</code>).
</p>
<p>Finally, interest may also be focused on differences in the variability of the measurements at the two measurement occasions (or between the two matched groups). Here, the (log transformed) ratio of the coefficient of variation (also called the coefficient of variation ratio) can be a useful measure (Nakagawa et al., 2015). If focus is solely on the variability of the measurements, then the (log transformed) ratio of the standard deviations (also called the variability ratio) can be used (Nakagawa et al., 2015). For the latter, one only needs to specify <code>sd1i</code>, <code>sd2i</code>, <code>ni</code>, and <code>ri</code>. The options for the <code>measure</code> argument are:
</p>

<ul>
<li> <p><code>"CVRC"</code> for the <em>log transformed coefficient of variation ratio</em>,
</p>
</li>
<li> <p><code>"VRC"</code> for the <em>log transformed variability ratio</em>.
</p>
</li></ul>

<p>The definitions of these measures are the same as given in Nakagawa et al. (2015) but are here computed for two sets of dependent measurements. Hence, the computation of the sampling variances are adjusted to take the correlation between the measurements into consideration.
</p>



<h5>(4b) Measures for Dichotomous Variables</h5>

<p>The data for a study examining change in a dichotomous variable gives rise to a paired \(2 \times 2\) table, which is of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
                         &emsp; </td><td style="text-align: center;"> </td><td style="text-align: center;"> trt 2 outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> trt 2 outcome 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
         trt 1 outcome 1 &emsp; </td><td style="text-align: center;"> </td><td style="text-align: center;"> <code>ai</code>       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         trt 1 outcome 2 &emsp; </td><td style="text-align: center;"> </td><td style="text-align: center;"> <code>ci</code>       </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies. Note that &lsquo;trt1&rsquo; and &lsquo;trt2&rsquo; may be applied to a single group of subjects or to matched pairs of subjects. Also, &lsquo;trt1&rsquo; and &lsquo;trt2&rsquo; might refer to two different time points (e.g., before and after a treatment). In any case, the data from such a study can be rearranged into a marginal table of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
               </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1    </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
         trt 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai+bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci+di</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
         trt 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai+ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi+di</code></td>
</tr>

</table>

<p>which is of the same form as a \(2 \times 2\) table that would arise in a study comparing/contrasting two independent groups.
</p>
<p>The options for the <code>measure</code> argument that will compute outcome measures based on the marginal table are:
</p>

<ul>
<li> <p><code>"MPRR"</code> for the matched pairs <em>marginal log risk ratio</em>,
</p>
</li>
<li> <p><code>"MPOR"</code> for the matched pairs <em>marginal log odds ratio</em>,
</p>
</li>
<li> <p><code>"MPRD"</code> for the matched pairs <em>marginal risk difference</em>.
</p>
</li></ul>

<p>See Becker and Balagtas (1993), Curtin et al. (2002), Elbourne et al. (2002), Fagerland et al. (2014), May and Johnson (1997), Newcombe (1998), Stedman et al. (2011), and Zou (2007) for discussions of these measures.
</p>
<p>The options for the <code>measure</code> argument that will compute outcome measures based on the paired table are:
</p>

<ul>
<li> <p><code>"MPORC"</code> for the <em>conditional log odds ratio</em>,
</p>
</li>
<li> <p><code>"MPPETO"</code> for the <em>conditional log odds ratio</em> estimated with Peto's method.
</p>
</li></ul>

<p>See Curtin et al. (2002) and Zou (2007) for discussions of these measures.
</p>
<p>If only marginal tables are available, then another possibility is to compute the marginal log odds ratios based on these table directly. However, for the correct computation of the sampling variances, the correlations (phi coefficients) from the paired tables must be known (or &lsquo;guestimated&rsquo;). To use this approach, set <code>measure="MPORM"</code> and use argument <code>ri</code> to specify the correlation coefficients. Instead of specifying <code>ri</code>, one can use argument <code>pi</code> to specify the proportions (or &lsquo;guestimates&rsquo; thereof) of individuals (or pairs) that experienced the outcome of interest (i.e., &lsquo;outcome1&rsquo; in the paired \(2 \times 2\) table) under both treatments (i.e., <code>pi=ai/(ai+bi+ci+di)</code>). Based on these proportions, the correlation coefficients are then back-calculated and used to calculate the correct sampling variances. Note that the values in the marginal tables put constraints on the possible values for <code>ri</code> and <code>pi</code>. If a specified value for <code>ri</code> or <code>pi</code> is not feasible under a given table, the corresponding sampling variance will be <code>NA</code>.
</p>




<h4>(5) Other Outcome Measures for Meta-Analyses</h4>

<p>Other outcome measures are sometimes used for meta-analyses that do not directly fall into the categories above. These are described in this section.
</p>


<h5>Cronbach's alpha and Transformations Thereof</h5>

<p>Meta-analytic methods can also be used to aggregate Cronbach's alpha values from multiple studies. This is usually referred to as a &lsquo;reliability generalization meta-analysis&rsquo; (Vacha-Haase, 1998). Here, one needs to specify <code>ai</code>, <code>mi</code>, and <code>ni</code> for the observed alpha values, the number of items/replications/parts of the measurement instrument, and the sample sizes, respectively. One can either directly analyze the raw Cronbach's alpha values or transformations thereof (Bonett, 2002, 2010; Hakstian &amp; Whalen, 1976). The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"ARAW"</code> for <em>raw alpha</em> values,
</p>
</li>
<li> <p><code>"AHW"</code> for <em>transformed alpha values</em> (Hakstian &amp; Whalen, 1976),
</p>
</li>
<li> <p><code>"ABT"</code> for <em>transformed alpha values</em> (Bonett, 2002).
</p>
</li></ul>

<p>Note that the transformations implemented here are slightly different from the ones described by Hakstian and Whalen (1976) and Bonett (2002). In particular, for <code>"AHW"</code>, the transformation \(1-(1-\textrm{ai})^{1/3}\) is used, while for <code>"ABT"</code>, the transformation \(-\log(1-\textrm{ai})\) is used. This ensures that the transformed values are monotonically increasing functions of \(\textrm{ai}\).
</p>
<p>A dataset corresponding to data of this type is provided in <code><a href="metadat.html#topic+dat.bonett2010">dat.bonett2010</a></code>.
</p>



<h5>Partial and Semi-Partial Correlations</h5>

<p>Aloe and Becker (2012), Aloe and Thompson (2013), and Aloe (2014) describe the use of partial and semi-partial correlation coefficients for meta-analyzing the results from regression models (when the focus is on a common regression coefficient of interest across studies). To compute these measures, one needs to specify <code>ti</code> for the test statistics (i.e., t-tests) of the &lsquo;focal&rsquo; regression coefficient of interest, <code>ni</code> for the sample sizes of the studies, <code>mi</code> for the total number of predictors in the regression models (counting the focal predictor of interest), and <code>r2i</code> for the \(R^2\) values of the regression models (the latter is only needed when <code>measure="SPCOR"</code> or <code>measure="ZSPCOR"</code>). The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"PCOR"</code> for the <em>partial correlation coefficient</em>,
</p>
</li>
<li> <p><code>"ZPCOR"</code> for <em>Fisher's r-to-z transformed partial correlation coefficient</em>,
</p>
</li>
<li> <p><code>"SPCOR"</code> for the <em>semi-partial correlation coefficient</em>,
</p>
</li>
<li> <p><code>"ZSPCOR"</code> for <em>Fisher's r-to-z transformed semi-partial correlation coefficient</em>.
</p>
</li></ul>

<p>Note that the signs of the (semi-)partial correlation coefficients is determined based on the signs of the values specified via the <code>ti</code> argument. Also, while the Fisher transformation can be applied to both measures, it is only a variance-stabilizing transformation for partial correlation coefficients.
</p>
<p>If the test statistic (i.e., t-test) of the regression coefficient of interest is unknown for some studies, but the (two-sided) p-values corresponding to the t-tests are known, one can specify those values via argument <code>pi</code>. However, since a two-sided p-value does not carry information about the sign of the test statistic (and hence neither about the correlation), the sign of the p-values (which can be negative) is used as the sign of the correlation coefficients (e.g., <code>escalc(measure="PCOR", pi=-0.07, mi=5, ni=30)</code> yields a negative partial correlation of <code>-0.3610</code>).
</p>
<p>In the rare case that the (semi-)partial correlations are known for some of the studies, then these can be directly specified via the <code>ri</code> argument. This can be useful, for example, when \(\eta^2_p\) (i.e., partial eta squared) is known for the regression coefficient of interest, since the square root thereof is identical to the absolute value of the partial correlation (although the correct sign then still needs to be reconstructed based on other information).
</p>
<p>A dataset corresponding to data of this type is provided in <code><a href="metadat.html#topic+dat.aloe2013">dat.aloe2013</a></code>.
</p>



<h5>Coefficients of Determination</h5>

<p>One can in principle also meta-analyze coefficients of determination (i.e., \(R^2\) values / R-squared values) obtained from a series of linear regression models (however, see the caveats mentioned below). For this, one needs to specify <code>r2i</code> for the \(R^2\) values of the regression models, <code>ni</code> for the sample sizes of the studies, and <code>mi</code> for the number of predictors in the regression models. The options for the <code>measure</code> argument are then:
</p>

<ul>
<li> <p><code>"R2"</code> for the <em>raw coefficient of determination</em>,
</p>
</li>
<li> <p><code>"ZR2"</code> for the <em>r-to-z transformed coefficient of determination</em>.
</p>
</li></ul>

<p>If the \(R^2\) values are unknown for some studies, but the F-statistics (for the omnibus test of the regression coefficients) are available, one can specify those values via argument <code>fi</code>, which are then transformed into the corresponding \(R^2\) values within the function. If only the p-values corresponding to the F-tests are known, one can specify those values via argument <code>pi</code> (which are then transformed into the F-statistics and then further into the \(R^2\) values).
</p>
<p>For <code>measure="R2"</code>, one can choose to compute the sampling variances with <code>vtype="LS"</code> (the default) for the large-sample approximation given by equation 27.88 in Kendall and Stuart (1979), <code>vtype="LS2"</code> for the large-sample approximation given by equation 27.87, or <code>vtype="AV"</code> and <code>vtype="AV2"</code> which use the same approximations but plugging the sample-size weighted average of the \(R^2\) values into the equations. For <code>measure="ZR2"</code>, the variance-stabilizing transformation \(\frac{1}{2} \log\mathopen{}\left(\frac{1+\sqrt{\textrm{r2i}}}{1-\sqrt{\textrm{r2i}}}\right)\mathclose{}\) is used (see Olkin &amp; Finn, 1995, but with the additional \(\frac{1}{2}\) factor), which uses \(1/\textrm{ni}\) as the large-sample approximation to the sampling variances.
</p>
<p>The equations used for these measures were derived under the assumption that the values of the outcome variable and the predictors were sampled from a multivariate normal distribution within each study and that the sample sizes of the studies are large. Moreover, the equations assume that the true \(R^2\) values are all non-zero. Similarly, given that observed \(R^2\) values cannot be negative, there is no possibility for values to cancel each other out and hence it is guaranteed that the pooled estimate is positive. Hence, a meta-analysis of \(R^2\) values cannot be used to test if the pooled estimate is different from zero (it is by construction as long as the number of studies is sufficiently large).
</p>



<h5>Relative Excess Heterozygosity</h5>

<p>Ziegler et al. (2011) describe the use of meta-analytic methods to examine deviations from the Hardy-Weinberg equilibrium across multiple studies. The relative excess heterozygosity (REH) is the proposed measure for such a meta-analysis, which can be computed by setting <code>measure="REH"</code>. Here, one needs to specify <code>ai</code> for the number of individuals with homozygous dominant alleles, <code>bi</code> for the number of individuals with heterozygous alleles, and <code>ci</code> for the number of individuals with homozygous recessives alleles.
</p>
<p>Note that the log is taken of the REH values, which makes this outcome measure symmetric around 0 and results in a sampling distribution that is closer to normality.
</p>
<p>A dataset corresponding to data of this type is provided in <code><a href="metadat.html#topic+dat.frank2008">dat.frank2008</a></code>.
</p>




<h4>(6) Converting a Data Frame to an 'escalc' Object</h4>

<p>The function can also be used to convert a regular data frame to an &lsquo;escalc&rsquo; object. One simply sets the <code>measure</code> argument to one of the options described above (or to <code>measure="GEN"</code> for a generic outcome measure not further specified) and passes the observed effect sizes or outcomes via the <code>yi</code> argument and the corresponding sampling variances via the <code>vi</code> argument (or the standard errors via the <code>sei</code> argument) to the function.
</p>



<h3>Value</h3>

<p>An object of class <code>c("escalc","data.frame")</code>. The object is a data frame containing the following components:
</p>
<table>
<tr><td><code>yi</code></td>
<td>
<p>vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code>vi</code></td>
<td>
<p>vector with the corresponding sampling variances.</p>
</td></tr>
</table>
<p>If a data frame was specified via the <code>data</code> argument and <code>append=TRUE</code>, then variables <code>yi</code> and <code>vi</code> are appended to this data frame. Note that the <code>var.names</code> argument actually specifies the names of these two variables (<code>"yi"</code> and <code>"vi"</code> are the defaults).
</p>
<p>If the data frame already contains two variables with names as specified by the <code>var.names</code> argument, the values for these two variables will be overwritten when <code>replace=TRUE</code> (which is the default). By setting <code>replace=FALSE</code>, only values that are <code>NA</code> will be replaced.
</p>
<p>The <code>subset</code> argument can be used to select the studies that will be included in the data frame returned by the function. On the other hand, the <code>include</code> argument simply selects for which studies the measure will be computed (if it shouldn't be computed for all of them).
</p>
<p>The object is formatted and printed with the <code><a href="#topic+print.escalc">print</a></code> function. The <code><a href="#topic+summary.escalc">summary</a></code> function can be used to obtain confidence intervals for the individual outcomes. See <code><a href="#topic+methods.escalc">methods.escalc</a></code> for some additional method functions for <code>"escalc"</code> objects.
</p>
<p>With the <code><a href="#topic+aggregate.escalc">aggregate</a></code> function, one can aggregate multiple effect sizes or outcomes belonging to the same study (or some other clustering variable) into a single combined effect size or outcome.
</p>


<h3>Note</h3>

<p>The variable names specified under <code>var.names</code> should be syntactically valid variable names. If necessary, they are adjusted so that they are.
</p>
<p>Although the default value for <code>add</code> is <code>1/2</code>, for certain measures the use of such a bias correction makes little sense and for these measures, the function internally sets <code>add=0</code>. This applies to the following measures: <code>"AS"</code>, <code>"PHI"</code>, <code>"ZPHI"</code>, <code>"RTET"</code>, <code>"ZTET"</code>, <code>"IRSD"</code>, <code>"PAS"</code>, <code>"PFT"</code>, <code>"IRS"</code>, and <code>"IRFT"</code>. One can still force the use of the bias correction by explicitly setting the <code>add</code> argument to some non-zero value when calling the function.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Aloe, A. M. (2014). An empirical investigation of partial effect sizes in meta-analysis of correlational data. <em>Journal of General Psychology</em>, <b>141</b>(1), 47&ndash;64. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/00221309.2013.853021&#8288;</code>
</p>
<p>Aloe, A. M., &amp; Becker, B. J. (2012). An effect size for regression predictors in meta-analysis. <em>Journal of Educational and Behavioral Statistics</em>, <b>37</b>(2), 278&ndash;297. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/1076998610396901&#8288;</code>
</p>
<p>Aloe, A. M., &amp; Thompson, C. G. (2013). The synthesis of partial effect sizes. <em>Journal of the Society for Social Work and Research</em>, <b>4</b>(4), 390&ndash;405. <code style="white-space: pre;">&#8288;https://doi.org/10.5243/jsswr.2013.24&#8288;</code>
</p>
<p>Bagos, P. G., &amp; Nikolopoulos, G. K. (2009). Mixed-effects Poisson regression models for meta-analysis of follow-up studies with constant or varying durations. <em>The International Journal of Biostatistics</em>, <b>5</b>(1). <code style="white-space: pre;">&#8288;https://doi.org/10.2202/1557-4679.1168&#8288;</code>
</p>
<p>Becker, B. J. (1988). Synthesizing standardized mean-change measures. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>41</b>(2), 257&ndash;278.  <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.2044-8317.1988.tb00901.x&#8288;</code>
</p>
<p>Becker, M. P., &amp; Balagtas, C. C. (1993). Marginal modeling of binary cross-over data. <em>Biometrics</em>, <b>49</b>(4), 997&ndash;1009. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2532242&#8288;</code>
</p>
<p>Bonett, D. G. (2002). Sample size requirements for testing and estimating coefficient alpha. <em>Journal of Educational and Behavioral Statistics</em>, <b>27</b>(4), 335&ndash;340. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986027004335&#8288;</code>
</p>
<p>Bonett, D. G. (2008). Confidence intervals for standardized linear contrasts of means. <em>Psychological Methods</em>, <b>13</b>(2), 99&ndash;109. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.13.2.99&#8288;</code>
</p>
<p>Bonett, D. G. (2009). Meta-analytic interval estimation for standardized and unstandardized mean differences. <em>Psychological Methods</em>, <b>14</b>(3), 225&ndash;238. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/a0016619&#8288;</code>
</p>
<p>Bonett, D. G. (2010). Varying coefficient meta-analytic methods for alpha reliability. <em>Psychological Methods</em>, <b>15</b>(4), 368&ndash;385. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/a0020142&#8288;</code>
</p>
<p>Borenstein, M. (2009). Effect sizes for continuous data. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 221&ndash;235). New York: Russell Sage Foundation.
</p>
<p>Chinn, S. (2000). A simple method for converting an odds ratio to effect size for use in meta-analysis. <em>Statistics in Medicine</em>, <b>19</b>(22), 3127&ndash;3131. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/1097-0258(20001130)19:22&lt;3127::aid-sim784&gt;3.0.co;2-m&#8288;</code>
</p>
<p>Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum Associates.
</p>
<p>Cousineau, D. (2020). Approximating the distribution of Cohen's d_p in within-subject designs. <em>The Quantitative Methods for Psychology</em>, <b>16</b>(4), 418&ndash;421. <code style="white-space: pre;">&#8288;https://doi.org/10.20982/tqmp.16.4.p418&#8288;</code>
</p>
<p>Cox, D. R., &amp; Snell, E. J. (1989). <em>Analysis of binary data</em> (2nd ed.). London: Chapman &amp; Hall.
</p>
<p>Curtin, F., Elbourne, D., &amp; Altman, D. G. (2002). Meta-analysis combining parallel and cross-over clinical trials. II: Binary outcomes. <em>Statistics in Medicine</em>, <b>21</b>(15), 2145&ndash;2159. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1206&#8288;</code>
</p>
<p>Elbourne, D. R., Altman, D. G., Higgins, J. P. T., Curtin, F., Worthington, H. V., &amp; Vail, A. (2002). Meta-analyses involving cross-over trials: Methodological issues. <em>International Journal of Epidemiology</em>, <b>31</b>(1), 140&ndash;149. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/ije/31.1.140&#8288;</code>
</p>
<p>Fagerland, M. W., Lydersen, S., &amp; Laake, P. (2014). Recommended tests and confidence intervals for paired binomial proportions. <em>Statistics in Medicine</em>, <b>33</b>(16), 2850&ndash;2875. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.6148&#8288;</code>
</p>
<p>Fisher, R. A. (1921). On the &ldquo;probable error&rdquo; of a coefficient of correlation deduced from a small sample. <em>Metron</em>, <b>1</b>, 1&ndash;32. <code style="white-space: pre;">&#8288;http://hdl.handle.net/2440/15169&#8288;</code>
</p>
<p>Fleiss, J. L., &amp; Berlin, J. (2009). Effect sizes for dichotomous data. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 237&ndash;253). New York: Russell Sage Foundation.
</p>
<p>Freeman, M. F., &amp; Tukey, J. W. (1950). Transformations related to the angular and the square root. <em>Annals of Mathematical Statistics</em>, <b>21</b>(4), 607&ndash;611. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/aoms/1177729756&#8288;</code>
</p>
<p>Gibbons, R. D., Hedeker, D. R., &amp; Davis, J. M. (1993). Estimation of effect size from a series of experiments involving paired comparisons. <em>Journal of Educational Statistics</em>, <b>18</b>(3), 271&ndash;279. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986018003271&#8288;</code>
</p>
<p>Hakstian, A. R., &amp; Whalen, T. E. (1976). A k-sample significance test for independent alpha coefficients. <em>Psychometrika</em>, <b>41</b>(2), 219&ndash;231. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/BF02291840&#8288;</code>
</p>
<p>Hasselblad, V., &amp; Hedges, L. V. (1995). Meta-analysis of screening and diagnostic tests. Psychological Bulletin, 117(1), 167-178. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.117.1.167&#8288;</code>
</p>
<p>Hedges, L. V. (1981). Distribution theory for Glass's estimator of effect size and related estimators. <em>Journal of Educational Statistics</em>, <b>6</b>(2), 107&ndash;128. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986006002107&#8288;</code>
</p>
<p>Hedges, L. V. (1982). Estimation of effect size from a series of independent experiments. <em>Psychological Bulletin</em>, <b>92</b>(2), 490&ndash;499. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.92.2.490&#8288;</code>
</p>
<p>Hedges, L. V. (1983). A random effects model for effect sizes. <em>Psychological Bulletin</em>, <b>93</b>(2), 388&ndash;395. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.93.2.388&#8288;</code>
</p>
<p>Hedges, L. V. (1989). An unbiased correction for sampling error in validity generalization studies. <em>Journal of Applied Psychology</em>, <b>74</b>(3), 469&ndash;477. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0021-9010.74.3.469&#8288;</code>
</p>
<p>Hedges, L. V., Gurevitch, J., &amp; Curtis, P. S. (1999). The meta-analysis of response ratios in experimental ecology. <em>Ecology</em>, <b>80</b>(4), 1150&ndash;1156. <code style="white-space: pre;">&#8288;https://doi.org/10.1890/0012-9658(1999)080[1150:TMAORR]2.0.CO;2&#8288;</code>
</p>
<p>Higgins, J. P. T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M. J., &amp; Welch, V. A. (Eds.) (2019). <em>Cochrane handbook for systematic reviews of interventions</em> (2nd ed.). Chichester, UK: Wiley. <code style="white-space: pre;">&#8288;https://training.cochrane.org/handbook&#8288;</code>
</p>
<p>Jacobs, P., &amp; Viechtbauer, W. (2017). Estimation of the biserial correlation and its sampling variance for use in meta-analysis. <em>Research Synthesis Methods</em>, <b>8</b>(2), 161&ndash;180. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1218&#8288;</code>
</p>
<p>Kendall, M., &amp; Stuart, A. (1979). <em>Kendall's advanced theory of statistics, Vol. 2: Inference and relationship</em> (4th ed.). New York: Macmillan.
</p>
<p>Kirk, D. B. (1973). On the numerical approximation of the bivariate normal (tetrachoric) correlation coefficient. <em>Psychometrika</em>, <b>38</b>(2), 259&ndash;268. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/BF02291118&#8288;</code>
</p>
<p>Lajeunesse, M. J. (2011). On the meta-analysis of response ratios for studies with correlated and multi-group designs. <em>Ecology</em>, <b>92</b>(11), 2049&ndash;2055. <code style="white-space: pre;">&#8288;https://doi.org/10.1890/11-0423.1&#8288;</code>
</p>
<p>May, W. L., &amp; Johnson, W. D. (1997). Confidence intervals for differences in correlated binary proportions. <em>Statistics in Medicine</em>, <b>16</b>(18), 2127&ndash;2136. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(SICI)1097-0258(19970930)16:18&lt;2127::AID-SIM633&gt;3.0.CO;2-W&#8288;</code>
</p>
<p>Morris, S. B. (2000). Distribution of the standardized mean change effect size for meta-analysis on repeated measures. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>53</b>(1), 17&ndash;29. <code style="white-space: pre;">&#8288;https://doi.org/10.1348/000711000159150&#8288;</code>
</p>
<p>Morris, S. B., &amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. <em>Psychological Methods</em>, <b>7</b>(1), 105&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989x.7.1.105&#8288;</code>
</p>
<p>Nakagawa, S., Poulin, R., Mengersen, K., Reinhold, K., Engqvist, L., Lagisz, M., &amp; Senior, A. M. (2015). Meta-analysis of variation: Ecological and evolutionary applications and beyond. <em>Methods in Ecology and Evolution</em>, <b>6</b>(2), 143&ndash;152. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/2041-210x.12309&#8288;</code>
</p>
<p>Nakagawa, S., Noble, D. W. A., Lagisz, M., Spake, R., Viechtbauer, W., &amp; Senior, A. M. (2023). A robust and readily implementable method for the meta-analysis of response ratios with and without missing standard deviations. <em>Ecology Letters</em>, <b>26</b>(2), 232&ndash;244. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/ele.14144&#8288;</code>
</p>
<p>Newcombe, R. G. (1998). Improved confidence intervals for the difference between binomial proportions based on paired data. <em>Statistics in Medicine</em>, <b>17</b>(22), 2635&ndash;2650. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(SICI)1097-0258(19981130)17:22&lt;2635::AID-SIM954&gt;3.0.CO;2-C&#8288;</code>
</p>
<p>Olkin, I., &amp; Finn, J. D. (1995). Correlations redux. <em>Psychological Bulletin</em>, <b>118</b>(1), 155&ndash;164. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.118.1.155&#8288;</code>
</p>
<p>Olkin, I., &amp; Pratt, J. W. (1958). Unbiased estimation of certain correlation coefficients. <em>Annals of Mathematical Statistics</em>, <b>29</b>(1), 201&ndash;211. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/aoms/1177706717&#8288;</code>
</p>
<p>Pearson, K. (1900). Mathematical contributions to the theory of evolution. VII. On the correlation of characters not quantitatively measurable. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, <b>195</b>, 1&ndash;47. <code style="white-space: pre;">&#8288;https://doi.org/10.1098/rsta.1900.0022&#8288;</code>
</p>
<p>Pearson, K. (1909). On a new method of determining correlation between a measured character A, and a character B, of which only the percentage of cases wherein B exceeds (or falls short of) a given intensity is recorded for each grade of A. <em>Biometrika</em>, <b>7</b>(1/2), 96&ndash;105. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/biomet/7.1-2.96&#8288;</code>
</p>
<p>Raudenbush, S. W., &amp; Bryk, A. S. (1987). Examining correlates of diversity. <em>Journal of Educational Statistics</em>, <b>12</b>(3), 241&ndash;269. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986012003241&#8288;</code>
</p>
<p>Rothman, K. J., Greenland, S., &amp; Lash, T. L. (2008). <em>Modern epidemiology</em> (3rd ed.). Philadelphia: Lippincott Williams &amp; Wilkins.
</p>
<p>R√ºcker, G., Schwarzer, G., Carpenter, J., &amp; Olkin, I. (2009). Why add anything to nothing? The arcsine difference as a measure of treatment effect in meta-analysis with zero cells. <em>Statistics in Medicine</em>, <b>28</b>(5), 721&ndash;738. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.3511&#8288;</code>
</p>
<p>S√°nchez-Meca, J., Mar√≠n-Mart√≠nez, F., &amp; Chac√≥n-Moscoso, S. (2003). Effect-size indices for dichotomized outcomes in meta-analysis. <em>Psychological Methods</em>, <b>8</b>(4), 448&ndash;467. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.8.4.448&#8288;</code>
</p>
<p>Soper, H. E. (1914). On the probable error of the bi-serial expression for the correlation coefficient. <em>Biometrika</em>, <b>10</b>(2/3), 384&ndash;390. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/biomet/10.2-3.384&#8288;</code>
</p>
<p>Stedman, M. R., Curtin, F., Elbourne, D. R., Kesselheim, A. S., &amp; Brookhart, M. A. (2011). Meta-analyses involving cross-over trials: Methodological issues. <em>International Journal of Epidemiology</em>, <b>40</b>(6), 1732&ndash;1734. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/ije/dyp345&#8288;</code>
</p>
<p>Tate, R. F. (1954). Correlation between a discrete and a continuous variable: Point-biserial correlation. <em>Annals of Mathematical Statistics</em>, <b>25</b>(3), 603&ndash;607. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/aoms/1177728730&#8288;</code>
</p>
<p>Vacha-Haase, T. (1998). Reliability generalization: Exploring variance in measurement error affecting score reliability across studies. <em>Educational and Psychological Measurement</em>, <b>58</b>(1), 6&ndash;20. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/0013164498058001002&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Yule, G. U. (1912). On the methods of measuring association between two attributes. <em>Journal of the Royal Statistical Society</em>, <b>75</b>(6), 579&ndash;652. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2340126&#8288;</code>
</p>
<p>Yusuf, S., Peto, R., Lewis, J., Collins, R., &amp; Sleight, P. (1985). Beta blockade during and after myocardial infarction: An overview of the randomized trials. <em>Progress in Cardiovascular Disease</em>, <b>27</b>(5), 335&ndash;371. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/s0033-0620(85)80003-7&#8288;</code>
</p>
<p>Ziegler, A., Steen, K. V. &amp; Wellek, S. (2011). Investigating Hardy-Weinberg equilibrium in case-control or cohort studies or meta-analysis. <em>Breast Cancer Research and Treatment</em>, <b>128</b>(1), 197&ndash;201. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/s10549-010-1295-z&#8288;</code>
</p>
<p>Zou, G. Y. (2007). One relative risk versus two odds ratios: Implications for meta-analyses involving paired and unpaired binary data. <em>Clinical Trials</em>, <b>4</b>(1), 25&ndash;31. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1740774506075667&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.escalc">print</a></code> and <code><a href="#topic+summary.escalc">summary</a></code> for the print and summary methods.
</p>
<p><code><a href="#topic+conv.2x2">conv.2x2</a></code> for a function to reconstruct the cell frequencies of \(2 \times 2\) tables based on other summary statistics.
</p>
<p><code><a href="#topic+conv.fivenum">conv.fivenum</a></code> for a function to convert five-number summary values to means and standard deviations (needed to compute various effect size measures, such as raw or standardized mean differences and ratios of means / response ratios).
</p>
<p><code><a href="#topic+conv.wald">conv.wald</a></code> for a function to convert Wald-type confidence intervals and test statistics to sampling variances.
</p>
<p><code><a href="#topic+conv.delta">conv.delta</a></code> for a function to transform observed effect sizes or outcomes and their sampling variances using the delta method.
</p>
<p><code><a href="#topic+vcalc">vcalc</a></code> for a function to construct or approximate the variance-covariance matrix of dependent effect sizes or outcomes.
</p>
<p><code><a href="#topic+rcalc">rcalc</a></code> for a function to construct the variance-covariance matrix of dependent correlation coefficients.
</p>
<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> for model fitting functions that can take the calculated effect sizes or outcomes (and the corresponding sampling variances) as input.
</p>
<p><code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.glmm">rma.glmm</a></code> for model fitting functions that take similar inputs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### data from the meta-analysis by Coliditz et al. (1994) on the efficacy of
### BCG vaccine in the prevention of tuberculosis dat.bcg
dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
dat

### suppose that for a particular study, yi and vi are known (i.e., have
### already been calculated) but the 2x2 table counts are not known; with
### replace=FALSE, the yi and vi values for that study are not replaced
dat[1:12,10:11] &lt;- NA
dat[13,4:7] &lt;- NA
dat
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat, replace=FALSE)
dat

### illustrate difference between 'subset' and 'include' arguments
escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, subset=1:6)
escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, include=1:6)

### illustrate the 'var.names' argument
escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, var.names=c("lnrr","var"))

### illustrate the 'slab' argument
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
              data=dat.bcg, slab=paste0(author, ", ", year))
dat

### note: the output looks the same but the study labels are stored as an attribute with the
### effect size estimates (together with the total sample size of the studies and the chosen
### effect size measure)
dat$yi

### this information can then be used by other functions; for example in a forest plot
forest(dat$yi, dat$vi, header=TRUE, top=2)

############################################################################

### convert a regular data frame to an 'escalc' object
### dataset from Lipsey &amp; Wilson (2001), Table 7.1, page 130
dat &lt;- data.frame(id = c(100, 308, 1596, 2479, 9021, 9028, 161, 172, 537, 7049),
                  yi = c(-0.33, 0.32, 0.39, 0.31, 0.17, 0.64, -0.33, 0.15, -0.02, 0.00),
                  vi = c(0.084, 0.035, 0.017, 0.034, 0.072, 0.117, 0.102, 0.093, 0.012, 0.067),
                  random = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1),
                  intensity = c(7, 3, 7, 5, 7, 7, 4, 4, 5, 6))
dat
dat &lt;- escalc(measure="SMD", yi=yi, vi=vi, data=dat, slab=paste("Study ID:", id), digits=3)
dat

############################################################################
</code></pre>

<hr>
<h2 id='fitstats'>Fit Statistics and Information Criteria for 'rma' Objects</h2><span id='topic+fitstats'></span><span id='topic+fitstats.rma'></span><span id='topic+logLik.rma'></span><span id='topic+deviance.rma'></span><span id='topic+AIC.rma'></span><span id='topic+BIC.rma'></span><span id='topic+nobs.rma'></span><span id='topic+df.residual.rma'></span>

<h3>Description</h3>

<p>Functions to extract the log-likelihood, deviance, AIC, BIC, and AICc values from objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitstats(object, ...)

## S3 method for class 'rma'
fitstats(object, ..., REML)

## S3 method for class 'rma'
logLik(object, REML, ...)
## S3 method for class 'rma'
deviance(object, REML, ...)

## S3 method for class 'rma'
AIC(object, ..., k=2, correct=FALSE)
## S3 method for class 'rma'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitstats_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="fitstats_+3A_...">...</code></td>
<td>
<p>optionally more fitted model objects (only for <code>fitstats()</code>, <code>AIC()</code>, and <code>BIC()</code>).</p>
</td></tr>
<tr><td><code id="fitstats_+3A_reml">REML</code></td>
<td>
<p>logical to specify whether the regular or restricted likelihood function should be used to obtain the fit statistics and information criteria. Defaults to the method of estimation used (i.e., <code>TRUE</code> if <code>object</code> was fitted with <code>method="REML"</code> and <code>FALSE</code> otherwise).</p>
</td></tr>
<tr><td><code id="fitstats_+3A_k">k</code></td>
<td>
<p>numeric value to specify the penalty per parameter. The default (<code>k=2</code>) is the classical AIC. See <code><a href="stats.html#topic+AIC">AIC</a></code> for more details.</p>
</td></tr>
<tr><td><code id="fitstats_+3A_correct">correct</code></td>
<td>
<p>logical to specify whether the regular (default) or corrected (i.e., AICc) should be extracted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>fitstats</code>, a data frame with the (restricted) log-likelihood, deviance, AIC, BIC, and AICc values for each model passed to the function.
</p>
<p>For <code>logLik</code>, an object of class <code>"logLik"</code>, providing the (restricted) log-likelihood of the model evaluated at the estimated coefficient(s).
</p>
<p>For <code>deviance</code>, a numeric value with the corresponding deviance.
</p>
<p>For <code>AIC</code> and <code>BIC</code>, either a numeric value with the corresponding AIC, AICc, or BIC or a data frame with rows corresponding to the models and columns representing the number of parameters in the model (<code>df</code>) and the AIC, AICc, or BIC.
</p>


<h3>Note</h3>

<p>Variance components in the model (e.g., \(\tau^2\) in random/mixed-effects models fitted with <code><a href="#topic+rma.uni">rma.uni</a></code>) are counted as additional parameters in the calculation of the AIC, BIC, and AICc. Also, the fixed effects are counted as parameters in the calculation of the AIC, BIC, and AICc even when using REML estimation.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which fit statistics and information criteria can be extracted.
</p>
<p><code><a href="#topic+anova.rma">anova</a></code> for a function to conduct likelihood ratio tests.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### random-effects model
res1 &lt;- rma(yi, vi, data=dat, method="ML")

### mixed-effects model with absolute latitude and publication year as moderators
res2 &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat, method="ML")

### compare fit statistics
fitstats(res1, res2)

### log-likelihoods
logLik(res1)
logLik(res2)

### deviances
deviance(res1)
deviance(res2)

### AIC, AICc, and BIC values
AIC(res1, res2)
AIC(res1, res2, correct=TRUE)
BIC(res1, res2)
</code></pre>

<hr>
<h2 id='fitted.rma'>Fitted Values for 'rma' Objects</h2><span id='topic+fitted'></span><span id='topic+fitted.rma'></span>

<h3>Description</h3>

<p>Function to compute the fitted values for objects of class <code>"rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
fitted(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="fitted.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the fitted values.
</p>


<h3>Note</h3>

<p>The <code><a href="#topic+predict.rma">predict</a></code> function also provides standard errors and confidence intervals for the fitted values. Best linear unbiased predictions (BLUPs) that combine the fitted values based on the fixed effects and the estimated contributions of the random effects can be obtained with <code><a href="#topic+blup.rma.uni">blup</a></code> (only for objects of class <code>"rma.uni"</code>).
</p>
<p>For objects not involving moderators, the fitted values are all identical to the estimated value of the model intercept.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.rma">predict</a></code> for a function to computed predicted values and <code><a href="#topic+blup.rma.uni">blup</a></code> for a function to compute BLUPs that combine the fitted values and predicted random effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### compute the fitted values
fitted(res)
</code></pre>

<hr>
<h2 id='forest'>Forest Plots</h2><span id='topic+forest'></span>

<h3>Description</h3>

<p>Function to create forest plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forest(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forest_+3A_x">x</code></td>
<td>
<p>either an object of class <code>"rma"</code>, a vector with the observed effect sizes or outcomes, or an object of class <code>"cumul.rma"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, methods exist for three types of situations.
</p>
<p>In the first case, object <code>x</code> is a fitted model object coming from the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, or <code><a href="#topic+rma.peto">rma.peto</a></code> functions. The corresponding method is then <code><a href="#topic+forest.rma">forest.rma</a></code>.
</p>
<p>Alternatively, object <code>x</code> can be a vector with observed effect sizes or outcomes. The corresponding method is then <code><a href="#topic+forest.default">forest.default</a></code>.
</p>
<p>Finally, object <code>x</code> can be an object coming from the <code><a href="#topic+cumul.rma.uni">cumul.rma.uni</a></code>, <code><a href="#topic+cumul.rma.mh">cumul.rma.mh</a></code>, or <code><a href="#topic+cumul.rma.peto">cumul.rma.peto</a></code> functions. The corresponding method is then <code><a href="#topic+forest.cumul.rma">forest.cumul.rma</a></code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Lewis, S., &amp; Clarke, M. (2001). Forest plots: Trying to see the wood and the trees. <em>British Medical Journal</em>, <b>322</b>(7300), 1479&ndash;1480. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.322.7300.1479&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest.rma">forest.rma</a></code>, <code><a href="#topic+forest.default">forest.default</a></code>, and <code><a href="#topic+forest.cumul.rma">forest.cumul.rma</a></code> for the specific method functions.
</p>

<hr>
<h2 id='forest.cumul.rma'>Forest Plots (Method for 'cumul.rma' Objects)</h2><span id='topic+forest.cumul.rma'></span>

<h3>Description</h3>

<p>Function to create forest plots for objects of class <code>"cumul.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cumul.rma'
forest(x, annotate=TRUE, header=FALSE,
       xlim, alim, olim, ylim, at, steps=5,
       level=x$level, refline=0, digits=2L, width,
       xlab, ilab, ilab.xpos, ilab.pos,
       transf, atransf, targs, rows,
       efac=1, pch, psize, col, shade, colshade,
       lty, fonts, cex, cex.lab, cex.axis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forest.cumul.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"cumul.rma"</code> obtained with <code><a href="#topic+cumul">cumul</a></code>.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_annotate">annotate</code></td>
<td>
<p>logical to specify whether annotations should be added to the plot (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_header">header</code></td>
<td>
<p>logical to specify whether column headings should be added to the plot (the default is <code>FALSE</code>). Can also be a character vector to specify the left and right headings (or only the left one).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_xlim">xlim</code></td>
<td>
<p>horizontal limits of the plot region. If unspecified, the function sets the horizontal plot limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_alim">alim</code></td>
<td>
<p>the x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_ylim">ylim</code></td>
<td>
<p>the y-axis limits of the plot. If unspecified, the function sets the y-axis limits to some sensible values. Can also be a single value to set the lower bound (while the upper bound is still set automatically).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_at">at</code></td>
<td>
<p>position of the x-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_steps">steps</code></td>
<td>
<p>the number of tick marks for the x-axis (the default is 5). Ignored when the positions are specified via the <code>at</code> argument.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (see <a href="#topic+misc-options">here</a> for details). The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_refline">refline</code></td>
<td>
<p>numeric value to specify the location of the vertical &lsquo;reference&rsquo; line (the default is 0). The line can be suppressed by setting this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the tick mark labels of the x-axis and the annotations should be rounded (the default is <code>2L</code>). Can also be a vector of two integers, the first to specify the number of decimal places for the annotations, the second for the x-axis labels. When specifying an integer (e.g., <code>2L</code>), trailing zeros after the decimal mark are dropped for the x-axis labels. When specifying a numeric value (e.g., <code>2</code>), trailing zeros are retained.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations (either a single integer or a vector of the same length as the number of annotation columns).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title. Can also be a vector of three/two values (to also/only add labels at the end points of the x-axis limits).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_ilab">ilab</code></td>
<td>
<p>optional vector, matrix, or data frame providing additional information about the studies that should be added to the plot.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_ilab.xpos">ilab.xpos</code></td>
<td>
<p>numeric vector to specify the horizontal position(s) of the variable(s) given via <code>ilab</code>.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_ilab.pos">ilab.pos</code></td>
<td>
<p>integer(s) (either 1, 2, 3, or 4) to specify the alignment of the vector(s) given via <code>ilab</code> (2 means right, 4 mean left aligned). If unspecified, the default is to center the labels.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the estimates and confidence interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the x-axis labels and annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_rows">rows</code></td>
<td>
<p>optional vector to specify the rows (or more generally, the horizontal positions) for plotting the outcomes. Can also be a single value to specify the row (horizontal position) of the first outcome (the remaining outcomes are then plotted below this starting row).</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_efac">efac</code></td>
<td>
<p>vertical expansion factor for confidence interval limits and arrows. The default value of 1 should usually work okay. Can also be a vector of two numbers, the first for CI limits, the second for arrows.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the estimates. By default, a filled square is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_psize">psize</code></td>
<td>
<p>numeric value to specify the point sizes for the estimates (the default is 1). Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the estimates. Can also be a vector.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_shade">shade</code></td>
<td>
<p>optional character string or a (logical or numeric) vector for shading rows of the plot.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_colshade">colshade</code></td>
<td>
<p>optional argument to specify the color for the shading.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the confidence intervals. If unspecified, the function sets this to <code>"solid"</code> by default.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the study labels, annotations, and the extra information (if specified via <code>ilab</code>). If unspecified, the default font is used.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_cex">cex</code></td>
<td>
<p>optional character and symbol expansion factor. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_cex.lab">cex.lab</code></td>
<td>
<p>optional expansion factor for the x-axis title. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_cex.axis">cex.axis</code></td>
<td>
<p>optional expansion factor for the x-axis labels. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.cumul.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot shows the estimated (average) outcome with corresponding confidence interval as one study at a time is added to the analysis.
</p>
<p>See <code><a href="#topic+forest.default">forest.default</a></code> and <code><a href="#topic+forest.rma">forest.rma</a></code> for further details on the purpose of the various arguments.
</p>


<h3>Note</h3>

<p>The function sets some sensible values for the optional arguments, but it may be necessary to adjust these in certain circumstances.
</p>
<p>The function actually returns some information about the chosen values invisibly. Printing this information is useful as a starting point to make adjustments to the plot.
</p>
<p>If the number of studies is quite large, the labels, annotations, and symbols may become quite small and impossible to read. Stretching the plot window vertically may then provide a more readable figure (one should call the function again after adjusting the window size, so that the label/symbol sizes can be properly adjusted). Also, the <code>cex</code>, <code>cex.lab</code>, and <code>cex.axis</code> arguments are then useful to adjust the symbol and text sizes.
</p>
<p>If the outcome measure used for creating the plot is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those limits (the observed outcomes and confidence intervals cannot exceed those bounds then).
</p>
<p>The <code>lty</code> argument can also be a vector of two elements, the first for specifying the line type of the individual CIs (<code>"solid"</code> by default), the second for the line type of the horizontal line that is automatically added to the plot (<code>"solid"</code> by default; set to <code>"blank"</code> to remove it).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Chalmers, T. C., &amp; Lau, J. (1993). Meta-analytic stimulus for changes in clinical trials. <em>Statistical Methods in Medical Research</em>, <b>2</b>(2), 161&ndash;172. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/096228029300200204&#8288;</code>
</p>
<p>Lau, J., Schmid, C. H., &amp; Chalmers, T. C. (1995). Cumulative meta-analysis of clinical trials builds evidence for exemplary medical care. <em>Journal of Clinical Epidemiology</em>, <b>48</b>(1), 45&ndash;57. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/0895-4356(94)00106-z&#8288;</code>
</p>
<p>Lewis, S., &amp; Clarke, M. (2001). Forest plots: Trying to see the wood and the trees. <em>British Medical Journal</em>, <b>322</b>(7300), 1479&ndash;1480. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.322.7300.1479&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for an overview of the various <code>forest</code> functions.
</p>
<p><code><a href="#topic+cumul">cumul</a></code> for the function to create <code>cumul.rma</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
              data=dat.bcg, slab=paste(author, year, sep=", "))

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### draw cumulative forest plots
x &lt;- cumul(res, order=year)
forest(x, cex=0.8, header=TRUE, top=2)
forest(x, xlim=c(-4,2.5), alim=c(-2,1), cex=0.8, header=TRUE, top=2)

### meta-analysis of the (log) risk ratios using the Mantel-Haenszel method
res &lt;- rma.mh(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
              slab=paste(author, year, sep=", "))

### draw cumulative forest plot
x &lt;- cumul(res, order=year)
forest(x, xlim=c(-4,2.5), alim=c(-2,1), cex=0.8, header=TRUE, top=2)
</code></pre>

<hr>
<h2 id='forest.default'>Forest Plots (Default Method)</h2><span id='topic+forest.default'></span>

<h3>Description</h3>

<p>Function to create forest plots for a given set of data. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
forest(x, vi, sei, ci.lb, ci.ub,
       annotate=TRUE, showweights=FALSE, header=FALSE,
       xlim, alim, olim, ylim, at, steps=5,
       level=95, refline=0, digits=2L, width,
       xlab, slab, ilab, ilab.xpos, ilab.pos,
       order, subset, transf, atransf, targs, rows,
       efac=1, pch, psize, plim=c(0.5,1.5), col,
       shade, colshade, lty, fonts, cex, cex.lab, cex.axis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forest.default_+3A_x">x</code></td>
<td>
<p>vector of length \(k\) with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_vi">vi</code></td>
<td>
<p>vector of length \(k\) with the corresponding sampling variances.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_sei">sei</code></td>
<td>
<p>vector of length \(k\) with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ci.lb">ci.lb</code></td>
<td>
<p>vector of length \(k\) with the corresponding lower confidence interval bounds. Not needed if <code>vi</code> or <code>sei</code> is specified. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ci.ub">ci.ub</code></td>
<td>
<p>vector of length \(k\) with the corresponding upper confidence interval bounds. Not needed if <code>vi</code> or <code>sei</code> is specified. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_annotate">annotate</code></td>
<td>
<p>logical to specify whether annotations should be added to the plot (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_showweights">showweights</code></td>
<td>
<p>logical to specify whether the annotations should also include the inverse variance weights (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_header">header</code></td>
<td>
<p>logical to specify whether column headings should be added to the plot (the default is <code>FALSE</code>). Can also be a character vector to specify the left and right headings  (or only the left one).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_xlim">xlim</code></td>
<td>
<p>horizontal limits of the plot region. If unspecified, the function sets the horizontal plot limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_alim">alim</code></td>
<td>
<p>the x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ylim">ylim</code></td>
<td>
<p>the y-axis limits of the plot. If unspecified, the function sets the y-axis limits to some sensible values. Can also be a single value to set the lower bound (while the upper bound is still set automatically).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_at">at</code></td>
<td>
<p>position of the x-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_steps">steps</code></td>
<td>
<p>the number of tick marks for the x-axis (the default is 5). Ignored when the positions are specified via the <code>at</code> argument.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_refline">refline</code></td>
<td>
<p>numeric value to specify the location of the vertical &lsquo;reference&rsquo; line (the default is 0). The line can be suppressed by setting this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the tick mark labels of the x-axis and the annotations should be rounded (the default is <code>2L</code>). Can also be a vector of two integers, the first to specify the number of decimal places for the annotations, the second for the x-axis labels (when <code>showweights=TRUE</code>, can also specify a third value for the weights). When specifying an integer (e.g., <code>2L</code>), trailing zeros after the decimal mark are dropped for the x-axis labels. When specifying a numeric value (e.g., <code>2</code>), trailing zeros are retained.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations (either a single integer or a vector of the same length as the number of annotation columns).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title. Can also be a vector of three/two values (to also/only add labels at the end points of the x-axis limits).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies. If unspecified, the function tries to extract study labels from <code>x</code> and otherwise simple labels are created within the function. To suppress labels, set this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ilab">ilab</code></td>
<td>
<p>optional vector, matrix, or data frame providing additional information about the studies that should be added to the plot.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ilab.xpos">ilab.xpos</code></td>
<td>
<p>numeric vector to specify the horizontal position(s) of the variable(s) given via <code>ilab</code>.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_ilab.pos">ilab.pos</code></td>
<td>
<p>integer(s) (either 1, 2, 3, or 4) to specify the alignment of the vector(s) given via <code>ilab</code> (2 means right, 4 mean left aligned). If unspecified, the default is to center the labels.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_order">order</code></td>
<td>
<p>optional character string to specify how the studies should be ordered. Can also be a variable based on which the studies will be ordered. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the plot.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the observed outcomes and corresponding confidence interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the x-axis labels and annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_rows">rows</code></td>
<td>
<p>optional vector to specify the rows (or more generally, the horizontal positions) for plotting the outcomes. Can also be a single value to specify the row (horizontal position) of the first outcome (the remaining outcomes are then plotted below this starting row).</p>
</td></tr>
<tr><td><code id="forest.default_+3A_efac">efac</code></td>
<td>
<p>vertical expansion factor for confidence interval limits and arrows. The default value of 1 should usually work okay. Can also be a vector of two numbers, the first for CI limits, the second for arrows.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the observed outcomes. By default, a filled square is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_psize">psize</code></td>
<td>
<p>optional numeric value to specify the point sizes for the observed outcomes. If unspecified, the point sizes are a function of the precision of the estimates. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_plim">plim</code></td>
<td>
<p>numeric vector of length 2 to scale the point sizes (ignored when <code>psize</code> is specified). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the observed outcomes. Can also be a vector.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_shade">shade</code></td>
<td>
<p>optional character string or a (logical or numeric) vector for shading rows of the plot. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_colshade">colshade</code></td>
<td>
<p>optional argument to specify the color for the shading.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the confidence intervals. If unspecified, the function sets this to <code>"solid"</code> by default.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the study labels, annotations, and the extra information (if specified via <code>ilab</code>). If unspecified, the default font is used.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_cex">cex</code></td>
<td>
<p>optional character and symbol expansion factor. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_cex.lab">cex.lab</code></td>
<td>
<p>optional expansion factor for the x-axis title. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_cex.axis">cex.axis</code></td>
<td>
<p>optional expansion factor for the x-axis labels. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.default_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot shows the observed effect sizes or outcomes with corresponding confidence intervals. To use the function, one should specify the observed outcomes (via the <code>x</code> argument) together with the corresponding sampling variances (via the <code>vi</code> argument) or with the corresponding standard errors (via the <code>sei</code> argument). Alternatively, one can specify the observed outcomes together with the corresponding confidence interval bounds (via the <code>ci.lb</code> and <code>ci.ub</code> arguments).
</p>
<p>With the <code>transf</code> argument, the observed outcomes and corresponding confidence interval bounds can be transformed with some suitable function. For example, when plotting log odds ratios, then one could use <code>transf=exp</code> to obtain a forest plot showing the odds ratios. Alternatively, one can use the <code>atransf</code> argument to transform the x-axis labels and annotations (e.g., <code>atransf=exp</code>). See also <a href="#topic+transf">transf</a> for some other useful transformation functions in the context of a meta-analysis. The examples below illustrate the use of these arguments.
</p>
<p>By default, the studies are ordered from top to bottom (i.e., the first study in the dataset will be placed in row \(k\), the second study in row \(k-1\), and so on, until the last study, which is placed in the first row). The studies can be reordered with the <code>order</code> argument:
</p>

<ul>
<li> <p><code>order="obs"</code>: the studies are ordered by the observed outcomes,
</p>
</li>
<li> <p><code>order="prec"</code>: the studies are ordered by their sampling variances.
</p>
</li></ul>

<p>Alternatively, it is also possible to set <code>order</code> equal to a variable based on which the studies will be ordered (see &lsquo;Examples&rsquo;).
</p>
<p>Additional columns with information about the studies can be added to the plot via the <code>ilab</code> argument. This can either be a single variable or an entire matrix / data frame (with as many rows as there are studies in the forest plot). The <code>ilab.xpos</code> argument can then also be specified to indicate the horizontal position of the variables specified via <code>ilab</code>.
</p>
<p>Summary estimates can be added to the plot with the <code><a href="#topic+addpoly">addpoly</a></code> function. See the documentation for that function for examples.
</p>
<p>By default (i.e., when <code>psize</code> is not specified), the point sizes are a function of the precision (i.e., inverse standard errors) of the outcomes. This way, more precise estimates are visually more prominent in the plot. By making the point sizes a function of the inverse standard errors of the estimates, their areas are proportional to the inverse sampling variances, which corresponds to the weights they would receive in an equal-effects model. However, the point sizes are rescaled so that the smallest point size is <code>plim[1]</code> and the largest point size is <code>plim[2]</code>. As a result, their relative sizes (i.e., areas) no longer exactly correspond to their relative weights in such a model. If exactly relative point sizes are desired, one can set <code>plim[2]</code> to <code>NA</code>, in which case the points are rescaled so that the smallest point size corresponds to <code>plim[1]</code> and all other points are scaled accordingly. As a result, the largest point may be very large. Alternatively, one can set <code>plim[1]</code> to <code>NA</code>, in which case the points are rescaled so that the largest point size corresponds to <code>plim[2]</code> and all other points are scaled accordingly. As a result, the smallest point may be very small and essentially indistinguishable from the confidence interval line. To avoid the latter, one can also set <code>plim[3]</code>, which enforces a minimal point size.
</p>
<p>With the <code>shade</code> argument, one can shade rows of the plot. The argument can be set to one of the following character strings: <code>"zebra"</code> (same as <code>shade=TRUE</code>) or <code>"zebra2"</code> to use zebra-style shading (starting either at the first or second study) or to <code>"all"</code> in which case all rows are shaded. Alternatively, the argument can be set to a logical or numeric vector to indicates which rows should be shaded. The <code>colshade</code> argument can be used to set the color of shaded rows.
</p>


<h3>Note</h3>

<p>The function sets some sensible values for the optional arguments, but it may be necessary to adjust these in certain circumstances.
</p>
<p>The function actually returns some information about the chosen values invisibly. Printing this information is useful as a starting point to make adjustments to the plot.
</p>
<p>If the number of studies is quite large, the labels, annotations, and symbols may become quite small and impossible to read. Stretching the plot window vertically may then provide a more readable figure (one should call the function again after adjusting the window size, so that the label/symbol sizes can be properly adjusted). Also, the <code>cex</code>, <code>cex.lab</code>, and <code>cex.axis</code> arguments are then useful to adjust the symbol and text sizes.
</p>
<p>If the outcome measure used for creating the plot is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those limits (the observed outcomes and confidence intervals cannot exceed those bounds then).
</p>
<p>The <code>lty</code> argument can also be a vector of two elements, the first for specifying the line type of the individual CIs (<code>"solid"</code> by default), the second for the line type of the horizontal line that is automatically added to the plot (<code>"solid"</code> by default; set to <code>"blank"</code> to remove it).
</p>


<h3>Additional Optional Arguments</h3>

<p>There are some additional optional arguments that can be passed to the function via <code>...</code> (hence, they cannot be abbreviated):
</p>

<dl>
<dt>top</dt><dd><p>single numeric value to specify the amount of space (in terms of number of rows) to leave empty at the top of the plot (e.g., for adding headers). The default is 3.</p>
</dd>
<dt>annosym</dt><dd><p>vector of length 3 to select the left bracket, separation, and right bracket symbols for the annotations. The default is <code>c(" [", ", ", "]")</code>. Can also include a 4th element to adjust the look of the minus symbol, for example to use a proper minus sign (‚àí) instead of a hyphen-minus (-). Can also include a 5th element that should be a space-like symbol (e.g., an &lsquo;en space&rsquo;) that is used in place of numbers (only relevant when trying to line up numbers exactly). For example, <code>annosym=c(" [", ", ", "]", "\u2212", "\u2002")</code> would use a proper minus sign and an &lsquo;en space&rsquo; for the annotations.</p>
</dd>
<dt>tabfig</dt><dd><p>single numeric value (either a 1, 2, or 3) to set <code>annosym</code> automatically to a vector that will exactly align the numbers in the annotations when using a font that provides &lsquo;tabular figures&rsquo;. Value 1 corresponds to using <code>"\u2212"</code> (a minus) and <code>"\u2002"</code> (an &lsquo;en space&rsquo;) in <code>annoyym</code> as shown above. Value 2 corresponds to <code>"\u2013"</code> (an &lsquo;en dash&rsquo;) and <code>"\u2002"</code> (an &lsquo;en space&rsquo;). Value 3 corresponds to <code>"\u2212"</code> (a minus) and <code>"\u2007"</code> (a &lsquo;figure space&rsquo;). The appropriate value for this argument depends on the font used. For example, for fonts Calibri and Carlito, 1 or 2 should work; for fonts Source Sans 3 and Palatino Linotype, 1, 2, and 3 should all work; for Computer/Latin Modern and Segoe UI, 2 should work; for Lato, Roboto, and Open Sans (and maybe Arial), 3 should work. Other fonts may work as well, but this is untested.</p>
</dd>
<dt>textpos</dt><dd><p>numeric vector of length 2 to specify the placement of the study labels and the annotations. The default is to use the horizontal limits of the plot region, i.e., the study labels to the right of <code>xlim[1]</code> and the annotations to the left of <code>xlim[2]</code>.</p>
</dd>
<dt>rowadj</dt><dd><p>numeric vector of length 3 to vertically adjust the position of the study labels, the annotations, and the extra information (if specified via <code>ilab</code>). This is useful for fine-tuning the position of text added with different positional alignments (i.e., argument <code>pos</code> in the <code><a href="graphics.html#topic+text">text</a></code> function).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Lewis, S., &amp; Clarke, M. (2001). Forest plots: Trying to see the wood and the trees. <em>British Medical Journal</em>, <b>322</b>(7300), 1479&ndash;1480. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.322.7300.1479&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for an overview of the various <code>forest</code> functions and especially <code><a href="#topic+forest.rma">forest.rma</a></code> for the function draw forest plots including a summary polygon.
</p>
<p><code><a href="#topic+addpoly">addpoly</a></code> for a function to add polygons to forest plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
              data=dat.bcg, slab=paste(author, year, sep=", "))

### default forest plot of the observed log risk ratios
forest(dat$yi, dat$vi, header=TRUE)

### the with() function can be used to avoid having to retype dat$... over and over
with(dat, forest(yi, vi, header=TRUE))

### forest plot of the observed risk ratios (transform outcomes)
with(dat, forest(yi, vi, transf=exp, alim=c(0,2), steps=5,
                 xlim=c(-2.5,4), refline=1, cex=0.9, header=TRUE, top=2))

### forest plot of the observed risk ratios (transformed x-axis)
with(dat, forest(yi, vi, atransf=exp, at=log(c(0.05,0.25,1,4,20)),
                 xlim=c(-10,8), cex=0.9, header=TRUE, top=2))

### forest plot of the observed risk ratios with studies ordered by the RRs
with(dat, forest(yi, vi, atransf=exp, at=log(c(0.05,0.25,1,4,20)),
                 xlim=c(-10,8), cex=0.9, header=TRUE, top=2, order="obs"))

### forest plot of the observed risk ratios with studies ordered by absolute latitude
with(dat, forest(yi, vi, atransf=exp, at=log(c(0.05,0.25,1,4,20)),
                 xlim=c(-10,8), cex=0.9, header=TRUE, top=2, order=ablat))

### see also examples for the forest.rma function
</code></pre>

<hr>
<h2 id='forest.rma'>Forest Plots (Method for 'rma' Objects)</h2><span id='topic+forest.rma'></span>

<h3>Description</h3>

<p>Function to create forest plots for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
forest(x, annotate=TRUE, addfit=TRUE, addpred=FALSE,
       showweights=FALSE, header=FALSE,
       xlim, alim, olim, ylim, at, steps=5,
       level=x$level, refline=0, digits=2L, width,
       xlab, slab, mlab, ilab, ilab.xpos, ilab.pos,
       order, transf, atransf, targs, rows,
       efac=1, pch, psize, plim=c(0.5,1.5), colout, col, border,
       shade, colshade, lty, fonts, cex, cex.lab, cex.axis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forest.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_annotate">annotate</code></td>
<td>
<p>logical to specify whether annotations should be added to the plot (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_addfit">addfit</code></td>
<td>
<p>logical to specify whether the summary estimate (for models without moderators) or fitted values (for models with moderators) should be added to the plot (the default is <code>TRUE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_addpred">addpred</code></td>
<td>
<p>logical to specify whether the bounds of the prediction interval should be added to the plot (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_showweights">showweights</code></td>
<td>
<p>logical to specify whether the annotations should also include the weights given to the observed outcomes during the model fitting (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_header">header</code></td>
<td>
<p>logical to specify whether column headings should be added to the plot (the default is <code>FALSE</code>). Can also be a character vector to specify the left and right headings  (or only the left one).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_xlim">xlim</code></td>
<td>
<p>horizontal limits of the plot region. If unspecified, the function sets the horizontal plot limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_alim">alim</code></td>
<td>
<p>the x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_ylim">ylim</code></td>
<td>
<p>the y-axis limits of the plot. If unspecified, the function sets the y-axis limits to some sensible values. Can also be a single value to set the lower bound (while the upper bound is still set automatically).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_at">at</code></td>
<td>
<p>position of the x-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_steps">steps</code></td>
<td>
<p>the number of tick marks for the x-axis (the default is 5). Ignored when the positions are specified via the <code>at</code> argument.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (see <a href="#topic+misc-options">here</a> for details). The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_refline">refline</code></td>
<td>
<p>numeric value to specify the location of the vertical &lsquo;reference&rsquo; line (the default is 0). The line can be suppressed by setting this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the tick mark labels of the x-axis and the annotations should be rounded (the default is <code>2L</code>). Can also be a vector of two integers, the first to specify the number of decimal places for the annotations, the second for the x-axis labels (when <code>showweights=TRUE</code>, can also specify a third value for the weights). When specifying an integer (e.g., <code>2L</code>), trailing zeros after the decimal mark are dropped for the x-axis labels. When specifying a numeric value (e.g., <code>2</code>), trailing zeros are retained.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_width">width</code></td>
<td>
<p>optional integer to manually adjust the width of the columns for the annotations (either a single integer or a vector of the same length as the number of annotation columns).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title. Can also be a vector of three/two values (to also/only add labels at the end points of the x-axis limits).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies. If unspecified, the function tries to extract study labels from <code>x</code> or simple labels are created within the function. To suppress labels, set this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_mlab">mlab</code></td>
<td>
<p>optional character string giving a label to the summary estimate from an equal- or a random-effects model. If unspecified, the label is created within the function.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_ilab">ilab</code></td>
<td>
<p>optional vector, matrix, or data frame providing additional information about the studies that should be added to the plot.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_ilab.xpos">ilab.xpos</code></td>
<td>
<p>numeric vector to specify the horizontal position(s) of the variable(s) given via <code>ilab</code>.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_ilab.pos">ilab.pos</code></td>
<td>
<p>integer(s) (either 1, 2, 3, or 4) to specify the alignment of the vector(s) given via <code>ilab</code> (2 means right, 4 mean left aligned). If unspecified, the default is to center the labels.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_order">order</code></td>
<td>
<p>optional character string to specify how the studies should be ordered. Can also be a variable based on which the studies will be ordered. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the observed outcomes, summary estimates, fitted values, and confidence interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the x-axis labels and annotations (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_rows">rows</code></td>
<td>
<p>optional vector to specify the rows (or more generally, the horizontal positions) for plotting the outcomes. Can also be a single value to specify the row (horizontal position) of the first outcome (the remaining outcomes are then plotted below this starting row).</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_efac">efac</code></td>
<td>
<p>vertical expansion factor for confidence interval limits, arrows, and the symbol used to denote summary estimates. The default value of 1 should usually work okay. Can also be a vector of two numbers, the first for CI limits and arrows, the second for summary estimates. Can also be a vector of three numbers, the first for CI limits, the second for arrows, the third for summary estimates.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the observed outcomes. By default, a filled square is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_psize">psize</code></td>
<td>
<p>optional numeric value to specify the point sizes for the observed outcomes. If unspecified, the point sizes are a function of the model weights. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_plim">plim</code></td>
<td>
<p>numeric vector of length 2 to scale the point sizes (ignored when <code>psize</code> is specified). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_colout">colout</code></td>
<td>
<p>optional character string to specify the color of the observed outcomes. Can also be a vector.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the summary polygon or fitted values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_border">border</code></td>
<td>
<p>optional character string to specify the border color of the summary polygon or fitted values.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_shade">shade</code></td>
<td>
<p>optional character string or a (logical or numeric) vector for shading rows of the plot. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_colshade">colshade</code></td>
<td>
<p>optional argument to specify the color for the shading.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the confidence intervals. If unspecified, the function sets this to <code>"solid"</code> by default.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_fonts">fonts</code></td>
<td>
<p>optional character string to specify the font for the study labels, annotations, and the extra information (if specified via <code>ilab</code>). If unspecified, the default font is used.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_cex">cex</code></td>
<td>
<p>optional character and symbol expansion factor. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_cex.lab">cex.lab</code></td>
<td>
<p>optional expansion factor for the x-axis title. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_cex.axis">cex.axis</code></td>
<td>
<p>optional expansion factor for the x-axis labels. If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="forest.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot shows the observed effect sizes or outcomes with corresponding confidence intervals.
</p>
<p>For an equal- and a random-effects model (i.e., for models without moderators), a four-sided polygon, sometimes called a summary &lsquo;diamond&rsquo;, is added to the bottom of the forest plot, showing the summary estimate based on the model (with the center of the polygon corresponding to the estimate and the left/right edges indicating the confidence interval limits). The <code>col</code> and <code>border</code> arguments can be used to adjust the (border) color of the polygon. Drawing of the polgyon can be suppressed by setting <code>addfit=FALSE</code>.
</p>
<p>For random-effects models and if <code>addpred=TRUE</code>, a dotted line is added to the summary polygon which indicates the bounds of the prediction interval (the interval estimates where <code>level</code>% of the true outcomes are expected to fall) (Riley et al., 2011). For random-effects models of class <code>"rma.mv"</code> (see <code><a href="#topic+rma.mv">rma.mv</a></code>) with multiple \(\tau^2\) values, the <code>addpred</code> argument can be used to specify for which level of the inner factor the prediction interval should be provided (since the intervals differ depending on the \(\tau^2\) value). If the model should also contain multiple \(\gamma^2\) values, the <code>addpred</code> argument should then be of length 2 to specify the levels of both inner factors. See also <code><a href="#topic+predict.rma">predict</a></code>, which is used to compute these interval bounds.
</p>
<p>For meta-regression models (i.e., models involving moderators), the fitted value for each study is added as a polygon to the plot. By default, the width of the polygons corresponds to the confidence interval limits for the fitted values. By setting <code>addpred=TRUE</code>, the width reflects the prediction interval limits. Again, the <code>col</code> and <code>border</code> arguments can be used to adjust the (border) color of the polygons. These polygons can be suppressed by setting <code>addfit=FALSE</code>.
</p>
<p>With the <code>transf</code> argument, the observed outcomes, summary estimate, fitted values, confidence interval bounds, and prediction interval bounds can be transformed with some suitable function. For example, when plotting log odds ratios, one could use <code>transf=exp</code> to obtain a forest plot showing the odds ratios. Alternatively, one can use the <code>atransf</code> argument to transform the x-axis labels and annotations (e.g., <code>atransf=exp</code>). See also <a href="#topic+transf">transf</a> for some other useful transformation functions in the context of a meta-analysis. The examples below illustrate the use of these arguments.
</p>
<p>By default, the studies are ordered from top to bottom (i.e., the first study in the dataset will be placed in row \(k\), the second study in row \(k-1\), and so on, until the last study, which is placed in the first row). The studies can be reordered with the <code>order</code> argument:
</p>

<ul>
<li> <p><code>order="obs"</code>: the studies are ordered by the observed outcomes,
</p>
</li>
<li> <p><code>order="fit"</code>: the studies are ordered by the fitted values,
</p>
</li>
<li> <p><code>order="prec"</code>: the studies are ordered by their sampling variances,
</p>
</li>
<li> <p><code>order="resid"</code>: the studies are ordered by the size of their residuals,
</p>
</li>
<li> <p><code>order="rstandard"</code>: the studies are ordered by the size of their standardized residuals,
</p>
</li>
<li> <p><code>order="abs.resid"</code>: the studies are ordered by the size of their absolute residuals,
</p>
</li>
<li> <p><code>order="abs.rstandard"</code>: the studies are ordered by the size of their absolute standardized residuals.
</p>
</li></ul>

<p>Alternatively, it is also possible to set <code>order</code> equal to a variable based on which the studies will be ordered (see &lsquo;Examples&rsquo;).
</p>
<p>Additional columns with information about the studies can be added to the plot via the <code>ilab</code> argument. This can either be a single variable or an entire matrix / data frame (with as many rows as there are studies in the forest plot). The <code>ilab.xpos</code> argument can then also be specified to indicate the horizontal position of the variables specified via <code>ilab</code>.
</p>
<p>The figure below illustrates how the elements in a forest plot can be arranged and the meaning of the some of the arguments such as <code>xlim</code>, <code>alim</code> or <code>at</code>, <code>ilab</code>, and <code>ilab.xpos</code>.
</p>
<p><img src="../help/figures/forest-arrangement.png" width=800 alt="forest-arrangement.png" />
</p>
<p>The figure corresponds to the following code: </p>
<pre>dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
res &lt;- rma(yi, vi, data=dat, slab=paste(author, year, sep=", "))
forest(res, addpred=TRUE, xlim=c(-16,7), at=seq(-3,2,by=1), shade="zebra",
       ilab=cbind(tpos, tneg, cpos, cneg), ilab.xpos=c(-9.5,-8,-6,-4.5),
       cex=0.75, header="Author(s) and Year")
text(c(-9.5,-8,-6,-4.5), res$k+2, c("TB+", "TB-", "TB+", "TB-"), cex=0.75, font=2)
text(c(-8.75,-5.25),     res$k+3, c("Vaccinated", "Control"),    cex=0.75, font=2)</pre>

<p>Additional summary estimates can be added to the plot with the <code><a href="#topic+addpoly">addpoly</a></code> function. See the documentation for that function for examples.
</p>
<p>When <code>showweights=TRUE</code>, the annotations will include information about the weights given to the observed outcomes during the model fitting. For simple models (such as those fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function), these weights correspond to the &lsquo;inverse-variance weights&rsquo; (but are given in percent). For models fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, the weights are based on the diagonal of the weight matrix. Note that the weighting structure is typically more complex in such models (i.e., the weight matrix is usually not just a diagonal matrix) and the weights shown therefore do not reflect this complexity. See <code><a href="#topic+weights.rma">weights</a></code> for more details (for the special case that <code>x</code> is an intercept-only <code>"rma.mv"</code> model, one can also set <code>showweights="rowsum"</code> to show the &lsquo;row-sum weights&rsquo;).
</p>
<p>By default (i.e., when <code>psize</code> is not specified), the point sizes are a function of the square root of the model weights. This way, their areas are proportional to the weights. However, the point sizes are rescaled so that the smallest point size is <code>plim[1]</code> and the largest point size is <code>plim[2]</code>. As a result, their relative sizes (i.e., areas) no longer exactly correspond to their relative weights. If exactly relative point sizes are desired, one can set <code>plim[2]</code> to <code>NA</code>, in which case the points are rescaled so that the smallest point size corresponds to <code>plim[1]</code> and all other points are scaled accordingly. As a result, the largest point may be very large. Alternatively, one can set <code>plim[1]</code> to <code>NA</code>, in which case the points are rescaled so that the largest point size corresponds to <code>plim[2]</code> and all other points are scaled accordingly. As a result, the smallest point may be very small and essentially indistinguishable from the confidence interval line. To avoid the latter, one can also set <code>plim[3]</code>, which enforces a minimal point size.
</p>
<p>With the <code>shade</code> argument, one can shade rows of the plot. The argument can be set to one of the following character strings: <code>"zebra"</code> (same as <code>shade=TRUE</code>) or <code>"zebra2"</code> to use zebra-style shading (starting either at the first or second study) or to <code>"all"</code> in which case all rows are shaded. Alternatively, the argument can be set to a logical or numeric vector to indicates which rows should be shaded. The <code>colshade</code> argument can be used to set the color of shaded rows.
</p>


<h3>Note</h3>

<p>The function sets some sensible values for the optional arguments, but it may be necessary to adjust these in certain circumstances.
</p>
<p>The function actually returns some information about the chosen values invisibly. Printing this information is useful as a starting point to make adjustments to the plot (see &lsquo;Examples&rsquo;).
</p>
<p>Arguments <code>slab</code> and <code>ilab</code> and when specifying vectors for arguments <code>pch</code>, <code>psize</code>, <code>order</code>, and/or <code>colout</code> (and when <code>shade</code> is a logical vector), the variables specified are assumed to be of the same length as the data originally passed to the model fitting function (and if the <code>data</code> argument was used in the original model fit, then the variables will be searched for within this data frame first). Any subsetting and removal of studies with missing values is automatically applied to the variables specified via these arguments.
</p>
<p>If the number of studies is quite large, the labels, annotations, and symbols may become quite small and impossible to read. Stretching the plot window vertically may then provide a more readable figure (one should call the function again after adjusting the window size, so that the label/symbol sizes can be properly adjusted). Also, the <code>cex</code>, <code>cex.lab</code>, and <code>cex.axis</code> arguments are then useful to adjust the symbol and text sizes.
</p>
<p>If the outcome measure used for creating the plot is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those limits (the observed outcomes and confidence/prediction intervals cannot exceed those bounds then).
</p>
<p>The models without moderators, the <code>col</code> argument can also be a vector of two elements, the first for specifying the color of the summary polygon, the second for specifying the color of the line for the prediction interval.
</p>
<p>The <code>lty</code> argument can also be a vector of up to three elements, the first for specifying the line type of the individual CIs (<code>"solid"</code> by default), the second for the line type of the prediction interval (<code>"dotted"</code> by default), the third for the line type of the horizontal lines that are automatically added to the plot (<code>"solid"</code> by default; set to <code>"blank"</code> to remove them).
</p>


<h3>Additional Optional Arguments</h3>

<p>There are some additional optional arguments that can be passed to the function via <code>...</code> (hence, they cannot be abbreviated):
</p>

<dl>
<dt>top</dt><dd><p>single numeric value to specify the amount of space (in terms of number of rows) to leave empty at the top of the plot (e.g., for adding headers). The default is 3.</p>
</dd>
<dt>annosym</dt><dd><p>vector of length 3 to select the left bracket, separation, and right bracket symbols for the annotations. The default is <code>c(" [", ", ", "]")</code>. Can also include a 4th element to adjust the look of the minus symbol, for example to use a proper minus sign (‚àí) instead of a hyphen-minus (-). Can also include a 5th element that should be a space-like symbol (e.g., an &lsquo;en space&rsquo;) that is used in place of numbers (only relevant when trying to line up numbers exactly). For example, <code>annosym=c(" [", ", ", "]", "\u2212", "\u2002")</code> would use a proper minus sign and an &lsquo;en space&rsquo; for the annotations.</p>
</dd>
<dt>tabfig</dt><dd><p>single numeric value (either a 1, 2, or 3) to set <code>annosym</code> automatically to a vector that will exactly align the numbers in the annotations when using a font that provides &lsquo;tabular figures&rsquo;. Value 1 corresponds to using <code>"\u2212"</code> (a minus) and <code>"\u2002"</code> (an &lsquo;en space&rsquo;) in <code>annoyym</code> as shown above. Value 2 corresponds to <code>"\u2013"</code> (an &lsquo;en dash&rsquo;) and <code>"\u2002"</code> (an &lsquo;en space&rsquo;). Value 3 corresponds to <code>"\u2212"</code> (a minus) and <code>"\u2007"</code> (a &lsquo;figure space&rsquo;). The appropriate value for this argument depends on the font used. For example, for fonts Calibri and Carlito, 1 or 2 should work; for fonts Source Sans 3 and Palatino Linotype, 1, 2, and 3 should all work; for Computer/Latin Modern and Segoe UI, 2 should work; for Lato, Roboto, and Open Sans (and maybe Arial), 3 should work. Other fonts may work as well, but this is untested.</p>
</dd>
<dt>textpos</dt><dd><p>numeric vector of length 2 to specify the placement of the study labels and the annotations. The default is to use the horizontal limits of the plot region, i.e., the study labels to the right of <code>xlim[1]</code> and the annotations to the left of <code>xlim[2]</code>.</p>
</dd>
<dt>rowadj</dt><dd><p>numeric vector of length 3 to vertically adjust the position of the study labels, the annotations, and the extra information (if specified via <code>ilab</code>). This is useful for fine-tuning the position of text added with different positional alignments (i.e., argument <code>pos</code> in the <code><a href="graphics.html#topic+text">text</a></code> function).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Lewis, S., &amp; Clarke, M. (2001). Forest plots: Trying to see the wood and the trees. <em>British Medical Journal</em>, <b>322</b>(7300), 1479&ndash;1480. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.322.7300.1479&#8288;</code>
</p>
<p>Riley, R. D., Higgins, J. P. T., &amp; Deeks, J. J. (2011). Interpretation of random effects meta-analyses. <em>British Medical Journal</em>, <b>342</b>, d549. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.d549&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for an overview of the various <code>forest</code> functions and <code><a href="#topic+forest.default">forest.default</a></code> for the function draw forest plots without a summary polygon.
</p>
<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which forest plots can be drawn.
</p>
<p><code><a href="#topic+addpoly">addpoly</a></code> for a function to add polygons to forest plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           slab=paste(author, year, sep=", "))

### default forest plot of the log risk ratios and summary estimate
forest(res, header=TRUE)

### summary estimate in row -1; studies in rows k=13 through 1; horizontal
### lines in rows 0 and k+1; two extra lines of space at the top for headings,
### and other annotations; headings (if requested) in line k+2
op &lt;- par(xpd=TRUE)
text(x=-8.1, y=-1:16, -1:16, pos=4, cex=0.6, col="red")
par(op)

### can also inspect defaults chosen
defaults &lt;- forest(res)
defaults

### several forest plots illustrating the use of various arguments
forest(res, cex=0.8)
forest(res, cex=0.8, addpred=TRUE)
forest(res, cex=0.8, alim=c(-3,3))
forest(res, cex=0.8, alim=c(-3,3), order="prec")
forest(res, cex=0.8, alim=c(-3,3), order="obs")
forest(res, cex=0.8, alim=c(-3,3), order=ablat)

### adjust xlim values to see how that changes the plot
forest(res)
par("usr")[1:2] # this shows what xlim values were chosen by default
forest(res, xlim=c(-16,14))
forest(res, xlim=c(-18,10))
forest(res, xlim=c(-10,12))

### illustrate transf argument
forest(res, transf=exp, at=0:7, xlim=c(-8,12), cex=0.8, refline=1, header=TRUE)

### illustrate atransf argument
forest(res, atransf=exp, at=log(c(0.05,0.25,1,4,20)), xlim=c(-8,7), cex=0.8, header=TRUE)

### showweights argument
forest(res, atransf=exp, at=log(c(0.05,0.25,1,4,20)), xlim=c(-8,8),
       order="prec", showweights=TRUE, cex=0.8)

### illustrade shade argument
forest(res, header=TRUE, shade="zebra")
forest(res, header=TRUE, shade=year &gt;= 1970)
forest(res, header=TRUE, shade=c(1,5,10))

### forest plot with extra annotations
### note: may need to widen plotting device to avoid overlapping text
forest(res, atransf=exp, at=log(c(0.05, 0.25, 1, 4)), xlim=c(-16,6),
       ilab=cbind(tpos, tneg, cpos, cneg), ilab.xpos=c(-9.5,-8,-6,-4.5),
       cex=0.75, header="Author(s) and Year")
op &lt;- par(cex=0.75, font=2)
text(c(-9.5,-8,-6,-4.5), res$k+2, c("TB+", "TB-", "TB+", "TB-"))
text(c(-8.75,-5.25),     res$k+3, c("Vaccinated", "Control"))
par(op)

### mixed-effects model with absolute latitude as moderator
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, mods = ~ ablat,
           data=dat.bcg, slab=paste(author, year, sep=", "))

### forest plot with observed and fitted values
forest(res, xlim=c(-9,5), order="fit", cex=0.8, ilab=ablat,
       ilab.xpos=-4, atransf=exp, at=log(c(0.05,0.25,1,4)),
       header="Author(s) and Year")
text(-4, res$k+2, "Latitude", cex=0.8, font=2)

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg,
           slab=paste(author, year, sep=", "))

### for more complicated plots, the ylim and rows arguments may be useful
forest(res)
forest(res, ylim=c(-1.5, 16)) # the default
forest(res, ylim=c(-1.5, 20)) # extra space in plot
forest(res, ylim=c(-1.5, 20), rows=c(17:15, 12:6, 3:1)) # set positions

### forest plot with subgrouping of studies
### note: may need to widen plotting device to avoid overlapping text
tmp &lt;- forest(res, xlim=c(-16, 4.6), at=log(c(0.05, 0.25, 1, 4)), atransf=exp,
              ilab=cbind(tpos, tneg, cpos, cneg), ilab.xpos=c(-9.5,-8,-6,-4.5),
              cex=0.75, ylim=c(0.5, 21),
              order=alloc, rows=c(1:2,5:11,14:17),
              header="Author(s) and Year", shade=c(3,12,18))
op &lt;- par(cex=0.75, font=2)
text(c(-9.5,-8,-6,-4.5), tmp$ylim[2]-1, c("TB+", "TB-", "TB+", "TB-"))
text(c(-8.75,-5.25),     tmp$ylim[2],   c("Vaccinated", "Control"))
par(font=4)
text(-16, c(18,12,3), c("Systematic Allocation", "Random Allocation",
                        "Alternate Allocation"), pos=4)
par(op)

### see also the addpoly.rma function for an example where summaries
### for the three subgroups are added to such a forest plot

### illustrate the efac argument
forest(res, header=TRUE)
forest(res, header=TRUE, efac=c(0,1))

### illustrate use of olim argument with a meta-analysis of raw correlation
### coefficients (data from Pritz, 1997); without olim=c(0,1), some of the
### CIs would have upper bounds larger than 1
dat &lt;- escalc(measure="PR", xi=xi, ni=ni, data=dat.pritz1997)
res &lt;- rma(yi, vi, data=dat, slab=paste0(study, ") ", authors))
forest(res, xlim=c(-0.8,1.6), alim=c(0,1), psize=1, refline=coef(res), olim=c(0,1), header=TRUE)

### an example of a forest plot where the data have a multilevel structure and
### we want to reflect this by grouping together estimates from the same cluster
dat &lt;- dat.konstantopoulos2011
res &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat,
              slab=paste0("District ", district, ", School: ", school))
dd &lt;- c(0,diff(dat$district))
dd[dd &gt; 0] &lt;- 1
rows &lt;- (1:res$k) + cumsum(dd)
op &lt;- par(tck=-0.01, mgp = c(1.6,0.2,0), mar=c(3,8,1,6))
forest(res, cex=0.5, header=TRUE, rows=rows, ylim=c(0.5,max(rows)+3))
abline(h = rows[c(1,diff(rows)) == 2] - 1, lty="dotted")
par(op)
</code></pre>

<hr>
<h2 id='formatters'>Formatter Functions</h2><span id='topic+formatters'></span><span id='topic+fmtp'></span><span id='topic+fmtx'></span><span id='topic+fmtt'></span>

<h3>Description</h3>

<p>Functions to format various types of outputs. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fmtp(p, digits=4, pname="", equal=FALSE, sep=FALSE, add0=FALSE, quote=FALSE)
fmtx(x, digits=4, flag="", quote=FALSE, ...)
fmtt(val, tname, df, df1, df2, pval, digits=4,
     pname="p-val", format=1, sep=TRUE, quote=FALSE, call=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<p><em>Arguments for <code>fmtp</code>:</em>
</p>
<table>
<tr><td><code id="formatters_+3A_p">p</code></td>
<td>
<p>vector of p-values to be formatted.</p>
</td></tr>
<tr><td><code id="formatters_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the values should be rounded. For <code>fmmt</code>, can be a vector of length 2, to specify the number of digits for the test statistic and the p-value, respectively.</p>
</td></tr>
<tr><td><code id="formatters_+3A_pname">pname</code></td>
<td>
<p>string to add as a prefix to the p-value (e.g., something like <code>"p-val"</code> or just <code>"p"</code>).</p>
</td></tr>
<tr><td><code id="formatters_+3A_equal">equal</code></td>
<td>
<p>logical to specify whether an equal symbol should be shown before the p-value (when it is larger than the rounding cutoff).</p>
</td></tr>
<tr><td><code id="formatters_+3A_sep">sep</code></td>
<td>
<p>logical to specify whether a space should be added between <code>pname</code>, the equal/lesser symbol, and the p-value.</p>
</td></tr>
<tr><td><code id="formatters_+3A_add0">add0</code></td>
<td>
<p>logical to specify whether a 0 should be shown before the decimal point when the p-value is below the rounding cutoff.</p>
</td></tr>
<tr><td><code id="formatters_+3A_quote">quote</code></td>
<td>
<p>logical to specify whether formatted strings should be quoted when printed.</p>
</td></tr>
</table>
<p><em>Arguments specific for <code>fmtx</code>:</em>
</p>
<table>
<tr><td><code id="formatters_+3A_x">x</code></td>
<td>
<p>vector of numeric values to be formatted.</p>
</td></tr>
<tr><td><code id="formatters_+3A_flag">flag</code></td>
<td>
<p>a character string giving a format modifier as defined for <code><a href="base.html#topic+formatC">formatC</a></code>.</p>
</td></tr>
</table>
<p><em>Arguments specific for <code>fmtt</code>:</em>
</p>
<table>
<tr><td><code id="formatters_+3A_val">val</code></td>
<td>
<p>test statistic value to be formatted.</p>
</td></tr>
<tr><td><code id="formatters_+3A_tname">tname</code></td>
<td>
<p>character string for the name of the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_df">df</code></td>
<td>
<p>optional value for the degrees of freedom of the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_df1">df1</code></td>
<td>
<p>optional value for the numerator degrees of freedom of the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_df2">df2</code></td>
<td>
<p>optional value for the denominator degrees of freedom of the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_pval">pval</code></td>
<td>
<p>the p-value corresponding to the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_format">format</code></td>
<td>
<p>either <code>1</code> or <code>2</code> to denote whether the degrees of freedom should be given before the test statistic (in parentheses) or after the test statistic.</p>
</td></tr>
<tr><td><code id="formatters_+3A_call">call</code></td>
<td>
<p>logical to specify whether the formatted test result should be returned as a call or not.</p>
</td></tr>
<tr><td><code id="formatters_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>fmtp</code> function takes one or multiple p-values as input and rounds them to the chosen number of digits. For p-values that are smaller than <code>10^(-digits)</code> (e.g., <code>0.0001</code> for <code>digits=4</code>), the value is shown to fall below this bound (e.g., <code>&lt;.0001</code>).
</p>
<p>The <code>fmtx</code> function takes one or multiple numeric values as input and rounds them to the chosen number of digits, without using scientific notation and without dropping trailing zeros (using <code><a href="base.html#topic+formatC">formatC</a></code>).
</p>
<p>The <code>fmtt</code> function takes a single test statistic value as input (and, if applicable, its degrees of freedom via argument <code>df</code> or its numerator and denominator degrees of freedom via arguments <code>df1</code> and <code>df2</code>) and the corresponding p-value and formats it for printing. Two different formats are available (chosen via the <code>format</code> argument), one giving the degrees of freedom before the test statistic (in parentheses) and one after the test statistic.
</p>


<h3>Value</h3>

<p>A character vector with the formatted values. By default (i.e., when <code>quote=FALSE</code>), formatted strings are not quoted when printed.
</p>


<h3>Note</h3>

<p>The option in <code>fmtt</code> to return the formatted test result as a call can be useful when adding the output to a plot with <code><a href="graphics.html#topic+text">text</a></code> and one would like to use <code><a href="grDevices.html#topic+plotmath">plotmath</a></code> formatting for <code>tname</code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># examples for fmtp()
fmtp(c(.0002, .00008), quote=TRUE, digits=4)
fmtp(c(.0002, .00008), quote=TRUE, digits=4, equal=TRUE)
fmtp(c(.0002, .00008), quote=TRUE, digits=4, equal=TRUE, sep=TRUE)
fmtp(c(.0002, .00008), quote=TRUE, digits=4, equal=TRUE, sep=TRUE, add0=TRUE)

# examples for fmtx()
fmtx(c(1.0002, 2.00008, 3.00004), digits=4)
fmtx(c(-1, 1), digits=4)
fmtx(c(-1, 1), digits=4, flag=" ")

# examples for fmtt()
fmtt(2.45, "z", pval=0.01429, digits=2)
fmtt(3.45, "z", pval=0.00056, digits=2)
fmtt(2.45, "t", df=23, pval=0.02232, digits=2)
fmtt(3.45, "t", df=23, pval=0.00218, digits=2)
fmtt(3.45, "t", df=23, pval=0.00218, digits=2, format=2)
fmtt(46.23, "Q", df=29, pval=0.0226, digits=2)
fmtt(46.23, "Q", df=29, pval=0.0226, digits=2, format=2)
fmtt(8.75, "F", df1=2, df2=35, pval=0.00083, digits=c(2,3))
fmtt(8.75, "F", df1=2, df2=35, pval=0.00083, digits=c(2,3), format=2, pname="p")
fmtt(8.75, "F", df1=2, df2=35, pval=0.00083, digits=c(2,3), format=2, pname="p", sep=FALSE)
</code></pre>

<hr>
<h2 id='formula.rma'>Extract the Model Formula from 'rma' Objects</h2><span id='topic+formula'></span><span id='topic+formula.rma'></span>

<h3>Description</h3>

<p>Function to extract the model formula from objects of class <code>"rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
formula(x, type="mods", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="formula.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="formula.rma_+3A_type">type</code></td>
<td>
<p>the formula which should be returned; either <code>"mods"</code> (default), <code>"yi"</code> (in case argument <code>yi</code> was used to specify a formula), or <code>"scale"</code> (only for location-scale models).</p>
</td></tr>
<tr><td><code id="formula.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The requested formula.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which a model formula can be extracted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy BCG vaccine data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat,
              slab=paste(author, ", ", year, sep=""))

### mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ ablat + alloc, data=dat)
formula(res, type="mods")

### specify moderators via 'yi' argument
res &lt;- rma(yi ~ ablat + alloc, vi, data=dat)
formula(res, type="yi")
</code></pre>

<hr>
<h2 id='fsn'>Fail-Safe N Analysis (File Drawer Analysis)</h2><span id='topic+fsn'></span>

<h3>Description</h3>

<p>Function to compute the fail-safe N (also called a file drawer analysis). <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsn(x, vi, sei, subset, data, type, alpha=.05, target,
    method, exact=FALSE, verbose=FALSE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsn_+3A_x">x</code></td>
<td>
<p>a vector with the observed effect sizes or outcomes or an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="fsn_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="fsn_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="fsn_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be used for the calculation (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="fsn_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="fsn_+3A_type">type</code></td>
<td>
<p>optional character string to specify the type of method to use for the calculation of the fail-safe N. Possible options are <code>"Rosenthal"</code> (the default when <code>x</code> is a vector with the observed effect sizes or outcomes), <code>"Orwin"</code>, <code>"Rosenberg"</code>, or <code>"General"</code> (the default when <code>x</code> is an object of class <code>"rma"</code>). Can be abbreviated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="fsn_+3A_alpha">alpha</code></td>
<td>
<p>target alpha level for the Rosenthal, Rosenberg, and General methods (the default is .05).</p>
</td></tr>
<tr><td><code id="fsn_+3A_target">target</code></td>
<td>
<p>target average effect size or outcome for the Orwin and General methods.</p>
</td></tr>
<tr><td><code id="fsn_+3A_method">method</code></td>
<td>
<p>optional character string to specify the model fitting method for <code>type="General"</code> (if unspecified, either <code>"REML"</code> by default or the method that was used in fitting the <code>"rma"</code> model). See <code><a href="#topic+rma.uni">rma.uni</a></code> for options.</p>
</td></tr>
<tr><td><code id="fsn_+3A_exact">exact</code></td>
<td>
<p>logical to indicate whether the general method should be based on exact (but slower) or approximate (but faster) calculations.</p>
</td></tr>
<tr><td><code id="fsn_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the calculations for <code>type="General"</code> (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="fsn_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded.</p>
</td></tr>
<tr><td><code id="fsn_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to calculate the &lsquo;fail-safe N&rsquo;, that is, the minimum number of studies averaging null results that would have to be added to a given set of \(k\) studies to change the conclusion of a meta-analysis. If this number is small (in relation to the actual number of studies), then this indicates that the results based on the observed studies are not robust to publication bias (of the form assumed by the method, that is, where a set of studies averaging null results is missing). The method is also called a &lsquo;file drawer analysis&rsquo; as it assumes that there is a set of studies averaging null results hiding in file drawers, which can overturn the findings from a meta-analysis. There are various types of methods that are all based on the same principle, which are described in more detail further below. Note that <em>the fail-safe N is not an estimate of the number of missing studies</em>, only how many studies must be hiding in file drawers for the findings to be overturned.
</p>
<p>One can either pass a vector with the observed effect sizes or outcomes (via <code>x</code>) and the corresponding sampling variances via <code>vi</code> (or the standard errors via <code>sei</code>) to the function or an object of class <code>"rma"</code>. When passing a model object, the model must be a model without moderators (i.e., either an equal- or a random-effects model).
</p>


<h4>Rosenthal Method</h4>

<p>The Rosenthal method (<code>type="Rosenthal"</code>) calculates the minimum number of studies averaging null results that would have to be added to a given set of studies to reduce the (one-tailed) combined significance level (i.e., p-value) to a particular alpha level, which can be specified via the <code>alpha</code> argument (.05 by default). The calculation is based on Stouffer's method for combining p-values and is described in Rosenthal (1979). Note that the method is primarily of interest for historical reasons, but the other methods described below are more closely aligned with the way meta-analyses are typically conducted in practice.
</p>



<h4>Orwin Method</h4>

<p>The Orwin method (<code>type="Orwin"</code>) calculates the minimum number of studies averaging null results that would have to be added to a given set of studies to reduce the (unweighted or weighted) average effect size / outcome to a target value (as specified via the <code>target</code> argument). The method is described in Orwin (1983). When <code>vi</code> (or <code>sei</code>) is not specified, the method is based on the unweighted average of the effect sizes / outcomes; otherwise, the method uses the inverse-variance weighted average. If the <code>target</code> argument is not specified, then the target value will be equal to the observed average effect size / outcome divided by 2 (which is entirely arbitrary and will always lead to a fail-safe N number that is equal to \(k\)). One should really set <code>target</code> to a value that reflects an effect size / outcome that would be considered to be practically irrelevant. Note that if <code>target</code> has the opposite sign as the actually observed average, then its sign is automatically flipped.
</p>



<h4>Rosenberg Method</h4>

<p>The Rosenberg method (<code>type="Rosenberg"</code>) calculates the minimum number of studies averaging null results that would have to be added to a given set of studies to reduce the significance level (i.e., p-value) of the average effect size / outcome (as estimated based on an equal-effects model) to a particular alpha level, which can be specified via the <code>alpha</code> argument (.05 by default). The method is described in Rosenberg (2005). Note that the p-value is calculated based on a standard normal distribution (instead of a t-distribution, as suggested by Rosenberg, 2005), but the difference is typically negligible.
</p>



<h4>General Method</h4>

<p>This method is a generalization of the methods by Orwin and Rosenberg. By default (i.e., when <code>target</code> is not specified), it calculates the minimum number of studies averaging null results that would have to be added to a given set of studies to reduce the significance level (i.e., p-value) of the average effect size / outcome (as estimated based on a chosen model) to a particular alpha level, which can be specified via the <code>alpha</code> argument (.05 by default). The type of model that is used in the calculation is chosen via the <code>method</code> argument. If this is unspecified, then a random-effects model is automatically used (using <code>method="REML"</code>) or the method that was used in fitting the <code>"rma"</code> model (see <code><a href="#topic+rma.uni">rma.uni</a></code> for options). Therefore, when setting <code>method="EE"</code>, then an equal-effects model is used, which yields (essentially) identical results as Rosenberg's method.
</p>
<p>If <code>target</code> is specified, then the method calculates the minimum number of studies averaging null results that would have to be added to a given set of studies to reduce the average effect size / outcome (as estimated based on a chosen model) to a target value (as specified via the <code>target</code> argument). As described above, the type of model that is used in the calculation is chosen via the <code>method</code> argument. When setting <code>method="EE"</code>, then an equal-effects model is used, which yields (essentially) identical results as Orwin's method with inverse-variance weights.
</p>
<p>The method uses an iterative algorithm for calculating the fail-safe N, which can be computationally expensive especially when N is large. By default, the method uses approximate (but faster) calculations, but when setting <code>exact=TRUE</code>, the method uses exact (but slower) calculations. The difference between the two is typically negligible. If N is larger than \(10^7\), then the calculated number is given as <code>&gt;1e+07</code>.
</p>



<h3>Value</h3>

<p>An object of class <code>"fsn"</code>. The object is a list containing the following components (some of which may be <code>NA</code> if they are not applicable to the chosen method):
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>the type of method used.</p>
</td></tr>
<tr><td><code>fsnum</code></td>
<td>
<p>the calculated fail-safe N.</p>
</td></tr>
<tr><td><code>est</code></td>
<td>
<p>the average effect size / outcome based on the observed studies.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>the estimated amount of heterogeneity based on the observed studies.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>the p-value of the observed results.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the specified target alpha level.</p>
</td></tr>
<tr><td><code>target</code></td>
<td>
<p>the target average effect size / outcome.</p>
</td></tr>
<tr><td><code>est.fsn</code></td>
<td>
<p>the average effect size / outcome when combining the observed studies with those in the file drawer.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>the estimated amount of heterogeneity when combining the observed studies with those in the file drawer.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>the p-value when combining the observed studies with those in the file drawer.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.fsn">print</a></code> function.
</p>


<h3>Note</h3>

<p>If the significance level of the observed studies is already above the specified alpha level or if the average effect size / outcome of the observed studies is already below the target average effect size / outcome, then the fail-safe N value is zero.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Rosenthal, R. (1979). The &quot;file drawer problem&quot; and tolerance for null results. <em>Psychological Bulletin</em>, <b>86</b>(3), 638&ndash;641. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.86.3.638&#8288;</code>
</p>
<p>Orwin, R. G. (1983). A fail-safe N for effect size in meta-analysis. <em>Journal of Educational Statistics</em>, <b>8</b>(2), 157&ndash;159. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986008002157&#8288;</code>
</p>
<p>Rosenberg, M. S. (2005). The file-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta-analysis. <em>Evolution</em>, <b>59</b>(2), 464&ndash;468. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.0014-3820.2005.tb01004.x&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regtest">regtest</a></code> for the regression test, <code><a href="#topic+ranktest">ranktest</a></code> for the rank correlation test, <code><a href="#topic+trimfill">trimfill</a></code> for the trim and fill method, <code><a href="#topic+tes">tes</a></code> for the test of excess significance, and <code><a href="#topic+selmodel">selmodel</a></code> for selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit equal-effects model
rma(yi, vi, data=dat, method="EE")

### fail-safe N computations
fsn(yi, vi, data=dat)
fsn(yi, data=dat, type="Orwin", target=log(0.95)) # target corresponds to a 5% risk reduction
fsn(yi, vi, data=dat, type="Orwin", target=log(0.95)) # Orwin's method with 1/vi weights
fsn(yi, vi, data=dat, type="General", target=log(0.95), method="EE") # like Orwin's method
fsn(yi, vi, data=dat, type="Rosenberg")
fsn(yi, vi, data=dat, type="General", method="EE") # like Rosenberg's method
fsn(yi, vi, data=dat, type="General") # based on a random-effects model
fsn(yi, vi, data=dat, type="General", target=log(0.95)) # based on a random-effects model

### fit a random-effects model and use fsn() on the model object
res &lt;- rma(yi, vi, data=dat)
fsn(res)
fsn(res, target=log(0.95))
</code></pre>

<hr>
<h2 id='funnel'>Funnel Plots</h2><span id='topic+funnel'></span><span id='topic+funnel.rma'></span><span id='topic+funnel.default'></span>

<h3>Description</h3>

<p>Function to create funnel plots. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funnel(x, ...)

## S3 method for class 'rma'
funnel(x, yaxis="sei",
       xlim, ylim, xlab, ylab, slab,
       steps=5, at, atransf, targs, digits, level=x$level,
       addtau2=FALSE, type="rstandard",
       back, shade, hlines,
       refline, lty=3, pch, pch.fill, col, bg,
       label=FALSE, offset=0.4, legend=FALSE, ...)

## Default S3 method:
funnel(x, vi, sei, ni, subset, yaxis="sei",
       xlim, ylim, xlab, ylab, slab,
       steps=5, at, atransf, targs, digits, level=95,
       back, shade, hlines,
       refline=0, lty=3, pch, col, bg,
       label=FALSE, offset=0.4, legend=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funnel_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code> or a vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="funnel_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances (needed if <code>x</code> is a vector with the observed effect sizes or outcomes).</p>
</td></tr>
<tr><td><code id="funnel_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="funnel_+3A_ni">ni</code></td>
<td>
<p>vector with the corresponding sample sizes. Only relevant when passing a vector via <code>x</code>.</p>
</td></tr>
<tr><td><code id="funnel_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the plot. Only relevant when passing a vector via <code>x</code>.</p>
</td></tr>
<tr><td><code id="funnel_+3A_yaxis">yaxis</code></td>
<td>
<p>either <code>"sei"</code>, <code>"vi"</code>, <code>"seinv"</code>, <code>"vinv"</code>, <code>"ni"</code>, <code>"ninv"</code>, <code>"sqrtni"</code>, <code>"sqrtninv"</code>, <code>"lni"</code>, or <code>"wi"</code> to indicate what values should be placed on the y-axis. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="funnel_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="funnel_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="funnel_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="funnel_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies. If unspecified, the function tries to extract study labels from <code>x</code>.</p>
</td></tr>
<tr><td><code id="funnel_+3A_steps">steps</code></td>
<td>
<p>the number of tick marks for the y-axis (the default is 5).</p>
</td></tr>
<tr><td><code id="funnel_+3A_at">at</code></td>
<td>
<p>position of the x-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="funnel_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the x-axis labels (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="funnel_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="funnel_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the tick mark labels of the x- and y-axis should be rounded. Can also be a vector of two integers, the first to specify the number of decimal places for the x-axis, the second for the y-axis labels (e.g., <code>digits=c(2,3)</code>). If unspecified, the function tries to set the argument to some sensible values.</p>
</td></tr>
<tr><td><code id="funnel_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the level of the pseudo confidence interval region (see <a href="#topic+misc-options">here</a> for details). For <code>"rma"</code> objects, the default is to take the value from the object. May also be a vector of values to obtain multiple regions. See &lsquo;Examples&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_addtau2">addtau2</code></td>
<td>
<p>logical to indicate whether the amount of heterogeneity should be accounted for when drawing the pseudo confidence interval region (the default is <code>FALSE</code>). Ignored when <code>x</code> is a meta-regression model and residuals are plotted. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_type">type</code></td>
<td>
<p>either <code>"rstandard"</code> (default) or <code>"rstudent"</code> to specify whether the usual or deleted residuals should be used in creating the funnel plot when <code>x</code> is a meta-regression model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_back">back</code></td>
<td>
<p>optional character string to specify the color of the plotting region background.</p>
</td></tr>
<tr><td><code id="funnel_+3A_shade">shade</code></td>
<td>
<p>optional character string to specify the color of the pseudo confidence interval region. When <code>level</code> is a vector of values, different shading colors can be specified for each region.</p>
</td></tr>
<tr><td><code id="funnel_+3A_hlines">hlines</code></td>
<td>
<p>optional character string to specify the color of the horizontal reference lines.</p>
</td></tr>
<tr><td><code id="funnel_+3A_refline">refline</code></td>
<td>
<p>numeric value to specify the location of the vertical &lsquo;reference&rsquo; line and where the pseudo confidence interval should be centered. If unspecified, the reference line is drawn at the equal- or random-effects model estimate and at zero for meta-regression models (in which case the residuals are plotted) or when directly plotting observed outcomes.</p>
</td></tr>
<tr><td><code id="funnel_+3A_lty">lty</code></td>
<td>
<p>line type for the pseudo confidence interval region and the reference line. The default is to draw dotted lines (see <code><a href="graphics.html#topic+par">par</a></code> for other options). Can also be a vector to specify the two line types separately.</p>
</td></tr>
<tr><td><code id="funnel_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the observed outcomes. By default, a filled circle is used. Can also be a vector of values. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="funnel_+3A_pch.fill">pch.fill</code></td>
<td>
<p>plotting symbol to use for the outcomes filled in by the trim and fill method. By default, an open circle is used. Only relevant when plotting an object created by the <code><a href="#topic+trimfill">trimfill</a></code> function.</p>
</td></tr>
<tr><td><code id="funnel_+3A_col">col</code></td>
<td>
<p>optional character string to specify the (border) color of the points. Can also be a vector.</p>
</td></tr>
<tr><td><code id="funnel_+3A_bg">bg</code></td>
<td>
<p>optional character string to specify the background color of open plot symbols. Can also be a vector.</p>
</td></tr>
<tr><td><code id="funnel_+3A_label">label</code></td>
<td>
<p>argument to control the labeling of the points (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_offset">offset</code></td>
<td>
<p>argument to control the distance between the points and the corresponding labels.</p>
</td></tr>
<tr><td><code id="funnel_+3A_legend">legend</code></td>
<td>
<p>logical to indicate whether a legend should be added to the plot (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="funnel_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For equal- and random-effects models (i.e., models not involving moderators), the plot shows the observed effect sizes or outcomes on the x-axis against the corresponding standard errors (i.e., the square root of the sampling variances) on the y-axis. A vertical line indicates the estimate based on the model. A pseudo confidence interval region is drawn around this value with bounds equal to \(\pm 1.96 \mbox{SE}\), where \(\mbox{SE}\) is the standard error value from the y-axis (assuming <code>level=95</code>). If <code>addtau2=TRUE</code> (only for models of class <code>"rma.uni"</code>), then the bounds of the pseudo confidence interval region are equal to \(\pm 1.96 \sqrt{\mbox{SE}^2 + \hat{\tau}^2}\), where \(\hat{\tau}^2\) is the amount of heterogeneity as estimated by the model.
</p>
<p>For (mixed-effects) meta-regression models (i.e., models involving moderators), the plot shows the residuals on the x-axis against their corresponding standard errors. Either the usual or deleted residuals can be used for that purpose (set via the <code>type</code> argument). See <code><a href="#topic+residuals.rma">residuals</a></code> for more details on the different types of residuals.
</p>
<p>With the <code>atransf</code> argument, the labels on the x-axis can be transformed with some suitable function. For example, when plotting log odds ratios, one could use <code>transf=exp</code> to obtain a funnel plot with the values on the x-axis corresponding to the odds ratios. See also <a href="#topic+transf">transf</a> for some other useful transformation functions in the context of a meta-analysis.
</p>
<p>Instead of placing the standard errors on the y-axis, several other options are available by setting the <code>yaxis</code> argument to:
</p>

<ul>
<li> <p><code>yaxis="vi"</code> for the sampling variances,
</p>
</li>
<li> <p><code>yaxis="seinv"</code> for the inverse of the standard errors,
</p>
</li>
<li> <p><code>yaxis="vinv"</code> for the inverse of the sampling variances,
</p>
</li>
<li> <p><code>yaxis="ni"</code> for the sample sizes,
</p>
</li>
<li> <p><code>yaxis="ninv"</code> for the inverse of the sample sizes,
</p>
</li>
<li> <p><code>yaxis="sqrtni"</code> for the square root of the sample sizes,
</p>
</li>
<li> <p><code>yaxis="sqrtninv"</code> for the inverse square root of the sample sizes,
</p>
</li>
<li> <p><code>yaxis="lni"</code> for the log of the sample sizes,
</p>
</li>
<li> <p><code>yaxis="wi"</code> for the weights.
</p>
</li></ul>

<p>However, only when <code>yaxis="sei"</code> (the default) will the pseudo confidence region have the expected (upside-down) funnel shape with straight lines. Also, when placing (a function of) the sample sizes on the y-axis or the weights, then the pseudo confidence region cannot be drawn. See Sterne and Egger (2001) for more details on the choice of the y-axis.
</p>
<p>If the object passed to the function comes from the <code><a href="#topic+trimfill">trimfill</a></code> function, the studies that are filled in by the trim and fill method are also added to the funnel plot. The symbol to use for plotting the filled in studies can be specified via the <code>pch.fill</code> argument. Arguments <code>col</code> and <code>bg</code> can then be of length 2 to specify the (border) color and background color of the observed and filled in studies.
</p>
<p>One can also directly pass a vector with the observed effect sizes or outcomes (via <code>x</code>) and the corresponding sampling variances (via <code>vi</code>), standard errors (via <code>sei</code>), and/or sample sizes (via <code>ni</code>) to the function. By default, the vertical reference line is then drawn at zero.
</p>
<p>The arguments <code>back</code>, <code>shade</code>, and <code>hlines</code> can be set to <code>NULL</code> to suppress the shading and the horizontal reference line. One can also suppress the funnel by setting <code>refline</code> to <code>NULL</code>.
</p>
<p>With the <code>label</code> argument, one can control whether points in the plot will be labeled. If <code>label="all"</code> (or <code>label=TRUE</code>), all points in the plot will be labeled. If <code>label="out"</code>, points falling outside of the pseudo confidence region will be labeled. Finally, one can also set this argument to a numeric value (between 1 and \(k\)) to specify how many of the most extreme points should be labeled (e.g., with <code>label=1</code> only the most extreme point are labeled, while with <code>label=3</code>, the most extreme, and the second and third most extreme points are labeled). With the <code>offset</code> argument, one can adjust the distance between the labels and the corresponding points.
</p>
<p>By setting the <code>legend</code> argument to <code>TRUE</code>, a legend is added to the plot. One can also specify a keyword for this argument to indicate the position of the legend (e.g., <code>legend="topleft"</code>; see <code><a href="graphics.html#topic+legend">legend</a></code> for options). Finally, this argument can also be a list, with elements <code>x</code>, <code>y</code>, <code>inset</code>, <code>bty</code>, and <code>bg</code>, which are passed on as the corresponding arguments to the <code><a href="graphics.html#topic+legend">legend</a></code> function for even more control (elements not specified are set to defaults). The list can also include elements <code>studies</code> (a logical to indicate whether to include &lsquo;Studies&rsquo; in the legend; default is <code>TRUE</code>) and <code>show</code> (either <code>"pvals"</code> to show the p-values corresponding to the shade regions, <code>"cis"</code> to show the confidence interval levels corresponding to the shade regions, or <code>NA</code> to show neither; default is <code>"pvals"</code>).
</p>


<h3>Value</h3>

<p>A data frame with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>slab</code></td>
<td>
<p>the study labels.</p>
</td></tr>
</table>
<p>Note that the data frame is returned invisibly.
</p>


<h3>Note</h3>

<p>Placing (a function of) the sample sizes on the y-axis (i.e., using <code>yaxis="ni"</code>, <code>yaxis="ninv"</code>, <code>yaxis="sqrtni"</code>, <code>yaxis="sqrtninv"</code>, or <code>yaxis="lni"</code>) is only possible when information about the sample sizes is actually stored within the object passed to the <code>funnel</code> function. That should automatically be the case when the observed effect sizes or outcomes were computed with the <code><a href="#topic+escalc">escalc</a></code> function or when the observed effect sizes or outcomes were computed within the model fitting function. On the other hand, this will not be the case when <code><a href="#topic+rma.uni">rma.uni</a></code> was used together with the <code>yi</code> and <code>vi</code> arguments and the <code>yi</code> and <code>vi</code> values were <em>not</em> computed with <code><a href="#topic+escalc">escalc</a></code>. In that case, it is still possible to pass information about the sample sizes to the <code><a href="#topic+rma.uni">rma.uni</a></code> function (e.g., use <code>rma.uni(yi, vi, ni=ni, data=dat)</code>, where data frame <code>dat</code> includes a variable called <code>ni</code> with the sample sizes).
</p>
<p>When using unweighted estimation, using <code>yaxis="wi"</code> will place all points on a horizontal line. When directly passing a vector with the observed effect sizes or outcomes to the function, <code>yaxis="wi"</code> is equivalent to <code>yaxis="vinv"</code>, except that the weights are expressed in percent.
</p>
<p>Argument <code>slab</code> and when specifying vectors for arguments <code>pch</code>, <code>col</code>, and/or <code>bg</code> and <code>x</code> is an object of class <code>"rma"</code>, the variables specified are assumed to be of the same length as the data passed to the model fitting function (and if the <code>data</code> argument was used in the original model fit, then the variables will be searched for within this data frame first). Any subsetting and removal of studies with missing values is automatically applied to the variables specified via these arguments.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Light, R. J., &amp; Pillemer, D. B. (1984). <em>Summing up: The science of reviewing research</em>. Cambridge, MA: Harvard University Press.
</p>
<p>Peters, J. L., Sutton, A. J., Jones, D. R., Abrams, K. R., &amp; Rushton, L. (2008). Contour-enhanced meta-analysis funnel plots help distinguish publication bias from other causes of asymmetry. <em>Journal of Clinical Epidemiology</em>, <b>61</b>(10), 991&ndash;996. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.jclinepi.2007.11.010&#8288;</code>
</p>
<p>Sterne, J. A. C., &amp; Egger, M. (2001). Funnel plots for detecting bias in meta-analysis: Guidelines on choice of axis. <em>Journal of Clinical Epidemiology</em>, <b>54</b>(10), 1046&ndash;1055. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/s0895-4356(01)00377-8&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which funnel plots can be drawn.
</p>
<p><code><a href="#topic+trimfill">trimfill</a></code> for the trim and fill method, <code><a href="#topic+regtest">regtest</a></code> for the regression test, and <code><a href="#topic+ranktest">ranktest</a></code> for the rank correlation test.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy BCG vaccine data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat, slab=paste(author, year, sep=", "))

### draw a standard funnel plot
funnel(res)

### show risk ratio values on x-axis (log scale)
funnel(res, atransf=exp)

### label points outside of the pseudo confidence interval region
funnel(res, atransf=exp, label="out")

### passing log risk ratios and sampling variances directly to the function
### note: same plot, except that the reference line is centered at zero
funnel(dat$yi, dat$vi)

### the with() function can be used to avoid having to retype dat$... over and over
with(dat, funnel(yi, vi))

### can accomplish the same thing by setting refline=0
funnel(res, refline=0)

### adjust the position of the x-axis labels, number of digits, and y-axis limits
funnel(res, atransf=exp, at=log(c(.125, .25, .5, 1, 2)), digits=3L, ylim=c(0,.8))

### contour-enhanced funnel plot centered at 0 (see Peters et al., 2008)
funnel(res, level=c(90, 95, 99), shade=c("white", "gray55", "gray75"), refline=0, legend=TRUE)

### same, but show risk ratio values on the x-axis and some further adjustments
funnel(res, level=c(90, 95, 99), shade=c("white", "gray55", "gray75"), digits=3L, ylim=c(0,.8),
       atransf=exp, at=log(c(.125, .25, .5, 1, 2, 4, 8)), refline=0, legend=TRUE)

### same, but show confidence interval levels in the legend
funnel(res, level=c(90, 95, 99), shade=c("white", "gray55", "gray75"), digits=3L, ylim=c(0,.8),
       atransf=exp, at=log(c(.125, .25, .5, 1, 2, 4, 8)), refline=0, legend=list(show="cis"))

### illustrate the use of vectors for 'pch' and 'col'
res &lt;- rma(yi, vi, data=dat, subset=2:10)
funnel(res, pch=ifelse(yi &gt; -1, 19, 21), col=ifelse(sqrt(vi) &gt; .3, "red", "blue"))

### can add a second funnel via (undocumented) argument refline2
funnel(res, atransf=exp, at=log(c(.125, .25, .5, 1, 2, 4)), digits=3L, ylim=c(0,.8), refline2=0)

### mixed-effects model with absolute latitude in the model
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)

### funnel plot of the residuals
funnel(res)

### simulate a large meta-analytic dataset (correlations with rho = 0.2)
### with no heterogeneity or publication bias; then try out different
### versions of the funnel plot

gencor &lt;- function(rhoi, ni) {
   x1 &lt;- rnorm(ni, mean=0, sd=1)
   x2 &lt;- rnorm(ni, mean=0, sd=1)
   x3 &lt;- rhoi*x1 + sqrt(1-rhoi^2)*x2
   cor(x1, x3)
}

set.seed(1234)
k  &lt;- 200                               # number of studies to simulate
ni &lt;- round(rchisq(k, df=2) * 20 + 20)  # simulate sample sizes (skewed distribution)
ri &lt;- mapply(gencor, rep(0.2,k), ni)    # simulate correlations

res &lt;- rma(measure="ZCOR", ri=ri, ni=ni, method="EE") # use r-to-z transformed correlations

funnel(res, yaxis="sei")
funnel(res, yaxis="vi")
funnel(res, yaxis="seinv")
funnel(res, yaxis="vinv")
funnel(res, yaxis="ni")
funnel(res, yaxis="ninv")
funnel(res, yaxis="sqrtni")
funnel(res, yaxis="sqrtninv")
funnel(res, yaxis="lni")
funnel(res, yaxis="wi")
</code></pre>

<hr>
<h2 id='gosh'>GOSH Plots for 'rma' Objects</h2><span id='topic+gosh'></span><span id='topic+gosh.rma'></span>

<h3>Description</h3>

<p>Function to create GOSH plots for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gosh(x, ...)

## S3 method for class 'rma'
gosh(x, subsets, progbar=TRUE, parallel="no", ncpus=1, cl, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gosh_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="gosh_+3A_subsets">subsets</code></td>
<td>
<p>optional integer to specify the number of subsets.</p>
</td></tr>
<tr><td><code id="gosh_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="gosh_+3A_parallel">parallel</code></td>
<td>
<p>character string to specify whether parallel processing should be used (the default is <code>"no"</code>). For parallel processing, set to either <code>"snow"</code> or <code>"multicore"</code>. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="gosh_+3A_ncpus">ncpus</code></td>
<td>
<p>integer to specify the number of processes to use in the parallel processing.</p>
</td></tr>
<tr><td><code id="gosh_+3A_cl">cl</code></td>
<td>
<p>optional cluster to use if <code>parallel="snow"</code>. If unspecified, a cluster on the local machine is created for the duration of the call.</p>
</td></tr>
<tr><td><code id="gosh_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model specified via <code>x</code> must be a model fitted with either the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, or <code><a href="#topic+rma.peto">rma.peto</a></code> functions.
</p>
<p>Olkin et al. (2012) proposed the GOSH (graphical display of study heterogeneity) plot, which is based on examining the results of an equal-effects model in all possible subsets of size \(1, \ldots, k\) of the \(k\) studies included in a meta-analysis. In a homogeneous set of studies, the model estimates obtained this way should form a roughly symmetric, contiguous, and unimodal distribution. On the other hand, when the distribution is multimodal, then this suggests the presence of heterogeneity, possibly due to outliers and/or distinct subgroups of studies. Plotting the estimates against some measure of heterogeneity (e.g., \(I^2\), \(H^2\), or the \(Q\)-statistic) can also help to reveal subclusters, which are indicative of heterogeneity. The same type of plot can be produced by first fitting an equal-effects model with either the <code><a href="#topic+rma.uni">rma.uni</a></code> (using <code>method="EE"</code>), <code><a href="#topic+rma.mh">rma.mh</a></code>, or <code><a href="#topic+rma.peto">rma.peto</a></code> functions and then passing the fitted model object to the <code>gosh</code> function and then plotting the results.
</p>
<p>For models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function (which may be random-effects or mixed-effects meta-regressions models), the idea underlying this type of plot can be generalized (Viechtbauer, 2021) by examining the distribution of all model coefficients, plotting them against each other, and against some measure of (residual) heterogeneity (including the estimate of \(\tau^2\) or its square root).
</p>
<p>Note that for models without moderators, application of the method requires fitting a total of \(2^k - 1\) models, which could be an excessively large number when \(k\) is large. For example, for \(k=10\), there are only 1023 possible subsets, but for \(k=20\), this number already grows to 1,048,575. For even larger \(k\), it may become computationally infeasible to consider all possible subsets. Instead, we can then examine (a sufficiently large number of) random subsets.
</p>
<p>By default, if the number of possible subsets is \(\le 10^6\), the function will consider all possible subsets and otherwise \(10^6\) random subsets. One can use the <code>subsets</code> argument to specify a different number of subsets to consider. If <code>subsets</code> is specified and it is actually larger than the number of possible subsets, then the function automatically only considers the possible subsets and does not use random subsets.
</p>
<p>When <code>x</code> is an equal-effects model or a random-effects model fitted using <code>method="DL"</code>, provisions have been made to speed up the model fitting to the various subsets. For random-effects models using some other estimator of \(\tau^2\) (especially an iterative one like <code>method="REML"</code>), the computations will be considerably slower.
</p>


<h3>Value</h3>

<p>An object of class <code>"gosh.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>res</code></td>
<td>
<p>a data frame with the results for each subset (various heterogeneity statistics and the model coefficient(s)).</p>
</td></tr>
<tr><td><code>incl</code></td>
<td>
<p>a matrix indicating which studies were included in which subset.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results can be printed with the <code><a href="#topic+print.gosh.rma">print</a></code> function and plotted with the <code><a href="#topic+plot.gosh.rma">plot</a></code> function.
</p>


<h3>Note</h3>

<p>On machines with multiple cores, one can try to speed things up by delegating the model fitting to separate worker processes, that is, by setting <code>parallel="snow"</code> or <code>parallel="multicore"</code> and <code>ncpus</code> to some value larger than 1. Parallel processing makes use of the <code><a href="parallel.html#topic+parallel">parallel</a></code> package, using the <code><a href="parallel.html#topic+makePSOCKcluster">makePSOCKcluster</a></code> and <code><a href="parallel.html#topic+parLapply">parLapply</a></code> functions when <code>parallel="snow"</code> or using <code><a href="parallel.html#topic+mclapply">mclapply</a></code> when <code>parallel="multicore"</code> (the latter only works on Unix/Linux-alikes). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Olkin, I., Dahabreh, I. J., &amp; Trikalinos, T. A. (2012). GOSH - a graphical display of study heterogeneity. <em>Research Synthesis Methods</em>, <b>3</b>(3), 214&ndash;223. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1053&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, and <code><a href="#topic+rma.peto">rma.peto</a></code> for functions to fit models for which GOSH plots can be drawn.
</p>
<p><code><a href="#topic+influence.rma.uni">influence</a></code> for other model diagnostics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat.egger2001)

### meta-analysis of all trials including ISIS-4 using an equal-effects model
res &lt;- rma(yi, vi, data=dat, method="EE")

### fit FE model to all possible subsets (65535 models)
## Not run: 
sav &lt;- gosh(res, progbar=FALSE)
sav

### create GOSH plot
### red points for subsets that include and blue points
### for subsets that exclude study 16 (the ISIS-4 trial)
plot(sav, out=16, breaks=100)

## End(Not run)
</code></pre>

<hr>
<h2 id='hc'>Meta-Analysis based on the Method by Henmi and Copas (2010)</h2><span id='topic+hc'></span><span id='topic+hc.rma.uni'></span>

<h3>Description</h3>

<p>Function to obtain an estimate of the average true outcome and corresponding confidence interval under a random-effects model using the method described by Henmi and Copas (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hc(object, ...)

## S3 method for class 'rma.uni'
hc(object, digits, transf, targs, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hc_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="hc_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="hc_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the estimate and the corresponding interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="hc_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="hc_+3A_control">control</code></td>
<td>
<p>list of control values for the iterative algorithm. If unspecified, default values are used. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="hc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model specified via <code>object</code> must be a model without moderators (i.e., either an equal- or a random-effects model).
</p>
<p>When using the usual method for fitting a random-effects model (i.e., weighted estimation with inverse-variance weights), the weights assigned to smaller and larger studies become more uniform as the amount of heterogeneity increases. As a consequence, the estimated average outcome could become increasingly biased under certain forms of publication bias (where smaller studies on one side of the funnel plot are missing). The method by Henmi and Copas (2010) counteracts this problem by providing an estimate of the average true outcome that is based on inverse-variance weights as used under an equal-effects model, which are not affected by the amount of heterogeneity. The amount of heterogeneity is still estimated (with the DerSimonian-Laird estimator) and incorporated into the standard error of the estimated average outcome and the corresponding confidence interval.
</p>
<p>Currently, there is only a method for handling objects of class <code>"rma.uni"</code> with the <code>hc</code> function. It therefore provides a method for conducting a sensitivity analysis after the model has been fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function.
</p>


<h3>Value</h3>

<p>An object of class <code>"hc.rma.uni"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated average true outcome.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard error.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the average true outcome.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the average true outcome.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.hc.rma.uni">print</a></code> function.
</p>


<h3>Note</h3>

<p>The method makes use of the <code><a href="stats.html#topic+uniroot">uniroot</a></code> function. By default, the desired accuracy is set equal to <code>.Machine$double.eps^0.25</code> and the maximum number of iterations to <code>1000</code>. The desired accuracy (<code>tol</code>) and the maximum number of iterations (<code>maxiter</code>) can be adjusted with the <code>control</code> argument (i.e., <code>control=list(tol=value, maxiter=value)</code>).
</p>


<h3>Author(s)</h3>

<p>Original code by Henmi and Copas (2010). Corrected for typos by Michael Dewey (<a href="mailto:lists@dewey.myzen.co.uk">lists@dewey.myzen.co.uk</a>). Incorporated into the package with some small adjustments for consistency with the other functions in the package by Wolfgang Viechtbauer (<a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a>).
</p>


<h3>References</h3>

<p>Henmi, M., &amp; Copas, J. B. (2010). Confidence intervals for random effects meta-analysis and robustness to publication bias. <em>Statistics in Medicine</em>, <b>29</b>(29), 2969&ndash;2983. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4029&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit <code>rma.uni</code> models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat.lee2004)
dat

### meta-analysis based on log odds ratios
res &lt;- rma(yi, vi, data=dat)
res

### funnel plot as in Henmi and Copas (2010)
funnel(res, yaxis="seinv", refline=0, xlim=c(-3,3), ylim=c(.5,3.5), steps=7, digits=1, back="white")

### use method by Henmi and Copas (2010) as a sensitivity analysis
hc(res)

### back-transform results to odds ratio scale
hc(res, transf=exp)
</code></pre>

<hr>
<h2 id='influence.rma.mv'>Model Diagnostics for 'rma.mv' Objects</h2><span id='topic+influence.rma.mv'></span><span id='topic+cooks.distance.rma.mv'></span><span id='topic+dfbetas.rma.mv'></span><span id='topic+hatvalues.rma.mv'></span>

<h3>Description</h3>

<p>Functions to compute various outlier and influential study diagnostics (some of which indicate the influence of deleting one study at a time on the model fit or the fitted/residual values) for objects of class <code>"rma.mv"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.mv'
cooks.distance(model, progbar=FALSE, cluster,
               reestimate=TRUE, parallel="no", ncpus=1, cl, ...)

## S3 method for class 'rma.mv'
dfbetas(model, progbar=FALSE, cluster,
        reestimate=TRUE, parallel="no", ncpus=1, cl, ...)

## S3 method for class 'rma.mv'
hatvalues(model, type="diagonal", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="influence.rma.mv_+3A_model">model</code></td>
<td>
<p>an object of class <code>"rma.mv"</code>.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_cluster">cluster</code></td>
<td>
<p>optional vector to specify a clustering variable to use for computing the Cook's distances or DFBETAS values. If unspecified, these measures are computed for the individual observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_reestimate">reestimate</code></td>
<td>
<p>logical to specify whether variance/correlation components should be re-estimated after deletion of the \(i\textrm{th}\) case (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_parallel">parallel</code></td>
<td>
<p>character string to specify whether parallel processing should be used (the default is <code>"no"</code>). For parallel processing, set to either <code>"snow"</code> or <code>"multicore"</code>. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_ncpus">ncpus</code></td>
<td>
<p>integer to specify the number of processes to use in the parallel processing.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_cl">cl</code></td>
<td>
<p>optional cluster to use if <code>parallel="snow"</code>. If unspecified, a cluster on the local machine is created for the duration of the call.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_type">type</code></td>
<td>
<p>character string to specify whether only the diagonal of the hat matrix (<code>"diagonal"</code>) or the entire hat matrix (<code>"matrix"</code>) should be returned.</p>
</td></tr>
<tr><td><code id="influence.rma.mv_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The term &lsquo;case&rsquo; below refers to a particular row from the dataset used in the model fitting (when argument <code>cluster</code> is not specified) or each level of the variable specified via <code>cluster</code>.
</p>
<p>Cook's distance for the \(i\textrm{th}\) case can be interpreted as the Mahalanobis distance between the entire set of predicted values once with the \(i\textrm{th}\) case included and once with the \(i\textrm{th}\) case excluded from the model fitting.
</p>
<p>The DFBETAS value(s) essentially indicate(s) how many standard deviations the estimated coefficient(s) change(s) after excluding the \(i\textrm{th}\) case from the model fitting.
</p>


<h3>Value</h3>

<p>The <code>cooks.distance</code> function returns a vector. The <code>dfbetas</code> function returns a data frame. The <code>hatvalues</code> function returns either a vector with the diagonal elements of the hat matrix or the entire hat matrix.
</p>


<h3>Note</h3>

<p>The variable specified via <code>cluster</code> is assumed to be of the same length as the data originally passed to the <code>rma.mv</code> function (and if the <code>data</code> argument was used in the original model fit, then the variable will be searched for within this data frame first). Any subsetting and removal of studies with missing values that was applied during the model fitting is also automatically applied to the variable specified via the <code>cluster</code> argument.
</p>
<p>Leave-one-out diagnostics are calculated by refitting the model \(k\) times (where \(k\) denotes the number of cases). Depending on how large \(k\) is, it may take a few moments to finish the calculations. For complex models fitted with <code><a href="#topic+rma.mv">rma.mv</a></code>, this can become computationally expensive.
</p>
<p>On machines with multiple cores, one can try to speed things up by delegating the model fitting to separate worker processes, that is, by setting <code>parallel="snow"</code> or <code>parallel="multicore"</code> and <code>ncpus</code> to some value larger than 1. Parallel processing makes use of the <code><a href="parallel.html#topic+parallel">parallel</a></code> package, using the <code><a href="parallel.html#topic+makePSOCKcluster">makePSOCKcluster</a></code> and <code><a href="parallel.html#topic+parLapply">parLapply</a></code> functions when <code>parallel="snow"</code> or using <code><a href="parallel.html#topic+mclapply">mclapply</a></code> when <code>parallel="multicore"</code> (the latter only works on Unix/Linux-alikes). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
<p>Alternatively (or in addition to using parallel processing), one can also set <code>reestimate=FALSE</code>, in which case any variance/correlation components in the model are not re-estimated after deleting the \(i\textrm{th}\) case from the dataset. Doing so only yields an approximation to the Cook's distances and DFBETAS values that ignores the influence of the \(i\textrm{th}\) case on the variance/correlation components, but is considerably faster (and often yields similar results).
</p>
<p>It may not be possible to fit the model after deletion of the \(i\textrm{th}\) case from the dataset. This will result in <code>NA</code> values for that case.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E., &amp; Welsch, R. E. (1980). <em>Regression diagnostics</em>. New York: Wiley.
</p>
<p>Cook, R. D., &amp; Weisberg, S. (1982). <em>Residuals and influence in regression</em>. London: Chapman and Hall.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; Cheung, M. W.-L. (2010). Outlier and influence diagnostics for meta-analysis. <em>Research Synthesis Methods</em>, <b>1</b>(2), 112&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.11&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rstudent.rma.mv">rstudent</a></code> for externally standardized residuals and <code><a href="#topic+weights.rma.mv">weights</a></code> for model fitting weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data from Konstantopoulos (2011) into 'dat'
dat &lt;- dat.konstantopoulos2011

### multilevel random-effects model
res &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat)
print(res, digits=3)

### Cook's distance for each observed outcome
x &lt;- cooks.distance(res)
x
plot(x, type="o", pch=19, xlab="Observed Outcome", ylab="Cook's Distance")

### Cook's distance for each district
x &lt;- cooks.distance(res, cluster=district)
x
plot(x, type="o", pch=19, xlab="District", ylab="Cook's Distance", xaxt="n")
axis(side=1, at=seq_along(x), labels=as.numeric(names(x)))

### hat values
hatvalues(res)
</code></pre>

<hr>
<h2 id='influence.rma.uni'>Model Diagnostics for 'rma.uni' Objects</h2><span id='topic+influence'></span><span id='topic+cooks.distance'></span><span id='topic+dfbetas'></span><span id='topic+hatvalues'></span><span id='topic+influence.rma.uni'></span><span id='topic+print.infl.rma.uni'></span><span id='topic+cooks.distance.rma.uni'></span><span id='topic+dfbetas.rma.uni'></span><span id='topic+hatvalues.rma.uni'></span>

<h3>Description</h3>

<p>Functions to compute various outlier and influential study diagnostics (some of which indicate the influence of deleting one study at a time on the model fit or the fitted/residual values) for objects of class <code>"rma.uni"</code>. For the corresponding documentation for <code>"rma.mv"</code> objects, see <code><a href="#topic+influence.rma.mv">influence</a></code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
influence(model, digits, progbar=FALSE, ...)

## S3 method for class 'infl.rma.uni'
print(x, digits=x$digits, infonly=FALSE, ...)

## S3 method for class 'rma.uni'
cooks.distance(model, progbar=FALSE, ...)
## S3 method for class 'rma.uni'
dfbetas(model, progbar=FALSE, ...)
## S3 method for class 'rma.uni'
hatvalues(model, type="diagonal", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="influence.rma.uni_+3A_model">model</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_x">x</code></td>
<td>
<p>an object of class <code>"infl.rma.uni"</code> (for <code>print</code>).</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_infonly">infonly</code></td>
<td>
<p>logical to specify whether only the influential cases should be printed (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_type">type</code></td>
<td>
<p>character string to specify whether only the diagonal of the hat matrix (<code>"diagonal"</code>) or the entire hat matrix (<code>"matrix"</code>) should be returned.</p>
</td></tr>
<tr><td><code id="influence.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The term &lsquo;case&rsquo; below refers to a particular row from the dataset used in the model fitting (which is typically synonymous with &lsquo;study&rsquo;).
</p>
<p>The <code>influence</code> function calculates the following leave-one-out diagnostics for each case:
</p>

<ul>
<li><p> externally standardized residual,
</p>
</li>
<li><p> DFFITS value,
</p>
</li>
<li><p> Cook's distance,
</p>
</li>
<li><p> covariance ratio,
</p>
</li>
<li><p> the leave-one-out amount of (residual) heterogeneity,
</p>
</li>
<li><p> the leave-one-out test statistic of the test for (residual) heterogeneity,
</p>
</li>
<li><p> DFBETAS value(s).
</p>
</li></ul>

<p>The diagonal elements of the hat matrix and the weights (in %) given to the observed effect sizes or outcomes during the model fitting are also provided (except for their scaling, the hat values and weights are the same for models without moderators, but will differ when moderators are included).
</p>
<p>For details on externally standardized residuals, see <code><a href="#topic+rstudent.rma.uni">rstudent</a></code>.
</p>
<p>The DFFITS value essentially indicates how many standard deviations the predicted (average) effect or outcome for the \(i\textrm{th}\) case changes after excluding the \(i\textrm{th}\) case from the model fitting.
</p>
<p>Cook's distance can be interpreted as the Mahalanobis distance between the entire set of predicted values once with the \(i\textrm{th}\) case included and once with the \(i\textrm{th}\) case excluded from the model fitting.
</p>
<p>The covariance ratio is defined as the determinant of the variance-covariance matrix of the parameter estimates based on the dataset with the \(i\textrm{th}\) case removed divided by the determinant of the variance-covariance matrix of the parameter estimates based on the complete dataset. A value below 1 therefore indicates that removal of the \(i\textrm{th}\) case yields more precise estimates of the model coefficients.
</p>
<p>The leave-one-out amount of (residual) heterogeneity is the estimated value of \(\tau^2\) based on the dataset with the \(i\textrm{th}\) case removed. This is always equal to 0 for equal-effects models.
</p>
<p>Similarly, the leave-one-out test statistic of the test for (residual) heterogeneity is the value of the test statistic of the test for (residual) heterogeneity calculated based on the dataset with the \(i\textrm{th}\) case removed.
</p>
<p>Finally, the DFBETAS value(s) essentially indicate(s) how many standard deviations the estimated coefficient(s) change(s) after excluding the \(i\textrm{th}\) case from the model fitting.
</p>
<p>A case may be considered to be &lsquo;influential&rsquo; if at least one of the following is true:
</p>

<ul>
<li><p> The absolute DFFITS value is larger than \(3 \times \sqrt{p/(k-p)}\), where \(p\) is the number of model coefficients and \(k\) the number of cases.
</p>
</li>
<li><p> The lower tail area of a chi-square distribution with \(p\) degrees of freedom cut off by the Cook's distance is larger than 50%.
</p>
</li>
<li><p> The hat value is larger than \(3 \times (p/k)\).
</p>
</li>
<li><p> Any DFBETAS value is larger than \(1\).
</p>
</li></ul>

<p>Cases which are considered influential with respect to any of these measures are marked with an asterisk. Note that the chosen cut-offs are (somewhat) arbitrary. Substantively informed judgment should always be used when examining the influence of each case on the results.
</p>


<h3>Value</h3>

<p>An object of class <code>"infl.rma.uni"</code>, which is a list containing the following components:
</p>
<table>
<tr><td><code>inf</code></td>
<td>
<p>an element of class <code>"list.rma"</code> with the externally standardized residuals, DFFITS values, Cook's distances, covariance ratios, leave-one-out \(\tau^2\) estimates, leave-one-out (residual) heterogeneity test statistics, hat values, weights, and an indicator whether a case is influential.</p>
</td></tr>
<tr><td><code>dfbs</code></td>
<td>
<p>an element of class <code>"list.rma"</code> with the DFBETAS values.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are printed with <code>print</code> and plotted with <code><a href="#topic+plot.infl.rma.uni">plot</a></code>. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>Leave-one-out diagnostics are calculated by refitting the model \(k\) times. Depending on how large \(k\) is, it may take a few moments to finish the calculations. There are shortcuts for calculating at least some of these values without refitting the model each time, but these are currently not implemented (and may not exist for all of the leave-one-out diagnostics calculated by the function).
</p>
<p>It may not be possible to fit the model after deletion of the \(i\textrm{th}\) case from the dataset. This will result in <code>NA</code> values for that case.
</p>
<p>Certain relationships between the leave-one-out diagnostics and the (internally or externally) standardized residuals (Belsley, Kuh, &amp; Welsch, 1980; Cook &amp; Weisberg, 1982) no longer hold for meta-analytic models. Maybe there are other relationships. These remain to be determined.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E., &amp; Welsch, R. E. (1980). <em>Regression diagnostics</em>. New York: Wiley.
</p>
<p>Cook, R. D., &amp; Weisberg, S. (1982). <em>Residuals and influence in regression</em>. London: Chapman and Hall.
</p>
<p>Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical methods for meta-analysis</em>. San Diego, CA: Academic Press.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; Cheung, M. W.-L. (2010). Outlier and influence diagnostics for meta-analysis. <em>Research Synthesis Methods</em>, <b>1</b>(2), 112&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.11&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.infl.rma.uni">plot</a></code> for a method to plot the outlier and influential case diagnostics.
</p>
<p><code><a href="#topic+rstudent.rma.uni">rstudent</a></code> for externally standardized residuals and <code><a href="#topic+weights.rma.uni">weights</a></code> for model fitting weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### compute the diagnostics
inf &lt;- influence(res)
inf

### plot the values
plot(inf)

### compute Cook's distances, DFBETAS values, and hat values
cooks.distance(res)
dfbetas(res)
hatvalues(res)
</code></pre>

<hr>
<h2 id='labbe'>L'Abbe Plots for 'rma' Objects</h2><span id='topic+labbe'></span><span id='topic+labbe.rma'></span>

<h3>Description</h3>

<p>Function to create L'Abb√© plots for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>labbe(x, ...)

## S3 method for class 'rma'
labbe(x, xlim, ylim, xlab, ylab,
      add=x$add, to=x$to, transf, targs,
      pch=21, psize, plim=c(0.5,3.5),
      col, bg, grid=FALSE, lty, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labbe_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="labbe_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="labbe_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="labbe_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="labbe_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="labbe_+3A_add">add</code></td>
<td>
<p>See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="labbe_+3A_to">to</code></td>
<td>
<p>See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="labbe_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the outcomes (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="labbe_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="labbe_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the outcomes. By default, an open circle is used. Can also be a vector of values. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="labbe_+3A_psize">psize</code></td>
<td>
<p>optional numeric vector to specify the point sizes for the outcomes. If unspecified, the point sizes are a function of the precision of the outcomes. Can also be a vector of values.</p>
</td></tr>
<tr><td><code id="labbe_+3A_plim">plim</code></td>
<td>
<p>numeric vector of length 2 to scale the point sizes (ignored when <code>psize</code> is specified). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="labbe_+3A_col">col</code></td>
<td>
<p>optional character string to specify the (border) color of the points. Can also be a vector.</p>
</td></tr>
<tr><td><code id="labbe_+3A_bg">bg</code></td>
<td>
<p>optional character string to specify the background color of open plot symbols. Can also be a vector. Set to <code>NA</code> to make the plotting symbols transparent.</p>
</td></tr>
<tr><td><code id="labbe_+3A_grid">grid</code></td>
<td>
<p>logical to specify whether a grid should be added to the plot. Can also be a color name.</p>
</td></tr>
<tr><td><code id="labbe_+3A_lty">lty</code></td>
<td>
<p>optional character vector to specify the line type for the diagonal reference line of no effect and the line that indicates the estimated effect based on the fitted model. If unspecified, the function sets this to <code>c("solid","dashed")</code> by default (use <code>"blank"</code> to suppress a line).</p>
</td></tr>
<tr><td><code id="labbe_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model specified via <code>x</code> must be a model without moderators (i.e., either an equal- or a random-effects model) fitted with either the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, or <code><a href="#topic+rma.glmm">rma.glmm</a></code> functions. Moreover, the model must have been fitted with <code>measure</code> set equal to <code>"RD"</code> (for risk differences), <code>"RR"</code> (for risk ratios), <code>"OR"</code> (for odds ratios), <code>"AS"</code> (for arcsine square root transformed risk differences), <code>"IRR"</code> (for incidence rate ratios), <code>"IRD"</code> (for incidence rate differences), or <code>"IRSD"</code> (for square root transformed incidence rate differences).
</p>
<p>The function calculates the arm-level outcomes for the two groups (e.g., treatment and control) and plots them against each other. In particular, the function plots the raw proportions of the two groups against each other when analyzing risk differences, the log of the proportions when analyzing (log) risk ratios, the log odds when analyzing (log) odds ratios, the arcsine square root transformed proportions when analyzing arcsine square root transformed risk differences, the raw incidence rates when analyzing incidence rate differences, the log of the incidence rates when analyzing (log) incidence rate ratios, and the square root transformed incidence rates when analyzing square root transformed incidence rate differences. The <code>transf</code> argument can be used to transform these values (e.g., <code>transf=exp</code> to transform the log of the proportions back to raw proportions; see also <a href="#topic+transf">transf</a>).
</p>
<p>As described under the documentation for the <code><a href="#topic+escalc">escalc</a></code> function, zero cells can lead to problems when calculating particular outcomes. Adding a small constant to the cells of the \(2 \times 2\) tables is a common solution to this problem. By default, the functions adopts the same method for handling zero cells as was used when fitting the model.
</p>
<p>By default (i.e., when <code>psize</code> is not specified), the point sizes are a function of the precision (i.e., inverse standard errors) of the outcomes. This way, more precise estimates are visually more prominent in the plot. By making the point sizes a function of the inverse standard errors of the estimates, their areas are proportional to the inverse sampling variances, which corresponds to the weights they would receive in an equal-effects model. However, the point sizes are rescaled so that the smallest point size is <code>plim[1]</code> and the largest point size is <code>plim[2]</code>. As a result, their relative sizes (i.e., areas) no longer exactly correspond to their relative weights in such a model. If exactly relative point sizes are desired, one can set <code>plim[2]</code> to <code>NA</code>, in which case the points are rescaled so that the smallest point size corresponds to <code>plim[1]</code> and all other points are scaled accordingly. As a result, the largest point may be very large. Alternatively, one can set <code>plim[1]</code> to <code>NA</code>, in which case the points are rescaled so that the largest point size corresponds to <code>plim[2]</code> and all other points are scaled accordingly.
</p>
<p>The solid line corresponds to identical outcomes in the two groups (i.e., the absence of a difference between the two groups). The dashed line indicates the estimated effect based on the fitted model.
</p>


<h3>Value</h3>

<p>A data frame with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>cex</code></td>
<td>
<p>the point sizes.</p>
</td></tr>
<tr><td><code>pch</code></td>
<td>
<p>the plotting symbols.</p>
</td></tr>
<tr><td><code>col</code></td>
<td>
<p>the point colors.</p>
</td></tr>
<tr><td><code>bg</code></td>
<td>
<p>the background colors.</p>
</td></tr>
<tr><td><code>ids</code></td>
<td>
<p>the study id numbers.</p>
</td></tr>
<tr><td><code>slab</code></td>
<td>
<p>the study labels.</p>
</td></tr>
</table>
<p>Note that the data frame is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>L'Abb√©, K. A., Detsky, A. S., &amp; O'Rourke, K. (1987). Meta-analysis in clinical research. <em>Annals of Internal Medicine</em>, <b>107</b>(2), 224&ndash;233. <code style="white-space: pre;">&#8288;https://doi.org/10.7326/0003-4819-107-2-224&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.glmm">rma.glmm</a></code> for functions to fit models for which L'Abb√© plots can be drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### default plot
labbe(res)

### funnel plot with risk values on the x- and y-axis and add grid
labbe(res, transf=exp, grid=TRUE)
</code></pre>

<hr>
<h2 id='leave1out'>Leave-One-Out Diagnostics for 'rma' Objects</h2><span id='topic+leave1out'></span><span id='topic+leave1out.rma.uni'></span><span id='topic+leave1out.rma.mh'></span><span id='topic+leave1out.rma.peto'></span>

<h3>Description</h3>

<p>Functions to carry out a &lsquo;leave-one-out analysis&rsquo;, by repeatedly fitting the specified model leaving out one study at a time. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leave1out(x, ...)

## S3 method for class 'rma.uni'
leave1out(x, digits, transf, targs, progbar=FALSE, ...)
## S3 method for class 'rma.mh'
leave1out(x, digits, transf, targs, progbar=FALSE, ...)
## S3 method for class 'rma.peto'
leave1out(x, digits, transf, targs, progbar=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leave1out_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, or <code>"rma.peto"</code>.</p>
</td></tr>
<tr><td><code id="leave1out_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="leave1out_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the model coefficients and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="leave1out_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="leave1out_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="leave1out_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>"rma.uni"</code> objects, the model specified via <code>x</code> must be a model without moderators (i.e., either an equal- or a random-effects model).
</p>


<h3>Value</h3>

<p>An object of class <code>"list.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>estimated (average) outcomes.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>corresponding test statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bounds of the confidence intervals.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bounds of the confidence intervals.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>test statistics for the test of heterogeneity.</p>
</td></tr>
<tr><td><code>Qp</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated amount of heterogeneity (only for random-effects models).</p>
</td></tr>
<tr><td><code>I2</code></td>
<td>
<p>values of \(I^2\).</p>
</td></tr>
<tr><td><code>H2</code></td>
<td>
<p>values of \(H^2\).</p>
</td></tr>
</table>
<p>When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then <code>zval</code> is called <code>tval</code> in the object that is returned by the function.
</p>
<p>The object is formatted and printed with the <code><a href="#topic+print.list.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>When using the <code>transf</code> option, the transformation is applied to the estimated coefficients and the corresponding interval bounds. The standard errors are then set equal to <code>NA</code> and are omitted from the printed output.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; Cheung, M. W.-L. (2010). Outlier and influence diagnostics for meta-analysis. <em>Research Synthesis Methods</em>, <b>1</b>(2), 112&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.11&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, and <code><a href="#topic+rma.peto">rma.peto</a></code> for functions to fit models for which leave-one-out diagnostics can be computed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### random-effects model
res &lt;- rma(yi, vi, data=dat)

### leave-one-out analysis
leave1out(res)
leave1out(res, transf=exp)

### meta-analysis of the (log) risk ratios using the Mantel-Haenszel method
res &lt;- rma.mh(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### leave-one-out analysis
leave1out(res)
leave1out(res, transf=exp)

### meta-analysis of the (log) odds ratios using Peto's method
res &lt;- rma.peto(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### leave-one-out analysis
leave1out(res)
leave1out(res, transf=exp)
</code></pre>

<hr>
<h2 id='llplot'>Plot of Likelihoods for Individual Studies</h2><span id='topic+llplot'></span>

<h3>Description</h3>

<p>Function to plot the likelihood of a certain parameter corresponding to an effect size or outcome measure given the study data. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llplot(measure, yi, vi, sei, ai, bi, ci, di, n1i, n2i, data, subset, drop00=TRUE,
       xvals=1000, xlim, ylim, xlab, ylab, scale=TRUE,
       lty, lwd, col, level=99.99, refline=0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="llplot_+3A_measure">measure</code></td>
<td>
<p>a character string to specify for which effect size or outcome measure the likelihoods should be calculated. See &lsquo;Details&rsquo; for possible options and how the data should then be specified.</p>
</td></tr>
<tr><td><code id="llplot_+3A_yi">yi</code></td>
<td>
<p>vector with the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="llplot_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances.</p>
</td></tr>
<tr><td><code id="llplot_+3A_sei">sei</code></td>
<td>
<p>vector to specify the corresponding standard.</p>
</td></tr>
<tr><td><code id="llplot_+3A_ai">ai</code></td>
<td>
<p>vector to specify the \(2 \times 2\) table frequencies (upper left cell).</p>
</td></tr>
<tr><td><code id="llplot_+3A_bi">bi</code></td>
<td>
<p>vector to specify the \(2 \times 2\) table frequencies (upper right cell).</p>
</td></tr>
<tr><td><code id="llplot_+3A_ci">ci</code></td>
<td>
<p>vector to specify the \(2 \times 2\) table frequencies (lower left cell).</p>
</td></tr>
<tr><td><code id="llplot_+3A_di">di</code></td>
<td>
<p>vector to specify the \(2 \times 2\) table frequencies (lower right cell).</p>
</td></tr>
<tr><td><code id="llplot_+3A_n1i">n1i</code></td>
<td>
<p>vector to specify the group sizes or row totals (first group/row).</p>
</td></tr>
<tr><td><code id="llplot_+3A_n2i">n2i</code></td>
<td>
<p>vector to specify the group sizes or row totals (second group/row).</p>
</td></tr>
<tr><td><code id="llplot_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="llplot_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the plot.</p>
</td></tr>
<tr><td><code id="llplot_+3A_drop00">drop00</code></td>
<td>
<p>logical to specify whether studies with no cases (or only cases) in both groups should be dropped. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="llplot_+3A_xvals">xvals</code></td>
<td>
<p>integer to specify for how many distinct values the likelihood should be evaluated.</p>
</td></tr>
<tr><td><code id="llplot_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="llplot_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="llplot_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="llplot_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="llplot_+3A_scale">scale</code></td>
<td>
<p>logical to specify whether the likelihood values should be scaled, so that the total area under each curve is (approximately) equal to 1.</p>
</td></tr>
<tr><td><code id="llplot_+3A_lty">lty</code></td>
<td>
<p>the line types (either a single value or a vector of length \(k\)). If unspecified, the function sets the line types according to some characteristics of the likelihood function. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="llplot_+3A_lwd">lwd</code></td>
<td>
<p>the line widths (either a single value or a vector of length \(k\)). If unspecified, the function sets the widths according to the sampling variances (so that the line is thicker for more precise studies and vice-versa).</p>
</td></tr>
<tr><td><code id="llplot_+3A_col">col</code></td>
<td>
<p>the line colors (either a single value or a vector of length \(k\)). If unspecified, the function uses various shades of gray according to the sampling variances (so that darker shades are used for more precise studies and vice-versa).</p>
</td></tr>
<tr><td><code id="llplot_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the plotting limits for each likelihood line in terms of the confidence interval (the default is 99.99).</p>
</td></tr>
<tr><td><code id="llplot_+3A_refline">refline</code></td>
<td>
<p>numeric value to specify the location of the vertical &lsquo;reference&rsquo; line (the default is 0). The line can be suppressed by setting this argument to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="llplot_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At the moment, the function only accepts <code>measure="GEN"</code> or <code>measure="OR"</code>.
</p>
<p>For <code>measure="GEN"</code>, one must specify arguments <code>yi</code> for the observed effect sizes or outcomes and <code>vi</code> for the corresponding sampling variances (instead of specifying <code>vi</code>, one can specify the standard errors via the <code>sei</code> argument). The function then plots the likelihood of the true effect size or outcome based on a normal sampling distribution with observed outcome as given by <code>yi</code> and variance as given by <code>vi</code> for each study.
</p>
<p>For <code>measure="OR"</code>, one must specify arguments <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code>, which denote the cell frequencies of the \(2 \times 2\) tables. Alternatively, one can specify <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code>. See <code><a href="#topic+escalc">escalc</a></code> function for more details. The function then plots the likelihood of the true log odds ratio based on the non-central hypergeometric distribution for each \(2 \times 2\) table. Since studies with no cases (or only cases) in both groups have a flat likelihood and are not informative about the odds ratio, they are dropped by default (i.e., <code>drop00=TRUE</code>) and are hence not drawn (if <code>drop00=FALSE</code>, these likelihood are indicated by dotted lines). For studies that have a single zero count, the MLE of the odds ratio is infinite and these likelihoods are indicated by dashed lines.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>van Houwelingen, H. C., Zwinderman, K. H., &amp; Stijnen, T. (1993). A bivariate approach to meta-analysis. <em>Statistics in Medicine</em>, <b>12</b>(24), 2273&ndash;2284. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780122405&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.glmm">rma.glmm</a></code> for model fitting functions that are based on corresponding likelihood functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### draw likelihoods
llplot(measure="GEN", yi=yi, vi=vi, data=dat, lwd=1, refline=NA, xlim=c(-3,2))

### create plot (Figure 2 in van Houwelingen, Zwinderman, &amp; Stijnen, 1993)
llplot(measure="OR", ai=b.xci, n1i=nci, ci=b.xti, n2i=nti, data=dat.collins1985a,
       lwd=1, refline=NA, xlim=c(-4,4), drop00=FALSE)
</code></pre>

<hr>
<h2 id='matreg'>Fit Regression Models based on Correlation and Covariance Matrices</h2><span id='topic+matreg'></span>

<h3>Description</h3>

<p>Function to fit regression models based on correlation and covariance matrices. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matreg(y, x, R, n, V, cov=FALSE, means, ztor=FALSE,
       nearpd=FALSE, level=95, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matreg_+3A_y">y</code></td>
<td>
<p>index (or name given as a character string) of the outcome variable.</p>
</td></tr>
<tr><td><code id="matreg_+3A_x">x</code></td>
<td>
<p>indices (or names given as a character vector) of the predictor variables.</p>
</td></tr>
<tr><td><code id="matreg_+3A_r">R</code></td>
<td>
<p>correlation or covariance matrix (or only the lower triangular part including the diagonal).</p>
</td></tr>
<tr><td><code id="matreg_+3A_n">n</code></td>
<td>
<p>sample size based on which the elements in the correlation/covariance matrix were computed.</p>
</td></tr>
<tr><td><code id="matreg_+3A_v">V</code></td>
<td>
<p>variance-covariance matrix of the lower triangular elements of the correlation/covariance matrix. Either <code>V</code> or <code>n</code> should be specified, not both. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="matreg_+3A_cov">cov</code></td>
<td>
<p>logical to specify whether <code>R</code> is a covariance matrix (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="matreg_+3A_means">means</code></td>
<td>
<p>optional vector to specify the means of the variables (only relevant when <code>cov=TRUE</code>).</p>
</td></tr>
<tr><td><code id="matreg_+3A_ztor">ztor</code></td>
<td>
<p>logical to specify whether <code>R</code> is a matrix of r-to-z transformed correlations and hence should be back-transformed to raw correlations (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="matreg_+3A_nearpd">nearpd</code></td>
<td>
<p>logical to specify whether the <code><a href="Matrix.html#topic+nearPD">nearPD</a></code> function from the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package should be used when the \(R_{x,x}\) matrix cannot be inverted. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="matreg_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="matreg_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded.</p>
</td></tr>
<tr><td><code id="matreg_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let \(R\) be a \(p \times p\) correlation or covariance matrix. Let \(y\) denote the row/column of the outcome variable and \(x\) the row(s)/column(s) of the predictor variable(s) in this matrix. Let \(R_{x,x}\) and \(R_{x,y}\) denote the corresponding submatrices of \(R\). Then \[b = R_{x,x}^{-1} R_{x,y}\] yields the standardized or raw regression coefficients (depending on whether \(R\) is a correlation or covariance matrix, respectively) when regressing the outcome variable on the predictor variable(s).
</p>
<p>The \(R\) matrix may be computed based on a single sample of \(n\) subjects. In this case, one should specify the sample size via argument <code>n</code>. The variance-covariance matrix of the standardized regression coefficients is then given by \(\mbox{Var}[b] = \mbox{MSE} \times R_{x,x}^{-1}\), where \(\mbox{MSE} = (1 - b'R_{x,y}) / (n - m)\) and \(m\) denotes the number of predictor variables. The standard errors are then given by the square root of the diagonal elements of \(\mbox{Var}[b]\). Test statistics (in this case, t-statistics) and the corresponding p-values can then be computed as in a regular regression analysis. When \(R\) is a covariance matrix, one should set <code>cov=TRUE</code> and specify the means of the \(p\) variables via argument <code>means</code> to obtain raw regression coefficients including the intercept and corresponding standard errors.
</p>
<p>Alternatively, \(R\) may be the result of a meta-analysis of correlation coefficients. In this case, the elements in \(R\) are pooled correlation coefficients and the variance-covariance matrix of these pooled coefficients should be specified via argument <code>V</code>. The order of elements in <code>V</code> should correspond to the order of elements in the lower triangular part of \(R\) column-wise. For example, if \(R\) is a \(4 \times 4\) matrix of the form: \[\begin{bmatrix} 1 & & & \\ r_{21} & 1 & & \\ r_{31} & r_{32} & 1 & \\ r_{41} & r_{42} & r_{43} & 1 \end{bmatrix}\] then the elements are \(r_{21}\), \(r_{31}\), \(r_{41}\), \(r_{32}\), \(r_{42}\), and \(r_{43}\) and hence <code>V</code> should be a \(6 \times 6\) variance-covariance matrix of these elements in this order. The variance-covariance matrix of the standardized regression coefficients (i.e., \(\mbox{Var}[b]\)) is then computed as a function of <code>V</code> as described in Becker (1992) using the multivariate delta method. The standard errors are then again given by the square root of the diagonal elements of \(\mbox{Var}[b]\). Test statistics (in this case, z-statistics) and the corresponding p-values can then be computed in the usual manner.
</p>
<p>In case \(R\) is the result of a meta-analysis of Fisher r-to-z transformed correlation coefficients (and hence <code>V</code> is then the corresponding variance-covariance matrix of these pooled transformed coefficients), one should set argument <code>ztor=TRUE</code>, so that the appropriate back-transformation is then applied to <code>R</code> (and <code>V</code>) within the function.
</p>
<p>Finally, \(R\) may be a covariance matrix based on a meta-analysis (e.g., the estimated variance-covariance matrix of the random effects in a multivariate model). In this case, one should set <code>cov=TRUE</code> and <code>V</code> should again be the variance-covariance matrix of the elements in \(R\), but now including the diagonal. Hence, if \(R\) is a \(4 \times 4\) matrix of the form: \[\begin{bmatrix} \tau_1^2 & & & \\ \tau_{21} & \tau_2^2 & & \\ \tau_{31} & \tau_{32} & \tau_3^2 & \\ \tau_{41} & \tau_{42} & \tau_{43} & \tau_4^2 \end{bmatrix}\] then the elements are \(\tau^2_1\), \(\tau_{21}\), \(\tau_{31}\), \(\tau_{41}\), \(\tau^2_2\), \(\tau_{32}\), \(\tau_{42}\), \(\tau^2_3\), \(\tau_{43}\), and \(\tau^2_4\), and hence <code>V</code> should be a \(10 \times 10\) variance-covariance matrix of these elements in this order. Argument <code>means</code> can then again be used to specify the means of the variables.
</p>


<h3>Value</h3>

<p>An object of class <code>"matreg"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>tab</code></td>
<td>
<p>a data frame with the estimated model coefficients, standard errors, test statistics, degrees of freedom (only for t-tests), p-values, and lower/upper confidence interval bounds.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>the variance-covariance matrix of the estimated model coefficients.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.matreg">print</a></code> function. Extractor functions include <code><a href="#topic+coef.matreg">coef</a></code> and <code><a href="#topic+vcov.matreg">vcov</a></code>.
</p>


<h3>Note</h3>

<p>Only the lower triangular part of <code>R</code> (and <code>V</code> if it is specified) is used in the computations.
</p>
<p>If \(R_{x,x}\) is not invertible, an error will be issued. In this case, one can set argument <code>nearpd=TRUE</code>, in which case the <code><a href="Matrix.html#topic+nearPD">nearPD</a></code> function from the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package will be used to find the nearest positive semi-definite matrix, which should be invertible. The results should be treated with caution when this is done.
</p>
<p>When \(R\) is a covariance matrix with <code>V</code> and <code>means</code> specified, the means are treated as known constants when estimating the standard error of the intercept.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Becker, B. J. (1992). Using results from replicated studies to estimate linear models. <em>Journal of Educational Statistics</em>, <b>17</b>(4), 341&ndash;362. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986017004341&#8288;</code>
</p>
<p>Becker, B. J. (1995). Corrections to &quot;Using results from replicated studies to estimate linear models&quot;. <em>Journal of Educational and Behavioral Statistics</em>, <b>20</b>(1), 100&ndash;102. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986020001100&#8288;</code>
</p>
<p>Becker, B. J., &amp; Aloe, A. (2019). Model-based meta-analysis and related approaches. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (3rd ed., pp. 339&ndash;363). New York: Russell Sage Foundation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.mv">rma.mv</a></code> for a function to meta-analyze multiple correlation coefficients that can be used to construct an \(R\) matrix.
</p>
<p><code><a href="#topic+rcalc">rcalc</a></code> for a function to construct the variance-covariance matrix of dependent correlation coefficients.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### first an example unrelated to meta-analysis, simply demonstrating that
### one can obtain the same results from lm() and matreg()

### fit a regression model with lm() to the 'mtcars' dataset
res &lt;- lm(mpg ~ hp + wt + am, data=mtcars)
summary(res)

### covariance matrix of the dataset
S &lt;- cov(mtcars)

### fit the same regression model using matreg()
res &lt;- matreg(y="mpg", x=c("hp","wt","am"), R=S, cov=TRUE,
              means=colMeans(mtcars), n=nrow(mtcars))
summary(res)

### copy the 'mtcars' dataset to 'dat' and standardize all variables
dat &lt;- mtcars
dat[] &lt;- scale(dat)

### fit a regression model with lm() to obtain standardized regression coefficients ('betas')
res &lt;- lm(mpg ~ 0 + hp + wt + am, data=dat)
summary(res)

### correlation matrix of the dataset
R &lt;- cor(mtcars)

### fit the same regression model using matreg()
res &lt;- matreg(y="mpg", x=c("hp","wt","am"), R=R, n=nrow(mtcars))
summary(res)

### note: the standard errors of the betas should not be used to construct CIs
### as they assume that the null hypothesis (H0: beta_j = 0) is true

### construct the var-cov matrix of correlations in R
V &lt;- rcalc(R, ni=nrow(mtcars))$V

### fit the same regression model using matreg() but now supply V
res &lt;- matreg(y="mpg", x=c("hp","wt","am"), R=R, V=V)
summary(res)

### the standard errors computed in this way can now be used to construct
### CIs for the betas (here, the difference is relatively small)

############################################################################

### copy data into 'dat'
dat &lt;- dat.craft2003

### construct dataset and var-cov matrix of the correlations
tmp &lt;- rcalc(ri ~ var1 + var2 | study, ni=ni, data=dat)
V &lt;- tmp$V
dat &lt;- tmp$dat

### turn var1.var2 into a factor with the desired order of levels
dat$var1.var2 &lt;- factor(dat$var1.var2,
   levels=c("acog.perf", "asom.perf", "conf.perf", "acog.asom", "acog.conf", "asom.conf"))

### multivariate random-effects model
res &lt;- rma.mv(yi, V, mods = ~ var1.var2 - 1, random = ~ var1.var2 | study, struct="UN", data=dat)
res

### restructure estimated mean correlations into a 4x4 matrix
R &lt;- vec2mat(coef(res))
rownames(R) &lt;- colnames(R) &lt;- c("perf", "acog", "asom", "conf")
round(R, digits=3)

### check that order in vcov(res) corresponds to order in R
round(vcov(res), digits=4)

### fit regression model with 'perf' as outcome and 'acog', 'asom', and 'conf' as predictors
matreg(1, 2:4, R=R, V=vcov(res))

### can also specify variable names
matreg("perf", c("acog","asom","conf"), R=R, V=vcov(res))

## Not run: 
### repeat the above but with r-to-z transformed correlations
dat &lt;- dat.craft2003
tmp &lt;- rcalc(ri ~ var1 + var2 | study, ni=ni, data=dat, rtoz=TRUE)
V &lt;- tmp$V
dat &lt;- tmp$dat
dat$var1.var2 &lt;- factor(dat$var1.var2,
   levels=c("acog.perf", "asom.perf", "conf.perf", "acog.asom", "acog.conf", "asom.conf"))
res &lt;- rma.mv(yi, V, mods = ~ var1.var2 - 1, random = ~ var1.var2 | study, struct="UN", data=dat)
R &lt;- vec2mat(coef(res))
rownames(R) &lt;- colnames(R) &lt;- c("perf", "acog", "asom", "conf")
matreg(1, 2:4, R=R, V=vcov(res), ztor=TRUE)

## End(Not run)

############################################################################

### a different example based on van Houwelingen et al. (2002)

### create dataset in long format
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
                    data=dat.colditz1994, append=FALSE)
dat.long &lt;- escalc(measure="PLO", xi=out1, mi=out2, data=dat.long)
levels(dat.long$group) &lt;- c("CON", "EXP")
dat.long

### fit bivariate model
res &lt;- rma.mv(yi, vi, mods = ~ group - 1, random = ~ group | study, struct="UN",
              data=dat.long, method="ML")
res

### regression of log(odds)_EXP on log(odds)_CON
matreg(y=2, x=1, R=res$G, cov=TRUE, means=coef(res), n=res$g.levels.comb.k)

### but the SE of the CON coefficient is not computed correctly, since above we treat res$G as if
### it was a var-cov matrix computed from raw data based on res$g.levels.comb.k (= 13) data points

### fit bivariate model and get the var-cov matrix of the estimates in res$G
res &lt;- rma.mv(yi, vi, mods = ~ group - 1, random = ~ group | study, struct="UN",
              data=dat.long, method="ML", cvvc="varcov", control=list(nearpd=TRUE))

### now use res$vvc as the var-cov matrix of the estimates in res$G
matreg(y=2, x=1, R=res$G, cov=TRUE, means=coef(res), V=res$vvc)
</code></pre>

<hr>
<h2 id='metafor.news'>Read News File of the Metafor Package</h2><span id='topic+metafor.news'></span>

<h3>Description</h3>

<p>Function to read the &lsquo;<span class="file">NEWS</span>&rsquo; file of the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metafor.news()
</code></pre>


<h3>Details</h3>

<p>The function is just a wrapper for <code>news(package="metafor")</code> which parses and displays the &lsquo;<span class="file">NEWS</span>&rsquo; file of the package.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
metafor.news()

## End(Not run)
</code></pre>

<hr>
<h2 id='methods.anova.rma'>Methods for 'anova.rma' Objects</h2><span id='topic+methods.anova.rma'></span><span id='topic+as.data.frame.anova.rma'></span><span id='topic+as.data.frame.list.anova.rma'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"anova.rma"</code> and <code>"list.anova.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'anova.rma'
as.data.frame(x, ...)
## S3 method for class 'list.anova.rma'
as.data.frame(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods.anova.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"anova.rma"</code> or <code>"list.anova.rma"</code>.</p>
</td></tr>
<tr><td><code id="methods.anova.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

### fit mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ alloc + ablat, data=dat)

### test the allocation factor
sav &lt;- anova(res, btt="alloc")
sav

### turn object into a regular data frame
as.data.frame(sav)

### test the contrast between levels random and systematic
sav &lt;- anova(res, X=c(0,1,-1,0))
sav

### turn object into a regular data frame
as.data.frame(sav)

### fit random-effects model
res0 &lt;- rma(yi, vi, data=dat)

### LRT comparing the two models
sav &lt;- anova(res, res0, refit=TRUE)
sav

### turn object into a regular data frame
as.data.frame(sav)
</code></pre>

<hr>
<h2 id='methods.confint.rma'>Methods for 'confint.rma' Objects</h2><span id='topic+methods.confint.rma'></span><span id='topic+as.data.frame.confint.rma'></span><span id='topic+as.data.frame.list.confint.rma'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"confint.rma"</code> and <code>"list.confint.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'confint.rma'
as.data.frame(x, ...)
## S3 method for class 'list.confint.rma'
as.data.frame(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods.confint.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"confint.rma"</code> or <code>"list.confint.rma"</code>.</p>
</td></tr>
<tr><td><code id="methods.confint.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### get 95% CI for tau^2, tau, I^2, and H^2
sav &lt;- confint(res)
sav

### turn object into a regular data frame
as.data.frame(sav)

############################################################################

### copy data into 'dat'
dat &lt;- dat.berkey1998

### construct block diagonal var-cov matrix of the observed outcomes based on variables v1i and v2i
V &lt;- vcalc(vi=1, cluster=author, rvars=c(v1i, v2i), data=dat)

### fit multivariate model
res &lt;- rma.mv(yi, V, mods = ~ outcome - 1, random = ~ outcome | trial, struct="UN", data=dat)

### get 95% CI for variance components and correlation
sav &lt;- confint(res)
sav

### turn object into a regular data frame
as.data.frame(sav)
</code></pre>

<hr>
<h2 id='methods.escalc'>Methods for 'escalc' Objects</h2><span id='topic+methods.escalc'></span><span id='topic++5B.escalc'></span><span id='topic++24+3C-.escalc'></span><span id='topic+cbind.escalc'></span><span id='topic+rbind.escalc'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"escalc"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'escalc'
x[i, ...]
## S3 replacement method for class 'escalc'
x$name &lt;- value
## S3 method for class 'escalc'
cbind(..., deparse.level=1)
## S3 method for class 'escalc'
rbind(..., deparse.level=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods.escalc_+3A_x">x</code></td>
<td>
<p>an object of class <code>"escalc"</code>.</p>
</td></tr>
<tr><td><code id="methods.escalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For the <code>`[`</code> method, any variables specified as part of the <code>i</code> argument will be searched for within object <code>x</code> first (see &lsquo;Examples&rsquo;).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### select rows where variable 'alloc' is equal to 'random'
dat[dat$alloc == "random",]

### variables specified are automatically searched for within the object itself
dat[alloc == "random",]

### note: this behavior is specific to 'escalc' objects; this doesn't work for regular data frames
</code></pre>

<hr>
<h2 id='methods.list.rma'>Methods for 'list.rma' Objects</h2><span id='topic+methods.list.rma'></span><span id='topic+as.data.frame.list.rma'></span><span id='topic+as.matrix.list.rma'></span><span id='topic++5B.list.rma'></span><span id='topic+head.list.rma'></span><span id='topic+tail.list.rma'></span><span id='topic++24+3C-.list.rma'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"list.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'list.rma'
as.data.frame(x, ...)
## S3 method for class 'list.rma'
as.matrix(x, ...)
## S3 method for class 'list.rma'
x[i, ...]
## S3 method for class 'list.rma'
head(x, n=6L, ...)
## S3 method for class 'list.rma'
tail(x, n=6L, ...)
## S3 replacement method for class 'list.rma'
x$name &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods.list.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"list.rma"</code>.</p>
</td></tr>
<tr><td><code id="methods.list.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For the <code>`[`</code> method, any variables specified as part of the <code>i</code> argument will be searched for within object <code>x</code> first (see &lsquo;Examples&rsquo;).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat' and examine data
dat &lt;- dat.viechtbauer2021

### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=xTi, n1i=nTi, ci=xCi, n2i=nCi, add=1/2, to="all", data=dat)

### fit mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ dose, data=dat)

### get studentized residuals
sav &lt;- rstudent(res)
sav

### studies with studentized residuals larger than +-1.96
sav[abs(sav$z) &gt; 1.96,]

### variables specified are automatically searched for within the object itself
sav[abs(z) &gt; 1.96,]

### note: this behavior is specific to 'rma.list' objects; this doesn't work for regular data frames
</code></pre>

<hr>
<h2 id='methods.vif.rma'>Methods for 'vif.rma' Objects</h2><span id='topic+methods.vif.rma'></span><span id='topic+as.data.frame.vif.rma'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"vif.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vif.rma'
as.data.frame(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods.vif.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"vif.rma"</code>.</p>
</td></tr>
<tr><td><code id="methods.vif.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

### fit mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ ablat + year + alloc, data=dat)

### get variance inflation factors for all individual coefficients
sav &lt;- vif(res)
sav

### turn object into a regular data frame
as.data.frame(sav)

### get VIFs for ablat and year and the generalized VIF for alloc
sav &lt;- vif(res, btt=list("ablat","alloc","year"))
sav

### turn object into a regular data frame
as.data.frame(sav)
</code></pre>

<hr>
<h2 id='mfopt'>Getting and Setting Package Options</h2><span id='topic+mfopt'></span><span id='topic+getmfopt'></span><span id='topic+setmfopt'></span>

<h3>Description</h3>

<p>Functions for getting and setting <span class="pkg">metafor</span> package options. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getmfopt(x, default=NULL)
setmfopt(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mfopt_+3A_x">x</code></td>
<td>
<p>The name of an option. If unspecified, all options are returned.</p>
</td></tr>
<tr><td><code id="mfopt_+3A_default">default</code></td>
<td>
<p>value to return if the option name does not exist.</p>
</td></tr>
<tr><td><code id="mfopt_+3A_...">...</code></td>
<td>
<p>one or more option names and the corresponding values to which they should be set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <span class="pkg">metafor</span> package stores some of its options as a list element called <code>"metafor"</code> in the system options (see <code><a href="base.html#topic+options">options</a></code>). Hence, <code>getmfopt()</code> is the same as <code>getOption("metafor")</code>. One can also set <code>x</code> to the name of an option to return. With <code>setmfopt()</code>, one can set one or more options to their desired values.
</p>
<p>Currently, the following options are supported:
</p>

<dl>
<dt><code>check</code></dt><dd><p>logical to specify whether a version check should be carried out when loading the package (the default is <code>TRUE</code>). See <a href="#topic+misc-options">here</a> for details. Obviously, this option must be set before loading the package (e.g., with <code>options(metafor=list(check=FALSE))</code>).</p>
</dd>
<dt><code>silent</code></dt><dd><p>logical to specify whether a startup message should be issued when loading the package (the default is <code>FALSE</code>). Obviously, this option must be set before loading the package (e.g., with <code>options(metafor=list(silent=TRUE))</code>). Note that messages about required packages that are automatically loaded are not suppressed by this. To fully suppress all startup messages, load the package with <code><a href="base.html#topic+suppressPackageStartupMessages">suppressPackageStartupMessages</a></code>.</p>
</dd>
<dt><code>space</code></dt><dd><p>logical to specify whether an empty line should be added before and after the output (the default is <code>TRUE</code>). See <a href="#topic+misc-options">here</a> for details.</p>
</dd>
<dt><code>digits</code></dt><dd><p>a named vector to specify how various aspects of the output should be rounded (unset by default). See <a href="#topic+misc-options">here</a> for details.</p>
</dd>
<dt><code>style</code></dt><dd><p>a list whose elements specify the styles for various parts of the output when the <a href="https://cran.r-project.org/package=crayon">crayon</a> package is loaded and a terminal is used that supports &lsquo;ANSI&rsquo; color/highlight codes (unset by default). See <a href="#topic+misc-options">here</a> for details. Can also be a logical and set to <code>FALSE</code> to switch off output styling when the <code>crayon</code> package is loaded.</p>
</dd>
<dt><code>theme</code></dt><dd><p>character string to specify how plots created by the package should be themed. The default is <code>"default"</code>, which means that the default foreground and background colors of plotting devices are used. Alternative options are <code>"light"</code> and <code>"dark"</code>, which forces plots to be drawn with a light or dark background, respectively. See <a href="#topic+misc-options">here</a> for further details. RStudio users can also set this to <code>"auto"</code>, in which case plotting colors are chosen depending on the RStudio theme used (for some themes, using <code>"auto2"</code> might be aesthetically more pleasing). One can also use <code>setmfopt(theme="custom", fg=&lt;color&gt;, bg=&lt;color&gt;)</code> to set the foreground and background colors to custom choices (depending on the colors chosen, using <code>"custom2"</code> might be aesthetically more pleasing).</p>
</dd>
</dl>



<h3>Value</h3>

<p>Either a vector with the value for the chosen option or a list with all options.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getmfopt()
getmfopt(space)
setmfopt(space=FALSE)
getmfopt()
setmfopt(space=TRUE)
getmfopt()
</code></pre>

<hr>
<h2 id='misc-models'>Fixed-Effects and Random-Effects Models in Meta-Analysis <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script></h2><span id='topic+misc-models'></span><span id='topic+misc_models'></span>

<h3>Description</h3>

<p>Books and articles about meta-analysis often describe and discuss the difference between the so-called &lsquo;fixed-effects model&rsquo; and the &lsquo;random-effects model&rsquo; (e.g., Cooper et al., 2009). The former term is (mostly) avoided throughout the documentation of the <span class="pkg">metafor</span> package. The term &lsquo;equal-effects model&rsquo; is used instead, since it more concretely describes the main assumption underlying this model (i.e., that the underlying true effects/outcomes are homogeneous, or in other words, that they are all equal to each other). The terms &lsquo;common-effect(s) model&rsquo; or &lsquo;homogenous-effect(s) model&rsquo; have also sometimes been used in the literature to describe this model and are equally descriptive.
</p>
<p>Moreover, the term &lsquo;fixed-effects model&rsquo; creates a bit of a conundrum. When authors use this term, they are really typically referring to the equal-effects model. There is however another type of model, the &lsquo;real&rsquo; fixed-effects model, that is different from the equal-effects model, but now we would need to invent (unnecessarily) a different term to refer to this model. Some have done so or tried to make a distinction between the &lsquo;fixed-effect model&rsquo; (without the s!) and the &lsquo;fixed-effects model&rsquo;, but this subtle difference in terminology is easily overlooked/missed. Using the term &lsquo;equal-effects model&rsquo; avoids this confusion and is more informative.
</p>
<p>However, the question then remains what the real fixed-effects model is all about. The purpose of this page is to describe this model and to contrast it with the well-known random-effects model.
</p>


<h3>Details</h3>



<h4>Fixed-Effects Model</h4>

<p>Assume we have a set of \(i = 1, \ldots, k\) independent studies and let \(y_i\) denote the observed value of the effect size or outcome measure in the \(i\textrm{th}\) study. Let \(\theta_i\) denote the corresponding (unknown) true effect/outcome, such that \[y_i \mid \theta_i \sim N(\theta_i, v_i).\] In other words, the observed effect sizes or outcomes are assumed to be unbiased and normally distributed estimates of the corresponding true effects/outcomes with sampling variances equal to \(v_i\). The \(v_i\) values are assumed to be known.
</p>
<p>The fixed-effects model is simply given by \[y_i = \theta_i + \varepsilon_i,\] where the \(\theta_i\) values are the (fixed) true effects/outcomes of the \(k\) studies. Therefore, the model &lsquo;conditions&rsquo; on the true effects/outcomes and provides a <em>conditional inference</em> about the \(k\) studies included in the meta-analysis.
</p>
<p>When using weighted estimation (the default in <code><a href="#topic+rma.uni">rma.uni</a></code> when <code>method="FE"</code>), this implies that the fitted model provides an estimate of \[\bar{\theta}_w = \frac{\sum_{i=1}^k w_i \theta_i}{\sum_{i=1}^k w_i},\] that is, the <em>weighted average</em> of the true effects/outcomes in the \(k\) studies, with weights equal to \(w_i = 1/v_i\).
</p>
<p>As an example, consider the meta-analysis by Bangert-Drowns et al. (2004) on the effectiveness of writing-to-learn interventions on academic achievement. The dataset (<code><a href="metadat.html#topic+dat.bangertdrowns2004">dat.bangertdrowns2004</a></code>) includes the observed standardized mean differences (variable <code>yi</code>) and the corresponding sampling variances (variable <code>vi</code>) of 48 studies that have examined such an intervention. We can fit a fixed-effects model to these data with:
</p>
<pre># copy data into 'dat'
dat &lt;- dat.bangertdrowns2004

# fit a fixed-effects model
res &lt;- rma(yi, vi, data=dat, method="FE")
res

# Fixed-Effects Model (k = 48)
#
# I^2 (total heterogeneity / total variability):   56.12%
# H^2 (total variability / sampling variability):  2.28
#
# Test for Heterogeneity:
# Q(df = 47) = 107.1061, p-val &lt; .0001
#
# Model Results:
#
# estimate      se    zval    pval   ci.lb   ci.ub
#   0.1656  0.0269  6.1499  &lt;.0001  0.1128  0.2184</pre>
<p>The Q-test suggests that the underlying true standardized mean differences are heterogeneous \((Q(\textrm{df}=47) = 107.11, p < .0001).\) Therefore, if we believe this to be true, then the value shown under <code>estimate</code> is an estimate of the inverse-variance weighted average of the true standardized mean differences of these 48 studies (i.e., \(\hat{\bar{\theta}}_w = 0.17\)).
</p>
<p>One can also employ an unweighted estimation method (by setting <code>weighted=FALSE</code> in <code><a href="#topic+rma.uni">rma.uni</a></code>), which provides an estimate of the <em>unweighted average</em> of the true effects/outcomes in the \(k\) studies, that is, an estimate of \[\bar{\theta}_u = \frac{\sum_{i=1}^k \theta_i}{k}.\]
</p>
<p>Returning to the example, we then find:
</p>
<pre># fit a fixed-effects model using unweighted estimation
res &lt;- rma(yi, vi, data=dat, method="FE", weighted=FALSE)
res

# Fixed-Effects Model (k = 48)
#
# I^2 (total heterogeneity / total variability):   56.12%
# H^2 (total variability / sampling variability):  2.28
#
# Test for Heterogeneity:
# Q(df = 47) = 107.1061, p-val &lt; .0001
#
# Model Results:
#
# estimate      se    zval    pval   ci.lb   ci.ub
#   0.2598  0.0380  6.8366  &lt;.0001  0.1853  0.3343</pre>
<p>Therefore, the value shown under <code>estimate</code> is now an estimate of the unweighted average of the true standardized mean differences of these 48 studies (i.e., \(\hat{\bar{\theta}}_u = 0.26\)).
</p>
<p>For weighted estimation, one could also choose to estimate \(\bar{\theta}_w\), where the \(w_i\) values are user-defined weights (via argument <code>weights</code> in <code><a href="#topic+rma.uni">rma.uni</a></code>). Hence, using inverse-variance weights or unit weights (as in unweighted estimation) are just special cases. It is up to the user to decide to what extent \(\bar{\theta}_w\) is a meaningful parameter to estimate (regardless of the weights used).
</p>
<p>For example, we could use the sample sizes of the studies as weights:
</p>
<pre># fit a fixed-effects model using the sample sizes as weights
res &lt;- rma(yi, vi, data=dat, method="FE", weights=ni)
res

# Fixed-Effects Model (k = 48)
#
# I^2 (total heterogeneity / total variability):   56.12%
# H^2 (total variability / sampling variability):  2.28
#
# Test for Heterogeneity:
# Q(df = 47) = 107.1061, p-val &lt; .0001
#
# Model Results:
#
# estimate      se    zval    pval   ci.lb   ci.ub
#   0.1719  0.0269  6.3802  &lt;.0001  0.1191  0.2248</pre>
<p>We therefore obtain an estimate of the sample-size weighted average of the true standardized mean differences of these 48 studies (i.e., \(\hat{\bar{\theta}}_w = 0.17\)). Since the sample sizes and the inverse sampling variances are highly correlated (<code>cor(dat$ni, 1/dat$vi)</code> yields <code>0.999</code>), the results are almost identical to the ones we obtained earlier using inverse-variance weighting.
</p>



<h4>Random-Effects Model</h4>

<p>The random-effects model does not condition on the true effects/outcomes. Instead, the \(k\) studies included in the meta-analysis are assumed to be a random sample from a larger population of studies. In rare cases, the studies included in a meta-analysis are actually sampled from a larger collection of studies. More typically, all efforts have been made to find and include all relevant studies providing evidence about the phenomenon of interest and hence the population of studies is a hypothetical population of an essentially infinite set of studies comprising all of the studies that have been conducted, that could have been conducted, or that may be conducted in the future. We assume that \(\theta_i \sim N(\mu, \tau^2)\), that is, the true effects/outcomes in the population of studies are normally distributed with \(\mu\) denoting the average true effect/outcome and \(\tau^2\) the variance of the true effects/outcomes in the population (\(\tau^2\) is therefore often referred to as the amount of &lsquo;heterogeneity&rsquo; in the true effects/outcomes). The random-effects model can also be written as \[y_i = \mu + u_i + \varepsilon_i,\] where \(u_i \sim N(0, \tau^2)\) and \(\varepsilon_i \sim N(0, v_i)\). The fitted model provides estimates of \(\mu\) and \(\tau^2\). Consequently, the random-effects model provides an <em>unconditional inference</em> about the average true effect/outcome in the population of studies (from which the \(k\) studies included in the meta-analysis are assumed to be a random sample).
</p>
<p>Fitting a random-effects model to the example data yields:
</p>
<pre># fit a random-effects model (note: method="REML" is the default)
res &lt;- rma(yi, vi, data=dat)
res

# Random-Effects Model (k = 48; tau^2 estimator: REML)
#
# tau^2 (estimated amount of total heterogeneity): 0.0499 (SE = 0.0197)
# tau (square root of estimated tau^2 value):      0.2235
# I^2 (total heterogeneity / total variability):   58.37%
# H^2 (total variability / sampling variability):  2.40
#
# Test for Heterogeneity:
# Q(df = 47) = 107.1061, p-val &lt; .0001
#
# Model Results:
#
# estimate      se    zval    pval   ci.lb   ci.ub
#   0.2219  0.0460  4.8209  &lt;.0001  0.1317  0.3122</pre>
<p>The value shown under <code>estimate</code> is now an estimate of the average true standardized mean difference of studies in the population of studies from which the 48 studies included in this dataset have come (i.e., \(\hat{\mu} = 0.22\)).
</p>
<p>When using weighted estimation in the context of a random-effects model, the model is fitted with weights equal to \(w_i = 1/(\tau^2 + v_i)\), with \(\tau^2\) replaced by its estimate (the default in <code><a href="#topic+rma.uni">rma.uni</a></code> when <code>method</code> is set to one of the possible choices for estimating \(\tau^2\)). One can also choose unweighted estimation in the context of the random-effects model (<code>weighted=FALSE</code>) or specify user-defined weights (via <code>weights</code>), although the parameter that is estimated (i.e., \(\mu\)) remains the same regardless of the estimation method and weights used (as opposed to the fixed-effect model, where the parameter estimated is different for weighted versus unweighted estimation or when using different weights than the standard inverse-variance weights). Since weighted estimation with inverse-variance weights is most efficient, it is usually to be preferred for random-effects models (while in the fixed-effect model case, we must carefully consider whether \(\bar{\theta}_w\) or \(\bar{\theta}_u\) is the more meaningful parameter to estimate).
</p>



<h4>Conditional versus Unconditional Inferences</h4>

<p>Contrary to what is often stated in the literature, it is important to realize that the fixed-effects model does <em>not</em> assume that the true effects/outcomes are homogeneous (i.e., that \(\theta_i\) is equal to some common value \(\theta\) in all \(k\) studies). In other words, the fixed-effects model provides perfectly valid inferences under heterogeneity, as long as one is restricting these inferences to the set of studies included in the meta-analysis and one realizes that the model does not provide an estimate of \(\theta\) or \(\mu\), but of \(\bar{\theta}_w\) or \(\bar{\theta}_u\) (depending on the estimation method used).
</p>
<p>However, such inferences are conditional on the included studies. It is therefore not permissible to generalize those inferences beyond the set of studies included in a meta-analysis (or doing so requires &lsquo;extra-statistical&rsquo; arguments). In contrast, a random-effects model provides unconditional inferences and therefore allows a generalization beyond the set of included studies, although the population of studies to which we can generalize is typically only vaguely defined (since the included studies are not a proper random sample from a specified sampling frame). Instead, we simply must assume that the included studies are a representative sample of <em>some</em> population and it is to that population to which we are generalizing.
</p>
<p>Leaving aside this issue, the above implies that there is nothing wrong with fitting both the fixed- and random-effects models to the same data, since these models address inherently different questions (i.e., what was the average effect in the studies that have been conducted and are included in this meta-analysis versus what is the average effect in the larger population of studies?).
</p>



<h4>Equal-Effects Model</h4>

<p>In the special case that the true effects/outcomes are actually homogeneous (the equal-effects case), the distinction between the fixed- and random-effects models disappears, since homogeneity implies that \(\mu = \bar{\theta}_w = \bar{\theta}_u \equiv \theta\). Therefore, if one belives that the true effects/outcomes are homogeneous, then one can fit an equal-effects model (using weighted estimation), since this will provide the most efficient estimate of \(\theta\) (note that if the true effects/outcomes are really homogeneous but we fit a random-effects model, it can happen that the estimate of \(\tau^2\) is actually larger than 0, which then leads to a loss of efficiency).
</p>
<p>However, since there is no infallible method to test whether the true effects/outcomes are really homogeneous or not, a researcher should decide on the type of inference desired before examining the data and choose the model accordingly.
</p>
<p>Note that fitting an equal-effects model (with <code>method="EE"</code>) yields the exact same output as fitting a fixed-effects model, since the equations used to fit these two models are identical. However, the interpretation of the results is different. If we fit an equal-effects model, we make the assumption that the true effects are homogeneous and, if we believe this assumption to be justified, can interpret the estimate as an estimate of <em>the</em> true effect. On the other hand, if we reject the homogeneity assumption, then we should reject the model altogether. In contrast, if we fit a fixed-effects model, we do not assume homogeneity and instead interpret the estimate as an estimate of the (weighted) average true effect of the included studies.
</p>

<p>For further discussions of the distinction between the equal-, fixed-, and random-effects models, see Laird and Mosteller (1990) and Hedges and Vevea (1998).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Cooper, H., Hedges, L. V., &amp; Valentine, J. C. (Eds.) (2009). <em>The handbook of research synthesis and meta-analysis</em> (2nd ed.). New York: Russell Sage Foundation.
</p>
<p>Hedges, L. V., &amp; Vevea, J. L. (1998). Fixed- and random-effects models in meta-analysis. <em>Psychological Methods</em>, <b>3</b>(4), 486&ndash;504. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.3.4.486&#8288;</code>
</p>
<p>Laird, N. M., &amp; Mosteller, F. (1990). Some statistical methods for combining experimental results. <em>International Journal of Technology Assessment in Health Care</em>, <b>6</b>(1), 5&ndash;30. <code style="white-space: pre;">&#8288;https://doi.org/10.1017/S0266462300008916&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>

<hr>
<h2 id='misc-options'>Miscellaneous Options and Features</h2><span id='topic+misc-options'></span><span id='topic+misc_options'></span>

<h3>Description</h3>

<p>This page documents some miscellaneous options and features that do not fit very well elsewhere. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Details</h3>



<h4>Specifying the Confidence Level</h4>

<p>Several functions in the <span class="pkg">metafor</span> package have a <code>level</code> argument for specifying the confidence level when calculating confidence (and prediction) intervals. The default is to use a 95% level throughout the package by convention. Note that values \(>=1\) are treated as coverage percentages, values between 0.5 and 1 as coverage proportions, and values below 0.5 as (two-sided) alpha values, so <code>level=95</code> is the same as <code>level=.95</code> and <code>level=.05</code> (but <code>level=0</code> is always treated as a 0% confidence level).
</p>



<h4>Controlling the Number of Digits in the Output</h4>

<p>Many functions in the <span class="pkg">metafor</span> package have a <code>digits</code> argument, which can be used to control the number of digits that are displayed in the output when printing numeric values. For more control over the displayed output, one can set this argument to a named vector of the form:
</p>
<pre>digits=c(est=2, se=3, test=2, pval=3, ci=2, var=3, sevar=3, fit=3, het=3)</pre>
<p>where the elements control the displayed number of digits for various aspects of the output, namely:
</p>

<ul>
<li> <p><code>est</code> for estimates (e.g., effect sizes, model coefficients, predicted values),
</p>
</li>
<li> <p><code>se</code> for standard errors,
</p>
</li>
<li> <p><code>test</code> for test statistics,
</p>
</li>
<li> <p><code>pval</code> for p-values,
</p>
</li>
<li> <p><code>ci</code> for confidence/prediction interval bounds,
</p>
</li>
<li> <p><code>var</code> for sampling variances and variance components,
</p>
</li>
<li> <p><code>sevar</code> for standard errors thereof,
</p>
</li>
<li> <p><code>fit</code> for fit statistics,
</p>
</li>
<li> <p><code>het</code> for heterogeneity statistics.
</p>
</li></ul>

<p>Instead of setting this argument in each function call, one can use <code>setmfopt(digits = ...)</code> to set the desired number of digits for the various elements (see <code><a href="#topic+mfopt">mfopt</a></code> for getting and setting package options). For example, <code>setmfopt(digits = c(est=2, se=3, test=2, pval=3, ci=2, var=3, sevar=3, fit=3, het=3))</code> could be a sensible choice when analyzing various types of standardized effect size measures.
</p>



<h4>Styled Output with the crayon Package</h4>

<p>The <a href="https://cran.r-project.org/package=crayon">crayon</a> package provides a way to create colored output. The <span class="pkg">metafor</span> package is designed to automatically make use of this feature when the <code>crayon</code> package is installed (<code>install.packages("crayon")</code>) and loaded (<code>library(crayon)</code>). Note that this only works on terminals that support &lsquo;ANSI&rsquo; color/highlight codes (e.g., not under RGui on Windows or R.app on macOS, but the RStudio console and all modern terminals should support this).
</p>
<p>The default color style that is used is quite plain, but should work with a light or dark colored background. One can modify the color style with <code>setmfopt(style = ...)</code>, where <code>...</code> is a list whose elements specify the styles for various parts of the output (see below for some examples and the documentation of the <code>crayon</code> package for the syntax to specify styles). The following elements are recognized:
</p>

<ul>
<li> <p><code>header</code> for the header of tables (underlined by default),
</p>
</li>
<li> <p><code>body1</code> for odd numbered rows in the body of tables,
</p>
</li>
<li> <p><code>body2</code> for even numbered rows in the body of tables,
</p>
</li>
<li> <p><code>na</code> for missing values in tables,
</p>
</li>
<li> <p><code>section</code> for section headers (bold by default),
</p>
</li>
<li> <p><code>text</code> for descriptive text in the output,
</p>
</li>
<li> <p><code>result</code> for the corresponding result(s),
</p>
</li>
<li> <p><code>stop</code> for errors (bold red by default),
</p>
</li>
<li> <p><code>warning</code> for warnings (yellow by default),
</p>
</li>
<li> <p><code>message</code> for messages (green by default),
</p>
</li>
<li> <p><code>verbose</code> for the text in verbose output (cyan by default),
</p>
</li>
<li> <p><code>legend</code> for legends (gray by default).
</p>
</li></ul>

<p>Elements not specified are styled according to their defaults. For example, one could use:
</p>
<pre>setmfopt(style = list(header  = combine_styles("gray20", "underline"),
                      body1   = make_style("gray40"),
                      body2   = make_style("gray40"),
                      na      = bold,
                      section = combine_styles("gray15", "bold"),
                      text    = make_style("gray50"),
                      result  = make_style("gray30"),
                      legend  = make_style("gray70")))</pre>
<p>or
</p>
<pre>setmfopt(style = list(header  = combine_styles("gray80", "underline"),
                      body1   = make_style("gray60"),
                      body2   = make_style("gray60"),
                      na      = bold,
                      section = combine_styles("gray85", "bold"),
                      text    = make_style("gray50"),
                      result  = make_style("gray70"),
                      legend  = make_style("gray30")))</pre>
<p>for a light or dark colored background, respectively. A slightly more colorful style could be:
</p>
<pre>setmfopt(style = list(header  = combine_styles("snow", make_style("royalblue4", bg=TRUE)),
                      body1   = combine_styles("gray10", make_style("gray95", bg=TRUE)),
                      body2   = combine_styles("gray10", make_style("gray85", bg=TRUE)),
                      na      = combine_styles("orange4", "bold"),
                      section = combine_styles("black", "bold", make_style("gray90", bg=TRUE)),
                      text    = make_style("gray40"),
                      result  = make_style("blue"),
                      legend  = make_style("gray70")))</pre>
<p>or
</p>
<pre>setmfopt(style = list(header  = combine_styles("snow", make_style("royalblue4", bg=TRUE)),
                      body1   = combine_styles("gray90", make_style("gray10", bg=TRUE)),
                      body2   = combine_styles("gray90", make_style("gray15", bg=TRUE)),
                      na      = combine_styles("orange1", "bold"),
                      section = combine_styles("snow", "bold", make_style("gray10", bg=TRUE)),
                      text    = make_style("gray60"),
                      result  = make_style("steelblue1"),
                      legend  = make_style("gray30")))</pre>
<p>for a light and dark colored background, respectively.
</p>
<p>The following code snippet includes all output elements (except for an error) and can be used to test out a chosen color style:
</p>
<pre># calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg,
                            ci=cpos, di=cneg, data=dat.bcg)
dat$yi[1] &lt;- NA # set one estimate to missing so we get a warning below
dat

# fit random-effects model
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat, verbose=3)
summary(res)</pre>
<p>For example, using the color scheme above (for a light colored background), the output should look like this:
</p>
<p><img src="../help/figures/crayon1.png" width=800 alt="crayon1.png" />
<img src="../help/figures/crayon2.png" width=800 alt="crayon2.png" />
</p>
<p>Note that support for 256 different colors and text formatting (such as underlined and bold text) differs across terminals.
</p>
<p>To switch off output styling when the <code>crayon</code> package is loaded, use <code>setmfopt(style=FALSE</code>).
</p>



<h4>Removing Empty Lines Before and After the Output</h4>

<p>When printing output, an empty line is usually added before and after the output. For more compact output, this can be suppressed with <code>setmfopt(space=FALSE)</code> (see <code><a href="#topic+mfopt">mfopt</a></code> for getting and setting package options). For example, running the following code:
</p>
<pre># calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg,
                            ci=cpos, di=cneg, data=dat.bcg)

# fit a random-effects model
res &lt;- rma(yi, vi, data=dat)
res

setmfopt(space=FALSE)
res</pre>
<p>shows the difference.
</p>



<h4>Dark Mode for Plots</h4>

<p>By default, plots created in <span class="rlang"><b>R</b></span> have a white background and use black (and other darker colors) as the plotting color. Figures created with the <span class="pkg">metafor</span> package also adhere to this standard. However, all plotting functions in the package are designed in such a way that switching to a dark background is easily possible. For this, one should set the canvas/figure background to a dark color (e.g., <code>"black"</code> or <code>"gray10"</code>) and the foreground color to some bright color (e.g., <code>"gray90"</code>, <code>"gray95"</code>, or <code>"white"</code>). This can be easily accomplished with <code>setmfopt(theme="custom", fg="gray95", bg="gray10")</code> (see <code><a href="#topic+mfopt">mfopt</a></code> for getting and setting package options).
</p>
<p>Figures that make use of additional colors for various plot elements will by default then use colors that are compatible with the chosen background. For example, the following two figures illustrate the difference between the two styles:
</p>

<p><img src="../help/figures/plots-light.png" width=800 alt="plots-light.png" />
<img src="../help/figures/plots-dark.png" width=800 alt="plots-dark.png" />
</p>

<p>By setting <code>setmfopt(theme="dark")</code>, all plots created by the package will automatically use a dark mode. RStudio users can also set <code>setmfopt(theme="auto")</code>, in which case plotting colors are chosen depending on the RStudio theme used (for some themes, setting this to <code>"auto2"</code> might be aesthetically more pleasing).
</p>



<h4>Version Check</h4>

<p>When loading the <span class="pkg">metafor</span> package in an <code><a href="base.html#topic+interactive">interactive</a></code> session, an automatic check is carried out to compare the version number of the installed package with the one available on <a href="https://cran.r-project.org/package=metafor">CRAN</a>. If the installed version is older than the one available on CRAN, the user is notified that a new version is available. This check can be suppressed by setting the environment variable <span class="env">METAFOR_VERSION_CHECK</span> to <code>FALSE</code> (e.g., with <code>Sys.setenv(METAFOR_VERSION_CHECK=FALSE)</code>) or with <code>options(metafor=list(check=FALSE))</code> before loading the package (see <code><a href="#topic+mfopt">mfopt</a></code> for getting and setting package options).
</p>
<p>By setting the environment variable to <code>"devel"</code> (e.g., with <code>Sys.setenv(METAFOR_VERSION_CHECK="devel")</code>) or with <code>options(metafor=list(check="devel"))</code>, the version check is run against the &lsquo;development version&rsquo; of the package available on <a href="https://github.com/wviechtb/metafor">GitHub</a>.
</p>



<h4>Model Fitting / Processing Time</h4>

<p>The various model fitting functions (i.e., <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, <code><a href="#topic+rma.mv">rma.mv</a></code>, and <code><a href="#topic+selmodel">selmodel</a></code>) and various other functions (e.g., <code><a href="#topic+confint.rma.mv">confint</a></code>, <code><a href="#topic+cumul">cumul</a></code>, <code><a href="#topic+leave1out">leave1out</a></code>, <code><a href="#topic+profile.rma.mv">profile</a></code>, <code><a href="#topic+residuals.rma">rstudent</a></code>) automatically keep track of the model fitting / processing time. This information is stored as element <code>time</code> (in seconds) in the object that is returned. One can also use argument <code>time=TRUE</code> to nicely print this information. For example:
</p>
<pre># fit multilevel mixed-effects meta-regression model and print the processing time
res &lt;- rma.mv(yi, vi, mods = ~ condition,
              random = list(~ 1 | article/experiment/sample/id, ~ 1 | pairing),
              data=dat.mccurdy2020, sparse=TRUE, digits=3, time=TRUE)

# extract the processing time (should take somewhere around 10-20 seconds on a modern CPU)
res$time</pre>



<h4>Model Object Sizes</h4>

<p>The objects returned by model fitting functions like <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> contain information that is needed by some of the method functions that can be applied to such objects, but that can lead to objects that are relatively large in size. As an example, the model objects that are created as part of the example code for <code><a href="metadat.html#topic+dat.moura2021">dat.moura2021</a></code> are approximately 120MB in size. To reduce the object size, one can make use of the (undocumented) argument <code>outlist</code>. When setting <code>outlist="minimal"</code>, the resulting object contains only the minimal information needed to print the object (which results in an object that is around 13KB in size). Alternatively, one can set <code>outlist</code> to a string that specifies what objects that are created within the model fitting function should be returned (and under which name). For example, <code>outlist="coef=beta, vcov=vb"</code> would indicate that only the model coefficient(s) (with name <code>coef</code>) and the corresponding variance-covariance matrix (with name <code>vcov</code>) should be returned (the resulting object then is only around 2KB in size). Note that this requires knowledge of how objects within the model fitting function are named, so inspection of the source code of a function will then be necessary. Also, there is no guarantee that method functions will still work when including only a subset of the information that is typically stored in model objects.
</p>



<h4>Load Balancing</h4>

<p>Several functions in the <span class="pkg">metafor</span> package can make use of parallel processing (e.g., <code><a href="#topic+profile.rma.mv">profile</a></code>) to speed up intensive computations on machines with multiple cores. When using <code>parallel="snow"</code>, the default is to use the <code><a href="parallel.html#topic+parLapply">parLapply</a></code> function from the <code><a href="parallel.html#topic+parallel">parallel</a></code> package for this purpose. In some cases (especially when the parallelized computations take up quite variable amounts of time to complete), using &lsquo;load balancing&rsquo; may help to speed things up further (by using the <code><a href="parallel.html#topic+parLapplyLB">parLapplyLB</a></code> function). This can be enabled with <code>pbapply::pboptions(use_lb=TRUE)</code> before running the function that makes use of parallel processing. Whether this really does speed things up depends on many factors and is hard to predict.
</p>



<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>

<hr>
<h2 id='misc-recs'>Some Recommended Practices</h2><span id='topic+misc-recs'></span><span id='topic+misc_recs'></span>

<h3>Description</h3>

<p>This page documents some recommended practices when working with the <span class="pkg">metafor</span> package (and more generally when conducting meta-analyses). <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Details</h3>



<h4>Restricted Maximum Likelihood Estimation</h4>

<p>When fitting models with the <code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> functions, use of restricted maximum likelihood (REML) estimation is generally recommended. This is also the default setting (i.e., <code>method="REML"</code>). Various simulation studies have indicated that REML estimation tends to provide approximately unbiased estimates of the amount of heterogeneity (e.g., Langan et al., 2019; Veroniki et al., 2016; Viechtbauer, 2005), or more generally, of the variance components in more complex mixed-effects models (Harville, 1977).
</p>
<p>For models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, the empirical Bayes / Paule-Mandel estimators (i.e., <code>method="EB"</code> / <code>method="PM"</code>), which can actually be shown to be identical to each other despite their different derivations (Viechtbauer et al., 2015), also have some favorable properties. However, these estimators do not generalize in a straightforward manner to more complex models, such as those that can be fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function.
</p>



<h4>Improved Inference Methods</h4>

<p>When fitting models with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, tests of individual model coefficients and the corresponding confidence intervals are by default (i.e., when <code>test="z"</code>) based on a standard normal distribution, while the omnibus test is based on a chi-square distribution. These inference methods may not perform nominally (i.e., the Type I error rate of tests and the coverage rate of confidence intervals may deviate from the chosen level), especially when the number of studies, \(k\), is low. Therefore, it is highly recommended to use the method by Hartung (1999), Sidik and Jonkman (2002), and Knapp and Hartung (2003) (the Knapp-Hartung method; also referred to as the Hartung-Knapp-Sidik-Jonkman method) by setting <code>test="knha"</code> (or equivalently, <code>test="hksj"</code>). Then tests of individual coefficients and confidence intervals are based on a t-distribution with \(k-p\) degrees of freedom, while the omnibus test then uses an F-distribution with \(m\) and \(k-p\) degrees of freedom (with \(m\) denoting the number of coefficients tested and \(p\) the total number of model coefficients). Various simulation studies have shown that this method works very well in providing tests and confidence intervals with close to nominal performance (e.g., S√°nchez-Meca &amp; Mar√≠n-Mart√≠nez, 2008; Viechtbauer et al., 2015).
</p>
<p>Alternatively, one can also conduct permutation tests using the <code><a href="#topic+permutest">permutest</a></code> function. These also perform very well (and are, in a certain sense, &lsquo;exact&rsquo; tests), but are computationally expensive.
</p>
<p>For models fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> and <code><a href="#topic+rma.glmm">rma.glmm</a></code> functions, the Knapp-Hartung method and permutation tests are not available. Instead, one can set <code>test="t"</code> to also use t- and F-distributions for making inferences (although this does not involve the adjustment to the standard errors of the estimated model coefficients that is made as part of the Knapp-Hartung method). For <code><a href="#topic+rma.mv">rma.mv</a></code>, one should also set <code>dfs="contain"</code>, which uses an improved method for approximating the degrees of freedom of the t- and F-distributions.
</p>
<p>Note that <code>test="z"</code> is the default for the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mv">rma.mv</a></code>, and the <code><a href="#topic+rma.glmm">rma.glmm</a></code> functions. While the improved inference methods described above should ideally be the default, changing this now would break backwards compatibility.
</p>



<h4>General Workflow for Meta-Analyses Involving Complex Dependency Structures</h4>

<p>Many meta-analyses involve observed outcomes / effect size estimates that cannot be assumed to be independent, because some estimates were computed based on the same sample of subjects (or at least a partially overlapping set). In this case, one should compute the covariances for any pair of estimates that involve (fully or partially) overlapping subjects. Doing so is difficult, but we can often construct an approximate variance-covariance matrix (say \(V\)) of such dependent estimates. This can be done with the <code><a href="#topic+vcalc">vcalc</a></code> function (and/or see the <code><a href="#topic+rcalc">rcalc</a></code> function when dealing specifically with dependent correlation coefficients). We can then fit a multivariate/multilevel model to the estimates with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, using \(V\) as the approximate var-cov matrix of the estimates and adding fixed and random effects to the model as deemed necessary. However, since \(V\) is often just a rough approximation (and since the random effects structure may not fully capture all dependencies in the underlying true outcomes/effects), we can then apply cluster-robust inference methods (also known as robust variance estimation) to the model. This can be done with the <code><a href="#topic+robust">robust</a></code> function, which also interfaces with the improved inference methods implemented in the <a href="https://cran.r-project.org/package=clubSandwich">clubSandwich</a> package to obtain the cluster-robust tests and confidence intervals.\(^1\) Finally, we can compute predicted outcomes (with corresponding confidence intervals) and test sets of coefficients or linear combinations thereof using the <code><a href="#topic+predict.rma">predict</a></code> and <code><a href="#topic+anova.rma">anova</a></code> functions. See Pustejovsky and Tipton (2022) for a paper describing such a workflow for various cases.
</p>
<p>To summarize, the general workflow therefore will often consist of these steps:
</p>
<pre># construct/approximate the var-cov matrix of dependent estimates
V &lt;- vcalc(...)

# fit multivariate/multilevel model with appropriate fixed/random effects
res &lt;- rma.mv(yi, V, mods = ~ ..., random = ~ ...)

# apply cluster-robust inference methods (robust variance estimation)
# note: use the improved methods from the clubSandwich package
sav &lt;- robust(res, cluster = ..., clubSandwich = TRUE)
sav

# compute predicted outcomes (with corresponding CIs) as needed
predict(sav, ...)

# test sets of coefficients / linear combinations as needed
anova(sav, ...)</pre>
<p>How <code><a href="#topic+vcalc">vcalc</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> should be used (and the clustering variable specified for <code><a href="#topic+robust">robust</a></code>) will depend on the specifics of the application.
</p>
<p>See <code><a href="metadat.html#topic+dat.assink2016">dat.assink2016</a></code>, <code><a href="metadat.html#topic+dat.knapp2017">dat.knapp2017</a></code>, and <code><a href="metadat.html#topic+dat.tannersmith2016">dat.tannersmith2016</a></code> for some examples illustrating this workflow.
</p>



<h4>Profile Likelihood Plots to Check Parameter Identifiability</h4>

<p>When fitting complex models, it is not guaranteed that all parameters of the model are identifiable (i.e., that there is a unique set of values for the parameters that maximizes the (restricted) likelihood function). For models fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, this pertains especially to the variance/correlation components of the model (i.e., what is specified via the <code>random</code> argument). Therefore, it is strongly advised in general to do post model fitting checks to make sure that the likelihood surface around the ML/REML estimates is not flat for some combination of the parameter estimates (which would imply that the estimates are essentially arbitrary). For example, one can plot the (restricted) log-likelihood as a function of each variance/correlation component in the model to make sure that each profile plot shows a clear peak at the corresponding ML/REML estimate. The <code><a href="#topic+profile.rma">profile</a></code> function can be used for this purpose. See also Raue et al. (2009) for some further discussion of parameter identifiability and the use of profile likelihoods to check for this.
</p>
<p>The <code><a href="#topic+profile.rma">profile</a></code> function should also be used after fitting location-scale models (Viechtbauer &amp; L√≥pez-L√≥pez, 2022) with the <code><a href="#topic+rma.uni">rma.uni</a></code> function and after fitting selection models with the <code><a href="#topic+selmodel">selmodel</a></code> function.
</p>

<p>&mdash;&mdash;&mdash;
</p>
<p>\(^1\) In small meta-analyses, the (denominator) degrees of freedom for the approximate t- and F-tests provided by the cluster-robust inference methods might be very low, in which case the tests may not be trustworthy and overly conservative (Joshi et al., 2022). Under these circumstances, one can consider the use of cluster wild bootstrapping (as implemented in the <a href="https://cran.r-project.org/package=wildmeta">wildmeta</a> package) as an alternative method for making inferences.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Hartung, J. (1999). An alternative method for meta-analysis. <em>Biometrical Journal</em>, <b>41</b>(8), 901&ndash;916. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W&#8288;</code>
</p>
<p>Harville, D. A. (1977). Maximum likelihood approaches to variance component estimation and to related problems. <em>Journal of the American Statistical Association</em>, <b>72</b>(358), 320&ndash;338. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2286796&#8288;</code>
</p>
<p>Joshi, M., Pustejovsky, J. E., &amp; Beretvas, S. N. (2022). Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies. <em>Research Synthesis Methods</em>, <b>13</b>(4), 457&ndash;477. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1554&#8288;</code>
</p>
<p>Knapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <em>Statistics in Medicine</em>, <b>22</b>(17), 2693&ndash;2710. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1482&#8288;</code>
</p>
<p>Langan, D., Higgins, J. P. T., Jackson, D., Bowden, J., Veroniki, A. A., Kontopantelis, E., Viechtbauer, W. &amp; Simmonds, M. (2019). A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses. <em>Research Synthesis Methods</em>, <b>10</b>(1), 83&ndash;98. https://doi.org/10.1002/jrsm.1316
</p>
<p>Pustejovsky, J. E. &amp; Tipton, E. (2022). Meta-analysis with robust variance estimation: Expanding the range of working models. <em>Prevention Science</em>, <b>23</b>, 425&ndash;438. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/s11121-021-01246-3&#8288;</code>
</p>
<p>Raue, A., Kreutz, C., Maiwald, T., Bachmann, J., Schilling, M., Klingmuller, U., &amp; Timmer, J. (2009). Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood. <em>Bioinformatics</em>, <b>25</b>(15), 1923&ndash;1929. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/bioinformatics/btp358&#8288;</code>
</p>
<p>S√°nchez-Meca, J. &amp; Mar√≠n-Mart√≠nez, F. (2008). Confidence intervals for the overall effect size in random-effects meta-analysis. <em>Psychological Methods</em>, <b>13</b>(1), 31&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989x.13.1.31&#8288;</code>
</p>
<p>Sidik, K. &amp; Jonkman, J. N. (2002). A simple confidence interval for meta-analysis. <em>Statistics in Medicine</em>, <b>21</b>(21), 3153&ndash;3159. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1262&#8288;</code>
</p>
<p>Veroniki, A. A., Jackson, D., Viechtbauer, W., Bender, R., Bowden, J., Knapp, G., Kuss, O., Higgins, J. P., Langan, D., &amp; Salanti, G. (2016). Methods to estimate the between-study variance and its uncertainty in meta-analysis. <em>Research Synthesis Methods</em>, <b>7</b>(1), 55&ndash;79. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1164&#8288;</code>
</p>
<p>Viechtbauer, W. (2005). Bias and efficiency of meta-analytic variance estimators in the random-effects model. <em>Journal of Educational and Behavioral Statistics</em>, <b>30</b>(3), 261&ndash;293. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986030003261&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., L√≥pez-L√≥pez, J. A., S√°nchez-Meca, J., &amp; Mar√≠n-Mart√≠nez, F. (2015). A comparison of procedures to test for moderators in mixed-effects meta-regression models. <em>Psychological Methods</em>, <b>20</b>(3), 360&ndash;374. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/met0000023&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>

<hr>
<h2 id='model.matrix.rma'>Extract the Model Matrix from 'rma' Objects</h2><span id='topic+model.matrix'></span><span id='topic+model.matrix.rma'></span>

<h3>Description</h3>

<p>Function to extract the model matrix from objects of class <code>"rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
model.matrix(object, asdf, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.matrix.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="model.matrix.rma_+3A_asdf">asdf</code></td>
<td>
<p>logical to specify whether the model matrix should be turned into a data frame (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="model.matrix.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The model matrix.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which a model matrix can be extracted.
</p>
<p><code><a href="#topic+fitted.rma">fitted</a></code> for a function to extract the fitted values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### extract the model matrix
model.matrix(res)
</code></pre>

<hr>
<h2 id='permutest'>Permutation Tests for 'rma.uni' Objects</h2><span id='topic+permutest'></span><span id='topic+permutest.rma.uni'></span><span id='topic+permutest.rma.ls'></span>

<h3>Description</h3>

<p>Function to carry out permutation tests for objects of class <code>"rma.uni"</code> and <code>"rma.ls"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permutest(x, ...)

## S3 method for class 'rma.uni'
permutest(x, exact=FALSE, iter=1000, permci=FALSE,
          progbar=TRUE, digits, control, ...)

## S3 method for class 'rma.ls'
permutest(x, exact=FALSE, iter=1000,
          progbar=TRUE, digits, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permutest_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code> or <code>"rma.ls"</code>.</p>
</td></tr>
<tr><td><code id="permutest_+3A_exact">exact</code></td>
<td>
<p>logical to specify whether an exact permutation test should be carried out (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="permutest_+3A_iter">iter</code></td>
<td>
<p>integer to specify the number of iterations for the permutation test when not doing an exact test (the default is <code>1000</code>).</p>
</td></tr>
<tr><td><code id="permutest_+3A_permci">permci</code></td>
<td>
<p>logical to specify whether permutation-based confidence intervals (CIs) should also be constructed (the default is <code>FALSE</code>). Can also be a vector of indices to specify for which coefficients a permutation-based CI should be obtained.</p>
</td></tr>
<tr><td><code id="permutest_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="permutest_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="permutest_+3A_control">control</code></td>
<td>
<p>list of control values for numerical comparisons (<code>comptol</code>) and for <code><a href="stats.html#topic+uniroot">uniroot</a></code> (i.e., <code>tol</code> and <code>maxiter</code>). The latter is only relevant when <code>permci=TRUE</code>. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="permutest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For models without moderators, the permutation test is carried out by permuting the signs of the observed effect sizes or outcomes. The (two-sided) p-value of the permutation test is then equal to the proportion of times that the absolute value of the test statistic under the permuted data is as extreme or more extreme than under the actually observed data. See Follmann and Proschan (1999) for more details.
</p>
<p>For models with moderators, the permutation test is carried out by permuting the rows of the model matrix (i.e., \(X\)). The (two-sided) p-value for a particular model coefficient is then equal to the proportion of times that the absolute value of the test statistic for the coefficient under the permuted data is as extreme or more extreme than under the actually observed data. Similarly, for the omnibus test, the p-value is the proportion of times that the test statistic for the omnibus test is as extreme or more extreme than the actually observed one. See Higgins and Thompson (2004) and Viechtbauer et al. (2015) for more details.
</p>


<h4>Exact versus Approximate Permutation Tests</h4>

<p>If <code>exact=TRUE</code>, the function will try to carry out an exact permutation test. An exact permutation test requires fitting the model to each possible permutation. However, the number of possible permutations increases rapidly with the number of outcomes/studies (i.e., \(k\)). For models without moderators, there are \(2^k\) possible permutations of the signs. Therefore, for \(k=5\), there are 32 possible permutations, for \(k=10\), there are already 1024, and for \(k=20\), there are over one million such permutations.
</p>
<p>For models with moderators, the increase in the number of possible permutations may be even more severe. The total number of possible permutations of the model matrix is \(k!\). Therefore, for \(k=5\), there are 120 possible permutations, for \(k=10\), there are 3,628,800, and for \(k=20\), there are over \(10^{18}\) permutations of the model matrix.
</p>
<p>Therefore, going through all possible permutations may become infeasible. Instead of using an exact permutation test, one can set <code>exact=FALSE</code> (which is also the default). In that case, the function approximates the exact permutation-based p-value(s) by going through a smaller number (as specified by the <code>iter</code> argument) of <em>random</em> permutations. Therefore, running the function twice on the same data can yield (slightly) different p-values. Setting <code>iter</code> sufficiently large ensures that the results become stable. For full reproducibility, one can also set the seed of the random number generator before running the function (see &lsquo;Examples&rsquo;). Note that if <code>exact=FALSE</code> and <code>iter</code> is actually larger than the number of iterations required for an exact permutation test, then an exact test will automatically be carried out.
</p>
<p>For models with moderators, the exact permutation test actually only requires fitting the model to each <em>unique</em> permutation of the model matrix. The number of unique permutations will be smaller than \(k!\) when the model matrix contains recurring rows. This may be the case when only including categorical moderators (i.e., factors) in the model or when any quantitative moderators included in the model can only take on a small number of unique values. When <code>exact=TRUE</code>, the function therefore uses an algorithm to restrict the test to only the unique permutations of the model matrix, which may make the use of the exact test feasible even when \(k\) is large.
</p>
<p>One can also set <code>exact="i"</code> in which case the function just returns the number of iterations required for an exact permutation test.
</p>
<p>When using random permutations, the function ensures that the very first permutation will always correspond to the original data. This avoids p-values equal to 0.
</p>



<h4>Permutation-Based Confidence Intervals</h4>

<p>When <code>permci=TRUE</code>, the function also tries to obtain permutation-based confidence intervals (CIs) of the model coefficient(s). This is done by shifting the observed effect sizes or outcomes by some amount and finding the most extreme values for this amount for which the permutation-based test would just lead to non-rejection. The calculation of such CIs is computationally expensive and may take a long time to complete. For models with moderators, one can also set <code>permci</code> to a vector of indices to specify for which coefficient(s) a permutation-based CI should be obtained. When the algorithm fails to determine a particular CI bound, it will be shown as <code>NA</code> in the output.
</p>



<h4>Permutation Tests for Location-Scale Models</h4>

<p>The function also works with location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code> for details on such models). Permutation tests will then be carried out for both the location and scale parts of the model. However, note that permutation-based CIs are not available for location-scale models.
</p>



<h3>Value</h3>

<p>An object of class <code>"permutest.rma.uni"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>pval</code></td>
<td>
<p>p-value(s) based on the permutation test.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>p-value for the omnibus test of moderators based on the permutation test.</p>
</td></tr>
<tr><td><code>zval.perm</code></td>
<td>
<p>values of the test statistics of the coefficients under the various permutations.</p>
</td></tr>
<tr><td><code>b.perm</code></td>
<td>
<p>the model coefficients under the various permutations.</p>
</td></tr>
<tr><td><code>QM.perm</code></td>
<td>
<p>the test statistic of the omnibus test of moderators under the various permutations.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients (permutation-based when <code>permci=TRUE</code>).</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients (permutation-based when <code>permci=TRUE</code>).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values are passed on.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.permutest.rma.uni">print</a></code> function. One can also use <code><a href="#topic+coef.permutest.rma.uni">coef</a></code> to obtain the table with the model coefficients, corresponding standard errors, test statistics, p-values, and confidence interval bounds. The permutation distribution(s) can be plotted with the <code><a href="#topic+plot.permutest.rma.uni">plot</a></code> function.
</p>


<h3>Note</h3>

<p>The p-values obtained with permutation tests cannot reach conventional levels of statistical significance (i.e., \(p \le .05\)) when \(k\) is very small. In particular, for models without moderators, the smallest possible (two-sided) p-value is .0625 when \(k=5\) and .03125 when \(k=6\). Therefore, the permutation test is only able to reject the null hypothesis at \(\alpha=.05\) when \(k\) is at least equal to 6. For models with moderators, the smallest possible (two-sided) p-value for a particular model coefficient is .0833 when \(k=4\) and .0167 when \(k=5\) (assuming that each row in the model matrix is unique). Therefore, the permutation test is only able to reject the null hypothesis at \(\alpha=.05\) when \(k\) is at least equal to 5. Consequently, permutation-based CIs can also only be obtained when \(k\) is sufficiently large.
</p>
<p>When the number of permutations required for the exact test is so large as to be essentially indistinguishable from infinity (e.g., <code>factorial(200)</code>), the function will terminate with an error.
</p>
<p>Determining whether a test statistic under the permuted data is as extreme or more extreme than under the actually observed data requires making <code>&gt;=</code> or <code>&lt;=</code> comparisons. To avoid problems due to the finite precision with which computers generally represent numbers (see <a href="https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f">this</a> FAQ for details), the function uses a numerical tolerance (<code>control</code> argument <code>comptol</code>, which is set equal to <code>.Machine$double.eps^0.5</code> by default) when making such comparisons (e.g., instead of <code>sqrt(3)^2 &gt;= 3</code>, which may evaluate to <code>FALSE</code>, we use <code>sqrt(3)^2 &gt;= 3 - .Machine$double.eps^0.5</code>, which should evaluate to <code>TRUE</code>).
</p>
<p>When obtaining permutation-based CIs, the function makes use of <code><a href="stats.html#topic+uniroot">uniroot</a></code>. By default, the desired accuracy is set equal to <code>.Machine$double.eps^0.25</code> and the maximum number of iterations to <code>100</code>. The desired accuracy and the maximum number of iterations can be adjusted with the <code>control</code> argument (i.e., <code>control=list(tol=value, maxiter=value)</code>). Also, the interval searched for the CI bounds may be too narrow, leading to <code>NA</code> for a bound. In this case, one can try setting <code>control=list(distfac=value)</code> with a value larger than 1 to extend the interval (the value indicating a multiplicative factor by which to extend the width of the interval searched) or <code>control=list(extendInt="yes")</code> to allow <code><a href="stats.html#topic+uniroot">uniroot</a></code> to extend the interval dynamically (in which case it can happen that a bound may try to drift towards \(\pm \infty\)).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Follmann, D. A., &amp; Proschan, M. A. (1999). Valid inference in random effects meta-analysis. <em>Biometrics</em>, <b>55</b>(3), 732&ndash;737. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.0006-341x.1999.00732.x&#8288;</code>
</p>
<p>Good, P. I. (2009). <em>Permutation, parametric, and bootstrap tests of hypotheses</em> (3rd ed.). New York: Springer.
</p>
<p>Higgins, J. P. T., &amp; Thompson, S. G. (2004). Controlling the risk of spurious findings from meta-regression. <em>Statistics in Medicine</em>, <b>23</b>(11), 1663&ndash;1682. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1752&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., L√≥pez-L√≥pez, J. A., S√°nchez-Meca, J., &amp; Mar√≠n-Mart√≠nez, F. (2015). A comparison of procedures to test for moderators in mixed-effects meta-regression models. <em>Psychological Methods</em>, <b>20</b>(3), 360&ndash;374. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/met0000023&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit models for which permutation tests can be conducted.
</p>
<p><code><a href="#topic+print.permutest.rma.uni">print</a></code> and <code><a href="#topic+plot.permutest.rma.uni">plot</a></code> for the print and plot methods and <code><a href="#topic+coef.permutest.rma.uni">coef</a></code> for a method to extract the model results table.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### random-effects model
res &lt;- rma(yi, vi, data=dat)
res

## Not run: 
### permutation test (approximate and exact)
set.seed(1234) # for reproducibility
permutest(res)
permutest(res, exact=TRUE)

## End(Not run)

### mixed-effects model with two moderators (absolute latitude and publication year)
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)
res

### number of iterations required for an exact permutation test
permutest(res, exact="i")

## Not run: 
### permutation test (approximate only; exact not feasible)
set.seed(1234) # for reproducibility
permres &lt;- permutest(res, iter=10000)
permres

### plot of the permutation distribution for absolute latitude
### dashed horizontal line: the observed value of the test statistic (in both tails)
### black curve: standard normal density (theoretical reference/null distribution)
### blue curve: kernel density estimate of the permutation distribution
### note: the tail area under the permutation distribution is larger
### than under a standard normal density (hence, the larger p-value)
plot(permres, beta=2, lwd=c(2,3,3,4), xlim=c(-5,5))

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.cumul.rma'>Plot Method for 'cumul.rma' Objects</h2><span id='topic+plot.cumul.rma'></span>

<h3>Description</h3>

<p>Function to plot objects of class <code>"cumul.rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cumul.rma'
plot(x, yaxis, xlim, ylim, xlab, ylab,
     at, transf, atransf, targs, digits, cols,
     grid=TRUE, pch=19, cex=1, lwd=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cumul.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"cumul.rma"</code> obtained with <code><a href="#topic+cumul">cumul</a></code>.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_yaxis">yaxis</code></td>
<td>
<p>either <code>"tau2"</code>, <code>"I2"</code>, or <code>"H2"</code> to indicate what values should be placed on the y-axis. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_at">at</code></td>
<td>
<p>position of the x-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the summary estimates (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the x-axis labels (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the tick mark labels of the x- and y-axis should be rounded. Can also be a vector of two integers, the first to specify the number of decimal places for the x-axis, the second for the y-axis labels (e.g., <code>digits=c(2,3)</code>). If unspecified, the function tries to set the argument to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_cols">cols</code></td>
<td>
<p>vector with two or more colors for visualizing the order of the cumulative results.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_grid">grid</code></td>
<td>
<p>logical to specify whether a grid should be added to the plot. Can also be a color name.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use. By default, a filled circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_cex">cex</code></td>
<td>
<p>symbol expansion factor.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_lwd">lwd</code></td>
<td>
<p>line width.</p>
</td></tr>
<tr><td><code id="plot.cumul.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to visualize the results from a cumulative meta-analysis as obtained with the <code><a href="#topic+cumul">cumul</a></code> function.
</p>
<p>The plot shows the model estimate (i.e., the estimated overall/average outcome) on the x-axis and some measure of heterogeneity on the y-axis in the cumulative order of the results in the <code>"cumul.rma"</code> object. By default, \(\tau^2\) is shown on the y-axis for a random-effects model and \(I^2\) otherwise, but one can also use argument <code>yaxis</code> to specify the measure of heterogeneity to place on the y-axis.
</p>
<p>The color gradient of the points/lines indicates the order of the cumulative results (by default, light gray at the beginning, dark gray at the end). A different set of colors can be chosen via the <code>cols</code> argument. See &lsquo;Examples&rsquo;.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cumul.rma.uni">cumul</a></code> for the function to conduct a cumulative meta-analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### random-effects model
res &lt;- rma(yi, vi, data=dat)

### cumulative meta-analysis (in the order of publication year)
sav &lt;- cumul(res, order=year)

### plot of model estimate and tau^2 over time
plot(sav)

### illustrate some other plot options
plot(sav, yaxis="I2", ylim=c(0,100), transf=exp, xlim=c(0.25,0.55),
     lwd=5, cex=1.5, cols=c("green","blue","red"))
</code></pre>

<hr>
<h2 id='plot.gosh.rma'>Plot Method for 'gosh.rma' Objects</h2><span id='topic+plot.gosh.rma'></span>

<h3>Description</h3>

<p>Function to plot objects of class <code>"gosh.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gosh.rma'
plot(x, het="I2", pch=16, cex, out, col, alpha, border,
     xlim, ylim, xhist=TRUE, yhist=TRUE, hh=0.3, breaks,
     adjust, lwd, labels, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gosh.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"gosh.rma"</code> obtained with <code><a href="#topic+gosh">gosh</a></code>.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_het">het</code></td>
<td>
<p>character string to specify the heterogeneity measure to plot. Either <code>"I2"</code>, <code>"H2"</code>, <code>"QE"</code>, <code>"tau2"</code>, or <code>"tau"</code> (the last two only for random/mixed-effects models).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use. By default, a borderless filled circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_cex">cex</code></td>
<td>
<p>symbol expansion factor.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_out">out</code></td>
<td>
<p>optional integer to specify the number of a study that may be a potential outlier. If specified, subsets containing the specified study are drawn in a different color than those not containing the study.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the points (if unspecified, points are drawn in black). When <code>out</code> is used, two colors should be specified (if unspecified, red is used for subsets containing the specified study and blue otherwise).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_alpha">alpha</code></td>
<td>
<p>optional alpha transparency value for the points (0 means fully transparent and 1 means opaque). If unspecified, the function sets this to a sensible value.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_border">border</code></td>
<td>
<p>optional character string to specify the color of the borders of the histogram bars. Set to <code>FALSE</code> to omit the borders.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_xhist">xhist</code></td>
<td>
<p>logical to specify whether a histogram should be drawn for the x-axis (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_yhist">yhist</code></td>
<td>
<p>logical to specify whether a histogram should be drawn for the y-axis (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_hh">hh</code></td>
<td>
<p>numeric value (or vector of two values) to adjust the height of the histogram(s). Must be between 0 and 1, but should not be too close to 0 or 1, as otherwise the plot cannot be drawn.</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_breaks">breaks</code></td>
<td>
<p>optional argument passed on to <code><a href="graphics.html#topic+hist">hist</a></code> for choosing the (number of) breakpoints of the histogram(s).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_adjust">adjust</code></td>
<td>
<p>optional argument passed on to <code><a href="stats.html#topic+density">density</a></code> for adjusting the bandwidth of the kernel density estimate(s) (values larger than 1 result in more smoothing).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_lwd">lwd</code></td>
<td>
<p>optional numeric value to specify the line width of the estimated densities. Set to <code>0</code> to omit the line(s).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_labels">labels</code></td>
<td>
<p>optional argument to specify the x-axis and y-axis labels (or passed on to <code><a href="graphics.html#topic+pairs">pairs</a></code> to specify the names of the variables in the scatter plot matrix).</p>
</td></tr>
<tr><td><code id="plot.gosh.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For models without moderators, the function draws a scatter plot of the model estimates on the x-axis against the chosen measure of heterogeneity on the y-axis for the various subsets. Histograms of the respective distributions (with kernel density estimates superimposed) are shown in the margins (when <code>xhist=TRUE</code> and <code>yhist=TRUE</code>).
</p>
<p>For models with moderators, the function draws a scatter plot matrix (with the <code><a href="graphics.html#topic+pairs">pairs</a></code> function) of the chosen measure of heterogeneity and each of the model coefficients. Histograms of the variables plotted are shown along the diagonal, with kernel density estimates of the distributions superimposed. Arguments <code>xlim</code>, <code>ylim</code>, <code>xhist</code>, and <code>yhist</code> are then ignored, while argument <code>hh</code> can be used to compress/stretch the height of the distributions shown along the diagonal.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Olkin, I., Dahabreh, I. J., &amp; Trikalinos, T. A. (2012). GOSH - a graphical display of study heterogeneity. <em>Research Synthesis Methods</em>, <b>3</b>(3), 214&ndash;223. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1053&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gosh">gosh</a></code> for the function to create the input to a GOSH plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat.egger2001)

### meta-analysis of all trials including ISIS-4 using an equal-effects model
res &lt;- rma(yi, vi, data=dat, method="EE")

### fit FE model to all possible subsets (65535 models)
## Not run: 
sav &lt;- gosh(res, progbar=FALSE)

### create GOSH plot
### red points for subsets that include and blue points
### for subsets that exclude study 16 (the ISIS-4 trial)
plot(sav, out=16, breaks=100)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.infl.rma.uni'>Plot Method for 'infl.rma.uni' Objects</h2><span id='topic+plot.infl.rma.uni'></span>

<h3>Description</h3>

<p>Function to plot objects of class <code>"infl.rma.uni"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'infl.rma.uni'
plot(x, plotinf=TRUE, plotdfbs=FALSE, dfbsnew=FALSE, logcov=TRUE,
     layout, slab.style=1, las=0, pch=21, bg, bg.infl, col.na, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.infl.rma.uni_+3A_x">x</code></td>
<td>
<p>an object of class <code>"infl.rma.uni"</code> obtained with <code><a href="#topic+influence.rma.uni">influence</a></code>.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_plotinf">plotinf</code></td>
<td>
<p>logical to specify whether the various case diagnostics should be plotted (the default is <code>TRUE</code>). Can also be a vector of up to 8 integers to specify which plots to draw. See &lsquo;Details&rsquo; for the numbers corresponding to the various plots.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_plotdfbs">plotdfbs</code></td>
<td>
<p>logical to specify whether the DFBETAS values should be plotted (the default is <code>FALSE</code>). Can also be a vector of integers to specify for which coefficient(s) to plot the DFBETAS values.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_dfbsnew">dfbsnew</code></td>
<td>
<p>logical to specify whether a new device should be opened for plotting the DFBETAS values (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_logcov">logcov</code></td>
<td>
<p>logical to specify whether the covariance ratios should be plotted on a log scale (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_layout">layout</code></td>
<td>
<p>optional vector of two numbers to specify the number of rows and columns for the layout of the figure.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_slab.style">slab.style</code></td>
<td>
<p>integer to indicate the style of the x-axis labels: 1 = study number, 2 = study label, 3 = abbreviated study label. Note that study labels, even when abbreviated, may be too long to fit in the margins.)</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_las">las</code></td>
<td>
<p>integer between 0 and 3 to specify the alignment of the axis labels (see <code><a href="graphics.html#topic+par">par</a></code>). The most useful alternative to 0 is 3, so that the x-axis labels are drawn vertical to the axis.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use. By default, an open circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_bg">bg</code></td>
<td>
<p>optional character string to specify the background color of open plotting symbols. If unspecified, gray is used by default.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_bg.infl">bg.infl</code></td>
<td>
<p>optional character string to specify the background color when the point is considered influential. If unspecified, red is used by default.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_col.na">col.na</code></td>
<td>
<p>optional character string to specify the color for lines connecting two points with <code>NA</code> values in between. If unspecified, a light shade of gray is used by default.</p>
</td></tr>
<tr><td><code id="plot.infl.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>plotinf=TRUE</code>, the function plots the (1) externally standardized residuals, (2) DFFITS values, (3) Cook's distances, (4) covariance ratios, (5) leave-one-out \(\tau^2\) estimates, (6) leave-one-out (residual) heterogeneity test statistics, (7) hat values, and (8) weights. If <code>plotdfbs=TRUE</code>, the DFBETAS values are also plotted either after confirming the page change (if <code>dfbsnew=FALSE</code>) or on a separate device (if <code>dfbsnew=TRUE</code>).
</p>
<p>A case (which is typically synonymous with study) may be considered to be &lsquo;influential&rsquo; if at least one of the following is true:
</p>

<ul>
<li><p> The absolute DFFITS value is larger than \(3 \times \sqrt{p/(k-p)}\), where \(p\) is the number of model coefficients and \(k\) the number of cases.
</p>
</li>
<li><p> The lower tail area of a chi-square distribution with \(p\) degrees of freedom cut off by the Cook's distance is larger than 50%.
</p>
</li>
<li><p> The hat value is larger than \(3 \times (p/k)\).
</p>
</li>
<li><p> Any DFBETAS value is larger than \(1\).
</p>
</li></ul>

<p>Cases which are considered influential with respect to any of these measures are indicated by the color specified for the <code>bg.infl</code> argument (the default is <code>"red"</code>).
</p>
<p>The cut-offs described above are indicated in the plot with horizontal reference lines. In addition, on the plot of the externally standardized residuals, horizontal reference lines are drawn at -1.96, 0, and 1.96. On the plot of the covariance ratios, a horizontal reference line is drawn at 1. On the plot of leave-one-out \(\tau^2\) estimates, a horizontal reference line is drawn at the \(\tau^2\) estimate based on all cases. On the plot of leave-one-out (residual) heterogeneity test statistics, horizontal reference lines are drawn at the test statistic based on all cases and at \(k-p\), the degrees of freedom of the test statistic. On the plot of the hat values, a horizontal reference line is drawn at \(p/k\). Since the sum of the hat values is equal to \(p\), the value \(p/k\) indicates equal hat values for all \(k\) cases. Finally, on the plot of weights, a horizontal reference line is drawn at \(100/k\), corresponding to the value for equal weights (in %) for all \(k\) cases. Note that all weights will automatically be equal to each other when using unweighted model fitting. Also, the hat values will be equal to the weights (except for their scaling) in models without moderators.
</p>
<p>The chosen cut-offs are (somewhat) arbitrary. Substantively informed judgment should always be used when examining the influence of each case on the results.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; Cheung, M. W.-L. (2010). Outlier and influence diagnostics for meta-analysis. <em>Research Synthesis Methods</em>, <b>1</b>(2), 112&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.11&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+influence.rma.uni">influence</a></code> for the function to compute the various model diagnostics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### compute the diagnostics
inf &lt;- influence(res)

### plot the values
plot(inf)

### select which plots to show
plot(inf, plotinf=1:4)
plot(inf, plotinf=1:4, layout=c(4,1))

### plot the DFBETAS values
plot(inf, plotinf=FALSE, plotdfbs=TRUE)
</code></pre>

<hr>
<h2 id='plot.permutest.rma.uni'>Plot Method for 'permutest.rma.uni' Objects</h2><span id='topic+plot.permutest.rma.uni'></span>

<h3>Description</h3>

<p>Function to plot objects of class <code>"permutest.rma.uni"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'permutest.rma.uni'
plot(x, beta, alpha, QM=FALSE, QS=FALSE,
     breaks="Scott", freq=FALSE, col, border, col.out, col.ref, col.density,
     trim=0, adjust=1, lwd=c(2,0,0,4), layout, legend=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.permutest.rma.uni_+3A_x">x</code></td>
<td>
<p>an object of class <code>"permutest.rma.uni"</code> obtained with <code><a href="#topic+permutest">permutest</a></code>.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_beta">beta</code></td>
<td>
<p>optional vector of indices to specify which (location) coefficients should be plotted.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_alpha">alpha</code></td>
<td>
<p>optional vector of indices to specify which scale coefficients should be plotted. Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_qm">QM</code></td>
<td>
<p>logical to specify whether the permutation distribution of the omnibus test of the (location) coefficients should be plotted (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_qs">QS</code></td>
<td>
<p>logical to specify whether the permutation distribution of the omnibus test of the scale coefficients should be plotted (the default is <code>FALSE</code>). Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_breaks">breaks</code></td>
<td>
<p>argument to be passed on to the corresponding argument of <code><a href="graphics.html#topic+hist">hist</a></code> to set (the method for determining) the (number of) breakpoints.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_freq">freq</code></td>
<td>
<p>logical to indicate whether frequencies or probability densities should be plotted (the default is <code>FALSE</code> to plot densities).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the histogram bars.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_border">border</code></td>
<td>
<p>optional character string to specify the color of the borders around the bars.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_col.out">col.out</code></td>
<td>
<p>optional character string to specify the color of the bars that are more extreme than the observed test statistic (the default is a semi-transparent shade of red).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_col.ref">col.ref</code></td>
<td>
<p>optional character string to specify the color of the theoretical reference/null distribution that is superimposed on top of the histogram (the default is a dark shade of gray).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_col.density">col.density</code></td>
<td>
<p>optional character string to specify the color of the kernel density estimate of the permutation distribution that is superimposed on top of the histogram (the default is blue).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_trim">trim</code></td>
<td>
<p>the fraction (up to 0.5) of observations to be trimmed from the tails of each permutation distribution before its histogram is plotted.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_adjust">adjust</code></td>
<td>
<p>numeric value to be passed on to the corresponding argument of <code><a href="stats.html#topic+density">density</a></code> (for adjusting the bandwidth of the kernel density estimate).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_lwd">lwd</code></td>
<td>
<p>numeric vector to specify the width of the vertical lines corresponding to the value of the observed test statistic, of the theoretical reference/null distribution, of the density estimate, and of the vertical line at 0 (note: by default, the theoretical reference/null distribution and the density estimate both have a line width of 0 and are therefore not plotted).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_layout">layout</code></td>
<td>
<p>optional vector of two numbers to specify the number of rows and columns for the layout of the figure.</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_legend">legend</code></td>
<td>
<p>logical to indicate whether a legend should be added to the plot (the default is <code>FALSE</code>). Can also be a keyword to indicate the position of the legend (see <code><a href="graphics.html#topic+legend">legend</a></code>).</p>
</td></tr>
<tr><td><code id="plot.permutest.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function plots the permutation distribution of each model coefficient as a histogram.
</p>
<p>For models with moderators, one can choose via argument <code>beta</code> which coefficients to plot (by default, all permutation distributions except that of the intercept are plotted). One can also choose to plot the permutation distribution of the omnibus test of the model coefficients (by setting <code>QM=TRUE</code>).
</p>
<p>Arguments <code>breaks</code>, <code>freq</code>, <code>col</code>, and <code>border</code> are passed on to the <code><a href="graphics.html#topic+hist">hist</a></code> function for the plotting.
</p>
<p>Argument <code>trim</code> can be used to trim away a certain fraction of observations from the tails of each permutation distribution before its histogram is plotted. By setting this to a value above 0, one can quickly remove some of the extreme values that might lead to the bulk of the distribution getting squished together at the center (typically, a small value such as <code>trim=0.01</code> is sufficient for this purpose).
</p>
<p>The observed test statistic is indicated as a vertical dashed line (in both tails for a two-sided test).
</p>
<p>Argument <code>col.out</code> is used to specify the color for the bars in the histogram that are more extreme than the observed test statistic. The p-value of a permutation test corresponds to the area of these bars.
</p>
<p>One can superimpose the theoretical reference/null distribution on top of the histogram (i.e., the distribution as assumed by the model). The p-value for the standard (i.e., non-permutation) test is the area that is more extreme than the observed test statistic under this reference/null distribution.
</p>
<p>A kernel density estimate of the permutation distribution can also be superimposed on top of the histogram (as a smoothed representation of the permutation distribution).
</p>
<p>Note that the theoretical reference/null distribution and the kernel density estimate of the permutation distribution are only shown when setting the line width for these elements greater than 0 via the <code>lwd</code> argument (e.g., <code>lwd=c(2,2,2,4)</code>).
</p>
<p>For location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code> for details), one can also use arguments <code>alpha</code> and <code>QS</code> to specify which scale coefficients to plot and whether to also plot the permutation distribution of the omnibus test of the scale coefficients (by setting <code>QS=TRUE</code>).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permutest.rma.uni">permutest</a></code> for the function to create <code>permutest.rma.uni</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### random-effects model
res &lt;- rma(yi, vi, data=dat)
res

## Not run: 
### permutation test (exact)
permres &lt;- permutest(res, exact=TRUE)
permres

### plot of the permutation distribution
### dashed horizontal line: the observed value of the test statistic (in both tails)
### black curve: standard normal density (theoretical reference/null distribution)
### blue curve: kernel density estimate of the permutation distribution
plot(permres, lwd=c(2,3,3,4))

### mixed-effects model with two moderators (absolute latitude and publication year)
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)
res

### permutation test (approximate)
set.seed(1234) # for reproducibility
permres &lt;- permutest(res, iter=10000)
permres

### plot of the permutation distribution for absolute latitude
### note: the tail area under the permutation distribution is larger
### than under a standard normal density (hence, the larger p-value)
plot(permres, beta=2, lwd=c(2,3,3,4), xlim=c(-5,5))

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.rma'>Plot Method for 'rma' Objects</h2><span id='topic+plot.rma'></span><span id='topic+plot.rma.uni'></span><span id='topic+plot.rma.mh'></span><span id='topic+plot.rma.peto'></span><span id='topic+plot.rma.glmm'></span>

<h3>Description</h3>

<p>Functions to plot objects of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, and <code>"rma.glmm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
plot(x, qqplot=FALSE, ...)

## S3 method for class 'rma.mh'
plot(x, qqplot=FALSE, ...)

## S3 method for class 'rma.peto'
plot(x, qqplot=FALSE, ...)

## S3 method for class 'rma.glmm'
plot(x, qqplot=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, or <code>"rma.peto"</code>. The method is not yet implemented for objects of class <code>"rma.glmm"</code>.</p>
</td></tr>
<tr><td><code id="plot.rma_+3A_qqplot">qqplot</code></td>
<td>
<p>logical to specify whether a normal QQ plot should be drawn (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Four plots are produced. If the model does not contain any moderators, then a forest plot, funnel plot, radial plot, and a plot of the standardized residuals is provided. If <code>qqplot=TRUE</code>, the last plot is replaced by a normal QQ plot of the standardized residuals.
</p>
<p>If the model contains moderators, then a forest plot, funnel plot, plot of the standardized residuals against the fitted values, and a plot of the standardized residuals is provided. If <code>qqplot=TRUE</code>, the last plot is replaced by a normal QQ plot of the standardized residuals.
</p>


<h3>Note</h3>

<p>If the number of studies is large, the forest plot may become difficult to read due to the small font size. Stretching the plotting device vertically should provide more space.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forest">forest</a></code> for forest plots, <code><a href="#topic+funnel">funnel</a></code> for funnel plots, <code><a href="#topic+radial">radial</a></code> for radial plots, and <code><a href="#topic+qqnorm.rma">qqnorm</a></code> for normal QQ plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### plot results
plot(res, qqplot=TRUE)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### plot results
plot(res, qqplot=TRUE)
</code></pre>

<hr>
<h2 id='plot.rma.uni.selmodel'>Plot Method for 'plot.rma.uni.selmodel' Objects</h2><span id='topic+plot.rma.uni.selmodel'></span>

<h3>Description</h3>

<p>Function to plot objects of class <code>"plot.rma.uni.selmodel"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni.selmodel'
plot(x, xlim, ylim, n=1000, prec="max", scale=FALSE,
      ci=FALSE, reps=1000, shade=TRUE, rug=TRUE, add=FALSE,
      lty=c("solid","dotted"), lwd=c(2,1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.rma.uni.selmodel_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni.selmodel"</code> obtained with <code><a href="#topic+selmodel">selmodel</a></code>.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. Essentially the range of p-values for which the selection function should be drawn. If unspecified, the function sets the limits automatically.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the limits automatically.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_n">n</code></td>
<td>
<p>numeric value to specify for how many p-values within the x-axis limits the function value should be computed (the default is 1000).</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_prec">prec</code></td>
<td>
<p>either a character string (with options <code>"max"</code>, <code>"min"</code>, <code>"mean"</code>, or <code>"median"</code>) or a numeric value. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_scale">scale</code></td>
<td>
<p>logical to specify whether the function values should be rescaled to a 0 to 1 range (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_ci">ci</code></td>
<td>
<p>logical to specify whether a confidence interval should be drawn around the selection function (the default is <code>FALSE</code>). Can also be a string (with options <code>"boot"</code> or <code>"wald"</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_reps">reps</code></td>
<td>
<p>numeric value to specify the number of bootstrap samples to draw for generating the confidence interval bounds (the default is 1000).</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_shade">shade</code></td>
<td>
<p>logical to indicate whether the confidence interval region should be shaded (the default is <code>TRUE</code>). Can also be a character vector to specify the color for the shading.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_rug">rug</code></td>
<td>
<p>logical to specify whether the observed p-values should be added as tick marks on the x-axis (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_add">add</code></td>
<td>
<p>logical to specify whether the function should be added to an existing plot (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_lty">lty</code></td>
<td>
<p>the line types for the selection function and the confidence interval bounds.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_lwd">lwd</code></td>
<td>
<p>the line widths for the selection function and the confidence interval bounds.</p>
</td></tr>
<tr><td><code id="plot.rma.uni.selmodel_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be used to draw the estimated selection function based on objects of class <code>"plot.rma.uni.selmodel"</code>.
</p>
<p>When the selection function incorporates a measure of precision (which, strictly speaking, is really a measure of imprecision), one can specify for which level of precision the selection function should be drawn. When <code>prec="max"</code>, then the function is drawn for the <em>least</em> precise study (maximum imprecision), when <code>prec="min"</code>, then the function is drawn for the <em>most</em> precise study (minimum imprecision), while <code>prec="mean"</code> and <code>prec="median"</code> will show the function for the mean and median level of imprecision, respectively. Alternatively, one can specify a numeric value for argument <code>prec</code> to specify the precision value (where <code>prec="max"</code> corresponds to <code>prec=1</code> and higher levels of precision to <code>prec</code> values below 1).
</p>
<p>When <code>ci=TRUE</code> (or equivalently, <code>ci="boot"</code>), a confidence interval is drawn around the selection function. The bounds of this interval are generated using parametric bootstrapping, with argument <code>reps</code> controlling the number of bootstrap samples to draw for generating the confidence interval bounds. When both <code>n</code> and <code>reps</code> are large, constructing the confidence interval can take some time.
</p>
<p>For models where the selection function involves a single \(\delta\) parameter, one can also set <code>ci="wald"</code>, in which case the confidence interval will be constructed based on the Wald-type CI of the \(\delta\) parameter (doing so is much quicker than using parametric bootstrapping). This option is also available for step function models (even if they involve multiple \(\delta\) parameters).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+selmodel">selmodel</a></code> for the function to fit models for which the estimated selection function can be drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat' and examine data
dat &lt;- dat.hackshaw1998

### fit random-effects model using the log odds ratios
res &lt;- rma(yi, vi, data=dat, method="ML")
res

### fit step selection model
sel1 &lt;- selmodel(res, type="stepfun", steps=c(0.05, 0.10, 0.50, 1.00))

### plot selection function
plot(sel1, scale=TRUE)

### fit negative exponential selection model
sel2 &lt;- selmodel(res, type="negexp")

### add selection function to the existing plot
plot(sel2, add=TRUE, col="blue")

### plot selection function with CI
plot(sel1, ci="wald")

### plot selection function with CI
plot(sel2, ci="wald")
</code></pre>

<hr>
<h2 id='plot.vif.rma'>Plot Method for 'vif.rma' Objects</h2><span id='topic+plot.vif.rma'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"vif.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vif.rma'
plot(x, breaks="Scott", freq=FALSE, col, border, col.out, col.density,
     trim=0, adjust=1, lwd=c(2,0), layout, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vif.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"vif.rma"</code> obtained with <code><a href="#topic+vif.rma">vif</a></code>.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_breaks">breaks</code></td>
<td>
<p>argument to be passed on to the corresponding argument of <code><a href="graphics.html#topic+hist">hist</a></code> to set (the method for determining) the (number of) breakpoints.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_freq">freq</code></td>
<td>
<p>logical to indicate whether frequencies or probability densities should be plotted (the default is <code>FALSE</code> to plot densities).</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the color of the histogram bars.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_border">border</code></td>
<td>
<p>optional character string to specify the color of the borders around the bars.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_col.out">col.out</code></td>
<td>
<p>optional character string to specify the color of the bars that are more extreme than the observed (G)VIF value (the default is a semi-transparent shade of red).</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_col.density">col.density</code></td>
<td>
<p>optional character string to specify the color of the kernel density estimate of the distribution that is superimposed on top of the histogram (the default is blue).</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_trim">trim</code></td>
<td>
<p>the fraction (up to 0.5) of observations to be trimmed from the upper tail of each distribution before its histogram is plotted.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_adjust">adjust</code></td>
<td>
<p>numeric value to be passed on to the corresponding argument of <code><a href="stats.html#topic+density">density</a></code> (for adjusting the bandwidth of the kernel density estimate).</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_lwd">lwd</code></td>
<td>
<p>numeric vector to specify the width of the vertical lines corresponding to the value of the observed (G)VIFs and of the density estimate (note: by default, the density estimate has a line width of 0 and is therefore not plotted).</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_layout">layout</code></td>
<td>
<p>optional vector of two numbers to specify the number of rows and columns for the layout of the figure.</p>
</td></tr>
<tr><td><code id="plot.vif.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function plots the distribution of each (G)VIF as simulated under independence as a histogram.
</p>
<p>Arguments <code>breaks</code>, <code>freq</code>, <code>col</code>, and <code>border</code> are passed on to the <code><a href="graphics.html#topic+hist">hist</a></code> function for the plotting.
</p>
<p>Argument <code>trim</code> can be used to trim away a certain fraction of observations from the upper tail of each distribution before its histogram is plotted. By setting this to a value above 0, one can quickly remove some of the extreme values that might lead to the bulk of the distribution getting squished together at the left (typically, a small value such as <code>trim=0.01</code> is sufficient for this purpose).
</p>
<p>The observed (G)VIF value is indicated as a vertical dashed line. If the observed exceeds the upper plot limit, then this is indicated by an arrow pointing to the line.
</p>
<p>Argument <code>col.out</code> is used to specify the color for the bars in the histogram that are more extreme than the observed (G)VIF value.
</p>
<p>A kernel density estimate of the distribution can be superimposed on top of the histogram (as a smoothed representation of the distribution). Note that the kernel density estimate of the distribution is only shown when setting the line width for this element greater than 0 via the <code>lwd</code> argument (e.g., <code>lwd=c(2,2)</code>).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vif.rma">vif</a></code> for the function to create <code>vif.rma</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data from Bangert-Drowns et al. (2004) into 'dat'
dat &lt;- dat.bangertdrowns2004

### fit mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ length + wic + feedback + info + pers + imag + meta, data=dat)

### use the simulation approach to analyze the size of the VIFs
## Not run: 
vifs &lt;- vif(res, sim=TRUE, seed=1234)
vifs

### plot the simulated distributions of the VIFs
plot(vifs)

### add densities, trim away some extremes, and set break points
plot(vifs, lwd=c(2,2), trim=0.01, breaks=seq(1,2.2,by=0.05), adjust=1.5)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.rma'>Predicted Values for 'rma' Objects</h2><span id='topic+predict'></span><span id='topic+predict.rma'></span><span id='topic+predict.rma.ls'></span>

<h3>Description</h3>

<p>The function computes predicted values, corresponding standard errors, confidence intervals, and prediction intervals for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
predict(object, newmods, intercept, tau2.levels, gamma2.levels, addx=FALSE,
        level, digits, transf, targs, vcov=FALSE, ...)

## S3 method for class 'rma.ls'
predict(object, newmods, intercept, addx=FALSE, newscale, addz=FALSE,
        level, digits, transf, targs, vcov=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code> or <code>"rma.ls"</code>.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_newmods">newmods</code></td>
<td>
<p>optional vector or matrix to specify the values of the moderator values for which the predicted values should be calculated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_intercept">intercept</code></td>
<td>
<p>logical to specify whether the intercept should be included when calculating the predicted values for <code>newmods</code>. If unspecified, the intercept is automatically added when the original model also included an intercept.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_tau2.levels">tau2.levels</code></td>
<td>
<p>vector to specify the levels of the inner factor when computing prediction intervals. Only relevant for models of class <code>"rma.mv"</code> (see <code><a href="#topic+rma.mv">rma.mv</a></code>) and when the model includes more than a single \(\tau^2\) value. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_gamma2.levels">gamma2.levels</code></td>
<td>
<p>vector to specify the levels of the inner factor when computing prediction intervals. Only relevant for models of class <code>"rma.mv"</code> (see <code><a href="#topic+rma.mv">rma.mv</a></code>) and when the model includes more than a single \(\gamma^2\) value. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_addx">addx</code></td>
<td>
<p>logical to specify whether the values of the moderator variables should be added to the returned object. See &lsquo;Examples&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_newscale">newscale</code></td>
<td>
<p>optional vector or matrix to specify the values of the scale variables for which the predicted values should be calculated. Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_addz">addz</code></td>
<td>
<p>logical to specify whether the values of the scale variables should be added to the returned object.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence and prediction interval level (see <a href="#topic+misc-options">here</a> for details). If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the predicted values and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_vcov">vcov</code></td>
<td>
<p>logical to specify whether the variance-covariance matrix of the predicted values should also be returned (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="predict.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an equal-effects model, <code>predict(object)</code> returns the estimated (average) outcome in the set of studies included in the meta-analysis. This is the same as the estimated intercept in the equal-effects model (i.e., \(\hat{\theta}\)).
</p>
<p>For a random-effects model, <code>predict(object)</code> returns the estimated (average) outcome in the hypothetical population of studies from which the set of studies included in the meta-analysis are assumed to be a random selection. This is the same as the estimated intercept in the random-effects model (i.e., \(\hat{\mu}\)).
</p>
<p>For models including one or more moderators, <code>predict(object)</code> returns the estimated (average) outcomes for values of the moderator(s) equal to those of the \(k\) studies included in the meta-analysis (i.e., the &lsquo;fitted values&rsquo; for the \(k\) studies).
</p>
<p>For models including \(p'\) moderator variables, new moderator values (for \(k_{new}\) hypothetical new studies) can be specified by setting <code>newmods</code> equal to a \(k_{new} \times p'\) matrix with the corresponding new moderator values (if <code>newmods</code> is a vector, then only a single predicted value is computed unless the model only includes a single moderator variable, in which case predicted values corresponding to all the vector values are computed). If the model object included an intercept, then it should not be explicitly specified under <code>newmods</code>, as it will be added by default (unless one sets <code>intercept=FALSE</code>). Also, any factors in the original model get turned into the appropriate contrast variables within the <code>rma</code> function, so that <code>newmods</code> should actually include the values for the contrast variables. If the matrix specified via <code>newmods</code> has row names, then these are used to label the predicted values in the output. Examples are shown below.
</p>
<p>For random/mixed-effects models, a prediction interval is also computed (Riley et al., 2011). The interval estimates where <code>level</code>% of the true effect sizes or outcomes fall in the hypothetical population of studies (and hence where the true effect or outcome of a new study from the population of studies should fall in <code>level</code>% of the cases).
</p>
<p>For random-effects models that were fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, the model may actually include multiple \(\tau^2\) values (i.e., when the <code>random</code> argument includes an &lsquo;<code>~ inner | outer</code>&rsquo; term and <code>struct="HCS"</code>, <code>struct="DIAG"</code>, <code>struct="HAR"</code>, or <code>struct="UN"</code>). In that case, the function will provide prediction intervals for each level of the inner factor (since the prediction intervals differ depending on the \(\tau^2\) value). Alternatively, one can use the <code>tau2.levels</code> argument to specify for which level(s) the prediction interval should be provided. If the model includes a second &lsquo;<code>~ inner | outer</code>&rsquo; term with multiple \(\gamma^2\) values, prediction intervals for each combination of levels of the inner factors will be provided. Alternatively, one can use the <code>tau2.levels</code> and <code>gamma2.levels</code> arguments to specify for which level combination(s) the prediction interval should be provided.
</p>
<p>When using the <code>newmods</code> argument for mixed-effects models that were fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, if the model includes multiple \(\tau^2\) (and multiple \(\gamma^2\)) values, then one must use the <code>tau2.levels</code> (and <code>gamma2.levels</code>) argument to specify the levels of the inner factor(s) (i.e., a vector of length \(k_{new}\)) to obtain the appropriate prediction interval(s).
</p>
<p>For location-scale models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, one can use <code>newmods</code> to specify the values of the \(p'\) moderator variables included in the model and <code>newscale</code> to specify the values of the \(q'\) scale variables included in the model. Whenever <code>newmods</code> is specified, the function computes predicted effects/outcomes for the specified moderators values. To obtain the corresponding prediction intervals, one must also specify the corresponding <code>newscale</code> values. If only <code>newscale</code> is specified (and not <code>newmods</code>), the function computes the predicted log-transformed \(\tau^2\) values (when using a log link) for the specified scale values. By setting <code>transf=exp</code>, one can then obtain the predicted \(\tau^2\) values.
</p>


<h3>Value</h3>

<p>An object of class <code>c("predict.rma","list.rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted value(s).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard error(s).</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence interval(s).</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence interval(s).</p>
</td></tr>
<tr><td><code>pi.lb</code></td>
<td>
<p>lower bound of the prediction interval(s) (only for random/mixed-effects models).</p>
</td></tr>
<tr><td><code>pi.ub</code></td>
<td>
<p>upper bound of the prediction interval(s) (only for random/mixed-effects models).</p>
</td></tr>
<tr><td><code>tau2.level</code></td>
<td>
<p>the level(s) of the inner factor (only for models of class <code>"rma.mv"</code> with multiple \(\tau^2\) values).</p>
</td></tr>
<tr><td><code>gamma2.level</code></td>
<td>
<p>the level(s) of the inner factor (only for models of class <code>"rma.mv"</code> with multiple \(\gamma^2\) values).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the moderator value(s) used to calculate the predicted values (only when <code>addx=TRUE</code>).</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>the scale value(s) used to calculate the predicted values (only when <code>addz=TRUE</code> and only for location-scale models).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>If <code>vcov=TRUE</code>, then the returned object is a list with the first element equal to the one as described above and the second element equal to the variance-covariance matrix of the predicted values.
</p>
<p>The object is formatted and printed with the <code><a href="#topic+print.list.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>Confidence and prediction intervals are constructed based on the critical values from a standard normal distribution (i.e., \(\pm 1.96\) for <code>level=95</code>). When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then a t-distribution with \(k-p\) degrees of freedom is used.
</p>
<p>For a random-effects model (where \(p=1\)) fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, note that this differs slightly from Riley et al. (2011), who suggest to use a t-distribution with \(k-2\) degrees of freedom for constructing the prediction interval. Neither a normal, nor a t-distribution with \(k-1\) or \(k-2\) degrees of freedom is correct; all of these are approximations. The computations are done in the way described above, so that the prediction interval is identical to the confidence interval when \(\hat{\tau}^2 = 0\), which could be argued is the logical thing that should happen. If the prediction interval should be computed exactly as described by Riley et al. (2011), then one can use argument <code>pi.type="Riley"</code>.
</p>
<p>The predicted values are based only on the fixed effects of the model. Best linear unbiased predictions (BLUPs) that combine the fitted values based on the fixed effects and the estimated contributions of the random effects can be obtained with <code><a href="#topic+blup.rma.uni">blup</a></code> (currently only for objects of class <code>"rma.uni"</code>).
</p>
<p>When using the <code>transf</code> option, the transformation is applied to the predicted values and the corresponding interval bounds. The standard errors are omitted from the printed output. Also, <code>vcov=TRUE</code> is ignored when using the <code>transf</code> option.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical methods for meta-analysis</em>. San Diego, CA: Academic Press.
</p>
<p>Riley, R. D., Higgins, J. P. T., &amp; Deeks, J. J. (2011). Interpretation of random effects meta-analyses. <em>British Medical Journal</em>, <b>342</b>, d549. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.d549&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fitted.rma">fitted</a></code> for a function to extract the fitted values, <code><a href="#topic+blup.rma.uni">blup</a></code> for a function to compute BLUPs that combine the fitted values and predicted random effects, and <code><a href="#topic+addpoly.predict.rma">addpoly</a></code> to add polygons based on predicted values to a forest plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### average risk ratio with 95% CI
predict(res, transf=exp)

### fit mixed-effects model with absolute latitude as a moderator
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)

### predicted average risk ratios for given absolute latitude values
predict(res, transf=exp, addx=TRUE)

### predicted average risk ratios for 10-60 degrees absolute latitude
predict(res, newmods=c(10, 20, 30, 40, 50, 60), transf=exp, addx=TRUE)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### predicted average risk ratios for 10 and 60 degrees latitude in 1950 and 1980
predict(res, newmods=cbind(c(10,60,10,60),c(1950,1950,1980,1980)), transf=exp, addx=TRUE)

### predicted average risk ratios for 10 and 60 degrees latitude in 1970 (row names as labels)
predict(res, newmods=rbind(at10=c(10,1970), at60=c(60,1970)), transf=exp)

### fit mixed-effects model with two moderators (one of which is a factor)
res &lt;- rma(yi, vi, mods = ~ ablat + factor(alloc), data=dat)

### examine how the factor was actually coded for the studies in the dataset
predict(res, addx=TRUE)

### predicted average risk ratios at 30 degrees for the three factor levels
### note: the contrast (dummy) variables need to specified explicitly here
predict(res, newmods=c(30, 0, 0), addx=TRUE)   # for alternate  allocation
predict(res, newmods=c(30, 1, 0), addx=TRUE)   # for random     allocation
predict(res, newmods=c(30, 0, 1), addx=TRUE)   # for systematic allocation

### can also use a named vector with arbitrary order and abbreviated variable names
predict(res, newmods=c(sys=0, ran=0, abl=30))
predict(res, newmods=c(sys=0, ran=1, abl=30))
predict(res, newmods=c(sys=1, ran=0, abl=30))
</code></pre>

<hr>
<h2 id='print.anova.rma'>Print Methods for 'anova.rma' and 'list.anova.rma' Objects</h2><span id='topic+print.anova.rma'></span><span id='topic+print.list.anova.rma'></span>

<h3>Description</h3>

<p>Functions to print objects of class <code>"anova.rma"</code> and <code>"list.anova.rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'anova.rma'
print(x, digits=x$digits, ...)
## S3 method for class 'list.anova.rma'
print(x, digits=x[[1]]$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.anova.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"anova.rma"</code> or <code>"list.anova.rma"</code> obtained with <code><a href="#topic+anova.rma">anova</a></code>.</p>
</td></tr>
<tr><td><code id="print.anova.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.anova.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a Wald-type test of one or multiple model coefficients, the output includes the test statistic (either a chi-square or F-value) and the corresponding p-value.
</p>
<p>When testing one or multiple contrasts, the output includes the estimated value of the contrast, its standard error, test statistic (either a z- or a t-value), and the corresponding p-value.
</p>
<p>When comparing two model objects, the output includes:
</p>

<ul>
<li><p> the number of parameters in the full and the reduced model.
</p>
</li>
<li><p> the AIC, BIC, AICc, and log-likelihood of the full and the reduced model.
</p>
</li>
<li><p> the value of the likelihood ratio test statistic.
</p>
</li>
<li><p> the corresponding p-value.
</p>
</li>
<li><p> the test statistic of the test for (residual) heterogeneity for the full and the reduced model.
</p>
</li>
<li><p> the estimate of \(\tau^2\) from the full and the reduced model. Suppressed for equal-effects models.
</p>
</li>
<li><p> amount (in percent) of heterogeneity in the reduced model that is accounted for in the full model (<code>NA</code> for <code>"rma.mv"</code> objects). This can be regarded as a pseudo \(R^2\) statistic (Raudenbush, 2009). Note that the value may not be very accurate unless \(k\) is large (Lopez-Lopez et al., 2014).
</p>
</li></ul>

<p>The last two items are not provided when comparing <code>"rma.mv"</code> models.
</p>


<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>L√≥pez-L√≥pez, J. A., Mar√≠n-Mart√≠nez, F., S√°nchez-Meca, J., Van den Noortgate, W., &amp; Viechtbauer, W. (2014). Estimation of the predictive power of the model in mixed-effects meta-regression: A simulation study. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>67</b>(1), 30&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/bmsp.12002&#8288;</code>
</p>
<p>Raudenbush, S. W. (2009). Analyzing effect sizes: Random effects models. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 295&ndash;315). New York: Russell Sage Foundation.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anova.rma">anova</a></code> for the function to create <code>anova.rma</code> objects.
</p>

<hr>
<h2 id='print.confint.rma'>Print Methods for 'confint.rma' and 'list.confint.rma' Objects</h2><span id='topic+print.confint.rma'></span><span id='topic+print.list.confint.rma'></span>

<h3>Description</h3>

<p>Functions to print objects of class <code>"confint.rma"</code> and <code>"list.confint.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'confint.rma'
print(x, digits=x$digits, ...)
## S3 method for class 'list.confint.rma'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.confint.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"confint.rma"</code> or <code>"list.confint.rma"</code> obtained with <code><a href="#topic+confint.rma.uni">confint</a></code>.</p>
</td></tr>
<tr><td><code id="print.confint.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.confint.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output includes:
</p>

<ul>
<li><p> estimate of the model coefficient or variance/correlation parameter
</p>
</li>
<li><p> lower bound of the confidence interval
</p>
</li>
<li><p> upper bound of the confidence interval
</p>
</li></ul>



<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+confint.rma">confint</a></code> for the functions to create <code>confint.rma</code> and <code>list.confint.rma</code> objects.
</p>

<hr>
<h2 id='print.escalc'>Print and Summary Methods for 'escalc' Objects</h2><span id='topic+print.escalc'></span><span id='topic+summary.escalc'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"escalc"</code> (and to obtain inferences for the individual studies/rows in such an object). <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'escalc'
print(x, digits=attr(x,"digits"), ...)

## S3 method for class 'escalc'
summary(object, out.names=c("sei","zi","pval","ci.lb","ci.ub"), var.names,
        H0=0, append=TRUE, replace=TRUE, level=95, olim, digits, transf, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.escalc_+3A_x">x</code></td>
<td>
<p>an object of class <code>"escalc"</code> obtained with <code><a href="#topic+escalc">escalc</a></code>.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_object">object</code></td>
<td>
<p>an object of class <code>"escalc"</code> obtained with <code><a href="#topic+escalc">escalc</a></code>.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_out.names">out.names</code></td>
<td>
<p>character string with four elements to specify the variable names for the standard errors, test statistics, and lower/upper confidence interval bounds.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_var.names">var.names</code></td>
<td>
<p>character string with two elements to specify the variable names for the observed effect sizes or outcomes and the sampling variances (the default is to take the value from the object if possible).</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_h0">H0</code></td>
<td>
<p>numeric value to specify the value of the effect size or outcome under the null hypothesis (the default is 0).</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame specified via the <code>object</code> argument should be returned together with the additional variables that are calculated by the <code>summary</code> function (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_replace">replace</code></td>
<td>
<p>logical to specify whether existing values for <code>sei</code>, <code>zi</code>, <code>ci.lb</code>, and <code>ci.ub</code> in the data frame should be replaced. Only relevant when the data frame already contains these variables. If <code>replace=TRUE</code> (the default), all of the existing values will be overwritten. If <code>replace=FALSE</code>, only <code>NA</code> values will be replaced.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the observed effect sizes or outcomes and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used. Any additional arguments needed for the function specified here can be passed via <code>...</code>.</p>
</td></tr>
<tr><td><code id="print.escalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>print.escalc</code> function formats and prints the data frame, so that the observed effect sizes or outcomes and sampling variances are rounded (to the number of digits specified).
</p>
<p>The <code>summary.escalc</code> function creates an object that is a data frame containing the original data (if <code>append=TRUE</code>) and the following components:
</p>
<table>
<tr><td><code>yi</code></td>
<td>
<p>observed effect sizes or outcomes (transformed if <code>transf</code> is specified).</p>
</td></tr>
<tr><td><code>vi</code></td>
<td>
<p>corresponding sampling variances.</p>
</td></tr>
<tr><td><code>sei</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>zi</code></td>
<td>
<p>test statistics for testing \(\mbox{H}_0{:}\; \theta_i = \mbox{H0}\) (i.e., <code>(yi-H0)/sei</code>).</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower confidence interval bounds (transformed if <code>transf</code> is specified).</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper confidence interval bounds (transformed if <code>transf</code> is specified).</p>
</td></tr>
</table>
<p>When the <code>transf</code> argument is specified, elements <code>vi</code>, <code>sei</code>, <code>zi</code>, and <code>pval</code> are not included (since these only apply to the untransformed effect sizes or outcomes).
</p>
<p>Note that the actual variable names above depend on the <code>out.names</code> (and <code>var.names</code>) arguments. If the data frame already contains variables with names as specified by the <code>out.names</code> argument, the values for these variables will be overwritten when <code>replace=TRUE</code> (which is the default). By setting <code>replace=FALSE</code>, only values that are <code>NA</code> will be replaced.
</p>
<p>The <code>print.escalc</code> function again formats and prints the data frame, rounding the added variables to the number of digits specified.
</p>


<h3>Note</h3>

<p>If some transformation function has been specified for the <code>transf</code> argument, then <code>yi</code>, <code>ci.lb</code>, and <code>ci.ub</code> will be transformed accordingly. However, <code>vi</code> and <code>sei</code> then still reflect the sampling variances and standard errors of the untransformed values.
</p>
<p>The <code>summary.escalc</code> function computes <code>level</code>% Wald-type confidence intervals, which may or may not be the most accurate method for computing confidence intervals for the chosen effect size or outcome measure.
</p>
<p>If the outcome measure used is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those observation/outcome limits (the observed outcomes and confidence intervals cannot exceed those bounds then).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for the function to create <code>escalc</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
dat

### apply summary function
summary(dat)
summary(dat, transf=exp)
</code></pre>

<hr>
<h2 id='print.fsn'>Print Method for 'fsn' Objects</h2><span id='topic+print.fsn'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"fsn"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fsn'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fsn_+3A_x">x</code></td>
<td>
<p>an object of class <code>"fsn"</code> obtained with <code><a href="#topic+fsn">fsn</a></code>.</p>
</td></tr>
<tr><td><code id="print.fsn_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.fsn_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output shows the results from the fail-safe N calculation.
</p>


<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fsn">fsn</a></code> for the function to create <code>fsn</code> objects.
</p>

<hr>
<h2 id='print.gosh.rma'>Print Method for 'gosh.rma' Objects</h2><span id='topic+print.gosh.rma'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"gosh.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gosh.rma'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gosh.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"gosh.rma"</code> obtained with <code><a href="#topic+gosh">gosh</a></code>.</p>
</td></tr>
<tr><td><code id="print.gosh.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.gosh.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output shows how many model fits were attempted, how many succeeded, and summary statistics (i.e., the mean, minimum, first quartile, median, third quartile, and maximum) for the various measures of (residual) heterogeneity and the model coefficient(s) computed across all of the subsets.
</p>


<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gosh">gosh</a></code> for the function to create <code>gosh.rma</code> objects.
</p>

<hr>
<h2 id='print.hc.rma.uni'>Print Method for 'hc.rma.uni' Objects</h2><span id='topic+print.hc.rma.uni'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"hc.rma.uni"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hc.rma.uni'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.hc.rma.uni_+3A_x">x</code></td>
<td>
<p>an object of class <code>"hc.rma.uni"</code> obtained with <code><a href="#topic+hc.rma.uni">hc</a></code>.</p>
</td></tr>
<tr><td><code id="print.hc.rma.uni_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.hc.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output is a data frame with two rows, the first (labeled <code>rma</code>) corresponding to the results based on the usual estimation method, the second (labeled <code>hc</code>) corresponding to the results based on the method by Henmi and Copas (2010). The data frame includes the following variables:
</p>

<ul>
<li><p> the method used to estimate \(\tau^2\) (always <code>DL</code> for <code>hc</code>)
</p>
</li>
<li><p> the estimated amount of heterogeneity
</p>
</li>
<li><p> the estimated average true outcome
</p>
</li>
<li><p> the corresponding standard error (<code>NA</code> when <code>transf</code> argument has been used)
</p>
</li>
<li><p> the lower and upper confidence interval bounds
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the data frame invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hc.rma.uni">hc</a></code> for the function to create <code>hc.rma.uni</code> objects.
</p>

<hr>
<h2 id='print.list.rma'>Print Method for 'list.rma' Objects</h2><span id='topic+print.list.rma'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"list.rma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'list.rma'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.list.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"list.rma"</code>.</p>
</td></tr>
<tr><td><code id="print.list.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.list.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>See the documentation of the function that creates the <code>"list.rma"</code> object for details on what is printed. Regardless of what is printed, a data frame with the results is also returned invisibly.
</p>
<p>See <code><a href="#topic+methods.list.rma">methods.list.rma</a></code> for some additional method functions for <code>"list.rma"</code> objects.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>

<hr>
<h2 id='print.matreg'>Print and Summary Methods for 'matreg' Objects</h2><span id='topic+print.matreg'></span><span id='topic+summary.matreg'></span><span id='topic+print.summary.matreg'></span>

<h3>Description</h3>

<p>Functions to print objects of class <code>"matreg"</code> and <code>"summary.matreg"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'matreg'
print(x, digits, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)

## S3 method for class 'matreg'
summary(object, digits, ...)

## S3 method for class 'summary.matreg'
print(x, digits, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.matreg_+3A_x">x</code></td>
<td>
<p>an object of class <code>"matreg"</code> or <code>"summary.matreg"</code> (for <code>print</code>).</p>
</td></tr>
<tr><td><code id="print.matreg_+3A_object">object</code></td>
<td>
<p>an object of class <code>"matreg"</code> (for <code>summary</code>).</p>
</td></tr>
<tr><td><code id="print.matreg_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="print.matreg_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical to specify whether p-values should be encoded visually with &lsquo;significance stars&rsquo;. Defaults to the <code>show.signif.stars</code> slot of <code><a href="base.html#topic+options">options</a></code>.</p>
</td></tr>
<tr><td><code id="print.matreg_+3A_signif.legend">signif.legend</code></td>
<td>
<p>logical to specify whether the legend for the &lsquo;significance stars&rsquo; should be printed. Defaults to the value for <code>signif.stars</code>.</p>
</td></tr>
<tr><td><code id="print.matreg_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output is a table with the estimated coefficients, corresponding standard errors, test statistics, p-values, and confidence interval bounds. When using <code>summary</code>, the output includes additional statistics, including \(R^2\) and the omnibus test of the model coefficients (either an F- or a chi-square test).
</p>


<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matreg">matreg</a></code> for the function to create <code>matreg</code> objects.
</p>

<hr>
<h2 id='print.permutest.rma.uni'>Print Method for 'permutest.rma.uni' Objects</h2><span id='topic+print.permutest.rma.uni'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"permutest.rma.uni"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'permutest.rma.uni'
print(x, digits=x$digits, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.permutest.rma.uni_+3A_x">x</code></td>
<td>
<p>an object of class <code>"permutest.rma.uni"</code> obtained with <code><a href="#topic+permutest.rma.uni">permutest</a></code>.</p>
</td></tr>
<tr><td><code id="print.permutest.rma.uni_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.permutest.rma.uni_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical to specify whether p-values should be encoded visually with &lsquo;significance stars&rsquo;. Defaults to the <code>show.signif.stars</code> slot of <code><a href="base.html#topic+options">options</a></code>.</p>
</td></tr>
<tr><td><code id="print.permutest.rma.uni_+3A_signif.legend">signif.legend</code></td>
<td>
<p>logical to specify whether the legend for the &lsquo;significance stars&rsquo; should be printed. Defaults to the value for <code>signif.stars</code>.</p>
</td></tr>
<tr><td><code id="print.permutest.rma.uni_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output includes:
</p>

<ul>
<li><p> the results of the omnibus test of moderators. Suppressed if the model includes only one coefficient (e.g., only an intercept, like in the equal- and random-effects models). The p-value is based on the permutation test.
</p>
</li>
<li><p> a table with the estimated coefficients, corresponding standard errors, test statistics, p-values, and confidence interval bounds. The p-values are based on permutation tests. If <code>permci</code> was set to <code>TRUE</code>, then the permutation-based CI bounds are shown.
</p>
</li></ul>



<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permutest.rma.uni">permutest</a></code> for the function to create <code>permutest.rma.uni</code> objects.
</p>

<hr>
<h2 id='print.ranktest'>Print Method for 'ranktest' Objects</h2><span id='topic+print.ranktest'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"ranktest"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ranktest'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ranktest_+3A_x">x</code></td>
<td>
<p>an object of class <code>"ranktest"</code> obtained with <code><a href="#topic+ranktest">ranktest</a></code>.</p>
</td></tr>
<tr><td><code id="print.ranktest_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.ranktest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output includes:
</p>

<ul>
<li><p> the estimated value of Kendall's tau rank correlation coefficient
</p>
</li>
<li><p> the corresponding p-value for the test that the true tau is equal to zero
</p>
</li></ul>



<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ranktest">ranktest</a></code> for the function to create <code>ranktest</code> objects.
</p>

<hr>
<h2 id='print.regtest'>Print Method for 'regtest' Objects</h2><span id='topic+print.regtest'></span>

<h3>Description</h3>

<p>Function to print objects of class <code>"regtest"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'regtest'
print(x, digits=x$digits, ret.fit=x$ret.fit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.regtest_+3A_x">x</code></td>
<td>
<p>an object of class <code>"regtest"</code> obtained with <code><a href="#topic+regtest">regtest</a></code>.</p>
</td></tr>
<tr><td><code id="print.regtest_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded (the default is to take the value from the object).</p>
</td></tr>
<tr><td><code id="print.regtest_+3A_ret.fit">ret.fit</code></td>
<td>
<p>logical to specify whether the full results from the fitted model should also be returned. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="print.regtest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output includes:
</p>

<ul>
<li><p> the model used for the regression test
</p>
</li>
<li><p> the predictor used for the regression test
</p>
</li>
<li><p> the results from the fitted model (only when <code>ret.fit=TRUE</code>)
</p>
</li>
<li><p> the test statistic of the test that the predictor is unreleated to the outcomes
</p>
</li>
<li><p> the degrees of freedom of the test statistic (only if the test statistic follows a t-distribution)
</p>
</li>
<li><p> the corresponding p-value
</p>
</li>
<li><p> the &lsquo;limit estimate&rsquo; and its corresponding CI (only for predictors <code>"sei"</code> <code>"vi"</code>, <code>"ninv"</code>, or <code>"sqrtninv"</code> and when the model does not contain any additional moderators)
</p>
</li></ul>



<h3>Value</h3>

<p>The function does not return an object.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regtest">regtest</a></code> for the function to create <code>regtest</code> objects.
</p>

<hr>
<h2 id='print.rma'>Print and Summary Methods for 'rma' Objects</h2><span id='topic+print.rma'></span><span id='topic+print.rma.uni'></span><span id='topic+print.rma.mh'></span><span id='topic+print.rma.peto'></span><span id='topic+print.rma.glmm'></span><span id='topic+print.rma.mv'></span><span id='topic+summary'></span><span id='topic+summary.rma'></span><span id='topic+print.summary.rma'></span>

<h3>Description</h3>

<p>Functions to print objects of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, <code>"rma.glmm"</code>, <code>"rma.glmm"</code>, and <code>"rma.mv"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
print(x, digits, showfit=FALSE, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)

## S3 method for class 'rma.mh'
print(x, digits, showfit=FALSE, ...)

## S3 method for class 'rma.peto'
print(x, digits, showfit=FALSE, ...)

## S3 method for class 'rma.glmm'
print(x, digits, showfit=FALSE, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)

## S3 method for class 'rma.mv'
print(x, digits, showfit=FALSE, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)

## S3 method for class 'rma'
summary(object, digits, ...)

## S3 method for class 'summary.rma'
print(x, digits, showfit=TRUE, signif.stars=getOption("show.signif.stars"),
      signif.legend=signif.stars, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, <code>"rma.glmm"</code>, <code>"rma.mv"</code>, or <code>"summary.rma"</code> (for <code>print</code>).</p>
</td></tr>
<tr><td><code id="print.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code> (for <code>summary</code>).</p>
</td></tr>
<tr><td><code id="print.rma_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="print.rma_+3A_showfit">showfit</code></td>
<td>
<p>logical to specify whether the fit statistics and information criteria should be printed (the default is <code>FALSE</code> for <code>print</code> and <code>TRUE</code> for <code>summary</code>).</p>
</td></tr>
<tr><td><code id="print.rma_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical to specify whether p-values should be encoded visually with &lsquo;significance stars&rsquo;. Defaults to the <code>show.signif.stars</code> slot of <code><a href="base.html#topic+options">options</a></code>.</p>
</td></tr>
<tr><td><code id="print.rma_+3A_signif.legend">signif.legend</code></td>
<td>
<p>logical to specify whether the legend for the &lsquo;significance stars&rsquo; should be printed. Defaults to the value for <code>signif.stars</code>.</p>
</td></tr>
<tr><td><code id="print.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output includes:
</p>

<ul>
<li><p> the log-likelihood, deviance, AIC, BIC, and AICc value (when setting <code>showfit=TRUE</code> or by default for <code>summary</code>).
</p>
</li>
<li><p> for objects of class <code>"rma.uni"</code> and <code>"rma.glmm"</code>, the amount of (residual) heterogeneity in the random/mixed-effects model (i.e., the estimate of \(\tau^2\) and its square root). Suppressed for equal-effects models. The (asymptotic) standard error of the estimate of \(\tau^2\) is also provided (where possible).
</p>
</li>
<li><p> for objects of <code>"rma.mv"</code>, a table providing information about the variance components and correlations in the model. For \(\sigma^2\) components, the estimate and its square root are provided, in addition to the number of values/levels, whether the component was fixed or estimated, and the name of the grouping variable/factor. If the <code>R</code> argument was used to specify known correlation matrices, this is also indicated. For models with an &lsquo;<code>~ inner | outer</code>&rsquo; formula term, the name of the inner and outer grouping variable/factor are given and the number of values/levels of these variables/factors. In addition, for each \(\tau^2\) component, the estimate and its square root are provided, the number of effects or outcomes observed at each level of the inner grouping variable/factor (only for <code>struct="HCS"</code>, <code>struct="DIAG"</code>, <code>struct="HAR"</code>, and <code>struct="UN"</code>), and whether the component was fixed or estimated. Finally, either the estimate of \(\rho\) (for <code>struct="CS"</code>, <code>struct="AR"</code>, <code>struct="CAR"</code>, <code>struct="HAR"</code>, or <code>struct="HCS"</code>) or the entire estimated correlation matrix (for <code>struct="UN"</code>) between the levels of the inner grouping variable/factor is provided, again with information whether a particular correlation was fixed or estimated, and how often each combination of levels of the inner grouping variable/factor was observed across the levels of the outer grouping variable/factor. If there is a second &lsquo;<code>~ inner | outer</code>&rsquo; formula term, the same information as described above will be provided, but now for the \(\gamma^2\) and \(\phi\) components.
</p>
</li>
<li><p> the \(I^2\) statistic, which estimates (in percent) how much of the total variability in the observed effect sizes or outcomes (which is composed of heterogeneity plus sampling variability) can be attributed to heterogeneity among the true effects. For a meta-regression model, \(I^2\) estimates how much of the unaccounted variability (which is composed of residual heterogeneity plus sampling variability) can be attributed to residual heterogeneity. See &lsquo;Note&rsquo; for how \(I^2\) is computed.
</p>
</li>
<li><p> the \(H^2\) statistic, which estimates the ratio of the total amount of variability in the observed effect sizes or outcomes to the amount of sampling variability. For a meta-regression model, \(H^2\) estimates the ratio of the unaccounted variability in the observed effect sizes or outcomes to the amount of sampling variability. See &lsquo;Note&rsquo; for how \(H^2\) is computed.
</p>
</li>
<li><p> for objects of class <code>"rma.uni"</code>, the \(R^2\) statistic, which estimates the amount of heterogeneity accounted for by the moderators included in the model and can be regarded as a pseudo \(R^2\) statistic (Raudenbush, 2009). Only provided when fitting a model including moderators. This is suppressed (and set to <code>NULL</code>) for models without moderators or if the model does not contain an intercept. See &lsquo;Note&rsquo; for how \(R^2\) is computed.
</p>
</li>
<li><p> for objects of class <code>"rma.glmm"</code>, the amount of study level variability (only when using a model that models study level differences as a random effect).
</p>
</li>
<li><p> the results of the test for (residual) heterogeneity. This is the usual \(Q\)-test for heterogeneity when not including moderators in the model and the \(Q_E\)-test for residual heterogeneity when moderators are included. For objects of class <code>"rma.glmm"</code>, the results from a Wald-type test and a likelihood ratio test are provided (see <code><a href="#topic+rma.glmm">rma.glmm</a></code> for more details).
</p>
</li>
<li><p> the results of the omnibus (Wald-type) test of the coefficients in the model (the indices of the coefficients tested are also indicated). Suppressed if the model includes only one coefficient (e.g., only an intercept, like in the equal- and random-effects models).
</p>
</li>
<li><p> a table with the estimated coefficients, corresponding standard errors, test statistics, p-values, and confidence interval bounds.
</p>
</li>
<li><p> the Cochran-Mantel-Haenszel test and Tarone's test for heterogeneity (only when analyzing odds ratios using the Mantel-Haenszel method, i.e., <code>"rma.mh"</code>).
</p>
</li></ul>

<p>See also <a href="#topic+misc-options">here</a> for details on the option to create styled/colored output with the help of the <a href="https://cran.r-project.org/package=crayon">crayon</a> package.
</p>


<h3>Value</h3>

<p>The <code>print</code> functions do not return an object. The <code>summary</code> function returns the object passed to it (with additional class <code>"summary.rma"</code>).
</p>


<h3>Note</h3>

<p>For random-effects models, the \(I^2\) statistic is computed with \[I^2 = 100\% \times \frac{\hat{\tau}^2}{\hat{\tau}^2 + \tilde{v}},\] where \(\hat{\tau}^2\) is the estimated value of \(\tau^2\) and \[\tilde{v} = \frac{(k-1) \sum w_i}{(\sum w_i)^2 - \sum w_i^2},\] where \(w_i = 1 / v_i\) is the inverse of the sampling variance of the \(i\textrm{th}\) study (\(\tilde{v}\) is equation 9 in Higgins &amp; Thompson, 2002, and can be regarded as the &lsquo;typical&rsquo; within-study variance of the observed effect sizes or outcomes). The \(H^2\) statistic is computed with \[H^2 = \frac{\hat{\tau}^2 + \tilde{v}}{\tilde{v}}.\] Analogous equations are used for mixed-effects models.
</p>
<p>Therefore, depending on the estimator of \(\tau^2\) used, the values of \(I^2\) and \(H^2\) will change. For random-effects models, \(I^2\) and \(H^2\) are often computed with \(I^2 = (Q-(k-1))/Q\) and \(H^2 = Q/(k-1)\), where \(Q\) denotes the statistic of the test for heterogeneity and \(k\) the number of studies (i.e., observed effect sizes or outcomes) included in the meta-analysis. The equations used in the <span class="pkg">metafor</span> package to compute these statistics are more general and have the advantage that the values of \(I^2\) and \(H^2\) will be consistent with the estimated value of \(\tau^2\) (i.e., if \(\hat{\tau}^2 = 0\), then \(I^2 = 0\) and \(H^2 = 1\) and if \(\hat{\tau}^2 > 0\), then \(I^2 > 0\) and \(H^2 > 1\)).
</p>
<p>The two definitions of \(I^2\) and \(H^2\) actually coincide when using the DerSimonian-Laird estimator of \(\tau^2\) (i.e., the commonly used equations are actually special cases of the more general definitions given above). Therefore, if you prefer the more conventional definitions of these statistics, use <code>method="DL"</code> when fitting the random/mixed-effects model with the <code><a href="#topic+rma.uni">rma.uni</a></code> function. The conventional definitions are also automatically used when fitting an equal-effects models.
</p>
<p>For mixed-effects models, the pseudo \(R^2\) statistic (Raudenbush, 2009) is computed with \[R^2 = \frac{\hat{\tau}_{RE}^2 - \hat{\tau}_{ME}^2}{\hat{\tau}_{RE}^2},\] where \(\hat{\tau}_{RE}^2\) denotes the estimated value of \(\tau^2\) based on the random-effects model (i.e., the total amount of heterogeneity) and \(\hat{\tau}_{ME}^2\) denotes the estimated value of \(\tau^2\) based on the mixed-effects model (i.e., the residual amount of heterogeneity). It can happen that \(\hat{\tau}_{RE}^2 < \hat{\tau}_{ME}^2\), in which case \(R^2\) is set to zero (and also if \(\hat{\tau}_{RE}^2 = 0\)). Again, the value of \(R^2\) will change depending on the estimator of \(\tau^2\) used. This statistic is only computed when the mixed-effects model includes an intercept (so that the random-effects model is clearly nested within the mixed-effects model). You can also use the <code><a href="#topic+anova.rma">anova</a></code> function to compute \(R^2\) for any two models that are known to be nested. Note that the pseudo \(R^2\) statistic may not be very accurate unless \(k\) is large (Lopez-Lopez et al., 2014).
</p>
<p>For fixed-effects with moderators models, the \(R^2\) statistic is simply the standard \(R^2\) statistic (also known as the &lsquo;coefficient of determination&rsquo;) computed based on weighted least squares estimation. To be precise, the so-called &lsquo;adjusted&rsquo; \(R^2\) statistic is provided, since \(k\) is often relatively small in meta-analyses, in which case the adjustment is relevant.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Higgins, J. P. T., &amp; Thompson, S. G. (2002). Quantifying heterogeneity in a meta-analysis. <em>Statistics in Medicine</em>, <b>21</b>(11), 1539&ndash;1558. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1186&#8288;</code>
</p>
<p>L√≥pez-L√≥pez, J. A., Mar√≠n-Mart√≠nez, F., S√°nchez-Meca, J., Van den Noortgate, W., &amp; Viechtbauer, W. (2014). Estimation of the predictive power of the model in mixed-effects meta-regression: A simulation study. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>67</b>(1), 30&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/bmsp.12002&#8288;</code>
</p>
<p>Raudenbush, S. W. (2009). Analyzing effect sizes: Random effects models. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 295&ndash;315). New York: Russell Sage Foundation.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for the corresponding model fitting functions.
</p>

<hr>
<h2 id='profile.rma'>Profile Likelihood Plots for 'rma' Objects</h2><span id='topic+profile'></span><span id='topic+profile.rma'></span><span id='topic+profile.rma.uni'></span><span id='topic+profile.rma.mv'></span><span id='topic+profile.rma.uni.selmodel'></span><span id='topic+profile.rma.ls'></span><span id='topic+print.profile.rma'></span><span id='topic+plot.profile.rma'></span>

<h3>Description</h3>

<p>Functions to profile the (restricted) log-likelihood for objects of class <code>"rma.uni"</code>, <code>"rma.mv"</code>, <code>"rma.uni.selmodel"</code>, and <code>"rma.ls"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
profile(fitted, xlim, ylim, steps=20, lltol=1e-03,
        progbar=TRUE, parallel="no", ncpus=1, cl, plot=TRUE, ...)

## S3 method for class 'rma.mv'
profile(fitted, sigma2, tau2, rho, gamma2, phi, xlim, ylim, steps=20, lltol=1e-03,
        progbar=TRUE, parallel="no", ncpus=1, cl, plot=TRUE, ...)

## S3 method for class 'rma.uni.selmodel'
profile(fitted, tau2, delta, xlim, ylim, steps=20, lltol=1e-03,
        progbar=TRUE, parallel="no", ncpus=1, cl, plot=TRUE, ...)

## S3 method for class 'rma.ls'
profile(fitted, alpha, xlim, ylim, steps=20, lltol=1e-03,
        progbar=TRUE, parallel="no", ncpus=1, cl, plot=TRUE, ...)

## S3 method for class 'profile.rma'
print(x, ...)
## S3 method for class 'profile.rma'
plot(x, xlim, ylim, pch=19, xlab, ylab, main, refline=TRUE, cline=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profile.rma_+3A_fitted">fitted</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mv"</code>, <code>"rma.uni.selmodel"</code>, or <code>"rma.ls"</code>.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"profile.rma"</code> (for <code>plot</code> and <code>print</code>).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_sigma2">sigma2</code></td>
<td>
<p>optional integer to specify for which \(\sigma^2\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_tau2">tau2</code></td>
<td>
<p>optional integer to specify for which \(\tau^2\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_rho">rho</code></td>
<td>
<p>optional integer to specify for which \(\rho\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_gamma2">gamma2</code></td>
<td>
<p>optional integer to specify for which \(\gamma^2\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_phi">phi</code></td>
<td>
<p>optional integer to specify for which \(\phi\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_delta">delta</code></td>
<td>
<p>optional integer to specify for which \(\delta\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_alpha">alpha</code></td>
<td>
<p>optional integer to specify for which \(\alpha\) parameter the likelihood should be profiled.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_xlim">xlim</code></td>
<td>
<p>optional vector to specify the lower and upper limit of the parameter over which the profiling should be done. If unspecified, the function sets these limits automatically.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_ylim">ylim</code></td>
<td>
<p>optional vector to specify the y-axis limits when plotting the profiled likelihood. If unspecified, the function sets these limits automatically.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_steps">steps</code></td>
<td>
<p>number of points between <code>xlim[1]</code> and <code>xlim[2]</code> (inclusive) for which the likelihood should be evaluated (the default is 20). Can also be a numeric vector of length 2 or longer to specify for which parameter values the likelihood should be evaluated (in this case, <code>xlim</code> is automatically set to <code>range(steps)</code> if unspecified).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_lltol">lltol</code></td>
<td>
<p>numerical tolerance used when comparing values of the profiled log-likelihood with the log-likelihood of the fitted model (the default is 1e-03).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_parallel">parallel</code></td>
<td>
<p>character string to specify whether parallel processing should be used (the default is <code>"no"</code>). For parallel processing, set to either <code>"snow"</code> or <code>"multicore"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_ncpus">ncpus</code></td>
<td>
<p>integer to specify the number of processes to use in the parallel processing.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_cl">cl</code></td>
<td>
<p>optional cluster to use if <code>parallel="snow"</code>. If unspecified, a cluster on the local machine is created for the duration of the call.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_plot">plot</code></td>
<td>
<p>logical to specify whether the profile plot should be drawn after profiling is finished (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use. By default, a filled circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_refline">refline</code></td>
<td>
<p>logical to specify whether the value of the parameter estimate should be indicated by a dotted vertical line and its log-likelihood value by a dotted horizontal line (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_cline">cline</code></td>
<td>
<p>logical to specify whether a horizontal reference line should be added to the plot that indicates the log-likelihood value corresponding to the 95% profile confidence interval (the default is <code>FALSE</code>). Can also be a numeric value between 0 and 100 to specify the confidence interval level.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_main">main</code></td>
<td>
<p>title for the plot. If unspecified, the function sets an appropriate title.</p>
</td></tr>
<tr><td><code id="profile.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fixes a particular parameter of the model and then computes the maximized (restricted) log-likelihood over the remaining parameters of the model. By fixing the parameter of interest to a range of values, a profile of the (restricted) log-likelihood is constructed.
</p>


<h4>Selecting the Parameter(s) to Profile</h4>

<p>The parameters that can be profiled depend on the model object:
</p>

<ul>
<li><p> For objects of class <code>"rma.uni"</code> obtained with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, the function profiles over \(\tau^2\) (not for equal-effects models).
</p>
</li>
<li><p> For objects of class <code>"rma.mv"</code> obtained with the <code><a href="#topic+rma.mv">rma.mv</a></code> function, profiling is done by default over all variance and correlation components of the model. Alternatively, one can use the <code>sigma2</code>, <code>tau2</code>, <code>rho</code>, <code>gamma2</code>, or <code>phi</code> arguments to specify over which parameter the profiling should be done. Only one of these arguments can be used at a time. A single integer is used to specify the number of the parameter.
</p>
</li>
<li><p> For selection model objects of class <code>"rma.uni.selmodel"</code> obtained with the <code><a href="#topic+selmodel">selmodel</a></code> function, profiling is done by default over \(\tau^2\) (for models where this is an estimated parameter) and all selection model parameters. Alternatively, one can choose to profile only \(\tau^2\) by setting <code>tau2=TRUE</code> or one can select one of the selection model parameters to profile by specifying its number via the <code>delta</code> argument.
</p>
</li>
<li><p> For location-scale model objects of class <code>"rma.ls"</code> obtained with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, profiling is done by default over all \(\alpha\) parameters that are part of the scale model. Alternatively, one can select one of the parameters to profile by specifying its number via the <code>alpha</code> argument.
</p>
</li></ul>




<h4>Interpreting Profile Likelihood Plots</h4>

<p>A profile likelihood plot should show a single peak at the corresponding ML/REML estimate. If <code>refline=TRUE</code> (the default), the value of the parameter estimate is indicated by a dotted vertical line and its log-likelihood value by a dotted horizontal line. Hence, the intersection of these two lines should correspond to the peak (assuming that the model was fitted with ML/REML estimation).
</p>
<p>When profiling a variance component (or some other parameter that cannot be negative), the peak may be at zero (if this corresponds to the ML/REML estimate of the parameter). In this case, the profiled log-likelihood should be a monotonically decreasing function of the parameter. Similarly, when profiling a correlation component, the peak may be at -1 or +1.
</p>
<p>If the profiled log-likelihood has multiple peaks, this indicates that the likelihood surface is not unimodal. In such cases, the ML/REML estimate may correspond to a local optimum (when the intersection of the two dotted lines is not at the highest peak).
</p>
<p>If the profile is flat (over the entire parameter space or large portions of it), then this suggests that at least some of the parameters of the model are not identifiable (and the parameter estimates obtained are to some extent arbitrary). See Raue et al. (2009) for some further discussion of parameter identifiability and the use of profile likelihoods to check for this.
</p>
<p>The function checks whether any profiled log-likelihood value is actually larger than the log-likelihood of the fitted model (using a numerical tolerance of <code>lltol</code>). If so, a warning is issued as this might indicate that the optimizer did not identify the actual ML/REML estimate of the parameter profiled.
</p>



<h4>Parallel Processing</h4>

<p>Profiling requires repeatedly refitting the model, which can be slow when \(k\) is large and/or the model is complex (the latter especially applies to <code>"rma.mv"</code> objects and also to certain <code>"rma.uni.selmodel"</code> or <code>"rma.ls"</code> objects). On machines with multiple cores, one can try to speed things up by delegating the model fitting to separate worker processes, that is, by setting <code>parallel="snow"</code> or <code>parallel="multicore"</code> and <code>ncpus</code> to some value larger than 1. Parallel processing makes use of the <code><a href="parallel.html#topic+parallel">parallel</a></code> package, using the <code><a href="parallel.html#topic+makePSOCKcluster">makePSOCKcluster</a></code> and <code><a href="parallel.html#topic+parLapply">parLapply</a></code> functions when <code>parallel="snow"</code> or using <code><a href="parallel.html#topic+mclapply">mclapply</a></code> when <code>parallel="multicore"</code> (the latter only works on Unix/Linux-alikes). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>



<h3>Value</h3>

<p>An object of class <code>"profile.rma"</code>. The object is a list (or list of such lists) containing the following components:
</p>
<p>One of the following (depending on the parameter that was actually profiled):
</p>
<table>
<tr><td><code>sigma2</code></td>
<td>
<p>values of \(\sigma^2\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>values of \(\tau^2\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>values of \(\rho\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>gamma2</code></td>
<td>
<p>values of \(\gamma^2\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>values of \(\phi\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>values of \(\delta\) over which the likelihood was profiled.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>values of \(\alpha\) over which the likelihood was profiled.</p>
</td></tr>
</table>
<p>In addition, the following components are included:
</p>
<table>
<tr><td><code>ll</code></td>
<td>
<p>(restricted) log-likelihood values at the corresponding parameter values.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>a matrix with the estimated model coefficients at the corresponding parameter values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>a matrix with the lower confidence interval bounds of the model coefficients at the corresponding parameter values.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>a matrix with the upper confidence interval bounds of the model coefficients at the corresponding parameter values.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>Note that the list is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Raue, A., Kreutz, C., Maiwald, T., Bachmann, J., Schilling, M., Klingmuller, U., &amp; Timmer, J. (2009). Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood. <em>Bioinformatics</em>, <b>25</b>(15), 1923&ndash;1929. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/bioinformatics/btp358&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mv">rma.mv</a></code>, and <code><a href="#topic+selmodel.rma.uni">selmodel</a></code> for functions to fit models for which profile likelihood plots can be drawn.
</p>
<p><code><a href="#topic+confint.rma">confint</a></code> for functions to compute corresponding profile likelihood confidence intervals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model using rma.uni()
res &lt;- rma(yi, vi, data=dat)

### profile over tau^2
profile(res, progbar=FALSE)

### adjust xlim
profile(res, progbar=FALSE, xlim=c(0,1))

### specify tau^2 values at which to profile the likelihood
profile(res, progbar=FALSE, steps=c(seq(0,0.2,length=20),seq(0.3,1,by=0.1)))

### change data into long format
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, append=FALSE)

### set levels of group variable ("exp" = experimental/vaccinated; "con" = control/non-vaccinated)
levels(dat.long$group) &lt;- c("exp", "con")

### set "con" to reference level
dat.long$group &lt;- relevel(dat.long$group, ref="con")

### calculate log odds and corresponding sampling variances
dat.long &lt;- escalc(measure="PLO", xi=out1, mi=out2, data=dat.long)
dat.long

### fit bivariate random-effects model using rma.mv()
res &lt;- rma.mv(yi, vi, mods = ~ group, random = ~ group | study, struct="UN", data=dat.long)
res

### profile over tau^2_1, tau^2_2, and rho
### note: for rho, adjust region over which profiling is done ('zoom in' on area around estimate)
## Not run: 
par(mfrow=c(2,2))
profile(res, tau2=1)
profile(res, tau2=2)
profile(res, rho=1, xlim=c(0.90, 0.98))
par(mfrow=c(1,1))

## End(Not run)

### an example where the peak of the likelihood profile is at 0
dat &lt;- escalc(measure="RD", n1i=n1i, n2i=n2i, ai=ai, ci=ci, data=dat.hine1989)
res &lt;- rma(yi, vi, data=dat)
profile(res, progbar=FALSE)
</code></pre>

<hr>
<h2 id='qqnorm.rma'>Normal QQ Plots for 'rma' Objects</h2><span id='topic+qqnorm'></span><span id='topic+qqnorm.rma'></span><span id='topic+qqnorm.rma.uni'></span><span id='topic+qqnorm.rma.mh'></span><span id='topic+qqnorm.rma.peto'></span><span id='topic+qqnorm.rma.glmm'></span><span id='topic+qqnorm.rma.mv'></span>

<h3>Description</h3>

<p>Function to create normal QQ plots for objects of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, and <code>"rma.peto"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
qqnorm(y, type="rstandard", pch=21, col, bg,
       envelope=TRUE, level=y$level, bonferroni=FALSE, reps=1000, smooth=TRUE, bass=0,
       label=FALSE, offset=0.3, pos=13, lty, ...)
## S3 method for class 'rma.mh'
qqnorm(y, type="rstandard", pch=21, col, bg, label=FALSE, offset=0.3, pos=13, ...)
## S3 method for class 'rma.peto'
qqnorm(y, type="rstandard", pch=21, col, bg, label=FALSE, offset=0.3, pos=13, ...)
## S3 method for class 'rma.glmm'
qqnorm(y, ...)
## S3 method for class 'rma.mv'
qqnorm(y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qqnorm.rma_+3A_y">y</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, or <code>"rma.peto"</code>. The method is not yet implemented for objects of class <code>"rma.glmm"</code> or <code>"rma.mv"</code>.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_type">type</code></td>
<td>
<p>character string (either <code>"rstandard"</code> (default) or <code>"rstudent"</code>) to specify whether standardized residuals or studentized deleted residuals should be used in creating the plot. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the observed outcomes. By default, an open circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_col">col</code></td>
<td>
<p>optional character string to specify the (border) color of the points.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_bg">bg</code></td>
<td>
<p>optional character string to specify the background color of open plot symbols.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_envelope">envelope</code></td>
<td>
<p>logical to specify whether a pseudo confidence envelope should be simulated and added to the plot (the default is <code>TRUE</code>)). Only for objects of class <code>"rma.uni"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the level of the pseudo confidence envelope (see <a href="#topic+misc-options">here</a> for details). The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_bonferroni">bonferroni</code></td>
<td>
<p>logical to specify whether the bounds of the envelope should be Bonferroni corrected.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_reps">reps</code></td>
<td>
<p>numeric value to specify the number of iterations for simulating the pseudo confidence envelope (the default is 1000).</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_smooth">smooth</code></td>
<td>
<p>logical to specify whether the results from the simulation should be smoothed (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_bass">bass</code></td>
<td>
<p>numeric value that controls the degree of smoothing (the default is 0).</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_label">label</code></td>
<td>
<p>argument to control the labeling of the points (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_offset">offset</code></td>
<td>
<p>argument to control the distance between the points and the corresponding labels.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_pos">pos</code></td>
<td>
<p>argument to control the position of the labels.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_lty">lty</code></td>
<td>
<p>optional character string to specify the line type for the diagonal line and the pseudo confidence envelope. If unspecified, the function sets this to <code>c("solid", "dotted")</code> by default.</p>
</td></tr>
<tr><td><code id="qqnorm.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot shows the theoretical quantiles of a normal distribution on the horizontal axis against the observed quantiles for either the standardized residuals (<code>type="rstandard"</code>, the default) or the externally standardized residuals (<code>type="rstudent"</code>) on the vertical axis (see <code><a href="#topic+residuals.rma">residuals</a></code> for details on the definition of these residual types).
</p>
<p>For reference, a line is added to the plot with a slope of 1, going through the (0,0) point.
</p>
<p>For objects of class <code>"rma.uni"</code>, it is also possible to add a pseudo confidence envelope to the plot. The envelope is created based on the quantiles of sets of pseudo residuals simulated from the given model (for details, see Cook &amp; Weisberg, 1982). The number of sets simulated can be controlled with the <code>reps</code> argument. When <code>smooth=TRUE</code>, the simulated bounds are smoothed with Friedman's SuperSmoother (see <code><a href="stats.html#topic+supsmu">supsmu</a></code>). The <code>bass</code> argument can be set to a number between 0 and 10, with higher numbers indicating increasing smoothness. If <code>bonferroni=TRUE</code>, the envelope bounds are Bonferroni corrected, so that the envelope can be regarded as a confidence region for all \(k\) residuals simultaneously. The default however is <code>bonferroni=FALSE</code>, which makes the plot more sensitive to deviations from normality.
</p>
<p>With the <code>label</code> argument, one can control whether points in the plot will be labeled (e.g., to identify outliers). If <code>label="all"</code> (or <code>label=TRUE</code>), all points in the plot will be labeled. If <code>label="out"</code>, points falling outside of the confidence envelope will be labeled (only available for objects of class <code>"rma.uni"</code>). Finally, one can also set this argument to a numeric value (between 1 and \(k\)), indicating how many of the most extreme points should be labeled (for example, with <code>label=1</code> only the most extreme point is labeled, while with <code>label=3</code>, the most extreme, and the second and third most extreme points is labeled). With the <code>offset</code> argument, one can adjust the distance between the labels and the corresponding points. The <code>pos</code> argument is the position specifier for the labels (<code>1</code>, <code>2</code>, <code>3</code>, and <code>4</code>, respectively indicate positions below, to the left of, above, and to the right of the points; <code>13</code> places the labels below the points for points that fall below the reference line and above otherwise; <code>24</code> places the labels to the left of the points for points that fall above the reference line and to the right otherwise).
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
</table>
<p>Note that the list is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Cook, R. D., &amp; Weisberg, S. (1982). <em>Residuals and influence in regression</em>. London: Chapman and Hall.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>
<p>Wang, M. C., &amp; Bushman, B. J. (1998). Using the normal quantile plot to explore meta-analytic data sets. <em>Psychological Methods</em>, <b>3</b>(1), 46&ndash;54. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.3.1.46&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, and <code><a href="#topic+rma.peto">rma.peto</a></code> for functions to fit models for which normal QQ plots can be drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### draw QQ plot
qqnorm(res)

### fit mixed-effects model with absolute latitude as moderator
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)

### draw QQ plot
qqnorm(res)
</code></pre>

<hr>
<h2 id='radial'>Radial (Galbraith) Plots for 'rma' Objects</h2><span id='topic+radial'></span><span id='topic+galbraith'></span><span id='topic+radial.rma'></span>

<h3>Description</h3>

<p>Function to create radial (also called Galbraith) plots for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>radial(x, ...)
galbraith(x, ...)

## S3 method for class 'rma'
radial(x, center=FALSE, xlim, zlim, xlab, zlab,
       atz, aty, steps=7, level=x$level, digits=2,
       transf, targs, pch=21, col, bg, back, arc.res=100,
       cex, cex.lab, cex.axis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="radial_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="radial_+3A_center">center</code></td>
<td>
<p>logical to indicate whether the plot should be centered horizontally at the model estimate (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="radial_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="radial_+3A_zlim">zlim</code></td>
<td>
<p>z-axis limits. If unspecified, the function sets the z-axis limits to some sensible values (note that the z-axis limits are the actual vertical limit of the plotting region).</p>
</td></tr>
<tr><td><code id="radial_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="radial_+3A_zlab">zlab</code></td>
<td>
<p>title for the z-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="radial_+3A_atz">atz</code></td>
<td>
<p>position for the z-axis tick marks and labels. If unspecified, these values are set by the function.</p>
</td></tr>
<tr><td><code id="radial_+3A_aty">aty</code></td>
<td>
<p>position for the y-axis tick marks and labels. If unspecified, these values are set by the function.</p>
</td></tr>
<tr><td><code id="radial_+3A_steps">steps</code></td>
<td>
<p>the number of tick marks for the y-axis (the default is 7). Ignored when argument <code>aty</code> is used.</p>
</td></tr>
<tr><td><code id="radial_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the level of the z-axis error region. The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="radial_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the tick mark labels of the y-axis should be rounded (the default is 2).</p>
</td></tr>
<tr><td><code id="radial_+3A_transf">transf</code></td>
<td>
<p>argument to specify a function to transform the y-axis labels (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="radial_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code>.</p>
</td></tr>
<tr><td><code id="radial_+3A_pch">pch</code></td>
<td>
<p>plotting symbol. By default, an open circle is used. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="radial_+3A_col">col</code></td>
<td>
<p>character string to specify the (border) color of the points.</p>
</td></tr>
<tr><td><code id="radial_+3A_bg">bg</code></td>
<td>
<p>character string to specify the background color of open plot symbols.</p>
</td></tr>
<tr><td><code id="radial_+3A_back">back</code></td>
<td>
<p>character string to specify the background color of the z-axis error region. If unspecified, a shade of gray is used. Set to <code>NA</code> to suppress shading of the region.</p>
</td></tr>
<tr><td><code id="radial_+3A_arc.res">arc.res</code></td>
<td>
<p>integer to specify the number of line segments (i.e., the resolution) when drawing the y-axis and confidence interval arcs (the default is 100).</p>
</td></tr>
<tr><td><code id="radial_+3A_cex">cex</code></td>
<td>
<p>symbol expansion factor.</p>
</td></tr>
<tr><td><code id="radial_+3A_cex.lab">cex.lab</code></td>
<td>
<p>character expansion factor for axis labels.</p>
</td></tr>
<tr><td><code id="radial_+3A_cex.axis">cex.axis</code></td>
<td>
<p>character expansion factor for axis annotations.</p>
</td></tr>
<tr><td><code id="radial_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an equal-effects model, the plot shows the inverse of the standard errors on the horizontal axis (i.e., \(1/\sqrt{v_i}\), where \(v_i\) is the sampling variance of the observed effect size or outcome) against the observed effect sizes or outcomes standardized by their corresponding standard errors on the vertical axis (i.e., \(y_i/\sqrt{v_i}\)). Since the vertical axis corresponds to standardized values, it is referred to as the z-axis within this function. On the right hand side of the plot, an arc is drawn (referred to as the y-axis within this function) corresponding to the observed effect sizes or outcomes. A line projected from (0,0) through a particular point within the plot onto this arc indicates the value of the observed effect size or outcome for that point.
</p>
<p>For a random-effects model, the function uses \(1/\sqrt{v_i + \tau^2}\) for the horizontal axis, where \(\tau^2\) is the amount of heterogeneity as estimated based on the model. For the z-axis, \(y_i/\sqrt{v_i + \tau^2}\) is used to compute standardized values of the observed effect sizes or outcomes.
</p>
<p>The second (inner/smaller) arc that is drawn on the right hand side indicates the model estimate (in the middle of the arc) and the corresponding confidence interval (at the ends of the arc).
</p>
<p>The shaded region in the plot is the z-axis error region. For <code>level=95</code> (or if this was the <code>level</code> value when the model was fitted), this corresponds to z-axis values equal to \(\pm 1.96\). Under the assumptions of the equal/random-effects models, approximately 95% of the points should fall within this region.
</p>
<p>When <code>center=TRUE</code>, the values on the y-axis are centered around the model estimate. As a result, the plot is centered horizontally at the model estimate.
</p>
<p>If the z-axis label on the left is too close to the actual z-axis and/or the arc on the right is clipped, then this can be solved by increasing the margins on the right and/or left (see <code><a href="graphics.html#topic+par">par</a></code> and in particular the <code>mar</code> argument).
</p>
<p>Note that radial plots cannot be drawn for models that contain moderators.
</p>


<h3>Value</h3>

<p>A data frame with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>ids</code></td>
<td>
<p>the study id numbers.</p>
</td></tr>
<tr><td><code>slab</code></td>
<td>
<p>the study labels.</p>
</td></tr>
</table>
<p>Note that the data frame is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Galbraith, R. F. (1988). Graphical display of estimates having differing standard errors. <em>Technometrics</em>, <b>30</b>(3), 271&ndash;281. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/00401706.1988.10488400&#8288;</code>
</p>
<p>Galbraith, R. F. (1988). A note on graphical presentation of estimated odds ratios from several clinical trials. <em>Statistics in Medicine</em>, <b>7</b>(8), 889&ndash;894. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780070807&#8288;</code>
</p>
<p>Galbraith, R. F (1994). Some applications of radial plots. <em>Journal of the American Statistical Association</em>, <b>89</b>(428), 1232&ndash;1242. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/01621459.1994.10476864&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which radial plots can be drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
dat

### fit equal-effects model
res &lt;- rma(yi, vi, data=dat, method="EE")

### draw radial plot
radial(res)

### the line from (0,0) with a slope equal to the log risk ratio from the 4th study points
### to the corresponding effect size value on the arc (i.e., -1.44)
abline(a=0, b=dat$yi[4], lty="dotted")
dat$yi[4]

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(yi, vi, data=dat)

### draw radial plot
radial(res)

### center the values around the model estimate
radial(res, center=TRUE)

### show risk ratio values on the y-axis arc
radial(res, transf=exp)
</code></pre>

<hr>
<h2 id='ranef'>Best Linear Unbiased Predictions for 'rma.uni' and 'rma.mv' Objects</h2><span id='topic+ranef'></span><span id='topic+ranef.rma.uni'></span><span id='topic+ranef.rma.mv'></span>

<h3>Description</h3>

<p>Functions to compute best linear unbiased predictions (BLUPs) of the random effects for objects of class <code>"rma.uni"</code> and <code>"rma.mv"</code>. Corresponding standard errors and prediction interval bounds are also provided. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
ranef(object, level, digits, transf, targs, ...)
## S3 method for class 'rma.mv'
ranef(object, level, digits, transf, targs, verbose=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ranef_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma.uni"</code> or <code>"rma.mv"</code>.</p>
</td></tr>
<tr><td><code id="ranef_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the prediction interval level (see <a href="#topic+misc-options">here</a> for details). If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="ranef_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="ranef_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the predicted values and interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="ranef_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified under <code>transf</code>.</p>
</td></tr>
<tr><td><code id="ranef_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the computations (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ranef_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For objects of class <code>"rma.uni"</code>, an object of class <code>"list.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted values.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>pi.lb</code></td>
<td>
<p>lower bound of the prediction intervals.</p>
</td></tr>
<tr><td><code>pi.ub</code></td>
<td>
<p>upper bound of the prediction intervals.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The object is formatted and printed with the <code><a href="#topic+print.list.rma">print</a></code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>
<p>For objects of class <code>"rma.mv"</code>, a list of data frames with the same components as described above.
</p>


<h3>Note</h3>

<p>For best linear unbiased predictions that combine the fitted values based on the fixed effects and the estimated contributions of the random effects, see <code><a href="#topic+blup.rma.uni">blup</a></code>.
</p>
<p>For predicted/fitted values that are based only on the fixed effects of the model, see <code><a href="#topic+fitted.rma">fitted</a></code> and <code><a href="#topic+predict.rma">predict</a></code>.
</p>
<p>Equal-effects models do not contain random study effects. The BLUPs for these models will therefore be 0.
</p>
<p>When using the <code>transf</code> argument, the transformation is applied to the predicted values and the corresponding interval bounds. The standard errors are then set equal to <code>NA</code> and are omitted from the printed output.
</p>
<p>By default, a standard normal distribution is used to construct the prediction intervals. When the model was fitted with <code>test="t"</code>, <code>test="knha"</code>, <code>test="hksj"</code>, or <code>test="adhoc"</code>, then a t-distribution with \(k-p\) degrees of freedom is used.
</p>
<p>To be precise, it should be noted that the function actually computes empirical BLUPs (eBLUPs), since the predicted values are a function of the estimated variance component(s).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Kackar, R. N., &amp; Harville, D. A. (1981). Unbiasedness of two-stage estimation and prediction procedures for mixed linear models. Communications in Statistics, Theory and Methods, <b>10</b>(13), 1249&ndash;1261. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/03610928108828108&#8288;</code>
</p>
<p>Raudenbush, S. W., &amp; Bryk, A. S. (1985). Empirical Bayes meta-analysis. <em>Journal of Educational Statistics</em>, <b>10</b>(2), 75&ndash;98. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986010002075&#8288;</code>
</p>
<p>Robinson, G. K. (1991). That BLUP is a good thing: The estimation of random effects. <em>Statistical Science</em>, <b>6</b>(1), 15&ndash;32. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/ss/1177011926&#8288;</code>
</p>
<p>Searle, S. R., Casella, G., &amp; McCulloch, C. E. (1992). <em>Variance components</em>. Hoboken, NJ: Wiley.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which BLUPs of the random effects can be computed.
</p>
<p><code><a href="#topic+predict.rma">predict</a></code> and <code><a href="#topic+fitted.rma">fitted</a></code> for functions to compute the predicted/fitted values based only on the fixed effects and <code><a href="#topic+blup.rma.uni">blup</a></code> for a function to compute BLUPs that combine the fitted values and predicted random effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(yi, vi, data=dat)

### BLUPs of the random effects
ranef(res)
</code></pre>

<hr>
<h2 id='ranktest'>Rank Correlation Test for Funnel Plot Asymmetry</h2><span id='topic+ranktest'></span>

<h3>Description</h3>

<p>Function to carry out the rank correlation test for funnel plot asymmetry.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ranktest(x, vi, sei, subset, data, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ranktest_+3A_x">x</code></td>
<td>
<p>a vector with the observed effect sizes or outcomes or an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="ranktest_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="ranktest_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="ranktest_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the test (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="ranktest_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="ranktest_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded.</p>
</td></tr>
<tr><td><code id="ranktest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function carries out the rank correlation test as described by Begg and Mazumdar (1994). The test can be used to examine whether the observed effect sizes or outcomes and the corresponding sampling variances are correlated. A high correlation would indicate that the funnel plot is asymmetric, which may be a result of publication bias.
</p>
<p>One can either pass a vector with the observed effect sizes or outcomes (via <code>x</code>) and the corresponding sampling variances via <code>vi</code> (or the standard errors via <code>sei</code>) to the function or an object of class <code>"rma"</code>. When passing a model object, the model must be a model without moderators (i.e., either an equal- or a random-effects model).
</p>


<h3>Value</h3>

<p>An object of class <code>"ranktest"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>tau</code></td>
<td>
<p>the estimated value of Kendall's tau rank correlation coefficient.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>the corresponding p-value for the test that the true tau value is equal to zero.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.ranktest">print</a></code> function.
</p>


<h3>Note</h3>

<p>The method does not depend on the model fitted. Therefore, regardless of the model passed to the function, the results of the rank test will always be the same. See <code><a href="#topic+regtest">regtest</a></code> for tests of funnel plot asymmetry that are based on regression models and model dependent.
</p>
<p>The function makes use of the <code><a href="stats.html#topic+cor.test">cor.test</a></code> function with <code>method="kendall"</code>. If possible, an exact p-value is provided; otherwise, a large-sample approximation is used.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Begg, C. B., &amp; Mazumdar, M. (1994). Operating characteristics of a rank correlation test for publication bias. <em>Biometrics</em>, <b>50</b>(4), 1088&ndash;1101. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2533446&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regtest">regtest</a></code> for the regression test, <code><a href="#topic+trimfill">trimfill</a></code> for the trim and fill method, <code><a href="#topic+tes">tes</a></code> for the test of excess significance, <code><a href="#topic+fsn">fsn</a></code> to compute the fail-safe N (file drawer analysis), and <code><a href="#topic+selmodel">selmodel</a></code> for selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### carry out the rank correlation test
ranktest(res)

### can also pass the observed outcomes and corresponding sampling variances to the function
ranktest(yi, vi, data=dat)
</code></pre>

<hr>
<h2 id='rcalc'>Calculate the Variance-Covariance of Dependent Correlation Coefficients</h2><span id='topic+rcalc'></span>

<h3>Description</h3>

<p>Function to calculate the variance-covariance matrix of correlation coefficients computed based on the same sample of subjects. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcalc(x, ni, data, rtoz=FALSE, nfun="min", sparse=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcalc_+3A_x">x</code></td>
<td>
<p>a formula of the form <code>ri ~ var1 + var2 | study</code>. Can also be a correlation matrix or list thereof. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rcalc_+3A_ni">ni</code></td>
<td>
<p>vector to specify the sample sizes based on which the correlations were computed.</p>
</td></tr>
<tr><td><code id="rcalc_+3A_data">data</code></td>
<td>
<p>data frame containing the variables specified via the formula (and the sample sizes).</p>
</td></tr>
<tr><td><code id="rcalc_+3A_rtoz">rtoz</code></td>
<td>
<p>logical to specify whether to transform the correlations via Fisher's r-to-z transformation (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="rcalc_+3A_nfun">nfun</code></td>
<td>
<p>a character string to specify how the &lsquo;common&rsquo; sample size within each study should be computed. Possible options are <code>"min"</code> (for the minimum), <code>"harmonic"</code> (for the harmonic mean), or <code>"mean"</code> (for the arithmetic mean). Can also be a function. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rcalc_+3A_sparse">sparse</code></td>
<td>
<p>logical to specify whether the variance-covariance matrix should be returned as a sparse matrix (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="rcalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A meta-analysis of correlation coefficients may involve multiple correlation coefficients extracted from the same study. When these correlations are computed based on the same sample of subjects, then they are typically not independent. The <code>rcalc</code> function can be used to create a dataset with the correlation coefficients (possibly transformed with Fisher's r-to-z transformation) and the corresponding variance-covariance matrix. The dataset and variance-covariance matrix can then be further meta-analyzed using the <code><a href="#topic+rma.mv">rma.mv</a></code> function.
</p>
<p>When computing the covariance between two correlation coefficients, we can distinguish two cases:
</p>

<ol>
<li><p> In the first case, one of the variables involved in the two correlation coefficients is the same. For example, in \(r_{12}\) and \(r_{13}\), variable 1 is common to both correlation coefficients. This is sometimes called the (partially) &lsquo;overlapping&rsquo; case. The covariance between the two correlation coefficients, \(\mbox{Cov}[r_{12}, r_{13}]\), then depends on the degree of correlation between variables 2 and 3 (i.e., \(r_{23}\)).
</p>
</li>
<li><p> In the second case, none of the variables are common to both correlation coefficients. For example, this would be the case if we have correlations \(r_{12}\) and \(r_{34}\) based on 4 variables. This is sometimes called the &lsquo;non-overlapping&rsquo; case. The covariance between the two correlation coefficients, \(\mbox{Cov}[r_{12}, r_{34}]\), then depends on \(r_{13}\), \(r_{14}\), \(r_{23}\), and \(r_{24}\).
</p>
</li></ol>

<p>Equations to compute these covariances can be found, for example, in Steiger (1980) and Olkin and Finn (1990).
</p>
<p>To use the <code>rcalc</code> function, one needs to construct a data frame that contains a study identifier (say <code>study</code>), two variable identifiers (say <code>var1</code> and <code>var2</code>), the corresponding correlation coefficients (say <code>ri</code>), and the sample sizes based on which the correlation coefficients were computed (say <code>ni</code>). Then the first argument should be a formula of the form <code>ri ~ var1 + var2 | study</code>, argument <code>ni</code> is set equal to the variable name containing the sample sizes, and the data frame containing these variables is specified via the <code>data</code> argument. When using the function for a single study, one can leave out the study identifier from the formula.
</p>
<p>When argument <code>rtoz</code> is set to <code>TRUE</code>, then the correlations are transformed with Fisher's r-to-z transformation (Fisher, 1921) and the variance-covariance matrix is computed for the transformed values.
</p>
<p>In some cases, the sample size may not be identical within a study (e.g., \(r_{12}\) may have been computed based on 120 subjects while \(r_{13}\) was computed based on 118 subjects due to 2 missing values in variable 3). For constructing the variance-covariance matrix, we need to assume a &lsquo;common&rsquo; sample size for all correlation coefficients within the study. Argument <code>nfun</code> provides some options for how the common sample size should be computed. Possible options are <code>"min"</code> (for using the minimum sample size within a study as the common sample size), <code>"harmonic"</code> (for using the harmonic mean), or <code>"mean"</code> (for using the arithmetic mean). The default is <code>"min"</code>, which is a conservative choice (i.e., it will overestimate the sampling variances of coefficients that were computed based on a sample size that was actually larger than the minimum sample size). One can also specify a function via the <code>nfun</code> argument (which should take a numeric vector as input and return a single value).
</p>
<p>Instead of specifying a formula, one can also pass a correlation matrix to the function via argument <code>x</code>. Argument <code>ni</code> then specifies the (common) sample size based on which the elements in the correlation matrix were computed. One can also pass a list of correlation matrices via argument <code>x</code>, in which case argument <code>ni</code> should be a vector of sample sizes of the same length as <code>x</code>.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>dat</code></td>
<td>
<p>a data frame with the study identifier, the two variable identifiers, a variable pair identifier, the correlation coefficients (possibly transformed with Fisher's r-to-z transformation), and the (common) sample sizes.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>corresponding variance-covariance matrix (given as a sparse matrix when <code>sparse=TRUE</code>).</p>
</td></tr>
</table>
<p>Note that a particular covariance can only be computed when all of the correlation coefficients involved in the covariance equation are included in the dataset. If one or more coefficients needed for the computation are missing, then the resulting covariance will also be missing (i.e., <code>NA</code>).
</p>


<h3>Note</h3>

<p>For raw correlation coefficients, the variance-covariance matrix is computed with \(n-1\) in the denominator (instead of \(n\) as suggested in Steiger, 1980, and Olkin &amp; Finn, 1990). This is more consistent with the usual equation for computing the sampling variance of a correlation coefficient (which also typically uses \(n-1\) in the denominator).
</p>
<p>For raw and r-to-z transformed coefficients, the variance-covariance matrix will only be computed when the (common) sample size for a study is at least 5.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Fisher, R. A. (1921). On the &ldquo;probable error&rdquo; of a coefficient of correlation deduced from a small sample. <em>Metron</em>, <b>1</b>, 1&ndash;32. <code style="white-space: pre;">&#8288;http://hdl.handle.net/2440/15169&#8288;</code>
</p>
<p>Olkin, I., &amp; Finn, J. D. (1990). Testing correlated correlations. <em>Psychological Bulletin</em>, <b>108</b>(2), 330&ndash;333. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.108.2.330&#8288;</code>
</p>
<p>Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix. <em>Psychological Bulletin</em>, <b>87</b>(2), 245&ndash;251. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.87.2.245&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.mv">rma.mv</a></code> for a model fitting function that can be used to meta-analyze dependent correlation coefficients.
</p>
<p><code><a href="metadat.html#topic+dat.craft2003">dat.craft2003</a></code> for an illustrative example.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### copy data into 'dat' and examine the first 12 rows
dat &lt;- dat.craft2003
head(dat, 12)

### construct dataset and var-cov matrix of the correlations
tmp &lt;- rcalc(ri ~ var1 + var2 | study, ni=ni, data=dat)
V &lt;- tmp$V
dat &lt;- tmp$dat

### examine data and var-cov matrix for study 1
dat[dat$study == 1,]
blsplit(V, dat$study, round, 4)$`1`

### examine data and var-cov matrix for study 6
dat[dat$study == 6,]
blsplit(V, dat$study, round, 4)$`6`

### examine data and var-cov matrix for study 17
dat[dat$study == 17,]
blsplit(V, dat$study, round, 4)$`17`

############################################################################

### copy data into 'dat' and examine the first 12 rows
dat &lt;- dat.craft2003
head(dat, 12)

### restructure data from study 1 into a correlation matrix
R1 &lt;- diag(4)
R1[lower.tri(R1)] &lt;- dat$ri[dat$study == 1]
R1[upper.tri(R1)] &lt;- t(R1)[upper.tri(R1)]
rownames(R1) &lt;- colnames(R1) &lt;- c("perf", "acog", "asom", "conf")
R1

### restructure data from study 3 into a correlation matrix
R3 &lt;- diag(4)
R3[lower.tri(R3)] &lt;- dat$ri[dat$study == 3]
R3[upper.tri(R3)] &lt;- t(R3)[upper.tri(R3)]
rownames(R3) &lt;- colnames(R3) &lt;- c("perf", "acog", "asom", "conf")
R3

### an example where a correlation matrix is passed to rcalc()
rcalc(R1, ni=142)

### an example where a list of correlation matrices is passed to rcalc()
tmp &lt;- rcalc(list("1"=R1,"3"=R3), ni=c(142,37))
V &lt;- tmp$V
dat &lt;- tmp$dat

### examine data and var-cov matrix for study 1
dat[dat$id == 1,]
blsplit(V, dat$id, round, 4)$`1`

### examine data and var-cov matrix for study 3
dat[dat$id == 3,]
blsplit(V, dat$id, round, 4)$`3`

############################################################################
</code></pre>

<hr>
<h2 id='regplot'>Scatter Plots / Bubble Plots</h2><span id='topic+regplot'></span><span id='topic+regplot.rma'></span><span id='topic+points.regplot'></span>

<h3>Description</h3>

<p>Function to create scatter plots / bubble plots based on meta-regression models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regplot(x, ...)

## S3 method for class 'rma'
regplot(x, mod, pred=TRUE, ci=TRUE, pi=FALSE, shade=TRUE,
        xlim, ylim, predlim, olim, xlab, ylab, at, digits=2L,
        transf, atransf, targs, level=x$level,
        pch, psize, plim=c(0.5,3), col, bg, slab,
        grid=FALSE, refline, label=FALSE, offset=c(1,1), labsize=1,
        lcol, lwd, lty, legend=FALSE, xvals, ...)

## S3 method for class 'regplot'
points(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regplot_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mv"</code>, or <code>"rma.glmm"</code> including one or multiple moderators (or an object of class <code>"regplot"</code> for <code>points</code>).</p>
</td></tr>
<tr><td><code id="regplot_+3A_mod">mod</code></td>
<td>
<p>either a scalar to specify the position of the moderator variable in the model or a character string to specify the name of the moderator variable.</p>
</td></tr>
<tr><td><code id="regplot_+3A_pred">pred</code></td>
<td>
<p>logical to indicate whether the (marginal) regression line based on the moderator should be added to the plot (the default is <code>TRUE</code>). Can also be an object from <code><a href="#topic+predict.rma">predict</a></code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regplot_+3A_ci">ci</code></td>
<td>
<p>logical to indicate whether the corresponding confidence interval bounds should be added to the plot (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="regplot_+3A_pi">pi</code></td>
<td>
<p>logical to indicate whether the corresponding prediction interval bounds should be added to the plot (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="regplot_+3A_shade">shade</code></td>
<td>
<p>logical to indicate whether the confidence/prediction interval regions should be shaded (the default is <code>TRUE</code>). Can also be a two-element character vector to specify the colors for shading the confidence and prediction interval regions (if shading only the former, a single color can also be specified).</p>
</td></tr>
<tr><td><code id="regplot_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits. If unspecified, the function sets the x-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="regplot_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits. If unspecified, the function sets the y-axis limits to some sensible values.</p>
</td></tr>
<tr><td><code id="regplot_+3A_predlim">predlim</code></td>
<td>
<p>optional argument to specify the limits of the (marginal) regression line. If unspecified, the limits are based on the range of the moderator variable.</p>
</td></tr>
<tr><td><code id="regplot_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="regplot_+3A_xlab">xlab</code></td>
<td>
<p>title for the x-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="regplot_+3A_ylab">ylab</code></td>
<td>
<p>title for the y-axis. If unspecified, the function sets an appropriate axis title.</p>
</td></tr>
<tr><td><code id="regplot_+3A_at">at</code></td>
<td>
<p>position of the y-axis tick marks and corresponding labels. If unspecified, the function sets the tick mark positions/labels to some sensible values.</p>
</td></tr>
<tr><td><code id="regplot_+3A_digits">digits</code></td>
<td>
<p>integer to specify the number of decimal places to which the tick mark labels of the y-axis should be rounded. When specifying an integer (e.g., <code>2L</code>), trailing zeros after the decimal mark are dropped for the y-axis labels. When specifying a numeric value (e.g., <code>2</code>), trailing zeros are retained.</p>
</td></tr>
<tr><td><code id="regplot_+3A_transf">transf</code></td>
<td>
<p>optional argument to specify a function to transform the observed outcomes, predicted values, and confidence/prediction interval bounds (e.g., <code>transf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="regplot_+3A_atransf">atransf</code></td>
<td>
<p>optional argument to specify a function to transform the y-axis labels (e.g., <code>atransf=exp</code>; see also <a href="#topic+transf">transf</a>). If unspecified, no transformation is used.</p>
</td></tr>
<tr><td><code id="regplot_+3A_targs">targs</code></td>
<td>
<p>optional arguments needed by the function specified via <code>transf</code> or <code>atransf</code>.</p>
</td></tr>
<tr><td><code id="regplot_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence/prediction interval level (see <a href="#topic+misc-options">here</a> for details). The default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="regplot_+3A_pch">pch</code></td>
<td>
<p>plotting symbol to use for the observed outcomes. By default, an open circle is used. Can also be a vector of values. See <code><a href="graphics.html#topic+points">points</a></code> for other options.</p>
</td></tr>
<tr><td><code id="regplot_+3A_psize">psize</code></td>
<td>
<p>optional numeric value to specify the point sizes for the observed outcomes. If unspecified, the point sizes are a function of the model weights. Can also be a vector of values. Can also be a character string (either <code>"seinv"</code> or <code>"vinv"</code>) to make the point sizes proportional to the inverse standard errors or inverse sampling variances.</p>
</td></tr>
<tr><td><code id="regplot_+3A_plim">plim</code></td>
<td>
<p>numeric vector of length 2 to scale the point sizes (ignored when a numeric value or vector is specified for <code>psize</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regplot_+3A_col">col</code></td>
<td>
<p>character string to specify the (border) color of the points. Can also be a vector.</p>
</td></tr>
<tr><td><code id="regplot_+3A_bg">bg</code></td>
<td>
<p>character string to specify the background color of open plot symbols. Can also be a vector.</p>
</td></tr>
<tr><td><code id="regplot_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies. If unspecified, the function tries to extract study labels from <code>x</code>.</p>
</td></tr>
<tr><td><code id="regplot_+3A_grid">grid</code></td>
<td>
<p>logical to specify whether a grid should be added to the plot. Can also be a color name for the grid.</p>
</td></tr>
<tr><td><code id="regplot_+3A_refline">refline</code></td>
<td>
<p>optional numeric value to specify the location of a horizontal reference line that should be added to the plot.</p>
</td></tr>
<tr><td><code id="regplot_+3A_label">label</code></td>
<td>
<p>argument to control the labeling of the points (the default is <code>FALSE</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regplot_+3A_offset">offset</code></td>
<td>
<p>argument to control the distance between the points and the corresponding labels. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regplot_+3A_labsize">labsize</code></td>
<td>
<p>numeric value to control the size of the labels.</p>
</td></tr>
<tr><td><code id="regplot_+3A_lcol">lcol</code></td>
<td>
<p>optional vector of (up to) four elements to specify the color of the regression line, of the confidence interval bounds, of the prediction interval bounds, and of the horizontal reference line.</p>
</td></tr>
<tr><td><code id="regplot_+3A_lty">lty</code></td>
<td>
<p>optional vector of (up to) four elements to specify the line type of the regression line, of the confidence interval bounds, of the prediction interval bounds, and of the horizontal reference line.</p>
</td></tr>
<tr><td><code id="regplot_+3A_lwd">lwd</code></td>
<td>
<p>optional vector of (up to) four elements to specify the line width of the regression line, of the confidence interval bounds, of the prediction interval bounds, and of the horizontal reference line.</p>
</td></tr>
<tr><td><code id="regplot_+3A_legend">legend</code></td>
<td>
<p>logical to indicate whether a legend should be added to the plot (the default is <code>FALSE</code>). Can also be a keyword to indicate the position of the legend (see <code><a href="graphics.html#topic+legend">legend</a></code>).</p>
</td></tr>
<tr><td><code id="regplot_+3A_xvals">xvals</code></td>
<td>
<p>optional numeric vector to specify the values of the moderator for which predicted values should be computed. Needs to be specified when passing an object from <code><a href="#topic+predict.rma">predict</a></code> to the <code>pred</code> argument. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regplot_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function draws a scatter plot of the values of a moderator variable in a meta-regression model (on the x-axis) against the observed effect sizes or outcomes (on the y-axis). The regression line from the model (with corresponding confidence interval bounds) is added to the plot by default. These types of plots are also often referred to as &lsquo;bubble plots&rsquo; as the points are typically drawn in different sizes to reflect their precision or weight in the model.
</p>
<p>If the model includes multiple moderators, one must specify via argument <code>mod</code> either the position (as a number) or the name (as a string) of the moderator variable to place on the x-axis. The regression line then reflects the &lsquo;marginal&rsquo; relationship between the chosen moderator and the effect sizes or outcomes (i.e., all other moderators except the one being plotted are held constant at their means).
</p>
<p>By default (i.e., when <code>psize</code> is not specified), the size of the points is a function of the square root of the model weights. This way, their area is proportional to the weights. However, the point sizes are rescaled so that the smallest point size is <code>plim[1]</code> and the largest point size is <code>plim[2]</code>. As a result, their relative sizes (i.e., areas) no longer exactly correspond to their relative weights. If exactly relative point sizes are desired, one can set <code>plim[2]</code> to <code>NA</code>, in which case the points are rescaled so that the smallest point size corresponds to <code>plim[1]</code> and all other points are scaled accordingly. As a result, the largest point may be very large. Alternatively, one can set <code>plim[1]</code> to <code>NA</code>, in which case the points are rescaled so that the largest point size corresponds to <code>plim[2]</code> and all other points are scaled accordingly. As a result, the smallest point may be very small. To avoid the latter, one can also set <code>plim[3]</code>, which enforces a minimal point size.
</p>
<p>One can also set <code>psize</code> to a scalar (e.g., <code>psize=1</code>) to avoid that the points are drawn in different sizes. One can also specify the point sizes manually by passing a vector of the appropriate length to <code>psize</code>. Finally, one can also set <code>psize</code> to either <code>"seinv"</code> or <code>"vinv"</code> to make the point sizes proportional to the inverse standard errors or inverse sampling variances.
</p>
<p>With the <code>label</code> argument, one can control whether points in the plot will be labeled. If <code>label="all"</code> (or <code>label=TRUE</code>), all points in the plot will be labeled. If <code>label="ciout"</code> or <code>label="piout"</code>, points falling outside of the confidence/prediction interval will be labeled. Alternatively, one can set this argument to a logical or numeric vector to specify which points should be labeled. The labels are placed above the points when they fall above the regression line and otherwise below. With the <code>offset</code> argument, one can adjust the distance between the labels and the corresponding points. This can either be a single numeric value, which is used as a multiplicative factor for the point sizes (so that the distance between labels and points is larger for larger points) or a numeric vector with two values, where the first is used as an additive factor independent of the point sizes and the second again as a multiplicative factor for the point sizes. The values are given as percentages of the y-axis range. It may take some trial and error to find two values for the <code>offset</code> argument so that the labels are placed right next to the boundary of the points. With <code>labsize</code>, one can control the size of the labels.
</p>
<p>One can also pass an object from <code><a href="#topic+predict.rma">predict</a></code> to the <code>pred</code> argument. This can be useful when the meta-regression model reflects a more complex relationship between the moderator variable and the effect sizes or outcomes (e.g., when using polynomials or splines) or when the model involves interactions. In this case, one also needs to specify the <code>xvals</code> argument. See &lsquo;Examples&rsquo;.
</p>


<h3>Value</h3>

<p>An object of class <code>"regplot"</code> with components:
</p>
<table>
<tr><td><code>slab</code></td>
<td>
<p>the study labels</p>
</td></tr>
<tr><td><code>ids</code></td>
<td>
<p>the study ids</p>
</td></tr>
<tr><td><code>xi</code></td>
<td>
<p>the x-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>yi</code></td>
<td>
<p>the y-axis coordinates of the points that were plotted.</p>
</td></tr>
<tr><td><code>pch</code></td>
<td>
<p>the plotting symbols of the points that were plotted.</p>
</td></tr>
<tr><td><code>psize</code></td>
<td>
<p>the point sizes of the points that were plotted.</p>
</td></tr>
<tr><td><code>col</code></td>
<td>
<p>the colors of the points that were plotted.</p>
</td></tr>
<tr><td><code>bg</code></td>
<td>
<p>the background colors of the points that were plotted.</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>logical vector indicating whether a point was labeled.</p>
</td></tr>
</table>
<p>Note that the object is returned invisibly. Using <code>points.regplot</code>, one can redraw the points (and labels) in case one wants to superimpose the points on top of any elements that were added manually to the plot (see &lsquo;Examples&rsquo;).
</p>


<h3>Note</h3>

<p>For certain types of models, it may not be possible to draw the prediction interval bounds (if this is the case, a warning will be issued).
</p>
<p>Argument <code>slab</code> and when specifying vectors for arguments <code>pch</code>, <code>psize</code>, <code>col</code>, <code>bg</code>, and/or <code>label</code> (for a logical vector), the variables specified are assumed to be of the same length as the data passed to the model fitting function (and if the <code>data</code> argument was used in the original model fit, then the variables will be searched for within this data frame first). Any subsetting and removal of studies with missing values is automatically applied to the variables specified via these arguments.
</p>
<p>If the outcome measure used for creating the plot is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those limits (the observed outcomes and confidence/prediction intervals cannot exceed those bounds then).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Thompson, S. G., &amp; Higgins, J. P. T. (2002). How should meta-regression analyses be undertaken and interpreted? <em>Statistics in Medicine</em>, <b>21</b>(11), 1559&ndash;1573. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1187&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which scatter plots / bubble plots can be drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy BCG vaccine data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

############################################################################

### fit mixed-effects model with absolute latitude as a moderator
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)
res

### draw plot
regplot(res, mod="ablat", xlab="Absolute Latitude")

### adjust x-axis limits and back-transform to risk ratios
regplot(res, mod="ablat", xlab="Absolute Latitude", xlim=c(0,60), transf=exp)

### also extend the prediction limits for the regression line
regplot(res, mod="ablat", xlab="Absolute Latitude", xlim=c(0,60), predlim=c(0,60), transf=exp)

### add the prediction interval to the plot, add a reference line at 1, and add a legend
regplot(res, mod="ablat", pi=TRUE, xlab="Absolute Latitude",
        xlim=c(0,60), predlim=c(0,60), transf=exp, refline=1, legend=TRUE)

### label points outside of the prediction interval
regplot(res, mod="ablat", pi=TRUE, xlab="Absolute Latitude",
        xlim=c(0,60), predlim=c(0,60), transf=exp, refline=1, legend=TRUE,
        label="piout", labsize=0.8)

############################################################################

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)
res

### plot the marginal relationships
regplot(res, mod="ablat", xlab="Absolute Latitude")
regplot(res, mod="year",  xlab="Publication Year")

############################################################################

### fit a quadratic polynomial meta-regression model
res &lt;- rma(yi, vi, mods = ~ ablat + I(ablat^2), data=dat)
res

### compute predicted values using predict()
xs &lt;- seq(0,60,length=601)
tmp &lt;- predict(res, newmods=cbind(xs, xs^2))

### can now pass these results to the 'pred' argument (and have to specify xvals accordingly)
regplot(res, mod="ablat", pred=tmp, xlab="Absolute Latitude", xlim=c(0,60), xvals=xs)

### back-transform to risk ratios and add reference line
regplot(res, mod="ablat", pred=tmp, xlab="Absolute Latitude", xlim=c(0,60), xvals=xs,
        transf=exp, refline=1)

############################################################################

### fit a model with an interaction between a quantitative and a categorical predictor
### (note: just for illustration purposes; this model is too complex for this dataset)
res &lt;- rma(yi, vi, mods = ~ ablat * alloc, data=dat)
res

### draw bubble plot but do not add regression line or CI
tmp &lt;- regplot(res, mod="ablat", xlab="Absolute Latitude", xlim=c(0,60), pred=FALSE, ci=FALSE)

### add regression lines for the three alloc levels
xs &lt;- seq(0, 60, length=100)
preds &lt;- predict(res, newmods=cbind(xs, 0, 0, 0, 0))
lines(xs, preds$pred, lwd=3)
preds &lt;- predict(res, newmods=cbind(xs, 1, 0, xs, 0))
lines(xs, preds$pred, lwd=3)
preds &lt;- predict(res, newmods=cbind(xs, 0, 1, 0, xs))
lines(xs, preds$pred, lwd=3)

### add points back to the plot (so they are on top of the lines)
points(tmp)
</code></pre>

<hr>
<h2 id='regtest'>Regression Test for Funnel Plot Asymmetry</h2><span id='topic+regtest'></span>

<h3>Description</h3>

<p>Function to carry out (various versions of) Egger's regression test for funnel plot asymmetry. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regtest(x, vi, sei, ni, subset, data,
        model="rma", predictor="sei", ret.fit=FALSE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regtest_+3A_x">x</code></td>
<td>
<p>a vector with the observed effect sizes or outcomes or an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="regtest_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="regtest_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="regtest_+3A_ni">ni</code></td>
<td>
<p>optional vector with the corresponding sample sizes (only relevant when using the sample sizes (or a transformation thereof) as predictor).</p>
</td></tr>
<tr><td><code id="regtest_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the test (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="regtest_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="regtest_+3A_model">model</code></td>
<td>
<p>either <code>"rma"</code> or <code>"lm"</code> to indicate the type of model to use for the regression test. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regtest_+3A_predictor">predictor</code></td>
<td>
<p>either <code>"sei"</code> <code>"vi"</code>, <code>"ni"</code>, <code>"ninv"</code>, <code>"sqrtni"</code>, or <code>"sqrtninv"</code> to indicate the predictor to use for the regression test. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="regtest_+3A_ret.fit">ret.fit</code></td>
<td>
<p>logical to specify whether the full results from the fitted model should also be returned.</p>
</td></tr>
<tr><td><code id="regtest_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded.</p>
</td></tr>
<tr><td><code id="regtest_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Various tests for funnel plot asymmetry have been suggested in the literature, including the rank correlation test by Begg and Mazumdar (1994) and the regression test by Egger et al. (1997). Extensions, modifications, and further developments of the regression test are described (among others) by Macaskill, Walter, and Irwig (2001), Sterne and Egger (2005), Harbord, Egger, and Sterne (2006), Peters et al. (2006), R√ºcker et al. (2008), and Moreno et al. (2009). The various versions of the regression test differ in terms of the model (either a weighted regression model with a multiplicative dispersion term or a fixed/mixed-effects meta-regression model is used), in terms of the predictor variable that the observed effect sizes or outcomes are hypothesized to be related to when publication bias is present (suggested predictors include the standard error, the sampling variance, and the sample size or transformations thereof), and in terms of the outcome measure used (e.g., for \(2 \times 2\) table data, one has the choice between various outcome measures). The idea behind the various tests is the same though: If there is a relationship between the observed effect sizes or outcomes and the chosen predictor, then this usually implies asymmetry in the funnel plot, which in turn may be an indication of publication bias.
</p>
<p>The <code>regtest</code> function can be used to carry out various versions of the regression test. One can either pass a vector with the observed effect sizes or outcomes (via <code>x</code>) and the corresponding sampling variances via <code>vi</code> (or the standard errors via <code>sei</code>) to the function or an object of class <code>"rma"</code>.
</p>
<p>The model type for the regression test is chosen via the <code>model</code> argument, with <code>model="lm"</code> for a weighted regression model with a multiplicative dispersion term or <code>model="rma"</code> for a (mixed-effects) meta-regression model (the default).
</p>
<p>The predictor for the test is chosen via the <code>predictor</code> argument:
</p>

<ul>
<li> <p><code>predictor="sei"</code> for the standard errors (the default),
</p>
</li>
<li> <p><code>predictor="vi"</code> for the sampling variances,
</p>
</li>
<li> <p><code>predictor="ni"</code> for the sample sizes,
</p>
</li>
<li> <p><code>predictor="ninv"</code> for the inverse of the sample sizes,
</p>
</li>
<li> <p><code>predictor="sqrtni"</code> for the square root of the sample sizes, or
</p>
</li>
<li> <p><code>predictor="sqrtninv"</code> for the inverse square root of the sample sizes.
</p>
</li></ul>

<p>The outcome measure used for the regression test is simply determined by the values passed to the function or the measure that was used in fitting the original model (when passing an object of class <code>"rma"</code> to the function).
</p>
<p>When using the sample sizes (or a transformation thereof) as the predictor, one can use the <code>ni</code> argument to specify the sample sizes. When <code>x</code> is a vector with the observed effect sizes or outcomes and it was computed with <code><a href="#topic+escalc">escalc</a></code>, then the sample sizes should automatically be stored as an attribute of <code>x</code> and <code>ni</code> does not need to be specified. This should also be the case when passing an object of class <code>"rma"</code> to the function and the input to the model fitting function came from <code><a href="#topic+escalc">escalc</a></code>.
</p>
<p>When passing an object of class <code>"rma"</code> to the function, arguments such as <code>method</code>, <code>weighted</code>, and <code>test</code> as used during the initial model fitting are also used for the regression test. If the model already included one or more moderators, then <code>regtest</code> will add the chosen predictor to the moderator(s) already included in the model. This way, one can test for funnel plot asymmetry after accounting first for the influence of the moderator(s) already included in the model.
</p>
<p>The model used for conducting the regression test can also be used to obtain a &lsquo;limit estimate&rsquo; of the (average) true effect or outcome. In particular, when the standard errors, sampling variances, or inverse (square root) sample sizes are used as the predictor, the model intercept in essence reflects the estimate under infinite precision. This is sometimes (cautiously) interpreted as an estimate of the (average) true effect or outcome that is adjusted for publication bias.
</p>


<h3>Value</h3>

<p>An object of class <code>"regtest"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>the model used for the regression test.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>the predictor used for the regression test.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>the corresponding p-value</p>
</td></tr>
<tr><td><code>dfs</code></td>
<td>
<p>the degrees of freedom of the test statistic (if the test is based on a t-distribution).</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>the full results from the fitted model.</p>
</td></tr>
<tr><td><code>est</code></td>
<td>
<p>the limit estimate (only for predictors <code>"sei"</code> <code>"vi"</code>, <code>"ninv"</code>, or <code>"sqrtninv"</code> and when the model does not contain any additional moderators; <code>NULL</code> otherwise).</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence interval for the limit estimate.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the limit estimate.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.regtest">print</a></code> function.
</p>


<h3>Note</h3>

<p>The classical &lsquo;Egger test&rsquo; is obtained by setting <code>model="lm"</code> and <code>predictor="sei"</code>. For the random/mixed-effects version of the test, set <code>model="rma"</code> (this is the default). See Sterne and Egger (2005) for details on these two types of models/tests.
</p>
<p>When conducting a classical &lsquo;Egger test&rsquo;, the test of the limit estimate is the same as the &lsquo;precision-effect test&rsquo; (PET) of Stanley and Doucouliagos (2014). The limit estimate when using the sampling variance as predictor is sometimes called the &lsquo;precision-effect estimate with SE&rsquo; (PEESE) (Stanley &amp; Doucouliagos, 2014). A conditional procedure where we use the limit estimate when PET is not significant (i.e., when using the standard error as predictor) and the PEESE (i.e., when using the sampling variance as predictor) when PET is significant is sometimes called the PET-PEESE procedure (Stanley &amp; Doucouliagos, 2014).
</p>
<p>All of the tests do not directly test for publication bias, but for a relationship between the observed effect sizes or outcomes and the chosen predictor. If such a relationship is present, then this usually implies asymmetry in the funnel plot, which in turn may be an indication of publication bias. However, it is important to keep in mind that there can be other reasons besides publication bias that could lead to asymmetry in the funnel plot.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Begg, C. B., &amp; Mazumdar, M. (1994). Operating characteristics of a rank correlation test for publication bias. <em>Biometrics</em>, <b>50</b>(4), 1088&ndash;1101. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2533446&#8288;</code>
</p>
<p>Egger, M., Davey Smith, G., Schneider, M., &amp; Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. <em>British Medical Journal</em>, <b>315</b>(7109), 629&ndash;634. <code style="white-space: pre;">&#8288;https://doi.org/10.1136/bmj.315.7109.629 &#8288;</code>
</p>
<p>Harbord, R. M., Egger, M., &amp; Sterne, J. A. C. (2006). A modified test for small-study effects in meta-analyses of controlled trials with binary endpoints. <em>Statistics in Medicine</em>, <b>25</b>(20), 3443&ndash;3457. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2380&#8288;</code>
</p>
<p>Macaskill, P., Walter, S. D., &amp; Irwig, L. (2001). A comparison of methods to detect publication bias in meta-analysis. <em>Statistics in Medicine</em>, <b>20</b>(4), 641&ndash;654. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.698&#8288;</code>
</p>
<p>Moreno, S. G., Sutton, A. J., Ades, A. E., Stanley, T. D., Abrams, K. R., Peters, J. L., &amp; Cooper, N. J. (2009). Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study. <em>BMC Medical Research Methodology</em>, <b>9</b>, 2. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/1471-2288-9-2&#8288;</code>
</p>
<p>Peters, J. L., Sutton, A. J., Jones, D. R., Abrams, K. R., &amp; Rushton, L. (2006). Comparison of two methods to detect publication bias in meta-analysis. <em>Journal of the American Medical Association</em>, <b>295</b>(6), 676&ndash;680. <code style="white-space: pre;">&#8288;https://doi.org/10.1001/jama.295.6.676&#8288;</code>
</p>
<p>R√ºcker, G., Schwarzer, G., &amp; Carpenter, J. (2008). Arcsine test for publication bias in meta-analyses with binary outcomes. <em>Statistics in Medicine</em>, <b>27</b>(5), 746&ndash;763. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2971&#8288;</code>
</p>
<p>Stanley, T. D., &amp; Doucouliagos, H. (2014). Meta-regression approximations to reduce publication selection bias. <em>Research Synthesis Methods</em>, <b>5</b>(1), 60&ndash;78. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1095&#8288;</code>
</p>
<p>Sterne, J. A. C., &amp; Egger, M. (2005). Regression methods to detect publication and other bias in meta-analysis. In H. R. Rothstein, A. J. Sutton, &amp; M. Borenstein (Eds.) <em>Publication bias in meta-analysis: Prevention, assessment, and adjustments</em> (pp. 99&ndash;110). Chichester, England: Wiley.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ranktest">ranktest</a></code> for the rank correlation test, <code><a href="#topic+trimfill">trimfill</a></code> for the trim and fill method, <code><a href="#topic+tes">tes</a></code> for the test of excess significance, <code><a href="#topic+fsn">fsn</a></code> to compute the fail-safe N (file drawer analysis), and <code><a href="#topic+selmodel">selmodel</a></code> for selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data into 'dat' and examine data
dat &lt;- dat.egger2001

### calculate log odds ratios and corresponding sampling variances (but remove ISIS-4 trial)
dat &lt;- escalc(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat, subset=-16)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)
res

### classical Egger test
regtest(res, model="lm")

### mixed-effects meta-regression version of the Egger test
regtest(res)

### same tests, but passing outcomes directly
regtest(yi, vi, data=dat, model="lm")
regtest(yi, vi, data=dat)

### if dat$yi is computed with escalc(), sample size information is stored in attributes
dat$yi

### then this will also work
regtest(yi, vi, data=dat, predictor="ni")

### similarly when passing a model object to the function
regtest(res, model="lm", predictor="ni")
regtest(res, model="lm", predictor="ninv")
regtest(res, predictor="ni")
regtest(res, predictor="ninv")

### otherwise have to supply sample sizes manually
dat$yi &lt;- c(dat$yi) # this removes the 'ni' attribute from 'yi'
dat$nitotal &lt;- with(dat, n1i + n2i)
regtest(yi, vi, ni=nitotal, data=dat, predictor="ni")
res &lt;- rma(yi, vi, data=dat)
regtest(res, predictor="ni", ni=nitotal, data=dat)

### standard funnel plot (with standard errors on the y-axis)
funnel(res, refline=0)

### regression test (by default the standard errors are used as predictor)
reg &lt;- regtest(res)
reg

### add regression line to funnel plot
se &lt;- seq(0,1.8,length=100)
lines(coef(reg$fit)[1] + coef(reg$fit)[2]*se, se, lwd=3)

### regression test (using the sampling variances as predictor)
reg &lt;- regtest(res, predictor="vi")

### add regression line to funnel plot (using the sampling variances as predictor)
lines(coef(reg$fit)[1] + coef(reg$fit)[2]*se^2, se, lwd=3, lty="dotted")

### add legend
legend("bottomright", inset=0.02, lty=c("solid","dotted"), lwd=3, cex=0.9, bg="white",
       legend=c("Standard Errors as Predictor",
                "Sampling Variances as Predictor"))

### testing for asymmetry after accounting for the influence of a moderator
res &lt;- rma(yi, vi, mods = ~ year, data=dat)
regtest(res, model="lm")
regtest(res)
</code></pre>

<hr>
<h2 id='replmiss'>Replace Missing Values in a Vector</h2><span id='topic+replmiss'></span>

<h3>Description</h3>

<p>Function to replace missing (<code>NA</code>) values in a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replmiss(x, y, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="replmiss_+3A_x">x</code></td>
<td>
<p>vector that may include one or more missing values.</p>
</td></tr>
<tr><td><code id="replmiss_+3A_y">y</code></td>
<td>
<p>either a scalar or a vector of the same length as <code>x</code> with the value(s) to replace missing values with.</p>
</td></tr>
<tr><td><code id="replmiss_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector <code>x</code> with the missing values replaced based on the scalar or vector <code>y</code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(4,2,7,NA,1,NA,5)
x &lt;- replmiss(x,0)
x

x &lt;- c(4,2,7,NA,1,NA,5)
y &lt;- c(2,3,6,5,8,1,2)
x &lt;- replmiss(x,y)
x
</code></pre>

<hr>
<h2 id='reporter'>Dynamically Generated Analysis Reports for 'rma.uni' Objects</h2><span id='topic+reporter'></span><span id='topic+reporter.rma.uni'></span>

<h3>Description</h3>

<p>Function to dynamically generate an analysis report for objects of class <code>"rma.uni"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reporter(x, ...)

## S3 method for class 'rma.uni'
reporter(x, dir, filename, format="html_document", open=TRUE,
         digits, forest, funnel, footnotes=FALSE, verbose=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reporter_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="reporter_+3A_dir">dir</code></td>
<td>
<p>optional character string to specify the directory for creating the report. If unspecified, <code><a href="base.html#topic+tempdir">tempdir</a></code> will be used.</p>
</td></tr>
<tr><td><code id="reporter_+3A_filename">filename</code></td>
<td>
<p>optional character string to specify the filename (without file extension) for the report. If unspecified, the function sets a filename automatically.</p>
</td></tr>
<tr><td><code id="reporter_+3A_format">format</code></td>
<td>
<p>output format for the report (either <code>html_document</code>, <code>pdf_document</code>, or <code>word_document</code>). Can be abbreviated. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="reporter_+3A_open">open</code></td>
<td>
<p>logical to specify whether the report should be opened after it has been generated (the default is <code>TRUE</code>). See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="reporter_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="reporter_+3A_forest">forest</code></td>
<td>
<p>either a logical which will suppress the drawing of the forest plot when set to <code>FALSE</code> or a character string with arguments to be added to the call to <code><a href="#topic+forest.rma">forest</a></code> for generating the forest plot.</p>
</td></tr>
<tr><td><code id="reporter_+3A_funnel">funnel</code></td>
<td>
<p>either a logical which will suppress the drawing of the funnel plot when set to <code>FALSE</code> or a character string with arguments to be added to the call to <code><a href="#topic+funnel.rma">funnel</a></code> for generating the funnel plot.</p>
</td></tr>
<tr><td><code id="reporter_+3A_footnotes">footnotes</code></td>
<td>
<p>logical to specify whether additional explanatory footnotes should be added to the report (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="reporter_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether information on the progress of the report generation should be provided (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="reporter_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function dynamically generates an analysis report based on the model object. The report includes information about the model that was fitted, the distribution of the observed effect sizes or outcomes, the estimate of the average outcome based on the fitted model, tests and statistics that are informative about potential (residual) heterogeneity in the outcomes, checks for outliers and/or influential studies, and tests for funnel plot asymmetry. By default, a forest plot and a funnel plot are also provided (these can be suppressed by setting <code>forest=FALSE</code> and/or <code>funnel=FALSE</code>).
</p>


<h3>Value</h3>

<p>The function generates either a html, pdf, or docx file and returns (invisibly) the path to the generated document.
</p>


<h3>Note</h3>

<p>Since the report is created based on an R Markdown document that is generated by the function, the <a href="https://cran.r-project.org/package=rmarkdown">rmarkdown</a> package and <a href="https://pandoc.org">pandoc</a> must be installed.
</p>
<p>To render the report into a pdf document (i.e., using <code>format="pdf_document"</code>) requires a LaTeX installation. If LaTeX is not already installed, you could try using the <a href="https://cran.r-project.org/package=tinytex">tinytex</a> package to install a lightweight LaTeX distribution based on TeX Live.
</p>
<p>Once the report is generated, the function opens the output file (either a .html, .pdf, or .docx file) with an appropriate application (if <code>open=TRUE</code>). This will only work when an appropriate application for the file type is installed and associated with the extension.
</p>
<p>If <code>filename</code> is unspecified, the default is to use <code>report</code>, followed by an underscore (i.e., <code>_</code>) and the name of the object passed to the function. Both the R Markdown file (with extension .rmd) and the actual report (with extension .html, .pdf, or .docx) are named accordingly. To generate the report, the model object is also saved to a file (with the same filename as above, but with extension .rdata). Also, files <code>references.bib</code> and <code>apa.csl</code> are copied to the same directory (these files are needed to generate the references in APA format).
</p>
<p>Since the report is put together based on predefined text blocks, the writing is not very elegant. Also, using personal pronouns (&lsquo;I&rsquo; or &lsquo;we&rsquo;) does not make sense for such a report, so a lot of passive voice is used.
</p>
<p>The generated report provides an illustration of how the results of the model can be reported, but is not a substitute for a careful examination of the results.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit models for which an analysis report can be generated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy BCG vaccine data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat,
              slab=paste(author, ", ", year, sep=""))

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

## Not run: 
### generate report
reporter(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='residuals.rma'>Residual Values based on 'rma' Objects</h2><span id='topic+residuals'></span><span id='topic+rstandard'></span><span id='topic+rstudent'></span><span id='topic+residuals.rma'></span><span id='topic+rstandard.rma.uni'></span><span id='topic+rstandard.rma.mh'></span><span id='topic+rstandard.rma.mv'></span><span id='topic+rstandard.rma.peto'></span><span id='topic+rstudent.rma.uni'></span><span id='topic+rstudent.rma.mh'></span><span id='topic+rstudent.rma.mv'></span><span id='topic+rstudent.rma.peto'></span>

<h3>Description</h3>

<p>Functions to compute residuals and standardized versions thereof for models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> functions. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
residuals(object, type="response", ...)

## S3 method for class 'rma.uni'
rstandard(model, digits, type="marginal", ...)
## S3 method for class 'rma.mh'
rstandard(model, digits, ...)
## S3 method for class 'rma.peto'
rstandard(model, digits, ...)
## S3 method for class 'rma.mv'
rstandard(model, digits, cluster, ...)

## S3 method for class 'rma.uni'
rstudent(model, digits, progbar=FALSE, ...)
## S3 method for class 'rma.mh'
rstudent(model, digits, progbar=FALSE, ...)
## S3 method for class 'rma.peto'
rstudent(model, digits, progbar=FALSE, ...)
## S3 method for class 'rma.mv'
rstudent(model, digits, progbar=FALSE, cluster,
         reestimate=TRUE, parallel="no", ncpus=1, cl, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code> (for <code>residuals</code>).</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_type">type</code></td>
<td>
<p>the type of residuals which should be returned. For <code>residuals</code>, the alternatives are: <code>"response"</code> (default), <code>"rstandard"</code>, <code>"rstudent"</code>, and <code>"pearson"</code>. For <code>rstandard.rma.uni</code>, the alternatives are: <code>"marginal"</code> (default) and <code>"conditional"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_model">model</code></td>
<td>
<p>an object of class <code>"rma"</code> (for <code>residuals</code>) or an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, or <code>"rma.mv"</code> (for <code>rstandard</code> and <code>rstudent</code>).</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_cluster">cluster</code></td>
<td>
<p>optional vector to specify a clustering variable to use for computing cluster-level multivariate standardized residuals (only for <code>"rma.mv"</code> objects).</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_reestimate">reestimate</code></td>
<td>
<p>logical to specify whether variance/correlation components should be re-estimated after deletion of the \(i\textrm{th}\) case when computing externally standardized residuals for <code>"rma.mv"</code> objects (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_parallel">parallel</code></td>
<td>
<p>character string to specify whether parallel processing should be used (the default is <code>"no"</code>). For parallel processing, set to either <code>"snow"</code> or <code>"multicore"</code>. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_ncpus">ncpus</code></td>
<td>
<p>integer to specify the number of processes to use in the parallel processing.</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_cl">cl</code></td>
<td>
<p>optional cluster to use if <code>parallel="snow"</code>. If unspecified, a cluster on the local machine is created for the duration of the call.</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (only for <code>rstudent</code>) (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="residuals.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The observed residuals (obtained with <code>residuals</code>) are simply equal to the &lsquo;observed - fitted&rsquo; values. These can be obtained with <code>residuals(object)</code> (using the default <code>type="response"</code>).
</p>
<p>Dividing the observed residuals by the model-implied standard errors of the observed effect sizes or outcomes yields Pearson (or semi-standardized) residuals. These can be obtained with <code>residuals(object, type="pearson")</code>.
</p>
<p>Dividing the observed residuals by their corresponding standard errors yields (internally) standardized residuals. These can be obtained with <code>rstandard(model)</code> or <code>residuals(object, type="rstandard")</code>.
</p>
<p>With <code>rstudent(model)</code> (or <code>residuals(object, type="rstudent")</code>), one can obtain the externally standardized residuals (also called standardized deleted residuals or (externally) studentized residuals). The externally standardized residual for the \(i\textrm{th}\) case is obtained by deleting the \(i\textrm{th}\) case from the dataset, fitting the model based on the remaining cases, calculating the predicted value for the \(i\textrm{th}\) case based on the fitted model, taking the difference between the observed and the predicted value for the \(i\textrm{th}\) case (which yields the deleted residual), and then standardizing the deleted residual based on its standard error.
</p>
<p>If a particular case fits the model, its standardized residual follows (asymptotically) a standard normal distribution. A large standardized residual for a case therefore may suggest that the case does not fit the assumed model (i.e., it may be an outlier).
</p>
<p>For <code>"rma.uni"</code> objects, <code>rstandard(model, type="conditional")</code> computes conditional residuals, which are the deviations of the observed effect sizes or outcomes from the best linear unbiased predictions (BLUPs) of the study-specific true effect sizes or outcomes (see <code><a href="#topic+blup.rma.uni">blup</a></code>).
</p>
<p>For <code>"rma.mv"</code> objects, one can specify a clustering variable (via the <code>cluster</code> argument). If specified, <code>rstandard(model)</code> and <code>rstudent(model)</code> also compute cluster-level multivariate (internally or externally) standardized residuals. If all outcomes within a cluster fit the model, then the multivariate standardized residual for the cluster follows (asymptotically) a chi-square distribution with \(k_i\) degrees of freedom (where \(k_i\) denotes the number of outcomes within the cluster).
</p>
<p>See also <code><a href="#topic+influence.rma.uni">influence.rma.uni</a></code> and <code><a href="#topic+influence.rma.mv">influence.rma.mv</a></code> for other leave-one-out diagnostics that are useful for detecting influential cases in models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> functions.
</p>


<h3>Value</h3>

<p>Either a vector with the residuals of the requested type (for <code>residuals</code>) or an object of class <code>"list.rma"</code>, which is a list containing the following components:
</p>
<table>
<tr><td><code>resid</code></td>
<td>
<p>observed residuals (for <code>rstandard</code>) or deleted residuals (for <code>rstudent</code>).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>corresponding standard errors.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>standardized residuals (internally standardized for <code>rstandard</code> or externally standardized for <code>rstudent</code>).</p>
</td></tr>
</table>
<p>When a clustering variable is specified for <code>"rma.mv"</code> objects, the returned object is a list with the first element (named <code>obs</code>) as described above and a second element (named <code>cluster</code> of class <code>"list.rma"</code> with:
</p>
<table>
<tr><td><code>X2</code></td>
<td>
<p>cluster-level multivariate standardized residuals.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of observed effect sizes or outcomes within the clusters.</p>
</td></tr>
</table>
<p>The object is formatted and printed with <code><a href="#topic+print.list.rma">print</a></code>. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.list.rma">as.data.frame</a></code> function.
</p>


<h3>Note</h3>

<p>The externally standardized residuals (obtained with <code>rstudent</code>) are calculated by refitting the model \(k\) times (where \(k\) denotes the number of cases). Depending on how large \(k\) is, it may take a few moments to finish the calculations. For complex models fitted with <code><a href="#topic+rma.mv">rma.mv</a></code>, this can become computationally expensive.
</p>
<p>On machines with multiple cores, one can try to speed things up by delegating the model fitting to separate worker processes, that is, by setting <code>parallel="snow"</code> or <code>parallel="multicore"</code> and <code>ncpus</code> to some value larger than 1 (only for objects of class <code>"rma.mv"</code>). Parallel processing makes use of the <code><a href="parallel.html#topic+parallel">parallel</a></code> package, using the <code><a href="parallel.html#topic+makePSOCKcluster">makePSOCKcluster</a></code> and <code><a href="parallel.html#topic+parLapply">parLapply</a></code> functions when <code>parallel="snow"</code> or using <code><a href="parallel.html#topic+mclapply">mclapply</a></code> when <code>parallel="multicore"</code> (the latter only works on Unix/Linux-alikes). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
<p>Alternatively (or in addition to using parallel processing), one can also set <code>reestimate=FALSE</code>, in which case any variance/correlation components in the model are not re-estimated after deleting the \(i\textrm{th}\) case from the dataset. Doing so only yields an approximation to the externally standardized residuals (and the cluster-level multivariate standardized residuals) that ignores the influence of the \(i\textrm{th}\) case on the variance/correlation components, but is considerably faster (and often yields similar results).
</p>
<p>It may not be possible to fit the model after deletion of the \(i\textrm{th}\) case from the dataset. This will result in <code>NA</code> values for that case when calling <code>rstudent</code>.
</p>
<p>Also, for <code>"rma.mv"</code> objects with a clustering variable specified, it may not be possible to compute the cluster-level multivariate standardized residual for a particular cluster (if the var-cov matrix of the residuals within a cluster is not of full rank). This will result in <code>NA</code> for that cluster.
</p>
<p>The variable specified via <code>cluster</code> is assumed to be of the same length as the data originally passed to the <code>rma.mv</code> function (and if the <code>data</code> argument was used in the original model fit, then the variable will be searched for within this data frame first). Any subsetting and removal of studies with missing values that was applied during the model fitting is also automatically applied to the variable specified via the <code>cluster</code> argument.
</p>
<p>For objects of class <code>"rma.mh"</code> and <code>"rma.peto"</code>, <code>rstandard</code> actually computes Pearson (or semi-standardized) residuals.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical methods for meta-analysis</em>. San Diego, CA: Academic Press.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; Cheung, M. W.-L. (2010). Outlier and influence diagnostics for meta-analysis. <em>Research Synthesis Methods</em>, <b>1</b>(2), 112&ndash;125. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.11&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which the various types of residuals can be computed.
</p>
<p><code><a href="#topic+influence.rma.uni">influence.rma.uni</a></code> and <code><a href="#topic+influence.rma.mv">influence.rma.mv</a></code> for other model diagnostics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### compute the studentized residuals
rstudent(res)

### fit mixed-effects model with absolute latitude as moderator
res &lt;- rma(yi, vi, mods = ~ ablat, data=dat)

### compute the studentized residuals
rstudent(res)
</code></pre>

<hr>
<h2 id='rma.glmm'>Meta-Analysis via Generalized Linear (Mixed-Effects) Models</h2><span id='topic+rma.glmm'></span>

<h3>Description</h3>

<p>Function to fit meta-analytic equal-, fixed-, and random-effects models and (mixed-effects) meta-regression models using a generalized linear (mixed-effects) model framework. See below and the introduction to the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> for more details on these models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rma.glmm(ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i, xi, mi, ti, ni,
         mods, measure, intercept=TRUE, data, slab, subset,
         add=1/2, to="only0", drop00=TRUE, vtype="LS",
         model="UM.FS", method="ML", coding=1/2, cor=FALSE, test="z",
         level=95, btt, nAGQ=7, verbose=FALSE, digits, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rma.glmm_+3A_ai">ai</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_bi">bi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_ci">ci</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_di">di</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_n1i">n1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_n2i">n2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_x1i">x1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_x2i">x2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_t1i">t1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_t2i">t2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_xi">xi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_mi">mi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_ti">ti</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_ni">ni</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_mods">mods</code></td>
<td>
<p>optional argument to include one or more moderators in the model. A single moderator can be given as a vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving a matrix with \(k\) rows and as many columns as there are moderator variables. Alternatively, a model <code><a href="#topic+formula">formula</a></code> can be used to specify the model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_measure">measure</code></td>
<td>
<p>character string to specify the outcome measure to use for the meta-analysis. Possible options are <code>"OR"</code> for the (log transformed) odds ratio, <code>"IRR"</code> for the (log transformed) incidence rate ratio, <code>"PLO"</code> for the (logit transformed) proportion, or <code>"IRLN"</code> for the (log transformed) incidence rate.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_intercept">intercept</code></td>
<td>
<p>logical to specify whether an intercept should be added to the model (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_data">data</code></td>
<td>
<p>optional data frame containing the data supplied to the function.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be used for the analysis.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_add">add</code></td>
<td>
<p>non-negative number to specify the amount to add to zero cells, counts, or frequencies when calculating the observed effect sizes or outcomes of the individual studies. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_to">to</code></td>
<td>
<p>character string to specify when the values under <code>add</code> should be added (either <code>"only0"</code>, <code>"all"</code>, <code>"if0all"</code>, or <code>"none"</code>). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_drop00">drop00</code></td>
<td>
<p>logical to specify whether studies with no cases/events (or only cases) in both groups should be dropped. See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_vtype">vtype</code></td>
<td>
<p>character string to specify the type of sampling variances to calculate when calculating the observed effect sizes or outcomes. See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_model">model</code></td>
<td>
<p>character string to specify the general model type for the analysis. Either <code>"UM.FS"</code> (the default), <code>"UM.RS"</code>, <code>"CM.EL"</code>, or <code>"CM.AL"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_method">method</code></td>
<td>
<p>character string to specify whether an equal- or a random-effects model should be fitted. An equal-effects model is fitted when using <code>method="EE"</code>. A random-effects model is fitted by setting <code>method="ML"</code> (the default). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_coding">coding</code></td>
<td>
<p>numeric scalar to indicate how the group variable should be coded in the random effects structure for random/mixed-effects models (the default is <code>1/2</code>). See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_cor">cor</code></td>
<td>
<p>logical to indicate whether the random study effects should be allowed to be correlated with the random group effects for random/mixed-effects models when <code>model="UM.RS"</code> (the default is <code>FALSE</code>). See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_test">test</code></td>
<td>
<p>character string to specify how test statistics and confidence intervals for the fixed effects should be computed. By default (<code>test="z"</code>), Wald-type tests and CIs are obtained, which are based on a standard normal distribution. When <code>test="t"</code>, a t-distribution is used instead. See &lsquo;Details&rsquo; and also <a href="#topic+misc-recs">here</a> for some recommended practices.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_btt">btt</code></td>
<td>
<p>optional vector of indices to specify which coefficients to include in the omnibus test of moderators. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_nagq">nAGQ</code></td>
<td>
<p>positive integer to specify the number of points per axis for evaluating the adaptive Gauss-Hermite approximation to the log-likelihood. The default is 7. Setting this to 1 corresponds to the Laplacian approximation. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>). Can also be an integer. Values &gt; 1 generate more verbose output. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_control">control</code></td>
<td>
<p>optional list of control values for the estimation algorithms. If unspecified, default values are defined inside the function. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.glmm_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specifying the Data</h4>

<p>The function can be used in combination with the following effect sizes or outcome measures:
</p>

<ul>
<li> <p><code>measure="OR"</code> for (log transformed) odds ratios,
</p>
</li>
<li> <p><code>measure="IRR"</code> for (log transformed) incidence rate ratios,
</p>
</li>
<li> <p><code>measure="PLO"</code> for (logit transformed) proportions (i.e., log odds),
</p>
</li>
<li> <p><code>measure="IRLN"</code> for (log transformed) incidence rates.
</p>
</li></ul>

<p>The <code><a href="#topic+escalc">escalc</a></code> function describes the data/arguments that should be specified/used for these measures.
</p>



<h4>Specifying the Model</h4>

<p>A variety of model types are available when analyzing \(2 \times 2\) table data (i.e., when <code>measure="OR"</code>) or two-group event count data (i.e., when <code>measure="IRR"</code>):
</p>

<ul>
<li> <p><code>model="UM.FS"</code> for an unconditional generalized linear mixed-effects model with fixed study effects,
</p>
</li>
<li> <p><code>model="UM.RS"</code> for an unconditional generalized linear mixed-effects model with random study effects,
</p>
</li>
<li> <p><code>model="CM.AL"</code> for a conditional generalized linear mixed-effects model (approximate likelihood),
</p>
</li>
<li> <p><code>model="CM.EL"</code> for a conditional generalized linear mixed-effects model (exact likelihood).
</p>
</li></ul>

<p>For <code>measure="OR"</code>, models <code>"UM.FS"</code> and <code>"UM.RS"</code> are essentially (mixed-effects) logistic regression models, while for <code>measure="IRR"</code>, these models are (mixed-effects) Poisson regression models. The difference between <code>"UM.FS"</code> and <code>"UM.RS"</code> is how study level variability (i.e., differences in outcomes across studies irrespective of group membership) is modeled. One can choose between using fixed study effects (which means that \(k\) dummy variables are added to the model) or random study effects (which means that random effects corresponding to the levels of the study factor are added to the model).
</p>
<p>The conditional model (<code>model="CM.EL"</code>) avoids having to model study level variability by conditioning on the total numbers of cases/events in each study. For <code>measure="OR"</code>, this leads to a non-central hypergeometric distribution for the data within each study and the corresponding model is then a (mixed-effects) conditional logistic model. Fitting this model can be difficult and computationally expensive. When the number of cases in each study is small relative to the group sizes, one can approximate the exact likelihood by a binomial distribution, which leads to a regular (mixed-effects) logistic regression model (<code>model="CM.AL"</code>). For <code>measure="IRR"</code>, the conditional model leads directly to a binomial distribution for the data within each study and the resulting model is again a (mixed-effects) logistic regression model (no approximate likelihood model is needed here).
</p>
<p>When analyzing proportions (i.e., <code>measure="PLO"</code>) or incidence rates (i.e., <code>measure="IRLN"</code>) of individual groups, the model type is always a (mixed-effects) logistic or Poisson regression model, respectively (i.e., the <code>model</code> argument is not relevant here).
</p>
<p>Aside from choosing the general model type, one has to decide whether to fit an equal- or a random-effects model to the data. An <em>equal-effects model</em> is fitted by setting <code>method="EE"</code>. A <em>random-effects model</em> is fitted by setting <code>method="ML"</code> (the default). Note that random-effects models with dichotomous data are often referred to as &lsquo;binomial-normal&rsquo; models in the meta-analytic literature. Analogously, for event count data, such models could be referred to as &lsquo;Poisson-normal&rsquo; models.
</p>
<p>One or more moderators can be included in a model via the <code>mods</code> argument. A single moderator can be given as a (row or column) vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving an appropriate model matrix (i.e., \(X\)) with \(k\) rows and as many columns as there are moderator variables (e.g., <code>mods = cbind(mod1, mod2, mod3)</code>, where <code>mod1</code>, <code>mod2</code>, and <code>mod3</code> correspond to the names of the variables for three moderator variables). The intercept is added to the model matrix by default unless <code>intercept=FALSE</code>.
</p>
<p>Alternatively, one can use standard <code><a href="#topic+formula">formula</a></code> syntax to specify the model. In this case, the <code>mods</code> argument should be set equal to a one-sided formula of the form <code>mods = ~ model</code> (e.g., <code>mods = ~ mod1 + mod2 + mod3</code>). Interactions, polynomial terms, and factors can be easily added to the model in this manner. When specifying a model formula via the <code>mods</code> argument, the <code>intercept</code> argument is ignored. Instead, the inclusion/exclusion of the intercept is controlled by the specified formula (e.g., <code>mods = ~ mod1 + mod2 + mod3 - 1</code> would lead to the removal of the intercept).
</p>



<h4>Equal-, Saturated-, and Random/Mixed-Effects Models</h4>

<p>When fitting a particular model, actually up to three different models are fitted within the function:
</p>

<ul>
<li><p> the equal-effects model (i.e., where \(\tau^2\) is set to 0),
</p>
</li>
<li><p> the saturated model (i.e., the model with a deviance of 0), and
</p>
</li>
<li><p> the random/mixed-effects model (i.e., where \(\tau^2\) is estimated) (only if <code>method="ML"</code>).
</p>
</li></ul>

<p>The saturated model is obtained by adding as many dummy variables to the model as needed so that the model deviance is equal to zero. Even when <code>method="ML"</code>, the equal- and saturated models are also fitted, as they are used to compute the test statistics for the Wald-type and likelihood ratio tests for (residual) heterogeneity (see below).
</p>



<h4>Omnibus Test of Moderators</h4>

<p>For models including moderators, an omnibus test of all model coefficients is conducted that excludes the intercept (the first coefficient) if it is included in the model. If no intercept is included in the model, then the omnibus test includes all of the coefficients in the model including the first. Alternatively, one can manually specify the indices of the coefficients to test via the <code>btt</code> (&lsquo;betas to test&rsquo;) argument (i.e., to test \(\mbox{H}_0{:}\; \beta_{j \in \texttt{btt}} = 0\), where \(\beta_{j \in \texttt{btt}}\) is the set of coefficients to be tested). For example, with <code>btt=c(3,4)</code>, only the third and fourth coefficients from the model are included in the test (if an intercept is included in the model, then it corresponds to the first coefficient in the model). Instead of specifying the coefficient numbers, one can specify a string for <code>btt</code>. In that case, <code><a href="base.html#topic+grep">grep</a></code> will be used to search for all coefficient names that match the string. The omnibus test is called the \(Q_M\)-test and follows asymptotically a chi-square distribution with \(m\) degrees of freedom (with \(m\) denoting the number of coefficients tested) under the null hypothesis (that the true value of all coefficients tested is equal to 0).
</p>



<h4>Categorical Moderators</h4>

<p>Categorical moderator variables can be included in the model via the <code>mods</code> argument in the same way that appropriately (dummy) coded categorical variables can be included in linear models. One can either do the dummy coding manually or use a model formula together with the <code><a href="base.html#topic+factor">factor</a></code> function to automate the coding (note that string/character variables in a model formula are automatically converted to factors).
</p>



<h4>Tests and Confidence Intervals</h4>

<p>By default, tests of individual coefficients in the model (and the corresponding confidence intervals) are based on a standard normal distribution, while the omnibus test is based on a chi-square distribution (see above). As an alternative, one can set <code>test="t"</code>, in which case tests of individual coefficients and confidence intervals are based on a t-distribution with \(k-p\) degrees of freedom, while the omnibus test then uses an F-distribution with \(m\) and \(k-p\) degrees of freedom (with \(k\) denoting the total number of estimates included in the analysis and \(p\) the total number of model coefficients including the intercept if it is present). Note that <code>test="t"</code> is not the same as <code>test="knha"</code> in <code><a href="#topic+rma.uni">rma.uni</a></code>, as no adjustment to the standard errors of the estimated coefficients is made.
</p>



<h4>Tests for (Residual) Heterogeneity</h4>

<p>Two different tests for (residual) heterogeneity are automatically carried out by the function. The first is a Wald-type test, which tests the coefficients corresponding to the dummy variables added in the saturated model for significance. The second is a likelihood ratio test, which tests the same set of coefficients, but does so by computing \(-2\) times the difference in the log-likelihoods of the equal-effects and the saturated models. These two tests are not identical for the types of models fitted by the <code>rma.glmm</code> function and may even lead to conflicting conclusions.
</p>



<h4>Observed Effect Sizes or Outcomes of the Individual Studies</h4>

<p>The various models do not require the calculation of the observed effect sizes or outcomes of the individual studies (e.g., the observed log odds ratios of the \(k\) studies) and directly make use of the cell/event counts. Zero cells/events are not a problem (except in extreme cases, such as when one of the two outcomes never occurs or when there are no events in any of the studies). Therefore, it is unnecessary to add some constant to the cell/event counts when there are zero cells/events.
</p>
<p>However, for plotting and various other functions, it is necessary to calculate the observed effect sizes or outcomes for the \(k\) studies. Here, zero cells/events can be problematic, so adding a constant value to the cell/event counts ensures that all \(k\) values can be calculated. The <code>add</code> and <code>to</code> arguments are used to specify what value should be added to the cell/event counts and under what circumstances when calculating the observed effect sizes or outcomes. The documentation of the <code><a href="#topic+escalc">escalc</a></code> function explains how the <code>add</code> and <code>to</code> arguments work. Note that <code>drop00</code> is set to <code>TRUE</code> by default, since studies where <code>ai=ci=0</code> or <code>bi=di=0</code> or studies where <code>x1i=x2i=0</code> are uninformative about the size of the effect.
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.glmm","rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated coefficients of the model.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated amount of (residual) heterogeneity. Always <code>0</code> when <code>method="EE"</code>.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>estimated amount of study level variability (only for <code>model="UM.RS"</code>).</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of studies included in the analysis.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>number of coefficients in the model (including the intercept).</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of coefficients included in the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QE.Wld</code></td>
<td>
<p>Wald-type test statistic of the test for (residual) heterogeneity.</p>
</td></tr>
<tr><td><code>QEp.Wld</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>QE.LRT</code></td>
<td>
<p>likelihood ratio test statistic of the test for (residual) heterogeneity.</p>
</td></tr>
<tr><td><code>QEp.LRT</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>I2</code></td>
<td>
<p>value of \(I^2\).</p>
</td></tr>
<tr><td><code>H2</code></td>
<td>
<p>value of \(H^2\).</p>
</td></tr>
<tr><td><code>int.only</code></td>
<td>
<p>logical that indicates whether the model is an intercept-only model.</p>
</td></tr>
<tr><td><code>yi</code>, <code>vi</code>, <code>X</code></td>
<td>
<p>the vector of outcomes, the corresponding sampling variances, and the model matrix.</p>
</td></tr>
<tr><td><code>fit.stats</code></td>
<td>
<p>a list with the log-likelihood, deviance, AIC, BIC, and AICc values.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.glmm">print</a></code> function. If fit statistics should also be given, use <code><a href="#topic+summary.rma">summary</a></code> (or use the <code><a href="#topic+fitstats.rma">fitstats</a></code> function to extract them).
</p>


<h3>Note</h3>

<p>When <code>measure="OR"</code> or <code>measure="IRR"</code>, <code>model="UM.FS"</code> or <code>model="UM.RS"</code>, and <code>method="ML"</code>, one has to choose a coding scheme for the group variable in the random effects structure. When <code>coding=1/2</code> (the default), the two groups are coded with <code>+1/2</code> and <code>-1/2</code> (i.e., contrast coding), which is invariant under group label switching.
</p>
<p>When <code>coding=1</code>, the first group is coded with <code>1</code> and the second group with <code>0</code>. Finally, when <code>coding=0</code>, the first group is coded with <code>0</code> and the second group with <code>1</code>. Note that these coding schemes are not invariant under group label switching.
</p>
<p>When <code>model="UM.RS"</code> and <code>method="ML"</code>, one has to decide whether the random study effects are allowed to be correlated with the random group effects. By default (i.e., when <code>cor=FALSE</code>), no such correlation is allowed (which is typically an appropriate assumption when <code>coding=1/2</code>). When using a different coding scheme for the group variable (i.e., <code>coding=1</code> or <code>coding=0</code>), allowing the random study and group effects to be correlated (i.e., using <code>cor=TRUE</code>) is usually recommended.
</p>
<p>Fitting the various types of models requires several different iterative algorithms:
</p>

<ul>
<li><p> For <code>model="UM.FS"</code> and <code>model="CM.AL"</code>, iteratively reweighted least squares (IWLS) as implemented in the <code><a href="stats.html#topic+glm">glm</a></code> function is used for fitting the equal-effects and the saturated models. For <code>method="ML"</code>, adaptive Gauss-Hermite quadrature as implemented in the <code><a href="lme4.html#topic+glmer">glmer</a></code> function is used. The same applies when <code>model="CM.EL"</code> is used in combination with <code>measure="IRR"</code> or when <code>measure="PLO"</code> or <code>measure="IRLN"</code> (regardless of the model type).
</p>
</li>
<li><p> For <code>model="UM.RS"</code>, adaptive Gauss-Hermite quadrature as implemented in the <code><a href="lme4.html#topic+glmer">glmer</a></code> function is used to fit all of the models.
</p>
</li>
<li><p> For <code>model="CM.EL"</code> and <code>measure="OR"</code>, the quasi-Newton method optimizer as implemented in the <code><a href="stats.html#topic+nlminb">nlminb</a></code> function is used by default for fitting the equal-effects and the saturated models. For <code>method="ML"</code>, the same algorithm is used, together with adaptive quadrature as implemented in the <code><a href="stats.html#topic+integrate">integrate</a></code> function (for the integration over the density of the non-central hypergeometric distribution). Standard errors of the parameter estimates are obtained by inverting the Hessian, which is numerically approximated using the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function from the <code>numDeriv</code> package. One can also set <code>control=list(hesspack="pracma")</code> in which case the <code><a href="pracma.html#topic+hessian">hessian</a></code> function from the <code>pracma</code> package is used instead for approximating the Hessian. When \(\tau^2\) is estimated to be smaller than \(10^{-4}\), then \(\tau^2\) is effectively treated as zero for computing the standard errors (which helps to avoid numerical problems in approximating the Hessian). This cutoff can be adjusted via the <code>tau2tol</code> control argument (e.g., <code>control=list(tau2tol=0)</code> to switch off this behavior).
</p>
<p>One can also chose a different optimizer from <code><a href="stats.html#topic+optim">optim</a></code> via the <code>control</code> argument (e.g., <code>control=list(optimizer="BFGS")</code> or <code>control=list(optimizer="Nelder-Mead")</code>). Besides <code><a href="stats.html#topic+nlminb">nlminb</a></code> and one of the methods from <code><a href="stats.html#topic+optim">optim</a></code>, one can also choose one of the optimizers from the <code>minqa</code> package (i.e., <code><a href="minqa.html#topic+uobyqa">uobyqa</a></code>, <code><a href="minqa.html#topic+newuoa">newuoa</a></code>, or <code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>), one of the (derivative-free) algorithms from the <code><a href="nloptr.html#topic+nloptr">nloptr</a></code> package, the Newton-type algorithm implemented in <code><a href="stats.html#topic+nlm">nlm</a></code>, the various algorithms implemented in the <code>dfoptim</code> package (<code><a href="dfoptim.html#topic+hjk">hjk</a></code> for the Hooke-Jeeves, <code><a href="dfoptim.html#topic+nmk">nmk</a></code> for the Nelder-Mead, and <code><a href="dfoptim.html#topic+mads">mads</a></code> for the Mesh Adaptive Direct Searches algorithm), the quasi-Newton type optimizers <code><a href="ucminf.html#topic+ucminf">ucminf</a></code> and <code><a href="lbfgsb3c.html#topic+lbfgsb3c">lbfgsb3c</a></code> and the subspace-searching simplex algorithm <code><a href="subplex.html#topic+subplex">subplex</a></code> from the packages of the same name, the Barzilai-Borwein gradient decent method implemented in <code><a href="BB.html#topic+BBoptim">BBoptim</a></code>, or the parallelized version of the L-BFGS-B algorithm implemented in <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code> from the package of the same name.
</p>
<p>The optimizer name must be given as a character string (i.e., in quotes). Additional control parameters can be specified via the <code>optCtrl</code> elements of the <code>control</code> argument (e.g., <code>control=list(optCtrl=list(iter.max=1000, rel.tol=1e-8))</code>). For <code><a href="nloptr.html#topic+nloptr">nloptr</a></code>, the default is to use the BOBYQA implementation from that package with a relative convergence criterion of <code>1e-8</code> on the function value (i.e., log-likelihood), but this can be changed via the <code>algorithm</code> and <code>ftop_rel</code> arguments (e.g., <code>control=list(optimizer="nloptr", optCtrl=list(algorithm="NLOPT_LN_SBPLX", ftol_rel=1e-6))</code>). For <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code>, the control argument <code>ncpus</code> can be used to specify the number of cores to use for the parallelization (e.g., <code>control=list(optimizer="optimParallel", ncpus=2)</code>). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
</li></ul>

<p>When <code>model="CM.EL"</code> and <code>measure="OR"</code>, actually <code>model="CM.AL"</code> is used first to obtain starting values for <code><a href="stats.html#topic+optim">optim</a></code>, so either 4 (if <code>method="EE"</code>) or 6 (if <code>method="ML"</code>) models need to be fitted in total.
</p>
<p>Various additional control parameters can be adjusted via the <code>control</code> argument:
</p>

<ul>
<li> <p><code>glmCtrl</code> is a list of named arguments to be passed on to the <code>control</code> argument of the <code><a href="stats.html#topic+glm">glm</a></code> function,
</p>
</li>
<li> <p><code>glmerCtrl</code> is a list of named arguments to be passed on to the <code>control</code> argument of the <code><a href="lme4.html#topic+glmer">glmer</a></code> function,
</p>
</li>
<li> <p><code>intCtrl</code> is a list of named arguments (i.e., <code>rel.tol</code> and <code>subdivisions</code>) to be passed on to the <code><a href="stats.html#topic+integrate">integrate</a></code> function, and
</p>
</li>
<li> <p><code>hessianCtrl</code> is a list of named arguments to be passed on to the <code>method.args</code> argument of the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function. Most important is the <code>r</code> argument, which is set to 16 by default (i.e., <code>control=list(hessianCtrl=list(r=16))</code>). If the Hessian cannot be inverted, it may be necessary to adjust the <code>r</code> argument to a different number (e.g., try <code>r=4</code>, <code>r=6</code>, or <code>r=8</code>).
</p>
</li></ul>

<p>Also, for <code><a href="lme4.html#topic+glmer">glmer</a></code>, the <code>nAGQ</code> argument is used to specify the number of quadrature points. The default value is 7, which should provide sufficient accuracy in the evaluation of the log-likelihood in most cases, but at the expense of speed. Setting this to 1 corresponds to the Laplacian approximation (which is faster, but less accurate). Note that <code><a href="lme4.html#topic+glmer">glmer</a></code> does not allow values of <code>nAGQ &gt; 1</code> when <code>model="UM.RS"</code> and <code>method="ML"</code>, so this value is automatically set to 1 for this model.
</p>
<p>Instead of <code><a href="lme4.html#topic+glmer">glmer</a></code>, one can also choose to use <code><a href="GLMMadaptive.html#topic+mixed_model">mixed_model</a></code> from the <code>GLMMadaptive</code> package or <code><a href="glmmTMB.html#topic+glmmTMB">glmmTMB</a></code> from the <code>glmmTMB</code> package for the model fitting. This is done by setting <code>control=list(package="GLMMadaptive")</code> or <code>control=list(package="glmmTMB")</code>, respectively.
</p>
<p>Information on the progress of the various algorithms can be obtained by setting <code>verbose=TRUE</code>. Since fitting the various models can be computationally expensive, this option is useful to determine how the model fitting is progressing. One can also set <code>verbose</code> to an integer (<code>verbose=2</code> yields even more information and <code>verbose=3</code> also sets <code>option(warn=1)</code> temporarily).
</p>
<p>For <code>model="CM.EL"</code> and <code>measure="OR"</code>, optimization involves repeated calculation of the density of the non-central hypergeometric distribution. When <code>method="ML"</code>, this also requires integration over the same density. This is currently implemented in a rather brute-force manner and may not be numerically stable, especially when models with moderators are fitted. Stability can be improved by scaling the moderators in a similar manner (i.e., don't use a moderator that is coded 0 and 1, while another uses values in the 1000s). For models with an intercept and moderators, the function actually rescales (non-dummy) variables to z-scores during the model fitting (results are given after back-scaling, so this should be transparent to the user). For models without an intercept, this is not done, so sensitivity analyses are highly recommended here (to ensure that the results do not depend on the scaling of the moderators). Also, if a warning is issued that the standard errors of the fixed effects are unusually small, one should try sensitivity analyses with different optimizers and/or adjusted settings for the <code>hessianCtrl</code> and <code>tau2tol</code> control arguments.
</p>
<p>Finally, there is also (experimental!) support for the following measures:
</p>

<ul>
<li> <p><code>measure="RR"</code> for log transformed risk ratios,
</p>
</li>
<li> <p><code>measure="RD"</code> for raw risk differences,
</p>
</li>
<li> <p><code>measure="PLN"</code> for log transformed proportions,
</p>
</li>
<li> <p><code>measure="PR"</code> for raw proportions,
</p>
</li></ul>

<p>(the first two only for models <code>"UM.FS"</code> and <code>"UM.RS"</code>) by using log and identity links for the binomial models. However, model fitting with these measures will often lead to numerical problems.
</p>
<p>Via the (undocumented) <code>link</code> argument, one can also directly adjust the link function that is used (by default, measures <code>"OR"</code> and <code>"PLO"</code> use a <code>"logit"</code> link, measures <code>"RR"</code> and <code>"PLN"</code> use a <code>"log"</code> link, measures <code>"RD"</code> and <code>"PR"</code> use an <code>"identity"</code> link, and measures <code>"IRR"</code> and <code>"IRLN"</code> use a <code>"log"</code> link). See <code><a href="stats.html#topic+family">family</a></code> for alternative options. Changing these defaults is only recommended for users familiar with the consequences and the interpretation of the resulting estimates (when misused, the results could be meaningless).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>
<p>Code for computing the density of the non-central hypergeometric distribution comes from the <a href="https://cran.r-project.org/package=MCMCpack">MCMCpack</a> package, which in turn is based on Liao and Rosen (2001).
</p>


<h3>References</h3>

<p>Agresti, A. (2002). <em>Categorical data analysis</em> (2nd. ed). Hoboken, NJ: Wiley.
</p>
<p>Bagos, P. G., &amp; Nikolopoulos, G. K. (2009). Mixed-effects Poisson regression models for meta-analysis of follow-up studies with constant or varying durations. <em>The International Journal of Biostatistics</em>, <b>5</b>(1). <code style="white-space: pre;">&#8288;https://doi.org/10.2202/1557-4679.1168&#8288;</code>
</p>
<p>van Houwelingen, H. C., Zwinderman, K. H., &amp; Stijnen, T. (1993). A bivariate approach to meta-analysis. <em>Statistics in Medicine</em>, <b>12</b>(24), 2273&ndash;2284. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780122405&#8288;</code>
</p>
<p>Jackson, D., Law, M., Stijnen, T., Viechtbauer, W., &amp; White, I. R. (2018). A comparison of seven random-effects models for meta-analyses that estimate the summary odds ratio. <em>Statistics in Medicine</em>, <b>37</b>(7), 1059-1085. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.7588&#8288;</code>
</p>
<p>Liao, J. G., &amp; Rosen, O. (2001). Fast and stable algorithms for computing and sampling from the noncentral hypergeometric distribution. <em>American Statistician</em>, <b>55</b>(4), 366&ndash;369. <code style="white-space: pre;">&#8288;https://doi.org/10.1198/000313001753272547&#8288;</code>
</p>
<p>Simmonds, M. C., &amp; Higgins, J. P. T. (2016). A general framework for the use of logistic regression models in meta-analysis. <em>Statistical Methods in Medical Research</em>, <b>25</b>(6), 2858&ndash;2877. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/0962280214534409&#8288;</code>
</p>
<p>Stijnen, T., Hamza, T. H., &amp; Ozdemir, P. (2010). Random effects meta-analysis of event outcome in the framework of the generalized linear mixed model with applications in sparse data. <em>Statistics in Medicine</em>, <b>29</b>(29), 3046&ndash;3067. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4040&#8288;</code>
</p>
<p>Turner, R. M., Omar, R. Z., Yang, M., Goldstein, H., &amp; Thompson, S. G. (2000). A multilevel model framework for meta-analysis of clinical trials with binary outcomes. <em>Statistics in Medicine</em>, <b>19</b>(24), 3417&ndash;3432. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/1097-0258(20001230)19:24&lt;3417::aid-sim614&gt;3.0.co;2-l&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for other model fitting functions.
</p>
<p><code><a href="metadat.html#topic+dat.nielweise2007">dat.nielweise2007</a></code>, <code><a href="metadat.html#topic+dat.nielweise2008">dat.nielweise2008</a></code>, <code><a href="metadat.html#topic+dat.collins1985a">dat.collins1985a</a></code>, and <code><a href="metadat.html#topic+dat.pritz1997">dat.pritz1997</a></code> for further examples of the use of the <code>rma.glmm</code> function.
</p>
<p>For rare event data, see also the <a href="https://cran.r-project.org/package=rema">rema</a> package for a version of the conditional logistic model that uses a permutation approach for making inferences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### random-effects model using rma.uni() (standard RE model analysis)
rma(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, method="ML")

### random-effects models using rma.glmm() (requires 'lme4' package)

## Not run: 
### unconditional model with fixed study effects (the default)
rma.glmm(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, model="UM.FS")

### unconditional model with random study effects
rma.glmm(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, model="UM.RS")

### conditional model with approximate likelihood
rma.glmm(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, model="CM.AL")

### conditional model with exact likelihood
### note: fitting this model may take a bit of time, so be patient
rma.glmm(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, model="CM.EL")

## End(Not run)

############################################################################

### try some alternative measures

## Not run: 
rma.glmm(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
rma.glmm(measure="RD", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

## End(Not run)

############################################################################

### meta-analysis of proportions

## Not run: 
dat &lt;- dat.debruin2009

### binomial-normal model (with logit link) = mixed-effects logistic model
res &lt;- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat)
predict(res, transf=transf.ilogit)

### binomial-normal model with measure="PLN" (uses a log link)
res &lt;- rma.glmm(measure="PLN", xi=xi, ni=ni, data=dat)
predict(res, transf=exp)

### binomial-normal model with measure="PR" (uses an identity link)
res &lt;- rma.glmm(measure="PR", xi=xi, ni=ni, data=dat)
predict(res)

### binomial-normal model (with probit link) = mixed-effects probit model
res &lt;- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat, link="probit")
predict(res, transf=pnorm)

### further link functions that one could consider here
res &lt;- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat, link="cauchit")
predict(res, transf=pcauchy)
res &lt;- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat, link="cloglog")
predict(res, transf=\(x) 1-exp(-exp(x)))

## End(Not run)

############################################################################
</code></pre>

<hr>
<h2 id='rma.mh'>Meta-Analysis via the Mantel-Haenszel Method</h2><span id='topic+rma.mh'></span>

<h3>Description</h3>

<p>Function to fit equal-effects models to \(2 \times 2\) table and person-time data via the Mantel-Haenszel method. See below and the introduction to the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> for more details on these models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rma.mh(ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
       measure="OR", data, slab, subset,
       add=1/2, to="only0", drop00=TRUE,
       correct=TRUE, level=95, verbose=FALSE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rma.mh_+3A_ai">ai</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper left cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_bi">bi</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper right cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_ci">ci</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower left cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_di">di</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower right cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_n1i">n1i</code></td>
<td>
<p>vector with the group sizes or row totals (first group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_n2i">n2i</code></td>
<td>
<p>vector with the group sizes or row totals (second group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_x1i">x1i</code></td>
<td>
<p>vector with the number of events (first group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_x2i">x2i</code></td>
<td>
<p>vector with the number of events (second group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_t1i">t1i</code></td>
<td>
<p>vector with the total person-times (first group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_t2i">t2i</code></td>
<td>
<p>vector with the total person-times (second group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_measure">measure</code></td>
<td>
<p>character string to specify the outcome measure to use for the meta-analysis. Possible options are <code>"RR"</code> for the (log transformed) risk ratio, <code>"OR"</code> for the (log transformed) odds ratio, <code>"RD"</code> for the risk difference, <code>"IRR"</code> for the (log transformed) incidence rate ratio, or <code>"IRD"</code> for the incidence rate difference.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_data">data</code></td>
<td>
<p>optional data frame containing the data supplied to the function.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be used for the analysis.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_add">add</code></td>
<td>
<p>non-negative number to specify the amount to add to zero cells or even counts when calculating the observed effect sizes of the individual studies. Can also be a vector of two numbers, where the first number is used in the calculation of the observed effect sizes and the second number is used when applying the Mantel-Haenszel method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_to">to</code></td>
<td>
<p>character string to specify when the values under <code>add</code> should be added (either <code>"only0"</code>, <code>"all"</code>, <code>"if0all"</code>, or <code>"none"</code>). Can also be a character vector, where the first string again applies when calculating the observed effect sizes or outcomes and the second string when applying the Mantel-Haenszel method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_drop00">drop00</code></td>
<td>
<p>logical to specify whether studies with no cases/events (or only cases) in both groups should be dropped when calculating the observed effect sizes or outcomes (the outcomes for such studies are set to <code>NA</code>). Can also be a vector of two logicals, where the first applies to the calculation of the observed effect sizes or outcomes and the second when applying the Mantel-Haenszel method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_correct">correct</code></td>
<td>
<p>logical to specify whether to apply a continuity correction when computing the Cochran-Mantel-Haenszel test statistic.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="rma.mh_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specifying the Data</h4>

<p>When the outcome measure is either the risk ratio (measure=<code>"RR"</code>), odds ratio (<code>measure="OR"</code>), or risk difference (<code>measure="RD"</code>), the studies are assumed to provide data in terms of \(2 \times 2\) tables of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
              </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies and <code>n1i</code> and <code>n2i</code> the row totals. For example, in a set of randomized clinical trials (RCTs) or cohort studies, group 1 and group 2 may refer to the treatment/exposed and placebo/control/non-exposed group, respectively, with outcome 1 denoting some event of interest (e.g., death) and outcome 2 its complement. In a set of case-control studies, group 1 and group 2 may refer to the group of cases and the group of controls, with outcome 1 denoting, for example, exposure to some risk factor and outcome 2 non-exposure. For these outcome measures, one needs to specify the cell frequencies via the <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> arguments (or alternatively, one can use the <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code> arguments).
</p>
<p>Alternatively, when the outcome measure is the incidence rate ratio (<code>measure="IRR"</code>) or the incidence rate difference (<code>measure="IRD"</code>), the studies are assumed to provide data in terms of tables of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
              </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> events     </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> person-time </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>x1i</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>t1i</code>  </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>x2i</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>t2i</code></td>
</tr>

</table>

<p>where <code>x1i</code> and <code>x2i</code> denote the number of events in the first and the second group, respectively, and <code>t1i</code> and <code>t2i</code> the corresponding total person-times at risk.
</p>



<h4>Mantel-Haenszel Method</h4>

<p>An approach for aggregating data of these types was suggested by Mantel and Haenszel (1959) and later extended by various authors (see references). The Mantel-Haenszel method provides a weighted estimate under an equal-effects model. The method is particularly advantageous when aggregating a large number of studies with small sample sizes (the so-called sparse data or increasing strata case).
</p>
<p>When analyzing odds ratios, the Cochran-Mantel-Haenszel (CMH) test (Cochran, 1954; Mantel &amp; Haenszel, 1959) and Tarone's test for heterogeneity (Tarone, 1985) are also provided (by default, the CMH test statistic is computed with the continuity correction; this can be switched off with <code>correct=FALSE</code>). When analyzing incidence rate ratios, the Mantel-Haenszel (MH) test (Rothman et al., 2008) for person-time data is also provided (again, the <code>correct</code> argument controls whether the continuity correction is applied). When analyzing risk ratios, odds ratios, or incidence rate ratios, the printed results are given both in terms of the log and the raw units (for easier interpretation).
</p>



<h4>Observed Effect Sizes or Outcomes of the Individual Studies</h4>

<p>The Mantel-Haenszel method itself does not require the calculation of the observed effect sizes of the individual studies (e.g., the observed log odds ratios of the \(k\) studies) and directly makes use of the cell/event counts. Zero cells/events are not a problem (except in extreme cases, such as when one of the two outcomes never occurs in any of the \(2 \times 2\) tables or when there are no events for one of the two groups in any of the tables). Therefore, it is unnecessary to add some constant to the cell/event counts when there are zero cells/events.
</p>
<p>However, for plotting and various other functions, it is necessary to calculate the observed effect sizes for the \(k\) studies. Here, zero cells/events can be problematic, so adding a constant value to the cell/event counts ensures that all \(k\) values can be calculated. The <code>add</code> and <code>to</code> arguments are used to specify what value should be added to the cell/event counts and under what circumstances when calculating the observed effect sizes and when applying the Mantel-Haenszel method. Similarly, the <code>drop00</code> argument is used to specify how studies with no cases/events (or only cases) in both groups should be handled. The documentation of the <code><a href="#topic+escalc">escalc</a></code> function explains how the <code>add</code>, <code>to</code>, and <code>drop00</code> arguments work. If only a single value for these arguments is specified (as per default), then these values are used when calculating the observed effect sizes and no adjustment to the cell/event counts is made when applying the Mantel-Haenszel method. Alternatively, when specifying two values for these arguments, the first value applies when calculating the observed effect sizes and the second value when applying the Mantel-Haenszel method.
</p>
<p>Note that <code>drop00</code> is set to <code>TRUE</code> by default. Therefore, the observed effect sizes for studies where <code>ai=ci=0</code> or <code>bi=di=0</code> or studies where <code>x1i=x2i=0</code> are set to <code>NA</code>. When applying the Mantel-Haenszel method, such studies are not explicitly dropped (unless the second value of <code>drop00</code> argument is also set to <code>TRUE</code>), but this is practically not necessary, as they do not actually influence the results (assuming no adjustment to the cell/event counts are made when applying the Mantel-Haenszel method).
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.mh","rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>aggregated log risk ratio, log odds ratio, risk difference, log rate ratio, or rate difference.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard error of the aggregated value.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the aggregated value.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence interval.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence interval.</p>
</td></tr>
<tr><td><code>QE</code></td>
<td>
<p>test statistic of the test for heterogeneity.</p>
</td></tr>
<tr><td><code>QEp</code></td>
<td>
<p>correspinding p-value.</p>
</td></tr>
<tr><td><code>MH</code></td>
<td>
<p>Cochran-Mantel-Haenszel test statistic (<code>measure="OR"</code>) or Mantel-Haenszel test statistic (<code>measure="IRR"</code>).</p>
</td></tr>
<tr><td><code>MHp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>TA</code></td>
<td>
<p>test statistic of Tarone's test for heterogeneity (only when <code>measure="OR"</code>).</p>
</td></tr>
<tr><td><code>TAp</code></td>
<td>
<p>corresponding p-value (only when <code>measure="OR"</code>).</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of studies included in the analysis.</p>
</td></tr>
<tr><td><code>yi</code>, <code>vi</code></td>
<td>
<p>the vector of outcomes and corresponding sampling variances.</p>
</td></tr>
<tr><td><code>fit.stats</code></td>
<td>
<p>a list with the log-likelihood, deviance, AIC, BIC, and AICc values under the unrestricted and restricted likelihood.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.mh">print</a></code> function. If fit statistics should also be given, use <code><a href="#topic+summary.rma">summary</a></code> (or use the <code><a href="#topic+fitstats.rma">fitstats</a></code> function to extract them).
</p>
<p>The <code><a href="#topic+residuals.rma">residuals</a></code>, <code><a href="#topic+rstandard.rma.mh">rstandard</a></code>, and <code><a href="#topic+rstudent.rma.mh">rstudent</a></code> functions extract raw and standardized residuals. Leave-one-out diagnostics can be obtained with <code><a href="#topic+leave1out.rma.mh">leave1out</a></code>.
</p>
<p>Forest, funnel, radial, L'Abb√©, and Baujat plots can be obtained with <code><a href="#topic+forest.rma">forest</a></code>, <code><a href="#topic+funnel.rma">funnel</a></code>, <code><a href="#topic+radial.rma">radial</a></code>, <code><a href="#topic+labbe.rma">labbe</a></code>, and <code><a href="#topic+baujat.rma">baujat</a></code>. The <code><a href="#topic+qqnorm.rma.mh">qqnorm</a></code> function provides normal QQ plots of the standardized residuals. One can also just call <code><a href="#topic+plot.rma.mh">plot</a></code> on the fitted model object to obtain various plots at once.
</p>
<p>A cumulative meta-analysis (i.e., adding one observation at a time) can be obtained with <code><a href="#topic+cumul.rma.mh">cumul</a></code>.
</p>
<p>Other extractor functions include <code><a href="#topic+coef.rma">coef</a></code>, <code><a href="#topic+vcov.rma">vcov</a></code>, <code><a href="#topic+logLik.rma">logLik</a></code>, <code><a href="#topic+deviance.rma">deviance</a></code>, <code><a href="#topic+AIC.rma">AIC</a></code>, and <code><a href="#topic+BIC.rma">BIC</a></code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Cochran, W. G. (1954). Some methods for strengthening the common \(\chi^2\) tests. <em>Biometrics</em>, <b>10</b>(4), 417&ndash;451. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/3001616&#8288;</code>
</p>
<p>Greenland, S., &amp; Robins, J. M. (1985). Estimation of a common effect parameter from sparse follow-up data. <em>Biometrics</em>, <b>41</b>(1), 55&ndash;68. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2530643&#8288;</code>
</p>
<p>Mantel, N., &amp; Haenszel, W. (1959). Statistical aspects of the analysis of data from retrospective studies of disease. <em>Journal of the National Cancer Institute</em>, <b>22</b>(4), 719&ndash;748. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/jnci/22.4.719&#8288;</code>
</p>
<p>Nurminen, M. (1981). Asymptotic efficiency of general noniterative estimators of common relative risk. <em>Biometrika</em>, <b>68</b>(2), 525&ndash;530. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/biomet/68.2.525&#8288;</code>
</p>
<p>Robins, J., Breslow, N., &amp; Greenland, S. (1986). Estimators of the Mantel-Haenszel variance consistent in both sparse data and large-strata limiting models. <em>Biometrics</em>, <b>42</b>(2), 311&ndash;323. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2531052 &#8288;</code>
</p>
<p>Rothman, K. J., Greenland, S., &amp; Lash, T. L. (2008). <em>Modern epidemiology</em> (3rd ed.). Philadelphia: Lippincott Williams &amp; Wilkins.
</p>
<p>Sato, T., Greenland, S., &amp; Robins, J. M. (1989). On the variance estimator for the Mantel-Haenszel risk difference. <em>Biometrics</em>, <b>45</b>(4), 1323&ndash;1324. <code style="white-space: pre;">&#8288;https://www.jstor.org/stable/2531784&#8288;</code>
</p>
<p>Tarone, R. E. (1981). On summary estimators of relative risk. <em>Journal of Chronic Diseases</em>, <b>34</b>(9-10), 463&ndash;468. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/0021-9681(81)90006-0&#8288;</code>
</p>
<p>Tarone, R. E. (1985). On heterogeneity tests based on efficient scores. <em>Biometrika</em>, <b>72</b>(1), 91&ndash;95. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/biomet/72.1.91&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for other model fitting functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### meta-analysis of the (log) odds ratios using the Mantel-Haenszel method
rma.mh(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### meta-analysis of the (log) risk ratios using the Mantel-Haenszel method
rma.mh(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
</code></pre>

<hr>
<h2 id='rma.mv'>Meta-Analysis via Multivariate/Multilevel Linear (Mixed-Effects) Models</h2><span id='topic+rma.mv'></span>

<h3>Description</h3>

<p>Function to fit meta-analytic multivariate/multilevel fixed- and random/mixed-effects models with or without moderators via linear (mixed-effects) models. See below and the introduction to the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> for more details on these models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rma.mv(yi, V, W, mods, random, struct="CS", intercept=TRUE,
       data, slab, subset, method="REML",
       test="z", dfs="residual", level=95, btt,
       R, Rscale="cor", sigma2, tau2, rho, gamma2, phi,
       cvvc=FALSE, sparse=FALSE, verbose=FALSE, digits, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rma.mv_+3A_yi">yi</code></td>
<td>
<p>vector of length \(k\) with the observed effect sizes or outcomes. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_v">V</code></td>
<td>
<p>vector of length \(k\) with the corresponding sampling variances or a \(k \times k\) variance-covariance matrix of the sampling errors. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_w">W</code></td>
<td>
<p>optional argument to specify a vector of length \(k\) with user-defined weights or a \(k \times k\) user-defined weight matrix. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_mods">mods</code></td>
<td>
<p>optional argument to include one or more moderators in the model. A single moderator can be given as a vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving a matrix with \(k\) rows and as many columns as there are moderator variables. Alternatively, a model <code><a href="#topic+formula">formula</a></code> can be used to specify the model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_random">random</code></td>
<td>
<p>either a single one-sided formula or list of one-sided formulas to specify the random-effects structure of the model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_struct">struct</code></td>
<td>
<p>character string to specify the variance structure of an <code>~ inner | outer</code> formula in the <code>random</code> argument. Either <code>"CS"</code> for compound symmetry, <code>"HCS"</code> for heteroscedastic compound symmetry, <code>"UN"</code> or <code>"GEN"</code> for an unstructured variance-covariance matrix, <code>"ID"</code> for a scaled identity matrix, <code>"DIAG"</code> for a diagonal matrix, <code>"AR"</code> for an AR(1) autoregressive structure, <code>"HAR"</code> for a heteroscedastic AR(1) autoregressive structure, <code>"CAR"</code> for a continuous-time autoregressive structure, or one of <code>"SPEXP"</code>, <code>"SPGAU"</code>, <code>"SPLIN"</code>, <code>"SPRAT"</code>, or <code>"SPSPH"</code> for one of the spatial correlation structures. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_intercept">intercept</code></td>
<td>
<p>logical to specify whether an intercept should be added to the model (the default is <code>TRUE</code>). Ignored when <code>mods</code> is a formula.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_data">data</code></td>
<td>
<p>optional data frame containing the data supplied to the function.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) outcomes/studies.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies (or more precisely, rows of the dataset) that should be used for the analysis.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_method">method</code></td>
<td>
<p>character string to specify whether the model should be fitted via maximum likelihood (<code>"ML"</code>) or via restricted maximum likelihood (<code>"REML"</code>) estimation (the default is <code>"REML"</code>).</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_test">test</code></td>
<td>
<p>character string to specify how test statistics and confidence intervals for the fixed effects should be computed. By default (<code>test="z"</code>), Wald-type tests and CIs are obtained, which are based on a standard normal distribution. When <code>test="t"</code>, a t-distribution is used instead. See &lsquo;Details&rsquo; and also <a href="#topic+misc-recs">here</a> for some recommended practices.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_dfs">dfs</code></td>
<td>
<p>character string to specify how the (denominator) degrees of freedom should be calculated when <code>test="t"</code>. Either <code>dfs="residual"</code> or <code>dfs="contain"</code>. Can also be a numeric vector with the degrees of freedom for each model coefficient. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_btt">btt</code></td>
<td>
<p>optional vector of indices to specify which coefficients to include in the omnibus test of moderators. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_r">R</code></td>
<td>
<p>an optional named list of known correlation matrices corresponding to (some of) the components specified via the <code>random</code> argument. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_rscale">Rscale</code></td>
<td>
<p>character string, integer, or logical to specify how matrices specified via the <code>R</code> argument should be scaled. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_sigma2">sigma2</code></td>
<td>
<p>optional numeric vector (of the same length as the number of random intercept components specified via the <code>random</code> argument) to fix the corresponding \(\sigma^2\) value(s). A specific \(\sigma^2\) value can be fixed by setting the corresponding element of this argument to the desired value. A specific \(\sigma^2\) value will be estimated if the corresponding element is set equal to <code>NA</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_tau2">tau2</code></td>
<td>
<p>optional numeric value (for <code>struct="CS"</code>, <code>"AR"</code>, <code>"CAR"</code>, or a spatial correlation structure) or vector (for <code>struct="HCS"</code>, <code>"UN"</code>, or <code>"HAR"</code>) to fix the amount of (residual) heterogeneity for the levels of the <code>inner</code> factor corresponding to an <code>~ inner | outer</code> formula specified in the <code>random</code> argument. A numeric value fixes a particular \(\tau^2\) value, while <code>NA</code> means that the value should be estimated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_rho">rho</code></td>
<td>
<p>optional numeric value (for <code>struct="CS"</code>, <code>"HCS"</code>, <code>"AR"</code>, <code>"HAR"</code>, <code>"CAR"</code>, or a spatial correlation structure) or vector (for <code>struct="UN"</code>) to fix the correlation between the levels of the <code>inner</code> factor corresponding to an <code>~ inner | outer</code> formula specified in the <code>random</code> argument. A numeric value fixes a particular \(\rho\) value, while <code>NA</code> means that the value should be estimated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_gamma2">gamma2</code></td>
<td>
<p>as <code>tau2</code> argument, but for a second <code>~ inner | outer</code> formula specified in the <code>random</code> argument. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_phi">phi</code></td>
<td>
<p>as <code>rho</code> argument, but for a second <code>~ inner | outer</code> formula specified in the <code>random</code> argument. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_cvvc">cvvc</code></td>
<td>
<p>logical to specify whether to calculate the variance-covariance matrix of the variance/correlation component estimates (can also be set to <code>"varcov"</code> or <code>"varcor"</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_sparse">sparse</code></td>
<td>
<p>logical to specify whether the function should use sparse matrix objects to the extent possible (can speed up model fitting substantially for certain models). See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>). Can also be an integer. Values &gt; 1 generate more verbose output. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_control">control</code></td>
<td>
<p>optional list of control values for the estimation algorithms. If unspecified, default values are defined inside the function. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.mv_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specifying the Data</h4>

<p>The function can be used in combination with any of the usual effect sizes or outcome measures used in meta-analyses (e.g., log risk ratios, log odds ratios, risk differences, mean differences, standardized mean differences, log transformed ratios of means, raw correlation coefficients, correlation coefficients transformed with Fisher's r-to-z transformation), or, more generally, any set of estimates (with corresponding sampling variances) one would like to meta-analyze. Simply specify the observed effect sizes or outcomes via the <code>yi</code> argument and the corresponding sampling variances via the <code>V</code> argument. In case the sampling errors are correlated, then one can specify the entire variance-covariance matrix of the sampling errors via the <code>V</code> argument.
</p>
<p>The <code><a href="#topic+escalc">escalc</a></code> function can be used to compute a wide variety of effect sizes or outcome measures (and the corresponding sampling variances) based on summary statistics. Equations for computing the covariance between the sampling errors for a variety of different effect sizes or outcome measures can be found, for example, in Gleser and Olkin (2009), Lajeunesse (2011), and Wei and Higgins (2013). For raw and Fisher r-to-z transformed correlations, one can find suitable equations, for example, in Steiger (1980). The latter are implemented in the <code><a href="#topic+rcalc">rcalc</a></code> function. See also <code><a href="#topic+vcalc">vcalc</a></code> for a function that can be used to construct or approximate the variance-covariance matrix of dependent effect sizes or outcomes for a wide variety of circumstances. See also <a href="#topic+misc-recs">here</a> for some recommendations on a general workflow for meta-analyses involving complex dependency structures.
</p>



<h4>Specifying Fixed Effects</h4>

<p>With <code>rma.mv(yi, V)</code>, a fixed-effects model is fitted to the data (note: arguments <code>struct</code>, <code>sigma2</code>, <code>tau2</code>, <code>rho</code>, <code>gamma2</code>, <code>phi</code>, <code>R</code>, and <code>Rscale</code> are not relevant then and are ignored). The model is then simply given by \(y \sim N(\theta, V)\), where \(y\) is a (column) vector with the observed outcomes, \(\theta\) is the (average) true outcome, and \(V\) is the variance-covariance matrix of the sampling errors (if a vector of sampling variances is provided via the <code>V</code> argument, then \(V\) is assumed to be diagonal). Note that the argument is <code>V</code>, not <code>v</code> (<span class="rlang"><b>R</b></span> is case sensitive!).
</p>
<p>One or more moderators can be included in the model via the <code>mods</code> argument. A single moderator can be given as a (row or column) vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving an appropriate model matrix (i.e., \(X\)) with \(k\) rows and as many columns as there are moderator variables (e.g., <code>mods = cbind(mod1, mod2, mod3)</code>, where <code>mod1</code>, <code>mod2</code>, and <code>mod3</code> correspond to the names of the variables for the three moderator variables). The intercept is added to the model matrix by default unless <code>intercept=FALSE</code>.
</p>
<p>Alternatively, one can use standard <code><a href="#topic+formula">formula</a></code> syntax to specify the model. In this case, the <code>mods</code> argument should be set equal to a one-sided formula of the form <code>mods = ~ model</code> (e.g., <code>mods = ~ mod1 + mod2 + mod3</code>). Interactions, polynomial terms, and factors can be easily added to the model in this manner. When specifying a model formula via the <code>mods</code> argument, the <code>intercept</code> argument is ignored. Instead, the inclusion/exclusion of the intercept is controlled by the specified formula (e.g., <code>mods = ~ mod1 + mod2 + mod3 - 1</code> would lead to the removal of the intercept). One can also directly specify moderators via the <code>yi</code> argument (e.g., <code>rma.mv(yi ~ mod1 + mod2 + mod3, V)</code>). In that case, the <code>mods</code> argument is ignored and the inclusion/exclusion of the intercept again is controlled by the specified formula.
</p>
<p>With moderators included, the model is then given by \(y \sim N(X \beta, V)\), where \(X\) denotes the model matrix containing the moderator values (and possibly the intercept) and \(\beta\) is a column vector containing the corresponding model coefficients. The model coefficients (i.e., \(\beta\)) are then estimated with \(b = (X'WX')^{-1} X'Wy\), where \(W = V^{-1}\) is the weight matrix (without moderators, \(X\) is just a column vector of 1's). With the <code>W</code> argument, one can also specify user-defined weights (or a weight matrix).
</p>



<h4>Specifying Random Effects</h4>

<p>One can fit random/mixed-effects models to the data by specifying the desired random effects structure via the <code>random</code> argument. The <code>random</code> argument is either a single one-sided formula or a list of one-sided formulas. One formula type that can be specified via this argument is of the form <code>random = ~ 1 | id</code>. Such a formula adds a random effect corresponding to the grouping variable <code>id</code> to the model. Outcomes with the same value of the <code>id</code> variable receive the same value of the random effect, while outcomes with different values of the <code>id</code> variable are assumed to be independent. The variance component corresponding to such a formula is denoted by \(\sigma^2\). An arbitrary number of such formulas can be specified as a list of formulas (e.g., <code>random = list(~ 1 | id1, ~ 1 | id2)</code>), with variance components \(\sigma^2_1\), \(\sigma^2_2\), and so on. Nested random effects of this form can also be added using <code>random = ~ 1 | id1/id2</code>, which adds a random effect corresponding to the grouping variable <code>id1</code> and a random effect corresponding to <code>id2</code> within <code>id1</code> to the model. This can be extended to models with even more levels of nesting (e.g., <code>random = ~ 1 | id1/id2/id3</code>).
</p>
<p>Random effects of this form are useful to model clustering (and hence non-independence) induced by a multilevel structure in the data (e.g., outcomes derived from the same paper, lab, research group, or species may be more similar to each other than outcomes derived from different papers, labs, research groups, or species). See, for example, Konstantopoulos (2011) and Nakagawa and Santos (2012) for more details.
</p>
<p>See <code><a href="metadat.html#topic+dat.konstantopoulos2011">dat.konstantopoulos2011</a></code>, <code><a href="metadat.html#topic+dat.bornmann2007">dat.bornmann2007</a></code>, <code><a href="metadat.html#topic+dat.obrien2003">dat.obrien2003</a></code>, and <code><a href="metadat.html#topic+dat.crede2010">dat.crede2010</a></code> for examples of multilevel meta-analyses.
</p>
<p>In addition or alternatively to specifying one or multiple <code>~ 1 | id</code> terms, the <code>random</code> argument can also contain a formula of the form <code>~ inner | outer</code>. Outcomes with the same value of the <code>outer</code> grouping variable share correlated random effects corresponding to the levels of the <code>inner</code> grouping variable, while outcomes with different values of the <code>outer</code> grouping variable are assumed to be independent (note that the <code>inner</code> variable is automatically treated as a factor). The <code>struct</code> argument is used to specify the variance structure corresponding to the <code>inner</code> variable. With <code>struct="CS"</code>, a compound symmetric structure is assumed (i.e., a single variance component \(\tau^2\) corresponding to the \(j = 1, \ldots, J\) levels of the <code>inner</code> variable and a single correlation coefficient \(\rho\) for the correlation between the different levels). With <code>struct="HCS"</code>, a heteroscedastic compound symmetric structure is assumed (with \(\tau^2_j\) denoting the variance component corresponding to the \(j\textrm{th}\) level of the <code>inner</code> variable and a single correlation coefficient \(\rho\) for the correlation between the different levels). With <code>struct="UN"</code>, an unstructured (but positive definite) variance-covariance matrix is assumed (with \(\tau^2_j\) as described above and correlation coefficient \(\rho_{jj'}\) for the combination of the \(j\textrm{th}\) and \(j'\textrm{th}\) level of the <code>inner</code> variable). For example, for an <code>inner</code> variable with four levels, the three structures correspond to variance-covariance matrices of the form:
</p>
\[\begin{array}{ccc}\texttt{struct="CS"} & \texttt{struct="HCS"} & \texttt{struct="UN"} \\ \begin{bmatrix} \tau^2 & & & \\ \rho\tau^2 & \tau^2 & & \\ \rho\tau^2 & \rho\tau^2 & \tau^2 & \\ \rho\tau^2 & \rho\tau^2 & \rho\tau^2 & \tau^2 \end{bmatrix} & \begin{bmatrix} \tau_1^2 & & & \\ \rho\tau_2\tau_1 & \tau_2^2 & & \\ \rho\tau_3\tau_1 & \rho\tau_3\tau_2 & \tau_3^2 & \\ \rho\tau_4\tau_1 & \rho\tau_4\tau_2 & \rho\tau_4\tau_3 & \tau_4^2 \end{bmatrix} & \begin{bmatrix} \tau_1^2 & & & \\ \rho_{21}\tau_2\tau_1 & \tau_2^2 & & \\ \rho_{31}\tau_3\tau_1 & \rho_{32}\tau_3\tau_2 & \tau_3^2 & \\ \rho_{41}\tau_4\tau_1 & \rho_{42}\tau_4\tau_2 & \rho_{43}\tau_4\tau_3 & \tau_4^2 \end{bmatrix} \end{array}\]
<p>Structures <code>struct="ID"</code> and <code>struct="DIAG"</code> are just like <code>struct="CS"</code> and <code>struct="HCS"</code>, respectively, except that \(\rho\) is set to 0, so that we either get a scaled identity matrix or a diagonal matrix.
</p>
<p>With the <code>outer</code> term corresponding to a study identification variable and the <code>inner</code> term to a variable indicating the treatment type or study arm, such a random effect could be used to estimate how strongly different treatment effects or outcomes within the same study are correlated and/or whether the amount of heterogeneity differs across different treatment types/arms. Network meta-analyses (also known as mixed treatment comparisons) will also typically require such a random effect (e.g., Salanti et al., 2008). The meta-analytic bivariate model (e.g., van Houwelingen, Arends, &amp; Stijnen, 2002) can also be fitted in this manner (see the examples below). The <code>inner</code> term could also correspond to a variable indicating different types of outcomes measured within the same study, which allows for fitting multivariate models with multiple correlated effects/outcomes per study (e.g., Berkey et al., 1998; Kalaian &amp; Raudenbush, 1996).
</p>
<p>See <code><a href="metadat.html#topic+dat.berkey1998">dat.berkey1998</a></code>, <code><a href="metadat.html#topic+dat.assink2016">dat.assink2016</a></code>, <code><a href="metadat.html#topic+dat.kalaian1996">dat.kalaian1996</a></code>, <code><a href="metadat.html#topic+dat.dagostino1998">dat.dagostino1998</a></code>, and <code><a href="metadat.html#topic+dat.craft2003">dat.craft2003</a></code> for examples of multivariate meta-analyses with multiple outcomes. See <code><a href="metadat.html#topic+dat.knapp2017">dat.knapp2017</a></code>, <code><a href="metadat.html#topic+dat.mccurdy2020">dat.mccurdy2020</a></code>, and <code><a href="metadat.html#topic+dat.tannersmith2016">dat.tannersmith2016</a></code> for further examples of multilevel/multivariate models with complex data structures (see also <a href="#topic+misc-recs">here</a> for a related discussion of a recommended workflow for such cases). See <code><a href="metadat.html#topic+dat.kearon1998">dat.kearon1998</a></code> for an example using a bivariate model to analyze sensitivity and specificity. See <code><a href="metadat.html#topic+dat.hasselblad1998">dat.hasselblad1998</a></code>, <code><a href="metadat.html#topic+dat.pagliaro1992">dat.pagliaro1992</a></code>, <code><a href="metadat.html#topic+dat.lopez2019">dat.lopez2019</a></code>, and <code><a href="metadat.html#topic+dat.senn2013">dat.senn2013</a></code> for examples of network meta-analyses.
</p>
<p>For meta-analyses of studies reporting outcomes at multiple time points, it may also be reasonable to assume that the true effects/outcomes are correlated over time according to an autoregressive structure (Ishak et al., 2007; Trikalinos &amp; Olkin, 2012). For this purpose, one can choose <code>struct="AR"</code>, corresponding to a structure with a single variance component \(\tau^2\) and AR(1) autocorrelation among the values of the random effect. The values of the <code>inner</code> variable should then reflect the various time points, with increasing values reflecting later time points. This structure assumes equally spaced time points, so the actual values of the <code>inner</code> variable are not relevant, only their ordering. One can also use <code>struct="HAR"</code>, which allows for fitting a heteroscedastic AR(1) structure (with \(\tau^2_j\) denoting the variance component of the \(j\textrm{th}\) measurement occasion). Finally, when time points are not evenly spaced, one might consider using <code>struct="CAR"</code> for a continuous-time autoregressive structure, in which case the values of the <code>inner</code> variable should reflect the exact time points of the measurement occasions. For example, for an <code>inner</code> variable with four time points, these structures correspond to variance-covariance matrices of the form:
</p>
\[\begin{array}{ccc}\texttt{struct="AR"} & \texttt{struct="HAR"} & \texttt{struct="CAR"} \\ \begin{bmatrix} \tau^2 & & & \\ \rho\tau^2 & \tau^2 & & \\ \rho^2\tau^2 & \rho\tau^2 & \tau^2 & \\ \rho^3\tau^2 & \rho^2\tau^2 & \rho\tau^2 & \tau^2 \end{bmatrix} & \begin{bmatrix} \tau_1^2 & & & \\ \rho\tau_2\tau_1 & \tau_2^2 & & \\ \rho^2\tau_3\tau_1 & \rho\tau_3\tau_2 & \tau_3^2 & \\ \rho^3\tau_4\tau_1 & \rho^2\tau_4\tau_2 & \rho\tau_4\tau_3 & \tau_4^2 \end{bmatrix} & \begin{bmatrix} \tau^2 & & & \\ \rho^{|t_2-t_1|}\tau^2 & \tau^2 & & \\ \rho^{|t_3-t_1|}\tau^2 & \rho^{|t_3-t_2|}\tau^2 & \tau^2 & \\ \rho^{|t_4-t_1|}\tau^2 & \rho^{|t_4-t_2|}\tau^2 & \rho^{|t_4-t_3|}\tau^2 & \tau^2 \end{bmatrix} \end{array}\]
<p>See <code><a href="metadat.html#topic+dat.fine1993">dat.fine1993</a></code> and <code><a href="metadat.html#topic+dat.ishak2007">dat.ishak2007</a></code> for examples involving such structures.
</p>
<p>For outcomes that have a known spatial configuration, various spatial correlation structures are also available. For these structures, the formula is of the form <code>random = ~ var1 + var2 + ... | outer</code>, where <code>var1</code>, <code>var2</code>, and so on are variables to indicate the spatial coordinates (e.g., longitude and latitude) based on which distances (by default Euclidean) will be computed. Let \(d\) denote the distance between two points that share the same value of the <code>outer</code> variable (if all true effects/outcomes are allowed to be spatially correlated, simply set <code>outer</code> to a variable that is a constant). Then the correlation between the true effects/outcomes corresponding to these two points is a function of \(d\) and the parameter \(\rho\). The following table shows the types of spatial correlation structures that can be specified and the equations for the correlation. The covariance between the true effects/outcomes is then the correlation times \(\tau^2\).
</p>

<table>
<tr>
 <td style="text-align: left;">
      structure          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>struct</code>  </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> correlation </td>
</tr>
<tr>
 <td style="text-align: left;">
      exponential        </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"SPEXP"</code> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(\exp(-d/\rho)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      Gaussian           </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"SPGAU"</code> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(\exp(-d^2/\rho^2)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      linear             </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"SPLIN"</code> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \((1 - d/\rho) I(d < \rho)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      rational quadratic </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"SPRAT"</code> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(1 - (d/\rho)^2 / (1 + (d/\rho)^2)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      spherical          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"SPSPH"</code> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \((1 - 1.5(d/\rho) + 0.5(d/\rho)^3) I(d < \rho)\)</td>
</tr>

</table>

<p>Note that \(I(d < \rho)\) is equal to \(1\) if \(d < \rho\) and \(0\) otherwise. The parameterization of the various structures is based on Pinheiro and Bates (2000). Instead of Euclidean distances, one can also use other distance measures by setting (the undocumented) argument <code>dist</code> to either <code>"maximum"</code> for the maximum distance between two points (supremum norm), to <code>"manhattan"</code> for the absolute distance between the coordinate vectors (L1 norm), or to <code>"gcd"</code> for the great-circle distance (WGS84 ellipsoid method). In the latter case, only two variables, namely the longitude and latitude (in decimal degrees, with minus signs for West and South), must be specified.
</p>
<p>If a distance matrix has already been computed, one can also pass this matrix as a list element to the <code>dist</code> argument. In this case, one should use a formula of the form <code>random = ~ id | outer</code>, where <code>id</code> are location identifiers, with corresponding row/column names in the distance matrix specified via the <code>dist</code> argument.
</p>
<p>See <code><a href="metadat.html#topic+dat.maire2019">dat.maire2019</a></code> for an example of a meta-analysis with a spatial correlation structure.
</p>
<p>An <code>~ inner | outer</code> formula can also be used to add random effects to the model corresponding to a set of predictor variables when <code>struct="GEN"</code>. Here, the <code>inner</code> term is used to specify one or multiple variables (e.g., <code>random = ~ var1 + var2 | outer</code>) and corresponding &lsquo;random slopes&rsquo; are added to the model (and a &lsquo;random intercept&rsquo; unless the intercept is removed from the <code>inner</code> term). The variance-covariance matrix of the random effects added in this manner is assumed to be a general unstructured (but positive definite) matrix. Such a random effects structure may be useful in a meta-analysis examining the dose-response relationship between a moderator variable and the size of the true effects/outcomes (sometimes called a &lsquo;dose-response meta-analysis&rsquo;).
</p>
<p>See <code><a href="metadat.html#topic+dat.obrien2003">dat.obrien2003</a></code> for an example of a meta-analysis examining a dose-response relationship.
</p>
<p>The <code>random</code> argument can also contain a second formula of the form <code>~ inner | outer</code> (but no more!). A second formula of this form works exactly described as above, but its variance components are denoted by \(\gamma^2\) and its correlation components by \(\phi\). The <code>struct</code> argument should then be of length 2 to specify the variance-covariance structure for the first and second component, respectively.
</p>
<p>When the <code>random</code> argument contains a formula of the form <code>~ 1 | id</code>, one can use the (optional) argument <code>R</code> to specify a corresponding known correlation matrix for the random effect (i.e., <code>R = list(id = Cor)</code>, where <code>Cor</code> is the correlation matrix). In that case, outcomes with the same value of the <code>id</code> variable receive the same value for the random effect, while outcomes with different values of the <code>id</code> variable receive values that are correlated as specified in the corresponding correlation matrix given via the <code>R</code> argument. The column/row names of the correlation matrix given via the <code>R</code> argument must therefore correspond to the unique values of the <code>id</code> variable. When the <code>random</code> argument contains multiple formulas of the form <code>~ 1 | id</code>, one can specify known correlation matrices for none, some, or all of those terms (e.g., with <code>random = list(~ 1 | id1, ~ 1 | id2)</code>, one could specify <code>R = list(id1 = Cor1)</code> or <code>R = list(id1 = Cor1, id2 = Cor2)</code>, where <code>Cor1</code> and <code>Cor2</code> are the correlation matrices corresponding to the grouping variables <code>id1</code> and <code>id2</code>, respectively).
</p>
<p>Such a random effect with a known (or at least approximately known) correlation structure is useful in a variety of contexts. For example, such a component can be used to account for the correlations induced by the shared phylogenetic history among organisms (e.g., plants, fungi, animals). In that case, <code>~ 1 | species</code> is used to specify the species and argument <code>R</code> is used to specify the phylogenetic correlation matrix of the species studied in the meta-analysis. The corresponding variance component then indicates how much variance/heterogeneity is attributable to the specified phylogeny. See Nakagawa and Santos (2012) for more details. As another example, in a genetic meta-analysis studying disease association for several single nucleotide polymorphisms (SNPs), linkage disequilibrium (LD) among the SNPs can induce an approximately known degree of correlation among the effects/outcomes. In that case, <code>~ 1 | snp</code> could be used to specify the SNPs and <code>R</code> the corresponding LD correlation matrix for the SNPs included in the meta-analysis.
</p>
<p>The <code>Rscale</code> argument controls how matrices specified via the <code>R</code> argument are scaled. With <code>Rscale="none"</code> (or <code>Rscale=0</code> or <code>Rscale=FALSE</code>), no scaling is used. With <code>Rscale="cor"</code> (or <code>Rscale=1</code> or <code>Rscale=TRUE</code>), the <code><a href="stats.html#topic+cov2cor">cov2cor</a></code> function is used to ensure that the matrices are correlation matrices (assuming they were covariance matrices to begin with). With <code>Rscale="cor0"</code> (or <code>Rscale=2</code>), first <code><a href="stats.html#topic+cov2cor">cov2cor</a></code> is used and then the elements of each correlation matrix are scaled with \((R - \min(R)) / (1 - \min(R))\) (this ensures that a correlation of zero in a phylogenetic correlation matrix corresponds to the split at the root node of the tree comprising the species that are actually analyzed). Finally, <code>Rscale="cov0"</code> (or <code>Rscale=3</code>) only rescales with \(R - \min(R)\) (which ensures that a phylogenetic covariance matrix is rooted at the lowest split).
</p>
<p>See <code><a href="metadat.html#topic+dat.moura2021">dat.moura2021</a></code> and <code><a href="metadat.html#topic+dat.lim2014">dat.lim2014</a></code> for examples of meta-analyses with phylogenetic correlation structures.
</p>
<p>Together with the variance-covariance matrix of the sampling errors (i.e., \(V\)), the specified random effects structure of the model implies a particular &lsquo;marginal&rsquo; variance-covariance matrix of the observed effect sizes or outcomes. Once estimates of the variance components (i.e., of the \(\sigma^2\), \(\tau^2\), \(\rho\), \(\gamma^2\), and/or \(\phi\) values) have been obtained (either using maximum likelihood or restricted maximum likelihood estimation), the estimated marginal variance-covariance matrix can be constructed (denoted by \(M\)). The model coefficients (i.e., \(\beta\)) are then estimated with \(b = (X'WX')^{-1} X'Wy\), where \(W = M^{-1}\) is the weight matrix. With the <code>W</code> argument, one can again specify user-defined weights (or a weight matrix).
</p>



<h4>Fixing Variance/Correlation Components</h4>

<p>Arguments <code>sigma2</code>, <code>tau2</code>, <code>rho</code>, <code>gamma2</code>, and <code>phi</code> can be used to fix particular variance/correlation components at a given value. This is useful for sensitivity analyses (e.g., for plotting the regular/restricted log-likelihood as a function of a particular variance/correlation component), likelihood ratio tests, or for imposing a desired variance-covariance structure on the data.
</p>
<p>For example, if <code>random = list(~ 1 | id1, ~ 1 | id2)</code> or <code>random = ~ 1 | id1/id2</code>, then <code>sigma2</code> must be of length 2 (corresponding to \(\sigma^2_1\) and \(\sigma^2_2\)) and a fixed value can be assigned to either or both variance components. Setting a particular component to <code>NA</code> means that the component will be estimated by the function (e.g., <code>sigma2=c(0,NA)</code> would fix \(\sigma^2_1\) to 0 and estimate \(\sigma^2_2\)).
</p>
<p>Argument <code>tau2</code> is only relevant when the <code>random</code> argument contains an <code>~ inner | outer</code> formula. In that case, if the <code>tau2</code> argument is used, it must be either of length 1 (for <code>"CS"</code>, <code>"ID"</code>, <code>"AR"</code>, <code>"CAR"</code>, or one of the spatial correlation structures) or of the same length as the number of unique values of the <code>inner</code> variable (for <code>"HCS"</code>, <code>"DIAG"</code>, <code>"UN"</code>, or <code>"HAR"</code>). A numeric value in the <code>tau2</code> argument then fixes the corresponding variance component to that value, while <code>NA</code> means that the component will be estimated. Similarly, if argument <code>rho</code> is used, it must be either of length 1 (for <code>"CS"</code>, <code>"HCS"</code>, <code>"AR"</code>, <code>"HAR"</code>, or one of the spatial correlation structures) or of length \(J(J-1)/2\) (for <code>"UN"</code>), where \(J\) denotes the number of unique values of the <code>inner</code> variable. Again, a numeric value fixes the corresponding correlation, while <code>NA</code> means that the correlation will be estimated. For example, with <code>struct="CS"</code> and <code>rho=0</code>, the variance-covariance matrix of the <code>inner</code> variable will be diagonal with \(\tau^2\) along the diagonal. For <code>struct="UN"</code>, the values specified under <code>rho</code> should be given in column-wise order (e.g., for an <code>inner</code> variable with four levels, the order would be \(\rho_{21}\), \(\rho_{31}\), \(\rho_{41}\), \(\rho_{32}\), \(\rho_{42}\), \(\rho_{43}\)).
</p>
<p>Similarly, arguments <code>gamma2</code> and <code>phi</code> are only relevant when the <code>random</code> argument contains a second <code>~ inner | outer</code> formula. The arguments then work exactly as described above.
</p>



<h4>Omnibus Test of Moderators</h4>

<p>For models including moderators, an omnibus test of all model coefficients is conducted that excludes the intercept (the first coefficient) if it is included in the model. If no intercept is included in the model, then the omnibus test includes all coefficients in the model including the first. Alternatively, one can manually specify the indices of the coefficients to test via the <code>btt</code> (&lsquo;betas to test&rsquo;) argument (i.e., to test \(\mbox{H}_0{:}\; \beta_{j \in \texttt{btt}} = 0\), where \(\beta_{j \in \texttt{btt}}\) is the set of coefficients to be tested). For example, with <code>btt=c(3,4)</code>, only the third and fourth coefficients from the model are included in the test (if an intercept is included in the model, then it corresponds to the first coefficient in the model). Instead of specifying the coefficient numbers, one can specify a string for <code>btt</code>. In that case, <code><a href="base.html#topic+grep">grep</a></code> will be used to search for all coefficient names that match the string. The omnibus test is called the \(Q_M\)-test and follows asymptotically a chi-square distribution with \(m\) degrees of freedom (with \(m\) denoting the number of coefficients tested) under the null hypothesis (that the true value of all coefficients tested is equal to 0).
</p>



<h4>Categorical Moderators</h4>

<p>Categorical moderator variables can be included in the model via the <code>mods</code> argument in the same way that appropriately (dummy) coded categorical variables can be included in linear models. One can either do the dummy coding manually or use a model formula together with the <code><a href="base.html#topic+factor">factor</a></code> function to automate the coding (note that string/character variables in a model formula are automatically converted to factors).
</p>



<h4>Tests and Confidence Intervals</h4>

<p>By default, tests of individual coefficients in the model (and the corresponding confidence intervals) are based on a standard normal distribution, while the omnibus test is based on a chi-square distribution (see above). As an alternative, one can set <code>test="t"</code>, in which case tests of individual coefficients and confidence intervals are based on a t-distribution with \(k-p\) degrees of freedom, while the omnibus test then uses an F-distribution with \(m\) and \(k-p\) degrees of freedom (with \(k\) denoting the total number of estimates included in the analysis and \(p\) the total number of model coefficients including the intercept if it is present). Note that <code>test="t"</code> is not the same as <code>test="knha"</code> in <code><a href="#topic+rma.uni">rma.uni</a></code>, as no adjustment to the standard errors of the estimated coefficients is made.
</p>
<p>The method for calculating the (denominator) degrees of freedom described above (which corresponds to <code>dfs="residual"</code>) is quite simplistic and may lead to tests with inflated Type I error rates and confidence intervals that are too narrow on average. As an alternative, one can set <code>dfs="contain"</code> (which automatically also sets <code>test="t"</code>), in which case the degrees of freedom for the test of a particular model coefficient, \(b_j\), are determined by checking whether \(x_j\), the corresponding column of the model matrix \(X\), varies at the level corresponding to a particular random effect in the model. If such a random effect can be found, then the degrees of freedom are set to \(l-p\), where \(l\) denotes the number of unique values of this random effect (i.e., for an <code>~ 1 | id</code> term, the number of unique values of the <code>id</code> variable and for an <code>~ inner | outer</code> term, the number of unique values of the <code>outer</code> variable). If no such random effect can be found, then \(k-p\) is used as the degrees of freedom. For the omnibus F-test, the minimum of the degrees of freedom of all coefficients involved in the test is used as the denominator degrees of freedom. This approach for calculating the degrees of freedom should often lead to tests with better control of the Type I error rate and confidence intervals with closer to nominal coverage rates (see also <a href="#topic+misc-recs">here</a>).
</p>
<p>One can also set <code>dfs</code> to a numeric vector with the desired values for the degrees of freedom for testing the model coefficients (e.g., if some other method for determining the degrees of freedom was used).
</p>



<h4>Tests and Confidence Intervals for Variance/Correlation Components</h4>

<p>Depending on the random effects structure specified, the model may include one or multiple variance/correlation components. Profile likelihood confidence intervals for such components can be obtained using the <code><a href="#topic+confint.rma.mv">confint</a></code> function. Corresponding likelihood ratio tests can be obtained using the <code><a href="#topic+anova.rma">anova</a></code> function (by comparing two models where the size of the component to be tested is constrained to some null value in the reduced model). It is also always a good idea to examine plots of the (restricted) log-likelihood as a function of the variance/correlation components in the model using the <code><a href="#topic+profile.rma.mv">profile</a></code> function to check for parameter identifiability (see &lsquo;Note&rsquo;).
</p>



<h4>Test for (Residual) Heterogeneity</h4>

<p>A test for (residual) heterogeneity is automatically carried out by the function. Without moderators in the model, this test is the generalized/weighted least squares extension of Cochran's \(Q\)-test, which tests whether the variability in the observed effect sizes or outcomes is larger than one would expect based on sampling variability (and the given covariances among the sampling errors) alone. A significant test suggests that the true effects/outcomes are heterogeneous. When moderators are included in the model, this is the \(Q_E\)-test for residual heterogeneity, which tests whether the variability in the observed effect sizes or outcomes that is not accounted for by the moderators included in the model is larger than one would expect based on sampling variability (and the given covariances among the sampling errors) alone.
</p>



<h4>Var-Cov Matrix of the Variance/Correlation Component Estimates</h4>

<p>In some cases, one might want to obtain the variance-covariance matrix of the variance/correlation component estimates (i.e., of the estimated \(\sigma^2\), \(\tau^2\), \(\rho\), \(\gamma^2\), \(\phi\) values). The function will try to calculate this matrix when <code>cvvc=TRUE</code> (or equivalently, when <code>cvvc="varcor"</code>). This is done by inverting the Hessian, which is numerically approximated using the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function from the <code>numDeriv</code> package. Note that these computations may not be numerically stable, especially when the estimates are close to their parameter bounds and/or the likelihood surface is relatively flat around its maximum. When <code>struct="UN"</code>, one can also set <code>cvvc="varcov"</code> in which case the variance-covariance matrix is given for the variance and covariance components (instead of the correlation components).
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.mv","rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated coefficients of the model.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>estimated \(\sigma^2\) value(s).</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated \(\tau^2\) value(s).</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>estimated \(\rho\) value(s).</p>
</td></tr>
<tr><td><code>gamma2</code></td>
<td>
<p>estimated \(\gamma^2\) value(s).</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>estimated \(\phi\) value(s).</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of observed effect sizes or outcomes included in the analysis.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>number of coefficients in the model (including the intercept).</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of coefficients included in the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QE</code></td>
<td>
<p>test statistic of the test for (residual) heterogeneity.</p>
</td></tr>
<tr><td><code>QEp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>int.only</code></td>
<td>
<p>logical that indicates whether the model is an intercept-only model.</p>
</td></tr>
<tr><td><code>yi</code>, <code>V</code>, <code>X</code></td>
<td>
<p>the vector of outcomes, the corresponding variance-covariance matrix of the sampling errors, and the model matrix.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>the estimated marginal variance-covariance matrix of the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code>fit.stats</code></td>
<td>
<p>a list with the log-likelihood, deviance, AIC, BIC, and AICc values.</p>
</td></tr>
<tr><td><code>vvc</code></td>
<td>
<p>variance-covariance matrix of the variance/correlation component estimates (<code>NA</code> when <code>cvvc=FALSE</code>).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.mv">print</a></code> function. If fit statistics should also be given, use <code><a href="#topic+summary.rma">summary</a></code> (or use the <code><a href="#topic+fitstats.rma">fitstats</a></code> function to extract them). Full versus reduced model comparisons in terms of fit statistics and likelihood ratio tests can be obtained with <code><a href="#topic+anova.rma">anova</a></code>. Wald-type tests for sets of model coefficients or linear combinations thereof can be obtained with the same function. Tests and confidence intervals based on (cluster) robust methods can be obtained with <code><a href="#topic+robust.rma.mv">robust</a></code>.
</p>
<p>Predicted/fitted values can be obtained with <code><a href="#topic+predict.rma">predict</a></code> and <code><a href="#topic+fitted.rma">fitted</a></code>. For best linear unbiased predictions, see <code><a href="#topic+ranef.rma.mv">ranef</a></code>.
</p>
<p>The <code><a href="#topic+residuals.rma">residuals</a></code>, <code><a href="#topic+rstandard.rma.mv">rstandard</a></code>, and <code><a href="#topic+rstudent.rma.mv">rstudent</a></code> functions extract raw and standardized residuals. See <code><a href="#topic+influence.rma.mv">influence</a></code> for additional model diagnostics (e.g., to determine influential studies). For models with moderators, variance inflation factors can be obtained with <code><a href="#topic+vif.rma">vif</a></code>.
</p>
<p>Confidence intervals for any variance/correlation components in the model can be obtained with <code><a href="#topic+confint.rma.mv">confint</a></code>.
</p>
<p>For random/mixed-effects models, the <code><a href="#topic+profile.rma.mv">profile</a></code> function can be used to obtain a plot of the (restricted) log-likelihood as a function of a specific variance/correlation component of the model. For models with moderators, <code><a href="#topic+regplot.rma">regplot</a></code> draws scatter plots / bubble plots, showing the (marginal) relationship between the observed outcomes and a selected moderator from the model.
</p>
<p>Other extractor functions include <code><a href="#topic+coef.rma">coef</a></code>, <code><a href="#topic+vcov.rma">vcov</a></code>, <code><a href="#topic+logLik.rma">logLik</a></code>, <code><a href="#topic+deviance.rma">deviance</a></code>, <code><a href="#topic+AIC.rma">AIC</a></code>, <code><a href="#topic+BIC.rma">BIC</a></code>, <code><a href="#topic+hatvalues.rma.mv">hatvalues</a></code>, and <code><a href="#topic+weights.rma.mv">weights</a></code>.
</p>


<h3>Note</h3>

<p>Argument <code>V</code> also accepts a list of variance-covariance matrices for the observed effect sizes or outcomes. From the list elements, the full (block diagonal) variance-covariance matrix is then automatically constructed. For this to work correctly, the list elements must be in the same order as the observed outcomes.
</p>
<p>Model fitting is done via numerical optimization over the model parameters. By default, <code><a href="stats.html#topic+nlminb">nlminb</a></code> is used for the optimization. One can also chose a different optimizer from <code><a href="stats.html#topic+optim">optim</a></code> via the <code>control</code> argument (e.g., <code>control=list(optimizer="BFGS")</code> or <code>control=list(optimizer="Nelder-Mead")</code>). Besides <code><a href="stats.html#topic+nlminb">nlminb</a></code> and one of the methods from <code><a href="stats.html#topic+optim">optim</a></code>, one can also choose one of the optimizers from the <code>minqa</code> package (i.e., <code><a href="minqa.html#topic+uobyqa">uobyqa</a></code>, <code><a href="minqa.html#topic+newuoa">newuoa</a></code>, or <code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>), one of the (derivative-free) algorithms from the <code><a href="nloptr.html#topic+nloptr">nloptr</a></code> package, the Newton-type algorithm implemented in <code><a href="stats.html#topic+nlm">nlm</a></code>, the various algorithms implemented in the <code>dfoptim</code> package (<code><a href="dfoptim.html#topic+hjk">hjk</a></code> for the Hooke-Jeeves, <code><a href="dfoptim.html#topic+nmk">nmk</a></code> for the Nelder-Mead, and <code><a href="dfoptim.html#topic+mads">mads</a></code> for the Mesh Adaptive Direct Searches algorithm), the quasi-Newton type optimizers <code><a href="ucminf.html#topic+ucminf">ucminf</a></code> and <code><a href="lbfgsb3c.html#topic+lbfgsb3c">lbfgsb3c</a></code> and the subspace-searching simplex algorithm <code><a href="subplex.html#topic+subplex">subplex</a></code> from the packages of the same name, the Barzilai-Borwein gradient decent method implemented in <code><a href="BB.html#topic+BBoptim">BBoptim</a></code>, or the parallelized version of the L-BFGS-B algorithm implemented in <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code> from the package of the same name.
</p>
<p>The optimizer name must be given as a character string (i.e., in quotes). Additional control parameters can be specified via the <code>control</code> argument (e.g., <code>control=list(iter.max=1000, rel.tol=1e-8)</code>). For <code><a href="nloptr.html#topic+nloptr">nloptr</a></code>, the default is to use the BOBYQA implementation from that package with a relative convergence criterion of <code>1e-8</code> on the function value (i.e., log-likelihood), but this can be changed via the <code>algorithm</code> and <code>ftop_rel</code> arguments (e.g., <code>control=list(optimizer="nloptr", algorithm="NLOPT_LN_SBPLX", ftol_rel=1e-6)</code>). For <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code>, the control argument <code>ncpus</code> can be used to specify the number of cores to use for the parallelization (e.g., <code>control=list(optimizer="optimParallel", ncpus=2)</code>). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
<p>At the moment, the starting values are not chosen in a terribly clever way and could be far off. As a result, the optimizer may be slow to converge or may even get stuck at a local maximum. One can set the starting values manually for the various variance/correlation components in the model via the <code>control</code> argument by specifying the vectors <code>sigma2.init</code>, <code>tau2.init</code>, <code>rho.init</code>, <code>gamma2.init</code>, and/or <code>phi.init</code> as needed. Especially for complex models, it is a good idea to try out different starting values to make sure that the same estimates are obtained.
</p>
<p>Information on the progress of the optimization algorithm can be obtained by setting <code>verbose=TRUE</code> (this won't work when using parallelization). Since fitting complex models with many random effects can be computationally expensive, this option is useful to determine how the model fitting is progressing. One can also set <code>verbose</code> to an integer (<code>verbose=2</code> yields even more information and <code>verbose=3</code> also sets <code>option(warn=1)</code> temporarily).
</p>
<p>Whether a particular variance/correlation component is actually identifiable needs to be carefully examined when fitting complex models. The function does some limited checking internally to fix variances and/or correlations at zero when it is clear that insufficient information is available to estimate a particular parameter (e.g., if a particular factor has only a single level, the corresponding variance component cannot be estimated). However, it is strongly advised in general to do post model fitting checks to make sure that the likelihood surface around the ML/REML estimates is not flat for some combination of the parameter estimates (which would imply that the estimates are essentially arbitrary). For example, one can plot the (restricted) log-likelihood as a function of each variance/correlation component in the model to make sure that each profile plot shows a clear peak at the corresponding ML/REML estimate. The <code><a href="#topic+profile.rma.mv">profile</a></code> function can be used for this purpose.
</p>
<p>Finally, note that the model fitting is not done in a very efficient manner at the moment, which is partly a result of allowing for crossed random effects and correlations across the entire dataset (e.g., when using the <code>R</code> argument). As a result, the function works directly with the entire \(k \times k\) (marginal) variance-covariance matrix of the observed effect sizes or outcomes (instead of working with smaller blocks in a block diagonal structure). As a result, model fitting can be slow for large \(k\). However, when the variance-covariance structure is actually sparse, a lot of speed can be gained by setting <code>sparse=TRUE</code>, in which case sparse matrix objects are used (via the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package). Also, when model fitting appears to be slow, setting <code>verbose=TRUE</code> is useful to obtain information on how the model fitting is progressing.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Berkey, C. S., Hoaglin, D. C., Antczak-Bouckoms, A., Mosteller, F., &amp; Colditz, G. A. (1998). Meta-analysis of multiple outcomes by regression with random effects. <em>Statistics in Medicine</em>, <b>17</b>(22), 2537&ndash;2550. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(sici)1097-0258(19981130)17:22&lt;2537::aid-sim953&gt;3.0.co;2-c&#8288;</code>
</p>
<p>Gleser, L. J., &amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 357&ndash;376). New York: Russell Sage Foundation.
</p>
<p>van Houwelingen, H. C., Arends, L. R., &amp; Stijnen, T. (2002). Advanced methods in meta-analysis: Multivariate approach and meta-regression. <em>Statistics in Medicine</em>, <b>21</b>(4), 589&ndash;624. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1040&#8288;</code>
</p>
<p>Ishak, K. J., Platt, R. W., Joseph, L., Hanley, J. A., &amp; Caro, J. J. (2007). Meta-analysis of longitudinal studies. <em>Clinical Trials</em>, <b>4</b>(5), 525&ndash;539. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1740774507083567&#8288;</code>
</p>
<p>Kalaian, H. A., &amp; Raudenbush, S. W. (1996). A multivariate mixed linear model for meta-analysis. <em>Psychological Methods</em>, <b>1</b>(3), 227-235. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.1.3.227&#8288;</code>
</p>
<p>Konstantopoulos, S. (2011). Fixed effects and variance components estimation in three-level meta-analysis. <em>Research Synthesis Methods</em>, <b>2</b>(1), 61&ndash;76. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.35&#8288;</code>
</p>
<p>Lajeunesse, M. J. (2011). On the meta-analysis of response ratios for studies with correlated and multi-group designs. <em>Ecology</em>, <b>92</b>(11), 2049&ndash;2055. <code style="white-space: pre;">&#8288;https://doi.org/10.1890/11-0423.1&#8288;</code>
</p>
<p>Nakagawa, S., &amp; Santos, E. S. A. (2012). Methodological issues and advances in biological meta-analysis. <em>Evolutionary Ecology</em>, <b>26</b>(5), 1253&ndash;1274. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/s10682-012-9555-5&#8288;</code>
</p>
<p>Pinheiro, J. C., &amp; Bates, D. (2000). <em>Mixed-effects models in S and S-PLUS</em>. New York: Springer.
</p>
<p>Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix. <em>Psychological Bulletin</em>, <b>87</b>(2), 245&ndash;251. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.87.2.245&#8288;</code>
</p>
<p>Salanti, G., Higgins, J. P. T., Ades, A. E., &amp; Ioannidis, J. P. A. (2008). Evaluation of networks of randomized trials. <em>Statistical Methods in Medical Research</em>, <b>17</b>(3), 279&ndash;301. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/0962280207080643&#8288;</code>
</p>
<p>Trikalinos, T. A., &amp; Olkin, I. (2012). Meta-analysis of effect sizes reported at multiple time points: A multivariate approach. <em>Clinical Trials</em>, <b>9</b>(5), 610&ndash;620. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1740774512453218&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Wei, Y., &amp; Higgins, J. P. (2013). Estimating within-study covariances in multivariate meta-analysis with multiple outcomes. <em>Statistics in Medicine</em>, <b>32</b>(7), 1191&ndash;1205. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.5679&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.glmm">rma.glmm</a></code> for other model fitting functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
dat

### fit random-effects model using rma.uni()
rma(yi, vi, data=dat)

### fit random-effects model using rma.mv()
### note: sigma^2 in this model is the same as tau^2 from the previous model
rma.mv(yi, vi, random = ~ 1 | trial, data=dat)

### change data into long format
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, append=FALSE)

### set levels of group variable ("exp" = experimental/vaccinated; "con" = control/non-vaccinated)
levels(dat.long$group) &lt;- c("exp", "con")

### set "con" to reference level
dat.long$group &lt;- relevel(dat.long$group, ref="con")

### calculate log odds and corresponding sampling variances
dat.long &lt;- escalc(measure="PLO", xi=out1, mi=out2, data=dat.long)
dat.long

### fit bivariate random-effects model using rma.mv()
res &lt;- rma.mv(yi, vi, mods = ~ group, random = ~ group | study, struct="UN", data=dat.long)
res
</code></pre>

<hr>
<h2 id='rma.peto'>Meta-Analysis via Peto's Method</h2><span id='topic+rma.peto'></span>

<h3>Description</h3>

<p>Function to fit equal-effects models to \(2 \times 2\) table data via Peto's method. See below and the introduction to the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> for more details on these models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rma.peto(ai, bi, ci, di, n1i, n2i,
         data, slab, subset,
         add=1/2, to="only0", drop00=TRUE,
         level=95, verbose=FALSE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rma.peto_+3A_ai">ai</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper left cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_bi">bi</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper right cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_ci">ci</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower left cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_di">di</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower right cell). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_n1i">n1i</code></td>
<td>
<p>vector with the group sizes or row totals (first group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_n2i">n2i</code></td>
<td>
<p>vector with the group sizes or row totals (second group). See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_data">data</code></td>
<td>
<p>optional data frame containing the data supplied to the function.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be used for the analysis.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_add">add</code></td>
<td>
<p>non-negative number to specify the amount to add to zero cells when calculating the observed effect sizes of the individual studies. Can also be a vector of two numbers, where the first number is used in the calculation of the observed effect sizes and the second number is used when applying Peto's method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_to">to</code></td>
<td>
<p>character string to specify when the values under <code>add</code> should be added (either <code>"only0"</code>, <code>"all"</code>, <code>"if0all"</code>, or <code>"none"</code>). Can also be a character vector, where the first string again applies when calculating the observed effect sizes or outcomes and the second string when applying Peto's method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_drop00">drop00</code></td>
<td>
<p>logical to specify whether studies with no cases (or only cases) in both groups should be dropped when calculating the observed effect sizes or outcomes (the outcomes for such studies are set to <code>NA</code>). Can also be a vector of two logicals, where the first applies to the calculation of the observed effect sizes or outcomes and the second when applying Peto's method. See below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="rma.peto_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specifying the Data</h4>

<p>The studies are assumed to provide data in terms of \(2 \times 2\) tables of the form:
</p>

<table>
<tr>
 <td style="text-align: left;">
              </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
      group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies and <code>n1i</code> and <code>n2i</code> the row totals. For example, in a set of randomized clinical trials (RCTs) or cohort studies, group 1 and group 2 may refer to the treatment/exposed and placebo/control/non-exposed group, respectively, with outcome 1 denoting some event of interest (e.g., death) and outcome 2 its complement. In a set of case-control studies, group 1 and group 2 may refer to the group of cases and the group of controls, with outcome 1 denoting, for example, exposure to some risk factor and outcome 2 non-exposure.
</p>



<h4>Peto's Method</h4>

<p>An approach for aggregating data of this type was suggested by Peto (see Yusuf et al., 1985). The method provides a weighted estimate of the (log) odds ratio under an equal-effects model. The method is particularly advantageous when the event of interest is rare, but it should only be used when the group sizes within the individual studies are not too dissimilar and the effect sizes are generally small (Greenland &amp; Salvan, 1990; Sweeting et al., 2004; Bradburn et al., 2007). Note that the printed results are given both in terms of the log and the raw units (for easier interpretation).
</p>



<h4>Observed Effect Sizes or Outcomes of the Individual Studies</h4>

<p>Peto's method itself does not require the calculation of the observed log odds ratios of the individual studies and directly makes use of the cell frequencies in the \(2 \times 2\) tables. Zero cells are not a problem (except in extreme cases, such as when one of the two outcomes never occurs in any of the tables). Therefore, it is unnecessary to add some constant to the cell counts when there are zero cells.
</p>
<p>However, for plotting and various other functions, it is necessary to calculate the observed log odds ratios for the \(k\) studies. Here, zero cells can be problematic, so adding a constant value to the cell counts ensures that all \(k\) values can be calculated. The <code>add</code> and <code>to</code> arguments are used to specify what value should be added to the cell frequencies and under what circumstances when calculating the observed log odds ratios and when applying Peto's method. Similarly, the <code>drop00</code> argument is used to specify how studies with no cases (or only cases) in both groups should be handled. The documentation of the <code><a href="#topic+escalc">escalc</a></code> function explains how the <code>add</code>, <code>to</code>, and <code>drop00</code> arguments work. If only a single value for these arguments is specified (as per default), then these values are used when calculating the observed log odds ratios and no adjustment to the cell counts is made when applying Peto's method. Alternatively, when specifying two values for these arguments, the first value applies when calculating the observed log odds ratios and the second value when applying Peto's method.
</p>
<p>Note that <code>drop00</code> is set to <code>TRUE</code> by default. Therefore, the observed log odds ratios for studies where <code>ai=ci=0</code> or <code>bi=di=0</code> are set to <code>NA</code>. When applying Peto's method, such studies are not explicitly dropped (unless the second value of <code>drop00</code> argument is also set to <code>TRUE</code>), but this is practically not necessary, as they do not actually influence the results (assuming no adjustment to the cell counts are made when applying Peto's method).
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.peto","rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>aggregated log odds ratio.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard error of the aggregated value.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the aggregated value.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence interval.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence interval.</p>
</td></tr>
<tr><td><code>QE</code></td>
<td>
<p>test statistic of the test for heterogeneity.</p>
</td></tr>
<tr><td><code>QEp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of studies included in the analysis.</p>
</td></tr>
<tr><td><code>yi</code>, <code>vi</code></td>
<td>
<p>the vector of individual log odds ratios and corresponding sampling variances.</p>
</td></tr>
<tr><td><code>fit.stats</code></td>
<td>
<p>a list with the log-likelihood, deviance, AIC, BIC, and AICc values under the unrestricted and restricted likelihood.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.peto">print</a></code> function. If fit statistics should also be given, use <code><a href="#topic+summary.rma">summary</a></code> (or use the <code><a href="#topic+fitstats.rma">fitstats</a></code> function to extract them).
</p>
<p>The <code><a href="#topic+residuals.rma">residuals</a></code>, <code><a href="#topic+rstandard.rma.peto">rstandard</a></code>, and <code><a href="#topic+rstudent.rma.peto">rstudent</a></code> functions extract raw and standardized residuals. Leave-one-out diagnostics can be obtained with <code><a href="#topic+leave1out.rma.peto">leave1out</a></code>.
</p>
<p>Forest, funnel, radial, L'Abb√©, and Baujat plots can be obtained with <code><a href="#topic+forest.rma">forest</a></code>, <code><a href="#topic+funnel.rma">funnel</a></code>, <code><a href="#topic+radial.rma">radial</a></code>, <code><a href="#topic+labbe.rma">labbe</a></code>, and <code><a href="#topic+baujat.rma">baujat</a></code>. The <code><a href="#topic+qqnorm.rma.peto">qqnorm</a></code> function provides normal QQ plots of the standardized residuals. One can also just call <code><a href="#topic+plot.rma.peto">plot</a></code> on the fitted model object to obtain various plots at once.
</p>
<p>A cumulative meta-analysis (i.e., adding one observation at a time) can be obtained with <code><a href="#topic+cumul.rma.peto">cumul</a></code>.
</p>
<p>Other extractor functions include <code><a href="#topic+coef.rma">coef</a></code>, <code><a href="#topic+vcov.rma">vcov</a></code>, <code><a href="#topic+logLik.rma">logLik</a></code>, <code><a href="#topic+deviance.rma">deviance</a></code>, <code><a href="#topic+AIC.rma">AIC</a></code>, and <code><a href="#topic+BIC.rma">BIC</a></code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Bradburn, M. J., Deeks, J. J., Berlin, J. A., &amp; Localio, A. R. (2007). Much ado about nothing: A comparison of the performance of meta-analytical methods with rare events. <em>Statistics in Medicine</em>, <b>26</b>(1), 53&ndash;77. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2528&#8288;</code>
</p>
<p>Greenland, S., &amp; Salvan, A. (1990). Bias in the one-step method for pooling study results. <em>Statistics in Medicine</em>, <b>9</b>(3), 247&ndash;252. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780090307&#8288;</code>
</p>
<p>Sweeting, M. J., Sutton, A. J., &amp; Lambert, P. C. (2004). What to add to nothing? Use and avoidance of continuity corrections in meta-analysis of sparse data. <em>Statistics in Medicine</em>, <b>23</b>(9), 1351&ndash;1375. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1761&#8288;</code>
</p>
<p>Yusuf, S., Peto, R., Lewis, J., Collins, R., &amp; Sleight, P. (1985). Beta blockade during and after myocardial infarction: An overview of the randomized trials. <em>Progress in Cardiovascular Disease</em>, <b>27</b>(5), 335&ndash;371. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/s0033-0620(85)80003-7&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for other model fitting functions.
</p>
<p><code><a href="metadat.html#topic+dat.collins1985a">dat.collins1985a</a></code>, <code><a href="metadat.html#topic+dat.collins1985b">dat.collins1985b</a></code>, and <code><a href="metadat.html#topic+dat.yusuf1985">dat.yusuf1985</a></code> for further examples of the use of the <code>rma.peto</code> function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### meta-analysis of the (log) odds ratios using Peto's method
rma.peto(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
</code></pre>

<hr>
<h2 id='rma.uni'>Meta-Analysis via Linear (Mixed-Effects) Models</h2><span id='topic+rma.uni'></span><span id='topic+rma'></span>

<h3>Description</h3>

<p>Function to fit meta-analytic equal-, fixed-, and random-effects models and (mixed-effects) meta-regression models using a linear (mixed-effects) model framework. See below and the introduction to the <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> for more details on these models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rma.uni(yi, vi, sei, weights, ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
        m1i, m2i, sd1i, sd2i, xi, mi, ri, ti, fi, pi, sdi, r2i, ni, mods, scale,
        measure="GEN", intercept=TRUE, data, slab, subset,
        add=1/2, to="only0", drop00=FALSE, vtype="LS",
        method="REML", weighted=TRUE, test="z",
        level=95, btt, att, tau2, verbose=FALSE, digits, control, ...)
rma(yi, vi, sei, weights, ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
        m1i, m2i, sd1i, sd2i, xi, mi, ri, ti, fi, pi, sdi, r2i, ni, mods, scale,
        measure="GEN", intercept=TRUE, data, slab, subset,
        add=1/2, to="only0", drop00=FALSE, vtype="LS",
        method="REML", weighted=TRUE, test="z",
        level=95, btt, att, tau2, verbose=FALSE, digits, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rma.uni_+3A_yi">yi</code></td>
<td>
<p>vector of length \(k\) with the observed effect sizes or outcomes. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_vi">vi</code></td>
<td>
<p>vector of length \(k\) with the corresponding sampling variances. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_sei">sei</code></td>
<td>
<p>vector of length \(k\) with the corresponding standard errors (only relevant when not using <code>vi</code>). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_weights">weights</code></td>
<td>
<p>optional argument to specify a vector of length \(k\) with user-defined weights. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_ai">ai</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_bi">bi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_ci">ci</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_di">di</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_n1i">n1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_n2i">n2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_x1i">x1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_x2i">x2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_t1i">t1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_t2i">t2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_m1i">m1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_m2i">m2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_sd1i">sd1i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_sd2i">sd2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_xi">xi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_mi">mi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_ri">ri</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_ti">ti</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_fi">fi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_pi">pi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_sdi">sdi</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_r2i">r2i</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_ni">ni</code></td>
<td>
<p>see below and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_mods">mods</code></td>
<td>
<p>optional argument to include one or more moderators in the model. A single moderator can be given as a vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving a matrix with \(k\) rows and as many columns as there are moderator variables. Alternatively, a model <code><a href="#topic+formula">formula</a></code> can be used to specify the model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_scale">scale</code></td>
<td>
<p>optional argument to include one or more predictors for the scale part in a location-scale model. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_measure">measure</code></td>
<td>
<p>character string to specify the type of data supplied to the function. When <code>measure="GEN"</code> (default), the observed effect sizes or outcomes and corresponding sampling variances should be supplied to the function via the <code>yi</code> and <code>vi</code> arguments, respectively (instead of the sampling variances, one can supply the standard errors via the <code>sei</code> argument). Alternatively, one can set <code>measure</code> to one of the effect sizes or outcome measures described under the documentation for the <code><a href="#topic+escalc">escalc</a></code> function in which case one must specify the required data via the appropriate arguments (see <code><a href="#topic+escalc">escalc</a></code>).</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_intercept">intercept</code></td>
<td>
<p>logical to specify whether an intercept should be added to the model (the default is <code>TRUE</code>). Ignored when <code>mods</code> is a formula.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_data">data</code></td>
<td>
<p>optional data frame containing the data supplied to the function.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the \(k\) studies.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be used for the analysis.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_add">add</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_to">to</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_drop00">drop00</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_vtype">vtype</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_method">method</code></td>
<td>
<p>character string to specify whether an equal- or a random-effects model should be fitted. An equal-effects model is fitted when using <code>method="EE"</code>. A random-effects model is fitted by setting <code>method</code> equal to one of the following: <code>"DL"</code>, <code>"HE"</code>, <code>"HS"</code>, <code>"HSk"</code>, <code>"SJ"</code>, <code>"ML"</code>, <code>"REML"</code>, <code>"EB"</code>, <code>"PM"</code>, <code>"GENQ"</code>, <code>"PMM"</code>, or <code>"GENQM"</code>. The default is <code>"REML"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_weighted">weighted</code></td>
<td>
<p>logical to specify whether weighted (default) or unweighted estimation should be used to fit the model (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_test">test</code></td>
<td>
<p>character string to specify how test statistics and confidence intervals for the fixed effects should be computed. By default (<code>test="z"</code>), Wald-type tests and CIs are obtained, which are based on a standard normal distribution. When <code>test="t"</code>, a t-distribution is used instead. When <code>test="knha"</code>, the method by Knapp and Hartung (2003) is used. See &lsquo;Details&rsquo; and also <a href="#topic+misc-recs">here</a> for some recommended practices.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_level">level</code></td>
<td>
<p>numeric value between 0 and 100 to specify the confidence interval level (the default is 95; see <a href="#topic+misc-options">here</a> for details).</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_btt">btt</code></td>
<td>
<p>optional vector of indices to specify which coefficients to include in the omnibus test of moderators. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_att">att</code></td>
<td>
<p>optional vector of indices to specify which scale coefficients to include in the omnibus test. Only relevant for location-scale models. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_tau2">tau2</code></td>
<td>
<p>optional numeric value to specify the amount of (residual) heterogeneity in a random- or mixed-effects model (instead of estimating it). Useful for sensitivity analyses (e.g., for plotting results as a function of \(\tau^2\)). If unspecified, the value of \(\tau^2\) is estimated from the data.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>). Can also be an integer. Values &gt; 1 generate more verbose output. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is 4. See also <a href="#topic+misc-options">here</a> for further details on how to control the number of digits in the output.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_control">control</code></td>
<td>
<p>optional list of control values for the iterative estimation algorithms. If unspecified, default values are defined inside the function. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="rma.uni_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specifying the Data</h4>

<p>The function can be used in combination with any of the usual effect sizes or outcome measures used in meta-analyses (e.g., log risk ratios, log odds ratios, risk differences, mean differences, standardized mean differences, log transformed ratios of means, raw correlation coefficients, correlation coefficients transformed with Fisher's r-to-z transformation), or, more generally, any set of estimates (with corresponding sampling variances) one would like to analyze. Simply specify the observed effect sizes or outcomes via the <code>yi</code> argument and the corresponding sampling variances via the <code>vi</code> argument. Instead of specifying <code>vi</code>, one can specify the standard errors (the square root of the sampling variances) via the <code>sei</code> argument. The <code><a href="#topic+escalc">escalc</a></code> function can be used to compute a wide variety of effect sizes or outcome measures (and the corresponding sampling variances) based on summary statistics.
</p>
<p>Alternatively, the function can automatically calculate the values of a chosen effect size or outcome measure (and the corresponding sampling variances) when supplied with the necessary data. The <code><a href="#topic+escalc">escalc</a></code> function describes which effect sizes or outcome measures are currently implemented and what data/arguments should then be specified/used. The <code>measure</code> argument should then be set to the desired effect size or outcome measure.
</p>



<h4>Specifying the Model</h4>

<p>The function can be used to fit equal-, fixed-, and random-effects models, as well as (mixed-effects) meta-regression models including one or multiple moderators (the difference between the various models is described in detail on the introductory <span class="pkg"><a href="#topic+metafor-package">metafor-package</a></span> help page).
</p>
<p>Assuming the observed effect sizes or outcomes and corresponding sampling variances are supplied via the <code>yi</code> and <code>vi</code> arguments, an <em>equal-effects model</em> can be fitted with <code>rma(yi, vi, method="EE")</code>. Setting <code>method="FE"</code> fits a <em>fixed-effects model</em> (see <a href="#topic+misc-models">here</a> for a discussion of this model). Weighted estimation (with inverse-variance weights) is used by default. User-defined weights can be supplied via the <code>weights</code> argument. Unweighted estimation can be used by setting <code>weighted=FALSE</code> (which is the same as setting the weights equal to a constant).
</p>
<p>A <em>random-effects model</em> can be fitted with the same code but setting the <code>method</code> argument to one of the various estimators for the amount of heterogeneity:
</p>

<ul>
<li> <p><code>method="DL"</code> = DerSimonian-Laird estimator (DerSimonian &amp; Laird, 1986; Raudenbush, 2009),
</p>
</li>
<li> <p><code>method="HE"</code> = Hedges estimator (Hedges, 1983, 1992),
</p>
</li>
<li> <p><code>method="HS"</code> = Hunter-Schmidt estimator (Hunter &amp; Schmidt, 1990; Viechtbauer et al., 2015),
</p>
</li>
<li> <p><code>method="HSk"</code> = Hunter-Schmidt estimator with a small sample-size correction (Brannick et al., 2019),
</p>
</li>
<li> <p><code>method="SJ"</code> = Sidik-Jonkman estimator (Sidik &amp; Jonkman, 2005b, 2007),
</p>
</li>
<li> <p><code>method="ML"</code> = maximum likelihood estimator (Hardy &amp; Thompson, 1996; Raudenbush, 2009),
</p>
</li>
<li> <p><code>method="REML"</code> = restricted maximum likelihood estimator (Viechtbauer, 2005; Raudenbush, 2009)
</p>
</li>
<li> <p><code>method="EB"</code> = empirical Bayes estimator (Morris, 1983; Berkey et al. 1995),
</p>
</li>
<li> <p><code>method="PM"</code> = Paule-Mandel estimator (Paule &amp; Mandel, 1982; Viechtbauer et al., 2015),
</p>
</li>
<li> <p><code>method="GENQ"</code> = generalized Q-statistic estimator (DerSimonian &amp; Kacker, 2007; Jackson et al., 2014),
</p>
</li>
<li> <p><code>method="PMM"</code> = median-unbiased Paule-Mandel estimator (Viechtbauer, 2021),
</p>
</li>
<li> <p><code>method="GENQM"</code> = median-unbiased generalized Q-statistic estimator (Viechtbauer, 2021).
</p>
</li></ul>

<p>For a description of the various estimators, see Brannick et al. (2019), DerSimonian and Kacker (2007), Raudenbush (2009), Veroniki et al. (2016), Viechtbauer (2005), and Viechtbauer et al. (2015). Note that the Hedges estimator is also called the &lsquo;variance component estimator&rsquo; or &lsquo;Cochran estimator&rsquo;, the Sidik-Jonkman estimator is also called the &lsquo;model error variance estimator&rsquo;, the empirical Bayes estimator is actually identical to the Paule-Mandel estimator (Viechtbauer et al., 2015), and the generalized Q-statistic estimator is a general method-of-moments estimator (DerSimonian &amp; Kacker, 2007) requiring the specification of weights (the HE and DL estimators are just special cases with equal and inverse sampling variance weights, respectively). Finally, the two median-unbiased estimators are versions of the Paule-Mandel and generalized Q-statistic estimators that equate the respective estimating equations not to their expected values, but to the medians of their theoretical distributions (Viechtbauer, 2021).
</p>
<p>One or more moderators can be included in a model via the <code>mods</code> argument. A single moderator can be given as a (row or column) vector of length \(k\) specifying the values of the moderator. Multiple moderators are specified by giving an appropriate model matrix (i.e., \(X\)) with \(k\) rows and as many columns as there are moderator variables (e.g., <code>mods = cbind(mod1, mod2, mod3)</code>, where <code>mod1</code>, <code>mod2</code>, and <code>mod3</code> correspond to the names of the variables for three moderator variables). The intercept is added to the model matrix by default unless <code>intercept=FALSE</code>.
</p>
<p>Alternatively, one can use standard <code><a href="#topic+formula">formula</a></code> syntax to specify the model. In this case, the <code>mods</code> argument should be set equal to a one-sided formula of the form <code>mods = ~ model</code> (e.g., <code>mods = ~ mod1 + mod2 + mod3</code>). Interactions, polynomial terms, and factors can be easily added to the model in this manner. When specifying a model formula via the <code>mods</code> argument, the <code>intercept</code> argument is ignored. Instead, the inclusion/exclusion of the intercept is controlled by the specified formula (e.g., <code>mods = ~ mod1 + mod2 + mod3 - 1</code> would lead to the removal of the intercept).
</p>
<p>When the observed effect sizes or outcomes and corresponding sampling variances are supplied via the <code>yi</code> and <code>vi</code> (or <code>sei</code>) arguments, one can also specify moderators via the <code>yi</code> argument (e.g., <code>rma(yi ~ mod1 + mod2 + mod3, vi)</code>). In that case, the <code>mods</code> argument is ignored and the inclusion/exclusion of the intercept again is controlled by the specified formula.
</p>



<h4>Omnibus Test of Moderators</h4>

<p>For models including moderators, an omnibus test of all model coefficients is conducted that excludes the intercept (the first coefficient) if it is included in the model. If no intercept is included in the model, then the omnibus test includes all coefficients in the model including the first. Alternatively, one can manually specify the indices of the coefficients to test via the <code>btt</code> (&lsquo;betas to test&rsquo;) argument (i.e., to test \(\mbox{H}_0{:}\; \beta_{j \in \texttt{btt}} = 0\), where \(\beta_{j \in \texttt{btt}}\) is the set of coefficients to be tested). For example, with <code>btt=c(3,4)</code>, only the third and fourth coefficients from the model are included in the test (if an intercept is included in the model, then it corresponds to the first coefficient in the model). Instead of specifying the coefficient numbers, one can specify a string for <code>btt</code>. In that case, <code><a href="base.html#topic+grep">grep</a></code> will be used to search for all coefficient names that match the string. The omnibus test is called the \(Q_M\)-test and follows asymptotically a chi-square distribution with \(m\) degrees of freedom (with \(m\) denoting the number of coefficients tested) under the null hypothesis (that the true value of all coefficients tested is equal to 0).
</p>



<h4>Categorical Moderators</h4>

<p>Categorical moderator variables can be included in the model via the <code>mods</code> argument in the same way that appropriately (dummy) coded categorical variables can be included in linear models. One can either do the dummy coding manually or use a model formula together with the <code><a href="base.html#topic+factor">factor</a></code> function to automate the coding (note that string/character variables in a model formula are automatically converted to factors). An example to illustrate these different approaches is provided below.
</p>



<h4>Tests and Confidence Intervals</h4>

<p>By default, tests of individual coefficients in the model (and the corresponding confidence intervals) are based on a standard normal distribution, while the omnibus test is based on a chi-square distribution (see above). As an alternative, one can set <code>test="t"</code>, in which case tests of individual coefficients and confidence intervals are based on a t-distribution with \(k-p\) degrees of freedom, while the omnibus test then uses an F-distribution with \(m\) and \(k-p\) degrees of freedom (with \(k\) denoting the total number of estimates included in the analysis and \(p\) the total number of model coefficients including the intercept if it is present). Furthermore, when <code>test="knha"</code> (or equivalently, <code>test="hksj"</code>), the method by Hartung (1999), Sidik and Jonkman (2002), and Knapp and Hartung (2003) (the Knapp-Hartung method; also referred to as the Hartung-Knapp-Sidik-Jonkman method) is used, which applies an adjustment to the standard errors of the estimated coefficients (to account for the uncertainty in the estimate of the amount of (residual) heterogeneity) and uses t- and F-distributions as described above (see also <a href="#topic+misc-recs">here</a>). Finally, one can set <code>test="adhoc"</code>, in which case the Knapp-Hartung method is used, but with the restriction that the adjustment to the standard errors can never result in adjusted standard errors that are smaller than the unadjusted ones (see Jackson et al., 2017, section 4.3).
</p>



<h4>Test for (Residual) Heterogeneity</h4>

<p>A test for (residual) heterogeneity is automatically carried out by the function. Without moderators in the model, this is simply Cochran's \(Q\)-test (Cochran, 1954), which tests whether the variability in the observed effect sizes or outcomes is larger than would be expected based on sampling variability alone. A significant test suggests that the true effects/outcomes are heterogeneous. When moderators are included in the model, this is the \(Q_E\)-test for residual heterogeneity, which tests whether the variability in the observed effect sizes or outcomes not accounted for by the moderators included in the model is larger than would be expected based on sampling variability alone.
</p>



<h4>Location-Scale Models</h4>

<p>The function can also be used to fit so-called &lsquo;location-scale models&rsquo; (Viechtbauer &amp; L√≥pez-L√≥pez, 2022). In such models, one can specify not only predictors for the size of the average true outcome (i.e., for their &lsquo;location&rsquo;), but also predictors for the amount of heterogeneity in the outcomes (i.e., for their &lsquo;scale&rsquo;). The model is given by \[y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_{p'} x_{ip'} + u_i + \varepsilon_i,\] \[u_i \sim N(0, \tau_i^2), \; \varepsilon_i \sim N(0, v_i),\] \[\log(\tau_i^2) = \alpha_0 + \alpha_1 z_{i1} + \alpha_2 z_{i2} + \ldots + \alpha_{q'} z_{iq'},\] where \(x_{i1}, \ldots, x_{ip'}\) are the values of the \(p'\) predictor variables that may be related to the size of the average true outcome (letting \(p = p' + 1\) denote the total number of location coefficients in the model including the model intercept \(\beta_0\)) and \(z_{i1}, \ldots, z_{iq'}\) are the values of the \(q'\) scale variables that may be related to the amount of heterogeneity in the outcomes (letting \(q = q' + 1\) denote the total number of scale coefficients in the model including the model intercept \(\alpha_0\)). Location variables can be specified via the <code>mods</code> argument as described above (e.g., <code>mods = ~ mod1 + mod2 + mod3</code>). Scale variables can be specified via the <code>scale</code> argument (e.g., <code>scale = ~ var1 + var2 + var3</code>). A log link is used for specifying the relationship between the scale variables and the amount of heterogeneity so that \(\tau_i^2\) is guaranteed to be non-negative (one can also set (the undocumented) argument <code>link="identity"</code> to use an identity link, but this is more likely to lead to estimation problems). Estimates of the location and scale coefficients can be obtained either with maximum likelihood (<code>method="ML"</code>) or restricted maximum likelihood (<code>method="REML"</code>) estimation. An omnibus test of the scale coefficients is conducted as described above (where the <code>att</code> argument can be used to specify which scale coefficients to include in the test).
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.uni","rma")</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated coefficients of the model.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated amount of (residual) heterogeneity. Always <code>0</code> when <code>method="EE"</code>.</p>
</td></tr>
<tr><td><code>se.tau2</code></td>
<td>
<p>standard error of the estimated amount of (residual) heterogeneity.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of studies included in the analysis.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>number of coefficients in the model (including the intercept).</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of coefficients included in the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QE</code></td>
<td>
<p>test statistic of the test for (residual) heterogeneity.</p>
</td></tr>
<tr><td><code>QEp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>I2</code></td>
<td>
<p>value of \(I^2\). See <code><a href="#topic+print.rma.uni">print</a></code> for more details.</p>
</td></tr>
<tr><td><code>H2</code></td>
<td>
<p>value of \(H^2\). See <code><a href="#topic+print.rma.uni">print</a></code> for more details.</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>value of \(R^2\). See <code><a href="#topic+print.rma.uni">print</a></code> for more details.</p>
</td></tr>
<tr><td><code>int.only</code></td>
<td>
<p>logical that indicates whether the model is an intercept-only model.</p>
</td></tr>
<tr><td><code>yi</code>, <code>vi</code>, <code>X</code></td>
<td>
<p>the vector of outcomes, the corresponding sampling variances, and the model matrix.</p>
</td></tr>
<tr><td><code>fit.stats</code></td>
<td>
<p>a list with the log-likelihood, deviance, AIC, BIC, and AICc values under the unrestricted and restricted likelihood.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>For location-scale models, the object is of class <code>c("rma.ls","rma.uni","rma")</code> and includes the following components in addition to the ones listed above:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>estimated scale coefficients of the model.</p>
</td></tr>
<tr><td><code>se.alpha</code></td>
<td>
<p>standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval.alpha</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval.alpha</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb.alpha</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub.alpha</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>va</code></td>
<td>
<p>variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>as above, but now a vector of values.</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>number of scale coefficients in the model (including the intercept).</p>
</td></tr>
<tr><td><code>QS</code></td>
<td>
<p>test statistic of the omnibus test of the scale coefficients.</p>
</td></tr>
<tr><td><code>QSp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.uni">print</a></code> function. If fit statistics should also be given, use <code><a href="#topic+summary.rma">summary</a></code> (or use the <code><a href="#topic+fitstats.rma">fitstats</a></code> function to extract them). Full versus reduced model comparisons in terms of fit statistics and likelihood ratio tests can be obtained with <code><a href="#topic+anova.rma">anova</a></code>. Wald-type tests for sets of model coefficients or linear combinations thereof can be obtained with the same function. Permutation tests for the model coefficient(s) can be obtained with <code><a href="#topic+permutest.rma.uni">permutest</a></code>. Tests and confidence intervals based on (cluster) robust methods can be obtained with <code><a href="#topic+robust.rma.uni">robust</a></code>.
</p>
<p>Predicted/fitted values can be obtained with <code><a href="#topic+predict.rma">predict</a></code> and <code><a href="#topic+fitted.rma">fitted</a></code>. For best linear unbiased predictions, see <code><a href="#topic+blup.rma.uni">blup</a></code> and <code><a href="#topic+ranef.rma.uni">ranef</a></code>.
</p>
<p>The <code><a href="#topic+residuals.rma">residuals</a></code>, <code><a href="#topic+rstandard.rma.uni">rstandard</a></code>, and <code><a href="#topic+rstudent.rma.uni">rstudent</a></code> functions extract raw and standardized residuals. Additional model diagnostics (e.g., to determine influential studies) can be obtained with the <code><a href="#topic+influence.rma.uni">influence</a></code> function. For models without moderators, leave-one-out diagnostics can also be obtained with <code><a href="#topic+leave1out.rma.uni">leave1out</a></code>. For models with moderators, variance inflation factors can be obtained with <code><a href="#topic+vif.rma">vif</a></code>.
</p>
<p>A confidence interval for the amount of (residual) heterogeneity in the random/mixed-effects model can be obtained with <code><a href="#topic+confint.rma.uni">confint</a></code>. For location-scale models, <code><a href="#topic+confint.rma.ls">confint</a></code> can provide confidence intervals for the scale coefficients.
</p>
<p>Forest, funnel, radial, L'Abb√©, and Baujat plots can be obtained with <code><a href="#topic+forest.rma">forest</a></code>, <code><a href="#topic+funnel.rma">funnel</a></code>, <code><a href="#topic+radial.rma">radial</a></code>, <code><a href="#topic+labbe.rma">labbe</a></code>, and <code><a href="#topic+baujat.rma">baujat</a></code> (radial and L'Abb√© plots only for models without moderators). The <code><a href="#topic+qqnorm.rma.uni">qqnorm</a></code> function provides normal QQ plots of the standardized residuals. One can also just call <code><a href="#topic+plot.rma.uni">plot</a></code> on the fitted model object to obtain various plots at once. For random/mixed-effects models, the <code><a href="#topic+profile.rma.uni">profile</a></code> function can be used to obtain a plot of the (restricted) log-likelihood as a function of \(\tau^2\). For location-scale models, <code><a href="#topic+profile.rma.ls">profile</a></code> draws analogous plots based on the scale coefficients. For models with moderators, <code><a href="#topic+regplot.rma">regplot</a></code> draws scatter plots / bubble plots, showing the (marginal) relationship between the observed outcomes and a selected moderator from the model.
</p>
<p>Tests for funnel plot asymmetry (which may be indicative of publication bias) can be obtained with <code><a href="#topic+ranktest">ranktest</a></code> and <code><a href="#topic+regtest">regtest</a></code>. For models without moderators, the <code><a href="#topic+trimfill.rma.uni">trimfill</a></code> method can be used to carry out a trim and fill analysis and <code><a href="#topic+hc.rma.uni">hc</a></code> provides a random-effects model analysis that is more robust to publication bias (based on the method by Henmi &amp; Copas, 2010). The test of &lsquo;excess significance&rsquo; can be carried out with the <code><a href="#topic+tes">tes</a></code> function. The fail-safe N (based on a file drawer analysis) can be computed using <code><a href="#topic+fsn">fsn</a></code>. Selection models can be fitted with the <code><a href="#topic+selmodel">selmodel</a></code> function.
</p>
<p>For models without moderators, a cumulative meta-analysis (i.e., adding one observation at a time) can be obtained with <code><a href="#topic+cumul.rma.uni">cumul</a></code>.
</p>
<p>Other extractor functions include <code><a href="#topic+coef.rma">coef</a></code>, <code><a href="#topic+vcov.rma">vcov</a></code>, <code><a href="#topic+logLik.rma">logLik</a></code>, <code><a href="#topic+deviance.rma">deviance</a></code>, <code><a href="#topic+AIC.rma">AIC</a></code>, <code><a href="#topic+BIC.rma">BIC</a></code>, <code><a href="#topic+hatvalues.rma.uni">hatvalues</a></code>, and <code><a href="#topic+weights.rma.uni">weights</a></code>.
</p>


<h3>Note</h3>

<p>While the HS, HSk, HE, DL, SJ, and GENQ estimators of \(\tau^2\) are based on closed-form solutions, the ML, REML, and EB estimators must be obtained iteratively. For this, the function makes use of the Fisher scoring algorithm, which is robust to poor starting values and usually converges quickly (Harville, 1977; Jennrich &amp; Sampson, 1976). By default, the starting value is set equal to the value of the Hedges (HE) estimator and the algorithm terminates when the change in the estimated value of \(\tau^2\) is smaller than \(10^{-5}\) from one iteration to the next. The maximum number of iterations is 100 by default (which should be sufficient in most cases). Information on the progress of the algorithm can be obtained by setting <code>verbose=TRUE</code>. One can also set <code>verbose</code> to an integer (<code>verbose=2</code> yields even more information and <code>verbose=3</code> also sets <code>option(warn=1)</code> temporarily).
</p>
<p>A different starting value, threshold, and maximum number of iterations can be specified via the <code>control</code> argument by setting <code>control=list(tau2.init=value, threshold=value, maxiter=value)</code>. The step length of the Fisher scoring algorithm can also be adjusted by a desired factor with <code>control=list(stepadj=value)</code> (values below 1 will reduce the step length). If using <code>verbose=TRUE</code> shows the estimate jumping around erratically (or cycling through a few values), decreasing the step length (and increasing the maximum number of iterations) can often help with convergence (e.g., <code>control=list(stepadj=0.5, maxiter=1000)</code>).
</p>
<p>The PM, PMM, and GENQM estimators also involve iterative algorithms, which make use of the <code><a href="stats.html#topic+uniroot">uniroot</a></code> function. By default, the desired accuracy (<code>tol</code>) is set equal to <code>.Machine$double.eps^0.25</code> and the maximum number of iterations (<code>maxiter</code>) to <code>100</code> (as above). The upper bound of the interval searched (<code>tau2.max</code>) is set to the larger of 100 and <code>10*mad(yi)^2</code> (i.e., 10 times the squared median absolute deviation of the observed effect sizes or outcomes computed with the <code><a href="stats.html#topic+mad">mad</a></code> function). These values can be adjusted with <code>control=list(tol=value, maxiter=value, tau2.max=value)</code>.
</p>
<p>All of the heterogeneity estimators except SJ can in principle yield negative estimates for the amount of (residual) heterogeneity. However, negative estimates of \(\tau^2\) are outside of the parameter space. For the HS, HSk, HE, DL, and GENQ estimators, negative estimates are therefore truncated to zero. For the ML, REML, and EB estimators, the Fisher scoring algorithm makes use of step halving (Jennrich &amp; Sampson, 1976) to guarantee a non-negative estimate. Finally, for the PM, PMM, and GENQM estimators, the lower bound of the interval searched is set to zero by default. For those brave enough to step into risky territory, there is the option to set the lower bound for all these estimators to some other value besides zero (even a negative one) with <code>control=list(tau2.min=value)</code>, but the lowest value permitted is <code>-min(vi)</code> (to ensure that the marginal variances are always non-negative).
</p>
<p>The Hunter-Schmidt estimator for the amount of heterogeneity is defined in Hunter and Schmidt (1990) only in the context of the random-effects model when analyzing correlation coefficients. A general version of this estimator for random- and mixed-effects models not specific to any particular outcome measure is described in Viechtbauer (2005) and Viechtbauer et al. (2015) and is implemented here.
</p>
<p>The Sidik-Jonkman estimator starts with a crude estimate of \(\tau^2\), which is then updated as described in Sidik and Jonkman (2005b, 2007). If, instead of the crude estimate, one wants to use a better a priori estimate, one can do so by passing this value via <code>control=list(tau2.init=value)</code>.
</p>
<p>One can also specify a vector of estimators via the <code>method</code> argument (e.g., <code>rma(yi, vi, method=c("REML","DL"))</code>). The various estimators are then applied in turn until one converges. This is mostly useful for simulation studies where an estimator (like the REML estimator) is not guaranteed to converge and one can then substitute one (like the DL estimator) that does not involve iterative methods and is guaranteed to provide an estimate.
</p>
<p>Outcomes with non-positive sampling variances are problematic. If a sampling variance is equal to zero, then its weight will be \(1/0\) for equal-effects models when using weighted estimation. Switching to unweighted estimation is a possible solution then. For random/mixed-effects model, some estimators of \(\tau^2\) are undefined when there is at least one sampling variance equal to zero. Other estimators may work, but it may still be necessary to switch to unweighted model fitting, especially when the estimate of \(\tau^2\) converges to zero.
</p>
<p>When including moderators in the model, it is possible that the model matrix is not of full rank (i.e., there is a linear relationship between the moderator variables included in the model). The function automatically tries to reduce the model matrix to full rank by removing redundant predictors, but if this fails the model cannot be fitted and an error will be issued. Deleting (redundant) moderator variables from the model as needed should solve this problem.
</p>
<p>Some general words of caution about the assumptions underlying the models:
</p>

<ul>
<li><p> The sampling variances (i.e., the <code>vi</code> values) are treated as if they are known constants, even though in practice they are usually estimates themselves. This implies that the distributions of the test statistics and corresponding confidence intervals are only exact and have nominal coverage when the within-study sample sizes are large (i.e., when the error in the sampling variance estimates is small). Certain outcome measures (e.g., the arcsine square root transformed risk difference and Fisher's r-to-z transformed correlation coefficient) are based on variance stabilizing transformations that also help to make the assumption of known sampling variances much more reasonable.
</p>
</li>
<li><p> When fitting a mixed/random-effects model, \(\tau^2\) is estimated and then treated as a known constant thereafter. This ignores the uncertainty in the estimate of \(\tau^2\). As a consequence, the standard errors of the parameter estimates tend to be too small, yielding test statistics that are too large and confidence intervals that are not wide enough. The Knapp and Hartung (2003) adjustment (i.e., using <code>test="knha"</code>) can be used to counter this problem, yielding test statistics and confidence intervals whose properties are closer to nominal.
</p>
</li>
<li><p> Most effect sizes or outcome measures do not have exactly normal sampling distributions as assumed under the various models. However, the normal approximation usually becomes more accurate for most effect sizes or outcome measures as the within-study sample sizes increase. Therefore, sufficiently large within-study sample sizes are (usually) needed to be certain that the tests and confidence intervals have nominal levels/coverage. Again, certain outcome measures (e.g., Fisher's r-to-z transformed correlation coefficient) may be preferable from this perspective as well.
</p>
</li></ul>

<p>For location-scale models, model fitting is done via numerical optimization over the model parameters. By default, <code><a href="stats.html#topic+nlminb">nlminb</a></code> is used for the optimization. One can also chose a different optimizer from <code><a href="stats.html#topic+optim">optim</a></code> via the <code>control</code> argument (e.g., <code>control=list(optimizer="BFGS")</code> or <code>control=list(optimizer="Nelder-Mead")</code>). Besides <code><a href="stats.html#topic+nlminb">nlminb</a></code> and one of the methods from <code><a href="stats.html#topic+optim">optim</a></code>, one can also choose one of the optimizers from the <code>minqa</code> package (i.e., <code><a href="minqa.html#topic+uobyqa">uobyqa</a></code>, <code><a href="minqa.html#topic+newuoa">newuoa</a></code>, or <code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>), one of the (derivative-free) algorithms from the <code><a href="nloptr.html#topic+nloptr">nloptr</a></code> package, the Newton-type algorithm implemented in <code><a href="stats.html#topic+nlm">nlm</a></code>, the various algorithms implemented in the <code>dfoptim</code> package (<code><a href="dfoptim.html#topic+hjk">hjk</a></code> for the Hooke-Jeeves, <code><a href="dfoptim.html#topic+nmk">nmk</a></code> for the Nelder-Mead, and <code><a href="dfoptim.html#topic+mads">mads</a></code> for the Mesh Adaptive Direct Searches algorithm), the quasi-Newton type optimizers <code><a href="ucminf.html#topic+ucminf">ucminf</a></code> and <code><a href="lbfgsb3c.html#topic+lbfgsb3c">lbfgsb3c</a></code> and the subspace-searching simplex algorithm <code><a href="subplex.html#topic+subplex">subplex</a></code> from the packages of the same name, the Barzilai-Borwein gradient decent method implemented in <code><a href="BB.html#topic+BBoptim">BBoptim</a></code>, or the parallelized version of the L-BFGS-B algorithm implemented in <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code> from the package of the same name. When using an identity link with <code>link="identity"</code>, constrained optimization (to ensure non-negative \(\tau_i^2\) values) as implemented in <code><a href="stats.html#topic+constrOptim">constrOptim</a></code> is used by default. Alternative optimizers in this case are the <code><a href="Rsolnp.html#topic+solnp">solnp</a></code> solver from the <code>Rsolnp</code> package, <code><a href="nloptr.html#topic+nloptr">nloptr</a></code>, or the augmented Lagrangian adaptive barrier minimization algorithm <code><a href="alabama.html#topic+constrOptim.nl">constrOptim.nl</a></code> from the <code>alabama</code> package.
</p>
<p>The optimizer name must be given as a character string (i.e., in quotes). Additional control parameters can be specified via the <code>control</code> argument (e.g., <code>control=list(iter.max=1000, rel.tol=1e-8)</code>). For <code><a href="nloptr.html#topic+nloptr">nloptr</a></code>, the default is to use the BOBYQA implementation from that package with a relative convergence criterion of <code>1e-8</code> on the function value (i.e., log-likelihood), but this can be changed via the <code>algorithm</code> and <code>ftop_rel</code> arguments (e.g., <code>control=list(optimizer="nloptr", algorithm="NLOPT_LN_SBPLX", ftol_rel=1e-6)</code>) (note: when using <code>optimizer="nloptr"</code> in combination with an identity link, the <code>"NLOPT_LN_COBYLA"</code> algorithm is automatically used, since it allows for inequality constraints). For <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code>, the control argument <code>ncpus</code> can be used to specify the number of cores to use for the parallelization (e.g., <code>control=list(optimizer="optimParallel", ncpus=2)</code>). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
<p>Under certain circumstances (e.g., when the amount of heterogeneity is very small for certain combinations of values for the scale variables and scale coefficients), the values of the scale coefficients may try to drift towards minus or plus infinity, which can lead to problems with the optimization. One can impose constraints on the scale coefficients via <code>control=list(alpha.min=minval, alpha.max=maxval)</code> where <code>minval</code> and <code>maxval</code> are either scalars or vectors of the appropriate length.
</p>
<p>Finally, for location-scale models, the standard errors of the scale coefficients are obtained by inverting the Hessian, which is numerically approximated using the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function from the <code>numDeriv</code> package. This may fail (especially when using an identity link), leading to <code>NA</code> values for the standard errors and hence test statistics, p-values, and confidence interval bounds. One can set control argument <code>hessianCtrl</code> to a list of named arguments to be passed on to the <code>method.args</code> argument of the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function (the default is <code>control=list(hessianCtrl=list(r=8))</code>). One can also set <code>control=list(hesspack="pracma")</code> in which case the <code><a href="pracma.html#topic+hessian">hessian</a></code> function from the <code>pracma</code> package is used instead for approximating the Hessian.
</p>
<p>Even if the Hessian can be approximated and inverted, the standard errors may be unreasonably large when the likelihood surface is very flat around the estimated scale coefficients. This is more likely to happen when \(k\) is small and when the amount of heterogeneity is very small under some conditions as defined by the scale coefficients/variables. Setting constraints on the scale coefficients as described above can also help to mitigate this issue.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Berkey, C. S., Hoaglin, D. C., Mosteller, F., &amp; Colditz, G. A. (1995). A random-effects regression model for meta-analysis. <em>Statistics in Medicine</em>, <b>14</b>(4), 395&ndash;411. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780140406&#8288;</code>
</p>
<p>Brannick, M. T., Potter, S. M., Benitez, B., &amp; Morris, S. B. (2019). Bias and precision of alternate estimators in meta-analysis: Benefits of blending Schmidt‚ÄìHunter and Hedges approaches. <em>Organizational Research Methods</em>, <b>22</b>(2), 490&ndash;514. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1094428117741966&#8288;</code>
</p>
<p>Cochran, W. G. (1954). The combination of estimates from different experiments. <em>Biometrics</em>, <b>10</b>(1), 101&ndash;129. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/3001666&#8288;</code>
</p>
<p>DerSimonian, R., &amp; Laird, N. (1986). Meta-analysis in clinical trials. <em>Controlled Clinical Trials</em>, <b>7</b>(3), 177&ndash;188. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/0197-2456(86)90046-2&#8288;</code>
</p>
<p>DerSimonian, R., &amp; Kacker, R. (2007). Random-effects model for meta-analysis of clinical trials: An update. <em>Contemporary Clinical Trials</em>, <b>28</b>(2), 105&ndash;114. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.cct.2006.04.004&#8288;</code>
</p>
<p>Hardy, R. J. &amp; Thompson, S. G. (1996). A likelihood approach to meta-analysis with random effects. <em>Statistics in Medicine</em>, <b>15</b>(6), 619&ndash;629. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(SICI)1097-0258(19960330)15:6&lt;619::AID-SIM188&gt;3.0.CO;2-A&#8288;</code>
</p>
<p>Hartung, J. (1999). An alternative method for meta-analysis. <em>Biometrical Journal</em>, <b>41</b>(8), 901&ndash;916. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W&#8288;</code>
</p>
<p>Harville, D. A. (1977). Maximum likelihood approaches to variance component estimation and to related problems. <em>Journal of the American Statistical Association</em>, <b>72</b>(358), 320&ndash;338. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2286796&#8288;</code>
</p>
<p>Hedges, L. V. (1983). A random effects model for effect sizes. <em>Psychological Bulletin</em>, <b>93</b>(2), 388&ndash;395. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.93.2.388&#8288;</code>
</p>
<p>Hedges, L. V. (1992). Meta-analysis. <em>Journal of Educational Statistics</em>, <b>17</b>(4), 279&ndash;296. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986017004279&#8288;</code>
</p>
<p>Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical methods for meta-analysis</em>. San Diego, CA: Academic Press.
</p>
<p>Henmi, M., &amp; Copas, J. B. (2010). Confidence intervals for random effects meta-analysis and robustness to publication bias. <em>Statistics in Medicine</em>, <b>29</b>(29), 2969&ndash;2983. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4029&#8288;</code>
</p>
<p>Hunter, J. E., &amp; Schmidt, F. L. (1990). <em>Methods of meta-analysis: Correcting error and bias in research findings</em>. Thousand Oaks, CA: Sage.
</p>
<p>Jackson, D., Turner, R., Rhodes, K. &amp; Viechtbauer, W. (2014). Methods for calculating confidence and credible intervals for the residual between-study variance in random effects meta-regression models. <em>BMC Medical Research Methodology</em>, <b>14</b>, 103. <code style="white-space: pre;">&#8288;https://doi.org/10.1186/1471-2288-14-103&#8288;</code>
</p>
<p>Jackson, D., Law, M., R√ºcker, G., &amp; Schwarzer, G. (2017). The Hartung-Knapp modification for random-effects meta-analysis: A useful refinement but are there any residual concerns? <em>Statistics in Medicine</em>, <b>36</b>(25), 3923&ndash;3934. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.7411&#8288;</code>
</p>
<p>Jennrich, R. I., &amp; Sampson, P. F. (1976). Newton-Raphson and related algorithms for maximum likelihood variance component estimation. <em>Technometrics</em>, <b>18</b>(1), 11&ndash;17. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/1267911&#8288;</code>
</p>
<p>Knapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <em>Statistics in Medicine</em>, <b>22</b>(17), 2693&ndash;2710. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1482&#8288;</code>
</p>
<p>Morris, C. N. (1983). Parametric empirical Bayes inference: Theory and applications. <em>Journal of the American Statistical Association</em>, <b>78</b>(381), 47&ndash;55. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2287098&#8288;</code>
</p>
<p>Paule, R. C., &amp; Mandel, J. (1982). Consensus values and weighting factors. <em>Journal of Research of the National Bureau of Standards</em>, <b>87</b>(5), 377&ndash;385. <code style="white-space: pre;">&#8288;https://doi.org/10.6028/jres.087.022&#8288;</code>
</p>
<p>Raudenbush, S. W. (2009). Analyzing effect sizes: Random effects models. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (2nd ed., pp. 295&ndash;315). New York: Russell Sage Foundation.
</p>
<p>Sidik, K. &amp; Jonkman, J. N. (2002). A simple confidence interval for meta-analysis. <em>Statistics in Medicine</em>, <b>21</b>(21), 3153&ndash;3159. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.1262&#8288;</code>
</p>
<p>Sidik, K., &amp; Jonkman, J. N. (2005a). A note on variance estimation in random effects meta-regression. <em>Journal of Biopharmaceutical Statistics</em>, <b>15</b>(5), 823&ndash;838. <code style="white-space: pre;">&#8288;https://doi.org/10.1081/BIP-200067915&#8288;</code>
</p>
<p>Sidik, K., &amp; Jonkman, J. N. (2005b). Simple heterogeneity variance estimation for meta-analysis. <em>Journal of the Royal Statistical Society, Series C</em>, <b>54</b>(2), 367&ndash;384. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.1467-9876.2005.00489.x&#8288;</code>
</p>
<p>Sidik, K., &amp; Jonkman, J. N. (2007). A comparison of heterogeneity variance estimators in combining results of studies. <em>Statistics in Medicine</em>, <b>26</b>(9), 1964&ndash;1981. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.2688&#8288;</code>
</p>
<p>Veroniki, A. A., Jackson, D., Viechtbauer, W., Bender, R., Bowden, J., Knapp, G., Kuss, O., Higgins, J. P., Langan, D., &amp; Salanti, G. (2016). Methods to estimate the between-study variance and its uncertainty in meta-analysis. <em>Research Synthesis Methods</em>, <b>7</b>(1), 55&ndash;79. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1164&#8288;</code>
</p>
<p>Viechtbauer, W. (2005). Bias and efficiency of meta-analytic variance estimators in the random-effects model. <em>Journal of Educational and Behavioral Statistics</em>, <b>30</b>(3), 261&ndash;293. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986030003261&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Median-unbiased estimators for the amount of heterogeneity in meta-analysis. <em>European Congress of Methodology</em>, Valencia, Spain. <code style="white-space: pre;">&#8288;https://www.wvbauer.com/lib/exe/fetch.php/talks:2021_viechtbauer_eam_median_tau2.pdf&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>
<p>Viechtbauer, W., L√≥pez-L√≥pez, J. A., S√°nchez-Meca, J., &amp; Mar√≠n-Mart√≠nez, F. (2015). A comparison of procedures to test for moderators in mixed-effects meta-regression models. <em>Psychological Methods</em>, <b>20</b>(3), 360&ndash;374. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/met0000023&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for other model fitting functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit a random-effects model using the log risk ratios and sampling variances as input
### note: method="REML" is the default, so one could leave this out
rma(yi, vi, data=dat, method="REML")

### fit a random-effects model using the log risk ratios and standard errors as input
### note: the second argument of rma() is for the *sampling variances*, so we use the
### named argument 'sei' to supply the standard errors to the function
dat$sei &lt;- sqrt(dat$vi)
rma(yi, sei=sei, data=dat)

### fit a random-effects model supplying the 2x2 table cell frequencies to the function
rma(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)

### fit a mixed-effects model with two moderators (absolute latitude and publication year)
rma(yi, vi, mods=cbind(ablat, year), data=dat)

### using a model formula to specify the same model
rma(yi, vi, mods = ~ ablat + year, data=dat)

### using a model formula as part of the yi argument
rma(yi ~ ablat + year, vi, data=dat)

### manual dummy coding of the allocation factor
alloc.random     &lt;- ifelse(dat$alloc == "random",     1, 0)
alloc.alternate  &lt;- ifelse(dat$alloc == "alternate",  1, 0)
alloc.systematic &lt;- ifelse(dat$alloc == "systematic", 1, 0)

### test the allocation factor (in the presence of the other moderators)
### note: 'alternate' is the reference level of the allocation factor,
###       since this is the dummy/level we leave out of the model
### note: the intercept is the first coefficient, so with btt=2:3 we test
###       coefficients 2 and 3, corresponding to the coefficients for the
###       allocation factor
rma(yi, vi, mods = ~ alloc.random + alloc.systematic + year + ablat, data=dat, btt=2:3)

### using a model formula to specify the same model
rma(yi, vi, mods = ~ factor(alloc) + year + ablat, data=dat, btt=2:3)

### factor() is not needed as character variables are automatically converted to factors
rma(yi, vi, mods = ~ alloc + year + ablat, data=dat, btt=2:3)

### test all pairwise differences with Holm's method (using the 'multcomp' package if installed)
res &lt;- rma(yi, vi, mods = ~ factor(alloc) - 1, data=dat)
res
if (require(multcomp))
   summary(glht(res, linfct=contrMat(c("alternate"=1,"random"=1,"systematic"=1),
           type="Tukey")), test=adjusted("holm"))

### subgrouping versus using a single model with a factor (subgrouping provides
### an estimate of tau^2 within each subgroup, but the number of studies in each
### subgroup is quite small; the model with the allocation factor provides a
### single estimate of tau^2 based on a larger number of studies, but assumes
### that tau^2 is the same within each subgroup)
res.a &lt;- rma(yi, vi, data=dat, subset=(alloc=="alternate"))
res.r &lt;- rma(yi, vi, data=dat, subset=(alloc=="random"))
res.s &lt;- rma(yi, vi, data=dat, subset=(alloc=="systematic"))
res.a
res.r
res.s
res &lt;- rma(yi, vi, mods = ~ factor(alloc) - 1, data=dat)
res

############################################################################

### demonstrating that Q_E + Q_M = Q_Total for fixed-effects models
### note: this does not work for random/mixed-effects models, since Q_E and
### Q_Total are calculated under the assumption that tau^2 = 0, while the
### calculation of Q_M incorporates the estimate of tau^2
res &lt;- rma(yi, vi, data=dat, method="FE")
res ### this gives Q_Total
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat, method="FE")
res ### this gives Q_E and Q_M
res$QE + res$QM

### decomposition of Q_E into subgroup Q-values
res &lt;- rma(yi, vi, mods = ~ factor(alloc), data=dat)
res

res.a &lt;- rma(yi, vi, data=dat, subset=(alloc=="alternate"))
res.r &lt;- rma(yi, vi, data=dat, subset=(alloc=="random"))
res.s &lt;- rma(yi, vi, data=dat, subset=(alloc=="systematic"))

res.a$QE ### Q-value within subgroup "alternate"
res.r$QE ### Q-value within subgroup "random"
res.s$QE ### Q-value within subgroup "systematic"

res$QE
res.a$QE + res.r$QE + res.s$QE

############################################################################

### an example of a location-scale model
dat &lt;- dat.bangertdrowns2004

### fit a standard random-effects model
res &lt;- rma(yi, vi, data=dat)
res

### fit the same model as a location-scale model
res &lt;- rma(yi, vi, scale = ~ 1, data=dat)
res

### check that we obtain the same estimate for tau^2
predict(res, newscale=1, transf=exp)

### add the total sample size (per 100) as a location and scale predictor
dat$ni100 &lt;- dat$ni/100
res &lt;- rma(yi, vi, mods = ~ ni100, scale = ~ ni100, data=dat)
res

### variables in the location and scale parts can differ
res &lt;- rma(yi, vi, mods = ~ ni100 + meta, scale = ~ ni100 + imag, data=dat)
res
</code></pre>

<hr>
<h2 id='robust'>Cluster-Robust Tests and Confidence Intervals for 'rma' Objects</h2><span id='topic+robust'></span><span id='topic+robust.rma.uni'></span><span id='topic+robust.rma.mv'></span>

<h3>Description</h3>

<p>Function to obtain cluster-robust tests and confidence intervals (also known as robust variance estimation) of the model coefficients for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robust(x, cluster, ...)

## S3 method for class 'rma.uni'
robust(x, cluster, adjust=TRUE, clubSandwich=FALSE, digits, ...)
## S3 method for class 'rma.mv'
robust(x, cluster, adjust=TRUE, clubSandwich=FALSE, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robust_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code> or <code>"rma.mv"</code>.</p>
</td></tr>
<tr><td><code id="robust_+3A_cluster">cluster</code></td>
<td>
<p>vector to specify the clustering variable to use for constructing the sandwich estimator of the variance-covariance matrix.</p>
</td></tr>
<tr><td><code id="robust_+3A_adjust">adjust</code></td>
<td>
<p>logical to specify whether a small-sample correction should be applied to the variance-covariance matrix.</p>
</td></tr>
<tr><td><code id="robust_+3A_clubsandwich">clubSandwich</code></td>
<td>
<p>logical to specify whether the <a href="https://cran.r-project.org/package=clubSandwich">clubSandwich</a> package should be used to obtain the cluster-robust tests and confidence intervals.</p>
</td></tr>
<tr><td><code id="robust_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="robust_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function constructs a cluster-robust estimate of the variance-covariance matrix of the model coefficients based on a sandwich-type estimator and then computes tests and confidence intervals of the model coefficients. This function will often be part of a general workflow for meta-analyses involving complex dependency structures as described <a href="#topic+misc-recs">here</a>.
</p>
<p>By default, tests of individual coefficients and confidence intervals are based on a t-distribution with \(n-p\) degrees of freedom, while the omnibus test uses an F-distribution with \(m\) and \(n-p\) degrees of freedom, where \(n\) is the number of clusters, \(p\) denotes the total number of model coefficients (including the intercept if it is present), and \(m\) denotes the number of coefficients tested by the omnibus test. This is sometimes called the &lsquo;residual&rsquo; method for approximating the (denominator) degrees of freedom.
</p>
<p>When <code>adjust=TRUE</code> (the default), the cluster-robust estimate of the variance-covariance matrix is multiplied by the factor \(n/(n-p)\), which serves as a small-sample adjustment that tends to improve the performance of the method when the number of clusters is small. This is sometimes called the &lsquo;CR1&rsquo; adjustment/estimator (in contrast to &lsquo;CR0&rsquo; when <code>adjust=FALSE</code>).
</p>
<p>For an even better small-sample adjustment, one can set <code>clubSandwich=TRUE</code> in which case the <a href="https://cran.r-project.org/package=clubSandwich">clubSandwich</a> package is used to obtain the cluster-robust tests and confidence intervals. The variance-covariance matrix of the model coefficients is then estimated using the &lsquo;bias-reduced linearization&rsquo; adjustment proposed by Bell and McCaffrey (2002) and further developed in Tipton (2015) and Pustejovsky and Tipton (2018). This is sometimes called the &lsquo;CR2&rsquo; adjustment/estimator. The degrees of freedom of the t-tests are then estimated using a Satterthwaite approximation. F-tests are then based on an approximate Hotelling's T-squared reference distribution, with denominator degrees of freedom estimated using a method by Zhang (2012, 2013), as further described in Tipton and Pustejovky (2015).
</p>


<h3>Value</h3>

<p>An object of class <code>"robust.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated coefficients of the model.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>robust standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>robust variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>QM</code></td>
<td>
<p>test statistic of the omnibus test of moderators.</p>
</td></tr>
<tr><td><code>QMp</code></td>
<td>
<p>corresponding p-value.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code><a href="#topic+print.rma.uni">print.rma.uni</a></code> and <code><a href="#topic+print.rma.mv">print.rma.mv</a></code> functions (depending on the type of model).
</p>
<p>Predicted/fitted values based on <code>"robust.rma"</code> objects can be obtained with the <code><a href="#topic+predict.rma">predict</a></code> function. Tests for sets of model coefficients or linear combinations thereof can be obtained with the <code><a href="#topic+anova.rma">anova</a></code> function.
</p>


<h3>Note</h3>

<p>The variable specified via <code>cluster</code> is assumed to be of the same length as the data originally passed to the <code>rma.uni</code> or <code>rma.mv</code> functions (and if the <code>data</code> argument was used in the original model fit, then the variable will be searched for within this data frame first). Any subsetting and removal of studies with missing values that was applied during the model fitting is also automatically applied to the variable specified via the <code>cluster</code> argument.
</p>
<p>The idea of the robust (sandwich-type) estimator for models with unspecified heteroscedasticity can be traced back to Eicker (1967), Huber (1967), and White (1980, 1984). Hence, the method in general is often referred to as the Eicker-Huber-White method. Some small-sample improvements to the method are described by MacKinnon and White (1985). The extension to the cluster-robust estimator can be found in Froot (1989) and Williams (2000), which is also related to the GEE approach by Liang and Zeger (1986). Cameron and Miller (2015) provide an extensive overview of cluster-robust methods. Sidik and Jonkman (2005, 2006) introduced robust methods in the meta-analytic context for standard random/mixed-effects models. The use of cluster-robust methods for multivariate/multilevel meta-analytic models was introduced by Hedges, Tipton, and Johnson (2010).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Bell, R. M., &amp; McCaffry, D. F. (2002). Bias reduction in standard errors for linear regression with multi-stage samples. <em>Survey Methodology</em>, <b>28</b>(2), 169&ndash;181. <code style="white-space: pre;">&#8288;https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X20020029058&#8288;</code>
</p>
<p>Cameron, A. C., &amp; Miller, D. L. (2015). A practitioner's guide to cluster-robust inference. <em>Journal of Human Resources</em>, <b>50</b>(2), 317&ndash;372. <code style="white-space: pre;">&#8288;https://doi.org/10.3368/jhr.50.2.317&#8288;</code>
</p>
<p>Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. In L. M. LeCam &amp; J. Neyman (Eds.), <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> (pp. 59&ndash;82). Berkeley: University of California Press.
</p>
<p>Froot, K. A. (1989). Consistent covariance matrix estimation with cross-sectional dependence and heteroskedasticity in financial data. <em>Journal of Financial and Quantitative Analysis</em>, <b>24</b>(3), 333&ndash;355. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2330815&#8288;</code>
</p>
<p>Hedges, L. V., Tipton, E., &amp; Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. <em>Research Synthesis Methods</em>, <b>1</b>(1), 39&ndash;65. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.5&#8288;</code>
</p>
<p>Huber, P. (1967). The behavior of maximum-likelihood estimates under nonstandard conditions. In L. M. LeCam &amp; J. Neyman (Eds.), <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> (pp. 221&ndash;233). University of California Press.
</p>
<p>Liang, K. Y., &amp; Zeger, S. L. (1986). Longitudinal data analysis using generalized linear models. <em>Biometrika</em>, <b>73</b>(1), 13&ndash;22. <code style="white-space: pre;">&#8288;https://doi.org/10.1093/biomet/73.1.13&#8288;</code>
</p>
<p>MacKinnon, J. G., &amp; White, H. (1985). Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em>, <b>29</b>(3), 305&ndash;325. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/0304-4076(85)90158-7&#8288;</code>
</p>
<p>Tipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. <em>Psychological Methods</em>, <b>20</b>(3), 375&ndash;393. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/met0000011&#8288;</code>
</p>
<p>Tipton, E., &amp; Pustejovsky, J. E. (2015). Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression. <em>Journal of Educational and Behavioral Statistics</em>, <b>40</b>(6), 604&ndash;634. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/1076998615606099&#8288;</code>
</p>
<p>Sidik, K., &amp; Jonkman, J. N. (2005). A note on variance estimation in random effects meta-regression. <em>Journal of Biopharmaceutical Statistics</em>, <b>15</b>(5), 823&ndash;838. <code style="white-space: pre;">&#8288;https://doi.org/10.1081/BIP-200067915&#8288;</code>
</p>
<p>Sidik, K., &amp; Jonkman, J. N. (2006). Robust variance estimation for random effects meta-analysis. <em>Computational Statistics &amp; Data Analysis</em>, <b>50</b>(12), 3681&ndash;3701. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.csda.2005.07.019&#8288;</code>
</p>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <em>Econometrica</em>, <b>48</b>(4), 817&ndash;838. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/1912934&#8288;</code>
</p>
<p>White, H. (1984). <em>Asymptotic theory for econometricians</em>. Orlando, FL: Academic Press.
</p>
<p>Williams, R. L. (2000). A note on robust variance estimation for cluster-correlated data. <em>Biometrics</em>, <b>56</b>(2), 645&ndash;646. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.0006-341x.2000.00645.x&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Zhang, J.-T. (2012). An approximate Hotelling T2-test for heteroscedastic one-way MANOVA. <em>Open Journal of Statistics</em>, <b>2</b>(1), 1&ndash;11. <code style="white-space: pre;">&#8288;https://doi.org/10.4236/ojs.2012.21001&#8288;</code>
</p>
<p>Zhang, J.-T. (2013). Tests of linear hypotheses in the ANOVA under heteroscedasticity. <em>International Journal of Advanced Statistics and Probability</em>, <b>1</b>, 9&ndash;24. <code style="white-space: pre;">&#8288;https://doi.org/10.14419/ijasp.v1i2.908&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which cluster-robust tests and confidence intervals can be obtained.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### copy data from Bangert-Drowns et al. (2004) into 'dat'
dat &lt;- dat.bangertdrowns2004

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)
res

### use cluster-robust inference methods
robust(res, cluster=id)

### use methods from the clubSandwich package
robust(res, cluster=id, clubSandwich=TRUE)

### fit meta-regression model
res &lt;- rma(yi, vi, mods = ~ length, data=dat)
res

### use cluster-robust inference methods
robust(res, cluster=id)

### use methods from the clubSandwich package
robust(res, cluster=id, clubSandwich=TRUE)

############################################################################

### copy data from Konstantopoulos (2011) into 'dat'
dat &lt;- dat.konstantopoulos2011

### fit multilevel random-effects model
res &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat)
res

### use cluster-robust inference methods
robust(res, cluster=district)

### use methods from the clubSandwich package
robust(res, cluster=district, clubSandwich=TRUE)

############################################################################

### copy data from Berkey et al. (1998) into 'dat'
dat &lt;- dat.berkey1998

### variables v1i and v2i correspond to the 2x2 var-cov matrices of the studies;
### so use these variables to construct the V matrix (note: since v1i and v2i are
### var-cov matrices and not correlation matrices, set vi=1 for all rows)
V &lt;- vcalc(vi=1, cluster=author, rvars=c(v1i, v2i), data=dat)

### fit multivariate model
res &lt;- rma.mv(yi, V, mods = ~ outcome - 1, random = ~ outcome | trial, struct="UN", data=dat)
res

### use cluster-robust inference methods
robust(res, cluster=trial)

### use methods from the clubSandwich package
robust(res, cluster=trial, clubSandwich=TRUE)

############################################################################
</code></pre>

<hr>
<h2 id='selmodel'>Selection Models</h2><span id='topic+selmodel'></span><span id='topic+selmodel.rma.uni'></span>

<h3>Description</h3>

<p>Function to fit selection models. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selmodel(x, ...)

## S3 method for class 'rma.uni'
selmodel(x, type, alternative="greater", prec, delta,
         steps, decreasing=FALSE, verbose=FALSE, digits, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selmodel_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_type">type</code></td>
<td>
<p>character string to specify the type of selection model. Possible options are <code>"beta"</code>, <code>"halfnorm"</code>, <code>"negexp"</code>, <code>"logistic"</code>, <code>"power"</code>, <code>"negexppow"</code>, <code>"stepfun"</code>, <code>"trunc"</code>, and <code>"truncest"</code>. Can be abbreviated. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_alternative">alternative</code></td>
<td>
<p>character string to specify the sidedness of the hypothesis when testing the observed outcomes. Possible options are <code>"greater"</code> (the default), <code>"less"</code>, or <code>"two.sided"</code>. Can be abbreviated.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_prec">prec</code></td>
<td>
<p>optional character string to specify the measure of precision (only relevant for selection models that can incorporate this into the selection function). Possible options are <code>"sei"</code>, <code>"vi"</code>, <code>"ninv"</code>, or <code>"sqrtninv"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_delta">delta</code></td>
<td>
<p>optional numeric vector (of the same length as the number of selection model parameters) to fix the corresponding \(\delta\) value(s). A \(\delta\) value can be fixed by setting the corresponding element of this argument to the desired value. A \(\delta\) value will be estimated if the corresponding element is set equal to <code>NA</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_steps">steps</code></td>
<td>
<p>numeric vector of one or more values that can or must be specified for certain selection functions. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_decreasing">decreasing</code></td>
<td>
<p>logical to specify whether the \(\delta\) values in a step function selection model must be a monotonically decreasing function of the p-values (the default is <code>FALSE</code>). Only relevant when <code>type="stepfun"</code>. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the model fitting (the default is <code>FALSE</code>). Can also be an integer. Values &gt; 1 generate more verbose output. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_control">control</code></td>
<td>
<p>optional list of control values for the estimation algorithm. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="selmodel_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Selection models are a general class of models that attempt to model the process by which the studies included in a meta-analysis may have been influenced by some form of publication bias. If a particular selection model is an adequate approximation for the underlying selection process, then the model provides estimates of the parameters of interest (e.g., the average true outcome and the amount of heterogeneity in the true outcomes) that are &lsquo;corrected&rsquo; for this selection process (i.e., they are estimates of the parameters in the population of studies before any selection has taken place). The present function fits a variety of such selection models. To do so, one should pass an object fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function to the first argument. The model that will then be fitted is of the same form as the original model combined with the specific selection model chosen (see below for possible options). For example, if the original model was a random-effects model, then a random-effects selection model will be fitted. Similarly, if the original model included moderators, then they will also be accounted for in the selection model fitted. Model fitting is done via maximum likelihood (ML) estimation over the fixed- and random-effects parameters (e.g., \(\mu\) and \(\tau^2\) in a random-effects model) and the selection model parameters.
</p>
<p>Argument <code>type</code> determines the specific type of selection model that should be fitted. Many selection models are based on the idea that selection may haven taken place based on the p-values of the studies. In particular, let \(y_i\) and \(v_i\) denote the observed outcome and the corresponding sampling variance of the \(i\textrm{th}\) study. Then \(z_i = y_i / \sqrt{v_i}\) is the (Wald-type) test statistic for testing the null hypothesis \(\mbox{H}_0{:}\; \theta_i = 0\) and \(p_i = 1 - \Phi(z_i)\) (if <code>alternative="greater"</code>), \(p_i = \Phi(z_i)\) (if <code>alternative="less"</code>), or \(p_i = 2(1 - \Phi(|z_i|))\) (if <code>alternative="two.sided"</code>) the corresponding (one- or two-sided) p-value, where \(\Phi()\) denotes the cumulative distribution function of a standard normal distribution. Finally, let \(w(p_i)\) denote some function that specifies the relative likelihood of selection given the p-value of a study.
</p>
<p>If \(w(p_i) > w(p_{i'})\) when \(p_i < p_{i'}\) (i.e., \(w(p_i)\) is larger for smaller p-values), then <code>alternative="greater"</code> implies selection in favor of increasingly significant positive outcomes, <code>alternative="less"</code> implies selection in favor of increasingly significant negative outcomes, and <code>alternative="two.sided"</code> implies selection in favor of increasingly significant outcomes regardless of their direction.
</p>


<h4>Beta Selection Model</h4>

<p>When <code>type="beta"</code>, the function can be used to fit the &lsquo;beta selection model&rsquo; by Citkowicz and Vevea (2017). For this model, the selection function is given by \[w(p_i) = p_i^{\delta_1 - 1} \times (1 - p_i)^{\delta_2 - 1}\] where \(\delta_1 > 0\) and \(\delta_2 > 0\). The null hypothesis \(\mbox{H}_0{:}\; \delta_1 = \delta_2 = 1\) represents the case where there is no selection according to the model. The figure below illustrates with some examples how the relative likelihood of selection can depend on the p-value for various combinations of \(\delta_1\) and \(\delta_2\). Note that the model allows for a non-monotonic selection function.
</p>
<p><img src="../help/figures/selmodel-beta.png" width=600 alt="selmodel-beta.png" />

</p>



<h4>Half-Normal, Negative-Exponential, Logistic, and Power Selection Models</h4>

<p>Preston et al. (2004) suggested the first three of the following selection functions:
</p>

<table>
<tr>
 <td style="text-align: left;">
      <b>name</b>          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <b><code>type</code></b> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <b>selection function</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
      half-normal          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"halfnorm"</code>  </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = \exp(-\delta \times p_i^2)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      negative-exponential </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"negexp"</code>    </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = \exp(-\delta \times p_i)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      logistic             </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"logistic"</code>  </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = 2 \times \exp(-\delta \times p_i) / (1 + \exp(-\delta \times p_i))\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      power                </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"power"</code>     </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = (1-p_i)^\delta\)</td>
</tr>

</table>

<p>The power selection model is added here as it has similar properties as the models suggested by Preston et al. (2004). For all models, assume \(\delta \ge 0\), so that all functions imply a monotonically decreasing relationship between the p-value and the selection probability. For all functions, \(\mbox{H}_0{:}\; \delta = 0\) implies no selection. The figure below shows the relative likelihood of selection as a function of the p-value for \(\delta = 0\) and for the various selection functions when \(\delta = 6\).
</p>
<p><img src="../help/figures/selmodel-preston.png" width=600 alt="selmodel-preston.png" />

</p>
<p>Here, these functions are extended to allow for the possibility that \(w(p_i) = 1\) for p-values below a certain significance threshold denoted by \(\alpha\) (e.g., to model the case that the relative likelihood of selection is equally high for all significant studies but decreases monotonically for p-values above the significance threshold). To fit such a selection model, one should specify the \(\alpha\) value (with \(0 < \alpha < 1\)) via the <code>steps</code> argument. There should be at least one observed p-value below and one observed p-value above the chosen threshold to fit these models. The figure below shows some examples of the relative likelihood of selection when <code>steps=.05</code>.
</p>
<p><img src="../help/figures/selmodel-preston-step.png" width=600 alt="selmodel-preston-step.png" />

</p>
<p>Preston et al. (2004) also suggested selection functions where the relatively likelihood of selection not only depends on the p-value, but also the precision (e.g., standard error) of the estimate (if two studies have similar p-values, it may be plausible to assume that the larger / more precise study has a higher probability of selection). These selection functions (plus the corresponding power function) are given by:
</p>

<table>
<tr>
 <td style="text-align: left;">
      <b>name</b>          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <b><code>type</code></b> </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <b>selection function</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
      half-normal          </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"halfnorm"</code>  </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = \exp(-\delta \times \mathrm{prec}_i \times p_i^2)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      negative-exponential </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"negexp"</code>    </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = \exp(-\delta \times \mathrm{prec}_i \times p_i)\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      logistic             </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"logistic"</code>  </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = 2 \times \exp(-\delta \times \mathrm{prec}_i \times p_i) / (1 + \exp(-\delta \times \mathrm{prec}_i \times p_i))\) </td>
</tr>
<tr>
 <td style="text-align: left;">
      power                </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> <code>"power"</code>     </td><td style="text-align: left;"> &emsp; </td><td style="text-align: left;"> \(w(p_i) = (1-p_i)^{\delta \times \mathrm{prec}_i}\)</td>
</tr>

</table>

<p>where \(\mathrm{prec}_i = \sqrt{v_i}\) (i.e., the standard error of the \(i\textrm{th}\) study) according to Preston et al. (2004). Here, this idea is generalized to allow the user to specify the specific measure of precision to use (via the <code>prec</code> argument). Possible options are:
</p>

<ul>
<li> <p><code>prec="sei"</code> for the standard errors,
</p>
</li>
<li> <p><code>prec="vi"</code> for the sampling variances,
</p>
</li>
<li> <p><code>prec="ninv"</code> for the inverse of the sample sizes,
</p>
</li>
<li> <p><code>prec="sqrtninv"</code> for the inverse square root of the sample sizes.
</p>
</li></ul>

<p>Using some function of the sample sizes as a measure of precision is only possible when information about the sample sizes is actually stored within the object passed to the <code>selmodel</code> function. See &lsquo;Note&rsquo;.
</p>
<p>Note that \(\mathrm{prec}_i\) is really a measure of imprecision (with higher values corresponding to lower precision). Also, regardless of the specific measure chosen, the values are actually rescaled with \(\mathrm{prec}_i = \mathrm{prec}_i / \max(\mathrm{prec}_i)\) inside of the function, such that \(\mathrm{prec}_i = 1\) for the least precise study and \(\mathrm{prec}_i < 1\) for the remaining studies (the rescaling does not actually change the fit of the model, it only helps to improve the stability of model fitting algorithm). The figure below shows some examples of the relative likelihood of selection using these selection functions for two different precision values (note that lower values of \(\mathrm{prec}\) lead to a higher likelihood of selection).
</p>
<p><img src="../help/figures/selmodel-preston-prec.png" width=600 alt="selmodel-preston-prec.png" />

</p>
<p>One can also use the <code>steps</code> argument as described above in combination with these selection functions (studies with p-values below the chosen threshold then have \(w(p_i) = 1\) regardless of their exact p-value or precision).
</p>



<h4>Negative Exponential Power Selection Model</h4>

<p>As an extension of the half-normal and negative-exponential models, one can also choose <code>type="negexppow"</code> for a &lsquo;negative exponential power selection model&rsquo;. The selection function for this model is given by \[w(p_i) = \exp(-\delta_1 \times p_i^{1/\delta_2})\] where \(\delta_1 \ge 0\) and \(\delta_2 \ge 0\) (see Begg &amp; Mazumdar, 1994, although here a different parameterization is used, such that increasing \(\delta_2\) leads to more severe selection). The figure below shows some examples of this selection function when holding \(\delta_1\) constant while increasing \(\delta_2\).
</p>
<p><img src="../help/figures/selmodel-negexppow.png" width=600 alt="selmodel-negexppow.png" />

</p>
<p>This model affords greater flexibility in the shape of the selection function, but requires the estimation of the additional power parameter (the half-normal and negative-exponential models are therefore special cases when fixing \(\delta_2\) to 0.5 or 1, respectively). \(\mbox{H}_0{:}\; \delta_1 = 0\) again implies no selection, but so does \(\mbox{H}_0{:}\; \delta_2 = 0\).
</p>
<p>One can again use the <code>steps</code> argument to specify a single significance threshold, \(\alpha\), so that \(w(p_i) = 1\) for p-values below this threshold and otherwise \(w(p_i)\) follows the selection function as given above. One can also use the <code>prec</code> argument to specify a measure of precision in combination with this model, which leads to the selection function \[w(p_i) = \exp(-\delta_1 \times \mathrm{prec}_i \times p_i^{1/\delta_2})\] and hence is the logical extension of the negative exponential power selection model that also incorporates some measure of precision into the selection process.
</p>



<h4>Step Function Selection Models</h4>

<p>When <code>type="stepfun"</code>, the function can be used to fit &lsquo;step function models&rsquo; as described by Iyengar and Greenhouse (1988), Hedges (1992), Vevea and Hedges (1995), Vevea and Woods (2005), and others. For these models, one must specify one or multiple values via the <code>steps</code> argument, which define intervals in which the relative likelihood of selection is constant. Let \[\alpha_1 < \alpha_2 < \ldots < \alpha_c\] denote these cutpoints sorted in increasing order, with the constraint that \(\alpha_c = 1\) (if the highest value specified via <code>steps</code> is not 1, the function will automatically add this cutpoint), and define \(\alpha_0 = 0\). The selection function is then given by \(w(p_i) = \delta_j\) for \(\alpha_{j-1} < p_i \le \alpha_j\) where \(\delta_j \ge 0\). To make the model identifiable, we set \(\delta_1 = 1\). The \(\delta_j\) values therefore denote the likelihood of selection in the various intervals relative to the interval for p-values between 0 and \(\alpha_1\). Hence, the null hypothesis \(\mbox{H}_0{:}\; \delta_j = 1\) for \(j = 1, \ldots, c\) implies no selection.
</p>
<p>For example, if <code>steps=c(.05, .10, .50, 1)</code>, then \(\delta_2\) is the likelihood of selection for p-values between .05 and .10, \(\delta_3\) is the likelihood of selection for p-values between .10 and .50, and \(\delta_4\) is the likelihood of selection for p-values between .50 and 1 relative to the likelihood of selection for p-values between 0 and .05. The figure below shows the corresponding selection function for some arbitrarily chosen \(\delta_j\) values.
</p>
<p><img src="../help/figures/selmodel-stepfun.png" width=600 alt="selmodel-stepfun.png" />

</p>
<p>There should be at least one observed p-value within each interval to fit this model. If there are no p-values between \(\alpha_0 = 0\) and \(\alpha_1\) (i.e., within the first interval for which \(\delta_1 = 1\)), then estimates of \(\delta_2, \ldots, \delta_c\) will try to drift to infinity. If there are no p-values between \(\alpha_{j-1}\) and \(\alpha_j\) for \(j = 2, \ldots, c\), then \(\delta_j\) will try to drift to zero. In either case, results should be treated with great caution. A common practice is then to collapse and/or adjust the intervals until all intervals contain at least one study. By setting <code>ptable=TRUE</code>, the function just returns the p-value table and does not attempt any model fitting.
</p>
<p>Note that when <code>alternative="greater"</code> or <code>alternative="less"</code> (i.e., when we assume that the relative likelihood of selection is not only related to the p-values of the studies, but also the directionality of the outcomes), then it would usually make sense to divide conventional levels of significance (e.g., .05) by 2 before passing these values to the <code>steps</code> argument. For example, if we think that studies were selected for positive outcomes that are significant at two-tailed \(\alpha = .05\), then we should use <code>alternative="greater"</code> in combination with <code>steps=c(.025, 1)</code>.
</p>
<p>When specifying a single cutpoint in the context of a random-effects model (typically <code>steps=c(.025, 1)</code> with either <code>alternative="greater"</code> or <code>alternative="less"</code>), this model is sometimes called the &lsquo;three-parameter selection model&rsquo; (3PSM), corresponding to the parameters \(\mu\), \(\tau^2\), and \(\delta_2\) (e.g., Carter et al., 2019; McShane et al., 2016; Pustejovsky &amp; Rodgers, 2019). The same idea but in the context of an equal-effects model was also described by Iyengar and Greenhouse (1988).
</p>
<p>Note that \(\delta_j\) (for \(j = 2, \ldots, c\)) can be larger than 1 (implying a greater likelihood of selection for p-values in the corresponding interval relative to the first interval). With <code>control=list(delta.max=1)</code>, one can enforce that the likelihood of selection for p-values above the first cutpoint can never be greater than the likelihood of selection for p-values below it. This constraint should be used with caution, as it may force \(\delta_j\) estimates to fall on the boundary of the parameter space. Alternatively, one can set <code>decreasing=TRUE</code>, in which case the \(\delta_j\) values must be a monotonically decreasing function of the p-values (which also forces \(\delta_j \le 1\)). This feature should be considered experimental.
</p>
<p>One of the challenges when fitting this model with many cutpoints is the large number of parameters that need to be estimated (which is especially problematic when the number of studies is small). An alternative approach suggested by Vevea and Woods (2005) is to fix the \(\delta_j\) values to some a priori chosen values instead of estimating them. One can then conduct a sensitivity analysis by examining the results (e.g., the estimates of \(\mu\) and \(\tau^2\) in a random-effects model) for a variety of different sets of \(\delta_j\) values (reflecting more or less severe forms of selection). This can be done by specifying the \(\delta_j\) values via the <code>delta</code> argument. Table 1 in Vevea and Woods (2005) provides some illustrative examples of moderate and severe selection functions for one- and two-tailed selection. The code below creates a data frame that contains these functions.
</p>
<pre>tab &lt;- data.frame(
  steps = c(0.005, 0.01, 0.05, 0.10, 0.25, 0.35, 0.50, 0.65, 0.75, 0.90, 0.95, 0.99, 0.995, 1),
  delta.mod.1 = c(1, 0.99, 0.95, 0.80, 0.75, 0.65, 0.60, 0.55, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50),
  delta.sev.1 = c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.40, 0.35, 0.30, 0.25, 0.10, 0.10, 0.10, 0.10),
  delta.mod.2 = c(1, 0.99, 0.95, 0.90, 0.80, 0.75, 0.60, 0.60, 0.75, 0.80, 0.90, 0.95, 0.99, 1.00),
  delta.sev.2 = c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.25, 0.25, 0.50, 0.60, 0.75, 0.90, 0.99, 1.00))</pre>
<p>The figure below shows the corresponding selection functions.
</p>
<p><img src="../help/figures/selmodel-stepfun-fixed.png" width=600 alt="selmodel-stepfun-fixed.png" />

</p>
<p>These four functions are &ldquo;merely examples and should not be regarded as canonical&rdquo; (Vevea &amp; Woods, 2005).
</p>



<h4>Truncated Distribution Selection Model</h4>

<p>When <code>type="trunc"</code>, the model assumes that the relative likelihood of selection depends not on the p-value but on the value of the observed effect size or outcome of a study. Let \(y_c\) denote a single cutpoint (which can be specified via argument <code>steps</code> and which is assumed to be 0 when unspecified). Let \[w(y_i) = \left\{ \begin{matrix} \; 1 & \textrm{if} \; y_i > y_c \\ \; \delta_1 & \textrm{if} \; y_i \le y_c \\ \end{matrix} \right.\] denote the selection function when <code>alternative="greater"</code> and \[w(y_i) = \left\{ \begin{matrix} \; 1 & \textrm{if} \; y_i < y_c \\ \; \delta_1 & \textrm{if} \; y_i \ge y_c \\ \end{matrix} \right.\] when <code>alternative="less"</code> (note that <code>alternative="two.sided"</code> is not an option for this type of selection model). Therefore, when <code>alternative="greater"</code>, \(\delta_1\) denotes the likelihood of selection for observed effect sizes or outcomes that fall below the chosen cutpoint relative to those that fall above it (and vice-versa when <code>alternative="less"</code>). Hence, the null hypothesis \(\mbox{H}_0{:}\; \delta_1 = 1\) implies no selection.
</p>
<p>In principle, it is also possible to obtain a maximum likelihood estimate of the cutpoint. For this, one can set <code>type="truncest"</code>, in which case the selection function is given by \[w(y_i) = \left\{ \begin{matrix} \; 1 & \textrm{if} \; y_i > \delta_2 \\ \; \delta_1 & \textrm{if} \; y_i \le \delta_2 \\ \end{matrix} \right.\] when <code>alternative="greater"</code> and analogously when <code>alternative="less"</code>. Therefore, instead of specifying the cutpoint via the <code>steps</code> argument, it is estimated via \(\delta_2\). Note that estimating both \(\delta_1\) and \(\delta_2\) simultaneously is typically very difficult (the likelihood surface is often quite rugged with multiple local optima) and will require a large number of studies. The implementation of this selection function should be considered experimental.
</p>
<p>Models similar to those described above were proposed by Rust et al. (1990) and Formann (2008), but made various simplifying assumptions (e.g., Formann assumed \(\delta_1 = 0\)) and did not account for the heteroscedastic nature of the sampling variances of the observed effect sizes or outcomes, nor did they allow for heterogeneity in the true effects or the influence of moderators.
</p>



<h3>Value</h3>

<p>An object of class <code>c("rma.uni","rma")</code>. The object is a list containing the same components as a regular <code>c("rma.uni","rma")</code> object, but the parameter estimates are based on the selection model. Most importantly, the following elements are modified based on the selection model:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>estimated coefficients of the model.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>standard errors of the coefficients.</p>
</td></tr>
<tr><td><code>zval</code></td>
<td>
<p>test statistics of the coefficients.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>corresponding p-values.</p>
</td></tr>
<tr><td><code>ci.lb</code></td>
<td>
<p>lower bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>ci.ub</code></td>
<td>
<p>upper bound of the confidence intervals for the coefficients.</p>
</td></tr>
<tr><td><code>vb</code></td>
<td>
<p>variance-covariance matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>estimated amount of (residual) heterogeneity. Always <code>0</code> when <code>method="EE"</code>.</p>
</td></tr>
<tr><td><code>se.tau2</code></td>
<td>
<p>standard error of the estimated amount of (residual) heterogeneity.</p>
</td></tr>
</table>
<p>In addition, the object contains the following additional elements:
</p>
<table>
<tr><td><code>delta</code></td>
<td>
<p>estimated selection model parameter(s).</p>
</td></tr>
<tr><td><code>se.delta</code></td>
<td>
<p>corresponding standard error(s).</p>
</td></tr>
<tr><td><code>zval.delta</code></td>
<td>
<p>corresponding test statistic(s).</p>
</td></tr>
<tr><td><code>pval.delta</code></td>
<td>
<p>corresponding p-value(s).</p>
</td></tr>
<tr><td><code>ci.lb.delta</code></td>
<td>
<p>lower bound of the confidence intervals for the parameter(s).</p>
</td></tr>
<tr><td><code>ci.ub.delta</code></td>
<td>
<p>upper bound of the confidence intervals for the parameter(s).</p>
</td></tr>
<tr><td><code>LRT</code></td>
<td>
<p>test statistic of the likelihood ratio test for the selection model parameter(s).</p>
</td></tr>
<tr><td><code>LRTdf</code></td>
<td>
<p>degrees of freedom for the likelihood ratio test.</p>
</td></tr>
<tr><td><code>LRTp</code></td>
<td>
<p>p-value for the likelihood ratio test.</p>
</td></tr>
<tr><td><code>LRT.tau2</code></td>
<td>
<p>test statistic of the likelihood ratio test for testing \(\mbox{H}_0{:}\; \tau^2 = 0\) (<code>NA</code> when fitting an equal-effects model).</p>
</td></tr>
<tr><td><code>LRTp.tau2</code></td>
<td>
<p>p-value for the likelihood ratio test.</p>
</td></tr>
<tr><td><code>ptable</code></td>
<td>
<p>frequency table for the observed p-values falling into the intervals defined by the <code>steps</code> argument (<code>NA</code> when <code>steps</code> is not specified).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>


<h3>Methods</h3>

<p>The results of the fitted model are formatted and printed with the <code><a href="#topic+print.rma.uni">print</a></code> function. The estimated selection function can be drawn with <code><a href="#topic+plot.rma.uni.selmodel">plot</a></code>.
</p>
<p>The <code><a href="#topic+profile.rma.uni.selmodel">profile</a></code> function can be used to obtain a plot of the log-likelihood as a function of \(\tau^2\) and/or the selection model parameter(s) of the model. Corresponding confidence intervals can be obtained with the <code><a href="#topic+confint.rma.uni.selmodel">confint</a></code> function.
</p>


<h3>Note</h3>

<p>Model fitting is done via numerical optimization over the model parameters. By default, <code><a href="stats.html#topic+optim">optim</a></code> with method <code>"BFGS"</code> is used for the optimization. One can also chose a different optimizer from <code><a href="stats.html#topic+optim">optim</a></code> via the <code>control</code> argument (e.g., <code>control=list(optimizer="Nelder-Mead")</code>). Besides one of the methods from <code><a href="stats.html#topic+optim">optim</a></code>, one can also choose the quasi-Newton algorithm in <code><a href="stats.html#topic+nlminb">nlminb</a></code>, one of the optimizers from the <code>minqa</code> package (i.e., <code><a href="minqa.html#topic+uobyqa">uobyqa</a></code>, <code><a href="minqa.html#topic+newuoa">newuoa</a></code>, or <code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>), one of the (derivative-free) algorithms from the <code><a href="nloptr.html#topic+nloptr">nloptr</a></code> package, the Newton-type algorithm implemented in <code><a href="stats.html#topic+nlm">nlm</a></code>, the various algorithms implemented in the <code>dfoptim</code> package (<code><a href="dfoptim.html#topic+hjk">hjk</a></code> for the Hooke-Jeeves, <code><a href="dfoptim.html#topic+nmk">nmk</a></code> for the Nelder-Mead, and <code><a href="dfoptim.html#topic+mads">mads</a></code> for the Mesh Adaptive Direct Searches algorithm), the quasi-Newton type optimizers <code><a href="ucminf.html#topic+ucminf">ucminf</a></code> and <code><a href="lbfgsb3c.html#topic+lbfgsb3c">lbfgsb3c</a></code> and the subspace-searching simplex algorithm <code><a href="subplex.html#topic+subplex">subplex</a></code> from the packages of the same name, the Barzilai-Borwein gradient decent method implemented in <code><a href="BB.html#topic+BBoptim">BBoptim</a></code>, or the parallelized version of the L-BFGS-B algorithm implemented in <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code> from the package of the same name.
</p>
<p>The optimizer name must be given as a character string (i.e., in quotes). Additional control parameters can be specified via the <code>control</code> argument (e.g., <code>control=list(maxit=1000, reltol=1e-8)</code>). For <code><a href="nloptr.html#topic+nloptr">nloptr</a></code>, the default is to use the BOBYQA implementation from that package with a relative convergence criterion of <code>1e-8</code> on the function value (i.e., log-likelihood), but this can be changed via the <code>algorithm</code> and <code>ftop_rel</code> arguments (e.g., <code>control=list(optimizer="nloptr", algorithm="NLOPT_LN_SBPLX", ftol_rel=1e-6)</code>). For <code><a href="optimParallel.html#topic+optimParallel">optimParallel</a></code>, the control argument <code>ncpus</code> can be used to specify the number of cores to use for the parallelization (e.g., <code>control=list(optimizer="optimParallel", ncpus=2)</code>). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>
<p>All selection models (except for <code>type="stepfun"</code>, <code>type="trunc"</code>, and <code>type="truncest"</code>) require repeated evaluations of an integral, which is done via adaptive quadrature as implemented in the <code><a href="stats.html#topic+integrate">integrate</a></code> function. One can adjust the arguments of the <code>integrate</code> function via control element <code>intCtrl</code>, which is a list of named arguments (e.g., <code>control = list(intCtrl = list(rel.tol=1e-4, subdivisions=100))</code>).
</p>
<p>The starting values for the fixed effects, the \(\tau^2\) value (only relevant in random/mixed-effects selection models), and the \(\delta\) parameter(s) are chosen automatically by the function, but one can also set the starting values manually via the <code>control</code> argument by specifying a vector of the appropriate length for <code>beta.init</code>, a single value for <code>tau2.init</code>, and a vector of the appropriate length for <code>delta.init</code>.
</p>
<p>By default, the \(\delta\) parameter(s) are constrained to a certain range, which improves the stability of the optimization algorithm. For all models, the maximum is set to <code>100</code> and the minimum to <code>0</code> (except for <code>type="beta"</code>, where the minimum for both parameters is <code>1e-5</code>, and when <code>type="stepfun"</code> with <code>decreasing=TRUE</code>, in which case the maximum is set to 1). These defaults can be changed via the <code>control</code> argument by specifying a scalar or a vector of the appropriate length for <code>delta.min</code> and/or <code>delta.max</code>. For example, <code>control=list(delta.max=Inf)</code> lifts the upper bound. Note that when a parameter estimate drifts close to its imposed bound, a warning will be issued.
</p>
<p>A difficulty with fitting the beta selection model (i.e., <code>type="beta"</code>) is the behavior of \(w(p_i)\) when \(p_i = 0\) or \(p_i = 1\). When \(\delta_1 < 1\) or \(\delta_2 < 1\), then this leads to selection weights equal to infinity, which causes problems when computing the likelihood function. Following Citkowicz and Vevea (2017), this problem can be avoided by censoring p-values too close to 0 or 1. The specific censoring point can be set via the <code>pval.min</code> element of the <code>control</code> argument. The default for this selection model is <code>control=list(pval.min=1e-5)</code>. A similar issue arises for the power selection model (i.e., <code>type="power"</code>) when \(p_i = 1\). Again, <code>pval.min=1e-5</code> is used to circumvent this issue. For all other selection models, the default is <code>pval.min=0</code>.
</p>
<p>The variance-covariance matrix corresponding to the estimates of the fixed effects, the \(\tau^2\) value (only relevant in random/mixed-effects selection models), and the \(\delta\) parameter(s) is obtained by inverting the Hessian, which is numerically approximated using the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function from the <code>numDeriv</code> package. This may fail, leading to <code>NA</code> values for the standard errors and hence test statistics, p-values, and confidence interval bounds. One can set control argument <code>hessianCtrl</code> to a list of named arguments to be passed on to the <code>method.args</code> argument of the <code><a href="numDeriv.html#topic+hessian">hessian</a></code> function (the default is <code>control=list(hessianCtrl=list(r=6))</code>). One can also set <code>control=list(hesspack="pracma")</code> in which case the <code><a href="pracma.html#topic+hessian">hessian</a></code> function from the <code>pracma</code> package is used instead for approximating the Hessian. When \(\tau^2\) is estimated to be smaller than either \(10^{-4}\) or \(\min(v_1, \ldots, v_k)/10\) (where \(v_i\) denotes the sampling variances of the \(i\textrm{th}\) study), then \(\tau^2\) is effectively treated as zero for computing the standard errors (which helps to avoid numerical problems in approximating the Hessian). This cutoff can be adjusted via the <code>tau2tol</code> control argument (e.g., <code>control=list(tau2tol=0)</code> to switch off this behavior). Similarly, for <code>type="beta"</code> and <code>type="stepfun"</code>, \(\delta\) estimates below \(10^{-4}\) are treated as effectively zero for computing the standard errors. In this case, the corresponding standard errors are <code>NA</code>. This cutoff can be adjusted via the <code>deltatol</code> control argument (e.g., <code>control=list(deltatol=0)</code> to switch off this behavior).
</p>
<p>Information on the progress of the optimization algorithm can be obtained by setting <code>verbose=TRUE</code> (this won't work when using parallelization). One can also set <code>verbose</code> to an integer (<code>verbose=2</code> yields even more information and <code>verbose=3</code> also show the progress visually by drawing the selection function as the optimization proceeds).
</p>
<p>For selection functions where the <code>prec</code> argument is relevant, using a function of the sample sizes as the measure of precision (i.e., <code>prec="ninv"</code> or <code>prec="sqrtninv"</code>) is only possible when information about the sample sizes is actually stored within the object passed to the <code>selmodel</code> function. That should automatically be the case when the observed effect sizes or outcomes were computed with the <code><a href="#topic+escalc">escalc</a></code> function or when the observed effect sizes or outcomes were computed within the model fitting function. On the other hand, this will not be the case when <code><a href="#topic+rma.uni">rma.uni</a></code> was used together with the <code>yi</code> and <code>vi</code> arguments and the <code>yi</code> and <code>vi</code> values were <em>not</em> computed with <code><a href="#topic+escalc">escalc</a></code>. In that case, it is still possible to pass information about the sample sizes to the <code><a href="#topic+rma.uni">rma.uni</a></code> function (e.g., use <code>rma.uni(yi, vi, ni=ni, data=dat)</code>, where data frame <code>dat</code> includes a variable called <code>ni</code> with the sample sizes).
</p>
<p>Finally, the automatic rescaling of the chosen precision measure can be switched off by setting <code>scaleprec=FALSE</code>.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Begg, C. B., &amp; Mazumdar, M. (1994). Operating characteristics of a rank correlation test for publication bias. <em>Biometrics</em>, <b>50</b>(4), 1088&ndash;1101. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2533446&#8288;</code>
</p>
<p>Carter, E. C., Sch√∂nbrodt, F. D., Gervais, W. M., &amp; Hilgard, J. (2019). Correcting for bias in psychology: A comparison of meta-analytic methods. <em>Advances in Methods and Practices in Psychological Science</em>, <b>2</b>(2), 115&ndash;144. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/2515245919847196&#8288;</code>
</p>
<p>Citkowicz, M., &amp; Vevea, J. L. (2017). A parsimonious weight function for modeling publication bias. <em>Psychological Methods</em>, <b>22</b>(1), 28&ndash;41. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/met0000119&#8288;</code>
</p>
<p>Formann, A. K. (2008). Estimating the proportion of studies missing for meta-analysis due to publication bias. <em>Contemporary Clinical Trials</em>, <b>29</b>(5), 732&ndash;739. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.cct.2008.05.004&#8288;</code>
</p>
<p>Hedges, L. V. (1992). Modeling publication selection effects in meta-analysis. <em>Statistical Science</em>, <b>7</b>(2), 246&ndash;255. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/ss/1177011364&#8288;</code>
</p>
<p>Iyengar, S., &amp; Greenhouse, J. B. (1988). Selection models and the file drawer problem. <em>Statistical Science</em>, <b>3</b>(1), 109&ndash;117. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/ss/1177013012&#8288;</code>
</p>
<p>McShane, B. B., Bockenholt, U., &amp; Hansen, K. T. (2016). Adjusting for publication bias in meta-analysis: An evaluation of selection methods and some cautionary notes. <em>Perspectives on Psychological Science</em>, <b>11</b>(5), 730&ndash;749. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1745691616662243&#8288;</code>
</p>
<p>Preston, C., Ashby, D., &amp; Smyth, R. (2004). Adjusting for publication bias: Modelling the selection process. <em>Journal of Evaluation in Clinical Practice</em>, <b>10</b>(2), 313&ndash;322. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.1365-2753.2003.00457.x&#8288;</code>
</p>
<p>Pustejovsky, J. E., &amp; Rodgers, M. A. (2019). Testing for funnel plot asymmetry of standardized mean differences. <em>Research Synthesis Methods</em>, <b>10</b>(1), 57&ndash;71. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1332&#8288;</code>
</p>
<p>Rust, R. T., Lehmann, D. R. &amp; Farley, J. U. (1990). Estimating publication bias in meta-analysis. <em>Journal of Marketing Research</em>, <b>27</b>(2), 220&ndash;226. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/002224379002700209&#8288;</code>
</p>
<p>Vevea, J. L., &amp; Hedges, L. V. (1995). A general linear model for estimating effect size in the presence of publication bias. <em>Psychometrika</em>, <b>60</b>(3), 419&ndash;435. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/BF02294384&#8288;</code>
</p>
<p>Vevea, J. L., &amp; Woods, C. M. (2005). Publication bias in research synthesis: Sensitivity analysis using a priori weight functions. <em>Psychological Methods</em>, <b>10</b>(4), 428&ndash;443. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/1082-989X.10.4.428&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> for the function to fit models which can be extended with selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### example from Citkowicz and Vevea (2017) for beta selection model

# copy data into 'dat' and examine data
dat &lt;- dat.baskerville2012
dat

# fit random-effects model
res &lt;- rma(smd, se^2, data=dat, method="ML", digits=3)
res

# funnel plot
funnel(res, ylim=c(0,0.6), xlab="Standardized Mean Difference")

# fit beta selection model
## Not run: 
sel &lt;- selmodel(res, type="beta")
sel

# plot the selection function
plot(sel, ylim=c(0,40))

## End(Not run)

# fit mixed-effects meta-regression model with 'blind' dummy variable as moderator
res &lt;- rma(smd, se^2, data=dat, mods = ~ blind, method="ML", digits=3)
res

# predicted average effect for studies that do not and that do use blinding
predict(res, newmods=c(0,1))

# fit beta selection model
## Not run: 
sel &lt;- selmodel(res, type="beta")
sel
predict(sel, newmods=c(0,1))

## End(Not run)

############################################################################

### example from Preston et al. (2004)

# copy data into 'dat' and examine data
dat &lt;- dat.hahn2001
dat

### meta-analysis of (log) odds rations using the Mantel-Haenszel method
res &lt;- rma.mh(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat, digits=2, slab=study)
res

# calculate log odds ratios and corresponding sampling variances
dat &lt;- escalc(measure="OR", ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=dat, drop00=TRUE)
dat

# fit equal-effects model
res &lt;- rma(yi, vi, data=dat, method="EE")

# predicted odds ratio (with 95% CI)
predict(res, transf=exp, digits=2)

# funnel plot
funnel(res, atransf=exp, at=log(c(0.01,0.1,1,10,100)), ylim=c(0,2))

# fit half-normal, negative-exponential, logistic, and power selection models
## Not run: 
sel1 &lt;- selmodel(res, type="halfnorm", alternative="less")
sel2 &lt;- selmodel(res, type="negexp",   alternative="less")
sel3 &lt;- selmodel(res, type="logistic", alternative="less")
sel4 &lt;- selmodel(res, type="power",    alternative="less")

# plot the selection functions
plot(sel1)
plot(sel2, add=TRUE, col="blue")
plot(sel3, add=TRUE, col="red")
plot(sel4, add=TRUE, col="green")

# add legend
legend("topright", inset=0.02, lty="solid", lwd=2, col=c("black","blue","red","green"),
       legend=c("Half-normal", "Negative-exponential", "Logistic", "Power"))

# show estimates of delta (and corresponding SEs)
tab &lt;- data.frame(delta = c(sel1$delta, sel2$delta, sel3$delta, sel4$delta),
                  se    = c(sel1$se.delta, sel2$se.delta, sel3$se.delta, sel4$se.delta))
rownames(tab) &lt;- c("Half-normal", "Negative-exponential", "Logistic", "Power")
round(tab, 2)

# predicted odds ratios (with 95% CI)
predict(res,  transf=exp, digits=2)
predict(sel1, transf=exp, digits=2)
predict(sel2, transf=exp, digits=2)
predict(sel3, transf=exp, digits=2)
predict(sel4, transf=exp, digits=2)

## End(Not run)

# fit selection models including standard error as precision measure (note: using
# scaleprec=FALSE here since Preston et al. (2004) did not use the rescaling)
## Not run: 
sel1 &lt;- selmodel(res, type="halfnorm", prec="sei", alternative="less", scaleprec=FALSE)
sel2 &lt;- selmodel(res, type="negexp",   prec="sei", alternative="less", scaleprec=FALSE)
sel3 &lt;- selmodel(res, type="logistic", prec="sei", alternative="less", scaleprec=FALSE)
sel4 &lt;- selmodel(res, type="power",    prec="sei", alternative="less", scaleprec=FALSE)

# show estimates of delta (and corresponding SEs)
tab &lt;- data.frame(delta = c(sel1$delta, sel2$delta, sel3$delta, sel4$delta),
                  se    = c(sel1$se.delta, sel2$se.delta, sel3$se.delta, sel4$se.delta))
rownames(tab) &lt;- c("Half-normal", "Negative-exponential", "Logistic", "Power")
round(tab, 2)

# predicted odds ratio (with 95% CI)
predict(res,  transf=exp, digits=2)
predict(sel1, transf=exp, digits=2)
predict(sel2, transf=exp, digits=2)
predict(sel3, transf=exp, digits=2)
predict(sel4, transf=exp, digits=2)

## End(Not run)

############################################################################

### meta-analysis on the effect of environmental tobacco smoke on lung cancer risk

# copy data into 'dat' and examine data
dat &lt;- dat.hackshaw1998
dat

# fit random-effects model
res &lt;- rma(yi, vi, data=dat, method="ML")
res

# funnel plot
funnel(res, atransf=exp, at=log(c(0.25,0.5,1,2,4,8)), ylim=c(0,0.8))

# step function selection model
## Not run: 
sel &lt;- selmodel(res, type="stepfun", alternative="greater", steps=c(.025,.10,.50,1))
sel

# plot the selection function
plot(sel)

# truncated distribution selection model (with steps=0 by default)
sel &lt;- selmodel(res, type="trunc")
sel

## End(Not run)

############################################################################

### validity of student ratings example from Vevea &amp; Woods (2005)

# copy data into 'dat' and examine data
dat &lt;- dat.cohen1981
dat[c(1,4,5)]

# calculate r-to-z transformed correlations and corresponding sampling variances
dat &lt;- escalc(measure="ZCOR", ri=ri, ni=ni, data=dat[c(1,4,5)])
dat

# fit random-effects model
res &lt;- rma(yi, vi, data=dat, method="ML", digits=3)
res

# predicted average correlation (with 95% CI)
predict(res, transf=transf.ztor)

# funnel plot
funnel(res, ylim=c(0,0.4))

# selection functions from Vevea &amp; Woods (2005)
tab &lt;- data.frame(
   steps = c(0.005, 0.01, 0.05, 0.10, 0.25, 0.35, 0.50, 0.65, 0.75, 0.90, 0.95, 0.99, 0.995, 1),
   delta.mod.1 = c(1, 0.99, 0.95, 0.80, 0.75, 0.65, 0.60, 0.55, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50),
   delta.sev.1 = c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.40, 0.35, 0.30, 0.25, 0.10, 0.10, 0.10, 0.10),
   delta.mod.2 = c(1, 0.99, 0.95, 0.90, 0.80, 0.75, 0.60, 0.60, 0.75, 0.80, 0.90, 0.95, 0.99, 1.00),
   delta.sev.2 = c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.25, 0.25, 0.50, 0.60, 0.75, 0.90, 0.99, 1.00))

# apply step function model with a priori chosen selection weights
## Not run: 
sel &lt;- lapply(tab[-1], function(delta) selmodel(res, type="stepfun", steps=tab$steps, delta=delta))

# estimates (transformed correlation) and tau^2 values
sav &lt;- data.frame(estimate = round(c(res$beta, sapply(sel, function(x) x$beta)), 2),
                  varcomp  = round(c(res$tau2, sapply(sel, function(x) x$tau2)), 3))
sav

## End(Not run)

############################################################################
</code></pre>

<hr>
<h2 id='simulate.rma'>Simulate Method for 'rma' Objects</h2><span id='topic+simulate'></span><span id='topic+simulate.rma'></span>

<h3>Description</h3>

<p>Function to simulate effect sizes or outcomes based on <code>"rma"</code> model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
simulate(object, nsim=1, seed=NULL, olim, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="simulate.rma_+3A_nsim">nsim</code></td>
<td>
<p>number of response vectors to simulate (defaults to 1).</p>
</td></tr>
<tr><td><code id="simulate.rma_+3A_seed">seed</code></td>
<td>
<p>an object to specify if and how the random number generator should be initialized (&lsquo;seeded&rsquo;). Either <code>NULL</code> or an integer that will be used in a call to <code>set.seed</code> before simulating the response vectors. If set, the value is saved as the <code>"seed"</code> attribute of the returned value. The default, <code>NULL</code> will not change the random generator state, and return <code><a href="base.html#topic+.Random.seed">.Random.seed</a></code> as the <code>"seed"</code> attribute; see &lsquo;Value&rsquo;.</p>
</td></tr>
<tr><td><code id="simulate.rma_+3A_olim">olim</code></td>
<td>
<p>optional argument to specify observation/outcome limits for the simulated values. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="simulate.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model specified via <code>object</code> must be a model fitted with either the <code><a href="#topic+rma.uni">rma.uni</a></code> or <code><a href="#topic+rma.mv">rma.mv</a></code> functions.
</p>


<h3>Value</h3>

<p>A data frame with <code>nsim</code> columns with the simulated effect sizes or outcomes.
</p>
<p>The data frame comes with an attribute <code>"seed"</code>. If argument <code>seed</code> is <code>NULL</code>, the attribute is the value of <code><a href="base.html#topic+.Random.seed">.Random.seed</a></code> before the simulation was started; otherwise it is the value of the <code>seed</code> argument with a <code>"kind"</code> attribute with value <code>as.list(RNGkind())</code>.
</p>


<h3>Note</h3>

<p>If the outcome measure used for the analysis is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>olim</code> argument to enforce those observation/outcome limits when simulating values (simulated values cannot exceed those bounds then).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code> and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which simulated effect sizes or outcomes can be generated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy BCG vaccine data into 'dat'
dat &lt;- dat.bcg

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat)
dat

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)
res

### simulate 5 sets of new outcomes based on the fitted model
newdat &lt;- simulate(res, nsim=5, seed=1234)
newdat
</code></pre>

<hr>
<h2 id='tes'>Test of Excess Significance</h2><span id='topic+tes'></span><span id='topic+print.tes'></span>

<h3>Description</h3>

<p>Function to conduct the test of excess significance. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tes(x, vi, sei, subset, data, H0=0, alternative="two.sided", alpha=.05, theta, tau2,
    test, tes.alternative="greater", progbar=TRUE, tes.alpha=.10, digits, ...)

## S3 method for class 'tes'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<p><em>These arguments pertain to data input:</em>
</p>
<table>
<tr><td><code id="tes_+3A_x">x</code></td>
<td>
<p>a vector with the observed effect sizes or outcomes or an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="tes_+3A_vi">vi</code></td>
<td>
<p>vector with the corresponding sampling variances (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="tes_+3A_sei">sei</code></td>
<td>
<p>vector with the corresponding standard errors (note: only one of the two, <code>vi</code> or <code>sei</code>, needs to be specified).</p>
</td></tr>
<tr><td><code id="tes_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included (ignored if <code>x</code> is an object of class <code>"rma"</code>).</p>
</td></tr>
<tr><td><code id="tes_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
</table>
<p><em>These arguments pertain to the tests of the observed effect sizes or outcomes:</em>
</p>
<table>
<tr><td><code id="tes_+3A_h0">H0</code></td>
<td>
<p>numeric value to specify the value of the effect size or outcome under the null hypothesis (the default is 0).</p>
</td></tr>
<tr><td><code id="tes_+3A_alternative">alternative</code></td>
<td>
<p>character string to specify the sidedness of the hypothesis when testing the observed effect sizes or outcomes. Possible options are <code>"two.sided"</code> (the default), <code>"greater"</code>, or <code>"less"</code>. Can be abbreviated.</p>
</td></tr>
<tr><td><code id="tes_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for testing the observed effect sizes or outcomes (the default is .05).</p>
</td></tr>
</table>
<p><em>These arguments pertain to the power of the tests:</em>
</p>
<table>
<tr><td><code id="tes_+3A_theta">theta</code></td>
<td>
<p>optional numeric value to specify the value of the true effect size or outcome under the alternative hypothesis. If unspecified, it will be estimated based on the data or the value is taken from the <code>"rma"</code> object.</p>
</td></tr>
<tr><td><code id="tes_+3A_tau2">tau2</code></td>
<td>
<p>optional numeric value to specify the amount of heterogeneity in the true effect sizes or outcomes. If unspecified, the true effect sizes or outcomes are assumed to be homogeneous or the value is taken from the <code>"rma"</code> object.</p>
</td></tr>
</table>
<p><em>These arguments pertain to the test of excess significance:</em>
</p>
<table>
<tr><td><code id="tes_+3A_test">test</code></td>
<td>
<p>optional character string to specify the type of test to use for conducting the test of excess significance. Possible options are <code>"chi2"</code>, <code>"binom"</code>, or <code>"exact"</code>. Can be abbreviated. If unspecified, the function chooses the type of test based on the data.</p>
</td></tr>
<tr><td><code id="tes_+3A_tes.alternative">tes.alternative</code></td>
<td>
<p>character string to specify the sidedness of the hypothesis for the test of excess significance. Possible options are <code>"greater"</code> (the default), <code>"two.sided"</code>, or <code>"less"</code>. Can be abbreviated.</p>
</td></tr>
<tr><td><code id="tes_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown (the default is <code>TRUE</code>). Only relevant when conducting an exact test.</p>
</td></tr>
<tr><td><code id="tes_+3A_tes.alpha">tes.alpha</code></td>
<td>
<p>alpha level for the test of excess significance (the default is .10). Only relevant for finding the &lsquo;limit estimate&rsquo;.</p>
</td></tr>
</table>
<p><em>Miscellaneous arguments:</em>
</p>
<table>
<tr><td><code id="tes_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded.</p>
</td></tr>
<tr><td><code id="tes_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function carries out the test of excess significance described by Ioannidis and Trikalinos (2007). The test can be used to examine whether the observed number of significant findings is greater than the number of significant findings expected given the power of the tests. An overabundance of significant tests may suggest that the collection of studies is not representative of all studies conducted on a particular topic.
</p>
<p>One can either pass a vector with the observed effect sizes or outcomes (via <code>x</code>) and the corresponding sampling variances via <code>vi</code> (or the standard errors via <code>sei</code>) to the function or an object of class <code>"rma"</code>.
</p>
<p>The observed effect sizes or outcomes are tested for significance based on a standard Wald-type test, that is, by comparing \[z_i = \frac{y_i - \mbox{H}_0}{\sqrt{v_i}}\] against the appropriate critical value(s) of a standard normal distribution (e.g., \(\pm 1.96\) for <code>alternative="two.sided"</code> and <code>alpha=.05</code>, which are the defaults). Let \(O\) denote the observed number of significant tests.
</p>
<p>Given a particular value for the true effect or outcome denoted by \(\theta\) (which, if it is unspecified, is determined by computing the inverse-variance weighted average of the observed effect sizes or outcomes or the value is taken from the model object), let \(1-\beta_i\) denote the power of the \(i\textrm{th}\) test (where \(\beta_i\) denotes the Type II error probability). If \(\tau^2 > 0\), let \(1-\beta_i\) denote the expected power (computed based on integrating the power over a normal distribution with mean \(\theta\) and variance \(\tau^2\)). Let \(E = \sum_{i=1}^k (1-\beta_i)\) denote the expected number of significant tests.
</p>
<p>The test of excess significance then tests if \(O\) is significantly greater (if <code>tes.alternative="greater"</code>) than \(E\). This can be done using Pearson's chi-square test (if <code>test="chi2"</code>), a binomial test (if <code>test="binomial"</code>), or an exact test (if <code>test="exact"</code>). The latter is described in Francis (2013). If argument <code>test</code> is unspecified, the default is to do an exact test if the number of elements in the sum that needs to be computed is less than or equal to <code>10^6</code> and to do a chi-square test otherwise.
</p>
<p>One can also iteratively find the value of \(\theta\) such that the p-value of the test of excess significance is equal to <code>tes.alpha</code> (which is <code>.10</code> by default). The resulting value is called the &lsquo;limit estimate&rsquo; and is denoted \(\theta_{lim}\) by Ioannidis and Trikalinos (2007). Note that the limit estimate is not computable if the p-value is larger than <code>tes.alpha</code> even if \(\theta = \mbox{H}_0\).
</p>


<h3>Value</h3>

<p>An object of class <code>"tes"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>k</code></td>
<td>
<p>the number of studies included in the analysis.</p>
</td></tr>
<tr><td><code>O</code></td>
<td>
<p>the observed number of significant tests.</p>
</td></tr>
<tr><td><code>E</code></td>
<td>
<p>the expected number of significant tests.</p>
</td></tr>
<tr><td><code>OEratio</code></td>
<td>
<p>the ratio of O over E.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>the type of test conducted.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>the p-value of the test of excess significance.</p>
</td></tr>
<tr><td><code>power</code></td>
<td>
<p>the (estimated) power of the tests.</p>
</td></tr>
<tr><td><code>sig</code></td>
<td>
<p>logical vector indicating which tests were significant.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>the value of \(\theta\) used for computing the power of the tests.</p>
</td></tr>
<tr><td><code>theta.lim</code></td>
<td>
<p>the &lsquo;limit estimate&rsquo; (i.e., \(\theta_{lim}\)).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>The results are formatted and printed with the <code>print</code> function.
</p>


<h3>Note</h3>

<p>When <code>tes.alternative="greater"</code> (the default), then the function tests if \(O\) is significantly greater than \(E\) and hence this is indeed a test of excess significance. When <code>tes.alternative="two.sided"</code>, then the function tests if \(O\) differs significantly from \(E\) in either direction and hence it would be more apt to describe this as a test of (in)consistency (between \(O\) and \(E\)). Finally, one can also set <code>tes.alternative="less"</code>, in which case the function tests if \(O\) is significantly lower than \(E\), which could be considered a test of excess non-significance.
</p>
<p>When <code>tes.alternative="two.sided"</code>, one can actually compute two limit estimates. The function attempts to compute both.
</p>
<p>The function computes the significance and power of the studies based on Wald-type tests regardless of the effect size or outcome measure used as input. This works as an adequate approximation as long as the within-study sample sizes are not too small.
</p>
<p>Note that the test is not a test for publication bias but a test whether the set of studies includes an unusual number of significant findings given the power of the studies. The general usefulness of the test and its usefulness under particular circumstances (e.g., when there is substantial heterogeneity in the true effect sizes or outcomes) has been the subject of considerable debate. See Francis (2013) and the commentaries on this article in the same issue of the journal.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Francis, G. (2013). Replication, statistical consistency, and publication bias. <em>Journal of Mathematical Psychology</em>, <b>57</b>(5), 153&ndash;169. <code style="white-space: pre;">&#8288;https://doi.org/10.1016/j.jmp.2013.02.003&#8288;</code>
</p>
<p>Ioannidis, J. P. A., &amp; Trikalinos, T. A. (2007). An exploratory test for an excess of significant findings. <em>Clinical Trials</em>, <b>4</b>(3), 245&ndash;253. <code style="white-space: pre;">&#8288;https://doi.org/10.1177/1740774507079441&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regtest">regtest</a></code> for the regression test, <code><a href="#topic+ranktest">ranktest</a></code> for the rank correlation test, <code><a href="#topic+trimfill">trimfill</a></code> for the trim and fill method, <code><a href="#topic+fsn">fsn</a></code> to compute the fail-safe N (file drawer analysis), and <code><a href="#topic+selmodel">selmodel</a></code> for selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=x.a, n1i=n.a, ci=x.p, n2i=n.p, data=dat.dorn2007)

### conduct test of excess significance (using test="chi2" to speed things up)
tes(yi, vi, data=dat, test="chi2")

### same as fitting an EE model and then passing the object to the function
res &lt;- rma(yi, vi, data=dat, method="EE")
tes(res, test="chi2")

### illustrate limit estimate (value of theta where p-value of test is equal to tes.alpha)
thetas &lt;- seq(0,1,length=101)
pvals &lt;- sapply(thetas, function(theta) tes(yi, vi, data=dat, test="chi2", theta=theta)$pval)
plot(thetas, pvals, type="o", pch=19, ylim=c(0,1))
sav &lt;- tes(yi, vi, data=dat, test="chi2")
abline(h=sav$tes.alpha, lty="dotted")
abline(v=sav$theta.lim, lty="dotted")

### examine significance of test as a function of alpha (to examine 'significance chasing')
alphas &lt;- seq(.01,.99,length=101)
pvals &lt;- sapply(alphas, function(alpha) tes(yi, vi, data=dat, test="chi2", alpha=alpha)$pval)
plot(alphas, pvals, type="o", pch=19, ylim=c(0,1))
abline(v=.05, lty="dotted")
abline(h=.10, lty="dotted")
</code></pre>

<hr>
<h2 id='to.long'>Convert Data from Vector to Long Format</h2><span id='topic+to.long'></span>

<h3>Description</h3>

<p>Function to convert summary data in vector format to the corresponding long format. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.long(measure, ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
        m1i, m2i, sd1i, sd2i, xi, mi, ri, ti, sdi, ni, data, slab, subset,
        add=1/2, to="none", drop00=FALSE, vlong=FALSE, append=TRUE, var.names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to.long_+3A_measure">measure</code></td>
<td>
<p>a character string to specify the effect size or outcome measure corresponding to the summary data supplied. See &lsquo;Details&rsquo; and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for possible options.</p>
</td></tr>
<tr><td><code id="to.long_+3A_ai">ai</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper left cell).</p>
</td></tr>
<tr><td><code id="to.long_+3A_bi">bi</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper right cell).</p>
</td></tr>
<tr><td><code id="to.long_+3A_ci">ci</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower left cell).</p>
</td></tr>
<tr><td><code id="to.long_+3A_di">di</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower right cell).</p>
</td></tr>
<tr><td><code id="to.long_+3A_n1i">n1i</code></td>
<td>
<p>vector with the group sizes or row totals (first group/row).</p>
</td></tr>
<tr><td><code id="to.long_+3A_n2i">n2i</code></td>
<td>
<p>vector with the group sizes or row totals (second group/row).</p>
</td></tr>
<tr><td><code id="to.long_+3A_x1i">x1i</code></td>
<td>
<p>vector with the number of events (first group).</p>
</td></tr>
<tr><td><code id="to.long_+3A_x2i">x2i</code></td>
<td>
<p>vector with the number of events (second group).</p>
</td></tr>
<tr><td><code id="to.long_+3A_t1i">t1i</code></td>
<td>
<p>vector with the total person-times (first group).</p>
</td></tr>
<tr><td><code id="to.long_+3A_t2i">t2i</code></td>
<td>
<p>vector with the total person-times (second group).</p>
</td></tr>
<tr><td><code id="to.long_+3A_m1i">m1i</code></td>
<td>
<p>vector with the means (first group or time point).</p>
</td></tr>
<tr><td><code id="to.long_+3A_m2i">m2i</code></td>
<td>
<p>vector with the means (second group or time point).</p>
</td></tr>
<tr><td><code id="to.long_+3A_sd1i">sd1i</code></td>
<td>
<p>vector with the standard deviations (first group or time point).</p>
</td></tr>
<tr><td><code id="to.long_+3A_sd2i">sd2i</code></td>
<td>
<p>vector with the standard deviations (second group or time point).</p>
</td></tr>
<tr><td><code id="to.long_+3A_xi">xi</code></td>
<td>
<p>vector with the frequencies of the event of interest.</p>
</td></tr>
<tr><td><code id="to.long_+3A_mi">mi</code></td>
<td>
<p>vector with the frequencies of the complement of the event of interest or the group means.</p>
</td></tr>
<tr><td><code id="to.long_+3A_ri">ri</code></td>
<td>
<p>vector with the raw correlation coefficients.</p>
</td></tr>
<tr><td><code id="to.long_+3A_ti">ti</code></td>
<td>
<p>vector with the total person-times.</p>
</td></tr>
<tr><td><code id="to.long_+3A_sdi">sdi</code></td>
<td>
<p>vector with the standard deviations.</p>
</td></tr>
<tr><td><code id="to.long_+3A_ni">ni</code></td>
<td>
<p>vector with the sample/group sizes.</p>
</td></tr>
<tr><td><code id="to.long_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="to.long_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the studies.</p>
</td></tr>
<tr><td><code id="to.long_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should included in the data frame returned by the function.</p>
</td></tr>
<tr><td><code id="to.long_+3A_add">add</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.long_+3A_to">to</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.long_+3A_drop00">drop00</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.long_+3A_vlong">vlong</code></td>
<td>
<p>optional logical whether a very long format should be used (only relevant for \(2 \times 2\) or \(1 \times 2\) table data).</p>
</td></tr>
<tr><td><code id="to.long_+3A_append">append</code></td>
<td>
<p>logical to specify whether the data frame specified via the <code>data</code> argument (if one has been specified) should be returned together with the long format data (the default is <code>TRUE</code>). Can also be a character or numeric vector to indicate which variables from <code>data</code> to append.</p>
</td></tr>
<tr><td><code id="to.long_+3A_var.names">var.names</code></td>
<td>
<p>optional character vector with variable names (the length depends on the data type). If unspecified, the function sets appropriate variable names by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+escalc">escalc</a></code> function describes a wide variety of effect sizes or outcome measures that can be computed for a meta-analysis. The summary data used to compute those measures are typically contained in vectors, each element corresponding to a study. The <code>to.long</code> function takes this information and constructs a long format dataset from these data.
</p>
<p>For example, in various fields (such as the health and medical sciences), the response variable measured is often dichotomous (binary), so that the data from a study comparing two different groups can be expressed in terms of a \(2 \times 2\) table, such as:
</p>

<table>
<tr>
 <td style="text-align: left;">
           </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
   group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies (i.e., the number of individuals falling into a particular category) and <code>n1i</code> and <code>n2i</code> the row totals (i.e., the group sizes).
</p>
<p>The cell frequencies in \(k\) such \(2 \times 2\) tables can be specified via the <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> arguments (or alternatively, via the <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code> arguments). The function then creates the corresponding long format dataset. The <code>measure</code> argument should then be set equal to one of the outcome measures that can be computed based on this type of data, such as <code>"RR"</code>, <code>"OR"</code>, <code>"RD"</code> (it is not relevant which specific measure is chosen, as long as it corresponds to the specified summary data). See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details on the types of data formats available.
</p>
<p>The long format for data of this type consists of two rows per study, a factor indicating the study (default name <code>study</code>), a dummy variable indicating the group (default name <code>group</code>, coded as 1 and 2), and two variables indicating the number of individuals experiencing outcome 1 or outcome 2 (default names <code>out1</code> and <code>out2</code>). Alternatively, if <code>vlong=TRUE</code>, then the long format consists of four rows per study, a factor indicating the study (default name <code>study</code>), a dummy variable indicating the group (default name <code>group</code>, coded as 1 and 2), a dummy variable indicating the outcome (default name <code>outcome</code>, coded as 1 and 2), and a variable indicating the frequency of the respective outcome (default name <code>freq</code>).
</p>
<p>The default variable names can be changed via the <code>var.names</code> argument (must be of the appropriate length, depending on the data type).
</p>
<p>The examples below illustrate the use of this function.
</p>


<h3>Value</h3>

<p>A data frame with either \(k\), \(2 \times k\), or \(4 \times k\) rows and an appropriate number of columns (depending on the data type) with the data in long format. If <code>append=TRUE</code> and a data frame was specified via the <code>data</code> argument, then the data in long format are appended to the original data frame (with rows repeated an appropriate number of times).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute observed effect sizes or outcomes (and corresponding sampling variances) based on similar inputs.
</p>
<p><code><a href="#topic+to.table">to.table</a></code> for a function to turn similar inputs into tabular form.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### convert data to long format
dat.bcg
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
dat.long

### extra long format
dat &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, vlong=TRUE)
dat

### select variables to append
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
                    data=dat.bcg, append=c("author","year"))
dat.long
dat.long &lt;- to.long(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
                    data=dat.bcg, append=2:3)
dat.long

### convert data to long format
dat.long &lt;- to.long(measure="IRR", x1i=x1i, x2i=x2i, t1i=t1i, t2i=t2i,
                   data=dat.hart1999, var.names=c("id", "group", "events", "ptime"))
dat.long

### convert data to long format
dat.long &lt;- to.long(measure="MD", m1i=m1i, sd1i=sd1i, n1i=n1i,
                    m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999,
                    var.names=c("id", "group", "mean", "sd", "n"))
dat.long
</code></pre>

<hr>
<h2 id='to.table'>Convert Data from Vector to Table Format</h2><span id='topic+to.table'></span>

<h3>Description</h3>

<p>Function to convert summary data in vector format to the corresponding table format. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.table(measure, ai, bi, ci, di, n1i, n2i, x1i, x2i, t1i, t2i,
         m1i, m2i, sd1i, sd2i, xi, mi, ri, ti, sdi, ni, data, slab, subset,
         add=1/2, to="none", drop00=FALSE, rows, cols)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to.table_+3A_measure">measure</code></td>
<td>
<p>a character string to specify the effect size or outcome measure corresponding to the summary data supplied. See &lsquo;Details&rsquo; and the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for possible options.</p>
</td></tr>
<tr><td><code id="to.table_+3A_ai">ai</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper left cell).</p>
</td></tr>
<tr><td><code id="to.table_+3A_bi">bi</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (upper right cell).</p>
</td></tr>
<tr><td><code id="to.table_+3A_ci">ci</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower left cell).</p>
</td></tr>
<tr><td><code id="to.table_+3A_di">di</code></td>
<td>
<p>vector with the \(2 \times 2\) table frequencies (lower right cell).</p>
</td></tr>
<tr><td><code id="to.table_+3A_n1i">n1i</code></td>
<td>
<p>vector with the group sizes or row totals (first group/row).</p>
</td></tr>
<tr><td><code id="to.table_+3A_n2i">n2i</code></td>
<td>
<p>vector with the group sizes or row totals (second group/row).</p>
</td></tr>
<tr><td><code id="to.table_+3A_x1i">x1i</code></td>
<td>
<p>vector with the number of events (first group).</p>
</td></tr>
<tr><td><code id="to.table_+3A_x2i">x2i</code></td>
<td>
<p>vector with the number of events (second group).</p>
</td></tr>
<tr><td><code id="to.table_+3A_t1i">t1i</code></td>
<td>
<p>vector with the total person-times (first group).</p>
</td></tr>
<tr><td><code id="to.table_+3A_t2i">t2i</code></td>
<td>
<p>vector with the total person-times (second group).</p>
</td></tr>
<tr><td><code id="to.table_+3A_m1i">m1i</code></td>
<td>
<p>vector with the means (first group or time point).</p>
</td></tr>
<tr><td><code id="to.table_+3A_m2i">m2i</code></td>
<td>
<p>vector with the means (second group or time point).</p>
</td></tr>
<tr><td><code id="to.table_+3A_sd1i">sd1i</code></td>
<td>
<p>vector with the standard deviations (first group or time point).</p>
</td></tr>
<tr><td><code id="to.table_+3A_sd2i">sd2i</code></td>
<td>
<p>vector with the standard deviations (second group or time point).</p>
</td></tr>
<tr><td><code id="to.table_+3A_xi">xi</code></td>
<td>
<p>vector with the frequencies of the event of interest.</p>
</td></tr>
<tr><td><code id="to.table_+3A_mi">mi</code></td>
<td>
<p>vector with the frequencies of the complement of the event of interest or the group means.</p>
</td></tr>
<tr><td><code id="to.table_+3A_ri">ri</code></td>
<td>
<p>vector with the raw correlation coefficients.</p>
</td></tr>
<tr><td><code id="to.table_+3A_ti">ti</code></td>
<td>
<p>vector with the total person-times.</p>
</td></tr>
<tr><td><code id="to.table_+3A_sdi">sdi</code></td>
<td>
<p>vector with the standard deviations.</p>
</td></tr>
<tr><td><code id="to.table_+3A_ni">ni</code></td>
<td>
<p>vector with the sample/group sizes.</p>
</td></tr>
<tr><td><code id="to.table_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="to.table_+3A_slab">slab</code></td>
<td>
<p>optional vector with labels for the studies.</p>
</td></tr>
<tr><td><code id="to.table_+3A_subset">subset</code></td>
<td>
<p>optional (logical or numeric) vector to specify the subset of studies that should be included in the array returned by the function.</p>
</td></tr>
<tr><td><code id="to.table_+3A_add">add</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.table_+3A_to">to</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.table_+3A_drop00">drop00</code></td>
<td>
<p>see the documentation of the <code><a href="#topic+escalc">escalc</a></code> function.</p>
</td></tr>
<tr><td><code id="to.table_+3A_rows">rows</code></td>
<td>
<p>optional vector with row/group names.</p>
</td></tr>
<tr><td><code id="to.table_+3A_cols">cols</code></td>
<td>
<p>optional vector with column/outcome names.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+escalc">escalc</a></code> function describes a wide variety of effect sizes or outcome measures that can be computed for a meta-analysis. The summary data used to compute those measures are typically contained in vectors, each element corresponding to a study. The <code>to.table</code> function takes this information and constructs an array of \(k\) tables from these data.
</p>
<p>For example, in various fields (such as the health and medical sciences), the response variable measured is often dichotomous (binary), so that the data from a study comparing two different groups can be expressed in terms of a \(2 \times 2\) table, such as:
</p>

<table>
<tr>
 <td style="text-align: left;">
           </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> outcome 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> total      </td>
</tr>
<tr>
 <td style="text-align: left;">
   group 1 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ai</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>bi</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n1i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   group 2 </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>ci</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>di</code> </td><td style="text-align: center;"> &emsp; </td><td style="text-align: center;"> <code>n2i</code></td>
</tr>

</table>

<p>where <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> denote the cell frequencies (i.e., the number of individuals falling into a particular category) and <code>n1i</code> and <code>n2i</code> the row totals (i.e., the group sizes).
</p>
<p>The cell frequencies in \(k\) such \(2 \times 2\) tables can be specified via the <code>ai</code>, <code>bi</code>, <code>ci</code>, and <code>di</code> arguments (or alternatively, via the <code>ai</code>, <code>ci</code>, <code>n1i</code>, and <code>n2i</code> arguments). The function then creates the corresponding \(2 \times 2 \times k\) array of tables. The <code>measure</code> argument should then be set equal to one of the outcome measures that can be computed based on this type of data, such as <code>"RR"</code>, <code>"OR"</code>, <code>"RD"</code> (it is not relevant which specific measure is chosen, as long as it corresponds to the specified summary data). See the documentation of the <code><a href="#topic+escalc">escalc</a></code> function for more details on the types of data formats available.
</p>
<p>The examples below illustrate the use of this function.
</p>


<h3>Value</h3>

<p>An array with \(k\) elements each consisting of either 1 or 2 rows and an appropriate number of columns.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute observed effect sizes or outcomes (and corresponding sampling variances) based on similar inputs.
</p>
<p><code><a href="#topic+to.long">to.long</a></code> for a function to turn similar inputs into a long format dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### create tables
dat &lt;- to.table(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
                data=dat.bcg, slab=paste(author, year, sep=", "),
                rows=c("Vaccinated", "Not Vaccinated"), cols=c("TB+", "TB-"))
dat

### create tables
dat &lt;- to.table(measure="IRR", x1i=x1i, x2i=x2i, t1i=t1i, t2i=t2i,
                data=dat.hart1999, slab=paste(study, year, sep=", "),
                rows=c("Warfarin Group", "Placebo/Control Group"))
dat

### create tables
dat &lt;- to.table(measure="MD", m1i=m1i, sd1i=sd1i, n1i=n1i,
                m2i=m2i, sd2i=sd2i, n2i=n2i, data=dat.normand1999,
                slab=source, rows=c("Specialized Care", "Routine Care"))
dat
</code></pre>

<hr>
<h2 id='to.wide'>Convert Data from a Long to a Wide Format</h2><span id='topic+to.wide'></span>

<h3>Description</h3>

<p>Function to convert data given in long format to a wide format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.wide(data, study, grp, ref, grpvars, postfix=c(".1",".2"),
        addid=TRUE, addcomp=TRUE, adddesign=TRUE, minlen=2,
        var.names=c("id","comp","design"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to.wide_+3A_data">data</code></td>
<td>
<p>a data frame in long format.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_study">study</code></td>
<td>
<p>either the name (given as a character string) or the position (given as a single number) of the study variable in the data frame.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_grp">grp</code></td>
<td>
<p>either the name (given as a character string) or the position (given as a single number) of the group variable in the data frame.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_ref">ref</code></td>
<td>
<p>optional character string to specify the reference group (must be one of the groups in the group variable). If not given, the most frequently occurring group is used as the reference group.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_grpvars">grpvars</code></td>
<td>
<p>either the names (given as a character vector) or the positions (given as a numeric vector) of the group-level variables.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_postfix">postfix</code></td>
<td>
<p>a character string of length 2 giving the affix that is placed after the names of the group-level variables for the first and second group.</p>
</td></tr>
<tr><td><code id="to.wide_+3A_addid">addid</code></td>
<td>
<p>logical to specify whether a row id variable should be added to the data frame (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="to.wide_+3A_addcomp">addcomp</code></td>
<td>
<p>logical to specify whether a comparison id variable should be added to the data frame (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="to.wide_+3A_adddesign">adddesign</code></td>
<td>
<p>logical to specify whether a design id variable should be added to the data frame (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="to.wide_+3A_minlen">minlen</code></td>
<td>
<p>integer to specify the minimum length of the shortened group names for the comparison and design id variables (the default is 2).</p>
</td></tr>
<tr><td><code id="to.wide_+3A_var.names">var.names</code></td>
<td>
<p>character vector with three elements to specify the name of the id, comparison, and design variables (the defaults are <code>"id"</code>, <code>"comp"</code>, and <code>"design"</code>, respectively).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A meta-analytic dataset may be structured in a &lsquo;long&rsquo; format, where each row in the dataset corresponds to a particular study group (e.g., treatment arm). Using this function, such a dataset can be restructured into a &lsquo;wide&rsquo; format, where each group within a study is contrasted against a particular reference group.
</p>
<p>The <code>study</code> and <code>group</code> arguments are used to specify the study and group variables in the dataset (either as character strings or as numbers indicating the column positions of these variables in the dataset). Optional argument <code>ref</code> is used to specify the reference group (this must be one of the groups in the <code>group</code> variable). Argument <code>grpvars</code> is used to specify (either as a character vector or by giving the column positions) of those variables in the dataset that correspond to group-level outcomes (the remaining variables are treated as study-level outcomes).
</p>
<p>The dataset is restructured so that a two-group study will yield a single row in the restructured dataset, contrasting the first group against the second/reference group. For studies with more than two groups (often called &lsquo;multiarm&rsquo; or &lsquo;multitreatment&rsquo; studies in the medical literature), the reference group is repeated as many times as needed (so a three-group study would yield two rows in the restructured dataset, contrasting two groups against a common reference group).
</p>
<p>If a study does not include the reference group, then another group from the study will be used as the reference group. This group is chosen based on the factor levels of the <code>grp</code> variable (i.e., the last level that occurs in the study becomes the reference group).
</p>
<p>To distinguish the names of the group-level outcome variables for the two first and second group in the restructured dataset, the strings given for the <code>postfix</code> argument are placed after the respective variable names.
</p>
<p>If requested, row id, comparison id, and design id variables are added to the restructured dataset. The row id is simply a unique number for each row in the dataset. The comparison id variable indicates which two groups have been compared against each other). The design id variable indicates which groups were included in a particular study. The group names are shortened for the comparison and design variables (to at least <code>minlen</code>; the actual length might be longer to ensure uniqueness of the group names).
</p>
<p>The examples below illustrate the use of this function.
</p>


<h3>Value</h3>

<p>A data frame with rows contrasting groups against a reference group and an appropriate number of columns (depending on the number of group-level outcome variables).
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+contrmat">contrmat</a></code> for a function to construct a contrast matrix based on a dataset in wide format.
</p>
<p><code><a href="metadat.html#topic+dat.hasselblad1998">dat.hasselblad1998</a></code>, <code><a href="metadat.html#topic+dat.lopez2019">dat.lopez2019</a></code>, <code><a href="metadat.html#topic+dat.obrien2003">dat.obrien2003</a></code>, <code><a href="metadat.html#topic+dat.pagliaro1992">dat.pagliaro1992</a></code>, <code><a href="metadat.html#topic+dat.senn2013">dat.senn2013</a></code> for illustrative examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### data in long format
dat &lt;- dat.senn2013
dat &lt;- dat[c(1,4,3,2,5,6)]
dat

### restructure to wide format
dat &lt;- to.wide(dat, study="study", grp="treatment", ref="placebo", grpvars=4:6)
dat

### data in long format
dat &lt;- dat.hasselblad1998
dat

### restructure to wide format
dat &lt;- to.wide(dat, study="study", grp="trt", ref="no_contact", grpvars=6:7)
dat
</code></pre>

<hr>
<h2 id='transf'>Transformation Functions</h2><span id='topic+transf'></span><span id='topic+transf.rtoz'></span><span id='topic+transf.ztor'></span><span id='topic+transf.logit'></span><span id='topic+transf.ilogit'></span><span id='topic+transf.arcsin'></span><span id='topic+transf.iarcsin'></span><span id='topic+transf.pft'></span><span id='topic+transf.ipft'></span><span id='topic+transf.ipft.hm'></span><span id='topic+transf.isqrt'></span><span id='topic+transf.irft'></span><span id='topic+transf.iirft'></span><span id='topic+transf.ahw'></span><span id='topic+transf.iahw'></span><span id='topic+transf.abt'></span><span id='topic+transf.iabt'></span><span id='topic+transf.r2toz'></span><span id='topic+transf.ztor2'></span><span id='topic+transf.ztor.int'></span><span id='topic+transf.exp.int'></span><span id='topic+transf.ilogit.int'></span><span id='topic+transf.dtou1'></span><span id='topic+transf.dtou2'></span><span id='topic+transf.dtou3'></span><span id='topic+transf.dtocles'></span><span id='topic+transf.dtobesd'></span><span id='topic+transf.dtomd'></span><span id='topic+transf.dtorpb'></span><span id='topic+transf.dtorbis'></span><span id='topic+transf.rpbtorbis'></span><span id='topic+transf.rtorpb'></span><span id='topic+transf.rtod'></span><span id='topic+transf.rpbtod'></span><span id='topic+transf.lnortord'></span><span id='topic+transf.lnortorr'></span><span id='topic+transf.lnortod.norm'></span><span id='topic+transf.lnortod.logis'></span><span id='topic+transf.dtolnor.norm'></span><span id='topic+transf.dtolnor.logis'></span><span id='topic+transf.lnortortet.pearson'></span><span id='topic+transf.lnortortet.digby'></span>

<h3>Description</h3>

<p>Functions to carry out various types of transformations that are useful for meta-analyses. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transf.rtoz(xi)
transf.ztor(xi)
transf.logit(xi)
transf.ilogit(xi)
transf.arcsin(xi)
transf.iarcsin(xi)
transf.pft(xi, ni)
transf.ipft(xi, ni)
transf.ipft.hm(xi, targs)
transf.isqrt(xi)
transf.irft(xi, ti)
transf.iirft(xi, ti)
transf.ahw(xi)
transf.iahw(xi)
transf.abt(xi)
transf.iabt(xi)
transf.r2toz(xi)
transf.ztor2(xi)
transf.ztor.int(xi, targs)
transf.exp.int(xi, targs)
transf.ilogit.int(xi, targs)
transf.dtou1(xi)
transf.dtou2(xi)
transf.dtou3(xi)
transf.dtobesd(xi)
transf.dtomd(xi, targs)
transf.dtorpb(xi, n1i, n2i)
transf.dtorbis(xi, n1i, n2i)
transf.rpbtorbis(xi, pi)
transf.rtorpb(xi, pi)
transf.rtod(xi, n1i, n2i)
transf.rpbtod(xi, n1i, n2i)
transf.lnortord(xi, pc)
transf.lnortorr(xi, pc)
transf.lnortod.norm(xi)
transf.lnortod.logis(xi)
transf.dtolnor.norm(xi)
transf.dtolnor.logis(xi)
transf.lnortortet.pearson(xi)
transf.lnortortet.digby(xi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transf_+3A_xi">xi</code></td>
<td>
<p>vector of values to be transformed.</p>
</td></tr>
<tr><td><code id="transf_+3A_ni">ni</code></td>
<td>
<p>vector of sample sizes.</p>
</td></tr>
<tr><td><code id="transf_+3A_n1i">n1i</code></td>
<td>
<p>vector of sample sizes for the first group.</p>
</td></tr>
<tr><td><code id="transf_+3A_n2i">n2i</code></td>
<td>
<p>vector of sample sizes for the second group.</p>
</td></tr>
<tr><td><code id="transf_+3A_ti">ti</code></td>
<td>
<p>vector of person-times at risk.</p>
</td></tr>
<tr><td><code id="transf_+3A_pc">pc</code></td>
<td>
<p>control group risk (either a single value or a vector).</p>
</td></tr>
<tr><td><code id="transf_+3A_pi">pi</code></td>
<td>
<p>proportion of individuals falling into the first of the two groups that is created by the dichotomization.</p>
</td></tr>
<tr><td><code id="transf_+3A_targs">targs</code></td>
<td>
<p>list with additional arguments for the transformation function. See &lsquo;Details&rsquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following transformation functions are currently implemented:
</p>

<ul>
<li> <p><code>transf.rtoz</code>: Fisher's r-to-z transformation for correlation coefficients (same as <code>atanh(x)</code>).
</p>
</li>
<li> <p><code>transf.ztor</code>: inverse of the former (i.e., the z-to-r transformation; same as <code>tanh(x)</code>).
</p>
</li>
<li> <p><code>transf.logit</code>: logit (log odds) transformation for proportions (same as <code>qlogis(x)</code>).
</p>
</li>
<li> <p><code>transf.ilogit</code>: inverse of the former (same as <code>plogis(x)</code>).
</p>
</li>
<li> <p><code>transf.arcsin</code>: arcsine square root transformation for proportions.
</p>
</li>
<li> <p><code>transf.iarcsin</code>: inverse of the former.
</p>
</li>
<li> <p><code>transf.pft</code>: Freeman-Tukey (double arcsine) transformation for proportions. See Freeman &amp; Tukey (1950). The <code>xi</code> argument is used to specify the proportions and the <code>ni</code> argument the corresponding sample sizes.
</p>
</li>
<li> <p><code>transf.ipft</code>: inverse of the former. See Miller (1978).
</p>
</li>
<li> <p><code>transf.ipft.hm</code>: inverse of the former, using the harmonic mean of the sample sizes for the back-transformation. See Miller (1978). The sample sizes are specified via the <code>targs</code> argument (the list element should be called <code>ni</code>).
</p>
</li>
<li> <p><code>transf.isqrt</code>: inverse of the square root transformation (i.e., function to square a number).
</p>
</li>
<li> <p><code>transf.irft</code>: Freeman-Tukey transformation for incidence rates. See Freeman &amp; Tukey (1950). The <code>xi</code> argument is used to specify the incidence rates and the <code>ti</code> argument the corresponding person-times at risk.
</p>
</li>
<li> <p><code>transf.iirft</code>: inverse of the former.
</p>
</li>
<li> <p><code>transf.ahw</code>: transformation of coefficient alpha as suggested by Hakstian &amp; Whalen (1976), except that \(1-(1-\alpha)^{1/3}\) is used (so that the transformed values are a monotonically increasing function of the \(\alpha\) values).
</p>
</li>
<li> <p><code>transf.iahw</code>: inverse of the former.
</p>
</li>
<li> <p><code>transf.abt</code>: transformation of coefficient alpha as suggested by Bonett (2002), except that \(-\log(1-\alpha)\) is used (so that the transformed values are a monotonically increasing function of the \(\alpha\) values).
</p>
</li>
<li> <p><code>transf.iabt</code>: inverse of the former.
</p>
</li>
<li> <p><code>transf.r2toz</code>: variance stabilizing transformation for the coefficient of determination, given by \(z_i = \frac{1}{2} \log\mathopen{}\left(\frac{1+\sqrt{R_i^2}}{1-\sqrt{R_i^2}}\right)\mathclose{}\) (see Olkin &amp; Finn, 1995, but with the additional \(\frac{1}{2}\) factor).
</p>
</li>
<li> <p><code>transf.ztor2</code>: inverse of the former.
</p>
</li>
<li> <p><code>transf.ztor.int</code>: integral transformation method for the z-to-r transformation.
</p>
</li>
<li> <p><code>transf.exp.int</code>: integral transformation method for the exponential transformation.
</p>
</li>
<li> <p><code>transf.ilogit.int</code>: integral transformation method for the inverse logit transformation.
</p>
</li>
<li> <p><code>transf.dtou1</code>: transformation of standardized mean differences to Cohen's \(U_1\) values (Cohen, 1988). Under the assumption that the data for those in the first (say, treated) and second (say, control) group are normally distributed with equal variances but potentially different means, Cohen's \(U_1\) indicates the proportion of non-overlap between the two distributions (i.e., when \(d=0\), then \(U_1\) is equal to 0, which goes to 1 as \(d\) increases).
</p>
</li>
<li> <p><code>transf.dtou2</code>: transformation of standardized mean differences to Cohen's \(U_2\) values (Cohen, 1988). Under the same assumptions as above, Cohen's \(U_2\) indicates the proportion in the first group that exceeds the same proportion in the second group (i.e., when \(d=0\), then \(U_2\) is equal to 0.5, which goes to 1 as \(d\) increases).
</p>
</li>
<li> <p><code>transf.dtou3</code>: transformation of standardized mean differences to Cohen's \(U_3\) values (Cohen, 1988). Under the same assumptions as above, Cohen's \(U_3\) indicates the proportion of individuals in the first group that have a higher value than the mean of those in the second group (i.e., when \(d=0\), then \(U_3\) is equal to 0.5, which goes to 1 as \(d\) increases).
</p>
</li>
<li> <p><code>transf.dtocles</code>: transformation of standardized mean differences to common language effect size (CLES) values (McGraw &amp; Wong, 1992) (also called the probability of superiority). A CLES value indicates the probability that a randomly sampled individual from the first group has a higher value than a randomly sampled individual from the second group (i.e., when \(d=0\), then the CLES is equal to 0.5, which goes to 1 as \(d\) increases).
</p>
</li>
<li> <p><code>transf.dtobesd</code>: transformation of standardized mean differences to binomial effect size display values (Rosenthal &amp; Rubin, 1982). Note that the function only provides the proportion in the first group scoring above the median (the proportion in the second group scoring above the median is simply one minus this value).
</p>
</li>
<li> <p><code>transf.dtomd</code>: transformation of standardized mean differences to mean differences given a known standard deviation (which needs to be specified via the <code>targs</code> argument).
</p>
</li>
<li> <p><code>transf.dtorpb</code>: transformation of standardized mean differences to point-biserial correlations. Arguments <code>n1i</code> and <code>n2i</code> denote the number of individuals in the first and second group, respectively. If <code>n1i</code> and <code>n2i</code> are not specified, the function assumes <code>n1i = n2i</code> and uses the approximate formula \(r_{pb} = \frac{d}{\sqrt{d^2 + 4}}\). If <code>n1i</code> and <code>n2i</code> are specified, the function uses the exact transformation formula \(r_{pb} = \frac{d}{\sqrt{d^2 + h}}\), where \(h = \frac{m}{n_1} + \frac{m}{n_2}\) and \(m = n_1 + n_2 - 2\) (Jacobs &amp; Viechtbauer, 2017).
</p>
</li>
<li> <p><code>transf.dtorbis</code>: transformation of standardized mean differences to biserial correlations. Like <code>transf.dtorpb</code>, but the point-biserial correlations are then transformed to biserial correlations with \(r_{bis} = \frac{\sqrt{p(1-p)}}{f(z_p)} r_{pb}\), where \(p = \frac{n_1}{n_1+n_2}\) and \(f(z_p)\) denotes the density of the standard normal distribution at value \(z_p\), which is the point for which \(P(Z > z_p) = p\), with \(Z\) denoting a random variable following a standard normal distribution (Jacobs &amp; Viechtbauer, 2017).
</p>
</li>
<li> <p><code>transf.rpbtorbis</code>: transformation of point-biserial correlations to biserial correlations. Argument <code>pi</code> denotes the proportion of individuals falling into the first of the two groups that is created by the dichotomization (hence, <code>1-pi</code> falls into the second group). If <code>pi</code> is not specified, the function assumes <code>pi=0.5</code>, which corresponds to dichotomization at the median. The transformation is carried out as described for <code>transf.dtorbis</code>.
</p>
</li>
<li> <p><code>transf.rtorpb</code>: transformation of Pearson product-moment correlations to the corresponding point-biserial correlations, when one of the two variables is dichotomized. Argument <code>pi</code> can be used to denote the proportion of individuals falling into the first of the two groups that is created by the dichotomization (hence, <code>1-pi</code> falls into the second group). If <code>pi</code> is not specified, the function assumes <code>pi=0.5</code>, which corresponds to dichotomization at the median. This function is simply the inverse of <code>transf.rpbtorbis</code>.
</p>
</li>
<li> <p><code>transf.rtod</code>: transformation of Pearson product-moment correlations to the corresponding standardized mean differences, when one of the two variables is dichotomized. Arguments <code>n1i</code> and <code>n2i</code> can be used to denote the number of individuals in the first and second group created by the dichotomization. If <code>n1i</code> and <code>n2i</code> are not specified, the function assumes <code>n1i = n2i</code>. This function is simply the inverse of <code>transf.dtorbis</code>.
</p>
</li>
<li> <p><code>transf.rpbtod</code>: transformation of point-biserial correlations to standardized mean differences. This is simply the inverse of <code>transf.dtorpb</code>.
</p>
</li>
<li> <p><code>transf.lnortord</code>: transformation of log odds ratios to risk differences, assuming a particular value for the control group risk (which needs to be specified via the <code>pc</code> argument).
</p>
</li>
<li> <p><code>transf.lnortorr</code>: transformation of log odds ratios to risk ratios, assuming a particular value for the control group risk (which needs to be specified via the <code>pc</code> argument).
</p>
</li>
<li> <p><code>transf.lnortod.norm</code>: transformation of log odds ratios to standardized mean differences (assuming normal distributions) (Cox &amp; Snell, 1989).
</p>
</li>
<li> <p><code>transf.lnortod.logis</code>: transformation of log odds ratios to standardized mean differences (assuming logistic distributions) (Chinn, 2000).
</p>
</li>
<li> <p><code>transf.dtolnor.norm</code>: transformation of standardized mean differences to log odds ratios (assuming normal distributions) (Cox &amp; Snell, 1989).
</p>
</li>
<li> <p><code>transf.dtolnor.logis</code>: transformation of standardized mean differences to log odds ratios (assuming logistic distributions) (Chinn, 2000).
</p>
</li>
<li> <p><code>transf.lnortortet.pearson</code>: transformation of log odds ratios to tetrachoric correlations as suggested by Pearson (1900).
</p>
</li>
<li> <p><code>transf.lnortortet.digby</code>: transformation of log odds ratios to tetrachoric correlations as suggested by Digby (1983).
</p>
</li></ul>



<h3>Value</h3>

<p>A vector with the transformed values.
</p>


<h3>Note</h3>

<p>The integral transformation method for a transformation function \(h(z)\) is given by \[\int_{\textrm{lower}}^{\textrm{upper}} h(z) f(z) dz\] using the limits <code>targs$lower</code> and <code>targs$upper</code>, where \(f(z)\) is the density of a normal distribution with mean equal to <code>xi</code> and variance equal to <code>targs$tau2</code>. An example is provided below.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Bonett, D. G. (2002). Sample size requirements for testing and estimating coefficient alpha. <em>Journal of Educational and Behavioral Statistics</em>, <b>27</b>(4), 335&ndash;340. <code style="white-space: pre;">&#8288;https://doi.org/10.3102/10769986027004335&#8288;</code>
</p>
<p>Chinn, S. (2000). A simple method for converting an odds ratio to effect size for use in meta-analysis. <em>Statistics in Medicine</em>, <b>19</b>(22), 3127&ndash;3131. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/1097-0258(20001130)19:22&lt;3127::aid-sim784&gt;3.0.co;2-m&#8288;</code>
</p>
<p>Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum Associates.
</p>
<p>Cox, D. R., &amp; Snell, E. J. (1989). <em>Analysis of binary data</em> (2nd ed.). London: Chapman &amp; Hall.
</p>
<p>Digby, P. G. N. (1983). Approximating the tetrachoric correlation coefficient. <em>Biometrics</em>, <b>39</b>(3), 753&ndash;757. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2531104&#8288;</code>
</p>
<p>Fisher, R. A. (1921). On the &ldquo;probable error&rdquo; of a coefficient of correlation deduced from a small sample. <em>Metron</em>, <b>1</b>, 1&ndash;32. <code style="white-space: pre;">&#8288;http://hdl.handle.net/2440/15169&#8288;</code>
</p>
<p>Freeman, M. F., &amp; Tukey, J. W. (1950). Transformations related to the angular and the square root. <em>Annals of Mathematical Statistics</em>, <b>21</b>(4), 607&ndash;611. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/aoms/1177729756&#8288;</code>
</p>
<p>Hakstian, A. R., &amp; Whalen, T. E. (1976). A k-sample significance test for independent alpha coefficients. <em>Psychometrika</em>, <b>41</b>(2), 219&ndash;231. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/BF02291840&#8288;</code>
</p>
<p>Jacobs, P., &amp; Viechtbauer, W. (2017). Estimation of the biserial correlation and its sampling variance for use in meta-analysis. <em>Research Synthesis Methods</em>, <b>8</b>(2), 161&ndash;180. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1218&#8288;</code>
</p>
<p>McGraw, K. O., &amp; Wong, S. P. (1992). A common language effect size statistic. <em>Psychological Bulletin</em>, <b>111</b>(2), 361&ndash;365. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.111.2.361&#8288;</code>
</p>
<p>Miller, J. J. (1978). The inverse of the Freeman-Tukey double arcsine transformation. <em>American Statistician</em>, <b>32</b>(4), 138. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/00031305.1978.10479283&#8288;</code>
</p>
<p>Olkin, I., &amp; Finn, J. D. (1995). Correlations redux. <em>Psychological Bulletin</em>, <b>118</b>(1), 155&ndash;164. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0033-2909.118.1.155&#8288;</code>
</p>
<p>Pearson, K. (1900). Mathematical contributions to the theory of evolution. VII. On the correlation of characters not quantitatively measurable. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, <b>195</b>, 1&ndash;47. <code style="white-space: pre;">&#8288;https://doi.org/10.1098/rsta.1900.0022&#8288;</code>
</p>
<p>Rosenthal, R., &amp; Rubin, D. B. (1982). A simple, general purpose display of magnitude of experimental effect. <em>Journal of Educational Psychology</em>, <b>74</b>(2), 166&ndash;169. <code style="white-space: pre;">&#8288;https://doi.org/10.1037/0022-0663.74.2.166&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model
res &lt;- rma(yi, vi, data=dat)

### average risk ratio with 95% CI (but technically, this provides an
### estimate of the median risk ratio, not the mean risk ratio!)
predict(res, transf=exp)

### average risk ratio with 95% CI using the integral transformation
predict(res, transf=transf.exp.int, targs=list(tau2=res$tau2, lower=-4, upper=4))

### this also works
predict(res, transf=transf.exp.int, targs=list(tau2=res$tau2))

### this as well
predict(res, transf=transf.exp.int, targs=res$tau2)
</code></pre>

<hr>
<h2 id='trimfill'>Trim and Fill Analysis for 'rma.uni' Objects</h2><span id='topic+trimfill'></span><span id='topic+trimfill.rma.uni'></span>

<h3>Description</h3>

<p>Function to carry out a trim and fill analysis for objects of class <code>"rma.uni"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trimfill(x, ...)

## S3 method for class 'rma.uni'
trimfill(x, side, estimator="L0", maxiter=100, verbose=FALSE, ilim, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trimfill_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>.</p>
</td></tr>
<tr><td><code id="trimfill_+3A_side">side</code></td>
<td>
<p>optional character string (either <code>"left"</code> or <code>"right"</code>) to specify on which side of the funnel plot the missing studies should be imputed. If left unspecified, the side is chosen within the function depending on the results of the regression test (see <code><a href="#topic+regtest">regtest</a></code> for details on this test).</p>
</td></tr>
<tr><td><code id="trimfill_+3A_estimator">estimator</code></td>
<td>
<p>character string (either <code>"L0"</code>, <code>"R0"</code>, or <code>"Q0"</code>) to specify the estimator for the number of missing studies (the default is <code>"L0"</code>).</p>
</td></tr>
<tr><td><code id="trimfill_+3A_maxiter">maxiter</code></td>
<td>
<p>integer to specify the maximum number of iterations for the trim and fill method (the default is <code>100</code>).</p>
</td></tr>
<tr><td><code id="trimfill_+3A_verbose">verbose</code></td>
<td>
<p>logical to specify whether output should be generated on the progress of the iterative algorithm used as part of the trim and fill method (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="trimfill_+3A_ilim">ilim</code></td>
<td>
<p>limits for the imputed values. If unspecified, no limits are used.</p>
</td></tr>
<tr><td><code id="trimfill_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The trim and fill method is a nonparametric (rank-based) data augmentation technique proposed by Duval and Tweedie (2000a, 2000b; see also Duval, 2005). The method can be used to estimate the number of studies missing from a meta-analysis due to suppression of the most extreme results on one side of the funnel plot. The method then augments the observed data so that the funnel plot is more symmetric and recomputes the summary estimate based on the complete data. The trim and fill method can only be used in the context of an equal- or a random-effects model (i.e., in models without moderators). The method should not be regarded as a way of yielding a more &lsquo;valid&rsquo; estimate of the overall effect or outcome, but as a way of examining the sensitivity of the results to one particular selection mechanism (i.e., one particular form of publication bias).
</p>


<h3>Value</h3>

<p>An object of class <code>c("rma.uni.trimfill","rma.uni","rma")</code>. The object is a list containing the same components as objects created by <code><a href="#topic+rma.uni">rma.uni</a></code>, except that the data are augmented by the trim and fill method. The following components are also added:
</p>
<table>
<tr><td><code>k0</code></td>
<td>
<p>estimated number of missing studies.</p>
</td></tr>
<tr><td><code>side</code></td>
<td>
<p>either <code>"left"</code> or <code>"right"</code>, indicating on which side of the funnel plot the missing studies (if any) were imputed.</p>
</td></tr>
<tr><td><code>se.k0</code></td>
<td>
<p>standard error of k0.</p>
</td></tr>
<tr><td><code>p.k0</code></td>
<td>
<p>p-value for the test of \(\mbox{H}_0\): no missing studies on the chosen side (only when <code>estimator="R0"</code>; <code>NA</code> otherwise).</p>
</td></tr>
<tr><td><code>yi</code></td>
<td>
<p>the observed effect sizes or outcomes plus the imputed values (if there are any).</p>
</td></tr>
<tr><td><code>vi</code></td>
<td>
<p>the corresponding sampling variances</p>
</td></tr>
<tr><td><code>fill</code></td>
<td>
<p>a logical vector indicating which of the values in <code>yi</code> are the observed (<code>FALSE</code>) and the imputed (<code>TRUE</code>) data.</p>
</td></tr>
</table>
<p>The results of the fitted model after the data augmentation are printed with the <code><a href="#topic+print.rma.uni">print</a></code> function. Calling <code><a href="#topic+funnel.rma">funnel</a></code> on the object provides a funnel plot of the observed and imputed data.
</p>


<h3>Note</h3>

<p>Three different estimators for the number of missing studies were proposed by Duval and Tweedie (2000a, 2000b). Based on these articles and Duval (2005), <code>"R0"</code> and <code>"L0"</code> are recommended. An advantage of estimator <code>"R0"</code> is that it provides a test of the null hypothesis that the number of missing studies (on the chosen side) is zero.
</p>
<p>If the outcome measure used for the analysis is bounded (e.g., correlations are bounded between -1 and +1, proportions are bounded between 0 and 1), one can use the <code>ilim</code> argument to enforce those limits when imputing values (imputed values cannot exceed those bounds then).
</p>
<p>The model used during the trim and fill procedure is the same as used by the original model object. Hence, if an equal-effects model is passed to the function, then an equal-effects model is also used during the trim and fill procedure and the results provided are also based on an equal-effects model. This would be an &lsquo;equal-equal&rsquo; approach. Similarly, if a random-effects model is passed to the function, then the same model is used as part of the trim and fill procedure and for the final analysis. This would be a &lsquo;random-random&rsquo; approach. However, one can also easily fit a different model for the final analysis than was used for the trim and fill procedure. See &lsquo;Examples&rsquo; for an illustration of an &lsquo;equal-random&rsquo; approach.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Duval, S. J., &amp; Tweedie, R. L. (2000a). Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis. <em>Biometrics</em>, <b>56</b>(2), 455&ndash;463. <code style="white-space: pre;">&#8288;https://doi.org/10.1111/j.0006-341x.2000.00455.x&#8288;</code>
</p>
<p>Duval, S. J., &amp; Tweedie, R. L. (2000b). A nonparametric &quot;trim and fill&quot; method of accounting for publication bias in meta-analysis. <em>Journal of the American Statistical Association</em>, <b>95</b>(449), 89&ndash;98. <code style="white-space: pre;">&#8288;https://doi.org/10.1080/01621459.2000.10473905&#8288;</code>
</p>
<p>Duval, S. J. (2005). The trim and fill method. In H. R. Rothstein, A. J. Sutton, &amp; M. Borenstein (Eds.) <em>Publication bias in meta-analysis: Prevention, assessment, and adjustments</em> (pp. 127&ndash;144). Chichester, England: Wiley.
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+funnel.rma">funnel</a></code> for a function to create funnel plots of the observed and augmented data.
</p>
<p><code><a href="#topic+regtest">regtest</a></code> for the regression test, <code><a href="#topic+ranktest">ranktest</a></code> for the rank correlation test, <code><a href="#topic+tes">tes</a></code> for the test of excess significance, <code><a href="#topic+fsn">fsn</a></code> to compute the fail-safe N (file drawer analysis), and <code><a href="#topic+selmodel">selmodel</a></code> for selection models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### meta-analysis of the log risk ratios using an equal-effects model
res &lt;- rma(yi, vi, data=dat, method="EE")
taf &lt;- trimfill(res)
taf
funnel(taf, cex=1.2, legend=list(show="cis"))

### estimator "R0" also provides test of H0: no missing studies (on the chosen side)
taf &lt;- trimfill(res, estimator="R0")
taf

### meta-analysis of the log risk ratios using a random-effects model
res &lt;- rma(yi, vi, data=dat)
taf &lt;- trimfill(res)
taf
funnel(taf, cex=1.2, legend=list(show="cis"))

### the examples above are equal-equal and random-random approaches

### illustration of an equal-random approach
res &lt;- rma(yi, vi, data=dat, method="EE")
taf &lt;- trimfill(res)
filled &lt;- data.frame(yi = taf$yi, vi = taf$vi, fill = taf$fill)
filled
rma(yi, vi, data=filled)
</code></pre>

<hr>
<h2 id='update.rma'>Model Updating for 'rma' Objects</h2><span id='topic+update'></span><span id='topic+update.rma'></span>

<h3>Description</h3>

<p>Function to update and (by default) refit <code>"rma"</code> models. It does this by extracting the call stored in the object, updating the call, and (by default) evaluating that call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
update(object, formula., ..., evaluate=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="update.rma_+3A_formula.">formula.</code></td>
<td>
<p>changes to the formula. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="update.rma_+3A_...">...</code></td>
<td>
<p>additional arguments to the call, or arguments with changed values.</p>
</td></tr>
<tr><td><code id="update.rma_+3A_evaluate">evaluate</code></td>
<td>
<p>logical to specify whether to evaluate the new call or just return the call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For objects of class <code>"rma.uni"</code>, <code>"rma.glmm"</code>, and <code>"rma.mv"</code>, the <code>formula.</code> argument can be used to update the set of moderators included in the model (see &lsquo;Examples&rsquo;).
</p>


<h3>Value</h3>

<p>If <code>evaluate=TRUE</code> the fitted object, otherwise the updated call.
</p>


<h3>Author(s)</h3>

<p>The present function is based on <code><a href="stats.html#topic+update.default">update.default</a></code>, with changes made by Wolfgang Viechtbauer (<a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a>) so that the formula updating works with the (somewhat non-standard) interface of the <code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> functions.
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models which can be updated / refit.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit random-effects model (method="REML" is default)
res &lt;- rma(yi, vi, data=dat, digits=3)
res

### fit mixed-effects model with two moderators (absolute latitude and publication year)
res &lt;- update(res, ~ ablat + year)
res

### remove 'year' moderator
res &lt;- update(res, ~ . - year)
res

### fit model with ML estimation
update(res, method="ML")

### example with rma.glmm()
res &lt;- rma.glmm(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, digits=3)
res &lt;- update(res, mods = ~ ablat)
res

### fit conditional model with approximate likelihood
update(res, model="CM.AL")
</code></pre>

<hr>
<h2 id='vcalc'>Construct or Approximate the Variance-Covariance Matrix of Dependent Effect Sizes or Outcomes</h2><span id='topic+vcalc'></span>

<h3>Description</h3>

<p>Function to construct or approximate the variance-covariance matrix of dependent effect sizes or outcomes, or more precisely, of their sampling errors (i.e., the <code>V</code> matrix in <code><a href="#topic+rma.mv">rma.mv</a></code>). <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcalc(vi, cluster, subgroup, obs, type, time1, time2, grp1, grp2, w1, w2,
      data, rho, phi, rvars, checkpd=TRUE, nearpd=FALSE, sparse=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcalc_+3A_vi">vi</code></td>
<td>
<p>numeric vector to specify the sampling variances of the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_cluster">cluster</code></td>
<td>
<p>vector to specify the clustering variable (e.g., study).</p>
</td></tr>
<tr><td><code id="vcalc_+3A_subgroup">subgroup</code></td>
<td>
<p>optional vector to specify different (independent) subgroups within clusters.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_obs">obs</code></td>
<td>
<p>optional vector to distinguish different observed effect sizes or outcomes corresponding to the same construct or response/dependent variable.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_type">type</code></td>
<td>
<p>optional vector to distinguish different types of constructs or response/dependent variables underlying the observed effect sizes or outcomes.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_time1">time1</code></td>
<td>
<p>optional numeric vector to specify the time points when the observed effect sizes or outcomes were obtained (in the first condition if the observed effect sizes or outcomes represent contrasts between two conditions).</p>
</td></tr>
<tr><td><code id="vcalc_+3A_time2">time2</code></td>
<td>
<p>optional numeric vector to specify the time points when the observed effect sizes or outcomes were obtained in the second condition (only relevant when the observed effect sizes or outcomes represent contrasts between two conditions).</p>
</td></tr>
<tr><td><code id="vcalc_+3A_grp1">grp1</code></td>
<td>
<p>optional vector to specify the group of the first condition when the observed effect sizes or outcomes represent contrasts between two conditions.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_grp2">grp2</code></td>
<td>
<p>optional vector to specify the group of the second condition when the observed effect sizes or outcomes represent contrasts between two conditions.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_w1">w1</code></td>
<td>
<p>optional numeric vector to specify the size of the group (or more generally, the inverse-sampling variance weight) of the first condition when the observed effect sizes or outcomes represent contrasts between two conditions.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_w2">w2</code></td>
<td>
<p>optional numeric vector to specify the size of the group (or more generally, the inverse-sampling variance weight) of the second condition when the observed effect sizes or outcomes represent contrasts between two conditions.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables given to the arguments above.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_rho">rho</code></td>
<td>
<p>argument to specify the correlation(s) of observed effect sizes or outcomes measured concurrently. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_phi">phi</code></td>
<td>
<p>argument to specify the autocorrelation of observed effect sizes or outcomes measured at different time points. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_rvars">rvars</code></td>
<td>
<p>optional argument for specifying the variables that correspond to the correlation matrices of the studies (if this is specified, all arguments above except for <code>cluster</code> and <code>subgroup</code> are ignored). See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_checkpd">checkpd</code></td>
<td>
<p>logical to specify whether to check that the variance-covariance matrices within clusters are positive definite (the default is <code>TRUE</code>). See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_nearpd">nearpd</code></td>
<td>
<p>logical to specify whether the <code><a href="Matrix.html#topic+nearPD">nearPD</a></code> function from the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package should be used on variance-covariance matrices that are not positive definite. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="vcalc_+3A_sparse">sparse</code></td>
<td>
<p>logical to specify whether the variance-covariance matrix should be returned as a sparse matrix (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="vcalc_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standard meta-analytic models (such as those that can be fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function) assume that the observed effect sizes or outcomes (or more precisely, their sampling errors) are independent. This assumption is typically violated whenever multiple observed effect sizes or outcomes are computed based on the same sample of subjects (or whatever the study units are) or if there is at least partial overlap of subjects that contribute information to the computation of multiple effect sizes or outcomes.
</p>
<p>The present function can be used to construct or approximate the variance-covariance matrix of the sampling errors of dependent effect sizes or outcomes for a wide variety of circumstances (this variance-covariance matrix is the so-called <code>V</code> matrix that may be needed as input for multilevel/multivariate meta-analytic models as can be fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function; see also <a href="#topic+misc-recs">here</a> for some recommendations on a general workflow for meta-analyses involving complex dependency structures).
</p>
<p>Argument <code>cluster</code> is used to specify the clustering variable. Rows with the same value of this variable are allowed to be dependent, while rows with different values are assumed to be independent. Typically, <code>cluster</code> will be a study identifier.
</p>
<p>Within the same cluster, there may be different subgroups with no overlap of subjects across subgroups. Argument <code>subgroup</code> can be used to distinguish such subgroups. Rows with the same value of this variable within a cluster are allowed to be dependent, while rows with different values are assumed to be independent even if they come from the same cluster. Therefore, from hereon, &lsquo;cluster&rsquo; really refers to the combination of <code>cluster</code> and <code>subgroup</code>.
</p>
<p>Multiple effect sizes or outcomes belonging to the same cluster may be dependent due to a variety of reasons:
</p>

<ol>
<li><p> The same construct of interest (e.g., severity of depression) may have been measured using different scales or instruments within a study (e.g., using the Beck Depression Inventory (BDI) and the Hamilton Depression Rating Scale (HDRS)) based on which multiple effect sizes can be computed for the same group of subjects (e.g., contrasting a treatment versus a control group with respect to each scale). In this case, we have multiple effect sizes that are different &lsquo;observations&rsquo; of the effect with respect to the same type of construct.
</p>
<p>Argument <code>obs</code> is then used to distinguish different effect sizes corresponding to the same construct. If <code>obs</code> is specified, then argument <code>rho</code> must also be specified to indicate the degree of correlation among the sampling errors of the different effect sizes. Since this correlation is typically not known, the correlation among the various scales (or a rough &lsquo;guestimate&rsquo; thereof) can be used as a proxy (i.e., the (typical) correlation between BDI and HDRS measurements).
</p>
<p>One can also specify an entire correlation matrix via <code>rho</code> to indicate, for each possible pair of <code>obs</code> values, the corresponding correlation. The row/column names of the matrix must then correspond to the unique values of the <code>obs</code> variable.
</p>
</li>
<li><p> Multiple types of constructs (or more generally, types of response/dependent variables) may have been measured in the same group of subjects (e.g., severity of depression as measured with the Beck Depression Inventory (BDI) and severity of anxiety as measured with the State-Trait Anxiety Inventory (STAI)). If this is of interest for a meta-analysis, effect sizes can then be computed with respect to each &lsquo;type&rsquo; of construct.
</p>
<p>Argument <code>type</code> is then used to distinguish effect sizes corresponding to these different types of constructs. If <code>type</code> is specified, then argument <code>rho</code> must also be specified to indicate the degree of correlation among the sampling errors of effect sizes belonging to these different types. As above, the correlation among the various scales is typically used here as a proxy (i.e., the (typical) correlation between BDI and STAI measurements).
</p>
<p>One can also specify an entire correlation matrix via <code>rho</code> to indicate, for each possible pair of <code>type</code> values, the corresponding correlation. The row/column names of the matrix must then correspond to the unique values of the <code>type</code> variable.
</p>
</li>
<li><p> If there are multiple types of constructs, multiple scales or instruments may also have been used (in at least some of the studies) to measure the same construct and hence there may again be multiple effect sizes that are &lsquo;observations&rsquo; of the same type of construct. Arguments <code>type</code> and <code>obs</code> should then be used together to indicate the various construct types and observations thereof. In this case, argument <code>rho</code> must be a vector of two values, the first to specify the within-construct correlation and the second to specify the between-construct correlation.
</p>
<p>One can also specify a list with two elements for <code>rho</code>, the first element being either a scalar or an entire correlation matrix for the within-construct correlation(s) and the second element being a scalar or an entire correlation matrix for the between-construct correlation(s). As above, any matrices specified must have row/column names corresponding to the unique values of the <code>obs</code> and/or <code>type</code> variables.
</p>
</li>
<li><p> The same construct and scale may have been assessed/used multiple times, allowing the computation of multiple effect sizes for the same group of subjects at different time points (e.g., right after the end of a treatment, at a short-term follow-up, and at a long-term follow-up). Argument <code>time1</code> is then used to specify the time points when the observed effect sizes were obtained. Argument <code>phi</code> must then also be specified to indicate the autocorrelation among the sampling errors of two effect sizes that differ by one unit on the <code>time1</code> variable. As above, the autocorrelation of the measurements themselves can be used here as a proxy.
</p>
<p>If multiple constructs and/or multiple scales have also been assessed at the various time points, then arguments <code>type</code> and/or <code>obs</code> (together with argument <code>rho</code>) should be used as needed to differentiate effect sizes corresponding to the different constructs and/or scales.
</p>
</li>
<li><p> Many effect sizes or outcome measures (e.g., raw or standardized mean differences, log-transformed ratios of means, log risk/odds ratios and risk differences) reflect the difference between two conditions (i.e., a contrast). Within a study, there may be more than two conditions, allowing the computation of multiple such contrasts (e.g., treatment A versus a control condition and treatment B versus the same control condition) and hence corresponding effect sizes. The reuse of information from the &lsquo;shared&rsquo; condition (in this example, the control condition) then induces correlation among the effect sizes.
</p>
<p>To account for this, arguments <code>grp1</code> and <code>grp2</code> should be specified to indicate (within each cluster) which two groups were compared in the computation of each effect size (e.g., in the example above, the coding could be <code>grp1=c(2,3)</code> and <code>grp2=c(1,1)</code>; whether numbers or strings are used as identifiers is irrelevant).
</p>
<p>The degree of correlation between two contrast-type effect sizes that is induced by the use of a shared condition is a function of the size of the groups involved in the computation of the two effect sizes (or, more generally, the inverse-sampling variance weights of the condition-specific outcomes). By default, the group sizes (weights) are assumed to be identical across conditions, which implies a correlation of 0.5. If the group sizes (weights) are known, they can be specified via arguments <code>w1</code> and <code>w2</code> (in which case this information is used by the function to calculate a more accurate estimate of the correlation induced by the shared condition).
</p>
<p>Moreover, a contrast-type effect size can be based on a between- or a within-subjects design. When at least one or more of the contrast-type effect sizes are based on a within-subjects design, then <code>time1</code> and <code>time2</code> should be used in combination with <code>grp1</code> and <code>grp2</code> to indicate for each effect size the group(s) and time point(s) involved.
</p>
<p>For example, <code>grp1=c(2,3)</code> and <code>grp2=c(1,1)</code> as above in combination with <code>time1=c(1,1)</code> and <code>time2=c(1,1)</code> would imply a between-subjects design involving three groups where two effect sizes were computed contrasting groups 2 and 3 versus group 1 at a single time point. On the other hand, <code>grp1=c(1,1)</code> and <code>grp2=c(1,1)</code> in combination with <code>time1=c(2,3)</code> and <code>time2=c(1,1)</code> would imply a within-subjects design where two effect sizes were computed contrasting time points 2 and 3 versus time point 1 in a single group. Argument <code>phi</code> is then used as above to indicate the autocorrelation of the measurements within groups (i.e., for the within-subjects design above, it would be the autocorrelation between time points 2 and 1 or equivalently, between time points 3 and 2).
</p>
</li></ol>

<p>All of the arguments above can be specified together to account for a fairly wide variety of dependency types.
</p>


<h4>Using the <code>rvars</code> Argument</h4>

<p>The function also provides an alternative approach for constructing the variance-covariance matrix using the <code>rvars</code> argument. Here, one must specify the names of the variables in the dataset that correspond to the correlation matrices of the studies (the variables should be specified as a vector; e.g., <code>c(var1, var2, var3)</code>).
</p>
<p>In particular, let \(k_i\) denote the number of rows corresponding to the \(i\textrm{th}\) cluster. Then the values of the first \(k_i\) variables from <code>rvars</code> are used to construct the correlation matrix and, together with the sampling variances (specified via <code>vi</code>), the variance-covariance matrix. Say there are three studies, the first with two correlated estimates, the second with a single estimate, and the third with four correlated estimates. Then the data structure should look like this:
</p>
<pre>study  yi  vi  r1  r2  r3  r4
=============================
    1   .   .   1  NA  NA  NA
    1   .   .  .6   1  NA  NA
-----------------------------
    2   .   .   1  NA  NA  NA
-----------------------------
    3   .   .   1  NA  NA  NA
    3   .   .  .8   1  NA  NA
    3   .   .  .5  .5   1  NA
    3   .   .  .5  .5  .8   1
=============================</pre>
<p>with <code>rvars = c(r1, r2, r3, r4)</code>. If the <code>rvars</code> variables are a consecutive set in the data frame (as above), then one can use the shorthand notation <code>rvars = c(r1:r4)</code>, so <code>r1</code> denotes the first and <code>r4</code> the last variable in the set. Note that only the lower triangular part of the submatrices defined by the <code>rvars</code> variables is used.
</p>
<p>There must be as many variables specified via <code>rvars</code> as the number of rows in the <em>largest</em> cluster (in smaller clusters, the non-relevant variables can just be set to <code>NA</code>; see above).
</p>



<h3>Value</h3>

<p>A \(k \times k\) variance-covariance matrix (given as a sparse matrix when <code>sparse=TRUE</code>), where \(k\) denotes the length of the <code>vi</code> variable (i.e., the number of rows in the dataset).
</p>


<h3>Note</h3>

<p>Depending on the data structure, the specified variables, and the specified values for <code>rho</code> and/or <code>phi</code>, it is possible that the constructed variance-covariance matrix is not positive definite within one or more clusters (this is checked when <code>checkpd=TRUE</code>, which is the default). If such non-positive definite submatrices are found, the reasons for this should be carefully checked since this might indicate misapplication of the function and/or the specification of implausible values for <code>rho</code> and/or <code>phi</code>.
</p>
<p>When setting <code>nearpd=TRUE</code>, the <code><a href="Matrix.html#topic+nearPD">nearPD</a></code> function from the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package is used on variance-covariance submatrices that are not positive definite. This should only be used cautiously and after understanding why these matrices are not positive definite.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+escalc">escalc</a></code> for a function to compute the observed effect sizes or outcomes (and corresponding sampling variances) for which a variance-covariance matrix could be constructed.
</p>
<p><code><a href="#topic+rcalc">rcalc</a></code> for a function to construct the variance-covariance matrix of dependent correlation coefficients.
</p>
<p><code><a href="#topic+rma.mv">rma.mv</a></code> for a model fitting function that can be used to meta-analyze dependent effect sizes or outcomes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################################

### see help(dat.assink2016) for further details on this dataset

dat &lt;- dat.assink2016
head(dat, 9)

### assume that the effect sizes within studies are correlated with rho=0.6
V &lt;- vcalc(vi, cluster=study, obs=esid, data=dat, rho=0.6)

### show part of V matrix for studies 1 and 2
round(V[dat$study %in% c(1,2), dat$study %in% c(1,2)], 4)

### or show as list of matrices
blsplit(V, dat$study, round, 4)[1:2]

### use a correlation of 0.7 for effect sizes corresponding to the same type of
### delinquent behavior and a correlation of 0.5 for effect sizes corresponding
### to different types of delinquent behavior
V &lt;- vcalc(vi, cluster=study, type=deltype, obs=esid, data=dat, rho=c(0.7, 0.5))
blsplit(V, dat$study, round, 3)[16]

### examine the correlation matrix for study 16
blsplit(V, dat$study, cov2cor)[16]

############################################################################

### see help(dat.ishak2007) for further details on this dataset

dat &lt;- dat.ishak2007
head(dat, 5)

### create long format dataset
dat &lt;- reshape(dat, direction="long", idvar="study", v.names=c("yi","vi"),
               varying=list(c(2,4,6,8), c(3,5,7,9)))
dat &lt;- dat[order(study, time),]

### remove missing measurement occasions from dat
dat &lt;- dat[!is.na(yi),]
rownames(dat) &lt;- NULL

### show the data for the first 5 studies
head(dat, 8)

### construct the full (block diagonal) V matrix with an AR(1) structure
### assuming an autocorrelation of 0.97 as estimated by Ishak et al. (2007)
V &lt;- vcalc(vi, cluster=study, time1=time, phi=0.97, data=dat)
V[1:8, 1:8]
cov2cor(V[1:8, 1:8])

### or show as a list of matrices
blsplit(V, dat$study)[1:5]
blsplit(V, dat$study, cov2cor)[1:5]

############################################################################

### see help(dat.kalaian1996) for further details on this dataset

dat &lt;- dat.kalaian1996
head(dat, 12)

### construct the variance-covariance matrix assuming rho = 0.66 for effect sizes
### corresponding to the 'verbal' and 'math' outcome types
V &lt;- vcalc(vi, cluster=study, type=outcome, data=dat, rho=0.66)
round(V[1:12,1:12], 4)

############################################################################

### see help(dat.berkey1998) for further details on this dataset

dat &lt;- dat.berkey1998

### variables v1i and v2i correspond to the 2x2 var-cov matrices of the studies;
### so use these variables to construct the V matrix (note: since v1i and v2i are
### var-cov matrices and not correlation matrices, set vi=1 for all rows)
V &lt;- vcalc(vi=1, cluster=author, rvars=c(v1i, v2i), data=dat)
V
round(cov2cor(V), 2)

### or show as a list of matrices
blsplit(V, dat$author, function(x) round(cov2cor(x), 2))

### construct the variance-covariance matrix assuming rho = 0.4 for effect sizes
### corresponding to the 'PD' and 'AL' outcome types
V &lt;- vcalc(vi=vi, cluster=trial, type=outcome, data=dat, rho=0.4)
round(V,4)
cov2cor(V)

############################################################################

### see help(dat.knapp2017) for further details on this dataset

dat &lt;- dat.knapp2017
dat[-c(1:2)]

### create variable that indicates the task and difficulty combination as increasing integers
dat$task.diff &lt;- unlist(lapply(split(dat, dat$study), function(x) {
   task.int &lt;- as.integer(factor(x$task))
   diff.int &lt;- as.integer(factor(x$difficulty))
   diff.int[is.na(diff.int)] &lt;- 1
   paste0(task.int, ".", diff.int)}))

### construct correlation matrix for two tasks with four different difficulties where the
### correlation is 0.4 for different difficulties of the same task, 0.7 for the same
### difficulty of different tasks, and 0.28 for different difficulties of different tasks
R &lt;- matrix(0.4, nrow=8, ncol=8)
R[5:8,1:4] &lt;- R[1:4,5:8] &lt;- 0.28
diag(R[1:4,5:8]) &lt;- 0.7
diag(R[5:8,1:4]) &lt;- 0.7
diag(R) &lt;- 1
rownames(R) &lt;- colnames(R) &lt;- paste0(rep(1:2, each=4), ".", 1:4)
R

### construct an approximate V matrix accounting for the use of shared groups and
### for correlations among tasks/difficulties as specified in the R matrix above
V &lt;- vcalc(vi, cluster=study, grp1=group1, grp2=group2, w1=n_sz, w2=n_hc,
           obs=task.diff, rho=R, data=dat)
Vs &lt;- blsplit(V, dat$study)
cov2cor(Vs[[3]])  # study with multiple SZ groups and a single HC group
cov2cor(Vs[[6]])  # study with two task types and multiple difficulties
cov2cor(Vs[[12]]) # study with multiple difficulties for the same task
cov2cor(Vs[[24]]) # study with separate rows for males and females
cov2cor(Vs[[29]]) # study with separate rows for three genotypes

############################################################################
</code></pre>

<hr>
<h2 id='vcov.rma'>Extract Various Types of Variance-Covariance Matrices from 'rma' Objects</h2><span id='topic+vcov'></span><span id='topic+vcov.rma'></span>

<h3>Description</h3>

<p>Function to extract various types of variance-covariance matrices from objects of class <code>"rma"</code>. By default, the variance-covariance matrix of the fixed effects is returned. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
vcov(object, type="fixed", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma"</code>.</p>
</td></tr>
<tr><td><code id="vcov.rma_+3A_type">type</code></td>
<td>
<p>character string to specify the type of variance-covariance matrix to return: <code>type="fixed"</code> returns the variance-covariance matrix of the fixed effects (the default), <code>type="obs"</code> returns the marginal variance-covariance matrix of the observed effect sizes or outcomes, <code>type="fitted"</code> returns the variance-covariance matrix of the fitted values, <code>type="resid"</code> returns the variance-covariance matrix of the residuals.</p>
</td></tr>
<tr><td><code id="vcov.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code>type="obs"</code> currently only works for object of class <code>"rma.uni"</code> and <code>"rma.mv"</code>.
</p>
<p>For objects of class <code>"rma.uni"</code>, the marginal variance-covariance matrix of the observed effect sizes or outcomes is just a diagonal matrix with \(\hat{\tau}^2 + v_i\) along the diagonal, where \(\hat{\tau}^2\) is the estimated amount of (residual) heterogeneity (set to 0 in equal-effects models) and \(v_i\) is the sampling variance of the \(i\textrm{th}\) study.
</p>
<p>For objects of class <code>"rma.mv"</code>, the structure can be more complex and depends on the random effects included in the model.
</p>


<h3>Value</h3>

<p>A matrix corresponding to the requested variance-covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which the various types of variance-covariance matrices can be extracted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### var-cov matrix of the fixed effects (i.e., the model coefficients)
vcov(res)

### marginal var-cov matrix of the observed log risk ratios
round(vcov(res, type="obs"), 3)

### var-cov matrix of the fitted values
round(vcov(res, type="fitted"), 3)

### var-cov matrix of the residuals
round(vcov(res, type="resid"), 3)
</code></pre>

<hr>
<h2 id='vec2mat'>Convert a Vector into a Square Matrix</h2><span id='topic+vec2mat'></span>

<h3>Description</h3>

<p>Function to convert a vector into a square matrix by filling up the lower triangular part of the matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vec2mat(x, diag=FALSE, corr=!diag, dimnames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vec2mat_+3A_x">x</code></td>
<td>
<p>a vector of the correct length.</p>
</td></tr>
<tr><td><code id="vec2mat_+3A_diag">diag</code></td>
<td>
<p>logical to specify whether the vector also contains the diagonal values of the lower triangular part of the matrix (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="vec2mat_+3A_corr">corr</code></td>
<td>
<p>logical to specify whether the diagonal of the matrix should be replaced with 1's (the default is to do this when <code>diag=FALSE</code>).</p>
</td></tr>
<tr><td><code id="vec2mat_+3A_dimnames">dimnames</code></td>
<td>
<p>optional vector of the correct length with the dimension names of the matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The values in <code>x</code> are filled into the lower triangular part of a square matrix with the appropriate dimensions (which are determined based on the length of <code>x</code>). If <code>diag=TRUE</code>, then <code>x</code> is assumed to also contain the diagonal values of the lower triangular part of the matrix. If <code>corr=TRUE</code>, then the diagonal of the matrix is replaced with 1's.
</p>


<h3>Value</h3>

<p>A matrix.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vec2mat(1:6, corr=FALSE)
vec2mat(seq(0.2, 0.7, by=0.1), corr=TRUE)
vec2mat(1:10, diag=TRUE)
vec2mat(1:6, corr=FALSE, dimnames=c("A","B","C","D"))
</code></pre>

<hr>
<h2 id='vif'>Variance Inflation Factors for 'rma' Objects</h2><span id='topic+vif'></span><span id='topic+vif.rma'></span><span id='topic+print.vif.rma'></span>

<h3>Description</h3>

<p>Function to compute (generalized) variance inflation factors (VIFs) for objects of class <code>"rma"</code>. <script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vif(x, ...)

## S3 method for class 'rma'
vif(x, btt, att, table=FALSE, reestimate=FALSE, sim=FALSE, progbar=TRUE,
    seed=NULL, parallel="no", ncpus=1, cl, digits, ...)

## S3 method for class 'vif.rma'
print(x, digits=x$digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vif_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rma"</code> (for <code>vif</code>) or <code>"vif.rma"</code> (for <code>print</code>).</p>
</td></tr>
<tr><td><code id="vif_+3A_btt">btt</code></td>
<td>
<p>optional vector of indices (or list thereof) to specify a set of coefficients for which a generalized variance inflation factor (GVIF) should be computed. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for.</p>
</td></tr>
<tr><td><code id="vif_+3A_att">att</code></td>
<td>
<p>optional vector of indices (or list thereof) to specify a set of scale coefficients for which a generalized variance inflation factor (GVIF) should be computed. Can also be a string to <code><a href="base.html#topic+grep">grep</a></code> for. Only relevant for location-scale models (see <code><a href="#topic+rma.uni">rma.uni</a></code>).</p>
</td></tr>
<tr><td><code id="vif_+3A_table">table</code></td>
<td>
<p>logical to specify whether the VIFs should be added to the model coefficient table (the default is <code>FALSE</code>). Only relevant when <code>btt</code> (or <code>att</code>) is not specified.</p>
</td></tr>
<tr><td><code id="vif_+3A_reestimate">reestimate</code></td>
<td>
<p>logical to specify whether the model should be reestimated when removing moderator variables from the model for computing a (G)VIF (the default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="vif_+3A_sim">sim</code></td>
<td>
<p>logical to specify whether the distribution of each (G)VIF under independence should be simulated (the default is <code>FALSE</code>). Can also be an integer to specify how many values to simulate (when <code>sim=TRUE</code>, the default is <code>1000</code>).</p>
</td></tr>
<tr><td><code id="vif_+3A_progbar">progbar</code></td>
<td>
<p>logical to specify whether a progress bar should be shown when <code>sim=TRUE</code> (the default is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="vif_+3A_seed">seed</code></td>
<td>
<p>optional value to specify the seed of the random number generator when <code>sim=TRUE</code> (for reproducibility).</p>
</td></tr>
<tr><td><code id="vif_+3A_parallel">parallel</code></td>
<td>
<p>character string to specify whether parallel processing should be used (the default is <code>"no"</code>). For parallel processing, set to either <code>"snow"</code> or <code>"multicore"</code>. See &lsquo;Note&rsquo;.</p>
</td></tr>
<tr><td><code id="vif_+3A_ncpus">ncpus</code></td>
<td>
<p>integer to specify the number of processes to use in the parallel processing.</p>
</td></tr>
<tr><td><code id="vif_+3A_cl">cl</code></td>
<td>
<p>optional cluster to use if <code>parallel="snow"</code>. If unspecified, a cluster on the local machine is created for the duration of the call.</p>
</td></tr>
<tr><td><code id="vif_+3A_digits">digits</code></td>
<td>
<p>optional integer to specify the number of decimal places to which the printed results should be rounded. If unspecified, the default is to take the value from the object.</p>
</td></tr>
<tr><td><code id="vif_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes (generalized) variance inflation factors (VIFs) for meta-regression models. Hence, the model specified via argument <code>x</code> must include moderator variables (and more than one for this to be useful, as the VIF for a model with a single moderator variable will always be equal to 1).
</p>


<h4>VIFs for Individual Coefficients</h4>

<p>By default (i.e., if <code>btt</code> is not specified), VIFs are computed for the individual model coefficients.
</p>
<p>Let \(b_j\) denote the estimate of the \(j\textrm{th}\) model coefficient of a particular meta-regression model and \(\mbox{Var}[b_j]\) its variance (i.e., the corresponding diagonal element from the matrix obtained with the <code><a href="#topic+vcov.rma">vcov</a></code> function). Moreover, let \(b'_j\) denote the estimate of the same model coefficient if the other moderator variables in the model had <em>not</em> been included in the model and \(\mbox{Var}[b'_j]\) the corresponding variance. Then the VIF for the model coefficient is given by \[\mbox{VIF}[b_j] = \frac{\mbox{Var}[b_j]}{\mbox{Var}[b'_j]},\] which indicates the inflation in the variance of the estimated model coefficient due to potential collinearity of the \(j\textrm{th}\) moderator variable with the other moderator variables in the model. Taking the square root of a VIF gives the corresponding standard error inflation factor (SIF).
</p>



<h4>GVIFs for Sets of Coefficients</h4>

<p>If the model includes factors (coded in terms of multiple dummy variables) or other sets of moderator variables that belong together (e.g., for polynomials or cubic splines), one may want to examine how much the variance in all of the coefficients in the set is jointly impacted by collinearity with the other moderator variables in the model. For this, we can compute a generalized variance inflation factor (GVIF) (Fox &amp; Monette, 1992) by setting the <code>btt</code> argument equal to the indices of those coefficients for which the GVIF should be computed. The square root of a GVIF indicates the inflation in the confidence ellipse/(hyper)ellipsoid for the set of coefficients corresponding to the set due to collinearity. However, to make this value more directly comparable to SIFs (based on single coefficients), the function computes the generalized standard error inflation factor (GSIF) by raising the GVIF to the power of \(1/(2m)\) (where \(m\) denotes the number of coefficients in the set). One can also specify a list of indices/strings, in which case GVIFs/GSIFs of all list elements will be computed. See &lsquo;Examples&rsquo;.
</p>
<p>For location-scale models fitted with the <code><a href="#topic+rma.uni">rma.uni</a></code> function, one can use the <code>att</code> argument in an analogous manner to specify the indices of the scale coefficients for which a GVIF should be computed.
</p>



<h4>Re-Estimating the Model</h4>

<p>The way the VIF is typically computed for a particular model coefficient (or a set thereof for a GVIF) makes use of some clever linear algebra to avoid having to re-estimate the model when removing the other moderator variables from the model. This speeds up the computations considerably. However, this assumes that the other moderator variables do not impact other aspects of the model, in particular the amount of residual heterogeneity (or, more generally, any variance/correlation components in a more complex model, such as those that can be fitted with the <code><a href="#topic+rma.mv">rma.mv</a></code> function).
</p>
<p>For a more accurate (but slower) computation of each (G)VIF, one can set <code>reestimate=TRUE</code>, in which case the model is refitted to account for the impact that removal of the other moderator variables has on all aspects of the model. Note that refitting may fail, in which case the corresponding (G)VIF will be missing.
</p>



<h4>Interpreting the Size of (G)VIFs</h4>

<p>A large VIF value suggests that the precision with which we can estimate a particular model coefficient (or a set thereof for a GVIF) is negatively impacted by multicollinearity among the moderator variables. However, there is no specific cutoff for determining whether a particular (G)VIF is &lsquo;large&rsquo;. Sometimes, values such as 5 or 10 have been suggested as rules of thumb, but such cutoffs are essentially arbitrary.
</p>



<h4>Simulating the Distribution of (G)VIFs Under Independence</h4>

<p>As a more principled approach, we can simulate the distribution of a particular (G)VIF under independence and then examine how extreme the actually observed (G)VIF value is under this distribution. The distribution is simulated by randomly reshuffling the columns of the model matrix (to break any dependence between the moderators) and recomputing the (G)VIF. When setting <code>sim=TRUE</code>, this is done 1000 times (but one can also set <code>sim</code> to an integer to indicate how many (G)VIF values should be simulated).
</p>
<p>The way the model matrix is reshuffled depends on how the model was fitted. When the model was specified as a formula via the <code>mods</code> argument and the data was supplied via the <code>data</code> argument, then each column of the data frame specified via <code>data</code> is reshuffled and the formula is evaluated within the reshuffled data (creating the corresponding reshuffled model matrix). This way, factor/character variables are properly reshuffled and derived terms (e.g., interactions, polynomials, splines) are correct constructed. This is the recommended approach.
</p>
<p>On the other hand, if the model matrix was directly supplied to the <code>mods</code> argument, then each column of the matrix is directly reshuffled. This is not recommended, since this approach cannot account for any inherent relationships between variables in the model matrix (e.g., an interaction term is the product of two variables and should not be reshuffled by itself).
</p>
<p>Once the distribution of a (G)VIF under independence has been simulated, the proportion of simulated values that are smaller than the actually observed (G)VIF value is computed. If the proportion is close to 1, then this indicates that the actually observed (G)VIF value is extreme.
</p>
<p>The general principle underlying the simulation approach is the same as that underlying Horn's parallel analysis (1965) for determining the number of components / factors to keep in a principal component / factor analysis.
</p>



<h3>Value</h3>

<p>An object of class <code>"vif.rma"</code>. The object is a list containing the following components:
</p>
<table>
<tr><td><code>vif</code></td>
<td>
<p>a list of data frames with the (G)VIFs and (G)SIFs and some additional information.</p>
</td></tr>
<tr><td><code>vifs</code></td>
<td>
<p>a vector with just the (G)VIFs.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the model coefficient table (only when <code>table=TRUE</code>).</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>a matrix with the simulated (G)VIF values (only when <code>sim=TRUE</code>).</p>
</td></tr>
<tr><td><code>prop</code></td>
<td>
<p>vector with the proportions of simulated values that are smaller than the actually observed (G)VIF values (only when <code>sim=TRUE</code>).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional elements/values.</p>
</td></tr>
</table>
<p>When <code>x</code> was a location-scale model object and (G)VIFs can be computed for both the location and the scale coefficients, then the object is a list with elements <code>beta</code> and <code>alpha</code>, where each element is an <code>"vif.rma"</code> object as described above.
</p>
<p>The results are formatted and printed with the <code>print</code> function. To format the results as a data frame, one can use the <code><a href="#topic+as.data.frame.vif.rma">as.data.frame</a></code> function. When <code>sim=TRUE</code>, the distribution of each (G)VIF can be plotted with the <code><a href="#topic+plot.vif.rma">plot</a></code> function.
</p>


<h3>Note</h3>

<p>If the original model fitted involved redundant predictors that were dropped from the model, then <code>sim=TRUE</code> cannot be used. In this case, one should remove any redundancies in the original model fitted before using this method.
</p>
<p>When using <code>sim=TRUE</code>, the model needs to be refitted (by default) 1000 times. When <code>sim=TRUE</code> is combined with <code>reestimate=TRUE</code>, then this value needs to be multiplied by the total number of (G)VIF values that are computed by the function. Hence, the combination of <code>sim=TRUE</code> with <code>reestimate=TRUE</code> is computationally expensive, especially for more complex models where model fitting can be slow.
</p>
<p>When refitting the model fails, the simulated (G)VIF value(s) will be missing. It can also happen that one or multiple model coefficients become inestimable due to redundancies in the model matrix after the reshuffling. In this case, the corresponding simulated (G)VIF value(s) will be set to <code>Inf</code> (as that is the value of (G)VIFs in the limit as we approach perfect multicollinearity).
</p>
<p>On machines with multiple cores, one can try to speed things up by delegating the model fitting to separate worker processes, that is, by setting <code>parallel="snow"</code> or <code>parallel="multicore"</code> and <code>ncpus</code> to some value larger than 1. Parallel processing makes use of the <code><a href="parallel.html#topic+parallel">parallel</a></code> package, using the <code><a href="parallel.html#topic+makePSOCKcluster">makePSOCKcluster</a></code> and <code><a href="parallel.html#topic+parLapply">parLapply</a></code> functions when <code>parallel="snow"</code> or using <code><a href="parallel.html#topic+mclapply">mclapply</a></code> when <code>parallel="multicore"</code> (the latter only works on Unix/Linux-alikes). With <code>parallel::detectCores()</code>, one can check on the number of available cores on the local machine.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E., &amp; Welsch, R. E. (1980). <em>Regression diagnostics</em>. New York: Wiley.
</p>
<p>Fox, J., &amp; Monette, G. (1992). Generalized collinearity diagnostics. <em>Journal of the American Statistical Association</em>, <b>87</b>(417), 178&ndash;183. <code style="white-space: pre;">&#8288;https://doi.org/10.2307/2290467&#8288;</code>
</p>
<p>Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. <em>Psychometrika</em>, <b>30</b>(2), 179&ndash;185. <code style="white-space: pre;">&#8288;https://doi.org/10.1007/BF02289447&#8288;</code>
</p>
<p>Stewart, G. W. (1987). Collinearity and least squares regression. <em>Statistical Science</em>, <b>2</b>(1), 68-84. <code style="white-space: pre;">&#8288;https://doi.org/10.1214/ss/1177013439&#8288;</code>
</p>
<p>Wax, Y. (1992). Collinearity diagnosis for a relative risk regression-analysis: An application to assessment of diet cancer relationship in epidemiologic studies. <em>Statistics in Medicine</em>, <b>11</b>(10), 1273&ndash;1287. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/sim.4780111003&#8288;</code>
</p>
<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W., &amp; L√≥pez-L√≥pez, J. A. (2022). Location-scale models for meta-analysis. <em>Research Synthesis Methods</em>. <b>13</b>(6), 697&ndash;715. <code style="white-space: pre;">&#8288;https://doi.org/10.1002/jrsm.1562&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.glmm">rma.glmm</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which variance inflation factors can be computed.
</p>
<p><code><a href="#topic+plot.vif.rma">plot</a></code> for the plot method and <code><a href="#topic+as.data.frame.vif.rma">as.data.frame</a></code> for the method to format the results as a data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### copy data from Bangert-Drowns et al. (2004) into 'dat'
dat &lt;- dat.bangertdrowns2004

### fit mixed-effects meta-regression model
res &lt;- rma(yi, vi, mods = ~ length + wic + feedback + info + pers + imag + meta, data=dat)

### get variance inflation factors
vif(res)

### use the simulation approach to analyze the size of the VIFs
## Not run: 
vif(res, sim=TRUE, seed=1234)

## End(Not run)

### get variance inflation factors using the re-estimation approach
vif(res, reestimate=TRUE)

### show that VIFs are not influenced by scaling of the predictors
u &lt;- scale # to standardize the predictors
res &lt;- rma(yi, vi, mods = ~ u(length) + u(wic) + u(feedback) + u(info) +
                            u(pers) + u(imag) + u(meta), data=dat)
vif(res, reestimate=TRUE)

### get full table
vif(res, reestimate=TRUE, table=TRUE)

### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit meta-regression model where one predictor (alloc) is a three-level factor
res &lt;- rma(yi, vi, mods = ~ ablat + alloc + year, data=dat)

### get variance inflation factors for all individual coefficients
vif(res, table=TRUE)

### generalized variance inflation factor for the 'alloc' factor
vif(res, btt=3:4)

### can also specify a string to grep for
vif(res, btt="alloc")

### can also specify a list for the 'btt' argument (and use the simulation approach)
## Not run: 
vif(res, btt=list(2,3:4,5), sim=TRUE, seed=1234)

## End(Not run)
</code></pre>

<hr>
<h2 id='weights.rma'>Compute Weights for 'rma' Objects</h2><span id='topic+weights'></span><span id='topic+weights.rma'></span><span id='topic+weights.rma.uni'></span><span id='topic+weights.rma.mh'></span><span id='topic+weights.rma.peto'></span><span id='topic+weights.rma.glmm'></span><span id='topic+weights.rma.mv'></span>

<h3>Description</h3>

<p>Functions to compute the weights given to the observed effect sizes or outcomes during the model fitting for objects of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, and <code>"rma.mv"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma.uni'
weights(object, type="diagonal", ...)
## S3 method for class 'rma.mh'
weights(object, type="diagonal", ...)
## S3 method for class 'rma.peto'
weights(object, type="diagonal", ...)
## S3 method for class 'rma.glmm'
weights(object, ...)
## S3 method for class 'rma.mv'
weights(object, type="diagonal", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights.rma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>, or <code>"rma.mv"</code>. The method is not yet implemented for objects of class <code>"rma.glmm"</code>.</p>
</td></tr>
<tr><td><code id="weights.rma_+3A_type">type</code></td>
<td>
<p>character string to specify whether to return only the diagonal of the weight matrix (<code>"diagonal"</code>) or the entire weight matrix (<code>"matrix"</code>). For <code>"rma.mv"</code>, this can also be <code>"rowsum"</code> for &lsquo;row-sum weights&rsquo; (for intercept-only models).</p>
</td></tr>
<tr><td><code id="weights.rma_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector with the diagonal elements of the weight matrix or the entire weight matrix. When only the diagonal elements are returned, they are given in % (and they add up to 100%).
</p>
<p>When the entire weight matrix is requested, this is always a diagonal matrix for objects of class <code>"rma.uni"</code>, <code>"rma.mh"</code>, <code>"rma.peto"</code>.
</p>
<p>For <code>"rma.mv"</code>, the structure of the weight matrix depends on the model fitted (i.e., the random effects included and the variance-covariance matrix of the sampling errors) but is often more complex and not just diagonal.
</p>
<p>For intercept-only <code>"rma.mv"</code> models, one can also take the sum over the rows in the weight matrix, which are actually the weights assigned to the observed effect sizes or outcomes when estimating the model intercept. These weights can be obtained with <code>type="rowsum"</code> (as with <code>type="diagonal"</code>, they are also given in %). See <a href="https://www.metafor-project.org/doku.php/tips:weights_in_rma.mv_models">here</a> for a discussion of this.
</p>


<h3>Author(s)</h3>

<p>Wolfgang Viechtbauer <a href="mailto:wvb@metafor-project.org">wvb@metafor-project.org</a> <a href="https://www.metafor-project.org">https://www.metafor-project.org</a>
</p>


<h3>References</h3>

<p>Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <em>Journal of Statistical Software</em>, <b>36</b>(3), 1&ndash;48. <code style="white-space: pre;">&#8288;https://doi.org/10.18637/jss.v036.i03&#8288;</code>
</p>
<p>Viechtbauer, W. (2021). Model checking in meta-analysis. In C. H. Schmid, T. Stijnen, &amp; I. R. White (Eds.), <em>Handbook of meta-analysis</em> (pp. 219&ndash;254). Boca Raton, FL: CRC Press. <code style="white-space: pre;">&#8288;https://doi.org/10.1201/9781315119403&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rma.uni">rma.uni</a></code>, <code><a href="#topic+rma.mh">rma.mh</a></code>, <code><a href="#topic+rma.peto">rma.peto</a></code>, and <code><a href="#topic+rma.mv">rma.mv</a></code> for functions to fit models for which model fitting weights can be extracted.
</p>
<p><code><a href="#topic+influence.rma.uni">influence.rma.uni</a></code> and <code><a href="#topic+influence.rma.mv">influence.rma.mv</a></code> for other model diagnostics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### calculate log risk ratios and corresponding sampling variances
dat &lt;- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)

### fit mixed-effects model with absolute latitude and publication year as moderators
res &lt;- rma(yi, vi, mods = ~ ablat + year, data=dat)

### extract the model fitting weights (in %)
weights(res)

### extract the weight matrix
round(weights(res, type="matrix"), 4)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
