<!DOCTYPE html><html><head><title>Help for package neuralGAM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {neuralGAM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#neuralGAM-package'><p>neuralGAM: Interpretable Neural Network Based on Generalized Additive Models</p></a></li>
<li><a href='#autoplot.neuralGAM'><p>Advanced <code>neuralGAM</code> visualization with ggplot2 library</p></a></li>
<li><a href='#build_feature_NN'><p>Build and compile a single Neural Network</p></a></li>
<li><a href='#dev'><p>Deviance of the model</p></a></li>
<li><a href='#diriv'><p>Derivative of the link function</p></a></li>
<li><a href='#get_formula_elements'><p>Extract formula elements</p></a></li>
<li><a href='#install_neuralGAM'><p>Install neuralGAM python requirements</p></a></li>
<li><a href='#inv_link'><p>Inverse of the link functions</p></a></li>
<li><a href='#link'><p>Link function</p></a></li>
<li><a href='#neuralGAM'><p>Fit a <code>neuralGAM</code> model</p></a></li>
<li><a href='#plot.neuralGAM'><p>Visualization of <code>neuralGAM</code> object with base graphics</p></a></li>
<li><a href='#predict.neuralGAM'><p>Produces predictions from a fitted <code>neuralGAM</code> object</p></a></li>
<li><a href='#print.neuralGAM'><p>Short <code>neuralGAM</code> summary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#summary.neuralGAM'><p><code>neuralGAM</code> summary</p></a></li>
<li><a href='#weight'><p>Weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interpretable Neural Network Based on Generalized Additive
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ines Ortega-Fernandez &lt;iortega@gradiant.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Neural network framework based on Generalized Additive Models from Hastie &amp; Tibshirani (1990, ISBN:9780412343902), which trains a different neural network to estimate the contribution of each feature to the response variable. The networks are trained independently leveraging the local scoring and backfitting algorithms to ensure that the Generalized Additive Model converges and it is additive. The resultant Neural Network is a highly accurate and interpretable deep learning model, which can be used for high-risk AI practices where decision-making should be based on accountable and interpretable algorithms. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.mozilla.org/en-US/MPL/2.0/">MPL-2.0</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/inesortega/neuralGAM/issues">https://github.com/inesortega/neuralGAM/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>tensorflow, keras, ggplot2, magrittr, reticulate,
formula.tools, gridExtra</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>python (&gt;= 3.10), keras, tensorflow</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, testthat (&ge; 3.0.0), fs, withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://inesortega.github.io/neuralGAM/">https://inesortega.github.io/neuralGAM/</a>,
<a href="https://github.com/inesortega/neuralGAM">https://github.com/inesortega/neuralGAM</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-19 17:18:30 UTC; iortega</td>
</tr>
<tr>
<td>Author:</td>
<td>Ines Ortega-Fernandez
    <a href="https://orcid.org/0000-0002-8041-6860"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre,
    cph],
  Marta Sestelo <a href="https://orcid.org/0000-0003-4284-6509"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-19 17:42:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='neuralGAM-package'>neuralGAM: Interpretable Neural Network Based on Generalized Additive Models</h2><span id='topic+neuralGAM-package'></span><span id='topic+_PACKAGE'></span>

<h3>Description</h3>

<p>Neural network framework based on Generalized Additive Models from Hastie &amp; Tibshirani (1990, ISBN:9780412343902), which trains a different neural network to estimate the contribution of each feature to the response variable. The networks are trained independently leveraging the local scoring and backfitting algorithms to ensure that the Generalized Additive Model converges and it is additive. The resultant Neural Network is a highly accurate and interpretable deep learning model, which can be used for high-risk AI practices where decision-making should be based on accountable and interpretable algorithms.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Ines Ortega-Fernandez <a href="mailto:iortega@gradiant.org">iortega@gradiant.org</a> (<a href="https://orcid.org/0000-0002-8041-6860">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Marta Sestelo <a href="mailto:sestelo@uvigo.es">sestelo@uvigo.es</a> (<a href="https://orcid.org/0000-0003-4284-6509">ORCID</a>) [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://inesortega.github.io/neuralGAM/">https://inesortega.github.io/neuralGAM/</a>
</p>
</li>
<li> <p><a href="https://github.com/inesortega/neuralGAM">https://github.com/inesortega/neuralGAM</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/inesortega/neuralGAM/issues">https://github.com/inesortega/neuralGAM/issues</a>
</p>
</li></ul>


<hr>
<h2 id='autoplot.neuralGAM'>Advanced <code>neuralGAM</code> visualization with ggplot2 library</h2><span id='topic+autoplot.neuralGAM'></span>

<h3>Description</h3>

<p>Advanced <code>neuralGAM</code> visualization with ggplot2 library
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neuralGAM'
autoplot(object, select, xlab = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.neuralGAM_+3A_object">object</code></td>
<td>
<p>a fitted <code>neuralGAM</code> object as produced by <code>neuralGAM()</code>.</p>
</td></tr>
<tr><td><code id="autoplot.neuralGAM_+3A_select">select</code></td>
<td>
<p>selects the term to be plotted.</p>
</td></tr>
<tr><td><code id="autoplot.neuralGAM_+3A_xlab">xlab</code></td>
<td>
<p>A title for the <code>x</code> axis.</p>
</td></tr>
<tr><td><code id="autoplot.neuralGAM_+3A_ylab">ylab</code></td>
<td>
<p>A title for the <code>y</code> axis.</p>
</td></tr>
<tr><td><code id="autoplot.neuralGAM_+3A_...">...</code></td>
<td>
<p>other graphics parameters to pass on to plotting commands.
See details for ggplot2::geom_line options</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object, so you can use common features from the ggplot2 package
to manipulate the plot.
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;- x1 ** 2
f2 &lt;- 2 * x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )
autoplot(ngam, select="x1")

# add custom title
autoplot(ngam, select="x1") + ggplot2::ggtitle("Main Title")
# add labels
autoplot(ngam, select="x1") + ggplot2::xlab("test") + ggplot2::ylab("my y lab")
# plot multiple terms:
plots &lt;- lapply(c("x1", "x2", "x3"), function(x) autoplot(ngam, select = x))
gridExtra::grid.arrange(grobs = plots, ncol = 3, nrow = 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='build_feature_NN'>Build and compile a single Neural Network</h2><span id='topic+build_feature_NN'></span>

<h3>Description</h3>

<p>Builds and compiles a neural network using the keras library.
The architecture of the neural network is configurable using the
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_feature_NN(
  num_units,
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mean_squared_error",
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_feature_NN_+3A_num_units">num_units</code></td>
<td>
<p>Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a vector of values is provided, a multi-layer neural network with each element of the vector defining
the number of hidden units on each hidden layer is used.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Learning rate for the neural network optimizer.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_activation">activation</code></td>
<td>
<p>Activation function of the neural network. Defaults to <code>relu</code></p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (<code>glorot_normal</code>).</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_kernel_regularizer">kernel_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the kernel weights matrix.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_bias_regularizer">bias_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the bias vector.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_bias_initializer">bias_initializer</code></td>
<td>
<p>Optional initializer for the bias vector.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the output of the layer</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_loss">loss</code></td>
<td>
<p>Loss function to use during neural network training. Defaults to the mean squared error.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_name">name</code></td>
<td>
<p>Neural Network name.</p>
</td></tr>
<tr><td><code id="build_feature_NN_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+neuralGAM">neuralGAM</a></code>
</p>

<dl>
<dt><code>formula</code></dt><dd><p>An object of class &quot;formula&quot;: a description of the model to be fitted. You can add smooth terms using <code>s()</code>.</p>
</dd>
<dt><code>data</code></dt><dd><p>A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.</p>
</dd>
<dt><code>family</code></dt><dd><p>This is a family object specifying the distribution and link to use for fitting.
By default, it is <code>"gaussian"</code> but also works to <code>"binomial"</code> for logistic regression.</p>
</dd>
<dt><code>bf_threshold</code></dt><dd><p>Convergence criterion of the backfitting algorithm.
Defaults to <code>0.001</code></p>
</dd>
<dt><code>ls_threshold</code></dt><dd><p>Convergence criterion of the local scoring algorithm.
Defaults to <code>0.1</code></p>
</dd>
<dt><code>max_iter_backfitting</code></dt><dd><p>An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to <code>10</code>.</p>
</dd>
<dt><code>max_iter_ls</code></dt><dd><p>An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to <code>10</code>.</p>
</dd>
<dt><code>w_train</code></dt><dd><p>Optional sample weights</p>
</dd>
<dt><code>seed</code></dt><dd><p>A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Verbosity mode (0 = silent, 1 = print messages). Defaults to 1.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>compiled Neural Network
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>References</h3>

<p>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
</p>

<hr>
<h2 id='dev'>Deviance of the model</h2><span id='topic+dev'></span>

<h3>Description</h3>

<p>Computes the deviance of the model according to the distribution
family specified in the <code>"family"</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dev(muhat, y, family)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dev_+3A_muhat">muhat</code></td>
<td>
<p>current estimation of the response variable</p>
</td></tr>
<tr><td><code id="dev_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="dev_+3A_family">family</code></td>
<td>
<p>A description of the link function used in the model:
<code>"gaussian"</code> or <code>"binomial"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the deviance of the model
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>

<hr>
<h2 id='diriv'>Derivative of the link function</h2><span id='topic+diriv'></span>

<h3>Description</h3>

<p>Computes the derivative of the link function according to
the distribution family specified in the <code>"family"</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diriv(family, muhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diriv_+3A_family">family</code></td>
<td>
<p>A description of the link function used in the model:
<code>"gaussian"</code> or <code>"binomial"</code></p>
</td></tr>
<tr><td><code id="diriv_+3A_muhat">muhat</code></td>
<td>
<p>fitted values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>derivative of the link function for the fitted values
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>

<hr>
<h2 id='get_formula_elements'>Extract formula elements</h2><span id='topic+get_formula_elements'></span>

<h3>Description</h3>

<p>This function separates the model terms of a given formula into response,
all_terms, non-parametric terms and parametric terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_formula_elements(formula)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_formula_elements_+3A_formula">formula</code></td>
<td>
<p>A formula object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li><p> y: The response variable
</p>
</li>
<li><p> terms: A character vector with all model terms
</p>
</li>
<li><p> np_terms: A character vector with non-parametric terms
</p>
</li>
<li><p> p_terms: A character vector with parametric terms
</p>
</li>
<li><p> np_formula: The formula for the non-parametric terms
</p>
</li>
<li><p> p_formula: The formula for the parametric terms
</p>
</li>
<li><p> formula: The original formula object
</p>
</li></ul>


<hr>
<h2 id='install_neuralGAM'>Install neuralGAM python requirements</h2><span id='topic+install_neuralGAM'></span>

<h3>Description</h3>

<p>Creates a conda environment (installing miniconda if required) and set ups the
Python requirements to run neuralGAM (Tensorflow and Keras).
</p>
<p>Miniconda and related environments are generated in the user's cache directory
given by:
</p>
<p><code>tools::R_user_dir('neuralGAM', 'cache')</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_neuralGAM()
</code></pre>

<hr>
<h2 id='inv_link'>Inverse of the link functions</h2><span id='topic+inv_link'></span>

<h3>Description</h3>

<p>Computes the inverse of the link function according to the
distribution family specified in the <code>"family"</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inv_link(family, muhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inv_link_+3A_family">family</code></td>
<td>
<p>A description of the link function used in the model:
<code>"gaussian"</code> or <code>"binomial"</code></p>
</td></tr>
<tr><td><code id="inv_link_+3A_muhat">muhat</code></td>
<td>
<p>fitted values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the inverse link function specified by the <code>"family"</code>
distribution for the given fitted values
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>

<hr>
<h2 id='link'>Link function</h2><span id='topic+link'></span>

<h3>Description</h3>

<p>Applies the link function according to the distribution family
specified in the <code>"family"</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>link(family, muhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="link_+3A_family">family</code></td>
<td>
<p>A description of the link function used in the model:
<code>"gaussian"</code> or <code>"binomial"</code></p>
</td></tr>
<tr><td><code id="link_+3A_muhat">muhat</code></td>
<td>
<p>fitted values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the link function specified by the <code>"family"</code> distribution
for the given fitted values
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>

<hr>
<h2 id='neuralGAM'>Fit a <code>neuralGAM</code> model</h2><span id='topic+neuralGAM'></span>

<h3>Description</h3>

<p>Fits a <code>neuralGAM</code> model by building a neural network to attend to each covariate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuralGAM(
  formula,
  data,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mse",
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  verbose = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neuralGAM_+3A_formula">formula</code></td>
<td>
<p>An object of class &quot;formula&quot;: a description of the model to be fitted. You can add smooth terms using <code>s()</code>.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_data">data</code></td>
<td>
<p>A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_num_units">num_units</code></td>
<td>
<p>Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a vector of values is provided, a multi-layer neural network with each element of the vector defining
the number of hidden units on each hidden layer is used.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_family">family</code></td>
<td>
<p>This is a family object specifying the distribution and link to use for fitting.
By default, it is <code>"gaussian"</code> but also works to <code>"binomial"</code> for logistic regression.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Learning rate for the neural network optimizer.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_activation">activation</code></td>
<td>
<p>Activation function of the neural network. Defaults to <code>relu</code></p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (<code>glorot_normal</code>).</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_kernel_regularizer">kernel_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the kernel weights matrix.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_bias_regularizer">bias_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the bias vector.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_bias_initializer">bias_initializer</code></td>
<td>
<p>Optional initializer for the bias vector.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the output of the layer</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_loss">loss</code></td>
<td>
<p>Loss function to use during neural network training. Defaults to the mean squared error.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_w_train">w_train</code></td>
<td>
<p>Optional sample weights</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_bf_threshold">bf_threshold</code></td>
<td>
<p>Convergence criterion of the backfitting algorithm.
Defaults to <code>0.001</code></p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_ls_threshold">ls_threshold</code></td>
<td>
<p>Convergence criterion of the local scoring algorithm.
Defaults to <code>0.1</code></p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_max_iter_backfitting">max_iter_backfitting</code></td>
<td>
<p>An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to <code>10</code>.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_max_iter_ls">max_iter_ls</code></td>
<td>
<p>An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to <code>10</code>.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_seed">seed</code></td>
<td>
<p>A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_verbose">verbose</code></td>
<td>
<p>Verbosity mode (0 = silent, 1 = print messages). Defaults to 1.</p>
</td></tr>
<tr><td><code id="neuralGAM_+3A_...">...</code></td>
<td>
<p>Additional parameters for the Adam optimizer (see ?keras::optimizer_adam)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function builds one neural network to attend to each feature in x,
using the backfitting and local scoring algorithms to fit a weighted additive model
using neural networks as function approximators. The adjustment of the
dependent variable and the weights is determined by the distribution of the
response <code>y</code>, adjusted by the <code>family</code> parameter.
</p>


<h3>Value</h3>

<p>A trained <code>neuralGAM</code> object. Use <code>summary(ngam)</code> to see details.
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>References</h3>

<p>Hastie, T., &amp; Tibshirani, R. (1990). Generalized Additive Models. London: Chapman and Hall, 1931(11), 683â€“741.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;- x1 ** 2
f2 &lt;- 2 * x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )

ngam

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.neuralGAM'>Visualization of <code>neuralGAM</code> object with base graphics</h2><span id='topic+plot.neuralGAM'></span>

<h3>Description</h3>

<p>Visualization of <code>neuralGAM</code> object. Plots the learned partial effects by the neuralGAM object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neuralGAM'
plot(x, select = NULL, xlab = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.neuralGAM_+3A_x">x</code></td>
<td>
<p>a fitted <code>neuralGAM</code> object as produced by <code>neuralGAM()</code>.</p>
</td></tr>
<tr><td><code id="plot.neuralGAM_+3A_select">select</code></td>
<td>
<p>allows to plot a set of selected terms. e.g. if you just want to plot the first term,
select=&quot;X0&quot;</p>
</td></tr>
<tr><td><code id="plot.neuralGAM_+3A_xlab">xlab</code></td>
<td>
<p>if supplied, this value will be used as the <code>x</code> label for all plots</p>
</td></tr>
<tr><td><code id="plot.neuralGAM_+3A_ylab">ylab</code></td>
<td>
<p>if supplied, this value will be used as the <code>y</code> label for all plots</p>
</td></tr>
<tr><td><code id="plot.neuralGAM_+3A_...">...</code></td>
<td>
<p>other graphics parameters to pass on to plotting commands.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the partial effects plot.
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;-x1**2
f2 &lt;- 2*x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )
plot(ngam)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.neuralGAM'>Produces predictions from a fitted <code>neuralGAM</code> object</h2><span id='topic+predict.neuralGAM'></span>

<h3>Description</h3>

<p>Takes a fitted <code>neuralGAM</code> object produced by
<code>neuralGAM()</code> and produces predictions given a new set of values for the model covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neuralGAM'
predict(object, newdata = NULL, type = "link", terms = NULL, verbose = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.neuralGAM_+3A_object">object</code></td>
<td>
<p>a fitted 'neuralGAM' object</p>
</td></tr>
<tr><td><code id="predict.neuralGAM_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or list containing the values of covariates at which
predictions are required. If not provided, the function returns the predictions
for the original training data.</p>
</td></tr>
<tr><td><code id="predict.neuralGAM_+3A_type">type</code></td>
<td>
<p>when <code>type="link"</code> (default), the linear
predictor is returned. When <code>type="terms"</code> each component of the linear
predictor is returned separately on each column of a <code>data.frame</code>. When
<code>type="response"</code> predictions on the scale of the response are returned.</p>
</td></tr>
<tr><td><code id="predict.neuralGAM_+3A_terms">terms</code></td>
<td>
<p>If <code>type="terms"</code>, then only results for the terms named
in this list will be returned. If <code>NULL</code> then no terms are excluded (default).</p>
</td></tr>
<tr><td><code id="predict.neuralGAM_+3A_verbose">verbose</code></td>
<td>
<p>Verbosity mode (0 = silent, 1 = print messages). Defaults to 1.</p>
</td></tr>
<tr><td><code id="predict.neuralGAM_+3A_...">...</code></td>
<td>
<p>Other options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predicted values according to <code>type</code> parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;-x1**2
f2 &lt;- 2*x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )
n &lt;- 5000
x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)
test &lt;- data.frame(x1, x2, x3)

# Obtain linear predictor
eta &lt;- predict(ngam, test, type = "link")

# Obtain predicted response
yhat &lt;- predict(ngam, test, type = "response")

# Obtain each component of the linear predictor
terms &lt;- predict(ngam, test, type = "terms")

# Obtain only certain terms:
terms &lt;- predict(ngam, test, type = "terms", terms = c("x1", "x2"))

## End(Not run)
</code></pre>

<hr>
<h2 id='print.neuralGAM'>Short <code>neuralGAM</code> summary</h2><span id='topic+print.neuralGAM'></span>

<h3>Description</h3>

<p>Default print statement for a neuralGAM object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neuralGAM'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.neuralGAM_+3A_x">x</code></td>
<td>
<p><code>neuralGAM</code> object.</p>
</td></tr>
<tr><td><code id="print.neuralGAM_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The printed output of the object:
</p>

<ul>
<li><p> Distribution family
</p>
</li>
<li><p> Formula
</p>
</li>
<li><p> Intercept value
</p>
</li>
<li><p> Mean Squared Error (MSE)
</p>
</li>
<li><p> Training sample size
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;-x1**2
f2 &lt;- 2*x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )
print(ngam)

## End(Not run)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+autoplot'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
</dl>

<hr>
<h2 id='summary.neuralGAM'><code>neuralGAM</code> summary</h2><span id='topic+summary.neuralGAM'></span>

<h3>Description</h3>

<p>Summary of a fitted <code>neuralGAM</code> object. Prints
the distribution family, model formula, intercept value, sample size,
as well as neural network architecture and training history.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neuralGAM'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.neuralGAM_+3A_object">object</code></td>
<td>
<p><code>neuralGAM</code> object.</p>
</td></tr>
<tr><td><code id="summary.neuralGAM_+3A_...">...</code></td>
<td>
<p>Other options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The summary of the object:
</p>

<ul>
<li><p> Distribution family
</p>
</li>
<li><p> Formula
</p>
</li>
<li><p> Intercept value
</p>
</li>
<li><p> Mean Squared Error (MSE)
</p>
</li>
<li><p> Training sample size
</p>
</li>
<li><p> Training History
</p>
</li>
<li><p> Model Architecture
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;-x1**2
f2 &lt;- 2*x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )
summary(ngam)

## End(Not run)
</code></pre>

<hr>
<h2 id='weight'>Weights</h2><span id='topic+weight'></span>

<h3>Description</h3>

<p>Computes the weights for the Local Scoring Algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight(w, muhat, family)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_+3A_w">w</code></td>
<td>
<p>weights</p>
</td></tr>
<tr><td><code id="weight_+3A_muhat">muhat</code></td>
<td>
<p>fitted values</p>
</td></tr>
<tr><td><code id="weight_+3A_family">family</code></td>
<td>
<p>A description of the link function used in the model:
<code>"gaussian"</code> or <code>"binomial"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>computed weights for the Local Scoring algorithm
according to the <code>"family"</code> distribution
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
