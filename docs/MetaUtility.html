<!DOCTYPE html><html><head><title>Help for package MetaUtility</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MetaUtility}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calib_ests'><p>Return calibrated estimates of studies' population effect sizes</p></a></li>
<li><a href='#ci_to_var'><p>Calculate variance given confidence interval limit and point estimate</p></a></li>
<li><a href='#d_to_logRR'><p>Convert Cohen's d to log risk ratio</p></a></li>
<li><a href='#format_CI'><p>Manuscript-friendly confidence interval formatting</p></a></li>
<li><a href='#format_stat'><p>Manuscript-friendly number formatting</p></a></li>
<li><a href='#parse_CI_string'><p>Parse a string with point estimate and confidence interval</p></a></li>
<li><a href='#pct_pval'><p>Return sign test p-value for meta-analysis percentile</p></a></li>
<li><a href='#prop_stronger'><p>Estimate proportion of population effect sizes above or below a threshold</p></a></li>
<li><a href='#prop_stronger_sign'><p>Return sign test point estimate of proportion of effects above or below threshold.</p></a></li>
<li><a href='#r_to_d'><p>Convert Pearson's r to Cohen's d</p></a></li>
<li><a href='#r_to_z'><p>Convert Pearson's r to Fisher's z</p></a></li>
<li><a href='#round2'><p>Round while keeping trailing zeroes</p></a></li>
<li><a href='#scrape_meta'><p>Convert forest plot or summary table to meta-analytic dataset</p></a></li>
<li><a href='#tau_CI'><p>Return confidence interval for tau for a meta-analysis</p></a></li>
<li><a href='#z_to_r'><p>Convert Fisher's z to Pearson's r</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Utility Functions for Conducting and Interpreting Meta-Analyses</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Maya B. Mathur, Rui Wang, Tyler J. VanderWeele</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Maya B. Mathur &lt;mmathur@stanford.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains functions to estimate the proportion of effects stronger than a threshold
    of scientific importance (function prop_stronger), to nonparametrically characterize the distribution of effects in a meta-analysis (calib_ests, pct_pval),
    to make effect size conversions (r_to_d, r_to_z, z_to_r, d_to_logRR), to compute and format inference in a meta-analysis (format_CI, format_stat, tau_CI), to scrape results from 
    existing meta-analyses for re-analysis (scrape_meta, parse_CI_string, ci_to_var).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>metafor, metadat, stats, stringr, purrr, dplyr, tidyr, rlang</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-29 23:26:37 UTC; mmathur</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-30 09:50:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='calib_ests'>Return calibrated estimates of studies' population effect sizes</h2><span id='topic+calib_ests'></span>

<h3>Description</h3>

<p>Returns estimates of the population effect in each study based on the methods of Wang &amp; Lee (2019).
Unlike the point estimates themselves, these &quot;calibrated&quot; estimates have been
appropriately shrunk to correct the overdispersion that arises due to the studies' finite sample sizes.
By default, this function uses Dersimonian-Laird moments-based estimates of the mean and variance of the true
effects, as Wang &amp; Lee (2019) recommended.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calib_ests(yi, sei, method = "DL")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calib_ests_+3A_yi">yi</code></td>
<td>
<p>Vector of study-level point estimates</p>
</td></tr>
<tr><td><code id="calib_ests_+3A_sei">sei</code></td>
<td>
<p>Vector of study-level standard errors</p>
</td></tr>
<tr><td><code id="calib_ests_+3A_method">method</code></td>
<td>
<p>Estimation method for mean and variance of population effects (passed to <code>metafor::rma.uni</code>)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wang C-C &amp; Lee W-C (2019). A simple method to estimate prediction intervals and
predictive distributions: Summarizing meta-analyses
beyond means and confidence intervals. <em>Research Synthesis Methods</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = metafor::escalc(measure="RR", ai=tpos, bi=tneg,
                     ci=cpos, di=cneg, data=metadat::dat.bcg)

# calculate calibrated estimates
d$calib = calib_ests( yi = d$yi,
                      sei = sqrt(d$vi) )

# look at 5 studies with the largest calibrated estimates
d = d[ order(d$calib, decreasing = TRUE), ]
d$trial[1:5]

# look at kernel density estimate of calibrated estimates
plot(density(d$calib))
</code></pre>

<hr>
<h2 id='ci_to_var'>Calculate variance given confidence interval limit and point estimate</h2><span id='topic+ci_to_var'></span>

<h3>Description</h3>

<p>Returns approximate variance (i.e., squared standard error) of point estimate given either the upper or lower limit (<code>ci.lim</code>) of a Wald-type confidence interval. If degrees of freedom (<code>df</code>) are not provided, it is assumed that the confidence interval uses a z-distribution approximation. If degrees of freedom are provided, it is assumed that the confidence interval uses a t-distribution approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci_to_var(est, ci.lim, ci.level = 0.95, df = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci_to_var_+3A_est">est</code></td>
<td>
<p>Point estimate</p>
</td></tr>
<tr><td><code id="ci_to_var_+3A_ci.lim">ci.lim</code></td>
<td>
<p>Lower or upper confidence interval limit</p>
</td></tr>
<tr><td><code id="ci_to_var_+3A_ci.level">ci.level</code></td>
<td>
<p>Confidence interval level as a proportion (e.g., 0.95)</p>
</td></tr>
<tr><td><code id="ci_to_var_+3A_df">df</code></td>
<td>
<p>Degrees of freedom</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimate and confidence interval must be provided on a scale such that the confidence interval is symmetric. For example, if the point estimate is a relative risk, then the estimate and confidence interval should be provided on the log-relative risk scale.
</p>

<hr>
<h2 id='d_to_logRR'>Convert Cohen's d to log risk ratio</h2><span id='topic+d_to_logRR'></span>

<h3>Description</h3>

<p>Converts Cohen's d (computed with a binary X) to a log risk ratio
for use in meta-analysis. Under the assumption that Y is approximately normal, the resulting log risk ratio represents a dichotomization of Y that is near its median and otherwise will  tend to be conservative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>d_to_logRR(d, se = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="d_to_logRR_+3A_d">d</code></td>
<td>
<p>Cohen's d</p>
</td></tr>
<tr><td><code id="d_to_logRR_+3A_se">se</code></td>
<td>
<p>Standard error of d</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, this function first converts d to a log odds ratio using Chinn's (2000) conversion. The resulting log odds ratio approximates the value that would be obtained if Y were dichotomized; if Y is approximately normal, the log odds ratio is approximately invariant to the choice of threshold at which Y is dichotomized (Chinn, 2000). Then, the function converts the log odds ratio to a log risk ratio using VanderWeele's (2020) square-root conversion. That conversion is conservative in that it allows for the possibility that the dichotomized Y is not rare (i.e., the point of dichotomization is not at an extreme value of Y).
</p>


<h3>References</h3>

<p>VanderWeele, TJ (2020). Optimal approximate conversions of odds ratios and hazard ratios to risk ratios. <em>Biometrics</em>.
</p>
<p>Chinn S (2000). A simple method for converting an odds ratio to effect size for use in meta-analysis. <em>Statistics in Medicine</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_to_logRR( d = c(0.5, -0.2, .1),
        se = c(0.21, NA, 0.3) )
</code></pre>

<hr>
<h2 id='format_CI'>Manuscript-friendly confidence interval formatting</h2><span id='topic+format_CI'></span>

<h3>Description</h3>

<p>Formats confidence interval lower and upper bounds into a rounded string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_CI(lo, hi, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_CI_+3A_lo">lo</code></td>
<td>
<p>Confidence interval lower limit (numeric)</p>
</td></tr>
<tr><td><code id="format_CI_+3A_hi">hi</code></td>
<td>
<p>Confidence interval upper limit (numeric)</p>
</td></tr>
<tr><td><code id="format_CI_+3A_digits">digits</code></td>
<td>
<p>Digits for rounding</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>format_CI(0.36, 0.72, 3)
</code></pre>

<hr>
<h2 id='format_stat'>Manuscript-friendly number formatting</h2><span id='topic+format_stat'></span>

<h3>Description</h3>

<p>Formats a numeric result (e.g., p-value) as a manuscript-friendly string in which values below a minimum cutoff (e.g., 10^-5) are
reported for example as &quot;&lt; 10^-5&quot;, values between the minimum cutoff and a maximum cutoff (e.g., 0.01)
are reported in scientific notation, and p-values above the maximum cutoff are reported simply as, for example, 0.72.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_stat(x, digits = 2, cutoffs = c(0.01, 10^-5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_stat_+3A_x">x</code></td>
<td>
<p>Numeric value to format</p>
</td></tr>
<tr><td><code id="format_stat_+3A_digits">digits</code></td>
<td>
<p>Digits for rounding</p>
</td></tr>
<tr><td><code id="format_stat_+3A_cutoffs">cutoffs</code></td>
<td>
<p>A vector containing the two cutoffs</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>format_stat(0.735253)
format_stat(0.735253, digits = 4)
format_stat(0.0123)
format_stat(0.0001626)
format_stat(0.0001626, cutoffs = c(0.01, 10^-3))
</code></pre>

<hr>
<h2 id='parse_CI_string'>Parse a string with point estimate and confidence interval</h2><span id='topic+parse_CI_string'></span>

<h3>Description</h3>

<p>Given a vector of strings such as &quot;0.65 (0.6, 0.70)&quot;, for example obtained by running optical character recognition (OCR)
software on a screenshot of a published forest plot, parses the strings into a dataframe of
point estimates and upper confidence interval limits. Assumes that the point estimate occurs before
an opening bracket of the form &quot;(&quot; or &quot;[&quot; and that the confidence interval upper limit follows a
the character <code>sep</code> (by default a comma, but might be a hyphen, for example). To further parse this dataframe
into point estimates and variances, see <code>MetaUtility::scrape_meta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_CI_string(string, sep = ",")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_CI_string_+3A_string">string</code></td>
<td>
<p>A vector of strings to be parsed.</p>
</td></tr>
<tr><td><code id="parse_CI_string_+3A_sep">sep</code></td>
<td>
<p>The character (not including whitespaces) separating the lower from the upper limits.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># messy string of confidence intervals
mystring = c( "0.65 [0.6, 0.7]", "0.8(0.5, 0.9]", "1.2  [0.3, 1.5)")
parse_CI_string(mystring)

# now with a hyphen separator
mystring = c( "0.65 [0.6- 0.7]", "0.8(0.5 - 0.9]", "1.2  [0.3 -1.5)")
parse_CI_string(mystring, sep="-")
</code></pre>

<hr>
<h2 id='pct_pval'>Return sign test p-value for meta-analysis percentile</h2><span id='topic+pct_pval'></span>

<h3>Description</h3>

<p>Returns a p-value for testing the hypothesis that <code>mu</code> is the <code>pct</code>^th
percentile of the population effect distribution based on the nonparametric sign test
method of Wang et al. (2010). This function is also called by <code>prop_stronger</code>
when using the sign test method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pct_pval(yi, sei, mu, pct, R = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pct_pval_+3A_yi">yi</code></td>
<td>
<p>Vector of study-level point estimates</p>
</td></tr>
<tr><td><code id="pct_pval_+3A_sei">sei</code></td>
<td>
<p>Vector of study-level standard errors</p>
</td></tr>
<tr><td><code id="pct_pval_+3A_mu">mu</code></td>
<td>
<p>The effect size to test as the <code>pct</code>^th percentile</p>
</td></tr>
<tr><td><code id="pct_pval_+3A_pct">pct</code></td>
<td>
<p>The percentile of interest (e.g., 0.50 for the median)</p>
</td></tr>
<tr><td><code id="pct_pval_+3A_r">R</code></td>
<td>
<p>Number of simulation iterates to use when estimating null distribution of the test statistic.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wang R, Tian L, Cai T, &amp; Wei LJ (2010). Nonparametric inference procedure for percentiles
of the random effects distribution in meta-analysis. <em>Annals of Applied Statistics</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># calculate effect sizes for example dataset
d = metafor::escalc(measure="RR", ai=tpos, bi=tneg,
                   ci=cpos, di=cneg, data=metadat::dat.bcg)

# test H0: the median is -0.3
# using only R = 100 for speed, but should be much larger (e.g., 2000) in practice
pct_pval( yi = d$yi,
          sei = sqrt(d$vi),
          mu = -0.3,
          pct = 0.5,
          R = 100 )
</code></pre>

<hr>
<h2 id='prop_stronger'>Estimate proportion of population effect sizes above or below a threshold</h2><span id='topic+prop_stronger'></span>

<h3>Description</h3>

<p>Estimates the proportion of true (i.e., population parameter) effect sizes in a meta-analysis
that are above or below a specified threshold of scientific importance based on the parametric methods described in Mathur &amp; VanderWeele (2018), the nonparametric calibrated methods described in Mathur &amp; VanderWeele (2020b), and the cluster-bootstrapping methods described in Mathur &amp; VanderWeele (2020c).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_stronger(
  q,
  M = NA,
  t2 = NA,
  se.M = NA,
  se.t2 = NA,
  ci.level = 0.95,
  tail = NA,
  estimate.method = "calibrated",
  ci.method = "calibrated",
  calib.est.method = "DL",
  dat = NULL,
  R = 2000,
  bootstrap = "ifneeded",
  yi.name = "yi",
  vi.name = "vi",
  cluster.name = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_stronger_+3A_q">q</code></td>
<td>
<p>Population effect size that is the threshold for &quot;scientific importance&quot;</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_m">M</code></td>
<td>
<p>Pooled point estimate from meta-analysis (required only for parametric estimation/inference and for Shapiro p-value)</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_t2">t2</code></td>
<td>
<p>Estimated heterogeneity (tau^2) from meta-analysis (required only for parametric estimation/inference and for Shapiro p-value)</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_se.m">se.M</code></td>
<td>
<p>Estimated standard error of pooled point estimate from meta-analysis (required only for parametric inference)</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_se.t2">se.t2</code></td>
<td>
<p>Estimated standard error of tau^2 from meta-analysis (required only for parametric inference)</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_ci.level">ci.level</code></td>
<td>
<p>Confidence level as a proportion (e.g., 0.95 for a 95% confidence interval)</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_tail">tail</code></td>
<td>
<p><code>"above"</code> for the proportion of effects above <code>q</code>; <code>"below"</code> for
the proportion of effects below <code>q</code>.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_estimate.method">estimate.method</code></td>
<td>
<p>Method for point estimation of the proportion (<code>"calibrated"</code> or <code>"parametric"</code>). See Details.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_ci.method">ci.method</code></td>
<td>
<p>Method for confidence interval estimation (<code>"calibrated"</code>, <code>"parametric"</code>, or <code>"sign.test"</code>). See Details.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_calib.est.method">calib.est.method</code></td>
<td>
<p>Method for estimating the mean and variance of the population effects when computing calibrated estimates. See Details.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_dat">dat</code></td>
<td>
<p>Dataset of point estimates (with names equal to the passed <code>yi.name</code>) and their variances
(with names equal to the passed <code>vi.name</code>). Not required if using <code>ci.method = "parametric"</code> and bootstrapping is not needed.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_r">R</code></td>
<td>
<p>Number of bootstrap or simulation iterates (depending on the methods chosen). Not required if using <code>ci.method = "parametric"</code> and bootstrapping is not needed.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Argument only used when <code>ci.method = "parametric"</code> (because otherwise the bootstrap is always used). In that case, if <code>bootstrap = "ifneeded"</code>, bootstraps if estimated proportion is less than 0.15 or more than
0.85. If equal to <code>"never"</code>, instead does not return inference in the above edge cases.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_yi.name">yi.name</code></td>
<td>
<p>Name of the variable in <code>dat</code> containing the study-level point estimates. Used for bootstrapping and conducting Shapiro test.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_vi.name">vi.name</code></td>
<td>
<p>Name of the variable in <code>dat</code> containing the study-level variances. Used for bootstrapping and conducting Shapiro test.</p>
</td></tr>
<tr><td><code id="prop_stronger_+3A_cluster.name">cluster.name</code></td>
<td>
<p>Name of the variable in <code>dat</code> identifying clusters of studies. If left <code>NA</code>, assumes studies are independent (i.e., each study is its own cluster).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These methods perform well only in meta-analyses with at least 10 studies; we do not recommend reporting them in smaller
meta-analyses. By default, <code>prop_stronger</code> performs estimation using a &quot;calibrated&quot; method (Mathur &amp; VanderWeele, 2020b; 2020c) that extends work by Wang et al. (2019).
This method makes no assumptions about the distribution of population effects, performs well in meta-analyses with
as few as 10 studies, and can accommodate clustering of the studies (e.g., when articles contributed multiple studies on similar populations). Calculating the calibrated estimates involves first estimating the meta-analytic mean and variance,
which, by default, is done using the moments-based Dersimonian-Laird estimator as in Wang et al. (2019). To use a different method, which will be passed
to <code>metafor::rma.uni</code>, change the argument <code>calib.est.method</code> based on the documentation for <code>metafor::rma.uni</code>. For inference, the calibrated method uses bias-corrected
and accelerated bootstrapping that will account for clustered point estimates if the argument <code>cluster.name</code> is specified (Mathur &amp; VanderWeele, 2020c). The bootstrapping may fail to converge for some small meta-analyses for which the threshold is distant from the mean of the population effects.
In these cases, you can try choosing a threshold closer to the pooled point estimate of your meta-analysis.
The mean of the bootstrap estimates of the proportion is returned as a diagnostic for potential bias in the estimated proportion.
</p>
<p>The parametric method assumes that the population effects are approximately normal, that the number of studies is large, and that the studies are independent. When these conditions hold
and the proportion being estimated is not extreme (between 0.15 and 0.85), the parametric method may be more precise than the calibrated method.
to improve precision. When using the parametric method and the estimated proportion is less than 0.15 or more than 0.85, it is best to bootstrap the confidence interval
using the bias-corrected and accelerated (BCa) method (Mathur &amp; VanderWeele, 2018); this is the default behavior of <code>prop_stronger</code>.
Sometimes BCa confidence interval estimation fails, in which case <code>prop_stronger</code> instead uses the percentile method,
issuing a warning if this is the case (but note that the percentile method should <em>not</em> be used when bootstrapping the calibrated estimates
rather than the parametric estimates). We use a modified &quot;safe&quot; version of the <code>boot</code> package code for bootstrapping
such that if any bootstrap iterates fail (usually because of model estimation problems), the error message is printed but the
bootstrap iterate is simply discarded so that confidence interval estimation can proceed. As above, the mean of the bootstrapped
estimates of the proportion is returned as a diagnostic for potential bias in the estimated proportion.
</p>
<p>The sign test method (Mathur &amp; VanderWeele, 2020b) is an extension of work by Wang et al. (2010). This method was included in Mathur &amp; VanderWeele's (2020b) simulation study;
it performed adequately when there was high heterogeneity, but did not perform well with lower heterogeneity. However, in the absence of a clear criterion
for how much heterogeneity is enough for the method to perform well, we do not in general recommend its use. Additionally, this method requires effects that are reasonably symmetric and unimodal.
</p>


<h3>Value</h3>

<p>Returns a dataframe containing the point estimate for the proportion (<code>est</code>), its estimated standard error (<code>se</code>), lower and upper confidence
interval limits (<code>lo</code> and <code>hi</code>), and, depending on the user's specifications, the mean of the bootstrap estimates of the proportion (<code>bt.mn</code>)
and the p-value for a Shapiro test for normality conducted on the standardized point estimates (<code>shapiro.pval</code>).
</p>


<h3>References</h3>

<p>Mathur MB &amp; VanderWeele TJ (2018). New metrics for meta-analyses of heterogeneous effects. <em>Statistics in Medicine</em>.
</p>
<p>Mathur MB &amp; VanderWeele TJ (2020a).New statistical metrics for multisite replication projects. <em>Journal of the Royal Statistical Society: Series A.</em>
</p>
<p>Mathur MB &amp; VanderWeele TJ (2020b). Robust metrics and sensitivity analyses for meta-analyses of heterogeneous effects. <em>Epidemiology</em>.
</p>
<p>Mathur MB &amp; VanderWeele TJ (2020c). Meta-regression methods to characterize evidence strength using meaningful-effect percentages conditional on study characteristics. Preprint available: <a href="https://osf.io/bmtdq">https://osf.io/bmtdq</a>.
</p>
<p>Wang R, Tian L, Cai T, &amp; Wei LJ (2010). Nonparametric inference procedure for percentiles
of the random effects distribution in meta-analysis. <em>Annals of Applied Statistics</em>.
</p>
<p>Wang C-C &amp; Lee W-C (2019). A simple method to estimate prediction intervals and
predictive distributions: Summarizing meta-analyses
beyond means and confidence intervals. <em>Research Synthesis Methods</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##### Example 1: BCG Vaccine and Tuberculosis Meta-Analysis #####

# calculate effect sizes for example dataset
d = metafor::escalc(measure="RR", ai=tpos, bi=tneg,
                    ci=cpos, di=cneg, data=metadat::dat.bcg)

# fit random-effects model
# note that metafor package returns on the log scale
m = metafor::rma.uni(yi= d$yi, vi=d$vi, knha=TRUE,
                     measure="RR", method="REML" )

# pooled point estimate (RR scale)
exp(m$b)

# estimate the proportion of effects stronger than RR = 0.70
# as recommended, use the calibrated approach for both point estimation and CI
# bootstrap reps should be higher in practice (e.g., 1000)
# here using fewer for speed
prop_stronger( q = log(0.7),
               tail = "below",
               estimate.method = "calibrated",
               ci.method = "calibrated",
               dat = d,
               yi.name = "yi",
               vi.name = "vi",
               R = 100)
# warning goes away with more bootstrap iterates
# no Shapiro p-value because we haven't provided the dataset and its variable names

# now use the parametric approach (Mathur &amp; VanderWeele 2018)
# no bootstrapping will be needed for this choice of q
prop_stronger( q = log(0.7),
               M = as.numeric(m$b),
               t2 = m$tau2,
               se.M = as.numeric(m$vb),
               se.t2 = m$se.tau2,
               tail = "below",
               estimate.method = "parametric",
               ci.method = "parametric",
               bootstrap = "ifneeded")


##### Example 2: Meta-Analysis of Multisite Replication Studies #####

# replication estimates (Fisher's z scale) and SEs
# from moral credential example in reference #2
r.fis = c(0.303, 0.078, 0.113, -0.055, 0.056, 0.073,
          0.263, 0.056, 0.002, -0.106, 0.09, 0.024, 0.069, 0.074,
          0.107, 0.01, -0.089, -0.187, 0.265, 0.076, 0.082)

r.SE = c(0.111, 0.092, 0.156, 0.106, 0.105, 0.057,
         0.091, 0.089, 0.081, 0.1, 0.093, 0.086, 0.076,
         0.094, 0.065, 0.087, 0.108, 0.114, 0.073, 0.105, 0.04)

d = data.frame( yi = r.fis,
                vi = r.SE^2 )

# meta-analyze the replications
m = metafor::rma.uni( yi = r.fis, vi = r.SE^2, measure = "ZCOR" )

# probability of population effect above r = 0.10 = 28%
# convert threshold on r scale to Fisher's z
q = r_to_z(0.10)

# bootstrap reps should be higher in practice (e.g., 1000)
# here using only 100 for speed
prop_stronger( q = q,
               tail = "above",
               estimate.method = "calibrated",
               ci.method = "calibrated",
               dat = d,
               yi.name = "yi",
               vi.name = "vi",
               R = 100 )


# probability of population effect equally strong in opposite direction
q.star = r_to_z(-0.10)
prop_stronger( q = q.star,
               tail = "below",
               estimate.method = "calibrated",
               ci.method = "calibrated",
               dat = d,
               yi.name = "yi",
               vi.name = "vi",
               R = 100 )
# BCa fails to converge here
</code></pre>

<hr>
<h2 id='prop_stronger_sign'>Return sign test point estimate of proportion of effects above or below threshold.</h2><span id='topic+prop_stronger_sign'></span>

<h3>Description</h3>

<p>Internal function not intended for user to call. Uses an extension of the sign test method of Wang et al. (2010) to estimate the proportion of true (i.e., population parameter) effect sizes in a meta-analysis
that are above or below a specified threshold of scientific importance. See important caveats in the Details section of the documentation for
the function <code>prop_stronger</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_stronger_sign(
  q,
  yi,
  vi,
  ci.level = 0.95,
  tail = NA,
  R = 2000,
  return.vectors = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_stronger_sign_+3A_q">q</code></td>
<td>
<p>Population effect size that is the threshold for &quot;scientific importance&quot;</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_yi">yi</code></td>
<td>
<p>Study-level point estimates</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_vi">vi</code></td>
<td>
<p>study-level variances</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_ci.level">ci.level</code></td>
<td>
<p>Confidence level as a proportion</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_tail">tail</code></td>
<td>
<p><code>above</code> for the proportion of effects above <code>q</code>; <code>below</code> for
the proportion of effects below <code>q</code>.</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_r">R</code></td>
<td>
<p>Number of simulation iterates to estimate null distribution of sign test statistic</p>
</td></tr>
<tr><td><code id="prop_stronger_sign_+3A_return.vectors">return.vectors</code></td>
<td>
<p>Should all percents and p-values from the grid search be returned?</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wang R, Tian L, Cai T, &amp; Wei LJ (2010). Nonparametric inference procedure for percentiles
of the random effects distribution in meta-analysis. <em>Annals of Applied Statistics</em>.
</p>

<hr>
<h2 id='r_to_d'>Convert Pearson's r to Cohen's d</h2><span id='topic+r_to_d'></span>

<h3>Description</h3>

<p>Converts Pearson's r (computed with a continuous X and Y) to Cohen's d
for use in meta-analysis. The resulting Cohen's d represents the
estimated increase in standardized Y that is associated with a delta-unit
increase in X.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r_to_d(r, sx, delta, N = NA, Ns = N, sx.known = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r_to_d_+3A_r">r</code></td>
<td>
<p>Pearson's correlation</p>
</td></tr>
<tr><td><code id="r_to_d_+3A_sx">sx</code></td>
<td>
<p>Sample standard deviation of X</p>
</td></tr>
<tr><td><code id="r_to_d_+3A_delta">delta</code></td>
<td>
<p>Contrast in X for which to compute Cohen's d, specified in raw units of X (not standard deviations).</p>
</td></tr>
<tr><td><code id="r_to_d_+3A_n">N</code></td>
<td>
<p>Sample size used to estimate <code>r</code></p>
</td></tr>
<tr><td><code id="r_to_d_+3A_ns">Ns</code></td>
<td>
<p>Sample size used to estimate <code>sx</code>, if different from <code>N</code></p>
</td></tr>
<tr><td><code id="r_to_d_+3A_sx.known">sx.known</code></td>
<td>
<p>Is <code>sx</code> known rather than estimated? (By default, assumes <code>sx</code> is estimated, which will almost always be the case.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To preserve the sign of the effect size, the code takes the absolute value of <code>delta</code>. The standard error
estimate assumes that X is approximately normal and that <code>N</code> is large.
</p>


<h3>References</h3>

<p>Mathur MB &amp; VanderWeele TJ (2019). A simple, interpretable conversion from Pearson's correlation to Cohen's d for meta-analysis. <em>Epidemiology</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># d for a 1-unit vs. a 2-unit increase in X
r_to_d( r = 0.5,
        sx = 2,
        delta = 1,
        N = 100 )
r_to_d( r = 0.5,
        sx = 2,
        delta = 2,
        N = 100 )

# d when sx is estimated in the same vs. a smaller sample
# point estimate will be the same, but inference will be a little
# less precise in second case
r_to_d( r = -0.3,
         sx = 2,
         delta = 2,
         N = 300,
         Ns = 300 )

r_to_d( r = -0.3,
        sx = 2,
        delta = 2,
        N = 300,
        Ns = 30 )
</code></pre>

<hr>
<h2 id='r_to_z'>Convert Pearson's r to Fisher's z</h2><span id='topic+r_to_z'></span>

<h3>Description</h3>

<p>Converts Pearson's r to Fisher's z for use in meta-analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r_to_z(r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r_to_z_+3A_r">r</code></td>
<td>
<p>Pearson's correlation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>r_to_z( c(.22, -.9, NA) )
</code></pre>

<hr>
<h2 id='round2'>Round while keeping trailing zeroes</h2><span id='topic+round2'></span>

<h3>Description</h3>

<p>Rounds a numeric value and formats it as a string, keeping trailing zeroes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>round2(x, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="round2_+3A_x">x</code></td>
<td>
<p>Numeric value to round</p>
</td></tr>
<tr><td><code id="round2_+3A_digits">digits</code></td>
<td>
<p>Digits for rounding</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>round2(0.03000, digits = 4)

# compare to base round, which drops trailing zeroes and returns a numeric
round(0.03000, digits = 4)
</code></pre>

<hr>
<h2 id='scrape_meta'>Convert forest plot or summary table to meta-analytic dataset</h2><span id='topic+scrape_meta'></span>

<h3>Description</h3>

<p>Given relative risks (RR) and upper bounds of 95% confidence intervals (CI)
from a forest plot or summary table, returns a dataframe ready for meta-analysis
(e.g., via the <code>metafor</code> package) with the log-RRs and their variances.
Optionally, the user may indicate studies for which the point estimate is to be
interpreted as an odds ratios of a common outcome rather than a relative risk;
for such studies, the function applies VanderWeele (2017)'s square-root transformation to convert
the odds ratio to an approximate risk ratio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrape_meta(type = "RR", est, hi, sqrt = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scrape_meta_+3A_type">type</code></td>
<td>
<p><code>RR</code> if point estimates are RRs or ORs (to be handled on log scale); <code>raw</code> if point estimates are raw differences, standardized mean differences, etc. (such that they can be handled with no transformations)</p>
</td></tr>
<tr><td><code id="scrape_meta_+3A_est">est</code></td>
<td>
<p>Vector of study point estimates on RR or OR scale</p>
</td></tr>
<tr><td><code id="scrape_meta_+3A_hi">hi</code></td>
<td>
<p>Vector of upper bounds of 95% CIs on RRs</p>
</td></tr>
<tr><td><code id="scrape_meta_+3A_sqrt">sqrt</code></td>
<td>
<p>Vector of booleans (TRUE/FALSE) for whether each study measured an odds ratio of a common outcome that should be approximated as a risk ratio via the square-root transformation</p>
</td></tr>
</table>


<h3>References</h3>

<p>VanderWeele TJ (2017). On a square-root transformation of the odds ratio for a common outcome. <em>Epidemiology</em>.
</p>

<hr>
<h2 id='tau_CI'>Return confidence interval for tau for a meta-analysis</h2><span id='topic+tau_CI'></span>

<h3>Description</h3>

<p>Returns confidence interval lower and upper limits for tau (the estimated standard deviation of
the population effects) for a meta-analysis fit in <code>metafor::rma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tau_CI(meta, ci.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tau_CI_+3A_meta">meta</code></td>
<td>
<p>A meta-analysis object fit in <code>metafor::rma</code>.</p>
</td></tr>
<tr><td><code id="tau_CI_+3A_ci.level">ci.level</code></td>
<td>
<p>Confidence interval level as a proportion (e.g., 0.95)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># calculate effect sizes for example dataset
d = metafor::escalc(measure="RR", ai=tpos, bi=tneg,
                   ci=cpos, di=cneg, data=metadat::dat.bcg)

# fit random-effects model
# note that metafor package returns on the log scale
m = metafor::rma.uni(yi= d$yi, vi=d$vi, knha=TRUE,
measure="RR", method="REML" )

tau_CI(m)

# for nicer formatting
format_CI( tau_CI(m)[1], tau_CI(m)[2] )
</code></pre>

<hr>
<h2 id='z_to_r'>Convert Fisher's z to Pearson's r</h2><span id='topic+z_to_r'></span>

<h3>Description</h3>

<p>Converts Fisher's z to Pearson's r for use in meta-analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>z_to_r(z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z_to_r_+3A_z">z</code></td>
<td>
<p>Fisher's z</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>z_to_r( c(1.1, NA, -0.2) )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
