<!DOCTYPE html><html><head><title>Help for package xLLiM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {xLLiM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#xLLiM-package'>
<p>High Dimensional Locally-Linear Mapping</p></a></li>
<li><a href='#bllim'><p>EM Algorithm for Block diagonal Gaussian Locally Linear Mapping</p></a></li>
<li><a href='#data.xllim'><p>Simulated data to run examples of usage of <code>gllim</code> and <code>sllim</code> functions</p></a></li>
<li><a href='#data.xllim.test'><p>Testing data to run examples of usage of <code>gllim_inverse_map</code> and <code>sllim_inverse_map</code> functions</p></a></li>
<li><a href='#data.xllim.trueparameters'><p>True parameters used to simulate the datasets <code>data.xllim</code> and <code>data.xllim.test</code></p></a></li>
<li><a href='#emgm'><p>Perform EM algorithm for fitting a Gaussian mixture model (GMM)</p></a></li>
<li><a href='#gllim'><p>EM Algorithm for Gaussian Locally Linear Mapping</p></a></li>
<li><a href='#gllim_inverse_map'><p>Inverse Mapping from gllim or bllim parameters</p></a></li>
<li><a href='#preprocess_data'>
<p>A proposition of function to process high dimensional data before running gllim, sllim or bllim</p></a></li>
<li><a href='#sllim'><p>EM Algorithm for Student Locally Linear Mapping</p></a></li>
<li><a href='#sllim_inverse_map'><p>Inverse Mapping from sllim parameters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High Dimensional Locally-Linear Mapping</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr), Emilie Devijver (emilie.devijver@kuleuven.be), Melina Gallopin (melina.gallopin@u-psud.fr)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emeline Perthame &lt;emeline.perthame@pasteur.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a tool for non linear mapping (non linear regression) using a mixture of regression model and an inverse regression strategy. The methods include the GLLiM model (see Deleforge et al (2015) &lt;<a href="https://doi.org/10.1007%2Fs11222-014-9461-5">doi:10.1007/s11222-014-9461-5</a>&gt;) based on Gaussian mixtures and a robust version of GLLiM, named SLLiM (see Perthame et al (2016) &lt;<a href="https://doi.org/10.1016%2Fj.jmva.2017.09.009">doi:10.1016/j.jmva.2017.09.009</a>&gt;) based on a mixture of Generalized Student distributions. The methods also include BLLiM (see Devijver et al (2017) &lt;<a href="https://doi.org/10.48550/arXiv.1701.07899">doi:10.48550/arXiv.1701.07899</a>&gt;) which is an extension of GLLiM with a sparse block diagonal structure for large covariance matrices (particularly interesting for transcriptomic data).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS,abind,corpcor,Matrix,igraph,capushe,glmnet,randomForest,e1071,mda,progress,mixOmics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>shock</td>
</tr>
<tr>
<td>biocViews:</td>
<td>mixOmics</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-27 09:22:51 UTC; epertham</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-27 10:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='xLLiM-package'>
High Dimensional Locally-Linear Mapping
</h2><span id='topic+xLLiM-package'></span>

<h3>Description</h3>

<p>Provides a tool for non linear mapping (non linear regression) using a mixture of regression model and an inverse regression strategy. The methods include the GLLiM model (see Deleforge et al (2015) &lt;DOI:10.1007/s11222-014-9461-5&gt;) based on Gaussian mixtures and a robust version of GLLiM, named SLLiM (see Perthame et al (2016) &lt;https://hal.archives-ouvertes.fr/hal-01347455&gt;) based on a mixture of Generalized Student distributions. The methods also include BLLiM (see Devijver et al (2017) &lt;https://arxiv.org/abs/1701.07899&gt;) which is an extension of GLLiM with a sparse block diagonal structure for large covariance matrices (particularly interesting for transcriptomic data).
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> xLLiM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2017-05-23</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The methods implemented in this package adress the following non-linear mapping issue:
</p>
<p style="text-align: center;"><code class="reqn"> E(Y | X=x) = g(x),</code>
</p>

<p>where <code class="reqn">Y</code> is a L-vector of multivariate responses and <code class="reqn">X</code> is a large D-vector of covariates' profiles such that <code class="reqn">D \gg L</code>. The methods implemented in this package aims at estimating the non linear regression function <code class="reqn">g</code>.
</p>
<p>First, the methods of this package are based on an inverse regression strategy. The inverse conditional relation <code class="reqn">p(X | Y)</code> is specified in a way that the forward relation of interest <code class="reqn">p(Y | X)</code> can be deduced in closed-from. The large number <code class="reqn">D</code> of covariates is handled by this inverse regression trick, which acts as a dimension reduction technique. The number of parameters to estimate is therefore drastically reduced.
</p>
<p>Second, we propose to approximate the non linear <code class="reqn">g</code> regression function by a piecewise affine function. Therefore, a hidden discrete variable <code class="reqn">Z</code> is introduced, in order to separate the space in <code class="reqn">K</code> regressions such that an affine model holds in each  region <code class="reqn">k</code> between responses Y and variables X:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^K I_{Z=k} (A_k Y + b_k + E_k)</code>
</p>

<p>where <code class="reqn">A_k</code> is a <code class="reqn">D \times L</code> matrix of coeffcients for regression <code class="reqn">k</code>, <code class="reqn">b_k</code> is a D-vector of intercepts and <code class="reqn">E_k</code> is a random noise. 
</p>
<p>All the models implemented in this package are based on mixture of regression models. The components of the mixture are Gaussian for GLLiM. SLLiM is a robust extension of GLLiM, based on Generalized Student mixtures. Indeed, Generalized Student distributions are heavy-tailed distributions which improve the robustness of the model compared to their Gaussian counterparts. BLLiM is an extension of GLLiM designed to provide an interpretable prediction tool for the analysis of transcriptomic data. It assumes a block diagonal dependence structure between covariates (genes) conditionally to the response. The block structure is automatically chosen among a collection of models using the slope heuristics.
</p>
<p>For both GLLiM and SLLiM, this package provides the possibility to add <code class="reqn">L_w</code> latent variables, when the responses are partially observed. In this situation, the vector <code class="reqn">Y=(T,W)</code> is split into an observed <code class="reqn">L_t</code>-vector <code class="reqn">T</code> and an unobserved <code class="reqn">L_w</code>-vector <code class="reqn">W</code>. The total size of the response is therefore <code class="reqn">L=L_t+L_w</code> where <code class="reqn">L_w</code> is chosen by the user. See [1] for details but this amounts to consider factors and allows to add structure in the large dimensional covariance matrices. The user must choose the number of mixtures components <code class="reqn">K</code> and, if needed, the number of latent factors <code class="reqn">L_w</code>. For small datasets (less than 100 observations), we suggest to select both <code class="reqn">(K,L_w)</code> by minimizing the BIC criterion. For larger datasets, we suggest to set <code class="reqn">L_w</code> using BIC while setting <code class="reqn">K</code> to an arbitrary value large enough to catch non linear relations between responses and covariates and small enough to have several observations (at least 10) in each clusters. Indeed, for large datasets, the number of clusters should not have a strong impact on the results provided it is sufficiently large.
</p>
<p>We propose to assess the prediction accuracy of a new response <code class="reqn">x_{test}</code> by computing the NRMSE (Normalized Root Mean Square Error) which is the RMSE normalized by the RMSE of prediction by the mean of training responses:
</p>
<p style="text-align: center;"><code class="reqn">NRMSE = \frac{|| \hat{y} - x_{test}||_2}{|| \bar{y} - x_{test} ||_2}</code>
</p>

<p>where <code class="reqn">\hat{y}</code> is the predicted response, <code class="reqn">x_{test}</code> is the true testing response and <code class="reqn">\bar{y}</code> is the mean of training responses. 
</p>
<p>The functions available in this package are used in this order: 
</p>

<ul>
<li><p> Step 1 (optional): Initialization of the algorithm using  a Multivariate Gaussian mixture model and an EM algorithm implemented in the <code><a href="#topic+emgm">emgm</a></code> function. Responses and covariates must be concatenated as described in the documentation of <code><a href="#topic+emgm">emgm</a></code> which corresponds to a joint Gaussian Mixture Model (see Qiao et al, 2009).
</p>
</li>
<li><p> Step 2: Estimation of a regression model using one of the available models (<code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+sllim">sllim</a></code> or <code><a href="#topic+bllim">bllim</a></code>). User must specify the following arguments
</p>

<ul>
<li><p> for GLLiM or SLLiM: constraint on the large covariance matrices of covariates named <code class="reqn">\Sigma_k</code>. These matrices can be supposed diagonal and homoskedastic (isotropic) by setting <code>cstr=list(Sigma="i")</code> which is the default. Other constraints are diagonal and heteroskedastic <code>(Sigma="d")</code>, full matrix <code>(Sigma="")</code> or full but equal for each class <code>(Sigma="*")</code>. Except for the last constraint, in all previous constraints the matrices have their own parameterization.
</p>
</li>
<li><p> number of components <code class="reqn">K</code> in the  model. 
</p>
</li>
<li><p> for GLLiM or SLLiM: if needed, number of latent factors <code class="reqn">L_w</code>
</p>
</li></ul>

</li>
<li><p> Step 3: Prediction of responses for a testing dataset using the <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code> or <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code> functions. 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Devijver, M. Gallopin, E. Perthame. Nonlinear network-based quantitative trait prediction from transcriptomic data. Submitted, 2017, available at <a href="https://arxiv.org/abs/1701.07899">https://arxiv.org/abs/1701.07899</a>.
</p>
<p>[3] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>
<p>[4] X. Qiao and N. Minematsu. Mixture of probabilistic linear regressions: A unified view of GMM-based mapping techiques. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2009.
</p>
<p>The <code><a href="#topic+gllim">gllim</a></code> and <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code> functions have been converted to R from the original Matlab code of the GLLiM toolbox available on: <a href="https://team.inria.fr/perception/gllim_toolbox/">https://team.inria.fr/perception/gllim_toolbox/</a>
</p>


<h3>See Also</h3>

<p><code><a href="shock.html#topic+shock-package">shock-package</a></code>,<code><a href="capushe.html#topic+capushe-package">capushe-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Not run

## Example of inverse regression with GLLiM model
# data(data.xllim)  
# dim(data.xllim) #  size 52 y 100
# responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
# covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns

## Set 5 components in the model
# K = 5

## Step 1: initialization of the posterior probabilities (class assignments) 
## via standard EM for a joint Gaussian Mixture Model
# r = emgm(rbind(responses, covariates), init=K); 

## Step 2: estimation of the model
## Default Lw=0 and cstr$Sigma="i"
# mod = gllim(responses,covariates,in_K=K,in_r=r)

## Skip Step 1 and go to Step 2: automatic initialization and estimation of the model
# mod = gllim(responses,covariates,in_K=K)

## Alternative: Add Lw=1 latent factor to the model
# mod = gllim(responses,covariates,in_K=K,in_r=r,Lw=1)

## Different constraints on the large covariance matrices can be added: 
## see details in the documentation of the GLLiM function
## description
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="i")) #default
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="d"))
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma=""))
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="*"))
## End of example of inverse regression with GLLiM model

## Step 3: Prediction on a test dataset
# data(data.xllim.test) size 50 y 20
# pred = gllim_inverse_map(data.xllim.test,mod)
## Predicted responses using the mean of \eqn{p(y | x)}.
# pred$x_exp

## Example of leave-ntest-out (1 fold cross-validation) procedure 
# n = ncol(covariates)
# ntest=10
# id.test = sample(1:n,ntest)
# train.responses = responses[,-id.test]
# train.covariates = covariates[,-id.test]
# test.responses = responses[,id.test]
# test.covariates = covariates[,id.test]

## Learn the model on training data
# mod = gllim(train.responses, train.covariates,in_K=K)

## Predict responses on testing data 
# pred = gllim_inverse_map(test.covariates,mod)$x_exp

## nrmse : normalized root mean square error to measure prediction performance 
## the normalization term is the rmse of the prediction by the mean of training responses
## an nrmse larger than 1 means that the procedure performs worse than prediction by the mean
# norm_term = sqrt(rowMeans(sweep(test.responses,1,rowMeans(train.responses),"-")^2))
## Returns 1 value for each response variable 
# nrmse = sqrt(rowMeans((test.responses-pred)^2))/norm_term 

</code></pre>

<hr>
<h2 id='bllim'>EM Algorithm for Block diagonal Gaussian Locally Linear Mapping
</h2><span id='topic+bllim'></span>

<h3>Description</h3>

<p>EM Algorithm for Block diagonal Gaussian Locally Linear Mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bllim(tapp,yapp,in_K,in_r=NULL,ninit=20,maxiter=100,verb=0,in_theta=NULL,plot=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bllim_+3A_tapp">tapp</code></td>
<td>
<p>An <code>L x N</code> matrix of training responses with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="bllim_+3A_yapp">yapp</code></td>
<td>
<p>An <code>D x N</code> matrix of training covariates with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="bllim_+3A_in_k">in_K</code></td>
<td>
<p>Initial number of components or number of clusters</p>
</td></tr>
<tr><td><code id="bllim_+3A_in_r">in_r</code></td>
<td>
<p>Initial assignments (default NULL). If NULL, the model is initialized with the best initialisation among 20, computed by a joint Gaussian mixture model on both response and covariates.</p>
</td></tr>
<tr><td><code id="bllim_+3A_ninit">ninit</code></td>
<td>
<p>Number of random initializations. Not used of <code>in_r</code> is specified. Default is 20 and the random initialization which maximizes the likelihood is retained.</p>
</td></tr>
<tr><td><code id="bllim_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations (default 100). The algorithm stops if the number of iterations exceeds <code>maxiter</code> or if the difference of likelihood between two iterations is smaller than a threshold fixed to <code class="reqn">0.001 (max(LL)-min(LL))</code> where <code class="reqn">LL</code> is the vector of log-likelihoods at the successive iterations.</p>
</td></tr>
<tr><td><code id="bllim_+3A_verb">verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
<tr><td><code id="bllim_+3A_in_theta">in_theta</code></td>
<td>
<p>Initial parameters (default NULL), same structure as the output of this function. The EM algorithm can be  initialized either with initial assignments or initial parameters values.</p>
</td></tr>
<tr><td><code id="bllim_+3A_plot">plot</code></td>
<td>
<p>Displays plots to allow user to check that the slope heuristics can be applied confidently to select the conditional block structure of predictors, as in the <code><a href="capushe.html#topic+capushe-package">capushe-package</a></code> package. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The BLLiM model implemented in this function adresses the following non-linear mapping issue:
</p>
<p style="text-align: center;"><code class="reqn"> E(Y | X=x) = g(x),</code>
</p>

<p>where <code class="reqn">Y</code> is a L-vector of multivariate responses and <code class="reqn">X</code> is a large D-vector of covariates' profiles such that <code class="reqn">D \gg L</code>. As <code><a href="#topic+gllim">gllim</a></code> and <code><a href="#topic+sllim">sllim</a></code>, the <code>bllim</code> function aims at estimating the non linear regression function <code class="reqn">g</code>.
</p>
<p>First, the methods of this package are based on an inverse regression strategy. The inverse conditional relation <code class="reqn">p(X | Y)</code> is specified in a way that the forward relation of interest <code class="reqn">p(Y | X)</code> can be deduced in closed-from. Under some hypothesis on covariance structures, the large number <code class="reqn">D</code> of covariates is handled by this inverse regression trick, which acts as a dimension reduction technique. The number of parameters to estimate is therefore drastically reduced. Second, we propose to approximate the non linear <code class="reqn">g</code> regression function by a piecewise affine function. Therefore, a hidden discrete variable <code class="reqn">Z</code> is introduced, in order to divide the space into <code class="reqn">K</code> regions such that an affine model holds  between responses Y and variables X in each  region <code class="reqn">k</code>:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^K I_{Z=k} (A_k Y + b_k + E_k)</code>
</p>

<p>where <code class="reqn">A_k</code> is a <code class="reqn">D \times L</code> matrix of coeffcients for regression <code class="reqn">k</code>, <code class="reqn">b_k</code> is a D-vector of intercepts and <code class="reqn">E_k</code> is a Gaussian noise with covariance matrix <code class="reqn">\Sigma_k</code>. 
</p>
<p>BLLiM is defined as the following hierarchical Gaussian mixture model for the inverse conditional density <code class="reqn">(X | Y)</code>:
</p>
<p style="text-align: center;"><code class="reqn">p(X | Y=y,Z=k;\theta) = N(X; A_kx+b_k,\Sigma_k)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Y | Z=k; \theta) = N(Y; c_k,\Gamma_k)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Z=k)=\pi_k</code>
</p>

<p>where <code class="reqn">\Sigma_k</code> is a <code class="reqn">D \times D</code> block diagonal covariance structure automatically learnt from data. <code class="reqn">\theta</code> is the set of parameters <code class="reqn">\theta=(\pi_k,c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code>.
The forward conditional density of interest <code class="reqn">p(Y | X)</code> is deduced from these equations and is also a Gaussian mixture of regression model.
</p>
<p>For a given number of affine components (or clusters) K and a given block structure, the number of parameters to estimate is:
</p>
<p style="text-align: center;"><code class="reqn">(K-1)+ K(DL+D+L+ nbpar_{\Sigma}+L(L+1)/2)</code>
</p>

<p>where <code class="reqn">L</code> is the dimension of the response, <code class="reqn">D</code> is the dimension of covariates and <code class="reqn">nbpar_{\Sigma}</code> is the total number of parameters in the large covariance matrix <code class="reqn">\Sigma_k</code> in each cluster. This number of parameters depends on the number and size of blocks in each matrices. 
</p>
<p>Two hyperparameters must be estimated to run BLLiM: 
</p>
 
<ul>
<li><p> Number of mixtures components (or clusters) <code class="reqn">K</code>: we propose to use BIC criterion or slope heuristics as implemented in <code><a href="capushe.html#topic+capushe-package">capushe-package</a></code>
</p>
</li>
<li><p> For a given number of clusters K, the block structure of large covariance matrices specific of each cluster: the size and the number of blocks of each <code class="reqn">\Sigma_k</code> matrix is automatically learnt from data, using an extension of the shock procedure (see <code><a href="shock.html#topic+shock-package">shock-package</a></code>). This procedure is based on a successive thresholding of sample conditional covariance matrix within clusters, building a collection of block structure candidates. The final block structure is retained using slope heuristics. 
</p>
</li></ul>
 
<p>BLLiM is not only a prediction model but also an interpretable tool. For example, it is useful for the analysis of transcriptomic data. Indeed, if covariates are genes and response is a phenotype, the model provides clusters of individuals based on the relation between gene expression data and the phenotype, and also leads to infer a gene regulatory network specific for each cluster of individuals. 
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>LLf</code></td>
<td>
<p>Final log-likelihood</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Log-likelihood value at each iteration of the EM algorithm</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>A vector of length <code>K</code> of mixture weights i.e. prior probabilities for each  component</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p>An <code>(L x K)</code> matrix of means of responses (Y)</p>
</td></tr>
<tr><td><code>Gamma</code></td>
<td>
<p>An <code>(L x L x K)</code> array of <code>K</code> matrices of covariances of responses (Y) </p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>An <code>(D x L x K)</code> array of <code>K</code> matrices of linear transformation matrices</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>An <code>(D x K)</code> matrix in which affine transformation vectors are in columns</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>An <code>(D x D x K)</code> array of covariances of <code class="reqn">X</code></p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>An <code>(N x K)</code> matrix of posterior probabilities</p>
</td></tr>
<tr><td><code>nbpar</code></td>
<td>
<p>The number of parameters estimated in the model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@pasteur.fr), Emilie Devijver (emilie.devijver@kuleuven.be), Melina Gallopin (melina.gallopin@u-psud.fr)
</p>


<h3>References</h3>

<p>[1] E. Devijver, M. Gallopin, E. Perthame. Nonlinear network-based quantitative trait prediction from transcriptomic data. Submitted, 2017, available at <a href="https://arxiv.org/abs/1701.07899">https://arxiv.org/abs/1701.07899</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+emgm">emgm</a></code>, <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code>,<code><a href="capushe.html#topic+capushe-package">capushe-package</a></code>,<code><a href="shock.html#topic+shock-package">shock-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)

## Setting 5 components in the model
K = 5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(data.xllim, init=K); 
## and then the gllim model is estimated
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns

## if initialization is not specified, the model is automatically initialized by EMGM
# mod = bllim(responses,covariates,in_K=K)

## Prediction can be performed using prediction function gllim_inverse_map
# pred = gllim_inverse_map(covariates,mod)$x_exp
</code></pre>

<hr>
<h2 id='data.xllim'>Simulated data to run examples of usage of <code><a href="#topic+gllim">gllim</a></code> and <code><a href="#topic+sllim">sllim</a></code> functions
</h2><span id='topic+data.xllim'></span>

<h3>Description</h3>

<p>Matrix of simulated data, generated under a GLLiM model, with K=5 clusters from the true parameters available in object <code><a href="#topic+data.xllim.trueparameters">data.xllim.trueparameters</a></code>. The goal is to learn the non linear relation between the responses (Y) and the covariates (X) using <code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+bllim">bllim</a></code> or <code><a href="#topic+sllim">sllim</a></code>. Details are given hereafter. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.xllim)</code></pre>


<h3>Format</h3>

<p>A matrix of simulated data with 52 rows and 100 columns (observations). The first 2 rows are responses (Y) and the last 50 rows are covariates (X). The goal is to retrieve <code class="reqn">Y</code> from <code class="reqn">X</code> using <code><a href="#topic+gllim">gllim</a></code> or <code><a href="#topic+sllim">sllim</a></code>.
</p>


<h3>Details</h3>

<p>This dataset is generated under a GLLiM model with L=2, D=50 and N=100. 
</p>
<p>First, the responses <code class="reqn">Y</code> are generated according to a Gaussian Mixture model with K=5 clusters: 
</p>
<p style="text-align: center;"><code class="reqn">p(Y=y | Z=k)= N(y; c_k,\Gamma_k)</code>
</p>

<p>where each <code class="reqn">(c_k)_{k=1}^K</code> is a L-vector randomly sampled from a standardized Gaussian, <code class="reqn">(\Gamma_k)_{k=1}^K</code> are LxL random correlation matrix and <code class="reqn">Z</code> is a multinomial hidden variable which indicates the cluster membership of each observation:
</p>
<p style="text-align: center;"><code class="reqn">p(Z=k) =\pi_k</code>
</p>

<p>where the probabilities <code class="reqn">(\pi_k)_{k=1}^K</code> are sampled from a standard uniform distribution and normalized to sum to 1. 
</p>
<p>Then, the covariates <code class="reqn">X</code> are generated according to a Gaussian Mixture of regressions. It is recalled that GLLiM models the following inverse relation, which is used to generate <code class="reqn">X</code>:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^{K=5} I_{Z=k}(A_kX+b_k+E_k)</code>
</p>

<p>where <code class="reqn">Y</code> is the vector of L responses and <code class="reqn">X</code> is the vector of D covariates and <code class="reqn">Z</code> is the hidden variable of cluster membership introduced above. Regression coefficients <code class="reqn">A_k</code> and intercepts <code class="reqn">b_k</code> are sampled from a standard Gaussian and the covariance matrix of the noise <code class="reqn">\Sigma_k=Var(E_k)</code> is the identity.  
</p>
<p>The goal is to retrieve <code class="reqn">Y</code> from <code class="reqn">X</code> using <code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+bllim">bllim</a></code> or <code><a href="#topic+sllim">sllim</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+sllim">sllim</a></code>, <code><a href="#topic+data.xllim.test">data.xllim.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)
dim(data.xllim) # 52 100
Y = data.xllim[1:2,] # responses # 2 100
X = data.xllim[3:52,] # covariates # 50 100
</code></pre>

<hr>
<h2 id='data.xllim.test'>Testing data to run examples of usage of <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code> and <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code> functions
</h2><span id='topic+data.xllim.test'></span>

<h3>Description</h3>

<p><code>data.xllim.test</code> is a matrix of simulated testing data, generated under the same GLLiM model as <code><a href="#topic+data.xllim">data.xllim</a></code>, from the true parameters available in object <code><a href="#topic+data.xllim.trueparameters">data.xllim.trueparameters</a></code>. The goal is to train a GLLiM (resp. SLLiM and BLLiM) model on training data (see <code><a href="#topic+data.xllim">data.xllim</a></code>) and to retrieve the unknown responses from data.xllim.test using  <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code> (resp. <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.xllim.test)</code></pre>


<h3>Format</h3>

<p>A matrix of simulated testing data with 50 rows (covariates) and 20 columns (observations). 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+data.xllim">data.xllim</a></code>, <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code>, <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(data.xllim.test)
  dim(data.xllim.test) # 50 20
</code></pre>

<hr>
<h2 id='data.xllim.trueparameters'>True parameters used to simulate the datasets <code><a href="#topic+data.xllim">data.xllim</a></code> and <code><a href="#topic+data.xllim.test">data.xllim.test</a></code>
</h2><span id='topic+data.xllim.trueparameters'></span>

<h3>Description</h3>

<p><code>data.xllim.trueparameters</code> is a list containing the true parameters of the GLLiM model used to generate the datasets <code><a href="#topic+data.xllim">data.xllim</a></code> and <code><a href="#topic+data.xllim.test">data.xllim.test</a></code>. We set the number of covariates to D=50, number of responses to L=2 and we simulated a GLLiM model with K=5 components. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.xllim.trueparameters)</code></pre>


<h3>Format</h3>

<p>A list with the following elements
</p>

<dl>
<dt>pi</dt><dd><p>A vector of length <code>K</code> of mixture weights i.e. prior probabilities for each  component</p>
</dd>
<dt>c</dt><dd><p>An <code>(L x K)</code> matrix of means of responses (X)</p>
</dd>
<dt>Gamma</dt><dd><p>An <code>(L x L x K)</code> array of <code>K</code> matrices of covariances of responses (X)</p>
</dd>
<dt>A</dt><dd><p>An <code>(D x L x K)</code> array of <code>K</code> matrices of linear transformation matrices</p>
</dd>
<dt>b</dt><dd><p>An <code>(D x K)</code> matrix in which affine transformation vectors are in columns</p>
</dd>
<dt>Sigma</dt><dd><p>An <code>(D x D x K)</code> array of covariances of <code class="reqn">Y</code></p>
</dd>
</dl>

<p><code>data.xllim.trueparameters</code> has the same that the values returned by <code><a href="#topic+gllim">gllim</a></code> function.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+data.xllim">data.xllim</a></code>, <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code>, <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(data.xllim.trueparameters)
  ## data.xllim.trueparameters$pi # A vector with K=5 elements
  ## data.xllim.trueparameters$c # A matrix with dimension L=2 x K=5
  ## data.xllim.trueparameters$Gamma # An array with dimension L=2 x L=2 x K=5
  ## data.xllim.trueparameters$A # An array with dimension D=50 x L=2 x K=5
  ## data.xllim.trueparameters$b # A matrix with dimension D=50 x K=5
  ## data.xllim.trueparameters$Sigma # An array with dimension D=50 x D=50 x K=5
</code></pre>

<hr>
<h2 id='emgm'>Perform EM algorithm for fitting a Gaussian mixture model (GMM)
</h2><span id='topic+emgm'></span>

<h3>Description</h3>

<p>Perform EM algorithm for fitting a Gaussian mixture model (GMM). In the GLLiM context, this is done  jointly on both responses and covariates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emgm(X, init, maxiter,verb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emgm_+3A_x">X</code></td>
<td>
<p>An <code>(M x N)</code> matrix with variables in rows and observations in columns. <code>M</code> is <code>D+L</code> in the proposed approach</p>
</td></tr>
<tr><td><code id="emgm_+3A_init">init</code></td>
<td>
<p>This argument can be a number <code class="reqn">K</code> of classes (integer), a matrix of posterior probabilities (<code>(N x K)</code> matrix) or a matrix of centers (<code>(M x K)</code> matrix)</p>
</td></tr>
<tr><td><code id="emgm_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations for estimation of the GMM</p>
</td></tr>
<tr><td><code id="emgm_+3A_verb">verb</code></td>
<td>
<p>Print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>label</code></td>
<td>
<p>An <code>N</code> vector of class assignments provided by maximum a posteriori (MAP) on posterior probabilities to belong to each of the K components for each observation</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>A list with the estimated parameters of the  GMM</p>
</td></tr>
<tr><td><code>model$mu</code></td>
<td>
<p>An <code>(M x K)</code> matrix of estimations of means in each cluster of the joint GMM</p>
</td></tr>
<tr><td><code>model$Sigma</code></td>
<td>
<p>An <code>(M x M x K)</code> array of estimations of covariance matrix in each cluster of the  GMM</p>
</td></tr>
<tr><td><code>model$weight</code></td>
<td>
<p>An <code>K</code> vector of estimated prior probabilities of each cluster</p>
</td></tr>
<tr><td><code>llh</code></td>
<td>
<p>A vector of values of the log-likelihood for each iteration of the algorithm</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>An <code>N x K</code> matrix of estimations of posterior probabilities to belong to each of the K components for each observation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing,25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>
<p>[3] Y. Qiao and N. Minematsu. Mixture of probabilistic linear regressions: A unified view of GMM-based mapping techiques. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2009. 
</p>
<p>Converted to R from the Matlab code of the GLLiM toolbox available on: <a href="https://team.inria.fr/perception/gllim_toolbox/">https://team.inria.fr/perception/gllim_toolbox/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+sllim">sllim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data(data.xllim)
# K=5
# r = emgm(data.xllim, init=K, verb=0);  
# r$R # estimation of posterior probabilities to belong to 
## each of the K components for each observation
</code></pre>

<hr>
<h2 id='gllim'>EM Algorithm for Gaussian Locally Linear Mapping
</h2><span id='topic+gllim'></span>

<h3>Description</h3>

<p>EM Algorithm for Gaussian Locally Linear Mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gllim(tapp,yapp,in_K,in_r=NULL,maxiter=100,Lw=0,cstr=NULL,verb=0,in_theta=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gllim_+3A_tapp">tapp</code></td>
<td>
<p>An <code>Lt x N</code> matrix of training responses with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="gllim_+3A_yapp">yapp</code></td>
<td>
<p>An <code>D x N</code> matrix of training covariates with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="gllim_+3A_in_k">in_K</code></td>
<td>
<p>Initial number of components</p>
</td></tr>
<tr><td><code id="gllim_+3A_in_r">in_r</code></td>
<td>
<p>Initial assignments (default NULL)</p>
</td></tr>
<tr><td><code id="gllim_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations (default 100). The algorithm stops if the number of iterations exceeds <code>maxiter</code> or if the difference of likelihood between two iterations is smaller than a threshold fixed to <code class="reqn">0.001 (max(LL)-min(LL))</code> where <code class="reqn">LL</code> is the vector of log-likelihoods at the successive iterations.</p>
</td></tr>
<tr><td><code id="gllim_+3A_lw">Lw</code></td>
<td>
<p>Number of hidden components (default 0)</p>
</td></tr>
<tr><td><code id="gllim_+3A_cstr">cstr</code></td>
<td>
<p>Constraints on error covariance matrices. Must be a list as following <code>cstr=list(Sigma="i")</code> constraints <code class="reqn">\Sigma_k</code> to be diagonal and isotropic, which is the default. See details section hereafter to see the other available options to constraint the covariance matrices. </p>
</td></tr>
<tr><td><code id="gllim_+3A_verb">verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
<tr><td><code id="gllim_+3A_in_theta">in_theta</code></td>
<td>
<p>The EM algorithm can be  initialized either with initial assignments or initial parameters values. In that case, the initial parameters (default NULL) must have the same structure as the output <code>theta</code> of this function.</p>
</td></tr>
<tr><td><code id="gllim_+3A_...">...</code></td>
<td>
<p>other arguments to be passed for internal use only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The GLLiM model implemented in this function adresses the following non-linear mapping issue:
</p>
<p style="text-align: center;"><code class="reqn"> E(Y | X=x) = g(x),</code>
</p>

<p>where <code class="reqn">Y</code> is a L-vector of multivariate responses and <code class="reqn">X</code> is a large D-vector of covariates' profiles such that <code class="reqn">D \gg L</code>. The methods implemented in this package aims at estimating the non linear regression function <code class="reqn">g</code>.
</p>
<p>First, the methods of this package are based on an inverse regression strategy. The inverse conditional relation <code class="reqn">p(X | Y)</code> is specified in a way that the forward relation of interest <code class="reqn">p(Y | X)</code> can be deduced in closed-from. Under some hypothesis on covariance structures, the large number <code class="reqn">D</code> of covariates is handled by this inverse regression trick, which acts as a dimension reduction technique. The number of parameters to estimate is therefore drastically reduced. Second, we propose to approximate the non linear <code class="reqn">g</code> regression function by a piecewise affine function. Therefore, a hidden discrete variable <code class="reqn">Z</code> is introduced, in order to divide the space into <code class="reqn">K</code> regions such that an affine model holds  between responses Y and variables X in each  region <code class="reqn">k</code>:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^K I_{Z=k} (A_k Y + b_k + E_k)</code>
</p>

<p>where <code class="reqn">A_k</code> is a <code class="reqn">D \times L</code> matrix of coeffcients for regression <code class="reqn">k</code>, <code class="reqn">b_k</code> is a D-vector of intercepts and <code class="reqn">E_k</code> is a Gaussian noise with covariance matrix <code class="reqn">\Sigma_k</code>. 
</p>
<p>GLLiM is defined as the following hierarchical Gaussian mixture model for the inverse conditional density <code class="reqn">(X | Y)</code>:
</p>
<p style="text-align: center;"><code class="reqn">p(X | Y=y,Z=k;\theta) = N(X; A_kx+b_k,\Sigma_k)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Y | Z=k; \theta) = N(Y; c_k,\Gamma_k)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Z=k)=\pi_k</code>
</p>

<p>where <code class="reqn">\theta</code> is the set of parameters <code class="reqn">\theta=(\pi_k,c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code>.
The forward conditional density of interest <code class="reqn">p(Y | X)</code> is deduced from these equations and is also a Gaussian mixture of regression model.
</p>
<p><code><a href="#topic+gllim">gllim</a></code> allows the addition of  <code class="reqn">L_w</code> latent variables in order to account for correlation among covariates or if it is supposed that responses are only partially observed. Adding latent factors is known to improve prediction accuracy, if <code class="reqn">L_w</code> is not too large with regard to the number of covariates. When latent factors are added, the dimension of the response is <code class="reqn">L=L_t+L_w</code> and <code class="reqn">L=L_t</code> otherwise.
</p>
<p>For GLLiM, the number of parameters to estimate is:
</p>
<p style="text-align: center;"><code class="reqn">(K-1)+ K(DL+D+L_t+ nbpar_{\Sigma}+nbpar_{\Gamma})</code>
</p>

<p>where <code class="reqn">L=L_w+L_t</code> and <code class="reqn">nbpar_{\Sigma}</code> (resp. <code class="reqn">nbpar_{\Gamma}</code>) is the number of parameters in each of the large (resp. small) covariance matrix <code class="reqn">\Sigma_k</code> (resp. <code class="reqn">\Gamma_k</code>). For example,
</p>

<ul>
<li><p> if the constraint on <code class="reqn">\Sigma</code> is <code>cstr$Sigma="i"</code>, then <code class="reqn">nbpar_{\Sigma}=1</code>,which is the default constraint in the <code>gllim</code> function
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma</code> is <code>cstr$Sigma="d"</code>, then <code class="reqn">nbpar_{\Sigma}=D</code>, 
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma</code> is <code>cstr$Sigma=""</code>, then <code class="reqn">nbpar_{\Sigma}=D(D+1)/2</code>,
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma</code> is <code>cstr$Sigma="*"</code>, then <code class="reqn">nbpar_{\Sigma}=D(D+1)/(2K)</code>. 
</p>
</li></ul>

<p>The rule to compute the number of parameters of <code class="reqn">\Gamma</code> is the same as <code class="reqn">\Sigma</code>, replacing D by <code class="reqn">L_t</code>. Currently the <code class="reqn">\Gamma_k</code> matrices are not constrained and <code class="reqn">nbpar_{\Gamma}=L_t(L_t+1)/2</code> because for indentifiability reasons the <code class="reqn">L_w</code> part is set to the identity matrix.
</p>
<p>The user must choose the number of mixtures components <code class="reqn">K</code> and, if needed, the number of latent factors <code class="reqn">L_w</code>. For small datasets (less than 100 observations), it is suggested to select both <code class="reqn">(K,L_w)</code> by minimizing the BIC criterion. For larger datasets, it is suggested to save computational time, to set <code class="reqn">L_w</code> using BIC while setting <code class="reqn">K</code> to an arbitrary value large enough to catch non linear relations between responses and covariates and small enough to have several observations (at least 10) in each clusters. Indeed, for large datasets, the number of clusters should not have a strong impact on the results while it is sufficiently large.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>LLf</code></td>
<td>
<p>Final log-likelihood</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Log-likelihood value at each iteration of the EM algorithm</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>A vector of length <code>K</code> of mixture weights i.e. prior probabilities for each  component</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p>An <code>(L x K)</code> matrix of means of responses (Y) where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>Gamma</code></td>
<td>
<p>An <code>(L x L x K)</code> array of <code>K</code> matrices of covariances of responses (Y) where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>An <code>(D x L x K)</code> array of <code>K</code> matrices of linear transformation matrices where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>An <code>(D x K)</code> matrix in which affine transformation vectors are in columns</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>An <code>(D x D x K)</code> array of covariances of <code class="reqn">X</code></p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>An <code>(N x K)</code> matrix of posterior probabilities</p>
</td></tr>
<tr><td><code>nbpar</code></td>
<td>
<p>The number of parameters estimated in the model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>
<p>Converted to R from the Matlab code of the GLLiM toolbox available on: <a href="https://team.inria.fr/perception/gllim_toolbox/">https://team.inria.fr/perception/gllim_toolbox/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+emgm">emgm</a></code>, <code><a href="#topic+gllim_inverse_map">gllim_inverse_map</a></code>, <code><a href="#topic+sllim">sllim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)

## Setting 5 components in the model
K =5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(data.xllim, init=K); 
## and then the gllim model is estimated
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns
mod = gllim(responses,covariates,in_K=K,in_r=r);

## if initialization is not specified, the model is automatically initialized by EMGM
## mod = gllim(responses,covariates,in_K=K)

## Adding 1 latent factor 
## mod = gllim(responses,covariates,in_K=K,in_r=r,Lw=1)

## Some constraints on the covariance structure of \eqn{X} can be added
## mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="i")) 
# Isotropic covariances
# (same variance among covariates but different in each component)

## mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="d")) 
# Heteroskedastic covariances
# (variances are different among covariates and in each component)

## mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="")) 
# Unconstrained full matrix

## mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="*")) 
# Full matrix but equal between components
</code></pre>

<hr>
<h2 id='gllim_inverse_map'>Inverse Mapping from gllim or bllim parameters
</h2><span id='topic+gllim_inverse_map'></span>

<h3>Description</h3>

<p>This function computes the prediction of a new response from the estimation of the GLLiM model, returned by the function <code>gllim</code>. Given an observed <code class="reqn">X</code>, the prediction of the corresponding <code class="reqn">Y</code> is obtained by setting <code class="reqn">Y</code> to the mean of the distribution <code class="reqn">p(Y | X)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gllim_inverse_map(y,theta,verb=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gllim_inverse_map_+3A_y">y</code></td>
<td>
<p>An <code>D x N</code> matrix of input observations with variables in rows and subjects on columns</p>
</td></tr>
<tr><td><code id="gllim_inverse_map_+3A_theta">theta</code></td>
<td>
<p>An object returned by the <code>gllim</code> function corresponding to the learned GLLiM model</p>
</td></tr>
<tr><td><code id="gllim_inverse_map_+3A_verb">verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the prediction of a new response from the estimation of GLLiM or a BLLiM model, returned by functions <code>gllim</code> and <code>bllim</code>.
Indeed, if the inverse conditional density <code class="reqn">p(X | Y)</code> and the marginal density <code class="reqn">p(Y)</code> are defined according to a GLLiM model (or BLLiM) (as described on <code><a href="#topic+xLLiM-package">xLLiM-package</a></code> and <code><a href="#topic+gllim">gllim</a></code>), the forward conditional density <code class="reqn">p(Y | X)</code> can be deduced.
</p>
<p>Under GLLiM and BLLiM model, it is recalled that the inverse conditional <code class="reqn">p(X | Y)</code> is a mixture of Gaussian regressions with parameters <code class="reqn">(\pi_k,c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code>. Interestingly, the forward conditional <code class="reqn">p(Y | X)</code> is also a mixture of Gaussian regressions with parameters <code class="reqn">(\pi_k,c_k^*,\Gamma_k^*,A_k^*,b_k^*,\Sigma_k^*)_{k=1}^K</code>. These parameters have a closed-form expression depending only on <code class="reqn">(\pi_k,c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code>. 
</p>
<p>Finally, the forward density (of interest) has the following expression:
</p>
<p style="text-align: center;"><code class="reqn">p(Y | X=x) = \sum_{k=1}^K \frac{\pi_k N(x; c_k^*,\Gamma_k^*)}{\sum_j \pi_j N(x; c_j^*,\Gamma_j^*)} N(y; A_k^*x + b_k^*,\Sigma_k^*)</code>
</p>

<p>and a prediction of a new vector of responses is computed as:
</p>
<p style="text-align: center;"><code class="reqn">E (Y | X=x) = \sum_{k=1}^K \frac{\pi_k N(x; c_k^*,\Gamma_k^*)}{\sum_j \pi_j N(x; c_j^*,\Gamma_j^*)} (A_k^*x + b_k^*)</code>
</p>

<p>where <code class="reqn">x</code> is a  new vector of observed covariates.
</p>
<p>When applied on a BLLiM model (returned by function <code>bllim</code>), the prediction function <code>gllim_inverse_map</code> accounts for the block structure of covariance matrices of the model.  
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>x_exp</code></td>
<td>
<p>An <code>L x N</code> matrix of predicted responses by posterior mean. If <code class="reqn">L_w</code> latent factors are added to the model, the first <code class="reqn">Lt</code> rows (<code class="reqn">1:Lt</code>) are predictions of responses and rows <code class="reqn">(L_t+1):L</code> (recall that <code class="reqn">L=L_t+L_w</code>) are estimations of latent factors.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Weights of the posterior Gaussian mixture model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Devijver, M. Gallopin, E. Perthame. Nonlinear network-based quantitative trait prediction from transcriptomic data. Submitted, 2017, available at <a href="https://arxiv.org/abs/1701.07899">https://arxiv.org/abs/1701.07899</a>.
</p>
<p>[3] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>
<p>Converted to R from the Matlab code of the GLLiM toolbox available on: <a href="https://team.inria.fr/perception/gllim_toolbox/">https://team.inria.fr/perception/gllim_toolbox/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>,<code><a href="#topic+gllim">gllim</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)

## Setting 5 components in the model
K = 5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(data.xllim, init=K); 
## and then the gllim model is estimated
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns
mod = gllim(responses,covariates,in_K=K,in_r=r);

## Charge testing data
data(data.xllim.test)
## Prediction on a test dataset
pred = gllim_inverse_map(data.xllim.test,mod)
## Predicted responses
print(pred$x_exp)

## Can also be applied on an object returned by bllim function 
## Learn the BLLiM model
# mod = bllim(responses,covariates,in_K=K,in_r=r);
## Prediction on a test dataset
# pred = gllim_inverse_map(data.xllim.test,mod)
## Predicted responses
# print(pred$x_exp)
</code></pre>

<hr>
<h2 id='preprocess_data'>
A proposition of function to process high dimensional data before running gllim, sllim or bllim
</h2><span id='topic+preprocess_data'></span>

<h3>Description</h3>

<p>The goal of <code>preprocess_data()</code> is to get relevant clusters for G-, S-, or BLLiM initialization, coupled with a feature selection for high-dimensional datasets. This function is an alternative to the default initialization implemented in <code>gllim()</code>, <code>sllim()</code> and <code>bllim()</code>. 
</p>
<p>In this function, clusters are initialized with K-means, and variable selection is performed with a LASSO (<code>glmnet</code>) within each clusters. Then selected features are merged to get a subset variables before running any prediction method of xLLiM. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_data(tapp,yapp,in_K,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_data_+3A_tapp">tapp</code></td>
<td>
<p>An <code>L x N</code> matrix of training responses with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_yapp">yapp</code></td>
<td>
<p>An <code>D x N</code> matrix of training covariates with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_in_k">in_K</code></td>
<td>
<p>Initial number of components or number of clusters</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_...">...</code></td>
<td>
<p>Other arguments of glmnet can be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>selected.variables</code></td>
<td>
<p>Vector of the indexes of selected variables. Selection is made within clusters and merged hereafter.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>Initialization clusters with k-means</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@pasteur.fr), Emilie Devijver (emilie.devijver@kuleuven.be), Melina Gallopin (melina.gallopin@u-psud.fr)
</p>


<h3>References</h3>

<p>[1] E. Devijver, M. Gallopin, E. Perthame. Nonlinear network-based quantitative trait prediction from transcriptomic data. Submitted, 2017, available at <a href="https://arxiv.org/abs/1701.07899">https://arxiv.org/abs/1701.07899</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="glmnet.html#topic+glmnet-package">glmnet-package</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1
</code></pre>

<hr>
<h2 id='sllim'>EM Algorithm for Student Locally Linear Mapping
</h2><span id='topic+sllim'></span>

<h3>Description</h3>

<p>EM Algorithm for Student Locally Linear Mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sllim(tapp,yapp,in_K,in_r=NULL,maxiter=100,Lw=0,cstr=NULL,verb=0,in_theta=NULL,
 in_phi=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sllim_+3A_tapp">tapp</code></td>
<td>
<p>An <code>Lt x N</code> matrix of training responses with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="sllim_+3A_yapp">yapp</code></td>
<td>
<p>An <code>D x N</code> matrix of training covariates with variables in rows and subjects in columns</p>
</td></tr>
<tr><td><code id="sllim_+3A_in_k">in_K</code></td>
<td>
<p>Initial number of components</p>
</td></tr>
<tr><td><code id="sllim_+3A_in_r">in_r</code></td>
<td>
<p>Initial assignments (default NULL)</p>
</td></tr>
<tr><td><code id="sllim_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations (default 100). The algorithm stops if the number of iterations exceeds <code>maxiter</code> or if the difference of likelihood between two iterations is smaller than a threshold (fixed to <code class="reqn">0.001(max(LL)-min(LL))</code> where <code class="reqn">LL</code> is the vector of successive log-likelihood values at each iteration). </p>
</td></tr>
<tr><td><code id="sllim_+3A_lw">Lw</code></td>
<td>
<p>Number of hidden components (default 0)</p>
</td></tr>
<tr><td><code id="sllim_+3A_cstr">cstr</code></td>
<td>
<p>Constraints on <code class="reqn">X</code> covariance matrices. Must be a list as following <code>cstr=list(Sigma="i")</code> constraints <code class="reqn">\Sigma</code> to be diagonal and isotropic, which is the default. See details section hereafter to see the other available options to constraint the covariance matrix.</p>
</td></tr>
<tr><td><code id="sllim_+3A_verb">verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
<tr><td><code id="sllim_+3A_in_theta">in_theta</code></td>
<td>
<p>Initial parameters (default NULL), same structure as the output of this function</p>
</td></tr>
<tr><td><code id="sllim_+3A_in_phi">in_phi</code></td>
<td>
<p>Initial parameters (default NULL), same structure as the output of this function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the robust counterpart of GLLiM model and should be applied when outliers are present in the data.
</p>
<p>The SLLiM model implemented in this function addresses the following non-linear mapping issue:
</p>
<p style="text-align: center;"><code class="reqn"> E(Y | X=x) = g(x),</code>
</p>

<p>where <code class="reqn">Y</code> is a L-vector of multivariate responses and <code class="reqn">X</code> is a large D-vector of covariates' profiles such that <code class="reqn">D \gg L</code>. The methods implemented in this package aims at estimating the non linear regression function <code class="reqn">g</code>.
</p>
<p>First, the methods of this package are based on an inverse regression strategy. The inverse conditional relation <code class="reqn">p(X | Y)</code> is specified in a way that the forward relation of interest <code class="reqn">p(Y | X)</code> can be deduced in closed-from. Under some hypothesis on covariance structures, the large number <code class="reqn">D</code> of covariates is handled by this inverse regression trick, which acts as a dimension reduction technique. The number of parameters to estimate is therefore drastically reduced. Second, we propose to approximate the non linear <code class="reqn">g</code> regression function by a piecewise affine function. Therefore, an hidden discrete variable <code class="reqn">Z</code> is introduced, in order to divide the space in <code class="reqn">K</code> regions such that an affine model holds between responses Y and variables X, in each  region <code class="reqn">k</code>:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^K I_{Z=k} (A_k Y + b_k + E_k)</code>
</p>

<p>where <code class="reqn">A_k</code> is a <code class="reqn">D \times L</code> matrix of coefficients for regression <code class="reqn">k</code>, <code class="reqn">b_k</code> is a D-vector of intercepts and <code class="reqn">E_k</code> is a noise with covariance matrix proportional to <code class="reqn">\Sigma_k</code>.
</p>
<p>SLLiM is defined as the following hierarchical generalized Student mixture model for the inverse conditional density <code class="reqn">p(X | Y)</code>:
</p>
<p style="text-align: center;"><code class="reqn">p(X=x | Y=y,Z=k; \theta,\phi) = S(x; A_kx+b_k,\Sigma_k,\alpha_k^x,\gamma_k^x)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Y=y | Z=k; \theta,\phi) = S(y; c_k,\Gamma_k,\alpha_k,1)</code>
</p>

<p style="text-align: center;"><code class="reqn">p(Z=k | \phi)=\pi_k</code>
</p>

<p>where <code class="reqn">(\theta,\phi)</code> are the sets of parameters  <code class="reqn">\theta=(c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code> and <code class="reqn">\phi=(\pi_k,\alpha_k)_{k=1}^K</code>. In the previous expression, <code class="reqn">\alpha_k</code> and <code class="reqn">(\alpha_k^x,\gamma_k^x)</code> determine the heaviness of the tail of the generalized Student distribution, which gives robustness to the model. Note that <code class="reqn">\alpha_k^x=\alpha_k + L/2</code> and <code class="reqn">\gamma_k^x=1 + 1/2 \delta(y,c_k,\Gamma_k)</code> where <code class="reqn">\delta</code> is the Mahalanobis distance.
The forward conditional density of interest can be deduced from these equations and is also a Student mixture of regressions model.
</p>
<p>Like <code><a href="#topic+gllim">gllim</a></code>, <code><a href="#topic+sllim">sllim</a></code> allows the addition of latent variables in order to account for correlation among covariates or if it is supposed that responses are only partially observed. Adding latent factors is known to improve prediction accuracy, if <code>Lw</code> is not too large with regard to the number of covariates. When latent factors are added, the dimension of the response is <code>L=Lt+Lw</code> and <code>L=Lt</code> otherwise.
</p>
<p>For SLLiM, the number of parameters to estimate is:
</p>
<p style="text-align: center;"><code class="reqn">(K-1)+ K(1+DL+D+L_t+ nbpar_{\Sigma}+nbpar_{\Gamma})</code>
</p>

<p>where <code class="reqn">L=L_w+L_t</code> and <code class="reqn">nbpar_{\Sigma}</code> (resp. <code class="reqn">nbpar_{\Gamma}</code>) is the number of parameters in each of the large (resp. small) covariance matrix <code class="reqn">\Sigma_k</code> (resp. <code class="reqn">\Gamma_k</code>). For example,
</p>

<ul>
<li><p> if the constraint on <code class="reqn">\Sigma_k</code> is <code>cstr$Sigma="i"</code>, then <code class="reqn">nbpar_{\Sigma}=1</code>,which is the default constraint in the <code>gllim</code> function
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma_k</code> is <code>cstr$Sigma="d"</code>, then <code class="reqn">nbpar_{\Sigma}=D</code>,
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma_k</code> is <code>cstr$Sigma=""</code>, then <code class="reqn">nbpar_{\Sigma}=D(D+1)/2</code>,
</p>
</li>
<li><p> if the constraint on <code class="reqn">\Sigma_k</code> is <code>cstr$Sigma="*"</code>, then <code class="reqn">nbpar_{\Sigma}=D(D+1)/(2K)</code>.
</p>
</li></ul>

<p>The rule to compute the number of parameters of <code class="reqn">\Gamma_k</code> is the same as <code class="reqn">\Sigma_k</code>, replacing D by <code class="reqn">L_t</code>. Currently the <code class="reqn">\Gamma_k</code> matrices are not constrained and <code class="reqn">nbpar_{\Gamma}=L_t(L_t+1)/2</code> because for indentifiability reasons the <code class="reqn">L_w</code> part is set to the identity matrix.
</p>
<p>The user must choose the number of mixtures components <code class="reqn">K</code> and, if needed, the number of latent factors <code class="reqn">L_w</code>. For small datasets (less than 100 observations), we suggest to select both <code class="reqn">(K,L_w)</code> by minimizing the BIC criterion. For larger datasets, to save computation time, we suggest to set <code class="reqn">L_w</code> using BIC while setting <code class="reqn">K</code> to an arbitrary value large enough to catch non linear relations between responses and covariates and small enough to have several observations (at least 10) in each clusters. Indeed, for large datasets, the number of clusters should not have a strong impact on the results while it is sufficiently large.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>LLf</code></td>
<td>
<p>Final log-likelihood</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Log-likelihood value at each iteration of the EM algorithm</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>A list containing the estimations of parameters as follows:</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p>An <code>L x K</code> matrix of means of responses (Y) where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>Gamma</code></td>
<td>
<p>An <code>L x L x K</code> array of <code>K</code> matrices of covariances of responses (Y) where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>An <code>D x L x K</code> array of <code>K</code> matrices of affine transformation matrices where <code>L=Lt+Lw</code></p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>An <code>D x K</code> matrix in which affine transformation vectors are in columns</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>An <code>D x D x K</code> array of <code class="reqn">X</code> covariances</p>
</td></tr>
<tr><td><code>nbpar</code></td>
<td>
<p>The number of parameters estimated in the model</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>A list containing the estimations of parameters as follows:</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>An <code>N x K</code> matrix of posterior probabilities</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>A vector of length <code>K</code> of mixture weights i.e. prior probabilities of all components</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>A vector of length <code>K</code> of degree of freedom parameters (heaviness of the tail) for each Student component</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>, <code><a href="#topic+emgm">emgm</a></code>, <code><a href="#topic+sllim_inverse_map">sllim_inverse_map</a></code>, <code><a href="#topic+gllim">gllim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns

## Setting 5 components in the model
K = 5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(rbind(responses, covariates), init=K); 
## and then the sllim model is estimated
mod = sllim(responses,covariates,in_K=K,in_r=r);

## if initialization is not specified, the model is automatically initialized by EMGM
## mod = sllim(responses,covariates,in_K=K)

## Adding 1 latent factor 
## mod = sllim(responses,covariates,in_K=K,in_r=r,Lw=1)

## Some constraints on the covariance structure of \eqn{X} can be added
## mod = sllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="i")) 
# Isotropic covariance matrices
# (same variance among covariates but different in each component)

## mod = sllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="d")) 
# Heteroskedastic covariance matrices
# (variances are different among covariates and in each component)

## mod = sllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="")) 
# Unconstrained full covariance matrices

## mod = sllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="*")) 
# Full covariance matrices but equal for all components
</code></pre>

<hr>
<h2 id='sllim_inverse_map'>Inverse Mapping from sllim parameters
</h2><span id='topic+sllim_inverse_map'></span>

<h3>Description</h3>

<p>This function computes the prediction of a new response from the estimation of the SLLiM model, returned by the function <code>sllim</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sllim_inverse_map(y,theta,verb=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sllim_inverse_map_+3A_y">y</code></td>
<td>
<p>An <code>D x N</code> matrix of input observations  with variables in rows and subjects on columns</p>
</td></tr>
<tr><td><code id="sllim_inverse_map_+3A_theta">theta</code></td>
<td>
<p>An object returned by the <code>sllim</code> function</p>
</td></tr>
<tr><td><code id="sllim_inverse_map_+3A_verb">verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the prediction of a new response from the estimation of a SLLiM model, returned by the function <code>sllim</code>.
Indeed, if the inverse conditional density <code class="reqn">p(X | Y)</code> and the marginal density <code class="reqn">p(Y)</code> are defined according to a SLLiM model (as described in <code><a href="#topic+xLLiM-package">xLLiM-package</a></code> and <code><a href="#topic+sllim">sllim</a></code>), the forward conditional density <code class="reqn">p(Y | X)</code> can be deduced.
</p>
<p>Under SLLiM model, it is recalled that the inverse conditional <code class="reqn">p(X | Y)</code> is a mixture of Student regressions with parameters <code class="reqn">(c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. Interestingly, the forward conditional <code class="reqn">p(Y | X)</code> is also a mixture of Student regressions with parameters <code class="reqn">(c_k^*,\Gamma_k^*,A_k^*,b_k^*,\Sigma_k^*)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. These parameters have a closed-form expression depending only on <code class="reqn">(c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. 
</p>
<p>Finally, the forward density (of interest) has the following expression:
</p>
<p style="text-align: center;"><code class="reqn">p(Y | X=x) = \sum_k \frac{\pi_k S(x; c_k^*,\Gamma_k^*,\alpha_k,1)}{\sum_j \pi_j S(x; c_j^*,\Gamma_j^*,\alpha_j,1)} S(y; A_k^*x + b_k^*,\Sigma_k^*,\alpha_k^y,\gamma_k^y)</code>
</p>

<p>where <code class="reqn">(\alpha_k^y,\gamma_k^y)</code> determine the heaviness of the tail of the Generalized Student distribution.
Note that <code class="reqn">\alpha_k^y= \alpha_k + D/2</code>  and <code class="reqn">\gamma_k^y= 1 + 1/2 \delta(x,c_k^*,\Gamma_k^*)</code> where <code class="reqn">\delta</code> is the Mahalanobis distance. A prediction of a new vector of responses is computed by:
</p>
<p style="text-align: center;"><code class="reqn">E (Y | X=x) = \sum_k \frac{\pi_k S(x; c_k^*,\Gamma_k^*,\alpha_k,1)}{\sum_j \pi_j S(x; c_j^*,\Gamma_j^*,\alpha_j,1)} (A_k^*x + b_k^*)</code>
</p>

<p>where <code class="reqn">x</code> is a new  vector of observed covariates.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr><td><code>x_exp</code></td>
<td>
<p>An <code>L x N</code> matrix of predicted responses by posterior mean. If <code class="reqn">L_w</code> latent factors are added to the model, the first <code class="reqn">Lt</code> rows (<code class="reqn">1:Lt</code>) are predictions of responses and rows <code class="reqn">(L_t+1):L</code> (recall that <code class="reqn">L=L_t+L_w</code>) are estimations of latent factors.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Weights of the posterior Gaussian mixture model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893&ndash;911, 2015.
</p>
<p>[2] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1&ndash;14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xLLiM-package">xLLiM-package</a></code>,<code><a href="#topic+sllim">sllim</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.xllim)

## Setting 5 components in the model
K = 5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(data.xllim, init=K); 
## and then the sllim model is estimated
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns
mod = sllim(responses,covariates,in_K=K,in_r=r);

# Prediction on a test dataset
data(data.xllim.test)
pred = sllim_inverse_map(data.xllim.test,mod)
## Predicted responses
print(pred$x_exp)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
