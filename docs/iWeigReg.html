<!DOCTYPE html><html><head><title>Help for package iWeigReg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {iWeigReg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#iWeigReg-package'>
<p>A R package for improved methods for causal inference and missing data problems</p></a></li>
<li><a href='#ate.clik'><p>Calibrated likelihood estimator for the causal-inference setup</p></a></li>
<li><a href='#ate.creg'><p>Calibrated regression estimator for the causal-inference setup</p></a></li>
<li><a href='#ate.HT'><p>Horvitz-Thompson estimator for the causal-inference setup</p></a></li>
<li><a href='#ate.lik'><p>Non-calibrated likelihood estimator for the causal-inference setup</p></a></li>
<li><a href='#ate.reg'><p>Non-calibrated regression estimator for the causal-inference setup</p></a></li>
<li><a href='#histw'><p>Weighted histogram</p></a></li>
<li><a href='#KS.data'><p>A simulated dataset</p></a></li>
<li><a href='#loglik'><p>The non-calibrated objective function (&quot;log-likelihood&quot;)</p></a></li>
<li><a href='#loglik.g'><p>The calibrated objective function (&quot;log-likelihood&quot;)</p></a></li>
<li><a href='#mn.clik'><p>Calibrated likelihood estimator for the missing-data setup</p></a></li>
<li><a href='#mn.creg'><p>Calibrated regression estimator for the missing-data setup</p></a></li>
<li><a href='#mn.HT'><p>Horvitz-Thompson estimator for the missing-data setup</p></a></li>
<li><a href='#mn.lik'><p>Non-calibrated likelihood estimator for the missing-data setup</p></a></li>
<li><a href='#mn.reg'><p>Non-calibrated regression estimator for the missing-data setup</p></a></li>
<li><a href='#myinv'><p>Inverse of a matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Improved Methods for Causal Inference and Missing Data Problems</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-05-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Zhiqiang Tan and Heng Shu</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zhiqiang Tan &lt;ztan@stat.rutgers.edu&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Improved methods based on inverse probability weighting
        and outcome regression for causal inference and missing data
        problems.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.9.1), MASS (&ge; 7.2-1), trust</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-20 13:05:07 UTC; Ztan</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-20 13:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='iWeigReg-package'>
A R package for improved methods for causal inference and missing data problems
</h2><span id='topic+iWeigReg-package'></span><span id='topic+iWeigReg'></span>

<h3>Description</h3>

<p>Improved methods based on inverse probability weighting and outcome regression 
for causal inference and missing data problems.
</p>


<h3>Details</h3>

<p>The R package <code>iWeigReg</code> &ndash; version 1.0 can be used for two main tasks:
</p>

<ul>
<li><p> to estimate the mean of an outcome in the presence of missing data,
</p>
</li>
<li><p> to estimate the average treatment effect in causal inference.
</p>
</li></ul>

<p>There are 4 functions provided for the first task:
</p>

<ul>
<li> <p><code>mn.lik</code>: the non-calibrated (or non-doubly robust) likelihood estimator in Tan (2006),
</p>
</li>
<li> <p><code>mn.clik</code>: the calibrated (or doubly robust) likelihood estimator in Tan (2010),
</p>
</li>
<li> <p><code>mn.reg</code>: the non-calibrated (or non-doubly robust) regression estimator,
</p>
</li>
<li> <p><code>mn.creg</code>: the calibrated (or doubly robust) regression estimator in Tan (2006).
</p>
</li></ul>

<p>In parallel, there are also 4 functions for the second task, <code>ate.lik</code>, <code>ate.clik</code>, <code>ate.reg</code>, and <code>ate.creg</code>. Currently, the treatment is assumed to be binary (i.e., untreated or treated). Extensions to multi-valued treatments will be incorporated in later versions.
</p>
<p>In general, the function recommended to use is the calibrated (or doubly robust) likelihood estimator, <code>mn.clik</code> or <code>ate.clik</code>, which is a two-step procedure with the first step corresponding to the non-calibrated (or non-doubly robust) likelihood estimator. The calibrated (or doubly robust) regression estimator, <code>mn.creg</code> or <code>ate.creg</code>, is a close relative to the calibrated likelihood estimator, but may sometimes yield an estimate lying outside the sample range, for example, outside the unit interval (0,1) for estimating the mean of a binary outcome.
</p>
<p>The package also provides two functions, <code>mn.HT</code> and <code>ate.HT</code>, for the Horvitz-Thompson estimator, i.e., the unaugmented inverse probability weighted estimator. These functions can be used for balance checking.
</p>
<p>See the vignette for more details.
</p>

<hr>
<h2 id='ate.clik'>Calibrated likelihood estimator for the causal-inference setup</h2><span id='topic+ate.clik'></span>

<h3>Description</h3>

<p>This function implements the calibrated (or doubly robust) likelihood estimator of the average treatment effect in causal inference in Tan (2010), Biometrika.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ate.clik(y, tr, p, g0,g1, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ate.clik_+3A_y">y</code></td>
<td>
<p>A vector of observed outcomes.</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_tr">tr</code></td>
<td>
<p>A vector of treatment indicators (=1 if treated or 0 if untreated).</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_g0">g0</code></td>
<td>
<p>A matrix of calibration variables for treatment 0 (see the details).</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_g1">g1</code></td>
<td>
<p>A matrix of calibration variables for treatment 1 (see the details).</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="ate.clik_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two-step procedure in Tan (2010, Section 5.4) is used when dealing with estimated propensity scores. The first step corresponds to the non-calibrated (or non-doubly robust) likelihood estimator implemented in <code><a href="#topic+ate.lik">ate.lik</a></code>.
</p>
<p>The columns of <code>g0</code> (or respectively <code>g1</code>) correspond to calibration variables for treatment 0 (or treatment 1), which can be specified to include a constant and the fitted outcome regression function for treatment 0 (or treatment 1). See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted treatment-specific mean should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated means for treatments 1 and 0.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>The estimated average treatment effect.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variances of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>v.diff</code></td>
<td>
<p>The estimated variance of <code>diff</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>A matrix of two columns, giving calibrated weights for treatments 1 and 0 respectively.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>A matrix of two columns, giving lambda maximizing the log-likelihood for treatments 1 and 0 respectively.</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>A vector of two elements, giving the maximum norm (i.e., <code class="reqn">L_\infty</code> norm) of the gradient of the log-likelihood at the maximum for treatments 1 and 0 respectively.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>A vector of two elements, giving convergence status from <em>trust</em> for treatments 1 and 0 respectively.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

eta0.glm &lt;- glm(y ~ x, subset=tr==0, 
               family=y.fam, control=glm.control(maxit=1000))
eta0.hat &lt;- predict.glm(eta0.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.lik &lt;- ate.clik(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat))
out.lik$diff
out.lik$v.diff

#ppi.hat treated as estimated (see the details)
out.lik &lt;- ate.clik(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat), X)
out.lik$diff
out.lik$v.diff
</code></pre>

<hr>
<h2 id='ate.creg'>Calibrated regression estimator for the causal-inference setup</h2><span id='topic+ate.creg'></span>

<h3>Description</h3>

<p>This function implements the calibrated (or doubly robust) regression estimator of the average treatment effect in causal inference in Tan (2006), JASA.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ate.creg(y, tr, p, g0,g1, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ate.creg_+3A_y">y</code></td>
<td>
<p>A vector of observed outcomes.</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_tr">tr</code></td>
<td>
<p>A vector of treatment indicators (=1 if treated or 0 if untreated).</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_g0">g0</code></td>
<td>
<p>A matrix of calibration variables for treatment 0 (see the details).</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_g1">g1</code></td>
<td>
<p>A matrix of calibration variables for treatment 1 (see the details).</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="ate.creg_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g0</code> (or respectively <code>g1</code>) correspond to calibration variables for treatment 0 (or treatment 1), which can be specified to include a constant and the fitted outcome regression function for treatment 0 (or treatment 1). See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted treatment-specific mean should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated means for treatments 1 and 0.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>The estimated average treatment effect.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variances of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>v.diff</code></td>
<td>
<p>The estimated variance of <code>diff</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>A matrix of two colums, giving the vector of regression coefficients for treatments 1 and 0 respectively.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

eta0.glm &lt;- glm(y ~ x, subset=tr==0, 
               family=y.fam, control=glm.control(maxit=1000))
eta0.hat &lt;- predict.glm(eta0.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.reg &lt;- ate.creg(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat))
out.reg$diff
out.reg$v.diff

#ppi.hat treated as estimated
out.reg &lt;- ate.creg(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat), X)
out.reg$diff
out.reg$v.diff
</code></pre>

<hr>
<h2 id='ate.HT'>Horvitz-Thompson estimator for the causal-inference setup</h2><span id='topic+ate.HT'></span>

<h3>Description</h3>

<p>This function implements the Horvitz-Thompson estimator of the mean outcome of the average treatment effect in causal inference.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ate.HT(y, tr, p, X=NULL, bal=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ate.HT_+3A_y">y</code></td>
<td>
<p>A vector or a matrix of observed outcomes.</p>
</td></tr>
<tr><td><code id="ate.HT_+3A_tr">tr</code></td>
<td>
<p>A vector of treatment indicators (=1 if treated or 0 if untreated).</p>
</td></tr>
<tr><td><code id="ate.HT_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="ate.HT_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="ate.HT_+3A_bal">bal</code></td>
<td>
<p>Logical; if <code>TRUE</code>, the function is used for checking balance (see the details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variance estimation is based on asymptotic expansions, allowing for misspecification of the propensity score model.
</p>
<p>For balance checking with <code>bal=TRUE</code>, the input <code>y</code> should correpond to the covariates for which balance is to be checked, and the output <code>mu</code> gives the differences between the Horvitz-Thompson estimates and the overall sample means for these covariates.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated means for treatments 1 and 0 or, if <code>bal=TRUE</code>, their differences from the overall sample means.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>The estimated average treatment effect.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variances of <code>mu</code>.</p>
</td></tr>
<tr><td><code>v.diff</code></td>
<td>
<p>The estimated variance of <code>diff</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#ppi.hat treated as known
out.HT &lt;- ate.HT(y, tr, ppi.hat)
out.HT$diff
out.HT$v.diff

#ppi.hat treated as estimated
out.HT &lt;- ate.HT(y, tr, ppi.hat, X)
out.HT$diff
out.HT$v.diff

#balance checking 
out.HT &lt;- ate.HT(x, tr, ppi.hat, X, bal=TRUE)
out.HT$mu
out.HT$v

out.HT$mu/ sqrt(out.HT$v)   #t-statistic
</code></pre>

<hr>
<h2 id='ate.lik'>Non-calibrated likelihood estimator for the causal-inference setup</h2><span id='topic+ate.lik'></span>

<h3>Description</h3>

<p>This function implements the non-calibrated (or non-doubly robust) likelihood estimator of the average treatment effect in causal inference in Tan (2006), JASA.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ate.lik(y, tr, p, g0,g1, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ate.lik_+3A_y">y</code></td>
<td>
<p>A vector of observed outcomes.</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_tr">tr</code></td>
<td>
<p>A vector of treatment indicators (=1 if treated or 0 if untreated).</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_g0">g0</code></td>
<td>
<p>A matrix of calibration variables for treatment 0 (see the details).</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_g1">g1</code></td>
<td>
<p>A matrix of calibration variables for treatment 1 (see the details).</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="ate.lik_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g0</code> (or respectively <code>g1</code>) correspond to calibration variables for treatment 0 (or treatment 1), which can be specified to include a constant and the fitted outcome regression function for treatment 0 (or treatment 1). See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted treatment-specific mean should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated means for treatments 1 and 0.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>The estimated average treatment effect.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variances of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>v.diff</code></td>
<td>
<p>The estimated variance of <code>diff</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>The vector of calibrated weights.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>The vector of lambda maximizing the log-likelihood.</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>The maximum norm (i.e., <code class="reqn">L_\infty</code> norm) of the gradient of the log-likelihood at <code>lam</code>.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Convergence status from <em>trust</em>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

eta0.glm &lt;- glm(y ~ x, subset=tr==0, 
               family=y.fam, control=glm.control(maxit=1000))
eta0.hat &lt;- predict.glm(eta0.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.lik &lt;- ate.lik(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat))
out.lik$diff
out.lik$v.diff

#ppi.hat treated as estimated
out.lik &lt;- ate.lik(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat), X)
out.lik$diff
out.lik$v.diff
</code></pre>

<hr>
<h2 id='ate.reg'>Non-calibrated regression estimator for the causal-inference setup</h2><span id='topic+ate.reg'></span>

<h3>Description</h3>

<p>This function implements the non-calibrated (or non-doubly robust) regression estimator of the average treatment effect in causal inference.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ate.reg(y, tr, p, g0,g1, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ate.reg_+3A_y">y</code></td>
<td>
<p>A vector of observed outcomes.</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_tr">tr</code></td>
<td>
<p>A vector of treatment indicators (=1 if treated or 0 if untreated).</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_g0">g0</code></td>
<td>
<p>A matrix of calibration variables for treatment 0 (see the details).</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_g1">g1</code></td>
<td>
<p>A matrix of calibration variables for treatment 1 (see the details).</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="ate.reg_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g0</code> (or respectively <code>g1</code>) correspond to calibration variables for treatment 0 (or treatment 1), which can be specified to include a constant and the fitted outcome regression function for treatment 0 (or treatment 1). See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted treatment-specific mean should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions similar to those for <code><a href="#topic+ate.creg">ate.creg</a></code> in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated means for treatments 1 and 0.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>The estimated average treatment effect.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variances of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>v.diff</code></td>
<td>
<p>The estimated variance of <code>diff</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>A matrix of two colums, giving the vector of regression coefficients for treatments 1 and 0 respectively.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

eta0.glm &lt;- glm(y ~ x, subset=tr==0, 
               family=y.fam, control=glm.control(maxit=1000))
eta0.hat &lt;- predict.glm(eta0.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.reg &lt;- ate.reg(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat))
out.reg$diff
out.reg$v.diff

#ppi.hat treated as estimated
out.reg &lt;- ate.reg(y, tr, ppi.hat, 
                     g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat), X)
out.reg$diff
out.reg$v.diff
</code></pre>

<hr>
<h2 id='histw'>Weighted histogram</h2><span id='topic+histw'></span>

<h3>Description</h3>

<p>This function plots a weighted histogram.</p>


<h3>Usage</h3>

<pre><code class='language-R'>histw(x, w, xaxis, xmin, xmax, ymax, 
          bar=TRUE, add=FALSE, col="black", dens=TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="histw_+3A_x">x</code></td>
<td>
<p>A data vector.</p>
</td></tr>
<tr><td><code id="histw_+3A_w">w</code></td>
<td>
<p>A weight vector, which will be rescaled to sum up to one.</p>
</td></tr>
<tr><td><code id="histw_+3A_xaxis">xaxis</code></td>
<td>
<p>A vector of cut points.</p>
</td></tr>
<tr><td><code id="histw_+3A_xmin">xmin</code></td>
<td>
<p>The minimum of <code>x</code> coordinate.</p>
</td></tr>
<tr><td><code id="histw_+3A_xmax">xmax</code></td>
<td>
<p>The maximum of <code>x</code> coordinate.</p>
</td></tr>
<tr><td><code id="histw_+3A_ymax">ymax</code></td>
<td>
<p>The maximum of <code>y</code> coordinate.</p>
</td></tr>
<tr><td><code id="histw_+3A_bar">bar</code></td>
<td>
<p>bar plot (if <code>TRUE</code>) or line plot.</p>
</td></tr>
<tr><td><code id="histw_+3A_add">add</code></td>
<td>
<p>if <code>TRUE</code>, the plot is added to an existing plot.</p>
</td></tr>
<tr><td><code id="histw_+3A_col">col</code></td>
<td>
<p>color of lines.</p>
</td></tr>
<tr><td><code id="histw_+3A_dens">dens</code></td>
<td>
<p>if <code>TRUE</code>, the histogram has a total area of one.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, misspecified
ppi.glm &lt;- glm(tr~x, family=binomial(link=logit))

ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, correct
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ z, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

eta0.glm &lt;- glm(y ~ z, subset=tr==0, 
               family=y.fam, control=glm.control(maxit=1000))
eta0.hat &lt;- predict.glm(eta0.glm, 
               newdata=data.frame(x=x), type="response")

#causal inference
out.clik &lt;- ate.clik(y, tr, ppi.hat, 
               g0=cbind(1,eta0.hat),g1=cbind(1,eta1.hat))

#balance checking
gp1 &lt;- tr==1
gp0 &lt;- tr==0

par(mfrow=c(2,3))
look &lt;- z1

histw(look[gp1], rep(1,sum(gp1)), xaxis=seq(-3.5,3.5,.25),
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], rep(1,sum(gp0)), xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")

histw(look[gp1], 1/ppi.hat[gp1], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], 1/(1-ppi.hat[gp0]), xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")

histw(look[gp1], 1/out.clik$w[gp1,1], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], 1/out.clik$w[gp0,2], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")

look &lt;- z2

histw(look[gp1], rep(1,sum(gp1)), xaxis=seq(-3.5,3.5,.25),
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], rep(1,sum(gp0)), xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")

histw(look[gp1], 1/ppi.hat[gp1], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], 1/(1-ppi.hat[gp0]), xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")

histw(look[gp1], 1/out.clik$w[gp1,1], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8)
histw(look[gp0], 1/out.clik$w[gp0,2], xaxis=seq(-3.5,3.5,.25), 
    xmin=-3.5, xmax=3.5, ymax=.8, bar=0, add=TRUE, col="red")
</code></pre>

<hr>
<h2 id='KS.data'>A simulated dataset</h2><span id='topic+KS.data'></span>

<h3>Description</h3>

<p>A dataset simulated as in Kang and Schafer (2007).</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(KS.data)</code></pre>


<h3>Format</h3>

<p>A data frame containing 1000 rows and 10 columns.</p>


<h3>Details</h3>

 
<p>The dataset is generated as follows.
</p>
<pre>
set.seed(0)

n &lt;- 1000

z &lt;- matrix(rnorm(4*n, 0, 1), nrow=n)

ppi.tr &lt;- as.vector( 1/(1+exp(-z%*%c(-1,.5,-.25,-.1))) )
tr &lt;- rbinom(n, 1, ppi.tr)

y.mean &lt;- as.vector( 210+z
y &lt;- y.mean+rnorm(n, 0, 1)

x &lt;- cbind(exp(z[,1]/2), z[,2]/(1+exp(z[,1]))+10, 
          (z[,1]*z[,3]/25+.6)^3, (z[,2]+z[,4]+20)^2)
x &lt;- t(t(x)/c(1,1,1,400)-c(0,10,0,0))

KS.data &lt;- data.frame(y,tr,z,x)
colnames(KS.data) &lt;- 
   c("y", "tr", "z1", "z2", "z3", "z4", "x1", "x2", "x3", "x4")

save(KS.data, file="KS.data.rda")
</pre> 

<h3>References</h3>

<p>Kang, J.D.Y. and Schafer, J.L. (2007) &quot;Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data,&quot; <em>Statistical Science</em>, 22, 523-539.
</p>

<hr>
<h2 id='loglik'>The non-calibrated objective function (&quot;log-likelihood&quot;)</h2><span id='topic+loglik'></span>

<h3>Description</h3>

<p>This function computes the objective function, its gradient and its Hessian matrix for the non-calibrated likelihood estimator in Tan (2006), JASA.</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglik(lam, tr, h)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglik_+3A_lam">lam</code></td>
<td>
<p>A vector of parameters (&quot;lambda&quot;).</p>
</td></tr>
<tr><td><code id="loglik_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing or treatment indicators.</p>
</td></tr>
<tr><td><code id="loglik_+3A_h">h</code></td>
<td>
<p>A constraint matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective function.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>The gradient of the objective function.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>The Hessian matrix of objective function.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))
p &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#
g1 &lt;- cbind(1,eta1.hat)
h &lt;- cbind(p, (1-p)*g1)

loglik(lam=rep(0,dim(h)[2]-1), tr=tr, h=h)
</code></pre>

<hr>
<h2 id='loglik.g'>The calibrated objective function (&quot;log-likelihood&quot;)</h2><span id='topic+loglik.g'></span>

<h3>Description</h3>

<p>This function computes the objective function, its gradient and its Hessian matrix for the calibrated likelihood estimator in Tan (2010), Biometrika.</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglik.g_+3A_lam">lam</code></td>
<td>
<p>A vector of parameters (&quot;lambda&quot;).</p>
</td></tr>
<tr><td><code id="loglik.g_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing or treatment indicators.</p>
</td></tr>
<tr><td><code id="loglik.g_+3A_h">h</code></td>
<td>
<p>A constraint matrix.</p>
</td></tr>
<tr><td><code id="loglik.g_+3A_pr">pr</code></td>
<td>
<p>A vector of fitted propensity scores.</p>
</td></tr>
<tr><td><code id="loglik.g_+3A_g">g</code></td>
<td>
<p>A matrix of calibration variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective function.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>The gradient of the objective function.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>The Hessian matrix of the objective function.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))
p &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#
g1 &lt;- cbind(1,eta1.hat)
h &lt;- cbind(p, (1-p)*g1)

loglik.g(lam=rep(0,dim(g1)[2]), tr=tr, h=h, pr=p, g=g1)
</code></pre>

<hr>
<h2 id='mn.clik'>Calibrated likelihood estimator for the missing-data setup</h2><span id='topic+mn.clik'></span>

<h3>Description</h3>

<p>This function implements the calibrated (or doubly robust) likelihood estimator of the mean outcome in the presence of missing data in Tan (2010), Biometrika.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn.clik(y, tr, p, g, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn.clik_+3A_y">y</code></td>
<td>
<p>A vector of outcomes with missing data.</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing indicators (=1 if <code>y</code> is observed or 0 if <code>y</code> is missing).</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_g">g</code></td>
<td>
<p>A matrix of calibration variables (see the details).</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="mn.clik_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two-step procedure in Tan (2010, Section 3.3) is used when dealing with estimated propensity scores. The first step corresponds to the non-calibrated (or non-doubly robust) likelihood estimator implemented in <code><a href="#topic+mn.lik">mn.lik</a></code>.
</p>
<p>The columns of <code>g</code> correspond to calibration variables, which can be specified to include a constant and the fitted outcome regression function. See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted mean among &quot;responders&quot; should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated mean.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variance of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>The vector of calibrated weights.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>The vector of lambda maximizing the log-likelihood.</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>The maximum norm (i.e., <code class="reqn">L_\infty</code> norm) of the gradient of the log-likelihood at <code>lam</code>.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Convergence status from <em>trust</em>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#missing data
y[tr==0] &lt;- 0

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.lik &lt;- mn.clik(y, tr, ppi.hat, g=cbind(1,eta1.hat))  
out.lik$mu
out.lik$v

#ppi.hat treated as estimated
out.lik &lt;- mn.clik(y, tr, ppi.hat, g=cbind(1,eta1.hat), X)
out.lik$mu
out.lik$v
</code></pre>

<hr>
<h2 id='mn.creg'>Calibrated regression estimator for the missing-data setup</h2><span id='topic+mn.creg'></span>

<h3>Description</h3>

<p>This function implements the calibrated (or doubly robust) likelihood estimator of the mean outcome in the presence of missing data in Tan (2006), JASA.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn.creg(y, tr, p, g, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn.creg_+3A_y">y</code></td>
<td>
<p>A vector of outcomes with missing data.</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing indicators (=1 if <code>y</code> is observed or 0 if <code>y</code> is missing).</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_g">g</code></td>
<td>
<p>A matrix of calibration variables (see the details).</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="mn.creg_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g</code> correspond to calibration variables, which can be specified to include a constant and the fitted outcome regression function. See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted mean among &quot;responders&quot; should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated mean.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variance of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#missing data
y[tr==0] &lt;- 0

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.reg &lt;- mn.creg(y, tr, ppi.hat, g=cbind(1,eta1.hat))
out.reg$mu
out.reg$v

#ppi.hat treated as estimated
out.reg &lt;- mn.creg(y, tr, ppi.hat, g=cbind(1,eta1.hat), X)
out.reg$mu
out.reg$v
</code></pre>

<hr>
<h2 id='mn.HT'>Horvitz-Thompson estimator for the missing-data setup</h2><span id='topic+mn.HT'></span>

<h3>Description</h3>

<p>This function implements the Horvitz-Thompson estimator of the mean outcome in the presence of missing data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn.HT(y, tr, p, X=NULL, bal=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn.HT_+3A_y">y</code></td>
<td>
<p>A vector or a matrix of outcomes with missing data.</p>
</td></tr>
<tr><td><code id="mn.HT_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing indicators (=1 if <code>y</code> is observed or 0 if <code>y</code> is missing).</p>
</td></tr>
<tr><td><code id="mn.HT_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="mn.HT_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="mn.HT_+3A_bal">bal</code></td>
<td>
<p>Logical; if <code>TRUE</code>, the function is used for checking balance (see the details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variance estimation is based on asymptotic expansions, allowing for misspecification of the propensity score model.
</p>
<p>For balance checking with <code>bal=TRUE</code>, the input <code>y</code> should correpond to the covariates for which balance is to be checked, and the output <code>mu</code> gives the differences between the Horvitz-Thompson estimates and the overall sample means for these covariates.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated mean(s) or, if <code>bal=TRUE</code>, their differences from the overall sample means.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variance(s) of <code>mu</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#missing data
y[tr==0] &lt;- 0

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#ppi.hat treated as known
out.HT &lt;- mn.HT(y, tr, ppi.hat)
out.HT$mu
out.HT$v

#ppi.hat treated as estimated
out.HT &lt;- mn.HT(y, tr, ppi.hat, X)
out.HT$mu
out.HT$v

#balance checking 
out.HT &lt;- mn.HT(x, tr, ppi.hat, X, bal=TRUE)
out.HT$mu
out.HT$v

out.HT$mu/ sqrt(out.HT$v)   #t-statistic
</code></pre>

<hr>
<h2 id='mn.lik'>Non-calibrated likelihood estimator for the missing-data setup</h2><span id='topic+mn.lik'></span>

<h3>Description</h3>

<p>This function implements the non-calibrated (or non-doubly robust) likelihood estimator of the mean outcome in the presence of missing data in Tan (2006), JASA.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn.lik(y, tr, p, g, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn.lik_+3A_y">y</code></td>
<td>
<p>A vector of outcomes with missing data.</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing indicators (=1 if <code>y</code> is observed or 0 if <code>y</code> is missing).</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_g">g</code></td>
<td>
<p>A matrix of calibration variables (see the details).</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="mn.lik_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g</code> correspond to calibration variables, which can be specified to include a constant and the fitted outcome regression function. See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted mean among &quot;responders&quot; should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated mean.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variance of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>The vector of calibrated weights.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>The vector of lambda maximizing the log-likelihood.</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>The maximum norm (i.e., <code class="reqn">L_\infty</code> norm) of the gradient of the log-likelihood at <code>lam</code>.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Convergence status from <em>trust</em>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#missing data
y[tr==0] &lt;- 0

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.lik &lt;- mn.lik(y, tr, ppi.hat, g=cbind(1,eta1.hat))  
out.lik$mu
out.lik$v

#ppi.hat treated as estimated
out.lik &lt;- mn.lik(y, tr, ppi.hat, g=cbind(1,eta1.hat), X)
out.lik$mu
out.lik$v
</code></pre>

<hr>
<h2 id='mn.reg'>Non-calibrated regression estimator for the missing-data setup</h2><span id='topic+mn.reg'></span>

<h3>Description</h3>

<p>This function implements the non-calibrated (or non-doubly robust) likelihood estimator of the mean outcome in the presence of missing data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn.reg(y, tr, p, g, X=NULL, evar=TRUE, inv="solve")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn.reg_+3A_y">y</code></td>
<td>
<p>A vector of outcomes with missing data.</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_tr">tr</code></td>
<td>
<p>A vector of non-missing indicators (=1 if <code>y</code> is observed or 0 if <code>y</code> is missing).</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_p">p</code></td>
<td>
<p>A vector of known or fitted propensity scores.</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_g">g</code></td>
<td>
<p>A matrix of calibration variables (see the details).</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_x">X</code></td>
<td>
<p>The model matrix for the propensity score model, assumed to be logistic (set <code>X=NULL</code> if <code>p</code> is known or treated to be so).</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_evar">evar</code></td>
<td>
<p>Logical; if <code>FALSE</code>, no variance estimation.</p>
</td></tr>
<tr><td><code id="mn.reg_+3A_inv">inv</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of <code>g</code> correspond to calibration variables, which can be specified to include a constant and the fitted outcome regression function. See the examples below. In general, a calibration variable is a function of measured covariates selected to exploit the fact that its weighted mean among &quot;responders&quot; should equal to its unweighted population mean.
</p>
<p>To estimate the propensity scores, a logistic regression model is assumed.
The model matrix <code>X</code> does not need to be provided and can be set to <code>NULL</code>, in which case the estimated propensity scores are treated as known in the estimation.
If the model matrix <code>X</code> is provided, then the &quot;score,&quot; <code>(tr-p)X</code>, from the logistic regression is used to generate additional calibration constraints in the estimation. This may sometimes lead to unreliable estimates due to multicollinearity, as discussed in Tan (2006). Therefore, this option should be used with caution.
</p>
<p>Variance estimation is based on asymptotic expansions similar to those for <code><a href="#topic+mn.creg">mn.creg</a></code> in Tan (2013). Alternatively, resampling methods (e.g., bootstrap) can be used.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mu</code></td>
<td>
<p>The estimated mean.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>The estimated variance of <code>mu</code>, if <code>evar=TRUE</code>.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tan, Z. (2006) &quot;A distributional approach for causal inference using propensity scores,&quot; <em>Journal of the American Statistical Association</em>, 101, 1619-1637.
</p>
<p>Tan, Z. (2010) &quot;Bounded, efficient and doubly robust estimation with inverse weighting,&quot;
<em>Biometrika</em>, 97, 661-682.
</p>
<p>Tan, Z. (2013) &quot;Variance estimation under misspecified models,&quot;
unpublished manuscript, <a href="http://www.stat.rutgers.edu/~ztan">http://www.stat.rutgers.edu/~ztan</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(KS.data)
attach(KS.data)
z=cbind(z1,z2,z3,z4)
x=cbind(x1,x2,x3,x4)

#missing data
y[tr==0] &lt;- 0

#logistic propensity score model, correct
ppi.glm &lt;- glm(tr~z, family=binomial(link=logit))

X &lt;- model.matrix(ppi.glm)
ppi.hat &lt;- ppi.glm$fitted

#outcome regression model, misspecified
y.fam &lt;- gaussian(link=identity)

eta1.glm &lt;- glm(y ~ x, subset=tr==1, 
               family=y.fam, control=glm.control(maxit=1000))
eta1.hat &lt;- predict.glm(eta1.glm, 
               newdata=data.frame(x=x), type="response")

#ppi.hat treated as known
out.reg &lt;- mn.reg(y, tr, ppi.hat, g=cbind(1,eta1.hat))
out.reg$mu
out.reg$v

#ppi.hat treated as estimated
out.reg &lt;- mn.reg(y, tr, ppi.hat, g=cbind(1,eta1.hat), X)
out.reg$mu
out.reg$v
</code></pre>

<hr>
<h2 id='myinv'>Inverse of a matrix</h2><span id='topic+myinv'></span>

<h3>Description</h3>

<p>This function returns the inverse or generalized inverse of a matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>myinv(A, type = "solve")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="myinv_+3A_a">A</code></td>
<td>
<p>A matrix to be inverted.</p>
</td></tr>
<tr><td><code id="myinv_+3A_type">type</code></td>
<td>
<p>Type of matrix inversion, set to &quot;solve&quot; (default) or &quot;ginv&quot; (which can be used in the case of computational singularity).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The inverse of the given matrix <code>A</code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
