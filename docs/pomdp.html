<!DOCTYPE html><html><head><title>Help for package pomdp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pomdp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_policy'><p>Add a Policy to a POMDP Problem Description</p></a></li>
<li><a href='#colors'><p>Default Colors for Visualization in Package pomdp</p></a></li>
<li><a href='#estimate_belief_for_nodes'><p>Estimate the Belief for Policy Graph Nodes</p></a></li>
<li><a href='#Maze'><p>Steward Russell's 4x3 Maze MDP</p></a></li>
<li><a href='#MDP'><p>Define an MDP Problem</p></a></li>
<li><a href='#optimal_action'><p>Optimal action for a belief</p></a></li>
<li><a href='#plot_belief_space'><p>Plot a 2D or 3D Projection of the Belief Space</p></a></li>
<li><a href='#plot_policy_graph'><p>POMDP Plot Policy Graphs</p></a></li>
<li><a href='#policy'><p>Extract the Policy from a POMDP/MDP</p></a></li>
<li><a href='#policy_graph'><p>POMDP Policy Graphs</p></a></li>
<li><a href='#POMDP'><p>Define a POMDP Problem</p></a></li>
<li><a href='#POMDP_accessors'><p>Access to Parts of the POMDP Description</p></a></li>
<li><a href='#pomdp-package'><p>pomdp: Infrastructure for Partially Observable Markov Decision Processes (POMDP)</p></a></li>
<li><a href='#projection'><p>Defining a Belief Space Projection</p></a></li>
<li><a href='#regret'><p>Calculate the Regret of a Policy</p></a></li>
<li><a href='#reward'><p>Calculate the Reward for a POMDP Solution</p></a></li>
<li><a href='#round_stochastic'><p>Round a stochastic vector or a row-stochastic matrix</p></a></li>
<li><a href='#sample_belief_space'><p>Sample from the Belief Space</p></a></li>
<li><a href='#simulate_MDP'><p>Simulate Trajectories in a MDP</p></a></li>
<li><a href='#simulate_POMDP'><p>Simulate Trajectories in a POMDP</p></a></li>
<li><a href='#solve_MDP'><p>Solve an MDP Problem</p></a></li>
<li><a href='#solve_POMDP'><p>Solve a POMDP Problem using pomdp-solver</p></a></li>
<li><a href='#solve_SARSOP'><p>Solve a POMDP Problem using SARSOP</p></a></li>
<li><a href='#Tiger'><p>Tiger Problem POMDP Specification</p></a></li>
<li><a href='#transition_graph'><p>Transition Graph</p></a></li>
<li><a href='#update_belief'><p>Belief Update</p></a></li>
<li><a href='#value_function'><p>Value Function</p></a></li>
<li><a href='#write_POMDP'><p>Read and write a POMDP Model to a File in POMDP Format</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Infrastructure for Partially Observable Markov Decision
Processes (POMDP)</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-20</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Process (POMDP) models. Interfaces for various exact and approximate solution algorithms are available including value iteration, point-based value iteration and SARSOP. Smallwood and Sondik (1973) &lt;<a href="https://doi.org/10.1287%2Fopre.21.5.1071">doi:10.1287/opre.21.5.1071</a>&gt;.</td>
</tr>
<tr>
<td>Classification/ACM:</td>
<td>G.4, G.1.6, I.2.6</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mhahsler/pomdp">https://github.com/mhahsler/pomdp</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mhahsler/pomdp/issues">https://github.com/mhahsler/pomdp/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>pomdpSolve (&ge; 1.0.4), processx, stats, methods, Matrix, Rcpp,
foreach, igraph</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, Ternary, visNetwork, sarsop,
doParallel</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>Copyright (C) Michael Hahsler and Hossein Kamalzadeh.</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'AAA_check_installed.R' 'AAA_pomdp-package.R' 'AAA_shorten.R'
'POMDP.R' 'MDP.R' 'Maze.R' 'POMDP_accessors.R' 'RcppExports.R'
'Tiger.R' 'add_policy.R' 'colors.R'
'estimate_belief_for_nodes.R' 'foreach_helper.R'
'optimal_action.R' 'plot_belief_space.R' 'plot_policy_graph.R'
'policy.R' 'policy_graph.R' 'print.text.R' 'projection.R'
'queue.R' 'read_write_POMDP.R' 'read_write_pomdp_solve.R'
'regret.R' 'reward.R' 'round_stochchastic.R'
'sample_belief_space.R' 'simulate_MDP.R' 'simulate_POMDP.R'
'solve_MDP.R' 'solve_POMDP.R' 'solve_SARSOP.R' 'stack.R'
'transition_graph.R' 'update_belief.R' 'value_function.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-21 00:26:48 UTC; hahsler</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Hahsler <a href="https://orcid.org/0000-0003-2716-1405"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph, cre],
  Hossein Kamalzadeh [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Hahsler &lt;mhahsler@lyle.smu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-21 03:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_policy'>Add a Policy to a POMDP Problem Description</h2><span id='topic+add_policy'></span>

<h3>Description</h3>

<p>Add a policy to a POMDP problem description allows the user to
test policies on modified problem descriptions or to test manually created
policies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_policy(model, policy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_policy_+3A_model">model</code></td>
<td>
<p>a POMDP model description.</p>
</td></tr>
<tr><td><code id="add_policy_+3A_policy">policy</code></td>
<td>
<p>a POMDP policy as a solved POMDP or a policy data.frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The POMDP model description with the added policy.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

sol &lt;- solve_POMDP(Tiger)
sol

# Example 1: Use the solution policy on a changed POMDP problem
#            where listening is perfect and simulate the expected reward

perfect_Tiger &lt;- Tiger
perfect_Tiger$observation_prob &lt;- list(
  listen = "identity", 
  `open-left` = "uniform",
  `open-right` = "uniform"
)

sol_perfect &lt;- add_policy(perfect_Tiger, sol)
sol_perfect

simulate_POMDP(sol_perfect, n = 1000)$avg_reward

# Example 2: Handcraft a policy and apply it to the Tiger problem

# original policy
policy(sol)
plot_value_function(sol)
plot_belief_space(sol)

# create a policy manually where the agent opens a door at a believe of 
#  roughly 2/3 (note the alpha vectors do not represent 
#  a valid value function)
p &lt;- list(
data.frame(
  `tiger-left` = c(1, 0, -2),
  `tiger-right` = c(-2, 0, 1), 
  action = c("open-right", "listen", "open-left"),
  check.names = FALSE
))
p

custom_sol &lt;- add_policy(Tiger, p)
custom_sol

policy(custom_sol)
plot_value_function(custom_sol)
plot_belief_space(custom_sol)

simulate_POMDP(custom_sol, n = 1000)$avg_reward
</code></pre>

<hr>
<h2 id='colors'>Default Colors for Visualization in Package pomdp</h2><span id='topic+colors'></span><span id='topic+colors_discrete'></span><span id='topic+colors_continuous'></span>

<h3>Description</h3>

<p>Default discrete and continuous colors used in pomdp for states (nodes), beliefs and values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colors_discrete(n, col = NULL)

colors_continuous(val, col = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colors_+3A_n">n</code></td>
<td>
<p>number of states.</p>
</td></tr>
<tr><td><code id="colors_+3A_col">col</code></td>
<td>
<p>custom color palette. <code>colors_discrete()</code> uses the first n colors.
<code>colors_continuous()</code> uses these colors to calculate a palette (see <code><a href="grDevices.html#topic+colorRamp">grDevices::colorRamp()</a></code>)</p>
</td></tr>
<tr><td><code id="colors_+3A_val">val</code></td>
<td>
<p>a vector with values to be translated to colors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>colors_discrete()</code> returns a color palette and
<code>colors_continuous()</code> returns the colors associated with the supplied values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colors_discrete(5)

colors_continuous(runif(10))
</code></pre>

<hr>
<h2 id='estimate_belief_for_nodes'>Estimate the Belief for Policy Graph Nodes</h2><span id='topic+estimate_belief_for_nodes'></span>

<h3>Description</h3>

<p>Estimate a belief for each alpha vector (segment of the value function) which represents
a node in the policy graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_belief_for_nodes(
  x,
  method = "auto",
  belief = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_belief_for_nodes_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_method">method</code></td>
<td>
<p>character string specifying the estimation method. Methods include
<code>"auto"</code>, reuse <code>"solver_points"</code>, follow <code>"trajectories"</code>, sample <code>"random_sample"</code>
or <code>"regular_sample"</code>. Auto uses
solver points if available and follows trajectories otherwise.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_belief">belief</code></td>
<td>
<p>start belief used for method trajectories. <code>NULL</code> uses the start belief specified in the model.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_verbose">verbose</code></td>
<td>
<p>logical; show which method is used.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code>sample_belief_space()</code> or the code that follows trajectories.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>estimate_belief_for_nodes()</code> can estimate the belief in several ways:
</p>

<ol>
<li> <p><strong>Use belief points explored by the solver.</strong> Some solvers return explored belief points.
We assign the belief points to the nodes and average each nodes belief.
</p>
</li>
<li> <p><strong>Follow trajectories</strong> (breadth first) till all policy graph nodes have been visited and
return the encountered belief. This implementation returns the first (i.e., shallowest) belief point
that is encountered is used and no averaging is performed. parameter <code>n</code> can be used to
limit the number of nodes searched.
</p>
</li>
<li> <p><strong>Sample a large set</strong> of possible belief points, assigning them to the nodes and then averaging
the belief over the points assigned to each node. This will return a central belief for the node.
Additional parameters like <code>method</code> and the sample size <code>n</code> are passed on to <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.
If no belief point is generated for a segment, then a
warning is produced. In this case, the number of sampled points can be increased.
</p>
</li></ol>

<p><strong>Notes:</strong>
</p>

<ul>
<li><p> Each method may return a different answer. The only thing that is guaranteed is that the returned belief falls
in the range where the value function segment is maximal.
</p>
</li>
<li><p> If some nodes not belief points are sampled, or the node is not reachable from the initial belief,
then a vector with all <code>NaN</code>s will be returned with a warning.
</p>
</li></ul>



<h3>Value</h3>

<p>returns a list with matrices with a belief for each policy graph node. The list elements are the epochs and converged solutions
only have a single element.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# Infinite horizon case with converged solution
sol &lt;- solve_POMDP(model = Tiger, method = "grid")
sol

# default method auto uses the belief points used in the algorithm (if available).
estimate_belief_for_nodes(sol, verbose = TRUE)

# use belief points obtained from trajectories  
estimate_belief_for_nodes(sol, method = "trajectories", verbose = TRUE)

# use a random uniform sample 
estimate_belief_for_nodes(sol, method = "random", verbose = TRUE)

# Finite horizon example with three epochs. 
sol &lt;- solve_POMDP(model = Tiger, horizon = 3)
sol
estimate_belief_for_nodes(sol)
</code></pre>

<hr>
<h2 id='Maze'>Steward Russell's 4x3 Maze MDP</h2><span id='topic+Maze'></span><span id='topic+maze'></span>

<h3>Description</h3>

<p>The 4x3 maze described in Chapter 17 of the the textbook: &quot;Artificial Intelligence: A Modern Approach&quot; (AIMA).
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+MDP">MDP</a>.
</p>


<h3>Details</h3>

<p>The simple maze has the following layout:
</p>
<pre>
    1234        Transition model:
   ######             .8 (action direction)
  3#   +#              ^
  2# # -#              |
  1#S   #         .1 &lt;-|-&gt; .1
   ######
</pre>
<p>We represent the maze states as a matrix with 3 rows (up/down) and
4 columns (left/right). The states are labeled <code>s_1</code> through <code>s_12</code>
(bottom-left to top right) and are fully observable.
The # (state <code>s_5</code>) in the middle of the maze is an obstruction and not reachable.
Rewards are associated with transitions. The default reward (penalty) is -0.04.
The start state marked with <code>S</code> is <code>s_1</code>.
Transitioning to <code>+</code> (state <code>s_12</code>) gives a reward of +1.0, transitioning to <code>-</code> (state <code>s_11</code>)
has a reward of -1.0. States <code>s_11</code> and <code>s_12</code> are terminal (absorbing) states.
</p>
<p>Actions are movements (<code>up</code>, <code>down</code>, <code>left</code>, <code>right</code>). The actions are unreliable with a .8 chance
to move in the correct direction and a 0.1 chance to instead to move in a
perpendicular direction leading to a stochastic transition model.
</p>
<p>Note that the problem has reachable terminal states which leads to a proper policy
(that is guaranteed to reach a terminal state). This means that the solution also
converges without discounting (<code>discount = 1</code>).
</p>


<h3>References</h3>

<p>Russell, S. J. and Norvig, P., &amp; Davis, E. (2021). Artificial intelligence: a modern approach. 4rd ed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The problem can be loaded using data(Maze).

# Here is the complete problem definition:

S &lt;- paste0("s_", seq_len(3 * 4))
s2rc &lt;- function(s) {
  if(is.character(s)) s &lt;- match(s, S)
  c((s - 1) %% 3 + 1, (s - 1) %/% 3 + 1)
}
rc2s &lt;- function(rc) S[rc[1] + 3 * (rc[2] - 1)]

A &lt;- c("up", "down", "left", "right")

T &lt;- function(action, start.state, end.state) {
  action &lt;- match.arg(action, choices = A)
  
  if (start.state %in% c('s_11', 's_12', 's_5')) {
    if (start.state == end.state) return(1)
    else return(0)
  }

  if(action %in% c("up", "down")) error_direction &lt;- c("right", "left")
  else error_direction &lt;- c("up", "down")
  
  rc &lt;- s2rc(start.state)
  delta &lt;- list(up = c(+1, 0), down = c(-1, 0), 
                right = c(0, +1), left = c(0, -1))
  P &lt;- matrix(0, nrow = 3, ncol = 4)
  
  add_prob &lt;- function(P, rc, a, value) {
    new_rc &lt;- rc + delta[[a]]
    if (new_rc[1] &gt; 3 || new_rc[1] &lt; 1 || new_rc[2] &gt; 4 || new_rc[2] &lt; 1 
      || (new_rc[1] == 2 &amp;&amp; new_rc[2]== 2))
      new_rc &lt;- rc
    P[new_rc[1], new_rc[2]] &lt;- P[new_rc[1], new_rc[2]] + value
    P
  }
 
 P &lt;- add_prob(P, rc, action, .8)
 P &lt;- add_prob(P, rc, error_direction[1], .1)
 P &lt;- add_prob(P, rc, error_direction[2], .1)
 P[rbind(s2rc(end.state))]
}

T("up", "s_1", "s_2")

R &lt;- rbind(
 R_(end.state   = '*',     value = -0.04),
 R_(end.state   = 's_11',  value = -1),
 R_(end.state   = 's_12',  value = +1),
 R_(start.state = 's_11',  value = 0),
 R_(start.state = 's_12',  value = 0),
 R_(start.state = 's_5',  value = 0)
)


Maze &lt;- MDP(
 name = "Stuart Russell's 3x4 Maze",
 discount = 1,
 horizon = Inf,
 states = S,
 actions = A,
 start = 1,
 transition_prob = T,
 reward = R
) 

Maze
str(Maze) 

# Layout with state names
matrix(Maze$states,nrow = 3, dimnames = list(1:3, 1:4))[3:1, ]

maze_solved &lt;- solve_MDP(Maze, method = "value")
policy(maze_solved)

# show the utilities and optimal actions organized in the maze layout (like in the AIMA textbook)
matrix(policy(maze_solved)[[1]]$U, nrow = 3, dimnames = list(1:3, 1:4))[3:1, ]
matrix(policy(maze_solved)[[1]]$action, nrow = 3, dimnames = list(1:3, 1:4))[3:1, ]

# Note: the optimal actions for the states with a utility of 0 are artefacts and should be ignored. 
</code></pre>

<hr>
<h2 id='MDP'>Define an MDP Problem</h2><span id='topic+MDP'></span><span id='topic+MDP2POMDP'></span><span id='topic+is_solved_MDP'></span>

<h3>Description</h3>

<p>Defines all the elements of a MDP problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  name = NA
)

MDP2POMDP(x)

is_solved_MDP(x, stop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDP_+3A_states">states</code></td>
<td>
<p>a character vector specifying the names of the states.</p>
</td></tr>
<tr><td><code id="MDP_+3A_actions">actions</code></td>
<td>
<p>a character vector specifying the names of the available
actions.</p>
</td></tr>
<tr><td><code id="MDP_+3A_transition_prob">transition_prob</code></td>
<td>
<p>Specifies the transition probabilities between
states.</p>
</td></tr>
<tr><td><code id="MDP_+3A_reward">reward</code></td>
<td>
<p>Specifies the rewards dependent on action, states and
observations.</p>
</td></tr>
<tr><td><code id="MDP_+3A_discount">discount</code></td>
<td>
<p>numeric; discount rate between 0 and 1.</p>
</td></tr>
<tr><td><code id="MDP_+3A_horizon">horizon</code></td>
<td>
<p>numeric; Number of epochs. <code>Inf</code> specifies an infinite
horizon.</p>
</td></tr>
<tr><td><code id="MDP_+3A_start">start</code></td>
<td>
<p>Specifies in which state the MDP starts.</p>
</td></tr>
<tr><td><code id="MDP_+3A_name">name</code></td>
<td>
<p>a string to identify the MDP problem.</p>
</td></tr>
<tr><td><code id="MDP_+3A_x">x</code></td>
<td>
<p>a <code>MDP</code> object.</p>
</td></tr>
<tr><td><code id="MDP_+3A_stop">stop</code></td>
<td>
<p>logical; stop with an error.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MDPs are similar to POMDPs, however, states are completely observable and
observations are not necessary. The model is defined similar to <a href="#topic+POMDP">POMDP</a>
models, but observations are not specified and the <code>'observations'</code> column in
the the reward specification is always <code>'*'</code>.
</p>
<p><code>MDP2POMDP()</code> reformulates a MDP as a POMDP with one observation per state
that reveals the current state. This is achieved by defining identity
observation probability matrices.
</p>
<p>More details on specifying the model components can be found in the documentation
for <a href="#topic+POMDP">POMDP</a>.
</p>


<h3>Value</h3>

<p>The function returns an object of class MDP which is list with
the model specification. <code><a href="#topic+solve_MDP">solve_MDP()</a></code> reads the object and adds a list element called
<code>'solution'</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Michael's Sleepy Tiger Problem is like the POMDP Tiger problem, but
# has completely observable states because the tiger is sleeping in front
# of the door. This makes the problem an MDP.

STiger &lt;- MDP(
  name = "Michael's Sleepy Tiger Problem",
  discount = .9,

  states = c("tiger-left" , "tiger-right"),
  actions = c("open-left", "open-right", "do-nothing"),
  start = "uniform",

  # opening a door resets the problem
  transition_prob = list(
    "open-left" =  "uniform",
    "open-right" = "uniform",
    "do-nothing" = "identity"),

  # the reward helper R_() expects: action, start.state, end.state, observation, value
  reward = rbind(
    R_("open-left",  "tiger-left",  v = -100),
    R_("open-left",  "tiger-right", v =   10),
    R_("open-right", "tiger-left",  v =   10),
    R_("open-right", "tiger-right", v = -100),
    R_("do-nothing",                v =    0)
  )
)

STiger

sol &lt;- solve_MDP(STiger, eps = 1e-7)
sol

policy(sol)
plot_value_function(sol)

# convert the MDP into a POMDP and solve
STiger_POMDP &lt;- MDP2POMDP(STiger)
sol2 &lt;- solve_POMDP(STiger_POMDP)
sol2 

policy(sol2)
plot_value_function(sol2)
</code></pre>

<hr>
<h2 id='optimal_action'>Optimal action for a belief</h2><span id='topic+optimal_action'></span>

<h3>Description</h3>

<p>Determines the optimal action for a policy (solved POMDP) for a given belief
at a given epoch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_action(model, belief = NULL, epoch = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimal_action_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="optimal_action_+3A_belief">belief</code></td>
<td>
<p>The belief (probability distribution over the states) as a
vector or a matrix with multiple belief states as rows. If <code>NULL</code>, then the initial belief of the
model is used.</p>
</td></tr>
<tr><td><code id="optimal_action_+3A_epoch">epoch</code></td>
<td>
<p>what epoch of the policy should be used. Use 1 for converged policies.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The name of the optimal action.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
Tiger

sol &lt;- solve_POMDP(model = Tiger)

# these are the states
sol$states

# belief that tiger is to the left
optimal_action(sol, c(1, 0))
optimal_action(sol, "tiger-left")

# belief that tiger is to the right
optimal_action(sol, c(0, 1))
optimal_action(sol, "tiger-right")

# belief is 50/50
optimal_action(sol, c(.5, .5))
optimal_action(sol, "uniform")

# the POMDP is converged, so all epoch give the same result.
optimal_action(sol, "tiger-right", epoch = 10)

</code></pre>

<hr>
<h2 id='plot_belief_space'>Plot a 2D or 3D Projection of the Belief Space</h2><span id='topic+plot_belief_space'></span>

<h3>Description</h3>

<p>Plots the optimal action, the node in the policy graph or the reward for a
given set of belief points on a line (2D) or on a ternary plot (3D). If no
points are given, points are sampled using a regular arrangement or randomly
from the (projected) belief space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_belief_space(
  model,
  projection = NULL,
  epoch = 1,
  sample = "regular",
  n = 100,
  what = c("action", "pg_node", "reward"),
  legend = TRUE,
  pch = 20,
  col = NULL,
  jitter = 0,
  oneD = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_belief_space_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_epoch">epoch</code></td>
<td>
<p>display this epoch.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_sample">sample</code></td>
<td>
<p>a matrix with belief points as rows or a character string
specifying the <code>method</code> used for <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_n">n</code></td>
<td>
<p>number of points sampled.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_what">what</code></td>
<td>
<p>what to plot.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_legend">legend</code></td>
<td>
<p>logical; add a legend? If the legend is covered by the plot then you
need to increase the plotting region of the plotting device.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_pch">pch</code></td>
<td>
<p>plotting symbols.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_col">col</code></td>
<td>
<p>plotting colors.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_jitter">jitter</code></td>
<td>
<p>jitter amount for 2D belief spaces (good values are between 0 and 1, while using <code>ylim = c(0,1)</code>).</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_oned">oneD</code></td>
<td>
<p>plot projections on two states in one dimension.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to <code>plot</code> for 2D or
<code>TerneryPlot</code> for 3D.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns invisibly the sampled points.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># two-state POMDP
data("Tiger")
sol &lt;- solve_POMDP(Tiger)

plot_belief_space(sol)
plot_belief_space(sol, oneD = FALSE)
plot_belief_space(sol, n = 10)
plot_belief_space(sol, n = 100, sample = "random")

# plot the belief points used by the grid-based solver
plot_belief_space(sol, sample = sol $solution$belief_points_solver)

# plot different measures
plot_belief_space(sol, what = "pg_node")
plot_belief_space(sol, what = "reward")

# three-state POMDP
# Note: If the plotting region is too small then the legend might run into the plot
data("Three_doors")
sol &lt;- solve_POMDP(Three_doors)
sol

# plotting needs the suggested package Ternary
if ("Ternary" %in% installed.packages()) {
plot_belief_space(sol)
plot_belief_space(sol, n = 10000)
plot_belief_space(sol, what = "reward", sample = "random", n = 1000)
plot_belief_space(sol, what = "pg_node", n = 10000)

# holding tiger-left constant at .5 follows this line in the ternary plot 
Ternary::TernaryLines(list(c(.5, 0, .5), c(.5, .5, 0)), col = "black", lty = 2)
# we can plot the projection for this line 
plot_belief_space(sol, what = "pg_node", n = 1000, projection = c("tiger-left" = .5))

# plot the belief points used by the grid-based solver
plot_belief_space(sol, sample = sol$solution$belief_points_solver, what = "pg_node")

# plot the belief points obtained using simulated trajectories with an epsilon-greedy policy.
# Note that we only use n = 50 to save time.
plot_belief_space(sol, 
  sample = simulate_POMDP(sol, n = 50, horizon = 100,
    epsilon = 0.1, return_beliefs = TRUE)$belief_states)
}

# plot a 3-state belief space using ggtern (ggplot2)
## Not run: 
library(ggtern)
samp &lt;- sample_belief_space(sol, n = 1000)
df &lt;- cbind(as.data.frame(samp), reward_node_action(sol, belief = samp))
df$pg_node &lt;- factor(df$pg_node)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = pg_node), size = 2)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = action), size = 2)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = reward), size = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_policy_graph'>POMDP Plot Policy Graphs</h2><span id='topic+plot_policy_graph'></span><span id='topic+curve_multiple_directed'></span>

<h3>Description</h3>

<p>The function plots the POMDP policy graph for converged POMDP solution and the
policy tree for a finite-horizon solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_policy_graph(
  x,
  belief = NULL,
  engine = c("igraph", "visNetwork"),
  show_belief = TRUE,
  state_col = NULL,
  legend = TRUE,
  simplify_observations = TRUE,
  remove_unreachable_nodes = TRUE,
  ...
)

curve_multiple_directed(graph, start = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_policy_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_belief">belief</code></td>
<td>
<p>the initial belief is used to mark the initial belief state in the
graph of a converged solution and to identify the root node in a policy graph for a finite-horizon solution.
If <code>NULL</code> then the belief is taken from the model definition.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_engine">engine</code></td>
<td>
<p>The plotting engine to be used.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_show_belief">show_belief</code></td>
<td>
<p>logical; show estimated belief proportions as a pie chart or color in each node?</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the belief over states in each node. Only used if <code>show_belief</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_legend">legend</code></td>
<td>
<p>logical; display a legend for colors used belief proportions?</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_simplify_observations">simplify_observations</code></td>
<td>
<p>combine parallel observation arcs into a single arc.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_remove_unreachable_nodes">remove_unreachable_nodes</code></td>
<td>
<p>logical; remove nodes that are not reachable from the start state? Currently only implemented for policy trees for unconverged finite-time horizon POMDPs.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code>policy_graph()</code>, <code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code> and the functions
they use. Also, plotting options are passed on to the plotting engine <code><a href="igraph.html#topic+plot.igraph">igraph::plot.igraph()</a></code>
or <code><a href="visNetwork.html#topic+visNetwork-igraph">visNetwork::visIgraph()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_graph">graph</code></td>
<td>
<p>The input graph.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_start">start</code></td>
<td>
<p>The curvature at the two extreme edges.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The policy graph returned by <code><a href="#topic+policy_graph">policy_graph()</a></code> can be directly plotted. <code>plot_policy_graph()</code>
uses <code>policy_graph()</code> to get the policy graph and produces an
improved visualization (a legend, tree layout for finite-horizon solutions, better edge curving, etc.).
It also offers an interactive visualization using <code><a href="visNetwork.html#topic+visNetwork-igraph">visNetwork::visIgraph()</a></code>.
</p>
<p>Each policy graph node is represented by an alpha vector specifying a hyper plane segment. The convex hull of
the set of hyperplanes represents the the value function.
The policy specifies for each node an optimal action which is printed together with the node ID inside the node.
The arcs are labeled with observations.
Infinite-horizon converged solutions from a single policy graph.
For finite-horizon solution a policy tree is produced.
The levels of the tree and the first number in the node label represent the epochs.
</p>
<p>For better visualization, we provide a few features:
</p>

<ul>
<li><p> Show Belief, belief color and legend: A pie chart (or the color) in each node can be used
represent an example of the belief that the agent has if it is in this node.
This can help with interpreting the policy graph. The belief is obtained by calling
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code>.
</p>
</li>
<li><p> Simplify observations: In some cases, two observations can lead to the same node resulting in two parallel edges.
These edges can be collapsed into one labels with the observations.
</p>
</li>
<li><p> Remove unreachable nodes: Many algorithms produce
unused policy graph nodes which can be filtered to produce a smaller tree structure of actually used nodes.
Non-converged policies depend on the initial belief and if an initial belief is
specified, then different nodes will be filtered and the tree will look different.
</p>
</li></ul>

<p>These improvements can be disabled using parameters.
</p>


<h4>Auxiliary function</h4>

<p><code>curve_multiple_directed()</code> is a helper function for plotting igraph graphs similar to <code>igraph::curve_multiple()</code> but
it also adds curvature to parallel edges that point in opposite directions.
</p>



<h3>Value</h3>

<p>returns invisibly what the plotting engine returns.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

### Policy graphs for converged solutions
sol &lt;- solve_POMDP(model = Tiger)
sol

policy_graph(sol)

## visualization
plot_policy_graph(sol)

## use a different graph layout (circle and manual; needs igraph)
library("igraph")
plot_policy_graph(sol, layout = layout.circle)
plot_policy_graph(sol, layout = rbind(c(1,1), c(1,-1), c(0,0), c(-1,-1), c(-1,1)), margin = .2)
plot_policy_graph(sol,
  layout = rbind(c(1,0), c(.5,0), c(0,0), c(-.5,0), c(-1,0)), rescale = FALSE,
  vertex.size = 15, edge.curved = 2,
  main = "Tiger Problem")

## hide labels, beliefs and legend
plot_policy_graph(sol, show_belief = FALSE, edge.label = NA, vertex.label = NA, legend = FALSE)

## custom larger vertex labels (A, B, ...)
plot_policy_graph(sol,
  vertex.label = LETTERS[1:nrow(policy(sol)[[1]])],
  vertex.size = 60,
  vertex.label.cex = 2,
  edge.label.cex = .7,
  vertex.label.color = "white")

## plotting the igraph object directly
pg &lt;- policy_graph(sol, show_belief = TRUE, 
  simplify_observations = TRUE, remove_unreachable_nodes = TRUE)

## (e.g., using a tree layout)
plot(pg, layout = layout_as_tree(pg, root = 3, mode = "out"))

## change labels (abbreviate observations and use only actions to label the vertices)
plot(pg,
  edge.label = abbreviate(E(pg)$label),
  vertex.label = V(pg)$action,
  vertex.size = 20)

## use action to color vertices (requires a graph without a belief pie chart) 
##    and color edges to represent observations.
pg &lt;- policy_graph(sol, show_belief = FALSE, 
  simplify_observations = TRUE, remove_unreachable_nodes = TRUE)

plot(pg,
  vertex.label = NA,
  vertex.color = factor(V(pg)$action),
  vertex.size = 20,
  edge.color = factor(E(pg)$observation),
  edge.curved = .1
  )

acts &lt;- levels(factor(V(pg)$action))
legend("topright", legend = acts, title = "action",
  col = igraph::categorical_pal(length(acts)), pch = 15)
obs &lt;- levels(factor(E(pg)$observation))
legend("bottomright", legend = obs, title = "observation",
  col = igraph::categorical_pal(length(obs)), lty = 1) 

## plot interactive graphs using the visNetwork library.
## Note: the pie chart representation is not available, but colors are used instead.
plot_policy_graph(sol, engine = "visNetwork")

## add smooth edges and a layout (note, engine can be abbreviated)
plot_policy_graph(sol, engine = "visNetwork", layout = "layout_in_circle", smooth = TRUE)


### Policy trees for finite-horizon solutions
sol &lt;- solve_POMDP(model = Tiger, horizon = 4, method = "incprune")

policy_graph(sol)

plot_policy_graph(sol)
# Note: the first number in the node id is the epoch.

# plot the policy tree for an initial belief of 90% that the tiger is to the left
plot_policy_graph(sol, belief = c(0.9, 0.1))

# Plotting a larger graph (see ? igraph.plotting for plotting options)
sol &lt;- solve_POMDP(model = Tiger, horizon = 10, method = "incprune")

plot_policy_graph(sol, edge.arrow.size = .1,
  vertex.label.cex = .5, edge.label.cex = .5)

plot_policy_graph(sol, engine = "visNetwork")
</code></pre>

<hr>
<h2 id='policy'>Extract the Policy from a POMDP/MDP</h2><span id='topic+policy'></span>

<h3>Description</h3>

<p>Extracts the policy from a solved POMDP/MDP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy(x, alpha = TRUE, action = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_+3A_x">x</code></td>
<td>
<p>A solved <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a> object.</p>
</td></tr>
<tr><td><code id="policy_+3A_alpha">alpha</code></td>
<td>
<p>logical; include the parameters of the alpha vector defining the segment (POMDP only).</p>
</td></tr>
<tr><td><code id="policy_+3A_action">action</code></td>
<td>
<p>logical; include the action for that segment (POMDP only).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A list (one entry per epoch) with the optimal policy.
For converged, infinite-horizon problems solutions, a list with only the
converged solution is produced.
For a POMDP, the policy is a data.frame consisting of:
</p>

<ul>
<li><p> Part 1: The value function with one column per state (alpha vectors).
</p>
</li>
<li><p> Part 2: The last column contains the prescribed action.
</p>
</li></ul>

<p>For an MDP, the policy is a data.frame consisting of:
</p>

<ul>
<li><p> The state
</p>
</li>
<li><p> The state's discounted expected utility U if the policy is followed
</p>
</li>
<li><p> The prescribed action
</p>
</li></ul>



<h3>Value</h3>

<p>A list with the policy for each epoch.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# Infinite horizon
sol &lt;- solve_POMDP(model = Tiger)
sol

# policy with value function, optimal action and transitions for observations.
policy(sol)
plot_value_function(sol)

# Finite horizon (we use incremental pruning because grid does not converge)
sol &lt;- solve_POMDP(model = Tiger, method = "incprune", horizon = 3, discount = 1)
sol

policy(sol)
# Note: We see that it is initially better to listen till we make a decision in the final epoch.

# MDP policy
data(Maze)

sol &lt;- solve_MDP(Maze)

policy(sol)
</code></pre>

<hr>
<h2 id='policy_graph'>POMDP Policy Graphs</h2><span id='topic+policy_graph'></span>

<h3>Description</h3>

<p>The function creates a POMDP policy graph for converged POMDP solution and the
policy tree for a finite-horizon solution.
The graph is represented as an <span class="pkg">igraph</span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_graph(
  x,
  belief = NULL,
  show_belief = FALSE,
  state_col = NULL,
  simplify_observations = FALSE,
  remove_unreachable_nodes = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_belief">belief</code></td>
<td>
<p>the initial belief is used to mark the initial belief state in the
grave of a converged solution and to identify the root node in a policy graph for a finite-horizon solution.
If <code>NULL</code> then the belief is taken from the model definition.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_show_belief">show_belief</code></td>
<td>
<p>logical; show estimated belief proportions as a pie chart or color in each node?</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the belief over the states in each node. Only used if <code>show_belief</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_simplify_observations">simplify_observations</code></td>
<td>
<p>combine parallel observation arcs into a single arc.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_remove_unreachable_nodes">remove_unreachable_nodes</code></td>
<td>
<p>logical; remove nodes that are not reachable from the start state? Currently only implemented for policy trees for unconverged finite-time horizon POMDPs.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each policy graph node is represented by an alpha vector specifying a hyper plane segment. The convex hull of
the set of hyperplanes represents the the value function.
The policy specifies for each node an optimal action which is printed together with the node ID inside the node.
The arcs are labeled with observations.
Infinite-horizon converged solutions from a single policy graph.
For finite-horizon solution a policy tree is produced.
The levels of the tree and the first number in the node label represent the epochs.
</p>
<p>The parameters <code>show_belief</code>, <code>remove_unreachable_nodes</code>, and <code>simplify_observations</code> are
used by <code><a href="#topic+plot_policy_graph">plot_policy_graph()</a></code> (see there for details) to reduce clutter and make the visualization more readable.
These options are disabled by default for <code>policy_graph()</code>.
</p>


<h3>Value</h3>

<p>returns the policy graph as an igraph object.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

### Policy graphs for converged solutions
sol &lt;- solve_POMDP(model = Tiger)
sol

policy_graph(sol)

## visualization
plot_policy_graph(sol)

### Policy trees for finite-horizon solutions
sol &lt;- solve_POMDP(model = Tiger, horizon = 4, method = "incprune")

policy_graph(sol)
plot_policy_graph(sol)
# Note: the first number in the node id is the epoch.
</code></pre>

<hr>
<h2 id='POMDP'>Define a POMDP Problem</h2><span id='topic+POMDP'></span><span id='topic+is_solved_POMDP'></span><span id='topic+is_timedependent_POMDP'></span><span id='topic+epoch_to_episode'></span><span id='topic+is_converged_POMDP'></span><span id='topic+O_'></span><span id='topic+T_'></span><span id='topic+R_'></span>

<h3>Description</h3>

<p>Defines all the elements of a POMDP problem including the discount rate, the
set of states, the set of actions, the set of observations, the transition
probabilities, the observation probabilities, and rewards.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>POMDP(
  states,
  actions,
  observations,
  transition_prob,
  observation_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  terminal_values = NULL,
  start = "uniform",
  normalize = TRUE,
  name = NA
)

is_solved_POMDP(x, stop = FALSE, message = "")

is_timedependent_POMDP(x)

epoch_to_episode(x, epoch)

is_converged_POMDP(x, stop = FALSE, message = "")

O_(action = NA, end.state = NA, observation = NA, probability)

T_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, observation = NA, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="POMDP_+3A_states">states</code></td>
<td>
<p>a character vector specifying the names of the states. Note that
state names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_actions">actions</code></td>
<td>
<p>a character vector specifying the names of the available actions.
Note that action names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_observations">observations</code></td>
<td>
<p>a character vector specifying the names of the
observations. Note that observation names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_transition_prob">transition_prob</code></td>
<td>
<p>Specifies action-dependent transition probabilities
between states.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_observation_prob">observation_prob</code></td>
<td>
<p>Specifies the probability that an action/state
combination produces an observation.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_reward">reward</code></td>
<td>
<p>Specifies the rewards structure dependent on action, states
and observations.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_discount">discount</code></td>
<td>
<p>numeric; discount factor between 0 and 1.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_horizon">horizon</code></td>
<td>
<p>numeric; Number of epochs. <code>Inf</code> specifies an infinite
horizon.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_terminal_values">terminal_values</code></td>
<td>
<p>a vector with the terminal values for each state or a
matrix specifying the terminal rewards via a terminal value function (e.g.,
the alpha component produced by <code>solve_POMDP()</code>).  A single 0 specifies that all
terminal values are zero.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_start">start</code></td>
<td>
<p>Specifies the initial belief state of the agent. A vector with the
probability for each state is supplied. Also the string <code>'uniform'</code>
(default) can be used.  The belief is used to calculate the total expected cumulative
reward. It is also used by some solvers. See Details section for more
information.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_normalize">normalize</code></td>
<td>
<p>logical; should the description be normalized for faster access (see <code><a href="#topic+normalize_POMDP">normalize_POMDP()</a></code>)?</p>
</td></tr>
<tr><td><code id="POMDP_+3A_name">name</code></td>
<td>
<p>a string to identify the POMDP problem.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_x">x</code></td>
<td>
<p>a POMDP.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_stop">stop</code></td>
<td>
<p>logical; stop with an error.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_message">message</code></td>
<td>
<p>a error message to be displayed displayed</p>
</td></tr>
<tr><td><code id="POMDP_+3A_epoch">epoch</code></td>
<td>
<p>integer; an epoch that should be converted to the corresponding episode in a time-dependent
POMDP.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_action">action</code>, <code id="POMDP_+3A_start.state">start.state</code>, <code id="POMDP_+3A_end.state">end.state</code>, <code id="POMDP_+3A_observation">observation</code>, <code id="POMDP_+3A_probability">probability</code>, <code id="POMDP_+3A_value">value</code></td>
<td>
<p>Values
used in the helper functions <code>O_()</code>, <code>R_()</code>, and <code>T_()</code> to
create an entry for <code>observation_prob</code>, <code>reward</code>, or
<code>transition_prob</code> above, respectively. The default value <code>'*"'</code>
matches any action/state/observation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the following we use the following notation. The POMDP is a 7-duple:
</p>
<p><code class="reqn">(S,A,T,R, \Omega ,O, \gamma)</code>.
</p>
<p><code class="reqn">S</code> is the set of states; <code class="reqn">A</code>
is the set of actions; <code class="reqn">T</code> are the conditional transition probabilities
between states; <code class="reqn">R</code> is the reward function; <code class="reqn">\Omega</code> is the set of
observations; <code class="reqn">O</code> are the conditional observation probabilities; and
<code class="reqn">\gamma</code> is the discount factor. We will use lower case letters to
represent a member of a set, e.g., <code class="reqn">s</code> is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
<code class="reqn">|A|</code>.
</p>
<p>Note that the observation model is in the literature
often also denoted by the letter <code class="reqn">Z</code>.
</p>
<p><strong>Names used for mathematical symbols in code</strong>
</p>

<ul>
<li> <p><code class="reqn">S, s, s'</code>: <code style="white-space: pre;">&#8288;'states', start.state', 'end.state'&#8288;</code>
</p>
</li>
<li> <p><code class="reqn">A, a</code>: <code style="white-space: pre;">&#8288;'actions', 'action'&#8288;</code>
</p>
</li>
<li> <p><code class="reqn">\Omega, o</code>: <code style="white-space: pre;">&#8288;'observations', 'observation'&#8288;</code>
</p>
</li></ul>

<p>State names, actions and observations can be specified as strings or index numbers
(e.g., <code>start.state</code> can be specified as the index of the state in <code>states</code>).
For the specification as data.frames below, <code>NA</code> can be used to mean
any  <code>start.state</code>, <code>end.state</code>, <code>action</code> or <code>observation</code>. Note that some POMDP solvers and the POMDP
file format use <code>'*'</code> for this purpose.
</p>
<p>The specification below map to the format used by pomdp-solve
(see <a href="http://www.pomdp.org">http://www.pomdp.org</a>).
</p>
<p><strong>Specification of transition probabilities: <code class="reqn">T(s' | s, a)</code></strong>
</p>
<p>Transition probability to transition to state <code class="reqn">s'</code> from given state <code class="reqn">s</code>
and action <code class="reqn">a</code>. The transition probabilities can be
specified in the following ways:
</p>

<ul>
<li><p> A data.frame with columns exactly like the arguments of <code>T_()</code>.
You can use <code>rbind()</code> with helper function <code>T_()</code> to create this data
frame.
</p>
</li>
<li><p> A named list of matrices, one for each action. Each matrix is square with
rows representing start states <code class="reqn">s</code> and columns representing end states <code class="reqn">s'</code>.
Instead of a matrix, also the strings <code>'identity'</code> or <code>'uniform'</code> can be specified.
</p>
</li>
<li><p> A function with the same arguments are <code>T_()</code>, but no default values
that returns the transition probability.
</p>
</li></ul>

<p><strong>Specification of observation probabilities: <code class="reqn">O(o | a, s')</code></strong>
</p>
<p>The POMDP specifies the probability for each observation <code class="reqn">o</code> given an
action <code class="reqn">a</code> and that the system transitioned to the end state
<code class="reqn">s'</code>. These probabilities can be specified in the
following ways:
</p>

<ul>
<li><p> A data frame with columns named exactly like the arguments of <code>O_()</code>.
You can use <code>rbind()</code>
with helper function <code>O_()</code> to create this data frame.
</p>
</li>
<li><p> A named list of matrices, one for each action. Each matrix has
rows representing end states <code class="reqn">s'</code> and columns representing an observation <code class="reqn">o</code>.
Instead of a matrix, also the strings <code>'identity'</code> or <code>'uniform'</code> can be specified.
</p>
</li>
<li><p> A function with the same arguments are <code>O_()</code>, but no default values
that returns the observation probability.
</p>
</li></ul>

<p><strong>Specification of the reward function: <code class="reqn">R(s, s', o, a)</code></strong>
</p>
<p>The reward function can be specified in the following
ways:
</p>

<ul>
<li><p> A data frame with columns named exactly like the arguments of <code>R_()</code>.
You can use <code>rbind()</code>
with helper function <code>R_()</code> to create this data frame.
</p>
</li>
<li><p> A list of lists. The list levels are <code>'action'</code> and <code>'start.state'</code>. The list elements
are matrices with
rows representing end states <code class="reqn">s'</code> and columns representing an observation <code class="reqn">o</code>.
</p>
</li>
<li><p> A function with the same arguments are <code>R_()</code>, but no default values
that returns the reward.
</p>
</li></ul>

<p><strong>Start Belief</strong>
</p>
<p>The initial belief state of the agent is a distribution over the states. It is used to calculate the
total expected cumulative reward printed with the solved model. The function <code><a href="#topic+reward">reward()</a></code> can be
used to calculate rewards for any belief.
</p>
<p>Some methods use this belief to decide which belief states to explore (e.g.,
the finite grid method).
</p>
<p>Options to specify the start belief state are:
</p>

<ul>
<li><p> A probability distribution over the states. That is, a vector
of <code class="reqn">|S|</code> probabilities, that add up to <code class="reqn">1</code>.
</p>
</li>
<li><p> The string <code>"uniform"</code> for a uniform
distribution over all states.
</p>
</li>
<li><p> An integer in the range <code class="reqn">1</code> to <code class="reqn">n</code> to specify the index of a single starting state.
</p>
</li>
<li><p> A string specifying the name of a single starting state.
</p>
</li></ul>

<p>The default initial belief is a uniform
distribution over all states.
</p>
<p><strong>Convergence</strong>
</p>
<p>A infinite-horizon POMDP needs to converge to provide a valid value
function and policy.
</p>
<p>A finite-horizon POMDP may also converging to a infinite horizon solution
if the horizon is long enough.
</p>
<p><strong>Time-dependent POMDPs</strong>
</p>
<p>Time dependence of transition probabilities, observation probabilities and
reward structure can be modeled by considering a set of <strong>episodes</strong>
representing <strong>epoch</strong> with the same settings. The length of each episode is
specified as a vector for <code>horizon</code>, where the length is the number of
episodes and each value is the length of the episode in epochs. Transition
probabilities, observation probabilities and/or reward structure can contain
a list with the values for each episode. The helper function <code>epoch_to_episode()</code> converts
an epoch to the episode it belongs to.
</p>


<h3>Value</h3>

<p>The function returns an object of class POMDP which is list of the model specification.
<code><a href="#topic+solve_POMDP">solve_POMDP()</a></code> reads the object and adds a list element named
<code>'solution'</code>.
</p>


<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>pomdp-solve website: <a href="http://www.pomdp.org">http://www.pomdp.org</a>
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Defining the Tiger Problem (it is also available via data(Tiger), see ? Tiger)

Tiger &lt;- POMDP(
  name = "Tiger Problem",
  discount = 0.75,
  states = c("tiger-left" , "tiger-right"),
  actions = c("listen", "open-left", "open-right"),
  observations = c("tiger-left", "tiger-right"),
  start = "uniform",

  transition_prob = list(
    "listen" =     "identity",
    "open-left" =  "uniform",
    "open-right" = "uniform"
  ),

  observation_prob = list(
    "listen" = rbind(c(0.85, 0.15),
                     c(0.15, 0.85)),
    "open-left" =  "uniform",
    "open-right" = "uniform"
  ),

  # the reward helper expects: action, start.state, end.state, observation, value
  # missing arguments default to NA which matches any value (often denoted as * in POMDPs).
  reward = rbind(
    R_("listen",                    v =   -1),
    R_("open-left",  "tiger-left",  v = -100),
    R_("open-left",  "tiger-right", v =   10),
    R_("open-right", "tiger-left",  v =   10),
    R_("open-right", "tiger-right", v = -100)
  )
)

Tiger

### Defining the Tiger problem using functions

trans_f &lt;- function(action, start.state, end.state) {
  if(action == 'listen')
    if(end.state == start.state) return(1)
    else return(0)

  return(1/2) ### all other actions have a uniform distribution
}

obs_f &lt;- function(action, end.state, observation) {
  if(action == 'listen')
    if(end.state == observation) return(0.85)
  else return(0.15)

  return(1/2)
}

rew_f &lt;- function(action, start.state, end.state, observation) {
  if(action == 'listen') return(-1)
  if(action == 'open-left' &amp;&amp; start.state == 'tiger-left') return(-100)
  if(action == 'open-left' &amp;&amp; start.state == 'tiger-right') return(10)
  if(action == 'open-right' &amp;&amp; start.state == 'tiger-left') return(10)
  if(action == 'open-right' &amp;&amp; start.state == 'tiger-right') return(-100)
  stop('Not possible')
}

Tiger_func &lt;- POMDP(
  name = "Tiger Problem",
  discount = 0.75,
  states = c("tiger-left" , "tiger-right"),
  actions = c("listen", "open-left", "open-right"),
  observations = c("tiger-left", "tiger-right"),
  start = "uniform",
  transition_prob = trans_f,
  observation_prob = obs_f,
  reward = rew_f
)

Tiger_func

# Defining a Time-dependent version of the Tiger Problem called Scared Tiger

# The tiger reacts normally for 3 epochs (goes randomly two one
# of the two doors when a door was opened). After 3 epochs he gets
# scared and when a door is opened then he always goes to the other door.

# specify the horizon for each of the two different episodes
Tiger_time_dependent &lt;- Tiger
Tiger_time_dependent$name &lt;- "Scared Tiger Problem"
Tiger_time_dependent$horizon &lt;- c(normal_tiger = 3, scared_tiger = 3)
Tiger_time_dependent$transition_prob &lt;- list(
  normal_tiger = list(
    "listen" = "identity",
    "open-left" = "uniform",
    "open-right" = "uniform"),
  scared_tiger = list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )
)
</code></pre>

<hr>
<h2 id='POMDP_accessors'>Access to Parts of the POMDP Description</h2><span id='topic+POMDP_accessors'></span><span id='topic+transition_matrix'></span><span id='topic+transition_val'></span><span id='topic+observation_matrix'></span><span id='topic+observation_val'></span><span id='topic+reward_matrix'></span><span id='topic+reward_val'></span><span id='topic+start_vector'></span><span id='topic+normalize_POMDP'></span><span id='topic+normalize_MDP'></span>

<h3>Description</h3>

<p>Functions to provide uniform access to different parts of the POMDP description.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transition_matrix(
  x,
  action = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = TRUE,
  drop = TRUE
)

transition_val(x, action, start.state, end.state, episode = NULL, epoch = NULL)

observation_matrix(
  x,
  action = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = TRUE,
  drop = TRUE
)

observation_val(
  x,
  action,
  end.state,
  observation,
  episode = NULL,
  epoch = NULL
)

reward_matrix(
  x,
  action = NULL,
  start.state = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE,
  drop = TRUE
)

reward_val(
  x,
  action,
  start.state,
  end.state = NA,
  observation = NA,
  episode = NULL,
  epoch = NULL
)

start_vector(x)

normalize_POMDP(x, sparse = TRUE)

normalize_MDP(x, sparse = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="POMDP_accessors_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a> object.</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_action">action</code></td>
<td>
<p>name or index of an action.</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_episode">episode</code>, <code id="POMDP_accessors_+3A_epoch">epoch</code></td>
<td>
<p>Episode or epoch used for time-dependent POMDPs. Epochs are internally converted
to the episode using the model horizon.</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_sparse">sparse</code></td>
<td>
<p>logical; use sparse matrices when the density is below 50% and keeps data.frame representation
for the reward field. <code>NULL</code> returns the
representation stored in the problem description which saves the time for conversion.</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_drop">drop</code></td>
<td>
<p>logical; drop the action list if a single action is requested?</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_start.state">start.state</code>, <code id="POMDP_accessors_+3A_end.state">end.state</code></td>
<td>
<p>name or index of the state.</p>
</td></tr>
<tr><td><code id="POMDP_accessors_+3A_observation">observation</code></td>
<td>
<p>name or index of observation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several parts of the POMDP description can be defined in different ways. In particular,
the fields <code>transition_prob</code>, <code>observation_prob</code>, <code>reward</code>, and <code>start</code> can be defined using matrices, data frames or
keywords. See <a href="#topic+POMDP">POMDP</a> for details. The functions provided here, provide unified access to the data in these fields
to make writing code easier.
</p>


<h4>Transition Probabilities <code class="reqn">T(s'|s,a)</code></h4>

<p><code>transition_matrix()</code> returns a list with one element for each action. Each element contains a states x states matrix
with <code class="reqn">s</code> (<code>start.state</code>) as rows and <code class="reqn">s'</code> (<code>end.state</code>) as columns.
Matrices with a density below 50% can be requested in sparse format (as a <a href="Matrix.html#topic+dgCMatrix-class">Matrix::dgCMatrix</a>)
</p>
<p><code>transition_val()</code> retrieves a single entry more efficiently.
</p>



<h4>Observation Probabilities <code class="reqn">O(o|s',a)</code></h4>

<p><code>observation_matrix()</code> returns a list with one element for each action. Each element contains a states x states matrix
with <code class="reqn">s</code> (<code>start.state</code>) as rows and <code class="reqn">s'</code> (<code>end.state</code>) as columns.
Matrices with a density below 50% can be requested in sparse format (as a <a href="Matrix.html#topic+dgCMatrix-class">Matrix::dgCMatrix</a>)
</p>
<p><code>observation_val()</code> retrieves a single entry more efficiently.
</p>



<h4>Reward <code class="reqn">R(s,s',o,a)</code></h4>

<p><code>reward_matrix()</code> returns for the dense representation a list of lists. The list levels are <code class="reqn">a</code> (<code>action</code>)  and <code class="reqn">s</code> (<code>start.state</code>).
The list elements are matrices with rows representing the end state <code class="reqn">s'</code>  and columns representing observations <code class="reqn">o</code>.
Many reward structures cannot be efficiently stored using a standard sparse matrix since there might be a fixed cost for each action
resulting in no entries with 0. Therefore, the data.frame representation is used as a 'sparse' representation.
</p>
<p><code>observation_val()</code> retrieves a single entry more efficiently.
</p>



<h4>Initial Belief</h4>

<p><code>start_vector()</code> translates the initial probability vector description into a numeric vector.
</p>



<h4>Convert the Complete POMDP Description into a Consistent Form</h4>

<p><code>normalize_POMDP()</code> returns a new POMDP definition where <code>transition_prob</code>,
<code>observations_prob</code>, <code>reward</code>, and <code>start</code> are normalized to (lists of) matrices and vectors to
make direct access easy.  Also, <code>states</code>, <code>actions</code>, and <code>observations</code> are ordered as given in the problem
definition to make safe access using numerical indices possible. Normalized POMDP descriptions are used for
C++ based code (e.g., <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code>) and normalizing them once will save time if the code is
called repeatedly.
</p>



<h3>Value</h3>

<p>A list or a list of lists of matrices.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# List of |A| transition matrices. One per action in the from start.states x end.states
Tiger$transition_prob
transition_matrix(Tiger)
transition_val(Tiger, action = "listen", start.state = "tiger-left", end.state = "tiger-left")

# List of |A| observation matrices. One per action in the from states x observations
Tiger$observation_prob
observation_matrix(Tiger)
observation_val(Tiger, action = "listen", end.state = "tiger-left", observation = "tiger-left")

# List of list of reward matrices. 1st level is action and second level is the
#  start state in the form end state x observation
Tiger$reward
reward_matrix(Tiger)
reward_val(Tiger, action = "open-right", start.state = "tiger-left", end.state = "tiger-left",
  observation = "tiger-left")
  
# Note that the reward in the tiger problem only depends on the action and the start.state 
# so we can use:
reward_val(Tiger, action = "open-right", start.state = "tiger-left")

# Translate the initial belief vector
Tiger$start
start_vector(Tiger)

# Normalize the whole model
Tiger_norm &lt;- normalize_POMDP(Tiger)
Tiger_norm$transition_prob

## Visualize transition matrix for action 'open-left'
library("igraph")
g &lt;- graph_from_adjacency_matrix(transition_matrix(Tiger, action = "open-left"), weighted = TRUE)
edge_attr(g, "label") &lt;- edge_attr(g, "weight")

igraph.options("edge.curved" = TRUE)
plot(g, layout = layout_on_grid, main = "Transitions for action 'open=left'")

## Use a function for the Tiger transition model
trans &lt;- function(action, end.state, start.state) {
  ## listen has an identity matrix
  if (action == 'listen')
    if (end.state == start.state) return(1)
    else return(0)

  # other actions have a uniform distribution
  return(1/2)
}

Tiger$transition_prob &lt;- trans

# transition_matrix evaluates the function
transition_matrix(Tiger)
</code></pre>

<hr>
<h2 id='pomdp-package'>pomdp: Infrastructure for Partially Observable Markov Decision Processes (POMDP)</h2><span id='topic+pomdp-package'></span>

<h3>Description</h3>

<p>Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Process (POMDP) models. Interfaces for various exact and approximate solution algorithms are available including value iteration, Point-Based Value Iteration (PBVI) and Successive Approximations of the Reachable Space under Optimal Policies (SARSOP).
</p>


<h3>Key functions</h3>


<ul>
<li><p> Problem specification: <a href="#topic+POMDP">POMDP</a>, <a href="#topic+MDP">MDP</a>
</p>
</li>
<li><p> Solvers: <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>, <code><a href="#topic+solve_MDP">solve_MDP()</a></code>, <code><a href="#topic+solve_SARSOP">solve_SARSOP()</a></code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler
</p>

<hr>
<h2 id='projection'>Defining a Belief Space Projection</h2><span id='topic+projection'></span>

<h3>Description</h3>

<p>High dimensional belief spaces can be projected to lower dimension. This is useful for visualization and
to analyze the belief space and value functions. This definition is used by functions like <code><a href="#topic+plot_belief_space">plot_belief_space()</a></code>,
<code><a href="#topic+plot_value_function">plot_value_function()</a></code>, and <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projection(x = NULL, model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projection_+3A_x">x</code></td>
<td>
<p>specification of the projection (see Details section).</p>
</td></tr>
<tr><td><code id="projection_+3A_model">model</code></td>
<td>
<p>a <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The belief space is $n-1$ dimensional, were $n$ is the number of states. Note: it is n-1 dimensional since the probabilities
need to add up to 1. A projection fixes the belief value for a set of states. For example, for a 4-state POMDP
(s1, s2, s3, s4), we can project the belief space on s1 and s2 by holding s3 and s4 constant
which is represented by the vector <code>c(s1 = NA, s2 = NA, s3 = 0, s4 = .1)</code>. We use <code>NA</code> to represent that the values are not
fixed and the value that the other dimensions are held constant.
</p>
<p>We provide several ways to specify a projection:
</p>

<ul>
<li><p> A vector with values for all dimensions. <code>NA</code>s are used for the dimension projected on. This is the canonical form
used in this package. Example: <code>c(NA, NA, 0, .1)</code>
</p>
</li>
<li><p> A named vector with just the dimensions held constant. Example: <code>c(s3 = 0, s4 = .1)</code>
</p>
</li>
<li><p> A vector of state names to project on. All other dimensions are held constant at 0. Example: <code>c("s1", "s2")</code>
</p>
</li>
<li><p> A vector with indices of the states to project on. All other dimensions are held constant at 0. Example: <code>c(1, 2)</code>
</p>
</li></ul>



<h3>Value</h3>

<p>a canonical description of the projection.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- POMDP(
 states = 4,
 actions = 2,
 observations = 2,
 transition_prob = list("identity","identity"),
 observation_prob = list("uniform","uniform"),
 reward = rbind(R_(value = 1))
)

projection(NULL, model = model)
projection(1:2, model = model)
projection(c("s2", "s3"), model = model)
projection(c(1,4), model = model)
projection(c(s2 = .4, s3 = .2), model = model)
projection(c(s1 = .1, s2 = NA, s3 = NA, s4 = .3), model = model)
</code></pre>

<hr>
<h2 id='regret'>Calculate the Regret of a Policy</h2><span id='topic+regret'></span>

<h3>Description</h3>

<p>Calculates the regret of a policy relative to a benchmark policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regret(policy, benchmark, belief = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regret_+3A_policy">policy</code></td>
<td>
<p>a solved POMDP containing the policy to calculate the regret for.</p>
</td></tr>
<tr><td><code id="regret_+3A_benchmark">benchmark</code></td>
<td>
<p>a solved POMDP with the (optimal) policy. Regret is calculated relative to this
policy.</p>
</td></tr>
<tr><td><code id="regret_+3A_belief">belief</code></td>
<td>
<p>the used start belief. If NULL then the start belief of the <code>benchmark</code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the regret defined as <code class="reqn">J^{\pi^*}(b_0) - J^{\pi}(b_0)</code> with <code class="reqn">J^\pi</code> representing the expected long-term
reward given the policy <code class="reqn">\pi</code> and the start belief <code class="reqn">b_0</code>. Note that for regret usually the optimal policy <code class="reqn">\pi^*</code> is used as the benchmark.
Since the optimal policy may not be known, regret relative to the best known policy can be used.
</p>


<h3>Value</h3>

<p>the regret as a difference of expected long-term rewards.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

sol_optimal &lt;- solve_POMDP(Tiger)
sol_optimal

# perform exact value iteration for 10 epochs
sol_quick &lt;- solve_POMDP(Tiger, method = "enum", horizon = 10)
sol_quick

regret(sol_quick, sol_optimal)
</code></pre>

<hr>
<h2 id='reward'>Calculate the Reward for a POMDP Solution</h2><span id='topic+reward'></span><span id='topic+reward_node_action'></span>

<h3>Description</h3>

<p>This function calculates the expected total reward for a POMDP solution
given a starting belief state. The value is calculated using the value function stored
in the POMDP solution. In addition, the policy graph node that represents the belief state
and the optimal action can also be returned using <code>reward_node_action()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reward(x, belief = NULL, epoch = 1, ...)

reward_node_action(x, belief = NULL, epoch = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reward_+3A_x">x</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a> object.</p>
</td></tr>
<tr><td><code id="reward_+3A_belief">belief</code></td>
<td>
<p>specification of the current belief state (see argument start
in <a href="#topic+POMDP">POMDP</a> for details). By default the belief state defined in
the model as start is used. Multiple belief states can be specified as rows in a matrix.</p>
</td></tr>
<tr><td><code id="reward_+3A_epoch">epoch</code></td>
<td>
<p>return reward for this epoch. Use 1 for converged policies.</p>
</td></tr>
<tr><td><code id="reward_+3A_...">...</code></td>
<td>
<p>further arguments are passed on.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The reward is typically calculated using the value function (alpha vectors)
of the solution. If these are not available, then <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code> is
used instead with a warning.
</p>


<h3>Value</h3>

<p><code>reward()</code> returns a vector of reward values, one for each belief if a matrix is specified.
</p>
<p><code>reward_node_action()</code> returns a list with the components
</p>
<table>
<tr><td><code>belief_state</code></td>
<td>
<p>the belief state specified in <code>belief</code>.</p>
</td></tr>
<tr><td><code>reward</code></td>
<td>
<p>the total expected reward given a belief and epoch. </p>
</td></tr>
<tr><td><code>pg_node</code></td>
<td>
<p>the policy node that represents the belief state.</p>
</td></tr>
<tr><td><code>action</code></td>
<td>
<p>the optimal action.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
sol &lt;- solve_POMDP(model = Tiger)

# if no start is specified, a uniform belief is used.
reward(sol)

# we have additional information that makes us believe that the tiger
# is more likely to the left.
reward(sol, belief = c(0.85, 0.15))

# we start with strong evidence that the tiger is to the left.
reward(sol, belief = "tiger-left")

# Note that in this case, the total discounted expected reward is greater
# than 10 since the tiger problem resets and another game staring with
# a uniform belief is played which produces additional reward.

# return reward, the initial node in the policy graph and the optimal action for
# two beliefs.
reward_node_action(sol, belief = rbind(c(.5, .5), c(.9, .1)))

# manually combining reward with belief space sampling to show the value function
# (color signifies the optimal action)
samp &lt;- sample_belief_space(sol, n = 200)
rew &lt;- reward_node_action(sol, belief = samp)
plot(rew$belief[,"tiger-right"], rew$reward, col = rew$action, ylim = c(0, 15))
legend(x = "top", legend = levels(rew$action), title = "action", col = 1:3, pch = 1)

# this is the piecewise linear value function from the solution
plot_value_function(sol, ylim = c(0, 10))
</code></pre>

<hr>
<h2 id='round_stochastic'>Round a stochastic vector or a row-stochastic matrix</h2><span id='topic+round_stochastic'></span>

<h3>Description</h3>

<p>Rounds a vector such that the sum of 1 is preserved. Rounds a matrix such
that the rows still sum up to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>round_stochastic(x, digits = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="round_stochastic_+3A_x">x</code></td>
<td>
<p>a stochastic vector or a row-stochastic matrix.</p>
</td></tr>
<tr><td><code id="round_stochastic_+3A_digits">digits</code></td>
<td>
<p>number of digits for rounding.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rounds and adjusts one entry such that the rounding error is the smallest.
</p>


<h3>Value</h3>

<p>The rounded vector or matrix.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+round">round</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># regular rounding would not sum up to 1 
x &lt;- c(0.333, 0.334, 0.333)

round_stochastic(x)
round_stochastic(x, digits = 2)
round_stochastic(x, digits = 1)
round_stochastic(x, digits = 0)


# round a stochastic matrix
m &lt;- matrix(runif(15), ncol = 3)
m &lt;- sweep(m, 1, rowSums(m), "/")

m
round_stochastic(m, digits = 2)
round_stochastic(m, digits = 1)
round_stochastic(m, digits = 0)
</code></pre>

<hr>
<h2 id='sample_belief_space'>Sample from the Belief Space</h2><span id='topic+sample_belief_space'></span>

<h3>Description</h3>

<p>Sample points from belief space using a several sampling strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_belief_space(model, projection = NULL, n = 1000, method = "random", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_belief_space_+3A_model">model</code></td>
<td>
<p>a unsolved or solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_n">n</code></td>
<td>
<p>size of the sample. For trajectories, it is the number of trajectories.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_method">method</code></td>
<td>
<p>character string specifying the sampling strategy. Available
are <code>"random"</code>, <code>"regular"</code>, and <code>"trajectories"</code>.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_...">...</code></td>
<td>
<p>for the trajectory method, further arguments are passed on to <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code>. Further arguments are ignored for the other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of sampling from the belief space is to provide good coverage or to sample belief points
that are more likely to be encountered (see trajectory method).
The following sampling methods are available:
</p>

<ul>
<li> <p><code>'random'</code> samples uniformly sample from the projected belief space using
the method described by Luc Devroye (1986). Sampling is be done in parallel
after a foreach backend is registered.
</p>
</li>
<li> <p><code>'regular'</code> samples points using a
regularly spaced grid. This method is only available for projections on 2 or
3 states.
</p>
</li>
<li> <p><code>"trajectories"</code> returns the belief states encountered in <code>n</code> trajectories of length <code>horizon</code> starting at the
model's initial belief. Thus it returns <code>n</code> x <code>horizon</code> belief states and will contain duplicates.
Projection is not supported for trajectories. Additional
arguments can include the simulation <code>horizon</code> and the start <code>belief</code> which are passed on to <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code>.
</p>
</li></ul>



<h3>Value</h3>

<p>Returns a matrix. Each row is a sample from the belief space.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Luc Devroye, Non-Uniform Random Variate Generation, Springer
Verlag, 1986.
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# random sampling can be done in parallel after registering a backend.
# doparallel::registerDoParallel()

sample_belief_space(Tiger, n = 5)
sample_belief_space(Tiger, n = 5, method = "regular")
sample_belief_space(Tiger, n = 1, horizon = 5, method = "trajectories")

# sample, determine the optimal action and calculate the expected reward for a solved POMDP
# Note: check.names = FALSE is used to preserve the `-` for the state names in the dataframe.
sol &lt;- solve_POMDP(Tiger)
samp &lt;- sample_belief_space(sol, n = 5, method = "regular")
data.frame(samp, action = optimal_action(sol,  belief = samp), 
  reward = reward(sol, belief = samp), check.names = FALSE)
  
# sample from a 3 state problem
data(Three_doors)
Three_doors

sample_belief_space(Three_doors, n = 5)
sample_belief_space(Three_doors, n = 5, projection = c(`tiger-left` = .1))

if ("Ternary" %in% installed.packages()) {
sample_belief_space(Three_doors, n = 9, method = "regular")
sample_belief_space(Three_doors, n = 9, method = "regular", projection = c(`tiger-left` = .1))
}

sample_belief_space(Three_doors, n = 1, horizon = 5, method = "trajectories")
</code></pre>

<hr>
<h2 id='simulate_MDP'>Simulate Trajectories in a MDP</h2><span id='topic+simulate_MDP'></span>

<h3>Description</h3>

<p>Simulate trajectories through a MDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from an epsilon-greedy policy and then update the state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_MDP(
  model,
  n = 100,
  start = NULL,
  horizon = NULL,
  return_states = FALSE,
  epsilon = NULL,
  delta_horizon = 0.001,
  engine = "cpp",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_MDP_+3A_model">model</code></td>
<td>
<p>a MDP model.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_n">n</code></td>
<td>
<p>number of trajectories.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_start">start</code></td>
<td>
<p>probability distribution over the states for choosing the
starting states for the trajectories. Defaults to &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_horizon">horizon</code></td>
<td>
<p>number of epochs for the simulation. If <code>NULL</code> then the
horizon for the model is used.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_return_states">return_states</code></td>
<td>
<p>logical; return visited states.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_epsilon">epsilon</code></td>
<td>
<p>the probability of random actions  for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_delta_horizon">delta_horizon</code></td>
<td>
<p>precision used to determine the horizon for infinite-horizon problems.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_engine">engine</code></td>
<td>
<p><code>'cpp'</code> or <code>'r'</code> to perform simulation using a faster C++
or a native R implementation.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_verbose">verbose</code></td>
<td>
<p>report used parameters.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_...">...</code></td>
<td>
<p>further arguments are ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A native R implementation is available (<code>engine = 'r'</code>) and the default is a
faster C++ implementation (<code>engine = 'cpp'</code>).
</p>
<p>Both implementations support parallel execution using the package
<span class="pkg">foreach</span>. To enable parallel execution, a parallel backend like
<span class="pkg">doparallel</span> needs to be available needs to be registered (see
<code><a href="doParallel.html#topic+registerDoParallel">doParallel::registerDoParallel()</a></code>).
Note that small simulations are slower using parallelization. Therefore, C++ simulations
with n * horizon less than 100,000 are always executed using a single worker.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>

<ul>
<li> <p><code>avg_reward</code>: The average discounted reward.
</p>
</li>
<li> <p><code>reward</code>: Reward for each trajectory.
</p>
</li>
<li> <p><code>action_cnt</code>: Action counts.
</p>
</li>
<li> <p><code>state_cnt</code>: State counts.
</p>
</li>
<li> <p><code>states</code>: a vector with state ids.
Rows represent trajectories.
</p>
</li></ul>

<p>A vector with state ids (in the final epoch or all). Attributes containing action
counts, and rewards  for each trajectory may be available.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Maze)

# solve the POMDP for 5 epochs and no discounting
sol &lt;- solve_MDP(Maze, discount = 1)
sol

# U in the policy is and estimate of the utility of being in a state when using the optimal policy.
policy(sol)
matrix(policy(sol)[[1]]$action, nrow = 3, dimnames = list(1:3, 1:4))[3:1, ]

## Example 1: simulate 10 trajectories following the policy, only the final belief state is returned
sim &lt;- simulate_MDP(sol, n = 100, horizon = 10, verbose = TRUE)
sim

# Note that all simulations start at s_1 and that the simulated avg. reward 
# is therefore an estimate to the U value for the start state s_1.
policy(sol)[[1]][1,] 

# Calculate proportion of actions taken in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)

## Example 2: simulate starting following a uniform distribution over all
#             states and return all visited states
sim &lt;- simulate_MDP(sol, n = 100, start = "uniform", horizon = 10, return_states = TRUE)
sim$avg_reward

# how often was each state visited?
table(sim$states)
matrix(table(sim$states),nrow = 3, dimnames = list(1:3, 1:4))[3:1, ]
</code></pre>

<hr>
<h2 id='simulate_POMDP'>Simulate Trajectories in a POMDP</h2><span id='topic+simulate_POMDP'></span>

<h3>Description</h3>

<p>Simulate trajectories through a POMDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from the the epsilon-greedy policy and then updated using observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_POMDP(
  model,
  n = 1000,
  belief = NULL,
  horizon = NULL,
  return_beliefs = FALSE,
  epsilon = NULL,
  delta_horizon = 0.001,
  digits = 7L,
  engine = "cpp",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_POMDP_+3A_model">model</code></td>
<td>
<p>a POMDP model.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_n">n</code></td>
<td>
<p>number of trajectories.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_belief">belief</code></td>
<td>
<p>probability distribution over the states for choosing the
starting states for the trajectories.
Defaults to the start belief state specified in the model or &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_horizon">horizon</code></td>
<td>
<p>number of epochs for the simulation. If <code>NULL</code> then the
horizon for finite-horizon model is used. For infinite-horizon problems, a horizon is
calculated using the discount factor.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_return_beliefs">return_beliefs</code></td>
<td>
<p>logical; Return all visited belief states? This requires n x horizon memory.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_epsilon">epsilon</code></td>
<td>
<p>the probability of random actions for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_delta_horizon">delta_horizon</code></td>
<td>
<p>precision used to determine the horizon for infinite-horizon problems.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_digits">digits</code></td>
<td>
<p>round probabilities for belief points.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_engine">engine</code></td>
<td>
<p><code>'cpp'</code>, <code>'r'</code> to perform simulation using a faster C++ or a
native R implementation.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_verbose">verbose</code></td>
<td>
<p>report used parameters.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_...">...</code></td>
<td>
<p>further arguments are ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulates <code>n</code> trajectories.
If no simulation horizon is specified the horizon of finite-horizon problems
is used. For infinite-horizon problems with <code class="reqn">\gamma &lt; 1</code>, the simulation
horizon <code class="reqn">T</code> is chosen such that </p>
<p style="text-align: center;"><code class="reqn">abs(\gamma^T R_\text{max}) \le \delta_\text{horizon}.</code>
</p>

<p>A native R implementation (<code>engine = 'r'</code>) and a faster C++ implementation
(<code>engine = 'cpp'</code>) are available. Currently, only the R implementation supports
multi-episode problems.
</p>
<p>Both implementations support the simulation of trajectories in parallel using the package
<span class="pkg">foreach</span>. To enable parallel execution, a parallel backend like
<span class="pkg">doparallel</span> needs to be registered (see
<code><a href="doParallel.html#topic+registerDoParallel">doParallel::registerDoParallel()</a></code>).
Note that small simulations are slower using parallelization. C++ simulations
with <code>n * horizon</code> less than 100,000 are always executed using a single worker.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>

<ul>
<li> <p><code>avg_reward</code>: The average discounted reward.
</p>
</li>
<li> <p><code>belief_states</code>: A matrix with belief states as rows.
</p>
</li>
<li> <p><code>action_cnt</code>: Action counts.
</p>
</li>
<li> <p><code>state_cnt</code>: State counts.
</p>
</li>
<li> <p><code>reward</code>: Reward for each trajectory.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

# solve the POMDP for 5 epochs and no discounting
sol &lt;- solve_POMDP(Tiger, horizon = 5, discount = 1, method = "enum")
sol
policy(sol)

# uncomment the following line to register a parallel backend for simulation 
# (needs package doparallel installed)

# doParallel::registerDoParallel()
# foreach::getDoParWorkers()

## Example 1: simulate 10 trajectories
sim &lt;- simulate_POMDP(sol, n = 100, verbose = TRUE)
sim

# calculate the percentage that each action is used in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)

## Example 2: look at all belief states in the trajectory starting with an initial start belief.
sim &lt;- simulate_POMDP(sol, n = 100, belief = c(.5, .5), return_beliefs = TRUE)
head(sim$belief_states)

# plot with added density (the x-axis is the probability of the second belief state)
plot_belief_space(sol, sample = sim$belief_states, jitter = 2, ylim = c(0, 6))
lines(density(sim$belief_states[, 2], bw = .02)); axis(2); title(ylab = "Density")


## Example 3: simulate trajectories for an unsolved POMDP which uses an epsilon of 1
#             (i.e., all actions are randomized). The simulation horizon for the 
#             infinite-horizon Tiger problem is calculated. 
sim &lt;- simulate_POMDP(Tiger, n = 100, return_beliefs = TRUE, verbose = TRUE)
sim$avg_reward

plot_belief_space(sol, sample = sim$belief_states, jitter = 2, ylim = c(0, 6))
lines(density(sim$belief_states[, 1], bw = .05)); axis(2); title(ylab = "Density")
</code></pre>

<hr>
<h2 id='solve_MDP'>Solve an MDP Problem</h2><span id='topic+solve_MDP'></span><span id='topic+q_values_MDP'></span><span id='topic+random_MDP_policy'></span><span id='topic+approx_MDP_policy_evaluation'></span>

<h3>Description</h3>

<p>A simple implementation of value iteration and modified policy iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_MDP(
  model,
  horizon = NULL,
  discount = NULL,
  terminal_values = NULL,
  method = "value",
  eps = 0.01,
  max_iterations = 1000,
  k_backups = 10,
  verbose = FALSE
)

q_values_MDP(model, U = NULL)

random_MDP_policy(model, prob = NULL)

approx_MDP_policy_evaluation(pi, model, U = NULL, k_backups = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_MDP_+3A_model">model</code></td>
<td>
<p>a POMDP problem specification created with <code><a href="#topic+POMDP">POMDP()</a></code>.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_horizon">horizon</code></td>
<td>
<p>an integer with the number of epochs for problems with a
finite planning horizon. If set to <code>Inf</code>, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
<code>NULL</code>, then the horizon specified in <code>model</code> will be used.  For
time-dependent POMDPs a vector of horizons can be specified (see Details
section).</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">[0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_terminal_values">terminal_values</code></td>
<td>
<p>a vector with terminal utilities for each state. If
<code>NULL</code>, then a vector of all 0s is used.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_method">method</code></td>
<td>
<p>string; one of the following solution methods: <code>'value'</code>,
<code>'policy'</code>.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_eps">eps</code></td>
<td>
<p>maximum error allowed in the utility of any state
(i.e., the maximum policy loss).</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_max_iterations">max_iterations</code></td>
<td>
<p>maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_k_backups">k_backups</code></td>
<td>
<p>number of look ahead steps used for approximate policy evaluation
used by method <code>'policy'</code>.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the pomdp solver in the R console.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_u">U</code></td>
<td>
<p>a vector with state utilities (expected sum of discounted rewards from that point on).</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_prob">prob</code></td>
<td>
<p>probability vector for actions.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_pi">pi</code></td>
<td>
<p>a policy as a data.frame with columns state and action.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>solve_MDP()</code> returns an object of class POMDP which is a list with the
model specifications (<code>model</code>), the solution (<code>solution</code>).
The solution is a list with the elements:
</p>

<ul>
<li> <p><code>policy</code> a list representing the policy graph. The list only has one element for converged solutions.
</p>
</li>
<li> <p><code>converged</code> did the algorithm converge (<code>NA</code>) for finite-horizon problems.
</p>
</li>
<li> <p><code>delta</code> final delta (infinite-horizon only)
</p>
</li>
<li> <p><code>iterations</code> number of iterations to convergence (infinite-horizon only)
</p>
</li></ul>

<p><code>q_values_MDP()</code> returns a state by action matrix specifying the Q-function,
i.e., the utility value of executing each action in each state.
</p>
<p><code>random_MDP_policy()</code> returns a data.frame with columns state and action to define a policy.
</p>
<p><code>approx_MDP_policy_evaluation()</code> is used by the modified policy
iteration algorithm and returns an approximate utility vector U estimated by evaluating policy <code>pi</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other solver: 
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Maze)
Maze

# use value iteration
maze_solved &lt;- solve_MDP(Maze, method = "value")
policy(maze_solved)

# value function (utility function U)
plot_value_function(maze_solved)

# Q-function (states times action)
q_values_MDP(maze_solved)

# use modified policy iteration
maze_solved &lt;- solve_MDP(Maze, method = "policy")
policy(maze_solved)

# finite horizon
maze_solved &lt;- solve_MDP(Maze, method = "value", horizon = 3)
policy(maze_solved)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted &lt;- Maze
Maze_discounted$discount &lt;- .9
pi &lt;- random_MDP_policy(Maze_discounted, prob = c(n = .7, e = .1, s = .1, w = 0.1))
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved &lt;- solve_MDP(Maze)

approx_MDP_policy_evaluation(pi, Maze, k_backup = 100)
approx_MDP_policy_evaluation(policy(maze_solved)[[1]], Maze, k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)
</code></pre>

<hr>
<h2 id='solve_POMDP'>Solve a POMDP Problem using pomdp-solver</h2><span id='topic+solve_POMDP'></span><span id='topic+solve_POMDP_parameter'></span>

<h3>Description</h3>

<p>This function utilizes the C implementation of 'pomdp-solve' by Cassandra
(2015) to solve problems that are formulated as partially observable Markov
decision processes (POMDPs). The result is an optimal or approximately
optimal policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_POMDP(
  model,
  horizon = NULL,
  discount = NULL,
  initial_belief = NULL,
  terminal_values = NULL,
  method = "grid",
  digits = 7,
  parameter = NULL,
  timeout = Inf,
  verbose = FALSE
)

solve_POMDP_parameter()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_POMDP_+3A_model">model</code></td>
<td>
<p>a POMDP problem specification created with <code><a href="#topic+POMDP">POMDP()</a></code>.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_horizon">horizon</code></td>
<td>
<p>an integer with the number of epochs for problems with a
finite planning horizon. If set to <code>Inf</code>, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
<code>NULL</code>, then the horizon specified in <code>model</code> will be used.  For
time-dependent POMDPs a vector of horizons can be specified (see Details
section).</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">[0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_initial_belief">initial_belief</code></td>
<td>
<p>An initial belief vector. If <code>NULL</code>, then the
initial belief specified in <code>model</code> (as start) will be used.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_terminal_values">terminal_values</code></td>
<td>
<p>a vector with the terminal utility values for each state or a
matrix specifying the terminal rewards via a terminal value function (e.g.,
the alpha components produced by <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>).  If <code>NULL</code>, then, if available,
the terminal values specified in <code>model</code> will be used or a vector with all 0s otherwise.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_method">method</code></td>
<td>
<p>string; one of the following solution methods: <code>"grid"</code>,
<code>"enum"</code>, <code>"twopass"</code>, <code>"witness"</code>, or <code>"incprune"</code>.
The default is <code>"grid"</code> implementing the finite grid method.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_digits">digits</code></td>
<td>
<p>precision used when writing POMDP files (see
<code><a href="#topic+write_POMDP">write_POMDP()</a></code>).</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_parameter">parameter</code></td>
<td>
<p>a list with parameters passed on to the pomdp-solve
program.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_timeout">timeout</code></td>
<td>
<p>number of seconds for the solver to run.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the pomdp solver in the R console.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Parameters</h4>

<p><code>solve_POMDP_parameter()</code> displays available solver parameter options.
</p>
<p><strong>Horizon:</strong> Infinite-horizon POMDPs (<code>horizon = Inf</code>) converge to a
single policy graph. Finite-horizon POMDPs result in a policy tree of a
depth equal to the smaller of the horizon or the number of epochs to
convergence.  The policy (and the associated value function) are stored in a
list by epoch. The policy for the first epoch is stored as the first
element. Horizon can also be used to limit the number of epochs used
for value iteration.
</p>
<p><strong>Precision:</strong> The POMDP solver uses various epsilon values to control
precision for comparing alpha vectors to check for convergence, and solving
LPs. Overall precision can be changed using
<code>parameter = list(epsilon = 1e-3)</code>.
</p>
<p><strong>Methods:</strong> Several algorithms using exact value iteration are
available:
</p>

<ul>
<li><p> Enumeration (Sondik 1971).
</p>
</li>
<li><p> Two pass (Sondik 1971).
</p>
</li>
<li><p> Witness (Littman, Cassandra, Kaelbling, 1996).
</p>
</li>
<li><p> Incremental pruning (Zhang and Liu, 1996, Cassandra et al 1997).
</p>
</li></ul>

<p>In addition, the following approximate value iteration method is available:
</p>

<ul>
<li><p> Grid implements a variation of point-based value iteration
to solve larger POMDPs (PBVI; see Pineau 2003) without dynamic belief set expansion.
</p>
</li></ul>

<p>Details can be found in (Cassandra, 2015).
</p>
<p><strong>Note on POMDP problem size:</strong> Finding optimal policies for POMDPs is known to be
a prohibitively difficult problem because the belief space grows exponentially
with the number of states. Therefore, exact algorithms can be only used for
extremely small problems with only a few states. Typically, the researcher
needs to simplify the problem description (fewer states, actions and observations)
and choose an approximate algorithm with an acceptable level of
approximation to make the problem tractable.
</p>
<p><strong>Note on method grid:</strong> The grid method implements a version of Point
Based Value Iteration (PBVI). The used belief points are by default created
using points that are reachable from the initial belief (<code>start</code>) by
following all combinations of actions and observations. The size of the grid
can be set via <code>parameter = list(fg_points = 100)</code>. Alternatively,
different strategies can be chosen using the parameter <code>fg_type</code>. In
this implementation, the user can also specify manually a grid of belief
states by providing a matrix with belief states as produced by
<code><a href="#topic+sample_belief_space">sample_belief_space()</a></code> as the parameter <code>grid</code>.
</p>
<p>To guarantee convergence in point-based (finite grid) value iteration, the
initial value function must be a lower bound on the optimal value function.
If all rewards are strictly non-negative, an initial value function with an
all zero vector can be used and results will be similar to other methods.
However, if there are negative rewards, lower bounds can be guaranteed by
setting a single vector with the values <code class="reqn">min(reward)/(1 - discount)</code>.
The value function is guaranteed to converge to the true value function, but
finite-horizon value functions will not be as expected. <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>
produces a warning in this case.
</p>
<p><strong>Time-dependent POMDPs:</strong> Time dependence of transition probabilities,
observation probabilities and reward structure can be modeled by considering
a set of episodes representing epochs with the same settings. In the scared
tiger example (see Examples section), the tiger has the normal behavior for
the first three epochs (episode 1) and then becomes scared with different
transition probabilities for the next three epochs (episode 2). The episodes
can be solved in reverse order where the value function is used as the
terminal values of the preceding episode. This can be done by specifying a
vector of horizons (one horizon for each episode) and then lists with
transition matrices, observation matrices, and rewards. If the horizon
vector has names, then the lists also need to be named, otherwise they have
to be in the same order (the numeric index is used). Only the time-varying
matrices need to be specified. An example can be found in Example 4 in the
Examples section. The procedure can also be done by calling the solver
multiple times (see Example 5).
</p>



<h4>Solution</h4>

<p><strong>Policy:</strong>
Each policy is a data frame where each row representing a
policy graph node with an associated optimal action and a list of node IDs
to go to depending on the observation (specified as the column names). For
the finite-horizon case, the observation specific node IDs refer to nodes in
the next epoch creating a policy tree.  Impossible observations have a
<code>NA</code> as the next state.
</p>
<p><strong>Value function:</strong>
The value function specifies the value of the value function (the expected reward)
over the belief space. The dimensionality of the belief space is $n-1$ where $n$ is the number of states.
The value function is stored as a matrix. Each row is
associated with a node (row) in the policy graph and represents the
coefficients (alpha or V vector) of a hyperplane. It contains one
value per state which is the value for the belief state that has a probability
of 1 for that state and 0s for all others.
</p>



<h4>Temporary Files</h4>

<p>All temporary solver files are stored in the directory returned by <code>tempdir()</code>.
</p>



<h3>Value</h3>

<p>The solver returns an object of class POMDP which is a list with the
model specifications. Solved POMDPs also have an element called <code>solution</code> which is a list, and the
solver output (<code>solver_output</code>). The solution is a list that contains elements like:
</p>

<ul>
<li> <p><code>method</code> used solver method.
</p>
</li>
<li> <p><code>solver_output</code> output of the solver program.
</p>
</li>
<li> <p><code>converged</code> did the solution converge?
</p>
</li>
<li> <p><code>initial_belief</code> used initial belief used.
</p>
</li>
<li> <p><code>total_expected_reward</code> total expected reward starting from the the initial belief.
</p>
</li>
<li> <p><code>pg</code>, <code>initial_pg_node</code> the policy graph (see Details section).
</p>
</li>
<li> <p><code>alpha</code> value function as hyperplanes representing the nodes in the policy graph (see Details section).
</p>
</li>
<li> <p><code>belief_points_solver</code> optional; belief points used by the solver.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>Cassandra, A. (2015). pomdp-solve: POMDP Solver Software,
<a href="http://www.pomdp.org">http://www.pomdp.org</a>.
</p>
<p>Sondik, E. (1971). The Optimal Control of Partially Observable Markov
Processes. Ph.D. Dissertation, Stanford University.
</p>
<p>Cassandra, A., Littman M.L., Zhang L. (1997). Incremental Pruning: A Simple,
Fast, Exact Algorithm for Partially Observable Markov Decision Processes.
UAI'97: Proceedings of the Thirteenth conference on Uncertainty in
artificial intelligence, August 1997, pp. 54-61.
</p>
<p>Monahan, G. E. (1982). A survey of partially observable Markov decision
processes: Theory, models, and algorithms. Management Science 28(1):1-16.
</p>
<p>Littman, M. L.; Cassandra, A. R.; and Kaelbling, L. P. (1996). Efficient
dynamic-programming updates in partially observable Markov decision
processes. Technical Report CS-95-19, Brown University, Providence, RI.
</p>
<p>Zhang, N. L., and Liu, W. (1996). Planning in stochastic domains: Problem
characteristics and approximation. Technical Report HKUST-CS96-31,
Department of Computer Science, Hong Kong University of Science and
Technology.
</p>
<p>Pineau J., Geoffrey J Gordon G.J., Thrun S.B. (2003). Point-based value
iteration: an anytime algorithm for POMDPs. IJCAI'03: Proceedings of the
18th international joint conference on Artificial Intelligence. Pages
1025-1030.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other solver: 
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># display available solver options which can be passed on to pomdp-solve as parameters.
solve_POMDP_parameter()

################################################################
# Example 1: Solving the simple infinite-horizon Tiger problem
data("Tiger")
Tiger

# look at the model as a list
unclass(Tiger)

# inspect an individual field of the model (e.g., the transition probabilities and the reward)
Tiger$transition_prob
Tiger$reward

sol &lt;- solve_POMDP(model = Tiger)
sol

# look at the solution
sol$solution

# policy (value function (alpha vectors), optimal action and observation dependent transitions)
policy(sol)

# plot the policy graph of the infinite-horizon POMDP
plot_policy_graph(sol)

# value function
plot_value_function(sol, ylim = c(0,20))

################################################################
# Example 2: Solve a problem specified as a POMDP file
#            using a grid of size 20
sol &lt;- solve_POMDP("http://www.pomdp.org/examples/cheese.95.POMDP",
  method = "grid", parameter = list(fg_points = 20))
sol

policy(sol)
plot_policy_graph(sol)

# Example 3: Solving a finite-horizon POMDP using the incremental
#            pruning method (without discounting)
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1, method = "incprune")
sol

# look at the policy tree
policy(sol)
plot_policy_graph(sol)
# note: only open the door in epoch 3 if you get twice the same observation.

# Expected reward starting for the models initial belief (uniform):
#   listen twice and then open the door or listen 3 times
reward(sol)

# Expected reward for listen twice (-2) and then open-left (-1 + (-1) + 10 = 8)
reward(sol, belief = c(1,0))

# Expected reward for just opening the right door (10)
reward(sol, belief = c(1,0), epoch = 3)

# Expected reward for just opening the right door (0.5 * -100 + 0.95 * 10 = 4.5)
reward(sol, belief = c(.95,.05), epoch = 3)

################################################################
# Example 3: Using terminal values (state-dependent utilities after the final epoch)
#
# Specify 1000 if the tiger is right after 3 (horizon) epochs
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1,  method = "incprune",
  terminal_values = c(0, 1000))
sol

policy(sol)
# Note: The optimal strategy is to never open the left door. If we think the
#  Tiger is behind the right door, then we just wait for the final payout. If
#  we think the tiger might be behind the left door, then we open the right
#  door, are likely to get a small reward and the tiger has a chance of 50\% to
#  move behind the right door. The second episode is used to gather more
#  information for the more important #  final action.

################################################################
# Example 4: Model time-dependent transition probabilities

# The tiger reacts normally for 3 epochs (goes randomly two one
# of the two doors when a door was opened). After 3 epochs he gets
# scared and when a door is opened then he always goes to the other door.

# specify the horizon for each of the two different episodes
Tiger_time_dependent &lt;- Tiger
Tiger_time_dependent$name &lt;- "Scared Tiger Problem"
Tiger_time_dependent$horizon &lt;- c(normal_tiger = 3, scared_tiger = 3)
Tiger_time_dependent$transition_prob &lt;- list(
  normal_tiger = list(
    "listen" = "identity",
    "open-left" = "uniform",
    "open-right" = "uniform"),
  scared_tiger = list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )
)

# Tiger_time_dependent (a higher value for verbose will show more messages)

sol &lt;- solve_POMDP(model = Tiger_time_dependent, discount = 1,
  method = "incprune", verbose = 1)
sol

policy(sol)

# note that the default method to estimate the belief for nodes is following a
#  trajectory which uses only the first belief reached for each node. Random sampling
#  can find a better estimate of the central belief of the segment (see nodes 4-1 to 6-3
#  in the plots below).
plot_policy_graph(sol)
plot_policy_graph(sol, method = "random_sample")

################################################################
# Example 5: Alternative method to solve time-dependent POMDPs

# 1) create the scared tiger model
Tiger_scared &lt;- Tiger
Tiger_scared$transition_prob &lt;- list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )

# 2) Solve in reverse order. Scared tiger without terminal values first.
sol_scared &lt;- solve_POMDP(model = Tiger_scared,
  horizon = 3, discount = 1,  method = "incprune")
sol_scared
policy(sol_scared)

# 3) Solve the regular tiger with the value function of the scared tiger as terminal values
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1, method = "incprune",
  terminal_values = sol_scared$solution$alpha[[1]])
sol
policy(sol)
# Note: it is optimal to mostly listen till the Tiger gets in the scared mood. Only if
#  we are extremely sure in the first epoch, then opening a door is optimal.

################################################################
# Example 6: PBVI with a custom grid

# Create a search grid by sampling from the belief space in
#   10 regular intervals
custom_grid &lt;- sample_belief_space(Tiger, n = 10, method = "regular")
head(custom_grid)

# Visualize the search grid
plot_belief_space(sol, sample = custom_grid)

# Solve the POMDP using the grid for approximation
sol &lt;- solve_POMDP(Tiger, method = "grid", parameter = list(grid = custom_grid))
policy(sol)
plot_policy_graph(sol)

# note that plot_policy_graph() automatically remove nodes that are unreachable from the
#  initial node. This behavior can be switched off.
plot_policy_graph(sol, remove_unreachable_nodes = FALSE)
</code></pre>

<hr>
<h2 id='solve_SARSOP'>Solve a POMDP Problem using SARSOP</h2><span id='topic+solve_SARSOP'></span>

<h3>Description</h3>

<p>This function uses the C++ implementation of the SARSOP algorithm
by Kurniawati, Hsu and Lee (2008) interfaced in
package <span class="pkg">sarsop</span>
to solve infinite horizon problems that are formulated as partially observable Markov
decision processes (POMDPs). The result is an optimal or approximately
optimal policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_SARSOP(
  model,
  horizon = Inf,
  discount = NULL,
  terminal_values = NULL,
  method = "sarsop",
  digits = 7,
  parameter = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_SARSOP_+3A_model">model</code></td>
<td>
<p>a POMDP problem specification created with <code><a href="#topic+POMDP">POMDP()</a></code>.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_horizon">horizon</code></td>
<td>
<p>SARSOP only supports <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">[0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_terminal_values">terminal_values</code></td>
<td>
<p><code>NULL</code>. SARSOP does not use terminal values.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_method">method</code></td>
<td>
<p>string; there is only one method available called <code>"sarsop"</code>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_digits">digits</code></td>
<td>
<p>precision used when writing POMDP files (see
<code><a href="#topic+write_POMDP">write_POMDP()</a></code>).</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_parameter">parameter</code></td>
<td>
<p>a list with parameters passed on to
the function <code><a href="sarsop.html#topic+appl">sarsop::pomdpsol()</a></code> in package <span class="pkg">sarsop</span>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the solver in the R console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The solver returns an object of class POMDP which is a list with the
model specifications (<code>'model'</code>), the solution (<code>'solution'</code>), and the
solver output (<code>'solver_output'</code>).
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Carl Boettiger, Jeroen Ooms and Milad Memarzadeh (2020). sarsop:
Approximate POMDP Planning Software. R package version 0.6.6.
https://CRAN.R-project.org/package=sarsop
</p>
<p>H. Kurniawati, D. Hsu, and W.S. Lee (2008). SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Proc. Robotics: Science and Systems.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other solver: 
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Solving the simple infinite-horizon Tiger problem with SARSOP
# You need to install package "sarsop"
data("Tiger")
Tiger

sol &lt;- solve_SARSOP(model = Tiger)
sol

# look at solver output
sol$solver_output

# policy (value function (alpha vectors), optimal action and observation dependent transitions)
policy(sol)

# value function
plot_value_function(sol, ylim = c(0,20))

# plot the policy graph
plot_policy_graph(sol)

# reward of the optimal policy
reward(sol)

# Solve a problem specified as a POMDP file. The timeout is set to 10 seconds.
sol &lt;- solve_SARSOP("http://www.pomdp.org/examples/cheese.95.POMDP", parameter = list(timeout = 10))
sol

## End(Not run)

</code></pre>

<hr>
<h2 id='Tiger'>Tiger Problem POMDP Specification</h2><span id='topic+Tiger'></span><span id='topic+Three_doors'></span>

<h3>Description</h3>

<p>The model for the Tiger Problem introduces in Cassandra et al (1994).
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+POMDP">POMDP</a>.
</p>


<h3>Details</h3>

<p>The original Tiger problem was published in Cassandra et al (1994) as
follows:
</p>
<p>An agent is facing two closed doors and a tiger is put with equal
probability behind one of the two doors represented by the states
<code>tiger-left</code> and <code>tiger-right</code>, while treasure is put behind the other door.
The possible actions are <code>listen</code> for tiger noises or opening a door (actions
<code>open-left</code> and <code>open-right</code>). Listening is neither free (the action has a
reward of -1)  nor is it entirely accurate. There is a 15\
probability that the agent hears the tiger behind the left door while it is
actually behind the right door and vice versa. If the agent opens  door with
the tiger, it will get hurt (a negative reward of -100), but if it opens the
door with the treasure, it will receive a positive reward of 10. After a door
is opened, the problem is reset(i.e., the tiger is randomly assigned to a
door with chance 50/50) and the the agent gets another try.
</p>
<p>The three doors problem is an extension of the Tiger problem where the tiger
is behind one of three doors represented by three states (<code>tiger-left</code>,
<code>tiger-center</code>, and <code>tiger-right</code>) and treasure is behind the other two
doors. There are also three open actions and three different observations for
listening.
</p>


<h3>References</h3>

<p>Anthony R. Cassandra, Leslie P Kaelbling, and Michael L.
Littman (1994). Acting Optimally in Partially Observable Stochastic Domains.
In Proceedings of the Twelfth National Conference on Artificial
Intelligence, pp. 1023-1028.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
Tiger

data("Three_doors")
Three_doors
</code></pre>

<hr>
<h2 id='transition_graph'>Transition Graph</h2><span id='topic+transition_graph'></span>

<h3>Description</h3>

<p>Returns the transition model as an <span class="pkg">igraph</span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transition_graph(
  x,
  action = NULL,
  episode = NULL,
  epoch = NULL,
  state_col = NULL,
  simplify_transitions = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transition_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a>.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_action">action</code></td>
<td>
<p>the name or id of an action or a set of actions. Bey default the transition model for all actions is returned.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_episode">episode</code>, <code id="transition_graph_+3A_epoch">epoch</code></td>
<td>
<p>Episode or epoch used for time-dependent POMDPs. Epochs are internally converted to the episode using the model horizon.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the states.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_simplify_transitions">simplify_transitions</code></td>
<td>
<p>logical; combine parallel transition arcs into a single arc.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The transition model of a POMDP/MDP is a Markov Chain. This function extracts the transition model as
an igraph object.
</p>


<h3>Value</h3>

<p>returns the transition model as an igraph object.
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

g &lt;- transition_graph(Tiger)
g

library(igraph)
plot(g)

# plot with a fixed layout and curved edges
plot(g,
 layout = rbind(c(-1, 0), c(1, 0)), rescale = FALSE,
 edge.curved = curve_multiple_directed(g, .8),
 edge.loop.angle = -pi / 4,
 vertex.size = 60
 )

## Use visNetwork (if installed)
if(require(visNetwork)) {

g_vn &lt;- toVisNetworkData(g)
nodes &lt;- g_vn$nodes
edges &lt;- g_vn$edges

# add manual layout
nodes$x &lt;- c(-1, 1) * 200
nodes$y &lt;- 0

visNetwork(nodes, edges)  %&gt;%  
  visNodes(physics = FALSE) %&gt;% 
  visEdges(smooth = list(type = "curvedCW", roundness = .6), arrows = "to")
} 
 
## Plot an individual graph for each actions
for (a in Tiger$actions) {
 g &lt;- transition_graph(Tiger, action = a)

 plot(g,
  layout = rbind(c(-1, 0), c(1, 0)), rescale = FALSE,  
  edge.curved = curve_multiple_directed(g, .8),
  edge.loop.angle = cumsum(which_loop(g)) *  (-pi / 8),
  vertex.size = 60
 )
}
</code></pre>

<hr>
<h2 id='update_belief'>Belief Update</h2><span id='topic+update_belief'></span>

<h3>Description</h3>

<p>Update the belief given a taken action and observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_belief(
  model,
  belief = NULL,
  action = NULL,
  observation = NULL,
  episode = 1,
  digits = 7,
  drop = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_belief_+3A_model">model</code></td>
<td>
<p>a <a href="#topic+POMDP">POMDP</a> object.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_belief">belief</code></td>
<td>
<p>the current belief state.
Defaults to the start belief state specified in
the model or &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_action">action</code></td>
<td>
<p>the taken action. Can also be a vector of multiple actions or, if missing, then all actions are evaluated.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_observation">observation</code></td>
<td>
<p>the received observation. Can also be a vector of multiple observations or, if missing, then all observations are evaluated.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_episode">episode</code></td>
<td>
<p>Use transition and observation matrices for the given episode
for time-dependent POMDPs (see <a href="#topic+POMDP">POMDP</a>).</p>
</td></tr>
<tr><td><code id="update_belief_+3A_digits">digits</code></td>
<td>
<p>round decimals.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_drop">drop</code></td>
<td>
<p>logical; drop the result to a vector if only a single belief
state is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Update the belief state <code class="reqn">b</code> (<code>belief</code>) with an action <code class="reqn">a</code> and observation <code class="reqn">o</code> using the update
<code class="reqn">b' \leftarrow \tau(b, a, o)</code> defined so that
</p>
<p style="text-align: center;"><code class="reqn">b'(s') = \eta O(o | s',a) \sum_{s \in S} T(s' | s,a) b(s)</code>
</p>

<p>where <code class="reqn">\eta = 1/ \sum_{s' \in S}[ O(o | s',a) \sum_{s \in S} T(s' | s,a) b(s)]</code> normalizes the new belief state so the probabilities add up to one.
</p>


<h3>Value</h3>

<p>returns the updated belief state as a named vector.
If <code>action</code> or <code>observations</code> is a vector with multiple elements ot missing, then a matrix with all
resulting belief states is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

update_belief(c(.5,.5), model = Tiger)
update_belief(c(.5,.5), action = "listen", observation = "tiger-left", model = Tiger)
update_belief(c(.15,.85), action = "listen", observation = "tiger-right", model = Tiger)

</code></pre>

<hr>
<h2 id='value_function'>Value Function</h2><span id='topic+value_function'></span><span id='topic+plot_value_function'></span>

<h3>Description</h3>

<p>Extracts the value function from a solved model.
Extracts the alpha vectors describing the value function. This is similar to <code><a href="#topic+policy">policy()</a></code> which in addition returns the
action prescribed by the solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>value_function(model)

plot_value_function(
  model,
  projection = NULL,
  epoch = 1,
  ylim = NULL,
  legend = TRUE,
  col = NULL,
  lwd = 1,
  lty = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="value_function_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a>.</p>
</td></tr>
<tr><td><code id="value_function_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="value_function_+3A_epoch">epoch</code></td>
<td>
<p>the value function of what epoch should be plotted? Use 1 for
converged policies.</p>
</td></tr>
<tr><td><code id="value_function_+3A_ylim">ylim</code></td>
<td>
<p>the y limits of the plot.</p>
</td></tr>
<tr><td><code id="value_function_+3A_legend">legend</code></td>
<td>
<p>logical; add a legend?</p>
</td></tr>
<tr><td><code id="value_function_+3A_col">col</code></td>
<td>
<p>potting colors.</p>
</td></tr>
<tr><td><code id="value_function_+3A_lwd">lwd</code></td>
<td>
<p>line width.</p>
</td></tr>
<tr><td><code id="value_function_+3A_lty">lty</code></td>
<td>
<p>line type.</p>
</td></tr>
<tr><td><code id="value_function_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to <code><a href="stats.html#topic+line">stats::line()</a></code>'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the value function of a POMDP solution as a line plot. The solution is
projected on two states (i.e., the belief for the other states is held
constant at zero). The value function can also be visualized using <code><a href="#topic+plot_belief_space">plot_belief_space()</a></code>.
</p>


<h3>Value</h3>

<p>the function as a matrix with alpha vectors as rows.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
sol &lt;- solve_POMDP(model = Tiger)
sol

# value function for the converged solution
value_function(sol)

plot_value_function(sol, ylim = c(0,20))

## finite-horizon problem
sol &lt;- solve_POMDP(model = Tiger, horizon = 3, discount = 1,
  method = "enum")
sol

# inspect the value function for all epochs
value_function(sol)

plot_value_function(sol, epoch = 1, ylim = c(-5, 25))
plot_value_function(sol, epoch = 2, ylim = c(-5, 25))
plot_value_function(sol, epoch = 3, ylim = c(-5, 25))

## Not run: 
# using ggplot2 to plot the value function for epoch 3
library(ggplot2)
pol &lt;- policy(sol)[[3]]
ggplot(pol) +
 geom_segment(aes(x = 0, y = `tiger-left`, xend = 1, yend = `tiger-right`, color = action)) +
 coord_cartesian(ylim = c(-5, 15)) + ylab("Reward") + xlab("Belief")

## End(Not run)
</code></pre>

<hr>
<h2 id='write_POMDP'>Read and write a POMDP Model to a File in POMDP Format</h2><span id='topic+write_POMDP'></span><span id='topic+read_POMDP'></span>

<h3>Description</h3>

<p>Reads and write a POMDP file suitable for the <code>pomdp-solve</code> program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_POMDP(x, file, digits = 7)

read_POMDP(file, parse = TRUE, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_POMDP_+3A_x">x</code></td>
<td>
<p>an object of class <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_file">file</code></td>
<td>
<p>a file name. <code>read_POMDP()</code> also accepts <a href="base.html#topic+connections">connections</a> including URLs.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_digits">digits</code></td>
<td>
<p>precision for writing numbers (digits after the decimal
point).</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_parse">parse</code></td>
<td>
<p>logical; try to parse the model matrices.
Solvers still work with unparsed matrices, but helpers for simulation are not available.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_normalize">normalize</code></td>
<td>
<p>logical; should the description be normalized for faster access (see <code><a href="#topic+normalize_POMDP">normalize_POMDP()</a></code>)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p><a href="#topic+POMDP">POMDP</a> objects read from a POMDP file have an extra element called <code>problem</code> which contains the original
POMDP specification. <strong>The original specification is directly used by external solvers.</strong> In addition, the file
is parsed using an experimental POMDP file parser. The parsed information can be used with auxiliary functions
in this package that use fields like the transition matrix, the observation matrix and the reward structure.
</p>
<p><strong>Notes:</strong>
The parser for POMDP files is experimental. Please report
problems here: <a href="https://github.com/mhahsler/pomdp/issues">https://github.com/mhahsler/pomdp/issues</a>.
</p>


<h3>Value</h3>

<p><code>read_POMDP()</code> returns a <a href="#topic+POMDP">POMDP</a> object.
</p>


<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>POMDP solver website: https://www.pomdp.org
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+POMDP_accessors">POMDP_accessors</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

## show the POMDP file that would be written.
write_POMDP(Tiger, file = stdout())
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
