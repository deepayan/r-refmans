<!DOCTYPE html><html><head><title>Help for package pomdp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pomdp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pomdp-package'><p>pomdp: Infrastructure for Partially Observable Markov Decision Processes (POMDP)</p></a></li>
<li><a href='#accessors'><p>Access to Parts of the Model Description</p></a></li>
<li><a href='#actions'><p>Available Actions</p></a></li>
<li><a href='#add_policy'><p>Add a Policy to a POMDP Problem Description</p></a></li>
<li><a href='#Cliff_walking'><p>Cliff Walking Gridworld MDP</p></a></li>
<li><a href='#colors'><p>Default Colors for Visualization in Package pomdp</p></a></li>
<li><a href='#estimate_belief_for_nodes'><p>Estimate the Belief for Policy Graph Nodes</p></a></li>
<li><a href='#gridworld'><p>Helper Functions for Gridworld MDPs</p></a></li>
<li><a href='#Maze'><p>Steward Russell's 4x3 Maze Gridworld MDP</p></a></li>
<li><a href='#MDP'><p>Define an MDP Problem</p></a></li>
<li><a href='#MDP_policy_functions'><p>Functions for MDP Policies</p></a></li>
<li><a href='#MDP2POMDP'><p>Convert between MDPs and POMDPs</p></a></li>
<li><a href='#optimal_action'><p>Optimal action for a belief</p></a></li>
<li><a href='#plot_belief_space'><p>Plot a 2D or 3D Projection of the Belief Space</p></a></li>
<li><a href='#plot_policy_graph'><p>POMDP Plot Policy Graphs</p></a></li>
<li><a href='#policy'><p>Extract the Policy from a POMDP/MDP</p></a></li>
<li><a href='#policy_graph'><p>POMDP Policy Graphs</p></a></li>
<li><a href='#POMDP'><p>Define a POMDP Problem</p></a></li>
<li><a href='#POMDP_example_files'><p>POMDP Example Files</p></a></li>
<li><a href='#projection'><p>Defining a Belief Space Projection</p></a></li>
<li><a href='#reachable_and_absorbing'><p>Reachable and Absorbing States</p></a></li>
<li><a href='#regret'><p>Calculate the Regret of a Policy</p></a></li>
<li><a href='#reward'><p>Calculate the Reward for a POMDP Solution</p></a></li>
<li><a href='#round_stochastic'><p>Round a stochastic vector or a row-stochastic matrix</p></a></li>
<li><a href='#RussianTiger'><p>Russian Tiger Problem POMDP Specification</p></a></li>
<li><a href='#sample_belief_space'><p>Sample from the Belief Space</p></a></li>
<li><a href='#simulate_MDP'><p>Simulate Trajectories in a MDP</p></a></li>
<li><a href='#simulate_POMDP'><p>Simulate Trajectories in a POMDP</p></a></li>
<li><a href='#solve_MDP'><p>Solve an MDP Problem</p></a></li>
<li><a href='#solve_POMDP'><p>Solve a POMDP Problem using pomdp-solver</p></a></li>
<li><a href='#solve_SARSOP'><p>Solve a POMDP Problem using SARSOP</p></a></li>
<li><a href='#Tiger'><p>Tiger Problem POMDP Specification</p></a></li>
<li><a href='#transition_graph'><p>Transition Graph</p></a></li>
<li><a href='#update_belief'><p>Belief Update</p></a></li>
<li><a href='#value_function'><p>Value Function</p></a></li>
<li><a href='#Windy_gridworld'><p>Windy Gridworld MDP</p></a></li>
<li><a href='#write_POMDP'><p>Read and write a POMDP Model to a File in POMDP Format</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Infrastructure for Partially Observable Markov Decision
Processes (POMDP)</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-05-01</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Process (POMDP) models. Interfaces for various exact and approximate solution algorithms are available including value iteration, point-based value iteration and SARSOP. Smallwood and Sondik (1973) &lt;<a href="https://doi.org/10.1287%2Fopre.21.5.1071">doi:10.1287/opre.21.5.1071</a>&gt;.</td>
</tr>
<tr>
<td>Classification/ACM:</td>
<td>G.4, G.1.6, I.2.6</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mhahsler/pomdp">https://github.com/mhahsler/pomdp</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mhahsler/pomdp/issues">https://github.com/mhahsler/pomdp/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>pomdpSolve (&ge; 1.0.4), processx, stats, methods, Matrix, Rcpp,
foreach, igraph</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, gifski, testthat, Ternary, visNetwork,
sarsop, doParallel</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>Copyright (C) Michael Hahsler and Hossein Kamalzadeh.</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'AAA_check_installed.R' 'AAA_pomdp-package.R' 'AAA_shorten.R'
'Cliff_walking.R' 'POMDP.R' 'MDP.R' 'MDP_policy_functions.R'
'Maze.R' 'POMDP_file_examples.R' 'RcppExports.R'
'RussianTiger.R' 'Tiger.R' 'Windy_gridworld.R' 'accessors.R'
'accessors_reward.R' 'accessors_trans_obs.R' 'actions.R'
'add_policy.R' 'check_and_fix_MDP.R' 'colors.R'
'estimate_belief_for_nodes.R' 'foreach_helper.R' 'gridworld.R'
'make_partially_observable.R' 'optimal_action.R'
'plot_belief_space.R' 'plot_policy_graph.R' 'policy.R'
'policy_graph.R' 'print.text.R' 'projection.R' 'queue.R'
'reachable_and_absorbing.R' 'read_write_POMDP.R'
'read_write_pomdp_solve.R' 'regret.R' 'reward.R'
'round_stochchastic.R' 'sample_belief_space.R' 'simulate_MDP.R'
'simulate_POMDP.R' 'solve_MDP.R' 'solve_POMDP.R'
'solve_SARSOP.R' 'stack.R' 'transition_graph.R'
'update_belief.R' 'value_function.R' 'which_max_random.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-05 00:33:18 UTC; hahsler</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Hahsler <a href="https://orcid.org/0000-0003-2716-1405"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph, cre],
  Hossein Kamalzadeh [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Hahsler &lt;mhahsler@lyle.smu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-05 21:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='pomdp-package'>pomdp: Infrastructure for Partially Observable Markov Decision Processes (POMDP)</h2><span id='topic+pomdp'></span><span id='topic+pomdp-package'></span>

<h3>Description</h3>

<p>Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Process (POMDP) models. Interfaces for various exact and approximate solution algorithms are available including value iteration, point-based value iteration and SARSOP. Smallwood and Sondik (1973) <a href="https://doi.org/10.1287/opre.21.5.1071">doi:10.1287/opre.21.5.1071</a>.
</p>


<h3>Key functions</h3>


<ul>
<li><p> Problem specification: <a href="#topic+POMDP">POMDP</a>, <a href="#topic+MDP">MDP</a>
</p>
</li>
<li><p> Solvers: <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>, <code><a href="#topic+solve_MDP">solve_MDP()</a></code>, <code><a href="#topic+solve_SARSOP">solve_SARSOP()</a></code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Michael Hahsler <a href="mailto:mhahsler@lyle.smu.edu">mhahsler@lyle.smu.edu</a> (<a href="https://orcid.org/0000-0003-2716-1405">ORCID</a>) [copyright holder]
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Hossein Kamalzadeh [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/mhahsler/pomdp">https://github.com/mhahsler/pomdp</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mhahsler/pomdp/issues">https://github.com/mhahsler/pomdp/issues</a>
</p>
</li></ul>


<hr>
<h2 id='accessors'>Access to Parts of the Model Description</h2><span id='topic+accessors'></span><span id='topic+start_vector'></span><span id='topic+normalize_POMDP'></span><span id='topic+normalize_MDP'></span><span id='topic+reward_matrix'></span><span id='topic+reward_val'></span><span id='topic+transition_matrix'></span><span id='topic+transition_val'></span><span id='topic+observation_matrix'></span><span id='topic+observation_val'></span>

<h3>Description</h3>

<p>Functions to provide uniform access to different parts of the POMDP/MDP
problem description.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>start_vector(x)

normalize_POMDP(
  x,
  sparse = TRUE,
  trans_start = FALSE,
  trans_function = TRUE,
  trans_keyword = FALSE
)

normalize_MDP(
  x,
  sparse = TRUE,
  trans_start = FALSE,
  trans_function = TRUE,
  trans_keyword = FALSE
)

reward_matrix(
  x,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  observation = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE
)

reward_val(
  x,
  action,
  start.state,
  end.state = NULL,
  observation = NULL,
  episode = NULL,
  epoch = NULL
)

transition_matrix(
  x,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE,
  trans_keyword = TRUE
)

transition_val(x, action, start.state, end.state, episode = NULL, epoch = NULL)

observation_matrix(
  x,
  action = NULL,
  end.state = NULL,
  observation = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE,
  trans_keyword = TRUE
)

observation_val(
  x,
  action,
  end.state,
  observation,
  episode = NULL,
  epoch = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accessors_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a> object.</p>
</td></tr>
<tr><td><code id="accessors_+3A_sparse">sparse</code></td>
<td>
<p>logical; use sparse matrices when the density is below 50% and keeps data.frame representation
for the reward field. <code>NULL</code> returns the
representation stored in the problem description which saves the time for conversion.</p>
</td></tr>
<tr><td><code id="accessors_+3A_trans_start">trans_start</code></td>
<td>
<p>logical; expand the start to a probability vector?</p>
</td></tr>
<tr><td><code id="accessors_+3A_trans_function">trans_function</code></td>
<td>
<p>logical; convert functions into matrices?</p>
</td></tr>
<tr><td><code id="accessors_+3A_trans_keyword">trans_keyword</code></td>
<td>
<p>logical; convert distribution keywords (uniform and identity)
in <code>transition_prob</code> or <code>observation_prob</code> to matrices?</p>
</td></tr>
<tr><td><code id="accessors_+3A_action">action</code></td>
<td>
<p>name or index of an action.</p>
</td></tr>
<tr><td><code id="accessors_+3A_start.state">start.state</code>, <code id="accessors_+3A_end.state">end.state</code></td>
<td>
<p>name or index of the state.</p>
</td></tr>
<tr><td><code id="accessors_+3A_observation">observation</code></td>
<td>
<p>name or index of observation.</p>
</td></tr>
<tr><td><code id="accessors_+3A_episode">episode</code>, <code id="accessors_+3A_epoch">epoch</code></td>
<td>
<p>Episode or epoch used for time-dependent POMDPs. Epochs are internally converted
to the episode using the model horizon.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several parts of the POMDP/MDP description can be defined in different ways. In particular,
the fields <code>transition_prob</code>, <code>observation_prob</code>, <code>reward</code>, and <code>start</code> can be defined using matrices, data frames,
keywords, or functions. See <a href="#topic+POMDP">POMDP</a> for details. The functions provided here, provide unified access to the data in these fields
to make writing code easier.
</p>


<h4>Transition Probabilities <code class="reqn">T(s'|s,a)</code></h4>

<p><code>transition_matrix()</code> accesses the transition model. The complete model
is a list with one element for each action. Each element contains a states x states matrix
with <code class="reqn">s</code> (<code>start.state</code>) as rows and <code class="reqn">s'</code> (<code>end.state</code>) as columns.
Matrices with a density below 50% can be requested in sparse format
(as a <a href="Matrix.html#topic+dgCMatrix-class">Matrix::dgCMatrix</a>).
</p>



<h4>Observation Probabilities <code class="reqn">O(o|s',a)</code></h4>

<p><code>observation_matrix()</code> accesses the observation model. The complete model is a
list with one element for each action. Each element contains a states x observations matrix
with <code class="reqn">s</code> (<code>start.state</code>) as rows and <code class="reqn">o</code> (<code>observation</code>) as columns.
Matrices with a density below 50% can be requested in sparse format
(as a <a href="Matrix.html#topic+dgCMatrix-class">Matrix::dgCMatrix</a>)
</p>



<h4>Reward <code class="reqn">R(s,s',o,a)</code></h4>

<p><code>reward_matrix()</code> accesses the reward model.
The preferred representation is a data.frame with the
columns <code>action</code>, <code>start.state</code>, <code>end.state</code>,
<code>observation</code>, and <code>value</code>. This is a sparse representation.
The dense representation is a list of lists of matrices.
The list levels are <code class="reqn">a</code> (<code>action</code>)  and <code class="reqn">s</code> (<code>start.state</code>).
The matrices have rows representing <code class="reqn">s'</code> (<code>end.state</code>)
and columns representing <code class="reqn">o</code> (<code>observations</code>).
The reward structure cannot be efficiently stored using a standard sparse matrix
since there might be a fixed cost for each action
resulting in no entries with 0.
</p>



<h4>Initial Belief</h4>

<p><code>start_vector()</code> translates the initial probability vector description into a numeric vector.
</p>



<h4>Convert the Complete POMDP Description into a consistent form</h4>

<p><code>normalize_POMDP()</code> returns a new POMDP definition where <code>transition_prob</code>,
<code>observations_prob</code>, <code>reward</code>, and <code>start</code> are normalized.
</p>
<p>Also, <code>states</code>, <code>actions</code>, and <code>observations</code> are ordered as given in the problem
definition to make safe access using numerical indices possible. Normalized POMDP descriptions can be
used in custom code that expects consistently a certain format.
</p>



<h3>Value</h3>

<p>A list or a list of lists of matrices.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# List of |A| transition matrices. One per action in the from start.states x end.states
Tiger$transition_prob
transition_matrix(Tiger)
transition_val(Tiger, action = "listen", start.state = "tiger-left", end.state = "tiger-left")

# List of |A| observation matrices. One per action in the from states x observations
Tiger$observation_prob
observation_matrix(Tiger)
observation_val(Tiger, action = "listen", end.state = "tiger-left", observation = "tiger-left")

# List of list of reward matrices. 1st level is action and second level is the
#  start state in the form end state x observation
Tiger$reward
reward_matrix(Tiger)
reward_matrix(Tiger, sparse = TRUE)
reward_matrix(Tiger, action = "open-right", start.state = "tiger-left", end.state = "tiger-left",
  observation = "tiger-left")

# Translate the initial belief vector
Tiger$start
start_vector(Tiger)

# Normalize the whole model
Tiger_norm &lt;- normalize_POMDP(Tiger)
Tiger_norm$transition_prob

## Visualize transition matrix for action 'open-left'
plot_transition_graph(Tiger)

## Use a function for the Tiger transition model
trans &lt;- function(action, end.state, start.state) {
  ## listen has an identity matrix
  if (action == 'listen')
    if (end.state == start.state) return(1)
    else return(0)

  # other actions have a uniform distribution
  return(1/2)
}

Tiger$transition_prob &lt;- trans

# transition_matrix evaluates the function
transition_matrix(Tiger)
</code></pre>

<hr>
<h2 id='actions'>Available Actions</h2><span id='topic+actions'></span>

<h3>Description</h3>

<p>Determine the set of actions available in a state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>actions(x, state)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="actions_+3A_x">x</code></td>
<td>
<p>a <code>MDP</code> pr <code>POMDP</code> object.</p>
</td></tr>
<tr><td><code id="actions_+3A_state">state</code></td>
<td>
<p>a character vector of length one specifying the state.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unavailable actions are modeled here a actions that have an immediate
reward of <code>-Inf</code> in the reward function.
</p>


<h3>Value</h3>

<p>a character vector with the available actions.
</p>
<p>a vector with the available actions.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(RussianTiger)

# The normal actions are "listen", "open-left", and "open-right".
# In the state "done" only the action "nothing" is available. 

actions(RussianTiger, state = "tiger-left")
actions(RussianTiger, state = "tiger-right")
actions(RussianTiger, state = "done")
</code></pre>

<hr>
<h2 id='add_policy'>Add a Policy to a POMDP Problem Description</h2><span id='topic+add_policy'></span>

<h3>Description</h3>

<p>Add a policy to a POMDP problem description allows the user to
test policies on modified problem descriptions or to test manually created
policies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_policy(model, policy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_policy_+3A_model">model</code></td>
<td>
<p>a POMDP or MDP model description.</p>
</td></tr>
<tr><td><code id="add_policy_+3A_policy">policy</code></td>
<td>
<p>a policy data.frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The model description with the added policy.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

sol &lt;- solve_POMDP(Tiger)
sol

# Example 1: Use the solution policy on a changed POMDP problem
#            where listening is perfect and simulate the expected reward

perfect_Tiger &lt;- Tiger
perfect_Tiger$observation_prob &lt;- list(
  listen = diag(1, length(perfect_Tiger$states), 
    length(perfect_Tiger$observations)), 
  `open-left` = "uniform",
  `open-right` = "uniform"
)

sol_perfect &lt;- add_policy(perfect_Tiger, sol)
sol_perfect

simulate_POMDP(sol_perfect, n = 1000)$avg_reward

# Example 2: Handcraft a policy and apply it to the Tiger problem

# original policy
policy(sol)
plot_value_function(sol)
plot_belief_space(sol)

# create a policy manually where the agent opens a door at a believe of 
#  roughly 2/3 (note the alpha vectors do not represent 
#  a valid value function)
p &lt;- list(
data.frame(
  `tiger-left` = c(1, 0, -2),
  `tiger-right` = c(-2, 0, 1), 
  action = c("open-right", "listen", "open-left"),
  check.names = FALSE
))
p

custom_sol &lt;- add_policy(Tiger, p)
custom_sol

policy(custom_sol)
plot_value_function(custom_sol)
plot_belief_space(custom_sol)

simulate_POMDP(custom_sol, n = 1000)$avg_reward
</code></pre>

<hr>
<h2 id='Cliff_walking'>Cliff Walking Gridworld MDP</h2><span id='topic+Cliff_walking'></span><span id='topic+cliff_walking'></span>

<h3>Description</h3>

<p>The cliff walking gridworld MDP example from Chapter 6 of the textbook
&quot;Reinforcement Learning: An Introduction.&quot;
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+MDP">MDP</a>.
</p>


<h3>Details</h3>

<p>The cliff walking gridworld has the following layout:
</p>
<p><img src="../help/figures/cliff-walking-gridworld.png" alt="Cliff Walking Gridworld." />
</p>
<p>The gridworld is represented as a 4 x 12 matrix of states.
The states are labeled with their x and y coordinates.
The start state is in the bottom left corner.
Each action has a reward of -1, falling off the cliff has a reward of -100 and
returns the agent back to the start. The episode is finished once the agent
reaches the absorbing goal state in the bottom right corner.
No discounting is used (i.e., <code class="reqn">\gamma = 1</code>).
</p>


<h3>References</h3>

<p>Richard S. Sutton and Andrew G. Barto (2018). Reinforcement Learning: An Introduction
Second Edition, MIT Press, Cambridge, MA.
</p>


<h3>See Also</h3>

<p>Other MDP_examples: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+Maze">Maze</a></code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>
</p>
<p>Other gridworld: 
<code><a href="#topic+Maze">Maze</a></code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>,
<code><a href="#topic+gridworld">gridworld</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Cliff_walking)
Cliff_walking

gridworld_matrix(Cliff_walking)
gridworld_matrix(Cliff_walking, what = "labels")

# The Goal is an absorbing state 
which(absorbing_states(Cliff_walking))

# visualize the transition graph
gridworld_plot_transition_graph(Cliff_walking)

# solve using different methods
sol &lt;- solve_MDP(Cliff_walking) 
sol
policy(sol)
gridworld_plot_policy(sol)

sol &lt;- solve_MDP(Cliff_walking, method = "q_learning", N = 100) 
sol
policy(sol)
gridworld_plot_policy(sol)

sol &lt;- solve_MDP(Cliff_walking, method = "sarsa", N = 100) 
sol
policy(sol)
gridworld_plot_policy(sol)

sol &lt;- solve_MDP(Cliff_walking, method = "expected_sarsa", N = 100, alpha = 1) 
policy(sol)
gridworld_plot_policy(sol)
</code></pre>

<hr>
<h2 id='colors'>Default Colors for Visualization in Package pomdp</h2><span id='topic+colors'></span><span id='topic+colors_discrete'></span><span id='topic+colors_continuous'></span>

<h3>Description</h3>

<p>Default discrete and continuous colors used in pomdp for states (nodes), beliefs and values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colors_discrete(n, col = NULL)

colors_continuous(val, col = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colors_+3A_n">n</code></td>
<td>
<p>number of states.</p>
</td></tr>
<tr><td><code id="colors_+3A_col">col</code></td>
<td>
<p>custom color palette. <code>colors_discrete()</code> uses the first n colors.
<code>colors_continuous()</code> uses these colors to calculate a palette (see <code><a href="grDevices.html#topic+colorRamp">grDevices::colorRamp()</a></code>)</p>
</td></tr>
<tr><td><code id="colors_+3A_val">val</code></td>
<td>
<p>a vector with values to be translated to colors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>colors_discrete()</code> returns a color palette and
<code>colors_continuous()</code> returns the colors associated with the supplied values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colors_discrete(5)

colors_continuous(runif(10))
</code></pre>

<hr>
<h2 id='estimate_belief_for_nodes'>Estimate the Belief for Policy Graph Nodes</h2><span id='topic+estimate_belief_for_nodes'></span>

<h3>Description</h3>

<p>Estimate a belief for each alpha vector (segment of the value function) which represents
a node in the policy graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_belief_for_nodes(
  x,
  method = "auto",
  belief = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_belief_for_nodes_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_method">method</code></td>
<td>
<p>character string specifying the estimation method. Methods include
<code>"auto"</code>, reuse <code>"solver_points"</code>, follow <code>"trajectories"</code>, sample <code>"random_sample"</code>
or <code>"regular_sample"</code>. Auto uses
solver points if available and follows trajectories otherwise.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_belief">belief</code></td>
<td>
<p>start belief used for method trajectories. <code>NULL</code> uses the start belief specified in the model.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_verbose">verbose</code></td>
<td>
<p>logical; show which method is used.</p>
</td></tr>
<tr><td><code id="estimate_belief_for_nodes_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code>sample_belief_space()</code> or the code that follows trajectories.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>estimate_belief_for_nodes()</code> can estimate the belief in several ways:
</p>

<ol>
<li> <p><strong>Use belief points explored by the solver.</strong> Some solvers return explored belief points.
We assign the belief points to the nodes and average each nodes belief.
</p>
</li>
<li> <p><strong>Follow trajectories</strong> (breadth first) till all policy graph nodes have been visited and
return the encountered belief. This implementation returns the first (i.e., shallowest) belief point
that is encountered is used and no averaging is performed. parameter <code>n</code> can be used to
limit the number of nodes searched.
</p>
</li>
<li> <p><strong>Sample a large set</strong> of possible belief points, assigning them to the nodes and then averaging
the belief over the points assigned to each node. This will return a central belief for the node.
Additional parameters like <code>method</code> and the sample size <code>n</code> are passed on to <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.
If no belief point is generated for a segment, then a
warning is produced. In this case, the number of sampled points can be increased.
</p>
</li></ol>

<p><strong>Notes:</strong>
</p>

<ul>
<li><p> Each method may return a different answer. The only thing that is guaranteed is that the returned belief falls
in the range where the value function segment is maximal.
</p>
</li>
<li><p> If some nodes not belief points are sampled, or the node is not reachable from the initial belief,
then a vector with all <code>NaN</code>s will be returned with a warning.
</p>
</li></ul>



<h3>Value</h3>

<p>returns a list with matrices with a belief for each policy graph node. The list elements are the epochs and converged solutions
only have a single element.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# Infinite horizon case with converged solution
sol &lt;- solve_POMDP(model = Tiger, method = "grid")
sol

# default method auto uses the belief points used in the algorithm (if available).
estimate_belief_for_nodes(sol, verbose = TRUE)

# use belief points obtained from trajectories  
estimate_belief_for_nodes(sol, method = "trajectories", verbose = TRUE)

# use a random uniform sample 
estimate_belief_for_nodes(sol, method = "random", verbose = TRUE)

# Finite horizon example with three epochs. 
sol &lt;- solve_POMDP(model = Tiger, horizon = 3)
sol
estimate_belief_for_nodes(sol)
</code></pre>

<hr>
<h2 id='gridworld'>Helper Functions for Gridworld MDPs</h2><span id='topic+gridworld'></span><span id='topic+gridworld_init'></span><span id='topic+gridworld_maze_MDP'></span><span id='topic+gridworld_s2rc'></span><span id='topic+gridworld_rc2s'></span><span id='topic+gridworld_matrix'></span><span id='topic+gridworld_plot_policy'></span><span id='topic+gridworld_plot_transition_graph'></span><span id='topic+gridworld_animate'></span>

<h3>Description</h3>

<p>Helper functions for gridworld MDPs to convert between state names and
gridworld positions, and for visualizing policies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gridworld_init(
  dim,
  action_labels = c("up", "right", "down", "left"),
  unreachable_states = NULL,
  absorbing_states = NULL,
  labels = NULL
)

gridworld_maze_MDP(
  dim,
  start,
  goal,
  walls = NULL,
  action_labels = c("up", "right", "down", "left"),
  goal_reward = 1,
  step_cost = 0,
  restart = FALSE,
  discount = 0.9,
  horizon = Inf,
  info = NULL,
  name = NA
)

gridworld_s2rc(s)

gridworld_rc2s(rc)

gridworld_matrix(model, epoch = 1L, what = "states")

gridworld_plot_policy(
  model,
  epoch = 1L,
  actions = "character",
  states = FALSE,
  labels = TRUE,
  absorbing_state_action = FALSE,
  main = NULL,
  cex = 1,
  offset = 0.5,
  lines = TRUE,
  ...
)

gridworld_plot_transition_graph(
  x,
  hide_unreachable_states = TRUE,
  remove.loops = TRUE,
  vertex.color = "gray",
  vertex.shape = "square",
  vertex.size = 10,
  vertex.label = NA,
  edge.arrow.size = 0.3,
  margin = 0.2,
  main = NULL,
  ...
)

gridworld_animate(x, method, n, zlim = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gridworld_+3A_dim">dim</code></td>
<td>
<p>vector of length two with the x and y extent of the gridworld.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_action_labels">action_labels</code></td>
<td>
<p>vector with four action labels that move the agent up, right, down,
and left.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_unreachable_states">unreachable_states</code></td>
<td>
<p>a vector with state labels for unreachable states.
These states will be excluded.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_absorbing_states">absorbing_states</code></td>
<td>
<p>a vector with state labels for absorbing states.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_labels">labels</code></td>
<td>
<p>logical; show state labels.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_start">start</code>, <code id="gridworld_+3A_goal">goal</code></td>
<td>
<p>labels for the start state and the goal state.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_walls">walls</code></td>
<td>
<p>a vector with state labels for walls. Walls will
become unreachable states.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_goal_reward">goal_reward</code></td>
<td>
<p>reward to transition to the goal state.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_step_cost">step_cost</code></td>
<td>
<p>cost of each action that does not lead to the goal state.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_restart">restart</code></td>
<td>
<p>logical; if <code>TRUE</code> then the problem automatically restarts when
the agent reaches the goal state.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_discount">discount</code>, <code id="gridworld_+3A_horizon">horizon</code></td>
<td>
<p>MDP discount factor, and horizon.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_info">info</code></td>
<td>
<p>A list with additional information. Has to contain the gridworld
dimensions as element <code>gridworld_dim</code>.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_name">name</code></td>
<td>
<p>a string to identify the MDP problem.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_s">s</code></td>
<td>
<p>a state label.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_rc">rc</code></td>
<td>
<p>a vector of length two with the row and column coordinate of a
state in the gridworld matrix.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_model">model</code>, <code id="gridworld_+3A_x">x</code></td>
<td>
<p>a solved gridworld MDP.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_epoch">epoch</code></td>
<td>
<p>epoch for unconverged finite-horizon solutions.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_what">what</code></td>
<td>
<p>What should be returned in the matrix. Options are:
<code>"states"</code>, <code>"labels"</code>, <code>"values"</code>, <code>"actions"</code>, <code>"absorbing"</code>, and
<code>"reachable"</code>.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_actions">actions</code></td>
<td>
<p>how to show actions. Options are:
simple <code>"character"</code>, <code>"unicode"</code> arrows (needs to be supported by the used font),
<code>"label"</code> of the action, and  <code>"none"</code> to suppress showing the action.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_states">states</code></td>
<td>
<p>logical; show state names.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_absorbing_state_action">absorbing_state_action</code></td>
<td>
<p>logical; show the value and the action for absorbing states.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_main">main</code></td>
<td>
<p>a main title for the plot. Defaults to the name of the problem.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_cex">cex</code></td>
<td>
<p>expansion factor for the action.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_offset">offset</code></td>
<td>
<p>move the state labels out of the way (in fractions of a character width).</p>
</td></tr>
<tr><td><code id="gridworld_+3A_lines">lines</code></td>
<td>
<p>logical; draw lines to separate states.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_...">...</code></td>
<td>
<p>further arguments are passed on to <code>igraph::plot.igraph()</code>.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_hide_unreachable_states">hide_unreachable_states</code></td>
<td>
<p>logical; do not show unreachable states.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_remove.loops">remove.loops</code></td>
<td>
<p>logical; do not show transitions from a state back to itself.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_vertex.color">vertex.color</code>, <code id="gridworld_+3A_vertex.shape">vertex.shape</code>, <code id="gridworld_+3A_vertex.size">vertex.size</code>, <code id="gridworld_+3A_vertex.label">vertex.label</code>, <code id="gridworld_+3A_edge.arrow.size">edge.arrow.size</code></td>
<td>
<p>see <code>igraph::igraph.plotting</code> for details. Set <code>vertex.label = NULL</code> to show the
state labels on the graph.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_margin">margin</code></td>
<td>
<p>a single number specifying the margin of the plot. Can be used if the
graph does not fit inside the plotting area.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_method">method</code></td>
<td>
<p>a MDP solution method for <code><a href="#topic+solve_MDP">solve_MDP()</a></code>.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_n">n</code></td>
<td>
<p>number of iterations to animate.</p>
</td></tr>
<tr><td><code id="gridworld_+3A_zlim">zlim</code></td>
<td>
<p>limits for visualizing the state value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gridworlds are implemented with state names <code>s(row,col)</code>, where
<code>row</code> and <code>col</code> are locations in the matrix representing the gridworld.
The actions are <code>"up"</code>, <code>"right"</code>,  <code>"down"</code>, and  <code>"left"</code>.
</p>
<p><code>gridworld_init()</code> initializes a new gridworld creating a matrix
of states with the given dimensions. Other action names
can be specified, but they must have the same effects in the same order
as above. Unreachable states (walls) and absorbing state can be defined.
This information can be used to build a custom gridworld MDP.
</p>
<p>Several helper functions are provided
to use states, look at the state layout, and plot policies on the
gridworld.
</p>
<p><code>gridworld_maze_MDP()</code> helps to easily define maze-like gridworld MDPs.
By default, the goal state is absorbing, but with <code>restart = TRUE</code>, the
agent restarts the problem at the start state every time it reaches the goal
and receives the reward. Note that this implies that the goal state itself
becomes unreachable.
</p>
<p><code>gridworld_animate()</code> applies algorithms from <code><a href="#topic+solve_MDP">solve_MDP()</a></code> iteration
by iteration and visualized the state utilities. This helps to understand
how the algorithms work.
</p>


<h3>See Also</h3>

<p>Other gridworld: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+Maze">Maze</a></code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Defines states, actions and a transition model for a standard gridworld
gw &lt;- gridworld_init(dim = c(7,7),
                unreachable_states = c("s(2,2)", "s(7,3)", "s(3,6)"),
                absorbing_states = "s(4,4)",
                labels = list("s(4,4)" = "Black Hole")
                )

gw$states
gw$actions
gw$info

# display the state labels in the gridworld
gridworld_matrix(gw)
gridworld_matrix(gw, what = "label")
gridworld_matrix(gw, what = "reachable")
gridworld_matrix(gw, what = "absorbing")

# a transition function for regular moves in the gridworld is provided
gw$transition_prob("right", "s(1,1)", "s(1,2)")
gw$transition_prob("right", "s(2,1)", "s(2,2)")  ### we cannot move into an unreachable state
gw$transition_prob("right", "s(2,1)", "s(2,1)")  ### but the agent stays in place

# convert between state names and row/column indices
gridworld_s2rc("s(1,1)")
gridworld_rc2s(c(1,1))

# The information in gw can be used to build a custom MDP.

# We modify the standard transition function so there is a 50% chance that
# you will get sucked into the black hole from the adjacent squares.
trans_black_hole &lt;- function(action = NA, start.state = NA, end.state = NA) {
  # ignore the action next to the black hole
  if (start.state %in% c("s(3,3)", "s(3,4)", "s(3,5)", "s(4,3)", "s(4,5)",
                         "s(5,3)", "s(5,4)", "s(5,5)")) {
        if(end.state == "s(4,4)")
            return(.5)
        else
            return(gw$transition_prob(action, start.state, end.state) * .5)
  }

  # use the standard gridworld movement
  gw$transition_prob(action, start.state, end.state)
}

black_hole &lt;- MDP(states = gw$states,
  actions = gw$actions,
  transition_prob = trans_black_hole,
  reward = rbind(R_(value = +1), R_(end.state = "s(4,4)", value = -100)),
  info = gw$info,
  name = "Black hole"
  )

black_hole

gridworld_plot_transition_graph(black_hole)

# solve the problem
sol &lt;- solve_MDP(black_hole)
gridworld_matrix(sol, what = "values")
gridworld_plot_policy(sol)
# the optimal policy is to fly around, but avoid the black hole.

# Build a Maze: The Dyna Maze from Chapter 8 in the RL book

Dyna_maze &lt;- gridworld_maze_MDP(
                dim = c(6,9),
                start = "s(3,1)",
                goal = "s(1,9)",
                walls = c("s(2,3)", "s(3,3)", "s(4,3)",
                          "s(5,6)",
                          "s(1,8)", "s(2,8)", "s(3,8)"),
                restart = TRUE,
                discount = 0.95,
                name = "Dyna Maze",
                )
Dyna_maze

gridworld_matrix(Dyna_maze)
gridworld_matrix(Dyna_maze, what = "labels")

gridworld_plot_transition_graph(Dyna_maze)
# Note that the problems resets if the goal state would be reached.

sol &lt;- solve_MDP(Dyna_maze)

gridworld_matrix(sol, what = "values")
gridworld_matrix(sol, what = "actions")
gridworld_plot_policy(sol)
gridworld_plot_policy(sol, actions = "label", cex = 1, states = FALSE)

# visualize the first 3 iterations of value iteration
gridworld_animate(Dyna_maze, method = "value", n = 3)
</code></pre>

<hr>
<h2 id='Maze'>Steward Russell's 4x3 Maze Gridworld MDP</h2><span id='topic+Maze'></span><span id='topic+maze'></span>

<h3>Description</h3>

<p>The 4x3 maze is described in Chapter 17 of the textbook
&quot;Artificial Intelligence: A Modern Approach&quot; (AIMA).
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+MDP">MDP</a>.
</p>


<h3>Details</h3>

<p>The simple maze has the following layout:
</p>
<pre>
    1234           Transition model:
   ######             .8 (action direction)
  1#   +#              ^
  2# # -#              |
  3#S   #         .1 &lt;-|-&gt; .1
   ######
</pre>
<p>We represent the maze states as a gridworld matrix with 3 rows and
4 columns. The states are labeled <code>s(row, col)</code> representing the position in
the matrix.
The # (state <code>s(2,2)</code>) in the middle of the maze is an obstruction and not reachable.
Rewards are associated with transitions. The default reward (penalty) is -0.04.
The start state marked with <code>S</code> is <code>s(3,1)</code>.
Transitioning to <code>+</code> (state <code>s(1,4)</code>) gives a reward of +1.0,
transitioning to <code>-</code> (state <code>s_(2,4)</code>)
has a reward of -1.0. Both these states are absorbing
(i.e., terminal) states.
</p>
<p>Actions are movements (<code>up</code>, <code>right</code>, <code>down</code>, <code>left</code>). The actions are
unreliable with a .8 chance
to move in the correct direction and a 0.1 chance to instead to move in a
perpendicular direction leading to a stochastic transition model.
</p>
<p>Note that the problem has reachable terminal states which leads to a proper policy
(that is guaranteed to reach a terminal state). This means that the solution also
converges without discounting (<code>discount = 1</code>).
</p>


<h3>References</h3>

<p>Russell,9 S. J. and Norvig, P. (2020). Artificial Intelligence:
A modern approach. 4rd ed.
</p>


<h3>See Also</h3>

<p>Other MDP_examples: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>
</p>
<p>Other gridworld: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>,
<code><a href="#topic+gridworld">gridworld</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The problem can be loaded using data(Maze).

# Here is the complete problem definition:
gw &lt;- gridworld_init(dim = c(3, 4), unreachable_states = c("s(2,2)"))
gridworld_matrix(gw)

# the transition function is stochastic so we cannot use the standard
# gridworld gw$transition_prob() function
T &lt;- function(action, start.state, end.state) {
  action &lt;- match.arg(action, choices = gw$actions)
  
  # absorbing states
  if (start.state %in% c('s(1,4)', 's(2,4)')) {
    if (start.state == end.state) return(1)
    else return(0)
  }
  
  # actions are stochastic so we cannot use gw$trans_prob
  if(action %in% c("up", "down")) error_direction &lt;- c("right", "left")
  else error_direction &lt;- c("up", "down")
  
  rc &lt;- gridworld_s2rc(start.state)
  delta &lt;- list(up = c(-1, 0), 
                down = c(+1, 0),
                right = c(0, +1), 
                left = c(0, -1))
  P &lt;- matrix(0, nrow = 3, ncol = 4)

  add_prob &lt;- function(P, rc, a, value) {
    new_rc &lt;- rc + delta[[a]]
    if (!(gridworld_rc2s(new_rc) %in% gw$states))
      new_rc &lt;- rc
    P[new_rc[1], new_rc[2]] &lt;- P[new_rc[1], new_rc[2]] + value
    P
  }

  P &lt;- add_prob(P, rc, action, .8)
  P &lt;- add_prob(P, rc, error_direction[1], .1)
  P &lt;- add_prob(P, rc, error_direction[2], .1)
  P[rbind(gridworld_s2rc(end.state))]
}

T("up", "s(3,1)", "s(2,1)")

R &lt;- rbind(
 R_(end.state   = NA,     value = -0.04),
 R_(end.state   = 's(2,4)',  value = -1),
 R_(end.state   = 's(1,4)',  value = +1),
 R_(start.state = 's(2,4)',  value = 0),
 R_(start.state = 's(1,4)',  value = 0)
)


Maze &lt;- MDP(
 name = "Stuart Russell's 3x4 Maze",
 discount = 1,
 horizon = Inf,
 states = gw$states,
 actions = gw$actions,
 start = "s(3,1)",
 transition_prob = T,
 reward = R,
 info = list(gridworld_dim = c(3, 4),
             gridworld_labels = list(
                "s(3,1)" = "Start",
                "s(2,4)" = "-1",
                "s(1,4)" = "Goal: +1"
                )
             )
)

Maze

str(Maze)

gridworld_matrix(Maze)
gridworld_matrix(Maze, what = "labels")

# find absorbing (terminal) states
which(absorbing_states(Maze))

maze_solved &lt;- solve_MDP(Maze)
policy(maze_solved)

gridworld_matrix(maze_solved, what = "values")
gridworld_matrix(maze_solved, what = "actions")

gridworld_plot_policy(maze_solved)
</code></pre>

<hr>
<h2 id='MDP'>Define an MDP Problem</h2><span id='topic+MDP'></span><span id='topic+is_solved_MDP'></span>

<h3>Description</h3>

<p>Defines all the elements of a finite state-space MDP problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  info = NULL,
  name = NA
)

is_solved_MDP(x, stop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDP_+3A_states">states</code></td>
<td>
<p>a character vector specifying the names of the states.</p>
</td></tr>
<tr><td><code id="MDP_+3A_actions">actions</code></td>
<td>
<p>a character vector specifying the names of the available
actions.</p>
</td></tr>
<tr><td><code id="MDP_+3A_transition_prob">transition_prob</code></td>
<td>
<p>Specifies the transition probabilities between
states.</p>
</td></tr>
<tr><td><code id="MDP_+3A_reward">reward</code></td>
<td>
<p>Specifies the rewards dependent on action, states and
observations.</p>
</td></tr>
<tr><td><code id="MDP_+3A_discount">discount</code></td>
<td>
<p>numeric; discount rate between 0 and 1.</p>
</td></tr>
<tr><td><code id="MDP_+3A_horizon">horizon</code></td>
<td>
<p>numeric; Number of epochs. <code>Inf</code> specifies an infinite
horizon.</p>
</td></tr>
<tr><td><code id="MDP_+3A_start">start</code></td>
<td>
<p>Specifies in which state the MDP starts.</p>
</td></tr>
<tr><td><code id="MDP_+3A_info">info</code></td>
<td>
<p>A list with additional information.</p>
</td></tr>
<tr><td><code id="MDP_+3A_name">name</code></td>
<td>
<p>a string to identify the MDP problem.</p>
</td></tr>
<tr><td><code id="MDP_+3A_x">x</code></td>
<td>
<p>a <code>MDP</code> object.</p>
</td></tr>
<tr><td><code id="MDP_+3A_stop">stop</code></td>
<td>
<p>logical; stop with an error.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Markov decision processes (MDPs) are discrete-time stochastic control
process with completely observable states. We implement here
MDPs with a finite state space. similar to <a href="#topic+POMDP">POMDP</a>
models, but without the observation model. The <code>'observations'</code> column in
the the reward specification is always missing.
</p>
<p><code><a href="#topic+make_partially_observable">make_partially_observable()</a></code> reformulates an MDP as a POMDP by adding an observation
model with one observation per state
that reveals the current state. This is achieved by adding identity
observation probability matrices.
</p>
<p>More details on specifying the model components can be found in the documentation
for <a href="#topic+POMDP">POMDP</a>.
</p>


<h3>Value</h3>

<p>The function returns an object of class MDP which is list with
the model specification. <code><a href="#topic+solve_MDP">solve_MDP()</a></code> reads the object and adds a list element called
<code>'solution'</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other MDP_examples: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+Maze">Maze</a></code>,
<code><a href="#topic+Windy_gridworld">Windy_gridworld</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Michael's Sleepy Tiger Problem is like the POMDP Tiger problem, but
# has completely observable states because the tiger is sleeping in front
# of the door. This makes the problem an MDP.

STiger &lt;- MDP(
  name = "Michael's Sleepy Tiger Problem",
  discount = .9,

  states = c("tiger-left" , "tiger-right"),
  actions = c("open-left", "open-right", "do-nothing"),
  start = "uniform",

  # opening a door resets the problem
  transition_prob = list(
    "open-left" =  "uniform",
    "open-right" = "uniform",
    "do-nothing" = "identity"),

  # the reward helper R_() expects: action, start.state, end.state, observation, value
  reward = rbind(
    R_("open-left",  "tiger-left",  v = -100),
    R_("open-left",  "tiger-right", v =   10),
    R_("open-right", "tiger-left",  v =   10),
    R_("open-right", "tiger-right", v = -100),
    R_("do-nothing",                v =    0)
  )
)

STiger

sol &lt;- solve_MDP(STiger)
sol

policy(sol)
plot_value_function(sol)

# convert the MDP into a POMDP and solve
STiger_POMDP &lt;- make_partially_observable(STiger)
sol2 &lt;- solve_POMDP(STiger_POMDP)
sol2

policy(sol2)
plot_value_function(sol2, ylim = c(80, 120))
</code></pre>

<hr>
<h2 id='MDP_policy_functions'>Functions for MDP Policies</h2><span id='topic+MDP_policy_functions'></span><span id='topic+q_values_MDP'></span><span id='topic+MDP_policy_evaluation'></span><span id='topic+greedy_MDP_action'></span><span id='topic+random_MDP_policy'></span><span id='topic+manual_MDP_policy'></span><span id='topic+greedy_MDP_policy'></span>

<h3>Description</h3>

<p>Implementation several functions useful to deal with MDP policies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q_values_MDP(model, U = NULL)

MDP_policy_evaluation(
  pi,
  model,
  U = NULL,
  k_backups = 1000,
  theta = 0.001,
  verbose = FALSE
)

greedy_MDP_action(s, Q, epsilon = 0, prob = FALSE)

random_MDP_policy(model, prob = NULL)

manual_MDP_policy(model, actions)

greedy_MDP_policy(Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDP_policy_functions_+3A_model">model</code></td>
<td>
<p>an MDP problem specification.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_u">U</code></td>
<td>
<p>a vector with value function representing the state utilities
(expected sum of discounted rewards from that point on).
If <code>model</code> is a solved model, then the state
utilities are taken from the solution.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_pi">pi</code></td>
<td>
<p>a policy as a data.frame with at least columns for states and action.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_k_backups">k_backups</code></td>
<td>
<p>number of look ahead steps used for approximate policy evaluation
used by the policy iteration method. Set k_backups to <code>Inf</code> to only use
<code class="reqn">\theta</code> as the stopping criterion.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_theta">theta</code></td>
<td>
<p>stop when the largest change in a state value is less
than <code class="reqn">\theta</code>.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_verbose">verbose</code></td>
<td>
<p>logical; should progress and approximation errors be printed.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_s">s</code></td>
<td>
<p>a state.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_q">Q</code></td>
<td>
<p>an action value function with Q-values as a state by action matrix.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_epsilon">epsilon</code></td>
<td>
<p>an <code>epsilon &gt; 0</code> applies an epsilon-greedy policy.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_prob">prob</code></td>
<td>
<p>probability vector for random actions for <code>random_MDP_policy()</code>.
a logical indicating if action probabilities should be returned for
<code>greedy_MDP_action()</code>.</p>
</td></tr>
<tr><td><code id="MDP_policy_functions_+3A_actions">actions</code></td>
<td>
<p>a vector with the action (either the action label or the
numeric id) for each state.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Implemented functions are:
</p>

<ul>
<li> <p><code>q_values_MDP()</code> calculates (approximates)
Q-values for a given model using the Bellman
optimality equation:
</p>
<p style="text-align: center;"><code class="reqn">q(s,a) = \sum_{s'} T(s'|s,a) [R(s,a) + \gamma U(s')]</code>
</p>

<p>Q-values can be used as the input for several other functions.
</p>
</li>
<li> <p><code>MDP_policy_evaluation()</code> evaluates a policy <code class="reqn">\pi</code> for a model and returns
(approximate) state values by applying the Bellman equation as an update
rule for each state and iteration <code class="reqn">k</code>:
</p>
<p style="text-align: center;"><code class="reqn">U_{k+1}(s) =\sum_a \pi{a|s} \sum_{s'} T(s' | s,a) [R(s,a) + \gamma U_k(s')]</code>
</p>

<p>In each iteration, all states are updated. Updating is stopped after
<code>k_backups</code> iterations or after the
largest update <code class="reqn">||U_{k+1} - U_k||_\infty &lt; \theta</code>.
</p>
</li>
<li> <p><code>greedy_MDP_action()</code> returns the action with the largest Q-value given a
state.
</p>
</li>
<li> <p><code>random_MDP_policy()</code>, <code>manual_MDP_policy()</code>, and <code>greedy_MDP_policy()</code>
generates different policies. These policies can be added to a problem
using <code><a href="#topic+add_policy">add_policy()</a></code>.
</p>
</li></ul>



<h3>Value</h3>

<p><code>q_values_MDP()</code> returns a state by action matrix specifying the Q-function,
i.e., the action value for executing each action in each state. The Q-values
are calculated from the value function (U) and the transition model.
</p>
<p><code>MDP_policy_evaluation()</code> returns a vector with (approximate)
state values (U).
</p>
<p><code>greedy_MDP_action()</code> returns the action with the highest q-value
for state <code>s</code>. If <code>prob = TRUE</code>, then a vector with
the probability for each action is returned.
</p>
<p><code>random_MDP_policy()</code> returns a data.frame with the columns state and action to define a policy.
</p>
<p><code>manual_MDP_policy()</code> returns a data.frame with the columns state and action to define a policy.
</p>
<p><code>greedy_MDP_policy()</code> returns the greedy policy given <code>Q</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Maze)
Maze

# create several policies:
# 1. optimal policy using value iteration
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration")
maze_solved
pi_opt &lt;- policy(maze_solved)
pi_opt
gridworld_plot_policy(add_policy(Maze, pi_opt), main = "Optimal Policy")

# 2. a manual policy (go up and in some squares to the right)
acts &lt;- rep("up", times = length(Maze$states))
names(acts) &lt;- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] &lt;- "right"
pi_manual &lt;- manual_MDP_policy(Maze, acts)
pi_manual
gridworld_plot_policy(add_policy(Maze, pi_manual), main = "Manual Policy")

# 3. a random policy
set.seed(1234)
pi_random &lt;- random_MDP_policy(Maze)
pi_random
gridworld_plot_policy(add_policy(Maze, pi_random), main = "Random Policy")

# 4. an improved policy based on one policy evaluation and
#   policy improvement step.
u &lt;- MDP_policy_evaluation(pi_random, Maze)
q &lt;- q_values_MDP(Maze, U = u)
pi_greedy &lt;- greedy_MDP_policy(q)
pi_greedy
gridworld_plot_policy(add_policy(Maze, pi_greedy), main = "Greedy Policy")

#' compare the approx. value functions for the policies (we restrict
#'    the number of backups for the random policy since it may not converge)
rbind(
  random = MDP_policy_evaluation(pi_random, Maze, k_backups = 100),
  manual = MDP_policy_evaluation(pi_manual, Maze),
  greedy = MDP_policy_evaluation(pi_greedy, Maze),
  optimal = MDP_policy_evaluation(pi_opt, Maze)
)

# For many functions, we first add the policy to the problem description
#   to create a "solved" MDP
maze_random &lt;- add_policy(Maze, pi_random)
maze_random

# plotting
plot_value_function(maze_random)
gridworld_plot_policy(maze_random)

# compare to a benchmark
regret(maze_random, benchmark = maze_solved)

# calculate greedy actions for state 1
q &lt;- q_values_MDP(maze_random)
q
greedy_MDP_action(1, q, epsilon = 0, prob = FALSE)
greedy_MDP_action(1, q, epsilon = 0, prob = TRUE)
greedy_MDP_action(1, q, epsilon = .1, prob = TRUE)
</code></pre>

<hr>
<h2 id='MDP2POMDP'>Convert between MDPs and POMDPs</h2><span id='topic+MDP2POMDP'></span><span id='topic+make_partially_observable'></span><span id='topic+make_fully_observable'></span>

<h3>Description</h3>

<p>Convert a MDP into POMDP by adding an observation model or
a POMDP into a MDP by making the states observable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_partially_observable(x, observations = NULL, observation_prob = NULL)

make_fully_observable(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDP2POMDP_+3A_x">x</code></td>
<td>
<p>a <code>MDP</code> object.</p>
</td></tr>
<tr><td><code id="MDP2POMDP_+3A_observations">observations</code></td>
<td>
<p>a character vector specifying the names of the available
observations.</p>
</td></tr>
<tr><td><code id="MDP2POMDP_+3A_observation_prob">observation_prob</code></td>
<td>
<p>Specifies the observation probabilities (see <a href="#topic+POMDP">POMDP</a> for details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>make_partially_observable()</code> adds an observation model to an MDP. If no observations and
observation probabilities are provided, then an observation for each state is created
with identity observation matrices. This means we have a fully observable model
encoded as a POMDP.
</p>
<p><code>make_fully_observable()</code> removes the observation model from a POMDP and returns
an MDP.
</p>


<h3>Value</h3>

<p>a <code>MDP</code> or a <code>POMDP</code> object.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Turn the Maze MDP into a partially observable problem.
# Here each state has an observation, so it is still a fully observable problem
# encoded as a POMDP.
data("Maze")
Maze

Maze_POMDP &lt;- make_partially_observable(Maze)
Maze_POMDP

sol &lt;- solve_POMDP(Maze_POMDP)
policy(sol)
simulate_POMDP(sol, n = 1, horizon = 100, return_trajectories = TRUE)$trajectories

# Make the Tiger POMDP fully observable
data("Tiger")
Tiger

Tiger_MDP &lt;- make_fully_observable(Tiger)
Tiger_MDP

sol &lt;- solve_MDP(Tiger_MDP)
policy(sol)
# The result is not exciting since we can observe where the tiger is!
</code></pre>

<hr>
<h2 id='optimal_action'>Optimal action for a belief</h2><span id='topic+optimal_action'></span>

<h3>Description</h3>

<p>Determines the optimal action for a policy (solved POMDP) for a given belief
at a given epoch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_action(model, belief = NULL, epoch = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimal_action_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="optimal_action_+3A_belief">belief</code></td>
<td>
<p>The belief (probability distribution over the states) as a
vector or a matrix with multiple belief states as rows. If <code>NULL</code>, then the initial belief of the
model is used.</p>
</td></tr>
<tr><td><code id="optimal_action_+3A_epoch">epoch</code></td>
<td>
<p>what epoch of the policy should be used. Use 1 for converged policies.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The name of the optimal action.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
Tiger

sol &lt;- solve_POMDP(model = Tiger)

# these are the states
sol$states

# belief that tiger is to the left
optimal_action(sol, c(1, 0))
optimal_action(sol, "tiger-left")

# belief that tiger is to the right
optimal_action(sol, c(0, 1))
optimal_action(sol, "tiger-right")

# belief is 50/50
optimal_action(sol, c(.5, .5))
optimal_action(sol, "uniform")

# the POMDP is converged, so all epoch give the same result.
optimal_action(sol, "tiger-right", epoch = 10)

</code></pre>

<hr>
<h2 id='plot_belief_space'>Plot a 2D or 3D Projection of the Belief Space</h2><span id='topic+plot_belief_space'></span>

<h3>Description</h3>

<p>Plots the optimal action, the node in the policy graph or the reward for a
given set of belief points on a line (2D) or on a ternary plot (3D). If no
points are given, points are sampled using a regular arrangement or randomly
from the (projected) belief space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_belief_space(
  model,
  projection = NULL,
  epoch = 1,
  sample = "regular",
  n = 100,
  what = c("action", "pg_node", "reward"),
  legend = TRUE,
  pch = 20,
  col = NULL,
  jitter = 0,
  oneD = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_belief_space_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_epoch">epoch</code></td>
<td>
<p>display this epoch.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_sample">sample</code></td>
<td>
<p>a matrix with belief points as rows or a character string
specifying the <code>method</code> used for <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_n">n</code></td>
<td>
<p>number of points sampled.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_what">what</code></td>
<td>
<p>what to plot.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_legend">legend</code></td>
<td>
<p>logical; add a legend? If the legend is covered by the plot then you
need to increase the plotting region of the plotting device.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_pch">pch</code></td>
<td>
<p>plotting symbols.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_col">col</code></td>
<td>
<p>plotting colors.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_jitter">jitter</code></td>
<td>
<p>jitter amount for 2D belief spaces (good values are between 0 and 1, while using <code>ylim = c(0,1)</code>).</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_oned">oneD</code></td>
<td>
<p>plot projections on two states in one dimension.</p>
</td></tr>
<tr><td><code id="plot_belief_space_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to <code>plot</code> for 2D or
<code>TerneryPlot</code> for 3D.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns invisibly the sampled points.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># two-state POMDP
data("Tiger")
sol &lt;- solve_POMDP(Tiger)

plot_belief_space(sol)
plot_belief_space(sol, oneD = FALSE)
plot_belief_space(sol, n = 10)
plot_belief_space(sol, n = 100, sample = "random")

# plot the belief points used by the grid-based solver
plot_belief_space(sol, sample = sol $solution$belief_points_solver)

# plot different measures
plot_belief_space(sol, what = "pg_node")
plot_belief_space(sol, what = "reward")

# three-state POMDP
# Note: If the plotting region is too small then the legend might run into the plot
data("Three_doors")
sol &lt;- solve_POMDP(Three_doors)
sol

# plotting needs the suggested package Ternary
if ("Ternary" %in% installed.packages()) {
plot_belief_space(sol)
plot_belief_space(sol, n = 10000)
plot_belief_space(sol, what = "reward", sample = "random", n = 1000)
plot_belief_space(sol, what = "pg_node", n = 10000)

# holding tiger-left constant at .5 follows this line in the ternary plot 
Ternary::TernaryLines(list(c(.5, 0, .5), c(.5, .5, 0)), col = "black", lty = 2)
# we can plot the projection for this line 
plot_belief_space(sol, what = "pg_node", n = 1000, projection = c("tiger-left" = .5))

# plot the belief points used by the grid-based solver
plot_belief_space(sol, sample = sol$solution$belief_points_solver, what = "pg_node")

# plot the belief points obtained using simulated trajectories with an epsilon-greedy policy.
# Note that we only use n = 50 to save time.
plot_belief_space(sol, 
  sample = simulate_POMDP(sol, n = 50, horizon = 100,
    epsilon = 0.1, return_beliefs = TRUE)$belief_states)
}

# plot a 3-state belief space using ggtern (ggplot2)
## Not run: 
library(ggtern)
samp &lt;- sample_belief_space(sol, n = 1000)
df &lt;- cbind(as.data.frame(samp), reward_node_action(sol, belief = samp))
df$pg_node &lt;- factor(df$pg_node)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = pg_node), size = 2)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = action), size = 2)

ggtern(df, aes(x = `tiger-left`, y = `tiger-center`, z = `tiger-right`)) +
  geom_point(aes(color = reward), size = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_policy_graph'>POMDP Plot Policy Graphs</h2><span id='topic+plot_policy_graph'></span><span id='topic+curve_multiple_directed'></span>

<h3>Description</h3>

<p>The function plots the POMDP policy graph for converged POMDP solution and the
policy tree for a finite-horizon solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_policy_graph(
  x,
  belief = NULL,
  engine = c("igraph", "visNetwork"),
  show_belief = TRUE,
  state_col = NULL,
  legend = TRUE,
  simplify_observations = TRUE,
  remove_unreachable_nodes = TRUE,
  ...
)

curve_multiple_directed(graph, start = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_policy_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_belief">belief</code></td>
<td>
<p>the initial belief is used to mark the initial belief state in the
graph of a converged solution and to identify the root node in a policy graph for a finite-horizon solution.
If <code>NULL</code> then the belief is taken from the model definition.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_engine">engine</code></td>
<td>
<p>The plotting engine to be used.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_show_belief">show_belief</code></td>
<td>
<p>logical; show estimated belief proportions as a pie chart or color in each node?</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the belief over states in each node. Only used if <code>show_belief</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_legend">legend</code></td>
<td>
<p>logical; display a legend for colors used belief proportions?</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_simplify_observations">simplify_observations</code></td>
<td>
<p>combine parallel observation arcs into a single arc.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_remove_unreachable_nodes">remove_unreachable_nodes</code></td>
<td>
<p>logical; remove nodes that are not reachable from the start state? Currently only implemented for policy trees for unconverged finite-time horizon POMDPs.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code>policy_graph()</code>, <code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code> and the functions
they use. Also, plotting options are passed on to the plotting engine <code><a href="igraph.html#topic+plot.igraph">igraph::plot.igraph()</a></code>
or <code><a href="visNetwork.html#topic+visNetwork-igraph">visNetwork::visIgraph()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_graph">graph</code></td>
<td>
<p>The input graph.</p>
</td></tr>
<tr><td><code id="plot_policy_graph_+3A_start">start</code></td>
<td>
<p>The curvature at the two extreme edges.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The policy graph returned by <code><a href="#topic+policy_graph">policy_graph()</a></code> can be directly plotted. <code>plot_policy_graph()</code>
uses <code>policy_graph()</code> to get the policy graph and produces an
improved visualization (a legend, tree layout for finite-horizon solutions, better edge curving, etc.).
It also offers an interactive visualization using <code><a href="visNetwork.html#topic+visNetwork-igraph">visNetwork::visIgraph()</a></code>.
</p>
<p>Each policy graph node is represented by an alpha vector specifying a hyper plane segment. The convex hull of
the set of hyperplanes represents the the value function.
The policy specifies for each node an optimal action which is printed together with the node ID inside the node.
The arcs are labeled with observations.
Infinite-horizon converged solutions from a single policy graph.
For finite-horizon solution a policy tree is produced.
The levels of the tree and the first number in the node label represent the epochs.
</p>
<p>For better visualization, we provide a few features:
</p>

<ul>
<li><p> Show Belief, belief color and legend: A pie chart (or the color) in each node can be used
represent an example of the belief that the agent has if it is in this node.
This can help with interpreting the policy graph. The belief is obtained by calling
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code>.
</p>
</li>
<li><p> Simplify observations: In some cases, two observations can lead to the same node resulting in two parallel edges.
These edges can be collapsed into one labels with the observations.
</p>
</li>
<li><p> Remove unreachable nodes: Many algorithms produce
unused policy graph nodes which can be filtered to produce a smaller tree structure of actually used nodes.
Non-converged policies depend on the initial belief and if an initial belief is
specified, then different nodes will be filtered and the tree will look different.
</p>
</li></ul>

<p>These improvements can be disabled using parameters.
</p>


<h4>Auxiliary function</h4>

<p><code>curve_multiple_directed()</code> is a helper function for plotting igraph graphs similar to <code>igraph::curve_multiple()</code> but
it also adds curvature to parallel edges that point in opposite directions.
</p>



<h3>Value</h3>

<p>returns invisibly what the plotting engine returns.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

### Policy graphs for converged solutions
sol &lt;- solve_POMDP(model = Tiger)
sol

policy_graph(sol)

## visualization
plot_policy_graph(sol)

## use a different graph layout (circle and manual; needs igraph)
library("igraph")
plot_policy_graph(sol, layout = layout.circle)
plot_policy_graph(sol, layout = rbind(c(1,1), c(1,-1), c(0,0), c(-1,-1), c(-1,1)), margin = .2)
plot_policy_graph(sol,
  layout = rbind(c(1,0), c(.5,0), c(0,0), c(-.5,0), c(-1,0)), rescale = FALSE,
  vertex.size = 15, edge.curved = 2,
  main = "Tiger Problem")

## hide labels, beliefs and legend
plot_policy_graph(sol, show_belief = FALSE, edge.label = NA, vertex.label = NA, legend = FALSE)

## custom larger vertex labels (A, B, ...)
plot_policy_graph(sol,
  vertex.label = LETTERS[1:nrow(policy(sol))],
  vertex.size = 60,
  vertex.label.cex = 2,
  edge.label.cex = .7,
  vertex.label.color = "white")

## plotting the igraph object directly
pg &lt;- policy_graph(sol, show_belief = TRUE, 
  simplify_observations = TRUE, remove_unreachable_nodes = TRUE)

## (e.g., using a tree layout)
plot(pg, layout = layout_as_tree(pg, root = 3, mode = "out"))

## change labels (abbreviate observations and use only actions to label the vertices)
plot(pg,
  edge.label = abbreviate(E(pg)$label),
  vertex.label = V(pg)$action,
  vertex.size = 20)

## use action to color vertices (requires a graph without a belief pie chart) 
##    and color edges to represent observations.
pg &lt;- policy_graph(sol, show_belief = FALSE, 
  simplify_observations = TRUE, remove_unreachable_nodes = TRUE)

plot(pg,
  vertex.label = NA,
  vertex.color = factor(V(pg)$action),
  vertex.size = 20,
  edge.color = factor(E(pg)$observation),
  edge.curved = .1
  )

acts &lt;- levels(factor(V(pg)$action))
legend("topright", legend = acts, title = "action",
  col = igraph::categorical_pal(length(acts)), pch = 15)
obs &lt;- levels(factor(E(pg)$observation))
legend("bottomright", legend = obs, title = "observation",
  col = igraph::categorical_pal(length(obs)), lty = 1) 

## plot interactive graphs using the visNetwork library.
## Note: the pie chart representation is not available, but colors are used instead.
plot_policy_graph(sol, engine = "visNetwork")

## add smooth edges and a layout (note, engine can be abbreviated)
plot_policy_graph(sol, engine = "visNetwork", layout = "layout_in_circle", smooth = TRUE)


### Policy trees for finite-horizon solutions
sol &lt;- solve_POMDP(model = Tiger, horizon = 4, method = "incprune")

policy_graph(sol)

plot_policy_graph(sol)
# Note: the first number in the node id is the epoch.

# plot the policy tree for an initial belief of 90% that the tiger is to the left
plot_policy_graph(sol, belief = c(0.9, 0.1))

# Plotting a larger graph (see ? igraph.plotting for plotting options)
sol &lt;- solve_POMDP(model = Tiger, horizon = 10, method = "incprune")

plot_policy_graph(sol, edge.arrow.size = .1,
  vertex.label.cex = .5, edge.label.cex = .5)

plot_policy_graph(sol, engine = "visNetwork")
</code></pre>

<hr>
<h2 id='policy'>Extract the Policy from a POMDP/MDP</h2><span id='topic+policy'></span>

<h3>Description</h3>

<p>Extracts the policy from a solved POMDP/MDP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy(x, drop = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_+3A_x">x</code></td>
<td>
<p>A solved <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a> object.</p>
</td></tr>
<tr><td><code id="policy_+3A_drop">drop</code></td>
<td>
<p>logical; drop the list for converged, epoch-independent policies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A list (one entry per epoch) with the optimal policy.
For converged, infinite-horizon problems solutions, a list with only the
converged solution is produced.
For a POMDP, the policy is a data.frame consisting of:
</p>

<ul>
<li><p> Part 1: The alpha vectors for the belief states (defines also the
utility of the belief). The columns have
the names of states.
</p>
</li>
<li><p> Part 2: The last column named <code>action</code> contains the prescribed action.
</p>
</li></ul>

<p>For an MDP, the policy is a data.frame with columns for:
</p>

<ul>
<li> <p><code>state</code>: The state.
</p>
</li>
<li> <p><code>U</code>: The state's value (discounted expected utility U) if the policy
is followed
</p>
</li>
<li> <p><code>action</code>: The prescribed action.
</p>
</li></ul>



<h3>Value</h3>

<p>A list with the policy for each epoch. Converged policies
have only one element. If <code>drop = TRUE</code> then the policy is returned
without a list.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# Infinite horizon
sol &lt;- solve_POMDP(model = Tiger)
sol

# policy with value function, optimal action and transitions for observations.
policy(sol)
plot_value_function(sol)

# Finite horizon (we use incremental pruning because grid does not converge)
sol &lt;- solve_POMDP(model = Tiger, method = "incprune", 
  horizon = 3, discount = 1)
sol

policy(sol)
# Note: We see that it is initially better to listen till we make 
#       a decision in the final epoch.

# MDP policy
data(Maze)

sol &lt;- solve_MDP(Maze)

policy(sol)
</code></pre>

<hr>
<h2 id='policy_graph'>POMDP Policy Graphs</h2><span id='topic+policy_graph'></span>

<h3>Description</h3>

<p>The function creates a POMDP policy graph for converged POMDP solution and the
policy tree for a finite-horizon solution.
The graph is represented as an <span class="pkg">igraph</span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_graph(
  x,
  belief = NULL,
  show_belief = FALSE,
  state_col = NULL,
  simplify_observations = FALSE,
  remove_unreachable_nodes = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> containing a solved and converged POMDP problem.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_belief">belief</code></td>
<td>
<p>the initial belief is used to mark the initial belief state in the
grave of a converged solution and to identify the root node in a policy graph for a finite-horizon solution.
If <code>NULL</code> then the belief is taken from the model definition.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_show_belief">show_belief</code></td>
<td>
<p>logical; show estimated belief proportions as a pie chart or color in each node?</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the belief over the states in each node. Only used if <code>show_belief</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_simplify_observations">simplify_observations</code></td>
<td>
<p>combine parallel observation arcs into a single arc.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_remove_unreachable_nodes">remove_unreachable_nodes</code></td>
<td>
<p>logical; remove nodes that are not reachable from the start state? Currently only implemented for policy trees for unconverged finite-time horizon POMDPs.</p>
</td></tr>
<tr><td><code id="policy_graph_+3A_...">...</code></td>
<td>
<p>parameters are passed on to <code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each policy graph node is represented by an alpha vector specifying a hyper plane segment. The convex hull of
the set of hyperplanes represents the the value function.
The policy specifies for each node an optimal action which is printed together with the node ID inside the node.
The arcs are labeled with observations.
Infinite-horizon converged solutions from a single policy graph.
For finite-horizon solution a policy tree is produced.
The levels of the tree and the first number in the node label represent the epochs.
</p>
<p>The parameters <code>show_belief</code>, <code>remove_unreachable_nodes</code>, and <code>simplify_observations</code> are
used by <code><a href="#topic+plot_policy_graph">plot_policy_graph()</a></code> (see there for details) to reduce clutter and make the visualization more readable.
These options are disabled by default for <code>policy_graph()</code>.
</p>


<h3>Value</h3>

<p>returns the policy graph as an igraph object.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

### Policy graphs for converged solutions
sol &lt;- solve_POMDP(model = Tiger)
sol

policy_graph(sol)

## visualization
plot_policy_graph(sol)

### Policy trees for finite-horizon solutions
sol &lt;- solve_POMDP(model = Tiger, horizon = 4, method = "incprune")

policy_graph(sol)
plot_policy_graph(sol)
# Note: the first number in the node id is the epoch.
</code></pre>

<hr>
<h2 id='POMDP'>Define a POMDP Problem</h2><span id='topic+POMDP'></span><span id='topic+is_solved_POMDP'></span><span id='topic+is_timedependent_POMDP'></span><span id='topic+epoch_to_episode'></span><span id='topic+is_converged_POMDP'></span><span id='topic+O_'></span><span id='topic+T_'></span><span id='topic+R_'></span>

<h3>Description</h3>

<p>Defines all the elements of a POMDP problem including the discount rate, the
set of states, the set of actions, the set of observations, the transition
probabilities, the observation probabilities, and rewards.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>POMDP(
  states,
  actions,
  observations,
  transition_prob,
  observation_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  terminal_values = NULL,
  start = "uniform",
  info = NULL,
  name = NA
)

is_solved_POMDP(x, stop = FALSE, message = "")

is_timedependent_POMDP(x)

epoch_to_episode(x, epoch)

is_converged_POMDP(x, stop = FALSE, message = "")

O_(action = NA, end.state = NA, observation = NA, probability)

T_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, observation = NA, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="POMDP_+3A_states">states</code></td>
<td>
<p>a character vector specifying the names of the states. Note that
state names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_actions">actions</code></td>
<td>
<p>a character vector specifying the names of the available actions.
Note that action names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_observations">observations</code></td>
<td>
<p>a character vector specifying the names of the
observations. Note that observation names have to start with a letter.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_transition_prob">transition_prob</code></td>
<td>
<p>Specifies action-dependent transition probabilities
between states.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_observation_prob">observation_prob</code></td>
<td>
<p>Specifies the probability that an action/state
combination produces an observation.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_reward">reward</code></td>
<td>
<p>Specifies the rewards structure dependent on action, states
and observations.  See Details section.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_discount">discount</code></td>
<td>
<p>numeric; discount factor between 0 and 1.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_horizon">horizon</code></td>
<td>
<p>numeric; Number of epochs. <code>Inf</code> specifies an infinite
horizon.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_terminal_values">terminal_values</code></td>
<td>
<p>a vector with the terminal values for each state or a
matrix specifying the terminal rewards via a terminal value function (e.g.,
the alpha component produced by <code>solve_POMDP()</code>).  A single 0 specifies that all
terminal values are zero.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_start">start</code></td>
<td>
<p>Specifies the initial belief state of the agent. A vector with the
probability for each state is supplied. Also the string <code>'uniform'</code>
(default) can be used.  The belief is used to calculate the total expected cumulative
reward. It is also used by some solvers. See Details section for more
information.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_info">info</code></td>
<td>
<p>A list with additional information.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_name">name</code></td>
<td>
<p>a string to identify the POMDP problem.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_x">x</code></td>
<td>
<p>a POMDP.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_stop">stop</code></td>
<td>
<p>logical; stop with an error.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_message">message</code></td>
<td>
<p>a error message to be displayed displayed</p>
</td></tr>
<tr><td><code id="POMDP_+3A_epoch">epoch</code></td>
<td>
<p>integer; an epoch that should be converted to the
corresponding episode in a time-dependent POMDP.</p>
</td></tr>
<tr><td><code id="POMDP_+3A_action">action</code>, <code id="POMDP_+3A_start.state">start.state</code>, <code id="POMDP_+3A_end.state">end.state</code>, <code id="POMDP_+3A_observation">observation</code>, <code id="POMDP_+3A_probability">probability</code>, <code id="POMDP_+3A_value">value</code></td>
<td>
<p>Values
used in the helper functions <code>O_()</code>, <code>R_()</code>, and <code>T_()</code> to
create an entry for <code>observation_prob</code>, <code>reward</code>, or
<code>transition_prob</code> above, respectively. The default value <code>'*"'</code>
matches any action/state/observation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the following we use the following notation. The POMDP is a 7-duple:
</p>
<p><code class="reqn">(S,A,T,R, \Omega ,O, \gamma)</code>.
</p>
<p><code class="reqn">S</code> is the set of states; <code class="reqn">A</code>
is the set of actions; <code class="reqn">T</code> are the conditional transition probabilities
between states; <code class="reqn">R</code> is the reward function; <code class="reqn">\Omega</code> is the set of
observations; <code class="reqn">O</code> are the conditional observation probabilities; and
<code class="reqn">\gamma</code> is the discount factor. We will use lower case letters to
represent a member of a set, e.g., <code class="reqn">s</code> is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
<code class="reqn">|A|</code>.
</p>
<p>Note that the observation model is in the literature
often also denoted by the letter <code class="reqn">Z</code>.
</p>
<p><strong>Names used for mathematical symbols in code</strong>
</p>

<ul>
<li> <p><code class="reqn">S, s, s'</code>: <code style="white-space: pre;">&#8288;'states', start.state', 'end.state'&#8288;</code>
</p>
</li>
<li> <p><code class="reqn">A, a</code>: <code style="white-space: pre;">&#8288;'actions', 'action'&#8288;</code>
</p>
</li>
<li> <p><code class="reqn">\Omega, o</code>: <code style="white-space: pre;">&#8288;'observations', 'observation'&#8288;</code>
</p>
</li></ul>

<p>State names, actions and observations can be specified as strings or index numbers
(e.g., <code>start.state</code> can be specified as the index of the state in <code>states</code>).
For the specification as data.frames below, <code>NA</code> can be used to mean
any  <code>start.state</code>, <code>end.state</code>, <code>action</code> or <code>observation</code>. Note that some POMDP solvers and the POMDP
file format use <code>'*'</code> for this purpose.
</p>
<p>The specification below map to the format used by pomdp-solve
(see <a href="http://www.pomdp.org">http://www.pomdp.org</a>).
</p>
<p><strong>Specification of transition probabilities: <code class="reqn">T(s' | s, a)</code></strong>
</p>
<p>Transition probability to transition to state <code class="reqn">s'</code> from given state <code class="reqn">s</code>
and action <code class="reqn">a</code>. The transition probabilities can be
specified in the following ways:
</p>

<ul>
<li><p> A data.frame with columns exactly like the arguments of <code>T_()</code>.
You can use <code>rbind()</code> with helper function <code>T_()</code> to create this data
frame. Probabilities can be specified multiple times and the definition that
appears last in the data.frame will take affect.
</p>
</li>
<li><p> A named list of matrices, one for each action. Each matrix is square with
rows representing start states <code class="reqn">s</code> and columns representing end states <code class="reqn">s'</code>.
Instead of a matrix, also the strings <code>'identity'</code> or <code>'uniform'</code> can be specified.
</p>
</li>
<li><p> A function with the same arguments are <code>T_()</code>, but no default values
that returns the transition probability.
</p>
</li></ul>

<p><strong>Specification of observation probabilities: <code class="reqn">O(o | a, s')</code></strong>
</p>
<p>The POMDP specifies the probability for each observation <code class="reqn">o</code> given an
action <code class="reqn">a</code> and that the system transitioned to the end state
<code class="reqn">s'</code>. These probabilities can be specified in the
following ways:
</p>

<ul>
<li><p> A data frame with columns named exactly like the arguments of <code>O_()</code>.
You can use <code>rbind()</code>
with helper function <code>O_()</code> to create this data frame. Probabilities can be
specified multiple times and the definition that
appears last in the data.frame will take affect.
</p>
</li>
<li><p> A named list of matrices, one for each action. Each matrix has
rows representing end states <code class="reqn">s'</code> and columns representing an observation <code class="reqn">o</code>.
Instead of a matrix, also the string <code>'uniform'</code> can be specified.
</p>
</li>
<li><p> A function with the same arguments are <code>O_()</code>, but no default values
that returns the observation probability.
</p>
</li></ul>

<p><strong>Specification of the reward function: <code class="reqn">R(a, s, s', o)</code></strong>
</p>
<p>The reward function can be specified in the following
ways:
</p>

<ul>
<li><p> A data frame with columns named exactly like the arguments of <code>R_()</code>.
You can use <code>rbind()</code>
with helper function <code>R_()</code> to create this data frame. Rewards can be specified
multiple times and the definition that
appears last in the data.frame will take affect.
</p>
</li>
<li><p> A list of lists. The list levels are <code>'action'</code> and <code>'start.state'</code>. The list elements
are matrices with
rows representing end states <code class="reqn">s'</code> and columns representing an observation <code class="reqn">o</code>.
</p>
</li>
<li><p> A function with the same arguments are <code>R_()</code>, but no default values
that returns the reward.
</p>
</li></ul>

<p>To avoid overflow problems with rewards, reward values should stay well within the
range of
<code style="white-space: pre;">&#8288;[-1e10, +1e10]&#8288;</code>. <code>-Inf</code> can be used as the reward for unavailable actions and
will be translated into a large negative reward for solvers that only support
finite reward values.
</p>
<p><strong>Start Belief</strong>
</p>
<p>The initial belief state of the agent is a distribution over the states. It is used to calculate the
total expected cumulative reward printed with the solved model. The function <code><a href="#topic+reward">reward()</a></code> can be
used to calculate rewards for any belief.
</p>
<p>Some methods use this belief to decide which belief states to explore (e.g.,
the finite grid method).
</p>
<p>Options to specify the start belief state are:
</p>

<ul>
<li><p> A probability distribution over the states. That is, a vector
of <code class="reqn">|S|</code> probabilities, that add up to <code class="reqn">1</code>.
</p>
</li>
<li><p> The string <code>"uniform"</code> for a uniform
distribution over all states.
</p>
</li>
<li><p> An integer in the range <code class="reqn">1</code> to <code class="reqn">n</code> to specify the index of a single starting state.
</p>
</li>
<li><p> A string specifying the name of a single starting state.
</p>
</li></ul>

<p>The default initial belief is a uniform
distribution over all states.
</p>
<p><strong>Convergence</strong>
</p>
<p>A infinite-horizon POMDP needs to converge to provide a valid value
function and policy.
</p>
<p>A finite-horizon POMDP may also converging to a infinite horizon solution
if the horizon is long enough.
</p>
<p><strong>Time-dependent POMDPs</strong>
</p>
<p>Time dependence of transition probabilities, observation probabilities and
reward structure can be modeled by considering a set of <strong>episodes</strong>
representing <strong>epoch</strong> with the same settings. The length of each episode is
specified as a vector for <code>horizon</code>, where the length is the number of
episodes and each value is the length of the episode in epochs. Transition
probabilities, observation probabilities and/or reward structure can contain
a list with the values for each episode. The helper function <code>epoch_to_episode()</code> converts
an epoch to the episode it belongs to.
</p>


<h3>Value</h3>

<p>The function returns an object of class POMDP which is list of the model specification.
<code><a href="#topic+solve_POMDP">solve_POMDP()</a></code> reads the object and adds a list element named
<code>'solution'</code>.
</p>


<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>pomdp-solve website: <a href="http://www.pomdp.org">http://www.pomdp.org</a>
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other POMDP_examples: 
<code><a href="#topic+POMDP_example_files">POMDP_example_files</a></code>,
<code><a href="#topic+RussianTiger">RussianTiger</a></code>,
<code><a href="#topic+Tiger">Tiger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Defining the Tiger Problem (it is also available via data(Tiger), see ? Tiger)

Tiger &lt;- POMDP(
  name = "Tiger Problem",
  discount = 0.75,
  states = c("tiger-left" , "tiger-right"),
  actions = c("listen", "open-left", "open-right"),
  observations = c("tiger-left", "tiger-right"),
  start = "uniform",

  transition_prob = list(
    "listen" =     "identity",
    "open-left" =  "uniform",
    "open-right" = "uniform"
  ),

  observation_prob = list(
    "listen" = rbind(c(0.85, 0.15),
                     c(0.15, 0.85)),
    "open-left" =  "uniform",
    "open-right" = "uniform"
  ),

  # the reward helper expects: action, start.state, end.state, observation, value
  # missing arguments default to NA which matches any value (often denoted as * in POMDPs).
  reward = rbind(
    R_("listen",                    v =   -1),
    R_("open-left",  "tiger-left",  v = -100),
    R_("open-left",  "tiger-right", v =   10),
    R_("open-right", "tiger-left",  v =   10),
    R_("open-right", "tiger-right", v = -100)
  )
)

Tiger

### Defining the Tiger problem using functions

trans_f &lt;- function(action, start.state, end.state) {
  if(action == 'listen')
    if(end.state == start.state) return(1)
    else return(0)

  return(1/2) ### all other actions have a uniform distribution
}

obs_f &lt;- function(action, end.state, observation) {
  if(action == 'listen')
    if(end.state == observation) return(0.85)
  else return(0.15)

  return(1/2)
}

rew_f &lt;- function(action, start.state, end.state, observation) {
  if(action == 'listen') return(-1)
  if(action == 'open-left' &amp;&amp; start.state == 'tiger-left') return(-100)
  if(action == 'open-left' &amp;&amp; start.state == 'tiger-right') return(10)
  if(action == 'open-right' &amp;&amp; start.state == 'tiger-left') return(10)
  if(action == 'open-right' &amp;&amp; start.state == 'tiger-right') return(-100)
  stop('Not possible')
}

Tiger_func &lt;- POMDP(
  name = "Tiger Problem",
  discount = 0.75,
  states = c("tiger-left" , "tiger-right"),
  actions = c("listen", "open-left", "open-right"),
  observations = c("tiger-left", "tiger-right"),
  start = "uniform",
  transition_prob = trans_f,
  observation_prob = obs_f,
  reward = rew_f
)

Tiger_func

# Defining a Time-dependent version of the Tiger Problem called Scared Tiger

# The tiger reacts normally for 3 epochs (goes randomly two one
# of the two doors when a door was opened). After 3 epochs he gets
# scared and when a door is opened then he always goes to the other door.

# specify the horizon for each of the two different episodes
Tiger_time_dependent &lt;- Tiger
Tiger_time_dependent$name &lt;- "Scared Tiger Problem"
Tiger_time_dependent$horizon &lt;- c(normal_tiger = 3, scared_tiger = 3)
Tiger_time_dependent$transition_prob &lt;- list(
  normal_tiger = list(
    "listen" = "identity",
    "open-left" = "uniform",
    "open-right" = "uniform"),
  scared_tiger = list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )
)
</code></pre>

<hr>
<h2 id='POMDP_example_files'>POMDP Example Files</h2><span id='topic+POMDP_example_files'></span>

<h3>Description</h3>

<p>Some POMDP example files are shipped with the package.
</p>


<h3>Details</h3>

<p>Currently, the following POMDP example files are available:
</p>

<ul>
<li> <p><code>"light_maze.POMDP"</code>: a simple maze introduced in Littman (2009).
</p>
</li>
<li> <p><code>"shuttle_95.POMDP"</code>: Transport goods between two space
stations (Chrisman, 1992).
</p>
</li>
<li> <p><code>"tiger_aaai.POMDP"</code>: Tiger Problem introduced in Cassandra et al (1994).
</p>
</li></ul>

<p>More files can be found at https://www.pomdp.org/examples/
</p>


<h3>References</h3>

<p>Anthony R. Cassandra, Leslie P Kaelbling, and Michael L. Littman (1994).
Acting Optimally in Partially Observable Stochastic Domains.
<em>In Proceedings of the Twelfth National Conference on Artificial
Intelligence,</em> pp. 1023-1028.
</p>
<p>Lonnie Chrisman (1992), Reinforcement Learning with Perceptual Aliasing: The
<em>Proceedings of the AAAI Conference on Artificial Intelligence,</em>
10, AAAI-92.
</p>
<p>Michael L. Littman (2009), A tutorial on partially observable Markov decision processes,
<em>Journal of Mathematical Psychology,</em> Volume 53, Issue 3, June 2009, Pages 119-125.
<a href="https://doi.org/10.1016/j.jmp.2009.01.005">doi:10.1016/j.jmp.2009.01.005</a>
</p>


<h3>See Also</h3>

<p>Other POMDP_examples: 
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+RussianTiger">RussianTiger</a></code>,
<code><a href="#topic+Tiger">Tiger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir(system.file("examples/", package = "pomdp"))

model &lt;- read_POMDP(system.file("examples/light_maze.POMDP", 
  package = "pomdp"))
model
</code></pre>

<hr>
<h2 id='projection'>Defining a Belief Space Projection</h2><span id='topic+projection'></span>

<h3>Description</h3>

<p>High dimensional belief spaces can be projected to lower dimension. This is useful for visualization and
to analyze the belief space and value functions. This definition is used by functions like <code><a href="#topic+plot_belief_space">plot_belief_space()</a></code>,
<code><a href="#topic+plot_value_function">plot_value_function()</a></code>, and <code><a href="#topic+sample_belief_space">sample_belief_space()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projection(x = NULL, model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projection_+3A_x">x</code></td>
<td>
<p>specification of the projection (see Details section).</p>
</td></tr>
<tr><td><code id="projection_+3A_model">model</code></td>
<td>
<p>a <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The belief space is $n-1$ dimensional, were $n$ is the number of states. Note: it is n-1 dimensional since the probabilities
need to add up to 1. A projection fixes the belief value for a set of states. For example, for a 4-state POMDP
(s1, s2, s3, s4), we can project the belief space on s1 and s2 by holding s3 and s4 constant
which is represented by the vector <code>c(s1 = NA, s2 = NA, s3 = 0, s4 = .1)</code>. We use <code>NA</code> to represent that the values are not
fixed and the value that the other dimensions are held constant.
</p>
<p>We provide several ways to specify a projection:
</p>

<ul>
<li><p> A vector with values for all dimensions. <code>NA</code>s are used for the dimension projected on. This is the canonical form
used in this package. Example: <code>c(NA, NA, 0, .1)</code>
</p>
</li>
<li><p> A named vector with just the dimensions held constant. Example: <code>c(s3 = 0, s4 = .1)</code>
</p>
</li>
<li><p> A vector of state names to project on. All other dimensions are held constant at 0. Example: <code>c("s1", "s2")</code>
</p>
</li>
<li><p> A vector with indices of the states to project on. All other dimensions are held constant at 0. Example: <code>c(1, 2)</code>
</p>
</li></ul>



<h3>Value</h3>

<p>a canonical description of the projection.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- POMDP(
 states = 4,
 actions = 2,
 observations = 2,
 transition_prob = list("identity","identity"),
 observation_prob = list("uniform","uniform"),
 reward = rbind(R_(value = 1))
)

projection(NULL, model = model)
projection(1:2, model = model)
projection(c("s2", "s3"), model = model)
projection(c(1,4), model = model)
projection(c(s2 = .4, s3 = .2), model = model)
projection(c(s1 = .1, s2 = NA, s3 = NA, s4 = .3), model = model)
</code></pre>

<hr>
<h2 id='reachable_and_absorbing'>Reachable and Absorbing States</h2><span id='topic+reachable_and_absorbing'></span><span id='topic+reachable_states'></span><span id='topic+absorbing_states'></span><span id='topic+remove_unreachable_states'></span>

<h3>Description</h3>

<p>Find reachable and absorbing states in the transition model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reachable_states(x, states = NULL)

absorbing_states(x, states = NULL)

remove_unreachable_states(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reachable_and_absorbing_+3A_x">x</code></td>
<td>
<p>a <code>MDP</code> pr <code>POMDP</code> object.</p>
</td></tr>
<tr><td><code id="reachable_and_absorbing_+3A_states">states</code></td>
<td>
<p>a character vector specifying the names of the states to be
checked. <code>NULL</code> checks all states.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>reachable_states()</code> checks if states
are reachable using the transition model.
</p>
<p>The function <code>absorbing_states()</code> checks if a state or a set of states are
absorbing (terminal states) with a zero reward (or <code>-Inf</code> for unavailable actions).
If no states are specified (<code>states = NULL</code>), then all model states are
checked. This information can be used in simulations to end an episode.
</p>
<p>The function <code>remove_unreachable_states()</code> simplifies a model by
removing unreachable states.
</p>


<h3>Value</h3>

<p><code>reachable_states()</code> returns a logical vector indicating
if the states are reachable.
</p>
<p><code>absorbing_states()</code> returns a logical vector indicating
if the states are absorbing (terminal).
</p>
<p>the model with all unreachable states removed
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Maze)

gridworld_matrix(Maze, what = "label")

# the states marked with +1 and -1 are absorbing
absorbing_states(Maze)
which(absorbing_states(Maze))

# all states in the model are reachable
reachable_states(Maze)
which(!reachable_states(Maze))

</code></pre>

<hr>
<h2 id='regret'>Calculate the Regret of a Policy</h2><span id='topic+regret'></span>

<h3>Description</h3>

<p>Calculates the regret of a policy relative to a benchmark policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regret(policy, benchmark, start = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regret_+3A_policy">policy</code></td>
<td>
<p>a solved POMDP containing the policy to calculate the regret for.</p>
</td></tr>
<tr><td><code id="regret_+3A_benchmark">benchmark</code></td>
<td>
<p>a solved POMDP with the (optimal) policy. Regret is calculated relative to this
policy.</p>
</td></tr>
<tr><td><code id="regret_+3A_start">start</code></td>
<td>
<p>the used start (belief) state. If NULL then the start (belief) state of the <code>benchmark</code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regret is defined as <code class="reqn">V^{\pi^*}(s_0) - V^{\pi}(s_0)</code> with <code class="reqn">V^\pi</code> representing the expected long-term
state value (represented by the value function) given the policy <code class="reqn">\pi</code> and the start
state <code class="reqn">s_0</code>. For POMDPs the start state is the start belief <code class="reqn">b_0</code>.
</p>
<p>Note that for regret usually the optimal policy <code class="reqn">\pi^*</code> is used as the benchmark.
Since the optimal policy may not be known, regret relative to the best known policy can be used.
</p>


<h3>Value</h3>

<p>the regret as a difference of expected long-term rewards.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

sol_optimal &lt;- solve_POMDP(Tiger)
sol_optimal

# perform exact value iteration for 10 epochs
sol_quick &lt;- solve_POMDP(Tiger, method = "enum", horizon = 10)
sol_quick

regret(sol_quick, benchmark = sol_optimal)
</code></pre>

<hr>
<h2 id='reward'>Calculate the Reward for a POMDP Solution</h2><span id='topic+reward'></span><span id='topic+reward_node_action'></span>

<h3>Description</h3>

<p>This function calculates the expected total reward for a POMDP solution
given a starting belief state. The value is calculated using the value function stored
in the POMDP solution. In addition, the policy graph node that represents the belief state
and the optimal action can also be returned using <code>reward_node_action()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reward(x, belief = NULL, epoch = 1, ...)

reward_node_action(x, belief = NULL, epoch = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reward_+3A_x">x</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a> object.</p>
</td></tr>
<tr><td><code id="reward_+3A_belief">belief</code></td>
<td>
<p>specification of the current belief state (see argument start
in <a href="#topic+POMDP">POMDP</a> for details). By default the belief state defined in
the model as start is used. Multiple belief states can be specified as rows in a matrix.</p>
</td></tr>
<tr><td><code id="reward_+3A_epoch">epoch</code></td>
<td>
<p>return reward for this epoch. Use 1 for converged policies.</p>
</td></tr>
<tr><td><code id="reward_+3A_...">...</code></td>
<td>
<p>further arguments are passed on.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The reward is typically calculated using the value function (alpha vectors)
of the solution. If these are not available, then <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code> is
used instead with a warning.
</p>


<h3>Value</h3>

<p><code>reward()</code> returns a vector of reward values, one for each belief if a matrix is specified.
</p>
<p><code>reward_node_action()</code> returns a list with the components
</p>
<table>
<tr><td><code>belief_state</code></td>
<td>
<p>the belief state specified in <code>belief</code>.</p>
</td></tr>
<tr><td><code>reward</code></td>
<td>
<p>the total expected reward given a belief and epoch. </p>
</td></tr>
<tr><td><code>pg_node</code></td>
<td>
<p>the policy node that represents the belief state.</p>
</td></tr>
<tr><td><code>action</code></td>
<td>
<p>the optimal action.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
sol &lt;- solve_POMDP(model = Tiger)

# if no start is specified, a uniform belief is used.
reward(sol)

# we have additional information that makes us believe that the tiger
# is more likely to the left.
reward(sol, belief = c(0.85, 0.15))

# we start with strong evidence that the tiger is to the left.
reward(sol, belief = "tiger-left")

# Note that in this case, the total discounted expected reward is greater
# than 10 since the tiger problem resets and another game staring with
# a uniform belief is played which produces additional reward.

# return reward, the initial node in the policy graph and the optimal action for
# two beliefs.
reward_node_action(sol, belief = rbind(c(.5, .5), c(.9, .1)))

# manually combining reward with belief space sampling to show the value function
# (color signifies the optimal action)
samp &lt;- sample_belief_space(sol, n = 200)
rew &lt;- reward_node_action(sol, belief = samp)
plot(rew$belief[,"tiger-right"], rew$reward, col = rew$action, ylim = c(0, 15))
legend(x = "top", legend = levels(rew$action), title = "action", col = 1:3, pch = 1)

# this is the piecewise linear value function from the solution
plot_value_function(sol, ylim = c(0, 10))
</code></pre>

<hr>
<h2 id='round_stochastic'>Round a stochastic vector or a row-stochastic matrix</h2><span id='topic+round_stochastic'></span>

<h3>Description</h3>

<p>Rounds a vector such that the sum of 1 is preserved. Rounds a matrix such
that each row sum up to 1. One entry is adjusted after rounding
such that the rounding error is the smallest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>round_stochastic(x, digits = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="round_stochastic_+3A_x">x</code></td>
<td>
<p>a stochastic vector or a row-stochastic matrix.</p>
</td></tr>
<tr><td><code id="round_stochastic_+3A_digits">digits</code></td>
<td>
<p>number of digits for rounding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The rounded vector or matrix.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+round">round</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># regular rounding would not sum up to 1 
x &lt;- c(0.333, 0.334, 0.333)

round_stochastic(x)
round_stochastic(x, digits = 2)
round_stochastic(x, digits = 1)
round_stochastic(x, digits = 0)


# round a stochastic matrix
m &lt;- matrix(runif(15), ncol = 3)
m &lt;- sweep(m, 1, rowSums(m), "/")

m
round_stochastic(m, digits = 2)
round_stochastic(m, digits = 1)
round_stochastic(m, digits = 0)
</code></pre>

<hr>
<h2 id='RussianTiger'>Russian Tiger Problem POMDP Specification</h2><span id='topic+RussianTiger'></span>

<h3>Description</h3>

<p>This is a variation of the Tiger Problem introduces in Cassandra et al (1994)
with an absorbing state after a door is opened.
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+POMDP">POMDP</a>.
</p>


<h3>Details</h3>

<p>The original Tiger problem is available as <a href="#topic+Tiger">Tiger</a>. The original problem is
an infinite-horizon problem, where when the agent opens a door then the
problem starts over. The infinite-horizon problem can be solved if
a discount factor <code class="reqn">\gamma &lt; 1</code> is used.
</p>
<p>The Russian Tiger problem uses no discounting, but instead
adds an absorbing state <code>done</code>  which is reached
after the agent opens a door. It adds the action <code>nothing</code> to indicate
that the agent does nothing. The <code>nothing</code> action is only available in the
state <code>done</code> indicated by a reward of <code>-Inf</code> from all after states. A new
observation <code>done</code> is only emitted by the state <code>done</code>. Also, the Russian
tiger inflicts more pain with a negative reward of -1000.
</p>


<h3>See Also</h3>

<p>Other POMDP_examples: 
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+POMDP_example_files">POMDP_example_files</a></code>,
<code><a href="#topic+Tiger">Tiger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("RussianTiger")
RussianTiger

# states, actions, and observations
RussianTiger$states  
RussianTiger$actions 
RussianTiger$observations

# reward (-Inf indicates unavailable actions)
RussianTiger$reward

sapply(RussianTiger$states, FUN = function(s) actions(RussianTiger, s))

plot_transition_graph(RussianTiger, vertex.size = 30, edge.arrow.size = .3, margin = .5)

# absorbing states
absorbing_states(RussianTiger)

# solve the problem.
sol &lt;- solve_POMDP(RussianTiger)
policy(sol)
plot_policy_graph(sol)
</code></pre>

<hr>
<h2 id='sample_belief_space'>Sample from the Belief Space</h2><span id='topic+sample_belief_space'></span>

<h3>Description</h3>

<p>Sample points from belief space using a several sampling strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_belief_space(model, projection = NULL, n = 1000, method = "random", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_belief_space_+3A_model">model</code></td>
<td>
<p>a unsolved or solved <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_n">n</code></td>
<td>
<p>size of the sample. For trajectories, it is the number of trajectories.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_method">method</code></td>
<td>
<p>character string specifying the sampling strategy. Available
are <code>"random"</code>, <code>"regular"</code>, and <code>"trajectories"</code>.</p>
</td></tr>
<tr><td><code id="sample_belief_space_+3A_...">...</code></td>
<td>
<p>for the trajectory method, further arguments are passed on to <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code>. Further arguments are ignored for the other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of sampling from the belief space is to provide good coverage or to sample belief points
that are more likely to be encountered (see trajectory method).
The following sampling methods are available:
</p>

<ul>
<li> <p><code>'random'</code> samples uniformly sample from the projected belief space using
the method described by Luc Devroye (1986). Sampling is be done in parallel
after a foreach backend is registered.
</p>
</li>
<li> <p><code>'regular'</code> samples points using a
regularly spaced grid. This method is only available for projections on 2 or
3 states.
</p>
</li>
<li> <p><code>"trajectories"</code> returns the belief states encountered in <code>n</code> trajectories of length <code>horizon</code> starting at the
model's initial belief. Thus it returns <code>n</code> x <code>horizon</code> belief states and will contain duplicates.
Projection is not supported for trajectories. Additional
arguments can include the simulation <code>horizon</code> and the start <code>belief</code> which are passed on to <code><a href="#topic+simulate_POMDP">simulate_POMDP()</a></code>.
</p>
</li></ul>



<h3>Value</h3>

<p>Returns a matrix. Each row is a sample from the belief space.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Luc Devroye, Non-Uniform Random Variate Generation, Springer
Verlag, 1986.
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

# random sampling can be done in parallel after registering a backend.
# doparallel::registerDoParallel()

sample_belief_space(Tiger, n = 5)
sample_belief_space(Tiger, n = 5, method = "regular")
sample_belief_space(Tiger, n = 1, horizon = 5, method = "trajectories")

# sample, determine the optimal action and calculate the expected reward for a solved POMDP
# Note: check.names = FALSE is used to preserve the `-` for the state names in the dataframe.
sol &lt;- solve_POMDP(Tiger)
samp &lt;- sample_belief_space(sol, n = 5, method = "regular")
data.frame(samp, action = optimal_action(sol,  belief = samp), 
  reward = reward(sol, belief = samp), check.names = FALSE)
  
# sample from a 3 state problem
data(Three_doors)
Three_doors

sample_belief_space(Three_doors, n = 5)
sample_belief_space(Three_doors, n = 5, projection = c(`tiger-left` = .1))

if ("Ternary" %in% installed.packages()) {
sample_belief_space(Three_doors, n = 9, method = "regular")
sample_belief_space(Three_doors, n = 9, method = "regular", projection = c(`tiger-left` = .1))
}

sample_belief_space(Three_doors, n = 1, horizon = 5, method = "trajectories")
</code></pre>

<hr>
<h2 id='simulate_MDP'>Simulate Trajectories in a MDP</h2><span id='topic+simulate_MDP'></span>

<h3>Description</h3>

<p>Simulate trajectories through a MDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from an epsilon-greedy policy and then update the state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_MDP(
  model,
  n = 100,
  start = NULL,
  horizon = NULL,
  epsilon = NULL,
  delta_horizon = 0.001,
  return_trajectories = FALSE,
  engine = "cpp",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_MDP_+3A_model">model</code></td>
<td>
<p>a MDP model.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_n">n</code></td>
<td>
<p>number of trajectories.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_start">start</code></td>
<td>
<p>probability distribution over the states for choosing the
starting states for the trajectories. Defaults to &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_horizon">horizon</code></td>
<td>
<p>epochs end once an absorbing state is reached or after
the maximal number of epochs specified via <code>horizon</code>. If <code>NULL</code> then the
horizon for the model is used.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_epsilon">epsilon</code></td>
<td>
<p>the probability of random actions  for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_delta_horizon">delta_horizon</code></td>
<td>
<p>precision used to determine the horizon for infinite-horizon problems.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_return_trajectories">return_trajectories</code></td>
<td>
<p>logical; return the complete trajectories.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_engine">engine</code></td>
<td>
<p><code>'cpp'</code> or <code>'r'</code> to perform simulation using a faster C++
or a native R implementation.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_verbose">verbose</code></td>
<td>
<p>report used parameters.</p>
</td></tr>
<tr><td><code id="simulate_MDP_+3A_...">...</code></td>
<td>
<p>further arguments are ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A native R implementation is available (<code>engine = 'r'</code>) and the default is a
faster C++ implementation (<code>engine = 'cpp'</code>).
</p>
<p>Both implementations support parallel execution using the package
<span class="pkg">foreach</span>. To enable parallel execution, a parallel backend like
<span class="pkg">doparallel</span> needs to be available needs to be registered (see
<code><a href="doParallel.html#topic+registerDoParallel">doParallel::registerDoParallel()</a></code>).
Note that small simulations are slower using parallelization. Therefore, C++ simulations
with n * horizon less than 100,000 are always executed using a single worker.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>

<ul>
<li> <p><code>avg_reward</code>: The average discounted reward.
</p>
</li>
<li> <p><code>reward</code>: Reward for each trajectory.
</p>
</li>
<li> <p><code>action_cnt</code>: Action counts.
</p>
</li>
<li> <p><code>state_cnt</code>: State counts.
</p>
</li>
<li> <p><code>trajectories</code>: A data.frame with the trajectories. Each row
contains the <code>episode</code> id, the <code>time</code> step, the state <code>s</code>,
the chosen action <code>a</code>,
the reward <code>r</code>, and the next state <code>s_prime</code>. Trajectories are
only returned for <code>return_trajectories = TRUE</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># enable parallel simulation 
# doParallel::registerDoParallel()

data(Maze)

# solve the POMDP for 5 epochs and no discounting
sol &lt;- solve_MDP(Maze, discount = 1)
sol

# U in the policy is and estimate of the utility of being in a state when using the optimal policy.
policy(sol)
gridworld_matrix(sol, what = "action")

## Example 1: simulate 100 trajectories following the policy, 
#             only the final belief state is returned
sim &lt;- simulate_MDP(sol, n = 100, horizon = 10, verbose = TRUE)
sim

# Note that all simulations start at s_1 and that the simulated avg. reward
# is therefore an estimate to the U value for the start state s_1.
policy(sol)[1,]

# Calculate proportion of actions taken in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)

## Example 2: simulate starting following a uniform distribution over all
#             states and return all trajectories
sim &lt;- simulate_MDP(sol, n = 100, start = "uniform", horizon = 10, 
  return_trajectories = TRUE)
head(sim$trajectories)   
  
# how often was each state visited?
table(sim$trajectories$s)
</code></pre>

<hr>
<h2 id='simulate_POMDP'>Simulate Trajectories in a POMDP</h2><span id='topic+simulate_POMDP'></span>

<h3>Description</h3>

<p>Simulate trajectories through a POMDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from the the epsilon-greedy policy and then updated using observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_POMDP(
  model,
  n = 1000,
  belief = NULL,
  horizon = NULL,
  epsilon = NULL,
  delta_horizon = 0.001,
  digits = 7L,
  return_beliefs = FALSE,
  return_trajectories = FALSE,
  engine = "cpp",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_POMDP_+3A_model">model</code></td>
<td>
<p>a POMDP model.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_n">n</code></td>
<td>
<p>number of trajectories.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_belief">belief</code></td>
<td>
<p>probability distribution over the states for choosing the
starting states for the trajectories.
Defaults to the start belief state specified in the model or &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_horizon">horizon</code></td>
<td>
<p>number of epochs for the simulation. If <code>NULL</code> then the
horizon for finite-horizon model is used. For infinite-horizon problems, a horizon is
calculated using the discount factor.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_epsilon">epsilon</code></td>
<td>
<p>the probability of random actions for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_delta_horizon">delta_horizon</code></td>
<td>
<p>precision used to determine the horizon for infinite-horizon problems.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_digits">digits</code></td>
<td>
<p>round probabilities for belief points.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_return_beliefs">return_beliefs</code></td>
<td>
<p>logical; Return all visited belief states? This requires n x horizon memory.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_return_trajectories">return_trajectories</code></td>
<td>
<p>logical; Return the simulated trajectories as a data.frame?</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_engine">engine</code></td>
<td>
<p><code>'cpp'</code>, <code>'r'</code> to perform simulation using a faster C++ or a
native R implementation.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_verbose">verbose</code></td>
<td>
<p>report used parameters.</p>
</td></tr>
<tr><td><code id="simulate_POMDP_+3A_...">...</code></td>
<td>
<p>further arguments are ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulates <code>n</code> trajectories.
If no simulation horizon is specified, the horizon of finite-horizon problems
is used. For infinite-horizon problems with <code class="reqn">\gamma &lt; 1</code>, the simulation
horizon <code class="reqn">T</code> is chosen such that
the worst-case error is no more than <code class="reqn">\delta_\text{horizon}</code>. That is
</p>
<p style="text-align: center;"><code class="reqn">\gamma^T \frac{R_\text{max}}{\gamma} \le \delta_\text{horizon},</code>
</p>

<p>where <code class="reqn">R_\text{max}</code> is the largest possible absolute reward value used as a
perpetuity starting after <code class="reqn">T</code>.
</p>
<p>A native R implementation (<code>engine = 'r'</code>) and a faster C++ implementation
(<code>engine = 'cpp'</code>) are available. Currently, only the R implementation supports
multi-episode problems.
</p>
<p>Both implementations support the simulation of trajectories in parallel using the package
<span class="pkg">foreach</span>. To enable parallel execution, a parallel backend like
<span class="pkg">doparallel</span> needs to be registered (see
<code><a href="doParallel.html#topic+registerDoParallel">doParallel::registerDoParallel()</a></code>).
Note that small simulations are slower using parallelization. C++ simulations
with <code>n * horizon</code> less than 100,000 are always executed using a single worker.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>

<ul>
<li> <p><code>avg_reward</code>: The average discounted reward.
</p>
</li>
<li> <p><code>action_cnt</code>: Action counts.
</p>
</li>
<li> <p><code>state_cnt</code>: State counts.
</p>
</li>
<li> <p><code>reward</code>: Reward for each trajectory.
</p>
</li>
<li> <p><code>belief_states</code>: A matrix with belief states as rows.
</p>
</li>
<li> <p><code>trajectories</code>: A data.frame with the <code>episode</code> id, <code>time</code>, the state of the
simulation (<code>simulation_state</code>), the id of the used alpha vector given the current belief
(see <code>belief_states</code> above), the action <code>a</code> and the reward <code>r</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

# solve the POMDP for 5 epochs and no discounting
sol &lt;- solve_POMDP(Tiger, horizon = 5, discount = 1, method = "enum")
sol
policy(sol)

# uncomment the following line to register a parallel backend for simulation 
# (needs package doparallel installed)

# doParallel::registerDoParallel()
# foreach::getDoParWorkers()

## Example 1: simulate 100 trajectories
sim &lt;- simulate_POMDP(sol, n = 100, verbose = TRUE)
sim

# calculate the percentage that each action is used in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)


## Example 2: look at the belief states and the trajectories starting with 
#             an initial start belief.
sim &lt;- simulate_POMDP(sol, n = 100, belief = c(.5, .5), 
  return_beliefs = TRUE, return_trajectories = TRUE)
head(sim$belief_states)
head(sim$trajectories)

# plot with added density (the x-axis is the probability of the second belief state)
plot_belief_space(sol, sample = sim$belief_states, jitter = 2, ylim = c(0, 6))
lines(density(sim$belief_states[, 2], bw = .02)); axis(2); title(ylab = "Density")


## Example 3: simulate trajectories for an unsolved POMDP which uses an epsilon of 1
#             (i.e., all actions are randomized). The simulation horizon for the 
#             infinite-horizon Tiger problem is calculated using delta_horizon. 
sim &lt;- simulate_POMDP(Tiger, return_beliefs = TRUE, verbose = TRUE)
sim$avg_reward

hist(sim$reward, breaks = 20)

plot_belief_space(sol, sample = sim$belief_states, jitter = 2, ylim = c(0, 6))
lines(density(sim$belief_states[, 1], bw = .05)); axis(2); title(ylab = "Density")
</code></pre>

<hr>
<h2 id='solve_MDP'>Solve an MDP Problem</h2><span id='topic+solve_MDP'></span><span id='topic+solve_MDP_DP'></span><span id='topic+solve_MDP_TD'></span>

<h3>Description</h3>

<p>Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_MDP(model, method = "value", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  N_max = 1000,
  error = 0.01,
  k_backups = 10,
  U = NULL,
  verbose = FALSE
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = 0.5,
  epsilon = 0.1,
  N = 100,
  U = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_MDP_+3A_model">model</code></td>
<td>
<p>an MDP problem specification.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_method">method</code></td>
<td>
<p>string; one of the following solution methods: <code>'value_iteration'</code>,
<code>'policy_iteration'</code>, <code>'q_learning'</code>, <code>'sarsa'</code>, or <code>'expected_sarsa'</code>.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_...">...</code></td>
<td>
<p>further parameters are passed on to the solver function.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_horizon">horizon</code></td>
<td>
<p>an integer with the number of epochs for problems with a
finite planning horizon. If set to <code>Inf</code>, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
<code>NULL</code>, then the horizon specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">(0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_n_max">N_max</code></td>
<td>
<p>maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_error">error</code></td>
<td>
<p>value iteration: maximum error allowed in the utility of any state
(i.e., the maximum policy loss) used as the termination criterion.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_k_backups">k_backups</code></td>
<td>
<p>policy iteration: number of look ahead steps used for approximate policy evaluation
used by the policy iteration method.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_u">U</code></td>
<td>
<p>a vector with initial utilities used for each state. If
<code>NULL</code>, then the default of a vector of all 0s is used.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the solver in the R console.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_alpha">alpha</code></td>
<td>
<p>step size in <code style="white-space: pre;">&#8288;(0, 1]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_epsilon">epsilon</code></td>
<td>
<p>used for <code class="reqn">\epsilon</code>-greedy policies.</p>
</td></tr>
<tr><td><code id="solve_MDP_+3A_n">N</code></td>
<td>
<p>number of episodes used for learning.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Implemented are the following dynamic programming methods (following
Russell and Norvig, 2010):
</p>

<ul>
<li> <p><strong>Modified Policy Iteration</strong>
starts with a random policy and iteratively performs
a sequence of
</p>

<ol>
<li><p> approximate policy evaluation (estimate the value function for the
current policy using <code>k_backups</code> and function <code><a href="#topic+MDP_policy_evaluation">MDP_policy_evaluation()</a></code>), and
</p>
</li>
<li><p> policy improvement (calculate a greedy policy given the value function).
The algorithm stops when it converges to a stable policy (i.e., no changes
between two iterations).
</p>
</li></ol>

</li>
<li> <p><strong>Value Iteration</strong> starts with
an arbitrary value function (by default all 0s) and iteratively
updates the value function for each state using the Bellman equation.
The iterations
are terminated either after <code>N_max</code> iterations or when the solution converges.
Approximate convergence is achieved
for discounted problems (with <code class="reqn">\gamma &lt; 1</code>)
when the maximal value function change for any state <code class="reqn">\delta</code> is
<code class="reqn">\delta \le error (1-\gamma) / \gamma</code>. It can be shown that this means
that no state value is more than
<code class="reqn">error</code> from the value in the optimal value function. For undiscounted
problems, we use <code class="reqn">\delta \le error</code>.
</p>
<p>The greedy policy
is calculated from the final value function. Value iteration can be seen as
policy iteration with truncated policy evaluation.
</p>
</li></ul>

<p>Note that the policy converges earlier than the value function.
</p>
<p>Implemented are the following temporal difference control methods
described in Sutton and Barto (2020).
Note that the MDP transition and reward models are only used to simulate
the environment for these reinforcement learning methods.
The algorithms use a step size parameter <code class="reqn">\alpha</code> (learning rate) for the
updates and the exploration parameter <code class="reqn">\epsilon</code> for
the <code class="reqn">\epsilon</code>-greedy policy.
</p>
<p>If the model has absorbing states to terminate episodes, then no maximal episode length
(<code>horizon</code>) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 10,000 actions with a warning. For models without absorbing states,
a episode length has to be specified via <code>horizon</code>.
</p>

<ul>
<li> <p><strong>Q-Learning</strong> is an off-policy temporal difference method that uses
an <code class="reqn">\epsilon</code>-greedy behavior policy and learns a greedy target
policy.
</p>
</li>
<li> <p><strong>Sarsa</strong> is an on-policy method that follows and learns
an <code class="reqn">\epsilon</code>-greedy policy. The final <code class="reqn">\epsilon</code>-greedy policy
is converted into a greedy policy.
</p>
</li>
<li> <p><strong>Expected Sarsa</strong>: We implement an on-policy version that uses
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa
moves in expectation. Because it uses the expectation, we can
set the step size <code class="reqn">\alpha</code> to large values and even 1.
</p>
</li></ul>



<h3>Value</h3>

<p><code>solve_MDP()</code> returns an object of class POMDP which is a list with the
model specifications (<code>model</code>), the solution (<code>solution</code>).
The solution is a list with the elements:
</p>

<ul>
<li> <p><code>policy</code> a list representing the policy graph. The list only has one element for converged solutions.
</p>
</li>
<li> <p><code>converged</code> did the algorithm converge (<code>NA</code>) for finite-horizon problems.
</p>
</li>
<li> <p><code>delta</code> final <code class="reqn">\delta</code> (value iteration and infinite-horizon only)
</p>
</li>
<li> <p><code>iterations</code> number of iterations to convergence (infinite-horizon only)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Russell, S., Norvig, P. (2021). Artificial Intelligence: A Modern Approach.
Fourth edition. Prentice Hall.
</p>
<p>Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
</p>


<h3>See Also</h3>

<p>Other solver: 
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Maze)
Maze

# use value iteration
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Maze solutions can be visualized
gridworld_plot_policy(maze_solved)

# use modified policy iteration
maze_solved &lt;- solve_MDP(Maze, method = "policy_iteration")
policy(maze_solved)

# finite horizon
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
gridworld_plot_policy(maze_solved, epoch = 1)
gridworld_plot_policy(maze_solved, epoch = 2)
gridworld_plot_policy(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted &lt;- Maze
Maze_discounted$discount &lt;- .9
pi &lt;- random_MDP_policy(Maze_discounted, 
        prob = c(n = .7, e = .1, s = .1, w = 0.1))
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved &lt;- solve_MDP(Maze)

MDP_policy_evaluation(pi, Maze, k_backup = 100)
MDP_policy_evaluation(policy(maze_solved), Maze, k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned &lt;- solve_MDP(Maze, method = "q_learning", N = 100)
maze_learned

maze_learned$solution
policy(maze_learned)
plot_value_function(maze_learned)
gridworld_plot_policy(maze_learned)
</code></pre>

<hr>
<h2 id='solve_POMDP'>Solve a POMDP Problem using pomdp-solver</h2><span id='topic+solve_POMDP'></span><span id='topic+solve_POMDP_parameter'></span>

<h3>Description</h3>

<p>This function utilizes the C implementation of 'pomdp-solve' by Cassandra
(2015) to solve problems that are formulated as partially observable Markov
decision processes (POMDPs). The result is an optimal or approximately
optimal policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_POMDP(
  model,
  horizon = NULL,
  discount = NULL,
  initial_belief = NULL,
  terminal_values = NULL,
  method = "grid",
  digits = 7,
  parameter = NULL,
  timeout = Inf,
  verbose = FALSE
)

solve_POMDP_parameter()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_POMDP_+3A_model">model</code></td>
<td>
<p>a POMDP problem specification created with <code><a href="#topic+POMDP">POMDP()</a></code>.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_horizon">horizon</code></td>
<td>
<p>an integer with the number of epochs for problems with a
finite planning horizon. If set to <code>Inf</code>, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
<code>NULL</code>, then the horizon specified in <code>model</code> will be used.  For
time-dependent POMDPs a vector of horizons can be specified (see Details
section).</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">[0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_initial_belief">initial_belief</code></td>
<td>
<p>An initial belief vector. If <code>NULL</code>, then the
initial belief specified in <code>model</code> (as start) will be used.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_terminal_values">terminal_values</code></td>
<td>
<p>a vector with the terminal utility values for each state or a
matrix specifying the terminal rewards via a terminal value function (e.g.,
the alpha components produced by <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>).  If <code>NULL</code>, then, if available,
the terminal values specified in <code>model</code> will be used or a vector with all 0s otherwise.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_method">method</code></td>
<td>
<p>string; one of the following solution methods: <code>"grid"</code>,
<code>"enum"</code>, <code>"twopass"</code>, <code>"witness"</code>, or <code>"incprune"</code>.
The default is <code>"grid"</code> implementing the finite grid method.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_digits">digits</code></td>
<td>
<p>precision used when writing POMDP files (see
<code><a href="#topic+write_POMDP">write_POMDP()</a></code>).</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_parameter">parameter</code></td>
<td>
<p>a list with parameters passed on to the pomdp-solve
program.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_timeout">timeout</code></td>
<td>
<p>number of seconds for the solver to run.</p>
</td></tr>
<tr><td><code id="solve_POMDP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the pomdp solver in the R console.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Parameters</h4>

<p><code>solve_POMDP_parameter()</code> displays available solver parameter options.
</p>
<p><strong>Horizon:</strong> Infinite-horizon POMDPs (<code>horizon = Inf</code>) converge to a
single policy graph. Finite-horizon POMDPs result in a policy tree of a
depth equal to the smaller of the horizon or the number of epochs to
convergence.  The policy (and the associated value function) are stored in a
list by epoch. The policy for the first epoch is stored as the first
element. Horizon can also be used to limit the number of epochs used
for value iteration.
</p>
<p><strong>Precision:</strong> The POMDP solver uses various epsilon values to control
precision for comparing alpha vectors to check for convergence, and solving
LPs. Overall precision can be changed using
<code>parameter = list(epsilon = 1e-3)</code>.
</p>
<p><strong>Methods:</strong> Several algorithms using exact value iteration are
available:
</p>

<ul>
<li><p> Enumeration (Sondik 1971).
</p>
</li>
<li><p> Two pass (Sondik 1971).
</p>
</li>
<li><p> Witness (Littman, Cassandra, Kaelbling, 1996).
</p>
</li>
<li><p> Incremental pruning (Zhang and Liu, 1996, Cassandra et al 1997).
</p>
</li></ul>

<p>In addition, the following approximate value iteration method is available:
</p>

<ul>
<li><p> Grid implements a variation of point-based value iteration
to solve larger POMDPs (PBVI; see Pineau 2003) without dynamic belief set expansion.
</p>
</li></ul>

<p>Details can be found in (Cassandra, 2015).
</p>
<p><strong>Note on POMDP problem size:</strong> Finding optimal policies for POMDPs is known to be
a prohibitively difficult problem because the belief space grows exponentially
with the number of states. Therefore, exact algorithms can be only used for
extremely small problems with only a few states. Typically, the researcher
needs to simplify the problem description (fewer states, actions and observations)
and choose an approximate algorithm with an acceptable level of
approximation to make the problem tractable.
</p>
<p><strong>Note on method grid:</strong> The finite grid method implements a version of Point
Based Value Iteration (PBVI). The used belief points are by default created
using points that are reachable from the initial belief (<code>start</code>) by
following all combinations of actions and observations. The size of the grid is
by default 10,000 and
can be set via <code>parameter = list(fg_points = 100)</code>. Alternatively,
different strategies can be chosen using the parameter <code>fg_type</code>. In
this implementation, the user can also specify manually a grid of belief
states by providing a matrix with belief states as produced by
<code><a href="#topic+sample_belief_space">sample_belief_space()</a></code> as the parameter <code>grid</code>.
</p>
<p>To guarantee convergence in point-based (finite grid) value iteration, the
initial value function must be a lower bound on the optimal value function.
If all rewards are strictly non-negative, an initial value function with an
all zero vector can be used and results will be similar to other methods.
However, if there are negative rewards, lower bounds can be guaranteed by
setting a single vector with the values <code class="reqn">min(reward)/(1 - discount)</code>.
The value function is guaranteed to converge to the true value function, but
finite-horizon value functions will not be as expected. <code><a href="#topic+solve_POMDP">solve_POMDP()</a></code>
produces a warning in this case.
</p>
<p><strong>Time-dependent POMDPs:</strong> Time dependence of transition probabilities,
observation probabilities and reward structure can be modeled by considering
a set of episodes representing epochs with the same settings. In the scared
tiger example (see Examples section), the tiger has the normal behavior for
the first three epochs (episode 1) and then becomes scared with different
transition probabilities for the next three epochs (episode 2). The episodes
can be solved in reverse order where the value function is used as the
terminal values of the preceding episode. This can be done by specifying a
vector of horizons (one horizon for each episode) and then lists with
transition matrices, observation matrices, and rewards. If the horizon
vector has names, then the lists also need to be named, otherwise they have
to be in the same order (the numeric index is used). Only the time-varying
matrices need to be specified. An example can be found in Example 4 in the
Examples section. The procedure can also be done by calling the solver
multiple times (see Example 5).
</p>



<h4>Solution</h4>

<p><strong>Policy:</strong>
Each policy is a data frame where each row representing a
policy graph node with an associated optimal action and a list of node IDs
to go to depending on the observation (specified as the column names). For
the finite-horizon case, the observation specific node IDs refer to nodes in
the next epoch creating a policy tree.  Impossible observations have a
<code>NA</code> as the next state.
</p>
<p><strong>Value function:</strong>
The value function specifies the value of the value function (the expected reward)
over the belief space. The dimensionality of the belief space is $n-1$ where $n$ is the number of states.
The value function is stored as a matrix. Each row is
associated with a node (row) in the policy graph and represents the
coefficients (alpha or V vector) of a hyperplane. It contains one
value per state which is the value for the belief state that has a probability
of 1 for that state and 0s for all others.
</p>



<h4>Temporary Files</h4>

<p>All temporary solver files are stored in the directory returned by <code>tempdir()</code>.
</p>



<h3>Value</h3>

<p>The solver returns an object of class POMDP which is a list with the
model specifications. Solved POMDPs also have an element called <code>solution</code> which is a list, and the
solver output (<code>solver_output</code>). The solution is a list that contains elements like:
</p>

<ul>
<li> <p><code>method</code> used solver method.
</p>
</li>
<li> <p><code>solver_output</code> output of the solver program.
</p>
</li>
<li> <p><code>converged</code> did the solution converge?
</p>
</li>
<li> <p><code>initial_belief</code> used initial belief used.
</p>
</li>
<li> <p><code>total_expected_reward</code> total expected reward starting from the the initial belief.
</p>
</li>
<li> <p><code>pg</code>, <code>initial_pg_node</code> the policy graph (see Details section).
</p>
</li>
<li> <p><code>alpha</code> value function as hyperplanes representing the nodes in the policy graph (see Details section).
</p>
</li>
<li> <p><code>belief_points_solver</code> optional; belief points used by the solver.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>Cassandra, A. (2015). pomdp-solve: POMDP Solver Software,
<a href="http://www.pomdp.org">http://www.pomdp.org</a>.
</p>
<p>Sondik, E. (1971). The Optimal Control of Partially Observable Markov
Processes. Ph.D. Dissertation, Stanford University.
</p>
<p>Cassandra, A., Littman M.L., Zhang L. (1997). Incremental Pruning: A Simple,
Fast, Exact Algorithm for Partially Observable Markov Decision Processes.
UAI'97: Proceedings of the Thirteenth conference on Uncertainty in
artificial intelligence, August 1997, pp. 54-61.
</p>
<p>Monahan, G. E. (1982). A survey of partially observable Markov decision
processes: Theory, models, and algorithms. Management Science 28(1):1-16.
</p>
<p>Littman, M. L.; Cassandra, A. R.; and Kaelbling, L. P. (1996). Efficient
dynamic-programming updates in partially observable Markov decision
processes. Technical Report CS-95-19, Brown University, Providence, RI.
</p>
<p>Zhang, N. L., and Liu, W. (1996). Planning in stochastic domains: Problem
characteristics and approximation. Technical Report HKUST-CS96-31,
Department of Computer Science, Hong Kong University of Science and
Technology.
</p>
<p>Pineau J., Geoffrey J Gordon G.J., Thrun S.B. (2003). Point-based value
iteration: an anytime algorithm for POMDPs. IJCAI'03: Proceedings of the
18th international joint conference on Artificial Intelligence. Pages
1025-1030.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other solver: 
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># display available solver options which can be passed on to pomdp-solve as parameters.
solve_POMDP_parameter()

################################################################
# Example 1: Solving the simple infinite-horizon Tiger problem
data("Tiger")
Tiger

# look at the model as a list
unclass(Tiger)

# inspect an individual field of the model (e.g., the transition probabilities and the reward)
Tiger$transition_prob
Tiger$reward

sol &lt;- solve_POMDP(model = Tiger)
sol

# look at the solution
sol$solution

# policy (value function (alpha vectors), optimal action and observation dependent transitions)
policy(sol)

# plot the policy graph of the infinite-horizon POMDP
plot_policy_graph(sol)

# value function
plot_value_function(sol, ylim = c(0,20))

################################################################
# Example 2: Solve a problem specified as a POMDP file
#            using a grid of size 20
sol &lt;- solve_POMDP("http://www.pomdp.org/examples/cheese.95.POMDP",
  method = "grid", parameter = list(fg_points = 20))
sol

policy(sol)
plot_policy_graph(sol)

# Example 3: Solving a finite-horizon POMDP using the incremental
#            pruning method (without discounting)
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1, method = "incprune")
sol

# look at the policy tree
policy(sol)
plot_policy_graph(sol)
# note: only open the door in epoch 3 if you get twice the same observation.

# Expected reward starting for the models initial belief (uniform):
#   listen twice and then open the door or listen 3 times
reward(sol)

# Expected reward for listen twice (-2) and then open-left (-1 + (-1) + 10 = 8)
reward(sol, belief = c(1,0))

# Expected reward for just opening the right door (10)
reward(sol, belief = c(1,0), epoch = 3)

# Expected reward for just opening the right door (0.5 * -100 + 0.95 * 10 = 4.5)
reward(sol, belief = c(.95,.05), epoch = 3)

################################################################
# Example 3: Using terminal values (state-dependent utilities after the final epoch)
#
# Specify 1000 if the tiger is right after 3 (horizon) epochs
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1,  method = "incprune",
  terminal_values = c(0, 1000))
sol

policy(sol)
# Note: The optimal strategy is to never open the left door. If we think the
#  Tiger is behind the right door, then we just wait for the final payout. If
#  we think the tiger might be behind the left door, then we open the right
#  door, are likely to get a small reward and the tiger has a chance of 50\% to
#  move behind the right door. The second episode is used to gather more
#  information for the more important #  final action.

################################################################
# Example 4: Model time-dependent transition probabilities

# The tiger reacts normally for 3 epochs (goes randomly two one
# of the two doors when a door was opened). After 3 epochs he gets
# scared and when a door is opened then he always goes to the other door.

# specify the horizon for each of the two different episodes
Tiger_time_dependent &lt;- Tiger
Tiger_time_dependent$name &lt;- "Scared Tiger Problem"
Tiger_time_dependent$horizon &lt;- c(normal_tiger = 3, scared_tiger = 3)
Tiger_time_dependent$transition_prob &lt;- list(
  normal_tiger = list(
    "listen" = "identity",
    "open-left" = "uniform",
    "open-right" = "uniform"),
  scared_tiger = list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )
)

# Tiger_time_dependent (a higher value for verbose will show more messages)

sol &lt;- solve_POMDP(model = Tiger_time_dependent, discount = 1,
  method = "incprune", verbose = 1)
sol

policy(sol)

# note that the default method to estimate the belief for nodes is following a
#  trajectory which uses only the first belief reached for each node. Random sampling
#  can find a better estimate of the central belief of the segment (see nodes 4-1 to 6-3
#  in the plots below).
plot_policy_graph(sol)
plot_policy_graph(sol, method = "random_sample")

################################################################
# Example 5: Alternative method to solve time-dependent POMDPs

# 1) create the scared tiger model
Tiger_scared &lt;- Tiger
Tiger_scared$transition_prob &lt;- list(
    "listen" = "identity",
    "open-left" = rbind(c(0, 1), c(0, 1)),
    "open-right" = rbind(c(1, 0), c(1, 0))
  )

# 2) Solve in reverse order. Scared tiger without terminal values first.
sol_scared &lt;- solve_POMDP(model = Tiger_scared,
  horizon = 3, discount = 1,  method = "incprune")
sol_scared
policy(sol_scared)

# 3) Solve the regular tiger with the value function of the scared tiger as terminal values
sol &lt;- solve_POMDP(model = Tiger,
  horizon = 3, discount = 1, method = "incprune",
  terminal_values = sol_scared$solution$alpha[[1]])
sol
policy(sol)
# Note: it is optimal to mostly listen till the Tiger gets in the scared mood. Only if
#  we are extremely sure in the first epoch, then opening a door is optimal.

################################################################
# Example 6: PBVI with a custom grid

# Create a search grid by sampling from the belief space in
#   10 regular intervals
custom_grid &lt;- sample_belief_space(Tiger, n = 10, method = "regular")
head(custom_grid)

# Visualize the search grid
plot_belief_space(sol, sample = custom_grid)

# Solve the POMDP using the grid for approximation
sol &lt;- solve_POMDP(Tiger, method = "grid", parameter = list(grid = custom_grid))
policy(sol)
plot_policy_graph(sol)

# note that plot_policy_graph() automatically remove nodes that are unreachable from the
#  initial node. This behavior can be switched off.
plot_policy_graph(sol, remove_unreachable_nodes = FALSE)
</code></pre>

<hr>
<h2 id='solve_SARSOP'>Solve a POMDP Problem using SARSOP</h2><span id='topic+solve_SARSOP'></span>

<h3>Description</h3>

<p>This function uses the C++ implementation of the SARSOP algorithm
by Kurniawati, Hsu and Lee (2008) interfaced in
package <span class="pkg">sarsop</span>
to solve infinite horizon problems that are formulated as partially observable Markov
decision processes (POMDPs). The result is an optimal or approximately
optimal policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_SARSOP(
  model,
  horizon = Inf,
  discount = NULL,
  terminal_values = NULL,
  method = "sarsop",
  digits = 7,
  parameter = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_SARSOP_+3A_model">model</code></td>
<td>
<p>a POMDP problem specification created with <code><a href="#topic+POMDP">POMDP()</a></code>.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_horizon">horizon</code></td>
<td>
<p>SARSOP only supports <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_discount">discount</code></td>
<td>
<p>discount factor in range <code class="reqn">[0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_terminal_values">terminal_values</code></td>
<td>
<p><code>NULL</code>. SARSOP does not use terminal values.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_method">method</code></td>
<td>
<p>string; there is only one method available called <code>"sarsop"</code>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_digits">digits</code></td>
<td>
<p>precision used when writing POMDP files (see
<code><a href="#topic+write_POMDP">write_POMDP()</a></code>).</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_parameter">parameter</code></td>
<td>
<p>a list with parameters passed on to
the function <code><a href="sarsop.html#topic+appl">sarsop::pomdpsol()</a></code> in package <span class="pkg">sarsop</span>.</p>
</td></tr>
<tr><td><code id="solve_SARSOP_+3A_verbose">verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the solver in the R console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The solver returns an object of class POMDP which is a list with the
model specifications (<code>'model'</code>), the solution (<code>'solution'</code>), and the
solver output (<code>'solver_output'</code>).
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Carl Boettiger, Jeroen Ooms and Milad Memarzadeh (2020). sarsop:
Approximate POMDP Planning Software. R package version 0.6.6.
https://CRAN.R-project.org/package=sarsop
</p>
<p>H. Kurniawati, D. Hsu, and W.S. Lee (2008). SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Proc. Robotics: Science and Systems.
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>
<p>Other solver: 
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Solving the simple infinite-horizon Tiger problem with SARSOP
# You need to install package "sarsop"
data("Tiger")
Tiger

sol &lt;- solve_SARSOP(model = Tiger)
sol

# look at solver output
sol$solver_output

# policy (value function (alpha vectors), optimal action and observation dependent transitions)
policy(sol)

# value function
plot_value_function(sol, ylim = c(0,20))

# plot the policy graph
plot_policy_graph(sol)

# reward of the optimal policy
reward(sol)

# Solve a problem specified as a POMDP file. The timeout is set to 10 seconds.
sol &lt;- solve_SARSOP("http://www.pomdp.org/examples/cheese.95.POMDP", parameter = list(timeout = 10))
sol

## End(Not run)

</code></pre>

<hr>
<h2 id='Tiger'>Tiger Problem POMDP Specification</h2><span id='topic+Tiger'></span><span id='topic+Three_doors'></span>

<h3>Description</h3>

<p>The model for the Tiger Problem introduces in Cassandra et al (1994).
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+POMDP">POMDP</a>.
</p>


<h3>Details</h3>

<p>The original Tiger problem was published in Cassandra et al (1994) as
follows:
</p>
<p>An agent is facing two closed doors and a tiger is put with equal
probability behind one of the two doors represented by the states
<code>tiger-left</code> and <code>tiger-right</code>, while treasure is put behind the other door.
The possible actions are <code>listen</code> for tiger noises or opening a door (actions
<code>open-left</code> and <code>open-right</code>). Listening is neither free (the action has a
reward of -1)  nor is it entirely accurate. There is a 15\
probability that the agent hears the tiger behind the left door while it is
actually behind the right door and vice versa. If the agent opens  door with
the tiger, it will get hurt (a negative reward of -100), but if it opens the
door with the treasure, it will receive a positive reward of 10. After a door
is opened, the problem is reset(i.e., the tiger is randomly assigned to a
door with chance 50/50) and the the agent gets another try.
</p>
<p>The three doors problem is an extension of the Tiger problem where the tiger
is behind one of three doors represented by three states (<code>tiger-left</code>,
<code>tiger-center</code>, and <code>tiger-right</code>) and treasure is behind the other two
doors. There are also three open actions and three different observations for
listening.
</p>


<h3>References</h3>

<p>Anthony R. Cassandra, Leslie P Kaelbling, and Michael L.
Littman (1994). Acting Optimally in Partially Observable Stochastic Domains.
In Proceedings of the Twelfth National Conference on Artificial
Intelligence, pp. 1023-1028.
</p>


<h3>See Also</h3>

<p>Other POMDP_examples: 
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+POMDP_example_files">POMDP_example_files</a></code>,
<code><a href="#topic+RussianTiger">RussianTiger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
Tiger

data("Three_doors")
Three_doors
</code></pre>

<hr>
<h2 id='transition_graph'>Transition Graph</h2><span id='topic+transition_graph'></span><span id='topic+plot_transition_graph'></span>

<h3>Description</h3>

<p>Returns the transition model as an <span class="pkg">igraph</span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transition_graph(
  x,
  action = NULL,
  episode = NULL,
  epoch = NULL,
  state_col = NULL,
  simplify_transitions = TRUE,
  remove_unavailable_actions = TRUE
)

plot_transition_graph(
  x,
  action = NULL,
  episode = NULL,
  epoch = NULL,
  state_col = NULL,
  simplify_transitions = TRUE,
  main = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transition_graph_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a>.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_action">action</code></td>
<td>
<p>the name or id of an action or a set of actions. Bey default the transition model for all actions is returned.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_episode">episode</code>, <code id="transition_graph_+3A_epoch">epoch</code></td>
<td>
<p>Episode or epoch used for time-dependent POMDPs. Epochs are internally converted to the episode using the model horizon.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_state_col">state_col</code></td>
<td>
<p>colors used to represent the states.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_simplify_transitions">simplify_transitions</code></td>
<td>
<p>logical; combine parallel transition arcs into a single arc.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_remove_unavailable_actions">remove_unavailable_actions</code></td>
<td>
<p>logical; don't show arrows for unavailable actions.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_main">main</code></td>
<td>
<p>a main title for the plot.</p>
</td></tr>
<tr><td><code id="transition_graph_+3A_...">...</code></td>
<td>
<p>further arguments are passed on to <code>igraph::plot.igraph()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The transition model of a POMDP/MDP is a Markov Chain. This function extracts the transition model as
an igraph object.
</p>


<h3>Value</h3>

<p>returns the transition model as an igraph object.
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")

g &lt;- transition_graph(Tiger)
g

plot_transition_graph(Tiger)
plot_transition_graph(Tiger, vertex.size = 20, 
                      edge.label.cex = .5, edge.arrow.size = .5, margin = .5)
plot_transition_graph(Tiger, vertex.size = 60, 
                      edge.label = NA, edge.arrow.size = .5, 
                      layout = rbind(c(-1,0), c(+1,0)), rescale = FALSE)

## Plot an individual graph for each actions and use a manual layout.
for (a in Tiger$actions) {
 plot_transition_graph(Tiger, action = a, 
                        layout = rbind(c(-1,0), c(+1,0)), rescale = FALSE,
                        main = paste("action:", a))
}

## Plot using the igraph library
library(igraph)
plot(g)

# plot with a fixed layout and curved edges
plot(g,
 layout = rbind(c(-1, 0), c(1, 0)), rescale = FALSE,
 edge.curved = curve_multiple_directed(g, .8),
 edge.loop.angle = -pi / 4,
 vertex.size = 60
 )

## Use visNetwork (if installed)
if(require(visNetwork)) {

g_vn &lt;- toVisNetworkData(g)
nodes &lt;- g_vn$nodes
edges &lt;- g_vn$edges

# add manual layout
nodes$x &lt;- c(-1, 1) * 200
nodes$y &lt;- 0

visNetwork(nodes, edges)  %&gt;%
  visNodes(physics = FALSE) %&gt;%
  visEdges(smooth = list(type = "curvedCW", roundness = .6), arrows = "to")
}
</code></pre>

<hr>
<h2 id='update_belief'>Belief Update</h2><span id='topic+update_belief'></span>

<h3>Description</h3>

<p>Update the belief given a taken action and observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_belief(
  model,
  belief = NULL,
  action = NULL,
  observation = NULL,
  episode = 1,
  digits = 7,
  drop = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_belief_+3A_model">model</code></td>
<td>
<p>a <a href="#topic+POMDP">POMDP</a> object.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_belief">belief</code></td>
<td>
<p>the current belief state.
Defaults to the start belief state specified in
the model or &quot;uniform&quot;.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_action">action</code></td>
<td>
<p>the taken action. Can also be a vector of multiple actions or, if missing, then all actions are evaluated.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_observation">observation</code></td>
<td>
<p>the received observation. Can also be a vector of multiple observations or, if missing, then all observations are evaluated.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_episode">episode</code></td>
<td>
<p>Use transition and observation matrices for the given episode
for time-dependent POMDPs (see <a href="#topic+POMDP">POMDP</a>).</p>
</td></tr>
<tr><td><code id="update_belief_+3A_digits">digits</code></td>
<td>
<p>round decimals.</p>
</td></tr>
<tr><td><code id="update_belief_+3A_drop">drop</code></td>
<td>
<p>logical; drop the result to a vector if only a single belief
state is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Update the belief state <code class="reqn">b</code> (<code>belief</code>) with an action <code class="reqn">a</code> and observation <code class="reqn">o</code> using the update
<code class="reqn">b' \leftarrow \tau(b, a, o)</code> defined so that
</p>
<p style="text-align: center;"><code class="reqn">b'(s') = \eta O(o | s',a) \sum_{s \in S} T(s' | s,a) b(s)</code>
</p>

<p>where <code class="reqn">\eta = 1/ \sum_{s' \in S}[ O(o | s',a) \sum_{s \in S} T(s' | s,a) b(s)]</code> normalizes the new belief state so the probabilities add up to one.
</p>


<h3>Value</h3>

<p>returns the updated belief state as a named vector.
If <code>action</code> or <code>observations</code> is a vector with multiple elements ot missing, then a matrix with all
resulting belief states is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

update_belief(c(.5,.5), model = Tiger)
update_belief(c(.5,.5), action = "listen", observation = "tiger-left", model = Tiger)
update_belief(c(.15,.85), action = "listen", observation = "tiger-right", model = Tiger)

</code></pre>

<hr>
<h2 id='value_function'>Value Function</h2><span id='topic+value_function'></span><span id='topic+plot_value_function'></span>

<h3>Description</h3>

<p>Extracts the value function from a solved model.
Extracts the alpha vectors describing the value function. This is similar to <code><a href="#topic+policy">policy()</a></code> which in addition returns the
action prescribed by the solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>value_function(model, drop = TRUE)

plot_value_function(
  model,
  projection = NULL,
  epoch = 1,
  ylim = NULL,
  legend = TRUE,
  col = NULL,
  lwd = 1,
  lty = 1,
  ylab = "Value",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="value_function_+3A_model">model</code></td>
<td>
<p>a solved <a href="#topic+POMDP">POMDP</a> or <a href="#topic+MDP">MDP</a>.</p>
</td></tr>
<tr><td><code id="value_function_+3A_drop">drop</code></td>
<td>
<p>logical; drop the list for converged converged, epoch-independent value functions.</p>
</td></tr>
<tr><td><code id="value_function_+3A_projection">projection</code></td>
<td>
<p>Sample in a projected belief space. See <code><a href="#topic+projection">projection()</a></code> for details.</p>
</td></tr>
<tr><td><code id="value_function_+3A_epoch">epoch</code></td>
<td>
<p>the value function of what epoch should be plotted? Use 1 for
converged policies.</p>
</td></tr>
<tr><td><code id="value_function_+3A_ylim">ylim</code></td>
<td>
<p>the y limits of the plot.</p>
</td></tr>
<tr><td><code id="value_function_+3A_legend">legend</code></td>
<td>
<p>logical; show the actions in the visualization?</p>
</td></tr>
<tr><td><code id="value_function_+3A_col">col</code></td>
<td>
<p>potting colors.</p>
</td></tr>
<tr><td><code id="value_function_+3A_lwd">lwd</code></td>
<td>
<p>line width.</p>
</td></tr>
<tr><td><code id="value_function_+3A_lty">lty</code></td>
<td>
<p>line type.</p>
</td></tr>
<tr><td><code id="value_function_+3A_ylab">ylab</code></td>
<td>
<p>label for the y-axis.</p>
</td></tr>
<tr><td><code id="value_function_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to <code><a href="stats.html#topic+line">stats::line()</a></code>
or <code><a href="graphics.html#topic+barplot">graphics::barplot()</a></code>'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the value function of a POMDP solution as a line plot. The solution is
projected on two states (i.e., the belief for the other states is held
constant at zero). The value function can also be visualized using <code><a href="#topic+plot_belief_space">plot_belief_space()</a></code>.
</p>


<h3>Value</h3>

<p>the function as a matrix with alpha vectors as rows.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code><a href="#topic+estimate_belief_for_nodes">estimate_belief_for_nodes</a>()</code>,
<code><a href="#topic+optimal_action">optimal_action</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+plot_policy_graph">plot_policy_graph</a>()</code>,
<code><a href="#topic+policy">policy</a>()</code>,
<code><a href="#topic+policy_graph">policy_graph</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reward">reward</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>
</p>
<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+write_POMDP">write_POMDP</a>()</code>
</p>
<p>Other MDP: 
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+MDP_policy_functions">MDP_policy_functions</a></code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+gridworld">gridworld</a></code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+simulate_MDP">simulate_MDP</a>()</code>,
<code><a href="#topic+solve_MDP">solve_MDP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tiger")
sol &lt;- solve_POMDP(Tiger)
sol

# value function for the converged solution
value_function(sol)

plot_value_function(sol, ylim = c(0,20))

## finite-horizon problem
sol &lt;- solve_POMDP(model = Tiger, horizon = 3, discount = 1,
  method = "enum")
sol

# inspect the value function for all epochs
value_function(sol)

plot_value_function(sol, epoch = 1, ylim = c(-5, 25))
plot_value_function(sol, epoch = 2, ylim = c(-5, 25))
plot_value_function(sol, epoch = 3, ylim = c(-5, 25))

## Not run: 
# using ggplot2 to plot the value function for epoch 3
library(ggplot2)
pol &lt;- policy(sol)
ggplot(pol[[3]]) +
 geom_segment(aes(x = 0, y = `tiger-left`, xend = 1, yend = `tiger-right`, color = action)) +
 coord_cartesian(ylim = c(-5, 15)) + ylab("Value") + xlab("Belief space")

## End(Not run)
</code></pre>

<hr>
<h2 id='Windy_gridworld'>Windy Gridworld MDP</h2><span id='topic+Windy_gridworld'></span><span id='topic+windy_gridworld'></span>

<h3>Description</h3>

<p>The Windy gridworld MDP example from Chapter 6 of the textbook
&quot;Reinforcement Learning: An Introduction.&quot;
</p>


<h3>Format</h3>

<p>An object of class <a href="#topic+MDP">MDP</a>.
</p>


<h3>Details</h3>

<p>The gridworld has the following layout:
</p>
<p><img src="../help/figures/windy-gridworld.png" alt="Windy Gridworld." />
</p>
<p>The grid world is represented as a 7 x 10 matrix of states.
In the middle region the next states are shifted upward by wind
(the strength in number of squares is given below each column).
For example, if the agent is one cell to the right of the goal,
then the action left takes the agent to the cell just above the goal.
</p>
<p>No discounting is used (i.e., <code class="reqn">\gamma = 1</code>).
</p>


<h3>References</h3>

<p>Richard S. Sutton and Andrew G. Barto (2018). Reinforcement Learning: An Introduction
Second Edition, MIT Press, Cambridge, MA.
</p>


<h3>See Also</h3>

<p>Other MDP_examples: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+MDP">MDP</a>()</code>,
<code><a href="#topic+Maze">Maze</a></code>
</p>
<p>Other gridworld: 
<code><a href="#topic+Cliff_walking">Cliff_walking</a></code>,
<code><a href="#topic+Maze">Maze</a></code>,
<code><a href="#topic+gridworld">gridworld</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Windy_gridworld)
Windy_gridworld

gridworld_matrix(Windy_gridworld)
gridworld_matrix(Windy_gridworld, what = "labels")

# The Goal is an absorbing state 
which(absorbing_states(Windy_gridworld))

# visualize the transition graph
gridworld_plot_transition_graph(Windy_gridworld, 
  vertex.size = 10, vertex.label = NA)

# solve using value iteration
sol &lt;- solve_MDP(Windy_gridworld) 
sol
policy(sol)
gridworld_plot_policy(sol)
</code></pre>

<hr>
<h2 id='write_POMDP'>Read and write a POMDP Model to a File in POMDP Format</h2><span id='topic+write_POMDP'></span><span id='topic+read_POMDP'></span>

<h3>Description</h3>

<p>Reads and write a POMDP file suitable for the <code>pomdp-solve</code> program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_POMDP(x, file, digits = 7, labels = FALSE)

read_POMDP(file, parse = TRUE, normalize = FALSE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_POMDP_+3A_x">x</code></td>
<td>
<p>an object of class <a href="#topic+POMDP">POMDP</a>.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_file">file</code></td>
<td>
<p>a file name. <code>read_POMDP()</code> also accepts <a href="base.html#topic+connections">connections</a> including URLs.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_digits">digits</code></td>
<td>
<p>precision for writing numbers (digits after the decimal
point).</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_labels">labels</code></td>
<td>
<p>logical; write original labels or use index numbers? Labels are
restricted to <code style="white-space: pre;">&#8288;[a-zA-Z0-9_-]&#8288;</code> and the first character has to be a letter.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_parse">parse</code></td>
<td>
<p>logical; try to parse the model maotrices.
Solvers still work with unparsed matrices, but helpers for simulation are not available.</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_normalize">normalize</code></td>
<td>
<p>logical; should the description be normalized for faster access (see <code><a href="#topic+normalize_POMDP">normalize_POMDP()</a></code>)?</p>
</td></tr>
<tr><td><code id="write_POMDP_+3A_verbose">verbose</code></td>
<td>
<p>logical; report parsed lines. This is useful for debugging a POMDP file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><a href="#topic+POMDP">POMDP</a> objects read from a POMDP file have an extra element called <code>problem</code> which contains the original
POMDP specification. <strong>The original specification is directly used by external solvers.</strong> In addition, the file
is parsed using an experimental POMDP file parser. The parsed information can be used with auxiliary functions
in this package that use fields like the transition matrix, the observation matrix and the reward structure.
</p>
<p>The range of useful rewards is restricted by the solver. Here the values are restricted to the range
<code style="white-space: pre;">&#8288;[-1e10, 1e10]&#8288;</code>.
Unavailable actions have a reward of <code>-Inf</code> which is translated to -2 times the maximum
absolute reward value used in the model.
</p>
<p><strong>Notes:</strong>
The parser for POMDP files is experimental. Please report
problems here: <a href="https://github.com/mhahsler/pomdp/issues">https://github.com/mhahsler/pomdp/issues</a>.
</p>


<h3>Value</h3>

<p><code>read_POMDP()</code> returns a <a href="#topic+POMDP">POMDP</a> object.
</p>


<h3>Author(s)</h3>

<p>Hossein Kamalzadeh, Michael Hahsler
</p>


<h3>References</h3>

<p>POMDP solver website: https://www.pomdp.org
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code><a href="#topic+MDP2POMDP">MDP2POMDP</a></code>,
<code><a href="#topic+POMDP">POMDP</a>()</code>,
<code><a href="#topic+accessors">accessors</a></code>,
<code><a href="#topic+actions">actions</a>()</code>,
<code><a href="#topic+add_policy">add_policy</a>()</code>,
<code><a href="#topic+plot_belief_space">plot_belief_space</a>()</code>,
<code><a href="#topic+projection">projection</a>()</code>,
<code><a href="#topic+reachable_and_absorbing">reachable_and_absorbing</a></code>,
<code><a href="#topic+regret">regret</a>()</code>,
<code><a href="#topic+sample_belief_space">sample_belief_space</a>()</code>,
<code><a href="#topic+simulate_POMDP">simulate_POMDP</a>()</code>,
<code><a href="#topic+solve_POMDP">solve_POMDP</a>()</code>,
<code><a href="#topic+solve_SARSOP">solve_SARSOP</a>()</code>,
<code><a href="#topic+transition_graph">transition_graph</a>()</code>,
<code><a href="#topic+update_belief">update_belief</a>()</code>,
<code><a href="#topic+value_function">value_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tiger)

## show the POMDP file that would be written.
write_POMDP(Tiger, file = stdout())
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
