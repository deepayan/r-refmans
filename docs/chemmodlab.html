<!DOCTYPE html><html><head><title>Help for package chemmodlab</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {chemmodlab}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aid364'><p>Cytotoxicity assay using the Jurkat human T-Cell line</p></a></li>
<li><a href='#ApplicabilityDomain'><p>Compute applicability domain for a chemmodlab model</p></a></li>
<li><a href='#chemmodlab'><p>Constructor for the chemmodlab object</p></a></li>
<li><a href='#CombineSplits'><p>ANOVA and multiple comparisons for chemmodlab objects</p></a></li>
<li><a href='#HitEnrich'><p>Plot hit enrichment curves from observed scores and activities</p></a></li>
<li><a href='#HitEnrichDiff'><p>Plot differences between hit enrichment curves</p></a></li>
<li><a href='#MakeModelDefaults'><p>Model parameters for ModelTrain</p></a></li>
<li><a href='#ModelTrain'><p>Fit predictive models to sets of descriptors.</p></a></li>
<li><a href='#PerfCurveBands'><p>Construct a confidence band for a recall or precision curve</p></a></li>
<li><a href='#PerfCurveTest'><p>Perform a hypothesis test for the difference between two performance curves.</p></a></li>
<li><a href='#plot.chemmodlab'><p>Plot method for the chemmodlab class.</p></a></li>
<li><a href='#pparg'><p>Docking scores for 3212 ligands for target PPARg</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Cheminformatics Modeling Laboratory for Fitting and Assessing
Machine Learning Models</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-05-01</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains a set of methods for fitting models and methods for
    validating the resulting models. The statistical methodologies comprise
    a comprehensive collection of approaches whose validity and utility have
    been accepted by experts in the Cheminformatics field. As promising new
    methodologies emerge from the statistical and data-mining communities, they
    will be incorporated into the laboratory. These methods are aimed at discovering
    quantitative structure-activity relationships (QSARs). However, the user can
    directly input their own choices of descriptors and responses, so the capability
    for comparing models is effectively unlimited.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jrash/ChemModLab">https://github.com/jrash/ChemModLab</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jrash/ChemModLab/issues">https://github.com/jrash/ChemModLab/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>KernSmooth, MSQC, class (&ge; 7.3.14), e1071 (&ge; 1.6.7),
elasticnet (&ge; 1.1), lars(&ge; 1.2), MASS(&ge; 7.3.45), nnet(&ge;
7.3.12), pROC(&ge; 1.8), randomForest(&ge; 4.6.12), rpart(&ge;
4.1.10), tree (&ge; 1.0.37), pls (&ge; 2.5.0), caret (&ge; 6.0-71),
stats, graphics, grDevices, utils, methods</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Jacqueline Hughes-Oliver [aut],
  Jeremy Ash [aut, cre],
  Atina Brooks [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jeremy Ash &lt;jrash@ncsu.edu&gt;</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, vdiffr (&ge; 0.3.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-01 20:30:29 UTC; Jeremy Ash Local</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-01 23:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aid364'>Cytotoxicity assay using the Jurkat human T-Cell line</h2><span id='topic+aid364'></span>

<h3>Description</h3>

<p>These data are from a cytotoxicity assay conducted by the Scripps Research
Institute Molecular Screening Center. There are 500 compounds assessed for toxicity using the
the Jurkat human T-Cell line. 50 of these compounds were active (toxic). Visit 
<a href="https://pubchem.ncbi.nlm.nih.gov/bioassay/364">https://pubchem.ncbi.nlm.nih.gov/bioassay/364</a> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aid364
</code></pre>


<h3>Format</h3>

<p>Data frame with 500 rows and 173 columns.  The first column contains the compound ids. 
The second contains the outcome of the assay (a binary variable, indicating active/inactive).
The next columns are chemical descriptor columns.  Two descriptor sets are present.
Both of these sets were computed
using the software, PowerMV - see Liu et al. (2005) for more information. The first set of 24
continuous descriptors are a modification of the Burden number descriptors (Burden, 1989). 
The second set contains 147 binary descriptors,
indicating the presence/absence of &quot;pharmacophore&quot; features, described in more detail in
Liu et al. (2005).
</p>


<h3>Source</h3>

<p><a href="https://pubchem.ncbi.nlm.nih.gov/bioassay/364">https://pubchem.ncbi.nlm.nih.gov/bioassay/364</a>
</p>


<h3>References</h3>

<p>Burden, F. R. (1989). Molecular identification number for substructure searches.
Journal of Chemical Information and Computer Sciences, 29(3), 225-227.
</p>
<p>Liu, K., Feng, J., &amp; Young, S. S. (2005). PowerMV: a software environment for 
molecular viewing, descriptor generation, data analysis and hit evaluation. Journal 
of chemical information and modeling, 45(2), 515-522.
</p>

<hr>
<h2 id='ApplicabilityDomain'>Compute applicability domain for a chemmodlab model</h2><span id='topic+ApplicabilityDomain'></span>

<h3>Description</h3>

<p><code>ApplicabilityDomain</code> evaluates the applicability domain for a chemmodlab model
using a Hotteling T2 control chart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ApplicabilityDomain(traindata, testdata, pvalue = 0.01, desname = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ApplicabilityDomain_+3A_traindata">traindata</code></td>
<td>
<p>training data</p>
</td></tr>
<tr><td><code id="ApplicabilityDomain_+3A_testdata">testdata</code></td>
<td>
<p>test data</p>
</td></tr>
<tr><td><code id="ApplicabilityDomain_+3A_pvalue">pvalue</code></td>
<td>
<p>significance level for control limit threshold</p>
</td></tr>
<tr><td><code id="ApplicabilityDomain_+3A_desname">desname</code></td>
<td>
<p>descriptor set name</p>
</td></tr>
</table>

<hr>
<h2 id='chemmodlab'>Constructor for the chemmodlab object</h2><span id='topic+chemmodlab'></span>

<h3>Description</h3>

<p>Constructor for the chemmodlab object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chemmodlab(
  all.preds,
  all.probs,
  model.acc,
  classify,
  responses,
  data,
  params,
  des.names,
  models,
  nsplits
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chemmodlab_+3A_all.preds">all.preds</code></td>
<td>
<p>a list of lists of dataframes.  The elements of the outer
list correspond to each split performed by <code><a href="#topic+ModelTrain">ModelTrain</a></code>. The
elements of the inner list correspond to each descriptor set.  For each
descriptor set and CV split combination, the output is a dataframe
containing all model predictions.  The first column of each data frame
contains the true value of the response.  The remaining columns correspond
to the models fit to the data.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_all.probs">all.probs</code></td>
<td>
<p>a list of lists of dataframes. Constructed only if there is
a binary response.  The structure is the same as <code>all.preds</code>, except
that predictions are replaced by predicted probabilities of a response
value of one.  Predicted
probabilities are only reported for classification models (see
<code><a href="#topic+ModelTrain">ModelTrain</a></code>)</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_model.acc">model.acc</code></td>
<td>
<p>a list of lists of model accuracy measures.  The elements of
the outer list correspond each split performed by <code><a href="#topic+ModelTrain">ModelTrain</a></code>.
The elements of the inner list correspond to each descriptor set.  For each
descriptor set and CV split combination model accuracy measures for each model fit
to the data.  Regression models are assessed with Pearson's <code class="reqn">r</code> and
<code class="reqn">RMSE</code> Classification models are assessed with contingency tables.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_classify">classify</code></td>
<td>
<p>a logical.  Was classification models used for binary
response?</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_responses">responses</code></td>
<td>
<p>a numeric vector.  The true value of the response.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_data">data</code></td>
<td>
<p>a list of numeric matrices.  Each matrix is a descriptor set used
as model input. The first column of each matrix is the response vector, and
the remaining columns are descriptors.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_params">params</code></td>
<td>
<p>a list of dataframes as made by
<code><a href="#topic+MakeModelDefaults">MakeModelDefaults</a></code>.  Each dataframe contains the parameters to
be set for a particular model.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_des.names">des.names</code></td>
<td>
<p>a character vector specifying the descriptor set names.  NA if 
unspecified.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_models">models</code></td>
<td>
<p>a character vector specifying the models fit to the data.</p>
</td></tr>
<tr><td><code id="chemmodlab_+3A_nsplits">nsplits</code></td>
<td>
<p>number of CV splits performed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacqueline Hughes-Oliver, Jeremy Ash
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chemmodlab">chemmodlab</a></code>, <code><a href="#topic+plot.chemmodlab">plot.chemmodlab</a></code>,
<code><a href="#topic+CombineSplits">CombineSplits</a></code>,
</p>

<hr>
<h2 id='CombineSplits'>ANOVA and multiple comparisons for chemmodlab objects</h2><span id='topic+CombineSplits'></span><span id='topic+Performance'></span>

<h3>Description</h3>

<p><code>CombineSplits</code> evaluates a specified performance measure
across all splits created by <code><a href="#topic+ModelTrain">ModelTrain</a></code> and conducts
statistical tests to determine the best performing descriptor set and
model (D-M) combinations. <code>Performance</code> can evaluate many 
performance measures across all splits created by <code>ModelTrain</code>,
then outputs a data frame for each D-M combination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CombineSplits(cml.result, metric = "enhancement", m = NA, thresh = 0.5)

Performance(cml.result, metrics = "enhancement", m = NA, thresh = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CombineSplits_+3A_cml.result">cml.result</code></td>
<td>
<p>an object of class <code><a href="#topic+chemmodlab">chemmodlab</a></code>.</p>
</td></tr>
<tr><td><code id="CombineSplits_+3A_metric">metric</code></td>
<td>
<p>the model performance measure to use.  This should be
one of <code>error rate</code>, <code>enhancement</code>, <code>R2</code>,
<code>rho</code>, <code>auc</code>, <code>sensitivity</code>, <code>specificity</code>,
<code>ppv</code>, <code>fmeasure</code>.</p>
</td></tr>
<tr><td><code id="CombineSplits_+3A_m">m</code></td>
<td>
<p>the number of tests to use for binary model 
performance measures
(see Details). 
If <code>m</code> is not specified, 
<code>enhancement</code> uses <code>floor(min(300,n/4))</code>, 
where <code>n</code> is the number of observations. By default, 
all other binary performance measures are computed using all observations.</p>
</td></tr>
<tr><td><code id="CombineSplits_+3A_thresh">thresh</code></td>
<td>
<p>if the predicted probability that a binary response is 
1 is above this threshold, an observation is classified as 1. Used
to compute <code>error rate</code>, <code>sensitivity</code>, 
<code>specificity</code>, <code>ppv</code>, and <code>fmeasure</code>.</p>
</td></tr>
<tr><td><code id="CombineSplits_+3A_metrics">metrics</code></td>
<td>
<p>a character vector containing a subset of the performance
measures above.  <code>Performance</code> can compute several
measures.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CombineSplits</code>
quantifies how sensitive performance measures are to fold
assignments (assignments to training and test sets). 
Intuitively, this
assesses how much a performance measure may change if a slightly
different data set is used.
</p>
<p><code>ModelTrain</code> is a designed study in that 'experimental' 
conditions are defined according to two factors: method (D-M combination) 
and split (fold assignment).  The factor &quot;split&quot; is a blocking factor,
and factor &quot;method&quot; is of primary interest.  The design of this
experiment is amenable to an analysis
of variance to identify significant differences between 
performance measures according to factors and levels.
CombineSplits outputs such an analysis of variance decomposition.
</p>
<p>The multiple comparisons similarity (MCS) plot shows the results
for tests for signficance
in all pairwise differences of D-M mean performance measures.
Because there can be many 
estimated mean performance measures for a dataset, care must be taken 
to adjust for
multiple testing, and we do this using the Tukey-Kramer multiple
comparison procedure (see Tukey (1953) and Kramer (1956)).
If you are having trouble viewing all the components of the plot,
make the plotting window larger.
</p>
<p>By default, <code>CombineSplits</code> uses initial enhancement
proposed by Kearsley et al. (1996) to assess model performance. 
Enhancement at <code>m</code> tests is the hit
rate at <code>m</code> tests (accumulated actives at <code>m</code> tests
divided by <code>m</code>) divided by the proportion of actives in the entire 
collection. It is a relative measure of hit rate improvement offered
by the new method beyond what can be expected under random selection,
and values much larger than one are desired. Initial enhancement is
typically taken to be enhancement at <code>m</code>=300 tests.
</p>
<p>Root mean squared error (<code>RMSE</code>), despite its popularity
in statistics, may be  
inappropriate for continuous chemical assay responses because
it assumes losses 
are equal for both under-predicting and over-predicting biological 
activity.  A suitable alternative may be initial <code>enhancement</code>.
Other options are the coeffcient of determination (<code>R2</code>)
and Spearman's <code>rho</code>.
</p>
<p>For binary chemical assay responses, alternatives to 
misclassification rate (<code>error rate</code>) 
(which may be inappropriate because it assigns equal weights to false
positives and false negatives) include <code>sensitivity</code>,
<code>specificity</code>,
area under the receiver operating characteristic curve (<code>auc</code>),
positive predictive value, also known as precision (<code>ppv</code>), F1 measure (<code>fmeasure</code>),
and initial <code>enhancement</code>.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>Performance</code>: outputs a data frame with performance measures for each D-M
combination.
</p>
</li></ul>


<h3>Author(s)</h3>

<p>Jacqueline Hughes-Oliver, Jeremy Ash, Atina Brooks
</p>


<h3>References</h3>

<p>Kearsley, S.K., Sallamack, S., Fluder, E.M., Andose, J.D., Mosley, R.T.,
and Sheridan, R.P. (1996). Chemical similarity using physiochemical
property descriptors, J. Chem. Inf. Comput. Sci. 36, 118-127.
</p>
<p>Kramer, C. Y. (1956). Extension of multiple range tests to group means
with unequal numbers of replications. Biometrics 12, 307-310.
</p>
<p>Tukey, J. W. (1953). The problem of multiple comparisons. Unpublished
manuscript. In The Collected Works of John W. Tukey VIII. Multiple
Comparisons: 1948-1983, Chapman and Hall, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chemmodlab">chemmodlab</a></code>, <code><a href="#topic+ModelTrain">ModelTrain</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# A data set with  binary response and multiple descriptor sets
data(aid364)

cml &lt;- ModelTrain(aid364, ids = TRUE, xcol.lengths = c(24, 147),
                  des.names = c("BurdenNumbers", "Pharmacophores"))
CombineSplits(cml)

## End(Not run)

# A continuous response
cml &lt;- ModelTrain(USArrests, nsplits = 2, nfolds = 2,
                  models = c("KNN", "Lasso", "Tree"))
CombineSplits(cml)

</code></pre>

<hr>
<h2 id='HitEnrich'>Plot hit enrichment curves from observed scores and activities</h2><span id='topic+HitEnrich'></span>

<h3>Description</h3>

<p>Plot hit enrichment curves based on scores from multiple algorithms. Actual
activities are required. Additionally plot the ideal hit enrichment curve
that would result under perfect scoring, and the hit enrichment curve that
would result under random scoring. Optionally, simultaneous confidence bands
may also be requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HitEnrich(
  S.df,
  labels = NULL,
  y,
  x.max = NULL,
  log = TRUE,
  title = "",
  conf = FALSE,
  conf.level = 0.95,
  method = "sup-t",
  plus = TRUE,
  band.frac = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HitEnrich_+3A_s.df">S.df</code></td>
<td>
<p>Data frame where variables are numeric scores from different
algorithms. Rows represent unique compounds.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_labels">labels</code></td>
<td>
<p>Character vector of labels for the different algorithms in
<code>S.df</code>. If missing, variable names in <code>S.df</code> will be used.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_y">y</code></td>
<td>
<p>Numeric vector of activity values. Activity values must be either 0
(inactive/undesirable) or 1 (active/desirable); no other values are
accepted. Compounds are assumed to be in the same order as in <code>S.df</code>.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_x.max">x.max</code></td>
<td>
<p>Integer, the maximum number of tests allowed on the x axis.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_log">log</code></td>
<td>
<p>Logical. <code>TRUE</code> plots the x axis on a log scale.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_title">title</code></td>
<td>
<p>Character string</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_conf">conf</code></td>
<td>
<p>Logical. <code>TRUE</code> plots (simultaneous) confidence bands for
all hit enrichment curves.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_conf.level">conf.level</code></td>
<td>
<p>Numeric, confidence coefficient</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_method">method</code></td>
<td>
<p>Character indicates the method used to obtain confidence bands.
The default is <code>sup-t</code> but other options (not recommended) are
&quot;theta-proj&quot; and &quot;bonf&quot;.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_plus">plus</code></td>
<td>
<p>Logical. <code>TRUE</code> uses plus-adjusted version of <code>method</code>.</p>
</td></tr>
<tr><td><code id="HitEnrich_+3A_band.frac">band.frac</code></td>
<td>
<p>Numeric vector of fractions tested to be used in obtaining
confidence bands. Vector should be no longer than <code>y</code>, and should have
at least 20 entries. Entries should be in (0,1]. It is recommended that
entries be consistent with between 1 and <code>x.max</code> tests.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code>x.max</code> is <code>length(y)</code>, so that hit enrichment
curves are obtained for all observable fractions, i.e., fractions of
<code>(1:length(y))/length(y)</code>. By default, confidence bands are evaluated
based on a smaller grid of 40 fractions. This smaller grid is evenly spaced
on either the original grid of <code>(1:length(y))/length(y)</code>, or the log
scale of the original grid.
</p>

<hr>
<h2 id='HitEnrichDiff'>Plot differences between hit enrichment curves</h2><span id='topic+HitEnrichDiff'></span>

<h3>Description</h3>

<p>Plot differences between hit enrichment curves based on scores from multiple
algorithms. Actual activities are required. Additionally plot simultaneous
confidence bands for these differences. Plots may be used to determine if one
algorithm is &quot;better&quot; than another algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HitEnrichDiff(
  S.df,
  labels = NULL,
  y,
  x.max = NULL,
  log = TRUE,
  title = "",
  conf.level = 0.95,
  method = "sup-t",
  plus = TRUE,
  band.frac = NULL,
  yrange = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HitEnrichDiff_+3A_s.df">S.df</code></td>
<td>
<p>Data frame where variables are numeric scores from at least 2
different algorithms. Rows represent unique compounds.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_labels">labels</code></td>
<td>
<p>Character vector of labels for the different algorithms in
<code>S.df</code>. If missing, variable names in <code>S.df</code> will be used.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_y">y</code></td>
<td>
<p>Numeric vector of activity values. Activity values must be either 0
(inactive/undesirable) or 1 (active/desirable); no other values are
accepted. Compounds are assumed to be in the same order as in <code>S.df</code>.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_x.max">x.max</code></td>
<td>
<p>Integer, the maximum number of tests allowed on the x axis.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_log">log</code></td>
<td>
<p>Logical. <code>TRUE</code> plots the x axis on a log scale.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_title">title</code></td>
<td>
<p>Character string</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_conf.level">conf.level</code></td>
<td>
<p>Numeric, confidence coefficient</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_method">method</code></td>
<td>
<p>Character indicates the method used to obtain confidence bands.
The default is <code>sup-t</code> but other options (not recommended) are
&quot;theta-proj&quot; and &quot;bonf&quot;.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_plus">plus</code></td>
<td>
<p>Logical. <code>TRUE</code> uses plus-adjusted version of <code>method</code>.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_band.frac">band.frac</code></td>
<td>
<p>Numeric vector of fractions tested to be used in obtaining
confidence bands. Vector should be no longer than <code>y</code>, and should have
at least 20 entries. Entries should be in (0,1]. It is recommended that
entries be consistent with between 1 and <code>x.max</code> tests.</p>
</td></tr>
<tr><td><code id="HitEnrichDiff_+3A_yrange">yrange</code></td>
<td>
<p>Numeric vector of length 2. The desired range for the y axis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code>x.max</code> is <code>length(y)</code>, so that hit enrichment
curves are obtained for all observable fractions, i.e., fractions of
<code>(1:length(y))/length(y)</code>. By default, confidence bands are evaluated
based on a smaller grid of 40 fractions. This smaller grid is evenly spaced
on either the original grid of <code>(1:length(y))/length(y)</code>, or the log
scale of the original grid.
</p>

<hr>
<h2 id='MakeModelDefaults'>Model parameters for ModelTrain</h2><span id='topic+MakeModelDefaults'></span>

<h3>Description</h3>

<p>Makes a list containing the default parameters for all
models implemented in <code><a href="#topic+ModelTrain">ModelTrain</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MakeModelDefaults(n, p, classify, nfolds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MakeModelDefaults_+3A_n">n</code></td>
<td>
<p>The number of observations in the data.</p>
</td></tr>
<tr><td><code id="MakeModelDefaults_+3A_p">p</code></td>
<td>
<p>The number of descriptors in the data.</p>
</td></tr>
<tr><td><code id="MakeModelDefaults_+3A_classify">classify</code></td>
<td>
<p>A logical.  Will classification models be used? (is
the response binary?)
If false, regression models will be assumed.</p>
</td></tr>
<tr><td><code id="MakeModelDefaults_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds used for k-fold cross validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sensible default values are selected for each
tunable model parameter, however users may set any parameter
manually by generating a list with this function and assigning
the parameters.
</p>
<p>See <a href="https://pages.github.ncsu.edu/jrash/chemmodlab/">https://pages.github.ncsu.edu/jrash/chemmodlab/</a> for more information about the
models available (including model default parameters).
</p>


<h3>Value</h3>

<p>A list whose elements are dataframes containing the
default parameter values for models implemented in
<code><a href="#topic+ModelTrain">ModelTrain</a></code>.
</p>


<h3>Author(s)</h3>

<p>Jeremy Ash
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ModelTrain">ModelTrain</a></code>, <code><a href="#topic+chemmodlab">chemmodlab</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>params &lt;- MakeModelDefaults(n = nrow(USArrests),
 p = ncol(USArrests[, -1]), classify = TRUE, nfolds = 10)
params$Forest$mtry &lt;- ncol(USArrests[, -1])-1
params

cml &lt;- ModelTrain(USArrests, models = "RF", nsplits = 3,
 user.params = params)


</code></pre>

<hr>
<h2 id='ModelTrain'>Fit predictive models to sets of descriptors.</h2><span id='topic+ModelTrain'></span><span id='topic+ModelTrain.default'></span><span id='topic+ModelTrain.data.frame'></span>

<h3>Description</h3>

<p><code>ModelTrain</code> is a generic S3 function that fits a series of 
classification or regression
models to sets of descriptors and computes cross-validated measures
of model performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ModelTrain(...)

## Default S3 method:
ModelTrain(
  x,
  y,
  nfolds = 10,
  nsplits = 3,
  seed.in = NA,
  des.names = NA,
  models = c("NNet", "PLS", "LAR", "Lasso", "PLSLDA", "Tree", "SVM", "KNN", "RF"),
  user.params = NULL,
  verbose = FALSE,
  ...
)

## S3 method for class 'data.frame'
ModelTrain(
  d,
  ids = FALSE,
  xcol.lengths = ifelse(ids, length(d) - 2, length(d) - 1),
  xcols = NA,
  nfolds = 10,
  nsplits = 3,
  seed.in = NA,
  des.names = NA,
  models = c("NNet", "PLS", "LAR", "Lasso", "PLSLDA", "Tree", "SVM", "KNN", "RF"),
  user.params = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ModelTrain_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_x">x</code></td>
<td>
<p>a list of numeric descriptor set matrices.  At the moment, only
binary and continuous descriptors are supported.  Binary descriptors should
be numeric (0 or 1).</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_y">y</code></td>
<td>
<p>a numeric vector containing the binary or continuous response.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_nfolds">nfolds</code></td>
<td>
<p>the number of folds to use for each cross
validation split.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_nsplits">nsplits</code></td>
<td>
<p>the number of splits to use for repeated
cross validation.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_seed.in">seed.in</code></td>
<td>
<p>a numeric vector with length equal to <code>nsplits</code>.
The seeds are used to randomly assign folds to observations for each
repeated cross-validation split. If <code>NA</code>, the first seed will be 
11111, the second will be 22222, and so on.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_des.names">des.names</code></td>
<td>
<p>a character vector specifying the names for each
descriptor
set.  The length of the vector must match the number of descriptor sets.
If <code>NA</code>, each descriptor set will be named &quot;Descriptor Set i&quot;, where
i is the number of the descriptor set.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_models">models</code></td>
<td>
<p>a character vector specifying the regression or
classification models to use.  The strings must match models
implemented in 'chemmodlab' (see Details).</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_user.params">user.params</code></td>
<td>
<p>a list of data frames where each data frame contains
the parameter values for a model.  The list should have the format of
the list constructed by  <code><a href="#topic+MakeModelDefaults">MakeModelDefaults</a></code>. One can construct
a list of parameters using  <code><a href="#topic+MakeModelDefaults">MakeModelDefaults</a></code> and then
modify the parameters.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_verbose">verbose</code></td>
<td>
<p>verbose mode or not?</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_d">d</code></td>
<td>
<p>a data frame containing an (optional) ID column,
a response column, and descriptor columns.  The columns should be
provide in this order.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_ids">ids</code></td>
<td>
<p>a logical.  Is an ID column provided?</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_xcol.lengths">xcol.lengths</code></td>
<td>
<p>a vector of integers.  It is assumed that the columns
in <code>d</code> are grouped by descriptor set.  The integers specify the
number of descriptors in each descriptor set.  They should be ordered as
the descriptor sets are ordered in <code>d</code>.
Users can specify multiple descriptor sets. By default there is one
descriptor set, namely all columns in <code>d</code> except the response
column and
the optional ID column.  Specify <code>xcol.lengths</code> or <code>xcols</code>,
but not both.</p>
</td></tr>
<tr><td><code id="ModelTrain_+3A_xcols">xcols</code></td>
<td>
<p>A list of integer vectors.  Each vector contains
column indices
of <code>data</code> where a set of descriptor variables is located.
Users can specify multiple descriptor sets.  Specify <code>xcol.lengths</code> or <code>xcols</code>,
but not both.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Multiple descriptor sets can be specified
by the user. For each descriptor set, repeated k-fold cross validation
is performed for the specified regression and/or classification
models.
</p>
<p>Not all modeling strategies will be appropriate for all response
types. For example, partial least squares linear discriminant analysis
(&quot;PLSLDA&quot;)
is not directly appropriate for continuous response assays such as
percent inhibition, but it can be applied once a threshold value for
percent inhibition is used to create a binary (active/inactive) response.
</p>
<p>See <a href="https://jrash.github.io/chemmodlab/">https://jrash.github.io/chemmodlab/</a> for more 
information about the
models available (including model default parameters).
The default value for argument models includes only some of 
the possible values.
</p>
<p>Sensible default values are selected for each
tunable model parameter, however users may set any parameter
manually using <code><a href="#topic+MakeModelDefaults">MakeModelDefaults</a></code> and <code>user.params</code>.
</p>
<p><code><a href="#topic+ModelTrain">ModelTrain</a></code> predictions are based on k-fold cross-validation,
where the dataset is randomly divided into k parts, each containing
approximately equal numbers of compounds. Treating one of these parts
as a &quot;test set&quot; the remaining
k-1 parts are combined together as a &quot;training set&quot;
and used to build a model from the desired modeling technique and
descriptor set. This model is then applied to the &quot;test set&quot; to obtain
predictions. The process is repeated, holding out each of the k parts
in turn. One advantage of k-fold cross-validation is reduction in bias
from using the same data to both build and assess a model. Another
advantage is the increased precision of error estimation offered by
k-fold cross validation over a one-time split.
</p>
<p>Recognizing that the definition of folds in k-fold cross validation
may have an impact on the observed performance measures, all models
are built using the same definition of folds. This process is repeated
to obtain multiple separate k-fold cross validation runs resulting in
multiple separate definitions of folds.  The number of these &quot;splits&quot;
is specified by <code>nsplits</code>.
</p>
<p>Observed performance measures are
assessed across all splits using <code><a href="#topic+CombineSplits">CombineSplits</a></code>.  This
function assesses how sensitive performance measures are to fold
assignments, or changes to the training and test sets. 
Statistical tests are used to determine the best performing model and
descriptor set combination.
</p>


<h3>Value</h3>

<p>A list is returned of class <code><a href="#topic+chemmodlab">chemmodlab</a></code> containing:
</p>
<table>
<tr><td><code>all.preds</code></td>
<td>
<p>a list of lists of data frames.  The elements of the outer
list correspond to each CV split performed by <code><a href="#topic+ModelTrain">ModelTrain</a></code>. The
elements of the inner list correspond to each descriptor set.  For each
descriptor set and CV split combination, the output is a dataframe
containing all model predictions.  The first column of each data frame
contains the true value of the response.  The remaining columns contain
the predictions for each model.</p>
</td></tr>
<tr><td><code>all.probs</code></td>
<td>
<p>a list of lists of data frames. Constructed only if there is
a binary response.  The structure is the same as <code>all.preds</code>, except
that predictions are replaced by &quot;predicted probabilities&quot; (i.e. estimated
probabilities of a response
value of one).  Predicted
probabilities are only reported for classification models.</p>
</td></tr>
<tr><td><code>model.acc</code></td>
<td>
<p>a list of lists of model accuracy measures.  The elements of
the outer list correspond to each CV split performed by <code>ModelTrain</code>.
The elements of the inner list correspond to each descriptor set.  For each
descriptor set and CV split combination, a limited collection of 
performance measures are given for each model fit
to the data.  Regression models are assessed with Pearson's <code class="reqn">r</code> and
<code class="reqn">RMSE</code>. Classification models are assessed with contingency tables.
For additional model performance measures, see <code><a href="#topic+Performance">Performance</a></code></p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code>classify</code></td>
<td>
<p>a logical.  Were classification models used for binary
response?</p>
</td></tr>
<tr><td><code>responses</code></td>
<td>
<p>a numeric vector.  The observed value of the response.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>a list of numeric matrices.  Each matrix is a descriptor set used
as model input.</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>a list of data frames as made by
<code><a href="#topic+MakeModelDefaults">MakeModelDefaults</a></code>.  Each data frame contains the parameters to
be set for a particular model.</p>
</td></tr>
<tr><td><code>des.names</code></td>
<td>
<p>a character vector specifying the descriptor set names.  NA if 
unspecified.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>a character vector specifying the models fit to the data.</p>
</td></tr>
<tr><td><code>nsplits</code></td>
<td>
<p>number of CV splits performed.</p>
</td></tr>
</table>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>default</code>: Default S3 method
</p>
</li>
<li> <p><code>data.frame</code>: S3 method for class 'data.frame'
</p>
</li></ul>


<h3>Author(s)</h3>

<p>Jacqueline Hughes-Oliver, Jeremy Ash, Atina Brooks
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chemmodlab">chemmodlab</a></code>, <code><a href="#topic+plot.chemmodlab">plot.chemmodlab</a></code>,
<code><a href="#topic+CombineSplits">CombineSplits</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# A data set with  binary response and multiple descriptor sets
data(aid364)

cml &lt;- ModelTrain(aid364, ids = TRUE, xcol.lengths = c(24, 147),
                  des.names = c("BurdenNumbers", "Pharmacophores"))
cml

## End(Not run)

# A continuous response
cml &lt;- ModelTrain(USArrests, nsplits = 2, nfolds = 2,
                  models = c("KNN", "Lasso", "Tree"))
cml

</code></pre>

<hr>
<h2 id='PerfCurveBands'>Construct a confidence band for a recall or precision curve</h2><span id='topic+PerfCurveBands'></span>

<h3>Description</h3>

<p><code>PerfCurveBands</code> takes a pair of score and activity vectors as input.
A performance curve and confidence band is created for the selected testing fractions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PerfCurveBands(
  S,
  X,
  r,
  metric = "rec",
  type = "band",
  method = "sup-t",
  plus = T,
  conf.level = 0.95,
  boot.rep = 100,
  mc.rep = 1e+05,
  myseed = 111,
  h = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerfCurveBands_+3A_s">S</code></td>
<td>
<p>a vector of scores.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_x">X</code></td>
<td>
<p>a vector of activities.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_r">r</code></td>
<td>
<p>a vector of testing fractions.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_metric">metric</code></td>
<td>
<p>the performance curve to use. Options are recall (&quot;rec&quot;) and precision (&quot;prec&quot;).</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_type">type</code></td>
<td>
<p>specifies whether a point-wise confidence interval 
(&quot;pointwise&quot;) or a confidence band (&quot;band&quot;) should be constructed.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_method">method</code></td>
<td>
<p>the method to use. Point-wise confidence interval options
are &quot;binomial&quot;, &quot;JZ&quot;, &quot;bootstrap&quot;. Confidence band options are &quot;sup-t&quot;, &quot;theta-proj&quot;.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_plus">plus</code></td>
<td>
<p>should plus correction be used or not?</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_conf.level">conf.level</code></td>
<td>
<p>the confidence level for the bands.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_boot.rep">boot.rep</code></td>
<td>
<p>the number of replicates to use for the bootstrap method.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_mc.rep">mc.rep</code></td>
<td>
<p>the number of Monte Carlo replicates to use for the sup-t method.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_myseed">myseed</code></td>
<td>
<p>the random seed.</p>
</td></tr>
<tr><td><code id="PerfCurveBands_+3A_h">h</code></td>
<td>
<p>the bandwidth for the local regression estimator of Lambda.  If NULL, uses the default 
plugin estimator.</p>
</td></tr>
</table>

<hr>
<h2 id='PerfCurveTest'>Perform a hypothesis test for the difference between two performance curves.</h2><span id='topic+PerfCurveTest'></span>

<h3>Description</h3>

<p><code>PerfCurveTest</code> takes score vectors for two scoring algorithms and an activity vector.
A performance curve is created for the two scoring algorithms and hypothesis tests are performed
at the selected testing fractions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PerfCurveTest(
  S1,
  S2,
  X,
  r,
  metric = "rec",
  method = "EmProc",
  type = "pointwise",
  plus = T,
  pool = F,
  alpha = 0.05,
  h = NULL,
  seed = 111,
  mc.rep = 1e+05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerfCurveTest_+3A_s1">S1</code></td>
<td>
<p>a vector of scores for scoring algorithm 1.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_s2">S2</code></td>
<td>
<p>a vector of scores for scoring algorithm 2.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_x">X</code></td>
<td>
<p>a vector of activities.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_r">r</code></td>
<td>
<p>a vector of testing fractions.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_metric">metric</code></td>
<td>
<p>the performance curve to use. Options are recall (&quot;rec&quot;) and precision (&quot;prec&quot;).</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_method">method</code></td>
<td>
<p>the method to use. Recall options are 
c(&quot;EmProc&quot;, &quot;binomial&quot;, &quot;JZ ind&quot;, &quot;mcnemar&quot;, &quot;binomial ind&quot;). Precision options are
c(&quot;EmProc&quot;, &quot;binomial&quot;, &quot;JZ ind&quot;, &quot;stouffer&quot;, &quot;binomial ind&quot;).</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_type">type</code></td>
<td>
<p>specifies whether a point-wise confidence interval 
(&quot;pointwise&quot;) or a confidence band (&quot;band&quot;) should be constructed.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_plus">plus</code></td>
<td>
<p>should plus correction be applied to the confidence intervals?</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_pool">pool</code></td>
<td>
<p>use pooling for hypothesis tests? Only relevant to &quot;EmProc&quot;.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_alpha">alpha</code></td>
<td>
<p>the significance level.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_h">h</code></td>
<td>
<p>the bandwidth for the local regression estimator of Lambda.  If NULL, uses the default 
plugin estimator.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_seed">seed</code></td>
<td>
<p>the random seed.</p>
</td></tr>
<tr><td><code id="PerfCurveTest_+3A_mc.rep">mc.rep</code></td>
<td>
<p>the number of Monte Carlo replicates to use for the sup-t method.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.chemmodlab'>Plot method for the chemmodlab class.</h2><span id='topic+plot.chemmodlab'></span>

<h3>Description</h3>

<p><code>plot.chemmodlab</code> takes a <code><a href="#topic+chemmodlab">chemmodlab</a></code> object output by the
<code><a href="#topic+ModelTrain">ModelTrain</a></code> function and creates a series of accumulation curve
plots for assesing model and descriptor set performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chemmodlab'
plot(
  x,
  max.select = NA,
  splits = 1:x$nsplits,
  meths = x$models,
  series = "both",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.chemmodlab_+3A_x">x</code></td>
<td>
<p>an object of class <code><a href="#topic+chemmodlab">chemmodlab</a></code>.</p>
</td></tr>
<tr><td><code id="plot.chemmodlab_+3A_max.select">max.select</code></td>
<td>
<p>the maximum number of tests to plot for the
accumulation curve. If <code>max.select</code> is not specified, 
use <code>floor(min(300,n/4))</code>,
where <code>n</code> is the number of compounds.</p>
</td></tr>
<tr><td><code id="plot.chemmodlab_+3A_splits">splits</code></td>
<td>
<p>a numeric vector containing the indices of the splits to use to construct
accumulation curves.  Default is to use all splits. <code>NA</code> means the first series
of plots are not generated. See <code>Details</code>.</p>
</td></tr>
<tr><td><code id="plot.chemmodlab_+3A_meths">meths</code></td>
<td>
<p>a character vector with statistical methods implemented in
<code>chemmodlab</code>.  The 
statistical methods to use for the second series of plots.  
This argument can take the same values
as argument <code>models</code> in function <code><a href="#topic+ModelTrain">ModelTrain</a></code>. See <code>Details</code>.</p>
</td></tr>
<tr><td><code id="plot.chemmodlab_+3A_series">series</code></td>
<td>
<p>a character vector.  Which series of plots to construct. Can be one of
<code>"descriptors"</code>, <code>"methods"</code>, <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="plot.chemmodlab_+3A_...">...</code></td>
<td>
<p>other parameters to be passed through to plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a binary response, the accumulation curve plots the number of assay hits
identified as a function of the number of tests conducted, where testing
order is determined by the predicted probability of a response being positive
obtained from k-fold cross
validation. Given a particular compound collection, larger accumulations are
preferable.
</p>
<p>The accumulation curve has also been extended to continuous responses.
Assuming large positive values of a continuous response y are preferable,
<code>chemmodlab</code>
accumulates <code>y</code> so that <code class="reqn">\sum y_i</code> is the sum of the <code>y</code>
over the first <code>n</code> tests. This extension includes the binary-response
accumulation curve as a special case.
</p>
<p>By default, we display accumulation curves up to 300 tests, not for the
entire collection, to focus on the goal of finding actives as early as
possible.
</p>
<p>There are two main series of plots generated:
</p>


<h3>Methods plot series</h3>

<p>There is one plot per CV split and descriptor set
combination. The accumulation curves for each modeling
method is compared.
</p>


<h3>Descriptors plot series</h3>

<p>There is one plot per CV split and model fit. The
accumulation curves for each descriptor set is
compared.
</p>


<h3>Author(s)</h3>

<p>Jacqueline Hughes-Oliver, Jeremy Ash, Atina Brooks
</p>


<h3>References</h3>

<p>Modified from code originally written by
William J. Welch 2001-2002
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chemmodlab">chemmodlab</a></code>, <code><a href="#topic+ModelTrain">ModelTrain</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# A data set with  binary response and multiple descriptor sets
data(aid364)

cml &lt;- ModelTrain(aid364, ids = TRUE, xcol.lengths = c(24, 147), 
                  des.names = c("BurdenNumbers", "Pharmacophores"))
plot(cml)

## End(Not run)

# A continuous response
cml &lt;- ModelTrain(USArrests, nsplits = 2, nfolds = 2,
                  models = c("KNN", "Lasso", "Tree"))
plot(cml)

</code></pre>

<hr>
<h2 id='pparg'>Docking scores for 3212 ligands for target PPARg</h2><span id='topic+pparg'></span>

<h3>Description</h3>

<p>A dataset containing docking scores for the protein regulating gene
peroxisome proliferator-activated receptor gamma (PPARg). 3212 ligands are
scored. Scores are provided for three docking methods: Surflex-dock, ICM, and
Vina. Scores are also provided for two consensus methods: the minimum rank
consensus of Surflex-dock and ICM, and the maximum z-score consensus of
Surflex-dock and ICM. Docking scores have been rescaled so that larger values
suggest active ligands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pparg
</code></pre>


<h3>Format</h3>

<p>A data frame with 3212 rows and 15 variables:
</p>

<dl>
<dt>surf_id</dt><dd><p>unique ligand identifier</p>
</dd>
<dt>surf_scores</dt><dd><p>score from Surflex-dock</p>
</dd>
<dt>surf_actives</dt><dd><p>activity label of ligand: 1 means active, 0 means not active</p>
</dd>
<dt>icm_id</dt><dd><p>unique ligand identifier</p>
</dd>
<dt>icm_scores</dt><dd><p>score from ICM</p>
</dd>
<dt>icm_actives</dt><dd><p>activity label of ligand: 1 means active, 0 means not active</p>
</dd>
<dt>vina_id</dt><dd><p>unique ligand identifier</p>
</dd>
<dt>vina_scores</dt><dd><p>score from Vina</p>
</dd>
<dt>vina_actives</dt><dd><p>activity label of ligand: 1 means active, 0 means not active</p>
</dd>
<dt>minr_id</dt><dd><p>unique ligand identifier</p>
</dd>
<dt>minr_scores</dt><dd><p>score from the minimum rank consensus of Surflex-dock and ICM</p>
</dd>
<dt>minr_actives</dt><dd><p>activity label of ligand: 1 means active, 0 means not active</p>
</dd>
<dt>maxz_id</dt><dd><p>unique ligand identifier</p>
</dd>
<dt>maxz_scores</dt><dd><p>score from the maximum z-score consensus of Surflex-dock and ICM</p>
</dd>
<dt>maxz_actives</dt><dd><p>activity label of ligand: 1 means active, 0 means not active</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://stats.drugdesign.fr">http://stats.drugdesign.fr</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
