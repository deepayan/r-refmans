<!DOCTYPE html><html lang="en"><head><title>Help for package dynaTree</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dynaTree}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dynaTree-package'><p>Dynamic trees for learning and design</p></a></li>
<li><a href='#alcX.dynaTree'>
<p>Calculate the ALC or predictive entropy statistic at the X</p>
locations, or ALC at new XX predictive locations</a></li>
<li><a href='#deletecloud'>
<p>Copy and Delete C-side Clouds in dynaTree Objects</p></a></li>
<li><a href='#dynaTree-class'><p>Class &quot;dynaTree&quot;</p></a></li>
<li><a href='#dynaTree-internal'><p>Internal dynaTree Functions</p></a></li>
<li><a href='#dynaTrees'>
<p>Fitting Dynamic Tree Models</p></a></li>
<li><a href='#elec2'>
<p>The ELEC2 Data Set</p></a></li>
<li><a href='#getBF'>
<p>Extract a Path of (log) Bayes Factors</p></a></li>
<li><a href='#plot.dynaTree'>
<p>Plotting Predictive Distributions of Dynamic Tree models</p></a></li>
<li><a href='#predict.dynaTree'>
<p>Prediction for Dynamic Tree Models</p></a></li>
<li><a href='#rejuvenate.dynaTree'>
<p>Rejuvenate particles from the dynaTree posterior</p></a></li>
<li><a href='#relevance.dynaTree'>
<p>Calculate relevance statistics for input coordinates</p></a></li>
<li><a href='#retire.dynaTree'>
<p>Retire (i.e. remove) data from the a dynaTree model</p></a></li>
<li><a href='#sens.dynaTree'>
<p>Monte Carlo Sensitivity Analysis for dynaTree Models</p></a></li>
<li><a href='#update.dynaTree'>
<p>Updating a Dynamic Tree Model With New Data</p></a></li>
<li><a href='#varpropuse'>
<p>Calculate the proportion of variables used in tree splits, and</p>
average summary stats of tree heights and leaf sizes</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Dynamic Trees for Learning and Design</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2-17</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-08-21</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>interp, tgp, plgp, MASS</td>
</tr>
<tr>
<td>Description:</td>
<td>Inference by sequential Monte Carlo for 
  dynamic tree regression and classification models
  with hooks provided for sequential design and optimization, 
  fully online learning with drift, variable selection, and 
  sensitivity analysis of inputs.  Illustrative 
  examples from the original dynamic trees paper 
  (Gramacy, Taddy &amp; Polson (2011); &lt;<a href="https://doi.org/10.1198%2Fjasa.2011.ap09769">doi:10.1198/jasa.2011.ap09769</a>&gt;) are facilitated
  by demos in the package; see demo(package="dynaTree").</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robert B. Gramacy  &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-21 11:49:37 UTC; bobby</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert B. Gramacy [aut, cre],
  Matt A. Taddy [aut],
  Christoforos Anagnostopoulos [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-22 05:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='dynaTree-package'>Dynamic trees for learning and design</h2><span id='topic+dynaTree-package'></span>

<h3>Description</h3>

<p>Inference by sequential Monte Carlo for
dynamic tree regression and classification models 
with hooks provided for sequential design and optimization, 
fully online learning with drift, variable selection, and   
sensitivity analysis of inputs.  Illustrative
examples from the original dynamic trees paper are facilitated
by demos in the package; see demo(package=&quot;dynaTree&quot;)
</p>


<h3>Details</h3>

<p>For a fuller overview including a complete list of functions, and
demos, please use <code>help(package="dynaTree")</code>.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos
</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

 <p><span class="pkg">plgp</span>, <span class="pkg">tgp</span> </p>

<hr>
<h2 id='alcX.dynaTree'>
Calculate the ALC or predictive entropy statistic at the X
locations, or ALC at new XX predictive locations
</h2><span id='topic+alcX.dynaTree'></span><span id='topic+alc.dynaTree'></span><span id='topic+entropyX.dynaTree'></span>

<h3>Description</h3>

<p>Uses analytic integration (at the leaves) to calculate
the (regression) ALC statistic, or calculates the predictive (class) entropy
at the input (X) locations; or calculate ALC at new predictive
locations either analytically or numerically
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
alcX(object, rect = NULL, categ = NULL,
     approx = FALSE, verb = 0)
## S3 method for class 'dynaTree'
entropyX(object, verb = 0)
## S3 method for class 'dynaTree'
alc(object, XX, rect = NULL, categ = NULL,
     approx = FALSE, Xref = NULL, probs = NULL, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="alcX.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_rect">rect</code></td>
<td>

<p>for <code>alcX</code>, a <code>matrix</code> with two columns and
<code>ncol(object$X)</code> rows
describing the bounding rectangle for the ALC integration; the
default that is used when <code>rect = NULL</code> is the bounding
rectangle obtained by applying <code><a href="base.html#topic+range">range</a></code> to each
column of <code>object$X</code> (taking care to remove the
first/intercept column of <code>object$X</code> if <code>icept =
      "augmented"</code>; only applies to regression models 
(<code>object$model != "class"</code>); for <code>alc</code>, <code>rect</code>
must be a scalar logical: see <code>Xref</code> below
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_categ">categ</code></td>
<td>

<p>A vector of logicals of length <code>ncol(object$X)</code> indicating
which, if any, dimensions of the input space should be treated
as categorical; this input is used to help with the analytic
integrals from a <code>rect</code>-based calculation, which means
it should not specified along with <code>Xref</code>; the default <code>categ</code>
argument is <code>NULL</code> meaning that the categorical inputs
are derived from <code>object$X</code> in a sensible way
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_approx">approx</code></td>
<td>
<p> a scalar logical that, when <code>TRUE</code>, causes
the number of data points in a node/leaf to be used as a proxy for
its area in the analytic calculations </p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_xx">XX</code></td>
<td>

<p>a design <code>matrix</code> of predictive locations (where <code>ncol(XX) ==
      ncol(X)</code>; only used by <code>alc</code> 
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_xref">Xref</code></td>
<td>

<p><code>Xref</code> input can be optionally used to
specify a grid of reference locations for the numerical ALC
calculation - a <code>matrix</code> with <code>ncol(X)</code> columns.  If
<code>NULL</code>, the default, then the <code>XX</code> is taken as both
candidate and reference locations.
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_probs">probs</code></td>
<td>

<p>weights for the reference locations to be used in a Monte Carlo
approximation; usually these weights are class probabilities for
response surfaces under constraints
</p>
</td></tr>
<tr><td><code id="alcX.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many predictive locations
(iterations) after which a progress statement should be
printed to the console; a (default) value of <code>verb = 0</code> is quiet
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is most useful for selecting <code>object$X</code>
locations to remove from the analysis, perhaps in an online inference
setting. See <code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code> for more details.  The
output is the same as using <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>
using <code>XX = object$X</code>, <code>alc = "rect"</code>, and <code>Xref =
    rect</code>
</p>
<p><code>entropyX</code> only apples to classification models
(<code>object$model != "class"</code>), and <code>alcX</code> applies (only)
to the other, regression, models
</p>
<p>The <code>alc</code> function is more generic and allows ALC calculations
at new, predictive, <code>XX</code> locations.  This functionality used
to be part of the <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> function, but were
separated out for computational reasons.  The previous version was
Monte Carlo-based (using <code>Xref</code>) whereas the new version
also allows analytic calculation (now the default, via <code>rect</code>)
</p>


<h3>Value</h3>

<p>The entire <code>object</code> is returned with a new entry called
<code>alcX</code> containing a vector of length <code>nrow(X)</code> with
the ALC values, or <code>entropyX</code> containing the entropy values,
or <code>alc</code> if general ALC calculations at new <code>XX</code> locations
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt">rbg@vt</a>, <br />
Matt Taddy, and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Anagnostopoulos, C., Gramacy, R.B. (2013) &ldquo;Information-Theoretic 
Data Discarding for Dynamic Trees on Data Streams.&rdquo; Entropy, 15(12), 
5510-5535; arXiv:1201.5568
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, and
<code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fit the model to the parabola data
n &lt;- 100
Xp &lt;- runif(n,-3,3)
Yp &lt;- Xp + Xp^2 + rnorm(n, 0, .2)
rect &lt;- c(-3,3)
out &lt;- dynaTree(Xp, Yp, model="linear", icept="augmented")

## calculate the alcX
out &lt;- alcX(out, rect=rect)

## to compare to analytic 
out &lt;- alc(out, XX=out$X[,-1], rect=rect)

## plot comparison between alcX and predict-ALC
plot(out$X[,-1], out$alcX)
o &lt;- order(out$X[,2])
lines(out$X[o,-1], out$alc[o], col=2, lty=2)

## now compare to approximate analytic
## (which writes over out$alc)
out &lt;- alc(out, XX=out$X[,-1], rect=rect, approx=TRUE)
lines(out$X[o,-1], out$alc[o], col=3, lty=3)

## clean up
deletecloud(out)

## similarly with entropyX for classification models

## see demo("design") for more iterations and
## design under other active learning heuristics
## like ALC, and EI for optimization; also see
## demo("online") for an online learning example where
## ALC is used for retirement
</code></pre>

<hr>
<h2 id='deletecloud'>
Copy and Delete C-side Clouds in dynaTree Objects
</h2><span id='topic+deletecloud'></span><span id='topic+deleteclouds'></span><span id='topic+copy.dynaTree'></span>

<h3>Description</h3>

<p>Function(s) to free the memory used to represent particle clouds
stored on the <code>C</code>-side of a <code>"dynaTree"</code>-class object &ndash;
essential to avoid memory leaks; also a function to copy an entire
<code>"dynaTree"</code>-class object, which is required since explicit
code is needed to copy the <code>C</code>-side memory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
copy(obj)
deletecloud(obj)
deleteclouds()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deletecloud_+3A_obj">obj</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
with <code>obj$num != NULL</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>deletecloud</code> frees the particular <code>C</code>-side memory
associated to a particular object, and <code>deleteclouds</code> frees
all <code>C</code>-side clouds (i.e., all memory) 
</p>
<p>An object with <code>obj$num != NULL</code> has already had its
<code>C</code>-side cloud freed
</p>
<p><code>copy.dynaTree</code> returns a copied object with a new 
<code>obj$num</code> after having copied the <code>C</code>-side memory
contents pointed to by the old <code>obj$num</code>
</p>


<h3>Value</h3>

<p>The deleting functions do not return any value; the copy function
returns a copy of a <code>"dynaTree"</code>-class object
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>,<br />
Matt Taddy and Christoforos Anagnostopoulos 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>deleteclouds()
## see the dynaTree and update.dynaTree help files
## and the demos (demo(package="dynaTree")) for
## more examples
</code></pre>

<hr>
<h2 id='dynaTree-class'>Class &quot;dynaTree&quot;</h2><span id='topic+dynaTree-class'></span><span id='topic+retire-methods'></span><span id='topic+copy-methods'></span><span id='topic+sens-methods'></span><span id='topic+alcX-methods'></span><span id='topic+alc-methods'></span><span id='topic+entropyX-methods'></span><span id='topic+ieci-methods'></span><span id='topic+classprobs-methods'></span><span id='topic+rejuvenate-methods'></span><span id='topic+relevance-methods'></span><span id='topic+intervals-methods'></span><span id='topic+varpropuse-methods'></span><span id='topic+varproptotal-methods'></span><span id='topic+treestats-methods'></span><span id='topic+sameleaf-methods'></span><span id='topic+retire+2CdynaTree-method'></span><span id='topic+sens+2CdynaTree-method'></span><span id='topic+copy+2CdynaTree-method'></span><span id='topic+alcX+2CdynaTree-method'></span><span id='topic+alc+2CdynaTree-method'></span><span id='topic+qEI+2CdynaTree-method'></span><span id='topic+qEntropy+2CdynaTree-method'></span><span id='topic+entropyX+2CdynaTree-method'></span><span id='topic+ieci+2CdynaTree-method'></span><span id='topic+classprobs+2CdynaTree-method'></span><span id='topic+rejuvenate+2CdynaTree-method'></span><span id='topic+relevance+2CdynaTree-method'></span><span id='topic+intervals+2CdynaTree-method'></span><span id='topic+varpropuse+2CdynaTree-method'></span><span id='topic+varproptotal+2CdynaTree-method'></span><span id='topic+treestats+2CdynaTree-method'></span><span id='topic+sameleaf+2CdynaTree-method'></span><span id='topic+sens'></span><span id='topic+retire'></span><span id='topic+copy'></span><span id='topic+alcX'></span><span id='topic+alc'></span><span id='topic+entropyX'></span><span id='topic+ieci'></span><span id='topic+classprobs'></span><span id='topic+rejuvenate'></span><span id='topic+relevance'></span><span id='topic+intervals'></span><span id='topic+varpropuse'></span><span id='topic+varproptotal'></span><span id='topic+treestats'></span><span id='topic+sameleaf'></span>

<h3>Description</h3>

<p>A stub for class dynaTree and its custom
generic methods
</p>


<h3>Details</h3>

<p>This is just a stub file.  See <code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>
and <code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code> for more information on
the generic methods used in this package
</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Methods</h3>


<dl>
<dt>retire</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>sens</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>copy</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>alc</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>alcX</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>entropyX</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>ieci</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>classprobs</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>rejuvenate</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>relevance</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>varpropuse</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>varproptotal</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>treestats</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
<dt>sameleaf</dt><dd><p><code>signature(object = "dynaTree")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>,<br />
Matt Taddy and Christoforos Anagnostopoulos </p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, 
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code>, <code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code>,
<code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>, <code><a href="#topic+alcX.dynaTree">alcX.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("dynaTree")
</code></pre>

<hr>
<h2 id='dynaTree-internal'>Internal dynaTree Functions</h2><span id='topic+plotprogress'></span><span id='topic+plotal'></span><span id='topic+alcalc'></span><span id='topic+classprobs.dynaTree'></span><span id='topic+ieci.dynaTree'></span><span id='topic+intervals.dynaTree'></span><span id='topic+sameleaf.dynaTree'></span><span id='topic+qEI.dynaTree'></span><span id='topic+qEI'></span><span id='topic+qEntropy.dynaTree'></span><span id='topic+qEntropy'></span>

<h3>Description</h3>

<p>Internal <span class="pkg">dynaTree</span> functions
</p>


<h3>Details</h3>

<p>These functions are primarily provided to facilitate the
demos, and to provide for a cleaner exposition.  We encourage
users to inspect their contents in order to help develop 
similar functions to suit their particular needs
</p>

<hr>
<h2 id='dynaTrees'>
Fitting Dynamic Tree Models
</h2><span id='topic+dynaTrees'></span><span id='topic+dynaTree'></span>

<h3>Description</h3>

<p>A function to initialize and fit dynamic tree models
to regression and classification data by the sequential
Monte Carlo (SMC) method of particle learning (PL)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dynaTree(X, y, N = 1000, model = c("constant", "linear", "class", "prior"),
         nu0s20 = c(0,0), ab = c(0.95, 2), minp = NULL, sb = NULL, 
	       nstart = minp, icept = c("implicit", "augmented", "none"), 
         rprop = c("luvar", "luall", "reject"), verb = round(length(y)/10))
dynaTrees(X, y, N = 1000,  R = 10, sub = length(y),
          model = c("constant", "linear", "class", "prior"), nu0s20 = c(0,0),
          ab=c(0.95, 2), minp = NULL, sb = NULL, nstart = minp,
          icept =  c("implicit", "augmented", "none"),
          rprop = c("luvar", "luall", "reject"), XX = NULL, yy = NULL,
	  varstats = FALSE, lhs = NULL, plotit = FALSE, proj = 1,
          rorder = TRUE, verb = round(sub/10), pverb=round(N/10), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dynaTrees_+3A_x">X</code></td>
<td>

<p>A design <code>matrix</code> of real-valued predictors
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_y">y</code></td>
<td>

<p>A vector of length <code>nrow(X)</code> containing real-valued
responses (for regression) or positive
integer-valued class labels (for classification)
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_n">N</code></td>
<td>

<p>a positive scalar integer indicating the number
of particles to be used
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_r">R</code></td>
<td>

<p>a scalar integer <code>&gt;= 2</code> indicating the number of
&ldquo;repeats&rdquo; or passes through the data,
as facilitated by <code>dynaTrees</code>; see details below
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_sub">sub</code></td>
<td>

<p>Optional argument allowing only a subset of the <code>length(y)</code>
<code>X</code>-<code>y</code> pairs to be used in each repeat of 
<code>dynaTrees</code>; each repeat will use a different random subset
of size <code>sub</code>
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_model">model</code></td>
<td>

<p>indicates the type of model to be used at the leaves of the tree;
<code>"constant"</code> and <code>"linear"</code> apply to regression,
and <code>"class"</code> to multinomial classification; finally <code>"prior"</code>
was recently added to explore sampled without data
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_nu0s20">nu0s20</code></td>
<td>

<p>a two-vector indicating Inverse Gamma prior parameters
<code>c(nu0, sigma20)</code> for the variance in each leaf node,
<code class="reqn">\sigma^2</code>.  A <code>c(0,0)</code> setting indicates 
a default, scale-invariant, prior; does not apply to the
<code>"class"</code> model 
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_ab">ab</code></td>
<td>

<p>tree prior parameter <code>c(alpha, beta)</code>; see details below
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_minp">minp</code></td>
<td>

<p>a positive scalar integer describing the smallest allowable
region in the treed partition; if <code>NULL</code> (default) a
suitable minimum is calculated based on <code>dim(X)</code> and
the type of <code>model</code> being fit
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_sb">sb</code></td>
<td>

<p>an optional two-vector of positive integers indicating
<code>c(splitmin, basemax)</code> for the <code>"linear"</code> model.
It gives the first column of
<code>X</code> on which treed partitioning is allowed, and the last
column of <code>X</code> to use as covariates in the linear model
at the leaves, respectively
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_nstart">nstart</code></td>
<td>

<p>a positive scalar integer <code>&gt;= minp</code> indicating
the time index at which treed partitioning is allowed to start
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_icept">icept</code></td>
<td>

<p>indicates the type of intertcept term used (only applies to
<code>model="linear"</code>).  The default, <code>"implicit"</code> 
causes the inputs <code>X</code> to be centered so the intercept can be
implied as an afterthought; <code>"augmented"</code> causes the inputs 
<code>X</code> to automatically gain a leading column of ones in a way
that is transparent to the user; and <code>"none"</code> assumes that
no intercept is being used, or that the user has pre-treated
<code>X</code> to have a column of ones.  The main advantage of
<code>"implicit"</code> over <code>"augmented"</code> is that the former can
default to a constant model fit if leaf design matrices become
rank deficient.  The latter defaults to the zero-model in such
cases
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_xx">XX</code></td>
<td>

<p>a design <code>matrix</code> of predictive locations (where <code>ncol(XX) ==
      ncol(X)</code>) for <code>dynaTrees</code>; also see
<code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> and some explanation in the
details below
</p>
</td></tr> 	 
<tr><td><code id="dynaTrees_+3A_yy">yy</code></td>
<td>

<p>an optional vector of &ldquo;true&rdquo; responses at the <code>XX</code>
predictive locations at which the log posterior probability are
to be reported
</p>
</td></tr>  	 
<tr><td><code id="dynaTrees_+3A_varstats">varstats</code></td>
<td>

<p>if <code>TRUE</code> causes the <code><a href="#topic+varpropuse">varpropuse</a></code>,
<code><a href="#topic+varproptotal">varproptotal</a></code>, and <code><a href="#topic+relevance.dynaTree">relevance.dynaTree</a></code>
functions to be called on after each
repetition to collect the usage proportions of each input variable
(column of <code>X</code>); see those documentation files for more details
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_lhs">lhs</code></td>
<td>

<p>an optional <code>lhs</code> argument to
<code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code> if a sensitivity analysis step is
desired after each restart (<code>XX="sens"</code>)
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_plotit">plotit</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the fit should be plotted
after each of the <code>R</code> repeats; only applies to 1-d data
and <code>dynaTrees</code>
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_proj">proj</code></td>
<td>

<p>when <code>ncol(x$X) &gt; 1</code> and <code>plotit = TRUE</code>
this argument is passed to <code><a href="#topic+plot.dynaTree">plot.dynaTree</a></code> to make
a 1-d projection using <code>x$X[,proj]</code>
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_rorder">rorder</code></td>
<td>

<p>a scalar <code>logical</code> indicating if
the rows of <code>X</code> (and corresponding components of
<code>y</code>) should be randomly re-ordered for repeats
<code>2:R</code> in order to assess the how the time-ordering
of the SMC effects the Monte Carlo
error; only applies to <code>dynaTrees</code>.  Alternatively,
one can specify an <code>nrow(X)</code>-by-<code>(R-1)</code> matrix
of orderings (permutations of <code>1:nrow(X)</code>)
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_rprop">rprop</code></td>
<td>

<p>indicates the scheme used to construct a grow proposal.
The best setting, <code>"luall"</code> uses the lower (L) and upper (U)
rectangle method based on <code>minp</code> (above) as described 
in the seminal reference in which
the growing location and dimension is sampled uniformly.  It can
be computationally intensive for large input spaces.  A thriftier
option (the default) in this case is <code>"luvar"</code> which uniformly chooses the
splitting variable first and then uses the LU method marginally.
Thriftier still is <code>"reject"</code> which just proposes uniformly
in the bounding leaf rectangle and rejects subsequent grows that
lead to partitions with too few data points; (see the <code>minp</code>
argument)
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many time steps
(iterations) should pass before a progress statement is
printed to the console; a value of <code>verb = 0</code> is quiet
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_pverb">pverb</code></td>
<td>

<p>a positive scalar integer indicating after many particles
should be processed for prediction before a progress statement is
printed to the console; a value of <code>pverb = 0</code> is quiet
</p>
</td></tr>
<tr><td><code id="dynaTrees_+3A_...">...</code></td>
<td>

<p>extra arguments to <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> passed
from <code>dynaTrees</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>dynaTree</code> function processes the <code>X</code> and <code>y</code>
pairs serially via PL.  It builds up a particle cloud
which is stored as an object in <code>C</code>.  A &ldquo;pointer&rdquo; to that
object is the primary return value. The <code>dynaTrees</code> function
fits several (<code>R</code>) different dynamic tree models on different
time-orderings of the data indices and also
obtains samples from the posterior predictive distribution at
new <code>XX</code> locations.  These predictions can be averaged
over each repeat, or used to assess the Monte Carlo predictive
error.
</p>
<p>Three different leaf <code>model</code>s are supported: two for
regression and one for classification.  If <code>model == "class"</code>
then the <code>y</code> values must contain representatives from
every class (<code>1:max(y)</code>).  For details of these models and
the complete description of their use at the leaves of the dynamic
trees, see the Taddy, et al., (2009) reference, below.
</p>
<p>The tree prior is specified by <code>ab=c(alpha, beta)</code>
via the and <code>minp</code>.
It was originally described by Chipman et al., (1998, 2002)
</p>
<p style="text-align: center;"><code class="reqn">p_{\mbox{\tiny split}}(\eta, \mathcal{T}) =
       \alpha*(1+\eta)^\beta</code>
</p>

<p>and subsequently augmented to enforce a minimum number of points
(<code>minp</code>) in each region.
</p>
<p>Once a <code>"dynaTree"</code>-class object has been built (by
<code>dynaTree</code>), predictions and estimates of sequential design and
optimization criteria can be obtained via
<code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, a generic prediction method.
These values can be used to augment the design, and the
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code> function can be used to quickly
update the fit with the augmenting data
</p>


<h3>Value</h3>

<p>Both functions return an object of class <code>"dynaTree"</code>,
which is a list containing the following fields
</p>
<table role = "presentation">
<tr><td><code>m</code></td>
<td>
 <p><code>ncol(X)</code> </p>
</td></tr> 
<tr><td><code>T</code></td>
<td>
 <p><code>nrow(X)</code> </p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p> the number of particles used </p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p> a copy of the design matrix <code>X</code> </p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p> a copy of the responses <code>y</code> </p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p> a copy of the specified leaf model </p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p> a vector containing <code>c(nu0s20, alpha, beta, minp,
      sb, icept, rprop)</code>, where the latter two are in integer form </p>
</td></tr>
<tr><td><code>verb</code></td>
<td>
<p> a copy of the verbosity argument </p>
</td></tr>
<tr><td><code>lpred</code></td>
<td>
<p> a vector of <code>log</code> posterior probabilities
for each observation, conditional on the ones previous,
for all time <code>(2*minp):T</code>; see <code>getBF</code> for calculating
Bayes factors from these</p>
</td></tr>
<tr><td><code>icept</code></td>
<td>
<p> a copy of the intercept argument </p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p> the total computing time used to build the
particle cloud</p>
</td></tr>
<tr><td><code>num</code></td>
<td>
<p> a &ldquo;pointer&rdquo; to the <code>C</code>-side particle
cloud; see the note below </p>
</td></tr>
</table>
<p>-<br /> 
The <code>dynaTrees</code> function can obtain predictive samples
(via <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>) at each of the <code>R</code>
repeats.  Therefore, the <code>"dynaTree"</code> object returned contains
extra fields collecting these predictive samples, primarily
comprising of <code>R</code> columns of information for each of the fields
returned by <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>; see that function for
more details.  Likewise, when <code>varstats = TRUE</code> the returned
object also contains <code>vpu</code>, <code>vpt</code> and <code>parde[</code> fields
whose columns contain the <code><a href="#topic+varpropuse">varpropuse</a></code> and
<code><a href="#topic+varproptotal">varproptotal</a></code> outputs.
</p>
<p>Likewise, <code>dynaTrees</code>, can provide variable usage summaries
if <code>varstats = TRUE</code>, in which case the output includes
<code>vpu</code> and <code>vpt</code> fields; See <code><a href="#topic+varpropuse">varpropuse</a></code>
and <code><a href="#topic+varproptotal">varproptotal</a></code> for more details
</p>
<p>The <code>dynaTrees</code> function does not return <code>num</code> since 
it does not leave  any allocated particle clouds on the <code>C</code>-side
</p>


<h3>Note</h3>

<p>As mentioned in the details section, above, the
<code>dynaTree</code> function returns a pointer to a particle
cloud allocated in <code>C</code>.  This pointer is used
for prediction, via <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> and for
later updating/augmentation of data, via
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code>.
This information will not be &ldquo;freed&rdquo; unless
the user specifically calls <code><a href="#topic+deletecloud">deletecloud</a>(num)</code>
or <code><a href="#topic+deleteclouds">deleteclouds</a>()</code>.  Failing to call one
of these functions (when done with the corresponding
object(s)) could result in a memory leak;
see their documentation for more details.
</p>
<p>The <code>C</code>-side memory cannot be saved in the workspace,
so they cannot persist across <code>R</code> sessions
</p>
<p>To copy a <code>"dynaTree"</code>-class object, use
<code><a href="#topic+copy.dynaTree">copy.dynaTree</a></code>, which will also copy the <code>C</code>-side
memory allocated to the object
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos 
</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing&rdquo;.
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p>Chipman, H., George, E., &amp; McCulloch, R. (1998).
<em>Bayesian CART model search (with discussion).</em>
Journal of the American Statistical Association, <b>93</b>,
935&ndash;960.
</p>
<p>Chipman, H., George, E., &amp; McCulloch, R. (2002).
<em>Bayesian treed models.</em>
Machine Learning, <b>48</b>, 303&ndash;324.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>,
<code><a href="#topic+plot.dynaTree">plot.dynaTree</a></code>, <code><a href="#topic+deletecloud">deletecloud</a></code>,
<code><a href="#topic+copy.dynaTree">copy.dynaTree</a></code>, <code><a href="#topic+getBF">getBF</a></code>, 
<code><a href="#topic+varpropuse">varpropuse</a></code>, <code><a href="#topic+varproptotal">varproptotal</a></code>,
<code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>, <code><a href="#topic+relevance.dynaTree">relevance.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple parabolic data
n &lt;- 100
Xp &lt;- sort(runif(n,-3,3))
Yp &lt;- Xp + Xp^2 + rnorm(n, 0, .2)

## fit a piece-wise linear model
parab.fit &lt;- dynaTree(Xp, Yp, model="linear")

## obtain predictions at a new set of locations
## and plot
parab.fit &lt;- predict(parab.fit, XX=seq(-3, 3, length=100))
plot(parab.fit)

## try duplicating the object
parab.fit.copy &lt;- copy(parab.fit)

## must delete the cloud or memory may leak
deletecloud(parab.fit); parab.fit$num &lt;- NULL
## to delete all clouds, do:
deleteclouds()

## for more examples of dynaTree see update.dynaTree

## Motorcycle accident data
if(require("MASS")) {
  data(mcycle)
  Xm &lt;- mcycle[,1]
  Ym &lt;- mcycle[,2]
  XXm &lt;- seq(min(mcycle[,1]), max(mcycle[,1]), length=100)
  
  R &lt;- 2 ## use R &gt;= 10 for better results
  ## small R is for faster CRAN checks
  ## fit constant model with R=2 repeats and predictions
  moto.fit &lt;- dynaTrees(Xm, Ym, XX=XXm, R=R, plotit=TRUE)
  
  ## plot the averages
  plot(moto.fit, ptype="mean")
  
  ## clouds automatically deleted by dynaTrees
}

## Not run: 
## 2-d/3-class classification data
library(plgp)
library(tgp)
xx &lt;- seq(-2, 2, length=20)
XX &lt;- expand.grid(xx, xx)
X &lt;- dopt.gp(125, Xcand=XX)$XX
C &lt;- exp2d.C(X)

## fit a classification model with R=10 repeats, 
class.fit &lt;- dynaTrees(X, C, XX=XX, model="class")

## for plot the output (no generic plotting available)
cols &lt;- c(gray(0.85), gray(0.625), gray(0.4))
par(mfrow=c(1,2))
library(interp)

## plot R-averaged predicted class
mclass &lt;- apply(class.fit$p, 1, which.max)
image(interp(XX[,1], XX[,2], mclass), col=cols,
      xlab="x1", ylab="x2", main="repeated class mean")
points(X)
## plot R-averaged entropy
ment &lt;-  apply(class.fit$entropy, 1, mean)
image(interp(XX[,1], XX[,2], ment),
      xlab="x1", ylab="x2", main="repeated entropy mean")

## End(Not run)
</code></pre>

<hr>
<h2 id='elec2'>
The ELEC2 Data Set
</h2><span id='topic+elec2'></span>

<h3>Description</h3>

<p>Electricity Pricing Data Set Exhibiting Concept Drift
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(elec2)</code></pre>


<h3>Format</h3>

<p>A data frame with 27552 observations on the following 5 variables.
</p>

<dl>
<dt><code>x1</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>y</code></dt><dd><p>class label</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data has become a benchmark of sorts in streaming
classification.  It was first described by Harries (1999)
and used thereafter for several performance comparisons
[e.g., Baena-Garcia et al. (2006); Kuncheva and Plumpton, (2008)].
It holds information for the Australian New South Wales (NSW)
Electricity Market, containing 27552 records dated from
May 1996 to December 1998, each referring to a period of 30 minutes
subsampled as the completely observed portion of 45312 total records
with missing values.
These records have seven fields: a binary class label,
two time stamp indicators (day of week, time),
and four covariates capturing aspects of electricity demand and
supply.
</p>
<p>An appealing property of this dataset is that it is expected to
contain drifting data distributions since, during the recording
period,  the electricity market was expanded to include adjacent
areas. This allowed for the production surplus of one region
to be sold in the adjacent region, which in turn dampened price levels.
</p>


<h3>Source</h3>

<p>M. Harries. &ldquo;Splice-2 Comparative Evaluation: Electricity
Pricing&rdquo;. University of New South Wales, School of Computer Science
and Engineering technical report (1999)
</p>


<h3>References</h3>

<p>Anagnostopoulos, C., Gramacy, R.B. (2013) &ldquo;Information-Theoretic 
Data Discarding for Dynamic Trees on Data Streams.&rdquo; Entropy, 15(12), 
5510-5535; arXiv:1201.5568
</p>
<p>M. Baena-Garcia, J. del Campo-Avila, R., Fidalgo, A. Bifet,
R. Gavalda and R. Morales-Bueno. &ldquo;Early drift detection
method&rdquo;. <em>ECML PKDD 2006 Workshop on Knowledge Discovery from
Data Streams</em>, pp. 77-86 (2006)
</p>
<p>L.I. Kuncheva C.O. and Plumpton. &ldquo;Adaptive Learning Rate for
Online Linear Discriminant Classifiers&rdquo;. <em>SSPR and SPR 2008,
Lecture Notes in Computer Science (LNCS)</em>, 5342, pp. 510-519 (2008)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## this is a snipet from the "elec2" demo; see that demo
## for a full comparison to dynaTree models which can
## cope with drifting concepts

## set up data
data(elec2)
X &lt;- elec2[,1:4]
y &lt;- drop(elec2[,5])

## predictive likelihood for repated trials
T &lt;- 200 ## use nrow(X) for a longer version,
## short T is for faster CRAN checks
hits &lt;-  rep(NA, T)

## fit the initial model
n &lt;- 25; N &lt;- 1000
fit &lt;- dynaTree(X[1:n,], y[1:n], N=N, model="class")

w &lt;- 1
for(t in (n+1):T) {

  ## predict the next data point
  ## full model
  fit &lt;- predict(fit, XX=X[t,], yy=y[t])
  hits[t] &lt;- which.max(fit$p) == y[t]

  ## sanity check retiring index
  if(any(fit$X[w,] != X[t-n,])) stop("bad retiring")

  ## retire
  fit &lt;- retire(fit, w)
  ## update retiring index
  w &lt;- w + 1; if(w &gt;= n) w &lt;- 1

  ## update with new point
  fit &lt;- update(fit, X[t,], y[t], verb=100)
}

## free C-side memory
deleteclouds()

## plotting a moving window of hit rates over time
rhits &lt;- rep(0, length(hits))
for(i in (n+1):length(hits)) {
  rhits[i] &lt;- 0.05*as.numeric(hits[i]) + 0.95*rhits[i-1]
}

## plot moving window of hit rates
plot(rhits, type="l", main="moving window of hit rates",
     ylab="hit rates", xlab="t")
</code></pre>

<hr>
<h2 id='getBF'>
Extract a Path of (log) Bayes Factors
</h2><span id='topic+getBF'></span>

<h3>Description</h3>

<p>Extract a path (log) Bayes factors (BFs) from the log marginal posterior
probabilities of two <code>"dynaTree"</code>-class objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getBF(obj1, obj2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getBF_+3A_obj1">obj1</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="getBF_+3A_obj2">obj2</code></td>
<td>

<p>another <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simply calculates a difference in log marginal posterior
probabilities, setting BFs to zero for initial elements of the
path where one of the objects has more zero marginal probabilities
than the other.  The BF is for the model in <code>obj1</code> over
<code>obj2</code>. If the objects are the output of repeated
fits as obtained from <code><a href="#topic+dynaTrees">dynaTrees</a></code>, then multiple
traces are returned
</p>


<h3>Value</h3>

<p>Returns a vector or <code>matrix</code> of a trace(s) of Bayes factors that
can be plotted; see examples below
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos
</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>,
<code>link{logpost}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## parabola data
n &lt;- 100
Xp &lt;- sort(runif(n,-3,3))
Yp &lt;- Xp + Xp^2 + rnorm(n, 0, .2)
XXp &lt;- seq(-3,3,length=100)

## comparison by log Bayes Factor
R &lt;- 2 ## use R &gt;= 10 for better results
## small R is for faster CRAN checks
o &lt;- apply(matrix(runif(n*(R-1)), ncol=R-1), 2, order)
lpc.p &lt;- dynaTrees(Xp, Yp, R=R, rorder=o, verb=0)
lpl.p &lt;- dynaTrees(Xp, Yp, model="linear", R=R, rorder=o, verb=0)
bf.p &lt;- getBF(lpl.p, lpc.p)

## plot the log Bayes factors
matplot(bf.p, type="l", lty=1, col="gray", main="parabola",
        xlab="time", ylab="log Bayes factor")

## see demo("reg1d") for further examples
</code></pre>

<hr>
<h2 id='plot.dynaTree'>
Plotting Predictive Distributions of Dynamic Tree models
</h2><span id='topic+plot.dynaTree'></span>

<h3>Description</h3>

<p>Plotting predictive distributions constructed from dynamic
tree (regression) models for 1-d data &ndash; provided primarily
for use in our 1-d examples and for illustrative purposes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
plot(x, proj = 1, add = FALSE, ylim = NULL,
              col = 2, lwd = 1, ptype = c("each", "mean"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.dynaTree_+3A_x">x</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_add">add</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the lines/points should
be &ldquo;added&rdquo; to an existing plot
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_proj">proj</code></td>
<td>

<p>when <code>ncol(x$X) &gt; 1</code> this argument can be used to plot
a 1-d projection by specifying which column of <code>x$X</code>
should be used to make the plot
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_ylim">ylim</code></td>
<td>

<p>user-specified y-axis limits values; see <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_col">col</code></td>
<td>

<p>user-specified color value; see <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_lwd">lwd</code></td>
<td>

<p>user-specified line-width value; see <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_ptype">ptype</code></td>
<td>

<p>type of plot used to visualize several predictive
samples obtained from <code><a href="#topic+dynaTrees">dynaTrees</a></code>: <code>"each"</code>
shows each surface with its own set of three lines, and
<code>"mean"</code> shows the three lines obtained by averaging
</p>
</td></tr>
<tr><td><code id="plot.dynaTree_+3A_...">...</code></td>
<td>

<p>other arguments to the generic <code><a href="base.html#topic+plot">plot</a></code> method
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plotting function only handles the predictive output from
1-dimensional regression <code><a href="#topic+dynaTree">dynaTree</a></code> models as obtained by
first calling <code><a href="#topic+dynaTree">dynaTree</a></code> and then
<code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> on the resulting output at new
<code>XX</code> locations.  It is provided to help make the illustration
of our 1-d examples easier and to serve as an aid in a user's
development of custom plotting functions in higher dimensions
</p>


<h3>Value</h3>

<p>The only output of this function is a pretty plot
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos </p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, <code><a href="#topic+dynaTree">dynaTree</a></code>,
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see dynaTree, dynaTrees and update.dynaTree for examples
## which use this plot function
</code></pre>

<hr>
<h2 id='predict.dynaTree'>
Prediction for Dynamic Tree Models
</h2><span id='topic+predict.dynaTree'></span><span id='topic+coef.dynaTree'></span>

<h3>Description</h3>

<p>Predicting and calculating sequential design and optimization
statistics at new design points (i.e., active learning heuristics)
for dynamic tree models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
predict(object, XX, yy = NULL, quants = TRUE,
                 ei = FALSE, verb = 0, ...)
## S3 method for class 'dynaTree'
coef(object, XX, verb = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="predict.dynaTree_+3A_xx">XX</code></td>
<td>

<p>a design <code>matrix</code> of predictive locations (where <code>ncol(XX) ==
      ncol(X)</code>)
</p>
</td></tr>
<tr><td><code id="predict.dynaTree_+3A_yy">yy</code></td>
<td>

<p>an optional vector of &ldquo;true&rdquo; responses at the <code>XX</code>
predictive locations at which the log posterior probability are
to be reported
</p>
</td></tr>  	 
<tr><td><code id="predict.dynaTree_+3A_quants">quants</code></td>
<td>

<p>a scalar <code>logical</code> indicating if predictive quantiles
are desired (useful for visualization, but less so for active
learning); calculating predictive quantiles is expensive and should
be turned off if prediction is not being used for visualization,
e.g., if used for active learning
</p>
</td></tr>
<tr><td><code id="predict.dynaTree_+3A_ei">ei</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the expected improvement
statistic (for optimization) should be calculated and returned
</p>
</td></tr>
<tr><td><code id="predict.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many predictive locations
(iterations) after which a progress statement should be
printed to the console; a (default) value of <code>verb = 0</code> is quiet
</p>
</td></tr>
<tr><td><code id="predict.dynaTree_+3A_...">...</code></td>
<td>

<p>to comply with the generic <code><a href="stats.html#topic+predict">predict</a></code> method &ndash;
currently unused
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict</code> returns predictive summary statistics by averaging over the
samples from the posterior predictive distribution obtained
from each of the particles in the cloud pointed to by the
object (<code>object</code>)
</p>
<p><cite>coef</cite> returns a matrix of regression coefficients used in linear
model leaves (<code>model = "linear"</code>) leaves, averaged over all particles,
for each <code>XX</code> location. For other models it prints a warning and 
defaults to <code>predict</code>.
</p>
<p>The value(s) calculated are appended to <code>object</code>; the new
fields are described below
</p>
<p>Note that ALC calculations have been moved to the <code><a href="#topic+alc.dynaTree">alc.dynaTree</a></code>
function(s)
</p>


<h3>Value</h3>

<p>The object returned is of class <code>"dynaTree"</code>, which includes a
copy of the list elements from the <code>object</code> passed in, 
with the following (predictive)
additions depending on whether <code>object$model</code> is for
regression (<code>"constant"</code> or <code>"linear"</code>) or classification
(<code>"class"</code>).
</p>
<p>For regression:
</p>
<table role = "presentation">
<tr><td><code>mean</code></td>
<td>
<p> a vector containing an estimate of the predictive mean
at the <code>XX</code> locations </p>
</td></tr>
<tr><td><code>vmean</code></td>
<td>
<p> a vector containing an estimate of the variance of predictive mean
at the <code>XX</code> locations </p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p> a vector containing an estimate of the predictive
variance (average variance plus variance of mean) at the <code>XX</code> locations </p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p> a vector containing  the average degrees of freedom at the <code>XX</code> locations </p>
</td></tr>
<tr><td><code>q1</code></td>
<td>
<p> a vector containing an estimate of the 5% quantile of
the predictive distribution at the <code>XX</code> locations,
unless <code>quants = FALSE</code></p>
</td></tr>
<tr><td><code>q2</code></td>
<td>
<p> a vector containing an estimate of the 95% quantile of
the predictive distribution at the <code>XX</code> locations, unless
<code>quants = FALSE</code></p>
</td></tr>
<tr><td><code>yypred</code></td>
<td>
<p> if <code>yy != NULL</code> then this contains the
predictive probability of the true <code>yy</code> values at the
<code>XX</code> locations</p>
</td></tr>
<tr><td><code>ei</code></td>
<td>
<p> a vector containing an estimate of the EI statistic,
unless <code>ei = FALSE</code></p>
</td></tr></table>
<p>;
</p>
<p>For classification:
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p> a <code>nrow(XX)</code>-by-<code>max(object$y)</code> <code>matrix</code> of
mean class probabilities for each of <code>max(object$y)</code> classes
at the predictive data locations </p>
</td></tr>
<tr><td><code>entropy</code></td>
<td>
<p> a <code>nrow(XX)</code> vector of predictive entropys
at the predictive data locations</p>
</td></tr></table>
<p>;
</p>
<p>For <code>coef</code> a new <code>XXc</code> field is created so as not to trample
on <code>XX</code>s that may have been used in a previous <code>predict</code>,
plus
</p>
<table role = "presentation">
<tr><td><code>coef</code></td>
<td>
<p> a <code>nrow(XX)</code>-by-<code>m+icept</code></p>
</td></tr></table>
<p> matrix of particle-
averaged regression coefficients.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>,
<code><a href="#topic+plot.dynaTree">plot.dynaTree</a></code>, <code><a href="#topic+alc.dynaTree">alc.dynaTree</a></code>, 
<code><a href="#topic+entropyX.dynaTree">entropyX.dynaTree</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see the example(s) section(s) of dynaTree and
## update.dynaTree and the demos (demo(package=dynaTree))
</code></pre>

<hr>
<h2 id='rejuvenate.dynaTree'>
Rejuvenate particles from the dynaTree posterior
</h2><span id='topic+rejuvenate.dynaTree'></span>

<h3>Description</h3>

<p>Re-pass the <code>X</code>-<code>y</code> pairs in the <code>object</code>
in a random (or specified) order to temporarily double the
size of the particle set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
rejuvenate(object, odr = order(runif(length(object$y))),
     verb = round(length(object$y)/10))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rejuvenate.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="rejuvenate.dynaTree_+3A_odr">odr</code></td>
<td>

<p>an integer vector of <code>length(object$y)</code> specifying the order
in which the <code>object$X</code>-<code>object$y</code> paris should be
processed for the rejuvenated particles
</p>
</td></tr>
<tr><td><code id="rejuvenate.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many time steps
(iterations) should pass before a progress statement is
printed to the console; a value of <code>verb = 0</code> is quiet
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>rejuvenate</code> function causes the particle set to
temporarily double, to have size <code>2 * object$N</code>.  The new
<code>object$N</code> particles represent a discrete approximation
to the <code><a href="#topic+dynaTree">dynaTree</a></code> posterior under the ordering specified
by <code>odr</code>, which may be random.  Subsequent calls to
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code> cause the particle set to revert back
to <code>object$N</code> particles as only that many are obtained from
the particle learning resample step.
</p>
<p>This function can be particularly useful in online learning contexts,
where <code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code> is used to retain information
on discarded data, especially when the data is discarded historically
to deal with drifting concepts.  Since the new, rejuvenated, particles
are based only on the active data, <code>object$X</code>-<code>object$y</code>
pairs (and not the retired data via informative leaf priors),
subsequent <code><a href="#topic+update.dynaTree">update.dynaTree</a></code> steps allow the data
to dictate if old (informative prior) or new (default prior) particles
are best for the new concept
</p>


<h3>Value</h3>

<p>The returned list is the same as <code><a href="#topic+dynaTree">dynaTree</a></code> &ndash;
i.e., a <code>"dynaTree"</code>-class object but with <code>2 * object$N</code>
particles.  Note that <code>object$N</code> is not updated to reflect this
fact, but the C-side object will indeed have a double particle set.
Repeated calls to <code>rejuvenate</code> will cause the particle set to
double again.
</p>


<h3>Note</h3>

<p>The object (<code>object</code>) must contain a pointer to a particle
cloud (<code>object$num</code>) which has not been deleted by
<code><a href="#topic+deletecloud">deletecloud</a></code>.  In particular, it cannot be
an object returned from <code><a href="#topic+dynaTrees">dynaTrees</a></code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos
</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Anagnostopoulos, C., Gramacy, R.B. (2013) &ldquo;Information-Theoretic 
Data Discarding for Dynamic Trees on Data Streams.&rdquo; Entropy, 15(12), 
5510-5535; arXiv:1201.5568
</p>
<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing&rdquo;.
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+alcX.dynaTree">alcX.dynaTree</a></code>, 
<code><a href="#topic+entropyX.dynaTree">entropyX.dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>,
<code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see retire.dynaTree for a combined example
## illustrating rejuvenation
</code></pre>

<hr>
<h2 id='relevance.dynaTree'>
Calculate relevance statistics for input coordinates
</h2><span id='topic+relevance.dynaTree'></span>

<h3>Description</h3>

<p>Computes relevance statistics for each input coordinate by
calculating their particle-averaged mean reduction in variance
each time that coordinate is used as a splitting variable in
(an internal node of) the tree(s)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relevance.dynaTree(object, rect = NULL, categ = NULL,
     approx = FALSE, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relevance.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="relevance.dynaTree_+3A_rect">rect</code></td>
<td>

<p>an optional <code>matrix</code> with two columns and
<code>ncol(object$X)</code> rows describing the bounding rectangle
for the ALC integration; the
default that is used when <code>rect = NULL</code> is the bounding
rectangle obtained by applying <code><a href="base.html#topic+range">range</a></code> to each
column of <code>object$X</code> (taking care to remove the
first/intercept column of <code>object$X</code> if <code>icept =
      "augmented"</code></p>
</td></tr>
<tr><td><code id="relevance.dynaTree_+3A_categ">categ</code></td>
<td>

<p>A vector of logicals of length <code>ncol(object$X)</code> indicating
which, if any, dimensions of the input space should be treated
as categorical; the default <code>categ</code>
argument is <code>NULL</code> meaning that the categorical inputs
are derived from <code>object$X</code> in a sensible way</p>
</td></tr>
<tr><td><code id="relevance.dynaTree_+3A_approx">approx</code></td>
<td>

<p>a scalar logical indicating if the count of the number of
data points in the leaf should be used in place of its area;
this can help with numerical accuracy in high dimensional input
spaces
</p>
</td></tr>
<tr><td><code id="relevance.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many particles should
be processed (iterations) before a progress statement should be
printed to the console; a (default) value of <code>verb = 0</code> is quiet
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each binary split in the tree (in each particle) emits a reduction
in variance (for regression models) or a reduction in entropy
(for classification).  This function calculates these reductions
and attributes them to the variable(s) involved in the split(s).
Those with the largest relevances are the most useful for prediction.
A sensible variable selection rule based on these relevances is to
discard those variables whose median relevance is not positive.  See
the Gramacy, Taddy, &amp; Wild (2011) reference below for more details.
</p>
<p>The new set of particles is appended to the old set.  However
after a subsequent <code><a href="#topic+update.dynaTree">update.dynaTree</a></code> call the total
number of particles reverts to the original amount.
</p>
<p>Note that this does not work well with <code><a href="#topic+dynaTree">dynaTree</a></code> objects
which were built with <code>model="linear"</code>.  Rather, a full
sensitivity analysis (<code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>) is needed.  Usually
it is best to first do <code>model="constant"</code> and then use
<code><a href="#topic+relevance.dynaTree">relevance.dynaTree</a></code>.  Bayes factors (<code><a href="#topic+getBF">getBF</a></code>)
can be used to back up any variable selections implied by the
relevance.  Then, if desired, one can re-fit on the new (possibly
reduced) set of predictors with <code>model="linear"</code>.
</p>
<p>There are no caveats with <code>model="class"</code>
</p>


<h3>Value</h3>

<p>The entire <code>object</code> is returned with a new entry called
<code>relevance</code> containing a <code>matrix</code> with <code>ncol(X)</code>
columns.  Each row contains the sample from the relevance of
each input, and there is a row for each particle
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>,
<code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>
<code><a href="#topic+varpropuse">varpropuse</a></code>, <code><a href="#topic+varproptotal">varproptotal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see the examples in sens.dynaTree for the relevances;
## Also see varpropuse and the class2d demo via
## demo("class2d")
</code></pre>

<hr>
<h2 id='retire.dynaTree'>
Retire (i.e. remove) data from the a dynaTree model
</h2><span id='topic+retire.dynaTree'></span>

<h3>Description</h3>

<p>Allows the removal (or &ldquo;retireing&rdquo;
of <code>X</code>-<code>y</code> pairs from a
<code>"dynaTree"</code>-class object to facilitate online
learning; &ldquo;retireed&rdquo; pairs ar absorbed into
the leaf prior(s)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
retire(object, indices, lambda = 1, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="retire.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="retire.dynaTree_+3A_indices">indices</code></td>
<td>

<p>a vector of positive integers in <code>1:nrow(object$X)</code> indicating
which <code>X</code>-<code>y</code> pairs to &ldquo;retire&rdquo;; must
have <code>length(indices) &lt;= nrow(object$X)</code>
</p>
</td></tr>
<tr><td><code id="retire.dynaTree_+3A_lambda">lambda</code></td>
<td>

<p>a scalar proportion (forgetting factor) used to downweight the previous prior
summary statistics
</p>
</td></tr>
<tr><td><code id="retire.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a nonzero scalar causes info about the &ldquo;retireed&rdquo; indices,
i.e., their <code>X</code>-<code>y</code> values, to be printed to the
screen as they are &ldquo;retireed&rdquo;
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Primarily for use in online learning contexts.  After
&ldquo;retireing&rdquo; the predictive distribution remains unchanged,
because the sufficient statistics of the removed pairs enters
the prior in the leaves of the tree of each particle.  Further
<code><a href="#topic+update.dynaTree">update.dynaTree</a></code> calls (adding data) may cause
changes to the posterior predictive as grow moves cannot keep
the &ldquo;retires&rdquo;; see a forthcoming paper for more
details.  In many ways, <code><a href="#topic+retire.dynaTree">retire.dynaTree</a></code> is the
opposite of <code><a href="#topic+update.dynaTree">update.dynaTree</a></code> except that the loss of
information upon &ldquo;retireing&rdquo; is not complete.
</p>
<p>Drifting regression or classification relationships may be modeled
with a forgetting factor <code>lambda &lt; 1</code>
</p>
<p>The <code><a href="#topic+alcX.dynaTree">alcX.dynaTree</a></code> provides a good, and computationally
efficient, heuristic for choosing which points to &ldquo;retire&rdquo; for 
regression models, and likewise <code>link{entropyX.dynaTree}</code> for 
classification models.
</p>
<p>Note that classification models (<code>model = "class"</code>) are
not supported, and implicit intercepts (<code>icept = "implicit"</code>)
with linear models (<code>model = "linear"</code>) are not supported
at this time
</p>


<h3>Value</h3>

<p>returns a <code>"dynaTree"</code>-class object with updated attributes
</p>


<h3>Note</h3>

<p> In order to use <code>model = "linear"</code> with
<code><a href="#topic+dynaTree">dynaTree</a></code> and retirement one must also specify
<code>icept = "augmented"</code> which automatically augments an
extra column of ones onto the input <code>X</code> design matrix/matrices.
The <code>retire</code> function only supports this <code>icept</code> case
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Anagnostopoulos, C., Gramacy, R.B. (2013) &ldquo;Information-Theoretic 
Data Discarding for Dynamic Trees on Data Streams.&rdquo; Entropy, 15(12), 
5510-5535; arXiv:1201.5568
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+alcX.dynaTree">alcX.dynaTree</a></code>, 
<code><a href="#topic+entropyX.dynaTree">entropyX.dynaTree</a></code>, <code><a href="#topic+update.dynaTree">update.dynaTree</a></code>,
<code><a href="#topic+rejuvenate.dynaTree">rejuvenate.dynaTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100
Xp &lt;- runif(n,-3,3)
XX &lt;- seq(-3,3, length=200)
Yp &lt;- Xp + Xp^2 + rnorm(n, 0, .2)
rect &lt;- c(-3,3)
out &lt;- dynaTree(Xp, Yp, model="linear", icept="augmented")

## predict and plot
out &lt;- predict(out, XX)
plot(out, main="parabola data", lwd=2)

## randomly remove half of the data points
out &lt;- retire(out, sample(1:n, n/2, replace=FALSE))

## predict and add to plot -- shouldn't change anything
out &lt;- predict(out, XX)
plot(out, add=TRUE, col=3)
points(out$X[,-1], out$y, col=3)

## now illustrating rejuvenation, which should result
## in a change to the predictive surface
out &lt;- rejuvenate(out)
out &lt;- predict(out, XX)
plot(out, add=TRUE, col=4)
legend("top", c("original", "retired", "rejuvenated"),
       col=2:4, lty=1)

## clean up
deletecloud(out)

## see demo("online") for an online learning example
## where ALC is used for retirement
</code></pre>

<hr>
<h2 id='sens.dynaTree'>
Monte Carlo Sensitivity Analysis for dynaTree Models
</h2><span id='topic+sens.dynaTree'></span>

<h3>Description</h3>

<p>A Monte Carlo sensitivity analysis using random Latin
hypercube samples (LHSs) or bootstrap resamples for each particle
to estimate main effects as well as 1st order and total
sensitivity indices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
sens(object, class = NULL, nns = 1000, nME = 100,
              span = 0.3, method = c("lhs", "boot"),
              lhs = NULL, categ = NULL, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sens.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_class">class</code></td>
<td>

<p>only valid for <code>object$model = "class"</code>, allows the
user to specify the subset of class labels in
<code>unique(object$y)</code> for which sensitivity indices are
calculated.  The implementation loops over the vector
of labels provided.  The default of <code>NULL</code> results in
<code>class = unique(object$y)</code>
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_nns">nns</code></td>
<td>

<p>A positive integer scalar indicating the size of each
LHS or bootstrap drawn for use in the Monte Carlo
integration scheme underlying the sensitivity analysis;
the total number of locations is <code>nn.lhs*(ncol(X)+2)</code>
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_nme">nME</code></td>
<td>

<p>A positive integer scalar indicating number of grid points,
in each input dimension, upon which
main effects will be estimated
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_span">span</code></td>
<td>

<p>A positive real-valued scalar giving the smoothing parameter
for main effects integration: the fraction of <code>nns</code> points
that will be included in a moving average window that is used
to estimate main effects at the <code>nME</code> locations in each
input dimension
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_method">method</code></td>
<td>

<p>indicates whether LHS or bootstrap should be used
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_lhs">lhs</code></td>
<td>

<p>if <code>method = "lhs"</code> then this argument should be
a <code><a href="base.html#topic+list">list</a></code> with entries <code>rect</code>, <code>shape</code>
and <code>mode</code> describing the marginal distributions of
the Latin Hypercube; specify <code>NULL</code>
for a default specification for 
<code>method = "boot"</code>.  The fields should have the following
format(s):
</p>

<ul>
<li> <p><code>rect</code>: Optional rectangle describing the domain of
the uncertainty
distribution with respect to which the sensitivity is to be
determined.  This defines the domain from which the LH sample
is to be taken.  The rectangle should be a <code>matrix</code> or
<code>data.frame</code> with <code>ncol(rect) = 2</code>, and number of rows
equal to the dimension of the domain.  For 1-d data, a vector of
length 2 is allowed.  The default is the input
data range of each column of (<code>object$X</code>).
</p>
</li>
<li> <p><code>shape</code>: Optional vector of shape parameters for Beta
marginal distributions having length <code>ncol(object$X)</code> and
elements <code>&gt; 1</code>, i.e., concave Beta distributions. If
specified, the uncertainty distribution (i.e. the LHS) is
proportional to a joint pdf formed by independent Beta
distributions in each dimension of the domain, scaled and shifted
to have support defined by <code>rect</code>.  If unspecified, the
uncertainty distribution is uniform over <code>rect</code>. The
specification <code>shape[i]=0</code> instructs <code>sens</code> to treat the
i'th dimension as a binary variable. In this case, <code>mode[i]</code>
is the probability parameter for a bernoulli uncertainty
distribution, and we must also have <code>rect[i,]=c(0,1)</code>.
</p>
</li>
<li> <p><code>mode</code>: Optional vector of mode values for the Beta
uncertainty distribution. Vector of length equal to the dimension
of the domain, with elements within the support defined by
<code>rect</code>.  If <code>shape</code> is specified, but this is not,
then the scaled Beta distributions will be symmetric.
</p>
</li></ul>

</td></tr>
<tr><td><code id="sens.dynaTree_+3A_categ">categ</code></td>
<td>

<p>A vector of logicals of length <code>ncol(object$X)</code> indicating
which, if any, dimensions of the input space should be treated
as categorical; this input is used to help set the default
<code>lhs$shape</code> argument if not specified; the default <code>categ</code>
argument is <code>NULL</code> meaning that the categorical inputs
are derived from <code>object$X</code> in a sensible way
</p>
</td></tr>
<tr><td><code id="sens.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many predictive locations
(iterations) after which a progress statement should be
printed to the console; a (default) value of <code>verb = 0</code> is quiet
</p>
</td></tr></table>


<h3>Details</h3>

<p>Saltelli (2002) describes a Latin Hypercube sampling based method for
estimation of the 'Sobol' sensitivity indices:
</p>
<p>1st Order for input <code class="reqn">i</code>,
</p>
<p style="text-align: center;"><code class="reqn">S(i) = \mbox{Var}(E[f|x_i])/\mbox{Var}(f),</code>
</p>

<p>where <code class="reqn">x_i</code> is the  <code class="reqn">i</code>-th input.
</p>
<p>Total Effect for input <code class="reqn">i</code>,
</p>
<p style="text-align: center;"><code class="reqn">T(i) = E[\mbox{Var}(f|x_{-i})]/\mbox{Var}(f),</code>
</p>

<p>where <code class="reqn">x_{-i}</code> is all inputs except for the <code class="reqn">i</code>-th.
</p>
<p>All moments are with respect to the appropriate marginals of the
uncertainty distribution <code class="reqn">U</code> &ndash; that is, the probability
distribution on the inputs with respect to which sensitivity is being
investigated.
Under this approach, the integrals involved are approximated through
averages over properly chosen samples based on two LH samples
proportional to U. If <code>nns</code> is the sample size for the
Monte Carlo estimate, this scheme requires <code>nns*(ncol(X)+2)</code>
function evaluations.
</p>
<p>The <code>sens.dynaTree</code> function implements the method for unknown functions
<code class="reqn">f</code>, through prediction via one of the <span class="pkg">tgp</span> regression
models conditional on an observed set of <code>X</code> locations.
For each particle, treated as sample from the <code><a href="#topic+dynaTree">dynaTree</a></code>
model posterior,
the <code>nns*(ncol(X)+2)</code> locations are drawn randomly from the
LHS scheme and realizations of the sensitivity indices are
calculated. Thus we obtain a posterior sample of the indices,
incorporating variability from both the Monte Carlo estimation and
uncertainty about the function output. Since a subset of the
predictive locations are actually an LHS proportional to the
uncertainty distribution, we can also estimate the main effects
through simple non-parametric regression (a moving average).
</p>
<p>See the Gramacy, Taddy, &amp; Wild (2011) reference below for more details.
</p>
<p>If <code>method = "boot"</code> is used then simply replace LHS above
with a bootstrap resample of the <code>object$X</code> locations.
</p>
<p>As with prediction, the <code><a href="#topic+dynaTrees">dynaTrees</a></code> function enables
repeated calls to <code>sens.dynaTree</code>
</p>


<h3>Value</h3>

<p>The object returned is of class <code>"dynaTree"</code>, which includes a
copy of the list elements from the <code>object</code> passed in, 
with the following (sensitivity-analysis specific)
additions.
</p>
<table role = "presentation">
<tr><td><code>MEgrid</code></td>
<td>
<p> An <code>nME</code>-by-<code>ncol(object$X)</code> matrix
containing the main effects predictive grid at which the following
<code>MEmean</code>, <code>MEq1</code>, and <code>MEq2</code> quantities were obtained </p>
</td></tr>
<tr><td><code>MEmean</code></td>
<td>
<p> A <code><a href="base.html#topic+matrix">matrix</a></code> with <code>ncol(object$X)</code>
columns and <code>nME</code> rows containing the mean main effects
for each input dimension </p>
</td></tr>
<tr><td><code>MEq1</code></td>
<td>
<p> same as <code>MEmean</code> but containing the 5% quantiles </p>
</td></tr>
<tr><td><code>MEq2</code></td>
<td>
<p> same as <code>MEmean</code> but containing the 95% quantiles </p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p> An <code>object$N</code>-row and <code>ncol(object$X)</code>
<code><a href="base.html#topic+matrix">matrix</a></code> containing the posterior (samples) of the
1st Order Sobol sensitivity indices </p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p> same as <code>S</code> but containing the Total Effect indices </p>
</td></tr>
</table>
<p>In the case of <code>object$model = "class"</code> the entries
listed above will themselves be lists with an entry for each
<code>class</code> specified on input, or all classes as is the
default
</p>


<h3>Note</h3>

<p>The quality of sensitivity analysis is dependent on the size of
the LHSs used for integral approximation; as with any Monte
Carlo integration scheme, the sample size (<code>nns</code>) must
increase with the dimensionality of the problem.  The total
sensitivity indices <code class="reqn">T</code> are forced non-negative,
and if negative values occur it is necessary to increase
<code>nnd</code>. Postprocessing replaces negative values with <code>NA</code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Saltelli, A. (2002) 
<em>Making best use of model evaluations to compute sensitivity indices.</em>
Computer Physics Communications, 145, 280-297.
</p>
<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>,
<code><a href="#topic+relevance.dynaTree">relevance.dynaTree</a></code>, 
<code><a href="#topic+varpropuse">varpropuse</a></code>, <code><a href="#topic+varproptotal">varproptotal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## friedman data
if(require("tgp")) {
  f &lt;- friedman.1.data(1000)
  X &lt;- f[,1:6]
  Z &lt;- f$Y
  
  ## fit the model and do the sensitivity analysis
  N &lt;- 100 ## use N &gt;= 1000 for better results
  ## small N is for fast CRAN checks
  out &lt;- dynaTree(X=X, y=Z, N=N, ab=c(0.01,2))
  ## also try with model="linear"
  
  ## gather relevance statistics
  out &lt;- relevance(out)
  boxplot(out$relevance)
  abline(h=0, col=2, lty=2)
  ## relevance stats are not as useful when model="linear"
  ## since it will appear that x4 and x5 not helpful; these
  ## interact linearly with the response
  
  ## full simulation-based sensitivity analysis, the dynaTree::
  ## part is only needed if the tgp package is loaded
  out &lt;- dynaTree::sens(out, verb=100)
  
  ## plot the main effects
  r &lt;- range(rbind(c(out$MEmean, out$MEq1, out$MEq2)))
  par(mfrow=c(1,ncol(out$X)), mar=c(5,3,2,1))
  plot(out$MEgrid[,1], out$MEmean[,1], type="l", ylim=r, lwd=2,
       ylab="", xlab=colnames(out$MEmean)[1])
  lines(out$MEgrid[,1], out$MEq1[,1], lty=2, lwd=2)
  lines(out$MEgrid[,1], out$MEq2[,1], lty=2, lwd=2)
  if(ncol(out$X) &gt; 1) {
    for(d in 2:ncol(out$X)) {
      plot(out$MEgrid[,d], out$MEmean[,d], col=d, type="l", ylim=r,
           lwd=2, xlab=colnames(out$MEmean)[d], ylab="")
      lines(out$MEgrid[,d], out$MEq1[,d], col=d, lty=2)
      lines(out$MEgrid[,d], out$MEq2[,d], col=d, lty=2)
    }
  }
  
  ## Sobol indices
  par(mfrow=c(1,2), mar=c(5,4,4,2))
  boxplot(out$S, main="first order indices", xlab="inputs")
  boxplot(out$T, main="total indices", xlab="inputs")
  ## these look better when model="linear"
  
  ## clean up
  deletecloud(out)
  
  ## for a classification example using the sensitivity hooks
  ## in the dynaTrees function,  see the class2d demo
  ## i.e., demo("class2d")
  }
</code></pre>

<hr>
<h2 id='update.dynaTree'>
Updating a Dynamic Tree Model With New Data
</h2><span id='topic+update.dynaTree'></span>

<h3>Description</h3>

<p>Updating an already-initialized dynamic tree model with
new input/output pairs, primarily to facilitate sequential
design and optimization applications
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
update(object, X, y, verb = round(length(y)/10), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update.dynaTree_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
<tr><td><code id="update.dynaTree_+3A_x">X</code></td>
<td>

<p>an augmenting design <code>matrix</code> of real-valued predictors
with <code>ncol(X) = object$m</code>
</p>
</td></tr>
<tr><td><code id="update.dynaTree_+3A_y">y</code></td>
<td>

<p>an augmenting vector of real-valued responses or integer
categories with <code>length(y) = nrow(X)</code>
</p>
</td></tr>
<tr><td><code id="update.dynaTree_+3A_verb">verb</code></td>
<td>

<p>a positive scalar integer indicating how many time steps
(iterations) after which a progress statement should be
printed to the console; a value of <code>verb = 0</code> is quiet
</p>
</td></tr>
<tr><td><code id="update.dynaTree_+3A_...">...</code></td>
<td>

<p>to comply with the generic <code><a href="stats.html#topic+predict">predict</a></code> method &ndash;
currently unused
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function updates the <code><a href="#topic+dynaTree">dynaTree</a></code> fit with
new <code>(X,y)</code> pairs by the Particle Learning (PL)
algorithm.  The updated fit will be for data combined
as <code>rbind(object$X, X)</code> and <code>c(object$y, y)</code>.
</p>
<p>The primary use of this function is to facilitate sequential
design by optimization and active learning.  Typically one
would use <code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code> to estimate active
learning statistics at candidate location.
These are used to pick new <code>(X,y)</code>
locations to add to the design &ndash; the new fit being facilitated
by this function; see the examples below
</p>


<h3>Value</h3>

<p>The returned list is the same as <code><a href="#topic+dynaTree">dynaTree</a></code> &ndash;
i.e., a <code>"dynaTree"</code>-class object
</p>


<h3>Note</h3>

<p>The object (<code>object</code>) must contain a pointer to a particle
cloud (<code>object$num</code>) which has not been deleted by
<code><a href="#topic+deletecloud">deletecloud</a></code>.  In particular, it cannot be
an object returned from <code><a href="#topic+dynaTrees">dynaTrees</a></code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos
</p>


<h3>References</h3>

<p>Taddy, M.A., Gramacy, R.B., and Polson, N. (2011).
&ldquo;Dynamic trees for learning and design&rdquo;
Journal of the American Statistical Association, 106(493), pp. 109-123;
arXiv:0912.1586
</p>
<p>Anagnostopoulos, C., Gramacy, R.B. (2013) &ldquo;Information-Theoretic 
Data Discarding for Dynamic Trees on Data Streams.&rdquo; Entropy, 15(12), 
5510-5535; arXiv:1201.5568
</p>
<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing&rdquo;.
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.dynaTree">predict.dynaTree</a></code>, <code><a href="#topic+dynaTree">dynaTree</a></code>,
<code><a href="#topic+plot.dynaTree">plot.dynaTree</a></code>, <code><a href="#topic+deletecloud">deletecloud</a></code>,
<code><a href="#topic+getBF">getBF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple function describing (x,y) data
f1d &lt;- function(x, sd=0.1){
  return( sin(x) - dcauchy(x,1.6,0.15) + rnorm(1,0,sd))
} 

## initial (x,y) data
X &lt;- seq(0, 7, length=30)
y &lt;- f1d(X)

## PL fit to initial data
obj &lt;- dynaTree(X=X, y=y, N=1000, model="linear")

## a predictive grid
XX &lt;- seq(0,7, length=100)
obj &lt;- predict(obj, XX, quants=FALSE)

## follow the ALM algorithm and choose the next
## point with the highest predictive variance
m &lt;- which.max(obj$var)
xstar &lt;- drop(obj$XX[m,])
ystar &lt;- f1d(xstar)

## plot the next chosen point
par(mfrow=c(2,1))
plot(obj, ylab="y", xlab="x", main="fitted surface")
points(xstar, ystar, col=3, pch=20)
plot(obj$XX, sqrt(obj$var), type="l", xlab="x",
     ylab="predictive sd", main="active learning")

## update the fit with (xstar, ystar)
obj &lt;- update(obj, xstar, ystar)

## new predictive surface
obj &lt;- predict(obj, XX, quants=FALSE)

## plotted
plot(obj, ylab="y", xlab="x", main="updated fitted surface")
plot(obj$XX, sqrt(obj$var), type="l", xlab="x",
     ylab="predictive sd", main="active learning")

## delete the cloud to prevent a memory leak
deletecloud(obj); obj$num &lt;- NULL

## see demo("design") for more iterations and
## design under other active learning heuristics
## like ALC, and EI for optimization; also see
## demo("online") for an online learning example
</code></pre>

<hr>
<h2 id='varpropuse'>
Calculate the proportion of variables used in tree splits, and
average summary stats of tree heights and leaf sizes
</h2><span id='topic+varpropuse.dynaTree'></span><span id='topic+varproptotal.dynaTree'></span><span id='topic+treestats.dynaTree'></span>

<h3>Description</h3>

<p>Calculates the proportion of particles which use each input
to make a tree split and the proportion of all splits in trees
of each particle that correspond to each input variable; also
provides tree height and leaf size summary information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dynaTree'
varpropuse(object)
## S3 method for class 'dynaTree'
varproptotal(object)
## S3 method for class 'dynaTree'
treestats(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varpropuse_+3A_object">object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code><a href="#topic+dynaTree">dynaTree</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>varpropuse</code> gives the proportion of times a particle
uses each input variable in a tree split; <code>varproptotal</code> gives
the proportion of total uses by the tree in each particle (i.e.,
averaged over the total number of splits used in the tree).
</p>
<p>Usually, <code>varpropuse</code> returns a vector of (nearly) all ones
unless there are variables which are not useful in predicting
the response.  Using <code>model = "linear"</code> is not recommended
for this sort of variable selection.
</p>
<p><code>treestats</code> returns the average tree height, and the average
leaf size, both active and retired
</p>


<h3>Value</h3>

<p>For <code>varprop*</code>, a
vector of proportions of length <code>ncol(object$X))</code> is returned;
for <code>treestats</code> a 1-row, 4-column <code><a href="base.html#topic+data.frame">data.frame</a></code> is
returned
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br />
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
&ldquo;Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning&rdquo;
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+dynaTree">dynaTree</a></code>, <code><a href="#topic+sens.dynaTree">sens.dynaTree</a></code>,
<code><a href="#topic+relevance.dynaTree">relevance.dynaTree</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## ffit a dynaTree model to the Ozone data
X &lt;- airquality[,2:4]
y &lt;- airquality$Ozone
na &lt;- apply(is.na(X), 1, any) | is.na(y)
out &lt;- dynaTree(X=X[!na,], y=y[!na])

## obtain variable usage proportions
varpropuse(out)
varproptotal(out)

## gather relevance statistics which are more meaningful
out &lt;- relevance(out)
boxplot(out$relevance)
abline(h=0, col=2, lty=2)

## obtain tree statistics
treestats(out)

## clean up
deletecloud(out)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
