<!DOCTYPE html><html><head><title>Help for package pivmet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pivmet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#MUS'><p>MUS algorithm</p></a></li>
<li><a href='#piv_KMeans'><p>k-means Clustering Using Pivotal Algorithms For Seeding</p></a></li>
<li><a href='#piv_MCMC'><p>JAGS/Stan Sampling for Gaussian Mixture Models and Clustering via Co-Association Matrix.</p></a></li>
<li><a href='#piv_plot'><p>Plotting outputs from pivotal relabelling</p></a></li>
<li><a href='#piv_rel'><p>Performing the pivotal relabelling step and computing the relabelled posterior estimates</p></a></li>
<li><a href='#piv_sel'><p>Pivotal Selection via Co-Association Matrix</p></a></li>
<li><a href='#piv_sim'><p>Generate Data from a Gaussian Nested Mixture</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Pivotal Methods for Bayesian Relabelling and k-Means Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-05-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Leonardo Egidi[aut, cre], 
        Roberta Pappadà[aut], 
        Francesco Pauli[aut], 
        Nicola Torelli[aut] </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Leonardo Egidi &lt;legidi@units.it&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of pivotal algorithms 
             for: relabelling the MCMC chains in order to undo the label 
             switching problem in Bayesian mixture models;
             fitting sparse finite mixtures;
             initializing the centers of the classical k-means algorithm 
             in order to obtain a better clustering solution. 
             For further details see
             Egidi, Pappadà, Pauli and Torelli (2018b)&lt;ISBN:9788891910233&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/leoegidi/pivmet">https://github.com/leoegidi/pivmet</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>pandoc (&gt;= 1.12.3), pandoc-citeproc</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cluster, mclust, MASS, corpcor, runjags, rstan, bayesmix,
rjags, mvtnorm, bayesplot, scales</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>BuildManual:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-29 09:34:37 UTC; 17245</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-30 00:20:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='MUS'>MUS algorithm</h2><span id='topic+MUS'></span>

<h3>Description</h3>

<p>Perform Maxima Units Search (MUS) algorithm on a large and sparse matrix in
order to find a set of pivotal units through a sequential search
in the given matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MUS(C, clusters, prec_par = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MUS_+3A_c">C</code></td>
<td>
<p><code class="reqn">N \times N</code> matrix with a non-negligible number of zeros.
For instance, a similarity matrix estimated from a <code class="reqn">N \times D</code> data matrix whose rows
are statistical units, or a co-association matrix resulting from clustering
ensembles.</p>
</td></tr>
<tr><td><code id="MUS_+3A_clusters">clusters</code></td>
<td>
<p>A vector of integers from <code class="reqn">1:k</code>
indicating the cluster to which each point is allocated (it requires <code class="reqn">k &lt; 5</code>, see Details).</p>
</td></tr>
<tr><td><code id="MUS_+3A_prec_par">prec_par</code></td>
<td>
<p>Optional argument. The maximum number of candidate pivots for each group.
Default is 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consider <code class="reqn">H</code> distinct partitions of a set of <code class="reqn">N</code> <code class="reqn">d</code>-dimensional
statistical units into <code class="reqn">k</code>
groups determined by some
clustering technique.  A <code class="reqn">N \times N</code> co-association matrix
<code class="reqn">C</code> with generic element <code class="reqn">c_{i,j}=n_{i,j}/H</code> can be constructed,
where <code class="reqn">n_{i,j}</code> is the number of times the <code class="reqn">i</code>-th and the <code class="reqn">j</code>-th unit
are assigned to the same cluster with respect to the clustering ensemble.
Units which are very distant
from each other are likely to have zero co-occurrences; as a consequence,
<code class="reqn">C</code> is
a square symmetric matrix expected  to contain a non-negligible number of zeros.
The main task of the MUS algorithm is to detect submatrices of small
rank from the co-association matrix
and extract those units&mdash;pivots&mdash;such
that the <code class="reqn">k \times k</code> submatrix of <code class="reqn">C</code>,
determined by only the pivotal rows
and columns indexes, is identical or nearly identical.
Practically, the resulting units
have the desirable property to be representative of
the group they belong to.
</p>
<p>With the argument <code>prec_par</code> the user may increase
the powerful of the underlying MUS algorithm (see @egidi2018mus for details).
Given the default value 10, the function internally computes an
effective <code>prec_par</code> as <code class="reqn">\min( 10, \min n_j )</code>,
where <code class="reqn">n_j</code> is the number of units belonging to the group
<code class="reqn">j, \ j=1,\ldots,k</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pivots</code></td>
<td>
<p> A vector of integers in 1:N denoting the indeces of the <code>k</code> selcted pivotal units.</p>
</td></tr>
<tr><td><code>prec_par</code></td>
<td>
<p>The effective number of alternative pivots considered for each group. See Details.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>, Roberta Pappadà
</p>


<h3>References</h3>

<p>Egidi, L., Pappadà, R., Pauli, F., Torelli, N. (2018).
Maxima Units Search(MUS) algorithm:
methodology and applications. In: Perna, C. , Pratesi, M., Ruiz-Gazen A. (eds.) Studies in
Theoretical and Applied Statistics,
Springer Proceedings in Mathematics and Statistics 227, pp. 71–81.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data generated from a mixture of three bivariate Gaussian distributions

## Not run: 
N &lt;- 620
centers  &lt;- 3
n1 &lt;- 20
n2 &lt;- 100
n3 &lt;- 500
x  &lt;- matrix(NA, N,2)
truegroup &lt;- c( rep(1,n1), rep(2, n2), rep(3, n3))


 x[1:n1,]=rmvnorm(n1, c(1,5), sigma=diag(2))
 x[(n1+1):(n1+n2),]=rmvnorm(n2, c(4,0), sigma=diag(2))
 x[(n1+n2+1):(n1+n2+n3),]=rmvnorm(n3, c(6,6), sigma=diag(2))

# Build a similarity matrix from clustering ensembles

H &lt;- 1000
a &lt;- matrix(NA, H, N)

for (h in 1:H){
   a[h,] &lt;- kmeans(x,centers)$cluster
}

sim_matr &lt;- matrix(NA, N,N)
for (i in 1:(N-1)){
  for (j in (i+1):N){
     sim_matr[i,j] &lt;- sum(a[,i]==a[,j])/H
     sim_matr[j,i] &lt;- sim_matr[i,j]
     }
}

# Obtain a clustering solution via kmeans with multiple random seeds

cl &lt;- KMeans(x, centers)$cluster

# Find three pivots

mus_alg &lt;- MUS(C = sim_matr, clusters = cl)

## End(Not run)

</code></pre>

<hr>
<h2 id='piv_KMeans'>k-means Clustering Using Pivotal Algorithms For Seeding</h2><span id='topic+piv_KMeans'></span>

<h3>Description</h3>

<p>Perform classical k-means clustering on a data matrix using pivots as
initial centers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_KMeans(
  x,
  centers,
  alg.type = c("kmeans", "hclust"),
  method = "average",
  piv.criterion = c("MUS", "maxsumint", "minsumnoint", "maxsumdiff"),
  H = 1000,
  iter.max = 10,
  nstart = 10,
  prec_par = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_KMeans_+3A_x">x</code></td>
<td>
<p>A <code class="reqn">N \times D</code> data matrix, or an object that can be coerced to such a matrix (such as a numeric vector or a dataframe with all numeric columns).</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_centers">centers</code></td>
<td>
<p>The number of groups for the the <code class="reqn">k</code>-means solution.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_alg.type">alg.type</code></td>
<td>
<p>The clustering algorithm for the initial partition of the
<code class="reqn">N</code> units into the desired number of clusters.
Possible choices are <code>"kmeans"</code> (default) and <code>"hclust"</code>.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_method">method</code></td>
<td>
<p>If <code>alg.type</code> is <code>"hclust"</code>, the character string
defining the clustering method. The methods implemented are  <code>"single"</code>,
<code>"complete"</code>, <code>"average"</code>, <code>"ward.D"</code>, <code>"ward.D2"</code>, <code>"mcquitty"</code>,
<code>"median"</code>, <code>"centroid"</code>. The default is <code>"average"</code>.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_piv.criterion">piv.criterion</code></td>
<td>
<p>The pivotal criterion used for identifying one pivot
for each group. Possible choices are: <code>"MUS", "maxsumint", "minsumnoint",
"maxsumdiff"</code>.
If <code>centers &lt;= 4</code>, the default method is <code>"MUS"</code>;
otherwise, the default method is <code>"maxsumint"</code> (see the details and
the vignette).</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_h">H</code></td>
<td>
<p>The number of distinct <code class="reqn">k</code>-means runs used for building the <code class="reqn">N \times N</code> co-association matrix. Default is 10^3.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_iter.max">iter.max</code></td>
<td>
<p>If <code>alg.type</code> is <code>"kmeans"</code>, the maximum number of iterations to be passed to <code>kmeans()</code>. Default is 10.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_nstart">nstart</code></td>
<td>
<p>If <code>alg.type</code> is <code>"kmeans"</code>, the number of different starting random seeds to be passed to <code>kmeans()</code>. Default is 10.</p>
</td></tr>
<tr><td><code id="piv_KMeans_+3A_prec_par">prec_par</code></td>
<td>
<p>If <code>piv.criterion</code> is <code>"MUS"</code>, the maximum number of competing pivots in each group. If groups' sizes are less than the default value, which is 10,
then it is set equal to the cardinality of the smallest group in the initial partition.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function implements a modified version of k-means which aims at
improving the clustering solution starting from a careful seeding.
In particular, it performs a pivot-based initialization step
using pivotal methods to find the initial centers
for the clustering procedure. The starting point consists of multiple
runs of the classical k-means by selecting <code>nstart&gt;1</code> in the
<code>kmeans</code> function,
with a fixed number of clusters
in order to build the co-association matrix of data units.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A vector of integers indicating the cluster to which each point is allocated.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers (centroids).</p>
</td></tr>
<tr><td><code>coass</code></td>
<td>
<p>The co-association matrix built from ensemble clustering.</p>
</td></tr>
<tr><td><code>pivots</code></td>
<td>
<p>The pivotal units identified by the selected pivotal criterion.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>The within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>
<p>The within-cluster sum of squares summed across clusters.</p>
</td></tr>
<tr><td><code>betwennss</code></td>
<td>
<p>The between-cluster sum of squared distances.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p> The number of points in each cluster.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of (outer) iterations.</p>
</td></tr>
<tr><td><code>ifault</code></td>
<td>
<p>integer: indicator of a possible algorithm problem (for experts).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>, Roberta Pappada
</p>


<h3>References</h3>

<p>Egidi, L., Pappadà, R., Pauli, F., Torelli, N. (2018).
K-means seeding via MUS algorithm. Conference Paper,
Book of Short Papers, SIS2018, ISBN: 9788891910233.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data generated from a mixture of three bivariate Gaussian distributions

## Not run: 
N  &lt;- 620
k  &lt;- 3
n1 &lt;- 20
n2 &lt;- 100
n3 &lt;- 500
x  &lt;- matrix(NA, N,2)
truegroup &lt;- c( rep(1,n1), rep(2, n2), rep(3, n3))

 x[1:n1,] &lt;- rmvnorm(n1, c(1,5), sigma=diag(2))
 x[(n1+1):(n1+n2),] &lt;- rmvnorm(n2, c(4,0), sigma=diag(2))
 x[(n1+n2+1):(n1+n2+n3),] &lt;- rmvnorm(n3, c(6,6), sigma=diag(2))

# Apply piv_KMeans with MUS as pivotal criterion

res &lt;- piv_KMeans(x, k)

# Apply piv_KMeans with maxsumdiff as pivotal criterion

res2 &lt;- piv_KMeans(x, k, piv.criterion ="maxsumdiff")

# Plot the data and the clustering solution

par(mfrow=c(1,2), pty="s")
colors_cluster &lt;- c("grey", "darkolivegreen3", "coral")
colors_centers &lt;- c("black", "darkgreen", "firebrick")
graphics::plot(x, col = colors_cluster[truegroup],
   bg= colors_cluster[truegroup], pch=21, xlab="x[,1]",
   ylab="x[,2]", cex.lab=1.5,
   main="True data", cex.main=1.5)

graphics::plot(x, col = colors_cluster[res$cluster],
   bg=colors_cluster[res$cluster], pch=21, xlab="x[,1]",
   ylab="x[,2]", cex.lab=1.5,
   main="piv_KMeans", cex.main=1.5)
points(x[res$pivots, 1], x[res$pivots, 2],
      pch=24, col=colors_centers,bg=colors_centers,
      cex=1.5)
points(res$centers, col = colors_centers[1:k],
   pch = 8, cex = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='piv_MCMC'>JAGS/Stan Sampling for Gaussian Mixture Models and Clustering via Co-Association Matrix.</h2><span id='topic+piv_MCMC'></span>

<h3>Description</h3>

<p>Perform MCMC JAGS sampling or HMC Stan sampling for Gaussian mixture models, post-process the chains and apply a clustering technique to the MCMC sample. Pivotal units for each group are selected among four alternative criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_MCMC(
  y,
  k,
  nMC,
  priors,
  piv.criterion = c("MUS", "maxsumint", "minsumnoint", "maxsumdiff"),
  clustering = c("diana", "hclust"),
  software = c("rjags", "rstan"),
  burn = 0.5 * nMC,
  chains = 4,
  cores = 1,
  sparsity = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_MCMC_+3A_y">y</code></td>
<td>
<p><code class="reqn">N</code>-dimensional vector for univariate data or
<code class="reqn">N \times D</code> matrix for multivariate data.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_k">k</code></td>
<td>
<p>Number of mixture components.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_nmc">nMC</code></td>
<td>
<p>Number of MCMC iterations for the JAGS/Stan function execution.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_priors">priors</code></td>
<td>
<p>Input prior hyperparameters (see Details for default options).</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_piv.criterion">piv.criterion</code></td>
<td>
<p>The pivotal criterion used for identifying one pivot
for each group. Possible choices are: <code>"MUS", "maxsumint", "minsumnoint",
"maxsumdiff"</code>.
The default method is <code>"maxsumint"</code> (see the Details and
the vignette).</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_clustering">clustering</code></td>
<td>
<p>The algorithm adopted for partitioning the
<code class="reqn">N</code> observations into <code>k</code> groups. Possible choices are <code>"diana"</code> (default) or
<code>"hclust"</code> for divisive and agglomerative hierarchical clustering, respectively.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_software">software</code></td>
<td>
<p>The selected MCMC method to fit the model: <code>"rjags"</code> for the JAGS method, <code>"rstan"</code> for the Stan method.
Default is <code>"rjags"</code>.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_burn">burn</code></td>
<td>
<p>The burn-in period (only if method <code>"rjags"</code> is selected). Default is <code>0.5</code><code class="reqn">\times</code> <code>nMC</code>.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_chains">chains</code></td>
<td>
<p>A positive integer specifying the number of Markov chains. The default is 4.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use when executing the Markov chains in parallel (only if
<code>software="rstan"</code>). Default is 1.</p>
</td></tr>
<tr><td><code id="piv_MCMC_+3A_sparsity">sparsity</code></td>
<td>
<p>Allows for sparse finite mixtures, default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fits univariate and multivariate Bayesian Gaussian mixture models of the form
(here for univariate only):
</p>
<p style="text-align: center;"><code class="reqn">(Y_i|Z_i=j) \sim \mathcal{N}(\mu_j,\sigma_j),</code>
</p>

<p>where the <code class="reqn">Z_i</code>, <code class="reqn">i=1,\ldots,N</code>, are i.i.d. random variables, <code class="reqn">j=1,\dots,k</code>,
<code class="reqn">\sigma_j</code> is the group variance,  <code class="reqn">Z_i \in {1,\ldots,k }</code> are the
latent group allocation, and
</p>
<p style="text-align: center;"><code class="reqn">P(Z_i=j)=\eta_j.</code>
</p>

<p>The likelihood of the model is then
</p>
<p style="text-align: center;"><code class="reqn">L(y;\mu,\eta,\sigma) = \prod_{i=1}^N \sum_{j=1}^k \eta_j \mathcal{N}(\mu_j,\sigma_j),</code>
</p>

<p>where <code class="reqn">(\mu, \sigma)=(\mu_{1},\dots,\mu_{k},\sigma_{1},\ldots,\sigma_{k})</code>
are the component-specific parameters and <code class="reqn">\eta=(\eta_{1},\dots,\eta_{k})</code>
the mixture weights. Let <code class="reqn">\nu</code> denote a permutation of <code class="reqn">{ 1,\ldots,k }</code>,
and let <code class="reqn">\nu(\mu)= (\mu_{\nu(1)},\ldots,</code> <code class="reqn"> \mu_{\nu(k)})</code>,
<code class="reqn">\nu(\sigma)= (\sigma_{\nu(1)},\ldots,</code> <code class="reqn"> \sigma_{\nu(k)})</code>,
<code class="reqn"> \nu(\eta)=(\eta_{\nu(1)},\ldots,\eta_{\nu(k)})</code> be the
corresponding permutations of <code class="reqn">\mu</code>, <code class="reqn">\sigma</code> and <code class="reqn">\eta</code>.
Denote by <code class="reqn">V</code> the set of all the permutations of the indexes
<code class="reqn">{1,\ldots,k }</code>, the likelihood above is invariant under any
permutation <code class="reqn">\nu \in V</code>, that is
</p>
<p style="text-align: center;"><code class="reqn">
L(y;\mu,\eta,\sigma) = L(y;\nu(\mu),\nu(\eta),\nu(\sigma)).</code>
</p>

<p>As a consequence, the model is unidentified with respect to an
arbitrary permutation of the labels.
When Bayesian inference for the model is performed,
if the prior distribution <code class="reqn">p_0(\mu,\eta,\sigma)</code> is invariant under a permutation of the indices, then so is the posterior. That is, if <code class="reqn">p_0(\mu,\eta,\sigma) = p_0(\nu(\mu),\nu(\eta),\sigma)</code>, then
</p>
<p style="text-align: center;"><code class="reqn">
p(\mu,\eta,\sigma| y) \propto p_0(\mu,\eta,\sigma)L(y;\mu,\eta,\sigma)</code>
</p>

<p>is multimodal with (at least) <code class="reqn">k!</code> modes.
</p>
<p>Depending on the selected software, the model parametrization
changes in terms of the prior choices.
Precisely, the JAGS philosophy with the underlying Gibbs sampling
is to use noninformative priors, and conjugate priors are
preferred for computational speed.
Conversely, Stan adopts weakly informative priors,
with no need to explicitly use the conjugacy.
For univariate mixtures, when
<code>software="rjags"</code> the specification is the same as the function <code>BMMmodel</code> of the
<code>bayesmix</code> package:
</p>
<p style="text-align: center;"><code class="reqn">\mu_j \sim \mathcal{N}(\mu_0, 1/B0inv)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_j \sim \mbox{invGamma}(nu0Half, nu0S0Half)</code>
</p>

<p style="text-align: center;"><code class="reqn">\eta \sim \mbox{Dirichlet}(1,\ldots,1)</code>
</p>

<p style="text-align: center;"><code class="reqn">S0 \sim \mbox{Gamma}(g0Half, g0G0Half),</code>
</p>

<p>with default values: <code class="reqn">\mu_0=0, B0inv=0.1, nu0Half =10, S0=2,
 nu0S0Half= nu0Half\times S0,
 g0Half = 5e-17, g0G0Half = 5e-33</code>, in accordance with the default
specification:
</p>
<p><code>priors=list(kind = "independence", parameter = "priorsFish",
 hierarchical = "tau")</code>
</p>
<p>(see <code>bayesmix</code> for further details and choices).
</p>
<p>When <code>software="rstan"</code>, the prior specification is:
</p>
<p style="text-align: center;"><code class="reqn">\mu_j \sim \mathcal{N}(\mu_0, 1/B0inv)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_j \sim \mbox{Lognormal}(\mu_{\sigma}, \tau_{\sigma})</code>
</p>

<p style="text-align: center;"><code class="reqn">\eta_j \sim \mbox{Uniform}(0,1),</code>
</p>

<p>with default values: <code class="reqn">\mu_0=0, B0inv=0.1, \mu_{\sigma}=0, \tau_{\sigma}=2</code>.
The users may specify new hyperparameter values with the argument:
</p>
<p><code>priors=list(mu_0=1, B0inv=0.2, mu_sigma=3, tau_sigma=5)</code>
</p>
<p>For multivariate mixtures, when <code>software="rjags"</code> the prior specification is the following:
</p>
<p style="text-align: center;"><code class="reqn"> \bm{\mu}_j  \sim \mathcal{N}_D(\bm{\mu}_0, S2)</code>
</p>

<p style="text-align: center;"><code class="reqn"> \Sigma^{-1} \sim \mbox{Wishart}(S3, D+1)</code>
</p>

<p style="text-align: center;"><code class="reqn">\eta \sim \mbox{Dirichlet}(\bm{\alpha}),</code>
</p>

<p>where  <code class="reqn">\bm{\alpha}</code> is a <code class="reqn">k</code>-dimensional vector
and <code class="reqn">S_2</code> and <code class="reqn">S_3</code>
are positive definite matrices. By default, <code class="reqn">\bm{\mu}_0=\bm{0}</code>,
<code class="reqn">\bm{\alpha}=(1,\ldots,1)</code> and <code class="reqn">S_2</code> and <code class="reqn">S_3</code> are diagonal matrices,
with diagonal elements
equal to 1e+05. The user may specify other values for the hyperparameters
<code class="reqn">\bm{\mu}_0, S_2, S_3</code> and <code class="reqn">\bm{\alpha}</code> via <code>priors</code> argument in such a way:
</p>
<p><code>priors =list(mu_0 = c(1,1), S2 = ..., S3 = ..., alpha = ...)</code>
</p>
<p>with the constraint for <code class="reqn">S2</code> and <code class="reqn">S3</code> to be positive definite,
and <code class="reqn">\bm{\alpha}</code> a vector of dimension <code class="reqn">k</code> with nonnegative elements.
</p>
<p>When <code>software="rstan"</code>, the prior specification is:
</p>
<p style="text-align: center;"><code class="reqn"> \bm{\mu}_j  \sim \mathcal{N}_D(\bm{\mu}_0, LD*L^{T})</code>
</p>

<p style="text-align: center;"><code class="reqn">L \sim \mbox{LKJ}(\epsilon)</code>
</p>

<p style="text-align: center;"><code class="reqn">D^*_j \sim \mbox{HalfCauchy}(0, \sigma_d).</code>
</p>

<p>The covariance matrix is expressed in terms of the LDL decomposition as <code class="reqn">LD*L^{T}</code>,
a variant of the classical Cholesky decomposition, where <code class="reqn">L</code> is a <code class="reqn">D \times D</code>
lower unit triangular matrix and <code class="reqn">D*</code> is a <code class="reqn">D \times D</code> diagonal matrix.
The Cholesky correlation factor <code class="reqn">L</code> is assigned a LKJ prior with <code class="reqn">\epsilon</code> degrees of freedom,  which,
combined with priors on the standard deviations of each component, induces a prior on the covariance matrix;
as <code class="reqn">\epsilon \rightarrow \infty</code> the magnitude of correlations between components decreases,
whereas <code class="reqn">\epsilon=1</code> leads to a uniform prior distribution for <code class="reqn">L</code>.
By default, the hyperparameters are <code class="reqn">\bm{\mu}_0=\bm{0}</code>, <code class="reqn">\sigma_d=2.5, \epsilon=1</code>.
The user may propose some different values with the argument:
</p>
<p><code>priors=list(mu_0=c(1,2), sigma_d = 4, epsilon =2)</code>
</p>
<p>If <code>software="rjags"</code> the function performs JAGS sampling using the <code>bayesmix</code> package
for univariate Gaussian mixtures, and the <code>runjags</code>
package for multivariate Gaussian mixtures. If <code>software="rstan"</code> the function performs
Hamiltonian Monte Carlo (HMC) sampling via the <code>rstan</code> package (see the vignette and the Stan project
for any help).
</p>
<p>After MCMC sampling, this function
clusters the units in <code>k</code> groups,
calls the <code>piv_sel()</code> function and yields the
pivots obtained from one among four different
methods (the user may specify one among them via <code>piv.criterion</code>
argument):
<code>"maxsumint"</code>, <code>"minsumnoint"</code>, <code>"maxsumdiff"</code>
and <code>"MUS"</code> (available only if <code>k &lt;= 4</code>)
(see the vignette for thorough details). Due to computational reasons
clarified in the Details section of the function <code>piv_rel</code>, the
length of the MCMC chains will be minor or equal than the input
argument <code>nMC</code>; this length, corresponding to the value
<code>true.iter</code> returned by the procedure, is the number of
MCMC iterations for which
the number of JAGS/Stan groups exactly coincides with the prespecified
number of groups <code>k</code>.
</p>


<h3>Value</h3>

<p>The function gives the MCMC output, the clustering
solutions and the pivotal indexes. Here there is a complete list of outputs.
</p>
<table>
<tr><td><code>true.iter</code></td>
<td>
<p> The number of MCMC iterations for which
the number of JAGS/Stan groups exactly coincides with the prespecified
number of groups <code>k</code>.</p>
</td></tr>
<tr><td><code>Mu</code></td>
<td>
<p>An estimate of the groups' means.</p>
</td></tr>
<tr><td><code>groupPost</code></td>
<td>
 <p><code class="reqn">true.iter \times N</code> matrix
with values from <code>1:k</code> indicating the post-processed group allocation
vector.</p>
</td></tr>
<tr><td><code>mcmc_mean</code></td>
<td>
<p>  If <code>y</code> is a vector, a <code class="reqn">true.iter \times k</code>
matrix with the post-processed MCMC chains for the mean parameters; if
<code>y</code> is a matrix, a <code class="reqn">true.iter \times D \times k</code> array with
the post-processed MCMC chains for the mean parameters.</p>
</td></tr>
<tr><td><code>mcmc_sd</code></td>
<td>
<p>  If <code>y</code> is a vector, a <code class="reqn">true.iter \times k</code>
matrix with the post-processed MCMC chains for the sd parameters; if
<code>y</code> is a matrix, a <code class="reqn">true.iter \times D</code> array with
the post-processed MCMC chains for the sd parameters.</p>
</td></tr>
<tr><td><code>mcmc_weight</code></td>
<td>
<p>A <code class="reqn">true.iter \times k</code>
matrix with the post-processed MCMC chains for the weights parameters.</p>
</td></tr>
<tr><td><code>mcmc_mean_raw</code></td>
<td>
<p> If <code>y</code> is a vector, a <code class="reqn">(nMC-burn) \times k</code> matrix
with the raw MCMC chains for the mean parameters as given by JAGS; if
<code>y</code> is a matrix, a <code class="reqn">(nMC-burn) \times D \times k</code> array with the raw MCMC chains
for the mean parameters as given by JAGS/Stan.</p>
</td></tr>
<tr><td><code>mcmc_sd_raw</code></td>
<td>
<p> If <code>y</code> is a vector, a <code class="reqn">(nMC-burn) \times k</code> matrix
with the raw MCMC chains for the sd parameters as given by JAGS/Stan; if
<code>y</code> is a matrix, a <code class="reqn">(nMC-burn) \times D</code> array with the raw MCMC chains
for the sd parameters as given by JAGS/Stan.</p>
</td></tr>
<tr><td><code>mcmc_weight_raw</code></td>
<td>
<p>A <code class="reqn">(nMC-burn) \times k</code> matrix
with the raw MCMC chains for the weights parameters as given by JAGS/Stan.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The <code class="reqn">N \times N</code> co-association matrix constructed from the MCMC sample.</p>
</td></tr>
<tr><td><code>grr</code></td>
<td>
<p>The vector of cluster membership returned by
<code>"diana"</code> or <code>"hclust"</code>.</p>
</td></tr>
<tr><td><code>pivots</code></td>
<td>
<p>The vector of indices of pivotal units identified by the selected pivotal criterion.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>The JAGS/Stan model code. Apply the <code>"cat"</code> function for a nice visualization of the code.</p>
</td></tr>
<tr><td><code>stanfit</code></td>
<td>
<p>An object of S4 class <code>stanfit</code> for the fitted model (only if <code>software="rstan"</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>
</p>


<h3>References</h3>

<p>Egidi, L., Pappadà, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Bivariate simulation

## Not run: 
N   &lt;- 200
k   &lt;- 4
D   &lt;- 2
nMC &lt;- 1000
M1  &lt;- c(-.5,8)
M2  &lt;- c(25.5,.1)
M3  &lt;- c(49.5,8)
M4  &lt;- c(63.0,.1)
Mu  &lt;- rbind(M1,M2,M3,M4)
Sigma.p1 &lt;- diag(D)
Sigma.p2 &lt;- 20*diag(D)
W &lt;- c(0.2,0.8)
sim &lt;- piv_sim(N = N, k = k, Mu = Mu,
               Sigma.p1 = Sigma.p1,
               Sigma.p2 = Sigma.p2, W = W)

## rjags (default)
res &lt;- piv_MCMC(y = sim$y, k =k, nMC = nMC)

## rstan
res_stan &lt;- piv_MCMC(y = sim$y, k =k, nMC = nMC,
                     software ="rstan")

# changing priors
res2 &lt;- piv_MCMC(y = sim$y,
                 priors = list (
                 mu_0=c(1,1),
                 S2 = matrix(c(0.002,0,0, 0.1),2,2, byrow=TRUE),
                 S3 = matrix(c(0.1,0,0,0.1), 2,2, byrow =TRUE)),
                 k = k, nMC = nMC)

## End(Not run)


### Fishery data (bayesmix package)

## Not run: 
library(bayesmix)
data(fish)
y &lt;- fish[,1]
k &lt;- 5
nMC &lt;- 5000
res &lt;- piv_MCMC(y = y, k = k, nMC = nMC)

# changing priors
res2   &lt;- piv_MCMC(y = y,
                   priors = list(kind = "condconjugate",
                   parameter = "priorsRaftery",
                   hierarchical = "tau"),  k =k, nMC = nMC)

## End(Not run)
</code></pre>

<hr>
<h2 id='piv_plot'>Plotting outputs from pivotal relabelling</h2><span id='topic+piv_plot'></span>

<h3>Description</h3>

<p>Plot and visualize MCMC outputs and posterior relabelled chains/estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_plot(
  y,
  mcmc,
  rel_est,
  par = c("mean", "sd", "weight", "all"),
  type = c("chains", "hist")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_plot_+3A_y">y</code></td>
<td>
<p>Data vector or matrix.</p>
</td></tr>
<tr><td><code id="piv_plot_+3A_mcmc">mcmc</code></td>
<td>
<p>The ouptut of the raw MCMC sampling, as provided by <code>piv_MCMC</code>.</p>
</td></tr>
<tr><td><code id="piv_plot_+3A_rel_est">rel_est</code></td>
<td>
<p>Pivotal estimates as provided by <code>piv_rel</code>.</p>
</td></tr>
<tr><td><code id="piv_plot_+3A_par">par</code></td>
<td>
<p>The parameters for which estimates are displayed. Choose among: <code>"mean"</code>, <code>"sd"</code>, <code>"weight"</code> and <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="piv_plot_+3A_type">type</code></td>
<td>
<p>Type of plots required. Choose among: <code>"chains"</code>,  <code>"hist"</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fishery data
## Not run: 
library(bayesmix)
data(fish)
y &lt;- fish[,1]
N &lt;- length(y)
k &lt;- 5
nMC &lt;- 5000
res &lt;- piv_MCMC(y = y, k = k, nMC = nMC)
rel &lt;- piv_rel(mcmc=res, nMC = nMC)
piv_plot(y, res, rel, "chains")
piv_plot(y, res, rel, "estimates")
piv_plot(y, res, rel, "hist")

## End(Not run)

</code></pre>

<hr>
<h2 id='piv_rel'>Performing the pivotal relabelling step and computing the relabelled posterior estimates</h2><span id='topic+piv_rel'></span>

<h3>Description</h3>

<p>This function allows to perform the pivotal relabelling procedure described in Egidi et al. (2018) and to obtain the relabelled posterior estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_rel(mcmc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_rel_+3A_mcmc">mcmc</code></td>
<td>
<p>The output of the MCMC sampling from <code>piv_MCMC</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prototypical models in which the label switching problem arises
are mixture models, as explained in the Details section of
the <code>piv_MCMC</code> function.
</p>
<p>These models are unidentified with respect to an arbitrary permutation
of the labels <code class="reqn">1,...,k</code>. Relabelling means permuting
the labels at each iteration of the Markov chain in such
a way that the relabelled chain can be used to draw inferences
on component-specific parameters.
</p>
<p>We assume here that a MCMC sample is obtained for the
posterior distribution of a Gaussian mixture model&ndash;for instance via
<code>piv_MCMC</code> function&ndash;with a prior distribution which is
labelling invariant.
Furthermore, suppose that we can find <code class="reqn">k</code> units, one
for each group, which are (pairwise) separated with (posterior)
probability one
(that is, the posterior probability of any two of them being
in the same group
is zero).
It is then straightforward to use the <code class="reqn">k</code> units,
called pivots in what follows and denoted by the indexes
<code class="reqn">i_1,\ldots,i_k</code>, to identify the groups and to
relabel the chains:
for each MCMC iteration <code class="reqn">h=1,\ldots, H</code> (<code class="reqn">H</code> corresponds to
the argument <code>nMC</code>) and group
<code class="reqn">j=1,\ldots,k</code>, set
</p>
<p style="text-align: center;"><code class="reqn">
[\mu_j]_h=[\mu_{[Z_{i_{j}}]_h}]_h;
</code>
</p>

<p style="text-align: center;"><code class="reqn">
[Z_{i}]_h=j \mbox{ for } i:[Z_i]_h=[Z_{i_{j}}]_h.
</code>
</p>

<p>The applicability of this strategy is limited by the existence of the pivots,
which is not guaranteed. The existence of the pivots is a requirement of the
method, meaning that its use is restricted to those chains—or
those parts of a chain—for which the pivots are present. First, although the
model is based on a mixture of <code class="reqn">k</code> components, each iteration of the chain
may imply a different number of non-empty groups. Let then <code class="reqn">[k]_h \leq k</code>
be the number of non-empty groups at iteration <code class="reqn">h</code>,
</p>
<p style="text-align: center;"><code class="reqn">
 [k]_h = \#\{j: [Z_i]_h=j\mbox{ for some }i\},
</code>
</p>

<p>where <code class="reqn">\#A</code> is the cardinality of the set <code class="reqn">A</code>. Hence, the relabelling
procedure outlined above can be used only for the subset of the chain
for which <code class="reqn">[k]_h=k</code>; let it be </p>
<p style="text-align: center;"><code class="reqn">\mathcal{H}_k=\{h:[k]_h= k\},</code>
</p>

<p>which correspond to the argument <code>true.iter</code> given by <code>piv_MCMC</code>.
This means that the resulting relabelled chain is not a sample (of size <code class="reqn">H</code>)
from the posterior distribution, but a sample (of size <code class="reqn">\#\mathcal{H}_k</code>)
from the posterior
distribution conditional on there being (exactly) <code class="reqn">k</code> non-empty groups.
Even if <code class="reqn">k</code> non-empty groups are available, however,
there may not be <code class="reqn">k</code> perfectly separated units. Let us define
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{H}^{*}_k=\{ h\in\mathcal{H}_k : \exists r,s \mbox{ s.t. }
 [Z_{i_r}]_h=[Z_{i_s}]_h \}</code>
</p>

<p>that is, the set of iterations where (at least) two pivots are in the same
group.
In order for the pivot method to be applicable,
we need to exclude iterations <code class="reqn">\mathcal{H}^{*}_k</code>;
that is, we can perform the pivot relabelling on <code class="reqn">\mathcal{H}_k-
\mathcal{H}^{*}_{k}</code>, corresponding to the argument <code>final_it</code>.
</p>


<h3>Value</h3>

<p>This function gives the relabelled posterior estimates&ndash;both mean and medians&ndash;obtained from the Markov chains of the MCMC sampling.
</p>
<table>
<tr><td><code>final_it</code></td>
<td>
<p>The final number of valid MCMC iterations,
as explained in Details.</p>
</td></tr>
<tr><td><code>final_it_p</code></td>
<td>
<p>The proportion of final valid MCMC iterations.</p>
</td></tr>
<tr><td><code>rel_mean</code></td>
<td>
<p>The relabelled chains of the means: a <code>final_it</code> <code class="reqn">\times k</code> matrix for univariate data,
or a <code>final_it</code> <code class="reqn">\times D \times k</code> array for multivariate data.</p>
</td></tr>
<tr><td><code>rel_sd</code></td>
<td>
<p>The relabelled chains of the sd's: a <code>final_it</code> <code class="reqn">\times k</code> matrix for univariate data,
or a <code>final_it</code> <code class="reqn">\times D</code> matrix for multivariate data.</p>
</td></tr>
<tr><td><code>rel_weight</code></td>
<td>
<p>The relabelled chains of the weights: a <code>final_it</code> <code class="reqn">\times k</code> matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>
</p>


<h3>References</h3>

<p>Egidi, L., Pappadà, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Univariate simulation
## Not run: 
N   &lt;- 250
nMC &lt;- 2500
k   &lt;- 3
p   &lt;- rep(1/k,k)
x   &lt;- 3
stdev &lt;- cbind(rep(1,k), rep(20,k))
Mu    &lt;- seq(-trunc(k/2)*x,trunc(k/2)*x,length=k)
W     &lt;- c(0.2,0.8)
sim   &lt;- piv_sim(N = N, k = k, Mu = Mu,
                 stdev = stdev, W=W)
res   &lt;- piv_MCMC(y = sim$y, k =k, nMC = nMC)
rel   &lt;- piv_rel(mcmc=res)

## End(Not run)

#Bivariate simulation
## Not run: 
N &lt;- 200
k &lt;- 3
D &lt;- 2
nMC &lt;- 5000
M1  &lt;- c(-.5,8)
M2  &lt;- c(25.5,.1)
M3  &lt;- c(49.5,8)
Mu  &lt;- matrix(rbind(M1,M2,M3),c(k,2))
Sigma.p1 &lt;- diag(D)
Sigma.p2 &lt;- 20*diag(D)
W &lt;- c(0.2,0.8)
sim &lt;- piv_sim(N = N, k = k, Mu = Mu,
               Sigma.p1 = Sigma.p1,
               Sigma.p2 = Sigma.p2, W = W)
res &lt;- piv_MCMC(y = sim$y, k = k, nMC = nMC)
rel &lt;- piv_rel(mcmc = res)
piv_plot(y=sim$y, mcmc=res, rel_est = rel, type="chains")
piv_plot(y=sim$y, mcmc=res, rel_est = rel,
         type="hist")

## End(Not run)

</code></pre>

<hr>
<h2 id='piv_sel'>Pivotal Selection via Co-Association Matrix</h2><span id='topic+piv_sel'></span>

<h3>Description</h3>

<p>Finding pivotal units from a data partition and a
co-association matrix C
according to three different methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_sel(C, clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_sel_+3A_c">C</code></td>
<td>
<p>A <code class="reqn">N \times N</code> co-association matrix, i.e.
a matrix whose elements are co-occurrences of pair of units
in the same cluster among <code class="reqn">H</code> distinct partitions.</p>
</td></tr>
<tr><td><code id="piv_sel_+3A_clusters">clusters</code></td>
<td>
<p>A vector of integers from <code class="reqn">1:k</code> indicating
a partition of the <code class="reqn">N</code> units into, say, <code class="reqn">k</code> groups.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a set of <code class="reqn">N</code> observations <code class="reqn">(y_{1},y_{2},...,y_{N})</code>
(<code class="reqn">y_i</code> may be a <code class="reqn">d</code>-dimensional vector, <code class="reqn">d \ge 1</code>),
consider clustering methods to obtain <code class="reqn">H</code> distinct partitions
into <code class="reqn">k</code> groups.
The matrix <code>C</code> is the co-association matrix,
where <code class="reqn">c_{i,p}=n_{i,p}/H</code>, with <code class="reqn">n_{i,p}</code> the number of times
the pair <code class="reqn">(y_{i},y_{p})</code> is assigned to the same
cluster among the <code class="reqn">H</code> partitions.
</p>
<p>Let <code class="reqn">j</code> be the group containing units <code class="reqn">\mathcal J_j</code>,
the user may choose <code class="reqn">{i^*}\in\mathcal J_j</code> that
maximizes one of the quantities:
</p>
<p style="text-align: center;"><code class="reqn">
 \sum_{p\in\mathcal J_j} c_{{i^*}p}</code>
</p>

<p>or
</p>
<p style="text-align: center;"><code class="reqn">\sum_{p\in\mathcal J_j} c_{{i^*}p} - \sum_{j\not\in\mathcal J_j} c_{{i^*}p}.
</code>
</p>

<p>These methods give the unit that maximizes the global
within similarity (<code>"maxsumint"</code>) and the unit that
maximizes the difference between global within and
between similarities (<code>"maxsumdiff"</code>), respectively.
Alternatively, we may choose <code class="reqn">i^{*} \in\mathcal J_j</code>, which minimizes:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{p\not\in\mathcal J_j} c_{i^{*}p},</code>
</p>

<p>obtaining the most distant unit among the members
that minimize the global dissimilarity between one group
and all the others (<code>"minsumnoint"</code>).
See the vignette for further details.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pivots</code></td>
<td>
<p>A matrix with <code class="reqn">k</code> rows and three
columns containing the indexes of the pivotal units for each method.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Leonardo Egidi <a href="mailto:legidi@units.it">legidi@units.it</a>
</p>


<h3>References</h3>

<p>Egidi, L., Pappadà, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data

data(iris)
# select the columns of variables
x&lt;- iris[,1:4]
N &lt;- nrow(x)
H &lt;- 1000
a &lt;- matrix(NA, H, N)

# Perform H k-means partitions

for (h in 1:H){
 a[h,] &lt;- kmeans(x, centers = 3)$cluster
}
# Build the co-association matrix

C &lt;- matrix(NA, N,N)
for (i in 1:(N-1)){
 for (j in (i+1):N){
   C[i,j] &lt;- sum(a[,i]==a[,j])/H
   C[j,i] &lt;- C[i,j]
 }}

km &lt;- kmeans(x, centers =3)

# Apply three pivotal criteria to the co-association matrix

ris &lt;- piv_sel(C, clusters = km$cluster)

graphics::plot(iris[,1], iris[,2], xlab ="Sepal.Length", ylab= "Sepal.Width",
col = km$cluster)

 # Add the pivots chosen by the maxsumdiff criterion

points( x[ris$pivots[,3], 1:2], col = 1:3,
cex =2, pch = 8 )

</code></pre>

<hr>
<h2 id='piv_sim'>Generate Data from a Gaussian Nested Mixture</h2><span id='topic+piv_sim'></span>

<h3>Description</h3>

<p>Simulate <code class="reqn">N</code> observations from a nested Gaussian mixture model
with <code class="reqn">k</code> pre-specified components under uniform group probabilities <code class="reqn">1/k</code>,
where each group is in turn
drawn from a further level consisting of two subgroups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piv_sim(
  N,
  k,
  Mu,
  stdev,
  Sigma.p1 = diag(2),
  Sigma.p2 = 100 * diag(2),
  W = c(0.5, 0.5)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piv_sim_+3A_n">N</code></td>
<td>
<p>The desired sample size.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_k">k</code></td>
<td>
<p>The desired number of mixture components.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_mu">Mu</code></td>
<td>
<p>The input mean vector of length <code class="reqn">k</code> for univariate
Gaussian mixtures; the input <code class="reqn">k \times D</code> matrix with the
means' coordinates for multivariate Gaussian mixtures.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_stdev">stdev</code></td>
<td>
<p>For univariate mixtures, the  <code class="reqn">k \times 2</code> matrix
of input standard deviations,
where the first column contains the parameters for subgroup 1,
and the second column contains the parameters for subgroup 2.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_sigma.p1">Sigma.p1</code></td>
<td>
<p>The <code class="reqn">D \times D</code> covariance matrix for the first subgroup. For multivariate mixtures only.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_sigma.p2">Sigma.p2</code></td>
<td>
<p>The <code class="reqn">D \times D</code> covariance matrix for the second subgroup. For multivariate mixtures only.</p>
</td></tr>
<tr><td><code id="piv_sim_+3A_w">W</code></td>
<td>
<p>The vector for the mixture weights of the two subgroups.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions allows to simulate values from a double (nested) univariate
Gaussian mixture:
</p>
<p style="text-align: center;"><code class="reqn">
(Y_i|Z_i=j) \sim \sum_{s=1}^{2} p_{js}\, \mathcal{N}(\mu_{j}, \sigma^{2}_{js}),
</code>
</p>

<p>or from a multivariate nested Gaussian mixture:
</p>
<p style="text-align: center;"><code class="reqn">
(Y_i|Z_i=j) \sim \sum_{s=1}^{2} p_{js}\, \mathcal{N}_{D}(\bm{\mu}_{j}, \Sigma_{s}),
</code>
</p>

<p>where <code class="reqn">\sigma^{2}_{js}</code> is the variance for the group <code class="reqn">j</code> and
the subgroup <code class="reqn">s</code> (<code>stdev</code> is the
argument for specifying the <code>k x 2</code> standard deviations
for univariate mixtures);
<code class="reqn">\Sigma_s</code> is the covariance matrix for the
subgroup <code class="reqn">s, s=1,2</code>, where the two matrices are
specified by <code>Sigma.p1</code>
and <code>Sigma.p2</code> respectively; <code class="reqn">\mu_j</code> and
<code class="reqn">\bm{\mu}_j, \ j=1,\ldots,k</code>
are the mean input vector and matrix respectively,
specified by the argument <code>Mu</code>;
<code>W</code> is a vector of dimension 2 for the subgroups weights.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y</code></td>
<td>
<p>The <code class="reqn">N</code> simulated observations.</p>
</td></tr>
<tr><td><code>true.group</code></td>
<td>
<p>A vector of integers from <code class="reqn">1:k</code>
indicating the values of the latent variables <code class="reqn">Z_i</code>.</p>
</td></tr>
<tr><td><code>subgroups</code></td>
<td>
<p>A <code class="reqn">k \times N</code> matrix where
each row contains the index subgroup for the observations
in the <code class="reqn">k</code>-th group.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# Bivariate mixture simulation with three components

N  &lt;- 2000
k  &lt;- 3
D &lt;- 2
M1 &lt;- c(-45,8)
M2 &lt;- c(45,.1)
M3 &lt;- c(100,8)
Mu &lt;- rbind(M1,M2,M3)
Sigma.p1 &lt;- diag(D)
Sigma.p2 &lt;- 20*diag(D)
W   &lt;- c(0.2,0.8)
sim &lt;- piv_sim(N = N, k = k, Mu = Mu, Sigma.p1 = Sigma.p1,
Sigma.p2 = Sigma.p2, W = W)
graphics::plot(sim$y, xlab="y[,1]", ylab="y[,2]")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
