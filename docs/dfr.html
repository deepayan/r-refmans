<!DOCTYPE html><html lang="en"><head><title>Help for package dfr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dfr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dfr-package'><p>dfr: Dual Feature Reduction for SGL</p></a></li>
<li><a href='#dfr_adap_sgl'><p>Fit a DFR-aSGL model.</p></a></li>
<li><a href='#dfr_adap_sgl.cv'><p>Fit a DFR-aSGL model using k-fold cross-validation.</p></a></li>
<li><a href='#dfr_sgl'><p>Fit a DFR-SGL model.</p></a></li>
<li><a href='#dfr_sgl.cv'><p>Fit a DFR-SGL model using k-fold cross-validation.</p></a></li>
<li><a href='#plot.sgl'><p>Plot models of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</p></a></li>
<li><a href='#predict.sgl'><p>Predict using one of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</p></a></li>
<li><a href='#print.sgl'><p>Prints information for one of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Dual Feature Reduction for SGL</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-03-06</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fabio Feser &lt;ff120@ic.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of the Dual Feature Reduction (DFR) approach for the Sparse Group Lasso (SGL) and the Adaptive Sparse Group Lasso (aSGL) (Feser and Evangelou (2024) &lt;<a href="https://doi.org/10.48550%2FarXiv.2405.17094">doi:10.48550/arXiv.2405.17094</a>&gt;). The DFR approach is a feature reduction approach that applies strong screening to reduce the feature space before optimisation, leading to speed-up improvements for fitting SGL (Simon et al. (2013) &lt;<a href="https://doi.org/10.1080%2F10618600.2012.681250">doi:10.1080/10618600.2012.681250</a>&gt;) and aSGL (Mendez-Civieta et al. (2020) &lt;<a href="https://doi.org/10.1007%2Fs11634-020-00413-8">doi:10.1007/s11634-020-00413-8</a>&gt; and Poignard (2020) &lt;<a href="https://doi.org/10.1007%2Fs10463-018-0692-7">doi:10.1007/s10463-018-0692-7</a>&gt;) models. DFR is implemented using the Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018) &lt;<a href="https://doi.org/10.48550%2FarXiv.1804.02339">doi:10.48550/arXiv.1804.02339</a>&gt;) algorithm, with linear and logistic SGL models supported, both of which can be fit using k-fold cross-validation. Dense and sparse input matrices are supported.</td>
</tr>
<tr>
<td>Imports:</td>
<td>sgs, caret, MASS, methods, stats, grDevices, graphics, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>SGL, gglasso, glmnet, testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ff1201/dfr">https://github.com/ff1201/dfr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ff1201/dfr/issues">https://github.com/ff1201/dfr/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-06 17:47:16 UTC; ff120</td>
</tr>
<tr>
<td>Author:</td>
<td>Fabio Feser <a href="https://orcid.org/0009-0007-3088-9727"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-06 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='dfr-package'>dfr: Dual Feature Reduction for SGL</h2><span id='topic+dfr'></span><span id='topic+dfr-package'></span>

<h3>Description</h3>

<p>Implementation of the Dual Feature Reduction (DFR) approach for the Sparse Group Lasso (SGL) and the Adaptive Sparse Group Lasso (aSGL) (Feser and Evangelou (2024) <a href="https://doi.org/10.48550/arXiv.2405.17094">doi:10.48550/arXiv.2405.17094</a>). The DFR approach is a feature reduction approach that applies strong screening to reduce the feature space before optimisation, leading to speed-up improvements for fitting SGL (Simon et al. (2013) <a href="https://doi.org/10.1080/10618600.2012.681250">doi:10.1080/10618600.2012.681250</a>) and aSGL (Mendez-Civieta et al. (2020) <a href="https://doi.org/10.1007/s11634-020-00413-8">doi:10.1007/s11634-020-00413-8</a> and Poignard (2020) <a href="https://doi.org/10.1007/s10463-018-0692-7">doi:10.1007/s10463-018-0692-7</a>) models. DFR is implemented using the Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018) <a href="https://doi.org/10.48550/arXiv.1804.02339">doi:10.48550/arXiv.1804.02339</a>) algorithm, with linear and logistic SGL models supported, both of which can be fit using k-fold cross-validation. Dense and sparse input matrices are supported.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Fabio Feser <a href="mailto:ff120@ic.ac.uk">ff120@ic.ac.uk</a> (<a href="https://orcid.org/0009-0007-3088-9727">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/ff1201/dfr">https://github.com/ff1201/dfr</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ff1201/dfr/issues">https://github.com/ff1201/dfr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='dfr_adap_sgl'>Fit a DFR-aSGL model.</h2><span id='topic+dfr_adap_sgl'></span>

<h3>Description</h3>

<p>Adaptive Sparse-group lasso (aSGL) with DFR main fitting function. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfr_adap_sgl(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  alpha = 0.95,
  gamma_1 = 0.1,
  gamma_2 = 0.1,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  standardise = "l2",
  intercept = TRUE,
  path_length = 20,
  min_frac = 0.05,
  screen = TRUE,
  verbose = FALSE,
  v_weights = NULL,
  w_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dfr_adap_sgl_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_lambda">lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li><p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between the lasso and group lasso. Must be between 0 and 1. Recommended value is 0.95.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_gamma_1">gamma_1</code></td>
<td>
<p>Hyperparameter which determines the shape of the variable penalties.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_gamma_2">gamma_2</code></td>
<td>
<p>Hyperparameter which determines the shape of the group penalties.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/\sqrt{n}</code>.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/n</code>.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_path_length">path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_min_frac">min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_screen">screen</code></td>
<td>
<p>Logical flag for whether to apply the DFR screening rules (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_v_weights">v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
<tr><td><code id="dfr_adap_sgl_+3A_w_weights">w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dfr_adap_sgl()</code> fits a DFR-aSGL model (Feser and Evangelou (2024)) using Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018)).
It solves the convex optimisation problem given by (Poignard (2020) and Mendez-Civieta et al. (2020))
</p>
<p style="text-align: center;"><code class="reqn">
  \frac{1}{2n} f(b ; y, \mathbf{X}) + \lambda \alpha \sum_{i=1}^{p}v_i |b_i| + \lambda (1-\alpha)\sum_{g=1}^{m} w_g \sqrt{p_g} \|b^{(g)}\|_2,
</code>
</p>

<p>where <code class="reqn">f(\cdot)</code> is the loss function, <code class="reqn">p_g</code> are the group sizes, and <code class="reqn">(v,w)</code> are adaptive weights. In the case of the linear model, the loss function is given by the mean-squared error loss:
</p>
<p style="text-align: center;"><code class="reqn">
 f(b; y, \mathbf{X}) = \left\|y-\mathbf{X}b \right\|_2^2.
</code>
</p>

<p>In the logistic model, the loss function is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(b;y,\mathbf{X})=-1/n \log(\mathcal{L}(b; y, \mathbf{X})).
</code>
</p>

<p>where the log-likelihood is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{L}(b; y, \mathbf{X}) = \sum_{i=1}^{n}\left\{y_i b^\intercal x_i - \log(1+\exp(b^\intercal x_i)) \right\}.
</code>
</p>

<p>The adaptive weights are chosen as, for a group <code class="reqn">g</code> and variable <code class="reqn">i</code> (Mendez-Civieta et al. (2020))
</p>
<p style="text-align: center;"><code class="reqn">
   v_i = \frac{1}{|q_{1i}|^{\gamma_1}}, \; w_g = \frac{1}{\|q_1^{(g)}\|_2^{\gamma_2}},
</code>
</p>

<p>DFR uses the dual norm (the <code class="reqn">\epsilon</code>-norm) and the KKT conditions to discard features at <code class="reqn">\lambda_k</code> that would have been inactive at <code class="reqn">\lambda_{k+1}</code>.
It applies two layers of screening, so that it first screens out any groups that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
\|\nabla_g f(\hat{\beta}(\lambda_{k}))\|_{\epsilon_g'} \leq \gamma_g(2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>and then screens out any variables that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
|\nabla_i f(\hat{\beta}(\lambda_{k}))| \leq \alpha v_i (2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>leading to effective input dimensionality reduction. See Feser and Evangelou (2024) for full details.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>z</code>, which is usually the former. A filter is applied to remove very small values, where ATOS has not been able to shrink exactly to zero. Check this against <code>x</code> and <code>z</code>.</p>
</td></tr>
<tr><td><code>group_effects</code></td>
<td>
<p>The group values from the regression. Taken by applying the <code class="reqn">\ell_2</code> norm within each group on <code>beta</code>.</p>
</td></tr>
<tr><td><code>selected_var</code></td>
<td>
<p>A list containing the indicies of the active/selected variables for each <code>"lambda"</code> value. Index 1 corresponds to the first column in X.</p>
</td></tr>
<tr><td><code>selected_grp</code></td>
<td>
<p>A list containing the indicies of the active/selected groups for each <code>"lambda"</code> value. Index 1 corresponds to the first group in the <code>groups</code> vector. You can see the group order by running <code>unique(groups)</code>.</p>
</td></tr>
<tr><td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td></tr>
<tr><td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td></tr>
<tr><td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>screen_set_var</code></td>
<td>
<p>List of variables that were kept after screening step for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>screen_set_grp</code></td>
<td>
<p>List of groups that were kept after screening step for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>epsilon_set_var</code></td>
<td>
<p>List of variables that were used for fitting after screening for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>epsilon_set_grp</code></td>
<td>
<p>List of groups that were used for fitting after screening for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>kkt_violations_var</code></td>
<td>
<p>List of variables that violated the KKT conditions each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>kkt_violations_grp</code></td>
<td>
<p>List of groups that violated the KKT conditions each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>v_weights</code></td>
<td>
<p>Vector of the variable penalty sequence.</p>
</td></tr>
<tr><td><code>w_weights</code></td>
<td>
<p>Vector of the group penalty sequence.</p>
</td></tr>
<tr><td><code>screen</code></td>
<td>
<p>Logical flag indicating whether screening was performed.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value(s) of <code class="reqn">\lambda</code> used to fit the model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Feser, F., Evangelou, M. (2024). <em>Dual feature reduction for the sparse-group lasso and its adaptive variant</em>, <a href="https://arxiv.org/abs/2405.17094">https://arxiv.org/abs/2405.17094</a>
</p>
<p>Mendez-Civieta, A., Carmen Aguilera-Morillo, M., Lillo, R. (2020). <em>Adaptive sparse group LASSO in quantile regression</em>, <a href="https://doi.org/10.1007/s11634-020-00413-8">doi:10.1007/s11634-020-00413-8</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>
<p>Poignard, B. (2020). <em>Asymptotic theory of the adaptive Sparse Group Lasso</em>, <a href="https://doi.org/10.1007/s10463-018-0692-7">doi:10.1007/s10463-018-0692-7</a>
</p>


<h3>See Also</h3>

<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-aSGL 
model = dfr_adap_sgl(X = data$X, y = data$y, groups = groups, type="linear", path_length = 5, 
alpha=0.95, standardise = "l2", intercept = TRUE, verbose=FALSE)
</code></pre>

<hr>
<h2 id='dfr_adap_sgl.cv'>Fit a DFR-aSGL model using k-fold cross-validation.</h2><span id='topic+dfr_adap_sgl.cv'></span>

<h3>Description</h3>

<p>Function to fit a pathwise solution of the adaptive sparse-group lasso (aSGL) applied with DFR using k-fold cross-validation. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfr_adap_sgl.cv(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  path_length = 20,
  nfolds = 10,
  alpha = 0.95,
  gamma_1 = 0.1,
  gamma_2 = 0.1,
  backtracking = 0.7,
  max_iter = 5000,
  max_iter_backtracking = 100,
  tol = 1e-05,
  min_frac = 0.05,
  standardise = "l2",
  intercept = TRUE,
  error_criteria = "mse",
  screen = TRUE,
  verbose = FALSE,
  v_weights = NULL,
  w_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dfr_adap_sgl.cv_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_lambda">lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li><p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_path_length">path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to use in cross-validation.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between the lasso and group lasso. Must be between 0 and 1. Recommended value is 0.95.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_gamma_1">gamma_1</code></td>
<td>
<p>Hyperparameter which determines the shape of the variable penalties.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_gamma_2">gamma_2</code></td>
<td>
<p>Hyperparameter which determines the shape of the group penalties.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_min_frac">min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_error_criteria">error_criteria</code></td>
<td>
<p>The criteria used to discriminate between models along the path. Supported values are: <code>"mse"</code> (mean squared error) and <code>"mae"</code> (mean absolute error).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_screen">screen</code></td>
<td>
<p>Logical flag for whether to apply the DFR screening rules (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_v_weights">v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
<tr><td><code id="dfr_adap_sgl.cv_+3A_w_weights">w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits DFR-aSGL models under a pathwise solution using Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018)), picking the 1se model as optimum. Warm starts are implemented.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>all_models</code></td>
<td>
<p>A list of all the models fitted along the path.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The 1se chosen model, which is a <code>"sgl"</code> object type.</p>
</td></tr>
<tr><td><code>best_lambda</code></td>
<td>
<p>The value of <code class="reqn">\lambda</code> which generated the chosen model.</p>
</td></tr>
<tr><td><code>best_lambda_id</code></td>
<td>
<p>The path index for the chosen model.</p>
</td></tr>
<tr><td><code>errors</code></td>
<td>
<p>A table containing fitting information about the models on the path.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Feser, F., Evangelou, M. (2024). <em>Dual feature reduction for the sparse-group lasso and its adaptive variant</em>, <a href="https://arxiv.org/abs/2405.17094">https://arxiv.org/abs/2405.17094</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>
</p>
<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-SGL with cross-validation
cv_model = dfr_adap_sgl.cv(X = data$X, y = data$y, groups=groups, type = "linear", 
path_length = 5, nfolds=5, alpha = 0.95, min_frac = 0.05, 
standardise="l2",intercept=TRUE,verbose=TRUE)
</code></pre>

<hr>
<h2 id='dfr_sgl'>Fit a DFR-SGL model.</h2><span id='topic+dfr_sgl'></span>

<h3>Description</h3>

<p>Sparse-group lasso (SGL) with DFR main fitting function. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfr_sgl(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  alpha = 0.95,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  standardise = "l2",
  intercept = TRUE,
  path_length = 20,
  min_frac = 0.05,
  screen = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dfr_sgl_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_lambda">lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li><p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between the lasso and group lasso. Must be between 0 and 1. Recommended value is 0.95.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/\sqrt{n}</code>.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/n</code>.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_path_length">path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_min_frac">min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_screen">screen</code></td>
<td>
<p>Logical flag for whether to apply the DFR screening rules (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code id="dfr_sgl_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dfr_sgl()</code> fits a DFR-SGL model (Feser and Evangelou (2024)) using Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018)).
It solves the convex optimisation problem given by (Simon et al. (2013))
</p>
<p style="text-align: center;"><code class="reqn">
  \frac{1}{2n} f(b ; y, \mathbf{X}) + \lambda \alpha \sum_{i=1}^{p} |b_i| + \lambda (1-\alpha)\sum_{g=1}^{m}  \sqrt{p_g} \|b^{(g)}\|_2,
</code>
</p>

<p>where <code class="reqn">f(\cdot)</code> is the loss function and <code class="reqn">p_g</code> are the group sizes. In the case of the linear model, the loss function is given by the mean-squared error loss:
</p>
<p style="text-align: center;"><code class="reqn">
 f(b; y, \mathbf{X}) = \left\|y-\mathbf{X}b \right\|_2^2.
</code>
</p>

<p>In the logistic model, the loss function is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(b;y,\mathbf{X})=-1/n \log(\mathcal{L}(b; y, \mathbf{X})).
</code>
</p>

<p>where the log-likelihood is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{L}(b; y, \mathbf{X}) = \sum_{i=1}^{n}\left\{y_i b^\intercal x_i - \log(1+\exp(b^\intercal x_i)) \right\}.
</code>
</p>

<p>SGL can be seen to be a convex combination of the lasso and group lasso, balanced through <code>alpha</code>, such that it reduces to the lasso for <code>alpha = 1</code> and to the group lasso for <code>alpha = 0</code>.
By applying both the lasso and group lasso norms, SGL shrinks inactive groups to zero, as well as inactive variables in active groups.
DFR uses the dual norm (the <code class="reqn">\epsilon</code>-norm) and the KKT conditions to discard features at <code class="reqn">\lambda_k</code> that would have been inactive at <code class="reqn">\lambda_{k+1}</code>.
It applies two layers of screening, so that it first screens out any groups that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
\|\nabla_g f(\hat{\beta}(\lambda_{k}))\|_{\epsilon_g} \leq \tau_g(2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>and then screens out any variables that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
|\nabla_i f(\hat{\beta}(\lambda_{k}))| \leq \alpha (2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>leading to effective input dimensionality reduction. See Feser and Evangelou (2024) for full details.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>z</code>, which is usually the former. A filter is applied to remove very small values, where ATOS has not been able to shrink exactly to zero. Check this against <code>x</code> and <code>z</code>.</p>
</td></tr>
<tr><td><code>group_effects</code></td>
<td>
<p>The group values from the regression. Taken by applying the <code class="reqn">\ell_2</code> norm within each group on <code>beta</code>.</p>
</td></tr>
<tr><td><code>selected_var</code></td>
<td>
<p>A list containing the indicies of the active/selected variables for each <code>"lambda"</code> value. Index 1 corresponds to the first column in X.</p>
</td></tr>
<tr><td><code>selected_grp</code></td>
<td>
<p>A list containing the indicies of the active/selected groups for each <code>"lambda"</code> value. Index 1 corresponds to the first group entry in the <code>groups</code> vector. You can see the group order by running <code>unique(groups)</code>.</p>
</td></tr>
<tr><td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td></tr>
<tr><td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td></tr>
<tr><td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa and Gidel (2018)).</p>
</td></tr>
<tr><td><code>screen_set_var</code></td>
<td>
<p>List of variables that were kept after screening step for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>screen_set_grp</code></td>
<td>
<p>List of groups that were kept after screening step for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>epsilon_set_var</code></td>
<td>
<p>List of variables that were used for fitting after screening for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>epsilon_set_grp</code></td>
<td>
<p>List of groups that were used for fitting after screening for each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>kkt_violations_var</code></td>
<td>
<p>List of variables that violated the KKT conditions each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>kkt_violations_grp</code></td>
<td>
<p>List of groups that violated the KKT conditions each <code>"lambda"</code> value. (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code>screen</code></td>
<td>
<p>Logical flag indicating whether screening was performed.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value(s) of <code class="reqn">\lambda</code> used to fit the model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Feser, F., Evangelou, M. (2024). <em>Dual feature reduction for the sparse-group lasso and its adaptive variant</em>, <a href="https://arxiv.org/abs/2405.17094">https://arxiv.org/abs/2405.17094</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>
<p>Simon, N., Friedman, J., Hastie, T., Tibshirani, R. (2013). <em>A Sparse-Group Lasso</em>, <a href="https://doi.org/10.1080/10618600.2012.681250">doi:10.1080/10618600.2012.681250</a>
</p>


<h3>See Also</h3>

<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-SGL 
model = dfr_sgl(X = data$X, y = data$y, groups = groups, type="linear", path_length = 5, 
alpha=0.95, standardise = "l2", intercept = TRUE, verbose=FALSE)
</code></pre>

<hr>
<h2 id='dfr_sgl.cv'>Fit a DFR-SGL model using k-fold cross-validation.</h2><span id='topic+dfr_sgl.cv'></span>

<h3>Description</h3>

<p>Function to fit a pathwise solution of the sparse-group lasso (SGL) applied with DFR using k-fold cross-validation. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfr_sgl.cv(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  path_length = 20,
  nfolds = 10,
  alpha = 0.95,
  backtracking = 0.7,
  max_iter = 5000,
  max_iter_backtracking = 100,
  tol = 1e-05,
  min_frac = 0.05,
  standardise = "l2",
  intercept = TRUE,
  error_criteria = "mse",
  screen = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dfr_sgl.cv_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_lambda">lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li><p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_path_length">path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to use in cross-validation.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between the lasso and group lasso. Must be between 0 and 1. Recommended value is 0.95.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_min_frac">min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_error_criteria">error_criteria</code></td>
<td>
<p>The criteria used to discriminate between models along the path. Supported values are: <code>"mse"</code> (mean squared error) and <code>"mae"</code> (mean absolute error).</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_screen">screen</code></td>
<td>
<p>Logical flag for whether to apply the DFR screening rules (see Feser and Evangelou (2024)).</p>
</td></tr>
<tr><td><code id="dfr_sgl.cv_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits DFR-SGL models under a pathwise solution using Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018)), picking the 1se model as optimum. Warm starts are implemented.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>all_models</code></td>
<td>
<p>A list of all the models fitted along the path.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The 1se chosen model, which is a <code>"sgl"</code> object type.</p>
</td></tr>
<tr><td><code>best_lambda</code></td>
<td>
<p>The value of <code class="reqn">\lambda</code> which generated the chosen model.</p>
</td></tr>
<tr><td><code>best_lambda_id</code></td>
<td>
<p>The path index for the chosen model.</p>
</td></tr>
<tr><td><code>errors</code></td>
<td>
<p>A table containing fitting information about the models on the path.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Feser, F., Evangelou, M. (2024). <em>Dual feature reduction for the sparse-group lasso and its adaptive variant</em>, <a href="https://arxiv.org/abs/2405.17094">https://arxiv.org/abs/2405.17094</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>
</p>
<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-SGL with cross-validation
cv_model = dfr_sgl.cv(X = data$X, y = data$y, groups=groups, type = "linear", 
path_length = 5, nfolds=5, alpha = 0.95, min_frac = 0.05, 
standardise="l2",intercept=TRUE,verbose=TRUE)
</code></pre>

<hr>
<h2 id='plot.sgl'>Plot models of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</h2><span id='topic+plot.sgl'></span>

<h3>Description</h3>

<p>Plots the pathwise solution of a cross-validation fit, from a call to one of the following: <code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>, <code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv()</a></code>, <code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>, <code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgl'
plot(x, how_many = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.sgl_+3A_x">x</code></td>
<td>
<p>Object of one of the following classes: <code>"sgl"</code>, <code>"sgl_cv"</code>..</p>
</td></tr>
<tr><td><code id="plot.sgl_+3A_how_many">how_many</code></td>
<td>
<p>Defines how many predictors to plot. Plots the predictors in decreasing order of largest absolute value.</p>
</td></tr>
<tr><td><code id="plot.sgl_+3A_...">...</code></td>
<td>
<p>further arguments passed to base function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>response</code></td>
<td>
<p>The predicted response. In the logistic case, this represents the predicted class probabilities.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>The predicted class assignments. Only returned if type = &quot;logistic&quot; in the model object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>, <code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv()</a></code>, <code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>, <code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv()</a></code>
</p>
<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,2,2,3)
# generate data
data = sgs::gen_toy_data(p=5, n=4, groups = groups, seed_id=3,signal_mean=20,group_sparsity=1)
# run DFR-SGL
model = dfr_sgl(X = data$X, y = data$y, groups=groups, type = "linear", 
path_length = 20, alpha = 0.95, 
min_frac = 0.05, standardise="l2",intercept=TRUE,verbose=FALSE)
plot(model, how_many = 10)
</code></pre>

<hr>
<h2 id='predict.sgl'>Predict using one of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</h2><span id='topic+predict.sgl'></span>

<h3>Description</h3>

<p>Performs prediction from one of the following fits: <code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>, <code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv()</a></code>, <code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>, <code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv()</a></code>. The predictions are calculated for each <code>"lambda"</code> value in the path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgl'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.sgl_+3A_object">object</code></td>
<td>
<p>Object of one of the following classes: <code>"sgl"</code>, <code>"sgl_cv"</code>.</p>
</td></tr>
<tr><td><code id="predict.sgl_+3A_x">x</code></td>
<td>
<p>Input data to use for prediction.</p>
</td></tr>
<tr><td><code id="predict.sgl_+3A_...">...</code></td>
<td>
<p>further arguments passed to stats function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>response</code></td>
<td>
<p>The predicted response. In the logistic case, this represents the predicted class probabilities.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>The predicted class assignments. Only returned if type = &quot;logistic&quot; in the <code>"sgl"</code> or <code>"sgl_cv"</code> object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>, <code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv()</a></code>, <code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>, <code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv()</a></code>
</p>
<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+print.sgl">print.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-SGL 
model = dfr_sgl(X = data$X, y = data$y, groups = groups, type="linear", lambda = 1, alpha=0.95, 
standardise = "l2", intercept = TRUE, verbose=FALSE)
# use predict function
model_predictions = predict(model, x = data$X)
</code></pre>

<hr>
<h2 id='print.sgl'>Prints information for one of the following object types: <code>"sgl"</code>, <code>"sgl_cv"</code>.</h2><span id='topic+print.sgl'></span>

<h3>Description</h3>

<p>Prints out useful metric from a model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgl'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.sgl_+3A_x">x</code></td>
<td>
<p>Object of one of the following classes: <code>"sgl"</code>, <code>"sgl_cv"</code>.</p>
</td></tr>
<tr><td><code id="print.sgl_+3A_...">...</code></td>
<td>
<p>further arguments passed to base function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A summary of the model fit(s).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfr_sgl">dfr_sgl()</a></code>, <code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv()</a></code>, <code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl()</a></code>, <code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv()</a></code>
</p>
<p>Other SGL-methods: 
<code><a href="#topic+dfr_adap_sgl">dfr_adap_sgl</a>()</code>,
<code><a href="#topic+dfr_adap_sgl.cv">dfr_adap_sgl.cv</a>()</code>,
<code><a href="#topic+dfr_sgl">dfr_sgl</a>()</code>,
<code><a href="#topic+dfr_sgl.cv">dfr_sgl.cv</a>()</code>,
<code><a href="#topic+plot.sgl">plot.sgl</a>()</code>,
<code><a href="#topic+predict.sgl">predict.sgl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(rep(1:20, each=3),
          rep(21:40, each=4),
          rep(41:60, each=5),
          rep(61:80, each=6),
          rep(81:100, each=7))
# generate data
data = sgs::gen_toy_data(p=500, n=400, groups = groups, seed_id=3)
# run DFR-SGL 
model = dfr_sgl(X = data$X, y = data$y, groups = groups, type="linear", lambda = 1, alpha=0.95, 
standardise = "l2", intercept = TRUE, verbose=FALSE)
# print model
print(model)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
