<!DOCTYPE html><html><head><title>Help for package RPEnsemble</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RPEnsemble}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Other.classifier'><p>The users favourite classifier</p></a></li>
<li><a href='#R'><p>A rotation matrix</p></a></li>
<li><a href='#RPalpha'><p>Choose alpha</p></a></li>
<li><a href='#RPChoose'><p>Chooses projection</p></a></li>
<li><a href='#RPChooseSS'><p>A sample splitting version of <code>RPChoose</code></p></a></li>
<li><a href='#RPEnsemble-package'><p>Random Projection Ensemble Classification</p></a></li>
<li><a href='#RPEnsembleClass'><p>Classifies the test set using the random projection ensemble classifier</p></a></li>
<li><a href='#RPGenerate'><p>Generates random matrices</p></a></li>
<li><a href='#RPModel'><p>Generate pairs <code>(x,y)</code> from joint distribution</p></a></li>
<li><a href='#RPParallel'><p>Chooses a projection from each block in parallel</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-02-23</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Projection Ensemble Classification</td>
</tr>
<tr>
<td>Author:</td>
<td>Timothy I. Cannings and Richard J. Samworth</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Timothy I. Cannings &lt;timothy.cannings@ed.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the methodology of "Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035". The random projection ensemble classifier is a general method for classification of high-dimensional data, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower-dimensional space. The random projections are divided into non-overlapping blocks, and within each block the projection yielding the smallest estimate of the test error is selected. The random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0), MASS, parallel</td>
</tr>
<tr>
<td>Imports:</td>
<td>class, stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://arxiv.org/abs/1504.04595">https://arxiv.org/abs/1504.04595</a>,
<a href="https://www.maths.ed.ac.uk/~tcannings/">https://www.maths.ed.ac.uk/~tcannings/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-02-24 12:36:23 UTC; tc</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-02-24 13:40:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='Other.classifier'>The users favourite classifier</h2><span id='topic+Other.classifier'></span>

<h3>Description</h3>

<p>User defined code to convert existing <code>R</code> code for classification to the correct format</p>


<h3>Usage</h3>

<pre><code class='language-R'>Other.classifier(x, grouping, xTest, CV, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Other.classifier_+3A_x">x</code></td>
<td>
<p>An <code>n</code> by <code>p</code> matrix containing the training dataset</p>
</td></tr>
<tr><td><code id="Other.classifier_+3A_grouping">grouping</code></td>
<td>
<p>A vector of length <code>n</code> containing the training data classes</p>
</td></tr>
<tr><td><code id="Other.classifier_+3A_xtest">xTest</code></td>
<td>
<p>An <code>n.test</code> by <code>p</code> test dataset</p>
</td></tr>
<tr><td><code id="Other.classifier_+3A_cv">CV</code></td>
<td>
<p>If <code>TRUE</code> perform cross-validation (or otherwise) to classify training set. If <code>FALSE</code>, classify test set.</p>
</td></tr>
<tr><td><code id="Other.classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments e.g. tuning parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>User editable code for your choice of base classifier.</p>


<h3>Value</h3>

<table>
<tr><td><code>class</code></td>
<td>
<p>a vector of classes of the training or test set</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>

<hr>
<h2 id='R'>A rotation matrix</h2><span id='topic+R'></span>

<h3>Description</h3>

<p>The 100 by 100 rotation matrix used in Model 2 in Cannings and Samworth (2017).</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(R)</code></pre>


<h3>Format</h3>

<p>A 100 by 100 rotation matrix</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(R)
head(R%*%t(R))
</code></pre>

<hr>
<h2 id='RPalpha'>Choose alpha</h2><span id='topic+RPalpha'></span>

<h3>Description</h3>

<p>Chooses the best empirical value of the cutoff <code>alpha</code>, based on the
leave-one-out, resubstitution or sample-split estimates of the class labels.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPalpha(RP.out, Y, p1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPalpha_+3A_rp.out">RP.out</code></td>
<td>
<p>The result of a call to <code><a href="#topic+RPParallel">RPParallel</a></code></p>
</td></tr>
<tr><td><code id="RPalpha_+3A_y">Y</code></td>
<td>
<p>Vector of length <code>n</code> or <code>n.val</code> containing the training or validation dataset classes</p>
</td></tr>
<tr><td><code id="RPalpha_+3A_p1">p1</code></td>
<td>
<p>(Empirical) prior probability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See precise details in Cannings and Samworth (2015, Section 5.1).</p>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>The value of <code>alpha</code> that minimises the empirical error</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>See Also</h3>

<p><code><a href="#topic+RPParallel">RPParallel</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>Train &lt;- RPModel(1, 50, 100, 0.5)
Test &lt;- RPModel(1, 100, 100, 0.5)
Out &lt;- RPParallel(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, d = 2, B1 = 10, 
B2 = 10, base = "LDA", projmethod = "Haar", estmethod = "training", cores = 1)
alpha &lt;- RPalpha(RP.out = Out, Y = Train$y, p1 = sum(Train$y == 1)/length(Train$y))
alpha

</code></pre>

<hr>
<h2 id='RPChoose'>Chooses projection</h2><span id='topic+RPChoose'></span>

<h3>Description</h3>

<p>Chooses a the best projection from a set of size <code>B2</code> based on a test error estimate, then classifies the training and test sets using the chosen projection.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPChoose(XTrain, YTrain, XTest, d, B2 = 10, base = "LDA", k = c(3,5), 
projmethod = "Haar", estmethod = "training", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPChoose_+3A_xtrain">XTrain</code></td>
<td>
<p>An <code>n</code> by <code>p</code> matrix containing the training data feature vectors</p>
</td></tr>
<tr><td><code id="RPChoose_+3A_ytrain">YTrain</code></td>
<td>
<p>A vector of length <code>n</code> of the classes (either 1 or 2) of the training data</p>
</td></tr>
<tr><td><code id="RPChoose_+3A_xtest">XTest</code></td>
<td>
<p>An <code>n.test</code> by <code>p</code> matrix of the test data</p>
</td></tr>
<tr><td><code id="RPChoose_+3A_d">d</code></td>
<td>
<p>The lower dimension of the image space of the projections</p>
</td></tr>
<tr><td><code id="RPChoose_+3A_b2">B2</code></td>
<td>
<p>The block size</p>
</td></tr>
<tr><td><code id="RPChoose_+3A_base">base</code></td>
<td>
<p>The base classifier one of <code>"knn","LDA","QDA"</code> or <code>"other"</code></p>
</td></tr>
<tr><td><code id="RPChoose_+3A_k">k</code></td>
<td>
<p>The options for <code>k</code> if base is <code>"knn"</code></p>
</td></tr>
<tr><td><code id="RPChoose_+3A_projmethod">projmethod</code></td>
<td>
<p>Either <code>"Haar"</code>, <code>"Gaussian"</code> or <code>"axis"</code></p>
</td></tr>
<tr><td><code id="RPChoose_+3A_estmethod">estmethod</code></td>
<td>
<p>Method for estimating the test errors to choose the projection: either training error <code>"training"</code> or leave-one-out <code>"loo"</code></p>
</td></tr>
<tr><td><code id="RPChoose_+3A_...">...</code></td>
<td>
<p>Optional further arguments if <code>base = "other"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Randomly projects the the data <code>B2</code> times. Chooses the projection yielding the smallest estimate of the test error. Classifies the training set (via the same method as <code>estmethod</code>) and test set using the chosen projection.</p>


<h3>Value</h3>

<p>Returns a vector of length <code>n + n.test</code>: the first <code>n</code> entries are the estimated classes of the training set, the last <code>n.test</code> are the estimated classes of the test set.</p>


<h3>Note</h3>

<p>Resubstitution method unsuitable for the <code>k</code>-nearest neighbour classifier.</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>See Also</h3>

<p><code><a href="#topic+RPParallel">RPParallel</a></code>,  <code><a href="#topic+RPChooseSS">RPChooseSS</a></code>, <code>lda</code>, <code>qda</code>, <code>knn</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(100)
Train &lt;- RPModel(1, 50, 100, 0.5)
Test &lt;- RPModel(1, 100, 100, 0.5)
Choose.out5 &lt;- RPChoose(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, 
d = 2, B2 = 5, base = "QDA", projmethod = "Haar", estmethod = "loo")
Choose.out10 &lt;- RPChoose(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, 
d = 2, B2 = 10, base = "QDA", projmethod = "Haar", estmethod = "loo")
sum(Choose.out5[1:50] != Train$y)
sum(Choose.out10[1:50] != Train$y)
sum(Choose.out5[51:150] != Test$y)
sum(Choose.out10[51:150] != Test$y)
</code></pre>

<hr>
<h2 id='RPChooseSS'>A sample splitting version of <code><a href="#topic+RPChoose">RPChoose</a></code></h2><span id='topic+RPChooseSS'></span>

<h3>Description</h3>

<p>Chooses the best projection based on an estimate of the
test error of the classifier with training data <code>(XTrain, YTrain)</code>, the estimation method counts the number of errors made on the validation set <code>(XVal, YVal)</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPChooseSS(XTrain, YTrain, XVal, YVal, XTest, d, B2 = 100, base = "LDA",  
k = c(3, 5), projmethod = "Haar", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPChooseSS_+3A_xtrain">XTrain</code></td>
<td>
<p>An <code>n</code> by <code>p</code> matrix containing the training data feature vectors</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_ytrain">YTrain</code></td>
<td>
<p>A vector of length <code>n</code> of the classes (either 1 or 2) of the training data</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_xval">XVal</code></td>
<td>
<p>An <code>n.val</code> by <code>p</code> matrix containing the validation data feature vectors</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_yval">YVal</code></td>
<td>
<p>A vector of length <code>n.val</code> of the classes (either 1 or 2) of the validation data</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_xtest">XTest</code></td>
<td>
<p>An <code>n.test</code> by <code>p</code> matrix of the test data feature vectors</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_d">d</code></td>
<td>
<p>The lower dimension of the image space of the projections</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_b2">B2</code></td>
<td>
<p>The block size</p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_base">base</code></td>
<td>
<p>The base classifier one of <code>"knn","LDA","QDA"</code> or <code>"other"</code></p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_k">k</code></td>
<td>
<p>The options for <code>k</code> if <code>base = "knn"</code></p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_projmethod">projmethod</code></td>
<td>
<p>Either <code>"Haar"</code>, <code>"Gaussian"</code> or <code>"axis"</code></p>
</td></tr>
<tr><td><code id="RPChooseSS_+3A_...">...</code></td>
<td>
<p>Optional further arguments if <code>base = "other"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Maps the the data using <code>B2</code> random projections. For each projection the validation set is classified using the the training set and the projection yielding the smallest number of errors over the validation set is retained. The validation set and test set are then classified using the chosen projection.</p>


<h3>Value</h3>

<p>Returns a vector of length <code>n.val + n.test</code>: the first <code>n.val</code> entries are the estimated classes of the validation set, the last <code>n.test</code> are the estimated classes of the test set.</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>See Also</h3>

<p><code><a href="#topic+RPParallel">RPParallel</a></code>, <code><a href="#topic+RPChoose">RPChoose</a></code>, <code>lda</code>, <code>qda</code>, <code>knn</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(100)
Train &lt;- RPModel(1, 50, 100, 0.5)
Validate &lt;- RPModel(1, 50, 100, 0.5)
Test &lt;- RPModel(1, 100, 100, 0.5)
Choose.out5 &lt;- RPChooseSS(XTrain = Train$x, YTrain = Train$y, XVal = Validate$x, 
YVal = Validate$y, XTest = Test$x, d = 2, B2 = 5, base = "QDA", projmethod = "Haar")
Choose.out10 &lt;- RPChooseSS(XTrain = Train$x, YTrain = Train$y, XVal = Validate$x, 
YVal = Validate$y, XTest = Test$x, d = 2, B2 = 10, base = "QDA", projmethod = "Haar")
sum(Choose.out5[1:50] != Validate$y)
sum(Choose.out10[1:50] != Validate$y)
sum(Choose.out5[51:150] != Test$y)
sum(Choose.out10[51:150] != Test$y)
</code></pre>

<hr>
<h2 id='RPEnsemble-package'>Random Projection Ensemble Classification</h2><span id='topic+RPEnsemble-package'></span><span id='topic+RPEnsemble'></span>

<h3>Description</h3>

<p>Implements the methodology of Cannings and Samworth (2017).  The random projection ensemble classifier is a very general method for classification of high-dimensional data, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower-dimensional space. The random projections are divided into non-overlapping blocks, and within each block the projection yielding the smallest estimate of the test error is selected.  The random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment.</p>


<h3>Details</h3>

<p><code><a href="#topic+RPChoose">RPChoose</a></code> chooses the projection from a block of size <code>B2</code> that minimises an estimate of the test error (see Cannings and Samworth, 2017, Section 3), and classifies the training and test sets using the base classifier on the projected data.  <code><a href="#topic+RPParallel">RPParallel</a></code> makes many calls to <code><a href="#topic+RPChoose">RPChoose</a></code> in parallel.  <code><a href="#topic+RPalpha">RPalpha</a></code> chooses the best empirical value of alpha (see Cannings and Samworth, 2017, Section 5.1).  <code><a href="#topic+RPEnsembleClass">RPEnsembleClass</a></code> combines the results of many base classifications to classify the test set.
</p>
<p>The method can be used with any base classifier, any test error estimate and any distribution of the random projections.  This package provides code for the following options: Classifiers &ndash; linear discriminant analysis, quadratic discriminant analysis and the k-nearest neighbour classifier.  Error estimates &ndash; resubstitution and leave-one-out, we also provide code for the sample-splitting method described in Cannings and Samworth (2017, Section 7) (this can be done by setting <code>estmethod = samplesplit</code>).  Projection distribution &ndash; Haar, Gaussian or axis-aligned projections. 
</p>
<p>The package provides the option to add your own base classifier and estimation method, this can be done by editing the code in the function <code><a href="#topic+Other.classifier">Other.classifier</a></code>.  Moreover, one could edit the <code><a href="#topic+RPGenerate">RPGenerate</a></code> function to generate projections from different distributions.</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth
</p>
<p>Maintainer: Timothy I. Cannings &lt;timothy.cannings@ed.ac.uk&gt;
</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>Examples</h3>

<pre><code class='language-R'>#generate data from Model 1
set.seed(101)
Train &lt;- RPModel(2, 50, 100, 0.5)
Test &lt;- RPModel(2, 100, 100, 0.5)

#Classify the training and test set for B1 = 10 independent projections, each 
#one carefully chosen from a block of size B2 = 10, using the "knn" base 
#classifier and the leave-one-out test error estimate
Out &lt;- RPParallel(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, d = 2, 
B1 = 10, B2 = 10, base = "knn", projmethod = "Haar", estmethod = "loo",  
splitsample = FALSE, k = seq(1, 25, by = 3), clustertype = "Default")

#estimate the class 1 prior probability
phat &lt;- sum(Train$y == 1)/50

#choose the best empirical value of the voting threshold alpha
alphahat &lt;- RPalpha(RP.out = Out, Y = Train$y, p1 = phat)

#combine the base classifications  
Class &lt;- RPEnsembleClass(RP.out = Out, n = 50, 
n.test = 100, p1 = phat, alpha = alphahat)

#calculate the error
mean(Class != Test$y)



#Code for sample splitting version of the above
#n.val &lt;- 25
#s &lt;- sample(1:50,25)
#OutSS &lt;- RPParallel(XTrain = Train$x[-s,], YTrain = Train$y[-s], 
#XVal = Train$x[s,], YVal = Train$y[s],  XTest = Test$x, d = 2, 
#B1 = 50, B2 = 10, base = "knn", projmethod = "Haar", estmethod = "samplesplit",
#k = seq(1,13, by = 2), clustertype = "Fork", cores = 1)
#alphahatSS &lt;- RPalpha(RP.out = OutSS, Y = Train$y[s], p1 = phat)
#ClassSS &lt;- RPEnsembleClass(RP.out = OutSS, n.val = 25, n.test = 100, 
#p1 = phat, samplesplit = TRUE, alpha = alphahatSS)
#mean(ClassSS != Test$y)
</code></pre>

<hr>
<h2 id='RPEnsembleClass'>Classifies the test set using the random projection ensemble classifier</h2><span id='topic+RPEnsembleClass'></span>

<h3>Description</h3>

<p>Performs a biased majority vote over <code>B1</code> base classifications to assign the test set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPEnsembleClass(RP.out, n , n.val, n.test, p1, samplesplit, alpha, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPEnsembleClass_+3A_rp.out">RP.out</code></td>
<td>
<p>The result of a call to <code><a href="#topic+RPParallel">RPParallel</a></code></p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_n">n</code></td>
<td>
<p>Training set sample size</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_n.test">n.test</code></td>
<td>
<p>Test set sample size</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_n.val">n.val</code></td>
<td>
<p>Validation set sample size</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_p1">p1</code></td>
<td>
<p>Prior probability estimate</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_samplesplit">samplesplit</code></td>
<td>
<p><code>TRUE</code> if using sample-splitting method</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_alpha">alpha</code></td>
<td>
<p>The voting threshold</p>
</td></tr>
<tr><td><code id="RPEnsembleClass_+3A_...">...</code></td>
<td>
<p>Optional further arguments if <code>base = "other"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>An observation in the test set is assigned to class 1 if <code>B1*alpha</code> or more of the base classifications are class 1 (otherwise class 2).</p>


<h3>Value</h3>

<p>A vector of length <code>n.test</code> containing the class predictions of the test set (either 1 or 2).</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>See Also</h3>

<p><code><a href="#topic+RPParallel">RPParallel</a></code>, <code><a href="#topic+RPalpha">RPalpha</a></code>, <code><a href="#topic+RPChoose">RPChoose</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>Train &lt;- RPModel(1, 50, 100, 0.5)
Test &lt;- RPModel(1, 100, 100, 0.5)
Out &lt;- RPParallel(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, 
d = 2, B1 = 50, B2 = 10, base = "LDA", projmethod = "Haar", 
estmethod = "training", clustertype = "Default")
Class &lt;- RPEnsembleClass(RP.out = Out, n = length(Train$y), 
n.test = nrow(Test$x), p1 = sum(Train$y == 1)/length(Train$y),  
splitsample = FALSE,  alpha = RPalpha(Out, Y = Train$y, 
p1 = sum(Train$y == 1)/length(Train$y)))
mean(Class != Test$y)
</code></pre>

<hr>
<h2 id='RPGenerate'>Generates random matrices</h2><span id='topic+RPGenerate'></span>

<h3>Description</h3>

<p>Generates <code>B2</code> random <code>p</code> by <code>d</code> matrices according to Haar measure, Gaussian or axis-aligned projections</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPGenerate(p = 100, d = 10, method = "Haar", B2 = 10)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPGenerate_+3A_p">p</code></td>
<td>
<p>The original data dimension</p>
</td></tr>
<tr><td><code id="RPGenerate_+3A_d">d</code></td>
<td>
<p>The lower dimension</p>
</td></tr>
<tr><td><code id="RPGenerate_+3A_method">method</code></td>
<td>
<p>Projection distribution, either <code>"Haar"</code> for Haar distributed projections, <code>"Gaussian"</code> for Gaussian distributed projections with i.i.d. <code>N(0,1/p)</code> entries, <code>"axis"</code> for uniformly distributed axis aligned projections, or <code>"other"</code> for user defined method</p>
</td></tr>
<tr><td><code id="RPGenerate_+3A_b2">B2</code></td>
<td>
<p>the number of projections</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns <code>B2</code> <code>p</code> by <code>d</code> random matrices as a single <code>p</code> by <code>d*B2</code> matrix</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>Examples</h3>

<pre><code class='language-R'>R1 &lt;- RPGenerate(p = 20, d = 2, "Haar", B2 = 3)
t(R1)%*%R1
R2 &lt;- RPGenerate(p = 20, d = 2, "Gaussian", B2 = 3)
t(R2)%*%R2
R3 &lt;- RPGenerate(p = 20, d = 2, "axis", B2 = 3)
colSums(R3)
rowSums(R3)
</code></pre>

<hr>
<h2 id='RPModel'>Generate pairs <code>(x,y)</code> from joint distribution</h2><span id='topic+RPModel'></span>

<h3>Description</h3>

<p>Generates data from the models described in Cannings and Samworth (2017)</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPModel(Model.No, n, p, Pi = 1/2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPModel_+3A_model.no">Model.No</code></td>
<td>
<p>Model Number</p>
</td></tr>
<tr><td><code id="RPModel_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="RPModel_+3A_p">p</code></td>
<td>
<p>Data dimension</p>
</td></tr>
<tr><td><code id="RPModel_+3A_pi">Pi</code></td>
<td>
<p>Class one prior probability</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>An <code>n</code> by <code>p</code> data matrix &ndash; <code>n</code> observations of the <code>p</code>-dimensional features</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>A vector of length <code>n</code> containing the classes (either 1 or 2)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Models 1 and 2 require <code>p = 100</code> or <code>1000</code>.</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>Examples</h3>

<pre><code class='language-R'>Data &lt;- RPModel(Model.No = 1, 100, 100, Pi = 1/2)
table(Data$y)
colMeans(Data$x[Data$y==1,])
colMeans(Data$x[Data$y==2,])
</code></pre>

<hr>
<h2 id='RPParallel'>Chooses a projection from each block in parallel</h2><span id='topic+RPParallel'></span>

<h3>Description</h3>

<p>Makes <code>B1</code> calls to <code><a href="#topic+RPChoose">RPChoose</a></code> or <code><a href="#topic+RPChooseSS">RPChooseSS</a></code> in parallel and returns the results as a matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPParallel(XTrain, YTrain, XVal, YVal, XTest, d, B1 = 500, B2 = 50, 
base = "LDA",projmethod = "Gaussian", estmethod = "training", k = c(3,5,9), 
clustertype = "Default", cores = 1, machines = NULL, seed = 1, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPParallel_+3A_xtrain">XTrain</code></td>
<td>
<p>An <code>n</code> by <code>p</code> matrix containing the training data feature vectors</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_ytrain">YTrain</code></td>
<td>
<p>A vector of length <code>n</code> containing the classes (either 1 or 2) of the training data</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_xval">XVal</code></td>
<td>
<p>An <code>n.val</code> by <code>p</code> matrix containing the validation data feature vectors</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_yval">YVal</code></td>
<td>
<p>A vector of length <code>n.val</code> of the classes (either 1 or 2) of the validation data</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_xtest">XTest</code></td>
<td>
<p>An <code>n.test</code> by <code>p</code> matrix containing the test data feature vectors</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_d">d</code></td>
<td>
<p>The lower dimension of the image space of the projections</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_b1">B1</code></td>
<td>
<p>The number of blocks</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_b2">B2</code></td>
<td>
<p>The size of each block</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_base">base</code></td>
<td>
<p>The base classifier one of <code>"knn","LDA","QDA"</code> or <code>"other"</code></p>
</td></tr>
<tr><td><code id="RPParallel_+3A_k">k</code></td>
<td>
<p>The options for <code>k</code> if base is <code>"knn"</code></p>
</td></tr>
<tr><td><code id="RPParallel_+3A_projmethod">projmethod</code></td>
<td>
<p><code>"Haar"</code>, <code>"Gaussian"</code> or <code>"axis"</code></p>
</td></tr>
<tr><td><code id="RPParallel_+3A_estmethod">estmethod</code></td>
<td>
<p>Method for estimating the test errors to choose the projection: either training error <code>"training"</code>, leave-one-out <code>"loo"</code>, or sample split <code>"samplesplit"</code></p>
</td></tr>
<tr><td><code id="RPParallel_+3A_clustertype">clustertype</code></td>
<td>
<p>The type of cluster: <code>"Default"</code> uses just one core, <code>"Fork"</code> uses a single machine, <code>"Socket"</code> uses many machines. Note <code>"Fork"</code> and <code>"Socket"</code> are not supported on windows.</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_cores">cores</code></td>
<td>
<p>Required only if <code>clustertype==Fork</code>: the number of computer cores to use (note: <code>cores &gt; 1</code> not supported on Windows)</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_machines">machines</code></td>
<td>
<p>Required only if <code>clustertype==Socket</code>: the names of the machines to use e.g. <code>c("Computer1", "Computer2")</code> (not supported on Windows)</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_seed">seed</code></td>
<td>
<p>If not <code>NULL</code>, sets random seed for reproducible results</p>
</td></tr>
<tr><td><code id="RPParallel_+3A_...">...</code></td>
<td>
<p>Optional further arguments if <code>base = "other"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Makes <code>B1</code> calls to <code><a href="#topic+RPChoose">RPChoose</a></code> or <code><a href="#topic+RPChooseSS">RPChooseSS</a></code> in parallel.</p>


<h3>Value</h3>

<p>If <code>estmethod == "training"</code> or <code>"loo"</code> , then returns an <code>n+n.test</code> by <code>B1</code> matrix, each row containing the result of a call to <code><a href="#topic+RPChoose">RPChoose</a></code>. If <code>estmethod == "samplesplit"</code>, then returns an <code>n.val+n.test</code> by <code>B1</code> matrix, each row containing the result of a call to <code><a href="#topic+RPChooseSS">RPChooseSS</a></code>.</p>


<h3>Author(s)</h3>

<p>Timothy I. Cannings and Richard J. Samworth</p>


<h3>References</h3>

<p>Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959&ndash;1035</p>


<h3>See Also</h3>

<p><code><a href="#topic+RPChoose">RPChoose</a></code>, <code><a href="#topic+RPChooseSS">RPChooseSS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>Train &lt;- RPModel(1, 50, 100, 0.5)
Test &lt;- RPModel(1, 100, 100, 0.5)
Out &lt;- RPParallel(XTrain = Train$x, YTrain = Train$y, XTest = Test$x, d = 2, B1 = 10, 
B2 = 10, base = "LDA", projmethod = "Haar", estmethod = "training")
colMeans(Out)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
