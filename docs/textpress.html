<!DOCTYPE html><html lang="en"><head><title>Help for package textpress</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textpress}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#textpress-package'><p>textpress: A Lightweight and Versatile NLP Toolkit</p></a></li>
<li><a href='#.decode_duckduckgo_urls'><p>Decode DuckDuckGo Redirect URLs</p></a></li>
<li><a href='#.extract_links'><p>Extract links from a search engine result page</p></a></li>
<li><a href='#.get_site'><p>Get Site Content and Extract HTML Elements</p></a></li>
<li><a href='#.insert_highlight'><p>Insert Highlight in Text</p></a></li>
<li><a href='#.process_bing'><p>Process Bing search results</p></a></li>
<li><a href='#.process_duckduckgo'><p>Process DuckDuckGo search results</p></a></li>
<li><a href='#.process_yahoo'><p>Process Yahoo News search results</p></a></li>
<li><a href='#.translate_query'><p>Translate Search Query</p></a></li>
<li><a href='#abbreviations'><p>Common Abbreviations for Sentence Splitting</p></a></li>
<li><a href='#api_huggingface_embeddings'><p>Call Hugging Face API for Embeddings</p></a></li>
<li><a href='#extract_date'><p>Extract Date from HTML Content</p></a></li>
<li><a href='#nlp_build_chunks'><p>Build Chunks for NLP Analysis</p></a></li>
<li><a href='#nlp_cast_tokens'><p>Convert Token List to Data Frame</p></a></li>
<li><a href='#nlp_melt_tokens'><p>Tokenize Data Frame by Specified Column(s)</p></a></li>
<li><a href='#nlp_split_paragraphs'><p>Split Text into Paragraphs</p></a></li>
<li><a href='#nlp_split_sentences'><p>Split Text into Sentences</p></a></li>
<li><a href='#nlp_tokenize_text'><p>Tokenize Text Data (mostly) Non-Destructively</p></a></li>
<li><a href='#sem_nearest_neighbors'><p>Find Nearest Neighbors Based on Cosine Similarity</p></a></li>
<li><a href='#sem_search_corpus'><p>NLP Search Corpus</p></a></li>
<li><a href='#standardize_date'><p>Standardize Date Format</p></a></li>
<li><a href='#web_scrape_urls'><p>Scrape News Data from Various Sources</p></a></li>
<li><a href='#web_search'><p>Process search results from multiple search engines</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Lightweight and Versatile NLP Toolkit</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jason Timm &lt;JaTimm@salud.unm.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <a href="https://huggingface.co/docs/api-inference/index">https://huggingface.co/docs/api-inference/index</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, httr, Matrix, rvest, stringi, stringr, xml2,
pbapply, jsonlite, lubridate</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jaytimm/textpress">https://github.com/jaytimm/textpress</a>,
<a href="https://jaytimm.github.io/textpress/">https://jaytimm.github.io/textpress/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jaytimm/textpress/issues">https://github.com/jaytimm/textpress/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-10 21:33:20 UTC; jtimm</td>
</tr>
<tr>
<td>Author:</td>
<td>Jason Timm [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-14 12:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='textpress-package'>textpress: A Lightweight and Versatile NLP Toolkit</h2><span id='topic+textpress'></span><span id='topic+textpress-package'></span>

<h3>Description</h3>

<p>A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <a href="https://huggingface.co/docs/api-inference/index">https://huggingface.co/docs/api-inference/index</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jason Timm <a href="mailto:JaTimm@salud.unm.edu">JaTimm@salud.unm.edu</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/jaytimm/textpress">https://github.com/jaytimm/textpress</a>
</p>
</li>
<li> <p><a href="https://jaytimm.github.io/textpress/">https://jaytimm.github.io/textpress/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/jaytimm/textpress/issues">https://github.com/jaytimm/textpress/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.decode_duckduckgo_urls'>Decode DuckDuckGo Redirect URLs</h2><span id='topic+.decode_duckduckgo_urls'></span>

<h3>Description</h3>

<p>This function decodes the DuckDuckGo search result URLs that are redirected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.decode_duckduckgo_urls(redirected_urls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".decode_duckduckgo_urls_+3A_redirected_urls">redirected_urls</code></td>
<td>
<p>A vector of DuckDuckGo search result URLs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of decoded URLs.
</p>

<hr>
<h2 id='.extract_links'>Extract links from a search engine result page</h2><span id='topic+.extract_links'></span>

<h3>Description</h3>

<p>This function extracts all the links (href attributes) from a search engine result page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.extract_links(search_url)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".extract_links_+3A_search_url">search_url</code></td>
<td>
<p>The URL of the search engine result page.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of URLs.
</p>

<hr>
<h2 id='.get_site'>Get Site Content and Extract HTML Elements</h2><span id='topic+.get_site'></span>

<h3>Description</h3>

<p>This function attempts to retrieve the HTML content of a URL, extract specific
HTML elements (e.g., paragraphs, headings), and extract publication date information
using the <code>extract_date</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_site(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".get_site_+3A_x">x</code></td>
<td>
<p>A URL to extract content and publication date from.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns for the URL, HTML element types, text content, extracted date, and date source.
</p>

<hr>
<h2 id='.insert_highlight'>Insert Highlight in Text</h2><span id='topic+.insert_highlight'></span>

<h3>Description</h3>

<p>Inserts highlight markers around a specified substring in a text string.
Used to visually emphasize search query matches in the text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.insert_highlight(text, start, end, highlight)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".insert_highlight_+3A_text">text</code></td>
<td>
<p>The text string where highlighting is to be applied.</p>
</td></tr>
<tr><td><code id=".insert_highlight_+3A_start">start</code></td>
<td>
<p>The starting position of the substring to highlight.</p>
</td></tr>
<tr><td><code id=".insert_highlight_+3A_end">end</code></td>
<td>
<p>The ending position of the substring to highlight.</p>
</td></tr>
<tr><td><code id=".insert_highlight_+3A_highlight">highlight</code></td>
<td>
<p>A character vector of length two specifying the
opening and closing highlight markers.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string with the specified substring highlighted.
</p>

<hr>
<h2 id='.process_bing'>Process Bing search results</h2><span id='topic+.process_bing'></span>

<h3>Description</h3>

<p>This function retrieves and processes search results from Bing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_bing(
  search_term,
  num_pages,
  time_filter,
  insite,
  intitle,
  combined_pattern
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".process_bing_+3A_search_term">search_term</code></td>
<td>
<p>The search query.</p>
</td></tr>
<tr><td><code id=".process_bing_+3A_num_pages">num_pages</code></td>
<td>
<p>Number of result pages to retrieve.</p>
</td></tr>
<tr><td><code id=".process_bing_+3A_time_filter">time_filter</code></td>
<td>
<p>Optional time filter (&quot;week&quot;, &quot;month&quot;, &quot;year&quot;).</p>
</td></tr>
<tr><td><code id=".process_bing_+3A_insite">insite</code></td>
<td>
<p>Restrict search to a specific domain.</p>
</td></tr>
<tr><td><code id=".process_bing_+3A_intitle">intitle</code></td>
<td>
<p>Search within the title.</p>
</td></tr>
<tr><td><code id=".process_bing_+3A_combined_pattern">combined_pattern</code></td>
<td>
<p>A pattern for filtering out irrelevant URLs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'data.table' of search results from Bing.
</p>

<hr>
<h2 id='.process_duckduckgo'>Process DuckDuckGo search results</h2><span id='topic+.process_duckduckgo'></span>

<h3>Description</h3>

<p>This function handles the extraction of search results from DuckDuckGo.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_duckduckgo(
  search_term,
  num_pages,
  time_filter,
  insite,
  intitle,
  combined_pattern
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".process_duckduckgo_+3A_search_term">search_term</code></td>
<td>
<p>The search query.</p>
</td></tr>
<tr><td><code id=".process_duckduckgo_+3A_num_pages">num_pages</code></td>
<td>
<p>Number of result pages to retrieve.</p>
</td></tr>
<tr><td><code id=".process_duckduckgo_+3A_time_filter">time_filter</code></td>
<td>
<p>Optional time filter (&quot;week&quot;, &quot;month&quot;, &quot;year&quot;).</p>
</td></tr>
<tr><td><code id=".process_duckduckgo_+3A_insite">insite</code></td>
<td>
<p>Restrict search to a specific domain.</p>
</td></tr>
<tr><td><code id=".process_duckduckgo_+3A_intitle">intitle</code></td>
<td>
<p>Search within the title.</p>
</td></tr>
<tr><td><code id=".process_duckduckgo_+3A_combined_pattern">combined_pattern</code></td>
<td>
<p>A pattern for filtering out irrelevant URLs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'data.table' of search results from DuckDuckGo.
</p>

<hr>
<h2 id='.process_yahoo'>Process Yahoo News search results</h2><span id='topic+.process_yahoo'></span>

<h3>Description</h3>

<p>This function retrieves and processes search results from Yahoo News,
automatically sorting by the most recent articles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_yahoo(search_term, num_pages, combined_pattern = combined_pattern)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".process_yahoo_+3A_search_term">search_term</code></td>
<td>
<p>The search query.</p>
</td></tr>
<tr><td><code id=".process_yahoo_+3A_num_pages">num_pages</code></td>
<td>
<p>Number of result pages to retrieve.</p>
</td></tr>
<tr><td><code id=".process_yahoo_+3A_combined_pattern">combined_pattern</code></td>
<td>
<p>A pattern for filtering out irrelevant URLs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'data.table' of search results from Yahoo News.
</p>

<hr>
<h2 id='.translate_query'>Translate Search Query</h2><span id='topic+.translate_query'></span>

<h3>Description</h3>

<p>Translates a search query into a format suitable for regex matching,
particularly for inline searches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.translate_query(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".translate_query_+3A_x">x</code></td>
<td>
<p>The search query string to be translated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string representing the translated query.
</p>

<hr>
<h2 id='abbreviations'>Common Abbreviations for Sentence Splitting</h2><span id='topic+abbreviations'></span>

<h3>Description</h3>

<p>A character vector of common abbreviations used in English.
These abbreviations are used to assist in sentence splitting,
ensuring that sentence boundaries are not incorrectly identified
at these abbreviations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abbreviations
</code></pre>


<h3>Format</h3>

<p>A character vector with some common English abbreviations.
</p>


<h3>Source</h3>

<p>Developed internally for sentence splitting functionality.
</p>

<hr>
<h2 id='api_huggingface_embeddings'>Call Hugging Face API for Embeddings</h2><span id='topic+api_huggingface_embeddings'></span>

<h3>Description</h3>

<p>Retrieves embeddings for text data using Hugging Face's API. It can process a batch of texts or a single query.  Mostly for demo purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>api_huggingface_embeddings(
  tif,
  text_hierarchy,
  api_token,
  api_url = NULL,
  query = NULL,
  dims = 384,
  batch_size = 250,
  sleep_duration = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="api_huggingface_embeddings_+3A_tif">tif</code></td>
<td>
<p>A data frame containing text data.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_text_hierarchy">text_hierarchy</code></td>
<td>
<p>A character vector indicating the columns used to create row names.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_api_token">api_token</code></td>
<td>
<p>Token for accessing the Hugging Face API.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_api_url">api_url</code></td>
<td>
<p>The URL of the Hugging Face API endpoint (default is all-MiniLM-L6-v2).</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_query">query</code></td>
<td>
<p>An optional single text query for which embeddings are required.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_dims">dims</code></td>
<td>
<p>The dimension of the output embeddings.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_batch_size">batch_size</code></td>
<td>
<p>Number of rows in each batch sent to the API.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_sleep_duration">sleep_duration</code></td>
<td>
<p>Duration in seconds to pause between processing batches.</p>
</td></tr>
<tr><td><code id="api_huggingface_embeddings_+3A_verbose">verbose</code></td>
<td>
<p>A boolean specifying whether to include progress bar</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing embeddings, with each row corresponding to a text input.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
tif &lt;- data.frame(doc_id = c('1'), text = c("Hello world."))
embeddings &lt;- api_huggingface_embeddings(tif,
                                         text_hierarchy = 'doc_id',
                                         api_token = api_token)

## End(Not run)


</code></pre>

<hr>
<h2 id='extract_date'>Extract Date from HTML Content</h2><span id='topic+extract_date'></span>

<h3>Description</h3>

<p>This function attempts to extract a publication date from the HTML content
of a web page using various methods such as JSON-LD, OpenGraph meta tags,
standard meta tags, and common HTML elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_date(site)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_date_+3A_site">site</code></td>
<td>
<p>An HTML document (as parsed by xml2 or rvest) from which to extract the date.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with two columns: 'date' and 'source', indicating the extracted
date and the source from which it was extracted (e.g., JSON-LD, OpenGraph, etc.).
If no date is found, returns NA for both fields.
</p>

<hr>
<h2 id='nlp_build_chunks'>Build Chunks for NLP Analysis</h2><span id='topic+nlp_build_chunks'></span>

<h3>Description</h3>

<p>This function processes a data frame for NLP analysis by dividing text into chunks and providing context.
It generates chunks of text with a specified size and includes context based on the specified context size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_build_chunks(tif, text_hierarchy, chunk_size, context_size)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_build_chunks_+3A_tif">tif</code></td>
<td>
<p>A data.table containing the text to be chunked.</p>
</td></tr>
<tr><td><code id="nlp_build_chunks_+3A_text_hierarchy">text_hierarchy</code></td>
<td>
<p>A character vector specifying the columns used for grouping and chunking.</p>
</td></tr>
<tr><td><code id="nlp_build_chunks_+3A_chunk_size">chunk_size</code></td>
<td>
<p>An integer specifying the size of each chunk.</p>
</td></tr>
<tr><td><code id="nlp_build_chunks_+3A_context_size">context_size</code></td>
<td>
<p>An integer specifying the size of the context around each chunk.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with the chunked text and their respective contexts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Creating a data frame
tif &lt;- data.frame(doc_id = c('1', '1', '2'),
                 sentence_id = c('1', '2', '1'),
                 text = c("Hello world.",
                          "This is an example.",
                          "This is a party!"))

chunks &lt;- nlp_build_chunks(tif,
                           chunk_size = 2,
                           context_size = 1,
                           text_hierarchy = c('doc_id', 'sentence_id'))
</code></pre>

<hr>
<h2 id='nlp_cast_tokens'>Convert Token List to Data Frame</h2><span id='topic+nlp_cast_tokens'></span>

<h3>Description</h3>

<p>This function converts a list of tokens into a data frame, extracting and separating document and sentence identifiers if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_cast_tokens(tok)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_cast_tokens_+3A_tok">tok</code></td>
<td>
<p>A list where each element contains tokens corresponding to a document or a sentence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns for token name and token.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokens &lt;- list(c("Hello", "world", "."),
               c("This", "is", "an", "example", "." ),
               c("This", "is", "a", "party", "!"))
names(tokens) &lt;- c('1.1', '1.2', '2.1')
dtm &lt;- nlp_cast_tokens(tokens)

</code></pre>

<hr>
<h2 id='nlp_melt_tokens'>Tokenize Data Frame by Specified Column(s)</h2><span id='topic+nlp_melt_tokens'></span>

<h3>Description</h3>

<p>This function tokenizes a data frame based on a specified token column and groups the data by one or more specified columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_melt_tokens(
  df,
  melt_col = "token",
  parent_cols = c("doc_id", "sentence_id")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_melt_tokens_+3A_df">df</code></td>
<td>
<p>A data frame containing the data to be tokenized.</p>
</td></tr>
<tr><td><code id="nlp_melt_tokens_+3A_melt_col">melt_col</code></td>
<td>
<p>The name of the column in 'df' that contains the tokens.</p>
</td></tr>
<tr><td><code id="nlp_melt_tokens_+3A_parent_cols">parent_cols</code></td>
<td>
<p>A character vector indicating the column(s) by which to group the data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of vectors, each containing the tokens of a group defined by the 'by' parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dtm &lt;- data.frame(doc_id = as.character(c(1, 1, 1, 1, 1, 1, 1, 1)),
                  sentence_id = as.character(c(1, 1, 1, 2, 2, 2, 2, 2)),
                  token = c("Hello", "world", ".", "This", "is", "an", "example", "."))

tokens &lt;- nlp_melt_tokens(dtm, melt_col = 'token', parent_cols = c('doc_id', 'sentence_id'))


</code></pre>

<hr>
<h2 id='nlp_split_paragraphs'>Split Text into Paragraphs</h2><span id='topic+nlp_split_paragraphs'></span>

<h3>Description</h3>

<p>Splits text from the 'text' column of a data frame into individual paragraphs,
based on a specified paragraph delimiter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_split_paragraphs(tif, paragraph_delim = "\\n+")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_split_paragraphs_+3A_tif">tif</code></td>
<td>
<p>A data frame with at least two columns: 'doc_id' and 'text'.</p>
</td></tr>
<tr><td><code id="nlp_split_paragraphs_+3A_paragraph_delim">paragraph_delim</code></td>
<td>
<p>A regular expression pattern used to split text into paragraphs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with columns: 'doc_id', 'paragraph_id', and 'text'.
Each row represents a paragraph, along with its associated document and paragraph identifiers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tif &lt;- data.frame(doc_id = c('1', '2'),
                  text = c("Hello world.\n\nMind your business!",
                           "This is an example.n\nThis is a party!"))
paragraphs &lt;- nlp_split_paragraphs(tif)


</code></pre>

<hr>
<h2 id='nlp_split_sentences'>Split Text into Sentences</h2><span id='topic+nlp_split_sentences'></span>

<h3>Description</h3>

<p>This function splits text from a data frame into individual sentences based on specified columns and handles abbreviations effectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_split_sentences(
  tif,
  text_hierarchy = c("doc_id"),
  abbreviations = textpress::abbreviations
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_split_sentences_+3A_tif">tif</code></td>
<td>
<p>A data frame containing text to be split into sentences.</p>
</td></tr>
<tr><td><code id="nlp_split_sentences_+3A_text_hierarchy">text_hierarchy</code></td>
<td>
<p>A character vector specifying the columns to group by for sentence splitting, usually 'doc_id'.</p>
</td></tr>
<tr><td><code id="nlp_split_sentences_+3A_abbreviations">abbreviations</code></td>
<td>
<p>A character vector of abbreviations to handle during sentence splitting, defaults to textpress::abbreviations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with columns specified in 'by', 'sentence_id', and 'text'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tif &lt;- data.frame(doc_id = c('1'),
                  text = c("Hello world. This is an example. No, this is a party!"))
sentences &lt;- nlp_split_paragraphs(tif)


</code></pre>

<hr>
<h2 id='nlp_tokenize_text'>Tokenize Text Data (mostly) Non-Destructively</h2><span id='topic+nlp_tokenize_text'></span>

<h3>Description</h3>

<p>This function tokenizes text data from a data frame using the 'tokenizers' package, preserving the original text structure like capitalization and punctuation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_tokenize_text(
  tif,
  text_hierarchy = c("doc_id", "paragraph_id", "sentence_id")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_tokenize_text_+3A_tif">tif</code></td>
<td>
<p>A data frame containing the text to be tokenized and a document identifier in 'doc_id'.</p>
</td></tr>
<tr><td><code id="nlp_tokenize_text_+3A_text_hierarchy">text_hierarchy</code></td>
<td>
<p>A character string specifying grouping column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of tokens, where each list item corresponds to a document.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tif &lt;- data.frame(doc_id = c('1', '1', '2'),
                  sentence_id = c('1', '2', '1'),
                  text = c("Hello world.",
                           "This is an example.",
                           "This is a party!"))
tokens &lt;- nlp_tokenize_text(tif, text_hierarchy = c('doc_id', 'sentence_id'))


</code></pre>

<hr>
<h2 id='sem_nearest_neighbors'>Find Nearest Neighbors Based on Cosine Similarity</h2><span id='topic+sem_nearest_neighbors'></span>

<h3>Description</h3>

<p>This function identifies the nearest neighbors of a given term or vector
in a matrix based on cosine similarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sem_nearest_neighbors(x, matrix, n = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sem_nearest_neighbors_+3A_x">x</code></td>
<td>
<p>A character or numeric vector representing the term or vector.</p>
</td></tr>
<tr><td><code id="sem_nearest_neighbors_+3A_matrix">matrix</code></td>
<td>
<p>A numeric matrix or a sparse matrix against which the similarity is calculated.</p>
</td></tr>
<tr><td><code id="sem_nearest_neighbors_+3A_n">n</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the ranks, terms, and their cosine similarity scores.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
 api_token &lt;- ''
 matrix &lt;- api_huggingface_embeddings(tif,
                                      text_hierarchy = c('doc_id', 'sentence_id'),
                                      api_token = api_token)
 query &lt;- api_huggingface_embeddings(query = "Where's the party at?",
                                     api_token = api_token)
 neighbors &lt;- sem_nearest_neighbors(x = query, matrix = matrix)

## End(Not run)



</code></pre>

<hr>
<h2 id='sem_search_corpus'>NLP Search Corpus</h2><span id='topic+sem_search_corpus'></span>

<h3>Description</h3>

<p>Searches a text corpus for specified patterns, with support for parallel processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sem_search_corpus(
  tif,
  text_hierarchy = c("doc_id", "paragraph_id", "sentence_id"),
  search,
  context_size = 0,
  is_inline = FALSE,
  highlight = c("&lt;b&gt;", "&lt;/b&gt;"),
  cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sem_search_corpus_+3A_tif">tif</code></td>
<td>
<p>A data frame or data.table containing the text corpus.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_text_hierarchy">text_hierarchy</code></td>
<td>
<p>A character vector indicating the column(s) by which to group the data.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_search">search</code></td>
<td>
<p>The search pattern or query.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_context_size">context_size</code></td>
<td>
<p>Numeric, default 0. Specifies the context size, in sentences, around the found patterns.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_is_inline">is_inline</code></td>
<td>
<p>Logical, default FALSE. Indicates if the search should be inline.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_highlight">highlight</code></td>
<td>
<p>A character vector of length two, default c('&lt;b&gt;', '&lt;/b&gt;').
Used to highlight the found patterns in the text.</p>
</td></tr>
<tr><td><code id="sem_search_corpus_+3A_cores">cores</code></td>
<td>
<p>Numeric, default 1. The number of cores to use for parallel processing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with the search results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tif &lt;- data.frame(doc_id = c('1', '1', '2'),
                  sentence_id = c('1', '2', '1'),
                  text = c("Hello world.",
                           "This is an example.",
                           "This is a party!"))
sem_search_corpus(tif, search = 'This is', text_hierarchy = c('doc_id', 'sentence_id'))


</code></pre>

<hr>
<h2 id='standardize_date'>Standardize Date Format</h2><span id='topic+standardize_date'></span>

<h3>Description</h3>

<p>This function attempts to parse a date string using multiple formats and
standardizes it to &quot;YYYY-MM-DD&quot;. It first tries ISO 8601 formats,
and then common formats like ymd, dmy, and mdy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardize_date(date_str)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="standardize_date_+3A_date_str">date_str</code></td>
<td>
<p>A character string representing a date.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string representing the standardized date in &quot;YYYY-MM-DD&quot; format, or NA if the date cannot be parsed.
</p>

<hr>
<h2 id='web_scrape_urls'>Scrape News Data from Various Sources</h2><span id='topic+web_scrape_urls'></span>

<h3>Description</h3>

<p>Function scrapes content of provided list of URLs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>web_scrape_urls(x, cores = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="web_scrape_urls_+3A_x">x</code></td>
<td>
<p>A character vector of URLs.</p>
</td></tr>
<tr><td><code id="web_scrape_urls_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing scraped news data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
url &lt;- 'https://www.nytimes.com/2024/03/25/nyregion/trump-bond-reduced.html'
article_tif &lt;- web_scrape_urls(x = url, input = 'urls', cores = 1)

## End(Not run)


</code></pre>

<hr>
<h2 id='web_search'>Process search results from multiple search engines</h2><span id='topic+web_search'></span>

<h3>Description</h3>

<p>This function allows you to query different search engines (DuckDuckGo, Bing, Yahoo News),
retrieve search results, and filter them based on predefined patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>web_search(
  search_term,
  search_engine,
  num_pages = 1,
  time_filter = NULL,
  insite = NULL,
  intitle = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="web_search_+3A_search_term">search_term</code></td>
<td>
<p>The search query as a string.</p>
</td></tr>
<tr><td><code id="web_search_+3A_search_engine">search_engine</code></td>
<td>
<p>The search engine to use: &quot;DuckDuckGo&quot;, &quot;Bing&quot;, or &quot;Yahoo News&quot;.</p>
</td></tr>
<tr><td><code id="web_search_+3A_num_pages">num_pages</code></td>
<td>
<p>The number of result pages to retrieve (default: 1).</p>
</td></tr>
<tr><td><code id="web_search_+3A_time_filter">time_filter</code></td>
<td>
<p>Optional time filter (&quot;week&quot;, &quot;month&quot;, &quot;year&quot;).</p>
</td></tr>
<tr><td><code id="web_search_+3A_insite">insite</code></td>
<td>
<p>Restrict search to a specific domain (not supported for Yahoo).</p>
</td></tr>
<tr><td><code id="web_search_+3A_intitle">intitle</code></td>
<td>
<p>Search within the title (relevant for DuckDuckGo and Bing).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'data.table' containing search engine results with columns 'search_engine' and 'raw_url'.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
