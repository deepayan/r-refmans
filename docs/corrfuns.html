<!DOCTYPE html><html><head><title>Help for package corrfuns</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {corrfuns}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#corrfuns-package'>
<p>Correlation Coefficient Related Functions</p></a></li>
<li><a href='#Asymptotic+20p-value+20for+20a+20correlation+20coefficient'>
<p>Asymptotic p-value for a correlation coefficient</p></a></li>
<li><a href='#Asymptotic+20p-value+20for+20many+20correlation+20coefficients'>
<p>Asymptotic p-value for many correlation coefficients</p></a></li>
<li><a href='#Bootstrap+20p-value+20for+20the+20correlation+20coefficient'>
<p>Bootstrap p-value for the correlation coefficient</p></a></li>
<li><a href='#Hypothesis+20test+20for+20equality+20of+20two+20correlation+20coefficients'>
<p>Hypothesis test for equality of two correlation coefficients</p></a></li>
<li><a href='#Partial+20correlation+20between+20two+20variables'>
<p>Partial correlation between two variables</p></a></li>
<li><a href='#Partial+20correlation+20between+20two+20variables+20given+20a+20correlation+20matrix'>
<p>Partial correlation between two variables when a correlation matrix is given</p></a></li>
<li><a href='#Partial+20correlation+20matrix'>
<p>Partial correlation matrix</p></a></li>
<li><a href='#Permutation+20p-value+20for+20many+20correlation+20coefficients'>
<p>Permutation p-value for many correlation coefficients</p></a></li>
<li><a href='#Permutation+20p-value+20for+20the+20correlation+20coefficient'>
<p>Permutation p-value for the correlation coefficient</p></a></li>
<li><a href='#Squared+20multivariate+20correlation+20between+20two+20sets+20of+20variables'>
<p>Squared multivariate correlation between two sets of variables</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Correlation Coefficient Related Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-26</td>
</tr>
<tr>
<td>Author:</td>
<td>Michail Tsagris [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michail Tsagris &lt;mtsagris@uoc.gr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, Rfast, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Many correlation coefficient related functions are offered, such as correlations, partial correlations and hypothesis testing using asymptotic tests and computer intensive methods (bootstrap and permutation). References include Mardia K.V., Kent J.T. and Bibby J.M. (1979). "Multivariate Analysis". ISBN: 978-0124712522. London: Academic Press.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-26 18:22:14 UTC; mtsag</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-27 07:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='corrfuns-package'>
Correlation Coefficient Related Functions
</h2><span id='topic+corrfuns-package'></span>

<h3>Description</h3>

<p>Description: Many correlation coefficient related functions, such as partial correlations and
hypothesis testing. The package serves an educational purpose as well, since some functions are
written using a <b>for</b> loop and a <b>vectorised</b> version.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> corrfuns</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-10-26</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers</h3>

<p>Michail Tsagris &lt;mtsagris@uoc.gr&gt;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>

<hr>
<h2 id='Asymptotic+20p-value+20for+20a+20correlation+20coefficient'>
Asymptotic p-value for a correlation coefficient
</h2><span id='topic+correl'></span>

<h3>Description</h3>

<p>Asymptotic p-value a correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correl(y, x, type = "pearson", rho = 0, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_type">type</code></td>
<td>

<p>The type of correlation coefficient to compute, &quot;pearson&quot; or &quot;spearman&quot;.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_rho">rho</code></td>
<td>

<p>The hypothesized value of the true partial correlation.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_alpha">alpha</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fisher's transformation for the correlation coefficient is defined as
<code class="reqn">\hat{z}=\frac{1}{2}\log\frac{1+r}{1-r}</code> and its inverse is equal to <code class="reqn">
\frac{\exp\left(2\hat{z}\right)-1}{\exp\left(2\hat{z}\right)+1}</code>.
The estimated standard error of Fisher's transform is <code class="reqn">\frac{1}{\sqrt{n-3}}</code> (Efron and Tibshirani, 1993, pg. 54). If on the other hand, you choose  to calculate Spearman's correlation coefficients, the estimated standard error is slightly different <code class="reqn">\simeq \frac{ 1.029563}{\sqrt{n-3}}</code> (Fieller, Hartley and Pearson, 1957, Fieller and Pearson, 1961). R calculates confidence intervals based in a different way and does hypothesis testing for zero values only. The function calculates asymptotic confidence intervals based upon Fisher's transform, assuming asymptotic normality of the transform and performs hypothesis testing for the true (any, non only zero) value of the correlation. The sample distribution though is a <code class="reqn">t_{n-3}</code>.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>result</code></td>
<td>

<p>The correlation coefficient and the p-value for the test of zero correlation.
</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>The asymptotic <code class="reqn">(1-\alpha)\%</code> confidence interval for the true correlation coefficient.
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron B. and Tibshirani R.J. (1993). An introduction to the bootstrap. Chapman &amp; Hall/CRC.
</p>
<p>Fieller E.C., Hartley H.O. and Pearson E.S. (1957). Tests for rank correlation coefficients.
I. Biometrika, 44(3/4): 470&ndash;481.
</p>
<p>Fieller E.C. and Pearson E.S. (1961). Tests for rank correlation coefficients: II. Biometrika,
48(1/2): 29&ndash;40.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+correls">correls</a>, <a href="#topic+permcorrels">permcorrels</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- correl( iris[, 1], iris[, 2] )
</code></pre>

<hr>
<h2 id='Asymptotic+20p-value+20for+20many+20correlation+20coefficients'>
Asymptotic p-value for many correlation coefficients
</h2><span id='topic+correls'></span>

<h3>Description</h3>

<p>Asymptotic p-value for many correlation coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correls(y, x, type = "pearson", rho = 0, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_type">type</code></td>
<td>

<p>The type of correlation coefficient to compute, &quot;pearson&quot; or &quot;spearman&quot;.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_rho">rho</code></td>
<td>

<p>The hypothesized value of the true partial correlation.
</p>
</td></tr>
<tr><td><code id="Asymptotic+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_alpha">alpha</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose you have a (dependent) variable <code class="reqn">Y</code> and a matrix of <code class="reqn">p</code> variables <code class="reqn">\bf X</code> and you want to get all the correlations between <code class="reqn">Y</code> and <code class="reqn">X_i</code> for <code class="reqn">i=1,\ldots,p</code>. if you type cor(y, x) in you will get a vector of the correlations. What I offer here is confidence interval for each of the correlations, the test statistic and the p-values for the hypothesis that each of them is equal to some value <code class="reqn">\rho</code>. The p-values and test statistics are useful for meta-analysis for example, combination of the p-values in one or even to see the false discovery rate (see the package <b>fdrtool</b> by Korbinian Strimmer).
</p>


<h3>Value</h3>

<p>A matrix with 5 columns, the correlations, the test statistics, their associated p-values and the relevant <code class="reqn">(1-\alpha)\%</code> confidence intervals.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+correl">correl</a>, <a href="#topic+permcorrels">permcorrels</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(40)
x &lt;- matrix(rnorm(40 * 1000), ncol = 1000)
a &lt;- correls(y, x )
</code></pre>

<hr>
<h2 id='Bootstrap+20p-value+20for+20the+20correlation+20coefficient'>
Bootstrap p-value for the correlation coefficient
</h2><span id='topic+bootcor'></span><span id='topic+bootcor2'></span>

<h3>Description</h3>

<p>Bootstrap p-value for the correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootcor(x, B = 999)
bootcor2(x, B = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap+2B20p-value+2B20for+2B20the+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>A numerical matrix with two columns.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20p-value+2B20for+2B20the+2B20correlation+2B20coefficient_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to generate.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions perform non-parametric bootstrap hypothesis testing that the correlation coefficient is zero. A good pivotal statistic is the Fisher's transformation (see <code><a href="#topic+correl">correl</a></code>). Then the data have to be transformed under the null hypothesis (<code class="reqn">\rho=0</code>). This is doable via the eigen-analysis of the covariance matrix. We transform the bivariate data such that the covariance (and thus the correlation) matrix equals the identity matrix. remind that the correlation matrix is independent of measurements and is location free. The next step is easy, we draw bootstrap samples (from the transformed data) and every time we calculate the Fisher's transformation. The bootstrap p-value is calculated in the usual way (Davison and Hinkley, 1997).
</p>
<p>If you want to perform a non-parametric bootstrap hypothesis for a value of the correlation other than zero the procedure is similar. The data have already been transformed such that their correlation is zero. Now instead of the zeroes in the off-diagonal values of the identity matrix you will have the value of the correlation matrix you want to test. Eigen analysis of the matrix is performed and the square root of the matrix is used to multiply the transformed data. I could write a more general function to include all case, but I will leave this task to you. If you do write it please send it to me and I will put it with your name of course.
</p>
<p>The function &quot;bootcor()&quot; is a vectorised version of &quot;bootcor2()&quot;. Instead of using a <b>for</b> loop you can do things vectorised. This idea cam when I found the vectorised bootstrap correlation by Neto (2015). I cannot say I understood fully what he did, so I decided to write my own code based on the direction he pointed.
</p>
<p>Pearson's correlation coefficient of <code class="reqn">x</code> and <code class="reqn">y</code> for a sample size <code class="reqn">n</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
r = \frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}
{ \sqrt{\left(\sum_{i=1}^nx_i^2-n\bar{x}^2\right) \left(\sum_{i=1}^ny_i^2-n\bar{y}^2\right)} }
</code>
</p>

<p>So, we can see that need 5 terms to calculate, <code class="reqn">\sum_{i=1}^nx_iy_i, \bar{x}, \bar{y}, \sum_{i=1}^nx_i^2</code> and <code class="reqn">\sum_{i=1}^ny_i^2</code>. After transforming the data under the null hypothesis using the spectral decomposition we proceed as follows with <code class="reqn">B</code> number of resamples.
</p>
<p><b>Algorithm for vectorised bootstrap</b>
</p>
<p>1. Set a seed number in R. This is to make sure that the pairs of <code class="reqn">\left(x_i, y_i\right)</code> are still the same.
</p>
<p>2. Sample with replacement <code class="reqn">B \times n</code> values of <code class="reqn">x</code> and put them in a matrix with <code class="reqn">n</code> rows and <code class="reqn">B</code> columns, named <code class="reqn">X_B</code>.
</p>
<p>3. Sample with replacement <code class="reqn">B \times n</code> values of <code class="reqn">y</code> and put them in a matrix with <code class="reqn">n</code> rows and <code class="reqn">B</code> columns, names <code class="reqn">Y_B</code>.
</p>
<p>4. Calculate the mean vectors of <code class="reqn">X_B</code> and <code class="reqn">Y_B</code>.
</p>
<p>5. Calculate the sum vector of <code class="reqn">X_B^2</code> and <code class="reqn">Y_B^2</code>.
</p>
<p>6. Finally calculate the sum vector of <code class="reqn">X_B * Y_B</code>. This is the term <code class="reqn">\sum_{i=1}^nx_iy_i</code> for all resamples.
</p>
<p>So we now have 5 vectors containing the 5 terms we want. We calculate the correlation coefficient and then the Fisher's transformation (see <code><a href="#topic+correl">correl</a></code>) and so we have <code class="reqn">B</code> bootstrap test statistics. In order to see the time gain I tested both of these functions with <code class="reqn">B=9999</code> resamples and 1000 repetitions. The gain is not super wow, I would like it if it was 1/10, but even saw, it is still good. Parallelised versions reduce time to 1/3, so from this perspective, I did better. If we now put parallel inside this vectorised version, computations will be even faster. I leave this with you.
</p>
<p>But, I noticed one thing, the same thing Neto (2015) mentions. For big sample sizes, for example 1000 pairs, the time difference is not that big and perhaps a <b>for</b> loop is faster. The big difference is in the small to moderate sample sizes. At least for this example. What I mean by this is that you should not be afraid and say, then why? If I have big sample, I do not need vectorization. Maybe yes, but even then I still recommend it. Maybe someone else will have a better alternative for vectorization which is better even in the big samples, for the correlation of course. In the contour plots though, vectorised versions are always faster no matter what.
</p>


<h3>Value</h3>

<p>The correlation coefficient and the bootstrap based p-value for the test of zero correlation.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Davison A.C. and Hinkley D.V. (1997). Bootstrap methods and their application. Cambridge
university Press.
</p>
<p>Neto E.C. (2015). Speeding up non-parametric bootstrap computations for statistics based
on sample moments in small/moderate sample size applications. PloS ONE, 10(6): e0131333.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+permcor">permcor</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bootcor( iris[, 1:2] )
</code></pre>

<hr>
<h2 id='Hypothesis+20test+20for+20equality+20of+20two+20correlation+20coefficients'>
Hypothesis test for equality of two correlation coefficients
</h2><span id='topic+correls2.test'></span>

<h3>Description</h3>

<p>Hypothesis test for equality of two correlation coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correls2.test(r1, r2, n1, n2, type = "pearson")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20correlation+2B20coefficients_+3A_r1">r1</code></td>
<td>

<p>The value of the first correlation coefficient.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20correlation+2B20coefficients_+3A_r2">r2</code></td>
<td>

<p>The value of the second correlation coefficient.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20correlation+2B20coefficients_+3A_n1">n1</code></td>
<td>

<p>The sample size of the first sample from which the first correlation coefficient was computed.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20correlation+2B20coefficients_+3A_n2">n2</code></td>
<td>

<p>The sample size of the second sample from which the first correlation coefficient was computed.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20correlation+2B20coefficients_+3A_type">type</code></td>
<td>

<p>The type of correlation coefficients, &quot;pearson&quot; or &quot;spearman&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The test statistic for the hypothesis of equality of two correlation coefficients is the following:
</p>
<p style="text-align: center;"><code class="reqn">
Z=\frac{\hat{z}_1-\hat{z}_2}{\sqrt{1/\left(n1-3\right)+1/\left(n2-3\right)}},
</code>
</p>

<p>where <code class="reqn">\hat{z}_1</code> and <code class="reqn">\hat{z}_2</code> denote the Fisher's transformation (see <code><a href="#topic+correl">correl</a></code> applied to the two correlation coefficients and <code class="reqn">n_1</code> and <code class="reqn">n_2</code> denote the  sample sizes of the two correlation coefficients. The denominator is the sum of the variances of the two coefficients and as you can see we used a different variance estimator than the one we used before. This function performs hypothesis testing for the equality of two correlation coefficients. The result is the calculated p-value from the standard normal distribution.
</p>


<h3>Value</h3>

<p>The test statistic and its associated p-value for the test of equal correlations.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+correl">correl</a>, <a href="#topic+correls">correls</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(40)
x &lt;- matrix(rnorm(40 * 1000), ncol = 1000)
a &lt;- correls(y, x )
</code></pre>

<hr>
<h2 id='Partial+20correlation+20between+20two+20variables'>
Partial correlation between two variables
</h2><span id='topic+partialcor2'></span>

<h3>Description</h3>

<p>Partial correlation between two variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partialcor2(y, x, z, type = "pearson", rho = 0, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_z">z</code></td>
<td>

<p>A numerical vector or a numerical matrix.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_type">type</code></td>
<td>

<p>The type of partial correlation coefficient to compute, &quot;pearson&quot; or &quot;spearman&quot;.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_rho">rho</code></td>
<td>

<p>The hypothesized value of the true partial correlation.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_alpha">alpha</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose you want to calculate the correlation coefficient between two variables controlling for the effect of (or conditioning on) one or more other variables. So you cant to calculate <code class="reqn">\hat{\rho}\left(X,Y|{\bf Z}\right)</code>, where <code class="reqn">\bf Z</code> is a matrix, since it does not have to be just one variable. This idea was captures by Ronald Fisher some years ago. To calculate it, one can use linear regression as follows.
</p>
<p>1. Calculate the residuals <code class="reqn">\hat{e}_x</code> from the linear regression <code class="reqn">X=a+bZ</code>.
</p>
<p>2. Calculate the residuals <code class="reqn">\hat{e}_y</code> from the linear regression <code class="reqn">Y=c+dZ</code>.
</p>
<p>3. Calculate the correlation between <code class="reqn">\hat{e}_x</code> and <code class="reqn">\hat{e}_y</code>. This is the partial correlation coefficient between <code class="reqn">X</code> and <code class="reqn">Y</code> controlling for <code class="reqn">\bf Z</code>.
</p>
<p>The standard error of the Fisher's transformation of the sample partial correlation is Anderson (2003):
<code class="reqn">\text{SE}\left(\frac{1}{2}\log{\frac{1+\hat{\rho}\left(X,Y|{\bf Z}\right)}{1-\hat{\rho}\left(X,Y|{\bf Z}\right)}}\right)=\frac{1}{n-d-3}</code>, where <code class="reqn">n</code> is the sample size and <code class="reqn">d</code> is the number of variables upon which we control. The standard error is very similar to the one of the classical correlation coefficient. In fact, the latter one is a special case of the first when <code class="reqn">d=0</code> and thus there is no variable whose effect is to be controlled.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>result</code></td>
<td>

<p>The partial correlation coefficient and the p-value for the test of zero partial correlation.
</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>The asymptotic <code class="reqn">(1-\alpha)\%</code> confidence interval for the true partial correlation coefficient.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+partialcor">partialcor</a>, <a href="#topic+pcormat">pcormat</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- iris[, 1:4]
partialcor2(x[, 1], x[, 2], x[, 3:4])
</code></pre>

<hr>
<h2 id='Partial+20correlation+20between+20two+20variables+20given+20a+20correlation+20matrix'>
Partial correlation between two variables when a correlation matrix is given
</h2><span id='topic+partialcor'></span>

<h3>Description</h3>

<p>Partial correlation between two variables when a correlation matrix is given.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partialcor(R, indx, indy, indz, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables+2B20given+2B20a+2B20correlation+2B20matrix_+3A_r">R</code></td>
<td>

<p>A correlation matrix.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables+2B20given+2B20a+2B20correlation+2B20matrix_+3A_indx">indx</code></td>
<td>

<p>The index of the first variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables+2B20given+2B20a+2B20correlation+2B20matrix_+3A_indy">indy</code></td>
<td>

<p>The index of the second variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables+2B20given+2B20a+2B20correlation+2B20matrix_+3A_indz">indz</code></td>
<td>

<p>The index of the conditioning variables.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables+2B20given+2B20a+2B20correlation+2B20matrix_+3A_n">n</code></td>
<td>

<p>The sample size of the data from which the correlation matrix was computed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose you want to calculate the correlation coefficient between two variables controlling for the effect of (or conditioning on) one or more other variables. So you cant to calculate <code class="reqn">\hat{\rho}\left(X,Y|{\bf Z}\right)</code>, where <code class="reqn">\bf Z</code> is a matrix, since it does not have to be just one variable. Using the correlation matrix <code class="reqn">R</code> we can do the following:
</p>
<p style="text-align: center;"><code class="reqn">
r_{X,Y|{\bf Z}}=
{
\begin{array}{cc}
\frac{R_{X,Y} - R_{X, {\bf Z}} R_{Y,{\bf Z}}}{
\sqrt{ \left(1 - R_{X,{\bf Z}}^2\right)^T \left(1 - R_{Y,{\bf Z}}^2\right) }} &amp; \text{if} \ \ |{\bf Z}|=1 \\
-\frac{ {\bf A}_{1,2} }{ \sqrt{{\bf A}_{1,1}{\bf A}_{2,2}} } &amp; \text{if} \ \ |{\bf Z}| &gt; 1
\end{array}
}
</code>
</p>

<p>The <code class="reqn">R_{X,Y}</code> is the correlation between variables <code class="reqn">X</code> and <code class="reqn">Y</code>, <code class="reqn">R_{X,{\bf Z}}</code> and <code class="reqn">R_{Y,{\bf Z}}</code> denote the correlations between <code class="reqn">X</code> &amp; <code class="reqn">\bf Z</code> and <code class="reqn">Y</code> &amp; <code class="reqn">\bf Z</code>, <code class="reqn">{\bf A}={\bf R}_{X,Y,{\bf Z}}^{-1}</code>, with <code class="reqn">\bf A</code> denoting the correlation sub-matrix of variables <code class="reqn">X, Y, {\bf Z}</code> and <code class="reqn">A_{i,j}</code> denotes the element in the <code class="reqn">i</code>-th row and <code class="reqn">j</code>-th column of matrix <code class="reqn">A</code>. The <code class="reqn">|{\bf Z}|</code> denotes the cardinality of <code class="reqn">\bf Z</code>, i.e. the number of variables.
</p>


<h3>Value</h3>

<p>The partial correlation coefficient and the p-value for the test of zero partial correlation.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+partialcor2">partialcor2</a>, <a href="#topic+pcormat">pcormat</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>r &lt;- cor(iris[, 1:4])
partialcor(r, 1, 2, 3:4, 150)
</code></pre>

<hr>
<h2 id='Partial+20correlation+20matrix'>
Partial correlation matrix
</h2><span id='topic+pcormat'></span>

<h3>Description</h3>

<p>Partial correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcormat(x, type = "pearson")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20matrix_+3A_x">x</code></td>
<td>

<p>A numerical matrix with two columns.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20matrix_+3A_type">type</code></td>
<td>

<p>The type of the partial correlation matrix to compute, &quot;pearson&quot; or &quot;spearman&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the partial correlation matrix. Given a correlation matrix, it will return the partial correlation matrix. Each entry in the final matrix, is the partial correlation matrix between a pair of variables given all the rest variables.
</p>


<h3>Value</h3>

<p>The partial correlation matrix.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+partialcor">partialcor</a>, <a href="#topic+partialcor2">partialcor2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pcormat( iris[, 1:4] )
</code></pre>

<hr>
<h2 id='Permutation+20p-value+20for+20many+20correlation+20coefficients'>
Permutation p-value for many correlation coefficients
</h2><span id='topic+permcorrels'></span>

<h3>Description</h3>

<p>Permutation p-value for many correlation coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permcorrels(y, x, B = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_x">x</code></td>
<td>

<p>A numerical matrix with many columns.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20many+2B20correlation+2B20coefficients_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to generate.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the same function as <code><a href="#topic+correls">correls</a></code>, only this time the p-values are produced via permutations and no confidence intervals are produced.
</p>


<h3>Value</h3>

<p>A matrix with 2 columns, the correlations and their permutation based p-values.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+permcor">permcor</a>, <a href="#topic+correls">correls</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(40)
x &lt;- matrix(rnorm(40 * 1000), ncol = 1000)
a &lt;- permcorrels(y, x )
</code></pre>

<hr>
<h2 id='Permutation+20p-value+20for+20the+20correlation+20coefficient'>
Permutation p-value for the correlation coefficient
</h2><span id='topic+permcor'></span><span id='topic+permcor2'></span><span id='topic+permcor3'></span>

<h3>Description</h3>

<p>Permutation p-value for the correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permcor(x, B = 999, fast = TRUE)
permcor2(x, B = 999)
permcor3(x, B = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20the+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>A numerical matrix with two columns.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20the+2B20correlation+2B20coefficient_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to generate.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20p-value+2B20for+2B20the+2B20correlation+2B20coefficient_+3A_fast">fast</code></td>
<td>

<p>If you want the C++ implementation leave this TRUE. It is about 2 times faster.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are permutation based p-values for the test that the true correlation coefficient is zero. The function &quot;permcor2()&quot; is a vectorised version of &quot;permcor3()&quot;, whereas the function &quot;permcor() does the following.
</p>
<p>Instead of transforming the data under the null hypothesis and re-sampling with replacement we can permute the observations. Te basic difference is that the data are assumed to be under the null hypothesis already. Secondly, what we have to do, is to destroy the pairs. For example, the pairs (a, b), (c, d) and (e, f) in one permutation they can be (c, b), (a, f) and (e, d). And this thing will happen many times, say <code class="reqn">B=999</code>. Then we have <code class="reqn">B</code> pseudo-samples again. Everything else is the same as in the bootstrap case. A trick is that we need not change the order of both variables, just the one is enough. This will sped up the process. And guess what, it is faster than bootstrap. It does not require the data to be transformed under the null hypothesis and you only need to permute one variable, in contrast to the bootstrap case, where you must resample from both variables. See Chatzipantsiou et al. (2019) for more details.
</p>


<h3>Value</h3>

<p>The correlation coefficient and the permutation based p-value for the test of zero correlation.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Chatzipantsiou C., Dimitriadis M., Papadakis M. and Tsagris M. (2019). Extremely efficient permutation and bootstrap hypothesis tests using R. Journal of Modern Applied Statistical Methods, 18(2): eP2898.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bootcor">bootcor</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>permcor( iris[, 1:2] )
</code></pre>

<hr>
<h2 id='Squared+20multivariate+20correlation+20between+20two+20sets+20of+20variables'>
Squared multivariate correlation between two sets of variables
</h2><span id='topic+sq.correl'></span>

<h3>Description</h3>

<p>Squared multivariate correlation between two sets of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sq.correl(y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Squared+2B20multivariate+2B20correlation+2B20between+2B20two+2B20sets+2B20of+2B20variables_+3A_y">y</code></td>
<td>

<p>A numerical matrix.
</p>
</td></tr>
<tr><td><code id="Squared+2B20multivariate+2B20correlation+2B20between+2B20two+2B20sets+2B20of+2B20variables_+3A_x">x</code></td>
<td>

<p>A numerical matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mardia, Kent and Bibby (1979, pg. 171) defined two squared multiple correlation coefficient between the dependent variable <code class="reqn">\bf Y</code> and the independent variable <code class="reqn">\bf X</code>. They mention that these are a similar measure of the coefficient determination in the univariate regression. Assume that the multivariate regression model is written as <code class="reqn">{\bf Y}={\bf XB}+{\bf U}</code>, where <code class="reqn">\bf U</code> is the matrix of residuals. Then, they write <code class="reqn">{\bf D}=\left({\bf Y}^T{\bf Y}\right)^{-1}\hat{\bf U}^T\hat{\bf U}</code>, with <code class="reqn">\hat{\bf U}^T\hat{\bf U}={\bf Y}^T{\bf PY}</code> and <code class="reqn">\bf P</code> is <code class="reqn">{\bf P}={\bf I}_n-{\bf X}\left({\bf X}^T{\bf X}\right)^{-1}{\bf X}^T</code>. The matrix <code class="reqn">\bf D</code> is a generalization of <code class="reqn">1-R^2</code> in the univariate case. Mardia, Kent and Bibby (1979, pg. 171) mentioned that the dependent variable <code class="reqn">\bf Y</code> has to be centred.
</p>
<p>The squared multivariate correlation should lie between 0 and 1 and this property is satisfied by the trace correlation <code class="reqn">r_T</code> and the determinant correlation <code class="reqn">r_D</code>, defined as
<code class="reqn">r^2_T=d^{-1}\text{tr}\left({\bf I}-{\bf D}\right)</code> and <code class="reqn">r^2_D=\text{det}\left({\bf I}-{\bf D}\right)</code>
respectively, where <code class="reqn">d</code> denotes the dimensionality of <code class="reqn">\bf Y</code>. So, high values indicate high proportion of variance of the dependent variables explained. Alternatively, one can calculate the trace and the determinant of the matrix <code class="reqn">{\bf E}=\left({\bf Y}^T{\bf Y}\right)^{-1}\hat{\bf Y}^T\hat{\bf Y}</code>. Try something else also, use the function &quot;sq.correl()&quot; in a univariate regression example and then calculate the <code class="reqn">R^2</code> for the same dataset. Try this example again but without centering the dependent variable. In addition, take two variables and calculate their squared correlation coefficient and then square it and using &quot;sq.correl()&quot;.
</p>


<h3>Value</h3>

<p>A vector with two values, the trace and determinant <code class="reqn">R^2</code>.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+correl">correl</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sq.correl( iris[, 1:2], iris[, 3:4] )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
