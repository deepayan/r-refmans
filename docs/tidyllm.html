<!DOCTYPE html><html lang="en"><head><title>Help for package tidyllm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidyllm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tidyllm-package'><p>tidyllm: Tidy Integration of Large Language Models</p></a></li>
<li><a href='#azure_openai'><p>Azure OpenAI Endpoint Provider Function</p></a></li>
<li><a href='#azure_openai_chat'><p>Send LLM Messages to an OpenAI Chat Completions endpoint on Azure</p></a></li>
<li><a href='#azure_openai_embedding'><p>Generate Embeddings Using OpenAI API on Azure</p></a></li>
<li><a href='#cancel_openai_batch'><p>Cancel an In-Progress OpenAI Batch</p></a></li>
<li><a href='#chat'><p>Chat with a Language Model</p></a></li>
<li><a href='#chatgpt'><p>Alias for the OpenAI Provider Function</p></a></li>
<li><a href='#check_azure_openai_batch'><p>Check Batch Processing Status for Azure OpenAI Batch API</p></a></li>
<li><a href='#check_batch'><p>Check Batch Processing Status</p></a></li>
<li><a href='#check_claude_batch'><p>Check Batch Processing Status for Claude API</p></a></li>
<li><a href='#check_mistral_batch'><p>Check Batch Processing Status for Mistral Batch API</p></a></li>
<li><a href='#check_openai_batch'><p>Check Batch Processing Status for OpenAI Batch API</p></a></li>
<li><a href='#claude'><p>Provider Function for Claude models on the Anthropic API</p></a></li>
<li><a href='#claude_chat'><p>Interact with Claude AI models via the Anthropic API</p></a></li>
<li><a href='#claude_list_models'><p>List Available Models from the Anthropic Claude API</p></a></li>
<li><a href='#deepseek'><p>Deepseek Provider Function</p></a></li>
<li><a href='#deepseek_chat'><p>Send LLM Messages to the DeepSeek Chat API</p></a></li>
<li><a href='#df_llm_message'><p>Convert a Data Frame to an LLMMessage Object</p></a></li>
<li><a href='#embed'><p>Generate text embeddings</p></a></li>
<li><a href='#fetch_azure_openai_batch'><p>Fetch Results for an Azure OpenAI Batch</p></a></li>
<li><a href='#fetch_batch'><p>Fetch Results from a Batch API</p></a></li>
<li><a href='#fetch_claude_batch'><p>Fetch Results for a Claude Batch</p></a></li>
<li><a href='#fetch_mistral_batch'><p>Fetch Results for an Mistral Batch</p></a></li>
<li><a href='#fetch_openai_batch'><p>Fetch Results for an OpenAI Batch</p></a></li>
<li><a href='#field_chr'><p>Define Field Descriptors for JSON Schema</p></a></li>
<li><a href='#gemini'><p>Google Gemini Provider Function</p></a></li>
<li><a href='#gemini_chat'><p>Send LLMMessage to Gemini API</p></a></li>
<li><a href='#gemini_delete_file'><p>Delete a File from Gemini API</p></a></li>
<li><a href='#gemini_embedding'><p>Generate Embeddings Using the Google Gemini API</p></a></li>
<li><a href='#gemini_file_metadata'><p>Retrieve Metadata for a File from Gemini API</p></a></li>
<li><a href='#gemini_list_files'><p>List Files in Gemini API</p></a></li>
<li><a href='#gemini_upload_file'><p>Upload a File to Gemini API</p></a></li>
<li><a href='#get_logprobs'><p>Retrieve Log Probabilities from Assistant Replies</p></a></li>
<li><a href='#get_metadata'><p>Retrieve Metadata from Assistant Replies</p></a></li>
<li><a href='#get_reply'><p>Retrieve Assistant Reply as Text</p></a></li>
<li><a href='#get_reply_data'><p>Retrieve Assistant Reply as Structured Data</p></a></li>
<li><a href='#get_user_message'><p>Retrieve a User Message by Index</p></a></li>
<li><a href='#groq'><p>Groq API Provider Function</p></a></li>
<li><a href='#groq_chat'><p>Send LLM Messages to the Groq Chat API</p></a></li>
<li><a href='#groq_list_models'><p>List Available Models from the Groq API</p></a></li>
<li><a href='#groq_transcribe'><p>Transcribe an Audio File Using Groq transcription API</p></a></li>
<li><a href='#img'><p>Create an  Image Object</p></a></li>
<li><a href='#list_azure_openai_batches'><p>List Azure OpenAI Batch Requests</p></a></li>
<li><a href='#list_batches'><p>List all Batch Requests on a Batch API</p></a></li>
<li><a href='#list_claude_batches'><p>List Claude Batch Requests</p></a></li>
<li><a href='#list_mistral_batches'><p>List Mistral Batch Requests</p></a></li>
<li><a href='#list_models'><p>List Available Models for a Provider</p></a></li>
<li><a href='#list_openai_batches'><p>List OpenAI Batch Requests</p></a></li>
<li><a href='#llm_message'><p>Create or Update Large Language Model Message Object</p></a></li>
<li><a href='#LLMMessage'><p>Large Language Model Message Class</p></a></li>
<li><a href='#mistral'><p>Mistral Provider Function</p></a></li>
<li><a href='#mistral_chat'><p>Send LLMMessage to Mistral API</p></a></li>
<li><a href='#mistral_embedding'><p>Generate Embeddings Using Mistral API</p></a></li>
<li><a href='#mistral_list_models'><p>List Available Models from the Mistral API</p></a></li>
<li><a href='#ollama'><p>Ollama API Provider Function</p></a></li>
<li><a href='#ollama_chat'><p>Interact with local AI models via the Ollama API</p></a></li>
<li><a href='#ollama_delete_model'><p>Delete a model from the Ollama API</p></a></li>
<li><a href='#ollama_download_model'><p>Download a model from the Ollama API</p></a></li>
<li><a href='#ollama_embedding'><p>Generate Embeddings Using Ollama API</p></a></li>
<li><a href='#ollama_list_models'><p>Retrieve and return model information from the Ollama API</p></a></li>
<li><a href='#openai'><p>OpenAI Provider Function</p></a></li>
<li><a href='#openai_chat'><p>Send LLM Messages to the OpenAI Chat Completions API</p></a></li>
<li><a href='#openai_embedding'><p>Generate Embeddings Using OpenAI API</p></a></li>
<li><a href='#openai_list_models'><p>List Available Models from the OpenAI API</p></a></li>
<li><a href='#pdf_page_batch'><p>Batch Process PDF into LLM Messages</p></a></li>
<li><a href='#perplexity'><p>Perplexity Provider Function</p></a></li>
<li><a href='#perplexity_chat'><p>Send LLM Messages to the Perplexity Chat API</p></a></li>
<li><a href='#rate_limit_info'><p>Get the current rate limit information for all or a specific API</p></a></li>
<li><a href='#send_azure_openai_batch'><p>Send a Batch of Messages to Azure OpenAI Batch API</p></a></li>
<li><a href='#send_batch'><p>Send a batch of messages to a batch API</p></a></li>
<li><a href='#send_claude_batch'><p>Send a Batch of Messages to Claude API</p></a></li>
<li><a href='#send_mistral_batch'><p>Send a Batch of Requests to the Mistral API</p></a></li>
<li><a href='#send_ollama_batch'><p>Send a Batch of Messages to Ollama API</p></a></li>
<li><a href='#send_openai_batch'><p>Send a Batch of Messages to OpenAI Batch API</p></a></li>
<li><a href='#tidyllm_schema'><p>Create a JSON Schema for Structured Outputs</p></a></li>
<li><a href='#tidyllm_tool'><p>Create a Tool Definition for tidyllm</p></a></li>
<li><a href='#voyage'><p>Voyage Provider Function</p></a></li>
<li><a href='#voyage_embedding'><p>Generate Embeddings Using Voyage AI API</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Tidy Integration of Large Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.2</td>
</tr>
<tr>
<td>Description:</td>
<td>A tidy interface for integrating large language model (LLM) APIs such as 'Claude', 'Openai', 'Groq','Mistral' and local models via 'Ollama' into R workflows. The package supports text and media-based interactions, interactive message history, batch request APIs, and a tidy, pipeline-oriented interface for streamlined integration into data workflows. Web services are available at <a href="https://www.anthropic.com">https://www.anthropic.com</a>, <a href="https://openai.com">https://openai.com</a>, <a href="https://groq.com">https://groq.com</a>, <a href="https://mistral.ai/">https://mistral.ai/</a> and <a href="https://ollama.com">https://ollama.com</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://edubruell.github.io/tidyllm/">https://edubruell.github.io/tidyllm/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/edubruell/tidyllm/issues">https://github.com/edubruell/tidyllm/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0), tidyverse, httptest2,
httpuv, ellmer</td>
</tr>
<tr>
<td>Imports:</td>
<td>S7 (&ge; 0.2.0), base64enc, glue, jsonlite, curl, httr2,
lubridate, purrr, rlang, stringr, grDevices, pdftools, tibble,
cli, png, lifecycle</td>
</tr>
<tr>
<td>Collate:</td>
<td>'tidyllm-package.R' 'utilites.R' 'embedding_helpers.R'
'LLMMessage.R' 'APIProvider.R' 'llm_message.R' 'llm_verbs.R'
'media.R' 'message_retrieval.R' 'perform_api_requests.R'
'rate_limits.R' 'tidyllm_schema.R' 'tools.R' 'pdfbatch.R'
'api_openai.R' 'api_claude.R' 'api_gemini.R' 'api_ollama.R'
'api_azure_openai.R' 'api_groq.R' 'api_mistral.R'
'api_perplexity.R' 'api_deepseek.R' 'api_voyage.R' 'zzz.R'</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-07 21:58:28 UTC; ebr</td>
</tr>
<tr>
<td>Author:</td>
<td>Eduard Brüll [aut, cre],
  Jia Zhang [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eduard Brüll &lt;eduard.bruell@zew.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-08 00:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='tidyllm-package'>tidyllm: Tidy Integration of Large Language Models</h2><span id='topic+tidyllm'></span><span id='topic+tidyllm-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>A tidy interface for integrating large language model (LLM) APIs such as 'Claude', 'Openai', 'Groq','Mistral' and local models via 'Ollama' into R workflows. The package supports text and media-based interactions, interactive message history, batch request APIs, and a tidy, pipeline-oriented interface for streamlined integration into data workflows. Web services are available at <a href="https://www.anthropic.com">https://www.anthropic.com</a>, <a href="https://openai.com">https://openai.com</a>, <a href="https://groq.com">https://groq.com</a>, <a href="https://mistral.ai/">https://mistral.ai/</a> and <a href="https://ollama.com">https://ollama.com</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Eduard Brüll <a href="mailto:eduard.bruell@zew.de">eduard.bruell@zew.de</a>
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Jia Zhang [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://edubruell.github.io/tidyllm/">https://edubruell.github.io/tidyllm/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/edubruell/tidyllm/issues">https://github.com/edubruell/tidyllm/issues</a>
</p>
</li></ul>


<hr>
<h2 id='azure_openai'>Azure OpenAI Endpoint Provider Function</h2><span id='topic+azure_openai'></span>

<h3>Description</h3>

<p>The <code>azure_openai()</code> function acts as an interface for interacting with the Azure OpenAI API
through main <code>tidyllm</code> verbs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>azure_openai(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="azure_openai_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the Azure OpenAI API specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="azure_openai_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies which action (e.g.,
<code>chat</code>) the function is being invoked from.
This argument is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>azure_openai()</code> currently routes messages only to <code>azure_openai_chat()</code> when used with <code>chat()</code>.
</p>
<p><code>send_batch()</code>. It dynamically routes requests to OpenAI-specific functions
like <code>azure_openai_chat()</code> and <code>azure_openai_embedding()</code> based on the context of the call.
</p>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(currently, only an updated <code>LLMMessage</code> object for <code>azure_openai_chat()</code>).
</p>

<hr>
<h2 id='azure_openai_chat'>Send LLM Messages to an OpenAI Chat Completions endpoint on Azure</h2><span id='topic+azure_openai_chat'></span>

<h3>Description</h3>

<p>This function sends a message history to the Azure OpenAI Chat Completions API and returns the assistant's reply.
This function is work in progress and not fully tested
</p>


<h3>Usage</h3>

<pre><code class='language-R'>azure_openai_chat(
  .llm,
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .deployment = "gpt-4o-mini",
  .api_version = "2024-08-01-preview",
  .max_completion_tokens = NULL,
  .frequency_penalty = NULL,
  .logit_bias = NULL,
  .logprobs = FALSE,
  .top_logprobs = NULL,
  .presence_penalty = NULL,
  .seed = NULL,
  .stop = NULL,
  .stream = FALSE,
  .temperature = NULL,
  .top_p = NULL,
  .timeout = 60,
  .verbose = FALSE,
  .json_schema = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .tools = NULL,
  .tool_choice = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="azure_openai_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.deployment">.deployment</code></td>
<td>
<p>The identifier of the model that is deployed (default: &quot;gpt-4o-mini&quot;).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.api_version">.api_version</code></td>
<td>
<p>Which version of the API is deployed (default: &quot;2024-10-01-preview&quot;)</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.max_completion_tokens">.max_completion_tokens</code></td>
<td>
<p>An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.logit_bias">.logit_bias</code></td>
<td>
<p>A named list modifying the likelihood of specified tokens appearing in the completion.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.logprobs">.logprobs</code></td>
<td>
<p>Whether to return log probabilities of the output tokens (default: FALSE).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.top_logprobs">.top_logprobs</code></td>
<td>
<p>An integer between 0 and 20 specifying the number of most likely tokens to return at each token position.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.seed">.seed</code></td>
<td>
<p>If specified, the system will make a best effort to sample deterministically.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.stop">.stop</code></td>
<td>
<p>Up to 4 sequences where the API will stop generating further tokens.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.stream">.stream</code></td>
<td>
<p>If set to TRUE, the answer will be streamed to console as it comes (default: FALSE).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>What sampling temperature to use, between 0 and 2. Higher values make the output more random.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>An alternative to sampling with temperature, called nucleus sampling.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Should additional information be shown after the API call (default: FALSE).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object as R list to enforce the output structure (If defined has precedence over JSON mode).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object (default: FALSE).</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform request</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="azure_openai_chat_+3A_.tool_choice">.tool_choice</code></td>
<td>
<p>A character string specifying the tool-calling behavior; valid values are &quot;none&quot;, &quot;auto&quot;, or &quot;required&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage
msg &lt;- llm_message("What is R programming?")
result &lt;- azure_openai_chat(msg)

# With custom parameters
result2 &lt;- azure_openai_chat(msg, 
                 .deployment = "gpt-4o-mini",
                 .temperature = 0.7, 
                 .max_tokens = 1000)

## End(Not run)

</code></pre>

<hr>
<h2 id='azure_openai_embedding'>Generate Embeddings Using OpenAI API on Azure</h2><span id='topic+azure_openai_embedding'></span>

<h3>Description</h3>

<p>Generate Embeddings Using OpenAI API on Azure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>azure_openai_embedding(
  .input,
  .deployment = "text-embedding-3-small",
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .api_version = "2023-05-15",
  .truncate = TRUE,
  .timeout = 120,
  .dry_run = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="azure_openai_embedding_+3A_.input">.input</code></td>
<td>
<p>A character vector of texts to embed or an <code>LLMMesssage</code>object</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.deployment">.deployment</code></td>
<td>
<p>The embedding model identifier (default: &quot;text-embedding-3-small&quot;).</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.api_version">.api_version</code></td>
<td>
<p>What API-Version othe Azure OpenAI API should be used (default: &quot;2023-05-15&quot;)</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.truncate">.truncate</code></td>
<td>
<p>Whether to truncate inputs to fit the model's context length (default: TRUE).</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
<tr><td><code id="azure_openai_embedding_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns: <code>input</code> and <code>embeddings</code>.
The <code>input</code> column contains the texts sent to embed, and the <code>embeddings</code> column
is a list column where each row contains an embedding vector of the sent input.
</p>

<hr>
<h2 id='cancel_openai_batch'>Cancel an In-Progress OpenAI Batch</h2><span id='topic+cancel_openai_batch'></span>

<h3>Description</h3>

<p>This function cancels an in-progress batch created through the OpenAI API.
The batch will be moved to a &quot;cancelling&quot; state and, eventually, &quot;cancelled.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cancel_openai_batch(.batch_id, .dry_run = FALSE, .max_tries = 3, .timeout = 60)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cancel_openai_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>Character; the unique identifier for the batch to cancel.</p>
</td></tr>
<tr><td><code id="cancel_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="cancel_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="cancel_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the response from the OpenAI API about the cancellation status.
</p>

<hr>
<h2 id='chat'>Chat with a Language Model</h2><span id='topic+chat'></span>

<h3>Description</h3>

<p>The <code>chat()</code> function sends a message to a language model via a specified provider and returns the response.
It routes the provided <code>LLMMessage</code> object to the appropriate provider-specific chat function,
while allowing for the specification of common arguments applicable across different providers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat(
  .llm,
  .provider = getOption("tidyllm_chat_default"),
  .dry_run = NULL,
  .stream = NULL,
  .temperature = NULL,
  .timeout = NULL,
  .top_p = NULL,
  .max_tries = NULL,
  .model = NULL,
  .verbose = NULL,
  .json_schema = NULL,
  .tools = NULL,
  .seed = NULL,
  .stop = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the message or conversation history to send to the language model.</p>
</td></tr>
<tr><td><code id="chat_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>claude()</code>, etc.
You can also set a default provider function via the <code>tidyllm_chat_default</code> option.</p>
</td></tr>
<tr><td><code id="chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, simulates the request without sending it to the provider. Useful for testing.</p>
</td></tr>
<tr><td><code id="chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; if <code>TRUE</code>, streams the response from the provider in real-time.</p>
</td></tr>
<tr><td><code id="chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Numeric; controls the randomness of the model's output (0 = deterministic).</p>
</td></tr>
<tr><td><code id="chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Numeric; the maximum time (in seconds) to wait for a response.</p>
</td></tr>
<tr><td><code id="chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Numeric; nucleus sampling parameter, which limits the sampling to the top cumulative probability <code>p</code>.</p>
</td></tr>
<tr><td><code id="chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; the maximum number of retries for failed requests.</p>
</td></tr>
<tr><td><code id="chat_+3A_.model">.model</code></td>
<td>
<p>Character; the model identifier to use (e.g., <code>"gpt-4"</code>).</p>
</td></tr>
<tr><td><code id="chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if <code>TRUE</code>, prints additional information about the request and response.</p>
</td></tr>
<tr><td><code id="chat_+3A_.json_schema">.json_schema</code></td>
<td>
<p>List; A JSON schema object as R list to enforce the output structure</p>
</td></tr>
<tr><td><code id="chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="chat_+3A_.seed">.seed</code></td>
<td>
<p>Integer; sets a random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="chat_+3A_.stop">.stop</code></td>
<td>
<p>Character vector; specifies sequences where the model should stop generating further tokens.</p>
</td></tr>
<tr><td><code id="chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Numeric; adjusts the likelihood of repeating tokens (positive values decrease repetition).</p>
</td></tr>
<tr><td><code id="chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Numeric; adjusts the likelihood of introducing new tokens (positive values encourage novelty).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>chat()</code> function provides a unified interface for interacting with different language model providers.
Common arguments such as <code>.temperature</code>, <code>.model</code>, and <code>.stream</code> are supported by most providers and can be
passed directly to <code>chat()</code>. If a provider does not support a particular argument, an error will be raised.
</p>
<p>Advanced provider-specific configurations can be accessed via the provider functions.
</p>


<h3>Value</h3>

<p>An updated <code>LLMMessage</code> object containing the response from the language model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage with OpenAI provider
llm_message("Hello World") |&gt;
   chat(ollama(.ollama_server = "https://my-ollama-server.de"),.model="mixtral")
   
   chat(mistral,.model="mixtral")

# Use streaming with Claude provider
llm_message("Tell me a story") |&gt;
   chat(claude(),.stream=TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='chatgpt'>Alias for the OpenAI Provider Function</h2><span id='topic+chatgpt'></span>

<h3>Description</h3>

<p>The <code>chatgpt</code> function is an alias for the <code>openai()</code> provider function.
It provides a convenient way to interact with the OpenAI API for tasks such
as sending chat messages, generating embeddings, and handling batch operations
using <code>tidyllm</code> verbs like <code>chat()</code>, <code>embed()</code>, and <code>send_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chatgpt(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chatgpt_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate OpenAI-specific function,
such as model configuration, input text, or other API-specific options.</p>
</td></tr>
<tr><td><code id="chatgpt_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies the context (e.g.,
<code>chat</code>, <code>embed</code>, <code>send_batch</code>) in which the function is being
invoked. This is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>, or a matrix for <code>embed()</code>).
</p>

<hr>
<h2 id='check_azure_openai_batch'>Check Batch Processing Status for Azure OpenAI Batch API</h2><span id='topic+check_azure_openai_batch'></span>

<h3>Description</h3>

<p>This function retrieves the processing status and other details of a specified Azure OpenAI batch ID
from the Azure OpenAI Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_azure_openai_batch(
  .llms = NULL,
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_azure_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects.</p>
</td></tr>
<tr><td><code id="check_azure_openai_batch_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="check_azure_openai_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>A manually set batch ID.</p>
</td></tr>
<tr><td><code id="check_azure_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="check_azure_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request (default: 3).</p>
</td></tr>
<tr><td><code id="check_azure_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing.
</p>

<hr>
<h2 id='check_batch'>Check Batch Processing Status</h2><span id='topic+check_batch'></span>

<h3>Description</h3>

<p>This function retrieves the processing status and other details of a specified
batchid or a list of <code>LLMMessage</code> objects with batch attribute.
It routes the input to the appropriate provider-specific batch API function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_batch(
  .llms,
  .provider = getOption("tidyllm_cbatch_default"),
  .dry_run = NULL,
  .max_tries = NULL,
  .timeout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects or a character vector with a batch ID.</p>
</td></tr>
<tr><td><code id="check_batch_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>claude()</code>, etc.
You can also set a default provider function via the <code>tidyllm_cbatch_default</code> option.</p>
</td></tr>
<tr><td><code id="check_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it</p>
</td></tr>
<tr><td><code id="check_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request</p>
</td></tr>
<tr><td><code id="check_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing.
</p>

<hr>
<h2 id='check_claude_batch'>Check Batch Processing Status for Claude API</h2><span id='topic+check_claude_batch'></span>

<h3>Description</h3>

<p>This function retrieves the processing status and other details of a specified Claude batch ID
from the Claude API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_claude_batch(
  .llms = NULL,
  .batch_id = NULL,
  .api_url = "https://api.anthropic.com/",
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_claude_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects</p>
</td></tr>
<tr><td><code id="check_claude_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>A manually set batchid</p>
</td></tr>
<tr><td><code id="check_claude_batch_+3A_.api_url">.api_url</code></td>
<td>
<p>Character; base URL of the Claude API (default: &quot;https://api.anthropic.com/&quot;).</p>
</td></tr>
<tr><td><code id="check_claude_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="check_claude_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
<tr><td><code id="check_claude_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing
</p>

<hr>
<h2 id='check_mistral_batch'>Check Batch Processing Status for Mistral Batch API</h2><span id='topic+check_mistral_batch'></span>

<h3>Description</h3>

<p>This function retrieves the processing status and other details of a specified Mistral batch ID
from the Mistral Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_mistral_batch(
  .llms = NULL,
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_mistral_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects.</p>
</td></tr>
<tr><td><code id="check_mistral_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>A manually set batch ID.</p>
</td></tr>
<tr><td><code id="check_mistral_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="check_mistral_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request (default: 3).</p>
</td></tr>
<tr><td><code id="check_mistral_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing.
</p>

<hr>
<h2 id='check_openai_batch'>Check Batch Processing Status for OpenAI Batch API</h2><span id='topic+check_openai_batch'></span>

<h3>Description</h3>

<p>This function retrieves the processing status and other details of a specified OpenAI batch ID
from the OpenAI Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_openai_batch(
  .llms = NULL,
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects.</p>
</td></tr>
<tr><td><code id="check_openai_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>A manually set batch ID.</p>
</td></tr>
<tr><td><code id="check_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="check_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request (default: 3).</p>
</td></tr>
<tr><td><code id="check_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing.
</p>

<hr>
<h2 id='claude'>Provider Function for Claude models on the Anthropic API</h2><span id='topic+claude'></span>

<h3>Description</h3>

<p>The <code>claude()</code> function acts as an interface for interacting with the Anthropic API
through main <code>tidyllm</code> verbs such as <code>chat()</code>, <code>embed()</code>, and
<code>send_batch()</code>. It dynamically routes requests to Claude-specific functions
like <code>claude_chat()</code> and <code>send_claude_batch()</code> based on the context of the call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>claude(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="claude_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate OpenAI-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="claude_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies which action (e.g.,
<code>chat</code>, <code>send_batch</code>) the function is being invoked from.
This argument is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>, or a matrix for <code>embed()</code>).
</p>

<hr>
<h2 id='claude_chat'>Interact with Claude AI models via the Anthropic API</h2><span id='topic+claude_chat'></span>

<h3>Description</h3>

<p>Interact with Claude AI models via the Anthropic API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>claude_chat(
  .llm,
  .model = "claude-3-5-sonnet-20241022",
  .max_tokens = 1024,
  .temperature = NULL,
  .top_k = NULL,
  .top_p = NULL,
  .metadata = NULL,
  .stop_sequences = NULL,
  .tools = NULL,
  .api_url = "https://api.anthropic.com/",
  .verbose = FALSE,
  .max_tries = 3,
  .timeout = 60,
  .stream = FALSE,
  .dry_run = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="claude_chat_+3A_.llm">.llm</code></td>
<td>
<p>An LLMMessage object containing the conversation history and system prompt.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.model">.model</code></td>
<td>
<p>Character string specifying the Claude model version (default: &quot;claude-3-5-sonnet-20241022&quot;).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>Integer specifying the maximum number of tokens in the response (default: 1024).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Numeric between 0 and 1 controlling response randomness.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.top_k">.top_k</code></td>
<td>
<p>Integer controlling diversity by limiting the top K tokens.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Numeric between 0 and 1 for nucleus sampling.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.metadata">.metadata</code></td>
<td>
<p>List of additional metadata to include with the request.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.stop_sequences">.stop_sequences</code></td>
<td>
<p>Character vector of sequences that will halt response generation.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.tools">.tools</code></td>
<td>
<p>List of additional tools or functions the model can use.</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the Anthropic API (default: &quot;https://api.anthropic.com/&quot;).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, displays additional information about the API call (default: FALSE).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; if TRUE, streams the response piece by piece (default: FALSE).</p>
</td></tr>
<tr><td><code id="claude_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new LLMMessage object containing the original messages plus Claude's response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage
msg &lt;- llm_message("What is R programming?")
result &lt;- claude_chat(msg)

# With custom parameters
result2 &lt;- claude_chat(msg, 
                 .temperature = 0.7, 
                 .max_tokens = 1000)

## End(Not run)

</code></pre>

<hr>
<h2 id='claude_list_models'>List Available Models from the Anthropic Claude API</h2><span id='topic+claude_list_models'></span>

<h3>Description</h3>

<p>List Available Models from the Anthropic Claude API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>claude_list_models(
  .api_url = "https://api.anthropic.com",
  .timeout = 60,
  .max_tries = 3,
  .dry_run = FALSE,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="claude_list_models_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.anthropic.com&quot;).</p>
</td></tr>
<tr><td><code id="claude_list_models_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="claude_list_models_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries for the API request (default: 3).</p>
</td></tr>
<tr><td><code id="claude_list_models_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it.</p>
</td></tr>
<tr><td><code id="claude_list_models_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, prints additional information about the request.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information (columns include <code>type</code>,<code>id</code>, <code>display_name</code>, and <code>created_at</code>),
or NULL if no models are found.
</p>

<hr>
<h2 id='deepseek'>Deepseek Provider Function</h2><span id='topic+deepseek'></span>

<h3>Description</h3>

<p>The <code>deepseek()</code> function acts as a provider interface for interacting with the Deepseek API
through <code>tidyllm</code>'s <code>chat()</code> verb.
It dynamically routes requests to deepseek-specific function. At the moment this is only
<code>deepseek_chat()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepseek(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deepseek_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Deepseek-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="deepseek_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument specifying which action (e.g.,
<code>chat</code>, <code>embed</code>) the function is invoked from.
This argument is automatically managed by the <code>tidyllm</code> verbs and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>).
</p>

<hr>
<h2 id='deepseek_chat'>Send LLM Messages to the DeepSeek Chat API</h2><span id='topic+deepseek_chat'></span>

<h3>Description</h3>

<p>This function sends a message history to the DeepSeek Chat API and returns the assistant's reply.
Currently tool calls cause problems on the DeepSeek API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepseek_chat(
  .llm,
  .model = "deepseek-chat",
  .max_tokens = 2048,
  .temperature = NULL,
  .top_p = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL,
  .stop = NULL,
  .stream = FALSE,
  .logprobs = NULL,
  .top_logprobs = NULL,
  .tools = NULL,
  .tool_choice = NULL,
  .api_url = "https://api.deepseek.com/",
  .timeout = 60,
  .verbose = FALSE,
  .dry_run = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deepseek_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.model">.model</code></td>
<td>
<p>The identifier of the model to use (default: &quot;deepseek-chat&quot;).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>The maximum number of tokens that can be generated in the response (default: 2048).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Controls the randomness in the model's response. Values between 0 and 2 are allowed (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Nucleus sampling parameter that controls the proportion of probability mass considered (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Penalizes repeated tokens to reduce repetition (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Encourages new topics by penalizing tokens that have appeared so far (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.stop">.stop</code></td>
<td>
<p>One or more sequences where the API will stop generating further tokens (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; if TRUE, streams the response piece by piece (default: FALSE).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.logprobs">.logprobs</code></td>
<td>
<p>If TRUE, returns log probabilities of each output token (default: FALSE).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.top_logprobs">.top_logprobs</code></td>
<td>
<p>Number between 0 and 5 specifying the number of top log probabilities to return (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.tool_choice">.tool_choice</code></td>
<td>
<p>A character string specifying the tool-calling behavior; valid values are <code>"none"</code>, <code>"auto"</code>, or <code>"required"</code> (optional).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the DeepSeek API (default: <code>"https://api.deepseek.com/"</code>).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>If TRUE, displays additional information after the API call (default: FALSE).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, returns the constructed request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="deepseek_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request (default: 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>

<hr>
<h2 id='df_llm_message'>Convert a Data Frame to an LLMMessage Object</h2><span id='topic+df_llm_message'></span>

<h3>Description</h3>

<p>This function converts a data frame into an <code>LLMMessage</code> object representing a conversation history.
The data frame must have specific columns (<code>role</code> and <code>content</code>), with each row representing a message.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_llm_message(.df)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="df_llm_message_+3A_.df">.df</code></td>
<td>
<p>A data frame with at least two rows and columns <code>role</code> and <code>content</code>.
The <code>role</code> column should contain &quot;user&quot;, &quot;assistant&quot;, or &quot;system&quot;. The <code>content</code> column should contain the corresponding message text.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>LLMMessage</code> object representing the structured conversation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+llm_message">llm_message()</a></code>
</p>
<p>Other Message Creation Utilities: 
<code><a href="#topic+llm_message">llm_message</a>()</code>
</p>

<hr>
<h2 id='embed'>Generate text embeddings</h2><span id='topic+embed'></span>

<h3>Description</h3>

<p>The <code>embed()</code> function allows you to embed a text via a specified provider.
It routes the input to the appropriate provider-specific embedding function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed(
  .input,
  .provider = getOption("tidyllm_embed_default"),
  .model = NULL,
  .truncate = NULL,
  .timeout = NULL,
  .dry_run = NULL,
  .max_tries = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embed_+3A_.input">.input</code></td>
<td>
<p>A character vector of texts v, a list of texts and image objects,  or an <code>LLMMessage</code> object</p>
</td></tr>
<tr><td><code id="embed_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>ollama()</code>, etc.
You can also set a default provider function via the <code>tidyllm_embed_default</code> option.</p>
</td></tr>
<tr><td><code id="embed_+3A_.model">.model</code></td>
<td>
<p>The embedding model to use</p>
</td></tr>
<tr><td><code id="embed_+3A_.truncate">.truncate</code></td>
<td>
<p>Whether to truncate inputs to fit the model's context length</p>
</td></tr>
<tr><td><code id="embed_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds</p>
</td></tr>
<tr><td><code id="embed_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
<tr><td><code id="embed_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns: <code>input</code> and <code>embeddings</code>.
The <code>input</code> column contains the texts sent to embed, and the <code>embeddings</code> column
is a list column where each row contains an embedding vector of the sent input.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
c("What is the meaning of life, the universe and everything?",
 "How much wood would a woodchuck chuck?",
 "How does the brain work?") |&gt;
 embed(gemini)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='fetch_azure_openai_batch'>Fetch Results for an Azure OpenAI Batch</h2><span id='topic+fetch_azure_openai_batch'></span>

<h3>Description</h3>

<p>This function retrieves the results of a completed Azure OpenAI batch and updates
the provided list of <code>LLMMessage</code> objects with the responses. It aligns each
response with the original request using the <code>custom_id</code>s generated in <code>send_azure_openai_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_azure_openai_batch(
  .llms,
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fetch_azure_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects that were part of the batch.</p>
</td></tr>
<tr><td><code id="fetch_azure_openai_batch_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="fetch_azure_openai_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>Character; the unique identifier for the batch. By default this is NULL
and the function will attempt to use the <code>batch_id</code> attribute from <code>.llms</code>.</p>
</td></tr>
<tr><td><code id="fetch_azure_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="fetch_azure_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="fetch_azure_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='fetch_batch'>Fetch Results from a Batch API</h2><span id='topic+fetch_batch'></span>

<h3>Description</h3>

<p>This function retrieves the results of a completed  batch and updates
the provided list of <code>LLMMessage</code> objects with the responses. It aligns each
response with the original request using the <code>custom_id</code>s generated in <code>send_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_batch(
  .llms,
  .provider = getOption("tidyllm_fbatch_default"),
  .dry_run = NULL,
  .max_tries = NULL,
  .timeout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fetch_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="fetch_batch_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>claude()</code>, etc.
You can also set a default provider function via the <code>tidyllm_fbatch_default</code> option.</p>
</td></tr>
<tr><td><code id="fetch_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it</p>
</td></tr>
<tr><td><code id="fetch_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails</p>
</td></tr>
<tr><td><code id="fetch_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function routes the input to the appropriate provider-specific batch API function.
</p>


<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='fetch_claude_batch'>Fetch Results for a Claude Batch</h2><span id='topic+fetch_claude_batch'></span>

<h3>Description</h3>

<p>This function retrieves the results of a completed Claude batch and updates
the provided list of <code>LLMMessage</code> objects with the responses. It aligns each
response with the original request using the <code>custom_id</code>s generated in <code>send_claude_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_claude_batch(
  .llms,
  .batch_id = NULL,
  .api_url = "https://api.anthropic.com/",
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fetch_claude_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects that were part of the batch. The list should
have names (custom IDs) set by <code>send_claude_batch()</code> to ensure correct alignment.</p>
</td></tr>
<tr><td><code id="fetch_claude_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>Character; the unique identifier for the batch. By default this is NULL
and the function will attempt to use the <code>batch_id</code> attribute from <code>.llms</code>.</p>
</td></tr>
<tr><td><code id="fetch_claude_batch_+3A_.api_url">.api_url</code></td>
<td>
<p>Character; the base URL for the Claude API (default: &quot;https://api.anthropic.com/&quot;).</p>
</td></tr>
<tr><td><code id="fetch_claude_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="fetch_claude_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="fetch_claude_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='fetch_mistral_batch'>Fetch Results for an Mistral Batch</h2><span id='topic+fetch_mistral_batch'></span>

<h3>Description</h3>

<p>This function retrieves the results of a completed Mistral batch and updates
the provided list of <code>LLMMessage</code> objects with the responses. It aligns each
response with the original request using the <code>custom_id</code>s generated in <code>send_mistral_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_mistral_batch(
  .llms,
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fetch_mistral_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects that were part of the batch.</p>
</td></tr>
<tr><td><code id="fetch_mistral_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>Character; the unique identifier for the batch. By default this is NULL
and the function will attempt to use the <code>batch_id</code> attribute from <code>.llms</code>.</p>
</td></tr>
<tr><td><code id="fetch_mistral_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="fetch_mistral_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="fetch_mistral_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='fetch_openai_batch'>Fetch Results for an OpenAI Batch</h2><span id='topic+fetch_openai_batch'></span>

<h3>Description</h3>

<p>This function retrieves the results of a completed OpenAI batch and updates
the provided list of <code>LLMMessage</code> objects with the responses. It aligns each
response with the original request using the <code>custom_id</code>s generated in <code>send_openai_batch()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fetch_openai_batch(
  .llms,
  .batch_id = NULL,
  .dry_run = FALSE,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fetch_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects that were part of the batch.</p>
</td></tr>
<tr><td><code id="fetch_openai_batch_+3A_.batch_id">.batch_id</code></td>
<td>
<p>Character; the unique identifier for the batch. By default this is NULL
and the function will attempt to use the <code>batch_id</code> attribute from <code>.llms</code>.</p>
</td></tr>
<tr><td><code id="fetch_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the constructed request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="fetch_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; maximum number of retries if the request fails (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="fetch_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='field_chr'>Define Field Descriptors for JSON Schema</h2><span id='topic+field_chr'></span><span id='topic+field_fct'></span><span id='topic+field_dbl'></span><span id='topic+field_lgl'></span>

<h3>Description</h3>

<p>These functions create field descriptors used in <code>tidyllm_schema()</code> to define JSON schema fields. They support character, factor, numeric, and logical types.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>field_chr(.description = character(0), .vector = FALSE)

field_fct(.description = character(0), .levels, .vector = FALSE)

field_dbl(.description = character(0), .vector = FALSE)

field_lgl(.description = character(0), .vector = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="field_chr_+3A_.description">.description</code></td>
<td>
<p>A character string describing the field (optional).</p>
</td></tr>
<tr><td><code id="field_chr_+3A_.vector">.vector</code></td>
<td>
<p>A logical value indicating if the field is a vector (default: FALSE).</p>
</td></tr>
<tr><td><code id="field_chr_+3A_.levels">.levels</code></td>
<td>
<p>A character vector specifying allowable values (for <code>field_fct()</code> only).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S7 <code>tidyllm_field</code> object representing the field descriptor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>field_chr("A common street name")
field_fct("State abbreviation", .levels = c("CA", "TX", "Other"))
field_dbl("House number")
field_lgl("Is residential")
field_dbl("A list of appartment numbers at the address",.vector=TRUE )
</code></pre>

<hr>
<h2 id='gemini'>Google Gemini Provider Function</h2><span id='topic+gemini'></span>

<h3>Description</h3>

<p>The <code>gemini()</code> function acts as a provider interface for interacting with the Google Gemini API
through <code>tidyllm</code>'s main verbs such as <code>chat()</code> and <code>embed()</code>.
It dynamically routes requests to Gemini-specific functions
like <code>gemini_chat()</code> and <code>gemini_embedding()</code> based on the context of the call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Gemini-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="gemini_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument specifying which action (e.g.,
<code>chat</code>, <code>embed</code>) the function is invoked from.
This argument is automatically managed by the <code>tidyllm</code> verbs and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some functions, such as <code>gemini_upload_file()</code> and <code>gemini_delete_file()</code>,
are specific to Gemini and do not have general verb counterparts.
</p>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>).
</p>

<hr>
<h2 id='gemini_chat'>Send LLMMessage to Gemini API</h2><span id='topic+gemini_chat'></span>

<h3>Description</h3>

<p>Send LLMMessage to Gemini API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_chat(
  .llm,
  .model = "gemini-2.0-flash",
  .fileid = NULL,
  .temperature = NULL,
  .max_output_tokens = NULL,
  .top_p = NULL,
  .top_k = NULL,
  .grounding_threshold = NULL,
  .presence_penalty = NULL,
  .frequency_penalty = NULL,
  .stop_sequences = NULL,
  .safety_settings = NULL,
  .json_schema = NULL,
  .tools = NULL,
  .timeout = 120,
  .dry_run = FALSE,
  .max_tries = 3,
  .verbose = FALSE,
  .stream = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_chat_+3A_.llm">.llm</code></td>
<td>
<p>An existing LLMMessage object or an initial text prompt.</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.model">.model</code></td>
<td>
<p>The model identifier (default: &quot;gemini-1.5-flash&quot;).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.fileid">.fileid</code></td>
<td>
<p>Optional vector of file IDs uploaded via <code>gemini_upload_file()</code> (default: NULL).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Controls randomness in generation (default: NULL, range: 0.0-2.0).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.max_output_tokens">.max_output_tokens</code></td>
<td>
<p>Maximum tokens in the response (default: NULL).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Controls nucleus sampling (default: NULL, range: 0.0-1.0).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.top_k">.top_k</code></td>
<td>
<p>Controls diversity in token selection (default: NULL, range: 0 or more).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.grounding_threshold">.grounding_threshold</code></td>
<td>
<p>A grounding threshold between 0 and 1. With lower
grounding thresholds  Gemini will use Google to search for relevant information
before answering.  (default: NULL).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Penalizes new tokens (default: NULL, range: -2.0 to 2.0).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Penalizes frequent tokens (default: NULL, range: -2.0 to 2.0).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.stop_sequences">.stop_sequences</code></td>
<td>
<p>Optional character sequences to stop generation (default: NULL, up to 5).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.safety_settings">.safety_settings</code></td>
<td>
<p>A list of safety settings (default: NULL).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A schema to enforce an output structure</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>When should our connection time out (default: 120 seconds).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform request (default: 3).</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Should additional information be shown after the API call.</p>
</td></tr>
<tr><td><code id="gemini_chat_+3A_.stream">.stream</code></td>
<td>
<p>Should the response be streamed (default: FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>

<hr>
<h2 id='gemini_delete_file'>Delete a File from Gemini API</h2><span id='topic+gemini_delete_file'></span>

<h3>Description</h3>

<p>Deletes a specific file from the Gemini API using its file ID.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_delete_file(.file_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_delete_file_+3A_.file_name">.file_name</code></td>
<td>
<p>The file ID (e.g., &quot;files/abc-123&quot;) to delete.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns <code>NULL</code>. Prints a confirmation message upon successful deletion.
</p>

<hr>
<h2 id='gemini_embedding'>Generate Embeddings Using the Google Gemini API</h2><span id='topic+gemini_embedding'></span>

<h3>Description</h3>

<p>Generate Embeddings Using the Google Gemini API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_embedding(
  .input,
  .model = "text-embedding-004",
  .truncate = TRUE,
  .timeout = 120,
  .dry_run = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_embedding_+3A_.input">.input</code></td>
<td>
<p>A character vector of texts to embed or an <code>LLMMessage</code> object</p>
</td></tr>
<tr><td><code id="gemini_embedding_+3A_.model">.model</code></td>
<td>
<p>The embedding model identifier (default: &quot;text-embedding-3-small&quot;).</p>
</td></tr>
<tr><td><code id="gemini_embedding_+3A_.truncate">.truncate</code></td>
<td>
<p>Whether to truncate inputs to fit the model's context length (default: TRUE).</p>
</td></tr>
<tr><td><code id="gemini_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="gemini_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
<tr><td><code id="gemini_embedding_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix where each column corresponds to the embedding of a message in the message history.
</p>

<hr>
<h2 id='gemini_file_metadata'>Retrieve Metadata for a File from Gemini API</h2><span id='topic+gemini_file_metadata'></span>

<h3>Description</h3>

<p>Retrieves metadata for a specific file uploaded to the Gemini API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_file_metadata(.file_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_file_metadata_+3A_.file_name">.file_name</code></td>
<td>
<p>The file ID (e.g., &quot;files/abc-123&quot;) to retrieve metadata for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing metadata fields such as name, display name, MIME type, size, and URI.
</p>

<hr>
<h2 id='gemini_list_files'>List Files in Gemini API</h2><span id='topic+gemini_list_files'></span>

<h3>Description</h3>

<p>Lists metadata for files uploaded to the Gemini API, supporting pagination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_list_files(.page_size = 10, .page_token = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_list_files_+3A_.page_size">.page_size</code></td>
<td>
<p>The maximum number of files to return per page (default: 10, maximum: 100).</p>
</td></tr>
<tr><td><code id="gemini_list_files_+3A_.page_token">.page_token</code></td>
<td>
<p>A token for fetching the next page of results (default: NULL).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing metadata for each file, including fields such as name, display name, MIME type, and URI.
</p>

<hr>
<h2 id='gemini_upload_file'>Upload a File to Gemini API</h2><span id='topic+gemini_upload_file'></span>

<h3>Description</h3>

<p>Uploads a file to the Gemini API and returns its metadata as a tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gemini_upload_file(.file_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gemini_upload_file_+3A_.file_path">.file_path</code></td>
<td>
<p>The local file path of the file to upload.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing metadata about the uploaded file, including its name, URI, and MIME type.
</p>

<hr>
<h2 id='get_logprobs'>Retrieve Log Probabilities from Assistant Replies</h2><span id='topic+get_logprobs'></span>

<h3>Description</h3>

<p>Extracts token log probabilities from assistant replies within an <code>LLMMessage</code> object.
Each row represents a token with its log probability and top alternative tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_logprobs(.llm, .index = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_logprobs_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the message history.</p>
</td></tr>
<tr><td><code id="get_logprobs_+3A_.index">.index</code></td>
<td>
<p>A positive integer specifying which assistant reply's log probabilities to extract.
If <code>NULL</code> (default), log probabilities for all replies are returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An empty tibble is output if no logprobs were requested. Currently only works with <code>openai_chat()</code>
</p>
<p>Columns include:
</p>

<ul>
<li> <p><code>reply_index</code>: The index of the assistant reply in the message history.
</p>
</li>
<li> <p><code>token</code>: The generated token.
</p>
</li>
<li> <p><code>logprob</code>: The log probability of the generated token.
</p>
</li>
<li> <p><code>bytes</code>: The byte-level encoding of the token.
</p>
</li>
<li> <p><code>top_logprobs</code>: A list column containing the top alternative tokens with their log probabilities.
</p>
</li></ul>



<h3>Value</h3>

<p>A tibble containing log probabilities for the specified assistant reply or all replies.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_metadata">get_metadata()</a></code>
</p>

<hr>
<h2 id='get_metadata'>Retrieve Metadata from Assistant Replies</h2><span id='topic+get_metadata'></span><span id='topic+last_metadata'></span>

<h3>Description</h3>

<p>Retrieves metadata from assistant replies within an <code>LLMMessage</code> object.
It returns the metadata as a tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_metadata(.llm, .index = NULL)

last_metadata(.llm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_metadata_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the message history.</p>
</td></tr>
<tr><td><code id="get_metadata_+3A_.index">.index</code></td>
<td>
<p>A positive integer specifying which assistant reply's metadata to extract.
If <code>NULL</code> (default), metadata for all replies is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Metadata columns may include:
</p>

<ul>
<li> <p><code>model</code>: The model used for generating the reply.
</p>
</li>
<li> <p><code>timestamp</code>: The time when the reply was generated.
</p>
</li>
<li> <p><code>prompt_tokens</code>: The number of tokens in the input prompt.
</p>
</li>
<li> <p><code>completion_tokens</code>: The number of tokens in the assistant's reply.
</p>
</li>
<li> <p><code>total_tokens</code>: The total number of tokens (prompt + completion).
</p>
</li>
<li> <p><code>api_specific</code>: A list column with API-specific metadata.
</p>
</li></ul>

<p>For convenience, <code><a href="#topic+last_metadata">last_metadata()</a></code> is provided to retrieve the metadata for the last message.
</p>


<h3>Value</h3>

<p>A tibble containing metadata for the specified assistant reply or all replies.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+last_metadata">last_metadata()</a></code>
</p>

<hr>
<h2 id='get_reply'>Retrieve Assistant Reply as Text</h2><span id='topic+get_reply'></span><span id='topic+last_reply'></span>

<h3>Description</h3>

<p>Extracts the plain text content of the assistant's reply from an <code>LLMMessage</code> object.
Use <code><a href="#topic+get_reply_data">get_reply_data()</a></code> for structured replies in JSON format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_reply(.llm, .index = NULL)

last_reply(.llm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_reply_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the message history.</p>
</td></tr>
<tr><td><code id="get_reply_+3A_.index">.index</code></td>
<td>
<p>A positive integer indicating the index of the assistant reply to retrieve.
Defaults to <code>NULL</code>, which retrieves the last reply.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is the core utility for retrieving assistant replies by index.
For convenience, <code><a href="#topic+last_reply">last_reply()</a></code> is provided as a wrapper to retrieve the
latest assistant reply.
</p>


<h3>Value</h3>

<p>Returns a character string containing the assistant's reply, or <code>NA_character_</code> if no reply exists.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_reply_data">get_reply_data()</a></code>, <code><a href="#topic+last_reply">last_reply()</a></code>
</p>

<hr>
<h2 id='get_reply_data'>Retrieve Assistant Reply as Structured Data</h2><span id='topic+get_reply_data'></span><span id='topic+last_reply_data'></span>

<h3>Description</h3>

<p>Parses the assistant's reply as JSON and returns the corresponding structured data.
If the reply is not marked as JSON, attempts to extract and parse JSON content from the text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_reply_data(.llm, .index = NULL)

last_reply_data(.llm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_reply_data_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the message history.</p>
</td></tr>
<tr><td><code id="get_reply_data_+3A_.index">.index</code></td>
<td>
<p>A positive integer indicating the index of the assistant reply to retrieve.
Defaults to <code>NULL</code>, which retrieves the last reply.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For convenience, <code><a href="#topic+last_reply_data">last_reply_data()</a></code> is provided as a wrapper to retrieve the
latest assistant reply's data.
</p>


<h3>Value</h3>

<p>Returns the parsed data from the assistant's reply, or <code>NULL</code> if parsing fails.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_reply">get_reply()</a></code>, <code><a href="#topic+last_reply_data">last_reply_data()</a></code>
</p>

<hr>
<h2 id='get_user_message'>Retrieve a User Message by Index</h2><span id='topic+get_user_message'></span><span id='topic+last_user_message'></span>

<h3>Description</h3>

<p>Extracts the content of a user's message from an <code>LLMMessage</code> object at a specific index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_user_message(.llm, .index = NULL)

last_user_message(.llm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_user_message_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object.</p>
</td></tr>
<tr><td><code id="get_user_message_+3A_.index">.index</code></td>
<td>
<p>A positive integer indicating which user message to retrieve. Defaults to <code>NULL</code>, which retrieves the last message.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For convenience, <code><a href="#topic+last_user_message">last_user_message()</a></code> is provided as a wrapper to retrieve the
latest user message without specifying an index.
</p>


<h3>Value</h3>

<p>Returns the content of the user's message at the specified index. If no messages are found, returns <code>NA_character_</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+last_user_message">last_user_message()</a></code>
</p>

<hr>
<h2 id='groq'>Groq API Provider Function</h2><span id='topic+groq'></span>

<h3>Description</h3>

<p>The <code>groq()</code> function acts as an interface for interacting with the Groq API
through <code>tidyllm</code>'s main verbs. Currently, Groq only supports <code>groq_chat()</code>
for chat-based interactions and <code>groq_transcribe()</code> for transcription tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groq(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="groq_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the Groq-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="groq_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies which action (e.g.,
<code>chat</code>) the function is being invoked from.
This argument is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since <code>groq_transcribe()</code> is unique to Groq and does not have a general verb counterpart,
<code>groq()</code> currently routes messages only to <code>groq_chat()</code> when used with verbs like <code>chat()</code>.
</p>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(currently, only an updated <code>LLMMessage</code> object for <code>groq_chat()</code>).
</p>

<hr>
<h2 id='groq_chat'>Send LLM Messages to the Groq Chat API</h2><span id='topic+groq_chat'></span>

<h3>Description</h3>

<p>This function sends a message history to the Groq Chat API and returns the assistant's reply.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groq_chat(
  .llm,
  .model = "deepseek-r1-distill-llama-70b",
  .max_tokens = 1024,
  .temperature = NULL,
  .top_p = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL,
  .stop = NULL,
  .seed = NULL,
  .tools = NULL,
  .tool_choice = NULL,
  .api_url = "https://api.groq.com/",
  .json = FALSE,
  .timeout = 60,
  .verbose = FALSE,
  .stream = FALSE,
  .dry_run = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="groq_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.model">.model</code></td>
<td>
<p>The identifier of the model to use (default: &quot;llama-3.2-11b-vision-preview&quot;).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>The maximum number of tokens that can be generated in the response (default: 1024).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Controls the randomness in the model's response. Values between 0 and 2 are allowed, where higher values increase randomness (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Nucleus sampling parameter that controls the proportion of probability mass considered. Values between 0 and 1 are allowed (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize repeated tokens, reducing likelihood of repetition (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values encourage new topics by penalizing tokens that have appeared so far (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.stop">.stop</code></td>
<td>
<p>One or more sequences where the API will stop generating further tokens. Can be a string or a list of strings (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.seed">.seed</code></td>
<td>
<p>An integer for deterministic sampling. If specified, attempts to return the same result for repeated requests with identical parameters (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.tool_choice">.tool_choice</code></td>
<td>
<p>A character string specifying the tool-calling behavior; valid values are &quot;none&quot;, &quot;auto&quot;, or &quot;required&quot; (optional).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the Groq API (default: &quot;https://api.groq.com/&quot;).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.json">.json</code></td>
<td>
<p>Whether the response should be structured as JSON (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>If TRUE, displays additional information after the API call, including rate limit details (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; if TRUE, streams the response piece by piece (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, performs a dry run and returns the constructed request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage
msg &lt;- llm_message("What is Groq?")
result &lt;- groq_chat(msg)

# With custom parameters
result2 &lt;- groq_chat(msg, 
               .model = "llama-3.2-vision",
               .temperature = 0.5, 
               .max_tokens = 512)

## End(Not run)

</code></pre>

<hr>
<h2 id='groq_list_models'>List Available Models from the Groq API</h2><span id='topic+groq_list_models'></span>

<h3>Description</h3>

<p>List Available Models from the Groq API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groq_list_models(
  .api_url = "https://api.groq.com",
  .timeout = 60,
  .max_tries = 3,
  .dry_run = FALSE,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="groq_list_models_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.groq.com&quot;).</p>
</td></tr>
<tr><td><code id="groq_list_models_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="groq_list_models_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries for the API request (default: 3).</p>
</td></tr>
<tr><td><code id="groq_list_models_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it.</p>
</td></tr>
<tr><td><code id="groq_list_models_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, prints additional information about the request.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information (columns include <code>id</code>, <code>created</code>, <code>owned_by</code>, and <code>context_window</code>),
or NULL if no models are found.
</p>

<hr>
<h2 id='groq_transcribe'>Transcribe an Audio File Using Groq transcription API</h2><span id='topic+groq_transcribe'></span>

<h3>Description</h3>

<p>This function reads an audio file and sends it to the Groq transcription API for transcription.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groq_transcribe(
  .audio_file,
  .model = "whisper-large-v3",
  .language = NULL,
  .prompt = NULL,
  .temperature = 0,
  .api_url = "https://api.groq.com/openai/v1/audio/transcriptions",
  .dry_run = FALSE,
  .verbose = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="groq_transcribe_+3A_.audio_file">.audio_file</code></td>
<td>
<p>The path to the audio file (required). Supported formats include flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.model">.model</code></td>
<td>
<p>The model to use for transcription (default: &quot;whisper-large-v3&quot;).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.language">.language</code></td>
<td>
<p>The language of the input audio, in ISO-639-1 format (optional).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.prompt">.prompt</code></td>
<td>
<p>A prompt to guide the transcription style. It should match the audio language (optional).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.temperature">.temperature</code></td>
<td>
<p>Sampling temperature, between 0 and 1, with higher values producing more randomness (default: 0).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.groq.com/openai/v1/audio/transcriptions&quot;).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, performs a dry run and returns the request object without making the API call (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, rate limiting info is displayed after the API request (default: FALSE).</p>
</td></tr>
<tr><td><code id="groq_transcribe_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector containing the transcription.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage
groq_transcribe(.audio_file = "example.mp3")

## End(Not run)

</code></pre>

<hr>
<h2 id='img'>Create an  Image Object</h2><span id='topic+img'></span>

<h3>Description</h3>

<p>This function reads an image file from disk, encodes it in base64,
and returns a <code>tidyllm_image</code> object that can be used in multimodal
embedding requests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>img(.path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="img_+3A_.path">.path</code></td>
<td>
<p>The path to the image file on disk.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>tidyllm_image</code>, containing:
</p>

<ul>
<li> <p><code>imagepath</code>: The original file path
</p>
</li>
<li> <p><code>imagename</code>: The basename of the image
</p>
</li>
<li> <p><code>imagebase64</code>: a &quot;data:image/...;base64,...&quot; string
</p>
</li></ul>


<hr>
<h2 id='list_azure_openai_batches'>List Azure OpenAI Batch Requests</h2><span id='topic+list_azure_openai_batches'></span>

<h3>Description</h3>

<p>Retrieves batch request details from the Azure OpenAI Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_azure_openai_batches(
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .limit = 20,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_azure_openai_batches_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="list_azure_openai_batches_+3A_.limit">.limit</code></td>
<td>
<p>Maximum number of batches to retrieve (default: 20).</p>
</td></tr>
<tr><td><code id="list_azure_openai_batches_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="list_azure_openai_batches_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with batch details: batch ID, status, creation time, expiration time,
and request counts (total, completed, failed).
</p>

<hr>
<h2 id='list_batches'>List all Batch Requests on a Batch API</h2><span id='topic+list_batches'></span>

<h3>Description</h3>

<p>List all Batch Requests on a Batch API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_batches(.provider = getOption("tidyllm_lbatch_default"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_batches_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>claude()</code>, etc.
You can also set a default provider function via the <code>tidyllm_lbatch_default</code> option.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with information about the status of batch processing.
</p>

<hr>
<h2 id='list_claude_batches'>List Claude Batch Requests</h2><span id='topic+list_claude_batches'></span>

<h3>Description</h3>

<p>Retrieves batch request details from the Claude API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_claude_batches(
  .api_url = "https://api.anthropic.com/",
  .limit = 20,
  .max_tries = 3,
  .timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_claude_batches_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the Claude API (default: &quot;https://api.anthropic.com/&quot;).</p>
</td></tr>
<tr><td><code id="list_claude_batches_+3A_.limit">.limit</code></td>
<td>
<p>Maximum number of batches to retrieve (default: 20).</p>
</td></tr>
<tr><td><code id="list_claude_batches_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="list_claude_batches_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with batch details: batch ID, status, creation time, expiration time,
and request counts (succeeded, errored, expired, canceled).
</p>

<hr>
<h2 id='list_mistral_batches'>List Mistral Batch Requests</h2><span id='topic+list_mistral_batches'></span>

<h3>Description</h3>

<p>Retrieves batch request details from the OpenAI Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_mistral_batches(
  .limit = 100,
  .max_tries = 3,
  .timeout = 60,
  .status = NULL,
  .created_after = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_mistral_batches_+3A_.limit">.limit</code></td>
<td>
<p>Maximum number of batches to retrieve (default: 20).</p>
</td></tr>
<tr><td><code id="list_mistral_batches_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="list_mistral_batches_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="list_mistral_batches_+3A_.status">.status</code></td>
<td>
<p>Filter by status. (default: NULL)</p>
</td></tr>
<tr><td><code id="list_mistral_batches_+3A_.created_after">.created_after</code></td>
<td>
<p>created after a string specifiying a date-time  (default: NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with batch details for all batches fitting the request
</p>

<hr>
<h2 id='list_models'>List Available Models for a Provider</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>The <code>list_models()</code> function retrieves available models from the specified provider.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(.provider = getOption("tidyllm_lmodels_default"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_models_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the provider and any additional parameters.
You can also set a default provider via the <code>tidyllm_lmodels_default</code> option.</p>
</td></tr>
<tr><td><code id="list_models_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the provider-specific list_models function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information.
</p>

<hr>
<h2 id='list_openai_batches'>List OpenAI Batch Requests</h2><span id='topic+list_openai_batches'></span>

<h3>Description</h3>

<p>Retrieves batch request details from the OpenAI Batch API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_openai_batches(.limit = 20, .max_tries = 3, .timeout = 60)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_openai_batches_+3A_.limit">.limit</code></td>
<td>
<p>Maximum number of batches to retrieve (default: 20).</p>
</td></tr>
<tr><td><code id="list_openai_batches_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="list_openai_batches_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with batch details: batch ID, status, creation time, expiration time,
and request counts (total, completed, failed).
</p>

<hr>
<h2 id='llm_message'>Create or Update Large Language Model Message Object</h2><span id='topic+llm_message'></span>

<h3>Description</h3>

<p>This function creates a new <code>LLMMessage</code> object or updates an existing one.
It supports adding text prompts and various media types, such as images, PDFs, text files, or plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_message(
  .llm = NULL,
  .prompt = NULL,
  .role = "user",
  .system_prompt = "You are a helpful assistant",
  .imagefile = NULL,
  .pdf = NULL,
  .textfile = NULL,
  .capture_plot = FALSE,
  .f = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_message_+3A_.llm">.llm</code></td>
<td>
<p>An existing LLMMessage object or an initial text prompt.</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.prompt">.prompt</code></td>
<td>
<p>Text prompt to add to the message history.</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.role">.role</code></td>
<td>
<p>The role of the message sender, typically &quot;user&quot; or &quot;assistant&quot;.</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.system_prompt">.system_prompt</code></td>
<td>
<p>Default system prompt if a new LLMMessage needs to be created.</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.imagefile">.imagefile</code></td>
<td>
<p>Path to an image file to be attached (optional).</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.pdf">.pdf</code></td>
<td>
<p>Path to a PDF file to be attached (optional). Can be a character vector of length one (file path), or a list with <code>filename</code>, <code>start_page</code>, and <code>end_page</code>.</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.textfile">.textfile</code></td>
<td>
<p>Path to a text file to be read and attached (optional).</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.capture_plot">.capture_plot</code></td>
<td>
<p>Boolean to indicate whether a plot should be captured and attached as an image (optional).</p>
</td></tr>
<tr><td><code id="llm_message_+3A_.f">.f</code></td>
<td>
<p>An R function or an object coercible to a function via <code>rlang::as_function</code>, whose output should be captured and attached (optional).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an updated or new LLMMessage object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+df_llm_message">df_llm_message()</a></code>
</p>
<p>Other Message Creation Utilities: 
<code><a href="#topic+df_llm_message">df_llm_message</a>()</code>
</p>

<hr>
<h2 id='LLMMessage'>Large Language Model Message Class</h2><span id='topic+LLMMessage'></span>

<h3>Description</h3>

<p><code>LLMMessage</code> is an S7 class for managing a conversation history intended for use with large language models (LLMs). Please use
<code>llm_message()</code>to create or modify <code>LLMMessage</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LLMMessage(message_history = list(), system_prompt = character(0))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="LLMMessage_+3A_message_history">message_history</code></td>
<td>
<p>A list containing messages. Each message is a named list with keys like <code>role</code>, <code>content</code>, <code>media</code>, etc.</p>
</td></tr>
<tr><td><code id="LLMMessage_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A character string representing the default system prompt used for the conversation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>LLMMessage</code> class includes the following features:
</p>

<ul>
<li><p> Stores message history in a structured format.
</p>
</li>
<li><p> Supports attaching media and metadata to messages.
</p>
</li>
<li><p> Provides generics like <code>add_message()</code>, <code>has_image()</code>, and <code>remove_message()</code> for interaction.
</p>
</li>
<li><p> Enables API-specific formatting through the <code>to_api_format()</code> generic.
</p>
</li>
<li> <p><code>message_history</code>: A list containing messages. Each message is a named list with keys like <code>role</code>, <code>content</code>, <code>media</code>, etc.
</p>
</li>
<li> <p><code>system_prompt</code>: A character string representing the default system prompt used for the conversation.
</p>
</li></ul>


<hr>
<h2 id='mistral'>Mistral Provider Function</h2><span id='topic+mistral'></span>

<h3>Description</h3>

<p>The <code>mistral()</code> function acts as an interface for interacting with the Mistral API
through main <code>tidyllm</code> verbs such as <code>chat()</code> and <code>embed()</code>.
It dynamically routes requests to Mistral-specific functions
like <code>mistral_chat()</code> and <code>mistral_embedding()</code> based on the context of the call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mistral(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mistral_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Mistral-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="mistral_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies which action (e.g.,
<code>chat</code>, <code>embed</code>, <code>send_batch</code>) the function is being invoked from.
This argument is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>, or a matrix for <code>embed()</code>).
</p>

<hr>
<h2 id='mistral_chat'>Send LLMMessage to Mistral API</h2><span id='topic+mistral_chat'></span>

<h3>Description</h3>

<p>Send LLMMessage to Mistral API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mistral_chat(
  .llm,
  .model = "mistral-large-latest",
  .stream = FALSE,
  .seed = NULL,
  .json = FALSE,
  .temperature = 0.7,
  .top_p = 1,
  .stop = NULL,
  .safe_prompt = FALSE,
  .timeout = 120,
  .max_tries = 3,
  .max_tokens = 1024,
  .min_tokens = NULL,
  .dry_run = FALSE,
  .verbose = FALSE,
  .tools = NULL,
  .tool_choice = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mistral_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object.</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.model">.model</code></td>
<td>
<p>The model identifier to use (default: <code>"mistral-large-latest"</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.stream">.stream</code></td>
<td>
<p>Whether to stream back partial progress to the console. (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.seed">.seed</code></td>
<td>
<p>The seed to use for random sampling. If set, different calls will generate deterministic results (optional).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.json">.json</code></td>
<td>
<p>Whether the output should be in JSON mode(default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Sampling temperature to use, between <code>0.0</code> and <code>1.5</code>. Higher values make the output more random, while lower values make it more focused and deterministic (default: <code>0.7</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Nucleus sampling parameter, between <code>0.0</code> and <code>1.0</code>. The model considers tokens with top_p probability mass (default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.stop">.stop</code></td>
<td>
<p>Stop generation if this token is detected, or if one of these tokens is detected when providing a list (optional).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.safe_prompt">.safe_prompt</code></td>
<td>
<p>Whether to inject a safety prompt before all conversations (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>When should our connection time out in seconds (default: <code>120</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate in the completion. Must be <code style="white-space: pre;">&#8288;&gt;= 0&#8288;</code> (default: <code>1024</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.min_tokens">.min_tokens</code></td>
<td>
<p>The minimum number of tokens to generate in the completion. Must be <code style="white-space: pre;">&#8288;&gt;= 0&#8288;</code> (optional).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If <code>TRUE</code>, perform a dry run and return the request object (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Should additional information be shown after the API call? (default: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="mistral_chat_+3A_.tool_choice">.tool_choice</code></td>
<td>
<p>A character string specifying the tool-calling behavior; valid values are &quot;none&quot;, &quot;auto&quot;, or &quot;required&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an updated <code>LLMMessage</code> object.
</p>

<hr>
<h2 id='mistral_embedding'>Generate Embeddings Using Mistral API</h2><span id='topic+mistral_embedding'></span>

<h3>Description</h3>

<p>Generate Embeddings Using Mistral API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mistral_embedding(
  .input,
  .model = "mistral-embed",
  .timeout = 120,
  .max_tries = 3,
  .dry_run = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mistral_embedding_+3A_.input">.input</code></td>
<td>
<p>A character vector of texts to embed or an <code>LLMMessage</code> object</p>
</td></tr>
<tr><td><code id="mistral_embedding_+3A_.model">.model</code></td>
<td>
<p>The embedding model identifier (default: &quot;mistral-embed&quot;).</p>
</td></tr>
<tr><td><code id="mistral_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="mistral_embedding_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to peform request</p>
</td></tr>
<tr><td><code id="mistral_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix where each column corresponds to the embedding of a message in the message history.
</p>

<hr>
<h2 id='mistral_list_models'>List Available Models from the Mistral API</h2><span id='topic+mistral_list_models'></span>

<h3>Description</h3>

<p>List Available Models from the Mistral API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mistral_list_models(
  .api_url = "https://api.mistral.ai",
  .timeout = 60,
  .max_tries = 3,
  .dry_run = FALSE,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mistral_list_models_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.mistral.ai&quot;).</p>
</td></tr>
<tr><td><code id="mistral_list_models_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="mistral_list_models_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries for the API request (default: 3).</p>
</td></tr>
<tr><td><code id="mistral_list_models_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it.</p>
</td></tr>
<tr><td><code id="mistral_list_models_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, prints additional information about the request.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information (columns include <code>id</code> and <code>created</code>),
or NULL if no models are found.
</p>

<hr>
<h2 id='ollama'>Ollama API Provider Function</h2><span id='topic+ollama'></span>

<h3>Description</h3>

<p>The <code>ollama()</code> function acts as an interface for interacting with local AI models via the Ollama API.
It integrates seamlessly with the main <code>tidyllm</code> verbs such as <code>chat()</code> and <code>embed()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Ollama-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="ollama_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument specifying the verb (e.g., <code>chat</code>, <code>embed</code>)
the function is invoked from. This argument is automatically managed by <code>tidyllm</code> and
should not be set by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some functionalities, like <code>ollama_download_model()</code> or <code>ollama_list_models()</code>
are unique to the Ollama API and do not have a general verb counterpart.
These functions can be only accessed directly.
</p>
<p>Supported Verbs:
</p>

<ul>
<li> <p><strong><code>chat()</code></strong>: Sends a message to an Ollama model and retrieves the model's response.
</p>
</li>
<li> <p><strong><code>embed()</code></strong>: Generates embeddings for input texts using an Ollama model.
</p>
</li>
<li> <p><strong><code>send_batch()</code></strong>: Behaves different than the other <code>send_batch()</code> verbs since it immediately processes the answers
</p>
</li></ul>



<h3>Value</h3>

<p>The result of the requested action:
</p>

<ul>
<li><p> For <code>chat()</code>: An updated <code>LLMMessage</code> object containing the model's response.
</p>
</li>
<li><p> For <code>embed()</code>: A matrix where each column corresponds to an embedding.
</p>
</li></ul>


<hr>
<h2 id='ollama_chat'>Interact with local AI models via the Ollama API</h2><span id='topic+ollama_chat'></span>

<h3>Description</h3>

<p>Interact with local AI models via the Ollama API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama_chat(
  .llm,
  .model = "gemma2",
  .stream = FALSE,
  .seed = NULL,
  .json_schema = NULL,
  .temperature = NULL,
  .num_ctx = 2048,
  .num_predict = NULL,
  .top_k = NULL,
  .top_p = NULL,
  .min_p = NULL,
  .mirostat = NULL,
  .mirostat_eta = NULL,
  .mirostat_tau = NULL,
  .repeat_last_n = NULL,
  .repeat_penalty = NULL,
  .tools = NULL,
  .tfs_z = NULL,
  .stop = NULL,
  .ollama_server = "http://localhost:11434",
  .timeout = 120,
  .keep_alive = NULL,
  .dry_run = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_chat_+3A_.llm">.llm</code></td>
<td>
<p>An LLMMessage object containing the conversation history and system prompt.</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.model">.model</code></td>
<td>
<p>Character string specifying the Ollama model to use (default: &quot;gemma2&quot;)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; whether to stream the response (default: FALSE)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.seed">.seed</code></td>
<td>
<p>Integer; seed for reproducible generation (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object as R list to enforce the output structure (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Float between 0-2; controls randomness in responses (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.num_ctx">.num_ctx</code></td>
<td>
<p>Integer; sets the context window size (default: 2048)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.num_predict">.num_predict</code></td>
<td>
<p>Integer; maximum number of tokens to predict (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.top_k">.top_k</code></td>
<td>
<p>Integer; controls diversity by limiting top tokens considered (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Float between 0-1; nucleus sampling threshold (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.min_p">.min_p</code></td>
<td>
<p>Float between 0-1; minimum probability threshold (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.mirostat">.mirostat</code></td>
<td>
<p>Integer (0,1,2); enables Mirostat sampling algorithm (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.mirostat_eta">.mirostat_eta</code></td>
<td>
<p>Float; Mirostat learning rate (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.mirostat_tau">.mirostat_tau</code></td>
<td>
<p>Float; Mirostat target entropy (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.repeat_last_n">.repeat_last_n</code></td>
<td>
<p>Integer; tokens to look back for repetition (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.repeat_penalty">.repeat_penalty</code></td>
<td>
<p>Float; penalty for repeated tokens (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.tfs_z">.tfs_z</code></td>
<td>
<p>Float; tail free sampling parameter (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.stop">.stop</code></td>
<td>
<p>Character; custom stop sequence(s) (default: NULL)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>String; Ollama API endpoint (default: &quot;http://localhost:11434&quot;)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; API request timeout in seconds (default: 120)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.keep_alive">.keep_alive</code></td>
<td>
<p>Character; How long should the ollama model be kept in memory after request (default: NULL - 5 Minutes)</p>
</td></tr>
<tr><td><code id="ollama_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns request object without execution (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function provides extensive control over the generation process through various parameters:
</p>

<ul>
<li><p> Temperature (0-2): Higher values increase creativity, lower values make responses more focused
</p>
</li>
<li><p> Top-k/Top-p: Control diversity of generated text
</p>
</li>
<li><p> Mirostat: Advanced sampling algorithm for maintaining consistent complexity
</p>
</li>
<li><p> Repeat penalties: Prevent repetitive text
</p>
</li>
<li><p> Context window: Control how much previous conversation is considered
</p>
</li></ul>



<h3>Value</h3>

<p>A new LLMMessage object containing the original messages plus the model's response
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
llm_message("user", "Hello, how are you?")
response &lt;- ollama_chat(llm, .model = "gemma2", .temperature = 0.7)

# With custom parameters
response &lt;- ollama_chat(
  llm,
  .model = "llama2",
  .temperature = 0.8,
  .top_p = 0.9,
  .num_ctx = 4096
)

## End(Not run)

</code></pre>

<hr>
<h2 id='ollama_delete_model'>Delete a model from the Ollama API</h2><span id='topic+ollama_delete_model'></span>

<h3>Description</h3>

<p>This function sends a DELETE request to the Ollama API to remove a specified model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama_delete_model(.model, .ollama_server = "http://localhost:11434")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_delete_model_+3A_.model">.model</code></td>
<td>
<p>The name of the model to delete.</p>
</td></tr>
<tr><td><code id="ollama_delete_model_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>The base URL of the Ollama API (default is &quot;http://localhost:11434&quot;).</p>
</td></tr>
</table>

<hr>
<h2 id='ollama_download_model'>Download a model from the Ollama API</h2><span id='topic+ollama_download_model'></span>

<h3>Description</h3>

<p>This function sends a request to the Ollama API to download a specified model
from Ollama's large online library of models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama_download_model(.model, .ollama_server = "http://localhost:11434")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_download_model_+3A_.model">.model</code></td>
<td>
<p>The name of the model to download.</p>
</td></tr>
<tr><td><code id="ollama_download_model_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>The base URL of the Ollama API (default is &quot;http://localhost:11434&quot;).</p>
</td></tr>
</table>

<hr>
<h2 id='ollama_embedding'>Generate Embeddings Using Ollama API</h2><span id='topic+ollama_embedding'></span>

<h3>Description</h3>

<p>Generate Embeddings Using Ollama API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama_embedding(
  .input,
  .model = "all-minilm",
  .truncate = TRUE,
  .ollama_server = "http://localhost:11434",
  .timeout = 120,
  .dry_run = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_embedding_+3A_.input">.input</code></td>
<td>
<p>Aa charachter vector of texts to embed or an <code>LLMMessage</code> object</p>
</td></tr>
<tr><td><code id="ollama_embedding_+3A_.model">.model</code></td>
<td>
<p>The embedding model identifier (default: &quot;all-minilm&quot;).</p>
</td></tr>
<tr><td><code id="ollama_embedding_+3A_.truncate">.truncate</code></td>
<td>
<p>Whether to truncate inputs to fit the model's context length (default: TRUE).</p>
</td></tr>
<tr><td><code id="ollama_embedding_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>The URL of the Ollama server to be used (default: &quot;http://localhost:11434&quot;).</p>
</td></tr>
<tr><td><code id="ollama_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="ollama_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix where each column corresponds to the embedding of a message in the message history.
</p>

<hr>
<h2 id='ollama_list_models'>Retrieve and return model information from the Ollama API</h2><span id='topic+ollama_list_models'></span>

<h3>Description</h3>

<p>This function connects to the Ollama API and retrieves information
about available models, returning it as a tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ollama_list_models(.ollama_server = "http://localhost:11434")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ollama_list_models_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>The URL of the ollama server to be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information, or NULL if no models are found.
</p>

<hr>
<h2 id='openai'>OpenAI Provider Function</h2><span id='topic+openai'></span>

<h3>Description</h3>

<p>The <code>openai()</code> function acts as an interface for interacting with the OpenAI API
through main <code>tidyllm</code> verbs such as <code>chat()</code>, <code>embed()</code>, and
<code>send_batch()</code>. It dynamically routes requests to OpenAI-specific functions
like <code>openai_chat()</code> and <code>openai_embedding()</code> based on the context of the call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="openai_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate OpenAI-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="openai_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument that specifies which action (e.g.,
<code>chat</code>, <code>embed</code>, <code>send_batch</code>) the function is being invoked from.
This argument is automatically managed and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>, or a matrix for <code>embed()</code>).
</p>

<hr>
<h2 id='openai_chat'>Send LLM Messages to the OpenAI Chat Completions API</h2><span id='topic+openai_chat'></span>

<h3>Description</h3>

<p>This function sends a message history to the OpenAI Chat Completions API and returns the assistant's reply.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_chat(
  .llm,
  .model = "gpt-4o",
  .max_completion_tokens = NULL,
  .reasoning_effort = NULL,
  .frequency_penalty = NULL,
  .logit_bias = NULL,
  .presence_penalty = NULL,
  .seed = NULL,
  .stop = NULL,
  .stream = FALSE,
  .temperature = NULL,
  .top_p = NULL,
  .api_url = "https://api.openai.com/",
  .timeout = 60,
  .verbose = FALSE,
  .json_schema = NULL,
  .max_tries = 3,
  .dry_run = FALSE,
  .compatible = FALSE,
  .api_path = "/v1/chat/completions",
  .logprobs = NULL,
  .top_logprobs = NULL,
  .tools = NULL,
  .tool_choice = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="openai_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.model">.model</code></td>
<td>
<p>The identifier of the model to use (default: &quot;gpt-4o&quot;).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.max_completion_tokens">.max_completion_tokens</code></td>
<td>
<p>An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.reasoning_effort">.reasoning_effort</code></td>
<td>
<p>How long should reasoning models reason (can either be &quot;low&quot;,&quot;medium&quot; or &quot;high&quot;)</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.logit_bias">.logit_bias</code></td>
<td>
<p>A named list modifying the likelihood of specified tokens appearing in the completion.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.seed">.seed</code></td>
<td>
<p>If specified, the system will make a best effort to sample deterministically.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.stop">.stop</code></td>
<td>
<p>Up to 4 sequences where the API will stop generating further tokens.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.stream">.stream</code></td>
<td>
<p>If set to TRUE, the answer will be streamed to console as it comes (default: FALSE).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>What sampling temperature to use, between 0 and 2. Higher values make the output more random.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>An alternative to sampling with temperature, called nucleus sampling.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.openai.com/&quot;).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>Should additional information be shown after the API call (default: FALSE).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object provided by tidyllm schema or ellmer schemata.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform request</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object (default: FALSE).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.compatible">.compatible</code></td>
<td>
<p>If TRUE, skip API and rate-limit checks for OpenAI compatible APIs (default: FALSE).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.api_path">.api_path</code></td>
<td>
<p>The path relative to the base <code>.api_url</code> for the API (default: &quot;/v1/chat/completions&quot;).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.logprobs">.logprobs</code></td>
<td>
<p>If TRUE, get the log probabilities of each output token (default: NULL).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.top_logprobs">.top_logprobs</code></td>
<td>
<p>If specified, get the top N log probabilities of each output token (0-5, default: NULL).</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.tools">.tools</code></td>
<td>
<p>Either a single TOOL object or a list of TOOL objects representing the available functions for tool calls.</p>
</td></tr>
<tr><td><code id="openai_chat_+3A_.tool_choice">.tool_choice</code></td>
<td>
<p>A character string specifying the tool-calling behavior; valid values are &quot;none&quot;, &quot;auto&quot;, or &quot;required&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>

<hr>
<h2 id='openai_embedding'>Generate Embeddings Using OpenAI API</h2><span id='topic+openai_embedding'></span>

<h3>Description</h3>

<p>Generate Embeddings Using OpenAI API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_embedding(
  .input,
  .model = "text-embedding-3-small",
  .truncate = TRUE,
  .timeout = 120,
  .dry_run = FALSE,
  .max_tries = 3,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="openai_embedding_+3A_.input">.input</code></td>
<td>
<p>An existing LLMMessage object (or a character vector of texts to embed)</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.model">.model</code></td>
<td>
<p>The embedding model identifier (default: &quot;text-embedding-3-small&quot;).</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.truncate">.truncate</code></td>
<td>
<p>Whether to truncate inputs to fit the model's context length (default: TRUE).</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object.</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="openai_embedding_+3A_.verbose">.verbose</code></td>
<td>
<p>Should information about current ratelimits be printed? (default: FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns: <code>input</code> and <code>embeddings</code>.
The <code>input</code> column contains the texts sent to embed, and the <code>embeddings</code> column
is a list column where each row contains an embedding vector of the sent input.
</p>

<hr>
<h2 id='openai_list_models'>List Available Models from the OpenAI API</h2><span id='topic+openai_list_models'></span>

<h3>Description</h3>

<p>List Available Models from the OpenAI API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_list_models(
  .api_url = "https://api.openai.com",
  .timeout = 60,
  .max_tries = 3,
  .dry_run = FALSE,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="openai_list_models_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the API (default: &quot;https://api.openai.com&quot;).</p>
</td></tr>
<tr><td><code id="openai_list_models_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="openai_list_models_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries for the API request (default: 3).</p>
</td></tr>
<tr><td><code id="openai_list_models_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it.</p>
</td></tr>
<tr><td><code id="openai_list_models_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, prints additional information about the request.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing model information (columns include <code>id</code>, <code>created</code>, and <code>owned_by</code>),
or NULL if no models are found.
</p>

<hr>
<h2 id='pdf_page_batch'>Batch Process PDF into LLM Messages</h2><span id='topic+pdf_page_batch'></span>

<h3>Description</h3>

<p>This function processes a PDF file page by page. For each page, it extracts the text
and converts the page into an image. It creates a list of LLMMessage objects with
the text and the image for multimodal processing. Users can specify a range of pages
to process and provide a custom function to generate prompts for each page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdf_page_batch(
  .pdf,
  .general_prompt,
  .system_prompt = "You are a helpful assistant",
  .page_range = NULL,
  .prompt_fn = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pdf_page_batch_+3A_.pdf">.pdf</code></td>
<td>
<p>Path to the PDF file.</p>
</td></tr>
<tr><td><code id="pdf_page_batch_+3A_.general_prompt">.general_prompt</code></td>
<td>
<p>A default prompt that is applied to each page if <code>.prompt_fn</code> is not provided.</p>
</td></tr>
<tr><td><code id="pdf_page_batch_+3A_.system_prompt">.system_prompt</code></td>
<td>
<p>Optional system prompt to initialize the LLMMessage (default is &quot;You are a helpful assistant&quot;).</p>
</td></tr>
<tr><td><code id="pdf_page_batch_+3A_.page_range">.page_range</code></td>
<td>
<p>A vector of two integers specifying the start and end pages to process. If NULL, all pages are processed.</p>
</td></tr>
<tr><td><code id="pdf_page_batch_+3A_.prompt_fn">.prompt_fn</code></td>
<td>
<p>An optional custom function that generates a prompt for each page. The function takes the page text as input
and returns a string. If NULL, <code>.general_prompt</code> is used for all pages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of LLMMessage objects, each containing the text and image for a page.
</p>

<hr>
<h2 id='perplexity'>Perplexity Provider Function</h2><span id='topic+perplexity'></span>

<h3>Description</h3>

<p>The <code>perplexity()</code> function acts as a provider interface for interacting with the Perplexity API
through <code>tidyllm</code>'s <code>chat()</code> verb.
It dynamically routes requests to Perplxeity-specific function. At the moment this is only
<code>perplexity_chat()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perplexity(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="perplexity_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Perplexity-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument specifying which action (e.g.,
<code>chat</code>, <code>embed</code>) the function is invoked from.
This argument is automatically managed by the <code>tidyllm</code> verbs and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
(e.g., an updated <code>LLMMessage</code> object for <code>chat()</code>).
</p>

<hr>
<h2 id='perplexity_chat'>Send LLM Messages to the Perplexity Chat API</h2><span id='topic+perplexity_chat'></span>

<h3>Description</h3>

<p>This function sends a message history to the Perplexity Chat API and returns the assistant's reply.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perplexity_chat(
  .llm,
  .model = "sonar",
  .max_tokens = 1024,
  .temperature = NULL,
  .top_p = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL,
  .stop = NULL,
  .search_domain_filter = NULL,
  .return_images = FALSE,
  .search_recency_filter = NULL,
  .api_url = "https://api.perplexity.ai/",
  .json = FALSE,
  .timeout = 60,
  .verbose = FALSE,
  .stream = FALSE,
  .dry_run = FALSE,
  .max_tries = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="perplexity_chat_+3A_.llm">.llm</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.model">.model</code></td>
<td>
<p>The identifier of the model to use (default: &quot;sonar&quot;).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>The maximum number of tokens that can be generated in the response (default: 1024).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.temperature">.temperature</code></td>
<td>
<p>Controls the randomness in the model's response. Values between 0 (exclusive) and 2 (exclusive) are allowed, where higher values increase randomness (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.top_p">.top_p</code></td>
<td>
<p>Nucleus sampling parameter that controls the proportion of probability mass considered. Values between 0 (exclusive) and 1 (exclusive) are allowed (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number greater than 0. Values &gt; 1.0 penalize repeated tokens, reducing the likelihood of repetition (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values encourage new topics by penalizing tokens that have appeared so far (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.stop">.stop</code></td>
<td>
<p>One or more sequences where the API will stop generating further tokens. Can be a string or a list of strings (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.search_domain_filter">.search_domain_filter</code></td>
<td>
<p>A vector of domains to limit or exclude from search results. For exclusion, prefix domains with a &quot;-&quot; (optional, currently in closed beta).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.return_images">.return_images</code></td>
<td>
<p>Logical; if TRUE, enables returning images from the model's response (default: FALSE, currently in closed beta).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.search_recency_filter">.search_recency_filter</code></td>
<td>
<p>Limits search results to a specific time interval (e.g., &quot;month&quot;, &quot;week&quot;, &quot;day&quot;, or &quot;hour&quot;). Only applies to online models (optional).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the Perplexity API (default: &quot;https://api.perplexity.ai/&quot;).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.json">.json</code></td>
<td>
<p>Whether the response should be structured as JSON (default: FALSE).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.verbose">.verbose</code></td>
<td>
<p>If TRUE, displays additional information after the API call, including rate limit details (default: FALSE).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.stream">.stream</code></td>
<td>
<p>Logical; if TRUE, streams the response piece by piece (default: FALSE).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, performs a dry run and returns the constructed request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="perplexity_chat_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retries to perform the request (default: 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>LLMMessage</code> object containing the original messages plus the assistant's response.
</p>

<hr>
<h2 id='rate_limit_info'>Get the current rate limit information for all or a specific API</h2><span id='topic+rate_limit_info'></span>

<h3>Description</h3>

<p>This function retrieves the rate limit details for the specified API,
or for all APIs stored in the .tidyllm_rate_limit_env if no API is specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rate_limit_info(.api_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rate_limit_info_+3A_.api_name">.api_name</code></td>
<td>
<p>(Optional) The name of the API whose rate limit info you want to get
If not provided, the rate limit info for all APIs in the environment will be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing the rate limit information.
</p>

<hr>
<h2 id='send_azure_openai_batch'>Send a Batch of Messages to Azure OpenAI Batch API</h2><span id='topic+send_azure_openai_batch'></span>

<h3>Description</h3>

<p>This function creates and submits a batch of messages to the Azure OpenAI Batch API for asynchronous processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_azure_openai_batch(
  .llms,
  .deployment = "gpt-4o-mini",
  .endpoint_url = Sys.getenv("AZURE_ENDPOINT_URL"),
  .api_version = "2024-10-01-preview",
  .max_completion_tokens = NULL,
  .frequency_penalty = NULL,
  .logit_bias = NULL,
  .logprobs = FALSE,
  .top_logprobs = NULL,
  .presence_penalty = NULL,
  .seed = NULL,
  .stop = NULL,
  .temperature = NULL,
  .top_p = NULL,
  .dry_run = FALSE,
  .overwrite = FALSE,
  .max_tries = 3,
  .timeout = 60,
  .verbose = FALSE,
  .json_schema = NULL,
  .id_prefix = "tidyllm_azure_openai_req_"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_azure_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>An <code>LLMMessage</code> object containing the conversation history.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.deployment">.deployment</code></td>
<td>
<p>The identifier of the model that is deployed (default: &quot;gpt-4o-mini&quot;).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.endpoint_url">.endpoint_url</code></td>
<td>
<p>Base URL for the API (default:  Sys.getenv(&quot;AZURE_ENDPOINT_URL&quot;)).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.api_version">.api_version</code></td>
<td>
<p>Which version of the API is deployed (default: &quot;2024-10-01-preview&quot;)</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.max_completion_tokens">.max_completion_tokens</code></td>
<td>
<p>An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.logit_bias">.logit_bias</code></td>
<td>
<p>A named list modifying the likelihood of specified tokens appearing in the completion.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.logprobs">.logprobs</code></td>
<td>
<p>Whether to return log probabilities of the output tokens (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.top_logprobs">.top_logprobs</code></td>
<td>
<p>An integer between 0 and 20 specifying the number of most likely tokens to return at each token position.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.seed">.seed</code></td>
<td>
<p>If specified, the system will make a best effort to sample deterministically.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.stop">.stop</code></td>
<td>
<p>Up to 4 sequences where the API will stop generating further tokens.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>What sampling temperature to use, between 0 and 2. Higher values make the output more random.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>An alternative to sampling with temperature, called nucleus sampling.</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.overwrite">.overwrite</code></td>
<td>
<p>Logical; if TRUE, allows overwriting an existing batch ID (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries to perform the request (default: 3).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, additional info about the requests is printed (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object as R list to enforce the output structure (default: NULL).</p>
</td></tr>
<tr><td><code id="send_azure_openai_batch_+3A_.id_prefix">.id_prefix</code></td>
<td>
<p>Character string to specify a prefix for generating custom IDs when names in <code>.llms</code> are missing (default: &quot;tidyllm_openai_req_&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated and named list of <code>.llms</code> with identifiers that align with batch responses, including a <code>batch_id</code> attribute.
</p>

<hr>
<h2 id='send_batch'>Send a batch of messages to a batch API</h2><span id='topic+send_batch'></span>

<h3>Description</h3>

<p>The <code>send_batch()</code> function allows you to send a list of <code>LLMMessage</code> objects
to an API.
It routes the input to the appropriate provider-specific batch API function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_batch(
  .llms,
  .provider = getOption("tidyllm_sbatch_default"),
  .dry_run = NULL,
  .temperature = NULL,
  .timeout = NULL,
  .top_p = NULL,
  .max_tries = NULL,
  .model = NULL,
  .verbose = NULL,
  .json_schema = NULL,
  .seed = NULL,
  .stop = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL,
  .id_prefix = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of <code>LLMMessage</code> objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.provider">.provider</code></td>
<td>
<p>A function or function call specifying the language model provider and any additional parameters.
This should be a call to a provider function like <code>openai()</code>, <code>claude()</code>, etc.
You can also set a default provider function via the <code>tidyllm_sbatch_default</code> option.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, simulates the request without sending it to the provider. Useful for testing.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>Numeric; controls the randomness of the model's output (0 = deterministic).</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Numeric; the maximum time (in seconds) to wait for a response.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>Numeric; nucleus sampling parameter, which limits the sampling to the top cumulative probability <code>p</code>.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Integer; the maximum number of retries for failed requests.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.model">.model</code></td>
<td>
<p>Character; the model identifier to use (e.g., <code>"gpt-4"</code>).</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if <code>TRUE</code>, prints additional information about the request and response.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.json_schema">.json_schema</code></td>
<td>
<p>List; A JSON schema object as R list to enforce the output structure</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.seed">.seed</code></td>
<td>
<p>Integer; sets a random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.stop">.stop</code></td>
<td>
<p>Character vector; specifies sequences where the model should stop generating further tokens.</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Numeric; adjusts the likelihood of repeating tokens (positive values decrease repetition).</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Numeric; adjusts the likelihood of introducing new tokens (positive values encourage novelty).</p>
</td></tr>
<tr><td><code id="send_batch_+3A_.id_prefix">.id_prefix</code></td>
<td>
<p>Character string to specify a prefix for generating custom IDs when names in <code>.llms</code> are missing</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated and named list of <code>.llms</code> with identifiers that align with batch responses, including a <code>batch_id</code> attribute.
</p>

<hr>
<h2 id='send_claude_batch'>Send a Batch of Messages to Claude API</h2><span id='topic+send_claude_batch'></span>

<h3>Description</h3>

<p>This function creates and submits a batch of messages to the Claude API for asynchronous processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_claude_batch(
  .llms,
  .model = "claude-3-5-sonnet-20241022",
  .max_tokens = 1024,
  .temperature = NULL,
  .top_k = NULL,
  .top_p = NULL,
  .stop_sequences = NULL,
  .api_url = "https://api.anthropic.com/",
  .verbose = FALSE,
  .dry_run = FALSE,
  .overwrite = FALSE,
  .max_tries = 3,
  .timeout = 60,
  .id_prefix = "tidyllm_claude_req_"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_claude_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.model">.model</code></td>
<td>
<p>Character string specifying the Claude model version (default: &quot;claude-3-5-sonnet-20241022&quot;).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>Integer specifying the maximum tokens per response (default: 1024).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>Numeric between 0 and 1 controlling response randomness.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.top_k">.top_k</code></td>
<td>
<p>Integer for diversity by limiting the top K tokens.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>Numeric between 0 and 1 for nucleus sampling.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.stop_sequences">.stop_sequences</code></td>
<td>
<p>Character vector of sequences that halt response generation.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.api_url">.api_url</code></td>
<td>
<p>Base URL for the Claude API (default: &quot;https://api.anthropic.com/&quot;).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, prints a message with the batch ID (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.overwrite">.overwrite</code></td>
<td>
<p>Logical; if TRUE, allows overwriting an existing batch ID associated with the request (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries to perform the request.</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="send_claude_batch_+3A_.id_prefix">.id_prefix</code></td>
<td>
<p>Character string to specify a prefix for generating custom IDs when names in <code>.llms</code> are missing.
Defaults to &quot;tidyllm_claude_req_&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated and named list of <code>.llms</code> with identifiers that align with batch responses, including a <code>batch_id</code> attribute.
</p>

<hr>
<h2 id='send_mistral_batch'>Send a Batch of Requests to the Mistral API</h2><span id='topic+send_mistral_batch'></span>

<h3>Description</h3>

<p>Send a Batch of Requests to the Mistral API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_mistral_batch(
  .llms,
  .model = "mistral-small-latest",
  .endpoint = "/v1/chat/completions",
  .metadata = NULL,
  .temperature = 0.7,
  .top_p = 1,
  .max_tokens = 1024,
  .min_tokens = NULL,
  .seed = NULL,
  .stop = NULL,
  .dry_run = FALSE,
  .overwrite = FALSE,
  .max_tries = 3,
  .timeout = 60,
  .id_prefix = "tidyllm_mistral_req_"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_mistral_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.model">.model</code></td>
<td>
<p>The Mistral model version (default: &quot;mistral-small-latest&quot;).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.endpoint">.endpoint</code></td>
<td>
<p>The API endpoint (default: &quot;/v1/chat/completions&quot;).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.metadata">.metadata</code></td>
<td>
<p>Optional metadata for the batch.</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>Sampling temperature to use, between <code>0.0</code> and <code>1.5</code>. Higher values make the output more random (default: <code>0.7</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>Nucleus sampling parameter, between <code>0.0</code> and <code>1.0</code> (default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.max_tokens">.max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate in the completion (default: <code>1024</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.min_tokens">.min_tokens</code></td>
<td>
<p>The minimum number of tokens to generate (optional).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.seed">.seed</code></td>
<td>
<p>Random seed for deterministic outputs (optional).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.stop">.stop</code></td>
<td>
<p>Stop generation at specific tokens or strings (optional).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the prepared request without executing it (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.overwrite">.overwrite</code></td>
<td>
<p>Logical; if <code>TRUE</code>, allows overwriting existing custom IDs (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: <code>3</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Request timeout in seconds (default: <code>60</code>).</p>
</td></tr>
<tr><td><code id="send_mistral_batch_+3A_.id_prefix">.id_prefix</code></td>
<td>
<p>Prefix for generating custom IDs (default: <code>"tidyllm_mistral_req_"</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>prepared_llms</code> list with the <code>batch_id</code> attribute attached.
</p>

<hr>
<h2 id='send_ollama_batch'>Send a Batch of Messages to Ollama API</h2><span id='topic+send_ollama_batch'></span>

<h3>Description</h3>

<p>This function creates and submits a batch of messages to the Ollama API
Contrary to other batch functions, this functions waits for the batch to finish and receives requests.
The advantage compared to sending single messages via <code>chat()</code> is that Ollama handles large parallel
requests quicker than many individual chat requests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_ollama_batch(
  .llms,
  .model = "gemma2",
  .stream = FALSE,
  .seed = NULL,
  .json_schema = NULL,
  .temperature = NULL,
  .num_ctx = 2048,
  .num_predict = NULL,
  .top_k = NULL,
  .top_p = NULL,
  .min_p = NULL,
  .mirostat = NULL,
  .mirostat_eta = NULL,
  .mirostat_tau = NULL,
  .repeat_last_n = NULL,
  .repeat_penalty = NULL,
  .tfs_z = NULL,
  .stop = NULL,
  .ollama_server = "http://localhost:11434",
  .timeout = 120,
  .keep_alive = NULL,
  .dry_run = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_ollama_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.model">.model</code></td>
<td>
<p>Character string specifying the Ollama model to use (default: &quot;gemma2&quot;)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.stream">.stream</code></td>
<td>
<p>Logical; whether to stream the response (default: FALSE)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.seed">.seed</code></td>
<td>
<p>Integer; seed for reproducible generation (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object as R list to enforce the output structure (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>Float between 0-2; controls randomness in responses (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.num_ctx">.num_ctx</code></td>
<td>
<p>Integer; sets the context window size (default: 2048)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.num_predict">.num_predict</code></td>
<td>
<p>Integer; maximum number of tokens to predict (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.top_k">.top_k</code></td>
<td>
<p>Integer; controls diversity by limiting top tokens considered (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>Float between 0-1; nucleus sampling threshold (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.min_p">.min_p</code></td>
<td>
<p>Float between 0-1; minimum probability threshold (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.mirostat">.mirostat</code></td>
<td>
<p>Integer (0,1,2); enables Mirostat sampling algorithm (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.mirostat_eta">.mirostat_eta</code></td>
<td>
<p>Float; Mirostat learning rate (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.mirostat_tau">.mirostat_tau</code></td>
<td>
<p>Float; Mirostat target entropy (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.repeat_last_n">.repeat_last_n</code></td>
<td>
<p>Integer; tokens to look back for repetition (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.repeat_penalty">.repeat_penalty</code></td>
<td>
<p>Float; penalty for repeated tokens (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.tfs_z">.tfs_z</code></td>
<td>
<p>Float; tail free sampling parameter (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.stop">.stop</code></td>
<td>
<p>Character; custom stop sequence(s) (default: NULL)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.ollama_server">.ollama_server</code></td>
<td>
<p>String; Ollama API endpoint (default: &quot;http://localhost:11434&quot;)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer; API request timeout in seconds (default: 120)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.keep_alive">.keep_alive</code></td>
<td>
<p>Character; How long should the ollama model be kept in memory after request (default: NULL - 5 Minutes)</p>
</td></tr>
<tr><td><code id="send_ollama_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns request object without execution (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function provides extensive control over the generation process through various parameters:
</p>

<ul>
<li><p> Temperature (0-2): Higher values increase creativity, lower values make responses more focused
</p>
</li>
<li><p> Top-k/Top-p: Control diversity of generated text
</p>
</li>
<li><p> Mirostat: Advanced sampling algorithm for maintaining consistent complexity
</p>
</li>
<li><p> Repeat penalties: Prevent repetitive text
</p>
</li>
<li><p> Context window: Control how much previous conversation is considered
</p>
</li></ul>



<h3>Value</h3>

<p>A list of updated <code>LLMMessage</code> objects, each with the assistant's response added if successful.
</p>

<hr>
<h2 id='send_openai_batch'>Send a Batch of Messages to OpenAI Batch API</h2><span id='topic+send_openai_batch'></span>

<h3>Description</h3>

<p>This function creates and submits a batch of messages to the OpenAI Batch API for asynchronous processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_openai_batch(
  .llms,
  .model = "gpt-4o",
  .max_completion_tokens = NULL,
  .reasoning_effort = NULL,
  .frequency_penalty = NULL,
  .logit_bias = NULL,
  .presence_penalty = NULL,
  .seed = NULL,
  .stop = NULL,
  .temperature = NULL,
  .top_p = NULL,
  .logprobs = NULL,
  .top_logprobs = NULL,
  .dry_run = FALSE,
  .overwrite = FALSE,
  .json_schema = NULL,
  .max_tries = 3,
  .timeout = 60,
  .verbose = FALSE,
  .id_prefix = "tidyllm_openai_req_"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_openai_batch_+3A_.llms">.llms</code></td>
<td>
<p>A list of LLMMessage objects containing conversation histories.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.model">.model</code></td>
<td>
<p>Character string specifying the OpenAI model version (default: &quot;gpt-4o&quot;).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.max_completion_tokens">.max_completion_tokens</code></td>
<td>
<p>Integer specifying the maximum tokens per response (default: NULL).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.reasoning_effort">.reasoning_effort</code></td>
<td>
<p>How long should reasoning models reason (can either be &quot;low&quot;,&quot;medium&quot; or &quot;high&quot;)</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.frequency_penalty">.frequency_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.logit_bias">.logit_bias</code></td>
<td>
<p>A named list modifying the likelihood of specified tokens appearing in the completion.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.presence_penalty">.presence_penalty</code></td>
<td>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.seed">.seed</code></td>
<td>
<p>If specified, the system will make a best effort to sample deterministically.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.stop">.stop</code></td>
<td>
<p>Up to 4 sequences where the API will stop generating further tokens.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.temperature">.temperature</code></td>
<td>
<p>What sampling temperature to use, between 0 and 2. Higher values make the output more random.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.top_p">.top_p</code></td>
<td>
<p>An alternative to sampling with temperature, called nucleus sampling.</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.logprobs">.logprobs</code></td>
<td>
<p>If TRUE, get the log probabilities of each output token (default: NULL).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.top_logprobs">.top_logprobs</code></td>
<td>
<p>If specified, get the top N log probabilities of each output token (0-5, default: NULL).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.dry_run">.dry_run</code></td>
<td>
<p>Logical; if TRUE, returns the prepared request object without executing it (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.overwrite">.overwrite</code></td>
<td>
<p>Logical; if TRUE, allows overwriting an existing batch ID associated with the request (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.json_schema">.json_schema</code></td>
<td>
<p>A JSON schema object provided by tidyllm_schema or ellmer schemata (default: NULL).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum number of retries to perform the request (default: 3).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.timeout">.timeout</code></td>
<td>
<p>Integer specifying the request timeout in seconds (default: 60).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.verbose">.verbose</code></td>
<td>
<p>Logical; if TRUE, additional info about the requests is printed (default: FALSE).</p>
</td></tr>
<tr><td><code id="send_openai_batch_+3A_.id_prefix">.id_prefix</code></td>
<td>
<p>Character string to specify a prefix for generating custom IDs when names in <code>.llms</code> are missing (default: &quot;tidyllm_openai_req_&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated and named list of <code>.llms</code> with identifiers that align with batch responses, including a <code>batch_id</code> attribute.
</p>

<hr>
<h2 id='tidyllm_schema'>Create a JSON Schema for Structured Outputs</h2><span id='topic+tidyllm_schema'></span>

<h3>Description</h3>

<p>This function creates a JSON schema for structured outputs, supporting both character-based shorthand and S7 <code>tidyllm_field</code> objects.
It also integrates with <code>ellmer</code> types like <code>ellmer::type_string()</code> if ellmer is in your namespace
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidyllm_schema(name = "tidyllm_schema", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidyllm_schema_+3A_name">name</code></td>
<td>
<p>A character string specifying the schema name (default: &quot;tidyllm_schema&quot;).</p>
</td></tr>
<tr><td><code id="tidyllm_schema_+3A_...">...</code></td>
<td>
<p>Named arguments where each name represents a field, and each value is either a character string, a <code>tidyllm_field</code>, or an <code>ellmer</code> type.
</p>
<p>Supported character shorthand types:
</p>

<ul>
<li><p> &quot;character&quot; or &quot;string&quot; for character fields
</p>
</li>
<li><p> &quot;logical&quot; for boolean fields
</p>
</li>
<li><p> &quot;numeric&quot; for number fields
</p>
</li>
<li><p> &quot;factor(...)&quot; for enumerations
</p>
</li>
<li><p> Use <code style="white-space: pre;">&#8288;[]&#8288;</code> to indicate vectors, e.g., &quot;character[]&quot;
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A list representing the JSON schema, suitable for use with <code>.json_schema</code> in LLM API calls.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example using different field types
address_schema &lt;- tidyllm_schema(
  name = "AddressSchema",
  Street = field_chr("A common street name"),
  house_number = field_dbl(),
  City = field_chr("Name of a city"),
  State = field_fct("State abbreviation", .levels = c("CA", "TX", "Other")),
  Country = "string",
  PostalCode = "string"
)

llm_message("Imagine an address") |&gt; chat(openai, .json_schema = address_schema)

# Example with vector field
tidyllm_schema(
  plz = field_dbl(.vector = TRUE)
)

## End(Not run)
</code></pre>

<hr>
<h2 id='tidyllm_tool'>Create a Tool Definition for tidyllm</h2><span id='topic+tidyllm_tool'></span>

<h3>Description</h3>

<p>Creates a tool definition for use with Language Model API calls that support function calling.
This function wraps an existing R function with schema information for LLM interaction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidyllm_tool(.f, .description = character(0), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidyllm_tool_+3A_.f">.f</code></td>
<td>
<p>The function to wrap as a tool</p>
</td></tr>
<tr><td><code id="tidyllm_tool_+3A_.description">.description</code></td>
<td>
<p>Character string describing what the tool does</p>
</td></tr>
<tr><td><code id="tidyllm_tool_+3A_...">...</code></td>
<td>
<p>Named arguments providing schema definitions for each function parameter using tidyllm_fields</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each parameter schema in <code>...</code> should correspond to a parameter in the wrapped function.
All required function parameters must have corresponding schema definitions.
</p>


<h3>Value</h3>

<p>A <code>TOOL</code> class object that can be used with tidyllm <code>chat()</code> functions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_weather &lt;- function(location){}
weather_tool &lt;- tidyllm_tool(
  get_weather,
  "Get the current weather in a given location",
  location = field_chr("The city and state, e.g., San Francisco, CA")
)

</code></pre>

<hr>
<h2 id='voyage'>Voyage Provider Function</h2><span id='topic+voyage'></span>

<h3>Description</h3>

<p>The <code>voyage()</code> function acts as a provider interface for interacting with the Voyage.ai API
through <code>tidyllm</code>'s verbs.
It dynamically routes requests to voyage-specific functions. At the moment this is only
<code>voyage_embed()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>voyage(..., .called_from = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="voyage_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the appropriate Voyage-specific function,
such as model configuration, input text, or API-specific options.</p>
</td></tr>
<tr><td><code id="voyage_+3A_.called_from">.called_from</code></td>
<td>
<p>An internal argument specifying which action (e.g.,
<code>embed</code>) the function is invoked from.
This argument is automatically managed by the <code>tidyllm</code> verbs and should not be modified by the user.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the requested action, depending on the specific function invoked
</p>

<hr>
<h2 id='voyage_embedding'>Generate Embeddings Using Voyage AI API</h2><span id='topic+voyage_embedding'></span>

<h3>Description</h3>

<p>This function creates embedding vectors from text or multimodal inputs (text and images)
using the Voyage AI API. It supports three types of input:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>voyage_embedding(
  .input,
  .model = "voyage-3",
  .timeout = 120,
  .dry_run = FALSE,
  .max_tries = 3,
  .verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="voyage_embedding_+3A_.input">.input</code></td>
<td>
<p>Input to embed. Can be:
</p>

<ul>
<li><p> A character vector of texts
</p>
</li>
<li><p> An <code>LLMMessage</code> object (all textual components will be embedded)
</p>
</li>
<li><p> A list containing a mix of character strings and <code>tidyllm_image</code> objects created with <code>img()</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="voyage_embedding_+3A_.model">.model</code></td>
<td>
<p>The embedding model identifier. For text-only: &quot;voyage-3&quot; (default).
For multimodal inputs: &quot;voyage-multimodal-3&quot; is used automatically.</p>
</td></tr>
<tr><td><code id="voyage_embedding_+3A_.timeout">.timeout</code></td>
<td>
<p>Timeout for the API request in seconds (default: 120).</p>
</td></tr>
<tr><td><code id="voyage_embedding_+3A_.dry_run">.dry_run</code></td>
<td>
<p>If TRUE, perform a dry run and return the request object without sending.</p>
</td></tr>
<tr><td><code id="voyage_embedding_+3A_.max_tries">.max_tries</code></td>
<td>
<p>Maximum retry attempts for requests (default: 3).</p>
</td></tr>
<tr><td><code id="voyage_embedding_+3A_.verbose">.verbose</code></td>
<td>
<p>Should information about current rate limits be printed? (default: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li><p> Character vector: Embeds each text string separately
</p>
</li>
<li><p> LLMMessage object: Extracts and embeds text content from messages
</p>
</li>
<li><p> List of mixed content: Processes a combination of text strings and image objects created with <code>img()</code>
</p>
</li></ol>

<p>For multimodal inputs, the function automatically switches to Voyage's multimodal API
and formats the response with appropriate labels (e.g., <code>"[IMG] image.png"</code>) for images.
</p>


<h3>Value</h3>

<p>A tibble with two columns: <code>input</code> and <code>embeddings</code>.
</p>

<ul>
<li><p> The <code>input</code> column contains the input texts or image labels
</p>
</li>
<li><p> The <code>embeddings</code> column is a list column where each row contains an embedding vector
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Text embeddings
voyage_embedding("How does photosynthesis work?")

# Multimodal embeddings
list("A banana", img("banana.jpg"), "Yellow fruit") |&gt;
  voyage_embedding()

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
