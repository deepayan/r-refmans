<!DOCTYPE html><html><head><title>Help for package RSSL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RSSL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RSSL-package'><p>RSSL: Implementations of Semi-Supervised Learning Approaches for Classification</p></a></li>
<li><a href='#add_missinglabels_mar'><p>Throw out labels at random</p></a></li>
<li><a href='#adjacency_knn'><p>Calculate knn adjacency matrix</p></a></li>
<li><a href='#BaseClassifier'><p>Classifier used for enabling shared documenting of parameters</p></a></li>
<li><a href='#c.CrossValidation'><p>Merge result of cross-validation runs on single datasets into a the same object</p></a></li>
<li><a href='#clapply'><p>Use mclapply conditional on not being in RStudio</p></a></li>
<li><a href='#cov_ml'><p>Biased (maximum likelihood) estimate of the covariance matrix</p></a></li>
<li><a href='#CrossValidationSSL'><p>Cross-validation in semi-supervised setting</p></a></li>
<li><a href='#decisionvalues'><p>Decision values returned by a classifier for a set of objects</p></a></li>
<li><a href='#df_to_matrices'><p>Convert data.frame with missing labels to matrices</p></a></li>
<li><a href='#diabetes'><p>diabetes data for unit testing</p></a></li>
<li><a href='#EMLeastSquaresClassifier'><p>An Expectation Maximization like approach to Semi-Supervised Least Squares Classification</p></a></li>
<li><a href='#EMLinearDiscriminantClassifier'><p>Semi-Supervised Linear Discriminant Analysis using Expectation Maximization</p></a></li>
<li><a href='#EMNearestMeanClassifier'><p>Semi-Supervised Nearest Mean Classifier using Expectation Maximization</p></a></li>
<li><a href='#EntropyRegularizedLogisticRegression'><p>Entropy Regularized Logistic Regression</p></a></li>
<li><a href='#find_a_violated_label'><p>Find a violated label</p></a></li>
<li><a href='#gaussian_kernel'><p>calculated the gaussian kernel matrix</p></a></li>
<li><a href='#generate2ClassGaussian'><p>Generate data from 2 Gaussian distributed classes</p></a></li>
<li><a href='#generateABA'><p>Generate data from 2 alternating classes</p></a></li>
<li><a href='#generateCrescentMoon'><p>Generate Crescent Moon dataset</p></a></li>
<li><a href='#generateFourClusters'><p>Generate Four Clusters dataset</p></a></li>
<li><a href='#generateParallelPlanes'><p>Generate Parallel planes</p></a></li>
<li><a href='#generateSlicedCookie'><p>Generate Sliced Cookie dataset</p></a></li>
<li><a href='#generateSpirals'><p>Generate Intersecting Spirals</p></a></li>
<li><a href='#generateTwoCircles'><p>Generate data from 2 circles</p></a></li>
<li><a href='#geom_classifier'><p>Plot RSSL classifier boundary (deprecated)</p></a></li>
<li><a href='#geom_linearclassifier'><p>Plot linear RSSL classifier boundary</p></a></li>
<li><a href='#GRFClassifier'><p>Label propagation using Gaussian Random Fields and Harmonic functions</p></a></li>
<li><a href='#harmonic_function'><p>Direct R Translation of Xiaojin Zhu's Matlab code to determine harmonic solution</p></a></li>
<li><a href='#ICLeastSquaresClassifier'><p>Implicitly Constrained Least Squares Classifier</p></a></li>
<li><a href='#ICLinearDiscriminantClassifier'><p>Implicitly Constrained Semi-supervised Linear Discriminant Classifier</p></a></li>
<li><a href='#KernelICLeastSquaresClassifier'><p>Kernelized Implicitly Constrained Least Squares Classification</p></a></li>
<li><a href='#KernelLeastSquaresClassifier'><p>Kernelized Least Squares Classifier</p></a></li>
<li><a href='#LaplacianKernelLeastSquaresClassifier'><p>Laplacian Regularized Least Squares Classifier</p></a></li>
<li><a href='#LaplacianSVM'><p>Laplacian SVM classifier</p></a></li>
<li><a href='#LearningCurveSSL'><p>Compute Semi-Supervised Learning Curve</p></a></li>
<li><a href='#LeastSquaresClassifier'><p>Least Squares Classifier</p></a></li>
<li><a href='#line_coefficients'><p>Loss of a classifier or regression function</p></a></li>
<li><a href='#LinearDiscriminantClassifier'><p>Linear Discriminant Classifier</p></a></li>
<li><a href='#LinearSVM'><p>Linear SVM Classifier</p></a></li>
<li><a href='#LinearSVM-class'><p>LinearSVM Class</p></a></li>
<li><a href='#LinearTSVM'><p>Linear CCCP Transductive SVM classifier</p></a></li>
<li><a href='#localDescent'><p>Local descent</p></a></li>
<li><a href='#LogisticLossClassifier'><p>Logistic Loss Classifier</p></a></li>
<li><a href='#LogisticLossClassifier-class'><p>LogisticLossClassifier</p></a></li>
<li><a href='#LogisticRegression'><p>(Regularized) Logistic Regression implementation</p></a></li>
<li><a href='#LogisticRegressionFast'><p>Logistic Regression implementation that uses R's glm</p></a></li>
<li><a href='#logsumexp'><p>Numerically more stable way to calculate log sum exp</p></a></li>
<li><a href='#loss'><p>Loss of a classifier or regression function</p></a></li>
<li><a href='#losslogsum'><p>LogsumLoss of a classifier or regression function</p></a></li>
<li><a href='#losspart'><p>Loss of a classifier or regression function evaluated on partial labels</p></a></li>
<li><a href='#MajorityClassClassifier'><p>Majority Class Classifier</p></a></li>
<li><a href='#MCLinearDiscriminantClassifier'><p>Moment Constrained Semi-supervised Linear Discriminant Analysis.</p></a></li>
<li><a href='#MCNearestMeanClassifier'><p>Moment Constrained Semi-supervised Nearest Mean Classifier</p></a></li>
<li><a href='#MCPLDA'><p>Maximum Contrastive Pessimistic Likelihood Estimation for Linear Discriminant Analysis</p></a></li>
<li><a href='#measure_accuracy'><p>Performance measures used in classifier evaluation</p></a></li>
<li><a href='#minimaxlda'><p>Implements weighted likelihood estimation for LDA</p></a></li>
<li><a href='#missing_labels'><p>Access the true labels for the objects with missing labels when they are stored as an attribute in a data frame</p></a></li>
<li><a href='#NearestMeanClassifier'><p>Nearest Mean Classifier</p></a></li>
<li><a href='#plot.CrossValidation'><p>Plot CrossValidation object</p></a></li>
<li><a href='#plot.LearningCurve'><p>Plot LearningCurve object</p></a></li>
<li><a href='#posterior'><p>Class Posteriors of a classifier</p></a></li>
<li><a href='#predict,scaleMatrix-method'><p>Predict for matrix scaling inspired by stdize from the PLS package</p></a></li>
<li><a href='#PreProcessing'><p>Preprocess the input to a classification function</p></a></li>
<li><a href='#PreProcessingPredict'><p>Preprocess the input for a new set of test objects for classifier</p></a></li>
<li><a href='#print.CrossValidation'><p>Print CrossValidation object</p></a></li>
<li><a href='#print.LearningCurve'><p>Print LearningCurve object</p></a></li>
<li><a href='#projection_simplex'><p>Project an n-dim vector y to the simplex Dn</p></a></li>
<li><a href='#QuadraticDiscriminantClassifier'><p>Quadratic Discriminant Classifier</p></a></li>
<li><a href='#responsibilities'><p>Responsibilities assigned to the unlabeled objects</p></a></li>
<li><a href='#rssl-formatting'><p>Show RSSL classifier</p></a></li>
<li><a href='#rssl-predict'><p>Predict using RSSL classifier</p></a></li>
<li><a href='#S4VM'><p>Safe Semi-supervised Support Vector Machine (S4VM)</p></a></li>
<li><a href='#S4VM-class'><p>LinearSVM Class</p></a></li>
<li><a href='#sample_k_per_level'><p>Sample k indices per levels from a factor</p></a></li>
<li><a href='#scaleMatrix'><p>Matrix centering and scaling</p></a></li>
<li><a href='#SelfLearning'><p>Self-Learning approach to Semi-supervised Learning</p></a></li>
<li><a href='#solve_svm'><p>SVM solve.QP implementation</p></a></li>
<li><a href='#split_dataset_ssl'><p>Create Train, Test and Unlabeled Set</p></a></li>
<li><a href='#split_random'><p>Randomly split dataset in multiple parts</p></a></li>
<li><a href='#SSLDataFrameToMatrices'><p>Convert data.frame to matrices for semi-supervised learners</p></a></li>
<li><a href='#stat_classifier'><p>Plot RSSL classifier boundaries</p></a></li>
<li><a href='#stderror'><p>Calculate the standard error of the mean from a vector of numbers</p></a></li>
<li><a href='#summary.CrossValidation'><p>Summary of Crossvalidation results</p></a></li>
<li><a href='#svdinv'><p>Inverse of a matrix using the singular value decomposition</p></a></li>
<li><a href='#svdinvsqrtm'><p>Taking the inverse of the square root of the matrix using the singular value decomposition</p></a></li>
<li><a href='#svdsqrtm'><p>Taking the square root of a matrix using the singular value decomposition</p></a></li>
<li><a href='#SVM'><p>SVM Classifier</p></a></li>
<li><a href='#svmlin'><p>svmlin implementation by Sindhwani &amp; Keerthi (2006)</p></a></li>
<li><a href='#svmlin_example'><p>Test data from the svmlin implementation</p></a></li>
<li><a href='#svmproblem'><p>Train SVM</p></a></li>
<li><a href='#testdata'><p>Example semi-supervised problem</p></a></li>
<li><a href='#threshold'><p>Refine the prediction to satisfy the balance constraint</p></a></li>
<li><a href='#true_labels'><p>Access the true labels when they are stored as an attribute in a data frame</p></a></li>
<li><a href='#TSVM'><p>Transductive SVM classifier using the convex concave procedure</p></a></li>
<li><a href='#USMLeastSquaresClassifier'><p>Updated Second Moment Least Squares Classifier</p></a></li>
<li><a href='#USMLeastSquaresClassifier-class'><p>USMLeastSquaresClassifier</p></a></li>
<li><a href='#wdbc'><p>wdbc data for unit testing</p></a></li>
<li><a href='#WellSVM'><p>WellSVM for Semi-supervised Learning</p></a></li>
<li><a href='#wellsvm_direct'><p>wellsvm implements the wellsvm algorithm as shown in [1].</p></a></li>
<li><a href='#WellSVM_SSL'><p>Convex relaxation of S3VM by label generation</p></a></li>
<li><a href='#WellSVM_supervised'><p>A degenerated version of WellSVM where the labels are complete, that is, supervised learning</p></a></li>
<li><a href='#wlda'><p>Implements weighted likelihood estimation for LDA</p></a></li>
<li><a href='#wlda_error'><p>Measures the expected error of the LDA model defined by m, p,</p>
and iW on the data set a, where weights w are potentially taken into
account</a></li>
<li><a href='#wlda_loglik'><p>Measures the expected log-likelihood of the LDA model defined by m, p,</p>
and iW on the data set a, where weights w are potentially taken into
account</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.9.7</td>
</tr>
<tr>
<td>Title:</td>
<td>Implementations of Semi-Supervised Learning Approaches for
Classification</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, Rcpp, MASS, kernlab, quadprog, Matrix, dplyr, tidyr,
ggplot2, reshape2, scales, cluster</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, rmarkdown, SparseM, numDeriv, LiblineaR, covr</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of implementations of semi-supervised classifiers
    and methods to evaluate their performance. The package includes implementations
    of, among others, Implicitly Constrained Learning, Moment Constrained Learning,
    the Transductive SVM, Manifold regularization, Maximum Contrastive Pessimistic
    Likelihood estimation, S4VM and WellSVM.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jkrijthe/RSSL">https://github.com/jkrijthe/RSSL</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jkrijthe/RSSL">https://github.com/jkrijthe/RSSL</a></td>
</tr>
<tr>
<td>Collate:</td>
<td>'Generics.R' 'Classifier.R' 'CrossValidation.R'
'LeastSquaresClassifier.R' 'EMLeastSquaresClassifier.R'
'NormalBasedClassifier.R' 'LinearDiscriminantClassifier.R'
'EMLinearDiscriminantClassifier.R' 'NearestMeanClassifier.R'
'EMNearestMeanClassifier.R' 'LogisticRegression.R'
'EntropyRegularizedLogisticRegression.R' 'Evaluate.R'
'GRFClassifier.R' 'GenerateSSLData.R' 'HelperFunctions.R'
'ICLeastSquaresClassifier.R' 'ICLinearDiscriminantClassifier.R'
'KernelLeastSquaresClassifier.R'
'KernelICLeastSquaresClassifier.R'
'LaplacianKernelLeastSquaresClassifier.R' 'LaplacianSVM.R'
'LearningCurve.R' 'LinearSVM.R' 'LogisticLossClassifier.R'
'MCLinearDiscriminantClassifier.R' 'MCNearestMeanClassifier.R'
'MCPLDA.R' 'MajorityClassClassifier.R' 'Measures.R'
'Plotting.R' 'QuadraticDiscriminantClassifier.R'
'RSSL-package.R' 'RcppExports.R' 'S4VM.R' 'SVM.R'
'SelfLearning.R' 'TSVM.R' 'USMLeastSquaresClassifier.R'
'WellSVM.R' 'scaleMatrix.R' 'svmd.R' 'svmlin.R'
'testdata-data.R'</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-07 04:22:03 UTC; jkrijthe</td>
</tr>
<tr>
<td>Author:</td>
<td>Jesse Krijthe [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jesse Krijthe &lt;jkrijthe@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-07 06:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='RSSL-package'>RSSL: Implementations of Semi-Supervised Learning Approaches for Classification</h2><span id='topic+RSSL'></span><span id='topic+RSSL-package'></span>

<h3>Description</h3>

<p>A collection of implementations of semi-supervised classifiers and methods to evaluate their performance. The package includes implementations of, among others, Implicitly Constrained Learning, Moment Constrained Learning, the Transductive SVM, Manifold regularization, Maximum Contrastive Pessimistic Likelihood estimation, S4VM and WellSVM.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jesse Krijthe <a href="mailto:jkrijthe@gmail.com">jkrijthe@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/jkrijthe/RSSL">https://github.com/jkrijthe/RSSL</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/jkrijthe/RSSL">https://github.com/jkrijthe/RSSL</a>
</p>
</li></ul>


<hr>
<h2 id='add_missinglabels_mar'>Throw out labels at random</h2><span id='topic+add_missinglabels_mar'></span>

<h3>Description</h3>

<p>Original labels are saved in attribute <code>y_true</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_missinglabels_mar(df, formula = NULL, prob = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_missinglabels_mar_+3A_df">df</code></td>
<td>
<p>data.frame; Data frame of interest</p>
</td></tr>
<tr><td><code id="add_missinglabels_mar_+3A_formula">formula</code></td>
<td>
<p>formula; Formula to indicate the outputs</p>
</td></tr>
<tr><td><code id="add_missinglabels_mar_+3A_prob">prob</code></td>
<td>
<p>numeric; Probability of removing the label</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='adjacency_knn'>Calculate knn adjacency matrix</h2><span id='topic+adjacency_knn'></span>

<h3>Description</h3>

<p>Calculates symmetric adjacency: objects are neighbours is either one of them is in the set of nearest neighbours of the other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjacency_knn(X, distance = "euclidean", k = 6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjacency_knn_+3A_x">X</code></td>
<td>
<p>matrix; input matrix</p>
</td></tr>
<tr><td><code id="adjacency_knn_+3A_distance">distance</code></td>
<td>
<p>character; distance metric used in the <code>dist</code> function</p>
</td></tr>
<tr><td><code id="adjacency_knn_+3A_k">k</code></td>
<td>
<p>integer; Number of neighbours</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Symmetric binary adjacency matrix
</p>

<hr>
<h2 id='BaseClassifier'>Classifier used for enabling shared documenting of parameters</h2><span id='topic+BaseClassifier'></span>

<h3>Description</h3>

<p>Classifier used for enabling shared documenting of parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BaseClassifier(X, y, X_u, verbose, scale, eps, x_center, intercept, lambda,
  y_scale, kernel, use_Xu_for_scaling, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BaseClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_eps">eps</code></td>
<td>
<p>numeric; Stopping criterion for the maximinimization</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
<tr><td><code id="BaseClassifier_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='c.CrossValidation'>Merge result of cross-validation runs on single datasets into a the same object</h2><span id='topic+c.CrossValidation'></span>

<h3>Description</h3>

<p>Merge result of cross-validation runs on single datasets into a the same object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CrossValidation'
c(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="c.CrossValidation_+3A_...">...</code></td>
<td>
<p>Named arguments for the different objects, where the name reflects the dataset name</p>
</td></tr>
</table>

<hr>
<h2 id='clapply'>Use mclapply conditional on not being in RStudio</h2><span id='topic+clapply'></span>

<h3>Description</h3>

<p>Use mclapply conditional on not being in RStudio
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clapply(X, FUN, ..., mc.cores = getOption("mc.cores", 2L))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clapply_+3A_x">X</code></td>
<td>
<p>vector</p>
</td></tr>
<tr><td><code id="clapply_+3A_fun">FUN</code></td>
<td>
<p>function to be applied to the elements of X</p>
</td></tr>
<tr><td><code id="clapply_+3A_...">...</code></td>
<td>
<p>optional arguments passed to FUN</p>
</td></tr>
<tr><td><code id="clapply_+3A_mc.cores">mc.cores</code></td>
<td>
<p>number of cores to use</p>
</td></tr>
</table>

<hr>
<h2 id='cov_ml'>Biased (maximum likelihood) estimate of the covariance matrix</h2><span id='topic+cov_ml'></span>

<h3>Description</h3>

<p>Biased (maximum likelihood) estimate of the covariance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_ml(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_ml_+3A_x">X</code></td>
<td>
<p>matrix with observations</p>
</td></tr>
</table>

<hr>
<h2 id='CrossValidationSSL'>Cross-validation in semi-supervised setting</h2><span id='topic+CrossValidationSSL'></span><span id='topic+CrossValidationSSL.list'></span><span id='topic+CrossValidationSSL.matrix'></span>

<h3>Description</h3>

<p>Cross-validation for semi-supervised learning, in which the dataset is split in three parts: labeled training object, unlabeled training object and validation objects. This can be used to evaluate different approaches to semi-supervised classification under the assumption the labels are missing at random. Different cross-validation schemes are implemented. See below for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CrossValidationSSL(X, y, ...)

## S3 method for class 'list'
CrossValidationSSL(X, y, ..., verbose = FALSE, mc.cores = 1)

## S3 method for class 'matrix'
CrossValidationSSL(X, y, classifiers, measures = list(Error
  = measure_error), k = 10, repeats = 1, verbose = FALSE,
  leaveout = "test", n_labeled = 10, prop_unlabeled = 0.5, time = TRUE,
  pre_scale = FALSE, pre_pca = FALSE, n_min = 1, low_level_cores = 1,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CrossValidationSSL_+3A_x">X</code></td>
<td>
<p>design matrix of the labeled objects</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_y">y</code></td>
<td>
<p>vector with labels</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_...">...</code></td>
<td>
<p>arguments passed to underlying functions</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_mc.cores">mc.cores</code></td>
<td>
<p>integer; Number of cores to be used</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_classifiers">classifiers</code></td>
<td>
<p>list; Classifiers to crossvalidate</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_measures">measures</code></td>
<td>
<p>named list of functions giving the measures to be used</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_k">k</code></td>
<td>
<p>integer; Number of folds in the cross-validation</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_repeats">repeats</code></td>
<td>
<p>integer; Number of repeated assignments to folds</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_leaveout">leaveout</code></td>
<td>
<p>either &quot;labeled&quot; or &quot;test&quot;, see details</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_n_labeled">n_labeled</code></td>
<td>
<p>Number of labeled examples, used in both leaveout modes</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_prop_unlabeled">prop_unlabeled</code></td>
<td>
<p>numeric; proportion of unlabeled objects</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_time">time</code></td>
<td>
<p>logical; Whether execution time should be saved.</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_pre_scale">pre_scale</code></td>
<td>
<p>logical; Whether the features should be scaled before the dataset is used</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_pre_pca">pre_pca</code></td>
<td>
<p>logical; Whether the features should be preprocessed using a PCA step</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_n_min">n_min</code></td>
<td>
<p>integer; Minimum number of labeled objects per class</p>
</td></tr>
<tr><td><code id="CrossValidationSSL_+3A_low_level_cores">low_level_cores</code></td>
<td>
<p>integer; Number of cores to use compute repeats of the learning curve</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input to this function can be either: a dataset in the form of a feature matrix and factor containing the labels, a dataset in the form of a formula and data.frame or a named list of these two options.
There are two main modes in which the cross-validation can be carried out, controlled by the <code>leaveout</code> parameter. 
When leaveout is &quot;labeled&quot;, the folds are formed by non-overlapping labeled training sets of a user specified size. 
Each of these folds is used as a labeled set, while the rest of the objects are split into the an unlabeled and the test set, controlled by <code>prop_unlabeled</code> parameter. Note that objects can be used multiple times for testing, when training on a different fold, while other objects may never used for testing.
</p>
<p>The &quot;test&quot; option of <code>leaveout</code>, on the other hand, uses the folds as the test sets. This means every object will be used as a test object exactly once. The remaining objects in each training iteration are split randomly into a labeled and an unlabeled part, where the number of the labeled objects is controlled by the user through the n_labeled parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- model.matrix(Species~.-1,data=iris)
y &lt;- iris$Species

classifiers &lt;- list("LS"=function(X,y,X_u,y_u) {
  LeastSquaresClassifier(X,y,lambda=0)}, 
  "EM"=function(X,y,X_u,y_u) {
    SelfLearning(X,y,X_u,
                 method=LeastSquaresClassifier)}
)

measures &lt;- list("Accuracy" =  measure_accuracy,
                 "Loss" = measure_losstest,
                 "Loss labeled" = measure_losslab,
                 "Loss Lab+Unlab" = measure_losstrain
)

# Cross-validation making sure test folds are non-overlapping
cvresults1 &lt;- CrossValidationSSL(X,y, 
                                 classifiers=classifiers, 
                                 measures=measures,
                                 leaveout="test", k=10,
                                 repeats = 2,n_labeled = 10)
print(cvresults1)
plot(cvresults1)

# Cross-validation making sure labeled sets are non-overlapping
cvresults2 &lt;- CrossValidationSSL(X,y, 
                                 classifiers=classifiers, 
                                 measures=measures,
                                 leaveout="labeled", k=10,
                                 repeats = 2,n_labeled = 10,
                                 prop_unlabeled=0.5)
print(cvresults2)
plot(cvresults2)

</code></pre>

<hr>
<h2 id='decisionvalues'>Decision values returned by a classifier for a set of objects</h2><span id='topic+decisionvalues'></span><span id='topic+decisionvalues+2CLeastSquaresClassifier-method'></span><span id='topic+decisionvalues+2CKernelLeastSquaresClassifier-method'></span><span id='topic+decisionvalues+2CLinearSVM-method'></span><span id='topic+decisionvalues+2CSVM-method'></span><span id='topic+decisionvalues+2CTSVM-method'></span><span id='topic+decisionvalues+2CsvmlinClassifier-method'></span>

<h3>Description</h3>

<p>Returns decision values of a classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decisionvalues(object, newdata)

## S4 method for signature 'LeastSquaresClassifier'
decisionvalues(object, newdata)

## S4 method for signature 'KernelLeastSquaresClassifier'
decisionvalues(object, newdata)

## S4 method for signature 'LinearSVM'
decisionvalues(object, newdata)

## S4 method for signature 'SVM'
decisionvalues(object, newdata)

## S4 method for signature 'TSVM'
decisionvalues(object, newdata)

## S4 method for signature 'svmlinClassifier'
decisionvalues(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decisionvalues_+3A_object">object</code></td>
<td>
<p>Classifier object</p>
</td></tr>
<tr><td><code id="decisionvalues_+3A_newdata">newdata</code></td>
<td>
<p>new data to classify</p>
</td></tr>
</table>

<hr>
<h2 id='df_to_matrices'>Convert data.frame with missing labels to matrices</h2><span id='topic+df_to_matrices'></span>

<h3>Description</h3>

<p>Convert data.frame with missing labels to matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_to_matrices(df, formula = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="df_to_matrices_+3A_df">df</code></td>
<td>
<p>data.frame; Data</p>
</td></tr>
<tr><td><code id="df_to_matrices_+3A_formula">formula</code></td>
<td>
<p>formula; Description of problem</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='diabetes'>diabetes data for unit testing</h2><span id='topic+diabetes'></span>

<h3>Description</h3>

<p>Useful for testing the WellSVM implementation
</p>

<hr>
<h2 id='EMLeastSquaresClassifier'>An Expectation Maximization like approach to Semi-Supervised Least Squares Classification</h2><span id='topic+EMLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>As studied in Krijthe &amp; Loog (2016), minimizes the total loss of the labeled and unlabeled objects by finding the weight vector and labels that minimize the total loss. The algorithm proceeds similar to EM, by subsequently applying a weight update and a soft labeling of the unlabeled objects. This is repeated until convergence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMLeastSquaresClassifier(X, y, X_u, x_center = FALSE, scale = FALSE,
  verbose = FALSE, intercept = TRUE, lambda = 0, eps = 1e-09,
  y_scale = FALSE, alpha = 1, beta = 1, init = "supervised",
  method = "block", objective = "label", save_all = FALSE,
  max_iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_eps">eps</code></td>
<td>
<p>Stopping criterion for the minimization</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_alpha">alpha</code></td>
<td>
<p>numeric; the mixture of the new responsibilities and the old in each iteration of the algorithm (default: 1)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_beta">beta</code></td>
<td>
<p>numeric; value between 0 and 1 that determines how much to move to the new solution from the old solution at each step of the block gradient descent</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_init">init</code></td>
<td>
<p>objective character; &quot;random&quot; for random initialization of labels, &quot;supervised&quot; to use supervised solution as initialization or a numeric vector with a coefficient vector to use to calculate the initialization</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_method">method</code></td>
<td>
<p>character; one of &quot;block&quot;, for block gradient descent or &quot;simple&quot; for LBFGS optimization (default=&quot;block&quot;)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_objective">objective</code></td>
<td>
<p>character; &quot;responsibility&quot; for hard label self-learning or &quot;label&quot; for soft-label self-learning</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_save_all">save_all</code></td>
<td>
<p>logical; saves all classifiers trained during block gradient descent</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifier_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; maximum number of iterations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default (method=&quot;block&quot;) the weights of the classifier are updated, after which the unknown labels are updated. method=&quot;simple&quot; uses LBFGS to do this update simultaneously. Objective=&quot;responsibility&quot; corresponds to the responsibility based, instead of the label based, objective function in Krijthe &amp; Loog (2016), which is equivalent to hard-label self-learning.
</p>


<h3>References</h3>

<p>Krijthe, J.H. &amp; Loog, M., 2016. Optimistic Semi-supervised Least Squares Classification. In International Conference on Pattern Recognition (To Appear).
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(ggplot2)

set.seed(1)

df &lt;- generate2ClassGaussian(200,d=2,var=0.2) %&gt;% 
 add_missinglabels_mar(Class~.,prob = 0.96)

# Soft-label vs. hard-label self-learning
classifiers &lt;- list(
 "Supervised"=LeastSquaresClassifier(Class~.,df),
 "EM-Soft"=EMLeastSquaresClassifier(Class~.,df,objective="label"),
 "EM-Hard"=EMLeastSquaresClassifier(Class~.,df,objective="responsibility")
)

df %&gt;% 
 ggplot(aes(x=X1,y=X2,color=Class)) +
 geom_point() +
 coord_equal() +
 scale_y_continuous(limits=c(-2,2)) +
 stat_classifier(aes(linetype=..classifier..),
                 classifiers=classifiers)
                 
</code></pre>

<hr>
<h2 id='EMLinearDiscriminantClassifier'>Semi-Supervised Linear Discriminant Analysis using Expectation Maximization</h2><span id='topic+EMLinearDiscriminantClassifier'></span>

<h3>Description</h3>

<p>Expectation Maximization applied to the linear discriminant classifier assuming Gaussian classes with a shared covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMLinearDiscriminantClassifier(X, y, X_u, method = "EM", scale = FALSE,
  eps = 1e-08, verbose = FALSE, max_iter = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_method">method</code></td>
<td>
<p>character; Currently only &quot;EM&quot;</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_eps">eps</code></td>
<td>
<p>Stopping criterion for the maximinimization</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="EMLinearDiscriminantClassifier_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Starting from the supervised solution, uses the Expectation Maximization algorithm (see Dempster et al. (1977)) to iteratively update the means and shared covariance of the classes (Maximization step) and updates the responsibilities for the unlabeled objects (Expectation step).
</p>


<h3>References</h3>

<p>Dempster, A., Laird, N. &amp; Rubin, D., 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B, 39(1), pp.1-38.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='EMNearestMeanClassifier'>Semi-Supervised Nearest Mean Classifier using Expectation Maximization</h2><span id='topic+EMNearestMeanClassifier'></span>

<h3>Description</h3>

<p>Expectation Maximization applied to the nearest mean classifier assuming Gaussian classes with a spherical covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMNearestMeanClassifier(X, y, X_u, method = "EM", scale = FALSE,
  eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMNearestMeanClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifier_+3A_method">method</code></td>
<td>
<p>character; Currently only &quot;EM&quot;</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifier_+3A_scale">scale</code></td>
<td>
<p>Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifier_+3A_eps">eps</code></td>
<td>
<p>Stopping criterion for the maximinimization</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Starting from the supervised solution, uses the Expectation Maximization algorithm (see Dempster et al. (1977)) to iteratively update the means and shared covariance of the classes (Maximization step) and updates the responsibilities for the unlabeled objects (Expectation step).
</p>


<h3>References</h3>

<p>Dempster, A., Laird, N. &amp; Rubin, D., 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B, 39(1), pp.1-38.
</p>

<hr>
<h2 id='EntropyRegularizedLogisticRegression'>Entropy Regularized Logistic Regression</h2><span id='topic+EntropyRegularizedLogisticRegression'></span>

<h3>Description</h3>

<p>R Implementation of entropy regularized logistic regression implementation as proposed by Grandvalet &amp; Bengio (2005). An extra term is added to the objective function of logistic regression that penalizes the entropy of the posterior measured on the unlabeled examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EntropyRegularizedLogisticRegression(X, y, X_u = NULL, lambda = 0,
  lambda_entropy = 1, intercept = TRUE, init = NA, scale = FALSE,
  x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_lambda">lambda</code></td>
<td>
<p>l2 Regularization</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_lambda_entropy">lambda_entropy</code></td>
<td>
<p>Weight of the labeled observations compared to the unlabeled observations</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_init">init</code></td>
<td>
<p>Initial parameters for the gradient descent</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegression_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class EntropyRegularizedLogisticRegression with the following slots:
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>the names of the classes</p>
</td></tr>
</table>


<h3>References</h3>

<p>Grandvalet, Y. &amp; Bengio, Y., 2005. Semi-supervised learning by entropy minimization. In L. K. Saul, Y. Weiss, &amp; L. Bottou, eds. Advances in Neural Information Processing Systems 17. Cambridge, MA: MIT Press, pp. 529-536.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)


# An example where ERLR finds a low-density separator, which is not
# the correct solution.
set.seed(1)
df &lt;- generateSlicedCookie(1000,expected=FALSE) %&gt;% 
  add_missinglabels_mar(Class~.,0.98)

class_lr &lt;- LogisticRegression(Class~.,df,lambda = 0.01)
class_erlr &lt;- EntropyRegularizedLogisticRegression(Class~.,df,
                                lambda=0.01,lambda_entropy = 100)


ggplot(df,aes(x=X1,y=X2,color=Class)) +
  geom_point() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = list("LR"=class_lr,"ERLR"=class_erlr)) +
  scale_y_continuous(limits=c(-2,2)) +
  scale_x_continuous(limits=c(-2,2))

df_test &lt;- generateSlicedCookie(1000,expected=FALSE)
mean(predict(class_lr,df_test)==df_test$Class)
mean(predict(class_erlr,df_test)==df_test$Class)




</code></pre>

<hr>
<h2 id='find_a_violated_label'>Find a violated label</h2><span id='topic+find_a_violated_label'></span>

<h3>Description</h3>

<p>Find a violated label
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_a_violated_label(alpha, K, y, ind_y, lr, y_init)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_a_violated_label_+3A_alpha">alpha</code></td>
<td>
<p>classifier weights</p>
</td></tr>
<tr><td><code id="find_a_violated_label_+3A_k">K</code></td>
<td>
<p>kernel matrix</p>
</td></tr>
<tr><td><code id="find_a_violated_label_+3A_y">y</code></td>
<td>
<p>label vector</p>
</td></tr>
<tr><td><code id="find_a_violated_label_+3A_ind_y">ind_y</code></td>
<td>
<p>Labeled/Unlabeled indicator</p>
</td></tr>
<tr><td><code id="find_a_violated_label_+3A_lr">lr</code></td>
<td>
<p>positive ratio</p>
</td></tr>
<tr><td><code id="find_a_violated_label_+3A_y_init">y_init</code></td>
<td>
<p>label initialization</p>
</td></tr>
</table>

<hr>
<h2 id='gaussian_kernel'>calculated the gaussian kernel matrix</h2><span id='topic+gaussian_kernel'></span>

<h3>Description</h3>

<p>calculated the gaussian kernel matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_kernel(x, gamma, x_test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_kernel_+3A_x">x</code></td>
<td>
<p>A d x n training data matrix</p>
</td></tr>
<tr><td><code id="gaussian_kernel_+3A_gamma">gamma</code></td>
<td>
<p>kernel parameter</p>
</td></tr>
<tr><td><code id="gaussian_kernel_+3A_x_test">x_test</code></td>
<td>
<p>A d x m testing data matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>k - A n x m kernel matrix and dis_mat - A n x m distance matrix
</p>

<hr>
<h2 id='generate2ClassGaussian'>Generate data from 2 Gaussian distributed classes</h2><span id='topic+generate2ClassGaussian'></span>

<h3>Description</h3>

<p>Generate data from 2 Gaussian distributed classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate2ClassGaussian(n = 10000, d = 100, var = 1, expected = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate2ClassGaussian_+3A_n">n</code></td>
<td>
<p>integer; Number of examples to generate</p>
</td></tr>
<tr><td><code id="generate2ClassGaussian_+3A_d">d</code></td>
<td>
<p>integer; dimensionality of the problem</p>
</td></tr>
<tr><td><code id="generate2ClassGaussian_+3A_var">var</code></td>
<td>
<p>numeric; size of the variance parameter</p>
</td></tr>
<tr><td><code id="generate2ClassGaussian_+3A_expected">expected</code></td>
<td>
<p>logical; whether the decision boundary should be the expected or perpendicular</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- generate2ClassGaussian(n=1000,d=2,expected=FALSE)
plot(data[,1],data[,2],col=data$Class,asp=1)
</code></pre>

<hr>
<h2 id='generateABA'>Generate data from 2 alternating classes</h2><span id='topic+generateABA'></span>

<h3>Description</h3>

<p>Two clusters belonging to three classes: the cluster in the middle belongs to one class and the two on the outside to the others.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateABA(n = 100, d = 2, var = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateABA_+3A_n">n</code></td>
<td>
<p>integer; Number of examples to generate</p>
</td></tr>
<tr><td><code id="generateABA_+3A_d">d</code></td>
<td>
<p>integer; dimensionality of the problem</p>
</td></tr>
<tr><td><code id="generateABA_+3A_var">var</code></td>
<td>
<p>numeric; size of the variance parameter</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- generateABA(n=1000,d=2,var=1)
plot(data[,1],data[,2],col=data$Class,asp=1)
</code></pre>

<hr>
<h2 id='generateCrescentMoon'>Generate Crescent Moon dataset</h2><span id='topic+generateCrescentMoon'></span>

<h3>Description</h3>

<p>Generate a &quot;crescent moon&quot;/&quot;banana&quot; dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateCrescentMoon(n = 100, d = 2, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateCrescentMoon_+3A_n">n</code></td>
<td>
<p>integer; Number of objects to generate</p>
</td></tr>
<tr><td><code id="generateCrescentMoon_+3A_d">d</code></td>
<td>
<p>integer; Dimensionality of the dataset</p>
</td></tr>
<tr><td><code id="generateCrescentMoon_+3A_sigma">sigma</code></td>
<td>
<p>numeric; Noise added</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data&lt;-generateCrescentMoon(150,2,1)
plot(data$X1,data$X2,col=data$Class,asp=1)
</code></pre>

<hr>
<h2 id='generateFourClusters'>Generate Four Clusters dataset</h2><span id='topic+generateFourClusters'></span>

<h3>Description</h3>

<p>Generate a four clusters dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateFourClusters(n = 100, distance = 6, expected = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateFourClusters_+3A_n">n</code></td>
<td>
<p>integer; Number of observations to generate</p>
</td></tr>
<tr><td><code id="generateFourClusters_+3A_distance">distance</code></td>
<td>
<p>numeric; Distance between clusters (default: 6)</p>
</td></tr>
<tr><td><code id="generateFourClusters_+3A_expected">expected</code></td>
<td>
<p>logical; TRUE if the large margin equals the class boundary, FALSE if the class boundary is perpendicular to the large margin</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- generateFourClusters(1000,distance=6,expected=TRUE)
plot(data[,1],data[,2],col=data$Class,asp=1)
</code></pre>

<hr>
<h2 id='generateParallelPlanes'>Generate Parallel planes</h2><span id='topic+generateParallelPlanes'></span>

<h3>Description</h3>

<p>Generate Parallel planes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateParallelPlanes(n = 100, classes = 3, sigma = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateParallelPlanes_+3A_n">n</code></td>
<td>
<p>integer; Number of objects to generate</p>
</td></tr>
<tr><td><code id="generateParallelPlanes_+3A_classes">classes</code></td>
<td>
<p>integer; Number of classes</p>
</td></tr>
<tr><td><code id="generateParallelPlanes_+3A_sigma">sigma</code></td>
<td>
<p>double; Noise added</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
df &lt;- generateParallelPlanes(100,3)
ggplot(df, aes(x=x,y=y,color=Class,shape=Class)) +
 geom_point()

</code></pre>

<hr>
<h2 id='generateSlicedCookie'>Generate Sliced Cookie dataset</h2><span id='topic+generateSlicedCookie'></span>

<h3>Description</h3>

<p>Generate a sliced cookie dataset: a circle with a large margin in the middle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateSlicedCookie(n = 100, expected = FALSE, gap = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateSlicedCookie_+3A_n">n</code></td>
<td>
<p>integer; number of observations to generate</p>
</td></tr>
<tr><td><code id="generateSlicedCookie_+3A_expected">expected</code></td>
<td>
<p>logical; TRUE if the large margin equals the class boundary, FALSE if the class boundary is perpendicular to the large margin</p>
</td></tr>
<tr><td><code id="generateSlicedCookie_+3A_gap">gap</code></td>
<td>
<p>numeric; Size of the gap</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with n objects from the sliced cookie example
</p>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- generateSlicedCookie(1000,expected=FALSE)
plot(data[,1],data[,2],col=data$Class,asp=1)
</code></pre>

<hr>
<h2 id='generateSpirals'>Generate Intersecting Spirals</h2><span id='topic+generateSpirals'></span>

<h3>Description</h3>

<p>Generate Intersecting Spirals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateSpirals(n = 100, sigma = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateSpirals_+3A_n">n</code></td>
<td>
<p>integer; Number of objects to generate per class</p>
</td></tr>
<tr><td><code id="generateSpirals_+3A_sigma">sigma</code></td>
<td>
<p>numeric; Noise added</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateTwoCircles">generateTwoCircles</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- generateSpirals(100,sigma=0.1)
#plot3D::scatter3D(data$x,data$y,data$z,col="black")
</code></pre>

<hr>
<h2 id='generateTwoCircles'>Generate data from 2 circles</h2><span id='topic+generateTwoCircles'></span>

<h3>Description</h3>

<p>One circle circumscribes the other
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateTwoCircles(n = 100, noise_var = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateTwoCircles_+3A_n">n</code></td>
<td>
<p>integer; Number of examples to generate</p>
</td></tr>
<tr><td><code id="generateTwoCircles_+3A_noise_var">noise_var</code></td>
<td>
<p>numeric; size of the variance parameter</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL datasets: 
<code><a href="#topic+generate2ClassGaussian">generate2ClassGaussian</a>()</code>,
<code><a href="#topic+generateABA">generateABA</a>()</code>,
<code><a href="#topic+generateCrescentMoon">generateCrescentMoon</a>()</code>,
<code><a href="#topic+generateFourClusters">generateFourClusters</a>()</code>,
<code><a href="#topic+generateParallelPlanes">generateParallelPlanes</a>()</code>,
<code><a href="#topic+generateSlicedCookie">generateSlicedCookie</a>()</code>,
<code><a href="#topic+generateSpirals">generateSpirals</a>()</code>
</p>

<hr>
<h2 id='geom_classifier'>Plot RSSL classifier boundary (deprecated)</h2><span id='topic+geom_classifier'></span>

<h3>Description</h3>

<p>Deprecated: Use geom_linearclassifier or stat_classifier to plot classification boundaries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geom_classifier(..., show_guide = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geom_classifier_+3A_...">...</code></td>
<td>
<p>List of trained classifiers</p>
</td></tr>
<tr><td><code id="geom_classifier_+3A_show_guide">show_guide</code></td>
<td>
<p>logical (default: TRUE); Show legend</p>
</td></tr>
</table>

<hr>
<h2 id='geom_linearclassifier'>Plot linear RSSL classifier boundary</h2><span id='topic+geom_linearclassifier'></span>

<h3>Description</h3>

<p>Plot linear RSSL classifier boundary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geom_linearclassifier(..., show_guide = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geom_linearclassifier_+3A_...">...</code></td>
<td>
<p>List of trained classifiers</p>
</td></tr>
<tr><td><code id="geom_linearclassifier_+3A_show_guide">show_guide</code></td>
<td>
<p>logical (default: TRUE); Show legend</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
library(dplyr)

df &lt;- generate2ClassGaussian(100,d=2,var=0.2) %&gt;% 
 add_missinglabels_mar(Class~., 0.8)

df %&gt;% 
 ggplot(aes(x=X1,y=X2,color=Class)) +
 geom_point() +
 geom_linearclassifier("Supervised"=LinearDiscriminantClassifier(Class~.,df),
                       "EM"=EMLinearDiscriminantClassifier(Class~.,df))
</code></pre>

<hr>
<h2 id='GRFClassifier'>Label propagation using Gaussian Random Fields and Harmonic functions</h2><span id='topic+GRFClassifier'></span>

<h3>Description</h3>

<p>Implements the approach proposed in Zhu et al. (2003) to label propagation over an affinity graph. Note, as in the original paper, we consider the transductive scenario, so the implementation does not generalize to out of sample predictions. The approach minimizes the squared difference in labels assigned to different objects, where the contribution of each difference to the loss is weighted by the affinity between the objects. The default in this implementation is to use a knn adjacency matrix based on euclidean distance to determine this weight. Setting <code>adjacency="heat"</code> will use an RBF kernel over euclidean distances between objects to determine the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GRFClassifier(X, y, X_u, adjacency = "nn",
  adjacency_distance = "euclidean", adjacency_k = 6,
  adjacency_sigma = 0.1, class_mass_normalization = FALSE, scale = FALSE,
  x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GRFClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_adjacency">adjacency</code></td>
<td>
<p>character; &quot;nn&quot; for nearest neighbour graph or &quot;heat&quot; for radial basis adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_adjacency_distance">adjacency_distance</code></td>
<td>
<p>character; distance metric for nearest neighbour adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_adjacency_k">adjacency_k</code></td>
<td>
<p>integer; number of neighbours for the nearest neighbour adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_adjacency_sigma">adjacency_sigma</code></td>
<td>
<p>double; width of the rbf adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_class_mass_normalization">class_mass_normalization</code></td>
<td>
<p>logical; Should the Class Mass Normalization heuristic be applied? (default: FALSE)</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="GRFClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>


<h3>References</h3>

<p>Zhu, X., Ghahramani, Z. &amp; Lafferty, J., 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on Machine Learning. pp. 912-919.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

set.seed(1)
df_circles &lt;- generateTwoCircles(400,noise=0.1) %&gt;% 
  add_missinglabels_mar(Class~.,0.99)

# Visualize the problem
df_circles %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class)) +
  geom_point() + 
  coord_equal()

# Visualize the solution
class_grf &lt;- GRFClassifier(Class~.,df_circles,
                           adjacency="heat",
                           adjacency_sigma = 0.1)
df_circles %&gt;%
  filter(is.na(Class)) %&gt;% 
  mutate(Responsibility=responsibilities(class_grf)[,1]) %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Responsibility)) +
  geom_point() + 
  coord_equal()

# Generate problem
df_para &lt;- generateParallelPlanes()
df_para$Class &lt;- NA
df_para$Class[1] &lt;- "a"
df_para$Class[101] &lt;- "b"
df_para$Class[201] &lt;- "c"
df_para$Class &lt;- factor(df_para$Class)

# Visualize problem
df_para %&gt;% 
  ggplot(aes(x=x,y=y,color=Class)) +
  geom_point() + 
  coord_equal()

# Estimate GRF classifier with knn adjacency matrix (default)
class_grf &lt;- GRFClassifier(Class~.,df_para)

df_para %&gt;%
  filter(is.na(Class)) %&gt;% 
  mutate(Assignment=factor(apply(responsibilities(class_grf),1,which.max))) %&gt;% 
  ggplot(aes(x=x,y=y,color=Assignment)) +
  geom_point()
</code></pre>

<hr>
<h2 id='harmonic_function'>Direct R Translation of Xiaojin Zhu's Matlab code to determine harmonic solution</h2><span id='topic+harmonic_function'></span>

<h3>Description</h3>

<p>Direct R Translation of Xiaojin Zhu's Matlab code to determine harmonic solution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>harmonic_function(W, Y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="harmonic_function_+3A_w">W</code></td>
<td>
<p>matrix; weight matrix where the fist L rows/column correspond to the labeled examples.</p>
</td></tr>
<tr><td><code id="harmonic_function_+3A_y">Y</code></td>
<td>
<p>matrix; l by c 0,1 matrix encoding class assignments for the labeled objects</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The harmonic solution, i.e. eq (5) in the ICML paper, with or without class mass normalization
</p>

<hr>
<h2 id='ICLeastSquaresClassifier'>Implicitly Constrained Least Squares Classifier</h2><span id='topic+ICLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>Implementation of the Implicitly Constrained Least Squares Classifier (ICLS) of Krijthe &amp; Loog (2015) and the projected estimator of Krijthe &amp; Loog (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLeastSquaresClassifier(X, y, X_u = NULL, lambda1 = 0, lambda2 = 0,
  intercept = TRUE, x_center = FALSE, scale = FALSE, method = "LBFGS",
  projection = "supervised", lambda_prior = 0, trueprob = NULL,
  eps = 1e-09, y_scale = FALSE, use_Xu_for_scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>Vector or factor with class assignments</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_x_u">X_u</code></td>
<td>
<p>Design matrix of the unlabeled data, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_lambda1">lambda1</code></td>
<td>
<p>Regularization parameter in the unlabeled+labeled data regularized least squares</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_lambda2">lambda2</code></td>
<td>
<p>Regularization parameter in the labeled data only regularized least squares</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_intercept">intercept</code></td>
<td>
<p>TRUE if an intercept should be added to the model</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical; Whether the feature vectors should be centered</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; If TRUE, apply a z-transform to all observations in X and X_u before running the regression</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_method">method</code></td>
<td>
<p>Either &quot;LBFGS&quot; for solving using L-BFGS-B gradient descent or &quot;QP&quot; for a quadratic programming based solution</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_projection">projection</code></td>
<td>
<p>One of &quot;supervised&quot;, &quot;semisupervised&quot; or &quot;euclidean&quot;</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_lambda_prior">lambda_prior</code></td>
<td>
<p>numeric; prior on the deviation from the supervised mean y</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_trueprob">trueprob</code></td>
<td>
<p>numeric; true mean y for all data</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_eps">eps</code></td>
<td>
<p>numeric; Stopping criterion for the maximinimization</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="ICLeastSquaresClassifier_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In Implicitly Constrained semi-supervised Least Squares (ICLS) of Krijthe &amp; Loog (2015), we minimize the quadratic loss on the labeled objects, while enforcing that the solution has to be a solution that minimizes the quadratic loss for all objects for some (fractional) labeling of the data (the implicit constraints). The goal of this classifier is to use the unlabeled data to update the classifier, while making sure it still works well on the labeled data.
</p>
<p>The Projected estimator of Krijthe &amp; Loog (2016) builds on this by finding a classifier within the space of classifiers that minimize the quadratic loss on all objects for some labeling (the implicit constrained), that minimizes the distance to the supervised solution for some appropriately chosen distance measure. Using the projection=&quot;semisupervised&quot;, we get certain guarantees that this solution is always better than the supervised solution (see Krijthe &amp; Loog (2016)), while setting projection=&quot;supervised&quot; is equivalent to ICLS.
</p>
<p>Both methods (ICLS and the projection) can be formulated as a quadratic programming problem and solved using either a quadratic programming solver (method=&quot;QP&quot;) or using a gradient descent approach that takes into account certain bounds on the labelings (method=&quot;LBFGS&quot;). The latter is the preferred method.
</p>


<h3>Value</h3>

<p>S4 object of class ICLeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>theta</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>the names of the classes</p>
</td></tr>
<tr><td><code>modelform</code></td>
<td>
<p>formula object of the model used in regression</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>a scaling object containing the parameters of the z-transforms applied to the data</p>
</td></tr>
<tr><td><code>optimization</code></td>
<td>
<p>the object returned by the optim function</p>
</td></tr>
<tr><td><code>unlabels</code></td>
<td>
<p>the labels assigned to the unlabeled objects</p>
</td></tr>
</table>


<h3>References</h3>

<p>Krijthe, J.H. &amp; Loog, M., 2015. Implicitly Constrained Semi-Supervised Least Squares Classification. In E. Fromont, T. De Bie, &amp; M. van Leeuwen, eds. 14th International Symposium on Advances in Intelligent Data Analysis XIV (Lecture Notes in Computer Science Volume 9385). Saint Etienne. France, pp. 158-169.
</p>
<p>Krijthe, J.H. &amp; Loog, M., 2016. Projected Estimators for Robust Semi-supervised Classification. arXiv preprint arXiv:1602.07865.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(testdata)
w1 &lt;- LeastSquaresClassifier(testdata$X, testdata$y, 
                             intercept = TRUE,x_center = FALSE, scale=FALSE)
w2 &lt;- ICLeastSquaresClassifier(testdata$X, testdata$y, 
                               testdata$X_u, intercept = TRUE, x_center = FALSE, scale=FALSE)
plot(testdata$X[,1],testdata$X[,2],col=factor(testdata$y),asp=1)
points(testdata$X_u[,1],testdata$X_u[,2],col="darkgrey",pch=16,cex=0.5)

abline(line_coefficients(w1)$intercept,
       line_coefficients(w1)$slope,lty=2)
abline(line_coefficients(w2)$intercept,
       line_coefficients(w2)$slope,lty=1)

</code></pre>

<hr>
<h2 id='ICLinearDiscriminantClassifier'>Implicitly Constrained Semi-supervised Linear Discriminant Classifier</h2><span id='topic+ICLinearDiscriminantClassifier'></span>

<h3>Description</h3>

<p>Semi-supervised version of Linear Discriminant Analysis using implicit constraints as described in (Krijthe &amp; Loog 2014). This method finds the soft labeling of the unlabeled objects, whose resulting LDA solution gives the highest log-likelihood when evaluated on the labeled objects only. See also <code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICLinearDiscriminantClassifier(X, y, X_u, prior = NULL, scale = FALSE,
  init = NULL, sup_prior = FALSE, x_center = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_x">X</code></td>
<td>
<p>design matrix of the labeled objects</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_y">y</code></td>
<td>
<p>vector with labels</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_x_u">X_u</code></td>
<td>
<p>design matrix of the labeled objects</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_prior">prior</code></td>
<td>
<p>set a fixed class prior</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_init">init</code></td>
<td>
<p>not currently used</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_sup_prior">sup_prior</code></td>
<td>
<p>logical; use the prior estimates based only on the labeled data, not the imputed labels (default: FALSE)</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical; Whether the data should be centered</p>
</td></tr>
<tr><td><code id="ICLinearDiscriminantClassifier_+3A_...">...</code></td>
<td>
<p>Additional Parameters, Not used</p>
</td></tr>
</table>


<h3>References</h3>

<p>Krijthe, J.H. &amp; Loog, M., 2014. Implicitly Constrained Semi-Supervised Linear Discriminant Analysis. In International Conference on Pattern Recognition. Stockholm, pp. 3762-3767.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='KernelICLeastSquaresClassifier'>Kernelized Implicitly Constrained Least Squares Classification</h2><span id='topic+KernelICLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>A kernel version of the implicitly constrained least squares classifier, see <code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KernelICLeastSquaresClassifier(X, y, X_u, lambda = 0,
  kernel = vanilladot(), x_center = TRUE, scale = TRUE, y_scale = TRUE,
  lambda_prior = 0, classprior = 0, method = "LBFGS",
  projection = "semisupervised")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_lambda_prior">lambda_prior</code></td>
<td>
<p>numeric; regularization parameter for the posterior deviation from the prior</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_classprior">classprior</code></td>
<td>
<p>The classprior used to compare the estimated responsibilities to</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_method">method</code></td>
<td>
<p>character; Estimation method. One of c(&quot;LBFGS&quot;)</p>
</td></tr>
<tr><td><code id="KernelICLeastSquaresClassifier_+3A_projection">projection</code></td>
<td>
<p>character; The projection used. One of c(&quot;supervised&quot;,&quot;semisupervised&quot;)</p>
</td></tr>
</table>

<hr>
<h2 id='KernelLeastSquaresClassifier'>Kernelized Least Squares Classifier</h2><span id='topic+KernelLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>Use least squares regression as a classification technique using a numeric encoding of classes as targets. Note this method minimizes quadratic loss, not the truncated quadratic loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KernelLeastSquaresClassifier(X, y, lambda = 0, kernel = vanilladot(),
  x_center = TRUE, scale = TRUE, y_scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>Vector or factor with class assignments</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter of the l2 penalty in regularized least squares</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_kernel">kernel</code></td>
<td>
<p>kernlab kernel function</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>TRUE, whether the dependent variables (features) should be centered</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to the design matrix X before running the regression</p>
</td></tr>
<tr><td><code id="KernelLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>TRUE center the target vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class LeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>theta</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>the names of the classes</p>
</td></tr>
<tr><td><code>modelform</code></td>
<td>
<p>formula object of the model used in regression</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>a scaling object containing the parameters of the z-transforms applied to the data</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

# Two class problem
df &lt;- generateCrescentMoon(200)

class_lin &lt;- KernelLeastSquaresClassifier(Class~.,df,
                                          kernel=kernlab::vanilladot(), lambda=1)
class_rbf1 &lt;- KernelLeastSquaresClassifier(Class~.,df,
                                          kernel=kernlab::rbfdot(), lambda=1)
class_rbf5 &lt;- KernelLeastSquaresClassifier(Class~.,df,
                                          kernel=kernlab::rbfdot(5), lambda=1)
class_rbf10 &lt;- KernelLeastSquaresClassifier(Class~.,df,
                                           kernel=kernlab::rbfdot(10), lambda=1)

df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class,shape=Class)) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = list("Linear"=class_lin,
                                     "RBF sigma=1"=class_rbf1,
                                     "RBF sigma=5"=class_rbf5,
                                     "RBF sigma=10"=class_rbf10),
                  color="black")

# Second Example
dmat&lt;-model.matrix(Species~.-1,iris[51:150,])
tvec&lt;-droplevels(iris$Species[51:150])
testdata &lt;- data.frame(tvec,dmat[,1:2])
colnames(testdata)&lt;-c("Class","X1","X2")

precision&lt;-100
xgrid&lt;-seq(min(dmat[,1]),max(dmat[,1]),length.out=precision)
ygrid&lt;-seq(min(dmat[,2]),max(dmat[,2]),length.out=precision)
gridmat &lt;- expand.grid(xgrid,ygrid)

g_kernel&lt;-KernelLeastSquaresClassifier(dmat[,1:2],tvec,
                                       kernel=kernlab::rbfdot(0.01),
                                       lambda=0.000001,scale = TRUE)
plotframe &lt;- cbind(gridmat, decisionvalues(g_kernel,gridmat))
colnames(plotframe)&lt;- c("x","y","Output")
ggplot(plotframe, aes(x=x,y=y)) +
  geom_tile(aes(fill = Output)) +
  scale_fill_gradient(low="yellow", high="red",limits=c(0,1)) +
  geom_point(aes(x=X1,y=X2,shape=Class),data=testdata,size=3) +
  stat_classifier(classifiers=list(g_kernel))

# Multiclass problem
dmat&lt;-model.matrix(Species~.-1,iris)
tvec&lt;-iris$Species
testdata &lt;- data.frame(tvec,dmat[,1:2])
colnames(testdata)&lt;-c("Class","X1","X2")

precision&lt;-100
xgrid&lt;-seq(min(dmat[,1]),max(dmat[,1]),length.out=precision)
ygrid&lt;-seq(min(dmat[,2]),max(dmat[,2]),length.out=precision)
gridmat &lt;- expand.grid(xgrid,ygrid)

g_kernel&lt;-KernelLeastSquaresClassifier(dmat[,1:2],tvec,
                      kernel=kernlab::rbfdot(0.1),lambda=0.00001,
                      scale = TRUE,x_center=TRUE)

plotframe &lt;- cbind(gridmat, 
                   maxind=apply(decisionvalues(g_kernel,gridmat),1,which.max))
ggplot(plotframe, aes(x=Var1,y=Var2)) +
  geom_tile(aes(fill = factor(maxind,labels=levels(tvec)))) +
  geom_point(aes(x=X1,y=X2,shape=Class),data=testdata,size=4,alpha=0.5)
</code></pre>

<hr>
<h2 id='LaplacianKernelLeastSquaresClassifier'>Laplacian Regularized Least Squares Classifier</h2><span id='topic+LaplacianKernelLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>Implements manifold regularization through the graph Laplacian as proposed by Belkin et al. 2006. As an adjacency matrix, we use the k nearest neighbour graph based on a chosen distance (default: euclidean).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplacianKernelLeastSquaresClassifier(X, y, X_u, lambda = 0, gamma = 0,
  kernel = kernlab::vanilladot(), adjacency_distance = "euclidean",
  adjacency_k = 6, x_center = TRUE, scale = TRUE, y_scale = TRUE,
  normalized_laplacian = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_gamma">gamma</code></td>
<td>
<p>numeric; Weight of the unlabeled data</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_adjacency_distance">adjacency_distance</code></td>
<td>
<p>character; distance metric used to construct adjacency graph from the dist function. Default: &quot;euclidean&quot;</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_adjacency_k">adjacency_k</code></td>
<td>
<p>integer; Number of of neighbours used to construct adjacency graph.</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="LaplacianKernelLeastSquaresClassifier_+3A_normalized_laplacian">normalized_laplacian</code></td>
<td>
<p>logical; If TRUE use the normalized Laplacian, otherwise, the Laplacian is used</p>
</td></tr>
</table>


<h3>References</h3>

<p>Belkin, M., Niyogi, P. &amp; Sindhwani, V., 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7, pp.2399-2434.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

## Example 1: Half moons

# Generate a dataset
set.seed(2)
df_orig &lt;- generateCrescentMoon(100,sigma = 0.3) 
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.98)

lambda &lt;- 0.01
gamma &lt;- 10000
rbf_param &lt;- 0.125

# Train classifiers
## Not run: 
class_sup &lt;- KernelLeastSquaresClassifier(
                Class~.,df,
                kernel=kernlab::rbfdot(rbf_param),
                lambda=lambda,scale=FALSE)

class_lap &lt;- LaplacianKernelLeastSquaresClassifier(
                    Class~.,df,
                    kernel=kernlab::rbfdot(rbf_param),
                    lambda=lambda,gamma=gamma,
                    normalized_laplacian = TRUE,
                    scale=FALSE)

classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

# Plot classifiers (can take a couple of seconds)

df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class)) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black")


# Calculate the loss
lapply(classifiers,function(c) mean(loss(c,df_orig)))

## End(Not run)

## Example 2: Two circles
set.seed(1)
df_orig &lt;- generateTwoCircles(1000,noise=0.05)
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.994)

lambda &lt;- 10e-12
gamma &lt;- 100
rbf_param &lt;- 0.1

# Train classifiers
## Not run: 
class_sup &lt;- KernelLeastSquaresClassifier(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  lambda=lambda,scale=TRUE)

class_lap &lt;- LaplacianKernelLeastSquaresClassifier(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  adjacency_k = 30,
  lambda=lambda,gamma=gamma,
  normalized_laplacian = TRUE,
  scale=TRUE)

classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

# Plot classifiers (Can take a couple of seconds)
df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class,size=Class)) +
  scale_size_manual(values=c("1"=3,"2"=3),na.value=1) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black",size=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='LaplacianSVM'>Laplacian SVM classifier</h2><span id='topic+LaplacianSVM'></span>

<h3>Description</h3>

<p>Manifold regularization applied to the support vector machine as proposed in Belkin et al. (2006). As an adjacency matrix, we use the k nearest neighbour graph based on a chosen distance (default: euclidean).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplacianSVM(X, y, X_u = NULL, lambda = 1, gamma = 1, scale = TRUE,
  kernel = vanilladot(), adjacency_distance = "euclidean",
  adjacency_k = 6, normalized_laplacian = FALSE, eps = 1e-09)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplacianSVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_gamma">gamma</code></td>
<td>
<p>numeric; Weight of the unlabeled data</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_adjacency_distance">adjacency_distance</code></td>
<td>
<p>character; distance metric used to construct adjacency graph from the dist function. Default: &quot;euclidean&quot;</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_adjacency_k">adjacency_k</code></td>
<td>
<p>integer; Number of of neighbours used to construct adjacency graph.</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_normalized_laplacian">normalized_laplacian</code></td>
<td>
<p>logical; If TRUE use the normalized Laplacian, otherwise, the Laplacian is used</p>
</td></tr>
<tr><td><code id="LaplacianSVM_+3A_eps">eps</code></td>
<td>
<p>numeric; Small value to ensure positive definiteness of the matrix in the QP formulation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of type LaplacianSVM
</p>


<h3>References</h3>

<p>Belkin, M., Niyogi, P. &amp; Sindhwani, V., 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7, pp.2399-2434.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

## Example 1: Half moons

# Generate a dataset
set.seed(2)
df_orig &lt;- generateCrescentMoon(100,sigma = 0.3) 
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.98)

lambda &lt;- 0.001
C &lt;- 1/(lambda*2*sum(!is.na(df$Class)))
gamma &lt;- 10000
rbf_param &lt;- 0.125

# Train classifiers
class_sup &lt;- SVM(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  C=C,scale=FALSE)

class_lap &lt;- LaplacianSVM(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  lambda=lambda,gamma=gamma,
  normalized_laplacian = TRUE,
  scale=FALSE)

classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

# This takes a little longer to run:
# class_tsvm &lt;- TSVM(
#   Class~.,df,
#   kernel=kernlab::rbfdot(rbf_param),
#   C=C,Cstar=10,s=-0.8,
#   scale=FALSE,balancing_constraint=TRUE)
# classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup,"TSVM"=class_tsvm)

# Plot classifiers (Can take a couple of seconds)
## Not run: 
df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class)) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black")

## End(Not run)
  
# Calculate the loss
lapply(classifiers,function(c) mean(loss(c,df_orig)))

## Example 2: Two circles
set.seed(3)
df_orig &lt;- generateTwoCircles(1000,noise=0.05)
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.994)

lambda &lt;- 0.000001
C &lt;- 1/(lambda*2*sum(!is.na(df$Class)))
gamma &lt;- 100
rbf_param &lt;- 0.1

# Train classifiers (Takes a couple of seconds)
## Not run: 
class_sup &lt;- SVM(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  C=C,scale=FALSE)

class_lap &lt;- LaplacianSVM(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  adjacency_k=50, lambda=lambda,gamma=gamma,
  normalized_laplacian = TRUE,
  scale=FALSE)


classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

## End(Not run)

# Plot classifiers (Can take a couple of seconds)
## Not run: 
df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class,size=Class)) +
  scale_size_manual(values=c("1"=3,"2"=3),na.value=1) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black",size=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='LearningCurveSSL'>Compute Semi-Supervised Learning Curve</h2><span id='topic+LearningCurveSSL'></span><span id='topic+LearningCurveSSL.matrix'></span>

<h3>Description</h3>

<p>Evaluate semi-supervised classifiers for different amounts of unlabeled training examples or different fractions of unlabeled vs. labeled examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LearningCurveSSL(X, y, ...)

## S3 method for class 'matrix'
LearningCurveSSL(X, y, classifiers, measures = list(Accuracy
  = measure_accuracy), type = "unlabeled", n_l = NULL,
  with_replacement = FALSE, sizes = 2^(1:8), n_test = 1000,
  repeats = 100, verbose = FALSE, n_min = 1, dataset_name = NULL,
  test_fraction = NULL, fracs = seq(0.1, 0.9, 0.1), time = TRUE,
  pre_scale = FALSE, pre_pca = FALSE, low_level_cores = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LearningCurveSSL_+3A_x">X</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_y">y</code></td>
<td>
<p>vector of labels</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_...">...</code></td>
<td>
<p>arguments passed to underlying function</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_classifiers">classifiers</code></td>
<td>
<p>list; Classifiers to crossvalidate</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_measures">measures</code></td>
<td>
<p>named list of functions giving the measures to be used</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_type">type</code></td>
<td>
<p>Type of learning curve, either &quot;unlabeled&quot; or &quot;fraction&quot;</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_n_l">n_l</code></td>
<td>
<p>Number of labeled objects to be used in the experiments (see details)</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_with_replacement">with_replacement</code></td>
<td>
<p>Indicated whether the subsampling is done with replacement or not (default: FALSE)</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_sizes">sizes</code></td>
<td>
<p>vector with number of unlabeled objects for which to evaluate performance</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_n_test">n_test</code></td>
<td>
<p>Number of test points if with_replacement is TRUE</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_repeats">repeats</code></td>
<td>
<p>Number of learning curves to draw</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_verbose">verbose</code></td>
<td>
<p>Print progressbar during execution (default: FALSE)</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_n_min">n_min</code></td>
<td>
<p>Minimum number of labeled objects per class in</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_dataset_name">dataset_name</code></td>
<td>
<p>character; Name of the dataset</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_test_fraction">test_fraction</code></td>
<td>
<p>numeric; If not NULL a fraction of the object will be left out to serve as the test set</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_fracs">fracs</code></td>
<td>
<p>list; fractions of labeled data to use</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_time">time</code></td>
<td>
<p>logical; Whether execution time should be saved.</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_pre_scale">pre_scale</code></td>
<td>
<p>logical; Whether the features should be scaled before the dataset is used</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_pre_pca">pre_pca</code></td>
<td>
<p>logical; Whether the features should be preprocessed using a PCA step</p>
</td></tr>
<tr><td><code id="LearningCurveSSL_+3A_low_level_cores">low_level_cores</code></td>
<td>
<p>integer; Number of cores to use compute repeats of the learning curve</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>classifiers</code> is a named list of classifiers, where each classifier should be a function that accepts 4 arguments: a numeric design matrix of the labeled objects, a factor of labels, a numeric design  matrix of unlabeled objects and a factor of labels for the unlabeled objects.
</p>
<p><code>measures</code> is a named list of performance measures. These are functions that accept seven arguments: a trained classifier, a numeric design matrix of the labeled objects, a factor of labels, a numeric design  matrix of unlabeled objects and a factor of labels for the unlabeled objects, a numeric design matrix of the test objects and a factor of labels of the test objects. See <code><a href="#topic+measure_accuracy">measure_accuracy</a></code> for an example.
</p>
<p>This function allows for two different types of learning curves to be generated. If <code>type="unlabeled"</code>, the number of labeled objects remains fixed at the value of <code>n_l</code>, where <code>sizes</code> controls the number of unlabeled objects. <code>n_test</code> controls the number of objects used for the test set, while all remaining objects are used if <code>with_replacement=FALSE</code> in which case objects are drawn without replacement from the input dataset. We make sure each class is represented by at least <code>n_min</code> labeled objects of each class. For <code>n_l</code>, additional options include: &quot;enough&quot; which takes the max of the number of features and 20, max(ncol(X)+5,20), &quot;d&quot; which takes the number of features or &quot;2d&quot; which takes 2 times the number of features.
</p>
<p>If <code>type="fraction"</code> the total number of objects remains fixed, while the fraction of labeled objects is changed. <code>frac</code> sets the fractions of labeled objects that should be considered, while <code>test_fraction</code> determines the fraction of the total number of objects left out to serve as the test set.
</p>


<h3>Value</h3>

<p>LearningCurve object
</p>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
df &lt;- generate2ClassGaussian(2000,d=2,var=0.6)

classifiers &lt;- list("LS"=function(X,y,X_u,y_u) {
 LeastSquaresClassifier(X,y,lambda=0)}, 
  "Self"=function(X,y,X_u,y_u) {
    SelfLearning(X,y,X_u,LeastSquaresClassifier)}
)

measures &lt;- list("Accuracy" =  measure_accuracy,
                 "Loss Test" = measure_losstest,
                 "Loss labeled" = measure_losslab,
                 "Loss Lab+Unlab" = measure_losstrain
)

# These take a couple of seconds to run
## Not run: 
# Increase the number of unlabeled objects
lc1 &lt;- LearningCurveSSL(as.matrix(df[,1:2]),df$Class,
                        classifiers=classifiers,
                        measures=measures, n_test=1800,
                        n_l=10,repeats=3)

plot(lc1)

# Increase the fraction of labeled objects, example with 2 datasets
lc2 &lt;- LearningCurveSSL(X=list("Dataset 1"=as.matrix(df[,1:2]),
                               "Dataset 2"=as.matrix(df[,1:2])),
                        y=list("Dataset 1"=df$Class,
                               "Dataset 2"=df$Class),
                        classifiers=classifiers,
                        measures=measures,
                        type = "fraction",repeats=3,
                        test_fraction=0.9)

plot(lc2)

## End(Not run)
</code></pre>

<hr>
<h2 id='LeastSquaresClassifier'>Least Squares Classifier</h2><span id='topic+LeastSquaresClassifier'></span>

<h3>Description</h3>

<p>Classifier that minimizes the quadratic loss or, equivalently, least squares regression applied to a numeric encoding of the class labels as target. Note this method minimizes quadratic loss, not the truncated quadratic loss. Optionally, L2 regularization can be applied by setting the <code>lambda</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LeastSquaresClassifier(X, y, lambda = 0, intercept = TRUE,
  x_center = FALSE, scale = FALSE, method = "inverse", y_scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter of the l2 penalty</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_intercept">intercept</code></td>
<td>
<p>TRUE if an intercept should be added to the model</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>TRUE, whether the dependent variables (features) should be centered</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to the design matrix X before running the regression</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_method">method</code></td>
<td>
<p>Method to use for fitting. One of c(&quot;inverse&quot;,&quot;Normal&quot;,&quot;QR&quot;,&quot;BFGS&quot;)</p>
</td></tr>
<tr><td><code id="LeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>If True scale the target vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class LeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>theta</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>the names of the classes</p>
</td></tr>
<tr><td><code>modelform</code></td>
<td>
<p>formula object of the model used in regression</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>a scaling object containing the parameters of the z-transforms applied to the data</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='line_coefficients'>Loss of a classifier or regression function</h2><span id='topic+line_coefficients'></span><span id='topic+line_coefficients+2CLeastSquaresClassifier-method'></span><span id='topic+line_coefficients+2CNormalBasedClassifier-method'></span><span id='topic+line_coefficients+2CLogisticRegression-method'></span><span id='topic+line_coefficients+2CLinearSVM-method'></span><span id='topic+line_coefficients+2CLogisticLossClassifier-method'></span><span id='topic+line_coefficients+2CQuadraticDiscriminantClassifier-method'></span><span id='topic+line_coefficients+2CSelfLearning-method'></span>

<h3>Description</h3>

<p>Loss of a classifier or regression function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>line_coefficients(object, ...)

## S4 method for signature 'LeastSquaresClassifier'
line_coefficients(object)

## S4 method for signature 'NormalBasedClassifier'
line_coefficients(object)

## S4 method for signature 'LogisticRegression'
line_coefficients(object)

## S4 method for signature 'LinearSVM'
line_coefficients(object)

## S4 method for signature 'LogisticLossClassifier'
line_coefficients(object)

## S4 method for signature 'QuadraticDiscriminantClassifier'
line_coefficients(object)

## S4 method for signature 'SelfLearning'
line_coefficients(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="line_coefficients_+3A_object">object</code></td>
<td>
<p>Classifier; Trained Classifier object</p>
</td></tr>
<tr><td><code id="line_coefficients_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric of the total loss on the test data
</p>

<hr>
<h2 id='LinearDiscriminantClassifier'>Linear Discriminant Classifier</h2><span id='topic+LinearDiscriminantClassifier'></span>

<h3>Description</h3>

<p>Implementation of the linear discriminant classifier. Classes are modeled as Gaussians with different means but equal covariance matrices. The optimal covariance matrix and means for the classes are found using maximum likelihood, which, in this case, has a closed form solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinearDiscriminantClassifier(X, y, method = "closedform", prior = NULL,
  scale = FALSE, x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinearDiscriminantClassifier_+3A_x">X</code></td>
<td>
<p>Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="LinearDiscriminantClassifier_+3A_y">y</code></td>
<td>
<p>Vector or factor with class assignments</p>
</td></tr>
<tr><td><code id="LinearDiscriminantClassifier_+3A_method">method</code></td>
<td>
<p>the method to use. Either &quot;closedform&quot; for the fast closed form solution or &quot;ml&quot; for explicit maximum likelihood maximization</p>
</td></tr>
<tr><td><code id="LinearDiscriminantClassifier_+3A_prior">prior</code></td>
<td>
<p>A matrix with class prior probabilities. If NULL, this will be estimated from the data</p>
</td></tr>
<tr><td><code id="LinearDiscriminantClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; If TRUE, apply a z-transform to the design matrix X before running the regression</p>
</td></tr>
<tr><td><code id="LinearDiscriminantClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical; Whether the feature vectors should be centered</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class LeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>modelform</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>the prior probabilities of the classes</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>the estimates means of the classes</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>The estimated covariance matrix</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>a vector with the classnames for each of the classes</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>scaling object used to transform new observations</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='LinearSVM'>Linear SVM Classifier</h2><span id='topic+LinearSVM'></span>

<h3>Description</h3>

<p>Implementation of the Linear Support Vector Classifier. Can be solved in the Dual formulation, which is equivalent to <code><a href="#topic+SVM">SVM</a></code> or the Primal formulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinearSVM(X, y, C = 1, method = "Dual", scale = TRUE, eps = 1e-09,
  reltol = 1e-13, maxit = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinearSVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_c">C</code></td>
<td>
<p>Cost variable</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_method">method</code></td>
<td>
<p>Estimation procedure c(&quot;Dual&quot;,&quot;Primal&quot;,&quot;BGD&quot;)</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_scale">scale</code></td>
<td>
<p>Whether a z-transform should be applied (default: TRUE)</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_eps">eps</code></td>
<td>
<p>Small value to ensure positive definiteness of the matrix in QP formulation</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance using during BFGS optimization</p>
</td></tr>
<tr><td><code id="LinearSVM_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations for BFGS optimization</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of type LinearSVM
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='LinearSVM-class'>LinearSVM Class</h2><span id='topic+LinearSVM-class'></span>

<h3>Description</h3>

<p>LinearSVM Class
</p>

<hr>
<h2 id='LinearTSVM'>Linear CCCP Transductive SVM classifier</h2><span id='topic+LinearTSVM'></span>

<h3>Description</h3>

<p>Implementation for the Linear TSVM. This method is mostly for debugging purposes and does not allow for the balancing constraint or kernels, like the TSVM function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinearTSVM(X, y, X_u, C, Cstar, s = 0, x_center = FALSE, scale = FALSE,
  eps = 1e-06, verbose = FALSE, init = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinearTSVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_y">y</code></td>
<td>
<p>vector; Vector or factor with class assignments</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix of the unlabeled data, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_c">C</code></td>
<td>
<p>numeric; Cost parameter of the SVM</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_cstar">Cstar</code></td>
<td>
<p>numeric; Cost parameter of the unlabeled objects</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_s">s</code></td>
<td>
<p>numeric; parameter controlling the loss function of the unlabeled objects</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_scale">scale</code></td>
<td>
<p>logical; If TRUE, apply a z-transform to all observations in X and X_u before running the regression</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_eps">eps</code></td>
<td>
<p>numeric; Convergence criterion</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_verbose">verbose</code></td>
<td>
<p>logical; print debugging messages (default: FALSE)</p>
</td></tr>
<tr><td><code id="LinearTSVM_+3A_init">init</code></td>
<td>
<p>numeric; Initial classifier parameters to start the convex concave procedure</p>
</td></tr>
</table>


<h3>References</h3>

<p>Collobert, R. et al., 2006. Large scale transductive SVMs. Journal of Machine Learning Research, 7, pp.1687-1712.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='localDescent'>Local descent</h2><span id='topic+localDescent'></span>

<h3>Description</h3>

<p>Local descent used in S4VM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>localDescent(instance, label, labelNum, unlabelNum, gamma, C, beta, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="localDescent_+3A_instance">instance</code></td>
<td>
<p>Design matrix</p>
</td></tr>
<tr><td><code id="localDescent_+3A_label">label</code></td>
<td>
<p>label vector</p>
</td></tr>
<tr><td><code id="localDescent_+3A_labelnum">labelNum</code></td>
<td>
<p>Number of labeled objects</p>
</td></tr>
<tr><td><code id="localDescent_+3A_unlabelnum">unlabelNum</code></td>
<td>
<p>Number of unlabeled objects</p>
</td></tr>
<tr><td><code id="localDescent_+3A_gamma">gamma</code></td>
<td>
<p>Parameter for RBF kernel</p>
</td></tr>
<tr><td><code id="localDescent_+3A_c">C</code></td>
<td>
<p>cost parameter for SVM</p>
</td></tr>
<tr><td><code id="localDescent_+3A_beta">beta</code></td>
<td>
<p>Controls fraction of objects assigned to positive class</p>
</td></tr>
<tr><td><code id="localDescent_+3A_alpha">alpha</code></td>
<td>
<p>Controls fraction of objects assigned to positive class</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list(predictLabel=predictLabel,acc=acc,values=values,model=model)
</p>

<hr>
<h2 id='LogisticLossClassifier'>Logistic Loss Classifier</h2><span id='topic+LogisticLossClassifier'></span>

<h3>Description</h3>

<p>Find the linear classifier which minimizing the logistic loss on the training set, optionally using L2 regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LogisticLossClassifier(X, y, lambda = 0, intercept = TRUE, scale = FALSE,
  init = NA, x_center = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LogisticLossClassifier_+3A_x">X</code></td>
<td>
<p>Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_y">y</code></td>
<td>
<p>Vector with class assignments</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter used for l2 regularization</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_intercept">intercept</code></td>
<td>
<p>TRUE if an intercept should be added to the model</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to all observations in X and X_u before running the regression</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_init">init</code></td>
<td>
<p>Starting parameter vector for gradient descent</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical; Whether the feature vectors should be centered</p>
</td></tr>
<tr><td><code id="LogisticLossClassifier_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object with the following slots
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>the weight vector of the linear classifier</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>vector with names of the classes</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='LogisticLossClassifier-class'>LogisticLossClassifier</h2><span id='topic+LogisticLossClassifier-class'></span>

<h3>Description</h3>

<p>LogisticLossClassifier
</p>

<hr>
<h2 id='LogisticRegression'>(Regularized) Logistic Regression implementation</h2><span id='topic+LogisticRegression'></span>

<h3>Description</h3>

<p>Implementation of Logistic Regression that is useful for comparisons with semi-supervised logistic regression implementations, such as <code><a href="#topic+EntropyRegularizedLogisticRegression">EntropyRegularizedLogisticRegression</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LogisticRegression(X, y, lambda = 0, intercept = TRUE, scale = FALSE,
  init = NA, x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LogisticRegression_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_init">init</code></td>
<td>
<p>numeric; Initialization of parameters for the optimization</p>
</td></tr>
<tr><td><code id="LogisticRegression_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='LogisticRegressionFast'>Logistic Regression implementation that uses R's glm</h2><span id='topic+LogisticRegressionFast'></span>

<h3>Description</h3>

<p>Logistic Regression implementation that uses R's glm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LogisticRegressionFast(X, y, lambda = 0, intercept = TRUE, scale = FALSE,
  init = NA, x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LogisticRegressionFast_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_lambda">lambda</code></td>
<td>
<p>numeric; not used</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_init">init</code></td>
<td>
<p>numeric; not used</p>
</td></tr>
<tr><td><code id="LogisticRegressionFast_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>

<hr>
<h2 id='logsumexp'>Numerically more stable way to calculate log sum exp</h2><span id='topic+logsumexp'></span>

<h3>Description</h3>

<p>Numerically more stable way to calculate log sum exp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logsumexp(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logsumexp_+3A_m">M</code></td>
<td>
<p>matrix; m by n input matrix, sum with be over the rows</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix; m by 1 matrix
</p>

<hr>
<h2 id='loss'>Loss of a classifier or regression function</h2><span id='topic+loss'></span><span id='topic+loss+2CLeastSquaresClassifier-method'></span><span id='topic+loss+2CNormalBasedClassifier-method'></span><span id='topic+loss+2CLogisticRegression-method'></span><span id='topic+loss+2CKernelLeastSquaresClassifier-method'></span><span id='topic+loss+2CLinearSVM-method'></span><span id='topic+loss+2CLogisticLossClassifier-method'></span><span id='topic+loss+2CMajorityClassClassifier-method'></span><span id='topic+loss+2CSVM-method'></span><span id='topic+loss+2CSelfLearning-method'></span><span id='topic+loss+2CUSMLeastSquaresClassifier-method'></span><span id='topic+loss+2CsvmlinClassifier-method'></span>

<h3>Description</h3>

<p>Hinge loss on new objects of a trained LinearSVM
</p>
<p>Hinge loss on new objects of a trained SVM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loss(object, ...)

## S4 method for signature 'LeastSquaresClassifier'
loss(object, newdata, y = NULL, ...)

## S4 method for signature 'NormalBasedClassifier'
loss(object, newdata, y = NULL)

## S4 method for signature 'LogisticRegression'
loss(object, newdata, y = NULL)

## S4 method for signature 'KernelLeastSquaresClassifier'
loss(object, newdata, y = NULL, ...)

## S4 method for signature 'LinearSVM'
loss(object, newdata, y = NULL)

## S4 method for signature 'LogisticLossClassifier'
loss(object, newdata, y = NULL, ...)

## S4 method for signature 'MajorityClassClassifier'
loss(object, newdata, y = NULL)

## S4 method for signature 'SVM'
loss(object, newdata, y = NULL)

## S4 method for signature 'SelfLearning'
loss(object, newdata, y = NULL, ...)

## S4 method for signature 'USMLeastSquaresClassifier'
loss(object, newdata, y = NULL, ...)

## S4 method for signature 'svmlinClassifier'
loss(object, newdata, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loss_+3A_object">object</code></td>
<td>
<p>Classifier; Trained Classifier</p>
</td></tr>
<tr><td><code id="loss_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
<tr><td><code id="loss_+3A_newdata">newdata</code></td>
<td>
<p>data.frame; object with test data</p>
</td></tr>
<tr><td><code id="loss_+3A_y">y</code></td>
<td>
<p>factor; True classes of the test data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric; the total loss on the test data
</p>

<hr>
<h2 id='losslogsum'>LogsumLoss of a classifier or regression function</h2><span id='topic+losslogsum'></span><span id='topic+losslogsum+2CNormalBasedClassifier-method'></span>

<h3>Description</h3>

<p>LogsumLoss of a classifier or regression function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>losslogsum(object, ...)

## S4 method for signature 'NormalBasedClassifier'
losslogsum(object, newdata, Y, X_u, Y_u)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="losslogsum_+3A_object">object</code></td>
<td>
<p>Classifier or Regression object</p>
</td></tr>
<tr><td><code id="losslogsum_+3A_...">...</code></td>
<td>
<p>Additional parameters</p>
</td></tr>
<tr><td><code id="losslogsum_+3A_newdata">newdata</code></td>
<td>
<p>Design matrix of labeled objects</p>
</td></tr>
<tr><td><code id="losslogsum_+3A_y">Y</code></td>
<td>
<p>label matrix of labeled objects</p>
</td></tr>
<tr><td><code id="losslogsum_+3A_x_u">X_u</code></td>
<td>
<p>Design matrix of unlabeled objects</p>
</td></tr>
<tr><td><code id="losslogsum_+3A_y_u">Y_u</code></td>
<td>
<p>label matrix of unlabeled objects</p>
</td></tr>
</table>

<hr>
<h2 id='losspart'>Loss of a classifier or regression function evaluated on partial labels</h2><span id='topic+losspart'></span><span id='topic+losspart+2CNormalBasedClassifier-method'></span>

<h3>Description</h3>

<p>Loss of a classifier or regression function evaluated on partial labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>losspart(object, ...)

## S4 method for signature 'NormalBasedClassifier'
losspart(object, newdata, Y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="losspart_+3A_object">object</code></td>
<td>
<p>Classifier; Trained Classifier</p>
</td></tr>
<tr><td><code id="losspart_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
<tr><td><code id="losspart_+3A_newdata">newdata</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="losspart_+3A_y">Y</code></td>
<td>
<p>class responsibility matrix</p>
</td></tr>
</table>

<hr>
<h2 id='MajorityClassClassifier'>Majority Class Classifier</h2><span id='topic+MajorityClassClassifier'></span>

<h3>Description</h3>

<p>Classifier that returns the majority class in the training set as the prediction for new objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MajorityClassClassifier(X, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MajorityClassClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="MajorityClassClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="MajorityClassClassifier_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='MCLinearDiscriminantClassifier'>Moment Constrained Semi-supervised Linear Discriminant Analysis.</h2><span id='topic+MCLinearDiscriminantClassifier'></span>

<h3>Description</h3>

<p>A linear discriminant classifier that updates the estimates of the means and covariance matrix based on unlabeled examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCLinearDiscriminantClassifier(X, y, X_u, method = "invariant",
  prior = NULL, x_center = TRUE, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_method">method</code></td>
<td>
<p>character; One of c(&quot;invariant&quot;,&quot;closedform&quot;)</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_prior">prior</code></td>
<td>
<p>Matrix (k by 1); Class prior probabilities. If NULL, estimated from data</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="MCLinearDiscriminantClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method uses the parameter updates of the estimated means and covariance proposed in (Loog 2014). Using the method=&quot;invariant&quot; option, uses the scale invariant parameter update proposed in (Loog 2014), while method=&quot;closedform&quot; using the non-scale invariant version from (Loog 2012).
</p>


<h3>References</h3>

<p>Loog, M., 2012. Semi-supervised linear discriminant analysis using moment constraints. Partially Supervised Learning, LNCS, 7081, pp.32-41.
</p>
<p>Loog, M., 2014. Semi-supervised linear discriminant analysis through moment-constraint parameter estimation. Pattern Recognition Letters, 37, pp.24-31.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='MCNearestMeanClassifier'>Moment Constrained Semi-supervised Nearest Mean Classifier</h2><span id='topic+MCNearestMeanClassifier'></span>

<h3>Description</h3>

<p>Update the means based on the moment constraints as defined in Loog (2010). The means estimated using the labeled data are updated by making sure their weighted mean corresponds to the overall mean on all (labeled and unlabeled) data. Optionally, the estimated variance of the classes can be re-estimated after this update is applied by setting update_sigma to <code>TRUE</code>. To get the true nearest mean classifier, rather than estimate the class priors, set them to equal priors using, for instance <code>prior=matrix(0.5,2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCNearestMeanClassifier(X, y, X_u, update_sigma = FALSE, prior = NULL,
  x_center = FALSE, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCNearestMeanClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_update_sigma">update_sigma</code></td>
<td>
<p>logical; Whether the estimate of the variance should be updated after the means have been updated using the unlabeled data</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_prior">prior</code></td>
<td>
<p>matrix; Class priors for the classes</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Loog, M., 2010. Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier. In Proceedings of the 2010 European Conference on Machine learning and Knowledge Discovery in Databases. pp. 291-304.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='MCPLDA'>Maximum Contrastive Pessimistic Likelihood Estimation for Linear Discriminant Analysis</h2><span id='topic+MCPLDA'></span>

<h3>Description</h3>

<p>Maximum Contrastive Pessimistic Likelihood (MCPL) estimation (Loog 2016) attempts to find a semi-supervised solution that has a higher likelihood compared to the supervised solution on the labeled and unlabeled data even for the worst possible labeling of the data. This is done by attempting to find a saddle point of the maximin problem, where the max is over the parameters of the semi-supervised solution and the min is over the labeling, while the objective is the difference in likelihood between the semi-supervised and the supervised solution measured on the labeled and unlabeled data. The implementation is a translation of the Matlab code of Loog (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCPLDA(X, y, X_u, x_center = FALSE, scale = FALSE, max_iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCPLDA_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="MCPLDA_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="MCPLDA_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="MCPLDA_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="MCPLDA_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="MCPLDA_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Loog, M., 2016. Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3), pp.462-475.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='measure_accuracy'>Performance measures used in classifier evaluation</h2><span id='topic+measure_accuracy'></span><span id='topic+measure_error'></span><span id='topic+measure_losstest'></span><span id='topic+measure_losslab'></span><span id='topic+measure_losstrain'></span>

<h3>Description</h3>

<p>Classification accuracy on test set and other performance measure that can be used in <code><a href="#topic+CrossValidationSSL">CrossValidationSSL</a></code> and <code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measure_accuracy(trained_classifier, X_l = NULL, y_l = NULL, X_u = NULL,
  y_u = NULL, X_test = NULL, y_test = NULL)

measure_error(trained_classifier, X_l = NULL, y_l = NULL, X_u = NULL,
  y_u = NULL, X_test = NULL, y_test = NULL)

measure_losstest(trained_classifier, X_l = NULL, y_l = NULL, X_u = NULL,
  y_u = NULL, X_test = NULL, y_test = NULL)

measure_losslab(trained_classifier, X_l = NULL, y_l = NULL, X_u = NULL,
  y_u = NULL, X_test = NULL, y_test = NULL)

measure_losstrain(trained_classifier, X_l = NULL, y_l = NULL, X_u = NULL,
  y_u = NULL, X_test = NULL, y_test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measure_accuracy_+3A_trained_classifier">trained_classifier</code></td>
<td>
<p>the trained classifier object</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_x_l">X_l</code></td>
<td>
<p>design matrix with labeled object</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_y_l">y_l</code></td>
<td>
<p>labels of labeled objects</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_x_u">X_u</code></td>
<td>
<p>design matrix with unlabeled object</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_y_u">y_u</code></td>
<td>
<p>labels of unlabeled objects</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_x_test">X_test</code></td>
<td>
<p>design matrix with test object</p>
</td></tr>
<tr><td><code id="measure_accuracy_+3A_y_test">y_test</code></td>
<td>
<p>labels of test objects</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>measure_error()</code>: Classification error on test set
</p>
</li>
<li> <p><code>measure_losstest()</code>: Average Loss on test objects
</p>
</li>
<li> <p><code>measure_losslab()</code>: Average loss on labeled objects
</p>
</li>
<li> <p><code>measure_losstrain()</code>: Average loss on labeled and unlabeled objects
</p>
</li></ul>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='minimaxlda'>Implements weighted likelihood estimation for LDA</h2><span id='topic+minimaxlda'></span>

<h3>Description</h3>

<p>Implements weighted likelihood estimation for LDA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minimaxlda(a, w, u, iter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minimaxlda_+3A_a">a</code></td>
<td>
<p>is the data set</p>
</td></tr>
<tr><td><code id="minimaxlda_+3A_w">w</code></td>
<td>
<p>is an indicator matrix for the K classes of a or, potentially, a weight matrix in which the fraction with which a sample belongs to a particular class is indicated</p>
</td></tr>
<tr><td><code id="minimaxlda_+3A_u">u</code></td>
<td>
<p>is a bunch of unlabeled data</p>
</td></tr>
<tr><td><code id="minimaxlda_+3A_iter">iter</code></td>
<td>
<p>decides on the amount of time we spend on minimaxing the stuff</p>
</td></tr>
</table>


<h3>Value</h3>

<p>m contains the means, p contains the class priors, iW contains the INVERTED within covariance matrix, uw returns the weights for the unlabeled data, i returns the number of iterations used
</p>

<hr>
<h2 id='missing_labels'>Access the true labels for the objects with missing labels when they are stored as an attribute in a data frame</h2><span id='topic+missing_labels'></span>

<h3>Description</h3>

<p>Access the true labels for the objects with missing labels when they are stored as an attribute in a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>missing_labels(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="missing_labels_+3A_df">df</code></td>
<td>
<p>data.frame; data.frame with y_true attribute</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='NearestMeanClassifier'>Nearest Mean Classifier</h2><span id='topic+NearestMeanClassifier'></span>

<h3>Description</h3>

<p>Implementation of the nearest mean classifier modeled. Classes are modeled as gaussians with equal, spherical covariance matrices. The optimal covariance matrix and means for the classes are found using maximum likelihood, which, in this case, has a closed form solution. To get true nearest mean classification, set prior as a matrix with equal probability for all classes, i.e. <code>matrix(0.5,2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NearestMeanClassifier(X, y, prior = NULL, x_center = FALSE,
  scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NearestMeanClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="NearestMeanClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="NearestMeanClassifier_+3A_prior">prior</code></td>
<td>
<p>matrix; Class prior probabilities. If NULL, this will be estimated from the data</p>
</td></tr>
<tr><td><code id="NearestMeanClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="NearestMeanClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class LeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>modelform</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>the prior probabilities of the classes</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>the estimates means of the classes</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>The estimated covariance matrix</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>a vector with the classnames for each of the classes</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>scaling object used to transform new observations</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='plot.CrossValidation'>Plot CrossValidation object</h2><span id='topic+plot.CrossValidation'></span>

<h3>Description</h3>

<p>Plot CrossValidation object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CrossValidation'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.CrossValidation_+3A_x">x</code></td>
<td>
<p>CrossValidation object</p>
</td></tr>
<tr><td><code id="plot.CrossValidation_+3A_y">y</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="plot.CrossValidation_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='plot.LearningCurve'>Plot LearningCurve object</h2><span id='topic+plot.LearningCurve'></span>

<h3>Description</h3>

<p>Plot LearningCurve object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LearningCurve'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.LearningCurve_+3A_x">x</code></td>
<td>
<p>LearningCurve object</p>
</td></tr>
<tr><td><code id="plot.LearningCurve_+3A_y">y</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="plot.LearningCurve_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='posterior'>Class Posteriors of a classifier</h2><span id='topic+posterior'></span><span id='topic+posterior+2CNormalBasedClassifier-method'></span><span id='topic+posterior+2CLogisticRegression-method'></span>

<h3>Description</h3>

<p>Class Posteriors of a classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior(object, ...)

## S4 method for signature 'NormalBasedClassifier'
posterior(object, newdata)

## S4 method for signature 'LogisticRegression'
posterior(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_+3A_object">object</code></td>
<td>
<p>Classifier or Regression object</p>
</td></tr>
<tr><td><code id="posterior_+3A_...">...</code></td>
<td>
<p>Additional parameters</p>
</td></tr>
<tr><td><code id="posterior_+3A_newdata">newdata</code></td>
<td>
<p>matrix of dataframe of objects to be classified</p>
</td></tr>
</table>

<hr>
<h2 id='predict+2CscaleMatrix-method'>Predict for matrix scaling inspired by stdize from the PLS package</h2><span id='topic+predict+2CscaleMatrix-method'></span>

<h3>Description</h3>

<p>Predict for matrix scaling inspired by stdize from the PLS package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'scaleMatrix'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict+2B2CscaleMatrix-method_+3A_object">object</code></td>
<td>
<p>scaleMatrix object</p>
</td></tr>
<tr><td><code id="predict+2B2CscaleMatrix-method_+3A_newdata">newdata</code></td>
<td>
<p>data to be scaled</p>
</td></tr>
<tr><td><code id="predict+2B2CscaleMatrix-method_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='PreProcessing'>Preprocess the input to a classification function</h2><span id='topic+PreProcessing'></span>

<h3>Description</h3>

<p>The following actions are carried out: 1. data.frames are converted to matrix form and labels converted to an indicator matrix 2. An intercept column is added if requested 3. centering and scaling is applied if requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PreProcessing(X, y, X_u = NULL, scale = FALSE, intercept = FALSE,
  x_center = FALSE, use_Xu_for_scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PreProcessing_+3A_x">X</code></td>
<td>
<p>Design matrix, intercept term is added within the function</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_y">y</code></td>
<td>
<p>Vector or factor with class assignments</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_x_u">X_u</code></td>
<td>
<p>Design matrix of the unlabeled observations</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to the design matrix X</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_intercept">intercept</code></td>
<td>
<p>Whether to include an intercept in the design matrices</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_x_center">x_center</code></td>
<td>
<p>logical (default: TRUE); Whether the feature vectors should be centered</p>
</td></tr>
<tr><td><code id="PreProcessing_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical (default: TRUE); Should the unlabeled data be used to determine scaling?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object with the following objects:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>design matrix of the labeled data</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>integer vector indicating the labels of the labeled data</p>
</td></tr>
<tr><td><code>X_u</code></td>
<td>
<p>design matrix of the unlabeled data</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>names of the classes corresponding to the integers in y</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>a scaling object used to scale the test observations in the same way as the training set</p>
</td></tr>
<tr><td><code>modelform</code></td>
<td>
<p>a formula object containing the used model</p>
</td></tr>
</table>

<hr>
<h2 id='PreProcessingPredict'>Preprocess the input for a new set of test objects for classifier</h2><span id='topic+PreProcessingPredict'></span>

<h3>Description</h3>

<p>The following actions are carried out: 1. data.frames are converted to matrix form and labels converted to integers 2. An intercept column is added if requested 3. centering and scaling is applied if requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PreProcessingPredict(modelform, newdata, y = NULL, classnames = NULL,
  scaling = NULL, intercept = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PreProcessingPredict_+3A_modelform">modelform</code></td>
<td>
<p>Formula object with model</p>
</td></tr>
<tr><td><code id="PreProcessingPredict_+3A_newdata">newdata</code></td>
<td>
<p>data.frame object with objects</p>
</td></tr>
<tr><td><code id="PreProcessingPredict_+3A_y">y</code></td>
<td>
<p>Vector or factor with class assignments (default: NULL)</p>
</td></tr>
<tr><td><code id="PreProcessingPredict_+3A_classnames">classnames</code></td>
<td>
<p>Vector with class names</p>
</td></tr>
<tr><td><code id="PreProcessingPredict_+3A_scaling">scaling</code></td>
<td>
<p>Apply a given z-transform to the design matrix X (default: NULL)</p>
</td></tr>
<tr><td><code id="PreProcessingPredict_+3A_intercept">intercept</code></td>
<td>
<p>Whether to include an intercept in the design matrices</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object with the following objects:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>design matrix of the labeled data</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>integer vector indicating the labels of the labeled data</p>
</td></tr>
</table>

<hr>
<h2 id='print.CrossValidation'>Print CrossValidation object</h2><span id='topic+print.CrossValidation'></span>

<h3>Description</h3>

<p>Print CrossValidation object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CrossValidation'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.CrossValidation_+3A_x">x</code></td>
<td>
<p>CrossValidation object</p>
</td></tr>
<tr><td><code id="print.CrossValidation_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='print.LearningCurve'>Print LearningCurve object</h2><span id='topic+print.LearningCurve'></span>

<h3>Description</h3>

<p>Print LearningCurve object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LearningCurve'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.LearningCurve_+3A_x">x</code></td>
<td>
<p>LearningCurve object</p>
</td></tr>
<tr><td><code id="print.LearningCurve_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='projection_simplex'>Project an n-dim vector y to the simplex Dn</h2><span id='topic+projection_simplex'></span>

<h3>Description</h3>

<p>Where <code class="reqn">Dn = \{ 0 &lt;= x &lt;= 1, sum(x) = 1\}</code>. 
R translation of Loog's version of Xiaojing Ye's initial implementation.
The algorithm works row-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projection_simplex(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projection_simplex_+3A_y">y</code></td>
<td>
<p>matrix with vectors to be projected onto the simplex</p>
</td></tr>
</table>


<h3>Value</h3>

<p>projection of y onto the simplex
</p>


<h3>References</h3>

<p>Algorithm is explained in http://arxiv.org/abs/1101.6081
</p>

<hr>
<h2 id='QuadraticDiscriminantClassifier'>Quadratic Discriminant Classifier</h2><span id='topic+QuadraticDiscriminantClassifier'></span>

<h3>Description</h3>

<p>Implementation of the quadratic discriminant classifier. Classes are modeled as Gaussians with different covariance matrices. The optimal covariance matrix and means for the classes are found using maximum likelihood, which, in this case, has a closed form solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QuadraticDiscriminantClassifier(X, y, prior = NULL, scale = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QuadraticDiscriminantClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="QuadraticDiscriminantClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="QuadraticDiscriminantClassifier_+3A_prior">prior</code></td>
<td>
<p>A matrix with class prior probabilities. If NULL, this will be estimated from the data</p>
</td></tr>
<tr><td><code id="QuadraticDiscriminantClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="QuadraticDiscriminantClassifier_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S4 object of class LeastSquaresClassifier with the following slots:
</p>
<table>
<tr><td><code>modelform</code></td>
<td>
<p>weight vector</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>the prior probabilities of the classes</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>the estimates means of the classes</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>The estimated covariance matrix</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>a vector with the classnames for each of the classes</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>scaling object used to transform new observations</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='responsibilities'>Responsibilities assigned to the unlabeled objects</h2><span id='topic+responsibilities'></span>

<h3>Description</h3>

<p>Responsibilities assigned to the unlabeled objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>responsibilities(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="responsibilities_+3A_object">object</code></td>
<td>
<p>Classifier; Trained Classifier</p>
</td></tr>
<tr><td><code id="responsibilities_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric; responsibilities on the unlabeled objects
</p>

<hr>
<h2 id='rssl-formatting'>Show RSSL classifier</h2><span id='topic+rssl-formatting'></span><span id='topic+show+2CClassifier-method'></span><span id='topic+show+2CNormalBasedClassifier-method'></span><span id='topic+show+2CscaleMatrix-method'></span>

<h3>Description</h3>

<p>Show RSSL classifier
</p>
<p>Show the contents of a classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Classifier'
show(object)

## S4 method for signature 'NormalBasedClassifier'
show(object)

## S4 method for signature 'scaleMatrix'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rssl-formatting_+3A_object">object</code></td>
<td>
<p>classifier</p>
</td></tr>
</table>

<hr>
<h2 id='rssl-predict'>Predict using RSSL classifier</h2><span id='topic+rssl-predict'></span><span id='topic+predict+2CLeastSquaresClassifier-method'></span><span id='topic+predict+2CNormalBasedClassifier-method'></span><span id='topic+predict+2CLogisticRegression-method'></span><span id='topic+responsibilities+2CGRFClassifier-method'></span><span id='topic+predict+2CGRFClassifier-method'></span><span id='topic+predict+2CKernelLeastSquaresClassifier-method'></span><span id='topic+predict+2CLinearSVM-method'></span><span id='topic+predict+2CLogisticLossClassifier-method'></span><span id='topic+predict+2CMajorityClassClassifier-method'></span><span id='topic+predict+2CSVM-method'></span><span id='topic+predict+2CSelfLearning-method'></span><span id='topic+predict+2CUSMLeastSquaresClassifier-method'></span><span id='topic+predict+2CWellSVM-method'></span><span id='topic+decisionvalues+2CWellSVM-method'></span><span id='topic+predict+2CsvmlinClassifier-method'></span>

<h3>Description</h3>

<p>Predict using RSSL classifier
</p>
<p>For the SelfLearning Classifier the Predict Method delegates prediction to the specific model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'LeastSquaresClassifier'
predict(object, newdata, ...)

## S4 method for signature 'NormalBasedClassifier'
predict(object, newdata)

## S4 method for signature 'LogisticRegression'
predict(object, newdata)

## S4 method for signature 'GRFClassifier'
responsibilities(object, newdata, ...)

## S4 method for signature 'GRFClassifier'
predict(object, newdata = NULL, ...)

## S4 method for signature 'KernelLeastSquaresClassifier'
predict(object, newdata, ...)

## S4 method for signature 'LinearSVM'
predict(object, newdata)

## S4 method for signature 'LogisticLossClassifier'
predict(object, newdata)

## S4 method for signature 'MajorityClassClassifier'
predict(object, newdata)

## S4 method for signature 'SVM'
predict(object, newdata)

## S4 method for signature 'SelfLearning'
predict(object, newdata, ...)

## S4 method for signature 'USMLeastSquaresClassifier'
predict(object, newdata, ...)

## S4 method for signature 'WellSVM'
predict(object, newdata, ...)

## S4 method for signature 'WellSVM'
decisionvalues(object, newdata)

## S4 method for signature 'svmlinClassifier'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rssl-predict_+3A_object">object</code></td>
<td>
<p>classifier</p>
</td></tr>
<tr><td><code id="rssl-predict_+3A_newdata">newdata</code></td>
<td>
<p>objects to generate predictions for</p>
</td></tr>
<tr><td><code id="rssl-predict_+3A_...">...</code></td>
<td>
<p>Other arguments</p>
</td></tr>
</table>

<hr>
<h2 id='S4VM'>Safe Semi-supervised Support Vector Machine (S4VM)</h2><span id='topic+S4VM'></span>

<h3>Description</h3>

<p>R port of the MATLAB implementation of Li &amp; Zhou (2011) of the Safe Semi-supervised Support Vector Machine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S4VM(X, y, X_u = NULL, C1 = 100, C2 = 0.1, sample_time = 100,
  gamma = 0, x_center = FALSE, scale = FALSE, lambda_tradeoff = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S4VM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="S4VM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="S4VM_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="S4VM_+3A_c1">C1</code></td>
<td>
<p>double; Regularization parameter for labeled data</p>
</td></tr>
<tr><td><code id="S4VM_+3A_c2">C2</code></td>
<td>
<p>double; Regularization parameter for unlabeled data</p>
</td></tr>
<tr><td><code id="S4VM_+3A_sample_time">sample_time</code></td>
<td>
<p>integer; Number of low-density separators that are generated</p>
</td></tr>
<tr><td><code id="S4VM_+3A_gamma">gamma</code></td>
<td>
<p>double; Width of RBF kernel</p>
</td></tr>
<tr><td><code id="S4VM_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="S4VM_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="S4VM_+3A_lambda_tradeoff">lambda_tradeoff</code></td>
<td>
<p>numeric; Parameter that determines the amount of &quot;risk&quot; in obtaining a worse solution than the supervised solution, see Li &amp; Zhou (2011)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method randomly generates multiple low-density separators (controlled by the sample_time parameter) and merges their predictions by solving a linear programming problem meant to penalize the cost of decreasing the performance of the classifier, compared to the supervised SVM. S4VM is a bit of a misnomer, since it is a transductive method that only returns predicted labels for the unlabeled objects. The main difference in this implementation compared to the original implementation is the clustering of the low-density separators: in our implementation empty clusters are not dropped during the k-means procedure. In the paper by Li (2011) the features are first normalized to [0,1], which is not automatically done by this function. Note that the solution may not correspond to a linear classifier even if the linear kernel is used.
</p>


<h3>Value</h3>

<p>S4VM object with slots:
</p>
<table>
<tr><td><code>predictions</code></td>
<td>
<p>Predictions on the unlabeled objects</p>
</td></tr>
<tr><td><code>labelings</code></td>
<td>
<p>Labelings for the different clusters</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yu-Feng Li and Zhi-Hua Zhou. Towards Making Unlabeled Data Never Hurt. In: Proceedings of the 28th International Conference on Machine Learning (ICML'11), Bellevue, Washington, 2011.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(dplyr)
library(ggplot2)
library(tidyr)

set.seed(1)
df_orig &lt;- generateSlicedCookie(100,expected=TRUE)
df &lt;- df_orig %&gt;% add_missinglabels_mar(Class~.,0.95)
g_s &lt;- SVM(Class~.,df,C=1,scale=TRUE,x_center=TRUE)
g_s4 &lt;- S4VM(Class~.,df,C1=1,C2=0.1,lambda_tradeoff = 3,scale=TRUE,x_center=TRUE)

labs &lt;- g_s4@labelings[-c(1:5),]
colnames(labs) &lt;- paste("Class",seq_len(ncol(g_s4@labelings)),sep="-")

# Show the labelings that the algorithm is considering
df %&gt;%
  filter(is.na(Class)) %&gt;% 
  bind_cols(data.frame(labs,check.names = FALSE)) %&gt;% 
  select(-Class) %&gt;% 
  gather(Classifier,Label,-X1,-X2) %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Label)) +
  geom_point() +
  facet_wrap(~Classifier,ncol=5)

# Plot the final labeling that was selected
# Note that this may not correspond to a linear classifier
# even if the linear kernel is used.
# The solution does not seem to make a lot of sense,
# but this is what the current implementation returns
df %&gt;% 
  filter(is.na(Class)) %&gt;% 
  mutate(prediction=g_s4@predictions) %&gt;% 
  ggplot(aes(x=X1,y=X2,color=prediction)) +
  geom_point() +
  stat_classifier(color="black", classifiers=list(g_s))
</code></pre>

<hr>
<h2 id='S4VM-class'>LinearSVM Class</h2><span id='topic+S4VM-class'></span>

<h3>Description</h3>

<p>LinearSVM Class
</p>

<hr>
<h2 id='sample_k_per_level'>Sample k indices per levels from a factor</h2><span id='topic+sample_k_per_level'></span>

<h3>Description</h3>

<p>Sample k indices per levels from a factor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_k_per_level(y, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_k_per_level_+3A_y">y</code></td>
<td>
<p>factor; factor with levels</p>
</td></tr>
<tr><td><code id="sample_k_per_level_+3A_k">k</code></td>
<td>
<p>integer; number of indices to sample per level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with indices for sample
</p>

<hr>
<h2 id='scaleMatrix'>Matrix centering and scaling</h2><span id='topic+scaleMatrix'></span>

<h3>Description</h3>

<p>This function returns an object with a predict method to center and scale new data. Inspired by stdize from the PLS package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaleMatrix(x, center = TRUE, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scaleMatrix_+3A_x">x</code></td>
<td>
<p>matrix to be standardized</p>
</td></tr>
<tr><td><code id="scaleMatrix_+3A_center">center</code></td>
<td>
<p>TRUE if x should be centered</p>
</td></tr>
<tr><td><code id="scaleMatrix_+3A_scale">scale</code></td>
<td>
<p>logical; TRUE of x should be scaled by the standard deviation</p>
</td></tr>
</table>

<hr>
<h2 id='SelfLearning'>Self-Learning approach to Semi-supervised Learning</h2><span id='topic+SelfLearning'></span>

<h3>Description</h3>

<p>Use self-learning (also known as Yarowsky's algorithm or pseudo-labeling) to turn any supervised classifier into a semi-supervised method by iteratively labeling the unlabeled objects and adding these predictions to the set of labeled objects until the classifier converges.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SelfLearning(X, y, X_u = NULL, method, prob = FALSE, cautious = FALSE,
  max_iter = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SelfLearning_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_method">method</code></td>
<td>
<p>Supervised classifier to use. Any function that accepts as its first argument a design matrix X and as its second argument a vector of labels y and that has a predict method.</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_prob">prob</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_cautious">cautious</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
<tr><td><code id="SelfLearning_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to method</p>
</td></tr>
</table>


<h3>References</h3>

<p>McLachlan, G.J., 1975. Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis. Journal of the American Statistical Association, 70(350), pp.365-369.
</p>
<p>Yarowsky, D., 1995. Unsupervised word sense disambiguation rivaling supervised methods. Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pp.189-196.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(testdata)
t_self &lt;- SelfLearning(testdata$X,testdata$y,testdata$X_u,method=NearestMeanClassifier)
t_sup &lt;- NearestMeanClassifier(testdata$X,testdata$y)
# Classification Error
1-mean(predict(t_self, testdata$X_test)==testdata$y_test) 
1-mean(predict(t_sup, testdata$X_test)==testdata$y_test) 
loss(t_self, testdata$X_test, testdata$y_test)
</code></pre>

<hr>
<h2 id='solve_svm'>SVM solve.QP implementation</h2><span id='topic+solve_svm'></span>

<h3>Description</h3>

<p>SVM solve.QP implementation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_svm(K, y, C = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_svm_+3A_k">K</code></td>
<td>
<p>Kernel matrix</p>
</td></tr>
<tr><td><code id="solve_svm_+3A_y">y</code></td>
<td>
<p>Output vector</p>
</td></tr>
<tr><td><code id="solve_svm_+3A_c">C</code></td>
<td>
<p>Cost parameter</p>
</td></tr>
</table>

<hr>
<h2 id='split_dataset_ssl'>Create Train, Test and Unlabeled Set</h2><span id='topic+split_dataset_ssl'></span>

<h3>Description</h3>

<p>Create Train, Test and Unlabeled Set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_dataset_ssl(X, y, frac_train = 0.8, frac_ssl = 0.8)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_dataset_ssl_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix</p>
</td></tr>
<tr><td><code id="split_dataset_ssl_+3A_y">y</code></td>
<td>
<p>factor; Label vector</p>
</td></tr>
<tr><td><code id="split_dataset_ssl_+3A_frac_train">frac_train</code></td>
<td>
<p>numeric; Fraction of all objects to be used as training objects</p>
</td></tr>
<tr><td><code id="split_dataset_ssl_+3A_frac_ssl">frac_ssl</code></td>
<td>
<p>numeric; Fraction of training objects to used as unlabeled objects</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='split_random'>Randomly split dataset in multiple parts</h2><span id='topic+split_random'></span>

<h3>Description</h3>

<p>The data.frame should start with a vector containing labels, or formula should be defined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_random(df, formula = NULL, splits = c(0.5, 0.5), min_class = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_random_+3A_df">df</code></td>
<td>
<p>data.frame; Data frame of interest</p>
</td></tr>
<tr><td><code id="split_random_+3A_formula">formula</code></td>
<td>
<p>formula; Formula to indicate the outputs</p>
</td></tr>
<tr><td><code id="split_random_+3A_splits">splits</code></td>
<td>
<p>numeric; Probability of of assigning to each part, automatically normalized, should be &gt;1</p>
</td></tr>
<tr><td><code id="split_random_+3A_min_class">min_class</code></td>
<td>
<p>integer; minimum number of objects per class in each part</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames
</p>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

df &lt;- generate2ClassGaussian(200,d=2)
dfs &lt;- df %&gt;% split_random(Class~.,split=c(0.5,0.3,0.2),min_class=1) 
names(dfs) &lt;- c("Train","Validation","Test")
lapply(dfs,summary)

</code></pre>

<hr>
<h2 id='SSLDataFrameToMatrices'>Convert data.frame to matrices for semi-supervised learners</h2><span id='topic+SSLDataFrameToMatrices'></span>

<h3>Description</h3>

<p>Given a formula object and a data.frame, extract the design matrix X for the labeled observations, X_u for the unlabeled observations and y for the labels of the labeled observations. Note: always removes the intercept
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSLDataFrameToMatrices(model, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSLDataFrameToMatrices_+3A_model">model</code></td>
<td>
<p>Formula object with model</p>
</td></tr>
<tr><td><code id="SSLDataFrameToMatrices_+3A_d">D</code></td>
<td>
<p>data.frame object with objects</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object with the following objects:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>design matrix of the labeled data</p>
</td></tr>
<tr><td><code>X_u</code></td>
<td>
<p>design matrix of the unlabeled data</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>integer vector indicating the labels of the labeled data</p>
</td></tr>
<tr><td><code>classnames</code></td>
<td>
<p>names of the classes corresponding to the integers in y</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>,
<code><a href="#topic+true_labels">true_labels</a>()</code>
</p>

<hr>
<h2 id='stat_classifier'>Plot RSSL classifier boundaries</h2><span id='topic+stat_classifier'></span>

<h3>Description</h3>

<p>Plot RSSL classifier boundaries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat_classifier(mapping = NULL, data = NULL, show.legend = NA,
  inherit.aes = TRUE, breaks = 0, precision = 50, brute_force = FALSE,
  classifiers = classifiers, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_classifier_+3A_mapping">mapping</code></td>
<td>
<p>aes; aesthetic mapping</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_data">data</code></td>
<td>
<p>data.frame; data to be displayed</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_show.legend">show.legend</code></td>
<td>
<p>logical; Whether this layer should be included in the legend</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>logical; If FALSE, overrides the default aesthetics</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_breaks">breaks</code></td>
<td>
<p>double; decision value for which to plot the boundary</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_precision">precision</code></td>
<td>
<p>integer; grid size to sketch classification boundary</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_brute_force">brute_force</code></td>
<td>
<p>logical; If TRUE, uses numerical estimation even for linear classifiers</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_classifiers">classifiers</code></td>
<td>
<p>List of Classifier objects to plot</p>
</td></tr>
<tr><td><code id="stat_classifier_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to geom</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

df &lt;- generateCrescentMoon(200)

# This takes a couple of seconds to run
## Not run: 
g_svm &lt;- SVM(Class~.,df,kernel = kernlab::rbfdot(sigma = 1))
g_ls &lt;- LeastSquaresClassifier(Class~.,df)
g_nm &lt;- NearestMeanClassifier(Class~.,df)


df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class,shape=Class)) +
  geom_point(size=3) +
  coord_equal() +
  scale_x_continuous(limits=c(-20,20), expand=c(0,0)) +
  scale_y_continuous(limits=c(-20,20), expand=c(0,0)) +
  stat_classifier(aes(linetype=..classifier..),
                  color="black", precision=50,
                  classifiers=list("SVM"=g_svm,"NM"=g_nm,"LS"=g_ls)
  )

## End(Not run)   
</code></pre>

<hr>
<h2 id='stderror'>Calculate the standard error of the mean from a vector of numbers</h2><span id='topic+stderror'></span>

<h3>Description</h3>

<p>Calculate the standard error of the mean from a vector of numbers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stderror(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stderror_+3A_x">x</code></td>
<td>
<p>numeric; vector for which to calculate standard error</p>
</td></tr>
</table>

<hr>
<h2 id='summary.CrossValidation'>Summary of Crossvalidation results</h2><span id='topic+summary.CrossValidation'></span>

<h3>Description</h3>

<p>Summary of Crossvalidation results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CrossValidation'
summary(object, measure = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.CrossValidation_+3A_object">object</code></td>
<td>
<p>CrossValidation object</p>
</td></tr>
<tr><td><code id="summary.CrossValidation_+3A_measure">measure</code></td>
<td>
<p>Measure of interest</p>
</td></tr>
<tr><td><code id="summary.CrossValidation_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='svdinv'>Inverse of a matrix using the singular value decomposition</h2><span id='topic+svdinv'></span>

<h3>Description</h3>

<p>Inverse of a matrix using the singular value decomposition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svdinv(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svdinv_+3A_x">X</code></td>
<td>
<p>matrix; square input matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Y matrix; inverse of the input matrix
</p>

<hr>
<h2 id='svdinvsqrtm'>Taking the inverse of the square root of the matrix using the singular value decomposition</h2><span id='topic+svdinvsqrtm'></span>

<h3>Description</h3>

<p>Taking the inverse of the square root of the matrix using the singular value decomposition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svdinvsqrtm(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svdinvsqrtm_+3A_x">X</code></td>
<td>
<p>matrix; square input matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Y matrix; inverse of the square root of the input matrix
</p>

<hr>
<h2 id='svdsqrtm'>Taking the square root of a matrix using the singular value decomposition</h2><span id='topic+svdsqrtm'></span>

<h3>Description</h3>

<p>Taking the square root of a matrix using the singular value decomposition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svdsqrtm(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svdsqrtm_+3A_x">X</code></td>
<td>
<p>matrix; square input matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Y matrix; square root of the input matrix
</p>

<hr>
<h2 id='SVM'>SVM Classifier</h2><span id='topic+SVM'></span>

<h3>Description</h3>

<p>Support Vector Machine implementation using the <code>quadprog</code> solver.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SVM(X, y, C = 1, kernel = NULL, scale = TRUE, intercept = FALSE,
  x_center = TRUE, eps = 1e-09)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="SVM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="SVM_+3A_c">C</code></td>
<td>
<p>numeric; Cost variable</p>
</td></tr>
<tr><td><code id="SVM_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="SVM_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="SVM_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="SVM_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="SVM_+3A_eps">eps</code></td>
<td>
<p>numeric; Small value to ensure positive definiteness of the matrix in the QP formulation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation will typically be slower and use more memory than the svmlib implementation in the e1071 package. It is, however, useful for comparisons with the <code><a href="#topic+TSVM">TSVM</a></code> implementation.
</p>


<h3>Value</h3>

<p>S4 object of type SVM
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='svmlin'>svmlin implementation by Sindhwani &amp; Keerthi (2006)</h2><span id='topic+svmlin'></span>

<h3>Description</h3>

<p>R interface to the svmlin code by Vikas Sindhwani and S. Sathiya Keerthi for fast linear transductive SVMs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svmlin(X, y, X_u = NULL, algorithm = 1, lambda = 1, lambda_u = 1,
  max_switch = 10000, pos_frac = 0.5, Cp = 1, Cn = 1,
  verbose = FALSE, intercept = TRUE, scale = FALSE, x_center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svmlin_+3A_x">X</code></td>
<td>
<p>Matrix or sparseMatrix containing the labeled feature vectors, without intercept</p>
</td></tr>
<tr><td><code id="svmlin_+3A_y">y</code></td>
<td>
<p>factor containing class assignments</p>
</td></tr>
<tr><td><code id="svmlin_+3A_x_u">X_u</code></td>
<td>
<p>Matrix or sparseMatrix containing the unlabeled feature vectors, without intercept</p>
</td></tr>
<tr><td><code id="svmlin_+3A_algorithm">algorithm</code></td>
<td>
<p>integer; Algorithm choice, see details (default:1)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_lambda">lambda</code></td>
<td>
<p>double; Regularization parameter lambda (default 1)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_lambda_u">lambda_u</code></td>
<td>
<p>double; Regularization parameter lambda_u (default 1)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_max_switch">max_switch</code></td>
<td>
<p>integer; Maximum number of switches in TSVM (default 10000)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_pos_frac">pos_frac</code></td>
<td>
<p>double; Positive class fraction of unlabeled data  (default 0.5)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_cp">Cp</code></td>
<td>
<p>double; Relative cost for positive examples (only available with algorithm 1)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_cn">Cn</code></td>
<td>
<p>double; Relative cost for positive examples (only available with algorithm 1)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="svmlin_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="svmlin_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="svmlin_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The codes to select the algorithm are the following: 0. Regularized Least Squares Classification 1. SVM (L2-SVM-MFN) 2. Multi-switch Transductive SVM (using L2-SVM-MFN) 3. Deterministic Annealing Semi-supervised SVM (using L2-SVM-MFN).
</p>


<h3>References</h3>

<p>Vikas Sindhwani and S. Sathiya Keerthi. Large Scale Semi-supervised Linear SVMs. Proceedings of ACM SIGIR, 2006
@references V. Sindhwani and S. Sathiya Keerthi. Newton Methods for Fast Solution of Semi-supervised Linear SVMs. Book Chapter in Large Scale Kernel Machines, MIT Press, 2006
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(svmlin_example)
t_svmlin_1 &lt;- svmlin(svmlin_example$X_train[1:50,],
                 svmlin_example$y_train,X_u=NULL, lambda = 0.001)
t_svmlin_2 &lt;- svmlin(svmlin_example$X_train[1:50,],
                       svmlin_example$y_train,
                       X_u=svmlin_example$X_train[-c(1:50),], 
                       lambda = 10,lambda_u=100,algorithm = 2)
                       
# Calculate Accuracy
mean(predict(t_svmlin_1,svmlin_example$X_test)==svmlin_example$y_test)
mean(predict(t_svmlin_2,svmlin_example$X_test)==svmlin_example$y_test)

data(testdata)

g_svm &lt;- SVM(testdata$X,testdata$y)
g_sup &lt;- svmlin(testdata$X,testdata$y,testdata$X_u,algorithm = 3)
g_semi &lt;- svmlin(testdata$X,testdata$y,testdata$X_u,algorithm = 2)

mean(predict(g_svm,testdata$X_test)==testdata$y_test)
mean(predict(g_sup,testdata$X_test)==testdata$y_test)
mean(predict(g_semi,testdata$X_test)==testdata$y_test)
</code></pre>

<hr>
<h2 id='svmlin_example'>Test data from the svmlin implementation</h2><span id='topic+svmlin_example'></span>

<h3>Description</h3>

<p>Useful for testing the svmlin interface and to serve as an example
</p>

<hr>
<h2 id='svmproblem'>Train SVM</h2><span id='topic+svmproblem'></span>

<h3>Description</h3>

<p>Train SVM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svmproblem(K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svmproblem_+3A_k">K</code></td>
<td>
<p>kernel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>alpha, b, obj
</p>

<hr>
<h2 id='testdata'>Example semi-supervised problem</h2><span id='topic+testdata'></span>

<h3>Description</h3>

<p>A list containing a sample from the <code>GenerateSlicedCookie</code> dataset for unit testing and examples.
</p>

<hr>
<h2 id='threshold'>Refine the prediction to satisfy the balance constraint</h2><span id='topic+threshold'></span>

<h3>Description</h3>

<p>Refine the prediction to satisfy the balance constraint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>threshold(y1, options)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="threshold_+3A_y1">y1</code></td>
<td>
<p>predictions</p>
</td></tr>
<tr><td><code id="threshold_+3A_options">options</code></td>
<td>
<p>options passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>y2
</p>

<hr>
<h2 id='true_labels'>Access the true labels when they are stored as an attribute in a data frame</h2><span id='topic+true_labels'></span>

<h3>Description</h3>

<p>Access the true labels when they are stored as an attribute in a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>true_labels(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="true_labels_+3A_df">df</code></td>
<td>
<p>data.frame; data.frame with y_true attribute</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other RSSL utilities: 
<code><a href="#topic+LearningCurveSSL">LearningCurveSSL</a>()</code>,
<code><a href="#topic+SSLDataFrameToMatrices">SSLDataFrameToMatrices</a>()</code>,
<code><a href="#topic+add_missinglabels_mar">add_missinglabels_mar</a>()</code>,
<code><a href="#topic+df_to_matrices">df_to_matrices</a>()</code>,
<code><a href="#topic+measure_accuracy">measure_accuracy</a>()</code>,
<code><a href="#topic+missing_labels">missing_labels</a>()</code>,
<code><a href="#topic+split_dataset_ssl">split_dataset_ssl</a>()</code>,
<code><a href="#topic+split_random">split_random</a>()</code>
</p>

<hr>
<h2 id='TSVM'>Transductive SVM classifier using the convex concave procedure</h2><span id='topic+TSVM'></span>

<h3>Description</h3>

<p>Transductive SVM using the CCCP algorithm as proposed by Collobert et al. (2006) implemented in R using the quadprog package. The implementation does not handle large datasets very well, but can be useful for smaller datasets and visualization purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TSVM(X, y, X_u, C, Cstar, kernel = kernlab::vanilladot(),
  balancing_constraint = TRUE, s = 0, x_center = TRUE, scale = FALSE,
  eps = 1e-09, max_iter = 20, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TSVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="TSVM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="TSVM_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="TSVM_+3A_c">C</code></td>
<td>
<p>numeric; Cost parameter of the SVM</p>
</td></tr>
<tr><td><code id="TSVM_+3A_cstar">Cstar</code></td>
<td>
<p>numeric; Cost parameter of the unlabeled objects</p>
</td></tr>
<tr><td><code id="TSVM_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="TSVM_+3A_balancing_constraint">balancing_constraint</code></td>
<td>
<p>logical; Whether a balancing constraint should be enforced that causes the fraction of objects assigned to each label in the unlabeled data to be similar to the label fraction in the labeled data.</p>
</td></tr>
<tr><td><code id="TSVM_+3A_s">s</code></td>
<td>
<p>numeric; parameter controlling the loss function of the unlabeled objects (generally values between -1 and 0)</p>
</td></tr>
<tr><td><code id="TSVM_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="TSVM_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to all observations in X and X_u before running the regression</p>
</td></tr>
<tr><td><code id="TSVM_+3A_eps">eps</code></td>
<td>
<p>numeric; Stopping criterion for the maximinimization</p>
</td></tr>
<tr><td><code id="TSVM_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
<tr><td><code id="TSVM_+3A_verbose">verbose</code></td>
<td>
<p>logical; print debugging messages, only works for vanilladot() kernel (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>C is the cost associated with labeled objects, while Cstar is the cost for the unlabeled objects. s control the loss function used for the unlabeled objects: it controls the size of the plateau for the symmetric ramp loss function. The balancing constraint makes sure the label assignments of the unlabeled objects are similar to the prior on the classes that was observed on the labeled data.
</p>


<h3>References</h3>

<p>Collobert, R. et al., 2006. Large scale transductive SVMs. Journal of Machine Learning Research, 7, pp.1687-1712.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)

# Simple example with a few objects
X &lt;- matrix(c(0,0.001,1,-1),nrow=2)
X_u &lt;- matrix(c(-1,-1,-1,0,0,0,-0.4,-0.5,-0.6,1.2,1.3,1.25),ncol=2)
y &lt;- factor(c(-1,1))

g_sup &lt;- SVM(X,y,scale=FALSE)
g_constraint &lt;- TSVM(X=X,y=y,X_u=X_u,
                     C=1,Cstar=0.1,balancing_constraint = TRUE)

g_noconstraint &lt;- TSVM(X=X,y=y,X_u=X_u,
                       C=1,Cstar=0.1,balancing_constraint = FALSE)

g_lin &lt;- LinearTSVM(X=X,y=y,X_u=X_u,C=1,Cstar=0.1)

w1 &lt;- g_sup@alpha %*% X
w2 &lt;- g_constraint@alpha %*% rbind(X,X_u,X_u,colMeans(X_u))
w3 &lt;- g_noconstraint@alpha %*% rbind(X,X_u,X_u)
w4 &lt;- g_lin@w

plot(X[,1],X[,2],col=factor(y),asp=1,ylim=c(-3,3))
points(X_u[,1],X_u[,2],col="darkgrey",pch=16,cex=1)
abline(-g_sup@bias/w1[2],-w1[1]/w1[2],lty=2)
abline(((1-g_sup@bias)/w1[2]),-w1[1]/w1[2],lty=2) # +1 Margin
abline(((-1-g_sup@bias)/w1[2]),-w1[1]/w1[2],lty=2) # -1 Margin
abline(-g_constraint@bias/w2[2],-w2[1]/w2[2],lty=1,col="green")
abline(-g_noconstraint@bias/w3[2],-w3[1]/w3[2],lty=1,col="red")
abline(-w4[1]/w4[3],-w4[2]/w4[3],lty=1,lwd=3,col="blue")

# An example
set.seed(42)
data &lt;- generateSlicedCookie(200,expected=TRUE,gap=1)
X &lt;- model.matrix(Class~.-1,data)
y &lt;- factor(data$Class)

problem &lt;- split_dataset_ssl(X,y,frac_ssl=0.98)

X &lt;- problem$X
y &lt;- problem$y
X_u &lt;- problem$X_u
y_e &lt;- unlist(list(problem$y,problem$y_u))
Xe&lt;-rbind(X,X_u)

g_sup &lt;- SVM(X,y,x_center=FALSE,scale=FALSE,C = 10)
g_constraint &lt;- TSVM(X=X,y=y,X_u=X_u,
                     C=10,Cstar=10,balancing_constraint = TRUE,
                     x_center = FALSE,verbose=TRUE)

g_noconstraint &lt;- TSVM(X=X,y=y,X_u=X_u,
                       C=10,Cstar=10,balancing_constraint = FALSE,
                       x_center = FALSE,verbose=TRUE)

g_lin &lt;- LinearTSVM(X=X,y=y,X_u=X_u,C=10,Cstar=10,
                    verbose=TRUE,x_center = FALSE)

g_oracle &lt;- SVM(Xe,y_e,scale=FALSE)

w1 &lt;- c(g_sup@bias,g_sup@alpha %*% X)
w2 &lt;- c(g_constraint@bias,g_constraint@alpha %*% rbind(X,X_u,X_u,colMeans(X_u)))
w3 &lt;- c(g_noconstraint@bias,g_noconstraint@alpha %*% rbind(X,X_u,X_u))
w4 &lt;- g_lin@w
w5 &lt;- c(g_oracle@bias, g_oracle@alpha %*% Xe)
print(sum(abs(w4-w3)))

plot(X[,1],X[,2],col=factor(y),asp=1,ylim=c(-3,3))
points(X_u[,1],X_u[,2],col="darkgrey",pch=16,cex=1)
abline(-w1[1]/w1[3],-w1[2]/w1[3],lty=2)
abline(((1-w1[1])/w1[3]),-w1[2]/w1[3],lty=2) # +1 Margin
abline(((-1-w1[1])/w1[3]),-w1[2]/w1[3],lty=2) # -1 Margin

# Oracle:
abline(-w5[1]/w5[3],-w5[2]/w5[3],lty=1,col="purple")

# With balancing constraint:
abline(-w2[1]/w2[3],-w2[2]/w2[3],lty=1,col="green")

# Linear TSVM implementation (no constraint):
abline(-w4[1]/w4[3],-w4[2]/w4[3],lty=1,lwd=3,col="blue") 

# Without balancing constraint:
abline(-w3[1]/w3[3],-w3[2]/w3[3],lty=1,col="red")
</code></pre>

<hr>
<h2 id='USMLeastSquaresClassifier'>Updated Second Moment Least Squares Classifier</h2><span id='topic+USMLeastSquaresClassifier'></span>

<h3>Description</h3>

<p>This methods uses the closed form solution of the supervised least squares problem, except that the second moment matrix (X'X) is exchanged with a second moment matrix that is estimated based on all data. See for instance <cite>Shaffer1991</cite>, where in this implementation we use all data to estimate E(X'X), instead of just the labeled data. This method seems to work best when the data is first centered <code>x_center=TRUE</code> and the outputs are scaled using <code>y_scale=TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>USMLeastSquaresClassifier(X, y, X_u, lambda = 0, intercept = TRUE,
  x_center = FALSE, scale = FALSE, y_scale = FALSE, ...,
  use_Xu_for_scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="USMLeastSquaresClassifier_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifier_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shaffer, J.P., 1991. The Gauss-Markov Theorem and Random Regressors. The American Statistician, 45(4), pp.269-273.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+WellSVM">WellSVM</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>

<hr>
<h2 id='USMLeastSquaresClassifier-class'>USMLeastSquaresClassifier</h2><span id='topic+USMLeastSquaresClassifier-class'></span>

<h3>Description</h3>

<p>USMLeastSquaresClassifier
</p>

<hr>
<h2 id='wdbc'>wdbc data for unit testing</h2><span id='topic+wdbc'></span>

<h3>Description</h3>

<p>Useful for testing the S4VM and WellSVM implementations
</p>

<hr>
<h2 id='WellSVM'>WellSVM for Semi-supervised Learning</h2><span id='topic+WellSVM'></span>

<h3>Description</h3>

<p>WellSVM is a minimax relaxation of the mixed integer programming problem of finding the optimal labels for the unlabeled data in the SVM objective function. This implementation is a translation of the Matlab implementation of Li (2013) into R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WellSVM(X, y, X_u, C1 = 1, C2 = 0.1, gamma = 1, x_center = TRUE,
  scale = FALSE, use_Xu_for_scaling = FALSE, max_iter = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WellSVM_+3A_x">X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_y">y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_x_u">X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_c1">C1</code></td>
<td>
<p>double; A regularization parameter for labeled data, default 1;</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_c2">C2</code></td>
<td>
<p>double; A regularization parameter for unlabeled data, default 0.1;</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_gamma">gamma</code></td>
<td>
<p>double; Gaussian kernel parameter, i.e., k(x,y) = exp(-gamma^2||x-y||^2/avg) where avg is the average distance among instances; when gamma = 0, linear kernel is used. default gamma = 1;</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
<tr><td><code id="WellSVM_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y.-F. Li, I. W. Tsang, J. T. Kwok, and Z.-H. Zhou. Scalable and Convex Weakly Labeled SVMs. Journal of Machine Learning Research, 2013.
</p>
<p>R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training SVM. Journal of Machine Learning Research 6, 1889-1918, 2005.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code><a href="#topic+EMLeastSquaresClassifier">EMLeastSquaresClassifier</a></code>,
<code><a href="#topic+EMLinearDiscriminantClassifier">EMLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+GRFClassifier">GRFClassifier</a></code>,
<code><a href="#topic+ICLeastSquaresClassifier">ICLeastSquaresClassifier</a></code>,
<code><a href="#topic+ICLinearDiscriminantClassifier">ICLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+KernelLeastSquaresClassifier">KernelLeastSquaresClassifier</a></code>,
<code><a href="#topic+LaplacianKernelLeastSquaresClassifier">LaplacianKernelLeastSquaresClassifier</a>()</code>,
<code><a href="#topic+LaplacianSVM">LaplacianSVM</a></code>,
<code><a href="#topic+LeastSquaresClassifier">LeastSquaresClassifier</a></code>,
<code><a href="#topic+LinearDiscriminantClassifier">LinearDiscriminantClassifier</a></code>,
<code><a href="#topic+LinearSVM">LinearSVM</a></code>,
<code><a href="#topic+LinearTSVM">LinearTSVM</a>()</code>,
<code><a href="#topic+LogisticLossClassifier">LogisticLossClassifier</a></code>,
<code><a href="#topic+LogisticRegression">LogisticRegression</a></code>,
<code><a href="#topic+MCLinearDiscriminantClassifier">MCLinearDiscriminantClassifier</a></code>,
<code><a href="#topic+MCNearestMeanClassifier">MCNearestMeanClassifier</a></code>,
<code><a href="#topic+MCPLDA">MCPLDA</a></code>,
<code><a href="#topic+MajorityClassClassifier">MajorityClassClassifier</a></code>,
<code><a href="#topic+NearestMeanClassifier">NearestMeanClassifier</a></code>,
<code><a href="#topic+QuadraticDiscriminantClassifier">QuadraticDiscriminantClassifier</a></code>,
<code><a href="#topic+S4VM">S4VM</a></code>,
<code><a href="#topic+SVM">SVM</a></code>,
<code><a href="#topic+SelfLearning">SelfLearning</a></code>,
<code><a href="#topic+TSVM">TSVM</a></code>,
<code><a href="#topic+USMLeastSquaresClassifier">USMLeastSquaresClassifier</a></code>,
<code><a href="#topic+svmlin">svmlin</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RSSL)
library(ggplot2)
library(dplyr)

set.seed(1)
df_orig &lt;- generateSlicedCookie(200,expected=TRUE)
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.98)

classifiers &lt;- list("Well"=WellSVM(Class~.,df,C1 = 1, C2=0.1, 
                                   gamma = 0,x_center=TRUE,scale=TRUE),
                    "Sup"=SVM(Class~.,df,C=1,x_center=TRUE,scale=TRUE))

df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class)) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(color=..classifier..),
                  classifiers = classifiers)
</code></pre>

<hr>
<h2 id='wellsvm_direct'>wellsvm implements the wellsvm algorithm as shown in [1].</h2><span id='topic+wellsvm_direct'></span>

<h3>Description</h3>

<p>wellsvm implements the wellsvm algorithm as shown in [1].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wellsvm_direct(x, y, testx, testy, C1 = 1, C2 = 0.1, gamma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wellsvm_direct_+3A_x">x</code></td>
<td>
<p>A Nxd training data matrix, where N is the number of training instances and d is the dimension of instance;</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_y">y</code></td>
<td>
<p>A Nx1 training label vector, where y = 1/-1 means positive/negative, and y = 0 means unlabeled;</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_testx">testx</code></td>
<td>
<p>A Mxd testing data matrix, where M is the number of testing instances;</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_testy">testy</code></td>
<td>
<p>A Mx1 testing label vector</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_c1">C1</code></td>
<td>
<p>A regularization parameter for labeled data, default 1;</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_c2">C2</code></td>
<td>
<p>A regularization parameter for unlabeled data, default 0.1;</p>
</td></tr>
<tr><td><code id="wellsvm_direct_+3A_gamma">gamma</code></td>
<td>
<p>Gaussian kernel parameter, i.e., k(x,y) = exp(-gamma^2||x-y||^2/avg) where avg is the average distance among instances; when gamma = 0, linear kernel is used. default gamma = 1;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prediction   - A Mx1 predicted testing label vector; accuracy     - The accuracy of prediction; cputime     - cpu running time;
</p>


<h3>References</h3>

<p>Y.-F. Li, I. W. Tsang, J. T. Kwok, and Z.-H. Zhou. Scalable and Convex Weakly Labeled SVMs. Journal of Machine Learning Research, 2013.
</p>
<p>R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training SVM. Journal of Machine Learning Research 6, 1889-1918, 2005.
</p>

<hr>
<h2 id='WellSVM_SSL'>Convex relaxation of S3VM by label generation</h2><span id='topic+WellSVM_SSL'></span>

<h3>Description</h3>

<p>Convex relaxation of S3VM by label generation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WellSVM_SSL(K0, y, opt, yinit = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WellSVM_SSL_+3A_k0">K0</code></td>
<td>
<p>kernel matrix</p>
</td></tr>
<tr><td><code id="WellSVM_SSL_+3A_y">y</code></td>
<td>
<p>labels</p>
</td></tr>
<tr><td><code id="WellSVM_SSL_+3A_opt">opt</code></td>
<td>
<p>options</p>
</td></tr>
<tr><td><code id="WellSVM_SSL_+3A_yinit">yinit</code></td>
<td>
<p>label initialization (not used)</p>
</td></tr>
</table>

<hr>
<h2 id='WellSVM_supervised'>A degenerated version of WellSVM where the labels are complete, that is, supervised learning</h2><span id='topic+WellSVM_supervised'></span>

<h3>Description</h3>

<p>A degenerated version of WellSVM where the labels are complete, that is, supervised learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WellSVM_supervised(K0, y, opt, ind_y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WellSVM_supervised_+3A_k0">K0</code></td>
<td>
<p>kernel matrix</p>
</td></tr>
<tr><td><code id="WellSVM_supervised_+3A_y">y</code></td>
<td>
<p>labels</p>
</td></tr>
<tr><td><code id="WellSVM_supervised_+3A_opt">opt</code></td>
<td>
<p>options</p>
</td></tr>
<tr><td><code id="WellSVM_supervised_+3A_ind_y">ind_y</code></td>
<td>
<p>Labeled/Unlabeled indicator</p>
</td></tr>
</table>

<hr>
<h2 id='wlda'>Implements weighted likelihood estimation for LDA</h2><span id='topic+wlda'></span>

<h3>Description</h3>

<p>Implements weighted likelihood estimation for LDA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wlda(a, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wlda_+3A_a">a</code></td>
<td>
<p>is the data set</p>
</td></tr>
<tr><td><code id="wlda_+3A_w">w</code></td>
<td>
<p>is an indicator matrix for the K classes or, potentially, a weight matrix in which the fraction with which a sample belongs to a particular class is indicated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>m contains the means, p contains the class priors, iW contains the INVERTED within covariance matrix
</p>

<hr>
<h2 id='wlda_error'>Measures the expected error of the LDA model defined by m, p,
and iW on the data set a, where weights w are potentially taken into
account</h2><span id='topic+wlda_error'></span>

<h3>Description</h3>

<p>Measures the expected error of the LDA model defined by m, p,
and iW on the data set a, where weights w are potentially taken into
account
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wlda_error(m, p, iW, a, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wlda_error_+3A_m">m</code></td>
<td>
<p>means</p>
</td></tr>
<tr><td><code id="wlda_error_+3A_p">p</code></td>
<td>
<p>class prior</p>
</td></tr>
<tr><td><code id="wlda_error_+3A_iw">iW</code></td>
<td>
<p>is the inverse of the within covariance matrix</p>
</td></tr>
<tr><td><code id="wlda_error_+3A_a">a</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="wlda_error_+3A_w">w</code></td>
<td>
<p>weights</p>
</td></tr>
</table>

<hr>
<h2 id='wlda_loglik'>Measures the expected log-likelihood of the LDA model defined by m, p,
and iW on the data set a, where weights w are potentially taken into
account</h2><span id='topic+wlda_loglik'></span>

<h3>Description</h3>

<p>Measures the expected log-likelihood of the LDA model defined by m, p,
and iW on the data set a, where weights w are potentially taken into
account
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wlda_loglik(m, p, iW, a, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wlda_loglik_+3A_m">m</code></td>
<td>
<p>means</p>
</td></tr>
<tr><td><code id="wlda_loglik_+3A_p">p</code></td>
<td>
<p>class prior</p>
</td></tr>
<tr><td><code id="wlda_loglik_+3A_iw">iW</code></td>
<td>
<p>is the inverse of the within covariance matrix</p>
</td></tr>
<tr><td><code id="wlda_loglik_+3A_a">a</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="wlda_loglik_+3A_w">w</code></td>
<td>
<p>weights</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Average log likelihood
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
