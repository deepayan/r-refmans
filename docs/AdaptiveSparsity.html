<!DOCTYPE html><html><head><title>Help for package AdaptiveSparsity</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {AdaptiveSparsity}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#asggm'>
<p>Adaptively Sparse Gaussian Graphical Model</p></a></li>
<li><a href='#asggm-internal'>
<p>asggm internal functions</p></a></li>
<li><a href='#aslm'>
<p>Adaptive Sparse Linear Model</p></a></li>
<li><a href='#aslm-internal'>
<p>aslm internal functions</p></a></li>
<li><a href='#aslm-methods'>
<p>Methods handled by lm</p></a></li>
<li><a href='#aslm-package'>
<p>Adaptive Sparsity Models Model</p></a></li>
<li><a href='#summary.aslm'>
<p>Handling aslm objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Adaptive Sparsity Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-08-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Kristen Zygmunt, Eleanor Wong, Tom Fletcher</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kris Campbell &lt;kris@sci.utah.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements Figueiredo EM algorithm for adaptive sparsity (Jeffreys prior) (see Figueiredo, M.A.T.; , "Adaptive sparseness for supervised learning," Pattern Analysis and Machine Intelligence, IEEE Transactions on , vol.25, no.9, pp. 1150- 1159, Sept. 2003) and Wong algorithm for adaptively sparse gaussian geometric models (see Wong, Eleanor, Suyash Awate, and P. Thomas Fletcher. "Adaptive Sparsity in Gaussian Graphical Models." In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 311-319. 2013.)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3.0)</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, Matrix, Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp (&ge; 0.12.13), RcppArmadillo (&ge; 0.2.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-08-20 21:38:23 UTC; kris</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-08-20 22:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='asggm'>
Adaptively Sparse Gaussian Graphical Model
</h2><span id='topic+asggm'></span><span id='topic+asggm.formula'></span><span id='topic+asggm.default'></span>

<h3>Description</h3>

<p>implements a parameter-free adaptively sparse Gaussian graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
asggm(formula, data=list(), ...)
## Default S3 method:
asggm(x, iterations = 100000000, init = NULL, epsilon = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asggm_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;<a href="stats.html#topic+formula">formula</a>&rdquo; (or one that can be coerced to that class): a symbolic description of the model to be fitted.
See <code><a href="stats.html#topic+lm">lm</a></code> Details for further information.
</p>
</td></tr>
<tr><td><code id="asggm_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment containing the variables in the model.
</p>
</td></tr>
<tr><td><code id="asggm_+3A_x">x</code></td>
<td>

<p>design matrix
</p>
</td></tr>
<tr><td><code id="asggm_+3A_iterations">iterations</code></td>
<td>

<p>number of iterations of the algorithm to run.
</p>
</td></tr>
<tr><td><code id="asggm_+3A_init">init</code></td>
<td>

<p>optional initialization, for instance, the cholesky of <code>x</code>.  If NULL, it defaults to the cholesky of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="asggm_+3A_epsilon">epsilon</code></td>
<td>

<p>amount to add for numerical stability.
</p>
</td></tr>
<tr><td><code id="asggm_+3A_...">...</code></td>
<td>

<p>further arguments
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An effective approach to structure learning and parameter estimation
for Gaussian graphical models is to impose a sparsity prior, such as a
Laplace prior, on the entries of the precision matrix. We introduce a
parameter-free method for estimating a precision matrix with sparsity
that adapts to the data automatically, achieved by formulating a
hierarchical Bayesian model of the precision matrix with a
non-informative Jeffreys' hyperprior. We also naturally enforce the
symmetry and positive-definiteness constraints on the precision matrix
by parameterizing it with the Cholesky decomposition.
</p>


<h3>Value</h3>

<p><code>asggm</code> returns an object of class <code>"asggm"</code>.
</p>
<p>An object of class &ldquo;<code>asggm</code>&rdquo; is a list containing at least the following components:
</p>


<h3>Author(s)</h3>

<p>Kristen Zygmunt, Eleanor Wong, Tom Fletcher
</p>


<h3>References</h3>

<p><cite>Wong, Eleanor, Suyash Awate, and P. Thomas Fletcher. &ldquo;Adaptive Sparsity in Gaussian Graphical Models.&rdquo;In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 311-319. 2013.</cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A = diag(3)
asggm(A)
</code></pre>

<hr>
<h2 id='asggm-internal'>
asggm internal functions
</h2><span id='topic+rCSL'></span><span id='topic+genData'></span><span id='topic+genL'></span>

<h3>Description</h3>

<p>These are the fitting and initialization functions used by asggm.  These should generally <em>not</em> be used directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rCSL(x, iterations = 500, init = NULL, epsilon = 1e-05, ansL = NULL)
genL(kNodes, spP)
genData(L, nSamples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asggm-internal_+3A_x">x</code></td>
<td>

<p>design matrix
</p>
</td></tr>
<tr><td><code id="asggm-internal_+3A_iterations">iterations</code></td>
<td>

<p>number of iterations of the algorithm to run.
</p>
</td></tr>
<tr><td><code id="asggm-internal_+3A_init">init</code></td>
<td>

<p>optional initialization, for instance, the cholesky of <code>x</code>.  If NULL, it defaults to the cholesky of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="asggm-internal_+3A_epsilon">epsilon</code></td>
<td>

<p>amount to add for numerical stability.
</p>
</td></tr>
<tr><td><code id="asggm-internal_+3A_ansl">ansL</code></td>
<td>

</td></tr>
<tr><td><code id="asggm-internal_+3A_knodes">kNodes</code></td>
<td>

</td></tr>
<tr><td><code id="asggm-internal_+3A_spp">spP</code></td>
<td>

</td></tr>
<tr><td><code id="asggm-internal_+3A_l">L</code></td>
<td>

<p>L created by genL
</p>
</td></tr>
<tr><td><code id="asggm-internal_+3A_nsamples">nSamples</code></td>
<td>

<p>number of samples.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>rCSL calls the C++ code to compute the Wong EM algorithm.
genL and genData are used together to create example data.
</p>


<h3>Value</h3>

<p>rCSL returns a list with the following components:
</p>


<h3>References</h3>

<p><cite>Wong, Eleanor, Suyash Awate, and P. Thomas Fletcher. &ldquo;Adaptive Sparsity in Gaussian Graphical Models.&rdquo;In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 311-319. 2013.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asggm">asggm</a></code>, which should be used directly instead of these methods
</p>

<hr>
<h2 id='aslm'>
Adaptive Sparse Linear Model
</h2><span id='topic+aslm'></span><span id='topic+getSparseModel'></span><span id='topic+aslm.formula'></span><span id='topic+aslm.default'></span>

<h3>Description</h3>

<p>implements the adaptive sparse linear model using Figueiredo's EM algorithm 
for adaptive sparsity (Jeffreys prior)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
aslm(formula, data=list(), na.action=na.omit, ...)
## Default S3 method:
aslm(x, y, ...)
getSparseModel(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aslm_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;<a href="stats.html#topic+formula">formula</a>&rdquo; (or one that can be coerced to that class): a symbolic description of the model to be fitted.
See <code><a href="stats.html#topic+lm">lm</a></code> Details for further information.
</p>
</td></tr>
<tr><td><code id="aslm_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment containing the variables in the model.
</p>
</td></tr>
<tr><td><code id="aslm_+3A_na.action">na.action</code></td>
<td>

<p>action to use when data contains NAs.  Options include na.omit, na.exclude, na.fail
</p>
</td></tr>
<tr><td><code id="aslm_+3A_x">x</code></td>
<td>

<p>design matrix
</p>
</td></tr>
<tr><td><code id="aslm_+3A_y">y</code></td>
<td>

<p>vector of observations
</p>
</td></tr>
<tr><td><code id="aslm_+3A_...">...</code></td>
<td>

<p>further arguments
</p>
</td></tr>
<tr><td><code id="aslm_+3A_object">object</code></td>
<td>

<p>an object of class &ldquo;<a href="#topic+aslm">aslm</a>&rdquo;.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>aslm</code> returns an object of class <code>c("aslm", "lm")</code>.
</p>
<p>An object of class &ldquo;<code>aslm</code>&rdquo; is a list containing at least the following components:
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p> a named vector of coefficients </p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p> the residuals, that is response minus fitted values. </p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p> the fitted mean values </p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p> the numeric rnak of the fitted linear model </p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p> the residual degrees of freedom </p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p> the matched call </p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p> the terms object used </p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
  </td></tr>
</table>
<p><code>getSparseModel</code> returns an object of class <code>"lm"</code> that is a model consisting of only the sparse nonzero variables from the original model.
</p>


<h3>Author(s)</h3>

<p>Kristen Zygmunt, Eleanor Wong, Tom Fletcher
</p>


<h3>References</h3>

<p><cite>Figueiredo, M.A.T.; , &ldquo;Adaptive sparseness for supervised learning&rdquo;, Pattern Analysis and Machine Intelligence, IEEE Transactions on , vol.25, no.9, pp. 1150- 1159, Sept. 2003</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.aslm">summary.aslm</a></code>, <code><a href="#topic+logLik.aslm">logLik.aslm</a></code>, <code><a href="#topic+print.aslm">print.aslm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s = aslm(Infant.Mortality~.,data=swiss)
m = getSparseModel(s)

summary(s)
coef(m)
</code></pre>

<hr>
<h2 id='aslm-internal'>
aslm internal functions
</h2><span id='topic+figEM'></span><span id='topic+fit.ols.lm'></span><span id='topic+init.ones'></span><span id='topic+init.rnorm'></span><span id='topic+init.runif'></span>

<h3>Description</h3>

<p>These are the fitting and initialization functions used by aslm.  These should generally <em>not</em> be used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>figEM(x, y, init = NULL, stopDiff = 1e-08, epsilon = 1e-06, a = 1)
fit.ols.lm(x, y)
init.ones(x, y)
init.rnorm(x, y)
init.runif(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aslm-internal_+3A_x">x</code></td>
<td>

<p>design matrix of dimension <code> n * p </code>.
</p>
</td></tr>
<tr><td><code id="aslm-internal_+3A_y">y</code></td>
<td>

<p>vector of observations of length n, or a matrix with n rows.
</p>
</td></tr>
<tr><td><code id="aslm-internal_+3A_init">init</code></td>
<td>

<p>optional initialization, a list with components containing an initial estimate for <code>beta</code> and <code>sigma</code>
</p>
</td></tr>
<tr><td><code id="aslm-internal_+3A_stopdiff">stopDiff</code></td>
<td>

<p>convergence criteria.  Algorithm stops once difference in beta and sigma from one iteration to the next is less than stopDiff.
</p>
</td></tr>
<tr><td><code id="aslm-internal_+3A_epsilon">epsilon</code></td>
<td>

<p>amount to add to beta for numerical stability,
</p>
</td></tr>
<tr><td><code id="aslm-internal_+3A_a">a</code></td>
<td>

<p>scaling of sigmaSqr to provide numerical stability for solving steps.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>figEM computes the Figueiredo EM algorithm for adaptive sparsity using Jeffreys prior.
</p>
<p>fit.ols.lm computes an initial beta and sigma based on finding the lm.fit of the full design matrix.
</p>
<p>init.ones computes an initial beta that is all ones and computes the associated sigmas.
</p>
<p>init.rnorm computes an initial beta that is normally distributed with a mean of 0 and a standard deviation of 50
</p>
<p>init.runif computes an initial beta that is uniformly distributed from 0 to 1
</p>
<p>Currently, figEM uses fit.ols.lm to initialize beta and sigma if no init list is provided.
</p>


<h3>Value</h3>

<p>figEM returns a list with the following components:
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
 <p><code>p</code> vector (also known as <code>beta</code>). </p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p> variance-covariance matrix. </p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p> norm of the model error. </p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p> degrees of freedom of residuals. </p>
</td></tr>
</table>
<p>fit.ols.lm and init.ones are used to initialize beta and sigma if init is not provided to figEM.
Each of these functions returns a list with the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p> initial <code>p</code> vector. </p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p> initial norm of the model error based on this initial beta. </p>
</td></tr>
</table>


<h3>References</h3>

<p><cite>Figueiredo, M.A.T.; , &ldquo;Adaptive sparseness for supervised learning&rdquo;, Pattern Analysis and Machine Intelligence, IEEE Transactions on , vol.25, no.9, pp. 1150- 1159, Sept. 2003</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aslm">aslm</a></code>, which should be used directly instead of these methods
</p>

<hr>
<h2 id='aslm-methods'>
Methods handled by lm
</h2><span id='topic+logLik.aslm'></span><span id='topic+nobs.aslm'></span><span id='topic+predict.aslm'></span>

<h3>Description</h3>

<p>These methods are implemented by the <code><a href="stats.html#topic+lm">lm</a></code> parent class:
</p>

<ul>
<li><p><code>logLik</code>  &ndash; Extract log-likelihood 
</p>
</li>
<li><p><code>predict</code>  &ndash; Predict values based on linear model 
</p>
</li>
<li><p><code>nobs</code>  &ndash; Extract the number of observations from a fit 
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict.lm">predict.lm</a></code>, <code><a href="stats.html#topic+logLik.lm">logLik.lm</a></code>, <code><a href="stats.html#topic+nobs">nobs</a></code>
</p>

<hr>
<h2 id='aslm-package'>
Adaptive Sparsity Models Model
</h2><span id='topic+aslm-package'></span><span id='topic+AdaptiveSparsity'></span><span id='topic+asggm-package'></span>

<h3>Description</h3>

<p>implements the adaptive sparse linear model using Figueiredo's EM algorithm 
for adaptive sparsity (Jeffreys prior) and the adaptively sparse gaussian graphical model using Wong's parameter-free algorithm.
</p>


<h3>Author(s)</h3>

<p>Kristen Zygmunt, Eleanor Wong, Tom Fletcher
</p>
<p>Maintainer: Kristen Zygmunt &lt;krismz@sci.utah.edu&gt;
</p>


<h3>References</h3>

<p><cite>Figueiredo, M.A.T.; , &ldquo;Adaptive sparseness for supervised learning&rdquo;, Pattern Analysis and Machine Intelligence, IEEE Transactions on , vol.25, no.9, pp. 1150- 1159, Sept. 2003</cite>
</p>
<p><cite>Wong, Eleanor, Suyash Awate, and P. Thomas Fletcher. &ldquo;Adaptive Sparsity in Gaussian Graphical Models.&rdquo;In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 311-319. 2013.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aslm">aslm</a></code>, <code><a href="#topic+asggm">asggm</a></code>
</p>

<hr>
<h2 id='summary.aslm'>
Handling aslm objects
</h2><span id='topic+summary.aslm'></span><span id='topic+print.aslm'></span><span id='topic+print.summary.aslm'></span>

<h3>Description</h3>

<p>summary and print methods for class &ldquo;<code>aslm</code>&rdquo;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aslm'
summary(object, ...)
## S3 method for class 'summary.aslm'
print(x, ...)
## S3 method for class 'aslm'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.aslm_+3A_object">object</code></td>
<td>

<p>An object of class &ldquo;<code>aslm</code>&rdquo;, usually a result of a call to <code><a href="#topic+aslm">aslm</a></code>
</p>
</td></tr>
<tr><td><code id="summary.aslm_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;<code>summary.aslm</code>&rdquo; or &ldquo;<code>aslm</code>&rdquo;
</p>
</td></tr>
<tr><td><code id="summary.aslm_+3A_...">...</code></td>
<td>

<p>Further arguments
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>summary and print methods to help display and work with aslm objects.
</p>


<h3>Value</h3>

<p><code>print</code> prints a brief overview 
</p>


<h3>Author(s)</h3>

<p>Kristen Zygmunt, Eleanor Wong, Tom Fletcher
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aslm">aslm</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
