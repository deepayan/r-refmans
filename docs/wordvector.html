<!DOCTYPE html><html lang="en-US"><head><title>Help for package wordvector</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {wordvector}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#analogy'><p>Convert formula to named character vector</p></a></li>
<li><a href='#as.matrix.textmodel_wordvector'><p>Extract word vectors</p></a></li>
<li><a href='#data_corpus_news2014'><p>Yahoo News summaries from 2014</p></a></li>
<li><a href='#print.textmodel_docvector'><p>Print method for trained document vectors</p></a></li>
<li><a href='#print.textmodel_wordvector'><p>Print method for trained word vectors</p></a></li>
<li><a href='#probability'><p>Compute probability of words</p></a></li>
<li><a href='#similarity'><p>Compute similarity between word vectors</p></a></li>
<li><a href='#textmodel_doc2vec'><p>Create distributed representation of documents</p></a></li>
<li><a href='#textmodel_lsa'><p>Latent Semantic Analysis model</p></a></li>
<li><a href='#textmodel_word2vec'><p>Word2vec model</p></a></li>
<li><a href='#weights'><p>[experimental] Extract word vector weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Word and Document Vector Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kohei Watanabe &lt;watanabe.kohei@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) &lt;<a href="https://doi.org/10.48550%2FarXiv.1310.4546">doi:10.48550/arXiv.1310.4546</a>&gt; and Latent Semantic Analysis (Deerwester et al., 1990) &lt;<a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9">doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/koheiw/wordvector">https://github.com/koheiw/wordvector</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>quanteda (&ge; 4.1.0), methods, stringi, Matrix, proxyC,
RSpectra, irlba, rsvd</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, word2vec, spelling</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, quanteda</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-12 02:59:00 UTC; watan</td>
</tr>
<tr>
<td>Author:</td>
<td>Kohei Watanabe <a href="https://orcid.org/0000-0001-6519-5265"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Jan Wijffels [aut] (Original R code),
  BNOSAC [cph] (Original R code),
  Max Fomichev [ctb, cph] (Original C++ code)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-12 11:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='analogy'>Convert formula to named character vector</h2><span id='topic+analogy'></span>

<h3>Description</h3>

<p>Convert a formula to a named character vector in analogy tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>analogy(formula)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="analogy_+3A_formula">formula</code></td>
<td>
<p>a <a href="stats.html#topic+formula">formula</a> object that defines the relationship between words
using <code>+</code> or <code>-</code> operators.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named character vector to be passed to <code><a href="#topic+similarity">similarity()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+similarity">similarity()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>analogy(~ berlin - germany + france)
analogy(~ quick - quickly + slowly)
</code></pre>

<hr>
<h2 id='as.matrix.textmodel_wordvector'>Extract word vectors</h2><span id='topic+as.matrix.textmodel_wordvector'></span>

<h3>Description</h3>

<p>Extract word vectors from a <code>textmodel_wordvector</code> or <code>textmodel_docvector</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_wordvector'
as.matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.matrix.textmodel_wordvector_+3A_x">x</code></td>
<td>
<p>a <code>textmodel_wordvector</code> or <code>textmodel_docvector</code> object.</p>
</td></tr>
<tr><td><code id="as.matrix.textmodel_wordvector_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix that contain the word vectors in rows
</p>

<hr>
<h2 id='data_corpus_news2014'>Yahoo News summaries from 2014</h2><span id='topic+data_corpus_news2014'></span>

<h3>Description</h3>

<p>A corpus object containing 2,000 news summaries collected from Yahoo News via
RSS feeds in 2014. The title and description of the summaries are concatenated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_corpus_news2014
</code></pre>


<h3>Format</h3>

<p>An object of class <code>corpus</code> (inherits from <code>character</code>) of length 20000.
</p>


<h3>Source</h3>

<p><a href="https://www.yahoo.com/news/">https://www.yahoo.com/news/</a>
</p>


<h3>References</h3>

<p>Watanabe, K. (2018). Newsmap: A semi-supervised approach to
geographical news classification. Digital Journalism, 6(3), 294â€“309.
https://doi.org/10.1080/21670811.2017.1293487
</p>

<hr>
<h2 id='print.textmodel_docvector'>Print method for trained document vectors</h2><span id='topic+print.textmodel_docvector'></span>

<h3>Description</h3>

<p>Print method for trained document vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_docvector'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.textmodel_docvector_+3A_x">x</code></td>
<td>
<p>for print method, the object to be printed</p>
</td></tr>
<tr><td><code id="print.textmodel_docvector_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an invisible copy of <code>x</code>.
</p>

<hr>
<h2 id='print.textmodel_wordvector'>Print method for trained word vectors</h2><span id='topic+print.textmodel_wordvector'></span>

<h3>Description</h3>

<p>Print method for trained word vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_wordvector'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.textmodel_wordvector_+3A_x">x</code></td>
<td>
<p>for print method, the object to be printed</p>
</td></tr>
<tr><td><code id="print.textmodel_wordvector_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an invisible copy of <code>x</code>.
</p>

<hr>
<h2 id='probability'>Compute probability of words</h2><span id='topic+probability'></span>

<h3>Description</h3>

<p>Compute the probability of words given other words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probability(x, words, mode = c("words", "values"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="probability_+3A_x">x</code></td>
<td>
<p>a <code>textmodel_wordvector</code> object fitted with <code>normalize = FALSE</code>.</p>
</td></tr>
<tr><td><code id="probability_+3A_words">words</code></td>
<td>
<p>words for which probability is computed.</p>
</td></tr>
<tr><td><code id="probability_+3A_mode">mode</code></td>
<td>
<p>specify the type of resulting object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>matrix</code> of probability scores when <code>mode = "values"</code> or of words
sorted in descending order by the probability scores when <code>mode = "words"</code>.
When <code>words</code> is a named numeric vector, probability scores are weighted by
the  values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+similarity">similarity()</a></code>
</p>

<hr>
<h2 id='similarity'>Compute similarity between word vectors</h2><span id='topic+similarity'></span>

<h3>Description</h3>

<p>Compute the cosine similarity between word vectors for selected words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>similarity(x, words, mode = c("words", "values"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="similarity_+3A_x">x</code></td>
<td>
<p>a <code>textmodel_wordvector</code> object.</p>
</td></tr>
<tr><td><code id="similarity_+3A_words">words</code></td>
<td>
<p>words for which similarity is computed.</p>
</td></tr>
<tr><td><code id="similarity_+3A_mode">mode</code></td>
<td>
<p>specify the type of resulting object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>matrix</code> of cosine similarity scores when <code>mode = "values"</code> or of
words sorted in descending order by the similarity scores when <code>mode = "words"</code>.
When <code>words</code> is a named numeric vector, word vectors are weighted and summed
before computing similarity scores.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+probability">probability()</a></code>
</p>

<hr>
<h2 id='textmodel_doc2vec'>Create distributed representation of documents</h2><span id='topic+textmodel_doc2vec'></span>

<h3>Description</h3>

<p>Create distributed representation of documents as weighted word vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_doc2vec(x, model, group_data = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textmodel_doc2vec_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+tokens">quanteda::tokens</a> object.</p>
</td></tr>
<tr><td><code id="textmodel_doc2vec_+3A_model">model</code></td>
<td>
<p>a textmodel_wordvector object.</p>
</td></tr>
<tr><td><code id="textmodel_doc2vec_+3A_group_data">group_data</code></td>
<td>
<p>if <code>TRUE</code>, apply <code>dfm_group(x)</code> before creating document vectors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a textmodel_docvector object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>a matrix for document vectors.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>the size of the document vectors.</p>
</td></tr>
<tr><td><code>concatenator</code></td>
<td>
<p>the concatenator in <code>x</code>.</p>
</td></tr>
<tr><td><code>docvars</code></td>
<td>
<p>document variables compied from <code>x</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the command used to execute the function.</p>
</td></tr>
<tr><td><code>version</code></td>
<td>
<p>the version of the wordvector package.</p>
</td></tr>
</table>

<hr>
<h2 id='textmodel_lsa'>Latent Semantic Analysis model</h2><span id='topic+textmodel_lsa'></span>

<h3>Description</h3>

<p>Train a Latent Semantic Analysis model (Deerwester et al., 1990) on a <a href="quanteda.html#topic+tokens">quanteda::tokens</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_lsa(
  x,
  dim = 50,
  min_count = 5L,
  engine = c("RSpectra", "irlba", "rsvd"),
  weight = "count",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textmodel_lsa_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+tokens">quanteda::tokens</a> object.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_dim">dim</code></td>
<td>
<p>the size of the word vectors.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_min_count">min_count</code></td>
<td>
<p>the minimum frequency of the words. Words less frequent than
this in <code>x</code> are removed before training.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_engine">engine</code></td>
<td>
<p>select the engine perform SVD to generate word vectors.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_weight">weight</code></td>
<td>
<p>weighting scheme passed to <code><a href="quanteda.html#topic+dfm_weight">quanteda::dfm_weight()</a></code>.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, print the progress of training.</p>
</td></tr>
<tr><td><code id="textmodel_lsa_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a textmodel_wordvector object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>a matrix for word vectors values.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>a matrix for word vectors weights.</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>the frequency of words in <code>x</code>.</p>
</td></tr>
<tr><td><code>engine</code></td>
<td>
<p>the SVD engine used.</p>
</td></tr>
<tr><td><code>weight</code></td>
<td>
<p>weighting scheme.</p>
</td></tr>
<tr><td><code>concatenator</code></td>
<td>
<p>the concatenator in <code>x</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the command used to execute the function.</p>
</td></tr>
<tr><td><code>version</code></td>
<td>
<p>the version of the wordvector package.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., &amp; Harshman, R. A. (1990).
Indexing by latent semantic analysis. JASIS, 41(6), 391â€“407.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)
library(wordvector)

# pre-processing
corp &lt;- corpus_reshape(data_corpus_news2014)
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE) %&gt;% 
   tokens_remove(stopwords("en", "marimo"), padding = TRUE) %&gt;% 
   tokens_select("^[a-zA-Z-]+$", valuetype = "regex", case_insensitive = FALSE,
                 padding = TRUE) %&gt;% 
   tokens_tolower()

# train LSA
lsa &lt;- textmodel_lsa(toks, dim = 50, min_count = 5, verbose = TRUE)

# find similar words
head(similarity(lsa, c("berlin", "germany", "france"), mode = "words"))
head(similarity(lsa, c("berlin" = 1, "germany" = -1, "france" = 1), mode = "values"))
head(similarity(lsa, analogy(~ berlin - germany + france)))

</code></pre>

<hr>
<h2 id='textmodel_word2vec'>Word2vec model</h2><span id='topic+textmodel_word2vec'></span>

<h3>Description</h3>

<p>Train a Word2vec model (Mikolov et al., 2023) in different architectures on a <a href="quanteda.html#topic+tokens">quanteda::tokens</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_word2vec(
  x,
  dim = 50,
  type = c("cbow", "skip-gram"),
  min_count = 5L,
  window = ifelse(type == "cbow", 5L, 10L),
  iter = 10L,
  alpha = 0.05,
  use_ns = TRUE,
  ns_size = 5L,
  sample = 0.001,
  normalize = TRUE,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textmodel_word2vec_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+tokens">quanteda::tokens</a> object.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_dim">dim</code></td>
<td>
<p>the size of the word vectors.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_type">type</code></td>
<td>
<p>the architecture of the model; either &quot;cbow&quot; (continuous back of words) or &quot;skip-gram&quot;.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_min_count">min_count</code></td>
<td>
<p>the minimum frequency of the words. Words less frequent than
this in <code>x</code> are removed before training.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_window">window</code></td>
<td>
<p>the size of the word window. Words within this window are considered
to be the context of a target word.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_iter">iter</code></td>
<td>
<p>the number of iterations in model training.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_alpha">alpha</code></td>
<td>
<p>the initial learning rate.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_use_ns">use_ns</code></td>
<td>
<p>if <code>TRUE</code>, negative sampling is used. Otherwise, hierarchical softmax
is used.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_ns_size">ns_size</code></td>
<td>
<p>the size of negative samples. Only used when <code>use_ns = TRUE</code>.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_sample">sample</code></td>
<td>
<p>the rate of sampling of words based on their frequency. Sampling is
disabled when <code>sample = 1.0</code></p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_normalize">normalize</code></td>
<td>
<p>if <code>TRUE</code>, normalize the vectors in <code>values</code> and <code>weights</code>.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, print the progress of training.</p>
</td></tr>
<tr><td><code id="textmodel_word2vec_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>User can changed the number of processors used for the parallel computing via
<code>options(wordvector_threads)</code>.
</p>


<h3>Value</h3>

<p>Returns a textmodel_wordvector object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>a matrix for word vector values.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>a matrix for word vector weights.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>the size of the word vectors.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the architecture of the model.</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>the frequency of words in <code>x</code>.</p>
</td></tr>
<tr><td><code>window</code></td>
<td>
<p>the size of the word window.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>the number of iterations in model training.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the initial learning rate.</p>
</td></tr>
<tr><td><code>use_ns</code></td>
<td>
<p>the use of negative sampling.</p>
</td></tr>
<tr><td><code>ns_size</code></td>
<td>
<p>the size of negative samples.</p>
</td></tr>
<tr><td><code>concatenator</code></td>
<td>
<p>the concatenator in <code>x</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the command used to execute the function.</p>
</td></tr>
<tr><td><code>version</code></td>
<td>
<p>the version of the wordvector package.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013).
Distributed Representations of Words and Phrases and their Compositionality.
https://arxiv.org/abs/1310.4546.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)
library(wordvector)

# pre-processing
corp &lt;- data_corpus_news2014 
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE) %&gt;% 
   tokens_remove(stopwords("en", "marimo"), padding = TRUE) %&gt;% 
   tokens_select("^[a-zA-Z-]+$", valuetype = "regex", case_insensitive = FALSE,
                 padding = TRUE) %&gt;% 
   tokens_tolower()

# train word2vec
w2v &lt;- textmodel_word2vec(toks, dim = 50, type = "cbow", min_count = 5, sample = 0.001)

# find similar words
head(similarity(w2v, c("berlin", "germany", "france"), mode = "words"))
head(similarity(w2v, c("berlin" = 1, "germany" = -1, "france" = 1), mode = "values"))
head(similarity(w2v, analogy(~ berlin - germany + france), mode = "words"))

</code></pre>

<hr>
<h2 id='weights'>[experimental] Extract word vector weights</h2><span id='topic+weights'></span>

<h3>Description</h3>

<p>[experimental] Extract word vector weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights(x, mode = c("words", "values"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weights_+3A_x">x</code></td>
<td>
<p>a <code>textmodel_wordvector</code> object.</p>
</td></tr>
<tr><td><code id="weights_+3A_mode">mode</code></td>
<td>
<p>specify the type of resulting object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>matrix</code> of word vector weights when <code>mode = "value"</code> or of
words sorted in descending order by the weights when <code>mode = "word"</code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
