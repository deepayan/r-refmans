<!DOCTYPE html><html><head><title>Help for package BCDAG</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BCDAG}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_graphNEL'><p>Transform adjacency matrix into graphNEL object</p></a></li>
<li><a href='#causaleffect'><p>Compute causal effects between variables</p></a></li>
<li><a href='#confint.bcdagCE'><p>Credible Intervals for bcdagCE Object</p></a></li>
<li><a href='#get_causaleffect'><p>Estimate total causal effects from the MCMC output</p></a></li>
<li><a href='#get_diagnostics'><p>MCMC diagnostics</p></a></li>
<li><a href='#get_edgeprobs'><p>Compute posterior probabilities of edge inclusion from the MCMC output</p></a></li>
<li><a href='#get_MAPdag'><p>Compute the maximum a posteriori DAG model from the MCMC output</p></a></li>
<li><a href='#get_MPMdag'><p>Compute the median probability DAG model from the MCMC output</p></a></li>
<li><a href='#get_neighboringDAGs'><p>Enumerate all neighbors of a DAG</p></a></li>
<li><a href='#learn_DAG'><p>MCMC scheme for Gaussian DAG posterior inference</p></a></li>
<li><a href='#leukemia'><p>Protein levels for 68 diagnosed AML patients of subtype M2</p></a></li>
<li><a href='#plot.bcdag'><p>bcdag object plot</p></a></li>
<li><a href='#plot.bcdagCE'><p>bcdagCE object plot</p></a></li>
<li><a href='#print.bcdag'><p>bcdag object print</p></a></li>
<li><a href='#print.bcdagCE'><p>bcdagCE object print</p></a></li>
<li><a href='#rDAG'><p>Generate a Directed Acyclic Graph (DAG) randomly</p></a></li>
<li><a href='#rDAGWishart'><p>Random samples from a compatible DAG-Wishart distribution</p></a></li>
<li><a href='#summary.bcdag'><p>bcdag object summaries</p></a></li>
<li><a href='#summary.bcdagCE'><p>bcdagCE object summary</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Bayesian Structure and Causal Learning of Gaussian Directed
Graphs</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of functions for structure learning of causal networks and estimation of joint causal effects from observational Gaussian data. Main algorithm consists of a Markov chain Monte Carlo scheme for posterior inference of causal structures, parameters and causal effects between variables.
    References:
    F. Castelletti and A. Mascaro (2021) &lt;<a href="https://doi.org/10.1007%2Fs10260-021-00579-1">doi:10.1007/s10260-021-00579-1</a>&gt;,
    F. Castelletti and A. Mascaro (2022) &lt;<a href="https://doi.org/10.48550%2FarXiv.2201.12003">doi:10.48550/arXiv.2201.12003</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>graph, graphics, gRbase, grDevices, lattice, methods, mvtnorm,
Rgraphviz, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/alesmascaro/BCDAG">https://github.com/alesmascaro/BCDAG</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/alesmascaro/BCDAG/issues">https://github.com/alesmascaro/BCDAG/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-14 12:48:49 UTC; alessandromascaro</td>
</tr>
<tr>
<td>Author:</td>
<td>Federico Castelletti [aut],
  Alessandro Mascaro [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alessandro Mascaro &lt;alessandro.mascaro@upf.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-14 13:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_graphNEL'>Transform adjacency matrix into graphNEL object</h2><span id='topic+as_graphNEL'></span>

<h3>Description</h3>

<p>Function to transform an adjacency matrix into a graphNEL object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_graphNEL(DAG)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_graphNEL_+3A_dag">DAG</code></td>
<td>
<p>Adjacency matrix of a DAG</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A graphNEL object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate DAG
q &lt;- 4; w = 0.2
set.seed(123)
DAG &lt;- rDAG(q,w)
as_graphNEL(DAG)
</code></pre>

<hr>
<h2 id='causaleffect'>Compute causal effects between variables</h2><span id='topic+causaleffect'></span>

<h3>Description</h3>

<p>This function computes the total joint causal effect on variable <code>response</code> consequent to an intervention on variables <code>targets</code>
for a given a DAG structure and parameters <code>(D,L)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>causaleffect(targets, response, L, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="causaleffect_+3A_targets">targets</code></td>
<td>
<p>numerical vector with labels of target nodes</p>
</td></tr>
<tr><td><code id="causaleffect_+3A_response">response</code></td>
<td>
<p>numerical label of response variable</p>
</td></tr>
<tr><td><code id="causaleffect_+3A_l">L</code></td>
<td>
<p><code class="reqn">(q,q)</code> matrix of regression-coefficient parameters</p>
</td></tr>
<tr><td><code id="causaleffect_+3A_d">D</code></td>
<td>
<p><code class="reqn">(q,q)</code> diagonal matrix of conditional-variance parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We assume that the joint distribution of random variables <code class="reqn">X_1, \dots, X_q</code> is zero-mean Gaussian with covariance matrix Markov w.r.t. a Directed Acyclic Graph (DAG).
In addition, the allied Structural Equation Model (SEM) representation of a Gaussian DAG-model allows to express the covariance matrix as a function of the (Cholesky) parameters <code>(D,L)</code>,
collecting the conditional variances and regression coefficients of the SEM.
</p>
<p>The total causal effect on a given variable of interest (<code>response</code>) consequent to a joint intervention on a set of variables (<code>targets</code>)
is defined according to Pearl's do-calculus theory and under the Gaussian assumption can be expressed as a function of parameters <code>(D,L)</code>.
</p>


<h3>Value</h3>

<p>The joint total causal effect, represented as a vector of same length of <code>targets</code>
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>J. Pearl (2000). <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press, Cambridge.
</p>
<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>P. Nandy, M.H. Maathuis and T. Richardson (2017). Estimating the effect of joint interventions from observational data in sparse high-dimensional settings. <em>Annals of Statistics</em> 45(2), 647-674.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
# Total causal effect on node 1 of an intervention on {5,6}
causaleffect(targets = c(6,7), response = 1, L = L, D = D)
# Total causal effect on node 1 of an intervention on {5,7}
causaleffect(targets = c(5,7), response = 1, L = L, D = D)

</code></pre>

<hr>
<h2 id='confint.bcdagCE'>Credible Intervals for bcdagCE Object</h2><span id='topic+confint.bcdagCE'></span>

<h3>Description</h3>

<p>Computes credible (not confidence!) intervals for one or more target variables from objects of class <code>bcdagCE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdagCE'
confint(object, parm = "all", level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.bcdagCE_+3A_object">object</code></td>
<td>
<p>a <code>bcdagCE</code> object from which credible intervals are computed.</p>
</td></tr>
<tr><td><code id="confint.bcdagCE_+3A_parm">parm</code></td>
<td>
<p>an integer vector indexing the target variables for which credible intervals are computed. If missing, all variables are considered.</p>
</td></tr>
<tr><td><code id="confint.bcdagCE_+3A_level">level</code></td>
<td>
<p>the credible level required.</p>
</td></tr>
<tr><td><code id="confint.bcdagCE_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with columns giving lower and upper credible limits for each variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = w,
                     fast = TRUE, save.memory = FALSE, verbose = FALSE)
out_ce &lt;- get_causaleffect(out_mcmc, targets = c(4,6), response = 1)
confint(out_ce, c(4,6), 0.95)
</code></pre>

<hr>
<h2 id='get_causaleffect'>Estimate total causal effects from the MCMC output</h2><span id='topic+get_causaleffect'></span>

<h3>Description</h3>

<p>This function provides causal effect estimates from the output of <code>learn_DAG</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_causaleffect(learnDAG_output, targets, response, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_causaleffect_+3A_learndag_output">learnDAG_output</code></td>
<td>
<p>object of class <code>bcdag</code></p>
</td></tr>
<tr><td><code id="get_causaleffect_+3A_targets">targets</code></td>
<td>
<p>numerical <code class="reqn">(p,1)</code> vector with labels of target nodes</p>
</td></tr>
<tr><td><code id="get_causaleffect_+3A_response">response</code></td>
<td>
<p>numerical label of response variable</p>
</td></tr>
<tr><td><code id="get_causaleffect_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, progress bar of MCMC sampling is displayed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Output of <code>learn_dag</code> function consists of <code class="reqn">S</code> draws from the joint posterior of DAGs and DAG-parameters in a zero-mean Gaussian DAG-model;
see the documentation of <code>learn_DAG</code> for more details.
</p>
<p>The total causal effect on a given variable of interest (<code>response</code>) consequent to a joint intervention on a set of variables (<code>targets</code>)
is defined according to Pearl's do-calculus theory and under the Gaussian assumption can be expressed as a function of parameters <code>(D,L)</code>,
representing a (Cholesky) reparameterization of the covariance matrix.
</p>
<p>Specifically, to each intervened variable a causal effect coefficient is associated and the posterior distribution of the latter can be recovered from posterior draws
of the DAG parameters returned by <code>learn_DAG</code>. For each coefficient a sample of size <code class="reqn">S</code> from its posterior is available. If required, the only
Bayesian Model Average (BMA) estimate (obtained as the sample mean of the <code class="reqn">S</code> draws) can be returned by setting <code>BMA = TRUE</code>.
</p>
<p>Notice that, whenever implemented with <code>collapse = FALSE</code>, <code>learn_DAG</code> returns the marginal posterior distribution of DAGs only.
In this case, <code>get_causaleffect</code> preliminarly performs posterior inference of DAG parameters by drawing samples from the posterior of <code>(D,L)</code>.
</p>
<p>Print, summary and plot methods are available for this function. <code>print</code> returns the values of the prior hyperparameters used in the learnDAG function. <code>summary</code> returns, for each causal effect parameter, the marginal posterior mean and quantiles for different <code class="reqn">\alpha</code> levels, and posterior probabilities of negative, null and positive causal effects. <code>plot</code> provides graphical summaries (boxplot and histogram of the distribution) for the posterior of each causal effect parameter.
</p>


<h3>Value</h3>

<p>An S3 object of class <code>bcdagCE</code> containing <code class="reqn">S</code> draws from the joint posterior distribution of the <code class="reqn">p</code> causal effect coefficients, organized into an <code class="reqn">(S,p)</code> matrix, posterior means and credible intervals (under different <code class="reqn">(1-\alpha)</code> levels) for each causal effect coefficient, and marginal posterior probabilities of positive, null and negative causal effects.
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>J. Pearl (2000). <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press, Cambridge.
</p>
<p>F. Castelletti and A. Mascaro (2021) Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>P. Nandy, M.H. Maathuis and T. Richardson (2017). Estimating the effect of joint interventions from observational data in sparse high-dimensional settings. <em>Annals of Statistics</em> 45(2), 647-674.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = w,
                     fast = TRUE, save.memory = FALSE)
head(out_mcmc$Graphs)
head(out_mcmc$L)
head(out_mcmc$D)
# Compute the BMA estimate of coefficients representing
# the causal effect on node 1 of an intervention on {3,4}
out_causal = get_causaleffect(learnDAG_output = out_mcmc, targets = c(3,4), response = 1)$post_mean

# Methods
print(out_causal)
summary(out_causal)
plot(out_causal)
</code></pre>

<hr>
<h2 id='get_diagnostics'>MCMC diagnostics</h2><span id='topic+get_diagnostics'></span>

<h3>Description</h3>

<p>This function provides diagnostics of convergence for the MCMC output of <code>learn_DAG</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_diagnostics(learnDAG_output, ask = TRUE, nodes = integer(0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_diagnostics_+3A_learndag_output">learnDAG_output</code></td>
<td>
<p>object of class <code>bcdag</code></p>
</td></tr>
<tr><td><code id="get_diagnostics_+3A_ask">ask</code></td>
<td>
<p>Boolean argument passed to par() for visualization;</p>
</td></tr>
<tr><td><code id="get_diagnostics_+3A_nodes">nodes</code></td>
<td>
<p>Numerical vector indicating those nodes for which we want to compute the posterior probability of edge inclusion;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>learn_DAG</code> implements a Markov Chain Monte Carlo (MCMC) algorithm for structure learning and posterior inference of Gaussian DAGs.
Output of the algorithm is a collection of <code class="reqn">S</code> DAG structures (represented as <code class="reqn">(q,q)</code> adjacency matrices) and DAG parameters <code class="reqn">(D,L)</code>
approximately drawn from the joint posterior.
In addition, if <code>learn_DAG</code> is implemented with <code>collapse = TRUE</code>, the only approximate marginal posterior of DAGs (represented by the collection of <code class="reqn">S</code> DAG structures) is returned;
see the documentation of <code>learn_DAG</code> for more details.
</p>
<p>Diagnostics of convergence for the MCMC output are conducted by monitoring across MCMC iterations: (1) the number of edges in the DAGs;
(2) the posterior probability of edge inclusion for each possible edge <code class="reqn">u -&gt; v</code>.
With regard to (1), a traceplot of the number of edges in the DAGs visited by the MCMC chain at each step <code class="reqn">s = 1, ..., S</code> is first provided as the output of the function.
The absence of trends in the plot can provide information on a genuine convergence of the MCMC chain.
In addition, the traceplot of the average number of edges in the DAGs visited up to time <code class="reqn">s</code>, for <code class="reqn">s = 1, ..., S</code>, is also returned.
The convergence of the curve around a &quot;stable&quot; average size generally suggests good convergence of the algorithm.
With regard to (2), for each edge <code class="reqn">u -&gt; v</code>, the posterior probability at time <code class="reqn">s</code>, for <code class="reqn">s = 1, ..., S</code>, can be estimated as
as the proportion of DAGs visited by the MCMC up to time <code class="reqn">s</code> which contain the directed edge <code class="reqn">u -&gt; v</code>.
Output is organized in <code class="reqn">q</code> plots (one for each node <code class="reqn">v = 1, ..., q</code>), each summarizing the posterior probabilities of edges <code class="reqn">u -&gt; v</code>, <code class="reqn">u = 1, ..., q</code>.
If the number of nodes is larger than 30 the traceplot of a random sample of 30 nodes is returned.
</p>


<h3>Value</h3>

<p>A collection of plots summarizing the behavior of the number of edges and the posterior probabilities of edge inclusion computed from the MCMC output.
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>F. Castelletti (2020). Bayesian model selection of Gaussian Directed Acyclic Graph structures. <em>International Statistical Review</em> 88 752-775.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC for posterior inference of DAGs only (collapse = TRUE)
out_mcmc = learn_DAG(S = 5000, burn = 1000, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                                   fast = TRUE, save.memory = FALSE, collapse = TRUE)
# Produce diagnostic plots
get_diagnostics(out_mcmc)
</code></pre>

<hr>
<h2 id='get_edgeprobs'>Compute posterior probabilities of edge inclusion from the MCMC output</h2><span id='topic+get_edgeprobs'></span>

<h3>Description</h3>

<p>This function computes the posterior probability of inclusion for each edge <code class="reqn">u -&gt; v</code> given the MCMC output of <code>learn_DAG</code>;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_edgeprobs(learnDAG_output)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_edgeprobs_+3A_learndag_output">learnDAG_output</code></td>
<td>
<p>object of class <code>bcdag</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Output of <code>learn_dag</code> function consists of <code class="reqn">S</code> draws from the joint posterior of DAGs and DAG-parameters in a zero-mean Gaussian DAG-model;
see the documentation of <code>learn_DAG</code> for more details.
</p>
<p>The posterior probability of inclusion of <code class="reqn">u -&gt; v</code> is estimated as the frequency of DAGs visited by the MCMC which contain the directed edge <code class="reqn">u -&gt; v</code>.
Posterior probabilities are collected in a <code class="reqn">(q,q)</code> matrix with <code class="reqn">(u,v)</code>-element representing the estimated posterior probability
of edge <code class="reqn">u -&gt; v</code>.
</p>


<h3>Value</h3>

<p>A <code class="reqn">(q,q)</code> matrix with posterior probabilities of edge inclusion
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
# Generate observations from a Gaussian DAG-model
n = 200
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (Set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                     fast = TRUE, save.memory = FALSE)
# Compute posterior probabilities of edge inclusion
get_edgeprobs(out_mcmc)

</code></pre>

<hr>
<h2 id='get_MAPdag'>Compute the maximum a posteriori DAG model from the MCMC output</h2><span id='topic+get_MAPdag'></span>

<h3>Description</h3>

<p>This function computes the maximum a posteriori DAG model estimate (MAP) from the MCMC output of <code>learn_DAG</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_MAPdag(learnDAG_output)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_MAPdag_+3A_learndag_output">learnDAG_output</code></td>
<td>
<p>object of class <code>bcdag</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Output of <code>learn_dag</code> function consists of <code class="reqn">S</code> draws from the joint posterior of DAGs and DAG-parameters in a zero-mean Gaussian DAG-model;
see the documentation of <code>learn_DAG</code> for more details.
</p>
<p>The Maximum A Posteriori (MAP) model estimate is defined as the DAG visited by the MCMC with the highest associated posterior probability.
Each DAG posterior probability is estimated as the frequency of visits of the DAG in the MCMC chain.
The MAP estimate is represented through its <code class="reqn">(q,q)</code> adjacency matrix, with <code class="reqn">(u,v)</code>-element equal to one whenever the MAP contains <code class="reqn">u -&gt; v</code>,
zero otherwise.
</p>


<h3>Value</h3>

<p>The <code class="reqn">(q,q)</code> adjacency matrix of the maximum a posteriori DAG model
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>G. Garcia-Donato and M.A. Martinez-Beneito (2013). On sampling strategies in Bayesian variable selection problems with large model spaces. <em>Journal of the American Statistical Association</em> 108 340-352.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
# Generate observations from a Gaussian DAG-model
n = 200
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (Set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                     fast = TRUE, save.memory = FALSE)
# Produce the MAP DAG estimate
get_MAPdag(out_mcmc)

</code></pre>

<hr>
<h2 id='get_MPMdag'>Compute the median probability DAG model from the MCMC output</h2><span id='topic+get_MPMdag'></span>

<h3>Description</h3>

<p>This function computes the Median Probability DAG Model estimate (MPM) from the MCMC output of <code>learn_DAG</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_MPMdag(learnDAG_output)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_MPMdag_+3A_learndag_output">learnDAG_output</code></td>
<td>
<p>object of class <code>bcdag</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Output of <code>learn_dag</code> function consists of <code class="reqn">S</code> draws from the joint posterior of DAGs and DAG-parameters in a zero-mean Gaussian DAG-model;
see the documentation of <code>learn_DAG</code> for more details.
</p>
<p>The Median Probability DAG Model estimate (MPM) is obtained by including all edges whose posterior probability exceeds 0.5.
The posterior probability of inclusion of <code class="reqn">u -&gt; v</code> is estimated as the frequency of DAGs visited by the MCMC which contain the directed edge <code class="reqn">u -&gt; v</code>;
see also function <code>get_edgeprobs</code> and the corresponding documentation.
</p>


<h3>Value</h3>

<p>The <code class="reqn">(q,q)</code> adjacency matrix of the median probability DAG model
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication
</p>
<p>M.M. Barbieri and J.O. Berger (2004). Optimal predictive model selection. <em>The Annals of Statistics</em> 32 870-897
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
# Generate observations from a Gaussian DAG-model
n = 200
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (Set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                     fast = TRUE, save.memory = FALSE)
# Produce the MPM DAG estimate
get_MPMdag(out_mcmc)

</code></pre>

<hr>
<h2 id='get_neighboringDAGs'>Enumerate all neighbors of a DAG</h2><span id='topic+get_neighboringDAGs'></span>

<h3>Description</h3>

<p>This functions takes any DAG with <code class="reqn">q</code> nodes as input and returns all the neighboring DAGs, i.e. all those DAGs that
can be reached by the addition, removal or reversal of an edge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_neighboringDAGs(DAG)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_neighboringDAGs_+3A_dag">DAG</code></td>
<td>
<p>Adjacency matrix of a DAG</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code class="reqn">(q,q,K)</code> array containing all neighboring DAGs, with <code class="reqn">K</code> being the total number of neighbors
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG
q &lt;- 4; w &lt;- 0.2
set.seed(123)
DAG &lt;- rDAG(q,w)
# Get neighbors
neighbors &lt;- get_neighboringDAGs(DAG)
neighbors
</code></pre>

<hr>
<h2 id='learn_DAG'>MCMC scheme for Gaussian DAG posterior inference</h2><span id='topic+learn_DAG'></span>

<h3>Description</h3>

<p>This function implements a Markov Chain Monte Carlo (MCMC) algorithm for structure learning of Gaussian
DAGs and posterior inference of DAG model parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_DAG(
  S,
  burn,
  data,
  a,
  U,
  w,
  fast = FALSE,
  save.memory = FALSE,
  collapse = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_DAG_+3A_s">S</code></td>
<td>
<p>integer final number of MCMC draws from the posterior of DAGs and parameters</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_burn">burn</code></td>
<td>
<p>integer initial number of burn-in iterations, needed by the MCMC chain to reach its stationary distribution and not included in the final output</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_data">data</code></td>
<td>
<p><code class="reqn">(n,q)</code> data matrix</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_a">a</code></td>
<td>
<p>common shape hyperparameter of the compatible DAG-Wishart prior, <code class="reqn">a &gt; q - 1</code></p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_u">U</code></td>
<td>
<p>position hyperparameter of the compatible DAG-Wishart prior, a <code class="reqn">(q, q)</code> s.p.d. matrix</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_w">w</code></td>
<td>
<p>edge inclusion probability hyperparameter of the DAG prior in <code class="reqn">[0,1]</code></p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_fast">fast</code></td>
<td>
<p>boolean, if <code>TRUE</code> an approximate proposal for the MCMC moves is implemented</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_save.memory">save.memory</code></td>
<td>
<p>boolean, if <code>TRUE</code> MCMC draws are stored as strings, instead of arrays</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_collapse">collapse</code></td>
<td>
<p>boolean, if <code>TRUE</code> only structure learning of DAGs is performed</p>
</td></tr>
<tr><td><code id="learn_DAG_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress bars are displayed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consider a collection of random variables <code class="reqn">X_1, \dots, X_q</code> whose distribution is zero-mean multivariate Gaussian with covariance matrix Markov w.r.t. a Directed Acyclic Graph (DAG).
Assuming the underlying DAG is unknown (model uncertainty), a Bayesian method for posterior inference on the joint space of DAG structures and parameters can be implemented.
The proposed method assigns a prior on each DAG structure through independent Bernoulli distributions, <code class="reqn">Ber(w)</code>, on the 0-1 elements of the DAG adjacency matrix.
Conditionally on a given DAG, a prior on DAG parameters <code class="reqn">(D,L)</code> (representing a Cholesky-type reparameterization of the covariance matrix) is assigned through a compatible DAG-Wishart prior;
see also function <code>rDAGWishart</code> for more details.
</p>
<p>Posterior inference on the joint space of DAGs and DAG parameters is carried out through a Partial Analytic Structure (PAS) algorithm.
Two steps are iteratively performed for <code class="reqn">s = 1, 2, ...</code> : (1) update of the DAG through a Metropolis Hastings (MH) scheme;
(2) sampling from the posterior distribution of the (updated DAG) parameters.
In step (1) the update of the (current) DAG is performed by drawing a new (direct successor) DAG from a suitable proposal distribution. The proposed DAG is obtained by applying a local move (insertion, deletion or edge reversal)
to the current DAG and is accepted with probability given by the MH acceptance rate.
The latter requires to evaluate the proposal distribution at both the current and proposed DAGs, which in turn involves the enumeration of
all DAGs that can be obtained from local moves from respectively the current and proposed DAG.
Because the ratio of the two proposals is approximately equal to one, and the approximation becomes as precise as <code class="reqn">q</code> grows, a faster strategy implementing such an approximation is provided with
<code>fast = TRUE</code>. The latter choice is especially recommended for moderate-to-large number of nodes <code class="reqn">q</code>.
</p>
<p>Output of the algorithm is a collection of <code class="reqn">S</code> DAG structures (represented as <code class="reqn">(q,q)</code> adjacency matrices) and DAG parameters <code class="reqn">(D,L)</code> approximately drawn from the joint posterior.
The various outputs are organized in <code class="reqn">(q,q,S)</code> arrays; see also the example below.
If the target is DAG learning only, a collapsed sampler implementing the only step (1) of the MCMC scheme can be obtained
by setting <code>collapse = TRUE</code>. In this case, the algorithm outputs a collection of <code class="reqn">S</code> DAG structures only.
See also functions <code>get_edgeprobs</code>, <code>get_MAPdag</code>, <code>get_MPMdag</code> for posterior summaries of the MCMC output.
</p>
<p>Print, summary and plot methods are available for this function. <code>print</code> provides information about the MCMC output and the values of the input prior hyperparameters. <code>summary</code> returns, besides the previous information, a <code class="reqn">(q,q)</code> matrix collecting the marginal posterior probabilities of edge inclusion. <code>plot</code> returns the estimated Median Probability DAG Model (MPM), a <code class="reqn">(q,q)</code> heat map with estimated marginal posterior probabilities of edge inclusion, and a barplot summarizing the distribution of the size of DAGs visited by the MCMC.
</p>


<h3>Value</h3>

<p>An S3 object of class <code>bcdag</code> containing <code class="reqn">S</code> draws from the posterior of DAGs and (if <code>collapse = FALSE</code>) of DAG parameters <code class="reqn">D</code> and <code class="reqn">L</code>. If <code>save.memory = FALSE</code>, these are stored in three arrays of dimension <code class="reqn">(q,q,S)</code>. Otherwise, they are stored as strings.
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>F. Castelletti and A. Mascaro (2022). BCDAG: An R package for Bayesian structural and Causal learning of Gaussian DAGs. <em>arXiv pre-print</em>, url: https://arxiv.org/abs/2201.12003
</p>
<p>F. Castelletti (2020). Bayesian model selection of Gaussian Directed Acyclic Graph structures. <em>International Statistical Review</em> 88 752-775.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
# Generate observations from a Gaussian DAG-model
n = 200
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)

## Set S = 5000 and burn = 1000 for better results

# [1] Run the MCMC for posterior inference of DAGs and parameters (collapse = FALSE)
out_mcmc = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                     fast = FALSE, save.memory = FALSE, collapse = FALSE)
# [2] Run the MCMC for posterior inference of DAGs only (collapse = TRUE)
out_mcmc_collapse = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                              fast = FALSE, save.memory = FALSE, collapse = TRUE)
# [3] Run the MCMC for posterior inference of DAGs only with approximate proposal
# distribution (fast = TRUE)
# out_mcmc_collapse_fast = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
#                                    fast = FALSE, save.memory = FALSE, collapse = TRUE)
# Compute posterior probabilities of edge inclusion and Median Probability DAG Model
# from the MCMC outputs [2] and [3]
get_edgeprobs(out_mcmc_collapse)
# get_edgeprobs(out_mcmc_collapse_fast)
get_MPMdag(out_mcmc_collapse)
# get_MPMdag(out_mcmc_collapse_fast)

# Methods
print(out_mcmc)
summary(out_mcmc)
plot(out_mcmc)

</code></pre>

<hr>
<h2 id='leukemia'>Protein levels for 68 diagnosed AML patients of subtype M2</h2><span id='topic+leukemia'></span>

<h3>Description</h3>

<p>A dataset containing the protein expression levels of 18 proteins for 68 AML patients of subtype M2 (according to French-American-British (FAB) classification system).
The 18 proteins selected are known to be involved in apoptosis and cell cycle regulation according to the KEGG database (Kanehisa et al. 2012).
This is a subset of the dataset presented in Kornblau et al. (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leukemia
</code></pre>


<h3>Format</h3>

<p>A data frame with 68 rows and 18 variables:
</p>

<dl>
<dt>AKT</dt><dd><p>AKT protein, expression level</p>
</dd>
<dt>AKT.p308</dt><dd><p>AKT.p308 protein, expression level</p>
</dd>
<dt>AKT.p473</dt><dd><p>AKT.p473 protein, expression level</p>
</dd>
<dt>BAD</dt><dd><p>BAD protein, expression level</p>
</dd>
<dt>BAD.p112</dt><dd><p>BAD.p112 protein, expression level</p>
</dd>
<dt>BAD.p136</dt><dd><p>BAD.p136 protein, expression level</p>
</dd>
<dt>BAD.p155</dt><dd><p>BAD.p155 protein, expression level</p>
</dd>
<dt>BAX</dt><dd><p>BAX protein, expression level</p>
</dd>
<dt>BCL2</dt><dd><p>BCL2 protein, expression level</p>
</dd>
<dt>BCLXL</dt><dd><p>BCLXL protein, expression level</p>
</dd>
<dt>CCND1</dt><dd><p>CCND1 protein, expression level</p>
</dd>
<dt>GSK3</dt><dd><p>GSK3 protein, expression level</p>
</dd>
<dt>GSK3.p</dt><dd><p>GSK3.p protein, expression level</p>
</dd>
<dt>MYC</dt><dd><p>MYC protein, expression level</p>
</dd>
<dt>PTEN</dt><dd><p>PTEN protein, expression level</p>
</dd>
<dt>PTEN.p</dt><dd><p>PTEN.p protein, expression level</p>
</dd>
<dt>TP53</dt><dd><p>TP53 protein, expression level</p>
</dd>
<dt>XIAP</dt><dd><p>XIAP protein, expression level</p>
</dd>
</dl>
<p>...

</p>


<h3>Source</h3>

<p><a href="http://bioinformatics.mdanderson.org/Supplements/Kornblau-AML-RPPA/aml-rppa.xls">http://bioinformatics.mdanderson.org/Supplements/Kornblau-AML-RPPA/aml-rppa.xls</a>
</p>


<h3>References</h3>

<p>Kornblau, S. M., Tibes, R., Qiu, Y. H., Chen, W., Kantarjian, H. M., Andreeff, M., ... &amp; Mills, G. B. (2009). Functional proteomic profiling of AML predicts response and survival. Blood, The Journal of the American Society of Hematology, 113(1), 154-164.
</p>
<p>Kanehisa, M., Goto, S., Sato, Y., Furumichi, M., &amp; Tanabe, M. (2012). KEGG for integration and interpretation of large-scale molecular data sets. Nucleic acids research, 40(D1), D109-D114.
</p>

<hr>
<h2 id='plot.bcdag'>bcdag object plot</h2><span id='topic+plot.bcdag'></span>

<h3>Description</h3>

<p>This method returns summary plots of the output of <code>learn_DAG()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdag'
plot(x, ..., ask = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bcdag_+3A_x">x</code></td>
<td>
<p>a <code>bcdag</code> object for which a plot is desired</p>
</td></tr>
<tr><td><code id="plot.bcdag_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
<tr><td><code id="plot.bcdag_+3A_ask">ask</code></td>
<td>
<p>Boolean argument passed to par() for visualization;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of the Median Probability DAG, a heatmap of the probabilities of edge inclusion and an histogram of the sizes of graphs visited by learn_DAG().
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
q &lt;- 4
DAG &lt;- matrix(c(0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0), nrow = q)

L &lt;- DAG
L[L != 0] &lt;- runif(q, 0.2, 1)
diag(L) &lt;- c(1,1,1,1)
D &lt;- diag(1, q)
Sigma &lt;- t(solve(L))%*%D%*%solve(L)

a &lt;- 6
g &lt;- 1/1000
U &lt;- g*diag(1,q)
w = 0.2

set.seed(1)
X &lt;- mvtnorm::rmvnorm(n, sigma = Sigma)

out &lt;- learn_DAG(1000, 0, X, a, U, w, fast = TRUE, collapse = TRUE, save.memory = FALSE)
plot(out)
</code></pre>

<hr>
<h2 id='plot.bcdagCE'>bcdagCE object plot</h2><span id='topic+plot.bcdagCE'></span>

<h3>Description</h3>

<p>This method returns summary plots of the output of <code>get_causaleffect()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdagCE'
plot(x, ..., which_ce = integer(0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bcdagCE_+3A_x">x</code></td>
<td>
<p>a <code>bcdagCE</code> object for which a plot is desired</p>
</td></tr>
<tr><td><code id="plot.bcdagCE_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
<tr><td><code id="plot.bcdagCE_+3A_which_ce">which_ce</code></td>
<td>
<p>specifies the list of nodes for which you intend to generate a boxplot and a histogram</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boxplot and histogram of the posterior distribution of the causal effects computed using get_causaleffect().
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = w,
                     fast = TRUE, save.memory = FALSE, verbose = FALSE)
out_ce &lt;- get_causaleffect(out_mcmc, targets = c(4,6), response = 1)
plot(out_ce)
</code></pre>

<hr>
<h2 id='print.bcdag'>bcdag object print</h2><span id='topic+print.bcdag'></span>

<h3>Description</h3>

<p>This method returns a summary of the input given to <code>learn_DAG()</code> to produce the <code>bcdag</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdag'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.bcdag_+3A_x">x</code></td>
<td>
<p>a <code>bcdag</code> object for which a summary is desired</p>
</td></tr>
<tr><td><code id="print.bcdag_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A printed message listing the inputs given to learn_DAG.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
q &lt;- 4
DAG &lt;- matrix(c(0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0), nrow = q)

L &lt;- DAG
L[L != 0] &lt;- runif(q, 0.2, 1)
diag(L) &lt;- c(1,1,1,1)
D &lt;- diag(1, q)
Sigma &lt;- t(solve(L))%*%D%*%solve(L)

a &lt;- 6
g &lt;- 1/1000
U &lt;- g*diag(1,q)
w = 0.2

set.seed(1)
X &lt;- mvtnorm::rmvnorm(n, sigma = Sigma)

out &lt;- learn_DAG(1000, 0, X, a, U, w, fast = TRUE, collapse = TRUE, save.memory = FALSE)
print(out)
</code></pre>

<hr>
<h2 id='print.bcdagCE'>bcdagCE object print</h2><span id='topic+print.bcdagCE'></span>

<h3>Description</h3>

<p>This method returns a summary of the inputs given to <code>learn_DAG()</code> and <code>get_causaleffect()</code> to obtain the <code>bcdagCE</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdagCE'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.bcdagCE_+3A_x">x</code></td>
<td>
<p>a <code>bcdagCE</code> object for which a summary is desired</p>
</td></tr>
<tr><td><code id="print.bcdagCE_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A printed message listing the inputs given to learn_DAG and get_causaleffect.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = w,
                     fast = TRUE, save.memory = FALSE, verbose = FALSE)
out_ce &lt;- get_causaleffect(out_mcmc, targets = c(4,6), response = 1)
print(out_ce)
</code></pre>

<hr>
<h2 id='rDAG'>Generate a Directed Acyclic Graph (DAG) randomly</h2><span id='topic+rDAG'></span>

<h3>Description</h3>

<p>This function randomly generates a Directed Acyclic Graph (DAG) with <code>q</code> nodes and probability of edge inclusion <code>w</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rDAG(q, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rDAG_+3A_q">q</code></td>
<td>
<p>number of nodes</p>
</td></tr>
<tr><td><code id="rDAG_+3A_w">w</code></td>
<td>
<p>probability of edge inclusion in <code class="reqn">[0,1]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">0-1</code> adjacency matrix of the DAG is generated by drawing each element in the lower triangular part in <code class="reqn">{0,1}</code> with probability <code class="reqn">{1-w, w}</code>.
Accordingly, the DAG has lower-triangular adjacency matrix and nodes are numerically labeled according to a topological ordering implying <code class="reqn">u &gt; v</code> whenever <code class="reqn">u -&gt; v</code>.
</p>


<h3>Value</h3>

<p>DAG <code class="reqn">(q,q)</code> adjacency matrix of the generated DAG
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG on q = 8 nodes with probability of edge inclusion w = 0.2
q = 8
w = 0.2
set.seed(123)
rDAG(q = q, w = w)

</code></pre>

<hr>
<h2 id='rDAGWishart'>Random samples from a compatible DAG-Wishart distribution</h2><span id='topic+rDAGWishart'></span>

<h3>Description</h3>

<p>This function implements a direct sampling from a compatible DAG-Wishart distribution with parameters <code>a</code> and <code>U</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rDAGWishart(n, DAG, a, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rDAGWishart_+3A_n">n</code></td>
<td>
<p>number of samples</p>
</td></tr>
<tr><td><code id="rDAGWishart_+3A_dag">DAG</code></td>
<td>
<p><code class="reqn">(q, q)</code> adjacency matrix of the DAG</p>
</td></tr>
<tr><td><code id="rDAGWishart_+3A_a">a</code></td>
<td>
<p>common shape hyperparameter of the compatible DAG-Wishart, <code class="reqn">a &gt; q - 1</code></p>
</td></tr>
<tr><td><code id="rDAGWishart_+3A_u">U</code></td>
<td>
<p>position hyperparameter of the compatible DAG-Wishart, a <code class="reqn">(q, q)</code> s.p.d. matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume the joint distribution of random variables <code class="reqn">X_1, \dots, X_q</code> is zero-mean Gaussian with covariance matrix Markov w.r.t. a Directed Acyclic Graph (DAG).
The allied Structural Equation Model (SEM) representation of a Gaussian DAG-model allows to express the covariance matrix as a function of the (Cholesky) parameters <code class="reqn">(D,L)</code>,
collecting the regression coefficients and conditional variances of the SEM.
</p>
<p>The DAG-Wishart distribution (Cao et. al, 2019) with shape hyperparameter <code class="reqn">a = (a_1, ..., a_q)</code> and position hyperparameter <code class="reqn">U</code> (a s.p.d. <code class="reqn">(q,q)</code> matrix) provides a conjugate prior for parameters <code class="reqn">(D,L)</code>.
In addition, to guarantee compatibility among Markov equivalent DAGs (same marginal likelihood), the default choice (here implemented) <code class="reqn">a_j = a + |pa(j)| - q + 1</code> <code class="reqn">(a &gt; q - 1)</code>, with <code class="reqn">|pa(j)|</code> the number of parents of node <code class="reqn">j</code> in the DAG,
was introduced by Peluso and Consonni (2020).
</p>


<h3>Value</h3>

<p>A list of two elements: a <code class="reqn">(q,q,n)</code> array collecting <code class="reqn">n</code> sampled matrices <code class="reqn">L</code> and a <code class="reqn">(q,q,n)</code> array collecting <code class="reqn">n</code> sampled matrices <code class="reqn">D</code>
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>X. Cao, K. Khare and M. Ghosh (2019). Posterior graph selection and estimation consistency for high-dimensional Bayesian DAG models. <em>The Annals of Statistics</em> 47 319-348.
</p>
<p>S. Peluso and G. Consonni (2020). Compatible priors for model selection of high-dimensional Gaussian DAGs. <em>Electronic Journal of Statistics</em> 14(2) 4110 - 4132.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Randomly generate a DAG on q = 8 nodes with probability of edge inclusion w = 0.2
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
# Draw from a compatible DAG-Wishart distribution with parameters a = q and U = diag(1,q)
outDL = rDAGWishart(n = 5, DAG = DAG, a = q, U = diag(1, q))
outDL
</code></pre>

<hr>
<h2 id='summary.bcdag'>bcdag object summaries</h2><span id='topic+summary.bcdag'></span>

<h3>Description</h3>

<p>This method produces summaries of the input and output of the function <code>learn_DAG()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdag'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bcdag_+3A_object">object</code></td>
<td>
<p>a <code>bcdag</code> object for which a summary is desired</p>
</td></tr>
<tr><td><code id="summary.bcdag_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A printed message listing the inputs given to learn_DAG and the estimated posterior probabilities of edge inclusion.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
q &lt;- 4
DAG &lt;- matrix(c(0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0), nrow = q)

L &lt;- DAG
L[L != 0] &lt;- runif(q, 0.2, 1)
diag(L) &lt;- c(1,1,1,1)
D &lt;- diag(1, q)
Sigma &lt;- t(solve(L))%*%D%*%solve(L)

a &lt;- 6
g &lt;- 1/1000
U &lt;- g*diag(1,q)
w = 0.2

set.seed(1)
X &lt;- mvtnorm::rmvnorm(n, sigma = Sigma)

out &lt;- learn_DAG(1000, 0, X, a, U, w, fast = TRUE, collapse = TRUE, save.memory = FALSE)
summary(out)
</code></pre>

<hr>
<h2 id='summary.bcdagCE'>bcdagCE object summary</h2><span id='topic+summary.bcdagCE'></span>

<h3>Description</h3>

<p>This method produces summaries of the input and output of the function <code>get_causaleffect()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bcdagCE'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bcdagCE_+3A_object">object</code></td>
<td>
<p>a <code>bcdagCE</code> object for which a summary is desired</p>
</td></tr>
<tr><td><code id="summary.bcdagCE_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the summary produced</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A printed message listing the inputs given to learn_DAG() and get_causaleffect() and summary statistics of the posterior distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
n = 200
# Generate observations from a Gaussian DAG-model
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)
# Run the MCMC (set S = 5000 and burn = 1000 for better results)
out_mcmc = learn_DAG(S = 500, burn = 100, a = q, U = diag(1,q)/n, data = X, w = w,
                     fast = TRUE, save.memory = FALSE, verbose = FALSE)
out_ce &lt;- get_causaleffect(out_mcmc, targets = c(4,6), response = 1)
# summary(out_ce)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
