<!DOCTYPE html><html><head><title>Help for package lightgbm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lightgbm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agaricus.test'><p>Test part from Mushroom Data Set</p></a></li>
<li><a href='#agaricus.train'><p>Training part from Mushroom Data Set</p></a></li>
<li><a href='#bank'><p>Bank Marketing Data Set</p></a></li>
<li><a href='#dim.lgb.Dataset'><p>Dimensions of an <code>lgb.Dataset</code></p></a></li>
<li><a href='#dimnames.lgb.Dataset'><p>Handling of column names of <code>lgb.Dataset</code></p></a></li>
<li><a href='#get_field'><p>Get one attribute of a <code>lgb.Dataset</code></p></a></li>
<li><a href='#getLGBMThreads'><p>Get default number of threads used by LightGBM</p></a></li>
<li><a href='#lgb_shared_dataset_params'><p>Shared Dataset parameter docs</p></a></li>
<li><a href='#lgb_shared_params'><p>Shared parameter docs</p></a></li>
<li><a href='#lgb.configure_fast_predict'><p>Configure Fast Single-Row Predictions</p></a></li>
<li><a href='#lgb.convert_with_rules'><p>Data preparator for LightGBM datasets with rules (integer)</p></a></li>
<li><a href='#lgb.cv'><p>Main CV logic for LightGBM</p></a></li>
<li><a href='#lgb.Dataset'><p>Construct <code>lgb.Dataset</code> object</p></a></li>
<li><a href='#lgb.Dataset.construct'><p>Construct Dataset explicitly</p></a></li>
<li><a href='#lgb.Dataset.create.valid'><p>Construct validation data</p></a></li>
<li><a href='#lgb.Dataset.save'><p>Save <code>lgb.Dataset</code> to a binary file</p></a></li>
<li><a href='#lgb.Dataset.set.categorical'><p>Set categorical feature of <code>lgb.Dataset</code></p></a></li>
<li><a href='#lgb.Dataset.set.reference'><p>Set reference of <code>lgb.Dataset</code></p></a></li>
<li><a href='#lgb.drop_serialized'><p>Drop serialized raw bytes in a LightGBM model object</p></a></li>
<li><a href='#lgb.dump'><p>Dump LightGBM model to json</p></a></li>
<li><a href='#lgb.get.eval.result'><p>Get record evaluation result from booster</p></a></li>
<li><a href='#lgb.importance'><p>Compute feature importance in a model</p></a></li>
<li><a href='#lgb.interprete'><p>Compute feature contribution of prediction</p></a></li>
<li><a href='#lgb.load'><p>Load LightGBM model</p></a></li>
<li><a href='#lgb.make_serializable'><p>Make a LightGBM object serializable by keeping raw bytes</p></a></li>
<li><a href='#lgb.model.dt.tree'><p>Parse a LightGBM model json dump</p></a></li>
<li><a href='#lgb.plot.importance'><p>Plot feature importance as a bar graph</p></a></li>
<li><a href='#lgb.plot.interpretation'><p>Plot feature contribution as a bar graph</p></a></li>
<li><a href='#lgb.restore_handle'><p>Restore the C++ component of a de-serialized LightGBM model</p></a></li>
<li><a href='#lgb.save'><p>Save LightGBM model</p></a></li>
<li><a href='#lgb.slice.Dataset'><p>Slice a dataset</p></a></li>
<li><a href='#lgb.train'><p>Main training logic for LightGBM</p></a></li>
<li><a href='#lightgbm'><p>Train a LightGBM model</p></a></li>
<li><a href='#predict.lgb.Booster'><p>Predict method for LightGBM model</p></a></li>
<li><a href='#print.lgb.Booster'><p>Print method for LightGBM model</p></a></li>
<li><a href='#set_field'><p>Set one attribute of a <code>lgb.Dataset</code> object</p></a></li>
<li><a href='#setLGBMThreads'><p>Set maximum number of threads used by LightGBM</p></a></li>
<li><a href='#summary.lgb.Booster'><p>Summary method for LightGBM model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Light Gradient Boosting Machine</td>
</tr>
<tr>
<td>Version:</td>
<td>4.4.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Tree based algorithms can be improved by introducing boosting frameworks.
    'LightGBM' is one such framework, based on Ke, Guolin et al. (2017) <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision">https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision</a>.
    This package offers an R interface to work with it.
    It is designed to be distributed and efficient with the following advantages:
        1. Faster training speed and higher efficiency.
        2. Lower memory usage.
        3. Better accuracy.
        4. Parallel learning supported.
        5. Capable of handling large-scale data.
    In recognition of these advantages, 'LightGBM' has been widely-used in many winning solutions of machine learning competitions.
    Comparison experiments on public datasets suggest that 'LightGBM' can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. In addition, parallel experiments suggest that in certain circumstances, 'LightGBM' can achieve a linear speed-up in training time by using multiple machines.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/Microsoft/LightGBM">https://github.com/Microsoft/LightGBM</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Microsoft/LightGBM/issues">https://github.com/Microsoft/LightGBM/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, markdown, RhpcBLASctl, testthat</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>R6 (&ge; 2.0), data.table (&ge; 1.9.6), graphics, jsonlite (&ge;
1.0), Matrix (&ge; 1.1-0), methods, parallel, utils</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-14 21:41:37 UTC; vsts_azpcontainer</td>
</tr>
<tr>
<td>Author:</td>
<td>Yu Shi [aut],
  Guolin Ke [aut],
  Damien Soukhavong [aut],
  James Lamb [aut, cre],
  Qi Meng [aut],
  Thomas Finley [aut],
  Taifeng Wang [aut],
  Wei Chen [aut],
  Weidong Ma [aut],
  Qiwei Ye [aut],
  Tie-Yan Liu [aut],
  Nikita Titov [aut],
  Yachen Yan [ctb],
  Microsoft Corporation [cph],
  Dropbox, Inc. [cph],
  Alberto Ferreira [ctb],
  Daniel Lemire [ctb],
  Victor Zverovich [cph],
  IBM Corporation [ctb],
  David Cortes [aut],
  Michael Mayer [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Lamb &lt;jaylamb20@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-15 05:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='agaricus.test'>Test part from Mushroom Data Set</h2><span id='topic+agaricus.test'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
This data set includes the following fields:
</p>

<ul>
<li><p><code>label</code>: the label for each record
</p>
</li>
<li><p><code>data</code>: a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.test)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 1611
rows and 126 variables
</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='agaricus.train'>Training part from Mushroom Data Set</h2><span id='topic+agaricus.train'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
This data set includes the following fields:
</p>

<ul>
<li><p><code>label</code>: the label for each record
</p>
</li>
<li><p><code>data</code>: a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.train)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 6513
rows and 127 variables
</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='bank'>Bank Marketing Data Set</h2><span id='topic+bank'></span>

<h3>Description</h3>

<p>This data set is originally from the Bank Marketing data set,
UCI Machine Learning Repository.
</p>
<p>It contains only the following: bank.csv with 10
randomly selected from 3 (older version of this dataset with less inputs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bank)
</code></pre>


<h3>Format</h3>

<p>A data.table with 4521 rows and 17 variables
</p>


<h3>References</h3>

<p>http://archive.ics.uci.edu/ml/datasets/Bank+Marketing
</p>
<p>S. Moro, P. Cortez and P. Rita. (2014)
A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems
</p>

<hr>
<h2 id='dim.lgb.Dataset'>Dimensions of an <code>lgb.Dataset</code></h2><span id='topic+dim.lgb.Dataset'></span>

<h3>Description</h3>

<p>Returns a vector of numbers of rows and of columns in an <code>lgb.Dataset</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lgb.Dataset'
dim(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dim.lgb.Dataset_+3A_x">x</code></td>
<td>
<p>Object of class <code>lgb.Dataset</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: since <code>nrow</code> and <code>ncol</code> internally use <code>dim</code>, they can also
be directly used with an <code>lgb.Dataset</code> object.
</p>


<h3>Value</h3>

<p>a vector of numbers of rows and of columns
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

stopifnot(nrow(dtrain) == nrow(train$data))
stopifnot(ncol(dtrain) == ncol(train$data))
stopifnot(all(dim(dtrain) == dim(train$data)))

</code></pre>

<hr>
<h2 id='dimnames.lgb.Dataset'>Handling of column names of <code>lgb.Dataset</code></h2><span id='topic+dimnames.lgb.Dataset'></span><span id='topic+dimnames+3C-.lgb.Dataset'></span>

<h3>Description</h3>

<p>Only column names are supported for <code>lgb.Dataset</code>, thus setting of
row names would have no effect and returned row names would be NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lgb.Dataset'
dimnames(x)

## S3 replacement method for class 'lgb.Dataset'
dimnames(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimnames.lgb.Dataset_+3A_x">x</code></td>
<td>
<p>object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="dimnames.lgb.Dataset_+3A_value">value</code></td>
<td>
<p>a list of two elements: the first one is ignored
and the second one is column names</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generic <code>dimnames</code> methods are used by <code>colnames</code>.
Since row names are irrelevant, it is recommended to use <code>colnames</code> directly.
</p>


<h3>Value</h3>

<p>A list with the dimension names of the dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
lgb.Dataset.construct(dtrain)
dimnames(dtrain)
colnames(dtrain)
colnames(dtrain) &lt;- make.names(seq_len(ncol(train$data)))
print(dtrain, verbose = TRUE)

</code></pre>

<hr>
<h2 id='get_field'>Get one attribute of a <code>lgb.Dataset</code></h2><span id='topic+get_field'></span><span id='topic+get_field.lgb.Dataset'></span>

<h3>Description</h3>

<p>Get one attribute of a <code>lgb.Dataset</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_field(dataset, field_name)

## S3 method for class 'lgb.Dataset'
get_field(dataset, field_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_field_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="get_field_+3A_field_name">field_name</code></td>
<td>
<p>String with the name of the attribute to get. One of the following.
</p>

<ul>
<li> <p><code>label</code>: label lightgbm learns from ;
</p>
</li>
<li> <p><code>weight</code>: to do a weight rescale ;
</p>
</li>
<li><p><code>group</code>: used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results to be ranked.
For example, if you have a 100-document dataset with <code>group = c(10, 20, 40, 10, 10, 10)</code>,
that means that you have 6 groups, where the first 10 records are in the first group,
records 11-30 are in the second group, etc.
</p>
</li>
<li> <p><code>init_score</code>: initial score is the base prediction lightgbm will boost from.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>requested attribute
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
lgb.Dataset.construct(dtrain)

labels &lt;- lightgbm::get_field(dtrain, "label")
lightgbm::set_field(dtrain, "label", 1 - labels)

labels2 &lt;- lightgbm::get_field(dtrain, "label")
stopifnot(all(labels2 == 1 - labels))

</code></pre>

<hr>
<h2 id='getLGBMThreads'>Get default number of threads used by LightGBM</h2><span id='topic+getLGBMThreads'></span><span id='topic+getLGBMthreads'></span>

<h3>Description</h3>

<p>LightGBM attempts to speed up many operations by using multi-threading.
The number of threads used in those operations can be controlled via the
<code>num_threads</code> parameter passed through <code>params</code> to functions like
<a href="#topic+lgb.train">lgb.train</a> and <a href="#topic+lgb.Dataset">lgb.Dataset</a>. However, some operations (like materializing
a model from a text file) are done via code paths that don't explicitly accept thread-control
configuration.
</p>
<p>Use this function to see the default number of threads LightGBM will use for such operations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getLGBMthreads()
</code></pre>


<h3>Value</h3>

<p>number of threads as an integer. <code>-1</code> means that in situations where parameter <code>num_threads</code> is
not explicitly supplied, LightGBM will choose a number of threads to use automatically.
</p>


<h3>See Also</h3>

<p><a href="#topic+setLGBMthreads">setLGBMthreads</a>
</p>

<hr>
<h2 id='lgb_shared_dataset_params'>Shared Dataset parameter docs</h2><span id='topic+lgb_shared_dataset_params'></span>

<h3>Description</h3>

<p>Parameter docs for fields used in <code>lgb.Dataset</code> construction
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb_shared_dataset_params_+3A_label">label</code></td>
<td>
<p>vector of labels to use as the target variable</p>
</td></tr>
<tr><td><code id="lgb_shared_dataset_params_+3A_weight">weight</code></td>
<td>
<p>numeric vector of sample weights</p>
</td></tr>
<tr><td><code id="lgb_shared_dataset_params_+3A_init_score">init_score</code></td>
<td>
<p>initial score is the base prediction lightgbm will boost from</p>
</td></tr>
<tr><td><code id="lgb_shared_dataset_params_+3A_group">group</code></td>
<td>
<p>used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results
to be ranked. For example, if you have a 100-document dataset with
<code>group = c(10, 20, 40, 10, 10, 10)</code>, that means that you have 6 groups,
where the first 10 records are in the first group, records 11-30 are in the
second group, etc.</p>
</td></tr>
</table>

<hr>
<h2 id='lgb_shared_params'>Shared parameter docs</h2><span id='topic+lgb_shared_params'></span>

<h3>Description</h3>

<p>Parameter docs shared by <code>lgb.train</code>, <code>lgb.cv</code>, and <code>lightgbm</code>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb_shared_params_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_data">data</code></td>
<td>
<p>a <code>lgb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+lgb.cv">lgb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_eval">eval</code></td>
<td>
<p>evaluation function(s). This can be a character vector, function, or list with a mixture of
strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
If you provide a character vector to this argument, it should contain strings with valid
evaluation metrics.
See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric">
The &quot;metric&quot; section of the documentation</a>
for a list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effective when verbose &gt; 0 and <code>valids</code> has been provided</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_init_model">init_model</code></td>
<td>
<p>path of model file or <code>lgb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_nrounds">nrounds</code></td>
<td>
<p>number of training rounds</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_obj">obj</code></td>
<td>
<p>objective function, can be character or custom objective function. Examples include
<code>regression</code>, <code>regression_l1</code>, <code>huber</code>,
<code>binary</code>, <code>lambdarank</code>, <code>multiclass</code>, <code>multiclass</code></p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_params">params</code></td>
<td>
<p>a list of parameters. See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">
the &quot;Parameters&quot; section of the documentation</a> for a list of parameters and valid values.</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0 and <code>valids</code> has been provided, also will disable the
printing of evaluation during training</p>
</td></tr>
<tr><td><code id="lgb_shared_params_+3A_serializable">serializable</code></td>
<td>
<p>whether to make the resulting objects serializable through functions such as
<code>save</code> or <code>saveRDS</code> (see section &quot;Model serialization&quot;).</p>
</td></tr>
</table>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Model serialization</h3>

<p>LightGBM model objects can be serialized and de-serialized through functions such as <code>save</code>
or <code>saveRDS</code>, but similarly to libraries such as 'xgboost', serialization works a bit differently
from typical R objects. In order to make models serializable in R, a copy of the underlying C++ object
as serialized raw bytes is produced and stored in the R model object, and when this R object is
de-serialized, the underlying C++ model object gets reconstructed from these raw bytes, but will only
do so once some function that uses it is called, such as <code>predict</code>. In order to forcibly
reconstruct the C++ object after deserialization (e.g. after calling <code>readRDS</code> or similar), one
can use the function <a href="#topic+lgb.restore_handle">lgb.restore_handle</a> (for example, if one makes predictions in parallel or in
forked processes, it will be faster to restore the handle beforehand).
</p>
<p>Producing and keeping these raw bytes however uses extra memory, and if they are not required,
it is possible to avoid producing them by passing 'serializable=FALSE'. In such cases, these raw
bytes can be added to the model on demand through function <a href="#topic+lgb.make_serializable">lgb.make_serializable</a>.
</p>
<p><em>New in version 4.0.0</em>
</p>

<hr>
<h2 id='lgb.configure_fast_predict'>Configure Fast Single-Row Predictions</h2><span id='topic+lgb.configure_fast_predict'></span>

<h3>Description</h3>

<p>Pre-configures a LightGBM model object to produce fast single-row predictions
for a given input data type, prediction type, and parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.configure_fast_predict(
  model,
  csr = FALSE,
  start_iteration = NULL,
  num_iteration = NULL,
  type = "response",
  params = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.configure_fast_predict_+3A_model">model</code></td>
<td>
<p>LighGBM model object (class <code>lgb.Booster</code>).
</p>
<p><b>The object will be modified in-place</b>.</p>
</td></tr>
<tr><td><code id="lgb.configure_fast_predict_+3A_csr">csr</code></td>
<td>
<p>Whether the prediction function is going to be called on sparse CSR inputs.
If <code>FALSE</code>, will be assumed that predictions are going to be called on single-row
regular R matrices.</p>
</td></tr>
<tr><td><code id="lgb.configure_fast_predict_+3A_start_iteration">start_iteration</code></td>
<td>
<p>int or None, optional (default=None)
Start index of the iteration to predict.
If None or &lt;= 0, starts from the first iteration.</p>
</td></tr>
<tr><td><code id="lgb.configure_fast_predict_+3A_num_iteration">num_iteration</code></td>
<td>
<p>int or None, optional (default=None)
Limit number of iterations in the prediction.
If None, if the best iteration exists and start_iteration is None or &lt;= 0, the
best iteration is used; otherwise, all iterations from start_iteration are used.
If &lt;= 0, all iterations from start_iteration are used (no limits).</p>
</td></tr>
<tr><td><code id="lgb.configure_fast_predict_+3A_type">type</code></td>
<td>
<p>Type of prediction to output. Allowed types are:</p>

<ul>
<li> <p><code>"response"</code>: will output the predicted score according to the objective function being
optimized (depending on the link function that the objective uses), after applying any necessary
transformations - for example, for <code>objective="binary"</code>, it will output class probabilities.
</p>
</li>
<li> <p><code>"class"</code>: for classification objectives, will output the class with the highest predicted
probability. For other objectives, will output the same as &quot;response&quot;. Note that <code>"class"</code> is
not a supported type for <a href="#topic+lgb.configure_fast_predict">lgb.configure_fast_predict</a> (see the documentation of that function
for more details).
</p>
</li>
<li> <p><code>"raw"</code>: will output the non-transformed numbers (sum of predictions from boosting iterations'
results) from which the &quot;response&quot; number is produced for a given objective function - for example,
for <code>objective="binary"</code>, this corresponds to log-odds. For many objectives such as
&quot;regression&quot;, since no transformation is applied, the output will be the same as for &quot;response&quot;.
</p>
</li>
<li> <p><code>"leaf"</code>: will output the index of the terminal node / leaf at which each observations falls
in each tree in the model, outputted as integers, with one column per tree.
</p>
</li>
<li> <p><code>"contrib"</code>: will return the per-feature contributions for each prediction, including an
intercept (each feature will produce one column).
</p>
</li></ul>

<p>Note that, if using custom objectives, types &quot;class&quot; and &quot;response&quot; will not be available and will
default towards using &quot;raw&quot; instead.
</p>
<p>If the model was fit through function <a href="#topic+lightgbm">lightgbm</a> and it was passed a factor as labels,
passing the prediction type through <code>params</code> instead of through this argument might
result in factor levels for classification objectives not being applied correctly to the
resulting output.
</p>
<p><em>New in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="lgb.configure_fast_predict_+3A_params">params</code></td>
<td>
<p>a list of additional named parameters. See
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#predict-parameters">
the &quot;Predict Parameters&quot; section of the documentation</a> for a list of parameters and
valid values. Where these conflict with the values of keyword arguments to this function,
the values in <code>params</code> take precedence.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calling this function multiple times with different parameters might not override
the previous configuration and might trigger undefined behavior.
</p>
<p>Any saved configuration for fast predictions might be lost after making a single-row
prediction of a different type than what was configured (except for types &quot;response&quot; and
&quot;class&quot;, which can be switched between each other at any time without losing the configuration).
</p>
<p>In some situations, setting a fast prediction configuration for one type of prediction
might cause the prediction function to keep using that configuration for single-row
predictions even if the requested type of prediction is different from what was configured.
</p>
<p>Note that this function will not accept argument <code>type="class"</code> - for such cases, one
can pass <code>type="response"</code> to this function and then <code>type="class"</code> to the
<code>predict</code> function - the fast configuration will not be lost or altered if the switch
is between &quot;response&quot; and &quot;class&quot;.
</p>
<p>The configuration does not survive de-serializations, so it has to be generated
anew in every R process that is going to use it (e.g. if loading a model object
through <code>readRDS</code>, whatever configuration was there previously will be lost).
</p>
<p>Requesting a different prediction type or passing parameters to <a href="#topic+predict.lgb.Booster">predict.lgb.Booster</a>
will cause it to ignore the fast-predict configuration and take the slow route instead
(but be aware that an existing configuration might not always be overriden by supplying
different parameters or prediction type, so make sure to check that the output is what
was expected when a prediction is to be made on a single row for something different than
what is configured).
</p>
<p>Note that, if configuring a non-default prediction type (such as leaf indices),
then that type must also be passed in the call to <a href="#topic+predict.lgb.Booster">predict.lgb.Booster</a> in
order for it to use the configuration. This also applies for <code>start_iteration</code>
and <code>num_iteration</code>, but <b>the <code>params</code> list must be empty</b> in the call to <code>predict</code>.
</p>
<p>Predictions about feature contributions do not allow a fast route for CSR inputs,
and as such, this function will produce an error if passing <code>csr=TRUE</code> and
<code>type = "contrib"</code> together.
</p>


<h3>Value</h3>

<p>The same <code>model</code> that was passed as input, invisibly, with the desired
configuration stored inside it and available to be used in future calls to
<a href="#topic+predict.lgb.Booster">predict.lgb.Booster</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


library(lightgbm)
data(mtcars)
X &lt;- as.matrix(mtcars[, -1L])
y &lt;- mtcars[, 1L]
dtrain &lt;- lgb.Dataset(X, label = y, params = list(max_bin = 5L))
params &lt;- list(
  min_data_in_leaf = 2L
  , num_threads = 2L
)
model &lt;- lgb.train(
  params = params
 , data = dtrain
 , obj = "regression"
 , nrounds = 5L
 , verbose = -1L
)
lgb.configure_fast_predict(model)

x_single &lt;- X[11L, , drop = FALSE]
predict(model, x_single)

# Will not use it if the prediction to be made
# is different from what was configured
predict(model, x_single, type = "leaf")

</code></pre>

<hr>
<h2 id='lgb.convert_with_rules'>Data preparator for LightGBM datasets with rules (integer)</h2><span id='topic+lgb.convert_with_rules'></span>

<h3>Description</h3>

<p>Attempts to prepare a clean dataset to prepare to put in a <code>lgb.Dataset</code>.
Factor, character, and logical columns are converted to integer. Missing values
in factors and characters will be filled with 0L. Missing values in logicals
will be filled with -1L.
</p>
<p>This function returns and optionally takes in &quot;rules&quot; the describe exactly
how to convert values in columns.
</p>
<p>Columns that contain only NA values will be converted by this function but will
not show up in the returned <code>rules</code>.
</p>
<p>NOTE: In previous releases of LightGBM, this function was called <code>lgb.prepare_rules2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.convert_with_rules(data, rules = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.convert_with_rules_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table to prepare.</p>
</td></tr>
<tr><td><code id="lgb.convert_with_rules_+3A_rules">rules</code></td>
<td>
<p>A set of rules from the data preparator, if already used. This should be an R list,
where names are column names in <code>data</code> and values are named character
vectors whose names are column values and whose values are new values to
replace them with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the cleaned dataset (<code>data</code>) and the rules (<code>rules</code>).
Note that the data must be converted to a matrix format (<code>as.matrix</code>) for input in
<code>lgb.Dataset</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

str(iris)

new_iris &lt;- lgb.convert_with_rules(data = iris)
str(new_iris$data)

data(iris) # Erase iris dataset
iris$Species[1L] &lt;- "NEW FACTOR" # Introduce junk factor (NA)

# Use conversion using known rules
# Unknown factors become 0, excellent for sparse datasets
newer_iris &lt;- lgb.convert_with_rules(data = iris, rules = new_iris$rules)

# Unknown factor is now zero, perfect for sparse datasets
newer_iris$data[1L, ] # Species became 0 as it is an unknown factor

newer_iris$data[1L, 5L] &lt;- 1.0 # Put back real initial value

# Is the newly created dataset equal? YES!
all.equal(new_iris$data, newer_iris$data)

# Can we test our own rules?
data(iris) # Erase iris dataset

# We remapped values differently
personal_rules &lt;- list(
  Species = c(
    "setosa" = 3L
    , "versicolor" = 2L
    , "virginica" = 1L
  )
)
newest_iris &lt;- lgb.convert_with_rules(data = iris, rules = personal_rules)
str(newest_iris$data) # SUCCESS!

</code></pre>

<hr>
<h2 id='lgb.cv'>Main CV logic for LightGBM</h2><span id='topic+lgb.cv'></span>

<h3>Description</h3>

<p>Cross validation logic used by LightGBM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.cv(
  params = list(),
  data,
  nrounds = 100L,
  nfold = 3L,
  label = NULL,
  weight = NULL,
  obj = NULL,
  eval = NULL,
  verbose = 1L,
  record = TRUE,
  eval_freq = 1L,
  showsd = TRUE,
  stratified = TRUE,
  folds = NULL,
  init_model = NULL,
  colnames = NULL,
  categorical_feature = NULL,
  early_stopping_rounds = NULL,
  callbacks = list(),
  reset_data = FALSE,
  serializable = TRUE,
  eval_train_metric = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.cv_+3A_params">params</code></td>
<td>
<p>a list of parameters. See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">
the &quot;Parameters&quot; section of the documentation</a> for a list of parameters and valid values.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_data">data</code></td>
<td>
<p>a <code>lgb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+lgb.cv">lgb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_nrounds">nrounds</code></td>
<td>
<p>number of training rounds</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_nfold">nfold</code></td>
<td>
<p>the original dataset is randomly partitioned into <code>nfold</code> equal size subsamples.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_label">label</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_weight">weight</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_obj">obj</code></td>
<td>
<p>objective function, can be character or custom objective function. Examples include
<code>regression</code>, <code>regression_l1</code>, <code>huber</code>,
<code>binary</code>, <code>lambdarank</code>, <code>multiclass</code>, <code>multiclass</code></p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_eval">eval</code></td>
<td>
<p>evaluation function(s). This can be a character vector, function, or list with a mixture of
strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
If you provide a character vector to this argument, it should contain strings with valid
evaluation metrics.
See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric">
The &quot;metric&quot; section of the documentation</a>
for a list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="lgb.cv_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0 and <code>valids</code> has been provided, also will disable the
printing of evaluation during training</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effective when verbose &gt; 0 and <code>valids</code> has been provided</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_showsd">showsd</code></td>
<td>
<p><code>boolean</code>, whether to show standard deviation of cross validation.
This parameter defaults to <code>TRUE</code>. Setting it to <code>FALSE</code> can lead to a
slight speedup by avoiding unnecessary computation.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_stratified">stratified</code></td>
<td>
<p>a <code>boolean</code> indicating whether sampling of folds should be stratified
by the values of outcome labels.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_folds">folds</code></td>
<td>
<p><code>list</code> provides a possibility to use a list of pre-defined CV folds
(each element must be a vector of test fold's indices). When folds are supplied,
the <code>nfold</code> and <code>stratified</code> parameters are ignored.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_init_model">init_model</code></td>
<td>
<p>path of model file or <code>lgb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_colnames">colnames</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_reset_data">reset_data</code></td>
<td>
<p>Boolean, setting it to TRUE (not the default value) will transform the booster model
into a predictor model which frees up memory and the original datasets</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_serializable">serializable</code></td>
<td>
<p>whether to make the resulting objects serializable through functions such as
<code>save</code> or <code>saveRDS</code> (see section &quot;Model serialization&quot;).</p>
</td></tr>
<tr><td><code id="lgb.cv_+3A_eval_train_metric">eval_train_metric</code></td>
<td>
<p><code>boolean</code>, whether to add the cross validation results on the
training data. This parameter defaults to <code>FALSE</code>. Setting it to <code>TRUE</code>
will increase run time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained model <code>lgb.CVBooster</code>.
</p>


<h3>Deprecated Arguments</h3>

<p>A future release of <code>lightgbm</code> will require passing an <code>lgb.Dataset</code>
to argument <code>'data'</code>. It will also remove support for passing arguments
<code>'categorical_feature'</code>, <code>'colnames'</code>, <code>'label'</code>, and <code>'weight'</code>.
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
model &lt;- lgb.cv(
  params = params
  , data = dtrain
  , nrounds = 5L
  , nfold = 3L
)


</code></pre>

<hr>
<h2 id='lgb.Dataset'>Construct <code>lgb.Dataset</code> object</h2><span id='topic+lgb.Dataset'></span>

<h3>Description</h3>

<p>Construct <code>lgb.Dataset</code> object from dense matrix, sparse matrix
or local file (that was created previously by saving an <code>lgb.Dataset</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset(
  data,
  params = list(),
  reference = NULL,
  colnames = NULL,
  categorical_feature = NULL,
  free_raw_data = TRUE,
  label = NULL,
  weight = NULL,
  group = NULL,
  init_score = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code> object,
a character representing a path to a text file (CSV, TSV, or LibSVM),
or a character representing a path to a binary <code>lgb.Dataset</code> file</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_params">params</code></td>
<td>
<p>a list of parameters. See
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#dataset-parameters">
The &quot;Dataset Parameters&quot; section of the documentation</a> for a list of parameters
and valid values.</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_reference">reference</code></td>
<td>
<p>reference dataset. When LightGBM creates a Dataset, it does some preprocessing like binning
continuous features into histograms. If you want to apply the same bin boundaries from an existing
dataset to new <code>data</code>, pass that existing Dataset to this argument.</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_colnames">colnames</code></td>
<td>
<p>names of columns</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_free_raw_data">free_raw_data</code></td>
<td>
<p>LightGBM constructs its data format, called a &quot;Dataset&quot;, from tabular data.
By default, that Dataset object on the R side does not keep a copy of the raw data.
This reduces LightGBM's memory consumption, but it means that the Dataset object
cannot be changed after it has been constructed. If you'd prefer to be able to
change the Dataset object after construction, set <code>free_raw_data = FALSE</code>.</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_label">label</code></td>
<td>
<p>vector of labels to use as the target variable</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_weight">weight</code></td>
<td>
<p>numeric vector of sample weights</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_group">group</code></td>
<td>
<p>used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results
to be ranked. For example, if you have a 100-document dataset with
<code>group = c(10, 20, 40, 10, 10, 10)</code>, that means that you have 6 groups,
where the first 10 records are in the first group, records 11-30 are in the
second group, etc.</p>
</td></tr>
<tr><td><code id="lgb.Dataset_+3A_init_score">init_score</code></td>
<td>
<p>initial score is the base prediction lightgbm will boost from</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data_file &lt;- tempfile(fileext = ".data")
lgb.Dataset.save(dtrain, data_file)
dtrain &lt;- lgb.Dataset(data_file)
lgb.Dataset.construct(dtrain)

</code></pre>

<hr>
<h2 id='lgb.Dataset.construct'>Construct Dataset explicitly</h2><span id='topic+lgb.Dataset.construct'></span>

<h3>Description</h3>

<p>Construct Dataset explicitly
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset.construct(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset.construct_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>lgb.Dataset</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
lgb.Dataset.construct(dtrain)

</code></pre>

<hr>
<h2 id='lgb.Dataset.create.valid'>Construct validation data</h2><span id='topic+lgb.Dataset.create.valid'></span>

<h3>Description</h3>

<p>Construct validation data according to training data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset.create.valid(
  dataset,
  data,
  label = NULL,
  weight = NULL,
  group = NULL,
  init_score = NULL,
  params = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset.create.valid_+3A_dataset">dataset</code></td>
<td>
<p><code>lgb.Dataset</code> object, training data</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code> object,
a character representing a path to a text file (CSV, TSV, or LibSVM),
or a character representing a path to a binary <code>Dataset</code> file</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_label">label</code></td>
<td>
<p>vector of labels to use as the target variable</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_weight">weight</code></td>
<td>
<p>numeric vector of sample weights</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_group">group</code></td>
<td>
<p>used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results
to be ranked. For example, if you have a 100-document dataset with
<code>group = c(10, 20, 40, 10, 10, 10)</code>, that means that you have 6 groups,
where the first 10 records are in the first group, records 11-30 are in the
second group, etc.</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_init_score">init_score</code></td>
<td>
<p>initial score is the base prediction lightgbm will boost from</p>
</td></tr>
<tr><td><code id="lgb.Dataset.create.valid_+3A_params">params</code></td>
<td>
<p>a list of parameters. See
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#dataset-parameters">
The &quot;Dataset Parameters&quot; section of the documentation</a> for a list of parameters
and valid values. If this is an empty list (the default), the validation Dataset
will have the same parameters as the Dataset passed to argument <code>dataset</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)

# parameters can be changed between the training data and validation set,
# for example to account for training data in a text file with a header row
# and validation data in a text file without it
train_file &lt;- tempfile(pattern = "train_", fileext = ".csv")
write.table(
  data.frame(y = rnorm(100L), x1 = rnorm(100L), x2 = rnorm(100L))
  , file = train_file
  , sep = ","
  , col.names = TRUE
  , row.names = FALSE
  , quote = FALSE
)

valid_file &lt;- tempfile(pattern = "valid_", fileext = ".csv")
write.table(
  data.frame(y = rnorm(100L), x1 = rnorm(100L), x2 = rnorm(100L))
  , file = valid_file
  , sep = ","
  , col.names = FALSE
  , row.names = FALSE
  , quote = FALSE
)

dtrain &lt;- lgb.Dataset(
  data = train_file
  , params = list(has_header = TRUE)
)
dtrain$construct()

dvalid &lt;- lgb.Dataset(
  data = valid_file
  , params = list(has_header = FALSE)
)
dvalid$construct()

</code></pre>

<hr>
<h2 id='lgb.Dataset.save'>Save <code>lgb.Dataset</code> to a binary file</h2><span id='topic+lgb.Dataset.save'></span>

<h3>Description</h3>

<p>Please note that <code>init_score</code> is not saved in binary file.
If you need it, please set it again after loading Dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset.save(dataset, fname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset.save_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="lgb.Dataset.save_+3A_fname">fname</code></td>
<td>
<p>object filename of output file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
lgb.Dataset.save(dtrain, tempfile(fileext = ".bin"))

</code></pre>

<hr>
<h2 id='lgb.Dataset.set.categorical'>Set categorical feature of <code>lgb.Dataset</code></h2><span id='topic+lgb.Dataset.set.categorical'></span>

<h3>Description</h3>

<p>Set the categorical features of an <code>lgb.Dataset</code> object. Use this function
to tell LightGBM which features should be treated as categorical.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset.set.categorical(dataset, categorical_feature)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset.set.categorical_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="lgb.Dataset.set.categorical_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data_file &lt;- tempfile(fileext = ".data")
lgb.Dataset.save(dtrain, data_file)
dtrain &lt;- lgb.Dataset(data_file)
lgb.Dataset.set.categorical(dtrain, 1L:2L)

</code></pre>

<hr>
<h2 id='lgb.Dataset.set.reference'>Set reference of <code>lgb.Dataset</code></h2><span id='topic+lgb.Dataset.set.reference'></span>

<h3>Description</h3>

<p>If you want to use validation data, you should set reference to training data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.Dataset.set.reference(dataset, reference)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.Dataset.set.reference_+3A_dataset">dataset</code></td>
<td>
<p>object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="lgb.Dataset.set.reference_+3A_reference">reference</code></td>
<td>
<p>object of class <code>lgb.Dataset</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the dataset you passed in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# create training Dataset
data(agaricus.train, package ="lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

# create a validation Dataset, using dtrain as a reference
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset(test$data, label = test$label)
lgb.Dataset.set.reference(dtest, dtrain)

</code></pre>

<hr>
<h2 id='lgb.drop_serialized'>Drop serialized raw bytes in a LightGBM model object</h2><span id='topic+lgb.drop_serialized'></span>

<h3>Description</h3>

<p>If a LightGBM model object was produced with argument 'serializable=TRUE', the R object will keep
a copy of the underlying C++ object as raw bytes, which can be used to reconstruct such object after getting
serialized and de-serialized, but at the cost of extra memory usage. If these raw bytes are not needed anymore,
they can be dropped through this function in order to save memory. Note that the object will be modified in-place.
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.drop_serialized(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.drop_serialized_+3A_model">model</code></td>
<td>
<p><code>lgb.Booster</code> object which was produced with 'serializable=TRUE'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>lgb.Booster</code> (the same 'model' object that was passed as input, as invisible).
</p>


<h3>See Also</h3>

<p><a href="#topic+lgb.restore_handle">lgb.restore_handle</a>, <a href="#topic+lgb.make_serializable">lgb.make_serializable</a>.
</p>

<hr>
<h2 id='lgb.dump'>Dump LightGBM model to json</h2><span id='topic+lgb.dump'></span>

<h3>Description</h3>

<p>Dump LightGBM model to json
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.dump(booster, num_iteration = NULL, start_iteration = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.dump_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="lgb.dump_+3A_num_iteration">num_iteration</code></td>
<td>
<p>Number of iterations to be dumped. NULL or &lt;= 0 means use best iteration</p>
</td></tr>
<tr><td><code id="lgb.dump_+3A_start_iteration">start_iteration</code></td>
<td>
<p>Index (1-based) of the first boosting round to dump.
For example, passing <code>start_iteration=5, num_iteration=3</code> for a regression model
means &quot;dump the fifth, sixth, and seventh tree&quot;
</p>
<p><em>New in version 4.4.0</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>json format of model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(lightgbm)


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 10L
  , valids = valids
  , early_stopping_rounds = 5L
)
json_model &lt;- lgb.dump(model)

</code></pre>

<hr>
<h2 id='lgb.get.eval.result'>Get record evaluation result from booster</h2><span id='topic+lgb.get.eval.result'></span>

<h3>Description</h3>

<p>Given a <code>lgb.Booster</code>, return evaluation results for a
particular metric on a particular dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.get.eval.result(
  booster,
  data_name,
  eval_name,
  iters = NULL,
  is_err = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.get.eval.result_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="lgb.get.eval.result_+3A_data_name">data_name</code></td>
<td>
<p>Name of the dataset to return evaluation results for.</p>
</td></tr>
<tr><td><code id="lgb.get.eval.result_+3A_eval_name">eval_name</code></td>
<td>
<p>Name of the evaluation metric to return results for.</p>
</td></tr>
<tr><td><code id="lgb.get.eval.result_+3A_iters">iters</code></td>
<td>
<p>An integer vector of iterations you want to get evaluation results for. If NULL
(the default), evaluation results for all iterations will be returned.</p>
</td></tr>
<tr><td><code id="lgb.get.eval.result_+3A_is_err">is_err</code></td>
<td>
<p>TRUE will return evaluation error instead</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of evaluation result
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# train a regression model
data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
  , valids = valids
)

# Examine valid data_name values
print(setdiff(names(model$record_evals), "start_iter"))

# Examine valid eval_name values for dataset "test"
print(names(model$record_evals[["test"]]))

# Get L2 values for "test" dataset
lgb.get.eval.result(model, "test", "l2")

</code></pre>

<hr>
<h2 id='lgb.importance'>Compute feature importance in a model</h2><span id='topic+lgb.importance'></span>

<h3>Description</h3>

<p>Creates a <code>data.table</code> of feature importances in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.importance(model, percentage = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.importance_+3A_model">model</code></td>
<td>
<p>object of class <code>lgb.Booster</code>.</p>
</td></tr>
<tr><td><code id="lgb.importance_+3A_percentage">percentage</code></td>
<td>
<p>whether to show importance in relative percentage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For a tree model, a <code>data.table</code> with the following columns:
</p>

<ul>
<li><p><code>Feature</code>: Feature names in the model.
</p>
</li>
<li><p><code>Gain</code>: The total gain of this feature's splits.
</p>
</li>
<li><p><code>Cover</code>: The number of observation related to this feature.
</p>
</li>
<li><p><code>Frequency</code>: The number of times a feature splited in trees.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.1
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
  , num_threads = 2L
)
model &lt;- lgb.train(
    params = params
    , data = dtrain
    , nrounds = 5L
)

tree_imp1 &lt;- lgb.importance(model, percentage = TRUE)
tree_imp2 &lt;- lgb.importance(model, percentage = FALSE)

</code></pre>

<hr>
<h2 id='lgb.interprete'>Compute feature contribution of prediction</h2><span id='topic+lgb.interprete'></span>

<h3>Description</h3>

<p>Computes feature contribution components of rawscore prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.interprete(model, data, idxset, num_iteration = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.interprete_+3A_model">model</code></td>
<td>
<p>object of class <code>lgb.Booster</code>.</p>
</td></tr>
<tr><td><code id="lgb.interprete_+3A_data">data</code></td>
<td>
<p>a matrix object or a dgCMatrix object.</p>
</td></tr>
<tr><td><code id="lgb.interprete_+3A_idxset">idxset</code></td>
<td>
<p>an integer vector of indices of rows needed.</p>
</td></tr>
<tr><td><code id="lgb.interprete_+3A_num_iteration">num_iteration</code></td>
<td>
<p>number of iteration want to predict with, NULL or &lt;= 0 means use best iteration.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For regression, binary classification and lambdarank model, a <code>list</code> of <code>data.table</code>
with the following columns:
</p>

<ul>
<li><p><code>Feature</code>: Feature names in the model.
</p>
</li>
<li><p><code>Contribution</code>: The total contribution of this feature's splits.
</p>
</li></ul>

<p>For multiclass classification, a <code>list</code> of <code>data.table</code> with the Feature column and
Contribution columns to each class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


Logit &lt;- function(x) log(x / (1.0 - x))
data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
set_field(
  dataset = dtrain
  , field_name = "init_score"
  , data = rep(Logit(mean(train$label)), length(train$label))
)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test

params &lt;- list(
    objective = "binary"
    , learning_rate = 0.1
    , max_depth = -1L
    , min_data_in_leaf = 1L
    , min_sum_hessian_in_leaf = 1.0
    , num_threads = 2L
)
model &lt;- lgb.train(
    params = params
    , data = dtrain
    , nrounds = 3L
)

tree_interpretation &lt;- lgb.interprete(model, test$data, 1L:5L)

</code></pre>

<hr>
<h2 id='lgb.load'>Load LightGBM model</h2><span id='topic+lgb.load'></span>

<h3>Description</h3>

<p>Load LightGBM takes in either a file path or model string.
If both are provided, Load will default to loading from file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.load(filename = NULL, model_str = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.load_+3A_filename">filename</code></td>
<td>
<p>path of model file</p>
</td></tr>
<tr><td><code id="lgb.load_+3A_model_str">model_str</code></td>
<td>
<p>a str containing the model (as a <code>character</code> or <code>raw</code> vector)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>lgb.Booster
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
  , valids = valids
  , early_stopping_rounds = 3L
)
model_file &lt;- tempfile(fileext = ".txt")
lgb.save(model, model_file)
load_booster &lt;- lgb.load(filename = model_file)
model_string &lt;- model$save_model_to_string(NULL) # saves best iteration
load_booster_from_str &lt;- lgb.load(model_str = model_string)

</code></pre>

<hr>
<h2 id='lgb.make_serializable'>Make a LightGBM object serializable by keeping raw bytes</h2><span id='topic+lgb.make_serializable'></span>

<h3>Description</h3>

<p>If a LightGBM model object was produced with argument 'serializable=FALSE', the R object will not
be serializable (e.g. cannot save and load with <code>saveRDS</code> and <code>readRDS</code>) as it will lack the raw bytes
needed to reconstruct its underlying C++ object. This function can be used to forcibly produce those serialized
raw bytes and make the object serializable. Note that the object will be modified in-place.
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.make_serializable(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.make_serializable_+3A_model">model</code></td>
<td>
<p><code>lgb.Booster</code> object which was produced with 'serializable=FALSE'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>lgb.Booster</code> (the same 'model' object that was passed as input, as invisible).
</p>


<h3>See Also</h3>

<p><a href="#topic+lgb.restore_handle">lgb.restore_handle</a>, <a href="#topic+lgb.drop_serialized">lgb.drop_serialized</a>.
</p>

<hr>
<h2 id='lgb.model.dt.tree'>Parse a LightGBM model json dump</h2><span id='topic+lgb.model.dt.tree'></span>

<h3>Description</h3>

<p>Parse a LightGBM model json dump into a <code>data.table</code> structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.model.dt.tree(model, num_iteration = NULL, start_iteration = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.model.dt.tree_+3A_model">model</code></td>
<td>
<p>object of class <code>lgb.Booster</code>.</p>
</td></tr>
<tr><td><code id="lgb.model.dt.tree_+3A_num_iteration">num_iteration</code></td>
<td>
<p>Number of iterations to include. NULL or &lt;= 0 means use best iteration.</p>
</td></tr>
<tr><td><code id="lgb.model.dt.tree_+3A_start_iteration">start_iteration</code></td>
<td>
<p>Index (1-based) of the first boosting round to include in the output.
For example, passing <code>start_iteration=5, num_iteration=3</code> for a regression model
means &quot;return information about the fifth, sixth, and seventh trees&quot;.
</p>
<p><em>New in version 4.4.0</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with detailed information about model trees' nodes and leafs.
</p>
<p>The columns of the <code>data.table</code> are:
</p>

<ul>
<li><p><code>tree_index</code>: ID of a tree in a model (integer)
</p>
</li>
<li><p><code>split_index</code>: ID of a node in a tree (integer)
</p>
</li>
<li><p><code>split_feature</code>: for a node, it's a feature name (character);
for a leaf, it simply labels it as <code>"NA"</code>
</p>
</li>
<li><p><code>node_parent</code>: ID of the parent node for current node (integer)
</p>
</li>
<li><p><code>leaf_index</code>: ID of a leaf in a tree (integer)
</p>
</li>
<li><p><code>leaf_parent</code>: ID of the parent node for current leaf (integer)
</p>
</li>
<li><p><code>split_gain</code>: Split gain of a node
</p>
</li>
<li><p><code>threshold</code>: Splitting threshold value of a node
</p>
</li>
<li><p><code>decision_type</code>: Decision type of a node
</p>
</li>
<li><p><code>default_left</code>: Determine how to handle NA value, TRUE -&gt; Left, FALSE -&gt; Right
</p>
</li>
<li><p><code>internal_value</code>: Node value
</p>
</li>
<li><p><code>internal_count</code>: The number of observation collected by a node
</p>
</li>
<li><p><code>leaf_value</code>: Leaf value
</p>
</li>
<li><p><code>leaf_count</code>: The number of observation collected by a leaf
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.01
  , num_leaves = 63L
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
  , num_threads = 2L
)
model &lt;- lgb.train(params, dtrain, 10L)

tree_dt &lt;- lgb.model.dt.tree(model)

</code></pre>

<hr>
<h2 id='lgb.plot.importance'>Plot feature importance as a bar graph</h2><span id='topic+lgb.plot.importance'></span>

<h3>Description</h3>

<p>Plot previously calculated feature importance: Gain, Cover and Frequency, as a bar graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.plot.importance(
  tree_imp,
  top_n = 10L,
  measure = "Gain",
  left_margin = 10L,
  cex = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.plot.importance_+3A_tree_imp">tree_imp</code></td>
<td>
<p>a <code>data.table</code> returned by <code><a href="#topic+lgb.importance">lgb.importance</a></code>.</p>
</td></tr>
<tr><td><code id="lgb.plot.importance_+3A_top_n">top_n</code></td>
<td>
<p>maximal number of top features to include into the plot.</p>
</td></tr>
<tr><td><code id="lgb.plot.importance_+3A_measure">measure</code></td>
<td>
<p>the name of importance measure to plot, can be &quot;Gain&quot;, &quot;Cover&quot; or &quot;Frequency&quot;.</p>
</td></tr>
<tr><td><code id="lgb.plot.importance_+3A_left_margin">left_margin</code></td>
<td>
<p>(base R barplot) allows to adjust the left margin size to fit feature names.</p>
</td></tr>
<tr><td><code id="lgb.plot.importance_+3A_cex">cex</code></td>
<td>
<p>(base R barplot) passed as <code>cex.names</code> parameter to <code><a href="graphics.html#topic+barplot">barplot</a></code>.
Set a number smaller than 1.0 to make the bar labels smaller than R's default and values
greater than 1.0 to make them larger.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph represents each feature as a horizontal bar of length proportional to the defined importance of a feature.
Features are shown ranked in a decreasing importance order.
</p>


<h3>Value</h3>

<p>The <code>lgb.plot.importance</code> function creates a <code>barplot</code>
and silently returns a processed data.table with <code>top_n</code> features sorted by defined importance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

params &lt;- list(
    objective = "binary"
    , learning_rate = 0.1
    , min_data_in_leaf = 1L
    , min_sum_hessian_in_leaf = 1.0
    , num_threads = 2L
)

model &lt;- lgb.train(
    params = params
    , data = dtrain
    , nrounds = 5L
)

tree_imp &lt;- lgb.importance(model, percentage = TRUE)
lgb.plot.importance(tree_imp, top_n = 5L, measure = "Gain")

</code></pre>

<hr>
<h2 id='lgb.plot.interpretation'>Plot feature contribution as a bar graph</h2><span id='topic+lgb.plot.interpretation'></span>

<h3>Description</h3>

<p>Plot previously calculated feature contribution as a bar graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.plot.interpretation(
  tree_interpretation_dt,
  top_n = 10L,
  cols = 1L,
  left_margin = 10L,
  cex = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.plot.interpretation_+3A_tree_interpretation_dt">tree_interpretation_dt</code></td>
<td>
<p>a <code>data.table</code> returned by <code><a href="#topic+lgb.interprete">lgb.interprete</a></code>.</p>
</td></tr>
<tr><td><code id="lgb.plot.interpretation_+3A_top_n">top_n</code></td>
<td>
<p>maximal number of top features to include into the plot.</p>
</td></tr>
<tr><td><code id="lgb.plot.interpretation_+3A_cols">cols</code></td>
<td>
<p>the column numbers of layout, will be used only for multiclass classification feature contribution.</p>
</td></tr>
<tr><td><code id="lgb.plot.interpretation_+3A_left_margin">left_margin</code></td>
<td>
<p>(base R barplot) allows to adjust the left margin size to fit feature names.</p>
</td></tr>
<tr><td><code id="lgb.plot.interpretation_+3A_cex">cex</code></td>
<td>
<p>(base R barplot) passed as <code>cex.names</code> parameter to <code>barplot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph represents each feature as a horizontal bar of length proportional to the defined
contribution of a feature. Features are shown ranked in a decreasing contribution order.
</p>


<h3>Value</h3>

<p>The <code>lgb.plot.interpretation</code> function creates a <code>barplot</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


Logit &lt;- function(x) {
  log(x / (1.0 - x))
}
data(agaricus.train, package = "lightgbm")
labels &lt;- agaricus.train$label
dtrain &lt;- lgb.Dataset(
  agaricus.train$data
  , label = labels
)
set_field(
  dataset = dtrain
  , field_name = "init_score"
  , data = rep(Logit(mean(labels)), length(labels))
)

data(agaricus.test, package = "lightgbm")

params &lt;- list(
  objective = "binary"
  , learning_rate = 0.1
  , max_depth = -1L
  , min_data_in_leaf = 1L
  , min_sum_hessian_in_leaf = 1.0
  , num_threads = 2L
)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
)

tree_interpretation &lt;- lgb.interprete(
  model = model
  , data = agaricus.test$data
  , idxset = 1L:5L
)
lgb.plot.interpretation(
  tree_interpretation_dt = tree_interpretation[[1L]]
  , top_n = 3L
)

</code></pre>

<hr>
<h2 id='lgb.restore_handle'>Restore the C++ component of a de-serialized LightGBM model</h2><span id='topic+lgb.restore_handle'></span>

<h3>Description</h3>

<p>After a LightGBM model object is de-serialized through functions such as <code>save</code> or
<code>saveRDS</code>, its underlying C++ object will be blank and needs to be restored to able to use it. Such
object is restored automatically when calling functions such as <code>predict</code>, but this function can be
used to forcibly restore it beforehand. Note that the object will be modified in-place.
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.restore_handle(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.restore_handle_+3A_model">model</code></td>
<td>
<p><code>lgb.Booster</code> object which was de-serialized and whose underlying C++ object and R handle
need to be restored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be aware that fast single-row prediction configurations are not restored through this
function. If you wish to make fast single-row predictions using a <code>lgb.Booster</code> loaded this way,
call <a href="#topic+lgb.configure_fast_predict">lgb.configure_fast_predict</a> on the loaded <code>lgb.Booster</code> object.
</p>


<h3>Value</h3>

<p><code>lgb.Booster</code> (the same 'model' object that was passed as input, invisibly).
</p>


<h3>See Also</h3>

<p><a href="#topic+lgb.make_serializable">lgb.make_serializable</a>, <a href="#topic+lgb.drop_serialized">lgb.drop_serialized</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(lightgbm)


data("agaricus.train")
model &lt;- lightgbm(
  agaricus.train$data
  , agaricus.train$label
  , params = list(objective = "binary")
  , nrounds = 5L
  , verbose = 0
  , num_threads = 2L
)
fname &lt;- tempfile(fileext="rds")
saveRDS(model, fname)

model_new &lt;- readRDS(fname)
model_new$check_null_handle()
lgb.restore_handle(model_new)
model_new$check_null_handle()

</code></pre>

<hr>
<h2 id='lgb.save'>Save LightGBM model</h2><span id='topic+lgb.save'></span>

<h3>Description</h3>

<p>Save LightGBM model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.save(booster, filename, num_iteration = NULL, start_iteration = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.save_+3A_booster">booster</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="lgb.save_+3A_filename">filename</code></td>
<td>
<p>Saved filename</p>
</td></tr>
<tr><td><code id="lgb.save_+3A_num_iteration">num_iteration</code></td>
<td>
<p>Number of iterations to save, NULL or &lt;= 0 means use best iteration</p>
</td></tr>
<tr><td><code id="lgb.save_+3A_start_iteration">start_iteration</code></td>
<td>
<p>Index (1-based) of the first boosting round to save.
For example, passing <code>start_iteration=5, num_iteration=3</code> for a regression model
means &quot;save the fifth, sixth, and seventh tree&quot;
</p>
<p><em>New in version 4.4.0</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>lgb.Booster
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


library(lightgbm)
data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 10L
  , valids = valids
  , early_stopping_rounds = 5L
)
lgb.save(model, tempfile(fileext = ".txt"))

</code></pre>

<hr>
<h2 id='lgb.slice.Dataset'>Slice a dataset</h2><span id='topic+lgb.slice.Dataset'></span>

<h3>Description</h3>

<p>Get a new <code>lgb.Dataset</code> containing the specified rows of
original <code>lgb.Dataset</code> object
</p>
<p><em>Renamed from</em> <code>slice()</code> <em>in 4.4.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.slice.Dataset(dataset, idxset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.slice.Dataset_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="lgb.slice.Dataset_+3A_idxset">idxset</code></td>
<td>
<p>an integer vector of indices of rows needed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed sub dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)

dsub &lt;- lgb.slice.Dataset(dtrain, seq_len(42L))
lgb.Dataset.construct(dsub)
labels &lt;- lightgbm::get_field(dsub, "label")

</code></pre>

<hr>
<h2 id='lgb.train'>Main training logic for LightGBM</h2><span id='topic+lgb.train'></span>

<h3>Description</h3>

<p>Low-level R interface to train a LightGBM model. Unlike <code><a href="#topic+lightgbm">lightgbm</a></code>,
this function is focused on performance (e.g. speed, memory efficiency). It is also
less likely to have breaking API changes in new releases than <code><a href="#topic+lightgbm">lightgbm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgb.train(
  params = list(),
  data,
  nrounds = 100L,
  valids = list(),
  obj = NULL,
  eval = NULL,
  verbose = 1L,
  record = TRUE,
  eval_freq = 1L,
  init_model = NULL,
  colnames = NULL,
  categorical_feature = NULL,
  early_stopping_rounds = NULL,
  callbacks = list(),
  reset_data = FALSE,
  serializable = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgb.train_+3A_params">params</code></td>
<td>
<p>a list of parameters. See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">
the &quot;Parameters&quot; section of the documentation</a> for a list of parameters and valid values.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_data">data</code></td>
<td>
<p>a <code>lgb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+lgb.cv">lgb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_nrounds">nrounds</code></td>
<td>
<p>number of training rounds</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_valids">valids</code></td>
<td>
<p>a list of <code>lgb.Dataset</code> objects, used for validation</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_obj">obj</code></td>
<td>
<p>objective function, can be character or custom objective function. Examples include
<code>regression</code>, <code>regression_l1</code>, <code>huber</code>,
<code>binary</code>, <code>lambdarank</code>, <code>multiclass</code>, <code>multiclass</code></p>
</td></tr>
<tr><td><code id="lgb.train_+3A_eval">eval</code></td>
<td>
<p>evaluation function(s). This can be a character vector, function, or list with a mixture of
strings and functions.
</p>

<ul>
<li><p><b>a. character vector</b>:
If you provide a character vector to this argument, it should contain strings with valid
evaluation metrics.
See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric">
The &quot;metric&quot; section of the documentation</a>
for a list of valid metrics.

</p>
</li>
<li><p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li><p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li><p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li></ul>


</li>
<li><p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li></ul>
</td></tr>
<tr><td><code id="lgb.train_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0 and <code>valids</code> has been provided, also will disable the
printing of evaluation during training</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_record">record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td></tr>
<tr><td><code id="lgb.train_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effective when verbose &gt; 0 and <code>valids</code> has been provided</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_init_model">init_model</code></td>
<td>
<p>path of model file or <code>lgb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_colnames">colnames</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>Deprecated. See &quot;Deprecated Arguments&quot; section below.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_reset_data">reset_data</code></td>
<td>
<p>Boolean, setting it to TRUE (not the default value) will transform the
booster model into a predictor model which frees up memory and the
original datasets</p>
</td></tr>
<tr><td><code id="lgb.train_+3A_serializable">serializable</code></td>
<td>
<p>whether to make the resulting objects serializable through functions such as
<code>save</code> or <code>saveRDS</code> (see section &quot;Model serialization&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained booster model <code>lgb.Booster</code>.
</p>


<h3>Deprecated Arguments</h3>

<p>A future release of <code>lightgbm</code> will remove support for passing arguments
<code>'categorical_feature'</code> and <code>'colnames'</code>. Pass those things to
<code>lgb.Dataset</code> instead.
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
  , valids = valids
  , early_stopping_rounds = 3L
)


</code></pre>

<hr>
<h2 id='lightgbm'>Train a LightGBM model</h2><span id='topic+lightgbm'></span>

<h3>Description</h3>

<p>High-level R interface to train a LightGBM model. Unlike <code><a href="#topic+lgb.train">lgb.train</a></code>, this function
is focused on compatibility with other statistics and machine learning interfaces in R.
This focus on compatibility means that this interface may experience more frequent breaking API changes
than <code><a href="#topic+lgb.train">lgb.train</a></code>.
For efficiency-sensitive applications, or for applications where breaking API changes across releases
is very expensive, use <code><a href="#topic+lgb.train">lgb.train</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lightgbm(
  data,
  label = NULL,
  weights = NULL,
  params = list(),
  nrounds = 100L,
  verbose = 1L,
  eval_freq = 1L,
  early_stopping_rounds = NULL,
  init_model = NULL,
  callbacks = list(),
  serializable = TRUE,
  objective = "auto",
  init_score = NULL,
  num_threads = NULL,
  colnames = NULL,
  categorical_feature = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lightgbm_+3A_data">data</code></td>
<td>
<p>a <code>lgb.Dataset</code> object, used for training. Some functions, such as <code><a href="#topic+lgb.cv">lgb.cv</a></code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_label">label</code></td>
<td>
<p>Vector of labels, used if <code>data</code> is not an <code><a href="#topic+lgb.Dataset">lgb.Dataset</a></code></p>
</td></tr>
<tr><td><code id="lightgbm_+3A_weights">weights</code></td>
<td>
<p>Sample / observation weights for rows in the input data. If <code>NULL</code>, will assume that all
observations / rows have the same importance / weight.
</p>
<p><em>Changed from 'weight', in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="lightgbm_+3A_params">params</code></td>
<td>
<p>a list of parameters. See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">
the &quot;Parameters&quot; section of the documentation</a> for a list of parameters and valid values.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_nrounds">nrounds</code></td>
<td>
<p>number of training rounds</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_verbose">verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0 and <code>valids</code> has been provided, also will disable the
printing of evaluation during training</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_eval_freq">eval_freq</code></td>
<td>
<p>evaluation output frequency, only effective when verbose &gt; 0 and <code>valids</code> has been provided</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_init_model">init_model</code></td>
<td>
<p>path of model file or <code>lgb.Booster</code> object, will continue training from this model</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_callbacks">callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_serializable">serializable</code></td>
<td>
<p>whether to make the resulting objects serializable through functions such as
<code>save</code> or <code>saveRDS</code> (see section &quot;Model serialization&quot;).</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_objective">objective</code></td>
<td>
<p>Optimization objective (e.g. '&quot;regression&quot;', '&quot;binary&quot;', etc.).
For a list of accepted objectives, see
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective">
the &quot;objective&quot; item of the &quot;Parameters&quot; section of the documentation</a>.
</p>
<p>If passing <code>"auto"</code> and <code>data</code> is not of type <code>lgb.Dataset</code>, the objective will
be determined according to what is passed for <code>label</code>:</p>

<ul>
<li><p> If passing a factor with two variables, will use objective <code>"binary"</code>.
</p>
</li>
<li><p> If passing a factor with more than two variables, will use objective <code>"multiclass"</code>
(note that parameter <code>num_class</code> in this case will also be determined automatically from
<code>label</code>).
</p>
</li>
<li><p> Otherwise (or if passing <code>lgb.Dataset</code> as input), will use objective <code>"regression"</code>.
</p>
</li></ul>

<p><em>New in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="lightgbm_+3A_init_score">init_score</code></td>
<td>
<p>initial score is the base prediction lightgbm will boost from
</p>
<p><em>New in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="lightgbm_+3A_num_threads">num_threads</code></td>
<td>
<p>Number of parallel threads to use. For best speed, this should be set to the number of
physical cores in the CPU - in a typical x86-64 machine, this corresponds to half the
number of maximum threads.
</p>
<p>Be aware that using too many threads can result in speed degradation in smaller datasets
(see the parameters documentation for more details).
</p>
<p>If passing zero, will use the default number of threads configured for OpenMP
(typically controlled through an environment variable <code>OMP_NUM_THREADS</code>).
</p>
<p>If passing <code>NULL</code> (the default), will try to use the number of physical cores in the
system, but be aware that getting the number of cores detected correctly requires package
<code>RhpcBLASctl</code> to be installed.
</p>
<p>This parameter gets overriden by <code>num_threads</code> and its aliases under <code>params</code>
if passed there.
</p>
<p><em>New in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="lightgbm_+3A_colnames">colnames</code></td>
<td>
<p>Character vector of features. Only used if <code>data</code> is not an <code><a href="#topic+lgb.Dataset">lgb.Dataset</a></code>.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_categorical_feature">categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say &quot;the first and tenth columns&quot;).
Only used if <code>data</code> is not an <code><a href="#topic+lgb.Dataset">lgb.Dataset</a></code>.</p>
</td></tr>
<tr><td><code id="lightgbm_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+lgb.train">lgb.train</a></code>. For example
</p>

<ul>
<li><p><code>valids</code>: a list of <code>lgb.Dataset</code> objects, used for validation
</p>
</li>
<li><p><code>obj</code>: objective function, can be character or custom objective function. Examples include
<code>regression</code>, <code>regression_l1</code>, <code>huber</code>,
<code>binary</code>, <code>lambdarank</code>, <code>multiclass</code>, <code>multiclass</code>
</p>
</li>
<li><p><code>eval</code>: evaluation function, can be (a list of) character or custom eval function
</p>
</li>
<li><p><code>record</code>: Boolean, TRUE will record iteration message to <code>booster$record_evals</code>
</p>
</li>
<li><p><code>reset_data</code>: Boolean, setting it to TRUE (not the default value) will transform the booster model
into a predictor model which frees up memory and the original datasets
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained <code>lgb.Booster</code>
</p>


<h3>Early Stopping</h3>

<p>&quot;early stopping&quot; refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the &quot;first&quot; one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>

<hr>
<h2 id='predict.lgb.Booster'>Predict method for LightGBM model</h2><span id='topic+predict.lgb.Booster'></span>

<h3>Description</h3>

<p>Predicted values based on class <code>lgb.Booster</code>
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lgb.Booster'
predict(
  object,
  newdata,
  type = "response",
  start_iteration = NULL,
  num_iteration = NULL,
  header = FALSE,
  params = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lgb.Booster_+3A_object">object</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_newdata">newdata</code></td>
<td>
<p>a <code>matrix</code> object, a <code>dgCMatrix</code>, a <code>dgRMatrix</code> object, a <code>dsparseVector</code> object,
or a character representing a path to a text file (CSV, TSV, or LibSVM).
</p>
<p>For sparse inputs, if predictions are only going to be made for a single row, it will be faster to
use CSR format, in which case the data may be passed as either a single-row CSR matrix (class
<code>dgRMatrix</code> from package <code>Matrix</code>) or as a sparse numeric vector (class
<code>dsparseVector</code> from package <code>Matrix</code>).
</p>
<p>If single-row predictions are going to be performed frequently, it is recommended to
pre-configure the model object for fast single-row sparse predictions through function
<a href="#topic+lgb.configure_fast_predict">lgb.configure_fast_predict</a>.
</p>
<p><em>Changed from 'data', in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_type">type</code></td>
<td>
<p>Type of prediction to output. Allowed types are:</p>

<ul>
<li> <p><code>"response"</code>: will output the predicted score according to the objective function being
optimized (depending on the link function that the objective uses), after applying any necessary
transformations - for example, for <code>objective="binary"</code>, it will output class probabilities.
</p>
</li>
<li> <p><code>"class"</code>: for classification objectives, will output the class with the highest predicted
probability. For other objectives, will output the same as &quot;response&quot;. Note that <code>"class"</code> is
not a supported type for <a href="#topic+lgb.configure_fast_predict">lgb.configure_fast_predict</a> (see the documentation of that function
for more details).
</p>
</li>
<li> <p><code>"raw"</code>: will output the non-transformed numbers (sum of predictions from boosting iterations'
results) from which the &quot;response&quot; number is produced for a given objective function - for example,
for <code>objective="binary"</code>, this corresponds to log-odds. For many objectives such as
&quot;regression&quot;, since no transformation is applied, the output will be the same as for &quot;response&quot;.
</p>
</li>
<li> <p><code>"leaf"</code>: will output the index of the terminal node / leaf at which each observations falls
in each tree in the model, outputted as integers, with one column per tree.
</p>
</li>
<li> <p><code>"contrib"</code>: will return the per-feature contributions for each prediction, including an
intercept (each feature will produce one column).
</p>
</li></ul>

<p>Note that, if using custom objectives, types &quot;class&quot; and &quot;response&quot; will not be available and will
default towards using &quot;raw&quot; instead.
</p>
<p>If the model was fit through function <a href="#topic+lightgbm">lightgbm</a> and it was passed a factor as labels,
passing the prediction type through <code>params</code> instead of through this argument might
result in factor levels for classification objectives not being applied correctly to the
resulting output.
</p>
<p><em>New in version 4.0.0</em></p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_start_iteration">start_iteration</code></td>
<td>
<p>int or None, optional (default=None)
Start index of the iteration to predict.
If None or &lt;= 0, starts from the first iteration.</p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_num_iteration">num_iteration</code></td>
<td>
<p>int or None, optional (default=None)
Limit number of iterations in the prediction.
If None, if the best iteration exists and start_iteration is None or &lt;= 0, the
best iteration is used; otherwise, all iterations from start_iteration are used.
If &lt;= 0, all iterations from start_iteration are used (no limits).</p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_header">header</code></td>
<td>
<p>only used for prediction for text file. True if text file has header</p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_params">params</code></td>
<td>
<p>a list of additional named parameters. See
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#predict-parameters">
the &quot;Predict Parameters&quot; section of the documentation</a> for a list of parameters and
valid values. Where these conflict with the values of keyword arguments to this function,
the values in <code>params</code> take precedence.</p>
</td></tr>
<tr><td><code id="predict.lgb.Booster_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the model object has been configured for fast single-row predictions through
<a href="#topic+lgb.configure_fast_predict">lgb.configure_fast_predict</a>, this function will use the prediction parameters
that were configured for it - as such, extra prediction parameters should not be passed
here, otherwise the configuration will be ignored and the slow route will be taken.
</p>


<h3>Value</h3>

<p>For prediction types that are meant to always return one output per observation (e.g. when predicting
<code>type="response"</code> or <code>type="raw"</code> on a binary classification or regression objective), will
return a vector with one element per row in <code>newdata</code>.
</p>
<p>For prediction types that are meant to return more than one output per observation (e.g. when predicting
<code>type="response"</code> or <code>type="raw"</code> on a multi-class objective, or when predicting
<code>type="leaf"</code>, regardless of objective), will return a matrix with one row per observation in
<code>newdata</code> and one column per output.
</p>
<p>For <code>type="leaf"</code> predictions, will return a matrix with one row per observation in <code>newdata</code>
and one column per tree. Note that for multiclass objectives, LightGBM trains one tree per class at each
boosting iteration. That means that, for example, for a multiclass model with 3 classes, the leaf
predictions for the first class can be found in columns 1, 4, 7, 10, etc.
</p>
<p>For <code>type="contrib"</code>, will return a matrix of SHAP values with one row per observation in
<code>newdata</code> and columns corresponding to features. For regression, ranking, cross-entropy, and binary
classification objectives, this matrix contains one column per feature plus a final column containing the
Shapley base value. For multiclass objectives, this matrix will represent <code>num_classes</code> such matrices,
in the order &quot;feature contributions for first class, feature contributions for second class, feature
contributions for third class, etc.&quot;.
</p>
<p>If the model was fit through function <a href="#topic+lightgbm">lightgbm</a> and it was passed a factor as labels, predictions
returned from this function will retain the factor levels (either as values for <code>type="class"</code>, or
as column names for <code>type="response"</code> and <code>type="raw"</code> for multi-class objectives). Note that
passing the requested prediction type under <code>params</code> instead of through <code>type</code> might result in
the factor levels not being present in the output.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
data(agaricus.test, package = "lightgbm")
test &lt;- agaricus.test
dtest &lt;- lgb.Dataset.create.valid(dtrain, test$data, label = test$label)
params &lt;- list(
  objective = "regression"
  , metric = "l2"
  , min_data = 1L
  , learning_rate = 1.0
  , num_threads = 2L
)
valids &lt;- list(test = dtest)
model &lt;- lgb.train(
  params = params
  , data = dtrain
  , nrounds = 5L
  , valids = valids
)
preds &lt;- predict(model, test$data)

# pass other prediction parameters
preds &lt;- predict(
    model,
    test$data,
    params = list(
        predict_disable_shape_check = TRUE
   )
)

</code></pre>

<hr>
<h2 id='print.lgb.Booster'>Print method for LightGBM model</h2><span id='topic+print.lgb.Booster'></span>

<h3>Description</h3>

<p>Show summary information about a LightGBM model object (same as <code>summary</code>).
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lgb.Booster'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.lgb.Booster_+3A_x">x</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="print.lgb.Booster_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same input <code>x</code>, returned as invisible.
</p>

<hr>
<h2 id='set_field'>Set one attribute of a <code>lgb.Dataset</code> object</h2><span id='topic+set_field'></span><span id='topic+set_field.lgb.Dataset'></span>

<h3>Description</h3>

<p>Set one attribute of a <code>lgb.Dataset</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_field(dataset, field_name, data)

## S3 method for class 'lgb.Dataset'
set_field(dataset, field_name, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_field_+3A_dataset">dataset</code></td>
<td>
<p>Object of class <code>lgb.Dataset</code></p>
</td></tr>
<tr><td><code id="set_field_+3A_field_name">field_name</code></td>
<td>
<p>String with the name of the attribute to set. One of the following.
</p>

<ul>
<li> <p><code>label</code>: label lightgbm learns from ;
</p>
</li>
<li> <p><code>weight</code>: to do a weight rescale ;
</p>
</li>
<li><p><code>group</code>: used for learning-to-rank tasks. An integer vector describing how to
group rows together as ordered results from the same set of candidate results to be ranked.
For example, if you have a 100-document dataset with <code>group = c(10, 20, 40, 10, 10, 10)</code>,
that means that you have 6 groups, where the first 10 records are in the first group,
records 11-30 are in the second group, etc.
</p>
</li>
<li> <p><code>init_score</code>: initial score is the base prediction lightgbm will boost from.
</p>
</li></ul>
</td></tr>
<tr><td><code id="set_field_+3A_data">data</code></td>
<td>
<p>The data for the field. See examples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>lgb.Dataset</code> you passed in.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(agaricus.train, package = "lightgbm")
train &lt;- agaricus.train
dtrain &lt;- lgb.Dataset(train$data, label = train$label)
lgb.Dataset.construct(dtrain)

labels &lt;- lightgbm::get_field(dtrain, "label")
lightgbm::set_field(dtrain, "label", 1 - labels)

labels2 &lt;- lightgbm::get_field(dtrain, "label")
stopifnot(all.equal(labels2, 1 - labels))

</code></pre>

<hr>
<h2 id='setLGBMThreads'>Set maximum number of threads used by LightGBM</h2><span id='topic+setLGBMThreads'></span><span id='topic+setLGBMthreads'></span>

<h3>Description</h3>

<p>LightGBM attempts to speed up many operations by using multi-threading.
The number of threads used in those operations can be controlled via the
<code>num_threads</code> parameter passed through <code>params</code> to functions like
<a href="#topic+lgb.train">lgb.train</a> and <a href="#topic+lgb.Dataset">lgb.Dataset</a>. However, some operations (like materializing
a model from a text file) are done via code paths that don't explicitly accept thread-control
configuration.
</p>
<p>Use this function to set the maximum number of threads LightGBM will use for such operations.
</p>
<p>This function affects all LightGBM operations in the same process.
</p>
<p>So, for example, if you call <code>setLGBMthreads(4)</code>, no other multi-threaded LightGBM
operation in the same process will use more than 4 threads.
</p>
<p>Call <code>setLGBMthreads(-1)</code> to remove this limitation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setLGBMthreads(num_threads)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setLGBMThreads_+3A_num_threads">num_threads</code></td>
<td>
<p>maximum number of threads to be used by LightGBM in multi-threaded operations</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+getLGBMthreads">getLGBMthreads</a>
</p>

<hr>
<h2 id='summary.lgb.Booster'>Summary method for LightGBM model</h2><span id='topic+summary.lgb.Booster'></span>

<h3>Description</h3>

<p>Show summary information about a LightGBM model object (same as <code>print</code>).
</p>
<p><em>New in version 4.0.0</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lgb.Booster'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.lgb.Booster_+3A_object">object</code></td>
<td>
<p>Object of class <code>lgb.Booster</code></p>
</td></tr>
<tr><td><code id="summary.lgb.Booster_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same input <code>object</code>, returned as invisible.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
