<!DOCTYPE html><html><head><title>Help for package spectralGraphTopology</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {spectralGraphTopology}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#spectralGraphTopology-package'><p>Package spectralGraphTopology</p></a></li>
<li><a href='#A'><p>Computes the Adjacency linear operator which maps a vector of weights into</p>
a valid Adjacency matrix.</a></li>
<li><a href='#accuracy'><p>Computes the accuracy between two matrices</p></a></li>
<li><a href='#Astar'><p>Computes the Astar operator.</p></a></li>
<li><a href='#block_diag'><p>Constructs a block diagonal matrix from a list of square matrices</p></a></li>
<li><a href='#cluster_k_component_graph'><p>Cluster a k-component graph from data using the Constrained Laplacian Rank algorithm</p>
</p>
<p>Cluster a k-component graph on the basis of an observed data matrix.</p>
Check out https://mirca.github.io/spectralGraphTopology for code examples.</a></li>
<li><a href='#D'><p>Computes the degree operator from the vector of edge weights.</p></a></li>
<li><a href='#Dstar'><p>Computes the Dstar operator, i.e., the adjoint of the D operator.</p></a></li>
<li><a href='#fdr'><p>Computes the false discovery rate between two matrices</p></a></li>
<li><a href='#fscore'><p>Computes the fscore between two matrices</p></a></li>
<li><a href='#L'><p>Computes the Laplacian linear operator which maps a vector of weights into</p>
a valid Laplacian matrix.</a></li>
<li><a href='#learn_bipartite_graph'><p>Learn a bipartite graph</p>
</p>
<p>Learns a bipartite graph on the basis of an observed data matrix</p></a></li>
<li><a href='#learn_bipartite_k_component_graph'><p>Learns a bipartite k-component graph</p>
</p>
<p>Jointly learns the Laplacian and Adjacency matrices of a graph on the basis</p>
of an observed data matrix</a></li>
<li><a href='#learn_combinatorial_graph_laplacian'><p>Learn the Combinatorial Graph Laplacian from data</p>
</p>
<p>Learns a graph Laplacian matrix using the Combinatorial Graph Laplacian (CGL)</p>
algorithm proposed by Egilmez et. al. (2017)</a></li>
<li><a href='#learn_graph_sigrep'><p>Learn graphs from a smooth signal representation approach</p>
</p>
<p>This function learns a graph from a observed data matrix using the</p>
method proposed by Dong (2016).</a></li>
<li><a href='#learn_k_component_graph'><p>Learn the Laplacian matrix of a k-component graph</p>
</p>
<p>Learns a k-component graph on the basis of an observed data matrix.</p>
Check out https://mirca.github.io/spectralGraphTopology for code examples.</a></li>
<li><a href='#learn_laplacian_gle_admm'><p>Learn the weighted Laplacian matrix of a graph using the ADMM method</p></a></li>
<li><a href='#learn_laplacian_gle_mm'><p>Learn the weighted Laplacian matrix of a graph using the MM method</p></a></li>
<li><a href='#learn_smooth_approx_graph'><p>Learns a smooth approximated graph from an observed data matrix.</p>
Check out https://mirca.github.io/spectralGraphTopology for code examples.</a></li>
<li><a href='#learn_smooth_graph'><p>Learn a graph from smooth signals</p>
</p>
<p>This function learns a connected graph given an observed signal matrix</p>
using the method proposed by Kalofilias (2016).</a></li>
<li><a href='#Lstar'><p>Computes the Lstar operator.</p></a></li>
<li><a href='#npv'><p>Computes the negative predictive value between two matrices</p></a></li>
<li><a href='#recall'><p>Computes the recall between two matrices</p></a></li>
<li><a href='#relative_error'><p>Computes the relative error between the true and estimated matrices</p></a></li>
<li><a href='#specificity'><p>Computes the specificity between two matrices</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Learning Graphs from Data via Spectral Constraints</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-03-12</td>
</tr>
<tr>
<td>Description:</td>
<td>In the era of big data and hyperconnectivity, learning
    high-dimensional structures such as graphs from data has become a prominent
    task in machine learning and has found applications in many fields such as
    finance, health care, and networks. 'spectralGraphTopology' is an open source,
    documented, and well-tested R package for learning graphs from data. It
    provides implementations of state of the art algorithms such as Combinatorial
    Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation
    based on Majorization-Minimization (GLE-MM), and Graph Estimation based on
    Alternating Direction Method of Multipliers (GLE-ADMM). In addition, graph
    learning has been widely employed for clustering, where specific algorithms
    are available in the literature. To this end, we provide an implementation of
    the Constrained Laplacian Rank (CLR) algorithm.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ze Vinicius &lt;jvmirca@gmail.com&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/dppalomar/spectralGraphTopology">https://github.com/dppalomar/spectralGraphTopology</a>,
<a href="https://mirca.github.io/spectralGraphTopology/">https://mirca.github.io/spectralGraphTopology/</a>,
<a href="https://www.danielppalomar.com">https://www.danielppalomar.com</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dppalomar/spectralGraphTopology/issues">https://github.com/dppalomar/spectralGraphTopology/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppEigen</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0), MASS, Matrix, progress, rlist</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>CVXR, bookdown, knitr, prettydoc, rmarkdown, R.rsp, testthat,
patrick, corrplot, igraph, kernlab, pals, clusterSim, viridis,
quadprog, matrixcalc</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>CVXR, knitr, rmarkdown, R.rsp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-12 01:00:46 UTC; mirca</td>
</tr>
<tr>
<td>Author:</td>
<td>Ze Vinicius [cre, aut],
  Daniel P. Palomar [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-14 09:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='spectralGraphTopology-package'>Package spectralGraphTopology</h2><span id='topic+spectralGraphTopology-package'></span>

<h3>Description</h3>

<p>This package provides estimators to learn k-component, bipartite,
and k-component bipartite graphs from data by imposing spectral constraints
on the eigenvalues and eigenvectors of the Laplacian and adjacency matrices.
Those estimators leverages spectral properties of the graphical models as a
prior information, which turn out to play key roles in unsupervised machine
learning tasks such as community detection.
</p>


<h3>Functions</h3>

<p><code><a href="#topic+learn_k_component_graph">learn_k_component_graph</a></code>
<code><a href="#topic+learn_bipartite_graph">learn_bipartite_graph</a></code>
<code><a href="#topic+learn_bipartite_k_component_graph">learn_bipartite_k_component_graph</a></code>
<code><a href="#topic+cluster_k_component_graph">cluster_k_component_graph</a></code>
<code><a href="#topic+learn_laplacian_gle_mm">learn_laplacian_gle_mm</a></code>
<code><a href="#topic+learn_laplacian_gle_admm">learn_laplacian_gle_admm</a></code>
<code><a href="#topic+L">L</a></code>
<code><a href="#topic+A">A</a></code>
</p>


<h3>Help</h3>

<p>For a quick help see the README file:
<a href="https://github.com/dppalomar/spectralGraphTopology/blob/master/README.md">GitHub-README</a>.
</p>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel P. Palomar
</p>


<h3>References</h3>

<p>S. Kumar, J. Ying, J. V. de Miranda Cardoso, and D. P. Palomar (2019).
&lt;https://arxiv.org/abs/1904.09792&gt;
</p>
<p>N., Feiping, W., Xiaoqian, J., Michael I., and H., Heng. (2016).
The Constrained Laplacian Rank Algorithm for Graph-based Clustering,
AAAI'16. &lt;http://dl.acm.org/citation.cfm?id=3016100.3016174&gt;
</p>
<p>Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P. Palomar. Optimization
Algorithms for Graph Laplacian Estimation via ADMM and MM IEEE Trans. on Signal
Processing, vol. 67, no. 16, pp. 4231-4244, Aug. 2019
</p>

<hr>
<h2 id='A'>Computes the Adjacency linear operator which maps a vector of weights into
a valid Adjacency matrix.</h2><span id='topic+A'></span>

<h3>Description</h3>

<p>Computes the Adjacency linear operator which maps a vector of weights into
a valid Adjacency matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>A(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="A_+3A_w">w</code></td>
<td>
<p>weight vector of the graph</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Aw the Adjacency matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
Aw &lt;- A(c(1, 0, 1))
Aw
</code></pre>

<hr>
<h2 id='accuracy'>Computes the accuracy between two matrices</h2><span id='topic+accuracy'></span>

<h3>Description</h3>

<p>Computes the accuracy between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracy_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="accuracy_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="accuracy_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
accuracy(X, X)
</code></pre>

<hr>
<h2 id='Astar'>Computes the Astar operator.</h2><span id='topic+Astar'></span>

<h3>Description</h3>

<p>Computes the Astar operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Astar(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Astar_+3A_m">M</code></td>
<td>
<p>matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>w vector
</p>

<hr>
<h2 id='block_diag'>Constructs a block diagonal matrix from a list of square matrices</h2><span id='topic+block_diag'></span>

<h3>Description</h3>

<p>Constructs a block diagonal matrix from a list of square matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>block_diag(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="block_diag_+3A_...">...</code></td>
<td>
<p>list of matrices or individual matrices</p>
</td></tr>
</table>


<h3>Value</h3>

<p>block diagonal matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
Y &lt;- L(c(1, 0, 1, 0, 0, 1))
B &lt;- block_diag(X, Y)
B
</code></pre>

<hr>
<h2 id='cluster_k_component_graph'>Cluster a k-component graph from data using the Constrained Laplacian Rank algorithm
Cluster a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.</h2><span id='topic+cluster_k_component_graph'></span>

<h3>Description</h3>

<p>Cluster a k-component graph from data using the Constrained Laplacian Rank algorithm
</p>
<p>Cluster a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_k_component_graph(
  Y,
  k = 1,
  m = 5,
  lmd = 1,
  eigtol = 1e-09,
  edgetol = 1e-06,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_k_component_graph_+3A_y">Y</code></td>
<td>
<p>a pxn data matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_k">k</code></td>
<td>
<p>the number of components of the graph</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_m">m</code></td>
<td>
<p>the maximum number of possible connections for a given node used
to build an affinity matrix</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_lmd">lmd</code></td>
<td>
<p>L2-norm regularization hyperparameter</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_eigtol">eigtol</code></td>
<td>
<p>value below which eigenvalues are considered to be zero</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_edgetol">edgetol</code></td>
<td>
<p>value below which edge weights are considered to be zero</p>
</td></tr>
<tr><td><code id="cluster_k_component_graph_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>eigvals</code></td>
<td>
<p>the eigenvalues of the Laplacian Matrix</p>
</td></tr>
<tr><td><code>lmd_seq</code></td>
<td>
<p>sequence of lmd values at every iteration</p>
</td></tr>
<tr><td><code>elapsed_time</code></td>
<td>
<p>elapsed time at every iteration</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>Nie, Feiping and Wang, Xiaoqian and Jordan, Michael I. and Huang, Heng.
The Constrained Laplacian Rank Algorithm for Graph-based Clustering, 2016,
AAAI'16. http://dl.acm.org/citation.cfm?id=3016100.3016174
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(clusterSim)
library(spectralGraphTopology)
library(igraph)
set.seed(1)
# number of nodes per cluster
N &lt;- 30
# generate datapoints
twomoon &lt;- shapes.two.moon(N)
# estimate underlying graph
graph &lt;- cluster_k_component_graph(twomoon$data, k = 2)
# build network
net &lt;- graph_from_adjacency_matrix(graph$adjacency, mode = "undirected", weighted = TRUE)
# colorify nodes and edges
colors &lt;- c("#706FD3", "#FF5252", "#33D9B2")
V(net)$cluster &lt;- twomoon$clusters
E(net)$color &lt;- apply(as.data.frame(get.edgelist(net)), 1,
                      function(x) ifelse(V(net)$cluster[x[1]] == V(net)$cluster[x[2]],
                                        colors[V(net)$cluster[x[1]]], '#000000'))
V(net)$color &lt;- c(colors[1], colors[2])[twomoon$clusters]
# plot network
plot(net, layout = twomoon$data, vertex.label = NA, vertex.size = 3)
</code></pre>

<hr>
<h2 id='D'>Computes the degree operator from the vector of edge weights.</h2><span id='topic+D'></span>

<h3>Description</h3>

<p>Computes the degree operator from the vector of edge weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>D(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="D_+3A_w">w</code></td>
<td>
<p>vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dw vector
</p>

<hr>
<h2 id='Dstar'>Computes the Dstar operator, i.e., the adjoint of the D operator.</h2><span id='topic+Dstar'></span>

<h3>Description</h3>

<p>Computes the Dstar operator, i.e., the adjoint of the D operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dstar(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dstar_+3A_w">w</code></td>
<td>
<p>vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dstar(w) vector
</p>

<hr>
<h2 id='fdr'>Computes the false discovery rate between two matrices</h2><span id='topic+fdr'></span>

<h3>Description</h3>

<p>Computes the false discovery rate between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fdr(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fdr_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="fdr_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="fdr_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
fdr(X, X)
</code></pre>

<hr>
<h2 id='fscore'>Computes the fscore between two matrices</h2><span id='topic+fscore'></span>

<h3>Description</h3>

<p>Computes the fscore between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fscore(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fscore_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="fscore_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="fscore_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
fscore(X, X)
</code></pre>

<hr>
<h2 id='L'>Computes the Laplacian linear operator which maps a vector of weights into
a valid Laplacian matrix.</h2><span id='topic+L'></span>

<h3>Description</h3>

<p>Computes the Laplacian linear operator which maps a vector of weights into
a valid Laplacian matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="L_+3A_w">w</code></td>
<td>
<p>weight vector of the graph</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Lw the Laplacian matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
Lw &lt;- L(c(1, 0, 1))
Lw
</code></pre>

<hr>
<h2 id='learn_bipartite_graph'>Learn a bipartite graph
Learns a bipartite graph on the basis of an observed data matrix</h2><span id='topic+learn_bipartite_graph'></span>

<h3>Description</h3>

<p>Learn a bipartite graph
</p>
<p>Learns a bipartite graph on the basis of an observed data matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_bipartite_graph(
  S,
  is_data_matrix = FALSE,
  z = 0,
  nu = 10000,
  alpha = 0,
  w0 = "naive",
  m = 7,
  maxiter = 10000,
  abstol = 1e-06,
  reltol = 1e-04,
  record_weights = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_bipartite_graph_+3A_s">S</code></td>
<td>
<p>either a pxp sample covariance/correlation matrix, or a pxn data
matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_is_data_matrix">is_data_matrix</code></td>
<td>
<p>whether the matrix S should be treated as data matrix
or sample covariance matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_z">z</code></td>
<td>
<p>the number of zero eigenvalues for the Adjancecy matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_nu">nu</code></td>
<td>
<p>regularization hyperparameter for the term ||A(w) - V Psi V'||^2_F</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_w0">w0</code></td>
<td>
<p>initial estimate for the weight vector the graph or a string
selecting an appropriate method. Available methods are: &quot;qp&quot;: finds w0 that minimizes
||ginv(S) - L(w0)||_F, w0 &gt;= 0; &quot;naive&quot;: takes w0 as the negative of the
off-diagonal elements of the pseudo inverse, setting to 0 any elements s.t.
w0 &lt; 0</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_m">m</code></td>
<td>
<p>in case is_data_matrix = TRUE, then we build an affinity matrix based
on Nie et. al. 2017, where m is the maximum number of possible connections
for a given node</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_record_weights">record_weights</code></td>
<td>
<p>whether to record the edge values at each iteration</p>
</td></tr>
<tr><td><code id="learn_bipartite_graph_+3A_verbose">verbose</code></td>
<td>
<p>whether to output a progress bar showing the evolution of the
iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the estimated weight vector</p>
</td></tr>
<tr><td><code>psi</code></td>
<td>
<p>optimization variable accounting for the eigenvalues of the Adjacency matrix</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>eigenvectors of the estimated Adjacency matrix</p>
</td></tr>
<tr><td><code>elapsed_time</code></td>
<td>
<p>elapsed time recorded at every iteration</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>negloglike</code></td>
<td>
<p>values of the negative loglikelihood at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>w_seq</code></td>
<td>
<p>sequence of weight vectors at every iteration in case record_weights = TRUE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>S. Kumar, J. Ying, J. V. M. Cardoso, D. P. Palomar. A unified
framework for structured graph learning via spectral constraints.
Journal of Machine Learning Research, 2020.
http://jmlr.org/papers/v21/19-276.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
library(igraph)
library(viridis)
library(corrplot)
set.seed(42)
n1 &lt;- 10
n2 &lt;- 6
n &lt;- n1 + n2
pc &lt;- .9
bipartite &lt;- sample_bipartite(n1, n2, type="Gnp", p = pc, directed=FALSE)
# randomly assign edge weights to connected nodes
E(bipartite)$weight &lt;- runif(gsize(bipartite), min = 0, max = 1)
# get true Laplacian and Adjacency
Ltrue &lt;- as.matrix(laplacian_matrix(bipartite))
Atrue &lt;- diag(diag(Ltrue)) - Ltrue
# get samples
Y &lt;- MASS::mvrnorm(100 * n, rep(0, n), Sigma = MASS::ginv(Ltrue))
# compute sample covariance matrix
S &lt;- cov(Y)
# estimate Adjacency matrix
graph &lt;- learn_bipartite_graph(S, z = 4, verbose = FALSE)
graph$adjacency[graph$adjacency &lt; 1e-3] &lt;- 0
# Plot Adjacency matrices: true, noisy, and estimated
corrplot(Atrue / max(Atrue), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1.25)
corrplot(graph$adjacency / max(graph$adjacency), is.corr = FALSE,
         method = "square", addgrid.col = NA, tl.pos = "n", cl.cex = 1.25)
# build networks
estimated_bipartite &lt;- graph_from_adjacency_matrix(graph$adjacency,
                                                   mode = "undirected",
                                                   weighted = TRUE)
V(estimated_bipartite)$type &lt;- c(rep(0, 10), rep(1, 6))
la = layout_as_bipartite(estimated_bipartite)
colors &lt;- viridis(20, begin = 0, end = 1, direction = -1)
c_scale &lt;- colorRamp(colors)
E(estimated_bipartite)$color = apply(
  c_scale(E(estimated_bipartite)$weight / max(E(estimated_bipartite)$weight)), 1,
                          function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
E(bipartite)$color = apply(c_scale(E(bipartite)$weight / max(E(bipartite)$weight)), 1,
                      function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
la = la[, c(2, 1)]
# Plot networks: true and estimated
plot(bipartite, layout = la, vertex.color=c("red","black")[V(bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
plot(estimated_bipartite, layout = la,
     vertex.color=c("red","black")[V(estimated_bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(estimated_bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
</code></pre>

<hr>
<h2 id='learn_bipartite_k_component_graph'>Learns a bipartite k-component graph
Jointly learns the Laplacian and Adjacency matrices of a graph on the basis
of an observed data matrix</h2><span id='topic+learn_bipartite_k_component_graph'></span>

<h3>Description</h3>

<p>Learns a bipartite k-component graph
</p>
<p>Jointly learns the Laplacian and Adjacency matrices of a graph on the basis
of an observed data matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_bipartite_k_component_graph(
  S,
  is_data_matrix = FALSE,
  z = 0,
  k = 1,
  w0 = "naive",
  m = 7,
  alpha = 0,
  beta = 10000,
  rho = 0.01,
  fix_beta = TRUE,
  beta_max = 1e+06,
  nu = 10000,
  lb = 0,
  ub = 10000,
  maxiter = 10000,
  abstol = 1e-06,
  reltol = 1e-04,
  eigtol = 1e-09,
  record_weights = FALSE,
  record_objective = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_s">S</code></td>
<td>
<p>either a pxp sample covariance/correlation matrix, or a pxn data
matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_is_data_matrix">is_data_matrix</code></td>
<td>
<p>whether the matrix S should be treated as data matrix
or sample covariance matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_z">z</code></td>
<td>
<p>the number of zero eigenvalues for the Adjancecy matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_k">k</code></td>
<td>
<p>the number of components of the graph</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_w0">w0</code></td>
<td>
<p>initial estimate for the weight vector the graph or a string
selecting an appropriate method. Available methods are: &quot;qp&quot;: finds w0 that minimizes
||ginv(S) - L(w0)||_F, w0 &gt;= 0; &quot;naive&quot;: takes w0 as the negative of the
off-diagonal elements of the pseudo inverse, setting to 0 any elements s.t.
w0 &lt; 0</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_m">m</code></td>
<td>
<p>in case is_data_matrix = TRUE, then we build an affinity matrix based
on Nie et. al. 2017, where m is the maximum number of possible connections
for a given node</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_beta">beta</code></td>
<td>
<p>regularization hyperparameter for the term ||L(w) - U Lambda U'||^2_F</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_rho">rho</code></td>
<td>
<p>how much to increase (decrease) beta in case fix_beta = FALSE</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_fix_beta">fix_beta</code></td>
<td>
<p>whether or not to fix the value of beta. In case this parameter
is set to false, then beta will increase (decrease) depending whether the number of
zero eigenvalues is lesser (greater) than k</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_beta_max">beta_max</code></td>
<td>
<p>maximum allowed value for beta</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_nu">nu</code></td>
<td>
<p>regularization hyperparameter for the term ||A(w) - V Psi V'||^2_F</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_lb">lb</code></td>
<td>
<p>lower bound for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_ub">ub</code></td>
<td>
<p>upper bound for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_eigtol">eigtol</code></td>
<td>
<p>value below which eigenvalues are considered to be zero</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_record_weights">record_weights</code></td>
<td>
<p>whether to record the edge values at each iteration</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_record_objective">record_objective</code></td>
<td>
<p>whether to record the objective function values at
each iteration</p>
</td></tr>
<tr><td><code id="learn_bipartite_k_component_graph_+3A_verbose">verbose</code></td>
<td>
<p>whether to output a progress bar showing the evolution of the
iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the estimated weight vector</p>
</td></tr>
<tr><td><code>psi</code></td>
<td>
<p>optimization variable accounting for the eigenvalues of the Adjacency matrix</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>optimization variable accounting for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>eigenvectors of the estimated Adjacency matrix</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>eigenvectors of the estimated Laplacian matrix</p>
</td></tr>
<tr><td><code>elapsed_time</code></td>
<td>
<p>elapsed time recorded at every iteration</p>
</td></tr>
<tr><td><code>beta_seq</code></td>
<td>
<p>sequence of values taken by beta in case fix_beta = FALSE</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>negloglike</code></td>
<td>
<p>values of the negative loglikelihood at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>w_seq</code></td>
<td>
<p>sequence of weight vectors at every iteration in case record_weights = TRUE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>S. Kumar, J. Ying, J. V. M. Cardoso, D. P. Palomar. A unified
framework for structured graph learning via spectral constraints.
Journal of Machine Learning Research, 2020.
http://jmlr.org/papers/v21/19-276.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
library(igraph)
library(viridis)
library(corrplot)
set.seed(42)
w &lt;- c(1, 0, 0, 1, 0, 1) * runif(6)
Laplacian &lt;- block_diag(L(w), L(w))
Atrue &lt;- diag(diag(Laplacian)) - Laplacian
bipartite &lt;- graph_from_adjacency_matrix(Atrue, mode = "undirected", weighted = TRUE)
n &lt;- ncol(Laplacian)
Y &lt;- MASS::mvrnorm(40 * n, rep(0, n), MASS::ginv(Laplacian))
graph &lt;- learn_bipartite_k_component_graph(cov(Y), k = 2, beta = 1e2, nu = 1e2, verbose = FALSE)
graph$adjacency[graph$adjacency &lt; 1e-2] &lt;- 0
# Plot Adjacency matrices: true, noisy, and estimated
corrplot(Atrue / max(Atrue), is.corr = FALSE, method = "square", addgrid.col = NA, tl.pos = "n",
         cl.cex = 1.25)
corrplot(graph$adjacency / max(graph$adjacency), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1.25)
# Plot networks
estimated_bipartite &lt;- graph_from_adjacency_matrix(graph$adjacency, mode = "undirected",
                                                   weighted = TRUE)
V(bipartite)$type &lt;- rep(c(TRUE, FALSE), 4)
V(estimated_bipartite)$type &lt;- rep(c(TRUE, FALSE), 4)
la = layout_as_bipartite(estimated_bipartite)
colors &lt;- viridis(20, begin = 0, end = 1, direction = -1)
c_scale &lt;- colorRamp(colors)
E(estimated_bipartite)$color = apply(
               c_scale(E(estimated_bipartite)$weight / max(E(estimated_bipartite)$weight)), 1,
                                     function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
E(bipartite)$color = apply(c_scale(E(bipartite)$weight / max(E(bipartite)$weight)), 1,
                           function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
la = la[, c(2, 1)]
# Plot networks: true and estimated
plot(bipartite, layout = la,
     vertex.color = c("red","black")[V(bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
plot(estimated_bipartite, layout = la,
     vertex.color = c("red","black")[V(estimated_bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(estimated_bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
</code></pre>

<hr>
<h2 id='learn_combinatorial_graph_laplacian'>Learn the Combinatorial Graph Laplacian from data
Learns a graph Laplacian matrix using the Combinatorial Graph Laplacian (CGL)
algorithm proposed by Egilmez et. al. (2017)</h2><span id='topic+learn_combinatorial_graph_laplacian'></span>

<h3>Description</h3>

<p>Learn the Combinatorial Graph Laplacian from data
</p>
<p>Learns a graph Laplacian matrix using the Combinatorial Graph Laplacian (CGL)
algorithm proposed by Egilmez et. al. (2017)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_combinatorial_graph_laplacian(
  S,
  A_mask = NULL,
  alpha = 0,
  reltol = 1e-05,
  max_cycle = 10000,
  regtype = 1,
  record_objective = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_s">S</code></td>
<td>
<p>sample covariance matrix</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_a_mask">A_mask</code></td>
<td>
<p>binary adjacency matrix of the graph</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_alpha">alpha</code></td>
<td>
<p>L1-norm regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_reltol">reltol</code></td>
<td>
<p>minimum relative error considered for the stopping criteri</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_max_cycle">max_cycle</code></td>
<td>
<p>maximum number of cycles</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_regtype">regtype</code></td>
<td>
<p>type of L1-norm regularization. If reg_type == 1, then all
elements of the Laplacian matrix will be regularized. If reg_type == 2,
only the off-diagonal elements will be regularized</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_record_objective">record_objective</code></td>
<td>
<p>whether or not to record the objective function value
at every iteration. Default is FALSE</p>
</td></tr>
<tr><td><code id="learn_combinatorial_graph_laplacian_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, then a progress bar will be displayed in the console. Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>elapsed_time</code></td>
<td>
<p>elapsed time recorded at every iteration</p>
</td></tr>
<tr><td><code>frod_norm</code></td>
<td>
<p>relative Frobenius norm between consecutive estimates of the Laplacian matrix</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>whether or not the algorithm has converged within the tolerance and max number of iterations</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>objective function value at every iteration, in case record_objective = TRUE</p>
</td></tr>
</table>


<h3>References</h3>

<p>H. E. Egilmez, E. Pavez and A. Ortega, &quot;Graph Learning From Data
Under Laplacian and Structural Constraints&quot;, in IEEE Journal of
Selected Topics in Signal Processing, vol. 11, no. 6, pp. 825-841, Sept. 2017.
Original MATLAB source code is available at: https://github.com/STAC-USC/Graph_Learning
</p>

<hr>
<h2 id='learn_graph_sigrep'>Learn graphs from a smooth signal representation approach
This function learns a graph from a observed data matrix using the
method proposed by Dong (2016).</h2><span id='topic+learn_graph_sigrep'></span>

<h3>Description</h3>

<p>Learn graphs from a smooth signal representation approach
</p>
<p>This function learns a graph from a observed data matrix using the
method proposed by Dong (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_graph_sigrep(
  X,
  alpha = 0.001,
  beta = 0.5,
  maxiter = 1000,
  ftol = 1e-04,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_graph_sigrep_+3A_x">X</code></td>
<td>
<p>a p-by-n data matrix, where p is the number of nodes and n is the
number of observations</p>
</td></tr>
<tr><td><code id="learn_graph_sigrep_+3A_alpha">alpha</code></td>
<td>
<p>hyperparameter that controls the importance of the Dirichlet
energy penalty</p>
</td></tr>
<tr><td><code id="learn_graph_sigrep_+3A_beta">beta</code></td>
<td>
<p>hyperparameter that controls the importance of the L2-norm
regularization</p>
</td></tr>
<tr><td><code id="learn_graph_sigrep_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_graph_sigrep_+3A_ftol">ftol</code></td>
<td>
<p>relative error on the objective function to be used as the
stopping criteria</p>
</td></tr>
<tr><td><code id="learn_graph_sigrep_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, then a progress bar will be displayed in the console. Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a smoothed approximation of the data matrix X</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>whether or not the algorithm has converged within the tolerance and max number of iterations</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>objective function value at every iteration, in case record_objective = TRUE</p>
</td></tr>
</table>


<h3>References</h3>

<p>X. Dong, D. Thanou, P. Frossard and P. Vandergheynst, &quot;Learning
Laplacian Matrix in Smooth Graph Signal Representations,&quot;
in IEEE Transactions on Signal Processing, vol. 64, no. 23,
pp. 6160-6173, Dec.1, 2016.
</p>

<hr>
<h2 id='learn_k_component_graph'>Learn the Laplacian matrix of a k-component graph
Learns a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.</h2><span id='topic+learn_k_component_graph'></span>

<h3>Description</h3>

<p>Learn the Laplacian matrix of a k-component graph
</p>
<p>Learns a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_k_component_graph(
  S,
  is_data_matrix = FALSE,
  k = 1,
  w0 = "naive",
  lb = 0,
  ub = 10000,
  alpha = 0,
  beta = 10000,
  beta_max = 1e+06,
  fix_beta = TRUE,
  rho = 0.01,
  m = 7,
  eps = 1e-04,
  maxiter = 10000,
  abstol = 1e-06,
  reltol = 1e-04,
  eigtol = 1e-09,
  record_objective = FALSE,
  record_weights = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_k_component_graph_+3A_s">S</code></td>
<td>
<p>either a pxp sample covariance/correlation matrix, or a pxn data
matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_is_data_matrix">is_data_matrix</code></td>
<td>
<p>whether the matrix S should be treated as data matrix
or sample covariance matrix</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_k">k</code></td>
<td>
<p>the number of components of the graph</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_w0">w0</code></td>
<td>
<p>initial estimate for the weight vector the graph or a string
selecting an appropriate method. Available methods are: &quot;qp&quot;: finds w0 that minimizes
||ginv(S) - L(w0)||_F, w0 &gt;= 0; &quot;naive&quot;: takes w0 as the negative of the
off-diagonal elements of the pseudo inverse, setting to 0 any elements s.t.
w0 &lt; 0</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_lb">lb</code></td>
<td>
<p>lower bound for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_ub">ub</code></td>
<td>
<p>upper bound for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_alpha">alpha</code></td>
<td>
<p>reweighted l1-norm regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_beta">beta</code></td>
<td>
<p>regularization hyperparameter for the term ||L(w) - U Lambda U'||^2_F</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_beta_max">beta_max</code></td>
<td>
<p>maximum allowed value for beta</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_fix_beta">fix_beta</code></td>
<td>
<p>whether or not to fix the value of beta. In case this parameter
is set to false, then beta will increase (decrease) depending whether the number of
zero eigenvalues is lesser (greater) than k</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_rho">rho</code></td>
<td>
<p>how much to increase (decrease) beta in case fix_beta = FALSE</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_m">m</code></td>
<td>
<p>in case is_data_matrix = TRUE, then we build an affinity matrix based
on Nie et. al. 2017, where m is the maximum number of possible connections
for a given node</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_eps">eps</code></td>
<td>
<p>small positive constant</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_eigtol">eigtol</code></td>
<td>
<p>value below which eigenvalues are considered to be zero</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_record_objective">record_objective</code></td>
<td>
<p>whether to record the objective function values at
each iteration</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_record_weights">record_weights</code></td>
<td>
<p>whether to record the edge values at each iteration</p>
</td></tr>
<tr><td><code id="learn_k_component_graph_+3A_verbose">verbose</code></td>
<td>
<p>whether to output a progress bar showing the evolution of the
iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the estimated weight vector</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>optimization variable accounting for the eigenvalues of the Laplacian matrix</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>eigenvectors of the estimated Laplacian matrix</p>
</td></tr>
<tr><td><code>elapsed_time</code></td>
<td>
<p>elapsed time recorded at every iteration</p>
</td></tr>
<tr><td><code>beta_seq</code></td>
<td>
<p>sequence of values taken by beta in case fix_beta = FALSE</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>negloglike</code></td>
<td>
<p>values of the negative loglikelihood at every iteration in case record_objective = TRUE</p>
</td></tr>
<tr><td><code>w_seq</code></td>
<td>
<p>sequence of weight vectors at every iteration in case record_weights = TRUE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>S. Kumar, J. Ying, J. V. M. Cardoso, D. P. Palomar. A unified
framework for structured graph learning via spectral constraints.
Journal of Machine Learning Research, 2020.
http://jmlr.org/papers/v21/19-276.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'># design true Laplacian
Laplacian &lt;- rbind(c(1, -1, 0, 0),
                   c(-1, 1, 0, 0),
                   c(0, 0, 1, -1),
                   c(0, 0, -1, 1))
n &lt;- ncol(Laplacian)
# sample data from multivariate Gaussian
Y &lt;- MASS::mvrnorm(n * 500, rep(0, n), MASS::ginv(Laplacian))
# estimate graph on the basis of sampled data
graph &lt;- learn_k_component_graph(cov(Y), k = 2, beta = 10)
graph$laplacian
</code></pre>

<hr>
<h2 id='learn_laplacian_gle_admm'>Learn the weighted Laplacian matrix of a graph using the ADMM method</h2><span id='topic+learn_laplacian_gle_admm'></span>

<h3>Description</h3>

<p>Learn the weighted Laplacian matrix of a graph using the ADMM method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_laplacian_gle_admm(
  S,
  A_mask = NULL,
  alpha = 0,
  rho = 1,
  maxiter = 10000,
  reltol = 1e-05,
  record_objective = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_laplacian_gle_admm_+3A_s">S</code></td>
<td>
<p>a pxp sample covariance/correlation matrix</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_a_mask">A_mask</code></td>
<td>
<p>the binary adjacency matrix of the graph</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_rho">rho</code></td>
<td>
<p>ADMM convergence rate hyperparameter</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance on the Laplacian matrix estimation</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_record_objective">record_objective</code></td>
<td>
<p>whether or not to record the objective function. Default is FALSE</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_admm_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, then a progress bar will be displayed in the console. Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr><td><code>Laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>Adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius, Jiaxi Ying, and Daniel Palomar
</p>


<h3>References</h3>

<p>Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P. Palomar.
Optimization Algorithms for Graph Laplacian Estimation via ADMM and MM.
IEEE Trans. on Signal Processing, vol. 67, no. 16, pp. 4231-4244, Aug. 2019
</p>

<hr>
<h2 id='learn_laplacian_gle_mm'>Learn the weighted Laplacian matrix of a graph using the MM method</h2><span id='topic+learn_laplacian_gle_mm'></span>

<h3>Description</h3>

<p>Learn the weighted Laplacian matrix of a graph using the MM method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_laplacian_gle_mm(
  S,
  A_mask = NULL,
  alpha = 0,
  maxiter = 10000,
  reltol = 1e-05,
  record_objective = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_laplacian_gle_mm_+3A_s">S</code></td>
<td>
<p>a pxp sample covariance/correlation matrix</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_a_mask">A_mask</code></td>
<td>
<p>the binary adjacency matrix of the graph</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization hyperparameter</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance on the weight vector w</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_record_objective">record_objective</code></td>
<td>
<p>whether or not to record the objective function. Default is FALSE</p>
</td></tr>
<tr><td><code id="learn_laplacian_gle_mm_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, then a progress bar will be displayed in the console. Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
<tr><td><code>Adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td></tr>
<tr><td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius, Jiaxi Ying, and Daniel Palomar
</p>


<h3>References</h3>

<p>Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P. Palomar.
Optimization Algorithms for Graph Laplacian Estimation via ADMM and MM.
IEEE Trans. on Signal Processing, vol. 67, no. 16, pp. 4231-4244, Aug. 2019
</p>

<hr>
<h2 id='learn_smooth_approx_graph'>Learns a smooth approximated graph from an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.</h2><span id='topic+learn_smooth_approx_graph'></span>

<h3>Description</h3>

<p>Learns a smooth approximated graph from an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_smooth_approx_graph(Y, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_smooth_approx_graph_+3A_y">Y</code></td>
<td>
<p>a p-by-n data matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td></tr>
<tr><td><code id="learn_smooth_approx_graph_+3A_m">m</code></td>
<td>
<p>the maximum number of possible connections for a given node used
to build an affinity matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>
<table>
<tr><td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>Nie, Feiping and Wang, Xiaoqian and Jordan, Michael I. and Huang, Heng.
The Constrained Laplacian Rank Algorithm for Graph-based Clustering, 2016,
AAAI'16. http://dl.acm.org/citation.cfm?id=3016100.3016174
</p>

<hr>
<h2 id='learn_smooth_graph'>Learn a graph from smooth signals
This function learns a connected graph given an observed signal matrix
using the method proposed by Kalofilias (2016).</h2><span id='topic+learn_smooth_graph'></span>

<h3>Description</h3>

<p>Learn a graph from smooth signals
</p>
<p>This function learns a connected graph given an observed signal matrix
using the method proposed by Kalofilias (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_smooth_graph(
  X,
  alpha = 0.01,
  beta = 1e-04,
  step_size = 0.01,
  maxiter = 1000,
  tol = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learn_smooth_graph_+3A_x">X</code></td>
<td>
<p>a p-by-n data matrix, where p is the number of nodes and n is the
number of observations</p>
</td></tr>
<tr><td><code id="learn_smooth_graph_+3A_alpha">alpha</code></td>
<td>
<p>hyperparameter that controls the importance of the Dirichlet
energy penalty</p>
</td></tr>
<tr><td><code id="learn_smooth_graph_+3A_beta">beta</code></td>
<td>
<p>hyperparameter that controls the importance of the L2-norm
regularization</p>
</td></tr>
<tr><td><code id="learn_smooth_graph_+3A_step_size">step_size</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="learn_smooth_graph_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="learn_smooth_graph_+3A_tol">tol</code></td>
<td>
<p>relative tolerance used as stopping criteria</p>
</td></tr>
</table>


<h3>References</h3>

<p>V. Kalofolias, &quot;How to learn a graph from smooth signals&quot;, in Proc. Int.
Conf. Artif. Intell. Statist., 2016, pp. 920–929.
</p>

<hr>
<h2 id='Lstar'>Computes the Lstar operator.</h2><span id='topic+Lstar'></span>

<h3>Description</h3>

<p>Computes the Lstar operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lstar(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lstar_+3A_m">M</code></td>
<td>
<p>matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>w vector
</p>

<hr>
<h2 id='npv'>Computes the negative predictive value between two matrices</h2><span id='topic+npv'></span>

<h3>Description</h3>

<p>Computes the negative predictive value between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npv(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npv_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="npv_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="npv_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
npv(X, X)
</code></pre>

<hr>
<h2 id='recall'>Computes the recall between two matrices</h2><span id='topic+recall'></span>

<h3>Description</h3>

<p>Computes the recall between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recall(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recall_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="recall_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="recall_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
recall(X, X)
</code></pre>

<hr>
<h2 id='relative_error'>Computes the relative error between the true and estimated matrices</h2><span id='topic+relative_error'></span>

<h3>Description</h3>

<p>Computes the relative error between the true and estimated matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relative_error(West, Wtrue)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relative_error_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="relative_error_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
relative_error(X, X)
</code></pre>

<hr>
<h2 id='specificity'>Computes the specificity between two matrices</h2><span id='topic+specificity'></span>

<h3>Description</h3>

<p>Computes the specificity between two matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specificity(Wtrue, West, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="specificity_+3A_wtrue">Wtrue</code></td>
<td>
<p>true matrix</p>
</td></tr>
<tr><td><code id="specificity_+3A_west">West</code></td>
<td>
<p>estimated matrix</p>
</td></tr>
<tr><td><code id="specificity_+3A_eps">eps</code></td>
<td>
<p>real number such that edges whose values are smaller than eps are
not considered in the computation of the fscore</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(spectralGraphTopology)
X &lt;- L(c(1, 0, 1))
specificity(X, X)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
