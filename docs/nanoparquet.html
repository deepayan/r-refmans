<!DOCTYPE html><html lang="en-US"><head><title>Help for package nanoparquet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nanoparquet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#nanoparquet-package'><p>nanoparquet: Read and Write 'Parquet' Files</p></a></li>
<li><a href='#append_parquet'><p>Append a data frame to an existing Parquet file</p></a></li>
<li><a href='#infer_parquet_schema'><p>Infer Parquet schema of a data frame</p></a></li>
<li><a href='#nanoparquet-types'><p>nanoparquet's type maps</p></a></li>
<li><a href='#parquet_column_types'><p>Map between R and Parquet data types</p></a></li>
<li><a href='#parquet_options'><p>Nanoparquet options</p></a></li>
<li><a href='#parquet_schema'><p>Create a Parquet schema</p></a></li>
<li><a href='#parquet-encodings'><p>Parquet encodings</p></a></li>
<li><a href='#read_parquet'><p>Read a Parquet file into a data frame</p></a></li>
<li><a href='#read_parquet_info'><p>Short summary of a Parquet file</p></a></li>
<li><a href='#read_parquet_metadata'><p>Read the metadata of a Parquet file</p></a></li>
<li><a href='#read_parquet_page'><p>Read a page from a Parquet file</p></a></li>
<li><a href='#read_parquet_pages'><p>Metadata of all pages of a Parquet file</p></a></li>
<li><a href='#read_parquet_schema'><p>Read the schema of a Parquet file</p></a></li>
<li><a href='#rle_decode_int'><p>RLE decode integers</p></a></li>
<li><a href='#rle_encode_int'><p>RLE encode integers</p></a></li>
<li><a href='#write_parquet'><p>Write a data frame to a Parquet file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Read and Write 'Parquet' Files</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Self-sufficient reader and writer for flat 'Parquet' files.
    Can read most 'Parquet' data types. Can write many 'R' data types,
    including factors and temporal types. See docs for limitations.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/r-lib/nanoparquet">https://github.com/r-lib/nanoparquet</a>,
<a href="https://nanoparquet.r-lib.org/">https://nanoparquet.r-lib.org/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/r-lib/nanoparquet/issues">https://github.com/r-lib/nanoparquet/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arrow, bit64, DBI, duckdb, hms, mockery, pillar, processx,
rprojroot, spelling, testthat, tzdb, withr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2.9000</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate, r-lib/pkgdown, dplyr, gt,
gtExtras, knitr, nycflights13, prettyunits, quarto, rmarkdown,
sessioninfo, svglite</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-22 10:23:40 UTC; gaborcsardi</td>
</tr>
<tr>
<td>Author:</td>
<td>Gábor Csárdi [aut, cre],
  Hannes Mühleisen <a href="https://orcid.org/0000-0001-8552-0029"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph],
  Google Inc. [cph],
  Apache Software Foundation [cph],
  Posit Software, PBC [cph],
  RAD Game Tools [cph],
  Valve Software [cph],
  Tenacious Software LLC [cph],
  Facebook, Inc. [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gábor Csárdi &lt;csardi.gabor@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-22 10:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='nanoparquet-package'>nanoparquet: Read and Write 'Parquet' Files</h2><span id='topic+nanoparquet'></span><span id='topic+nanoparquet-package'></span>

<h3>Description</h3>

<p>Self-sufficient reader and writer for flat 'Parquet' files. Can read most 'Parquet' data types. Can write many 'R' data types, including factors and temporal types. See docs for limitations.
</p>


<h3>Details</h3>

<p><code>nanoparquet</code> is a reader and writer for a common subset of Parquet files.
</p>


<h4>Features:</h4>


<ul>
<li><p> Read and write flat (i.e. non-nested) Parquet files.
</p>
</li>
<li><p> Can read most <a href="https://nanoparquet.r-lib.org/reference/nanoparquet-types.html">Parquet data types</a>.
</p>
</li>
<li><p> Can read a subset of columns from a Parquet file.
</p>
</li>
<li><p> Can write many R data types, including factors and temporal types
to Parquet.
</p>
</li>
<li><p> Can append a data frame to a Parquet file without first reading and then
rewriting the whole file.
</p>
</li>
<li><p> Completely dependency free.
</p>
</li>
<li><p> Supports Snappy, Gzip and Zstd compression.
</p>
</li>
<li> <p><a href="https://nanoparquet.r-lib.org/dev/articles/benchmarks.html">Competitive</a> with other
tools in terms of speed, memory use and file size.
</p>
</li></ul>




<h4>Limitations:</h4>


<ul>
<li><p> Nested Parquet types are not supported.
</p>
</li>
<li><p> Some Parquet logical types are not supported: <code>INTERVAL</code>,
<code>UNKNOWN</code>.
</p>
</li>
<li><p> Only Snappy, Gzip and Zstd compression is supported.
</p>
</li>
<li><p> Encryption is not supported.
</p>
</li>
<li><p> Reading files from URLs is not supported.
</p>
</li>
<li><p> nanoparquet always reads the data (or the selected subset of it) into
memory. It does not work with out-of-memory data in Parquet files like
Apache Arrow and DuckDB does.
</p>
</li></ul>




<h4>Installation</h4>

<p>Install the R package from CRAN:
</p>
<div class="sourceCode r"><pre>install.packages("nanoparquet")
</pre></div>



<h4>Usage</h4>



<h5>Read</h5>

<p>Call <code>read_parquet()</code> to read a Parquet file:
</p>
<div class="sourceCode r"><pre>df &lt;- nanoparquet::read_parquet("example.parquet")
</pre></div>
<p>To see the columns of a Parquet file and how their types are mapped to
R types by <code>read_parquet()</code>, call <code>read_parquet_schema()</code> first:
</p>
<div class="sourceCode r"><pre>nanoparquet::read_parquet_schema("example.parquet")
</pre></div>
<p>Folders of similar-structured Parquet files (e.g. produced by Spark)
can be read like this:
</p>
<div class="sourceCode r"><pre>df &lt;- data.table::rbindlist(lapply(
  Sys.glob("some-folder/part-*.parquet"),
  nanoparquet::read_parquet
))
</pre></div>



<h5>Write</h5>

<p>Call <code>write_parquet()</code> to write a data frame to a Parquet file:
</p>
<div class="sourceCode r"><pre>nanoparquet::write_parquet(mtcars, "mtcars.parquet")
</pre></div>
<p>To see how the columns of the data frame will be mapped to Parquet types
by <code>write_parquet()</code>, call <code>infer_parquet_schema()</code> first:
</p>
<div class="sourceCode r"><pre>nanoparquet::infer_parquet_schema(mtcars)
</pre></div>



<h5>Inspect</h5>

<p>Call <code>read_parquet_info()</code>, <code>read_parquet_schema()</code>, or
<code>read_parquet_metadata()</code> to see various kinds of metadata from a Parquet
file:
</p>

<ul>
<li> <p><code>read_parquet_info()</code> shows a basic summary of the file.
</p>
</li>
<li> <p><code>read_parquet_schema()</code> shows all columns, including non-leaf columns,
and how they are mapped to R types by <code>read_parquet()</code>.
</p>
</li>
<li> <p><code>read_parquet_metadata()</code> shows the most complete metadata information:
file meta data, the schema, the row groups and column chunks of the
file.
</p>
</li></ul>

<div class="sourceCode r"><pre>nanoparquet::read_parquet_info("mtcars.parquet")
nanoparquet::read_parquet_schema("mtcars.parquet")
nanoparquet::read_parquet_metadata("mtcars.parquet")
</pre></div>
<p>If you find a file that should be supported but isn't, please open an
issue here with a link to the file.
</p>




<h4>Options</h4>

<p>See also <code>?parquet_options()</code> for further details.
</p>

<ul>
<li> <p><code>nanoparquet.class</code>: extra class to add to data frames returned by
<code>read_parquet()</code>. If it is not defined, the default is <code>"tbl"</code>,
which changes how the data frame is printed if the pillar package is
loaded.
</p>
</li>
<li> <p><code>nanoparquet.compression_level</code>: See <code>?parquet_options()</code> for the
defaults and the possible values for each compression method. <code>Inf</code>
selects maximum compression for each method.
</p>
</li>
<li> <p><code>nanoparquet.num_rows_per_row_group</code>: The number of rows to put into a
row group by <code>write_parquet()</code>, if row groups are not specified
explicitly. It should be an integer scalar. Defaults to 10 million.
</p>
</li>
<li> <p><code>nanoparquet.use_arrow_metadata</code>: unless this is set to <code>FALSE</code>,
<code>read_parquet()</code> will make use of Arrow metadata in the Parquet file.
Currently this is used to detect factor columns.
</p>
</li>
<li> <p><code>nanoparquet.write_arrow_metadata</code>: unless this is set to <code>FALSE</code>,
<code>write_parquet()</code> will add Arrow metadata to the Parquet file.
This helps preserving classes of columns, e.g. factors will be read
back as factors, both by nanoparquet and Arrow.
</p>
</li>
<li> <p><code>nanoparquet.write_data_page_version</code>: Data version to write by default.
Possible values are 1 and 2. Default is 1.
</p>
</li>
<li> <p><code>nanoparquet.write_minmax_values</code>: Whether to write minimum and maximum
values per row group, for data types that support this in
<code>write_parquet()</code>.
</p>
</li></ul>




<h4>License</h4>

<p>MIT
</p>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Gábor Csárdi <a href="mailto:csardi.gabor@gmail.com">csardi.gabor@gmail.com</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Hannes Mühleisen (<a href="https://orcid.org/0000-0001-8552-0029">ORCID</a>) [copyright holder]
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Google Inc. [copyright holder]
</p>
</li>
<li><p> Apache Software Foundation [copyright holder]
</p>
</li>
<li><p> Posit Software, PBC [copyright holder]
</p>
</li>
<li><p> RAD Game Tools [copyright holder]
</p>
</li>
<li><p> Valve Software [copyright holder]
</p>
</li>
<li><p> Tenacious Software LLC [copyright holder]
</p>
</li>
<li><p> Facebook, Inc. [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/r-lib/nanoparquet">https://github.com/r-lib/nanoparquet</a>
</p>
</li>
<li> <p><a href="https://nanoparquet.r-lib.org/">https://nanoparquet.r-lib.org/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/r-lib/nanoparquet/issues">https://github.com/r-lib/nanoparquet/issues</a>
</p>
</li></ul>


<hr>
<h2 id='append_parquet'>Append a data frame to an existing Parquet file</h2><span id='topic+append_parquet'></span>

<h3>Description</h3>

<p>The schema of the data frame must be compatible with the schema of
the file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>append_parquet(
  x,
  file,
  compression = c("snappy", "gzip", "zstd", "uncompressed"),
  encoding = NULL,
  row_groups = NULL,
  options = parquet_options()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="append_parquet_+3A_x">x</code></td>
<td>
<p>Data frame to append.</p>
</td></tr>
<tr><td><code id="append_parquet_+3A_file">file</code></td>
<td>
<p>Path to the output file.</p>
</td></tr>
<tr><td><code id="append_parquet_+3A_compression">compression</code></td>
<td>
<p>Compression algorithm to use for the newly written
data. See <code><a href="#topic+write_parquet">write_parquet()</a></code>.</p>
</td></tr>
<tr><td><code id="append_parquet_+3A_encoding">encoding</code></td>
<td>
<p>Encoding to use for the newly written data. It does not
have to be the same as the encoding of data in <code>file</code>. See
<code><a href="#topic+write_parquet">write_parquet()</a></code> for possible values.</p>
</td></tr>
<tr><td><code id="append_parquet_+3A_row_groups">row_groups</code></td>
<td>
<p>Row groups of the new, extended Parquet file.
<code><a href="#topic+append_parquet">append_parquet()</a></code> can only change the last existing row group, and
if <code>row_groups</code> is specified, it has respect this. I.e. if the
existing file has <code>n</code> rows, and the last row group starts at <code>k</code>
(<code>k &lt;= n</code>), then the first row group in <code>row_groups</code> that refers to
the new data must start at <code>k</code> or <code>n+1</code>.
(It is simpler to specify <code>num_rows_per_row_group</code> in <code>options</code>, see
<code><a href="#topic+parquet_options">parquet_options()</a></code> instead of <code>row_groups</code>. Only use <code>row_groups</code> if
you need complete control.)</p>
</td></tr>
<tr><td><code id="append_parquet_+3A_options">options</code></td>
<td>
<p>Nanoparquet options, for the new data, see
<code><a href="#topic+parquet_options">parquet_options()</a></code>. The <code>keep_row_groups</code> option also affects whether
<code>append_parquet()</code> overwrites existing row groups in <code>file</code>.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This function is <strong>not</strong> atomic! If it is interrupted, it may leave
the file in a corrupt state. To work around this create a copy of the
original file, append the new data to the copy, and then rename the
new, extended file to the original one.
</p>


<h3>About row groups</h3>

<p>A Parquet file may be partitioned into multiple row groups, and indeed
most large Parquet files are. <code>append_parquet()</code> is only able to update
the existing file along the row group boundaries. There are two
possibilities:
</p>

<ul>
<li> <p><code>append_parquet()</code> keeps all existing row groups in <code>file</code>, and
creates new row groups for the new data. This mode can be forced by
the <code>keep_row_groups</code> option in <code>options</code>, see <code><a href="#topic+parquet_options">parquet_options()</a></code>.
</p>
</li>
<li><p> Alternatively, <code>write_parquet</code> will overwrite the <em>last</em> row group in
file, with its existing contents plus the (beginning of) the new data.
This mode makes more sense if the last row group is small, because
many small row groups are inefficient.
</p>
</li></ul>

<p>By default <code>append_parquet</code> chooses between the two modes automatically,
aiming to create row groups with at least <code>num_rows_per_row_group</code>
(see <code><a href="#topic+parquet_options">parquet_options()</a></code>) rows. You can customize this behavior with
the <code>keep_row_groups</code> options and the <code>row_groups</code> argument.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+write_parquet">write_parquet()</a></code>.
</p>

<hr>
<h2 id='infer_parquet_schema'>Infer Parquet schema of a data frame</h2><span id='topic+infer_parquet_schema'></span>

<h3>Description</h3>

<p>Infer Parquet schema of a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_parquet_schema(df, options = parquet_options())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="infer_parquet_schema_+3A_df">df</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="infer_parquet_schema_+3A_options">options</code></td>
<td>
<p>Return value of <code><a href="#topic+parquet_options">parquet_options()</a></code>, may modify the
R to Parquet type mappings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame, the inferred schema. It has the same columns as
the return value of <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code>:
<code>file_name</code>, <code>name</code>, <code>r_type</code>, <code>type</code>, <code>type_length</code>, <code>repetition_type</code>, <code>converted_type</code>, <code>logical_type</code>, <code>num_children</code>, <code>scale</code>, <code>precision</code>, <code>field_id</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> to read the schema of a Parquet file,
<code><a href="#topic+parquet_schema">parquet_schema()</a></code> to create a Parquet schema from scratch.
</p>

<hr>
<h2 id='nanoparquet-types'>nanoparquet's type maps</h2><span id='topic+nanoparquet-types'></span>

<h3>Description</h3>

<p>How nanoparquet maps R types to Parquet types.
</p>


<h3>R's data types</h3>

<p>When writing out a data frame, nanoparquet maps R's data types to Parquet
logical types. The following table is a summary of the mapping. For the
details see below.</p>

<table>
<tr>
 <td style="text-align: left;">
   R type </td><td style="text-align: left;"> Parquet type </td><td style="text-align: center;"> Default </td><td style="text-align: left;"> Notes </td>
</tr>
<tr>
 <td style="text-align: left;">
   character </td><td style="text-align: left;"> STRING (BYTE_ARRAY) </td><td style="text-align: center;"> x </td><td style="text-align: left;"> I.e. STRSXP. Converted to UTF-8. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> BYTE_ARRAY </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> FIXED_LEN_BYTE_ARRAY </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> ENUM </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> UUID </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   Date </td><td style="text-align: left;"> DATE </td><td style="text-align: center;"> x </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   difftime </td><td style="text-align: left;"> INT64 </td><td style="text-align: center;"> x </td><td style="text-align: left;"> If not hms::hms. Arrow metadata marks it as Duration(NS). </td>
</tr>
<tr>
 <td style="text-align: left;">
   factor </td><td style="text-align: left;"> STRING </td><td style="text-align: center;"> x </td><td style="text-align: left;"> Arrow metadata marks it as a factor. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> ENUM </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   hms::hms </td><td style="text-align: left;"> TIME(true, MILLIS) </td><td style="text-align: center;"> x </td><td style="text-align: left;"> Sub-milliseconds precision is lost. </td>
</tr>
<tr>
 <td style="text-align: left;">
   integer </td><td style="text-align: left;"> INT(32, true) </td><td style="text-align: center;"> x </td><td style="text-align: left;"> I.e. INTSXP. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT64 </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT96 </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> DECIMAL (INT32) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> DECIMAL (INT64) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT(8, *) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT(16, *) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT(32, signed) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   list </td><td style="text-align: left;"> BYTE_ARRAY </td><td style="text-align: center;">  </td><td style="text-align: left;"> Must be a list of raw vectors. Messing values are <code>NULL</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> FIXED_LEN_BYTE_ARRAY </td><td style="text-align: center;">  </td><td style="text-align: left;"> Must be a list of raw vectors of the same length. Missing values are <code>NULL</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   logical </td><td style="text-align: left;"> BOOLEAN </td><td style="text-align: center;"> x </td><td style="text-align: left;"> I.e. LGLSXP. </td>
</tr>
<tr>
 <td style="text-align: left;">
   numeric </td><td style="text-align: left;"> DOUBLE </td><td style="text-align: center;"> x </td><td style="text-align: left;"> I.e. REALSXP. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT96 </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> FLOAT </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> DECIMAL (INT32) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> DECIMAL (INT64) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> INT(*, *) </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> FLOAT16 </td><td style="text-align: center;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   POSIXct </td><td style="text-align: left;"> TIMESTAMP(true, MICROS) </td><td style="text-align: center;"> x </td><td style="text-align: left;"> Sub-microsecond precision is lost. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The non-default mappings can be selected via the <code>schema</code> argument. E.g.
to write out a factor column called 'name' as <code>ENUM</code>, use
</p>
<div class="sourceCode r"><pre>write_parquet(..., schema = parquet_schema(name = "ENUM"))
</pre></div>
<p>The detailed mapping rules are listed below, in order of preference.
These rules will likely change until nanoparquet reaches version 1.0.0.
</p>

<ol>
<li><p> Factors (i.e. vectors that inherit the <em>factor</em> class) are converted
to character vectors using <code>as.character()</code>, then written as a
<code>STRSXP</code> (character vector) type. The fact that a column is a factor
is stored in the Arrow metadata (see below), unless the
<code>nanoparquet.write_arrow_metadata</code> option is set to <code>FALSE</code>.
</p>
</li>
<li><p> Dates (i.e. the <code>Date</code> class) is written as <code>DATE</code> logical type, which
is an <code>INT32</code> type internally.
</p>
</li>
<li> <p><code>hms</code> objects (from the hms package) are written as <code>TIME(true, MILLIS)</code>.
logical type, which is internally the <code>INT32</code> Parquet type.
Sub-milliseconds precision is lost.
</p>
</li>
<li> <p><code>POSIXct</code> objects are written as <code>TIMESTAMP(true, MICROS)</code> logical type,
which is internally the <code>INT64</code> Parquet type.
Sub-microsecond precision is lost.
</p>
</li>
<li> <p><code>difftime</code> objects (that are not <code>hms</code> objects, see above), are
written as an <code>INT64</code> Parquet type, and noting in the Arrow metadata
(see below) that this column has type <code>Duration</code> with <code>NANOSECONDS</code>
unit.
</p>
</li>
<li><p> Integer vectors (<code>INTSXP</code>) are written as <code>INT(32, true)</code> logical type,
which corresponds to the <code>INT32</code> type.
</p>
</li>
<li><p> Real vectors (<code>REALSXP</code>) are written as the <code>DOUBLE</code> type.
</p>
</li>
<li><p> Character vectors (<code>STRSXP</code>) are written as the <code>STRING</code> logical type,
which has the <code>BYTE_ARRAY</code> type. They are always converted to UTF-8
before writing.
</p>
</li>
<li><p> Logical vectors (<code>LGLSXP</code>) are written as the <code>BOOLEAN</code> type.
</p>
</li>
<li><p> Other vectors error currently.
</p>
</li></ol>

<p>You can use <code><a href="#topic+infer_parquet_schema">infer_parquet_schema()</a></code> on a data frame to map R data types
to Parquet data types.
</p>
<p>To change the default R to Parquet mapping, use <code><a href="#topic+parquet_schema">parquet_schema()</a></code> and
the <code>schema</code> argument of <code><a href="#topic+write_parquet">write_parquet()</a></code>. Currently supported
non-default mappings are:
</p>

<ul>
<li> <p><code>integer</code> to <code>INT64</code>,
</p>
</li>
<li> <p><code>integer</code> to <code>INT96</code>,
</p>
</li>
<li> <p><code>double</code> to <code>INT96</code>,
</p>
</li>
<li> <p><code>double</code> to <code>FLOAT</code>,
</p>
</li>
<li> <p><code>character</code> to <code>BYTE_ARRAY</code>,
</p>
</li>
<li> <p><code>character</code> to <code>FIXED_LEN_BYTE_ARRAY</code>,
</p>
</li>
<li> <p><code>character</code> to <code>ENUM</code>,
</p>
</li>
<li> <p><code>factor</code> to <code>ENUM</code>,
</p>
</li>
<li> <p><code>integer</code> to <code>DECIAML</code> &amp; <code>INT32</code>,
</p>
</li>
<li> <p><code>integer</code> to <code>DECIAML</code> &amp; <code>INT64</code>,
</p>
</li>
<li> <p><code>double</code> to <code>DECIAML</code> &amp; <code>INT32</code>,
</p>
</li>
<li> <p><code>double</code> to <code>DECIAML</code> &amp; <code>INT64</code>,
</p>
</li>
<li> <p><code>integer</code> to <code style="white-space: pre;">&#8288;INT(8, *)&#8288;</code>, <code style="white-space: pre;">&#8288;INT(16, *)&#8288;</code>, <code>INT(32, signed)</code>,
</p>
</li>
<li> <p><code>double</code> to <code style="white-space: pre;">&#8288;INT(*, *)&#8288;</code>,
</p>
</li>
<li> <p><code>character</code> to <code>UUID</code>,
</p>
</li>
<li> <p><code>double</code> to <code>FLOAT16</code>,
</p>
</li>
<li> <p><code>list</code> of <code>raw</code> vectors to <code>BYTE_ARRAY</code>,
</p>
</li>
<li> <p><code>list</code> of <code>raw</code> vectors to <code>FIXED_LEN_BYTE_ARRAY</code>.
</p>
</li></ul>



<h3>Parquet's data types</h3>

<p>When reading a Parquet file nanoparquet also relies on logical types and
the Arrow metadata (if present, see below) in addition to the low level
data types. The following table summarizes the mappings. See more details
below.</p>

<table>
<tr>
 <td style="text-align: left;">
   Parquet type </td><td style="text-align: left;"> R type </td><td style="text-align: left;"> Notes </td>
</tr>
<tr>
 <td style="text-align: left;">
   <em>Logical types</em> </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   BSON </td><td style="text-align: left;"> character </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   DATE </td><td style="text-align: left;"> Date </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   DECIMAL </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP, potentially losing precision. </td>
</tr>
<tr>
 <td style="text-align: left;">
   ENUM </td><td style="text-align: left;"> character </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   FLOAT16 </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT(8, *) </td><td style="text-align: left;"> integer </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT(16, *) </td><td style="text-align: left;"> integer </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT(32, *) </td><td style="text-align: left;"> integer </td><td style="text-align: left;"> Large unsigned values may overflow! </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT(64, *) </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP </td>
</tr>
<tr>
 <td style="text-align: left;">
   INTERVAL </td><td style="text-align: left;"> list(raw) </td><td style="text-align: left;"> Missing values are <code>NULL</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   JSON </td><td style="text-align: left;"> character </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   LIST </td><td style="text-align: left;">  </td><td style="text-align: left;"> Not supported. </td>
</tr>
<tr>
 <td style="text-align: left;">
   MAP </td><td style="text-align: left;">  </td><td style="text-align: left;"> Not supported. </td>
</tr>
<tr>
 <td style="text-align: left;">
   STRING </td><td style="text-align: left;"> factor </td><td style="text-align: left;"> If Arrow metadata says it is a factor. Also UTF8. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> character </td><td style="text-align: left;"> Otherwise. Also UTF8. </td>
</tr>
<tr>
 <td style="text-align: left;">
   TIME </td><td style="text-align: left;"> hms::hms </td><td style="text-align: left;"> Also TIME_MILLIS and TIME_MICROS. </td>
</tr>
<tr>
 <td style="text-align: left;">
   TIMESTAMP </td><td style="text-align: left;"> POSIXct </td><td style="text-align: left;"> Also TIMESTAMP_MILLIS and TIMESTAMP_MICROS. </td>
</tr>
<tr>
 <td style="text-align: left;">
   UUID </td><td style="text-align: left;"> character </td><td style="text-align: left;"> In <code>00112233-4455-6677-8899-aabbccddeeff</code> form. </td>
</tr>
<tr>
 <td style="text-align: left;">
   UNKNOWN </td><td style="text-align: left;">  </td><td style="text-align: left;"> Not supported. </td>
</tr>
<tr>
 <td style="text-align: left;">
   <em>Primitive types</em> </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   BOOLEAN </td><td style="text-align: left;"> logical </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   BYTE_ARRAY </td><td style="text-align: left;"> factor </td><td style="text-align: left;"> If Arrow metadata says it is a factor. </td>
</tr>
<tr>
 <td style="text-align: left;">
   " </td><td style="text-align: left;"> list(raw) </td><td style="text-align: left;"> Otherwise. Missing values are <code>NULL</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   DOUBLE </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP </td>
</tr>
<tr>
 <td style="text-align: left;">
   FIXED_LEN_BYTE_ARRAY </td><td style="text-align: left;"> list(raw) </td><td style="text-align: left;"> Missing values are <code>NULL</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   FLOAT </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT32 </td><td style="text-align: left;"> integer </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT64 </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> REALSXP </td>
</tr>
<tr>
 <td style="text-align: left;">
   INT96 </td><td style="text-align: left;"> POSIXct </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The exact rules are below. These rules will likely change until nanoparquet
reaches version 1.0.0.
</p>

<ol>
<li><p> The <code>BOOLEAN</code> type is read as a logical vector (<code>LGLSXP</code>).
</p>
</li>
<li><p> The <code>STRING</code> logical type and the <code>UTF8</code> converted type is read as
a character vector with UTF-8 encoding.
</p>
</li>
<li><p> The <code>DATE</code> logical type and the <code>DATE</code> converted type are read as a
<code>Date</code> R object.
</p>
</li>
<li><p> The <code>TIME</code> logical type and the <code>TIME_MILLIS</code> and <code>TIME_MICROS</code>
converted types are read as an <code>hms</code> object, see the hms package.
</p>
</li>
<li><p> The <code>TIMESTAMP</code> logical type and the <code>TIMESTAMP_MILLIS</code> and
<code>TIMESTAMP_MICROS</code> converted types are read as <code>POSIXct</code> objects.
If the logical type has the <code>UTC</code> flag set, then the time zone of the
<code>POSIXct</code> object is set to <code>UTC</code>.
</p>
</li>
<li> <p><code>INT32</code> is read as an integer vector (<code>INTSXP</code>).
</p>
</li>
<li> <p><code>INT64</code>, <code>DOUBLE</code> and <code>FLOAT</code> are read as real vectors (<code>REALSXP</code>).
</p>
</li>
<li> <p><code>INT96</code> is read as a <code>POSIXct</code> read vector with the <code>tzone</code> attribute
set to <code>"UTC"</code>. It was an old convention to store time stamps as
<code>INT96</code> objects.
</p>
</li>
<li><p> The <code>DECIMAL</code> converted type (<code>FIXED_LEN_BYTE_ARRAY</code> or <code>BYTE_ARRAY</code>
type) is read as a real vector (<code>REALSXP</code>), potentially losing
precision.
</p>
</li>
<li><p> The <code>ENUM</code> logical type is read as a character vector.
</p>
</li>
<li><p> The <code>UUID</code> logical type is read as a character vector that uses the
<code>00112233-4455-6677-8899-aabbccddeeff</code> form.
</p>
</li>
<li><p> The <code>FLOAT16</code> logical type is read as a real vector (<code>REALSXP</code>).
</p>
</li>
<li> <p><code>BYTE_ARRAY</code> is read as a <em>factor</em> object if the file was written
by Arrow and the original data type of the column was a factor.
(See 'The Arrow metadata below.)
</p>
</li>
<li><p> Otherwise <code>BYTE_ARRAY</code> is read a list of raw vectors, with missing
values denoted by <code>NULL</code>.
</p>
</li></ol>

<p>Other logical and converted types are read as their annotated low level
types:
</p>

<ol>
<li> <p><code>INT(8, true)</code>, <code>INT(16, true)</code> and <code>INT(32, true)</code> are read as
integer vectors because they are <code>INT32</code> internally in Parquet.
</p>
</li>
<li> <p><code>INT(64, true)</code> is read as a real vector (<code>REALSXP</code>).
</p>
</li>
<li><p> Unsigned integer types <code>INT(8, false)</code>, <code>INT(16, false)</code> and
<code>INT(32, false)</code> are read as integer vectors (<code>INTSXP</code>). Large
positive values may overflow into negative values, this is a known
issue that we will fix.
</p>
</li>
<li> <p><code>INT(64, false)</code> is read as a real vector (<code>REALSXP</code>). Large
positive values may overflow into negative values, this is a known
issue that we will fix.
</p>
</li>
<li> <p><code>INTERVAL</code> is a fixed length byte array, and nanoparquet reads it as
a list of raw vectors. Missing values are denoted by <code>NULL</code>.
</p>
</li>
<li> <p><code>JSON</code> columns are read as character vectors (<code>STRSXP</code>).
</p>
</li>
<li> <p><code>BSON</code> columns are read as raw vectors (<code>RAWSXP</code>).
</p>
</li></ol>

<p>These types are not yet supported:
</p>

<ol>
<li><p> Nested types (<code>LIST</code>, <code>MAP</code>) are not supported.
</p>
</li>
<li><p> The <code>UNKNOWN</code> logical type is not supported.
</p>
</li></ol>

<p>You can use the <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> function to see how R would read
the columns of a Parquet file. Look at the <code>r_type</code> column.
</p>


<h3>The Arrow metadata</h3>

<p>Apache Arrow (i.e. the arrow R package) adds additional metadata to
Parquet files when writing them in <code>arrow::write_parquet()</code>. Then,
when reading the file in <code>arrow::read_parquet()</code>, it uses this metadata
to recreate the same Arrow and R data types as before writing.
</p>
<p><code>nanoparquet::write_parquet()</code> also adds the Arrow metadata to Parquet
files, unless the <code>nanoparquet.write_arrow_metadata</code> option is set to
<code>FALSE</code>.
</p>
<p>Similarly, <code>nanoparquet::read_parquet()</code> uses the Arrow metadata in the
Parquet file (if present), unless the <code>nanoparquet.use_arrow_metadata</code>
option is set to FALSE.
</p>
<p>The Arrow metadata is stored in the file level key-value metadata, with
key <code>ARROW:schema</code>.
</p>
<p>Currently nanoparquet uses the Arrow metadata for two things:
</p>

<ul>
<li><p> It uses it to detect factors. Without the Arrow metadata factors are
read as string vectors.
</p>
</li>
<li><p> It uses it to detect <code>difftime</code> objects. Without the arrow metadata
these are read as <code>INT64</code> columns, containing the time difference in
nanoseconds.
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+nanoparquet-package">nanoparquet-package</a> for options that modify the type
mappings.
</p>

<hr>
<h2 id='parquet_column_types'>Map between R and Parquet data types</h2><span id='topic+parquet_column_types'></span>

<h3>Description</h3>

<p>Note that this function is now deprecated. Please use
<code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> for files, and <code><a href="#topic+infer_parquet_schema">infer_parquet_schema()</a></code> for
data frames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parquet_column_types(x, options = parquet_options())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parquet_column_types_+3A_x">x</code></td>
<td>
<p>Path to a Parquet file, or a data frame.</p>
</td></tr>
<tr><td><code id="parquet_column_types_+3A_options">options</code></td>
<td>
<p>Nanoparquet options, see <code><a href="#topic+parquet_options">parquet_options()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works two ways. It can map the R types of a data frame to
Parquet types, to see how <code><a href="#topic+write_parquet">write_parquet()</a></code> would write out the data
frame. It can also map the types of a Parquet file to R types, to see
how <code><a href="#topic+read_parquet">read_parquet()</a></code> would read the file into R.
</p>


<h3>Value</h3>

<p>Data frame with columns:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>name</code>: column name.
</p>
</li>
<li> <p><code>type</code>: (low level) Parquet data type.
</p>
</li>
<li> <p><code>r_type</code>: the R type that corresponds to the Parquet type.
Might be <code>NA</code> if <code><a href="#topic+read_parquet">read_parquet()</a></code> cannot read this column. See
<a href="#topic+nanoparquet-types">nanoparquet-types</a> for the type mapping rules.
</p>
</li>
<li> <p><code>repetition_type</code>: whether the column in <code>REQUIRED</code> (cannot be
<code>NA</code>) or <code>OPTIONAL</code> (may be <code>NA</code>). <code>REPEATED</code> columns are not
currently supported by nanoparquet.
</p>
</li>
<li> <p><code>logical_type</code>: Parquet logical type in a list column.
An element has at least an entry called <code>type</code>, and potentially
additional entries, e.g. <code>bit_width</code>, <code>is_signed</code>, etc.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_metadata">read_parquet_metadata()</a></code> to read more metadata,
<code><a href="#topic+read_parquet_info">read_parquet_info()</a></code> for a very short summary.
<code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> for the complete Parquet schema.
<code><a href="#topic+read_parquet">read_parquet()</a></code>, <code><a href="#topic+write_parquet">write_parquet()</a></code>, <a href="#topic+nanoparquet-types">nanoparquet-types</a>.
</p>

<hr>
<h2 id='parquet_options'>Nanoparquet options</h2><span id='topic+parquet_options'></span>

<h3>Description</h3>

<p>Create a list of nanoparquet options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parquet_options(
  class = getOption("nanoparquet.class", "tbl"),
  compression_level = getOption("nanoparquet.compression_level", NA_integer_),
  keep_row_groups = FALSE,
  num_rows_per_row_group = getOption("nanoparquet.num_rows_per_row_group", 10000000L),
  use_arrow_metadata = getOption("nanoparquet.use_arrow_metadata", TRUE),
  write_arrow_metadata = getOption("nanoparquet.write_arrow_metadata", TRUE),
  write_data_page_version = getOption("nanoparquet.write_data_page_version", 1L),
  write_minmax_values = getOption("nanoparquet.write_minmax_values", TRUE)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parquet_options_+3A_class">class</code></td>
<td>
<p>The extra class or classes to add to data frames created
in <code><a href="#topic+read_parquet">read_parquet()</a></code>. By default nanoparquet adds the <code>"tbl"</code> class,
so data frames are printed differently if the pillar package is
loaded.</p>
</td></tr>
<tr><td><code id="parquet_options_+3A_compression_level">compression_level</code></td>
<td>
<p>The compression level in <code><a href="#topic+write_parquet">write_parquet()</a></code>.
<code>NA</code> is the default, and it specifies the default
compression level of each method. <code>Inf</code> always selects the highest
possible compression level. More details:
</p>

<ul>
<li><p> Snappy does not support compression levels currently.
</p>
</li>
<li><p> GZIP supports levels from 0 (uncompressed), 1 (fastest), to 9 (best).
The default is 6.
</p>
</li>
<li><p> ZSTD allows positive levels up to 22 currently. 20 and above require
more memory. Negative levels are also allowed, the lower the level,
the faster the speed, at the cost of compression. Currently the
smallest level is -131072. The default level is 3.
</p>
</li></ul>
</td></tr>
<tr><td><code id="parquet_options_+3A_keep_row_groups">keep_row_groups</code></td>
<td>
<p>This option is used when appending to a Parquet
file with <code><a href="#topic+append_parquet">append_parquet()</a></code>. If <code>TRUE</code> then the existing row groups
of the file are always kept as is and nanoparquet creates new row
groups for the new data. If <code>FALSE</code> (the default), then the last row
group of the file will be overwritten if it is smaller than the
default row group size, i.e. <code>num_rows_per_row_group</code>.</p>
</td></tr>
<tr><td><code id="parquet_options_+3A_num_rows_per_row_group">num_rows_per_row_group</code></td>
<td>
<p>The number of rows to put into a row
group, if row groups are not specified explicitly. It should be
an integer scalar. Defaults to 10 million.</p>
</td></tr>
<tr><td><code id="parquet_options_+3A_use_arrow_metadata">use_arrow_metadata</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, then
<code><a href="#topic+read_parquet">read_parquet()</a></code> and <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> will make use of the Apache
Arrow metadata to assign R classes to Parquet columns.
This is currently used to detect factor columns, and to detect
&quot;difftime&quot; columns.
</p>
<p>If this option is <code>FALSE</code>:
</p>

<ul>
<li><p> &quot;factor&quot; columns are read as character vectors.
</p>
</li>
<li><p> &quot;difftime&quot; columns are read as real numbers, meaning one
of seconds, milliseconds, microseconds or nanoseconds. Impossible
to tell which without using the Arrow metadata.
</p>
</li></ul>
</td></tr>
<tr><td><code id="parquet_options_+3A_write_arrow_metadata">write_arrow_metadata</code></td>
<td>
<p>Whether to add the Apache Arrow types as
metadata to the file <code><a href="#topic+write_parquet">write_parquet()</a></code>.</p>
</td></tr>
<tr><td><code id="parquet_options_+3A_write_data_page_version">write_data_page_version</code></td>
<td>
<p>Data version to write by default.
Possible values are 1 and 2. Default is 1.</p>
</td></tr>
<tr><td><code id="parquet_options_+3A_write_minmax_values">write_minmax_values</code></td>
<td>
<p>Whether to write minimum and maximum values
per row group, for data types that support this in <code><a href="#topic+write_parquet">write_parquet()</a></code>.
However, nanoparquet currently does not support minimum and maximum
values for the <code>DECIMAL</code>, <code>UUID</code> and <code>FLOAT16</code> logical types and the
<code>BOOLEAN</code>, <code>BYTE_ARRAY</code> and <code>FIXED_LEN_BYTE_ARRAY</code> primitive types
if they are writing without a logical type. Currently the default
is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of nanoparquet options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# the effect of using Arrow metadata
tmp &lt;- tempfile(fileext = ".parquet")
d &lt;- data.frame(
  fct = as.factor("a"),
  dft = as.difftime(10, units = "secs")
)
write_parquet(d, tmp)
read_parquet(tmp, options = parquet_options(use_arrow_metadata = TRUE))
read_parquet(tmp, options = parquet_options(use_arrow_metadata = FALSE))

</code></pre>

<hr>
<h2 id='parquet_schema'>Create a Parquet schema</h2><span id='topic+parquet_schema'></span>

<h3>Description</h3>

<p>You can use this schema to specify how to write out a data frame to
a Parquet file with <code><a href="#topic+write_parquet">write_parquet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parquet_schema(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parquet_schema_+3A_...">...</code></td>
<td>
<p>Parquet type specifications, see below.
For backwards compatibility, you can supply a file name
here, and then <code>parquet_schema</code> behaves as <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A schema is a list of potentially named type specifications. A schema
is stored in a data frame. Each (potentially named) argument of
<code>parquet_schema</code> may be a character scalar, or a list. Parameterized
types need to be specified as a list. Primitive Parquet types may be
specified as a string or a list.
</p>


<h3>Value</h3>

<p>Data frame with the same columns as <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code>:
<code>file_name</code>, <code>name</code>, <code>r_type</code>, <code>type</code>, <code>type_length</code>, <code>repetition_type</code>, <code>converted_type</code>, <code>logical_type</code>, <code>num_children</code>, <code>scale</code>, <code>precision</code>, <code>field_id</code>.
</p>


<h3>Possible types:</h3>

<p>Special type:
</p>

<ul>
<li> <p><code>"AUTO"</code>: this is not a Parquet type, but it tells <code><a href="#topic+write_parquet">write_parquet()</a></code>
to map the R type to Parquet automatically, using the default mapping
rules.
</p>
</li></ul>

<p>Primitive Parquet types:
</p>

<ul>
<li> <p><code>"BOOLEAN"</code>
</p>
</li>
<li> <p><code>"INT32"</code>
</p>
</li>
<li> <p><code>"INT64"</code>
</p>
</li>
<li> <p><code>"INT96"</code>
</p>
</li>
<li> <p><code>"FLOAT"</code>
</p>
</li>
<li> <p><code>"DOUBLE"</code>
</p>
</li>
<li> <p><code>"BYTE_ARRAY"</code>
</p>
</li>
<li> <p><code>"FIXED_LEN_BYTE_ARRAY"</code>: fixed-length byte array. It needs a
<code>type_length</code> parameter, an integer between 0 and 2^31-1.
</p>
</li></ul>

<p>Parquet logical types:
</p>

<ul>
<li> <p><code>"STRING"</code>
</p>
</li>
<li> <p><code>"ENUM"</code>
</p>
</li>
<li> <p><code>"UUID"</code>
</p>
</li>
<li> <p><code>"INTEGER"</code>: signed or unsigned integer. It needs a <code>bit_width</code> and
an <code>is_signed</code> parameter. <code>bit_width</code> must be 8, 16, 32 or 64.
<code>is_signed</code> must be <code>TRUE</code> or <code>FALSE</code>.
</p>
</li>
<li> <p><code>"INT"</code>: same as <code>"INTEGER"</code>. The Parquet documentation uses <code>"INT"</code>,
but the actual specification uses <code>"INTEGER"</code>. Both are supported in
nanoparquet.
</p>
</li>
<li> <p><code>"DECIMAL"</code>: decimal number of specified scale and precision.
It needs the <code>precision</code> and <code>primitive_type</code> parameters. Also
supports the <code>scale</code> parameter, it defaults to zero if not specified.
</p>
</li>
<li> <p><code>"FLOAT16"</code>
</p>
</li>
<li> <p><code>"DATE"</code>
</p>
</li>
<li> <p><code>"TIME"</code>: needs an <code>is_adjusted_utc</code> (<code>TRUE</code> or <code>FALSE</code>) and a
<code>unit</code> parameter. <code>unit</code> must be <code>"MILLIS"</code>, <code>"MICROS"</code> or <code>"NANOS"</code>.
</p>
</li>
<li> <p><code>"TIMESTAMP"</code>: needs an <code>is_adjusted_utc</code> (<code>TRUE</code> or <code>FALSE</code>) and a
<code>unit</code> parameter. <code>unit</code> must be <code>"MILLIS"</code>, <code>"MICROS"</code> or <code>"NANOS"</code>.
</p>
</li>
<li> <p><code>"JSON"</code>
</p>
</li>
<li> <p><code>"BSON"</code>
</p>
</li></ul>

<p>Logical types <code>MAP</code>, <code>LIST</code> and <code>UNKNOWN</code> are not supported currently.
</p>
<p>Converted types are deprecated in the Parquet specification in favor of
logical types, but <code>parquet_schema()</code> accepts some converted types as a
syntactic shortcut for the corresponding logical types:
</p>

<ul>
<li> <p><code>INT_8</code> mean <code>list("INT", bit_width = 8, is_signed = TRUE)</code>.
</p>
</li>
<li> <p><code>INT_16</code> mean <code>list("INT", bit_width = 16, is_signed = TRUE)</code>.
</p>
</li>
<li> <p><code>INT_32</code> mean <code>list("INT", bit_width = 32, is_signed = TRUE)</code>.
</p>
</li>
<li> <p><code>INT_64</code> mean <code>list("INT", bit_width = 64, is_signed = TRUE)</code>.
</p>
</li>
<li> <p><code>TIME_MICROS</code> means <code>list("TIME", is_adjusted_utc = TRUE, unit = "MICROS")</code>.
</p>
</li>
<li> <p><code>TIME_MILLIS</code> means <code>list("TIME", is_adjusted_utc = TRUE, unit = "MILLIS")</code>.
</p>
</li>
<li> <p><code>TIMESTAMP_MICROS</code> means <code>list("TIMESTAMP", is_adjusted_utc = TRUE, unit = "MICROS")</code>.
</p>
</li>
<li> <p><code>TIMESTAMP_MILLIS</code> means <code>list("TIMESTAMP", is_adjusted_utc = TRUE, unit = "MILLIS")</code>.
</p>
</li>
<li> <p><code>UINT_8</code> means <code>list("INT", bit_width = 8, is_signed = FALSE)</code>.
</p>
</li>
<li> <p><code>UINT_16</code> means <code>list("INT", bit_width = 16, is_signed = FALSE)</code>.
</p>
</li>
<li> <p><code>UINT_32</code> means <code>list("INT", bit_width = 32, is_signed = FALSE)</code>.
</p>
</li>
<li> <p><code>UINT_64</code> means <code>list("INT", bit_width = 64, is_signed = FALSE)</code>.
</p>
</li></ul>



<h4>Missing values</h4>

<p>Each type might also have a <code>repetition_type</code> parameter, with possible
values <code>"REQUIRED"</code>, <code>"OPTIONAL"</code> or <code>"REPEATED"</code>. <code>"REQUIRED"</code> columns
do not allow missing values. Missing values are allowed in <code>"OPTIONAL"</code>
columns. <code>"REPEATED"</code> columns are currently not supported in
<code><a href="#topic+write_parquet">write_parquet()</a></code>.
</p>



<h3>Examples</h3>

<pre><code class='language-R'>parquet_schema(
  c1 = "INT32",
  c2 = list("INT", bit_width = 64, is_signed = TRUE),
  c3 = list("STRING", repetition_type = "OPTIONAL")
)
</code></pre>

<hr>
<h2 id='parquet-encodings'>Parquet encodings</h2><span id='topic+parquet-encodings'></span>

<h3>Description</h3>

<p>Various Parquet encodings
</p>


<h3>Nanoparquet defaults</h3>

<p>Currently the defaults are decided based on the R types. This might
change in the future. In general, the defaults will likely change until
nanoparquet reaches version 1.0.0.
</p>
<p>Current encoding defaults:
</p>

<ul>
<li><p> Definition levels always use <code>RLE</code>. (Nanoparquet does not currently
write repetition levels, but they'll also use <code>RLE</code>, once implemented.)
</p>
</li>
<li> <p><code>factor</code> columns use <code>RLE_DICTIONARY</code>.
</p>
</li>
<li> <p><code>logical</code> columns use <code>RLE</code> if the average run length of the first
10,000 values is at least 15. Otherwise they use the <code>PLAIN</code> encoding.
</p>
</li>
<li> <p><code>integer</code>, <code>double</code> and <code>character</code> columns use <code>RLE_DICTIONARY</code> if at
least two third of their values are repeated. Otherwise they use
<code>PLAIN</code> encoding.
</p>
</li>
<li> <p><code>list</code> columns of <code>raw</code> vectors always use the <code>PLAIN</code> encoding
currently.
</p>
</li></ul>



<h3>Parquet encodings</h3>

<p>See <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">https://github.com/apache/parquet-format/blob/master/Encodings.md</a>
for more details on Parquet encodings.
</p>


<h4><code>PLAIN</code> encoding</h4>

<p>Supported types: all.
</p>
<p>In general values are written back to back:
</p>

<ul>
<li><p> Integer types are little endian.
</p>
</li>
<li><p> Floating point types follow the IEEE standard.
</p>
</li>
<li> <p><code>BYTE_ARRAY</code>: for each element, there is a little endian 4-byte length
and then the bytes themselves.
</p>
</li>
<li> <p><code>FIXED_LEN_BYTE_ARRAY</code>: bytes are written back to back.
</p>
</li></ul>

<p>Nanoparquet can read and write this encoding for all primitive types.
</p>



<h4><code>RLE_DICTIONARY</code> encoding</h4>

<p>Supported types: dictionary indices in data pages.
</p>
<p>This encoding combines run-length encoding and bit-packing.
Repeated sequences of the same value can be run-length encoded, and
non-repeated parts are bit packed.
It is used for data pages of dictionaries.
The dictionary pages themselves are <code>PLAIN</code> encoded.
</p>
<p>The deprecated <code>PLAIN_DICTIONARY</code> name is treated the same as
<code>RLE_DICTIONARY</code>.
</p>
<p>Nanoparquet can read and write this encoding.
</p>



<h4><code>RLE</code> encoding</h4>

<p>Supported types: <code>BOOLEAN</code>. Also for definition and repetition levels.
</p>
<p>This is the same encoding as <code>RLE_DICTIONARY</code>, with a slightly different
header. It combines run-length encoding and bit packing.
It is used for <code>BOOLEAN</code> columns, and also for definition and
repetition levels.
</p>
<p>Nanoparquet can read and write this encoding.
</p>



<h4><code>BIT_PACKED</code> encoding (deprecated in favor of <code>RLE</code>)</h4>

<p>Supported types: none. Only for definition and repetition levels, but
<code>RLE</code> should be used instead.
</p>
<p>This is a simple bit packing encoding for integers, that was previously
used for encoding definition and repetition levels. It is not used in new
Parquet files because the the <code>RLE</code> encoding includes it and it is better.
</p>
<p>Nanoparquet currently cannot read or write the <code>BIT_PACKED</code> encoding.
</p>



<h4><code>DELTA_BINARY_PACKED</code> encoding</h4>

<p>Supported types: <code>INT32</code>, <code>INT64</code>.
</p>
<p>This encoding efficiently encodes integer columns if the differences
between consecutive elements are often the same, and/or the differences
between consecutive elements are small. The extreme case of an arithmetic
sequence can be encoded in O(1) space.
</p>
<p>Nanoparquet can read this encoding, but cannot currently write it.
</p>



<h4><code>DELTA_LENGTH_BYTE_ARRAY</code> encoding</h4>

<p>Supported types: <code>BYTE_ARRAY</code>.
</p>
<p>This encoding uses <code>DELTA_BINARY_PACKED</code> to encode the length of all
byte array elements. It is especially efficient for short byte array
elements, i.e. a column of short strings.
</p>
<p>Nanoparquet can read this encoding, but cannot currently write it.
</p>



<h4><code>DELTA_BYTE_ARRAY</code> encoding</h4>

<p>Supported types: <code>BYTE_ARRAY</code>, <code>FIXED_LEN_BYTE_ARRAY</code>.
</p>
<p>This encoding is efficient if consecutive byte array elements share the
same prefix, because each element can reuse a prefix of the previous
element.
</p>
<p>Nanoparquet can read this encoding, but cannot currently write it.
</p>



<h4><code>BYTE_STREAM_SPLIT</code> encoding</h4>

<p>Supported types: <code>FLOAT</code>, <code>DOUBLE</code>, <code>INT32</code>, <code>INT64</code>,
<code>FIXED_LEN_BYTE_ARRAY</code>.
</p>
<p>This encoding stores the first bytes of the elements first, then the
second bytes, etc. It does not reduce the size in itself, but may allow
more efficient compression.
</p>
<p>Nanoparquet can read this encoding, but cannot currently write it.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+write_parquet">write_parquet()</a></code> on how to select a non-default encoding when
writing Parquet files.
</p>

<hr>
<h2 id='read_parquet'>Read a Parquet file into a data frame</h2><span id='topic+read_parquet'></span>

<h3>Description</h3>

<p>Converts the contents of the named Parquet file to a R data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet(file, col_select = NULL, options = parquet_options())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file. It may also be an R connection,
in which case it first reads all data from the connection, writes
it into a temporary file, then reads the temporary file, and
deletes it. The connection might be open, it which case it must be
a binary connection. If it is not open, then <code>read_parquet()</code> will
open it and also close it in the end.</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_col_select">col_select</code></td>
<td>
<p>Columns to read. It can be a numeric vector of column
indices, or a character vector of column names. It is an error to
select the same column multiple times. The order of the columns in
the result is the same as the order in <code>col_select</code>.</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_options">options</code></td>
<td>
<p>Nanoparquet options, see <code><a href="#topic+parquet_options">parquet_options()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.frame</code> with the file's contents.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+write_parquet">write_parquet()</a></code> to write Parquet files,
<a href="#topic+nanoparquet-types">nanoparquet-types</a> for the R &lt;-&gt; Parquet type mapping.
See <code><a href="#topic+read_parquet_info">read_parquet_info()</a></code>, for general information,
<code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> for information about the
columns, and <code><a href="#topic+read_parquet_metadata">read_parquet_metadata()</a></code> for the complete metadata.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_name &lt;- system.file("extdata/userdata1.parquet", package = "nanoparquet")
parquet_df &lt;- nanoparquet::read_parquet(file_name)
print(str(parquet_df))
</code></pre>

<hr>
<h2 id='read_parquet_info'>Short summary of a Parquet file</h2><span id='topic+read_parquet_info'></span><span id='topic+parquet_info'></span>

<h3>Description</h3>

<p>Short summary of a Parquet file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet_info(file)

parquet_info(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_info_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with columns:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>num_cols</code>: number of (leaf) columns.
</p>
</li>
<li> <p><code>num_rows</code>: number of rows.
</p>
</li>
<li> <p><code>num_row_groups</code>: number of row groups.
</p>
</li>
<li> <p><code>file_size</code>: file size in bytes.
</p>
</li>
<li> <p><code>parquet_version</code>: Parquet version.
</p>
</li>
<li> <p><code>created_by</code>: A string scalar, usually the name of the software
that created the file. <code>NA</code> if not available.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_metadata">read_parquet_metadata()</a></code> to read more metadata,
<code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> for column information.
<code><a href="#topic+read_parquet">read_parquet()</a></code>, <code><a href="#topic+write_parquet">write_parquet()</a></code>, <a href="#topic+nanoparquet-types">nanoparquet-types</a>.
</p>

<hr>
<h2 id='read_parquet_metadata'>Read the metadata of a Parquet file</h2><span id='topic+read_parquet_metadata'></span><span id='topic+parquet_metadata'></span>

<h3>Description</h3>

<p>This function should work on all files, even if <code><a href="#topic+read_parquet">read_parquet()</a></code> is
unable to read them, because of an unsupported schema, encoding,
compression or other reason.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet_metadata(file, options = parquet_options())

parquet_metadata(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_metadata_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file.</p>
</td></tr>
<tr><td><code id="read_parquet_metadata_+3A_options">options</code></td>
<td>
<p>Options that potentially alter the default Parquet to R
type mappings, see <code><a href="#topic+parquet_options">parquet_options()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with entries:
</p>

<ul>
<li> <p><code>file_meta_data</code>: a data frame with file meta data:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>version</code>: Parquet version, an integer.
</p>
</li>
<li> <p><code>num_rows</code>: total number of rows.
</p>
</li>
<li> <p><code>key_value_metadata</code>: list column of a data frames with two
character columns called <code>key</code> and <code>value</code>. This is the key-value
metadata of the file. Arrow stores its schema here.
</p>
</li>
<li> <p><code>created_by</code>: A string scalar, usually the name of the software
that created the file.
</p>
</li></ul>

</li>
<li> <p><code>schema</code>:
data frame, the schema of the file. It has one row for
each node (inner node or leaf node). For flat files this means one
root node (inner node), always the first one, and then one row for
each &quot;real&quot; column. For nested schemas, the rows are in depth-first
search order. Most important columns are:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>name</code>: column name.
</p>
</li>
<li> <p><code>r_type</code>: the R type that corresponds to the Parquet type.
Might be <code>NA</code> if <code><a href="#topic+read_parquet">read_parquet()</a></code> cannot read this column. See
<a href="#topic+nanoparquet-types">nanoparquet-types</a> for the type mapping rules.
</p>
</li>
<li> <p><code>r_type</code>:
</p>
</li>
<li> <p><code>type</code>: data type. One of the low level data types.
</p>
</li>
<li> <p><code>type_length</code>: length for fixed length byte arrays.
</p>
</li>
<li> <p><code>repettion_type</code>: character, one of <code>REQUIRED</code>, <code>OPTIONAL</code> or
<code>REPEATED</code>.
</p>
</li>
<li> <p><code>logical_type</code>: a list column, the logical types of the columns.
An element has at least an entry called <code>type</code>, and potentially
additional entries, e.g. <code>bit_width</code>, <code>is_signed</code>, etc.
</p>
</li>
<li> <p><code>num_children</code>: number of child nodes. Should be a non-negative
integer for the root node, and <code>NA</code> for a leaf node.
</p>
</li></ul>

</li>
<li> <p><code style="white-space: pre;">&#8288;$row_groups&#8288;</code>: a data frame, information about the row groups.
Some important columns:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>id</code>: row group id, integer from zero to number of row groups
minus one.
</p>
</li>
<li> <p><code>total_byte_size</code>: total uncompressed size of all column data.
</p>
</li>
<li> <p><code>num_rows</code>: number of rows.
</p>
</li>
<li> <p><code>file_offset</code>: where the row group starts in the file. This is
optional, so it might be <code>NA</code>.
</p>
</li>
<li> <p><code>total_compressed_size</code>: total byte size of all compressed
(and potentially encrypted) column data in this row group.
This is optional, so it might be <code>NA</code>.
</p>
</li>
<li> <p><code>ordinal</code>: ordinal position of the row group in the file, starting
from zero. This is optional, so it might be <code>NA</code>. If <code>NA</code>, then
the order of the row groups is as they appear in the metadata.
</p>
</li></ul>

</li>
<li> <p><code style="white-space: pre;">&#8288;$column_chunks&#8288;</code>: a data frame, information about all column chunks,
across all row groups. Some important columns:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>row_group</code>: which row group this chunk belongs to.
</p>
</li>
<li> <p><code>column</code>: which leaf column this chunks belongs to. The order is
the same as in <code style="white-space: pre;">&#8288;$schema&#8288;</code>, but only leaf columns (i.e. columns with
<code>NA</code> children) are counted.
</p>
</li>
<li> <p><code>file_path</code>: which file the chunk is stored in. <code>NA</code> means the
same file.
</p>
</li>
<li> <p><code>file_offset</code>: where the column chunk begins in the file.
</p>
</li>
<li> <p><code>type</code>: low level parquet data type.
</p>
</li>
<li> <p><code>encodings</code>: encodings used to store this chunk. It is a list
column of character vectors of encoding names. Current possible
encodings: &quot;PLAIN&quot;, &quot;GROUP_VAR_INT&quot;, &quot;PLAIN_DICTIONARY&quot;, &quot;RLE&quot;, &quot;BIT_PACKED&quot;, &quot;DELTA_BINARY_PACKED&quot;, &quot;DELTA_LENGTH_BYTE_ARRAY&quot;, &quot;DELTA_BYTE_ARRAY&quot;, &quot;RLE_DICTIONARY&quot;, &quot;BYTE_STREAM_SPLIT&quot;.
</p>
</li>
<li> <p><code>path_in_scema</code>: list column of character vectors. It is simply
the path from the root node. It is simply the column name for
flat schemas.
</p>
</li>
<li> <p><code>codec</code>: compression codec used for the column chunk. Possible
values are: &quot;UNCOMPRESSED&quot;, &quot;SNAPPY&quot;, &quot;GZIP&quot;, &quot;LZO&quot;, &quot;BROTLI&quot;, &quot;LZ4&quot;, &quot;ZSTD&quot;.
</p>
</li>
<li> <p><code>num_values</code>: number of values in this column chunk.
</p>
</li>
<li> <p><code>total_uncompressed_size</code>: total uncompressed size in bytes.
</p>
</li>
<li> <p><code>total_compressed_size</code>: total compressed size in bytes.
</p>
</li>
<li> <p><code>data_page_offset</code>: absolute position of the first data page of
the column chunk in the file.
</p>
</li>
<li> <p><code>index_page_offset</code>: absolute position of the first index page of
the column chunk in the file, or <code>NA</code> if there are no index pages.
</p>
</li>
<li> <p><code>dictionary_page_offset</code>: absolute position of the first
dictionary page of the column chunk in the file, or <code>NA</code> if there
are no dictionary pages.
</p>
</li>
<li> <p><code>null_count</code>: the number of missing values in the column chunk.
It may be <code>NA</code>.
</p>
</li>
<li> <p><code>min_value</code>: list column of raw vectors, the minimum value of the
column, in binary. If <code>NULL</code>, then then it is not specified.
This column is experimental.
</p>
</li>
<li> <p><code>max_value</code>: list column of raw vectors, the maximum value of the
column, in binary. If <code>NULL</code>, then then it is not specified.
This column is experimental.
</p>
</li>
<li> <p><code>is_min_value_exact</code>: whether the minimum value is an actual
value of a column, or a bound. It may be <code>NA</code>.
</p>
</li>
<li> <p><code>is_max_value_exact</code>: whether the maximum value is an actual
value of a column, or a bound. It may be <code>NA</code>.
</p>
</li></ul>

</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_info">read_parquet_info()</a></code> for a much shorter summary.
<code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> for column information.
<code><a href="#topic+read_parquet">read_parquet()</a></code> to read, <code><a href="#topic+write_parquet">write_parquet()</a></code> to write Parquet files,
<a href="#topic+nanoparquet-types">nanoparquet-types</a> for the R &lt;-&gt; Parquet type mappings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_name &lt;- system.file("extdata/userdata1.parquet", package = "nanoparquet")
nanoparquet::read_parquet_metadata(file_name)
</code></pre>

<hr>
<h2 id='read_parquet_page'>Read a page from a Parquet file</h2><span id='topic+read_parquet_page'></span>

<h3>Description</h3>

<p>Read a page from a Parquet file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet_page(file, offset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_page_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file.</p>
</td></tr>
<tr><td><code id="read_parquet_page_+3A_offset">offset</code></td>
<td>
<p>Integer offset of the start of the page in the file.
See <code><a href="#topic+read_parquet_pages">read_parquet_pages()</a></code> for a list of all pages and their offsets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list. Many entries correspond to the columns of
the result of <code><a href="#topic+read_parquet_pages">read_parquet_pages()</a></code>. Additional entries are:
</p>

<ul>
<li> <p><code>codec</code>: compression codec. Possible values:
</p>
</li>
<li> <p><code>has_repetition_levels</code>: whether the page has repetition levels.
</p>
</li>
<li> <p><code>has_definition_levels</code>: whether the page has definition levels.
</p>
</li>
<li> <p><code>schema_column</code>: which schema column the page corresponds to. Note
that only leaf columns have pages.
</p>
</li>
<li> <p><code>data_type</code>: low level Parquet data type. Possible values:
</p>
</li>
<li> <p><code>repetition_type</code>: whether the column the page belongs to is
<code>REQUIRED</code>, <code>OPTIONAL</code> or <code>REPEATED</code>.
</p>
</li>
<li> <p><code>page_header</code>: the bytes of the page header in a raw vector.
</p>
</li>
<li> <p><code>num_null</code>: number of missing (<code>NA</code>) values. Only set in V2 data
pages.
</p>
</li>
<li> <p><code>num_rows</code>: this is the same as <code>num_values</code> for flat tables, i.e.
files without repetition levels.
</p>
</li>
<li> <p><code>compressed_data</code>: the data of the page in a raw vector. It includes
repetition and definition levels, if any.
</p>
</li>
<li> <p><code>data</code>: the uncompressed data, if nanoparquet supports the
compression codec of the file (GZIP and SNAPPY at the time of
writing), or if the file is not compressed. In the latter case it
is the same as <code>compressed_data</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_pages">read_parquet_pages()</a></code> for a summary of all pages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
file_name &lt;- system.file("extdata/userdata1.parquet", package = "nanoparquet")
nanoparquet:::read_parquet_pages(file_name)
options(max.print = 100)  # otherwise long raw vector
nanoparquet:::read_parquet_page(file_name, 4L)

</code></pre>

<hr>
<h2 id='read_parquet_pages'>Metadata of all pages of a Parquet file</h2><span id='topic+read_parquet_pages'></span>

<h3>Description</h3>

<p>Metadata of all pages of a Parquet file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet_pages(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_pages_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reading all the page headers might be slow for large files, especially
if the file has many small pages.
</p>


<h3>Value</h3>

<p>Data frame with columns:
</p>

<ul>
<li> <p><code>file_name</code>: file name.
</p>
</li>
<li> <p><code>row_group</code>: id of the row group the page belongs to,
an integer between 0 and the number of row groups
minus one.
</p>
</li>
<li> <p><code>column</code>: id of the column. An integer between the
number of leaf columns minus one. Note that only leaf
columns are considered, as non-leaf columns do not
have any pages.
</p>
</li>
<li> <p><code>page_type</code>: <code>DATA_PAGE</code>, <code>INDEX_PAGE</code>, <code>DICTIONARY_PAGE</code> or
<code>DATA_PAGE_V2</code>.
</p>
</li>
<li> <p><code>page_header_offset</code>: offset of the data page (its header) in the
file.
</p>
</li>
<li> <p><code>uncompressed_page_size</code>: does not include the page header, as per
Parquet spec.
</p>
</li>
<li> <p><code>compressed_page_size</code>: without the page header.
</p>
</li>
<li> <p><code>crc</code>: integer, checksum, if present in the file, can be <code>NA</code>.
</p>
</li>
<li> <p><code>num_values</code>: number of data values in this page, include
<code>NULL</code> (<code>NA</code> in R) values.
</p>
</li>
<li> <p><code>encoding</code>: encoding of the page, current possible encodings:
&quot;PLAIN&quot;, &quot;GROUP_VAR_INT&quot;, &quot;PLAIN_DICTIONARY&quot;, &quot;RLE&quot;, &quot;BIT_PACKED&quot;, &quot;DELTA_BINARY_PACKED&quot;, &quot;DELTA_LENGTH_BYTE_ARRAY&quot;, &quot;DELTA_BYTE_ARRAY&quot;, &quot;RLE_DICTIONARY&quot;, &quot;BYTE_STREAM_SPLIT&quot;.
</p>
</li>
<li> <p><code>definition_level_encoding</code>: encoding of the definition levels,
see <code>encoding</code> for possible values. This can be missing in V2 data
pages, where they are always RLE encoded.
</p>
</li>
<li> <p><code>repetition_level_encoding</code>: encoding of the repetition levels,
see <code>encoding</code> for possible values. This can be missing in V2 data
pages, where they are always RLE encoded.
</p>
</li>
<li> <p><code>data_offset</code>: offset of the actual data in the file.
</p>
</li>
<li> <p><code>page_header_length</code>: size of the page header, in bytes.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_page">read_parquet_page()</a></code> to read a page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_name &lt;- system.file("extdata/userdata1.parquet", package = "nanoparquet")
nanoparquet:::read_parquet_pages(file_name)
</code></pre>

<hr>
<h2 id='read_parquet_schema'>Read the schema of a Parquet file</h2><span id='topic+read_parquet_schema'></span>

<h3>Description</h3>

<p>This function should work on all files, even if <code><a href="#topic+read_parquet">read_parquet()</a></code> is
unable to read them, because of an unsupported schema, encoding,
compression or other reason.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet_schema(file, options = parquet_options())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_parquet_schema_+3A_file">file</code></td>
<td>
<p>Path to a Parquet file.</p>
</td></tr>
<tr><td><code id="read_parquet_schema_+3A_options">options</code></td>
<td>
<p>Return value of <code><a href="#topic+parquet_options">parquet_options()</a></code>, options that
potentially modify the Parquet to R type mappings.</p>
</td></tr>
</table>


<h3>Value</h3>

<div class="sourceCode"><pre>Data frame, the schema of the file. It has one row for
each node (inner node or leaf node). For flat files this means one
root node (inner node), always the first one, and then one row for
each "real" column. For nested schemas, the rows are in depth-first
search order. Most important columns are:
- `file_name`: file name.
- `name`: column name.
- `r_type`: the R type that corresponds to the Parquet type.
  Might be `NA` if [read_parquet()] cannot read this column. See
  [nanoparquet-types] for the type mapping rules.
- `type`: data type. One of the low level data types.
- `type_length`: length for fixed length byte arrays.
- `repettion_type`: character, one of `REQUIRED`, `OPTIONAL` or
  `REPEATED`.
- `logical_type`: a list column, the logical types of the columns.
  An element has at least an entry called `type`, and potentially
  additional entries, e.g. `bit_width`, `is_signed`, etc.
- `num_children`: number of child nodes. Should be a non-negative
  integer for the root node, and `NA` for a leaf node.
</pre></div>


<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_metadata">read_parquet_metadata()</a></code> to read more metadata,
<code><a href="#topic+read_parquet_info">read_parquet_info()</a></code> to show only basic information.
<code><a href="#topic+read_parquet">read_parquet()</a></code>, <code><a href="#topic+write_parquet">write_parquet()</a></code>, <a href="#topic+nanoparquet-types">nanoparquet-types</a>.
</p>

<hr>
<h2 id='rle_decode_int'>RLE decode integers</h2><span id='topic+rle_decode_int'></span>

<h3>Description</h3>

<p>RLE decode integers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rle_decode_int(
  x,
  bit_width = attr(x, "bit_width"),
  length = attr(x, "length") %||% NA
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rle_decode_int_+3A_x">x</code></td>
<td>
<p>Raw vector of the encoded integers.</p>
</td></tr>
<tr><td><code id="rle_decode_int_+3A_bit_width">bit_width</code></td>
<td>
<p>Bit width used for the encoding.</p>
</td></tr>
<tr><td><code id="rle_decode_int_+3A_length">length</code></td>
<td>
<p>Length of the output. If <code>NA</code> then we assume that <code>x</code>
starts with length of the output, encoded as a 4 byte integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The decoded integer vector.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rle_encode_int">rle_encode_int()</a></code>
</p>
<p>Other encodings: 
<code><a href="#topic+rle_encode_int">rle_encode_int</a>()</code>
</p>

<hr>
<h2 id='rle_encode_int'>RLE encode integers</h2><span id='topic+rle_encode_int'></span>

<h3>Description</h3>

<p>RLE encode integers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rle_encode_int(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rle_encode_int_+3A_x">x</code></td>
<td>
<p>Integer vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Raw vector, the encoded integers. It has two attributes:
</p>

<ul>
<li> <p><code>bit_length</code>: the number of bits needed to encode the input, and
</p>
</li>
<li> <p><code>length</code>: length of the original integer input.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+rle_decode_int">rle_decode_int()</a></code>
</p>
<p>Other encodings: 
<code><a href="#topic+rle_decode_int">rle_decode_int</a>()</code>
</p>

<hr>
<h2 id='write_parquet'>Write a data frame to a Parquet file</h2><span id='topic+write_parquet'></span>

<h3>Description</h3>

<p>Writes the contents of an R data frame into a Parquet file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_parquet(
  x,
  file,
  schema = NULL,
  compression = c("snappy", "gzip", "zstd", "uncompressed"),
  encoding = NULL,
  metadata = NULL,
  row_groups = NULL,
  options = parquet_options()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_parquet_+3A_x">x</code></td>
<td>
<p>Data frame to write.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_file">file</code></td>
<td>
<p>Path to the output file. If this is the string <code>":raw:"</code>,
then the data frame is written to a memory buffer, and the memory
buffer is returned as a raw vector.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_schema">schema</code></td>
<td>
<p>Parquet schema. Specify a schema to tweak the default
nanoparquet R -&gt; Parquet type mappings. Use <code><a href="#topic+parquet_schema">parquet_schema()</a></code> to
create a schema that you can use here, or <code><a href="#topic+read_parquet_schema">read_parquet_schema()</a></code> to
use the schema of a Parquet file.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_compression">compression</code></td>
<td>
<p>Compression algorithm to use. Currently <code>"snappy"</code>
(the default), <code>"gzip"</code>, <code>"zstd"</code>, and <code>"uncompressed"</code> are supported.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_encoding">encoding</code></td>
<td>
<p>Encoding to use. Possible values:
</p>

<ul>
<li><p> If <code>NULL</code>, the appropriate encoding is selected automatically:
<code>RLE</code> or <code>PLAIN</code> for <code>BOOLEAN</code> columns, <code>RLE_DICTIONARY</code> for other
columns with many repeated values, and <code>PLAIN</code> otherwise.
</p>
</li>
<li><p> If It is a single (unnamed) character string, then it'll be used
for all columns.
</p>
</li>
<li><p> If it is an unnamed character vector of encoding names of the same
length as the number of columns in the data frame, then those
encodings will be used for each column.
</p>
</li>
<li><p> If it is a named character vector, then the named must be unique
and each name must match a column name, to specify the encoding of
that column. The special empty name (<code>""</code>) applies to the rest of
the columns. If there is no empty name, the rest of the columns
will use the default encoding.
</p>
</li></ul>

<p>If <code>NA_character_</code> is specified for a column, the default encoding is
used for the column.
</p>
<p>If a specified encoding is invalid for a certain column type,
or nanoparquet does not implement it, <code>write_parquet()</code> throws an
error.
</p>
<p>Currently <code>write_parquet()</code> supports the following encodings:
</p>

<ul>
<li> <p><code>PLAIN</code> for all column types,
</p>
</li>
<li> <p><code>PLAIN_DICTIONARY</code> and <code>RLE_DICTIONARY</code> for all column types,
</p>
</li>
<li> <p><code>RLE</code> for BOOLEAN columns.
</p>
</li></ul>

<p>See <a href="#topic+parquet-encodings">parquet-encodings</a> for more about encodings.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_metadata">metadata</code></td>
<td>
<p>Additional key-value metadata to add to the file.
This must be a named character vector, or a data frame with columns
character columns called <code>key</code> and <code>value</code>.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_row_groups">row_groups</code></td>
<td>
<p>Row groups of the Parquet file. If <code>NULL</code>, then the
<code>num_rows_per_row_group</code> option is used from the <code>options</code> argument,
see <code><a href="#topic+parquet_options">parquet_options()</a></code>. Otherwise it must be an integer vector,
specifying the starts of the row groups.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_options">options</code></td>
<td>
<p>Nanoparquet options, see <code><a href="#topic+parquet_options">parquet_options()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>write_parquet()</code> converts string columns to UTF-8 encoding by calling
<code><a href="base.html#topic+Encoding">base::enc2utf8()</a></code>. It does the same for factor levels.
</p>


<h3>Value</h3>

<p><code>NULL</code>, unless <code>file</code> is <code>":raw:"</code>, in which case the Parquet
file is returned as a raw vector.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read_parquet_metadata">read_parquet_metadata()</a></code>, <code><a href="#topic+read_parquet">read_parquet()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# add row names as a column, because `write_parquet()` ignores them.
mtcars2 &lt;- cbind(name = rownames(mtcars), mtcars)
write_parquet(mtcars2, "mtcars.parquet")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
