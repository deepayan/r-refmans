<!DOCTYPE html><html><head><title>Help for package spatialEco</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {spatialEco}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#all_pairwise'><p>All pairwise combinations</p></a></li>
<li><a href='#annulus.matrix'><p>Annulus matrix</p></a></li>
<li><a href='#ants'><p>Ant Biodiversity Data</p></a></li>
<li><a href='#aspline.downscale'><p>Raster Downscale using adaptive regression splines</p></a></li>
<li><a href='#background'><p>Background sample</p></a></li>
<li><a href='#bbox_poly'><p>Bounding box polygon</p></a></li>
<li><a href='#bearing.distance'><p>Bearing and Distance</p></a></li>
<li><a href='#breeding.density'><p>Breeding density areas (aka, core habitat areas)</p></a></li>
<li><a href='#built.index'><p>built index</p></a></li>
<li><a href='#cgls_urls'><p>Provide URL's for Copernicus Global Land Service datasets</p></a></li>
<li><a href='#chae'><p>Canine-Human Age Equivalent</p></a></li>
<li><a href='#chen'><p>Cross-correlation data from Chen (2015)</p></a></li>
<li><a href='#classBreaks'><p>Class breaks</p></a></li>
<li><a href='#collinear'><p>Collinearity test</p></a></li>
<li><a href='#combine'><p>raster combine</p></a></li>
<li><a href='#concordance'><p>Concordance test for binomial models</p></a></li>
<li><a href='#conf.interval'><p>Confidence interval for mean or median</p></a></li>
<li><a href='#cor.data'><p>Various correlation structures</p></a></li>
<li><a href='#correlogram'><p>Correlogram</p></a></li>
<li><a href='#cross.tab'><p>Class comparison between two nominal rasters</p></a></li>
<li><a href='#crossCorrelation'><p>Spatial cross correlation</p></a></li>
<li><a href='#csi'><p>Cosine Similarity Index</p></a></li>
<li><a href='#curvature'><p>Surface curvature</p></a></li>
<li><a href='#dahi'><p>Diurnal Anisotropic Heat Index</p></a></li>
<li><a href='#date_seq'><p>date sequence</p></a></li>
<li><a href='#daymet.point'><p>DAYMET point values</p></a></li>
<li><a href='#daymet.tiles'><p>DAYMET Tile ID's</p></a></li>
<li><a href='#dispersion'><p>Dispersion (H-prime)</p></a></li>
<li><a href='#dissection'><p>Dissection</p></a></li>
<li><a href='#divergence'><p>divergence</p></a></li>
<li><a href='#effect.size'><p>Cohen's-d effect size</p></a></li>
<li><a href='#elev'><p>Elevation raster</p></a></li>
<li><a href='#erase.point'><p>Erase points</p></a></li>
<li><a href='#extract.vertices'><p>Extract vertices for polygons or lines</p></a></li>
<li><a href='#fuzzySum'><p>Fuzzy Sum</p></a></li>
<li><a href='#gaussian.kernel'><p>Gaussian Kernel</p></a></li>
<li><a href='#geo.buffer'><p>Buffer geographic data</p></a></li>
<li><a href='#group.pdf'><p>Probability density plot by group</p></a></li>
<li><a href='#hexagons'><p>Hexagons</p></a></li>
<li><a href='#hli'><p>Heat Load Index</p></a></li>
<li><a href='#hli.pt'><p>Point estimate of Heat Load Index</p></a></li>
<li><a href='#hsp'><p>Hierarchical Slope Position</p></a></li>
<li><a href='#hybrid.kmeans'><p>Hybrid K-means</p></a></li>
<li><a href='#idw.smoothing'><p>Inverse Distance Weighted smoothing</p></a></li>
<li><a href='#impute.loess'><p>Impute loess</p></a></li>
<li><a href='#insert'><p>Insert a row or column into a data.frame</p></a></li>
<li><a href='#insert.values'><p>Insert Values</p></a></li>
<li><a href='#is.empty'><p>is.empty</p></a></li>
<li><a href='#kendall'><p>Kendall tau trend with continuity correction for time-series</p></a></li>
<li><a href='#kl.divergence'><p>Kullback-Leibler divergence (relative entropy)</p></a></li>
<li><a href='#knn'><p>Spatial K nearest neighbor</p></a></li>
<li><a href='#lai'><p>Leaf Area Index</p></a></li>
<li><a href='#local.min.max'><p>Local minimum and maximum</p></a></li>
<li><a href='#loess.boot'><p>Loess Bootstrap</p></a></li>
<li><a href='#loess.ci'><p>Loess with confidence intervals</p></a></li>
<li><a href='#logistic.regression'><p>Logistic and Auto-logistic regression</p></a></li>
<li><a href='#max_extent'><p>Maximum extent of multiple rasters</p></a></li>
<li><a href='#mean_angle'><p>Mean Angle</p></a></li>
<li><a href='#moments'><p>moments</p></a></li>
<li><a href='#morans.plot'><p>Autocorrelation Plot</p></a></li>
<li><a href='#nni'><p>Average Nearest Neighbor Index (NNI)</p></a></li>
<li><a href='#nth.values'><p>Nth values</p></a></li>
<li><a href='#o.ring'><p>Inhomogeneous O-ring</p></a></li>
<li><a href='#oli.asw'><p>Query AWS-OLI</p></a></li>
<li><a href='#optimal.k'><p>optimalK</p></a></li>
<li><a href='#optimized.sample.variance'><p>Optimized sample variance</p></a></li>
<li><a href='#outliers'><p>Outliers</p></a></li>
<li><a href='#overlap'><p>Niche overlap (Warren's-I)</p></a></li>
<li><a href='#parea.sample'><p>Percent area sample</p></a></li>
<li><a href='#parse.bits'><p>Parse bits</p></a></li>
<li><a href='#partial.cor'><p>Partial and Semi-partial correlation</p></a></li>
<li><a href='#plot.effect.size'><p>Plot effect size</p></a></li>
<li><a href='#plot.loess.boot'><p>Plot Loess Bootstrap</p></a></li>
<li><a href='#poly_trend'><p>Polynomial trend</p></a></li>
<li><a href='#poly.regression'><p>Local Polynomial Regression</p></a></li>
<li><a href='#polyPerimeter'><p>Polygon perimeter</p></a></li>
<li><a href='#pp.subsample'><p>Point process random subsample</p></a></li>
<li><a href='#print.cross.cor'><p>Print spatial cross correlation</p></a></li>
<li><a href='#print.effect.size'><p>Print effect size</p></a></li>
<li><a href='#print.loess.boot'><p>Print Loess bootstrap model</p></a></li>
<li><a href='#print.poly.trend'><p>Print poly_trend</p></a></li>
<li><a href='#proximity.index'><p>Proximity Index</p></a></li>
<li><a href='#pseudo.absence'><p>Pseudo-absence random samples</p></a></li>
<li><a href='#pu'><p>Biodiversity Planning Units</p></a></li>
<li><a href='#quadrats'><p>Quadrats</p></a></li>
<li><a href='#random.raster'><p>Random raster</p></a></li>
<li><a href='#raster.change'><p>Raster change between two nominal rasters</p></a></li>
<li><a href='#raster.deviation'><p>Raster local deviation from the global trend</p></a></li>
<li><a href='#raster.downscale'><p>Raster Downscale</p></a></li>
<li><a href='#raster.entropy'><p>Raster Entropy</p></a></li>
<li><a href='#raster.gaussian.smooth'><p>Gaussian smoothing of raster</p></a></li>
<li><a href='#raster.invert'><p>Invert raster</p></a></li>
<li><a href='#raster.kendall'><p>Kendall tau trend with continuity correction for raster time-series</p></a></li>
<li><a href='#raster.mds'><p>Raster multidimensional scaling (MDS)</p></a></li>
<li><a href='#raster.modified.ttest'><p>Dutilleul moving window bivariate raster correlation</p></a></li>
<li><a href='#raster.moments'><p>Raster moments</p></a></li>
<li><a href='#raster.transformation'><p>Statistical transformation for rasters</p></a></li>
<li><a href='#raster.vol'><p>Raster Percent Volume</p></a></li>
<li><a href='#raster.Zscore'><p>Modified z-score for a raster</p></a></li>
<li><a href='#rasterCorrelation'><p>Raster correlation</p></a></li>
<li><a href='#rasterDistance'><p>Raster Distance</p></a></li>
<li><a href='#remove_duplicates'><p>Remove duplicate geometries</p></a></li>
<li><a href='#remove.holes'><p>Remove or return polygon holes</p></a></li>
<li><a href='#rm.ext'><p>Remove extension</p></a></li>
<li><a href='#rotate.polygon'><p>Rotate polygon</p></a></li>
<li><a href='#sa.trans'><p>Trigonometric  transformation of a slope and aspect interaction</p></a></li>
<li><a href='#sample.annulus'><p>Sample annulus</p></a></li>
<li><a href='#sampleTransect'><p>Sample transect</p></a></li>
<li><a href='#sar'><p>Surface Area Ratio</p></a></li>
<li><a href='#separability'><p>separability</p></a></li>
<li><a href='#sf_dissolve'><p>Dissolve polygons</p></a></li>
<li><a href='#sf.kde'><p>Spatial kernel density estimate</p></a></li>
<li><a href='#sg.smooth'><p>Savitzky-Golay smoothing filter</p></a></li>
<li><a href='#shannons'><p>Shannon's Diversity (Entropy) Index</p></a></li>
<li><a href='#shift'><p>shift</p></a></li>
<li><a href='#sieve'><p>Sieve raster data</p></a></li>
<li><a href='#similarity'><p>Ecological similarity</p></a></li>
<li><a href='#smooth.time.series'><p>Smooth Raster Time-series</p></a></li>
<li><a href='#sobal'><p>Sobel-Feldman operator</p></a></li>
<li><a href='#spatial.select'><p>Spatial Select</p></a></li>
<li><a href='#spatialEcoNews'><p>spatialEco news</p></a></li>
<li><a href='#spectral.separability'><p>spectral separability</p></a></li>
<li><a href='#spherical.sd'><p>Spherical Variance or Standard Deviation of Surface</p></a></li>
<li><a href='#squareBuffer'><p>Square buffer</p></a></li>
<li><a href='#srr'><p>Surface Relief Ratio</p></a></li>
<li><a href='#stratified.random'><p>Stratified random sample</p></a></li>
<li><a href='#subsample.distance'><p>Distance-based subsampling</p></a></li>
<li><a href='#summary.cross.cor'><p>Summary of spatial cross correlation</p></a></li>
<li><a href='#summary.effect.size'><p>Summarizing effect size</p></a></li>
<li><a href='#summary.loess.boot'><p>Summarizing Loess bootstrap models</p></a></li>
<li><a href='#swvi'><p>Senescence weighted Vegetation Index (swvi)</p></a></li>
<li><a href='#time_to_event'><p>Time to event</p></a></li>
<li><a href='#TM5'><p>Landsat 5 TM Scene</p></a></li>
<li><a href='#topo.distance'><p>Topographic distance</p></a></li>
<li><a href='#tpi'><p>Topographic Position Index (tpi)</p></a></li>
<li><a href='#trasp'><p>Solar-radiation Aspect Index</p></a></li>
<li><a href='#trend.line'><p>trend.line</p></a></li>
<li><a href='#tri'><p>Terrain Ruggedness Index</p></a></li>
<li><a href='#vrm'><p>Vector Ruggedness Measure (VRM)</p></a></li>
<li><a href='#winsorize'><p>Winsorize transformation</p></a></li>
<li><a href='#wt.centroid'><p>Weighted centroid</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Spatial Analysis and Modelling Utilities</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-17</td>
</tr>
<tr>
<td>Description:</td>
<td>Utilities to support spatial data manipulation, query, sampling
    and modelling in ecological applications. Functions include models for species 
	population density, spatial smoothing, multivariate separability, point process 
	model for creating pseudo- absences and sub-sampling, Quadrant-based sampling and 
	analysis, auto-logistic modeling, sampling models, cluster optimization, statistical 
	exploratory tools and raster-based metrics.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>sf, terra</td>
</tr>
<tr>
<td>Suggests:</td>
<td>spatstat.geom (&ge; 3.0-3), spatstat.explore, spdep, ks,
cluster, readr, RCurl, RANN, rms, yaImpute, mgcv, zyp,
SpatialPack (&ge; 0.3), MASS, caret, dplyr, earth, Matrix, gstat,
spatstat.data, methods, units, sp, stringr, lwgeom, geodata</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jeffrey S. Evans &lt;jeffrey_evans@tnc.org&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jeffreyevans/spatialEco">https://github.com/jeffreyevans/spatialEco</a>,
<a href="https://jeffreyevans.github.io/spatialEco/">https://jeffreyevans.github.io/spatialEco/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jeffreyevans/spatialEco/issues">https://github.com/jeffreyevans/spatialEco/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-17 21:27:53 UTC; jeffrey_evans</td>
</tr>
<tr>
<td>Author:</td>
<td>Jeffrey S. Evans <a href="https://orcid.org/0000-0002-5533-7044"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Melanie A. Murphy [ctb],
  Karthik Ram [ctb]</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-17 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='all_pairwise'>All pairwise combinations</h2><span id='topic+all_pairwise'></span>

<h3>Description</h3>

<p>Creates all pairwise combinations list for iteration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>all_pairwise(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="all_pairwise_+3A_x">x</code></td>
<td>
<p>A numeric or character vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This returns a list of vector combinations starting with
pairwise, as the first nested list element, then in groups of 
threes, fours, to length of the vector.
</p>


<h3>Value</h3>

<p>A list object with increasing all combination objects,
the first list element are the pairwise comparisons
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans    &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>classes &lt;- paste0("class", 1:10)

all_pairwise(classes)[[1]]

#### How to use as an iterator
# dataframe with 4 cols, 100 rows
d &lt;- as.data.frame(matrix(runif(100*4), 100, 4)) 
  names(d) &lt;- paste0("class", 1:4) 

( idx &lt;- all_pairwise(colnames(d))[[1]] ) 

opar &lt;- par(no.readonly=TRUE)
  par(mfrow=c(2,3))
    lapply(idx, function(i) {
      plot(d[,i[1]], d[,i[2]], main=paste0(i[1], " vs ", i[2]) )
    })	
par(opar)

</code></pre>

<hr>
<h2 id='annulus.matrix'>Annulus matrix</h2><span id='topic+annulus.matrix'></span>

<h3>Description</h3>

<p>Creates a square matrix representing annulus position values of 1 
and defined null
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annulus.matrix(scale = 3, inner.scale = 0, outer.scale = 0, null.value = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="annulus.matrix_+3A_scale">scale</code></td>
<td>
<p>Number of rings (defines dimensions of matrix)</p>
</td></tr>
<tr><td><code id="annulus.matrix_+3A_inner.scale">inner.scale</code></td>
<td>
<p>Number of inner rings to set to null.value</p>
</td></tr>
<tr><td><code id="annulus.matrix_+3A_outer.scale">outer.scale</code></td>
<td>
<p>Number of outer rings to set to null.value</p>
</td></tr>
<tr><td><code id="annulus.matrix_+3A_null.value">null.value</code></td>
<td>
<p>Value to set inner and outer scale(s) to</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will return a matrix of 1 and defined null.value based on a specification
of the scale, inner scale and outer scale. The scale defines how many rings will be
represented in the matrix based on (2 * scale - 1). So, a scale of 3 will result in a
5x5 matrix. The inner.scale and outer.scale arguments represent the &gt; and &lt; rings that
will be set to the defined null.value (see examples). The resulting matrix can be used
as the specified window in a focal function.
</p>


<h3>Value</h3>

<p>A matrix object with defined null.value and 1, representing retained rings
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>annulus.matrix(5)                   # 5 concentric rings
annulus.matrix(5, 3)                # 5 concentric rings with the 3 inner set to 0
annulus.matrix(5, 3, null.value=NA) # 5 concentric rings with the 3 inner set to NA
annulus.matrix(5, 3, 5)             # 5 rings with 3 inner and 5 outer set to 0
annulus.matrix(9, 3, 7)             # 9 rings with 3 inner and 7 outer set to 0

</code></pre>

<hr>
<h2 id='ants'>Ant Biodiversity Data</h2><span id='topic+ants'></span>

<h3>Description</h3>

<p>Roth et al., (1994) Costa Rican ant diversity data
</p>


<h3>Format</h3>

<p>A data.frame with 82 rows (species) and 5 columns (covertypes):
</p>

<dl>
<dt>species</dt><dd><p>Ant species (family)</p>
</dd>
<dt>Primary.Forest</dt><dd><p>Primary forest type</p>
</dd>
<dt>Abandoned.cacao.plantations</dt><dd><p>Abandoned cacao plantations type</p>
</dd>
<dt>Productive.cacao.plantations</dt><dd><p>Active cacao plantations type</p>
</dd>
<dt>Banana.plantations</dt><dd><p>Active banana plantations type</p>
</dd>
</dl>



<h3>References</h3>

<p>Roth, D. S., I. Perfecto, and B. Rathcke (1994) The effects of management systems on 
ground-foraging ant diversity in Costa Rica. Ecological Applications 4(3):423-436.
</p>

<hr>
<h2 id='aspline.downscale'>Raster Downscale using adaptive regression splines</h2><span id='topic+aspline.downscale'></span>

<h3>Description</h3>

<p>Downscales a raster to a higher resolution raster multivariate adaptive
regression splines (MARS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aspline.downscale(
  x,
  y,
  add.coords = TRUE,
  keep.model = FALSE,
  grid.search = FALSE,
  plot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aspline.downscale_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object representing independent variable(s)</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_y">y</code></td>
<td>
<p>A terra SpatRaster object representing dependent variable</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_add.coords">add.coords</code></td>
<td>
<p>(FALSE/TRUE) Add spatial coordinates to model</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_keep.model">keep.model</code></td>
<td>
<p>(FALSE/TRUE) Keep MARS model (earth class object)</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_grid.search">grid.search</code></td>
<td>
<p>(FALSE/TRUE) perform a hyper-parameter grid se</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_plot">plot</code></td>
<td>
<p>(FALSE/TRUE) Plot results</p>
</td></tr>
<tr><td><code id="aspline.downscale_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to earth</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses Multivariate Adaptive Regression Splines, to downscale a raster based
on higher-resolution or more detailed raster data specified as covariate(s). This is similar
to the raster.downsample function which uses a robust regression and is a frequentest model for
fitting linear asymptotic relationships whereas, this approach is for fitting nonparametric
functions and should be used when the distributional relationship are complex/nonlinear.
Using add.coords adds spatial coordinates to the model, including creating the associated
rasters for prediction.
</p>


<h3>Value</h3>

<p>A list object containing:
</p>

<ul>
<li><p> downscale         Downscaled terra SpatRaster object
</p>
</li>
<li><p> GCV Generalized   Cross Validation (GCV)
</p>
</li>
<li><p> GRSq              Estimate of the predictive power
</p>
</li>
<li><p> RSS               Residual sum-of-squares (RSS)
</p>
</li>
<li><p> RSq               R-square
</p>
</li>
<li><p> model             earth MARS model object (if keep.model = TRUE)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Friedman (1991) Multivariate Adaptive Regression Splines (with discussion)
Annals of Statistics 19(1):1–141
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require(geodata, quietly = TRUE)) {
library(terra)
library(geodata)

# Download example data (requires geodata package)
  elev &lt;- elevation_30s(country="SWZ",  path=tempdir())
    slp &lt;- terrain(elev, v="slope")
	  x &lt;- c(elev,slp)
        names(x) &lt;- c("elev","slope")
  tmax &lt;- worldclim_country(country="SWZ", var="tmax", 
                                     path=tempdir())
    tmax &lt;- crop(tmax[[1]], ext(elev))
	  names(tmax) &lt;- "tmax"
	  
tmax.ds &lt;- aspline.downscale(x, tmax, add.coords=TRUE, keep.model=TRUE)
  plot(tmax.ds$model) 

  # plot prediction and parameters	
  opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,2))
      plot(tmax, main="Original Temp max")
      plot(x[[1]], main="elevation")
      plot(x[[2]], main="slope")
      plot(tmax.ds$downscale, main="Downscaled Temp max")
  par(opar)

} else { 
  cat("Please install geodata package to run example", "\n")
}


</code></pre>

<hr>
<h2 id='background'>Background sample</h2><span id='topic+background'></span>

<h3>Description</h3>

<p>Creates a point sample that can be used as 
a NULL for SDM's and other modeling approaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>background(
  x,
  p = 1000,
  known = NULL,
  d = NULL,
  type = c("regular", "random", "hexagon", "nonaligned")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="background_+3A_x">x</code></td>
<td>
<p>A sf class polygon defining sample region</p>
</td></tr>
<tr><td><code id="background_+3A_p">p</code></td>
<td>
<p>Size of sample</p>
</td></tr>
<tr><td><code id="background_+3A_known">known</code></td>
<td>
<p>An sf POINT class of known locations with same CSR as x</p>
</td></tr>
<tr><td><code id="background_+3A_d">d</code></td>
<td>
<p>Threshold distance for known proximity</p>
</td></tr>
<tr><td><code id="background_+3A_type">type</code></td>
<td>
<p>Type of sample c(&quot;systematic&quot;, &quot;random&quot;, &quot;hexagon&quot;, &quot;nonaligned&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a background point sample based on an extent 
or polygon sampling region. The known argument can be used with d 
to remove sample points based on distance-based proximity to existing  
locations (eg., known species locations). The size (p) of the resulting 
sample will be dependent on the known locations and the influence of 
the distance threshold (d). As such, if the know and d arguments are
provided the exact value provided in p will not be returned.
</p>


<h3>Value</h3>

<p>A sf POINT feature class or data.frame with x,y coordinates
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)

# define study area
sa &lt;- suppressWarnings(st_cast(st_read(
        system.file("shape/nc.shp", 
        package="sf")), "POLYGON"))
  sa &lt;- sa[10,]

# create "known" locations  
locs &lt;- st_sample(sa, 50)
  st_crs(locs) &lt;- st_crs(sa)

# systematic sample using extent polygon
e &lt;- st_as_sf(st_as_sfc(st_bbox(sa)))
  st_crs(e) &lt;- st_crs(sa)
s &lt;- background(e, p=1000, known=locs, d=1000)
  plot(st_geometry(s), pch=20)
    plot(st_geometry(locs), pch=20, col="red", add=TRUE)

# systematic sample using irregular polygon
s &lt;- background(sa, p=1000, known=locs, d=1000)
  plot(st_geometry(sa)) 
    plot(st_geometry(s), pch=20, add=TRUE)
      plot(st_geometry(locs), pch=20, col="red", add=TRUE)

# random sample using irregular polygon
s &lt;- background(sa, p=500, known=locs, 
                d=1000, type="random")
  plot(st_geometry(sa)) 
    plot(st_geometry(s), pch=20, add=TRUE)
      plot(st_geometry(locs), pch=20, col="red", add=TRUE)

</code></pre>

<hr>
<h2 id='bbox_poly'>Bounding box polygon</h2><span id='topic+bbox_poly'></span>

<h3>Description</h3>

<p>Creates a polygon from a vector or raster extent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bbox_poly(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bbox_poly_+3A_x">x</code></td>
<td>
<p>An sf or terra object or vector of bounding coordinates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If not a spatial object, expected order of input for x is: xmin, ymin, 
xmax, ymax. Where; xmin, ymin and the coordinates of top left corner of the 
bounding box and xmax, ymax represent the bottom right corner. The maximum 
value of xmax is width of the extent while maximum value of ymax is the height 
of the extent.
</p>


<h3>Value</h3>

<p>A single feature sf class polygon object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(sp, quietly = TRUE)) {
library(terra)
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")

# raster (terra)
r &lt;- rast(ext(meuse))
  r[] &lt;- runif(ncell(r))
 crs(r) &lt;- "epsg:28992"
e &lt;- bbox_poly(r)

plot(r)
  plot(st_geometry(e), border="red", add=TRUE)

# extent vector
e &lt;- bbox_poly(c(178605, 329714, 181390, 333611)) 
  plot(e)

# vector bounding box
e &lt;- bbox_poly(meuse)

plot(st_geometry(meuse), pch=20)
  plot(st_geometry(e), add=TRUE)

} else { 
  cat("Please install sp package to run this example", "\n")
}

</code></pre>

<hr>
<h2 id='bearing.distance'>Bearing and Distance</h2><span id='topic+bearing.distance'></span>

<h3>Description</h3>

<p>Calculates a new point [X,Y] based on defined 
bearing and distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bearing.distance(x, y, distance, azimuth, EastOfNorth = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bearing.distance_+3A_x">x</code></td>
<td>
<p>x coordinate</p>
</td></tr>
<tr><td><code id="bearing.distance_+3A_y">y</code></td>
<td>
<p>y coordinate</p>
</td></tr>
<tr><td><code id="bearing.distance_+3A_distance">distance</code></td>
<td>
<p>Distance to new point (in same units as x,y)</p>
</td></tr>
<tr><td><code id="bearing.distance_+3A_azimuth">azimuth</code></td>
<td>
<p>Azimuth to new point</p>
</td></tr>
<tr><td><code id="bearing.distance_+3A_eastofnorth">EastOfNorth</code></td>
<td>
<p>Specified surveying convention</p>
</td></tr>
</table>


<h3>Details</h3>

<p>East of north is a surveying convention and defaults to true.
</p>


<h3>Value</h3>

<p>a new point representing location of baring and distance
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> pt &lt;- cbind( x=480933, y=4479433)
 bearing.distance(pt[1], pt[2], 1000, 40)

</code></pre>

<hr>
<h2 id='breeding.density'>Breeding density areas (aka, core habitat areas)</h2><span id='topic+breeding.density'></span>

<h3>Description</h3>

<p>Calculates breeding density areas base on population counts and 
spatial point density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>breeding.density(x, pop, p = 0.75, bw = 6400, b = 8500, self = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="breeding.density_+3A_x">x</code></td>
<td>
<p>sf POINT object</p>
</td></tr>
<tr><td><code id="breeding.density_+3A_pop">pop</code></td>
<td>
<p>Population count/density column in x</p>
</td></tr>
<tr><td><code id="breeding.density_+3A_p">p</code></td>
<td>
<p>Target percent of population</p>
</td></tr>
<tr><td><code id="breeding.density_+3A_bw">bw</code></td>
<td>
<p>Bandwidth distance for the kernel estimate (default 8500)</p>
</td></tr>
<tr><td><code id="breeding.density_+3A_b">b</code></td>
<td>
<p>Buffer distance (default 8500)</p>
</td></tr>
<tr><td><code id="breeding.density_+3A_self">self</code></td>
<td>
<p>(TRUE/FALSE) Should source observations be included in 
density (default TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The breeding density areas model identifies the Nth-percent population exhibiting 
the highest spatial density and counts/frequency. It then buffers these points by 
a specified distance to produce breeding area polygons. If you would like to recreate 
the results in Doherty et al., (2010), then define bw = 6400m and b[if p &lt; 0.75 
b = 6400m, | p &gt;= 0.75 b = 8500m]
</p>


<h3>Value</h3>

<p>A list object with:
</p>
 
<ul>
<li><p> pop.pts   sf POINT object with points identified within the specified p
</p>
</li>
<li><p> pop.area  sf POLYGON object of buffered points specified by parameter b
</p>
</li>
<li><p> bandwidth Specified distance bandwidth used in identifying neighbor counts 
</p>
</li>
<li><p> buffer    Specified buffer distance used in buffering points for pop.area  
</p>
</li>
<li><p> p         Specified population percent
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Doherty, K.E., J.D. Tack, J.S. Evans, D.E. Naugle (2010) Mapping breeding densities of 
greater  sage-grouse: A tool for range-wide conservation planning.  
Bureau of Land Management. Number L10PG00911
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)

n=1500
bb &lt;- rbind(c(-1281299,-761876.5),c(1915337,2566433.5))
bb.mat &lt;- round(cbind(c(bb[1,1], bb[1,2], bb[1,2], bb[1,1]),
                  c(bb[2,1], bb[2,1], bb[2,2], bb[2,2])),2)
 bbp &lt;- st_sfc(st_polygon(list(rbind(bb.mat, bb.mat[1,]))))
   pop &lt;- st_as_sf(st_sample(bbp, n, type = "random"))
  st_geometry(pop) &lt;- "geometry"
     pop$ID &lt;- 1:nrow(pop)
  pop$counts &lt;- round(runif(nrow(pop), 1,250),0)
   
    bd75 &lt;- breeding.density(pop, pop='counts', p=0.75, b=8500, bw=6400)	 
      plot(st_geometry(bd75$pop.area), border = NA,  
        main='75% breeding density areas', col="grey")
         plot(st_geometry(pop), pch=20, col='black', add=TRUE)
         plot(st_geometry(bd75$pop.pts), pch=20, col='red', add=TRUE)
      legend("bottomright", legend=c("selected areas","selected sites", "all sites"),
             bg="white", fill=c("grey","red", "black"), pt.cex = 2) 

</code></pre>

<hr>
<h2 id='built.index'>built index</h2><span id='topic+built.index'></span>

<h3>Description</h3>

<p>Remote sensing built-up index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>built.index(
  green,
  red,
  nir,
  swir1,
  swir2,
  L = 0.5,
  method = c("Bouhennache", "Zha", "Xu")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="built.index_+3A_green">green</code></td>
<td>
<p>Green band (0.53 - 0.59mm), landsat 5&amp;7 band 3, OLI
(landsat 8) band 3</p>
</td></tr>
<tr><td><code id="built.index_+3A_red">red</code></td>
<td>
<p>Red band (0.636 - 0.673mm), landsat 5&amp;7 band 3, OLI
(landsat 8) band 4</p>
</td></tr>
<tr><td><code id="built.index_+3A_nir">nir</code></td>
<td>
<p>Near infrared band (0.851 - 0.879mm) landsat 5&amp;7 band 4,
OLI (landsat 8) band 5</p>
</td></tr>
<tr><td><code id="built.index_+3A_swir1">swir1</code></td>
<td>
<p>short-wave infrared band 1 (1.566 - 1.651mm), landsat 5&amp;7
band 5, OLI (landsat 8) band 6</p>
</td></tr>
<tr><td><code id="built.index_+3A_swir2">swir2</code></td>
<td>
<p>short-wave infrared band 2 (2.11 - 2.29mm), landsat 5&amp;7
band 7, OLI (landsat 8) band 7</p>
</td></tr>
<tr><td><code id="built.index_+3A_l">L</code></td>
<td>
<p>The L factor for the savi index</p>
</td></tr>
<tr><td><code id="built.index_+3A_method">method</code></td>
<td>
<p>Method to use for index options are &quot;Bouhennache&quot;, &quot;Zha&quot;, &quot;Xu&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the built-up index. Three methods are available:
</p>

<ul>
<li><p> Bouhennache is a new method that uses a larger portion of the VIR/NIR
following OLI bands (((b3+b4+b7)-b6)/3) / (((b3+b4+b7)+b6)/3)
</p>
</li>
<li><p> Zha is the original band ratio method using TM5 ndbi = (b5 - b4) / (b5 + b4)
</p>
</li>
<li><p> Xu is a modification to eliminate noise using ETM+7
(ndbi-((savi-nndwi)/2) / (ndbi+((savi-nndwi)/2)
</p>
</li></ul>

<p>Generally water has the highest values where built-up areas will occur in the
mid portion of the distribution. Since Bouhennache et al (2018) index exploits
a larger portion of the visible (Vis) and infra red spectrum, vegetation will
occur as the  lowest values and barren will exhibit greater values than the
vegetation and lower values than the built-up areas.
</p>
<p>Band wavelength (nanometers) designations for landsat
TM4, TM5 and ETM+7
</p>

<ul>
<li><p> band-2 0.52-0.60 (green)
</p>
</li>
<li><p> band-3 0.63-0.69 (red)
</p>
</li>
<li><p> band-4 0.76-0.90 (NIR)
</p>
</li>
<li><p> band-5 1.55-1.75 (SWIR 1)
</p>
</li>
<li><p> band-7 2.09-2.35 (SWIR 2)
</p>
</li></ul>

<p>OLI (Landsat 8)
</p>

<ul>
<li><p> band-3 0.53-0.59 (green)
</p>
</li>
<li><p> band-4 0.64-0.67 (red)
</p>
</li>
<li><p> band-5 0.85-0.88 (NIR)
</p>
</li>
<li><p> band-6 1.57-1.65 (SWIR 1)
</p>
</li>
<li><p> band-7 2.11-2.29 (SWIR 2)
</p>
</li></ul>



<h3>Value</h3>

<p>A terra raster object of the built index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Bouhennache, R., T. Bouden, A. Taleb-Ahmed &amp; A. Chaddad(2018) A new spectral index
for the extraction of built-up land features from Landsat 8 satellite imagery,
Geocarto International 34(14):1531-1551
</p>
<p>Xu H. (2008) A new index for delineating built-up land features in satellite imagery.
International Journal Remote Sensing 29(14):4269-4276.
</p>
<p>Zha G.Y., J. Gao, &amp; S. Ni (2003) Use of normalized difference built-up index in
automatically mapping urban areas from TM imagery. International Journal of
Remote Sensing 24(3):583-594
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
 lsat &lt;- rast(system.file("/extdata/Landsat_TM5.tif", package="spatialEco"))
   plotRGB(lsat, r=3, g=2, b=1, scale=1.0, stretch="lin")
			   
 # Using Bouhennache et al., (2018) method (needs green, red, swir1 and swir2) 
 ( bouh &lt;- built.index(red = lsat[[3]], green = lsat[[2]], swir1 = lsat[[5]], 
                      swir2 = lsat[[6]]) )
    plotRGB(lsat, r=3, g=2, b=1, scale=1, stretch="lin")
      plot(bouh, legend=FALSE, col=rev(terrain.colors(100, alpha=0.35)), 
	       add=TRUE )

 # Using simple Zha et al., (2003) method (needs nir and swir1)
 ( zha &lt;- built.index(nir = lsat[[4]], swir1 = lsat[[5]], method = "Zha") )
   plotRGB(lsat, r=3, g=2, b=1, scale=1, stretch="lin")
     plot(zha, legend=FALSE, col=rev(terrain.colors(100, alpha=0.35)), add=TRUE )

 # Using Xu (2008) normalized modification of Zha (needs green, red, nir and swir1)
 ( xu &lt;- built.index(green= lsat[[2]], red = lsat[[3]], nir = lsat[[4]], 
                     swir1 = lsat[[5]], , method = "Xu") )
   plotRGB(lsat, r=3, g=2, b=1, scale=1, stretch="lin")
     plot(xu, legend=FALSE, col=rev(terrain.colors(100, alpha=0.35)), add=TRUE ) 


</code></pre>

<hr>
<h2 id='cgls_urls'>Provide URL's for Copernicus Global Land Service datasets</h2><span id='topic+cgls_urls'></span>

<h3>Description</h3>

<p>Returns URL's of a product/version/resolution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cgls_urls(
  dates = NULL,
  resolution = c(1000, 300),
  product = c("fapar", "fcover", "lai", "ndvi"),
  ver = c("newest", "v1", "v2", "v3")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cgls_urls_+3A_dates">dates</code></td>
<td>
<p>Dates to subset default is NULL, returns all products</p>
</td></tr>
<tr><td><code id="cgls_urls_+3A_resolution">resolution</code></td>
<td>
<p>The product resolution c(&quot;1km&quot;, &quot;300m&quot;),</p>
</td></tr>
<tr><td><code id="cgls_urls_+3A_product">product</code></td>
<td>
<p>Which product to query options are &quot;fapar&quot;,</p>
</td></tr>
<tr><td><code id="cgls_urls_+3A_ver">ver</code></td>
<td>
<p>Product version options are &quot;newest&quot;, &quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Provides a query of the ESA's Copernicus Global Land Service global
The query is performed on the manifest files and return URL's
however, to download data you will need login credentials which,
can be acquired from: http://land.copernicus.eu
</p>
<p>If provided, dates need to be in a &quot;YYYY-MM-DD&quot; format. The dates
are an explicit search string and can contain dates that are not in
the imagery. As such, the user should generate a daily date string
representing the range of the desired download as not to have to
guess the available dates. Also note that multiple processing versions
of a given image are retained in the manifest. This means that if you
download a previous processing version, it could be an invalid image.
It is highly recommended that you do not change the default
ver=&quot;newest&quot; argument unless there is a specific reason to.
</p>
<p>Available products
</p>

<ul>
<li><p> fapar    Fraction of photosynthetically active radiation
absorbed by the vegetation
</p>
</li>
<li><p> fcover   Fraction of green vegetation cover
</p>
</li>
<li><p> lai      Leaf Area index
</p>
</li>
<li><p> ndvi     Normalized Difference Vegetation Index
</p>
</li></ul>

<p>Not yet implemented; Soil Water Index, Surface Soil Moisture, and Land Surface Temperature.
</p>


<h3>Value</h3>

<p>A vector of download URL's for the products
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create date string for query
d &lt;- seq(as.Date("2020/05/01"), as.Date("2020-09-01"), by="day")

# Search for 300m (333m) LAI within specified date range 
( dates.lai &lt;- cgls_urls(dates = d, resolution = 300, 
                        product = "lai") )

# Return all 300m LAI 
all.lai &lt;- cgls_urls(resolution = 300, product = "lai")
  nrow(all.lai)


## Not run: 						 
# Example for downloading URL's
# You need to define your login credentials to download data
#   username = "xxxx"  
#   password = "xxxx" 
  
  for(i in 1:length(dates.lai)){
    if(i &gt; 1){ Sys.sleep(3) }
    file.url &lt;- paste0("https://", paste(username, password, sep=":"), "@", 
                       sub(".*//", "", dates.lai[i]))  
      download.file(file.url, file.path(tempdir(), 
                    basename(dates.lai[i])), mode = 'wb') 
  }

## End(Not run)

</code></pre>

<hr>
<h2 id='chae'>Canine-Human Age Equivalent</h2><span id='topic+chae'></span>

<h3>Description</h3>

<p>Calculates canines equivalent human age (for fun)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chae(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chae_+3A_x">x</code></td>
<td>
<p>numeric vector, dog age</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector, equivalent human age
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Wang, T., J. M, A.N. Hogan, S. Fong, K. Licon et al.  (2020) quantitative 
translation of dog-to-human aging by conserved remodeling of epigenetic 
networks. Cell Systems 11(2)176-185
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(DogAge = seq(0,18,0.25),
             HumanAge=chae(seq(0,18,0.25)))[-1,]

plot(dat$DogAge, dat$HumanAge, "l",
     main="Canine-Human Age Equivalence",
	 ylab="Human Age", xlab="Dog Age")
  points( 15, chae(15), col="red", pch=19, cex=1.5)
  points( 10, chae(10), col="blue", pch=19, cex=1.5)
  points( 3, chae(3), col="black", pch=19, cex=1.5)
legend("bottomright", legend=c("Camas (15-YO)", "Kele (10-YO)", "Aster (3-YO)"), 
      pch=c(19,19,19), cex=c(1.5,1.5,1.5), 
	     col=c("red","blue","black"))  

</code></pre>

<hr>
<h2 id='chen'>Cross-correlation data from Chen (2015)</h2><span id='topic+chen'></span>

<h3>Description</h3>

<p>Urbanization and economic development data from Chen (2015)
compiled from, National Bureau of Statistics of China
</p>


<h3>Format</h3>

<p>A list object with 3 elements:
</p>

<dl>
<dt>X</dt><dd><p>per capita GRP(yuan)</p>
</dd>
<dt>Y</dt><dd><p>Level of urbanization percent</p>
</dd>
<dt>M</dt><dd><p>Railway Distance (km) matrix of 29 Chinese regions</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126158">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126158</a>
</p>


<h3>References</h3>

<p>Chen, Y.G. (2012) On the four types of weight functions for spatial contiguity 
matrix. Letters in Spatial and Resource Sciences 5(2):65-72
</p>
<p>Chen, Y.G. (2013) New approaches for calculating Moran’s index of spatial 
autocorrelation. PLoS ONE 8(7):e68336
</p>
<p>Chen, Y.G. (2015) A New Methodology of Spatial Cross-Correlation Analysis. 
PLoS One 10(5):e0126158. doi:10.1371/journal.pone.0126158
</p>

<hr>
<h2 id='classBreaks'>Class breaks</h2><span id='topic+classBreaks'></span>

<h3>Description</h3>

<p>Finds class breaks in a distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classBreaks(x, n, type = c("equal", "quantile", "std", "geometric"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classBreaks_+3A_x">x</code></td>
<td>
<p>A vector to find breaks for</p>
</td></tr>
<tr><td><code id="classBreaks_+3A_n">n</code></td>
<td>
<p>Number of breaks</p>
</td></tr>
<tr><td><code id="classBreaks_+3A_type">type</code></td>
<td>
<p>Statistic used to find breaks c(&quot;equal&quot;, &quot;quantile&quot;, &quot;std&quot;, &quot;geometric&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The robust std method uses sqrt(sum(x^2)/(n-1)) to center the data before deriving &quot;pretty&quot; breaks.
</p>


<h3>Value</h3>

<p>A vector containing class break values the length is n+1 to allow for 
specification of ranges
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> y &lt;- rnbinom(100, 10, 0.5)
   classBreaks(y, 10)  
   classBreaks(y, 10, type="quantile")
 
opar &lt;- par(no.readonly=TRUE)
   par(mfrow=c(2,2))
     d &lt;- density(y)
     plot(d, type="n", main="Equal Area breaks")
       polygon(d, col="cyan")
       abline(v=classBreaks(y, 10)) 
     plot(d, type="n", main="Quantile breaks")
       polygon(d, col="cyan")
       abline(v=classBreaks(y, 10, type="quantile"))
     plot(d, type="n", main="Robust Standard Deviation breaks")
       polygon(d, col="cyan")
       abline(v=classBreaks(y, 10, type="std"))
     plot(d, type="n", main="Geometric interval breaks")
       polygon(d, col="cyan")
       abline(v=classBreaks(y, 10, type="geometric"))
 par(opar)
	
 ( y.breaks &lt;- classBreaks(y, 10) )   	
 cut(y, y.breaks, include.lowest = TRUE, labels = 1:10)

</code></pre>

<hr>
<h2 id='collinear'>Collinearity test</h2><span id='topic+collinear'></span>

<h3>Description</h3>

<p>Test for linear or nonlinear collinearity/correlation in data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collinear(x, p = 0.85, nonlinear = FALSE, p.value = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collinear_+3A_x">x</code></td>
<td>
<p>A data.frame or matrix containing continuous data</p>
</td></tr>
<tr><td><code id="collinear_+3A_p">p</code></td>
<td>
<p>The correlation cutoff (default is 0.85)</p>
</td></tr>
<tr><td><code id="collinear_+3A_nonlinear">nonlinear</code></td>
<td>
<p>A boolean flag for calculating nonlinear correlations 
(FALSE/TRUE)</p>
</td></tr>
<tr><td><code id="collinear_+3A_p.value">p.value</code></td>
<td>
<p>If nonlinear is TRUE, the p value to accept as the 
significance of the correlation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Evaluation of the pairwise linear correlated variables to remove is 
accomplished through calculating the mean correlations of each variable 
and selecting the variable with higher mean. If nonlinear = TRUE, pairwise
nonlinear correlations are evaluated by fitting y as a semi-parametrically 
estimated function of x using a generalized additive model and testing 
whether or not that functional estimate is constant, which would indicate 
no relationship between y and x thus, avoiding potentially arbitrary decisions
regarding the order in a polynomial regression.
</p>


<h3>Value</h3>

<p>Messages and a vector of correlated variables
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>
<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cor.data)

# Evaluate linear correlations on linear dataCollinearity between
head( dat &lt;- cor.data[[4]] ) 
pairs(dat, pch=20)
  ( cor.vars &lt;- collinear( dat ) )

# Remove identified variable(s)
head( dat[,-which(names(dat) %in% cor.vars)] )


# Evaluate linear correlations on nonlinear data
#   using nonlinear correlation function
plot(cor.data[[1]], pch=20) 
  collinear(cor.data[[1]], p=0.80, nonlinear = TRUE ) 	
	       

</code></pre>

<hr>
<h2 id='combine'>raster combine</h2><span id='topic+combine'></span>

<h3>Description</h3>

<p>Combines rasters into all unique combinations of inputs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_+3A_x">x</code></td>
<td>
<p>raster stack/brick or SpatialPixelsDataFrame object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A single ratified raster object is returned with the summary table
as the raster attribute table, this is most similar to the ESRI 
format resulting from their combine function. 
</p>
<p>Please note that this is not a memory safe function that utilizes
out of memory in the manner that the terra package does.
</p>


<h3>Value</h3>

<p>A ratified (factor) terra SpatRaster representing unique combinations.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)

# Create example data (with a few NA's introduced)
 r1 &lt;- rast(nrows=100, ncol=100)
   names(r1) &lt;- "LC1"
   r1[] &lt;- round(runif(ncell(r1), 1,4),0)
     r1[c(8,10,50,100)] &lt;- NA
 r2 &lt;- rast(nrows=100, ncol=100)
   names(r2) &lt;- "LC2"
   r2[] &lt;- round(runif(ncell(r2), 2,6),0)
     r2[c(10,50,100)] &lt;- NA   
 r3 &lt;- rast(nrows=100, ncol=100)
   names(r3) &lt;- "LC3"
   r3[] &lt;- round(runif(ncell(r3), 2,6),0)
     r3[c(10,50,100)] &lt;- NA   
 r &lt;- c(r1,r2,r3)  
   names(r) &lt;- c("LC1","LC2","LC3")

 # Combine rasters with a multilayer stack
 cr &lt;- combine(r)
   head(cr$summary)
   plot(cr$combine)

# or, from separate layers
 cr &lt;- combine(c(r1,r3))

</code></pre>

<hr>
<h2 id='concordance'>Concordance test for binomial models</h2><span id='topic+concordance'></span>

<h3>Description</h3>

<p>Performs a concordance/disconcordance (C-statistic) test on 
binomial models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concordance(y, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concordance_+3A_y">y</code></td>
<td>
<p>vector of binomial response variable used in model</p>
</td></tr>
<tr><td><code id="concordance_+3A_p">p</code></td>
<td>
<p>estimated probabilities from fit binomial model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Test of binomial regression for the hypothesis that probabilities of all 
positives [1], are greater than the probabilities of the nulls [0]. The 
concordance would be 100
inverse of concordance, representing the null. The C-statistic has been  
show to be comparable to the area under an ROC
</p>
<p>Results are: concordance - percent of positives that are greater than 
probabilities of nulls. discordance - concordance inverse of concordance 
representing the null class, tied - number of tied probabilities and 
pairs - number of pairs compared
</p>


<h3>Value</h3>

<p>list object with: concordance, discordance, tied and pairs
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Austin, P.C. &amp; E.W. Steyerberg (2012) Interpreting the concordance statistic of a 
logistic regression model: relation to the variance and odds ratio of a continuous 
explanatory variable. BMC Medical Research Methodology, 12:82
</p>
<p>Harrell, F.E. (2001) Regression modelling strategies. Springer, New York, NY.
</p>
<p>Royston, P. &amp; D.G. Altman (2010) Visualizing and assessing discrimination in the 
logistic  regression model. Statistics in Medicine 29(24):2508-2520
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
dat &lt;- subset(mtcars, select=c(mpg, am, vs))
glm.reg &lt;- glm(vs ~ mpg, data = dat, family = binomial)
concordance(dat$vs, predict(glm.reg, type = "response")) 

</code></pre>

<hr>
<h2 id='conf.interval'>Confidence interval for mean or median</h2><span id='topic+conf.interval'></span>

<h3>Description</h3>

<p>Calculates confidence interval for the mean or median of a distribution with
unknown population variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf.interval(x, cl = 0.95, stat = "mean", std.error = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf.interval_+3A_x">x</code></td>
<td>
<p>Vector to calculate confidence interval for</p>
</td></tr>
<tr><td><code id="conf.interval_+3A_cl">cl</code></td>
<td>
<p>Percent confidence level (default = 0.95)</p>
</td></tr>
<tr><td><code id="conf.interval_+3A_stat">stat</code></td>
<td>
<p>Statistic (mean or median)</p>
</td></tr>
<tr><td><code id="conf.interval_+3A_std.error">std.error</code></td>
<td>
<p>Return standard error (TRUE/FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame contaning:
</p>

<ul>
<li><p> lci - Lower confidence interval value
</p>
</li>
<li><p> uci - Upper confidence interval value
</p>
</li>
<li><p> mean - If stat = &quot;mean&quot;, mean value of distribution
</p>
</li>
<li><p> mean - Value of the mean or median
</p>
</li>
<li><p> conf.level - Confidence level used for confidence interval
</p>
</li>
<li><p> std.error - If std.error = TRUE standard error of distribution
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- runif(100)
 cr &lt;- conf.interval(x, cl = 0.97) 
 print(cr)

 d &lt;- density(x)
 plot(d, type="n", main = "PDF with mean and 0.97 confidence interval")
   polygon(d, col="cyan3")
   abline(v=mean(x, na.rm = TRUE), lty = 2)
   segments( x0=cr[["lci"]], y0=mean(d$y), x1=cr[["uci"]], 
             y1=mean(d$y), lwd = 2.5, 
             col = "black")
 	legend("topright", legend = c("mean", "CI"), 
 	       lty = c(2,1), lwd = c(1,2.5)) 

</code></pre>

<hr>
<h2 id='cor.data'>Various correlation structures</h2><span id='topic+cor.data'></span>

<h3>Description</h3>

<p>linear and nonlinear correlated data examples
</p>
<p>A list object with various linear and nonlinear correlation structures
</p>


<h3>Format</h3>

<p>A list object with 4 elements containing data.frames:
</p>

<dl>
<dt>example 1</dt><dd><p>two columns with nonlinear wave function relationship</p>
</dd>
<dt>example 2</dt><dd><p>two columns with simple nonlinear relationship</p>
</dd>
<dt>example 3</dt><dd><p>two columns with nonlinear multi-level wave function relationship</p>
</dd>
<dt>example 4</dt><dd><p>4 columns with first two having linear relationship</p>
</dd>
</dl>


<hr>
<h2 id='correlogram'>Correlogram</h2><span id='topic+correlogram'></span>

<h3>Description</h3>

<p>Calculates and plots a correlogram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correlogram(x, v, dist = 5000, ns = 99, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correlogram_+3A_x">x</code></td>
<td>
<p>A sf POINT object</p>
</td></tr>
<tr><td><code id="correlogram_+3A_v">v</code></td>
<td>
<p>Test variable in x</p>
</td></tr>
<tr><td><code id="correlogram_+3A_dist">dist</code></td>
<td>
<p>Distance of correlation lags, if latlong=TRUE units are
great circle in kilometers</p>
</td></tr>
<tr><td><code id="correlogram_+3A_ns">ns</code></td>
<td>
<p>Number of simulations to derive simulation envelope</p>
</td></tr>
<tr><td><code id="correlogram_+3A_...">...</code></td>
<td>
<p>Arguments passed to cor ('pearson', 'kendall' or 'spearman')</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of correlogram and a list object containing:
</p>

<ul>
<li><p> autocorrelation is a data.frame object with the following components
</p>
</li>
<li><p> autocorrelation - Autocorrelation value for each distance lag
</p>
</li>
<li><p> dist - Value of distance lag
</p>
</li>
<li><p> lci - Lower confidence interval (p=0.025)
</p>
</li>
<li><p> uci - Upper confidence interval (p=0.975)
</p>
</li>
<li><p> CorrPlot recordedplot object to recall plot
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
if(require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")
}

zinc.cg &lt;- correlogram(x = meuse, v = meuse$zinc, dist = 250, ns = 9)

</code></pre>

<hr>
<h2 id='cross.tab'>Class comparison between two nominal rasters</h2><span id='topic+cross.tab'></span>

<h3>Description</h3>

<p>Creates a labeled cross tabulation between two nominal rasters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cross.tab(x, y, values = NULL, labs = NULL, pct = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cross.tab_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="cross.tab_+3A_y">y</code></td>
<td>
<p>A terra SpatRaster class object to compare to x</p>
</td></tr>
<tr><td><code id="cross.tab_+3A_values">values</code></td>
<td>
<p>Expected values in both rasters</p>
</td></tr>
<tr><td><code id="cross.tab_+3A_labs">labs</code></td>
<td>
<p>Labels associated with values argument</p>
</td></tr>
<tr><td><code id="cross.tab_+3A_pct">pct</code></td>
<td>
<p>(TRUE/FALSE) return proportions rather than counts</p>
</td></tr>
<tr><td><code id="cross.tab_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns a cross tabulation between two nominal rasters. 
Arguments allow for labeling the results and returning proportions 
rather than counts. It also accounts for asymmetrical classes between
the two rasters
</p>


<h3>Value</h3>

<p>a table with the cross tabulated counts
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Pontius Jr, R.G., Shusas, E., McEachern, M. (2004). Detecting important categorical land changes
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
 
e &lt;- ext(179407.8, 181087.9, 331134.4, 332332.1)
lulc2010 &lt;- rast(e, resolution=20)
  lulc2010[] &lt;- sample(1:5, ncell(lulc2010), replace=TRUE)
lulc2020 &lt;- rast(e, resolution=20)
  lulc2020[] &lt;- sample(1:5, ncell(lulc2020), replace=TRUE)
 
 ( v = sort(unique(c(lulc2010[], lulc2020[]))) )
 l = c("water","urban","forest",
       "ag","barren")

cross.tab(lulc2010, lulc2020) 
cross.tab(lulc2010, lulc2020, values = v, labs = l)
cross.tab(lulc2010, lulc2020, values = v, labs = l, pct=TRUE)

# Create asymmetrical classes 
na.idx &lt;- which(!is.na(lulc2010[]))
lulc2020[na.idx] &lt;- sample(c(1,2,4,5), length(na.idx), replace=TRUE)
cross.tab(lulc2010, lulc2020, values = v, labs = l, pct=TRUE)


</code></pre>

<hr>
<h2 id='crossCorrelation'>Spatial cross correlation</h2><span id='topic+crossCorrelation'></span>

<h3>Description</h3>

<p>Calculates univariate or bivariate spatial cross-correlation using
local Moran's-I (LISA), following Chen (2015)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossCorrelation(
  x,
  y = NULL,
  coords = NULL,
  w = NULL,
  type = c("LSCI", "GSCI"),
  k = 999,
  dist.function = c("inv.power", "neg.exponent", "none"),
  scale.xy = TRUE,
  scale.partial = FALSE,
  scale.matrix = FALSE,
  alpha = 0.05,
  clust = TRUE,
  return.sims = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossCorrelation_+3A_x">x</code></td>
<td>
<p>Vector of x response variables</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_y">y</code></td>
<td>
<p>Vector of y response variables, if not specified the
univariate  statistic is returned</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_coords">coords</code></td>
<td>
<p>A matrix of coordinates corresponding to (x,y), only
used if w = NULL. Can also be an sp object with relevant
x,y coordinate slot (ie., points or polygons)</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_w">w</code></td>
<td>
<p>Spatial neighbors/weights in matrix format. Dimensions
must match (n(x),n(y)) and be symmetrical. If w is not defined
then a default method is used.</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_type">type</code></td>
<td>
<p>c(&quot;LSCI&quot;,&quot;GSCI&quot;) Return Local Spatial Cross-correlation Index (LSCI)
or Global Spatial cross-correlation Index (GSCI)</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_k">k</code></td>
<td>
<p>Number of simulations for calculating permutation distribution
under the null hypothesis of no spatial autocorrelation</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_dist.function">dist.function</code></td>
<td>
<p>(&quot;inv.power&quot;, &quot;neg.exponent&quot;, &quot;none&quot;) If w = NULL, the default method
for deriving spatial weights matrix, options are: inverse power
or negative exponent, none is for use with a provided matrix</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_scale.xy">scale.xy</code></td>
<td>
<p>(TRUE/FALSE) scale the x,y vectors, if FALSE it is assumed that
they are already scaled following Chen (2015)</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_scale.partial">scale.partial</code></td>
<td>
<p>(FALSE/TRUE) rescale partial spatial autocorrelation statistics</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_scale.matrix">scale.matrix</code></td>
<td>
<p>(FALSE/TRUE) If a neighbor/distance matrix is passed, should it
be scaled using (w/sum(w))</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_alpha">alpha</code></td>
<td>
<p>= 0.05     confidence interval (default is 95 pct)</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_clust">clust</code></td>
<td>
<p>(FALSE/TRUE) Return approximated lisa clusters</p>
</td></tr>
<tr><td><code id="crossCorrelation_+3A_return.sims">return.sims</code></td>
<td>
<p>(FALSE/TRUE) Return randomizations vector n = k</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In specifying a distance matrix, you can pass a coordinates matrix or spatial
object to coords or alternately, pass a distance or spatial weights matrix to
the w argument. If the w matrix represents spatial weights dist.function=&quot;none&quot;
should be specified. Otherwise, w is assumed to represent distance and will be
converted to spatial weights using inv.power or neg.exponent. The w distances
can represent an alternate distance hypothesis (eg., road, stream, network distance)
Here are example argument usages for defining a matrix.
</p>

<ul>
<li><p> IF coords=x, w=NULL, dist.function= c(&quot;inv.power&quot;, &quot;neg.exponent&quot;)
A distance matrix is derived using the data passed to coords then
spatial weights derived using one of the dist.function options
</p>
</li>
<li><p> IF cords=NULL, w=x, dist.function= c(&quot;inv.power&quot;, &quot;neg.exponent&quot;)
It is expected that the distance matrix specified with w represent
some form of distance then the spatial weights are derived using
one of the dist.function options
</p>
</li>
<li><p> IF cords=NULL, w=x, dist.function=&quot;none&quot;
It is assumed that the matrix passed to w already represents
the spatial weights
</p>
</li></ul>



<h3>Value</h3>

<p>When not simulated k=0, a list containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value of NULL to represent p values were derived from observed data (k=0)
</p>
</li>
<li><p> p - Probability based observations above/below confidence interval
</p>
</li>
<li><p> t.test - Probability based on t-test
</p>
</li>
<li><p> clusters - If &quot;clust&quot; argument TRUE, vector representing LISA clusters
</p>
</li></ul>

<p>When simulated (k&gt;0), a list containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value representing number of simulations
</p>
</li>
<li><p> global.p - p-value of global autocorrelation statistic
</p>
</li>
<li><p> local.p - Probability based simulated data using successful rejection of t-test
</p>
</li>
<li><p> range.p - Probability based on range of probabilities resulting from paired t-test
</p>
</li>
<li><p> clusters - If &quot;clust&quot; argument TRUE, vector representing lisa clusters
</p>
</li></ul>



<h3>References</h3>

<p>Chen, Y.G. (2012) On the four types of weight functions for spatial contiguity
matrix. Letters in Spatial and Resource Sciences 5(2):65-72
</p>
<p>Chen, Y.G. (2013) New approaches for calculating Moran’s index of spatial
autocorrelation. PLoS ONE 8(7):e68336
</p>
<p>Chen, Y.G. (2015) A New Methodology of Spatial Cross-Correlation Analysis.
PLoS One 10(5):e0126158. doi:10.1371/journal.pone.0126158
</p>


<h3>Examples</h3>

<pre><code class='language-R'># replicate Chen (2015)
 data(chen)
( r &lt;- crossCorrelation(x=chen[["X"]], y=chen[["Y"]], w = chen[["M"]],  
                        clust=TRUE, type = "LSCI", k=0, 
                        dist.function = "inv.power") ) 


library(sf)
library(spdep)
 
  if (require(sp, quietly = TRUE)) {
   data(meuse, package = "sp")
   meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, agr = "constant")
  } 

#### Using a default spatial weights matrix method (inverse power function)
( I &lt;- crossCorrelation(meuse$zinc, meuse$copper, 
             coords = st_coordinates(meuse)[,1:2], k=99) )
  meuse$lisa &lt;- I$SCI[,"lsci.xy"]
    plot(meuse["lisa"], pch=20)

#### Providing a distance matrix
if (require(units, quietly = TRUE)) {
  Wij &lt;- units::drop_units(st_distance(meuse))
 ( I &lt;- crossCorrelation(meuse$zinc, meuse$copper, w = Wij, k=99) )

#### Providing an inverse power function weights matrix
  Wij &lt;- 1 / Wij
    diag(Wij) &lt;- 0 
      Wij &lt;- Wij / sum(Wij) 
        diag(Wij) &lt;- 0
 ( I &lt;- crossCorrelation(meuse$zinc, meuse$copper, w = Wij, 
                         dist.function = "none", k=99) )
}
 

</code></pre>

<hr>
<h2 id='csi'>Cosine Similarity Index</h2><span id='topic+csi'></span>

<h3>Description</h3>

<p>Calculates the cosine similarity and angular similarity on  
two vectors or a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csi(x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csi_+3A_x">x</code></td>
<td>
<p>A vector or matrix object</p>
</td></tr>
<tr><td><code id="csi_+3A_y">y</code></td>
<td>
<p>If x is a vector, then a vector object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cosine similarity index is a measure of similarity between two 
vectors of an inner product space. This index is bested suited for high-dimensional 
positive variable space. One useful application of the index is to measure separability 
of clusters derived from algorithmic approaches (e.g., k-means). It is a good common 
practice to center the data before calculating the index. It should be noted that the 
cosine similarity index is mathematically, and often numerically, equivalent to the 
Pearson's correlation coefficient   
</p>
<p>The cosine similarity index is derived: 
s(xy) = x * y / ||x|| * ||y||, where the expected is 1.0 (perfect similarity)  
to -1.0 (perfect dissimilarity). A normalized angle between the vectors can 
be used as a bounded similarity function within [0,1] 
angular similarity  = 1 - (cos(s)^-1/pi)
</p>


<h3>Value</h3>

<p>If x is a matrix, a list object with: similarity and angular.similarity matrices or,  
if x and y are vectors, a vector of similarity and angular.similarity
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compare two vectors (centered using scale)
x=runif(100)
y=runif(100)^2
csi(as.vector(scale(x)),as.vector(scale(y)))
  
# Compare columns (vectors) in a matrix (centered using scale)
x &lt;- matrix(round(runif(100),0),nrow=20,ncol=5)
( s &lt;- csi(scale(x)) )
    
# Compare vector (x) to each column in a matrix (y)
y &lt;- matrix(round(runif(500),3),nrow=100,ncol=5)
x=runif(100) 
csi(as.vector(scale(x)),scale(y))

</code></pre>

<hr>
<h2 id='curvature'>Surface curvature</h2><span id='topic+curvature'></span>

<h3>Description</h3>

<p>Calculates Zevenbergen &amp; Thorne, McNab's or Bolstad's curvature
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curvature(x, type = c("planform", "profile", "total", "mcnab", "bolstad"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="curvature_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="curvature_+3A_type">type</code></td>
<td>
<p>Method used c(&quot;planform&quot;, &quot;profile&quot;, &quot;total&quot;, &quot;mcnab&quot;, &quot;bolstad&quot;)</p>
</td></tr>
<tr><td><code id="curvature_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to focal</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The planform and profile curvatures are the second derivative(s) of the 
elevation surface, or the slope of the slope. Profile curvature is in 
the direction of the maximum slope, and the planform curvature is 
perpendicular to the direction of the maximum slope.
Negative values in the profile curvature indicate the surface is upwardly
convex whereas, positive values indicate that the surface is upwardly concave.  
Positive values in the planform curvature indicate an that the surface is 
laterally convex whereas, negative values indicate that the surface is 
laterally concave.
</p>
<p>Total curvature is the sigma of the profile and planform curvatures. A value of 
0 in profile, planform or total curvature, indicates the surface is flat. 
The planform, profile and total curvatures are derived using 
Zevenbergen &amp; Thorne (1987) via a quadratic equation fit to eight neighbors 
as such, the s (focal window size) argument is ignored. 
</p>
<p>McNab's and Bolstad's variants of the surface curvature (concavity/convexity) 
index (McNab 1993; Bolstad &amp; Lillesand 1992; McNab 1989). The index is based 
on features that confine the view from the center of a 
3x3 window. In the Bolstad equation, edge correction is addressed 
by dividing by the radius distance to the outermost cell (36.2m).
</p>


<h3>Value</h3>

<p>raster class object of surface curvature
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Bolstad, P.V., and T.M. Lillesand (1992). Improved classification of forest 
vegetation in northern Wisconsin through a rule-based combination of soils, 
terrain, and Landsat TM data. Forest Science. 38(1):5-20.
</p>
<p>Florinsky, I.V. (1998). Accuracy of Local Topographic Variables Derived from 
Digital Elevation Models. International Journal of Geographical Information 
Science, 12(1):47-62.
</p>
<p>McNab, H.W. (1989). Terrain shape index: quantifying effect of minor landforms 
on tree height. Forest Science. 35(1):91-104.
</p>
<p>McNab, H.W. (1993). A topographic index to quantify the effect of mesoscale 
landform on site productivity. Canadian Journal of Forest Research. 23:1100-1107.
</p>
<p>Zevenbergen, L.W. &amp; C.R. Thorne (1987). Quantitative Analysis of Land Surface 
Topography. Earth Surface Processes and Landforms, 12:47-56.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  library(terra)
  elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))

  crv &lt;- curvature(elev, type = "planform")
  mcnab.crv &lt;- curvature(elev, type = "mcnab")
      plot(mcnab.crv, main="McNab's curvature") 


</code></pre>

<hr>
<h2 id='dahi'>Diurnal Anisotropic Heat Index</h2><span id='topic+dahi'></span>

<h3>Description</h3>

<p>Simple approximation of the anisotropic diurnal heat (Ha) distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dahi(x, amax = 202.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dahi_+3A_x">x</code></td>
<td>
<p>An elevation raster of class terra SpatRaster</p>
</td></tr>
<tr><td><code id="dahi_+3A_amax">amax</code></td>
<td>
<p>The Alpha Max (amax) parameter in degrees defined 
as: minimum = 0, maximum = 360 with the default = 202.500</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Diurnal Anisotropic Heat Index is based on this equation.  
Ha = cos(amax - a) * arctan(b)
Where; amax defines the aspect with the maximum total heat  
surplus, a is the aspect and b is the slope angle.
</p>


<h3>Value</h3>

<p>terra SpatRaster class object Diurnal Anisotropic Heat Index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Boehner, J., and Antonic, O. (2009) Land-surface parameters specific to 
topo-climatology. In: Hengl, T., &amp; Reuter, H. (Eds.), Geomorphometry -
Concepts, Software, Applications. Developments in Soil Science, 
33:195-226
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
Ha &lt;- dahi(elev)
  plot(Ha)

</code></pre>

<hr>
<h2 id='date_seq'>date sequence</h2><span id='topic+date_seq'></span>

<h3>Description</h3>

<p>creates date sequence given start and stop dates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>date_seq(
  start,
  end,
  step = c("day", "week", "month", "quarter", "year", "minute"),
  rm.leap = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="date_seq_+3A_start">start</code></td>
<td>
<p>Start date in &quot;yyyy/mm/dd&quot; character format</p>
</td></tr>
<tr><td><code id="date_seq_+3A_end">end</code></td>
<td>
<p>End date in &quot;yyyy/mm/dd&quot; character format</p>
</td></tr>
<tr><td><code id="date_seq_+3A_step">step</code></td>
<td>
<p>Time step, options are c(&quot;day&quot;, &quot;week&quot;, &quot;month&quot;, &quot;quarter&quot;, 
&quot;year&quot;, &quot;minute&quot;)</p>
</td></tr>
<tr><td><code id="date_seq_+3A_rm.leap">rm.leap</code></td>
<td>
<p>Remove extra days in leap years</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A date vector of class POSIXct for minute and Date for other options
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># monthly steps 1990/01/01 - 2019/12/31
d &lt;- date_seq("1990/01/01", "2019/12/31", step="month")

# daily steps 1990/01/01 - 2019/12/31
d &lt;- date_seq("1990/01/01", "2019/12/31", step="day")

# daily steps 1990/01/01 - 2019/12/31 with leap days removed
d &lt;- date_seq("1990/01/01", "2019/12/31", step="day", rm.leap=TRUE)

# daily step 2008/12/29 - 2008/12/31, 2008 is leap year
d &lt;- date_seq("2008/12/29", "2008/12/31")

# minutes step 2008/12/29 - 2008/12/31, 2008 is leap year
d &lt;- date_seq("2008/12/29", "2008/12/31", step="minute")

</code></pre>

<hr>
<h2 id='daymet.point'>DAYMET point values</h2><span id='topic+daymet.point'></span>

<h3>Description</h3>

<p>Downloads DAYMET climate variables for specified point and time-period
</p>


<h3>Usage</h3>

<pre><code class='language-R'>daymet.point(
  lat,
  long,
  start.year,
  end.year,
  site = NULL,
  files = FALSE,
  echo = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="daymet.point_+3A_lat">lat</code></td>
<td>
<p>latitude of point (decimal degrees WGS84)</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_long">long</code></td>
<td>
<p>longitude pf point (decimal degrees WGS84)</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_start.year">start.year</code></td>
<td>
<p>First year of data</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_end.year">end.year</code></td>
<td>
<p>Last year of data</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_site">site</code></td>
<td>
<p>Unique identification value that is appended to data</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_files">files</code></td>
<td>
<p>(TRUE/FALSE) Write file to disk</p>
</td></tr>
<tr><td><code id="daymet.point_+3A_echo">echo</code></td>
<td>
<p>(TRUE/FALSE) Echo progress</p>
</td></tr>
</table>


<h3>Details</h3>

<p>data is available for Long -131.0 W and -53.0 W; lat 52.0 N and 14.5 N
Function uses the Single Pixel Extraction tool and returns year, yday, 
dayl(s), prcp (mm/day), srad (W/m^2), swe (kg/m^2), tmax (deg c), 
tmin (deg c), vp (Pa)
Metadata for DAYMET single pixel extraction: 
<a href="https://daymet.ornl.gov/files/UserGuides/current/readme_singlepointextraction.pdf">https://daymet.ornl.gov/files/UserGuides/current/readme_singlepointextraction.pdf</a>
</p>


<h3>Value</h3>

<p>A data.frame with geographic coordinate point-level climate results
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
( d &lt;- daymet.point(lat = 36.0133, long = -84.2625, start.year = 2013, 
                    end.year=2014, site = "1", files = FALSE, echo = FALSE) )


</code></pre>

<hr>
<h2 id='daymet.tiles'>DAYMET Tile ID's</h2><span id='topic+daymet.tiles'></span>

<h3>Description</h3>

<p>Returns a vector of DAYMET tile id's within a specified extent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>daymet.tiles(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="daymet.tiles_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of DAYMET tile IDS or if sp = TRUE a sp class SpatialPolygonsDataFrame
</p>


<h3>Note</h3>

<p>Function accepts sp, raster or extent class object or bounding coordinates. 
All input must be in the same projection as the tile index SpatialPolygonsDataFrame. 
The library includes the DAYMAT tile index &quot;DAYMET_tiles&quot; which can be add using 
data(), see examples.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>

<hr>
<h2 id='dispersion'>Dispersion (H-prime)</h2><span id='topic+dispersion'></span>

<h3>Description</h3>

<p>Calculates the dispersion (&quot;rarity&quot;) of targets associated 
with planning units
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dispersion(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dispersion_+3A_x">x</code></td>
<td>
<p>data.frame object of target values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dispersion index (H-prime) is calculated H = sum( sqrt(p) / sqrt(a) ) 
where; P = (sum of target in planning unit / sum of target across all 
planning units) and a = (count of planning units containing 
target / number of planning units)
</p>


<h3>Value</h3>

<p>data.frame with columns H values for each target, H , sH, sHmax
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Evans, J.S., S.R. Schill, G.T. Raber (2015) A Systematic Framework for Spatial 
Conservation Planning and Ecological Priority Design in St. Lucia, Eastern Caribbean. 
Chapter 26 in Central American Biodiversity : Conservation, Ecology and a Sustainable 
Future. F. Huettman (eds). Springer, NY.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
 library(sf)
   data(pu)
  
 d &lt;- dispersion(st_drop_geometry(pu[,2:ncol(pu)]))  
   p &lt;- d[,"H"]
 clr &lt;- c("#3288BD", "#99D594", "#E6F598", "#FEE08B", 
          "#FC8D59", "#D53E4F")      
 clrs &lt;- ifelse(p &lt; 0.5524462, clr[1], 
           ifelse(p &gt;= 0.5524462 &amp; p &lt; 1.223523, clr[2],
             ifelse(p &gt;= 1.223523 &amp; p &lt; 2.465613, clr[3],
 	          ifelse(p &gt;= 2.465613 &amp; p &lt; 4.76429, clr[4],
 	            ifelse(p &gt;= 4.76429 &amp; p &lt; 8.817699, clr[5],
 	              ifelse(p &gt;= 8.817699, clr[6], NA))))))
 plot(st_geometry(pu), col=clrs, border=NA)
   legend("bottomleft", legend=rev(c("Very Rare","Rare","Moderately Rare",
          "Somewhat Common","Common","Over Dispersed")),
          fill=clr, cex=0.6, bty="n") 
   box()


</code></pre>

<hr>
<h2 id='dissection'>Dissection</h2><span id='topic+dissection'></span>

<h3>Description</h3>

<p>Calculates the Evans (1972) Martonne's modified 
dissection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dissection(x, s = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dissection_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="dissection_+3A_s">s</code></td>
<td>
<p>Focal window size</p>
</td></tr>
<tr><td><code id="dissection_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::lapp</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dissection is calculated as: 
( z(s) - min(z(s)) ) / ( max(z(s)) - min(z(s)) )
</p>


<h3>Value</h3>

<p>A SpatRaster class object of Martonne's modified dissection
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  d &lt;- dissection(elev, s=3)
    plot(d, main="dissection") 
     

</code></pre>

<hr>
<h2 id='divergence'>divergence</h2><span id='topic+divergence'></span>

<h3>Description</h3>

<p>Kullback-Leibler Divergence (Cross-entropy)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>divergence(x, y, type = c("Kullback-Leibler", "cross-entropy"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="divergence_+3A_x">x</code></td>
<td>
<p>a vector of integer values, defining observed</p>
</td></tr>
<tr><td><code id="divergence_+3A_y">y</code></td>
<td>
<p>a vector of integer values, defining estimates</p>
</td></tr>
<tr><td><code id="divergence_+3A_type">type</code></td>
<td>
<p>Type of divergence statistic c(&quot;Kullback-Leibler&quot;, 
&quot;cross-entropy&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>single value vector with divergence statistic
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- round(runif(10,1,4),0)
y &lt;- round(runif(10,1,4),0)

divergence(x, y) 
divergence(x, y, type = "cross-entropy") 

</code></pre>

<hr>
<h2 id='effect.size'>Cohen's-d effect size</h2><span id='topic+effect.size'></span>

<h3>Description</h3>

<p>Cohen's-d effect size with pooled sd for a control and experimental group
</p>


<h3>Usage</h3>

<pre><code class='language-R'>effect.size(y, x, pooled = TRUE, conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="effect.size_+3A_y">y</code></td>
<td>
<p>A character or factor vector</p>
</td></tr>
<tr><td><code id="effect.size_+3A_x">x</code></td>
<td>
<p>A numeric vector, same length as y</p>
</td></tr>
<tr><td><code id="effect.size_+3A_pooled">pooled</code></td>
<td>
<p>Pooled or population standard deviation (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="effect.size_+3A_conf.level">conf.level</code></td>
<td>
<p>Specified confidence interval. Default is 0.95</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An effect.size class object with x, y and a data.frame with columns for effect 
size, lower confidence interval, lower confidence interval. The row names of 
the data frame represent the levels in y
</p>


<h3>Note</h3>

<p>This implementation will iterate through each class in y and treating a given class 
as the experimental group and all other classes as a control case. Each class had d 
and the confidence interval derived. A negative d indicate directionality with same 
magnitude. The expected range for d is 0 - 3 
d is derived; ( mean(experimental group) - mean(control group) ) / sigma(p)
pooled standard deviation is derived; 
sqrt(  ( (Ne - 1) * sigma(e)^2 + (Nc - 1) * sigma(c)^2 ) / (Ne + Nc - 2) )   
where; Ne, Nc = n of experimental and control groups.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Cohen, J., (1988) Statistical Power Analysis for the Behavioral Sciences 
(second ed.). Lawrence Erlbaum Associates.
</p>
<p>Cohen, J (1992) A power primer. Psychological Bulletin 112(1):155-159
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ( es &lt;- effect.size(iris$Species, iris$Sepal.Length) )
   plot(es)

</code></pre>

<hr>
<h2 id='elev'>Elevation raster</h2><span id='topic+elev'></span>

<h3>Description</h3>

<p>elevation raster of Switzerland
</p>


<h3>Format</h3>

<p>A raster RasterLayer class object:
</p>

<dl>
<dt>resoultion</dt><dd><p>5 arc-minute 0.00833 (10000m)</p>
</dd>
<dt>nrow</dt><dd><p>264</p>
</dd>
<dt>ncol</dt><dd><p>564</p>
</dd>
<dt>ncell</dt><dd><p>148896</p>
</dd>
<dt>xmin</dt><dd><p>5.9</p>
</dd>
<dt>xmax</dt><dd><p>10.6</p>
</dd>
<dt>ymin</dt><dd><p>45.7</p>
</dd>
<dt>ymax</dt><dd><p>47.9</p>
</dd>
<dt>proj4string</dt><dd><p>+proj=longlat +ellps=WGS84</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://www.diva-gis.org/Data">http://www.diva-gis.org/Data</a>
</p>

<hr>
<h2 id='erase.point'>Erase points</h2><span id='topic+erase.point'></span>

<h3>Description</h3>

<p>Removes points intersecting a polygon feature class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>erase.point(y, x, inside = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="erase.point_+3A_y">y</code></td>
<td>
<p>A sf POINT object</p>
</td></tr>
<tr><td><code id="erase.point_+3A_x">x</code></td>
<td>
<p>A sf POLYGON object</p>
</td></tr>
<tr><td><code id="erase.point_+3A_inside">inside</code></td>
<td>
<p>(TRUE/FALSE) Remove points inside polygon, else outside polygon</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used to erase points that intersect polygon(s). The default of inside=TRUE
erases points inside the polygons however, if inside=FALSE then
the function results in an intersection where points that
intersect the polygon are retained.
</p>


<h3>Value</h3>

<p>An sf POINT object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans    &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
  
if (require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, agr = "constant")

  s &lt;- st_as_sf(st_sample(st_as_sfc(st_bbox(meuse)), size=1000, 
                 type = "regular"))
    s$id &lt;- 1:nrow(s)
  b &lt;- st_buffer(s[sample(1:nrow(s),5),], dist=300)
    b$id &lt;- 1:nrow(b)
  
# Erase points based on polygons
in.erase &lt;- erase.point(s, b)
out.erase &lt;- erase.point(s, b, inside = FALSE)

 opar &lt;- par(no.readonly=TRUE)
 par(mfrow=c(2,2))
   plot(st_geometry(s), pch=20, main="original data")
     plot(st_geometry(b),add=TRUE)
   plot(st_geometry(in.erase), pch=20, main="erased data")
     plot(st_geometry(b),add=TRUE)
   plot(st_geometry(out.erase), pch=20,  
        main="erased data using inside=FALSE")
     plot(st_geometry(b),add=TRUE)
 par(opar)

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='extract.vertices'>Extract vertices for polygons or lines</h2><span id='topic+extract.vertices'></span>

<h3>Description</h3>

<p>Extracts [x,y] vertices from an sf line or polygon object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract.vertices(x, join = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract.vertices_+3A_x">x</code></td>
<td>
<p>An sf line or polygon class object</p>
</td></tr>
<tr><td><code id="extract.vertices_+3A_join">join</code></td>
<td>
<p>(TRUE/FALSE) Joint attributes from original object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns the vertices of a line or polygon object, as opposed
to the polygon centroids or line start/stop coordinates
</p>


<h3>Value</h3>

<p>An sf POINT object of extrated line or polygon vertices
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
nc &lt;- sf::st_read(system.file("shape/nc.shp", package="sf"))
  nc &lt;- suppressWarnings(sf::st_cast(nc, "POLYGON"))
    nc &lt;- nc[c(10,50),]
  
( v &lt;- extract.vertices(nc) )
  plot(st_geometry(nc))
    plot(st_geometry(v), pch=20, cex=2, col="red", add=TRUE)

</code></pre>

<hr>
<h2 id='fuzzySum'>Fuzzy Sum</h2><span id='topic+fuzzySum'></span>

<h3>Description</h3>

<p>Calculates the fuzzy sum of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzzySum(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzzySum_+3A_x">x</code></td>
<td>
<p>Vector of values to apply fuzzy sum</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The fuzzy sum is an increasing linear combination of values. 
This can be used to sum probabilities or results of multiple 
density functions.
</p>


<h3>Value</h3>

<p>Value of fuzzy sum
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = c(0.8,0.76,0.87)
  fuzzySum(p)
  sum(p)

p = c(0.3,0.2,0.1)
  fuzzySum(p)
  sum(p)  

</code></pre>

<hr>
<h2 id='gaussian.kernel'>Gaussian Kernel</h2><span id='topic+gaussian.kernel'></span>

<h3>Description</h3>

<p>Creates a Gaussian Kernel of specified size and sigma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian.kernel(sigma = 2, s = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian.kernel_+3A_sigma">sigma</code></td>
<td>
<p>sigma (standard deviation) of kernel (defaults 2)</p>
</td></tr>
<tr><td><code id="gaussian.kernel_+3A_s">s</code></td>
<td>
<p>scale defining the number of rows and columns for kernel (default 5)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Symmetrical (NxN) matrix of a Gaussian distribution
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>opar &lt;- par()
  par(mfrow=c(2,2))
  persp(gaussian.kernel(sigma=1, s=27), theta = 135, 
        phi = 30, col = "grey", ltheta = -120, shade = 0.6, 
        border=NA )
  persp(gaussian.kernel(sigma=2, s=27), theta = 135, phi = 30,
        col = "grey", ltheta = -120, shade = 0.6, border=NA )	
  persp(gaussian.kernel(sigma=3, s=27), theta = 135, phi = 30,
        col = "grey", ltheta = -120, shade = 0.6, border=NA )	
  persp(gaussian.kernel(sigma=4, s=27), theta = 135, phi = 30,
        col = "grey", ltheta = -120, shade = 0.6, border=NA )	
 par(opar) 			
</code></pre>

<hr>
<h2 id='geo.buffer'>Buffer geographic data</h2><span id='topic+geo.buffer'></span>

<h3>Description</h3>

<p>Buffers data in geographic (Latitude/Longitude) projection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geo.buffer(x, r, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geo.buffer_+3A_x">x</code></td>
<td>
<p>A sf or sp vector class object</p>
</td></tr>
<tr><td><code id="geo.buffer_+3A_r">r</code></td>
<td>
<p>Buffer radius in meters</p>
</td></tr>
<tr><td><code id="geo.buffer_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to sf::st_buffer</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Projects (Latitude/Longitude) data in decimal-degree geographic projection 
using an on-the-fly azimuthal equidistant projection in meters centered on
</p>


<h3>Value</h3>

<p>an sp or sf polygon class object representing buffer for each feature
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>See Also</h3>

<p><code><a href="sf.html#topic+st_buffer">st_buffer</a></code> for st_buffer ... arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
e &lt;- c(61.87125, 23.90153, 76.64458, 37.27042)
  names(e) &lt;- c("xmin", "ymin", "xmax", "ymax")
  s &lt;- st_as_sf(st_sample(st_as_sfc(st_bbox(e)), size=100, 
                 type = "regular"))
	st_crs(s) &lt;- st_crs(4326)
      s$id &lt;- 1:nrow(s)

b &lt;- geo.buffer(x=s, r=1000)
  plot(st_geometry(b[1,]))
     plot(st_geometry(s[1,]), pch=20,cex=2, add=TRUE)	 

</code></pre>

<hr>
<h2 id='group.pdf'>Probability density plot by group</h2><span id='topic+group.pdf'></span>

<h3>Description</h3>

<p>Creates a probability density plot of y for 
each group of x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group.pdf(
  x,
  y,
  col = NULL,
  lty = NULL,
  lwd = NULL,
  lx = "topleft",
  ly = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group.pdf_+3A_x">x</code></td>
<td>
<p>Numeric, character or factorial vector of grouping 
variable (must be same length as y)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_y">y</code></td>
<td>
<p>Numeric vector (density variable)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_col">col</code></td>
<td>
<p>Optional line colors (see par, col)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_lty">lty</code></td>
<td>
<p>Optional line types (see par, lty)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_lwd">lwd</code></td>
<td>
<p>Optional line widths (see par, lwd)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_lx">lx</code></td>
<td>
<p>Position of legend (x coordinate or 'topright', 'topleft', 
'bottomright', 'bottomleft')</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_ly">ly</code></td>
<td>
<p>Position of legend (y coordinate)</p>
</td></tr>
<tr><td><code id="group.pdf_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of grouped PDF's
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>References</h3>

<p>Simonoff, J. S. (1996). Smoothing Methods in Statistics. Springer-Verlag, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y=dnorm(runif(100))
x=rep(c(1,2,3), length.out=length(y)) 
group.pdf(x=as.factor(x), y=y, main='Probability Density of y by group(x)', 
ylab='PDF', xlab='Y', lty=c(1,2,3))

</code></pre>

<hr>
<h2 id='hexagons'>Hexagons</h2><span id='topic+hexagons'></span>

<h3>Description</h3>

<p>Create hexagon polygons
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hexagons(x, res = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hexagons_+3A_x">x</code></td>
<td>
<p>sf class object indicating extent</p>
</td></tr>
<tr><td><code id="hexagons_+3A_res">res</code></td>
<td>
<p>Area of resulting hexagons</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on extent of x, creates a hexagon mesh with size of hexagons defined by res argumnet
</p>


<h3>Value</h3>

<p>sf POLYGONS object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
if(require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")

hex &lt;- hexagons(meuse, res=300)   
  plot(st_geometry(hex))
    plot(st_geometry(meuse),pch=20,add=TRUE)

# subset hexagons to intersection with points
idx &lt;- which(apply(st_intersects(hex, meuse, sparse=FALSE), 1, any))
hex.sub &lt;- hex[idx,] 
  plot(st_geometry(hex.sub))
    plot(st_geometry(meuse),pch=20,add=TRUE)

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='hli'>Heat Load Index</h2><span id='topic+hli'></span>

<h3>Description</h3>

<p>Calculates the McCune &amp; Keon (2002) Heat Load Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hli(x, check = TRUE, force.hemisphere = c("none", "southern", "northern"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hli_+3A_x">x</code></td>
<td>
<p>terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="hli_+3A_check">check</code></td>
<td>
<p>(TRUE/FALSE) check for projection integrity and
calculate central latitude for non-geographic
projections</p>
</td></tr>
<tr><td><code id="hli_+3A_force.hemisphere">force.hemisphere</code></td>
<td>
<p>If country is split at the equator, force southern 
or northern hemisphere equation c(&quot;southern&quot;, &quot;northern&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Describes A southwest facing slope should have warmer temperatures than a 
southeast facing slope, even though the amount of solar radiation they receive 
is equivalent. The McCune and Keon (2002) method accounts for this by &quot;folding&quot; 
the aspect so that the highest values are southwest and the lowest values are  
northeast. Additionally, this method account for steepness of slope, which is 
not addressed in most other aspect rescaling equations. HLI values range 
from 0 (coolest) to 1 (hottest). 
</p>
<p>The equations follow McCune (2007) and support northern and southern hemisphere 
calculations. The folded aspect for northern hemispheres use (180 - (Aspect – 225) ) 
and for Southern hemisphere  ( 180 - ( Aspect – 315) ). If a country is split at the 
equator you can use the force.hemisphere argument to choose which equation to use. 
Valid values for this argument are &quot;southern&quot; and &quot;northern&quot; with the default &quot;none&quot;.
</p>


<h3>Value</h3>

<p>terra SpatRaster class object of McCune &amp; Keon (2002) Heat Load Index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>McCune, B., and D. Keon (2002) Equations for potential annual direct 
incident radiation and heat load index. Journal of Vegetation 
Science. 13:603-606.
</p>
<p>McCune, B. (2007). Improved estimates of incident radiation and heat load 
using non-parametric regression against topographic variables. Journal 
of Vegetation Science 18:751-754.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(terra)
  elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  heat.load &lt;- hli(elev)
    plot(heat.load, main="Heat Load Index") 
   
</code></pre>

<hr>
<h2 id='hli.pt'>Point estimate of Heat Load Index</h2><span id='topic+hli.pt'></span>

<h3>Description</h3>

<p>Calculates the McCune &amp; Keon (2002) Heat Load Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hli.pt(
  alpha,
  theta,
  latitude,
  direct = FALSE,
  scaled = TRUE,
  force.hemisphere = c("none", "southern", "northern")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hli.pt_+3A_alpha">alpha</code></td>
<td>
<p>Aspect in degrees</p>
</td></tr>
<tr><td><code id="hli.pt_+3A_theta">theta</code></td>
<td>
<p>Slope in degrees</p>
</td></tr>
<tr><td><code id="hli.pt_+3A_latitude">latitude</code></td>
<td>
<p>A latitude representing the centrality of the data</p>
</td></tr>
<tr><td><code id="hli.pt_+3A_direct">direct</code></td>
<td>
<p>Boolean (FALSE/TRUE) Return direct incident radiation 
else HLI (default)</p>
</td></tr>
<tr><td><code id="hli.pt_+3A_scaled">scaled</code></td>
<td>
<p>Boolean (TRUE/FALSE) Apply arithmetic scale using EXP(h)</p>
</td></tr>
<tr><td><code id="hli.pt_+3A_force.hemisphere">force.hemisphere</code></td>
<td>
<p>If country is split at the equator, force southern 
or northern hemisphere equation c(&quot;southern&quot;, &quot;northern&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Describes A southwest facing slope should have warmer temperatures than a 
southeast facing slope, even though the amount of solar radiation they receive 
is equivalent. The McCune and Keon (2002) method accounts for this by &quot;folding&quot; 
the aspect so that the highest values are southwest and the lowest values are  
northeast. Additionally, this method account for steepness of slope, which is 
not addressed in most other aspect rescaling equations. HLI values range 
from 0 (coolest) to 1 (hottest). 
</p>
<p>The equations follow McCune (2007) and support northern and southern hemisphere 
calculations. The folded aspect for northern hemispheres use (180 - (Aspect – 225) ) 
and for Southern hemisphere  ( 180 - ( Aspect – 315) ). If a country is split at the 
equator you can use the force.hemisphere argument to choose which equation to use. 
Valid values for this argument are &quot;southern&quot; and &quot;northern&quot; with the default &quot;none&quot;.
</p>


<h3>Value</h3>

<p>Vector of McCune &amp; Keon (2002) Heat Load Index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>McCune, B., and D. Keon (2002) Equations for potential annual direct 
incident radiation and heat load index. Journal of Vegetation 
Science. 13:603-606.
</p>
<p>McCune, B. (2007). Improved estimates of incident radiation and heat load 
using non-parametric regression against topographic variables. Journal 
of Vegetation Science 18:751-754.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Single point input
hli.pt(theta=180, alpha=30, latitude=40) 

# Multiple input, returns results from 
#   McCune, B., and D. Keon (2002)
# Raw -0.2551 -0.6280 0.0538 -0.6760 -1.1401 -0.2215
# arithmetic scale 0.7748 0.5337 1.0553 0.5086 0.3198 0.8013

slp = c(0, 30, 30, 0, 30, 30)
asp =c(0, 0, 180, 0, 0, 180)
lat =c(40, 40, 40, 60, 60, 60)
hli.pt(theta = slp, alpha = asp, latitude = lat)
  
</code></pre>

<hr>
<h2 id='hsp'>Hierarchical Slope Position</h2><span id='topic+hsp'></span>

<h3>Description</h3>

<p>Calculates a hierarchical scale decomposition of topographic 
position index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hsp(
  x,
  min.scale = 3,
  max.scale = 27,
  inc = 4,
  win = "rectangle",
  normalize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hsp_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="hsp_+3A_min.scale">min.scale</code></td>
<td>
<p>Minimum scale (window size)</p>
</td></tr>
<tr><td><code id="hsp_+3A_max.scale">max.scale</code></td>
<td>
<p>Maximum scale (window size)</p>
</td></tr>
<tr><td><code id="hsp_+3A_inc">inc</code></td>
<td>
<p>Increment to increase scales</p>
</td></tr>
<tr><td><code id="hsp_+3A_win">win</code></td>
<td>
<p>Window type, options are &quot;rectangle&quot; or &quot;circle&quot;</p>
</td></tr>
<tr><td><code id="hsp_+3A_normalize">normalize</code></td>
<td>
<p>Normalize results to 0-1 scale (FALSE | TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>if win  = &quot;circle&quot; units are distance, if win = &quot;rectangle&quot; units 
are number of cells
</p>


<h3>Value</h3>

<p>terra SpatRaster class object of slope position
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Murphy M.A., J.S. Evans, and A.S. Storfer (2010) Quantify Bufo boreas 
connectivity in Yellowstone National Park with landscape genetics. 
Ecology 91:252-261
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
  library(terra)
  elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  hsp27 &lt;- hsp(elev, 3, 27, 4, normalize = TRUE)
  plot(hsp27)
 

</code></pre>

<hr>
<h2 id='hybrid.kmeans'>Hybrid K-means</h2><span id='topic+hybrid.kmeans'></span>

<h3>Description</h3>

<p>Hybrid K-means clustering using hierarchical clustering to define 
cluster-centers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hybrid.kmeans(x, k = 2, hmethod = "ward.D", stat = mean, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hybrid.kmeans_+3A_x">x</code></td>
<td>
<p>A data.frame or matrix with data to be clustered</p>
</td></tr>
<tr><td><code id="hybrid.kmeans_+3A_k">k</code></td>
<td>
<p>Number of clusters</p>
</td></tr>
<tr><td><code id="hybrid.kmeans_+3A_hmethod">hmethod</code></td>
<td>
<p>The agglomeration method used in hclust</p>
</td></tr>
<tr><td><code id="hybrid.kmeans_+3A_stat">stat</code></td>
<td>
<p>The statistic to aggregate class centers (mean or median)</p>
</td></tr>
<tr><td><code id="hybrid.kmeans_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="stats.html#topic+kmeans">kmeans</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method uses hierarchical clustering to define the cluster-centers in the K-means 
clustering algorithm. This mitigates some of the know convergence issues in K-means.
</p>


<h3>Value</h3>

<p>returns an object of class &quot;kmeans&quot; which has a print and a fitted method
</p>


<h3>Note</h3>

<p>options for hmethod are: &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, 
&quot;complete&quot;, &quot;average&quot;, mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Singh, H., &amp; K. Kaur (2013) New Method for Finding Initial Cluster Centroids in 
K-means Algorithm. International Journal of Computer Application. 74(6):27-30
</p>
<p>Ward, J.H., (1963) Hierarchical grouping to optimize an objective function. Journal 
of the American Statistical Association. 58:236-24
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code> for available ... arguments and function details
</p>
<p><code><a href="stats.html#topic+hclust">hclust</a></code> for details on hierarchical clustering
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))

# Compare k-means to hybrid k-means with k=4		   
km &lt;- kmeans(x, 4)		   
hkm &lt;- hybrid.kmeans(x,k=4)		   

opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
  plot(x[,1],x[,2], col=km$cluster,pch=19, main="K-means")
  plot(x[,1],x[,2], col=hkm$cluster,pch=19, main="Hybrid K-means")
par(opar)

</code></pre>

<hr>
<h2 id='idw.smoothing'>Inverse Distance Weighted smoothing</h2><span id='topic+idw.smoothing'></span>

<h3>Description</h3>

<p>Distance weighted smoothing of a variable in a 
spatial point object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>idw.smoothing(x, y, d, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="idw.smoothing_+3A_x">x</code></td>
<td>
<p>An sf POINT class object</p>
</td></tr>
<tr><td><code id="idw.smoothing_+3A_y">y</code></td>
<td>
<p>Numeric data column in x to be smoothed</p>
</td></tr>
<tr><td><code id="idw.smoothing_+3A_d">d</code></td>
<td>
<p>Distance constraint</p>
</td></tr>
<tr><td><code id="idw.smoothing_+3A_k">k</code></td>
<td>
<p>Maximum number of k-nearest neighbors within d</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Smoothing is conducted with a weighted-mean where; weights represent inverse 
standardized distance lags Distance-based or neighbour-based smoothing can be 
specified by setting the desired neighbour smoothing method to a specified value 
then the other parameter to the potential maximum. For example; a constraint 
distance, including all neighbors within 1000 (d=1000) would require k to equal 
all of the potential neighbors (n-1 or k=nrow(x)-1).
</p>


<h3>Value</h3>

<p>A vector, same length as nrow(x), of smoothed y values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
if(require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")
      
 # Calculate distance weighted mean on cadmium variable in meuse data   
   cadmium.idw &lt;- idw.smoothing(meuse, 'cadmium', k=nrow(meuse), d = 1000)                
   meuse$cadmium.wm &lt;- cadmium.idw
 
   opar &lt;- par(no.readonly=TRUE)
     par(mfrow=c(2,1)) 
       plot(density(meuse$cadmium), main='Cadmium')
       plot(density(meuse$cadmium.wm), main='IDW Cadmium')
   par(opar)

plot(meuse[c("cadmium","cadmium.wm")], pch=20)   

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='impute.loess'>Impute loess</h2><span id='topic+impute.loess'></span>

<h3>Description</h3>

<p>Imputes missing data or smooths using Loess regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute.loess(y, s = 0.2, smooth = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute.loess_+3A_y">y</code></td>
<td>
<p>A vector to impute</p>
</td></tr>
<tr><td><code id="impute.loess_+3A_s">s</code></td>
<td>
<p>Smoothing parameter ()</p>
</td></tr>
<tr><td><code id="impute.loess_+3A_smooth">smooth</code></td>
<td>
<p>(FALSE/TRUE) Smooth data, else only replace NA's</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs a local polynomial regression to smooth data or to 
impute NA values. The minimal number of non-NA observations to reliably
impute/smooth values is 6. There is not a reliably way to impute NA's
on the tails of the distributions so if the missing data is in the
first or last position of the vector it will remain NA. Please note
that smooth needs to be TRUE to return a smoothed vector, else only
NA's will be imputed.
</p>


<h3>Value</h3>

<p>A vector the same length as x with NA values filled or the data smoothed (or both).
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cor.data)
d &lt;- cor.data[[1]][,2]
  plot(d, type="l")
  lines(impute.loess(d, s=0.3, smooth=TRUE), lwd=2, col="red")
 
# add some NA's
d &lt;- d[1:100]
  d[sample(30:70, 5)] &lt;- NA 
  d
  
impute.loess(d, s=0.2)
  
</code></pre>

<hr>
<h2 id='insert'>Insert a row or column into a data.frame</h2><span id='topic+insert'></span>

<h3>Description</h3>

<p>Inserts a new row or column into a data.frame at a specified location
</p>


<h3>Usage</h3>

<pre><code class='language-R'>insert(x, MARGIN = 1, value = NULL, idx, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="insert_+3A_x">x</code></td>
<td>
<p>Existing data.frame</p>
</td></tr>
<tr><td><code id="insert_+3A_margin">MARGIN</code></td>
<td>
<p>Insert a 1 = row or 2 = column</p>
</td></tr>
<tr><td><code id="insert_+3A_value">value</code></td>
<td>
<p>A vector of values equal to the length of MARGIN,
if nothing specified values with be NA</p>
</td></tr>
<tr><td><code id="insert_+3A_idx">idx</code></td>
<td>
<p>Index position to insert row or column</p>
</td></tr>
<tr><td><code id="insert_+3A_name">name</code></td>
<td>
<p>Name of new column (not used for rows, MARGIN=1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Where there are methods to easily add a row/column to the end or beginning of a data.frame, 
it is not straight forward to insert data at a specific location within the data.frame. 
This function allows for inserting a vector at a specific location eg., between columns or 
rows 1 and 2 where row/column 2 is moved to the 3rd position and a new vector of values is 
inserted into the 2nd position.
</p>


<h3>Value</h3>

<p>A data.frame with the new row or column inserted
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- data.frame(ID=1:10, y=runif(10))

# insert row
insert(d, idx=2)
insert(d, value=c(20,0), idx=2)

# insert column
insert(d, MARGIN=2, idx=2)
insert(d, MARGIN = 2, value = rep(0,10), idx=2, name="x")

</code></pre>

<hr>
<h2 id='insert.values'>Insert Values</h2><span id='topic+insert.values'></span>

<h3>Description</h3>

<p>Inserts new values into a vector at 
specified positions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>insert.values(x, value, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="insert.values_+3A_x">x</code></td>
<td>
<p>A vector to insert values</p>
</td></tr>
<tr><td><code id="insert.values_+3A_value">value</code></td>
<td>
<p>Values to insert into x</p>
</td></tr>
<tr><td><code id="insert.values_+3A_index">index</code></td>
<td>
<p>Index position(s) to insert y values into x</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function inserts new values at specified positions in a vector. It 
does not replace existing values. If a single value is provided for y 
and l represents multiple positions y will be replicated for the length 
of l. In this way you can insert the same value at multiple locations.
</p>


<h3>Value</h3>

<p>A vector with values of y inserted into x and the position(s) 
defined by the index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(x=1:10)

 # Insert single value in one location
 insert.values(x, 100, 2) 

 # Insert multiple values in multiple locations 
 insert.values(x, c(100,200), c(2,8)) 

 # Insert single value in multiple locations 
 insert.values(x, NA, c(2,8))

</code></pre>

<hr>
<h2 id='is.empty'>is.empty</h2><span id='topic+is.empty'></span>

<h3>Description</h3>

<p>evaluates empty elements in a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.empty(x, all.na = FALSE, na.empty = TRUE, trim = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.empty_+3A_x">x</code></td>
<td>
<p>A vector to evaluate elements</p>
</td></tr>
<tr><td><code id="is.empty_+3A_all.na">all.na</code></td>
<td>
<p>(FALSE / TRUE) Return a TRUE if all elements are NA</p>
</td></tr>
<tr><td><code id="is.empty_+3A_na.empty">na.empty</code></td>
<td>
<p>(TRUE / FALSE) Return TRUE if element is NA</p>
</td></tr>
<tr><td><code id="is.empty_+3A_trim">trim</code></td>
<td>
<p>(TRUE / FALSE) Trim empty strings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function evaluates if an element in a vector is empty the na.empty argument 
allows for evaluating NA values (TRUE if NA) and all.na returns a TRUE if all elements 
are NA. The trim argument trims a character string to account for the fact that c(&quot; &quot;) 
is not empty but, a vector with c(&quot;&quot;) is empty. Using trim = TRUE will force both 
to return TRUE
</p>


<h3>Value</h3>

<p>A Boolean indicating empty elements in a vector, if all.na = FALSE
a TRUE/FALSE value will be returned for each element in the vector
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is.empty( c("") )
is.empty( c(" ") )
is.empty( c(" "), trim=FALSE )

is.empty( c("",NA,1) )
is.empty( c("",NA,1), na.empty=FALSE)

is.empty( c(NA,NA,NA) )
is.empty( c(NA,NA,NA), all.na=TRUE )
is.empty( c(NA,2,NA), all.na=TRUE )

any( is.empty( c("",2,3) ) )
any( is.empty( c(1,2,3) ) )

</code></pre>

<hr>
<h2 id='kendall'>Kendall tau trend with continuity correction for time-series</h2><span id='topic+kendall'></span>

<h3>Description</h3>

<p>Calculates a nonparametric statistic for a monotonic trend based
on the Kendall tau statistic and the Theil-Sen slope modification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kendall(
  y,
  tau = TRUE,
  intercept = TRUE,
  p.value = TRUE,
  confidence = TRUE,
  method = c("zhang", "yuepilon", "none"),
  threshold = 6,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kendall_+3A_y">y</code></td>
<td>
<p>A vector representing a timeseries with &gt;= 8 obs</p>
</td></tr>
<tr><td><code id="kendall_+3A_tau">tau</code></td>
<td>
<p>(FALSE/TRUE) return tau values</p>
</td></tr>
<tr><td><code id="kendall_+3A_intercept">intercept</code></td>
<td>
<p>(FALSE/TRUE) return intercept values</p>
</td></tr>
<tr><td><code id="kendall_+3A_p.value">p.value</code></td>
<td>
<p>(FALSE/TRUE) return p.values</p>
</td></tr>
<tr><td><code id="kendall_+3A_confidence">confidence</code></td>
<td>
<p>(FALSE/TRUE) return 95 pct confidence levels</p>
</td></tr>
<tr><td><code id="kendall_+3A_method">method</code></td>
<td>
<p>Method for deriving tau and slope (&quot;zhang&quot;, &quot;yuepilon&quot;, &quot;none&quot;)</p>
</td></tr>
<tr><td><code id="kendall_+3A_threshold">threshold</code></td>
<td>
<p>The threshold for number of minimum observations in the time-series</p>
</td></tr>
<tr><td><code id="kendall_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements Kendall's nonparametric test for a monotonic trend
using the Theil-Sen (Theil 1950; Sen 1968; Siegel 1982) method to estimate
the slope and related confidence intervals. Critical values are Z &gt; 1.96
representing a significant increasing trend and a Z &lt; -1.96 a significant
decreasing trend (p &lt; 0.05). The null hypothesis can be rejected if Tau = 0.
Autocorrelation in the time-series is addressed using a prewhitened linear trend
following the Zhang et al., (2000) or  Yue &amp; Pilon (2002) methods. If you do not
have autocorrelation in the data, the &quot;none&quot; or &quot;yuepilon&quot; method is recommended.
Please note that changing the threshold to fewer than 6 observations (ideally 8) may
prevent the function from failing but, will likely invalidate the statistic.
A threshold of &lt;=4 will yield all NA values. If method= &quot;none&quot; a modification of the
EnvStats::kendallTrendTest code is implemented.
</p>


<h3>Value</h3>

<p>Depending on arguments, a vector containing:
</p>

<ul>
<li><p> Theil-Sen slope, always returned
</p>
</li>
<li><p> Kendall's tau two-sided test, if tau TRUE
</p>
</li>
<li><p> intercept for trend if intercept TRUE
</p>
</li>
<li><p> p value for trend fit if p.value TRUE
</p>
</li>
<li><p> lower confidence level at 95-pct if confidence TRUE
</p>
</li>
<li><p> upper confidence level at 95-pct if confidence TRUE
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Theil, H. (1950) A rank invariant method for linear and polynomial regression
analysis. Nederl. Akad. Wetensch. Proc. Ser. A 53:386-392 (Part I),
53:521-525 (Part II), 53:1397-1412 (Part III).
</p>
<p>Sen, P.K. (1968) Estimates of Regression Coefficient Based on Kendall's tau.
Journal of the American Statistical Association. 63(324):1379-1389.
</p>
<p>Siegel, A.F. (1982) Robust Regression Using Repeated Medians.
Biometrika, 69(1):242-244
</p>
<p>Yue, S., P. Pilon, B. Phinney and G. Cavadias, (2002) The influence of autocorrelation
on the ability to detect trend in hydrological series.
Hydrological Processes, 16: 1807-1829.
</p>
<p>Zhang, X., Vincent, L.A., Hogg, W.D. and Niitsoo, A., (2000) Temperature
and Precipitation Trends in Canada during the 20th Century.
Atmosphere-Ocean 38(3): 395-429.
</p>


<h3>See Also</h3>

<p><code><a href="zyp.html#topic+zyp.trend.vector">zyp.trend.vector</a></code> for model details
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(EuStockMarkets)
d &lt;- as.vector(EuStockMarkets[,1])
kendall(d)

</code></pre>

<hr>
<h2 id='kl.divergence'>Kullback-Leibler divergence (relative entropy)</h2><span id='topic+kl.divergence'></span>

<h3>Description</h3>

<p>Calculates the Kullback-Leibler divergence (relative entropy)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kl.divergence(object, eps = 10^-4, overlap = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kl.divergence_+3A_object">object</code></td>
<td>
<p>Matrix or dataframe object with &gt;=2 columns</p>
</td></tr>
<tr><td><code id="kl.divergence_+3A_eps">eps</code></td>
<td>
<p>Probabilities below this threshold are replaced by this 
threshold for numerical stability.</p>
</td></tr>
<tr><td><code id="kl.divergence_+3A_overlap">overlap</code></td>
<td>
<p>Logical, do not determine the KL divergence for those 
pairs where for each point at least one of the densities 
has a value smaller than eps.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Kullback-Leibler divergence (relative entropy) between 
unweighted theoretical component distributions. Divergence is calculated 
as: int [f(x) (log f(x) - log g(x)) dx] for distributions with densities 
f() and g().
</p>


<h3>Value</h3>

<p>pairwise Kullback-Leibler divergence index (matrix)
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Kullback S., and R. A. Leibler (1951) On information and sufficiency.  
The Annals of Mathematical Statistics 22(1):79-86
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(-3, 3, length=200)
y &lt;- cbind(n=dnorm(x), t=dt(x, df=10))
  matplot(x, y, type='l')
    kl.divergence(y)
   
# extract value for last column
  kl.divergence(y[,1:2])[3:3]

</code></pre>

<hr>
<h2 id='knn'>Spatial K nearest neighbor</h2><span id='topic+knn'></span>

<h3>Description</h3>

<p>Find K nearest neighbors for two spatial objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn(
  y,
  x,
  k = 1,
  d = NULL,
  ids = NULL,
  weights.y = NULL,
  weights.x = NULL,
  indexes = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn_+3A_y">y</code></td>
<td>
<p>Spatial sf object or coordinates matrix</p>
</td></tr>
<tr><td><code id="knn_+3A_x">x</code></td>
<td>
<p>Spatial points or polygons object or coordinates matrix</p>
</td></tr>
<tr><td><code id="knn_+3A_k">k</code></td>
<td>
<p>Number of neighbors</p>
</td></tr>
<tr><td><code id="knn_+3A_d">d</code></td>
<td>
<p>Optional search radius</p>
</td></tr>
<tr><td><code id="knn_+3A_ids">ids</code></td>
<td>
<p>Optional column of ID's in x</p>
</td></tr>
<tr><td><code id="knn_+3A_weights.y">weights.y</code></td>
<td>
<p>A vector or matrix representing covariates of y</p>
</td></tr>
<tr><td><code id="knn_+3A_weights.x">weights.x</code></td>
<td>
<p>A vector or matrix representing covariates of x</p>
</td></tr>
<tr><td><code id="knn_+3A_indexes">indexes</code></td>
<td>
<p>(FALSE/TRUE) Return row indexes of x neighbors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds nearest neighbor in x based on y and returns rownames, index and distance,
If ids is NULL, rownames of x are returned. If coordinate matrix provided, 
columns need to be ordered [X,Y]. If a radius for d is specified than a maximum 
search radius is imposed. If no neighbor is found, a neighbor is not returned  
</p>
<p>You can specify weights to act as covariates for x and y. The vectors or matrices
must match row dimensions with x and y as well as columns matching between weights.
In other words, the covariates must match and be numeric.
</p>


<h3>Value</h3>

<p>A data.frame with row indexes (optional), rownames, ids (optional) and 
distance of k
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>See Also</h3>

<p><code><a href="RANN.html#topic+nn2">nn2</a></code> for details on search algorithm
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(require(sp, quietly = TRUE)) {
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")
 
# create reference and target obs
idx &lt;- sample(1:nrow(meuse), 10) 
  pts &lt;- meuse[idx,]
    meuse &lt;- meuse[-idx,]
    meuse$IDS &lt;- 1:nrow(meuse)
 
# Find 2 neighbors in meuse
( nn &lt;- knn(pts, meuse, k=2, ids = "IDS", indexes = TRUE) )
  plot( st_geometry(pts), pch=19, main="KNN")
    plot(st_geometry(meuse[nn[,1],]), pch=19, col="red", add=TRUE)

# Using covariates (weights)
wx = as.matrix(st_drop_geometry(meuse[,1:3]))
wy = as.matrix(st_drop_geometry(pts[,1:3]))

( nn &lt;- knn(pts, meuse, k=2, ids = "IDS", indexes = TRUE,
            weights.y=wy, weights.x=wx) )
  plot(st_geometry(pts), pch=19, main="KNN")
    plot(st_geometry(meuse[nn[,1],]), pch=19, col="red")
	  
# Using coordinate matrices
y &lt;- st_coordinates(pts)[,1:2]
x &lt;- st_coordinates(meuse)[,1:2]
knn(y, x, k=2)

} else { 
  cat("Please install sp package to run example", "\n")
}

  
</code></pre>

<hr>
<h2 id='lai'>Leaf Area Index</h2><span id='topic+lai'></span>

<h3>Description</h3>

<p>Remote sensing measure of LAI (leaf area per ground-unit area)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lai(ndvi, method = c("Jonckheere", "Chen"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lai_+3A_ndvi">ndvi</code></td>
<td>
<p>NDVI in floating point standard scale range (-1 to 1)</p>
</td></tr>
<tr><td><code id="lai_+3A_method">method</code></td>
<td>
<p>Method to use for index options c(&quot;Jonckheere&quot;, &quot;Chen&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Leaf Area Index (LAI) representing the amount of leaf area 
per unit of ground area. This is an important parameter for understanding the structure 
and function of vegetation, as it affects processes such as photosynthesis, transpiration, 
and carbon cycling. These two approaches are based on the empirical relationship between 
NDVI and LAI, which has been observed in many studies, and it is a widely used method for 
estimating LAI from remote sensing data. The formulas are derived from the fact that vegetation 
with higher LAI tends to have higher reflectance in the near-infrared (NIR) band and lower 
reflectance in the red band, resulting in higher NDVI values. But still, the exact relationship 
between NDVI and LAI can vary depending on factors such as vegetation type, canopy structure, 
and environmental conditions.
</p>


<h3>Value</h3>

<p>A terra SpatRaster object with derived LAI vaues
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Jonckheere, I., Fleck, S., Nackaerts, K., Muys, B., Coppin, P. (2004). A comparison of two 
methods to retrieve the leaf area index (LAI) from SPOT-4 HRVIR data. International 
Journal of Remote Sensing, 25(21):4407–4425.
</p>
<p>Chen, J. M., Liu, R., &amp; Ju, W. (2014). A simple and effective method for estimating 
leaf area index from Landsat imagery. Remote Sensing of Environment, 152:538–548.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
lsat &lt;- rast(system.file("/extdata/Landsat_TM5.tif", package="spatialEco"))
   plotRGB(lsat, r=3, g=2, b=1, scale=1.0, stretch="lin")
	
  ndvi &lt;-  ( lsat[[4]] - lsat[[3]] ) / (lsat[[4]] + lsat[[3]]) 
		   
 # Using Jonckheere et al., (2004) method
 lai01 &lt;- lai(ndvi)
   plot(lai01)



</code></pre>

<hr>
<h2 id='local.min.max'>Local minimum and maximum</h2><span id='topic+local.min.max'></span>

<h3>Description</h3>

<p>Calculates the local minimums and maximums in a numeric vector,
indicating inflection points in the distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local.min.max(x, dev = mean, plot = TRUE, add.points = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local.min.max_+3A_x">x</code></td>
<td>
<p>A numeric vector</p>
</td></tr>
<tr><td><code id="local.min.max_+3A_dev">dev</code></td>
<td>
<p>Deviation statistic (mean or median)</p>
</td></tr>
<tr><td><code id="local.min.max_+3A_plot">plot</code></td>
<td>
<p>plot the minimum and maximum values with the
distribution (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="local.min.max_+3A_add.points">add.points</code></td>
<td>
<p>Should all points of x be added to
plot (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="local.min.max_+3A_...">...</code></td>
<td>
<p>Arguments passed to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful function for identifying inflection or enveloping points in
a distribution
</p>


<h3>Value</h3>

<p>A list object with:
</p>

<ul>
<li><p> minima - minimum local values of x
</p>
</li>
<li><p> maxima - maximum local values of x
</p>
</li>
<li><p> mindev - Absolute deviation of minimum from specified deviation
statistic (dev argument)
</p>
</li>
<li><p> maxdev - Absolute deviation of maximum from specified deviation
statistic (dev argument)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>                                                 
x &lt;- rnorm(100,mean=1500,sd=800) 
( lmm &lt;- local.min.max(x, dev=mean, add.points=TRUE, 
                       main="Local Minima and Maxima") )

# return only local minimum values
   local.min.max(x)$minima 
                                           
</code></pre>

<hr>
<h2 id='loess.boot'>Loess Bootstrap</h2><span id='topic+loess.boot'></span>

<h3>Description</h3>

<p>Bootstrap of a Local Polynomial Regression (loess)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loess.boot(x, y, nreps = 100, confidence = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loess.boot_+3A_x">x</code></td>
<td>
<p>Independent variable</p>
</td></tr>
<tr><td><code id="loess.boot_+3A_y">y</code></td>
<td>
<p>Dependent variable</p>
</td></tr>
<tr><td><code id="loess.boot_+3A_nreps">nreps</code></td>
<td>
<p>Number of bootstrap replicates</p>
</td></tr>
<tr><td><code id="loess.boot_+3A_confidence">confidence</code></td>
<td>
<p>Fraction of replicates contained in confidence
region</p>
</td></tr>
<tr><td><code id="loess.boot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to loess function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fits a loess curve and then calculates a symmetric nonparametric
bootstrap with a confidence region. Fitted curves are evaluated at a fixed number
of equally-spaced x values, regardless of the number of x values in the data. Some
replicates do not include the values at the lower and upper end of the range of x
values. If the number of such replicates is too large, it becomes impossible to
construct a confidence region that includes a fraction &quot;confidence&quot; of the bootstrap
replicates. In such cases, the left and/or right portion of the confidence region
is truncated.
</p>


<h3>Value</h3>

<p>list object containing
</p>

<ul>
<li><p> nreps        Number of bootstrap replicates
</p>
</li>
<li><p> confidence   Confidence interval (region)
</p>
</li>
<li><p> span         alpha (span) parameter used loess fit
</p>
</li>
<li><p> degree       polynomial degree used in loess fit
</p>
</li>
<li><p> normalize    Normalized data (TRUE/FALSE)
</p>
</li>
<li><p> family       Family of statistic used in fit
</p>
</li>
<li><p> parametric   Parametric approximation (TRUE/FALSE)
</p>
</li>
<li><p> surface      Surface fit, see loess.control
</p>
</li>
<li><p> data         data.frame of x,y used in model
</p>
</li>
<li><p> fit          data.frame including:
</p>

<ol>
<li><p> x - Equally-spaced x index (see NOTES)
</p>
</li>
<li><p> y.fit - loess fit
</p>
</li>
<li><p> up.lim - Upper confidence interval
</p>
</li>
<li><p> low.lim - Lower confidence interval
</p>
</li>
<li><p> stddev - Standard deviation of loess fit at each x value
</p>
</li></ol>

</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Cleveland, WS, (1979) Robust Locally Weighted Regression and Smoothing Plots Journal
of the American Statistical Association 74:829-836
</p>
<p>Efron, B., and R. Tibshirani (1993) An Introduction to the Bootstrap Chapman and
Hall, New York
</p>
<p>Hardle, W., (1989) Applied Nonparametric Regression Cambridge University Press, NY.
</p>
<p>Tibshirani, R. (1988) Variance stabilization and the bootstrap.
Biometrika 75(3):433-44.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> n=1000
 x &lt;- seq(0, 4, length.out=n)	 
 y &lt;- sin(2*x)+ 0.5*x + rnorm(n, sd=0.5)
 sb &lt;- loess.boot(x, y, nreps=99, confidence=0.90, span=0.40)
 plot(sb)
                     	
</code></pre>

<hr>
<h2 id='loess.ci'>Loess with confidence intervals</h2><span id='topic+loess.ci'></span>

<h3>Description</h3>

<p>Calculates a local polynomial regression fit
with associated confidence intervals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loess.ci(y, x, p = 0.95, plot = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loess.ci_+3A_y">y</code></td>
<td>
<p>Dependent variable, vector</p>
</td></tr>
<tr><td><code id="loess.ci_+3A_x">x</code></td>
<td>
<p>Independent variable, vector</p>
</td></tr>
<tr><td><code id="loess.ci_+3A_p">p</code></td>
<td>
<p>Percent confidence intervals (default is 0.95)</p>
</td></tr>
<tr><td><code id="loess.ci_+3A_plot">plot</code></td>
<td>
<p>Plot the fit and confidence intervals</p>
</td></tr>
<tr><td><code id="loess.ci_+3A_...">...</code></td>
<td>
<p>Arguments passed to loess</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object with:
</p>

<ul>
<li><p> loess Predicted values
</p>
</li>
<li><p> se Estimated standard error for each predicted value
</p>
</li>
<li><p> lci Lower confidence interval
</p>
</li>
<li><p> uci Upper confidence interval
</p>
</li>
<li><p> df Estimated degrees of freedom
</p>
</li>
<li><p> rs Residual scale of residuals used in computing the
standard errors
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>W. S. Cleveland, E. Grosse and W. M. Shyu (1992) Local regression models.
Chapter 8 of Statistical Models in S eds J.M. Chambers and T.J. Hastie,
Wadsworth &amp; Brooks/Cole.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- seq(-20, 20, 0.1)
 y &lt;- sin(x)/x + rnorm(length(x), sd=0.03)
 p &lt;- which(y == "NaN")
   y &lt;- y[-p]	
   x &lt;- x[-p]
 
opar &lt;- par(no.readonly=TRUE)
  par(mfrow=c(2,2))  
    lci &lt;- loess.ci(y, x, plot=TRUE, span=0.10)
    lci &lt;- loess.ci(y, x, plot=TRUE, span=0.30)
    lci &lt;- loess.ci(y, x, plot=TRUE, span=0.50)
    lci &lt;- loess.ci(y, x, plot=TRUE, span=0.80)
par(opar)

</code></pre>

<hr>
<h2 id='logistic.regression'>Logistic and Auto-logistic regression</h2><span id='topic+logistic.regression'></span>

<h3>Description</h3>

<p>Performs a logistic (binomial) or auto-logistic
(spatially lagged binomial) regression using maximum
likelihood or penalized maximum likelihood estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic.regression(
  ldata,
  y,
  x,
  penalty = TRUE,
  autologistic = FALSE,
  coords = NULL,
  bw = NULL,
  type = "inverse",
  style = "W",
  longlat = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logistic.regression_+3A_ldata">ldata</code></td>
<td>
<p>data.frame object containing variables</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_y">y</code></td>
<td>
<p>Dependent variable (y) in ldata</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_x">x</code></td>
<td>
<p>Independent variable(s) (x) in ldata</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_penalty">penalty</code></td>
<td>
<p>Apply regression penalty (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_autologistic">autologistic</code></td>
<td>
<p>Add auto-logistic term (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_coords">coords</code></td>
<td>
<p>Geographic coordinates for auto-logistic model matrix
or sp object.</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_bw">bw</code></td>
<td>
<p>Distance bandwidth to calculate spatial lags (if empty neighbors
result, need to increase bandwidth). If not provided it will be
calculated automatically based on the minimum distance that includes
at least one neighbor.</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_type">type</code></td>
<td>
<p>Neighbor weighting scheme (see autocov_dist)</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_style">style</code></td>
<td>
<p>Type of neighbor matrix (Wij), default is mean of neighbors</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_longlat">longlat</code></td>
<td>
<p>Are coordinates (coords) in geographic, lat/long (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="logistic.regression_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to lrm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It should be noted that the auto-logistic model (Besag 1972) is intended for
exploratory analysis of spatial effects. Auto-logistic are know to underestimate
the effect of environmental variables and tend to be unreliable (Dormann 2007).
Wij matrix options under style argument - B is the basic binary coding, W is row
standardized (sums over all links to n), C is globally standardized (sums over
all links to n), U is equal to C divided by the number of neighbours (sums over
all links to unity) and S is variance-stabilizing. Spatially lagged y defined as:
W(y)ij=sumj_(Wij yj)/ sumj_(Wij) where; Wij=1/Euclidean(i,j)
If the object passed to the function is an sp class there is no need to call the data
slot directly via &quot;object@data&quot;, just pass the object name.
</p>


<h3>Value</h3>

<p>A list class object with the following components:
</p>

<ul>
<li><p> model - lrm model object (rms class)
</p>
</li>
<li><p> bandwidth - If AutoCov = TRUE returns the distance bandwidth used for the
auto-covariance function
</p>
</li>
<li><p> diagTable - data.frame of regression diagnostics
</p>
</li>
<li><p> coefTable - data.frame of regression coefficients (log-odds)
</p>
</li>
<li><p> Residuals - data.frame of residuals and standardized residuals
</p>
</li>
<li><p> AutoCov - If an auto-logistic model, AutoCov represents lagged
auto-covariance term
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Besag, J.E., (1972) Nearest-neighbour systems and the auto-logistic model for binary
data. Journal of the Royal Statistical Society, Series B Methodological 34:75-83
</p>
<p>Dormann, C.F., (2007) Assessing the validity of autologistic regression. Ecological
Modelling 207:234-242
</p>
<p>Le Cessie, S., Van Houwelingen, J.C., (1992) Ridge estimators in logistic regression.
Applied Statistics 41:191-201
</p>
<p>Shao, J., (1993) Linear model selection by cross-validation. JASA 88:486-494
</p>


<h3>See Also</h3>

<p><code><a href="rms.html#topic+lrm">lrm</a></code>
</p>
<p><code><a href="spdep.html#topic+autocov_dist">autocov_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> p = c("sf", "sp", "spdep", "rms")
 if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
   m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
   message("Can't run examples, please install ", paste(p[m], collapse = " "))
 } else {
   invisible(lapply(p, require, character.only=TRUE))
 
 data(meuse, package = "sp")
 meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                   agr = "constant")
   meuse$DepVar &lt;- rbinom(nrow(meuse), 1, 0.5)
 
 #### Logistic model
 lmodel &lt;- logistic.regression(meuse, y='DepVar', 
                   x=c('dist','cadmium','copper')) 
   lmodel$model
     lmodel$diagTable
       lmodel$coefTable
 
 #### Logistic model with factorial variable
 lmodel &lt;- logistic.regression(meuse, y='DepVar', 
             x=c('dist','cadmium','copper', 'soil')) 
   lmodel$model
     lmodel$diagTable
       lmodel$coefTable
 
  ### Auto-logistic model using 'autocov_dist' in 'spdep' package
  lmodel &lt;- logistic.regression(meuse, y='DepVar', 
              x=c('dist','cadmium','copper'), autologistic=TRUE, 
              coords=st_coordinates(meuse), bw=5000) 
    lmodel$model
      lmodel$diagTable
        lmodel$coefTable
    est &lt;- predict(lmodel$model, type='fitted.ind')
  
  #### Add residuals, standardized residuals and estimated probabilities
  VarNames &lt;- rownames(lmodel$model$var)[-1]
    meuse$AutoCov &lt;- lmodel$AutoCov 
    meuse$Residual &lt;- lmodel$Residuals[,1]
    meuse$StdResid &lt;- lmodel$Residuals[,2]
    meuse$Probs &lt;- predict(lmodel$model, 
                          sf::st_drop_geometry(meuse[,VarNames]),
 	                     type='fitted')  
  
 #### Plot fit and probabilities
 
 resid(lmodel$model, "partial", pl="loess") 
 
 # plot residuals
 resid(lmodel$model, "partial", pl=TRUE) 
  
 # global test of goodness of fit 
 resid(lmodel$model, "gof")
  
 # Approx. leave-out linear predictors
 lp1 &lt;- resid(lmodel$model, "lp1")            
  
 # Approx leave-out-1 deviance            
 -2 * sum(meuse$DepVar * lp1 + log(1-plogis(lp1)))
  
 # plot estimated probabilities at points
 plot(meuse['Probs'], pch=20)
 
}
</code></pre>

<hr>
<h2 id='max_extent'>Maximum extent of multiple rasters</h2><span id='topic+max_extent'></span>

<h3>Description</h3>

<p>returns a extent polygon representing maximum extent of
input rasters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>max_extent(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="max_extent_+3A_x">x</code></td>
<td>
<p>terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="max_extent_+3A_...">...</code></td>
<td>
<p>additional SpatRaster class objects in same projection</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creates a maximum extent polygon of all specified rasters
</p>


<h3>Value</h3>

<p>An sf POLYGON class object representing maximum extents
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)

r1 &lt;- rast(ext(61.87125, 76.64458, 23.90153, 37.27042))
r2 &lt;- rast(ext(67.66625, 81.56847, 20.38458, 35.67347))
r3 &lt;- rast(ext(72.64792,84.38125,5.91125,28.13347 ))

( e &lt;- max_extent(r1, r2, r3) )
plot(e, border=NA)
  plot(ext(r1), border="red", add=TRUE)
  plot(ext(r2), border="green", add=TRUE)
  plot(ext(r3), border="blue", add=TRUE)
  plot(e, border="black", add=TRUE)
		    
 sf::st_bbox(e) # full extent

</code></pre>

<hr>
<h2 id='mean_angle'>Mean Angle</h2><span id='topic+mean_angle'></span>

<h3>Description</h3>

<p>Calculates the mean angle of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_angle(a, angle = c("degree", "radians"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_angle_+3A_a">a</code></td>
<td>
<p>vector of angle values</p>
</td></tr>
<tr><td><code id="mean_angle_+3A_angle">angle</code></td>
<td>
<p>(&quot;degree&quot;, &quot;radians&quot;) to define angle in degrees or radians</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The arithmetic mean is not correct for calculating the central tendency of
angles. This function is intended to return the mean angle for slope or aspect,
which could be used in a focal or zonal function.
</p>


<h3>Value</h3>

<p>A vector of mean angle
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
mean_angle(c(180, 10))
  mean(c(180, 10))
mean_angle(c(90, 180, 70, 60))
  mean(c(90, 180, 70, 60))
mean_angle(c(90, 180, 270, 360))
  mean(c(90, 180, 270, 360))

elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
asp &lt;- terrain(elev, v="aspect")
s &lt;- buffer(spatSample(asp, 20, as.points=TRUE, 
            na.rm=TRUE, values=FALSE), 5000)

plot(asp)
  plot(s, add=TRUE)

d &lt;- extract(asp, s)
cat("Mean angles of aspect", "\n")
  tapply(d[,2], d[,1], mean_angle) 
cat("arithmetic means of aspect", "\n")
  tapply(d[,2], d[,1], mean, na.rm=TRUE) 

</code></pre>

<hr>
<h2 id='moments'>moments</h2><span id='topic+moments'></span>

<h3>Description</h3>

<p>Calculate statistical moments of a distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moments(x, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moments_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="moments_+3A_plot">plot</code></td>
<td>
<p>plot of distribution (TRUE/FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the following values:
</p>

<ul>
<li><p> min Minimum
</p>
</li>
<li><p> 25th  25th percentile
</p>
</li>
<li><p> mean  Arithmetic mean
</p>
</li>
<li><p> gmean  Geometric mean
</p>
</li>
<li><p> hmean  Harmonic mean
</p>
</li>
<li><p> median  50th percentile
</p>
</li>
<li><p> 7th5  75th percentile
</p>
</li>
<li><p> max  Maximum
</p>
</li>
<li><p> stdv  Standard deviation
</p>
</li>
<li><p> var  Variance
</p>
</li>
<li><p> cv  Coefficient of variation (percent)
</p>
</li>
<li><p> mad  Median absolute deviation
</p>
</li>
<li><p> skew  Skewness
</p>
</li>
<li><p> kurt  Kurtosis
</p>
</li>
<li><p> nmodes  Number of modes
</p>
</li>
<li><p> mode  Mode (dominate)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- runif(1000,0,100)
( d &lt;- moments(x, plot=TRUE) )
( mode.x &lt;- moments(x, plot=FALSE)[16] )
 
</code></pre>

<hr>
<h2 id='morans.plot'>Autocorrelation Plot</h2><span id='topic+morans.plot'></span>

<h3>Description</h3>

<p>Autocorrelation plot (Anselin 1996), following Chen (2015), 
aka, Moran's-I plot (univariate or bivariate)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>morans.plot(
  x,
  y = NULL,
  coords = NULL,
  type.ac = c("xy", "yx"),
  dist.function = "inv.power",
  scale.xy = TRUE,
  scale.morans = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="morans.plot_+3A_x">x</code></td>
<td>
<p>Vector of x response variables</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_y">y</code></td>
<td>
<p>Vector of y response variables</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_coords">coords</code></td>
<td>
<p>A matrix of coordinates corresponding to [x,y]</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_type.ac">type.ac</code></td>
<td>
<p>Type of autocorrelation plot (&quot;xy&quot;, &quot;yx&quot;)</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_dist.function">dist.function</code></td>
<td>
<p>(&quot;inv.power&quot;, &quot;neg.exponent&quot;)</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_scale.xy">scale.xy</code></td>
<td>
<p>(TRUE/FALSE) scale the x,y vectors</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_scale.morans">scale.morans</code></td>
<td>
<p>(FALSE/TRUE) standardize the Moran's index to an 
expected [-1 to 1]?</p>
</td></tr>
<tr><td><code id="morans.plot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The argument &quot;type&quot; controls the plot for x influencing y (type=&quot;xy&quot;) or y 
influencing x (type=&quot;yx&quot;). If y is not defined then the statistic is univariate 
and only the &quot;xy&quot; plot will be available. The linear relationship between x and 
its spatial lag (Wx) is indicative of the spatial autoregressive process, underlying 
the spatial dependence. The statistic can be autocorrelation (univariate or 
cross-correlation (bivariate). The quadrants are the zero intercept for random 
autocorrelation and the red line represents the trend in autocorrelation. The quadrants 
in the plot indicate the type of spatial association/interaction (Anselin 1996). 
For example the upper-left quadrant represents negative associations of low values 
surrounded by high and the lower-right quadrant represents negative associations of
high values surrounded by low.  
</p>
<p>If y is not specified the univariate statistic for x is returned. the coords argument 
is only used if k = NULL. Can also be an sp object with relevant x,y coordinate slot 
(ie., points or polygons). If w = NULL, the default method for deriving spatial weights 
matrix, options are: inverse power or negative exponent. If scale.xy = FALSE it is 
assumed that they are already scaled following Chen (2015).
</p>


<h3>Value</h3>

<p>A plot of the scaled variable against its spatially lagged values.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Chen., Y. (2015) A New Methodology of Spatial Cross-Correlation Analysis. 
PLoS One 10(5):e0126158. doi:10.1371/journal.pone.0126158
</p>
<p>Anselin, L. (1996) The Moran scatterplot as an ESDA tool to assess local instability 
in spatial association. pp. 111-125 in M. M. Fischer, H. J. Scholten and D. Unwin (eds) 
Spatial analytical perspectives on GIS, London, Taylor and Francis
</p>
<p>Anselin, L. (1995) Local indicators of spatial association, Geographical Analysis, 
27:93-115
</p>


<h3>Examples</h3>

<pre><code class='language-R'> p = c("sf", "sp", "spdep")
 if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
   m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
   message("Can't run examples, please install ", paste(p[m], collapse = " "))
 } else {
 invisible(lapply(p, require, character.only=TRUE))
 
 data(meuse, package = "sp")
 meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, agr = "constant") 

# Autocorrelation (univariate)  
morans.plot(meuse$zinc, coords = st_coordinates(meuse)[,1:2])   

# Cross-correlation of: x influencing y and y influencing x
opar &lt;- par(no.readonly=TRUE)
  par(mfrow=c(1,2)) 
    morans.plot(x=meuse$zinc, y=meuse$copper, coords = st_coordinates(meuse)[,1:2], 
                scale.morans = TRUE)
    morans.plot(x=meuse$zinc, y=meuse$copper, coords = st_coordinates(meuse)[,1:2],
                scale.morans = TRUE, type.ac="yx") 
par(opar)
}                       
</code></pre>

<hr>
<h2 id='nni'>Average Nearest Neighbor Index (NNI)</h2><span id='topic+nni'></span>

<h3>Description</h3>

<p>Calculates the NNI as a measure of clustering or dispersal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nni(x, win = c("hull", "extent"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nni_+3A_x">x</code></td>
<td>
<p>An sf point object</p>
</td></tr>
<tr><td><code id="nni_+3A_win">win</code></td>
<td>
<p>Type of window 'hull' or 'extent'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The nearest neighbor index is expressed as the ratio of the observed distance
divided by the expected distance. The expected distance is the average distance
between neighbors in a hypothetical random distribution. If the index is less than 1,
the pattern exhibits clustering; if the index is greater than 1, the trend is toward
dispersion or competition. The Nearest Neighbor Index is calculated as:
</p>

<ul>
<li><p> Mean Nearest Neighbor Distance (observed) D(nn) = sum(min(Dij)/N)
</p>
</li>
<li><p> Mean Random Distance (expected) D(e) = 0.5 SQRT(A/N)
</p>
</li>
<li><p> Nearest Neighbor Index NNI = D(nn)/D(e)
Where; D=neighbor distance, A=Area
</p>
</li></ul>



<h3>Value</h3>

<p>list object containing NNI = nearest neighbor index, z.score = Z
Score value, p = p value, expected.mean.distance = Expected mean
distance, observed.mean.distance = Observed meand distance.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Clark, P.J., and F.C. Evans (1954) Distance to nearest neighbour as a measure
of spatial relationships in populations. Ecology 35:445-453
</p>
<p>Cressie, N (1991) Statistics for spatial data. Wiley &amp; Sons, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p = c("sf", "sp")
  if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
    m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
    message("Can't run examples, please install ", paste(p[m], collapse = " "))
  } else {
  invisible(lapply(p, require, character.only=TRUE))

  data(meuse, package = "sp")
  meuse &lt;- sf::st_as_sf(meuse, coords = c("x", "y"),  
                        crs = 28992, agr = "constant")
  nni(meuse)
  }

</code></pre>

<hr>
<h2 id='nth.values'>Nth values</h2><span id='topic+nth.values'></span>

<h3>Description</h3>

<p>Returns the Nth highest or lowest values in a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nth.values(x, N = 2, smallest = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nth.values_+3A_x">x</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
<tr><td><code id="nth.values_+3A_n">N</code></td>
<td>
<p>Number of (Nth) values returned</p>
</td></tr>
<tr><td><code id="nth.values_+3A_smallest">smallest</code></td>
<td>
<p>(FALSE/TRUE) Return the highest, else smallest values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns n lowest or highest elements in a vector
</p>


<h3>Value</h3>

<p>Numeric vector of Nth values
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nth.values(1:20, N=3, smallest = TRUE)                 
nth.values(1:20, N=3)

</code></pre>

<hr>
<h2 id='o.ring'>Inhomogeneous O-ring</h2><span id='topic+o.ring'></span>

<h3>Description</h3>

<p>Calculates the inhomogeneous O-ring point pattern statistic 
(Wiegand &amp; Maloney 2004)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>o.ring(x, inhomogeneous = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="o.ring_+3A_x">x</code></td>
<td>
<p>spatstat ppp object</p>
</td></tr>
<tr><td><code id="o.ring_+3A_inhomogeneous">inhomogeneous</code></td>
<td>
<p>(FALSE/TRUE) Run homogeneous (pcf) or inhomogeneous 
(pcfinhom)</p>
</td></tr>
<tr><td><code id="o.ring_+3A_...">...</code></td>
<td>
<p>additional arguments passed to pcf or pcfinhom</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function K(r) is the expected number of points in a circle of radius r centered 
at an arbitrary point (which is not counted), divided by the intensity l of the pattern. 
The alternative pair correlation function g(r), which arises if the circles of 
Ripley's K-function are replaced by rings, gives the expected number of points at 
distance r from an arbitrary point, divided by the intensity of the pattern. Of special 
interest is to determine whether a pattern is random, clumped, or regular. 
</p>
<p>Using rings instead of circles has the advantage that one can isolate specific 
distance classes, whereas the cumulative K-function confounds effects at larger 
distances with effects at shorter distances. Note that the K-function and the O-ring 
statistic respond to slightly different biological questions. The accumulative 
K-function can detect aggregation or dispersion up to a given distance r and is 
therefore appropriate if the process in question (e.g., the negative effect of 
competition) may work only up to a certain distance, whereas the O-ring statistic 
can detect aggregation or dispersion at a given distance r. The O-ring statistic 
has the additional advantage that it is a probability density function (or a 
conditioned probability spectrum) with the interpretation of a neighborhood 
density, which is more intuitive than an accumulative measure.
</p>


<h3>Value</h3>

<p>plot of o-ring and data.frame with plot labels and descriptions
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Wiegand T., and K. A. Moloney (2004) Rings, circles and null-models for point 
pattern analysis in ecology. Oikos 104:209-229
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (require(spatstat.explore, quietly = TRUE)) {
data(lansing)
  x  &lt;- spatstat.geom::unmark(split(lansing)$maple)
  o.ring(x)

} else { 
  cat("Please install spatstat.explore package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='oli.asw'>Query AWS-OLI</h2><span id='topic+oli.asw'></span>

<h3>Description</h3>

<p>Query of Amazon AWS OLI-Landsat 8 cloud service
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oli.asw(path, row, dates, cloud.cover = 10, processing)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oli.asw_+3A_path">path</code></td>
<td>
<p>landsat path</p>
</td></tr>
<tr><td><code id="oli.asw_+3A_row">row</code></td>
<td>
<p>landsat row</p>
</td></tr>
<tr><td><code id="oli.asw_+3A_dates">dates</code></td>
<td>
<p>dates, single or start-stop range in YYYY-MM-DD format</p>
</td></tr>
<tr><td><code id="oli.asw_+3A_cloud.cover">cloud.cover</code></td>
<td>
<p>percent cloud cover</p>
</td></tr>
<tr><td><code id="oli.asw_+3A_processing">processing</code></td>
<td>
<p>processing level (&quot;L1GT&quot; or &quot;L1T&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Amazons AWS cloud service is hosting OLI Landsat 8 data granules
<a href="https://registry.opendata.aws/landsat-8">https://registry.opendata.aws/landsat-8</a>
<a href="https://aws.amazon.com/blogs/aws/start-using-landsat-on-aws/">https://aws.amazon.com/blogs/aws/start-using-landsat-on-aws/</a>
</p>
<p>USGS Landsat collections: <a href="https://www.usgs.gov/landsat-missions">https://www.usgs.gov/landsat-missions</a>
Pre-collection processing levels: &quot;L1T&quot;, &quot;L1GT&quot;, &quot;L1G&quot;
Collection 1 processing levels: &quot;L1TP&quot;, &quot;L1GT&quot;, &quot;L1GS&quot;
&quot;L1T&quot; and &quot;L1TP&quot; - Radiomertically calibrated and orthorectified (highest level processing) 
&quot;L1GT&quot; and &quot;L1GT&quot; - Radiomertically calibrated and systematic geometric corrections   
&quot;L1G&quot; and &quot;L1GS&quot; - Radiomertically calibrated with systematic ephemeris correction
</p>


<h3>Value</h3>

<p>data.frame object with:
</p>
 
<ul>
<li><p> entityId -  Granule ID 
</p>
</li>
<li><p>      L = Landsat
</p>
</li>
<li><p>      X = Sensor
</p>
</li>
<li><p>      SS = Satellite
</p>
</li>
<li><p>      PPP = WRS path
</p>
</li>
<li><p>      RRR = WRS row
</p>
</li>
<li><p>      YYYYMMDD = Acquisition date
</p>
</li>
<li><p>      yyyymmdd = Processing date
</p>
</li>
<li><p>      CC = Collection number
</p>
</li>
<li><p>      TX = Collection category
</p>
</li>
<li><p> acquisitionDate - POSIXct YYYY-MM-DD (eg., 2015-01-02)
</p>
</li>
<li><p> cloudCover -  
</p>
</li>
<li><p> processingLevel - USGS processing level
</p>
</li>
<li><p> path - Landsat path
</p>
</li>
<li><p> row - Landsat row
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Query path 126, row 59, 2013-04-15 to 2017-03-09, &lt;20% cloud cover    
( p126r59.oli &lt;- oli.asw(path=126, row=59, dates = c("2013-04-15", "2017-03-09"), 
                          cloud.cover = 20) )

# Download images from query
 bands &lt;- c("_B1.TIF", "_B2.TIF", "_B3.TIF", "_B4.TIF", "_B5.TIF", 
            "_B6.TIF","_B7.TIF", "_B8.TIF", "_B9.TIF", "_B10.TIF",
         "_B11.TIF", "_BQA.TIF","_MTL.txt") 
  for(i in 1:length(p126r59.oli$download_url)) {
    oli.url &lt;- gsub("/index.html","",p126r59.oli$download_url[i])
 all.bands &lt;- paste(oli.url, paste0(unlist(strsplit(oli.url, "/"))[8], bands), sep="/")
   for(j in all.bands) {  
        try(utils::download.file(url=j, destfile=basename(j), mode = "wb"))
       }		 
  }


</code></pre>

<hr>
<h2 id='optimal.k'>optimalK</h2><span id='topic+optimal.k'></span>

<h3>Description</h3>

<p>Find optimal k of k-Medoid partitions using 
silhouette widths
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal.k(x, nk = 10, plot = TRUE, cluster = TRUE, clara = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimal.k_+3A_x">x</code></td>
<td>
<p>Numeric dataframe, matrix or vector</p>
</td></tr>
<tr><td><code id="optimal.k_+3A_nk">nk</code></td>
<td>
<p>Number of clusters to test (2:nk)</p>
</td></tr>
<tr><td><code id="optimal.k_+3A_plot">plot</code></td>
<td>
<p>(TRUE / FALSE) Plot cluster silhouettes(TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="optimal.k_+3A_cluster">cluster</code></td>
<td>
<p>(TRUE / FALSE) Create cluster object with optimal k</p>
</td></tr>
<tr><td><code id="optimal.k_+3A_clara">clara</code></td>
<td>
<p>(FALSE / TRUE) Use clara model for large data</p>
</td></tr>
<tr><td><code id="optimal.k_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to clara</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class clust &quot;pam&quot; or &quot;clara&quot; with tested silhouette values
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>References</h3>

<p>Theodoridis, S. &amp; K. Koutroumbas(2006) Pattern Recognition 3rd ed.
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+pam">pam</a></code> for details on Partitioning Around Medoids (PAM)
</p>
<p><code><a href="cluster.html#topic+clara">clara</a></code> for details on Clustering Large Applications (clara)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (require(cluster, quietly = TRUE)) {
  x &lt;- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
             cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))

  clust &lt;- optimal.k(x, 20, plot=TRUE, cluster=TRUE)
    plot(silhouette(clust$model), col = c('red', 'green'))
      plot(clust$model, which.plots=1, main='K-Medoid fit')

# Extract multivariate and univariate mediods (class centers)
  clust$model$medoids
    pam(x[,1], 1)$medoids  

# join clusters to data
  x &lt;- data.frame(x, k=clust$model$clustering) 

} else { 
  cat("Please install cluster package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='optimized.sample.variance'>Optimized sample variance</h2><span id='topic+optimized.sample.variance'></span>

<h3>Description</h3>

<p>Draws an optimal sample that minimizes or maximizes the sample variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimized.sample.variance(x, n, type = "maximized")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimized.sample.variance_+3A_x">x</code></td>
<td>
<p>A vector to draw a sample from</p>
</td></tr>
<tr><td><code id="optimized.sample.variance_+3A_n">n</code></td>
<td>
<p>Number of samples to draw</p>
</td></tr>
<tr><td><code id="optimized.sample.variance_+3A_type">type</code></td>
<td>
<p>Type of sample variance optimization 
c(&quot;maximized&quot;, &quot;minimized&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with &quot;idx&quot; representing the index of the original vector  
and &quot;y&quot; is the value of the sampled data
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
if (require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, agr = "constant")

 n = 15
 # Draw n samples that maximize the variance of y
 ( max.sv &lt;- optimized.sample.variance(meuse$zinc, 15) )
 
 # Draw n samples that minimize the variance of y
 ( min.sv &lt;- optimized.sample.variance(meuse$zinc, 15, 
               type="minimized") )
 
 # Plot results
 plot(st_geometry(meuse), pch=19, col="grey")
   plot(st_geometry(meuse[max.sv$idx,]), col="red", add=TRUE, pch=19)
     plot(st_geometry(meuse[min.sv$idx,]), col="blue", add=TRUE, pch=19)
 	  box()
     legend("topleft", legend=c("population","maximized variance", 
            "minimized variance"), col=c("grey","red","blue"),  
            pch=c(19,19,19))  
 
} else { 
  cat("Please install sp package to run example", "\n")
}
</code></pre>

<hr>
<h2 id='outliers'>Outliers</h2><span id='topic+outliers'></span>

<h3>Description</h3>

<p>Identify outliers using modified Z-score
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outliers(x, s = 1.4826)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outliers_+3A_x">x</code></td>
<td>
<p>A numeric vector</p>
</td></tr>
<tr><td><code id="outliers_+3A_s">s</code></td>
<td>
<p>Scaling factor for mad statistic</p>
</td></tr>
</table>


<h3>Value</h3>

<p>value for the modified Z-score
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Iglewicz, B. &amp; D.C. Hoaglin (1993) How to Detect and Handle Outliers, 
American Society for Quality Control, Milwaukee, WI.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Create data with 3 outliers
    x &lt;- seq(0.1, 5, length=100) 
    x[98:100] &lt;- c(100, 55, 250)
 
 # Calculate Z score
     Z &lt;- outliers(x) 
 
 # Show number of extreme outliers using Z-score
     length(Z[Z &gt; 9.9])
 
 # Remove extreme outliers 
     x &lt;- x[-which(Z &gt; 9.9)]

</code></pre>

<hr>
<h2 id='overlap'>Niche overlap (Warren's-I)</h2><span id='topic+overlap'></span>

<h3>Description</h3>

<p>Similarity Statistic for Quantifying Niche Overlap using Warren's-I
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overlap(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="overlap_+3A_x">x</code></td>
<td>
<p>A matrix or SpatRaster raster class object</p>
</td></tr>
<tr><td><code id="overlap_+3A_y">y</code></td>
<td>
<p>A matrix or SpatRaster raster class object
with the same dimensions of x</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The overlap function computes the I similarity statistic (Warren et al. 2008)  
of two overlapping niche estimates. Similarity is based on the Hellenger distance. 
It is assumed that the input data share the same extent and cellsize and all values 
are positive.
</p>
<p>The I similarity statistic sums the pair-wise differences between two
predictions to create a single value representing the similarity of the two
distributions. The I similarity statistic ranges from a value of 0, where
two distributions have no overlap, to 1 where two distributions are
identical (Warren et al., 2008). The function is based on code  
from Jeremy VanDerWal
</p>


<h3>Value</h3>

<p>A vector (single value) representing the I similarity statistic
</p>


<h3>Author(s)</h3>

<p>Jeffrey Evans &lt;jeffrey_evans@tnc.org&gt; and Jeremy VanDerWal
</p>


<h3>References</h3>

<p>Warren, D. L., R. E. Glor, M. Turelli, and D. Funk. (2008).
Environmental Niche Equivalency versus Conservatism: Quantitative 
Approaches to Niche Evolution. Evolution 62:2868-2883.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
# add degree of separation in two matrices 
p1 &lt;- abs(matrix(1:50,nr=50,nc=50) + 
         runif(n = 2500, min = -1, max = 1))
p2 &lt;- abs(matrix(1:50,nr=50,nc=50) + 
         rnorm(n = 2500, mean = 1, sd = 1))
 
# High overlap/similarity 
( I &lt;- overlap(p1,p2) ) 

</code></pre>

<hr>
<h2 id='parea.sample'>Percent area sample</h2><span id='topic+parea.sample'></span>

<h3>Description</h3>

<p>Creates a point sample of polygons where n is based 
on percent area
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parea.sample(x, pct = 0.1, join = FALSE, sf = 4046.86, stype = "random", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parea.sample_+3A_x">x</code></td>
<td>
<p>An sf POLYGON object</p>
</td></tr>
<tr><td><code id="parea.sample_+3A_pct">pct</code></td>
<td>
<p>Percent of area sampled</p>
</td></tr>
<tr><td><code id="parea.sample_+3A_join">join</code></td>
<td>
<p>FALSE/TRUE Join polygon attributed to point sample</p>
</td></tr>
<tr><td><code id="parea.sample_+3A_sf">sf</code></td>
<td>
<p>Scaling factor (default is meters to acres conversion factor)</p>
</td></tr>
<tr><td><code id="parea.sample_+3A_stype">stype</code></td>
<td>
<p>Sampling type ('random', 'regular', 'nonaligned', 'hexagonal')</p>
</td></tr>
<tr><td><code id="parea.sample_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to spsample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function results in an adaptive sample based on the area of 
each polygon. The default scaling factor (sf) converts meters to
acres. You can set sf=1 to stay in the native projection units
</p>


<h3>Value</h3>

<p>An sf POINT object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
nc &lt;- st_read(system.file("shape/nc.shp", package="sf"))
  nc &lt;- suppressWarnings(st_cast(nc[c(10,100),], "POLYGON"))
  
 ( ars &lt;- parea.sample(nc, pct=0.001, join = TRUE, stype='random') ) 
     plot(st_geometry(nc))
        plot(st_geometry(ars), pch=19, add=TRUE)  

</code></pre>

<hr>
<h2 id='parse.bits'>Parse bits</h2><span id='topic+parse.bits'></span>

<h3>Description</h3>

<p>Returns specified bit value based on integer input
</p>
<p>Data such as MODIS the QC band are stored in bits. This function returns the 
value(s) for specified bit. For example, the MODIS QC flag are bits 0-1 with 
the bit value 00 representing the &quot;LST produced, good quality&quot; flag. When 
exported from HDF the QC bands are often in an 8 bit integer range (0-255). 
With this function you can parse the values for each bit to assign the 
flag values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse.bits(x, bit, depth = 8, order = c("reverse", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse.bits_+3A_x">x</code></td>
<td>
<p>Integer value</p>
</td></tr>
<tr><td><code id="parse.bits_+3A_bit">bit</code></td>
<td>
<p>A single or vector of bits to return</p>
</td></tr>
<tr><td><code id="parse.bits_+3A_depth">depth</code></td>
<td>
<p>The depth (length) of the bit range, default is 8</p>
</td></tr>
<tr><td><code id="parse.bits_+3A_order">order</code></td>
<td>
<p>c(&quot;reverse&quot;, &quot;none&quot;) sort order for the bits</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector or data.frame of parsed interger value(s) associated with input bit
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return value for bit 5 for integer value 100
parse.bits(100, 5)
 
# Return value(s) for bits 0 and 1 for integer value 100
parse.bits(100, c(0,1))

# Return value(s) for bits 0 and 1 for integer values 0-255
for(i in 0:255) { print(parse.bits(i, c(0,1))) }
 

#### Applied Example using Harmonized Landsat Sentinel-2 QC 

# Create dummy data and qc band
library(terra)
r &lt;- rast(nrow=100, ncol=100)
  r[] &lt;- round(runif(ncell(r), 0,1)) 
qc &lt;- rast(nrow=100, ncol=100)
  qc[] &lt;- round(runif(ncell(qc), 64,234)) 

# Calculate bit values from QC table
( qc_bits &lt;- data.frame(int=0:255, 
	cloud = unlist(lapply(0:255, FUN=parse.bits, bit=1)),
	shadow = unlist(lapply(0:255, FUN=parse.bits, bit=3)),
	acloud = unlist(lapply(0:255, FUN=parse.bits, bit=2)),
	cirrus = unlist(lapply(0:255, FUN=parse.bits, bit=0)),
	aerosol = unlist(lapply(0:255, FUN=parse.bits, bit=c(7,6)))) )
		
# Query the results to create a vector of integer values indicating what to mask 
#  cloud is bit 1 and shadow bit 3	
m &lt;- sort(unique(qc_bits[c(which(qc_bits$cloud == 1),
                           which(qc_bits$shadow == 1)
						   ),]$int))

# Apply queried integer values to mask image with QA band
qc[qc %in% m] &lt;- NA
r &lt;- mask(r, qc)


</code></pre>

<hr>
<h2 id='partial.cor'>Partial and Semi-partial correlation</h2><span id='topic+partial.cor'></span>

<h3>Description</h3>

<p>Calculates a partial or semi-partial correlation
with parametric and nonparametric options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial.cor(
  x,
  y,
  z,
  method = c("partial", "semipartial"),
  statistic = c("kendall", "pearson", "spearman")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial.cor_+3A_x">x</code></td>
<td>
<p>A vector, data.frame or matrix with 3 columns</p>
</td></tr>
<tr><td><code id="partial.cor_+3A_y">y</code></td>
<td>
<p>A vector same length as x</p>
</td></tr>
<tr><td><code id="partial.cor_+3A_z">z</code></td>
<td>
<p>A vector same length as x</p>
</td></tr>
<tr><td><code id="partial.cor_+3A_method">method</code></td>
<td>
<p>Type of correlation: &quot;partial&quot; or &quot;semipartial&quot;</p>
</td></tr>
<tr><td><code id="partial.cor_+3A_statistic">statistic</code></td>
<td>
<p>Correlation statistic, options are: &quot;kendall&quot;,
&quot;pearson&quot;, &quot;spearman&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partial and semipartial correlations show the association between two
variables when one or more peripheral variables are controlled
to hold them constant.
</p>
<p>Suppose we have three variables, X, Y, and Z. Partial correlation holds
constant one variable when computing the relations two others. Suppose we
want to know the correlation between X and Y holding Z constant for both
X and Y. That would be the partial correlation between X and Y controlling
for Z. Semipartial correlation holds Z constant for either X or Y, but not
both, so if we wanted to control X for Z, we could compute the semipartial
correlation between X and Y holding Z constant for X.
</p>


<h3>Value</h3>

<p>data.frame containing:
</p>

<ul>
<li><p> correlation - correlation coefficient
</p>
</li>
<li><p> p.value - p-value of correlation
</p>
</li>
<li><p> test.statistic - test statistic
</p>
</li>
<li><p> n - sample size
</p>
</li>
<li><p> Method - indicating partial or semipartial correlation
</p>
</li>
<li><p> Statistic - the correlation statistic used
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>air.flow = stackloss[,1]
water.temperature = stackloss[,2]
acid = stackloss[,3]

# Partial using Kendall (nonparametric) correlation
partial.cor(air.flow, water.temperature, acid)

scholar &lt;- data.frame(
  HSGPA=c(3.0, 3.2, 2.8, 2.5, 3.2, 3.8, 3.9, 3.8, 3.5, 3.1), 
	 FGPA=c(2.8, 3.0, 2.8, 2.2, 3.3, 3.3, 3.5, 3.7, 3.4, 2.9),
  SATV =c(500, 550, 450, 400, 600, 650, 700, 550, 650, 550)) 

# Standard Pearson's correlations between HSGPA and FGPA  
cor(scholar[,1], scholar[,2])

# Partial correlation using Pearson (parametric) between HSGPA 
#   and FGPA, controlling for SATV
partial.cor(scholar, statistic="pearson")

# Semipartial using Pearson (parametric) correlation 
partial.cor(x=scholar[,2], y=scholar[,1], z=scholar[,3], 
            method="semipartial", statistic="pearson")

</code></pre>

<hr>
<h2 id='plot.effect.size'>Plot effect size</h2><span id='topic+plot.effect.size'></span>

<h3>Description</h3>

<p>Plot function for effect.size object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'effect.size'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.effect.size_+3A_x">x</code></td>
<td>
<p>A effect.size object</p>
</td></tr>
<tr><td><code id="plot.effect.size_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of effect size object with group effect sizes and 95
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>

<hr>
<h2 id='plot.loess.boot'>Plot Loess Bootstrap</h2><span id='topic+plot.loess.boot'></span>

<h3>Description</h3>

<p>Plot function for loess.boot object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loess.boot'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.loess.boot_+3A_x">x</code></td>
<td>
<p>A loess.boot object</p>
</td></tr>
<tr><td><code id="plot.loess.boot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plot of lowess bootstrap
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> n=1000
 x &lt;- seq(0, 4, length.out=n)	 
 y &lt;- sin(2*x)+ 0.5*x + rnorm(n, sd=0.5)
 sb &lt;- loess.boot(x, y, nreps = 99, confidence = 0.90, span = 0.40)
 plot(sb)
                   
</code></pre>

<hr>
<h2 id='poly_trend'>Polynomial trend</h2><span id='topic+poly_trend'></span>

<h3>Description</h3>

<p>Fits a polynomial trend using specified order
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly_trend(x, y, degree, ci = 0.95, plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_trend_+3A_x">x</code></td>
<td>
<p>Vector of x</p>
</td></tr>
<tr><td><code id="poly_trend_+3A_y">y</code></td>
<td>
<p>Vector of y</p>
</td></tr>
<tr><td><code id="poly_trend_+3A_degree">degree</code></td>
<td>
<p>Polynomial order (default 3)</p>
</td></tr>
<tr><td><code id="poly_trend_+3A_ci">ci</code></td>
<td>
<p>+/- confidence interval (default 0.95)</p>
</td></tr>
<tr><td><code id="poly_trend_+3A_plot">plot</code></td>
<td>
<p>Plot results (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="poly_trend_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A fit using a lm(y ~ x + I(X^2) + I(X^3)) form will be correlated which,
can cause problems. The function avoids undue correlation using orthogonal
polynomials
</p>


<h3>Value</h3>

<p>A poly.trend class (list) containing
</p>

<ul>
<li><p> trend        data.frame of fit polynomial and upper/lower confidence intervals
</p>
</li>
<li><p> model        Class lm model object fit with poly term
</p>
</li>
<li><p> prameterCI   Intercept confidence intervals of Nth order polynomials
</p>
</li>
<li><p> order        Specified polynomial order
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> set.seed(42)
 x &lt;- seq(from=0, to=20, by=0.1)
 y &lt;- (500 + 0.4 * (x-10)^3)
 noise &lt;- y + rnorm(length(x), mean=10, sd=80) 
 
 p &lt;- poly_trend(x, noise, degree = 3, ci = 0.95,
                 main="3rd degree polynomial")
 
 dev.new(height=6, width=12)
   layout(matrix(c(1,2), 1, 2, byrow = TRUE))
   p &lt;- poly_trend(x, noise, degree = 3, 
                   main="3rd degree polynomial")
   p &lt;- poly_trend(x, noise, degree = 6, 
                   main="6th degree polynomial")

cat("Confidence intervals for", "1 -", p$order, "polynomials",  "\n")
  p$prameterCI

</code></pre>

<hr>
<h2 id='poly.regression'>Local Polynomial Regression</h2><span id='topic+poly.regression'></span>

<h3>Description</h3>

<p>Calculates a Local Polynomial Regression for smoothing
or imputation of missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly.regression(
  y,
  x = NULL,
  s = 0.75,
  impute = FALSE,
  na.only = FALSE,
  ci = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly.regression_+3A_y">y</code></td>
<td>
<p>Vector to smooth or impute NA values</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_x">x</code></td>
<td>
<p>Optional x covariate data (must match dimensions of y)</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_s">s</code></td>
<td>
<p>Smoothing parameter (larger equates to more smoothing)</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_impute">impute</code></td>
<td>
<p>(FALSE/TRUE) Should NA values be inputed</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_na.only">na.only</code></td>
<td>
<p>(FALSE/TRUE) Should only NA values be change in y</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_ci">ci</code></td>
<td>
<p>(FALSE/TRUE) Should confidence intervals be returned</p>
</td></tr>
<tr><td><code id="poly.regression_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to loess</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper function for loess that simplifies data smoothing and imputation
of missing values. The function allows for smoothing a vector, based on an index
(derived automatically) or covariates. If the impute option is TRUE NA values are
imputed, otherwise the returned vector will still have NA's present. If impute and
na.only are both TRUE the vector is returned, without being smoothed but with imputed
NA values filled in. The loess weight function is defined using the tri-cube weight
function w(x) = (1-|x|^3)^3 where; x is the distance of a data point from the point
the curve being fitted.
</p>


<h3>Value</h3>

<p>If ci = FALSE, a vector of smoothed values,
otherwise a list object with:
</p>

<ul>
<li><p> loess - A vector, same length of y, representing the smoothed or
inputed data
</p>
</li>
<li><p> lower.ci - Lower confidence interval
</p>
</li>
<li><p> upper.ci - Upper confidence interval
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code> for loess ... model options
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- seq(-20, 20, 0.1)
 y &lt;- sin(x)/x + rnorm(length(x), sd=0.03)
 p &lt;- which(y == "NaN")
 y &lt;- y[-p]
 r &lt;- poly.regression(y, ci=TRUE, s=0.30)
 
 plot(y,type="l", lwd=0.5, main="s = 0.10")
   y.polygon &lt;- c((r$lower.ci)[1:length(y)], (r$upper.ci)[rev(1:length(y))])
   x.polygon &lt;- c(1:length(y), rev(1:length(y)))
   polygon(x.polygon, y.polygon, col="#00009933", border=NA) 
      lines(r$loess, lwd=1.5, col="red")
 
 # Impute NA values, replacing only NA's
 y.na &lt;- y
 y.na[c(100,200,300)] &lt;- NA 
 p.y &lt;- poly.regression(y.na, s=0.10, impute = TRUE, na.only = TRUE)
 y - p.y
 
 plot(p.y,type="l", lwd=1.5, col="blue", main="s = 0.10")
   lines(y, lwd=1.5, col="red")

</code></pre>

<hr>
<h2 id='polyPerimeter'>Polygon perimeter</h2><span id='topic+polyPerimeter'></span>

<h3>Description</h3>

<p>Calculates the perimeter length(s) for a polygon object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polyPerimeter(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polyPerimeter_+3A_x">x</code></td>
<td>
<p>sf POLYGON class object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of polygon perimeters in projection units
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
  polys &lt;- st_read(system.file("shape/nc.shp", package="sf"))
    polys &lt;- suppressWarnings(st_cast(polys[c(10,100),], "POLYGON"))
 
 polyPerimeter(polys)

</code></pre>

<hr>
<h2 id='pp.subsample'>Point process random subsample</h2><span id='topic+pp.subsample'></span>

<h3>Description</h3>

<p>Generates random subsample based on density estimate
of observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pp.subsample(
  x,
  n,
  window = "hull",
  sigma = "Scott",
  wts = NULL,
  gradient = 1,
  edge = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pp.subsample_+3A_x">x</code></td>
<td>
<p>An sf POINT class</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_n">n</code></td>
<td>
<p>Number of random samples to generate</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_window">window</code></td>
<td>
<p>Type of window (hull or extent)</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_sigma">sigma</code></td>
<td>
<p>Bandwidth selection method for KDE, default is 'Scott'.
Options are 'Scott', 'Stoyan', 'Diggle', 'likelihood',
and 'geometry'</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_wts">wts</code></td>
<td>
<p>Optional vector of weights corresponding to point pattern</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_gradient">gradient</code></td>
<td>
<p>A scaling factor applied to the sigma parameter used to
adjust the gradient decent of the density estimate. The
default is 1, for no adjustment (downweight &lt; 1 | upweight &gt; 1)</p>
</td></tr>
<tr><td><code id="pp.subsample_+3A_edge">edge</code></td>
<td>
<p>Apply Diggle edge correction (TRUE/FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The window type creates a convex hull by default or, optionally, uses the maximum
extent (envelope). The resulting bandwidth can vary widely by method. the 'diggle'
method is intended for  bandwidth representing 2nd order spatial variation whereas
the 'scott' method will represent 1st order trend. the 'geometry' approach will also
represent 1st order trend. for large datasets, caution should be used with the 2nd
order 'likelihood' approach, as it is slow and computationally expensive. finally,
the 'stoyan' method will produce very strong 2nd order results. '
</p>
<p>Available bandwidth selection methods are:
</p>

<ul>
<li><p> Scott - (Scott 1992), Scott's Rule for Bandwidth Selection (1st order)
</p>
</li>
<li><p> Diggle - (Berman &amp; Diggle 1989), Minimise the mean-square error via cross
validation (2nd order)
</p>
</li>
<li><p> likelihood - (Loader 1999), Maximum likelihood cross validation (2nd order)
</p>
</li>
<li><p> geometry - Bandwidth is based on simple window geometry (1st order)
</p>
</li>
<li><p> Stoyan - (Stoyan &amp; Stoyan 1995), Based on pair-correlation function (strong 2nd order)
</p>
</li>
<li><p> User defined - using a numeric value for sigma
</p>
</li></ul>



<h3>Value</h3>

<p>sf class POINT geometry containing random subsamples
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Berman, M. and Diggle, P. (1989) Estimating weighted integrals of the second-order
intensity of a spatial point process. Journal of the Royal Statistical Society,
series B 51, 81-92.
</p>
<p>Fithian, W &amp; T. Hastie (2013) Finite-sample equivalence in statistical models for
presence-only data. Annals of Applied Statistics 7(4): 1917-1939
</p>
<p>Hengl, T., H. Sierdsema, A. Radovic, and A. Dilo (2009) Spatial prediction of species
distributions from occurrence-only records: combining point pattern analysis,
ENFA and regression-kriging. Ecological Modelling, 220(24):3499-3511
</p>
<p>Loader, C. (1999) Local Regression and Likelihood. Springer, New York.
</p>
<p>Scott, D.W. (1992) Multivariate Density Estimation. Theory, Practice and Visualization.
New York, Wiley.
</p>
<p>Stoyan, D. and Stoyan, H. (1995) Fractals, random shapes and point fields: methods of
geometrical statistics. John Wiley and Sons.
</p>
<p>Warton, D.i., and L.C. Shepherd (2010) Poisson Point Process Models Solve the Pseudo-Absence
Problem for Presence-only Data in Ecology. The Annals of Applied Statistics, 4(3):1383-1402
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library(sf) 
if(require(spatstat.explore, quietly = TRUE)) { 
data(bei, package = "spatstat.data")

trees &lt;- st_as_sf(bei)
  trees &lt;- trees[-1,]

n=round(nrow(trees) * 0.10, digits=0)       
trees.wrs &lt;- pp.subsample(trees, n=n, window='hull')
  plot(st_geometry(trees), pch=19, col='black')
    plot(st_geometry(trees.wrs), pch=19, col='red', add=TRUE) 
      box()
       title('10% subsample')
    legend('bottomright', legend=c('Original sample', 'Subsample'), 
                 col=c('black','red'),pch=c(19,19))   

} else { 
  cat("Please install spatstat.explore package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='print.cross.cor'>Print spatial cross correlation</h2><span id='topic+print.cross.cor'></span>

<h3>Description</h3>

<p>print method for class &quot;cross.cor&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cross.cor'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cross.cor_+3A_x">x</code></td>
<td>
<p>Object of class cross.cor</p>
</td></tr>
<tr><td><code id="print.cross.cor_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When not simulated k=0, prints functions list object containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value of NULL to represent p values were derived from observed data (k=0)
</p>
</li>
<li><p> p - Probability based observations above/below confidence interval
</p>
</li>
<li><p> t.test - Probability based on t-test
</p>
</li></ul>

<p>When simulated (k&gt;0), prints functions list object containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value representing number of simulations
</p>
</li>
<li><p> global.p - p-value of global autocorrelation statistic
</p>
</li>
<li><p> local.p - Probability based simulated data using successful rejection of t-test
</p>
</li>
<li><p> range.p - Probability based on range of probabilities resulting from paired t-test
</p>
</li></ul>


<hr>
<h2 id='print.effect.size'>Print effect size</h2><span id='topic+print.effect.size'></span>

<h3>Description</h3>

<p>print method for class &quot;effect.size&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'effect.size'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.effect.size_+3A_x">x</code></td>
<td>
<p>Object of class effect.size</p>
</td></tr>
<tr><td><code id="print.effect.size_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints the output data.frame contaning; effect size with upper and lower confidence 
and, mean and sd by group
</p>

<hr>
<h2 id='print.loess.boot'>Print Loess bootstrap model</h2><span id='topic+print.loess.boot'></span>

<h3>Description</h3>

<p>print method for class &quot;loess.boot&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loess.boot'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.loess.boot_+3A_x">x</code></td>
<td>
<p>Object of class loess.boot</p>
</td></tr>
<tr><td><code id="print.loess.boot_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>same as summary lowess.boot of data.frame including;
</p>

<ul>
<li><p> nreps        Number of bootstrap replicates
</p>
</li>
<li><p> confidence   Confidence interval (region)
</p>
</li>
<li><p> span         alpha (span) parameter used loess fit
</p>
</li>
<li><p> degree       polynomial degree used in loess fit
</p>
</li>
<li><p> normalize    Normalized data (TRUE/FALSE)
</p>
</li>
<li><p> family       Family of statistic used in fit
</p>
</li>
<li><p> parametric   Parametric approximation (TRUE/FALSE)
</p>
</li>
<li><p> surface      Surface fit, see loess.control
</p>
</li>
<li><p> data         data.frame of x,y used in model
</p>
</li>
<li><p> fit          data.frame including:
</p>

<ol>
<li><p> x - Equally-spaced x index
</p>
</li>
<li><p> y.fit - loess fit
</p>
</li>
<li><p> up.lim - Upper confidence interval
</p>
</li>
<li><p> low.lim - Lower confidence interval
</p>
</li>
<li><p> stddev - Standard deviation of loess fit at each x value
</p>
</li></ol>

</li></ul>


<hr>
<h2 id='print.poly.trend'>Print poly_trend</h2><span id='topic+print.poly.trend'></span>

<h3>Description</h3>

<p>print method for class &quot;poly.trend&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'poly.trend'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.poly.trend_+3A_x">x</code></td>
<td>
<p>Object of class poly.trend</p>
</td></tr>
<tr><td><code id="print.poly.trend_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints trend model summary, order and trend confidence intervals
</p>

<hr>
<h2 id='proximity.index'>Proximity Index</h2><span id='topic+proximity.index'></span>

<h3>Description</h3>

<p>Calculates proximity index for a set of polygons
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proximity.index(x, y = NULL, min.dist = 0, max.dist = 1000, background = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proximity.index_+3A_x">x</code></td>
<td>
<p>A polygon class sp or sf object</p>
</td></tr>
<tr><td><code id="proximity.index_+3A_y">y</code></td>
<td>
<p>Optional column in data containing classes</p>
</td></tr>
<tr><td><code id="proximity.index_+3A_min.dist">min.dist</code></td>
<td>
<p>Minimum threshold distance</p>
</td></tr>
<tr><td><code id="proximity.index_+3A_max.dist">max.dist</code></td>
<td>
<p>Maximum neighbor distance</p>
</td></tr>
<tr><td><code id="proximity.index_+3A_background">background</code></td>
<td>
<p>Optional value in y column indicating background value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector equal to nrow(x) of proximity index values, if a background value is 
specified NA values will be returned in the position(s) of the specified class
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Gustafson, E.J., &amp; G.R. Parker (1994) Using an Index of Habitat Patch Proximity 
for Landscape Design. Landscape and Urban Planning 29:117-130
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(sf)
 if(require(sp, quietly = TRUE)) {
   data(meuse, package = "sp")
   meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                     agr = "constant")
   meuse &lt;- st_buffer(meuse, dist = meuse$elev * 5)
     meuse$LU &lt;- sample(c("forest","nonforest"), nrow(meuse), 
                       replace=TRUE) 

 # All polygon proximity index 1000 radius	
 ( pidx &lt;- proximity.index(meuse, min.dist = 1) )
   pidx[pidx &gt; 1000] &lt;- 1000
 
 # Class-level proximity index 1000 radius
 ( pidx.class &lt;- proximity.index(meuse, y = "LU", min.dist = 1) )
   
 # plot index for all polygons
 meuse$pidx &lt;- pidx
   plot(meuse["pidx"])
 
 # plot index for class-level polygons 
 meuse$cpidx &lt;- pidx.class
   plot(meuse["cpidx"])
 
 # plot index for just forest class
 forest &lt;- meuse[meuse$LU == "forest",]
  plot(forest["cpidx"])

} else { 
  cat("Please install sp package to run example", "\n")
}
   
</code></pre>

<hr>
<h2 id='pseudo.absence'>Pseudo-absence random samples</h2><span id='topic+pseudo.absence'></span>

<h3>Description</h3>

<p>Generates pseudo-absence samples based on density estimate
of known locations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pseudo.absence(
  x,
  n,
  window = "hull",
  ref = NULL,
  s = NULL,
  sigma = "Scott",
  wts = NULL,
  KDE = FALSE,
  gradient = 1,
  p = NULL,
  edge = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pseudo.absence_+3A_x">x</code></td>
<td>
<p>An sf POINT geometry object</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_n">n</code></td>
<td>
<p>Number of random samples to generate</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_window">window</code></td>
<td>
<p>Type of window (hull OR extent), overridden if mask provided</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_ref">ref</code></td>
<td>
<p>Optional terra SpatRaster class raster. The resolution of the
density estimate will match mask.</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_s">s</code></td>
<td>
<p>Optional resolution passed to window argument. Caution should be
used due to long processing times associated with high
resolution. In contrast, coarse resolution can exclude
known points.</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_sigma">sigma</code></td>
<td>
<p>Bandwidth selection method for KDE, default is 'Scott'.
Options are 'Scott', 'Stoyan', 'Diggle', 'likelihood',
and 'geometry'</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_wts">wts</code></td>
<td>
<p>Optional vector of weights corresponding to point pattern</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_kde">KDE</code></td>
<td>
<p>Return KDE raster (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_gradient">gradient</code></td>
<td>
<p>A scaling factor applied to the sigma parameter used to
adjust the gradient decent of the density estimate. The
default is 1, for no adjustment (downweight &lt; 1 | upweight &gt; 1)</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_p">p</code></td>
<td>
<p>Minimum value for probability distribution (must be &gt;  0)</p>
</td></tr>
<tr><td><code id="pseudo.absence_+3A_edge">edge</code></td>
<td>
<p>Apply Diggle edge correction (TRUE/FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The window type creates a convex hull by default or, optionally, uses the
maximum extent (envelope). If a mask is provided the kde will represent
areas defined by the mask and defines the area that pseudo absence data
will be generated.
</p>
<p>Available bandwidth selection methods are:
</p>

<ul>
<li><p> Scott (Scott 1992), Scott's Rule for Bandwidth Selection (1st order)
</p>
</li>
<li><p> Diggle (Berman &amp; Diggle 1989), Minimize the mean-square error via cross
</p>
</li>
<li><p> validation (2nd order)
</p>
</li>
<li><p> likelihood (Loader 1999), Maximum likelihood cross validation (2nd order)
</p>
</li>
<li><p> geometry, Bandwidth is based on simple window geometry (1st order)
</p>
</li>
<li><p> Stoyan (Stoyan &amp; Stoyan 1995), Based on pair-correlation function (strong 2nd order)
</p>
</li>
<li><p> User defined numeric distance bandwidth
</p>
</li></ul>



<h3>Value</h3>

<p>A list class object with the following components:
</p>

<ul>
<li><p> sample  A sf POINT geometry object containing random samples
</p>
</li>
<li><p> kde     A terra SpatRaster class of inverted Isotropic KDE estimates
used as sample weights (IF KDE = TRUE)
</p>
</li>
<li><p> sigma   Selected bandwidth of KDE
</p>
</li></ul>



<h3>Note</h3>

<p>resulting bandwidth can vary widely by method. the 'diggle' method
is intended for selecting bandwidth representing 2nd order spatial variation
whereas the 'scott' method will represent 1st order trend. the 'geometry' approach
will also represent 1st order trend. For large datasets, caution should be used with
the 2nd order 'likelihood' approach, as it is slow and computationally expensive.
finally, the 'stoyan' method will produce very strong 2nd order results.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Berman, M. and Diggle, P. (1989) Estimating weighted integrals of the second-order
intensity of a spatial point process. Journal of the Royal Statistical Society,
series B 51, 81-92.
</p>
<p>Fithian, W &amp; T. Hastie (2013) Finite-sample equivalence in statistical models for
presence-only data. Annals of Applied Statistics 7(4): 1917-1939
</p>
<p>Hengl, T., H. Sierdsema, A. Radovic, and A. Dilo (2009) Spatial prediction of species
distributions from occurrence-only records: combining point pattern analysis,
ENFA and regression-kriging. Ecological Modelling, 220(24):3499-3511
</p>
<p>Loader, C. (1999) Local Regression and Likelihood. Springer, New York.
</p>
<p>Scott, D.W. (1992) Multivariate Density Estimation. Theory, Practice and Visualization.
New York, Wiley.
</p>
<p>Stoyan, D. and Stoyan, H. (1995) Fractals, random shapes and point fields: methods of
geometrical statistics. John Wiley and Sons.
</p>
<p>Warton, D.i., and L.C. Shepherd (2010) Poisson Point Process Models Solve the Pseudo-Absence
Problem for Presence-only Data in Ecology. The Annals of Applied Statistics, 4(3):1383-1402
</p>


<h3>Examples</h3>

<pre><code class='language-R'> p = c("sf", "sp", "terra", "spatstat.data")
 if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
   m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
   message("Can't run examples, please install ", paste(p[m], collapse = " "))
 } else {
   invisible(lapply(p, require, character.only=TRUE))
  
data(meuse, package = "sp")
meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                  agr = "constant") 

# Using a raster mask   
r &lt;- rast(ext(meuse), resolution=40, crs=crs(meuse))
  r[] &lt;- rep(1,ncell(r))
  
pa &lt;- pseudo.absence(meuse, n=100, window='hull', KDE=TRUE, ref = r, 
                     sigma='Diggle', s=50) 
  col.br &lt;- colorRampPalette(c('blue','yellow'))
    plot(pa$kde, col=col.br(10))
      plot(st_geometry(meuse), pch=20, cex=1, add=TRUE)
        plot(st_geometry(pa$sample), col='red', pch=20, cex=1, add=TRUE)
          legend('top', legend=c('Presence', 'Pseudo-absence'), 
                 pch=c(20,20),col=c('black','red'), bg="white")

# With clustered data
data(bei, package = "spatstat.data")
  trees &lt;- st_as_sf(bei)
    trees &lt;- trees[-1,]

trees.abs &lt;- pseudo.absence(trees, n=100, window='extent', KDE=TRUE)
  col.br &lt;- colorRampPalette(c('blue','yellow'))
    plot(trees.abs$kde, col=col.br(10))
     plot(st_geometry(trees), pch=20, cex=0.50, add=TRUE)
        plot(st_geometry(trees.abs$sample), col='red', pch=20, cex=1, add=TRUE)
          legend('top', legend=c('Presence', 'Pseudo-absence'), 
                 pch=c(20,20),col=c('black','red'),bg="white")
}     
</code></pre>

<hr>
<h2 id='pu'>Biodiversity Planning Units</h2><span id='topic+pu'></span>

<h3>Description</h3>

<p>Subset of biodiversity planning units for Haiti ecoregional spatial reserve plan
</p>


<h3>Format</h3>

<p>A sp SpatialPolygonsDataFrame with 5919 rows and 46 variables:
</p>

<dl>
<dt>UNIT_ID</dt><dd><p>Unique planning unit ID</p>
</dd>
<dt>DR_Dr_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Dr_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Ms_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Ms_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_LM_M</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_M_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_R_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_LM_R_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Rn_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_LM_R_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Rn_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Ms_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Ms_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Ms_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Rn_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Rn_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_R_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>DR_Rn_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Th_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Th_L</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Th_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_U</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Dr_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_M_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_M_E</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_R_A</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_M_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_R_I</dt><dd><p>Biodiversity target</p>
</dd>
<dt>H_LM_R_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_S</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Ms_U</dt><dd><p>Biodiversity target</p>
</dd>
<dt>Ht_Rn_U</dt><dd><p>Biodiversity target</p>
</dd>
</dl>



<h3>Source</h3>

<p>&quot;The Nature Conservancy&quot;
</p>


<h3>References</h3>

<p>Evans, J.S., S.R. Schill, G.T. Raber (2015) A Systematic Framework for Spatial Conservation Planning and Ecological Priority Design in St. Lucia, Eastern Caribbean. Chapter 26 in Central American Biodiversity : Conservation, Ecology and a Sustainable Future. F. Huettman (eds). Springer, NY.
</p>

<hr>
<h2 id='quadrats'>Quadrats</h2><span id='topic+quadrats'></span>

<h3>Description</h3>

<p>Creates quadrat polygons for sampling or analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadrats(x, s = 250, n = 100, r = NULL, sp = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quadrats_+3A_x">x</code></td>
<td>
<p>An sf POLYGONS object defining extent</p>
</td></tr>
<tr><td><code id="quadrats_+3A_s">s</code></td>
<td>
<p>Radius defining single or range of sizes of quadrats</p>
</td></tr>
<tr><td><code id="quadrats_+3A_n">n</code></td>
<td>
<p>Number of quadrats</p>
</td></tr>
<tr><td><code id="quadrats_+3A_r">r</code></td>
<td>
<p>A rotation factor for random rotation, default is NULL</p>
</td></tr>
<tr><td><code id="quadrats_+3A_sp">sp</code></td>
<td>
<p>(FALSE | TRUE) Output sp class object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The radius (s) parameter can be a single value or a range of values, 
representing a randomization range of resulting quadrat sizes. The
rotation (r) parameter can also be used to defined a fixed rotation or
random range of quadrat rotations. You can specify each of these parameters
using an explicit vector that will be sampled eg., seq(100,300,0.5)
</p>


<h3>Value</h3>

<p>an sf POLYGONS object with rotated polygon(s)
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
library(terra) 

# read meuse data and create convex hull 
if (require(sp, quietly = TRUE)) {
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, agr = "constant")
  e &lt;- st_convex_hull(st_union(meuse))

 # Fixed size 250 and no rotation 
 s &lt;- quadrats(e, s = 250, n = 10)
   plot(st_geometry(s))
 
   
# Variable sizes 100-300 and rotation of 0-45 degrees
s &lt;- quadrats(e, s = c(100,300), n = 10, r = c(0,45))
  plot(st_geometry(s))
 
# Variable sizes 100-300 and no rotation 
s &lt;- quadrats(e, s = c(100,300), n = 10)
  plot(st_geometry(s))
   

} else { 
  cat("Please install sp package to run example", "\n")
}
 
</code></pre>

<hr>
<h2 id='random.raster'>Random raster</h2><span id='topic+random.raster'></span>

<h3>Description</h3>

<p>Create a random raster or raster stack using specified 
distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random.raster(
  r = NULL,
  n.row = 50,
  n.col = 50,
  n.layers = 1,
  x = seq(1, 10),
  min = 0,
  max = 1,
  mean = 0,
  sd = 1,
  p = 0.5,
  s = 1.5,
  mask = TRUE,
  distribution = c("random", "normal", "seq", "binominal", "gaussian")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random.raster_+3A_r">r</code></td>
<td>
<p>Optional existing terra raster defining nrow/ncol</p>
</td></tr>
<tr><td><code id="random.raster_+3A_n.row">n.row</code></td>
<td>
<p>Number of rows</p>
</td></tr>
<tr><td><code id="random.raster_+3A_n.col">n.col</code></td>
<td>
<p>Number of columns</p>
</td></tr>
<tr><td><code id="random.raster_+3A_n.layers">n.layers</code></td>
<td>
<p>Number of layers in resulting raster stack</p>
</td></tr>
<tr><td><code id="random.raster_+3A_x">x</code></td>
<td>
<p>A vector of values to sample if distribution is &quot;sample&quot;</p>
</td></tr>
<tr><td><code id="random.raster_+3A_min">min</code></td>
<td>
<p>Minimum value of raster</p>
</td></tr>
<tr><td><code id="random.raster_+3A_max">max</code></td>
<td>
<p>Maximum value of raster</p>
</td></tr>
<tr><td><code id="random.raster_+3A_mean">mean</code></td>
<td>
<p>Mean of centered distribution</p>
</td></tr>
<tr><td><code id="random.raster_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of centered distribution</p>
</td></tr>
<tr><td><code id="random.raster_+3A_p">p</code></td>
<td>
<p>p-value for binominal distribution</p>
</td></tr>
<tr><td><code id="random.raster_+3A_s">s</code></td>
<td>
<p>sigma value for Gaussian distribution</p>
</td></tr>
<tr><td><code id="random.raster_+3A_mask">mask</code></td>
<td>
<p>(TRUE/FALSE) If r is provided, mask results to r</p>
</td></tr>
<tr><td><code id="random.raster_+3A_distribution">distribution</code></td>
<td>
<p>Available distributions, c(&quot;random&quot;, &quot;normal&quot;, 
&quot;seq&quot;, &quot;binominal&quot;, &quot;gaussian&quot;, &quot;sample&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Options for distributions are; random, normal, seq, binominal, gaussian and sample raster(s)
</p>


<h3>Value</h3>

<p>terra SpatRaster object with random rasters
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)

# Using existing raster to create random binominal  
r &lt;- rast(system.file("ex/elev.tif", package="terra"))    
( rr &lt;- random.raster(r, n.layers = 3, distribution="binominal") )
  plot(c(r,rr)) 

# default; random, nrows=50, ncols=50, n.layers=5
( rr &lt;- random.raster() )

# specified; binominal, nrows=20, ncols=20, nlayers=5
( rr &lt;- random.raster(n.layer=5, n.col=20, n.row=20,  
                     distribution="binominal") )

# specified; gaussian, nrows=50, ncols=50, nlayers=1
( rr &lt;- random.raster(n.col=50, n.row=50, s=8,  
                      distribution="gaussian") )
   plot(rr)
 
# specified; sample, nrows=50, ncols=50, nlayers=1
( rr &lt;- random.raster(n.layer=1, x=c(2,6,10,15), 
                     distribution="sample" ) )
  freq(rr)

</code></pre>

<hr>
<h2 id='raster.change'>Raster change between two nominal rasters</h2><span id='topic+raster.change'></span>

<h3>Description</h3>

<p>Compares two categorical rasters with a variety of statistical options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.change(
  x,
  y,
  s = 3,
  stat = c("kappa", "t.test", "cor", "entropy", "cross-entropy", "divergence"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.change_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster</p>
</td></tr>
<tr><td><code id="raster.change_+3A_y">y</code></td>
<td>
<p>A terra SpatRaster for comparison to x</p>
</td></tr>
<tr><td><code id="raster.change_+3A_s">s</code></td>
<td>
<p>Integer or matrix for defining Kernel,
must be odd but not necessarily square</p>
</td></tr>
<tr><td><code id="raster.change_+3A_stat">stat</code></td>
<td>
<p>Statistic to use in comparison, please see details for
options.</p>
</td></tr>
<tr><td><code id="raster.change_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::focalPairs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a various statistics for comparing two classified maps.
Valid options are:
</p>

<ul>
<li><p> kappa - Cohen's Kappa
</p>
</li>
<li><p> t.test - Two-tailed paired t-test
</p>
</li>
<li><p> cor - Persons Correlation
</p>
</li>
<li><p> entropy - Delta entropy
</p>
</li>
<li><p> cross-entropy - Cross-entropy loss function
</p>
</li>
<li><p> divergence - Kullback-Leibler divergence (relative entropy)
</p>
</li></ul>

<p>Kappa and t-test values &lt; 0 are reported as 0. For a weighted kappa, a matrix must
be provided that correspond to the pairwise weights for all values in both rasters.
Delta entropy is derived by calculating Shannon's on each focal window then
differencing  them (e(x) - e(y)). The s argument can be a single scalar, defining
a symmetrical kernel, two scalers defining the dimensions of the kernel eg., c(3,5)
or a matrix defining the kernel say, resulting from terra::focalMat
</p>


<h3>Value</h3>

<p>A terra SpatRaster layer containing one of the following layers:
</p>

<ul>
<li><p> kappa - Kappa or Weighted Kappa statistic (if stat = &quot;kappa&quot;)
</p>
</li>
<li><p> correlation - Paired t.test statistic  (if stat = &quot;cor&quot;)
</p>
</li>
<li><p> entropy - Local entropy  (if stat = &quot;entropy&quot;)
</p>
</li>
<li><p> divergence - Kullback-Leibler divergence (if stat = &quot;divergence&quot;)
</p>
</li>
<li><p> cross.entropy - Local Cross-entropy (if stat = &quot;cross.entropy&quot;)
</p>
</li>
<li><p> t.test - Paired t.test statistic  (if stat = &quot;t.test&quot;)
</p>
</li>
<li><p> p.value - p-value of the paired t.test statistic (if stat = &quot;t.test&quot;)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational
and Psychological Measurement, 20:37-46
</p>
<p>McHugh M.L. (2012) Interrater reliability: the kappa statistic.
Biochemia medica, 22(3):276–282.
</p>
<p>Kullback, S., R.A. Leibler (1951). On information and sufficiency. Annals of
Mathematical Statistics. 22(1):79–86
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(sf) 
 library(terra) 
  
 e &lt;- ext(179407.8, 181087.9, 331134.4, 332332.1)
 r1 &lt;- rast(e, resolution=20)
   r1[] &lt;- sample(1:5, ncell(r1), replace=TRUE)
 r2 &lt;- rast(e, resolution=20)
   r2[] &lt;- sample(1:5, ncell(r2), replace=TRUE)
 	  
 d = 5 # kernel    
 ( r.kappa &lt;- raster.change(r1, r2, s = d) )   
 ( r.ttest &lt;- raster.change(r1, r2, s = d, stat="t.test") )
 ( r.ent &lt;- raster.change(r1, r2, s = d, stat="entropy") )   
 ( r.cor &lt;- raster.change(r1, r2, s = d, stat="cor") )
 ( r.ce &lt;- raster.change(r1, r2, s = d, stat = "cross-entropy") )
 ( r.kl &lt;- raster.change(r1, r2, s = d, stat = "divergence") )	
       
   opar &lt;- par(no.readonly=TRUE)
   par(mfrow=c(3,2))
     plot(r.kappa, main="Kappa")
     plot(r.ttest[[1]], main="Paired t-test")
     plot(r.ent, main="Delta Entropy")
     plot(r.cor, main="Rank Correlation")
     plot(r.kl, main="Kullback-Leibler")
     plot(r.ce, main="cross-entropy")
   par(opar) 


</code></pre>

<hr>
<h2 id='raster.deviation'>Raster local deviation from the global trend</h2><span id='topic+raster.deviation'></span>

<h3>Description</h3>

<p>Calculates the local deviation from the raster, a specified global statistic 
or a polynomial trend of the raster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.deviation(
  x,
  type = c("trend", "min", "max", "mean", "median"),
  s = 3,
  degree = 1,
  global = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.deviation_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="raster.deviation_+3A_type">type</code></td>
<td>
<p>The global statistic to represent the local deviation
options are: &quot;trend&quot;, &quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;median&quot;</p>
</td></tr>
<tr><td><code id="raster.deviation_+3A_s">s</code></td>
<td>
<p>Size of matrix (focal window), not used with type=&quot;trend&quot;</p>
</td></tr>
<tr><td><code id="raster.deviation_+3A_degree">degree</code></td>
<td>
<p>The polynomial degree if type is trend, default is 1st order.</p>
</td></tr>
<tr><td><code id="raster.deviation_+3A_global">global</code></td>
<td>
<p>Use single global value for deviation or cell-level values 
(FALSE/TRUE). Argument is ignored for type=&quot;trend&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The deviation from the trend is derived as [y-hat - y] where; y-hat is the 
Nth-order polynomial. Whereas the deviation from a global statistic is [y - y-hat] 
where; y-hat is the local (focal) statistic. The global = TRUE argument allows 
one to evaluate the local deviation from the global statistic [stat(x) - y-hat] 
where; stat(x) is the global value of the specified statistic and y-hat is the 
specified focal statistic.
</p>


<h3>Value</h3>

<p>A SpatRaster class object representing local deviation from the raster or the 
specified global statistic
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Magee, Lonnie (1998). Nonlocal Behavior in Polynomial Regressions. The American 
Statistician. American Statistical Association. 52(1):20-22
</p>
<p>Fan, J. (1996). Local Polynomial Modelling and Its Applications: From linear  
regression to nonlinear regression. Monographs on Statistics and Applied 
Probability. Chapman and  Hall/CRC. ISBN 0-412-98321-4
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))

# local deviation from first-order trend, global mean and raw value
r.dev.trend &lt;- raster.deviation(elev, type="trend", degree=1) 
r.dev.mean &lt;- raster.deviation(elev, type="mean", s=5)
r.gdev.mean &lt;- raster.deviation(elev, type="mean", s=5, global=TRUE)

opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
  plot(elev, main="original")
  plot(r.dev.trend, main="dev from trend")
  plot(r.dev.mean, main="dev of mean from raw values")
  plot(r.gdev.mean, main="local dev from global mean")
par(opar)

</code></pre>

<hr>
<h2 id='raster.downscale'>Raster Downscale</h2><span id='topic+raster.downscale'></span>

<h3>Description</h3>

<p>Downscales a raster to a higher resolution raster using
a robust regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.downscale(
  x,
  y,
  scatter = FALSE,
  full.res = FALSE,
  residuals = FALSE,
  se = FALSE,
  p = 0.95,
  uncertainty = c("none", "prediction", "confidence")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.downscale_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object representing independent variable(s)</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_y">y</code></td>
<td>
<p>A terra SpatRaster object representing dependent variable</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_scatter">scatter</code></td>
<td>
<p>(FALSE/TRUE) Optional scatter plot</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_full.res">full.res</code></td>
<td>
<p>(FALSE/TRUE) Use full resolution of x (see notes)</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_residuals">residuals</code></td>
<td>
<p>(FALSE/TRUE) Output raster residual error raster,
at same resolution as y</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_se">se</code></td>
<td>
<p>(FALSE/TRUE) Output standard error raster, using prediction or
confidence interval</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_p">p</code></td>
<td>
<p>The confidence/prediction interval (default is 95%)</p>
</td></tr>
<tr><td><code id="raster.downscale_+3A_uncertainty">uncertainty</code></td>
<td>
<p>Output uncertainty raster(s) of confidence or prediction interval,
at same resolution as y. Options are c(&quot;none&quot;, &quot;prediction&quot;, &quot;confidence&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses a robust regression, fit using an M-estimation with Tukey's biweight
initialized by a specific S-estimator, to downscale a raster based on higher-resolution
or more detailed raster data specified as covariate(s). You can optionally output residual
error, standard error and/or uncertainty rasters. However, please note that when choosing
the type of uncertainty, using a confidence interval (uncertainty around the mean predictions)
when you should be using the prediction interval (uncertainty around a single values) will
greatly underestimate the uncertainty in a given predicted value (Bruce &amp; Bruce 2017).
The full.res = TRUE option uses the x data to sample y rather than y to sample x. THis makes
the problem much more computationally and memory extensive and should be used with caution.
There is also the question of pseudo-replication (sample redundancy) in the dependent variable.
Statistically speaking one would expect to capture the sample variation of x by sampling at the
frequency of y thus supporting the downscaling estimate. Note that if uncertainty is not defined
the prediction interval for standard error defaults to &quot;confidence&quot; else is the same output as
uncertainty (eg., prediction or confidence).
</p>


<h3>Value</h3>

<p>A list object containing:
</p>

<ul>
<li><p> downscale - downscaled terra SpatRaster object
</p>
</li>
<li><p> model - MASS rlm model object
</p>
</li>
<li><p> MSE - Mean Square Error
</p>
</li>
<li><p> AIC - Akaike information criterion
</p>
</li>
<li><p> parm.ci - Parameter confidence intervals
</p>
</li>
<li><p> residuals - If residuals = TRUE, a SpatRaster of the residual error
</p>
</li>
<li><p> uncertainty - If pred.int = TRUE, SpatRaster's of the lower/upper prediction intervals
</p>
</li>
<li><p> std.error - If se = TRUE, SpatRaster's of the standard error
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Bruce, P., &amp; A. Bruce. (2017). Practical Statistics for Data Scientists. O’Reilly Media.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require(geodata, quietly = TRUE)) {
library(terra)
library(geodata)

# Download example data (requires geodata package)
elev &lt;- elevation_30s(country="SWZ", path=tempdir())
slp &lt;- terrain(elev, v="slope")
tmax &lt;- worldclim_country(country="SWZ", var="tmax", path=tempdir())
  tmax &lt;- crop(tmax[[1]], ext(elev))

# Downscale temperature
x=c(elev,slp)
  names(x) &lt;- c("elev","slope")
y=tmax
  names(y) &lt;- c("tmax")

tmax.ds &lt;- raster.downscale(x, y, scatter=TRUE, residuals = TRUE,
                            uncertainty = "prediction", se = TRUE)
	
  # plot prediction and parameters	
  opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,2))
      plot(tmax, main="Temp max")
      plot(x[[1]], main="elevation")
      plot(x[[2]], main="slope")
      plot(tmax.ds$downscale, main="Downscaled Temp max")
  par(opar)

  # Plot residual error and raw prediction +/- intervals
  opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,2))
      plot(tmax.ds$std.error, main="Standard Error")
      plot(tmax.ds$residuals, main="residuals")
      plot(tmax.ds$uncertainty[[1]], 
	       main="lower prediction interval")
      plot(tmax.ds$uncertainty[[2]], 
	       main="upper prediction interval")
  par(opar)
  
  # plot prediction uncertainty
  opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,1))
      plot(tmax.ds$downscale - tmax.ds$uncertainty[[1]], 
	       main="lower prediction interval")
      plot(tmax.ds$downscale - tmax.ds$uncertainty[[2]], 
	       main="upper prediction interval")  
  par(opar)  

} else { 
  cat("Please install geodata package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='raster.entropy'>Raster Entropy</h2><span id='topic+raster.entropy'></span>

<h3>Description</h3>

<p>Calculates entropy on integer raster (i.e., 8 bit 0-255)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.entropy(x, d = 5, categorical = FALSE, global = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.entropy_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object (requires integer raster)</p>
</td></tr>
<tr><td><code id="raster.entropy_+3A_d">d</code></td>
<td>
<p>Size of matrix (window)</p>
</td></tr>
<tr><td><code id="raster.entropy_+3A_categorical">categorical</code></td>
<td>
<p>Is the data categorical or continuous (FALSE/TRUE)</p>
</td></tr>
<tr><td><code id="raster.entropy_+3A_global">global</code></td>
<td>
<p>Should the model use a global or local n to calculate 
entropy (FALSE/TRUE)</p>
</td></tr>
<tr><td><code id="raster.entropy_+3A_...">...</code></td>
<td>
<p>Optional arguments passed terra focal function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Entropy calculated as: H = -sum(Pi*ln(Pi)) where; Pi, Proportion of one value 
to total values Pi=n(p)/m and m, Number of unique values. Expected range: 
0 to log(m) H=0 if window contains the same value in all cells.
H increases with the number of different values in the window. The ellipsis
arguments can be used to write to disk using the filename argument.
</p>


<h3>Value</h3>

<p>terra SpatRaster class object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Fuchs M., R. Hoffmann, F. Schwonke (2008) Change Detection with GRASS 
GIS - Comparison of images taken by different sensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
  r &lt;- rast(ncols=100, nrows=100)
    r[] &lt;- round(runif(ncell(r), 1,8), digits=0)

rEnt &lt;- raster.entropy(r, d=5, categorical = TRUE, global = TRUE)
  opar &lt;- par(no.readonly=TRUE)
    par(mfcol=c(2,1))
      plot(r)
        plot(rEnt)
  par(opar)

# Maximum entropy is reached when all values are different, same as log(m)
#   for example; log( length( unique(x) ) ) 

</code></pre>

<hr>
<h2 id='raster.gaussian.smooth'>Gaussian smoothing of raster</h2><span id='topic+raster.gaussian.smooth'></span>

<h3>Description</h3>

<p>Applies a Gaussian smoothing kernel to smooth raster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.gaussian.smooth(
  x,
  s = 2,
  n = 5,
  scale = FALSE,
  type = c("mean", "median", "sd", "convolution"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.gaussian.smooth_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster raster object</p>
</td></tr>
<tr><td><code id="raster.gaussian.smooth_+3A_s">s</code></td>
<td>
<p>Standard deviation (sigma) of kernel (default is 2)</p>
</td></tr>
<tr><td><code id="raster.gaussian.smooth_+3A_n">n</code></td>
<td>
<p>Size of the focal matrix, single value (default is 
5 for 5x5 window)</p>
</td></tr>
<tr><td><code id="raster.gaussian.smooth_+3A_scale">scale</code></td>
<td>
<p>(FALSE/TRUE) Scale sigma to the resolution of the raster</p>
</td></tr>
<tr><td><code id="raster.gaussian.smooth_+3A_type">type</code></td>
<td>
<p>The statistic to use in the smoothing operator; 
&quot;mean&quot;, &quot;median&quot;, &quot;sd&quot;, &quot;convolution&quot;</p>
</td></tr>
<tr><td><code id="raster.gaussian.smooth_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::focal</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This applies a Gaussian Kernel smoother. The convolution option performs
a Gaussian decomposition whereas the other options use the kernel
as weights for the given statistic.
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object of the local distributional moment
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))

# Calculate Gaussian smoothing with sigma = 2 and 7x7 window
g1 &lt;- raster.gaussian.smooth(elev, s=2, n=7)
    plot(c(elev,g1))

</code></pre>

<hr>
<h2 id='raster.invert'>Invert raster</h2><span id='topic+raster.invert'></span>

<h3>Description</h3>

<p>Inverts (flip) the values of a raster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.invert(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.invert_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Inverts raster values using the formula: (((x - max(x)) * -1) + min(x)
</p>


<h3>Value</h3>

<p>A terra SpatRaster object with inverted (flipped) raster values
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
r &lt;- rast(nrows=500, ncols=500, xmin=571823, xmax=616763, 
            ymin=4423540, ymax=4453690)
 crs(r) &lt;- "epsg:9001"
r[] &lt;- runif(ncell(r), 1000, 2500)
r &lt;- focal(r, focalMat(r, 150, "Gauss") )

r.inv &lt;- raster.invert(r)

opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(1,2))
      plot(r, main="original raster")
      plot(r.inv, main="inverted raster") 
par(opar)   

</code></pre>

<hr>
<h2 id='raster.kendall'>Kendall tau trend with continuity correction for raster time-series</h2><span id='topic+raster.kendall'></span>

<h3>Description</h3>

<p>Calculates a nonparametric statistic for a monotonic trend
based on the Kendall tau statistic and the Theil-Sen slope
modification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.kendall(
  x,
  intercept = TRUE,
  p.value = TRUE,
  confidence = TRUE,
  tau = TRUE,
  min.obs = 6,
  method = c("zhang", "yuepilon", "none"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.kendall_+3A_x">x</code></td>
<td>
<p>A multiband terra SpatRaster object with at least 5 layers</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_intercept">intercept</code></td>
<td>
<p>(FALSE/TRUE) return a raster with the pixel
wise intercept values</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_p.value">p.value</code></td>
<td>
<p>(FALSE/TRUE) return a raster with the pixel
wise p.values</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_confidence">confidence</code></td>
<td>
<p>(FALSE/TRUE) return a raster with the pixel
wise 95 pct confidence levels</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_tau">tau</code></td>
<td>
<p>(FALSE/TRUE) return a raster with the pixel wise
tau correlation values</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_min.obs">min.obs</code></td>
<td>
<p>The threshold of minimum number of observations (default 6)</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_method">method</code></td>
<td>
<p>Kendall method to use c(&quot;zhang&quot;, &quot;yuepilon&quot;,&quot;none&quot;), see kendall function</p>
</td></tr>
<tr><td><code id="raster.kendall_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the terra app function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements Kendall's nonparametric test for a monotonic trend
using the Theil-Sen (Theil 1950; Sen 1968; Siegel 1982) method to estimate
the slope and related confidence intervals.
</p>


<h3>Value</h3>

<p>Depending on arguments, a raster layer or rasterBrick object containing:
</p>

<ul>
<li><p> raster layer 1 - slope for trend, always returned
</p>
</li>
<li><p> raster layer 2 - Kendall's tau two-sided test, reject null at 0, if tau TRUE
</p>
</li>
<li><p> raster layer 3 - intercept for trend if intercept TRUE
</p>
</li>
<li><p> raster layer 4 - p value for trend fit if p.value TRUE
</p>
</li>
<li><p> raster layer 5 - lower confidence level at 95 pct, if confidence TRUE
</p>
</li>
<li><p> raster layer 6 - upper confidence level at 95 pct, if confidence TRUE
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Theil, H. (1950) A rank invariant method for linear and polynomial regression
analysis.  Nederl. Akad. Wetensch. Proc. Ser. A 53:386-392 (Part I),
53:521-525 (Part II), 53:1397-1412 (Part III).
</p>
<p>Sen, P.K. (1968) Estimates of Regression Coefficient Based on Kendall's tau.
Journal of the American Statistical Association. 63(324):1379-1389.
</p>
<p>Siegel, A.F. (1982) Robust Regression Using Repeated Medians.
Biometrika, 69(1):242-244
</p>


<h3>See Also</h3>

<p><code><a href="zyp.html#topic+zyp.trend.vector">zyp.trend.vector</a></code> for model details
</p>
<p><code><a href="terra.html#topic+app">app</a></code> for available ... arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(terra)

 # note; nonsense example with n=9
 r &lt;- c(rast(system.file("ex/logo.tif", package="terra")),
        rast(system.file("ex/logo.tif", package="terra")),
        rast(system.file("ex/logo.tif", package="terra"))) 
 
 # Calculate trend slope with p-value and confidence level(s)
 # ("slope","intercept", "p.value","z.value", "LCI","UCI","tau")
   k &lt;- raster.kendall(r, method="none")
   plot(k)


</code></pre>

<hr>
<h2 id='raster.mds'>Raster multidimensional scaling (MDS)</h2><span id='topic+raster.mds'></span>

<h3>Description</h3>

<p>Multidimensional scaling of raster values within an N x N focal window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.mds(r, s = 5, window.median = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.mds_+3A_r">r</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="raster.mds_+3A_s">s</code></td>
<td>
<p>Window size (may be a vector of 1 or 2) of 
n x n dimension.</p>
</td></tr>
<tr><td><code id="raster.mds_+3A_window.median">window.median</code></td>
<td>
<p>(FALSE/TRUE) Return the median of the MDS 
matrix values.</p>
</td></tr>
<tr><td><code id="raster.mds_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::focal</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An MDS focal function. If only one value provided for s, then a square matrix 
(window) will be used. If window.median = FALSE then the center value of the 
matrix is returned and not the median of the matrix
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Quinn, G.P., &amp; M.J. Keough (2002) Experimental design and data analysis 
for biologists. Cambridge University Press. Ch. 18. Multidimensional 
scaling and cluster analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
 library(terra)
 r &lt;- rast(system.file("ex/elev.tif", package="terra"))
   r &lt;- r[[1]] / max(global(r, "max", na.rm=TRUE)[,1])
 
 diss &lt;- raster.mds(r)
 diss.med &lt;- raster.mds(r, window.median = TRUE)

opar &lt;- par(no.readonly=TRUE)
   par(mfrow=c(2,2))
   plot(r)
     title("Elevation")
   plot( focal(r, w = matrix(1, nrow=5, ncol=5), fun = var) )
     title("Variance")		 
     plot(diss)
       title("MDS")
     plot(diss.med)
       title("Median MDS")
par(opar)


</code></pre>

<hr>
<h2 id='raster.modified.ttest'>Dutilleul moving window bivariate raster correlation</h2><span id='topic+raster.modified.ttest'></span>

<h3>Description</h3>

<p>A bivarate raster correlation using Dutilleul's
modified t-test
</p>
<p>This function provides a bivariate moving window correlation using the modified
t-test to account for spatial autocorrelation. Point based subsampling is provided
for computation tractability. The hexagon sampling is recommended as it it good
at capturing spatial process that includes nonstationarity and anistropy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.modified.ttest(
  x,
  y,
  d = "auto",
  sample = c("none", "random", "hexagonal", "regular"),
  p = 0.1,
  size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.modified.ttest_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="raster.modified.ttest_+3A_y">y</code></td>
<td>
<p>A terra SpatRaster class object, same dimensions as x</p>
</td></tr>
<tr><td><code id="raster.modified.ttest_+3A_d">d</code></td>
<td>
<p>Distance for finding neighbors</p>
</td></tr>
<tr><td><code id="raster.modified.ttest_+3A_sample">sample</code></td>
<td>
<p>Apply sub-sampling options; c(&quot;none&quot;, &quot;random&quot;, &quot;hexagonal&quot;, &quot;regular&quot;)</p>
</td></tr>
<tr><td><code id="raster.modified.ttest_+3A_p">p</code></td>
<td>
<p>If sample != &quot;none&quot;, what proportion of population
should be sampled</p>
</td></tr>
<tr><td><code id="raster.modified.ttest_+3A_size">size</code></td>
<td>
<p>Fixed sample size (default NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A terra SpatRaster or sf POINT class object with the following attributes:
</p>

<ul>
<li><p> corr - Correlation
</p>
</li>
<li><p> Fstat - The F-statistic calculated as degrees of freedom unscaled F-statistic
</p>
</li>
<li><p> p.value - p-value for the test
</p>
</li>
<li><p> moran.x - Moran's-I for x
</p>
</li>
<li><p> moran.y - Moran's-I for y
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Clifford, P., S. Richardson, D. Hemon (1989), Assessing the significance of the
correlationbetween two spatial processes. Biometrics 45:123-134.
</p>
<p>Dutilleul, P. (1993), Modifying the t test for assessing the correlation between
two spatial processes. Biometrics 49:305-314.
</p>


<h3>See Also</h3>

<p><code><a href="SpatialPack.html#topic+modified.ttest">modified.ttest</a></code> for test details
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 p = c("sf", "sp", "terra", "gstat")
 if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
   m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
   message("Can't run examples, please install ", paste(p[m], collapse = " "))
 } else {
   invisible(lapply(p, require, character.only=TRUE))

data(meuse, package = "sp")
meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                  agr = "constant") 
data(meuse.grid, package = "sp")                                      
meuse.grid &lt;- st_as_sf(meuse.grid, coords = c("x", "y"), crs = 28992, 
                  agr = "constant") 

ref &lt;- rast(ext(meuse.grid), resolution = 40)
  crs(ref) &lt;- crs(meuse)
e &lt;- ext(179407.8, 181087.9, 331134.4, 332332.1)
                                       
# GRID-1 log(copper): 
v1 &lt;- variogram(log(copper) ~ 1, meuse) 
  x1 &lt;- fit.variogram(v1, vgm(1, "Sph", 800, 1))           
  G1 &lt;- krige(zinc ~ 1, meuse, meuse.grid, x1, nmax = 30)
G1 &lt;- crop(rasterize(G1, ref, "var1.pred"),e)
names(G1) &lt;- "copper"
 
 # GRID-2 log(elev):
v2 &lt;- variogram(log(elev) ~ 1, meuse) 
  x2 &lt;- fit.variogram(v2, vgm(1, "Sph", 800, 1))           
  G2 &lt;- krige(zinc ~ 1, meuse, meuse.grid, x2, nmax = 30)
G2 &lt;- crop(rasterize(G2, ref, "var1.pred"),e)
names(G2) &lt;- "elev"

# Raster corrected correlation 
acor &lt;- raster.modified.ttest(G1, G2)
  plot(acor)
 
# Sample-based corrected correlation
( cor.hex &lt;- raster.modified.ttest(G1, G2, sample = "hexagonal") )	 
  plot(cor.hex["corr"], pch=20)
}

</code></pre>

<hr>
<h2 id='raster.moments'>Raster moments</h2><span id='topic+raster.moments'></span>

<h3>Description</h3>

<p>Calculates focal statistical moments of a raster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.moments(x, type = "mean", s = 3, p = 0.75, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.moments_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="raster.moments_+3A_type">type</code></td>
<td>
<p>The global statistic to represent the local deviation
options are: &quot;min&quot;, &quot;min&quot;, &quot;mean&quot;, &quot;median&quot;, &quot;var, &quot;sd&quot;, 
&quot;mad&quot;, &quot;kurt&quot;, &quot;skew&quot;, &quot;quantile&quot;</p>
</td></tr>
<tr><td><code id="raster.moments_+3A_s">s</code></td>
<td>
<p>Size of matrix (focal window), can be single value or two 
values defining the [x,y] dimensions of the focal matrix</p>
</td></tr>
<tr><td><code id="raster.moments_+3A_p">p</code></td>
<td>
<p>if type=&quot;quantile&quot;, the returned percentile.</p>
</td></tr>
<tr><td><code id="raster.moments_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::focal</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple wrapper for the terra focal function, returning local statistical moments
</p>


<h3>Value</h3>

<p>A terra SpatRaster object representing the local distributional moment
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
r &lt;- rast(nrows=500, ncols=500, xmin=571823, xmax=616763, 
            ymin=4423540, ymax=4453690)
  crs(r) &lt;- "epsg:9001"
r[] &lt;- runif(ncell(r), 1000, 2500)

# Calculate 10th percentile for 3x3 window
r.p10 &lt;- raster.moments(r, type="quantile", p=0.10) 


</code></pre>

<hr>
<h2 id='raster.transformation'>Statistical transformation for rasters</h2><span id='topic+raster.transformation'></span>

<h3>Description</h3>

<p>Transforms raster to a specified statistical transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.transformation(x, trans = "norm", smin = 0, smax = 255)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.transformation_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="raster.transformation_+3A_trans">trans</code></td>
<td>
<p>Transformation method: &quot;norm&quot;, &quot;rstd&quot;, &quot;std&quot;, &quot;stretch&quot;,
&quot;nl&quot;, &quot;slog&quot;, &quot;sr&quot; (please see notes)</p>
</td></tr>
<tr><td><code id="raster.transformation_+3A_smin">smin</code></td>
<td>
<p>Minimum value for stretch</p>
</td></tr>
<tr><td><code id="raster.transformation_+3A_smax">smax</code></td>
<td>
<p>Maximum value for stretch</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Transformation option details:
</p>

<ul>
<li><p> norm - (Normalization_ (0-1): if min(x) &lt; 0 ( x - min(x) ) / ( max(x) - min(x) )
</p>
</li>
<li><p> rstd - (Row standardize) (0-1): if min(x) &gt;= 0 x / max(x) This normalizes data
</p>
</li>
<li> 
<div class="sourceCode"><pre>   with negative distributions
</pre></div>
</li>
<li><p> std - (Standardize) (x - mean(x)) / sdv(x)
</p>
</li>
<li><p> stretch - (Stretch) ((x - min(x)) * max.stretch / (max(x) - min(x)) + min.stretch)
This will stretch values to the specified minimum and maximum values
(eg., 0-255 for 8-bit)
</p>
</li>
<li><p> nl - (Natural logarithms) if min(x) &gt; 0 log(x)
</p>
</li>
<li><p> slog - (Signed log 10) (for skewed data): if min(x) &gt;= 0 ifelse(abs(x) &lt;= 1, 0,
sign(x)*log10(abs(x)))
</p>
</li>
<li><p> sr - (Square-root) if min(x) &gt;= 0 sqrt(x)
</p>
</li></ul>



<h3>Value</h3>

<p>A terra SpatRaster class object of specified transformation
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
r &lt;- rast(nrows=500, ncols=500, xmin=571823, xmax=616763, 
            ymin=4423540, ymax=4453690)
  crs(r) &lt;- "epsg:9001"
r[] &lt;- runif(ncell(r), 1000, 2500)

 # Positive values so, can apply any transformation    
 for( i in c("norm", "rstd", "std", "stretch", "nl", "slog", "sr")) {
   print( raster.transformation(r, trans = i) ) 
 }

 # Negative values so, can't transform using "nl", "slog" or "sr"
 r[] &lt;- runif(ncell(r), -1, 1)
   for( i in c("norm", "rstd", "std", "stretch", "nl", "slog", "sr")) {
  try( print( raster.transformation(r, trans = i) ) ) 
   }


</code></pre>

<hr>
<h2 id='raster.vol'>Raster Percent Volume</h2><span id='topic+raster.vol'></span>

<h3>Description</h3>

<p>Calculates a percent volume on a raster or based on a 
systematic sample
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.vol(
  x,
  p = 0.75,
  sample = FALSE,
  spct = 0.05,
  type = c("random", "regular")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.vol_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="raster.vol_+3A_p">p</code></td>
<td>
<p>percent raster-value volume</p>
</td></tr>
<tr><td><code id="raster.vol_+3A_sample">sample</code></td>
<td>
<p>(FALSE/TRUE) base volume on systematic point sample</p>
</td></tr>
<tr><td><code id="raster.vol_+3A_spct">spct</code></td>
<td>
<p>sample percent, if sample (TRUE)</p>
</td></tr>
<tr><td><code id="raster.vol_+3A_type">type</code></td>
<td>
<p>If sample=TRUE type of sample, options are &quot;random&quot; or &quot;regular&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>if sample (FALSE) binary raster object with 1 representing designated 
percent volume else, if sample (TRUE) n sf POINT object with points 
that represent the percent volume of the sub-sample
</p>


<h3>Note</h3>

<p>Since this model needs to operate on all of the raster values, it is not memory safe
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
  r &lt;- rast(ncols=100, nrows=100)
    r[] &lt;- runif(ncell(r), 0, 1)
    r &lt;- focal(r, w=focalMat(r, 6, "Gauss"))
  #r[sample(1:ncell(r)),10] &lt;- NA
  
  # full raster percent volume 
  p30 &lt;- raster.vol(r, p=0.30)
  p50 &lt;- raster.vol(r, p=0.50)
  p80 &lt;- raster.vol(r, p=0.80)

opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,2))
    plot(r, col=cm.colors(10), main="original raster")
    plot(p30, breaks=c(0,0.1,1), col=c("cyan","red"), legend=FALSE,
      main="30% volume")
    plot(p50, breaks=c(0,0.1,1), col=c("cyan","red"), legend=FALSE,
      main="50% volume")
    plot(p80, breaks=c(0,0.1,1), col=c("cyan","red"), legend=FALSE,
      main="80% volume")
par(opar)  


</code></pre>

<hr>
<h2 id='raster.Zscore'>Modified z-score for a raster</h2><span id='topic+raster.Zscore'></span>

<h3>Description</h3>

<p>Calculates the modified z-score for raster values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>raster.Zscore(x, p.value = FALSE, file.name = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="raster.Zscore_+3A_x">x</code></td>
<td>
<p>A raster class object</p>
</td></tr>
<tr><td><code id="raster.Zscore_+3A_p.value">p.value</code></td>
<td>
<p>Return p-value rather than z-score 
raster (FALSE/TRUE)</p>
</td></tr>
<tr><td><code id="raster.Zscore_+3A_file.name">file.name</code></td>
<td>
<p>Name of raster written to disk</p>
</td></tr>
<tr><td><code id="raster.Zscore_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to writeRaster</p>
</td></tr>
</table>


<h3>Value</h3>

<p>raster class object or raster written to disk
</p>


<h3>Note</h3>

<p>Since this functions needs to operate on all of the raster values, 
it is not memory safe
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
r &lt;- rast(nrows=500, ncols=500)
  r[] &lt;- runif(ncell(r), 0, 1)

# Modified z-score
( z &lt;- raster.Zscore(r) )

# P-value
( p &lt;- raster.Zscore(r, p.value = TRUE) )
	

</code></pre>

<hr>
<h2 id='rasterCorrelation'>Raster correlation</h2><span id='topic+rasterCorrelation'></span>

<h3>Description</h3>

<p>Performs a moving window correlation between two rasters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasterCorrelation(x, y, s = 3, type = "pearson")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasterCorrelation_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object for x</p>
</td></tr>
<tr><td><code id="rasterCorrelation_+3A_y">y</code></td>
<td>
<p>A terra SpatRasterclass object for y</p>
</td></tr>
<tr><td><code id="rasterCorrelation_+3A_s">s</code></td>
<td>
<p>Scale of window. Can be a single value, two 
values for uneven window or a custom matrix. 
Must be odd number (eg., s=3, for 3x3 window or 
s=c(3,5) for 3 x 5 window)</p>
</td></tr>
<tr><td><code id="rasterCorrelation_+3A_type">type</code></td>
<td>
<p>Type of output, options are: &quot;pearson&quot;, &quot;spearman&quot;, 
&quot;covariance&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A terra SpatRaster class object
</p>


<h3>Note</h3>

<p>The NA behavior is set to na.rm = TRUE to make default outputs
consistent between the terra and raster packages.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
 
r &lt;- rast(system.file("ex/logo.tif", package="terra"))  
  x &lt;- r[[1]]
  y &lt;- r[[3]]
  
 r.cor &lt;- rasterCorrelation(x, y, s = 5, type = "spearman")
   plot(r.cor)


</code></pre>

<hr>
<h2 id='rasterDistance'>Raster Distance</h2><span id='topic+rasterDistance'></span>

<h3>Description</h3>

<p>Calculates the Euclidean distance of a defined raster class and
all the other cells in a taster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasterDistance(x, y, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasterDistance_+3A_x">x</code></td>
<td>
<p>A terra SpatRast or sf class object</p>
</td></tr>
<tr><td><code id="rasterDistance_+3A_y">y</code></td>
<td>
<p>Value(s) in x to to calculate distance to</p>
</td></tr>
<tr><td><code id="rasterDistance_+3A_scale">scale</code></td>
<td>
<p>(FALSE/TRUE) Perform a row standardization on results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This replicates the terra distance function but uses the Arya &amp; Mount
Approximate Near Neighbor (ANN) C++ library for calculating distances. Where this
results in a notable increase in performance it is not memory safe, needing to read
in the entire raster and does not use the GeographicLib (Karney, 2013) spheroid 
distance method for geographic data.
</p>


<h3>Value</h3>

<p>A terra SpatRast raster representing distances
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Arya S., Mount D. M., Netanyahu N. S., Silverman R. and Wu A. Y (1998), An 
optimal algorithm for approximate nearest neighbor searching, Journal of 
the ACM, 45, 891-923.
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+distance">distance</a>, <a href="terra.html#topic+distance">distance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
library(terra)

# read, project and subset 10 polygons
nc &lt;- suppressWarnings(st_cast(st_read(system.file("shape/nc.shp", 
         package="sf")), "POLYGON"))
  nc &lt;- st_transform(nc, st_crs("ESRI:102008"))
    nc.sub &lt;- nc[sample(1:nrow(nc),10),]

# create 1000m reference raster, rasterize subset polygons
ref &lt;- rast(ext(nc), resolution=1000)
  rnc &lt;- mask(rasterize(vect(nc.sub), field="CNTY_ID",
              ref, background=9999), vect(nc)) 
    crs(rnc) &lt;- "ESRI:102008"  
  
# Calculate distance to class 1 in rnc raster, plot results
ids &lt;- nc.sub$CNTY_ID 
rd &lt;- rasterDistance(rnc, y=ids) 
  plot(rd)
    plot( st_geometry(nc.sub), add=TRUE)


</code></pre>

<hr>
<h2 id='remove_duplicates'>Remove duplicate geometries</h2><span id='topic+remove_duplicates'></span>

<h3>Description</h3>

<p>Removes duplicate geometries in a single-part feature class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_duplicates(x, threshold = 0.00001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_duplicates_+3A_x">x</code></td>
<td>
<p>An sf POINT, POLYGON or LINESTRING object</p>
</td></tr>
<tr><td><code id="remove_duplicates_+3A_threshold">threshold</code></td>
<td>
<p>A distance threshold indicating fuzzy duplication,
default i 0.00001</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function removes duplicate geometries based on order and not &quot;non null&quot; 
attribution or other factors, the first feature gets to stay. If one needs to 
know which points were removed sf::st_difference can be used between original 
data and results of the function.
</p>


<h3>Value</h3>

<p>sf object, of same feature class as x, with duplicate geometries removed
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)

# data with 10 duplicate obs
s &lt;- data.frame(x = runif(100), y = runif(100))
  s &lt;- data.frame(rbind(s, s[sample(1:nrow(s), 10),]) ) 
    s &lt;- st_as_sf(s, coords = c("x", "y"))
      s$ID &lt;- 1:nrow(s)

nrow(s) 
nrow( srmd &lt;- remove_duplicates(s) )

</code></pre>

<hr>
<h2 id='remove.holes'>Remove or return polygon holes</h2><span id='topic+remove.holes'></span>

<h3>Description</h3>

<p>Removes or returns all holes (null geometry) in sf polygon 
class objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove.holes(x, only.holes = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove.holes_+3A_x">x</code></td>
<td>
<p>sf POLYGON or MULTIPOLYGON object</p>
</td></tr>
<tr><td><code id="remove.holes_+3A_only.holes">only.holes</code></td>
<td>
<p>Delete holes (FALSE) or returns only holes (FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A hole is considered a polygon within a polygon (island) representing null 
geometry. If you want to return only holes, no longer NULL, use keep = TRUE. 
To delete holes use default only.holes = FALSE. Single part features will be 
returned regardless of input.
</p>


<h3>Value</h3>

<p>sf POLYGON object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)

p &lt;- sf::st_as_sf(sf::st_sfc(
  sf::st_polygon(list(
  cbind(c(2,4,4,1,2),c(2,3,5,4,2)),
  cbind(c(2.33, 2.05, 3.25, 3.25, 2.33), 
      c(3.00, 3.56, 3.95, 3.46, 3.00)))),
  sf::st_polygon(list(	
    cbind(c(5,4,2,5),c(2,3,2,2)))),
  sf::st_polygon(list(  
    cbind(c(4,4,5,10,4),c(5,3,2,5,5)),
    cbind(c(5,6,6,5,5),c(4,4,3,3,4)) 
))))
  p$ID &lt;- 1:3

rh &lt;- remove.holes(p)
kh &lt;- remove.holes(p, only.holes=TRUE)

opar &lt;- par(no.readonly=TRUE)
   par(mfrow=c(2,2))
     plot(st_geometry(p), main="Original with holes")
     plot(st_geometry(rh), main="holes removed only.holes=FALSE")
	 plot(st_geometry(kh), main="return holes only.holes=TRUE")
par(opar)

</code></pre>

<hr>
<h2 id='rm.ext'>Remove extension</h2><span id='topic+rm.ext'></span>

<h3>Description</h3>

<p>Removes file extension (and path) from string
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.ext(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rm.ext_+3A_x">x</code></td>
<td>
<p>A character vector representing a file with extension</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The file name with extension and file path stripped off
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rm.ext("C:/path/file.txt")

</code></pre>

<hr>
<h2 id='rotate.polygon'>Rotate polygon</h2><span id='topic+rotate.polygon'></span>

<h3>Description</h3>

<p>rotates polygon by specified angle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotate.polygon(
  p,
  angle = 45,
  sp = FALSE,
  anchor = c("center", "lower.left", "upper.right")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rotate.polygon_+3A_p">p</code></td>
<td>
<p>A polygon object of sf or sp class</p>
</td></tr>
<tr><td><code id="rotate.polygon_+3A_angle">angle</code></td>
<td>
<p>Rotation angle in degrees</p>
</td></tr>
<tr><td><code id="rotate.polygon_+3A_sp">sp</code></td>
<td>
<p>(FALSE | TRUE) Output sp class object</p>
</td></tr>
<tr><td><code id="rotate.polygon_+3A_anchor">anchor</code></td>
<td>
<p>Location to rotate polygon on options are &quot;center&quot;, 
&quot;lower.left&quot; and &quot;upper.right&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The anchor is the location that the rotation is anchored to. The center
is the centroid where the lower.left and upper.right are based on the 
min or max of the coordinates respectively.
</p>


<h3>Value</h3>

<p>an sp or sf polygon object with rotated polygon
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
 
data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), 
                    crs = 28992, agr = "constant")
 
 e &lt;- st_convex_hull(st_union(meuse))
   e30 &lt;- rotate.polygon(e, angle=30)
 
 plot(e, main="rotated 30 degrees")
   plot(e30, add=TRUE)
 
</code></pre>

<hr>
<h2 id='sa.trans'>Trigonometric  transformation of a slope and aspect interaction</h2><span id='topic+sa.trans'></span>

<h3>Description</h3>

<p>The Trigonometric  Stage (1978) 
[slope * cos(aspect)] or [slope * sin(aspect)]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sa.trans(
  slope,
  aspect,
  type = "cos",
  slp.units = "degrees",
  asp.units = "degrees"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sa.trans_+3A_slope">slope</code></td>
<td>
<p>slope values in degrees, radians or percent</p>
</td></tr>
<tr><td><code id="sa.trans_+3A_aspect">aspect</code></td>
<td>
<p>aspect values in degrees or radians</p>
</td></tr>
<tr><td><code id="sa.trans_+3A_type">type</code></td>
<td>
<p>Type of transformation, options are: &quot;cos&quot;, &quot;sin&quot;</p>
</td></tr>
<tr><td><code id="sa.trans_+3A_slp.units">slp.units</code></td>
<td>
<p>Units of slope values, options are: &quot;degrees&quot;, 
&quot;radians&quot; or &quot;percent&quot;</p>
</td></tr>
<tr><td><code id="sa.trans_+3A_asp.units">asp.units</code></td>
<td>
<p>Units of aspect values, options are: 
&quot;degrees&quot; or &quot;radians&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An a priori assumption of a maximum in the NW quadrant (45 azimuth)
and a minimum in the SW quadrant can be replaced by an empirically
determined location of the optimum without repeated calculations of
the regression fit. In addition it is argued that expressions for
the effects of aspect should always be considered as terms involving
an interaction with slope (Stage, 1976)
</p>
<p>For slopes from 0
bounded from -1 to 1. Greater than 100
out of the -1 to 1 range.
</p>
<p>An alternative for slopes with values approaching infinity is
to take the square root of slope/100 to reduce the range of
values.By default this model test all values greater than 100
to 101
</p>


<h3>Value</h3>

<p>A vector of the modeled value
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Stage, A. R. 1976. An Expression of the Effects of Aspect, Slope, 
and Habitat Type on Tree Growth. Forest Science 22(3):457-460.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
sa.trans(slope = 48.146, aspect = 360.000)

# Example of creating slope*cos(aspect) raster
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
sa &lt;- terra::terrain(elev, v=c("slope", "aspect"), unit="degrees")
scosa &lt;- terra::lapp(c(sa[[1]], sa[[2]]), fun = sa.trans)

</code></pre>

<hr>
<h2 id='sample.annulus'>Sample annulus</h2><span id='topic+sample.annulus'></span>

<h3>Description</h3>

<p>Creates sample points based on annulus with defined 
inner and outer radius
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.annulus(x, r1, r2, size = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.annulus_+3A_x">x</code></td>
<td>
<p>An sf POINT class object</p>
</td></tr>
<tr><td><code id="sample.annulus_+3A_r1">r1</code></td>
<td>
<p>Numeric value defining inner radius of annulus 
(in projection units)</p>
</td></tr>
<tr><td><code id="sample.annulus_+3A_r2">r2</code></td>
<td>
<p>Numeric value defining outer radius of annulus 
(in projection units)</p>
</td></tr>
<tr><td><code id="sample.annulus_+3A_size">size</code></td>
<td>
<p>Number of samples</p>
</td></tr>
<tr><td><code id="sample.annulus_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to sf::st_sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function can be used for distance based sampling which is a sampling method 
that can be used to capture spatially lagged variation.
</p>


<h3>Value</h3>

<p>sf POINTS object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> library(sf)
 if(require(sp, quietly = TRUE)) {
   data(meuse, package = "sp")
   meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                     agr = "constant")

 xy &lt;- meuse[2,]
 rs100 &lt;- sample.annulus(xy, r1=50, r2=100, size = 50)
 rs200 &lt;- sample.annulus(xy, r1=100, r2=200, size = 50)
 
 plot(st_geometry(rs200), pch=20, col="red")
   plot(st_geometry(rs100), pch=20, col="blue", add=TRUE)
   plot(st_geometry(xy), pch=20, cex=2, col="black", add=TRUE)
 legend("topright", legend=c("50-100m", "100-200m", "source"), 
        pch=c(20,20,20), col=c("blue","red","black"))


# Run on multiple points
rs100 &lt;- sample.annulus(meuse[1:3,], r1=50, r2=100, 
                        size = 50)
rs200 &lt;- sample.annulus(meuse[1:3,], r1=50, r2=200, 
                        size = 50)
plot(st_geometry(rs200), pch=20, col="red")
  plot(st_geometry(rs100), pch=20, col="blue", add=TRUE)
    plot(st_geometry(meuse[1:3,]), pch=20, cex=2, col="black", add=TRUE)
 legend("topright", legend=c("50-100m", "100-200m", "source"), 
        pch=c(20,20,20), col=c("blue","red","black"))

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='sampleTransect'>Sample transect</h2><span id='topic+sampleTransect'></span>

<h3>Description</h3>

<p>Creates random transects from points and generates 
sample points along each transect
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleTransect(
  x,
  min.dist,
  max.dist,
  distance = NULL,
  azimuth = NULL,
  id = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleTransect_+3A_x">x</code></td>
<td>
<p>A sf point object</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_min.dist">min.dist</code></td>
<td>
<p>Minimum length of transect(s)</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_max.dist">max.dist</code></td>
<td>
<p>Maximum length of transect(s)</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_distance">distance</code></td>
<td>
<p>A vector of distances, same length as x, used
to define transect distances (length)</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_azimuth">azimuth</code></td>
<td>
<p>A vector of azimuths, same length as x, used
to define transect direction</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_id">id</code></td>
<td>
<p>A unique identification column in x</p>
</td></tr>
<tr><td><code id="sampleTransect_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to st_sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function create lines and samples using random or defined direction 
and length transects and then creates a point sample along each transect. 
The characteristic of the sample points are defined by arguments passed 
to the sf::st_sample function. The distance and azimuth arguments allow
for specifying the exact length and direction for each points transect.
</p>


<h3>Value</h3>

<p>A list object contaning sf LINES and POINTS objects representing random transects 
and sample points along each transect. The &quot;ID&quot; column links the resulting data.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(sp, quietly = TRUE)) {
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")
meuse &lt;- meuse[sample(1:nrow(meuse),10),]

transects &lt;- sampleTransect(meuse, min.dist=200, max.dist=500, 
                            type="regular", size=20)
   plot(st_geometry(transects$transects))
     plot(st_geometry(meuse), pch=19, cex=2, add=TRUE)
       plot(st_geometry(transects$samples), 
	        col="red", pch=19, add=TRUE)

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='sar'>Surface Area Ratio</h2><span id='topic+sar'></span>

<h3>Description</h3>

<p>Calculates the Berry (2002) Surface Area Ratio based on slope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sar(x, s = NULL, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sar_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="sar_+3A_s">s</code></td>
<td>
<p>cell resolution (default is NULL and not needed if projection 
is in planar units)</p>
</td></tr>
<tr><td><code id="sar_+3A_scale">scale</code></td>
<td>
<p>(TRUE/FALSE) Scale (row standardize) results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SAR is calculated as: resolution^2 * cos( (degrees(slope) * (pi / 180)) )
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object of the Surface Area Ratio
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Berry, J.K. (2002). Use surface area for realistic calculations. Geoworld 15(9):20-1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> library(terra)
 elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
 ( surface.ratio &lt;- sar(elev) )
   plot(surface.ratio)
    
</code></pre>

<hr>
<h2 id='separability'>separability</h2><span id='topic+separability'></span>

<h3>Description</h3>

<p>Calculates variety of two-class sample separability metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>separability(
  x,
  y,
  plot = FALSE,
  cols = c("red", "blue"),
  clabs = c("Class1", "Class2"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="separability_+3A_x">x</code></td>
<td>
<p>X vector</p>
</td></tr>
<tr><td><code id="separability_+3A_y">y</code></td>
<td>
<p>Y vector</p>
</td></tr>
<tr><td><code id="separability_+3A_plot">plot</code></td>
<td>
<p>plot separability (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="separability_+3A_cols">cols</code></td>
<td>
<p>colors for plot (must be equal to number of classes)</p>
</td></tr>
<tr><td><code id="separability_+3A_clabs">clabs</code></td>
<td>
<p>labels for two classes</p>
</td></tr>
<tr><td><code id="separability_+3A_...">...</code></td>
<td>
<p>additional arguments passes to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available statistics:
</p>

<ul>
<li><p> M-Statistic (Kaufman &amp; Remer 1994) - This is a measure of the difference of the
distributional peaks. A large M-statistic indicates good separation between the
two classes as within-class variance is minimized and between-class variance
maximized (M &lt;1 poor, M &gt;1 good).
</p>
</li>
<li><p> Bhattacharyya distance (Bhattacharyya 1943; Harold 2003) - Measures the similarity
of two discrete or continuous probability distributions.
</p>
</li>
<li><p> Jeffries-Matusita distance (Bruzzone et al., 2005; Swain et al., 1971) - The J-M
distance is a function of separability that directly relates to the probability of
how good a resultant classification will be. The J-M distance is asymptotic to v2,
where values of v2 suggest complete separability
</p>
</li>
<li><p> Divergence and transformed Divergence (Du et al., 2004) - Maximum likelihood approach.
Transformed divergence gives an exponentially decreasing weight to increasing distances
between the classes.
</p>
</li></ul>



<h3>Value</h3>

<p>A data.frame with the following separability metrics:
</p>

<ul>
<li><p> B - Bhattacharryya distance statistic
</p>
</li>
<li><p> JM - Jeffries-Matusita distance statistic
</p>
</li>
<li><p> M - M-Statistic
</p>
</li>
<li><p> D - Divergence index
</p>
</li>
<li><p> TD - Transformed Divergence index
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Anderson, M. J., &amp; Clements, A. (2000) Resolving environmental disputes: a
statistical method for choosing among competing cluster models. Ecological
Applications 10(5):1341-1355
</p>
<p>Bhattacharyya, A. (1943) On a measure of divergence between two statistical
populations defined by their probability distributions'. Bulletin of the
Calcutta Mathematical Society 35:99-109
</p>
<p>Bruzzone, L., F. Roli, S.B. Serpico (1995) An extension to multiclass cases of
the Jefferys-Matusita distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence 33:1318-1321
</p>
<p>Du, H., C.I. Chang, H. Ren, F.M. D'Amico, J. O. Jensen, J., (2004) New
Hyperspectral Discrimination Measure for Spectral Characterization. Optical
Engineering 43(8):1777-1786.
</p>
<p>Kailath, T., (1967) The Divergence and Bhattacharyya measures in signal
selection. IEEE Transactions on Communication Theory 15:52-60
</p>
<p>Kaufman Y., and L. Remer (1994) Detection of forests using mid-IR reflectance:
An application for aerosol studies. IEEE T. Geosci.Remote. 32(3):672-683.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   norm1 &lt;- dnorm(seq(-20,20,length=5000),mean=0,sd=1) 
   norm2 &lt;- dnorm(seq(-20,20,length=5000),mean=0.2,sd=2)                          
     separability(norm1, norm2) 
           
   s1 &lt;- c(1362,1411,1457,1735,1621,1621,1791,1863,1863,1838)
   s2 &lt;- c(1362,1411,1457,10030,1621,1621,1791,1863,1863,1838)
     separability(s1, s2, plot=TRUE) 
      
</code></pre>

<hr>
<h2 id='sf_dissolve'>Dissolve polygons</h2><span id='topic+sf_dissolve'></span>

<h3>Description</h3>

<p>Dissolve polygon feature calss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sf_dissolve(x, y = NULL, overlaps = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sf_dissolve_+3A_x">x</code></td>
<td>
<p>An sf POLYGON or MULTIPOLYGON object</p>
</td></tr>
<tr><td><code id="sf_dissolve_+3A_y">y</code></td>
<td>
<p>An attribute in x to dissolve by, default is NULL</p>
</td></tr>
<tr><td><code id="sf_dissolve_+3A_overlaps">overlaps</code></td>
<td>
<p>(FALSE/TRUE) Dissolve overlapping polygons, negates using attribute</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a dissolve attribute is defined, the result will be a 
MULTIPOLYGON with the grouping attribute column. If y=NULL all polygons
will be dissolved into a single attribute, unless there is spatial
discontinuity (eg., gaps) in the data. The intent of overlaps=TRUE is to 
provide functionality for dissolving overlapping polygons and should only
be used in this specialized case.
</p>


<h3>Value</h3>

<p>A dissolved POLYGON or MULTIPOLYGON object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
nc &lt;- st_read(system.file("shape/nc.shp", package="sf"))
  nc$group &lt;- ifelse(nc$CNTY_ &lt;= 1902, 1, 
                ifelse(nc$CNTY_ &gt; 1902 &amp; nc$CNTY_ &lt;= 1982, 2, 
				     ifelse(nc$CNTY_ &gt; 1982, 3, NA))) 

# Dissolve by group attribute
d &lt;- sf_dissolve(nc, "group")
  plot(st_geometry(nc), border="grey")
    plot(st_geometry(d), border="red", col=NA, 
         lwd=2, add=TRUE) 

# Dissolve all polygons
d &lt;- sf_dissolve(nc)
  plot(st_geometry(nc), border="grey")
    plot(st_geometry(d), border="red", col=NA, 
         lwd=2, add=TRUE)

# Dissolve overlapping polygons
sq &lt;- function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), 
     c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), 
	 c(pt - sz))))
pol &lt;- st_sf(box = 1:6, st_sfc(sq(c(4.2,4.2)), sq(c(0,0)), sq(c(1, -0.8)), 
          sq(c(0.5, 1.7)), sq(c(3,3)), sq(c(-3, -3))))
 st_geometry(pol) &lt;- "geometry" 		  

plot(pol)

d &lt;- sf_dissolve(pol, overlaps=TRUE)
  plot(d["diss"])

</code></pre>

<hr>
<h2 id='sf.kde'>Spatial kernel density estimate</h2><span id='topic+sf.kde'></span><span id='topic+sp.kde'></span>

<h3>Description</h3>

<p>A weighted or unweighted Gaussian Kernel Density estimate 
for point spatial data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sf.kde(
  x,
  y = NULL,
  bw = NULL,
  ref = NULL,
  res = NULL,
  standardize = FALSE,
  scale.factor = 10000,
  mask = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sf.kde_+3A_x">x</code></td>
<td>
<p>sf POINT object</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_y">y</code></td>
<td>
<p>Optional values, associated with x coordinates, 
to be used as weights</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_bw">bw</code></td>
<td>
<p>Distance bandwidth of Gaussian Kernel, must be units 
of projection</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_ref">ref</code></td>
<td>
<p>A terra SpatRaster, sf class object or c[xmin,xmax,ymin,ymax] 
vector to estimate the kde extent</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_res">res</code></td>
<td>
<p>Resolution of raster when ref not SpatRaster</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_standardize">standardize</code></td>
<td>
<p>Standardize results to 0-1 (FALSE/TRUE)</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_scale.factor">scale.factor</code></td>
<td>
<p>Numeric scaling factor for the KDE (defaults to 10000),
to account for very small estimate values</p>
</td></tr>
<tr><td><code id="sf.kde_+3A_mask">mask</code></td>
<td>
<p>(TRUE/FALSE) mask resulting raster if ref is provided
as a SpatRaster</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please note that ks methods for estimation has been reverted to the Gussian method proposed
in Venables &amp; Ripley (2002). There was not enought evendence that the Chacon &amp; Duong (2018)
multivariate method(s) for bandwidth selection and kernal estimation were suitable for 
spatial random fields.
</p>


<h3>Value</h3>

<p>a terra SpatRaster class object containing kernel density estimate
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Duong, T. &amp; Hazelton, M.L. (2005) Cross-validation bandwidth matrices for multivariate 
kernel density estimation. Scandinavian Journal of Statistics, 32, 485-506. 
</p>
<p>Wand, M.P. &amp; Jones, M.C. (1994) Multivariate plug-in bandwidth selection. Computational 
Statistics, 9, 97-116. 
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. 
Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library(sf) 
library(terra) 
  
data(meuse, package = "sp")
meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                  agr = "constant") 
  			
# Unweighted KDE (spatial locations only) with 40m resoultion  				
pt.kde &lt;- sf.kde(x = meuse, bw = 1000, standardize = TRUE, res=40)
  plot(pt.kde, main="Unweighted kde")
    plot(st_geometry(meuse), pch=20, col="red", add=TRUE) 

# cadmium weighted KDE usign extent with 40m resoultion and 500m and 1000m bw 
cadmium.kde.500 &lt;- sf.kde(x = meuse, y = meuse$cadmium, res=40, 
                          bw = 500, standardize = TRUE)
cadmium.kde.1000 &lt;- sf.kde(x = meuse, y = meuse$cadmium, res=40, 
                          bw = 1000, standardize = TRUE)						  
  plot(c(cadmium.kde.500, cadmium.kde.1000))
  			  			


</code></pre>

<hr>
<h2 id='sg.smooth'>Savitzky-Golay smoothing filter</h2><span id='topic+sg.smooth'></span>

<h3>Description</h3>

<p>Smoothing of time-series data using Savitzky-Golay 
convolution smoothing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sg.smooth(x, f = 4, l = 51, d = 1, na.rm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sg.smooth_+3A_x">x</code></td>
<td>
<p>A vector to be smoothed</p>
</td></tr>
<tr><td><code id="sg.smooth_+3A_f">f</code></td>
<td>
<p>Filter type (default 4 for quartic, specify 2 for quadratic)</p>
</td></tr>
<tr><td><code id="sg.smooth_+3A_l">l</code></td>
<td>
<p>Convolution filter length, must be odd number (default 51). 
Defines degree of smoothing</p>
</td></tr>
<tr><td><code id="sg.smooth_+3A_d">d</code></td>
<td>
<p>First derivative (default 1)</p>
</td></tr>
<tr><td><code id="sg.smooth_+3A_na.rm">na.rm</code></td>
<td>
<p>NA behavior</p>
</td></tr>
<tr><td><code id="sg.smooth_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the smoothed data equal to length of x. Please note; NA values are retained
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>References</h3>

<p>Savitzky, A., and Golay, M.J.E. (1964). Smoothing and Differentiation of Data 
by Simplified Least Squares Procedures. Analytical Chemistry. 36(8):1627-39
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(0.112220988, 0.055554941, 0.013333187, 0.055554941, 0.063332640, 0.014444285, 
       0.015555384, 0.057777140, 0.059999339, 0.034444068, 0.058888242, 0.136665165, 
       0.038888458, 0.096665606,0.141109571, 0.015555384, 0.012222088, 0.012222088, 
       0.072221428, 0.052221648, 0.087776810,0.014444285, 0.033332966, 0.012222088, 
       0.032221869, 0.059999339, 0.011110989, 0.011110989,0.042221759, 0.029999670, 
       0.018888680, 0.098887801, 0.016666483, 0.031110767, 0.061110441,0.022221979, 
       0.073332526, 0.012222088, 0.016666483, 0.012222088, 0.122220881, 0.134442955, 
       0.094443403, 0.128887475, 0.045555055, 0.152220547, 0.071110331, 0.018888680,
       0.022221979, 0.029999670, 0.035555165, 0.014444285, 0.049999449, 0.074443623, 
       0.068888135, 0.062221535, 0.032221869, 0.095554501, 0.143331751, 0.121109776,
       0.065554835, 0.074443623, 0.043332856, 0.017777583, 0.016666483, 0.036666263, 
       0.152220547, 0.032221869, 0.009999890, 0.009999890, 0.021110879, 0.025555275,
       0.099998899, 0.015555384, 0.086665712, 0.008888791, 0.062221535, 0.044443958, 
       0.081110224, 0.015555384, 0.089999005, 0.082221314, 0.056666043, 0.013333187,
       0.048888352, 0.075554721, 0.025555275, 0.056666043, 0.146665052, 0.118887581, 
       0.125554174, 0.024444176, 0.124443069, 0.012222088, 0.126665279, 0.048888352,
       0.046666153, 0.141109571, 0.015555384, 0.114443190)

plot(y, type="l", lty = 3, main="Savitzky-Golay with l = 51, 25, 10")
  lines(sg.smooth(y),col="red", lwd=2)
  lines(sg.smooth(y, l = 25),col="blue", lwd=2)
  lines(sg.smooth(y, l = 10),col="green", lwd=2)
  
#### function applied to a multi-band raster 
library(terra)
( r &lt;- spatialEco::random.raster(n.layers=20) )

# raster stack example
( r.sg &lt;- app(r, sg.smooth) )  
 
</code></pre>

<hr>
<h2 id='shannons'>Shannon's Diversity (Entropy) Index</h2><span id='topic+shannons'></span>

<h3>Description</h3>

<p>Calculates Shannon's Diversity Index and Shannon's Evenness Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shannons(x, counts = TRUE, ens = FALSE, margin = "row")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shannons_+3A_x">x</code></td>
<td>
<p>data.frame object containing counts or proportions</p>
</td></tr>
<tr><td><code id="shannons_+3A_counts">counts</code></td>
<td>
<p>Are data counts (TRUE) or relative proportions (FALSE)</p>
</td></tr>
<tr><td><code id="shannons_+3A_ens">ens</code></td>
<td>
<p>Calculate effective number of species (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="shannons_+3A_margin">margin</code></td>
<td>
<p>Calculate diversity for rows or columns. c(&quot;row&quot;, &quot;col&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The expected for H is 0-3+ where a value of 2 has been suggested as medium-high diversity, 
for evenness is 0-1 with 0 signifying no evenness and 1, complete evenness.
</p>


<h3>Value</h3>

<p>data.frame with &quot;H&quot; (Shannon's diversity) and &quot;evenness&quot; (Shannon's 
evenness where H / max( sum(x) ) ) and ESN
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Shannon, C. E. and W. Weaver (1948) A mathematical theory of communication. The Bell 
System Technical Journal, 27:379-423.
</p>
<p>Simpson, E. H. (1949) Measurement of diversity. Nature 163:688
</p>
<p>Roth, D. S., I. Perfecto, and B. Rathcke (1994) The effects of management systems on 
ground-foraging ant diversity in Costa Rica. Ecological Applications 4(3):423-436.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Using Costa Rican ant diversity data from Roth et al. (1994)
data(ants)
  
# Calculate diversity for each covertype ("col") 
shannons(ants[,2:ncol(ants)], ens = TRUE, counts = FALSE, margin = "col")

# Calculate diversity for each species ("row") 
ant.div &lt;- shannons(ants[,2:ncol(ants)], ens = TRUE, counts = FALSE, 
                    margin = "row")
  row.names(ant.div) &lt;- ants[,1]
  ant.div

</code></pre>

<hr>
<h2 id='shift'>shift</h2><span id='topic+shift'></span>

<h3>Description</h3>

<p>Shift a vector by specified positive or negative lag
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shift(x, lag = 1, pad = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shift_+3A_x">x</code></td>
<td>
<p>A vector</p>
</td></tr>
<tr><td><code id="shift_+3A_lag">lag</code></td>
<td>
<p>Number of lagged offsets, default is 1</p>
</td></tr>
<tr><td><code id="shift_+3A_pad">pad</code></td>
<td>
<p>Value to fill the lagged offset with, default is NA</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector, length equal to x, with offset length filled with pad values
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:10

shift(x, 1)     # shift positive (from beginning of vector) by 1
shift(x, -1)    # shift negative (from end of vector) by 1
shift(x, 5, 0)  # Shift by 5 and fill (pad) with 0

</code></pre>

<hr>
<h2 id='sieve'>Sieve raster data</h2><span id='topic+sieve'></span>

<h3>Description</h3>

<p>Removes contiguous cells &lt; specified query area
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve(x, a, unit = c("m", "km", "ha"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve_+3A_x">x</code></td>
<td>
<p>An integer terra SpatRaster</p>
</td></tr>
<tr><td><code id="sieve_+3A_a">a</code></td>
<td>
<p>Query area to remove</p>
</td></tr>
<tr><td><code id="sieve_+3A_unit">unit</code></td>
<td>
<p>The unit to use for area query options are c(&quot;m&quot;, &quot;km&quot;, &quot;ha&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sieve can be used to establish a minimal mapping unit where
contiguous cells &lt; specified query area are set to NA. These NA
values can then be filled using focal (majority, median, mean)
</p>


<h3>Value</h3>

<p>A terra SpatRaster with cells &lt; a set to NA
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
m &lt;- matrix(c(100,200,1,200,300,2,300,400,3,400,
       500,4, 500,600,5), ncol=3, byrow=TRUE)
  x &lt;- classify(elev, m)

# Sieve to a MMU of 60km
sv &lt;- spatialEco::sieve(x, a = 60, unit = "km")
  plot(c(x, sv))
 
</code></pre>

<hr>
<h2 id='similarity'>Ecological similarity</h2><span id='topic+similarity'></span>

<h3>Description</h3>

<p>Uses row imputation to identify &quot;k&quot; ecological similar observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>similarity(
  x,
  k = 4,
  method = "mahalanobis",
  frequency = TRUE,
  scale = TRUE,
  ID = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="similarity_+3A_x">x</code></td>
<td>
<p>data.frame containing ecological measures</p>
</td></tr>
<tr><td><code id="similarity_+3A_k">k</code></td>
<td>
<p>Number of k nearest neighbors (kNN)</p>
</td></tr>
<tr><td><code id="similarity_+3A_method">method</code></td>
<td>
<p>Method to compute multivariate distances c(&quot;mahalanobis&quot;, &quot;raw&quot;, 
&quot;euclidean&quot;, &quot;ica&quot;)</p>
</td></tr>
<tr><td><code id="similarity_+3A_frequency">frequency</code></td>
<td>
<p>Calculate frequency of each reference row (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="similarity_+3A_scale">scale</code></td>
<td>
<p>Scale multivariate distances to standard range (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="similarity_+3A_id">ID</code></td>
<td>
<p>Unique ID vector to use as reference ID's (rownames). Must be 
unique and same length as number of rows in x</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses row-based imputation to identify k similar neighbors for each 
observation. Has been used to identify offsets based on ecological similarity.
</p>


<h3>Value</h3>

<p>data.frame with k similar targets and associated distances. If frequency = TRUE  the 
freq column represents the number of times a row (ID) was selected as a neighbor.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Evans, J.S., S.R. Schill, G.T. Raber (2015) A Systematic Framework for Spatial 
Conservation Planning and Ecological Priority Design in St. Lucia, Eastern 
Caribbean. Chapter 26 in Central American Biodiversity : Conservation, Ecology 
and a Sustainable Future. F. Huettman (eds). Springer, NY.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
 library(sf)
 data(pu)
 kNN &lt;- similarity(st_drop_geometry(pu[2:ncol(pu)]), k = 4, 
                   frequency = TRUE, ID = pu$UNIT_ID)  
 p &lt;- kNN$freq   
 clr &lt;- c("#3288BD", "#99D594", "#E6F598", "#FEE08B", 
          "#FC8D59", "#D53E4F")   
 p &lt;- ifelse(p &lt;= 0, clr[1], 
        ifelse(p &gt; 0 &amp; p &lt; 10, clr[2],
          ifelse(p &gt;= 10 &amp; p &lt; 20, clr[3],
 	       ifelse(p &gt;= 20 &amp; p &lt; 50, clr[4],
 	         ifelse(p &gt;= 50 &amp; p &lt; 100, clr[5],
 	           ifelse(p &gt;= 100, clr[6], NA))))))
 plot(st_geometry(pu), col=p, border=NA)
   legend("topleft", legend=c("None","&lt;10","10-20",
          "20-50","50-100","&gt;100"),
          fill=clr, cex=0.6, bty="n") 
   box()

 
</code></pre>

<hr>
<h2 id='smooth.time.series'>Smooth Raster Time-series</h2><span id='topic+smooth.time.series'></span>

<h3>Description</h3>

<p>Smooths pixel-level data in raster time-series and can impute 
missing (NA) values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth.time.series(x, f = 0.8, smooth.data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.time.series_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster with &gt; 8 layers</p>
</td></tr>
<tr><td><code id="smooth.time.series_+3A_f">f</code></td>
<td>
<p>Smoothing parameter (see loess span argument)</p>
</td></tr>
<tr><td><code id="smooth.time.series_+3A_smooth.data">smooth.data</code></td>
<td>
<p>(FALSE/TRUE) Smooth all of the data or just impute NA values</p>
</td></tr>
<tr><td><code id="smooth.time.series_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::app (for 
writing results to disk)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses a LOESS regression to smooth the time-series. If the data is 
smoothed, (using the smooth.data = TRUE argument) it will be entirely replaced by  
a loess estimate of the time-series (estimated distribution at the pixel-level). 
Alternately, with smooth.data = FALSE, the function can be used to impute missing 
pixel data (NA) in raster time-series (stacks/bricks).
The results can dramatically be effected by the choice of the smoothing 
parameter (f) so caution is warranted and the effect of this parameter tested.
</p>


<h3>Value</h3>

<p>A terra SpatRaster containing imputed or smoothed data.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code> for details on the loess regression
</p>
<p><code><a href="terra.html#topic+app">app</a></code> for details on additional (...) arguments
</p>
<p><code><a href="#topic+impute.loess">impute.loess</a></code> for details on imputation model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
  random.raster &lt;- function(rows=50, cols=50, l=20, min=0, max=1){ 
    do.call(c, replicate(l, rast(matrix(runif(rows * cols, min, max), 
	        rows , cols))))
  }
r &lt;- random.raster()

#### Smooth time-series using raster stack/brick 
r.smooth &lt;- smooth.time.series(r, f = 0.4, smooth.data = TRUE)  

# extract pixel 100 for plotting
y &lt;- as.numeric(r[100])
ys &lt;- as.numeric(r.smooth[100])

# plot results	
plot(y, type="l")
  lines(ys, col="red")
    legend("bottomright", legend=c("original","smoothed"),
         lty=c(1,1), col=c("black","red"))	

</code></pre>

<hr>
<h2 id='sobal'>Sobel-Feldman operator</h2><span id='topic+sobal'></span>

<h3>Description</h3>

<p>An isotropic image gradient operator using a 3x3 window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobal(x, method = "intensity", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sobal_+3A_x">x</code></td>
<td>
<p>A raster class object</p>
</td></tr>
<tr><td><code id="sobal_+3A_method">method</code></td>
<td>
<p>Type of operator (&quot;intensity&quot;, &quot;direction&quot;, &quot;edge&quot;)</p>
</td></tr>
<tr><td><code id="sobal_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to raster::overlay or, 
if method=&quot;edge&quot;, raster::focal (if you want a file 
written to disk use filename = &quot;&quot; argument)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Sobel-Feldmanh operator is a discrete differentiation operator, deriving an 
approximation of the gradient of the intensity function. abrupt discontinuity 
in the gradient function represents edges, making this a common approach for edge 
detection. The Sobel-Feldman operator is based on convolving the image with a small, 
separable, and integer matrix in the horizontal and vertical directions. The operator 
uses two 3x3 kernels which are convolved with the original image to calculate 
approximations of the derivatives - one for horizontal changes, and one for vertical. 
Where x is defined here as increasing in the right-direction, and y as increasing in 
the down-direction. At each pixel in the raster, the resulting gradient can be combined 
to give the gradient intensity, using: SQRT( Gx^2 Gy^2 ). This can be expanded into the 
gradient direction using atan(Gx/Gy)
</p>


<h3>Value</h3>

<p>A raster class object or raster written to disk
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Sobel, I., &amp; G. Feldman, (1969) A 3x3 Isotropic Gradient Operator for Image Processing, 
presented at the Stanford Artificial Intelligence Project (SAIL).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)

r &lt;- rast(system.file("ex/logo.tif", package="terra"))  
  s.int &lt;- sobal(r[[1]])
  s.dir &lt;- sobal(r[[1]], method = "direction")
  s.edge &lt;- sobal(r[[1]], method = "edge")

opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
  plot(r[[1]])
  plot(s.int, main="intensity") 
  plot(s.dir, main="direction") 
  plot(s.edge, main="edge")
par(opar)   

</code></pre>

<hr>
<h2 id='spatial.select'>Spatial Select</h2><span id='topic+spatial.select'></span>

<h3>Description</h3>

<p>Performs a spatial select (feature subset) between a polygon(s) and other
feature class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatial.select(
  x,
  y = NULL,
  distance = NULL,
  predicate = c("intersection", "intersect", "contains", "covers", "touches",
    "proximity", "contingency"),
  neighbors = c("queen", "rook")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spatial.select_+3A_x">x</code></td>
<td>
<p>An sp or sf polygon(s) object that defines the spatial query</p>
</td></tr>
<tr><td><code id="spatial.select_+3A_y">y</code></td>
<td>
<p>A sp or sf feature class that will be subset by the query of x</p>
</td></tr>
<tr><td><code id="spatial.select_+3A_distance">distance</code></td>
<td>
<p>A proximity distance of features to select (within distance)</p>
</td></tr>
<tr><td><code id="spatial.select_+3A_predicate">predicate</code></td>
<td>
<p>Spatial predicate for intersection</p>
</td></tr>
<tr><td><code id="spatial.select_+3A_neighbors">neighbors</code></td>
<td>
<p>If predicate = &quot;contingency&quot; type of neighbors options are
c(&quot;queen&quot;, &quot;rook&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs a spatial select of features based on an overlay of a polygon (x),
which can represent multiple features, and a polygon, point or line feature
classes (y). User can specify a partial or complete intersection, using within
argument, or within a distance, using distance argument, predicated on the
query polygon. This function is similar to ArcGIS/Pro spatial select. Please note
that for point to point neighbor selections use the knn function.
Valid spatial predicates include: intersect, touches, covers, contains, proximity
and contingency.
See DE-9IM topology model for detailed information on following data predicates.
</p>

<ul>
<li><p> intersection  Create a spatial intersection between two features
</p>
</li>
<li><p> intersect     Boolean evaluation of features intersecting
</p>
</li>
<li><p> contains      Boolean evaluation of x containing y
</p>
</li>
<li><p> covers        Boolean evaluation of x covering y
</p>
</li>
<li><p> touches       Boolean evaluation of x touching y
</p>
</li>
<li><p> proximity     Evaluation of distance-based proximity of x to y (x and y can be the same)
</p>
</li>
<li><p> contingency   Evaluation of polygon contingency (eg., 1st, 2nd order)
</p>
</li></ul>



<h3>Value</h3>

<p>An sf object representing a subset of y based on the spatial query of x or,
if predicate = contingency a sparse matrix representing neighbor indexes
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="sf.html#topic+st_intersection">st_intersection</a></code> for details on intersection predicate
</p>
<p><code><a href="sf.html#topic+st_intersects">st_intersects</a></code> for details on intersect predicate
</p>
<p><code><a href="sf.html#topic+st_contains">st_contains</a></code> for details on contain predicate
</p>
<p><code><a href="sf.html#topic+st_covers">st_covers</a></code> for details on covers predicate
</p>
<p><code><a href="sf.html#topic+st_touches">st_touches</a></code> for details on touches predicate
</p>
<p><code><a href="sf.html#topic+st_is_within_distance">st_is_within_distance</a></code> for details on proximity predicate
</p>
<p><a href="https://en.wikipedia.org/wiki/DE-9IM">https://en.wikipedia.org/wiki/DE-9IM</a> for details on DE-9IM topology model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(sp, quietly = TRUE)) {
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")

spolys &lt;- hexagons(meuse, res=100)
  spolys$ID &lt;- 1:nrow(spolys)
    p &lt;- st_as_sf(st_sample(spolys, 500))
	  p$PTID &lt;- 1:nrow(p) 
	  sf::st_geometry(p) &lt;- "geometry"

  plot(st_geometry(spolys), main="all data")
    plot(st_geometry(p), pch=20, add=TRUE)
	
sub.int &lt;- spatial.select(p, spolys, predicate = "intersect")
  plot(st_geometry(sub.int), main="intersects")
    plot(st_geometry(p), pch=20, add=TRUE)	

sub.prox &lt;- spatial.select(p, spolys, distance=100, predicate = "proximity")	
  plot(st_geometry(sub.int), main="intersects")
    plot(st_geometry(p), pch=20, add=TRUE)

# For rook or queen polygon contingency 	
plot( spolys &lt;- sf::st_make_grid(sf::st_sfc(sf::st_point(c(0,0)), 
                 sf::st_point(c(3,3))), n = c(3,3)) )
  
spatial.select(x=spolys, predicate = "contingency")
spatial.select(spolys, predicate = "contingency", neighbors = "rook") 

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='spatialEcoNews'>spatialEco news</h2><span id='topic+spatialEcoNews'></span>

<h3>Description</h3>

<p>Displays release notes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatialEcoNews(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spatialEcoNews_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Shows package NEWS file
</p>

<hr>
<h2 id='spectral.separability'>spectral separability</h2><span id='topic+spectral.separability'></span>

<h3>Description</h3>

<p>Calculates spectral separability by class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spectral.separability(x, y, jeffries.matusita = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spectral.separability_+3A_x">x</code></td>
<td>
<p>data.frame, matrix or vector of spectral values must,
match classes defined in y</p>
</td></tr>
<tr><td><code id="spectral.separability_+3A_y">y</code></td>
<td>
<p>A vector or factor with grouping classes, must match
row wise values in x</p>
</td></tr>
<tr><td><code id="spectral.separability_+3A_jeffries.matusita">jeffries.matusita</code></td>
<td>
<p>(TRUE/FALSE) Return J-M distance (default) else Bhattacharyya</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available statistics:
</p>

<ul>
<li><p> Bhattacharyya distance (Bhattacharyya 1943; Harold 2003) measures the similarity
of two discrete or continuous probability distributions.
</p>
</li>
<li><p> Jeffries-Matusita (default) distance (Bruzzone et al., 2005; Swain et al., 1971)
is a scaled (0-2) version of Bhattacharyya. The J-M distance is asymptotic to 2,
where 2 suggest complete separability.
</p>
</li></ul>



<h3>Value</h3>

<p>A matrix of class-wise Jeffries-Matusita or Bhattacharyya distance separability values
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Bhattacharyya, A. (1943) On a measure of divergence between two statistical
populations defined by their probability distributions'. Bulletin of the
Calcutta Mathematical Society 35:99-109
</p>
<p>Bruzzone, L., F. Roli, S.B. Serpico (1995) An extension to multiclass cases of
the Jefferys-Matusita distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence 33:1318-1321
</p>
<p>Kailath, T., (1967) The Divergence and Bhattacharyya measures in signal
selection. IEEE Transactions on Communication Theory 15:52-60
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(MASS)                
# Create example data 
d &lt;- 6                 # Number of bands
n.class &lt;- 5           # Number of classes
n &lt;- rep(1000, 5)                
mu &lt;- round(matrix(rnorm(d*n.class, 128, 1), 
            ncol=n.class, byrow=TRUE), 0)

x &lt;- matrix(double(), ncol=d, nrow=0)
  classes &lt;- integer()
    for (i in 1:n.class) {
      f &lt;- svd(matrix(rnorm(d^2), ncol=d))
      sigma &lt;- t(f$v) %*% diag(rep(10, d)) %*% f$v
      x &lt;- rbind(x, mvrnorm(n[i], mu[, i], sigma))
      classes &lt;- c(classes, rep(i, n[i]))
    }
colnames(x) &lt;- paste0("band", 1:6)
classes &lt;- factor(classes, labels=c("water", "forest",
                  "shrub", "urban", "ag"))

# Separability for multi-band (multivariate) spectra 
spectral.separability(x, classes)

# Separability for single-band (univariate) spectra 
spectral.separability(x[,1], classes)

</code></pre>

<hr>
<h2 id='spherical.sd'>Spherical Variance or Standard Deviation of Surface</h2><span id='topic+spherical.sd'></span>

<h3>Description</h3>

<p>Derives the spherical standard deviation of a raster surface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spherical.sd(r, d, variance = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spherical.sd_+3A_r">r</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="spherical.sd_+3A_d">d</code></td>
<td>
<p>Size of focal window or a matrix to use in focal function</p>
</td></tr>
<tr><td><code id="spherical.sd_+3A_variance">variance</code></td>
<td>
<p>(FALSE|TRUE) Output spherical variance rather than standard deviation</p>
</td></tr>
<tr><td><code id="spherical.sd_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra:app (can write raster to disk here)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Surface variability using spherical variance/standard deviation. 
The variation can be assessed using the spherical standard deviation of the normal 
direction within a local neighborhood. This is found by expressing the normal 
directions on the surfaces cells in terms of their displacements in a Cartesian (x,y,z) 
coordinate system. Averaging the x-coordinates, y-coordinates, and z-coordinates 
separately gives a vector (xb, yb, zb) pointing in the direction of the average 
normal. This vector will be shorter when there is more variation of the normals and 
it will be longest&ndash;equal to unity&ndash;when there is no variation. Its squared length 
is (by the Pythagorean theorem) given by: R^2 = xb^2 + yb^2 + zb^2
where; x = cos(aspect) * sin(slope) and xb = nXn focal mean of x 
y = sin(aspect) * sin(slope) and  yb = nXn focal mean of y
z = cos(slope) and zb = nXn focal mean of z 	   
</p>
<p>The slope and aspect values are expected to be in radians. 
The value of (1 - R^2), which will lie between 0 and 1, is the spherical variance. 
and it's square root can be considered the spherical standard deviation.
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object of the spherical standard deviation
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans &lt;jeffrey_evans&lt;at&gt;tnc.org&gt;
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+app">app</a></code> for details on ... arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(terra)
 elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
 
 ssd &lt;- spherical.sd(elev, d=5)
 
 slope &lt;- terrain(elev, v='slope')
 aspect &lt;- terrain(elev, v='aspect')
 hill &lt;- shade(slope, aspect, 40, 270)
 plot(hill, col=grey(0:100/100), legend=FALSE, 
      main='terrain spherical standard deviation')
   plot(ssd, col=rainbow(25, alpha=0.35), add=TRUE)

 
</code></pre>

<hr>
<h2 id='squareBuffer'>Square buffer</h2><span id='topic+squareBuffer'></span>

<h3>Description</h3>

<p>Creates a square buffer of a feature class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squareBuffer(x, a, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squareBuffer_+3A_x">x</code></td>
<td>
<p>An sf object</p>
</td></tr>
<tr><td><code id="squareBuffer_+3A_a">a</code></td>
<td>
<p>Numeric single or vector indicating buffer distance(s)</p>
</td></tr>
<tr><td><code id="squareBuffer_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to st_buffer</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function creates a square buffer of feature class.
</p>


<h3>Value</h3>

<p>A single feature sf class polygon object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
xy &lt;- st_as_sf(data.frame(x = c(1,3,6,7),
               y = c(3,2,7,8), z = c(38,23,12,12),
               area = c(32,23,45,67)),
			      coords = c("x", "y"), 
			      agr = "constant") 

# With fixed buffer
sb &lt;- squareBuffer(xy, 32)
  plot(st_geometry(sb))
    plot(st_geometry(xy), pch=20, add=TRUE)
  
# With variable buffer
sb.var &lt;- squareBuffer(xy, xy$area)
  plot(st_geometry(sb.var))
    plot(st_geometry(xy), pch=20, add=TRUE)
  
</code></pre>

<hr>
<h2 id='srr'>Surface Relief Ratio</h2><span id='topic+srr'></span>

<h3>Description</h3>

<p>Calculates the Pike (1971) Surface Relief Ratio
</p>


<h3>Usage</h3>

<pre><code class='language-R'>srr(x, s = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="srr_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="srr_+3A_s">s</code></td>
<td>
<p>Focal window size</p>
</td></tr>
<tr><td><code id="srr_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::lapp</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Describes rugosity in continuous raster surface within a specified window. 
The implementation of SRR can be shown as: (mean(x) - min(x)) / (max(x) - min(x))
</p>


<h3>Value</h3>

<p>A terra SpatRaster object of Pike's (1971) Surface Relief Ratio
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  r.srr &lt;- srr(elev, s=5)
    plot(r.srr, main="Surface Relief Ratio") 
      
</code></pre>

<hr>
<h2 id='stratified.random'>Stratified random sample</h2><span id='topic+stratified.random'></span>

<h3>Description</h3>

<p>Creates a stratified random sample of an sp class object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stratified.random(x, strata, n = 10, reps = 1, replace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stratified.random_+3A_x">x</code></td>
<td>
<p>An sf class object</p>
</td></tr>
<tr><td><code id="stratified.random_+3A_strata">strata</code></td>
<td>
<p>Column in x with stratification factor</p>
</td></tr>
<tr><td><code id="stratified.random_+3A_n">n</code></td>
<td>
<p>Number of random samples</p>
</td></tr>
<tr><td><code id="stratified.random_+3A_reps">reps</code></td>
<td>
<p>Number of replicates per strata</p>
</td></tr>
<tr><td><code id="stratified.random_+3A_replace">replace</code></td>
<td>
<p>(TRUE/FALSE) Sampling with replacement</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If replace=FALSE features are removed from consideration in subsequent replicates.
Conversely, if replace=TRUE, a feature can be selected multiple times across 
replicates. Not applicable if rep=1.
</p>


<h3>Value</h3>

<p>An sf class object containing random samples
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Hudak, A.T., N.L. Crookston, J.S. Evans, M.J. Falkowski, A.M.S. Smith, P. Gessler 
and P. Morgan. (2006) Regression modelling and mapping of coniferous forest basal 
area and tree density from discrete-return lidar and multispectral satellite data. 
Canadian Journal of Remote Sensing 32: 126-138.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(sp, quietly = TRUE)) {
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")

# Create stratified variable using quartile breaks
x1 &lt;- cut(meuse$cadmium, summary(meuse$cadmium)[-4], 
          include.lowest=TRUE)
  levels(x1) &lt;- seq(1,nlevels(x1),1)
x2 &lt;- cut(meuse$lead, summary(meuse$lead)[-4], 
          include.lowest=TRUE)
  levels(x2) &lt;- seq(1,nlevels(x2),1) 
meuse$STRAT &lt;- paste(x1, x2, sep='.')

# Counts for each full strata (note; 2 strata have only 1 observation)
tapply(meuse$STRAT, meuse$STRAT, length)
   
# 2 replicates, 2 samples with replacement
ssample &lt;- stratified.random(meuse, strata='STRAT', n=2, reps=2, 
                             replace=TRUE)
  tapply(ssample$STRAT, ssample$STRAT, length)

# 2 replicates, 2 samples no replacement
ssample.nr &lt;- stratified.random(meuse, strata='STRAT', n=2, reps=2)
  tapply(ssample.nr$STRAT, ssample.nr$STRAT, length)

# n=1 and reps=10 for sequential numbering of samples 
ssample.ct &lt;- stratified.random(meuse, strata='STRAT', n=1, reps=10, 
                                replace=TRUE)
  tapply(ssample.ct$STRAT, ssample.ct$STRAT, length)

# Plot random samples colored by replacement
ssample$REP &lt;- factor(ssample$REP)
  plot(ssample['REP'], pch=20)

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='subsample.distance'>Distance-based subsampling</h2><span id='topic+subsample.distance'></span>

<h3>Description</h3>

<p>Draws a minimum, and optional maximum constrained, distance sub-sampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subsample.distance(x, size, d, d.max = NULL, replacement = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subsample.distance_+3A_x">x</code></td>
<td>
<p>A POLYGON or POINT sf object</p>
</td></tr>
<tr><td><code id="subsample.distance_+3A_size">size</code></td>
<td>
<p>Subsample size</p>
</td></tr>
<tr><td><code id="subsample.distance_+3A_d">d</code></td>
<td>
<p>Minimum sampling distance in meters</p>
</td></tr>
<tr><td><code id="subsample.distance_+3A_d.max">d.max</code></td>
<td>
<p>Maximum sampling distance in meters</p>
</td></tr>
<tr><td><code id="subsample.distance_+3A_replacement">replacement</code></td>
<td>
<p>(FALSE/TRUE) Subsample with replacement</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A subsampled POLYGON or POINT sf object
</p>


<h3>Note</h3>

<p>This function provides a distance constrained subsample of existing point 
or polygon data. Please note that units are in meters regardless of input
CRS projection units (including lat/long).
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>     

if(require(sp, quietly = TRUE)) {
library(sf)
  data(meuse, package = "sp")
  meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                    agr = "constant")

# Subsample with a 500m minimum sample spread 
sub.meuse &lt;- subsample.distance(meuse, size = 10, d = 500)  
  plot(st_geometry(meuse), pch=19, main="min dist = 500")
    plot(st_geometry(sub.meuse), pch=19, col="red", add=TRUE) 
 
# Check distances	
dm &lt;- st_distance(sub.meuse)
  diag(dm) &lt;- NA
cat("\n", "Min distance for subsample", min(dm, na.rm=TRUE), "\n")  
cat("Max distance for subsample", max(dm, na.rm=TRUE), "\n")  

} else { 
  cat("Please install sp package to run example", "\n")
}

</code></pre>

<hr>
<h2 id='summary.cross.cor'>Summary of spatial cross correlation</h2><span id='topic+summary.cross.cor'></span>

<h3>Description</h3>

<p>summary method for class &quot;cross.cor&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cross.cor'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.cross.cor_+3A_object">object</code></td>
<td>
<p>Object of class cross.cor</p>
</td></tr>
<tr><td><code id="summary.cross.cor_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When not simulated k=0, prints functions list object containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value of NULL to represent p values were derived from observed data (k=0)
</p>
</li>
<li><p> p - Probability based observations above/below confidence interval
</p>
</li>
<li><p> t.test - Probability based on t-test
</p>
</li></ul>

<p>When simulated (k&gt;0), prints functions list object containing:
</p>

<ul>
<li><p> I - Global autocorrelation statistic
</p>
</li>
<li><p> SCI - A data.frame with two columns representing the xy and yx autocorrelation
</p>
</li>
<li><p> nsim - value representing number of simulations
</p>
</li>
<li><p> global.p - p-value of global autocorrelation statistic
</p>
</li>
<li><p> local.p - Probability based simulated data using successful rejection of t-test
</p>
</li>
<li><p> range.p - Probability based on range of probabilities resulting from paired t-test
</p>
</li></ul>


<hr>
<h2 id='summary.effect.size'>Summarizing effect size</h2><span id='topic+summary.effect.size'></span>

<h3>Description</h3>

<p>Summary method for class &quot;effect.size&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'effect.size'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.effect.size_+3A_object">object</code></td>
<td>
<p>Object of class effect.size</p>
</td></tr>
<tr><td><code id="summary.effect.size_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints the output data.frame contaning; effect size with upper and lower confidence 
and, mean and sd by group
</p>

<hr>
<h2 id='summary.loess.boot'>Summarizing Loess bootstrap models</h2><span id='topic+summary.loess.boot'></span>

<h3>Description</h3>

<p>Summary method for class &quot;loess.boot&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loess.boot'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.loess.boot_+3A_object">object</code></td>
<td>
<p>Object of class loess.boot</p>
</td></tr>
<tr><td><code id="summary.loess.boot_+3A_...">...</code></td>
<td>
<p>Ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>same as print lowess.boot data.frame including;
</p>

<ul>
<li><p> nreps        Number of bootstrap replicates
</p>
</li>
<li><p> confidence   Confidence interval (region)
</p>
</li>
<li><p> span         alpha (span) parameter used loess fit
</p>
</li>
<li><p> degree       polynomial degree used in loess fit
</p>
</li>
<li><p> normalize    Normalized data (TRUE/FALSE)
</p>
</li>
<li><p> family       Family of statistic used in fit
</p>
</li>
<li><p> parametric   Parametric approximation (TRUE/FALSE)
</p>
</li>
<li><p> surface      Surface fit, see loess.control
</p>
</li>
<li><p> data         data.frame of x,y used in model
</p>
</li>
<li><p> fit          data.frame including:
</p>

<ol>
<li><p> x - Equally-spaced x index (see NOTES)
</p>
</li>
<li><p> y.fit - loess fit
</p>
</li>
<li><p> up.lim - Upper confidence interval
</p>
</li>
<li><p> low.lim - Lower confidence interval
</p>
</li>
<li><p> stddev - Standard deviation of loess fit at each x value
</p>
</li></ol>

</li></ul>


<hr>
<h2 id='swvi'>Senescence weighted Vegetation Index (swvi)</h2><span id='topic+swvi'></span>

<h3>Description</h3>

<p>Modified Soil-adjusted Vegetation Index (MSAVI) or Modified Triangular
Vegetation Index 2 (MTVI) weighted by the Normalized difference senescent
vegetation index (NDSVI)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swvi(
  red,
  nir,
  swir,
  green = NULL,
  mtvi = FALSE,
  senescence = 0,
  threshold = NULL,
  weight.factor = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="swvi_+3A_red">red</code></td>
<td>
<p>SpatRaster, Red band (0.636 - 0.673mm), landsat 5&amp;7 band 3, OLI
(landsat 8) band 4</p>
</td></tr>
<tr><td><code id="swvi_+3A_nir">nir</code></td>
<td>
<p>SpatRaster, Near infrared band (0.851 - 0.879mm) landsat 5&amp;7 band 4,
OLI (landsat 8) band 5</p>
</td></tr>
<tr><td><code id="swvi_+3A_swir">swir</code></td>
<td>
<p>SpatRaster, short-wave infrared band 1 (1.566 - 1.651mm), landsat 5&amp;7
band 5, OLI (landsat 8) band 6</p>
</td></tr>
<tr><td><code id="swvi_+3A_green">green</code></td>
<td>
<p>Green band if MTVI = TRUE</p>
</td></tr>
<tr><td><code id="swvi_+3A_mtvi">mtvi</code></td>
<td>
<p>(FALSE | TRUE) Use Modified Triangular Vegetation Index 2
instead of MSAVI</p>
</td></tr>
<tr><td><code id="swvi_+3A_senescence">senescence</code></td>
<td>
<p>The critical value, in NDSVI, representing senescent vegetation</p>
</td></tr>
<tr><td><code id="swvi_+3A_threshold">threshold</code></td>
<td>
<p>Threshold value for defining NA based on &lt; p</p>
</td></tr>
<tr><td><code id="swvi_+3A_weight.factor">weight.factor</code></td>
<td>
<p>Apply partial weights (w * weight.factor) to the NDSVI weights</p>
</td></tr>
<tr><td><code id="swvi_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::lapp function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The intent of this index is to correct the MSAVI or MTVI index for bias associated
with senescent vegetation. This is done by:
1 deriving the NDSVI
2 applying a threshold to limit NDSVI to values associated with senescent vegetation
3 converting the index to inverted weights (-1*(NDSVI/sum(NDSVI)))
4 applying weights to MSAVI or MTVI
</p>
<p>The MSAVI formula follows the modification proposed by Qi et al. (1994),
often referred to as MSAVI2. MSAVI index reduces soil noise and increases
the dynamic range of the vegetation signal. The implemented modified version
(MSAVI2) is based on an inductive method that does not use a constant L value, in
separating soil effects, an highlights healthy vegetation. The MTVI(2) index follows
Haboudane et al., (2004) and represents the area of a hypothetical triangle in spectral
space that connects (1) green peak reflectance, (2) minimum chlorophyll absorption, and
(3) the NIR shoulder. When chlorophyll absorption causes a decrease of red reflectance,
and leaf tissue abundance causes an increase in NIR reflectance, the total area of the
triangle increases. It is good for estimating green LAI, but its sensitivity to chlorophyll
increases with an increase in canopy density. The modified version of the index accounts
for the background signature of soils while preserving sensitivity to LAI  and resistance
to the influence of chlorophyll.
</p>
<p>The Normalized difference senescent vegetation index (NDSVI) follows methods from
Qi et a., (2000). The senescence is used to threshold the NDSVI. Values less then this value
will be NA. The threshold argument is used to apply a threshold to MSAVI. The default is NULL
but if specified all values (MSAVI &lt;= threshold) will be NA. Applying a weight.factor can be
used to change the influence of the weights on MSAVI.
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object of the weighted MSAVI metric
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Haboudane, D., et al. (2004) Hyperspectral Vegetation Indices and Novel Algorithms
for Predicting Green LAI of Crop Canopies: Modeling and Validation in the Context
of Precision Agriculture. Remote Sensing of Environment 90:337-352.
</p>
<p>Qi J., Chehbouni A., Huete A.R., Kerr Y.H., (1994). Modified Soil Adjusted Vegetation
Index (MSAVI). Remote Sens Environ 48:119-126.
</p>
<p>Qi J., Kerr Y., Chehbouni A., (1994). External factor consideration in vegetation
index development. Proc. of Physical Measurements and Signatures in Remote Sensing,
ISPRS, 723-730.
</p>
<p>Qi, J., Marsett, R., Moran, M.S., Goodrich, D.C., Heilman, P., Kerr, Y.H., Dedieu,
G., Chehbouni, A., Zhang, X.X. (2000). Spatial and temporal dynamics of vegetation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
lsat &lt;- rast(system.file("/extdata/Landsat_TM5.tif", package="spatialEco"))
 
# Using Modified Soil-adjusted Vegetation Index (MSAVI)
( wmsavi &lt;- swvi(red = lsat[[3]], nir = lsat[[4]], swir = lsat[[5]]) )
    plotRGB(lsat, r=6,g=5,b=2, scale=1, stretch="lin")
      plot(wmsavi, legend=FALSE, col=rev(terrain.colors(100, alpha=0.35)), add=TRUE )

# Using Modified Triangular Vegetation Index 2 (MTVI) 
( wmtvi &lt;- swvi(red = lsat[[3]], nir = lsat[[4]], swir = lsat[[5]],
                          green = lsat[[3]], mtvi = TRUE) )
  plotRGB(lsat, r=6,g=5,b=2, scale=1, stretch="lin")
    plot(wmtvi, legend=FALSE, col=rev(terrain.colors(100, alpha=0.35)), add=TRUE )


</code></pre>

<hr>
<h2 id='time_to_event'>Time to event</h2><span id='topic+time_to_event'></span>

<h3>Description</h3>

<p>Returns the time (sum to position) to a specified value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>time_to_event(
  x,
  y = 1,
  dir = c("LR", "RL"),
  int = FALSE,
  up.to = FALSE,
  na.action = c("fail", "ignore")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="time_to_event_+3A_x">x</code></td>
<td>
<p>A vector, representing time-series, to evaluate</p>
</td></tr>
<tr><td><code id="time_to_event_+3A_y">y</code></td>
<td>
<p>Threshold value tor return position for</p>
</td></tr>
<tr><td><code id="time_to_event_+3A_dir">dir</code></td>
<td>
<p>Direction of evaluation c(&quot;LR&quot;, &quot;RL&quot;)</p>
</td></tr>
<tr><td><code id="time_to_event_+3A_int">int</code></td>
<td>
<p>FALSE | TRUE - Evaluate as integer (rounds to 0 decimal places)</p>
</td></tr>
<tr><td><code id="time_to_event_+3A_up.to">up.to</code></td>
<td>
<p>FALSE | TRUE - Return value before event</p>
</td></tr>
<tr><td><code id="time_to_event_+3A_na.action">na.action</code></td>
<td>
<p>c(&quot;fail&quot;, &quot;ignore&quot;), if &quot;fail&quot; function will return error with NA's
with &quot;ignore&quot; NA values will be included in count to event</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The time to event represents the sum of positions, in the vector,
until the specified value is found ie., (0,0,1) would be 3 or, 
2 with up.to=TRUE. The int argument allows for rounding a continuous  
variable. Since it may be difficult to find an exact match to a floating 
point value rounding mitigates the problem. If you want a specific rounding 
value (eg., 1 decimal place) you can apply it to x first then pass it to 
the function. The up.to argument will stop one value before the specified value 
of (y) regardless of integer or float. For NA handling, na.action defines the
function behavior, causing it to fail or count NAs. Note that it makes no
sense to actually remove NAs as it will make the run uninterpretable.
</p>


<h3>Value</h3>

<p>A vector value representing the time to event
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Binomial instance
time_to_event(c(0,0,0,0,1,0,0,0,1,0))
time_to_event(c(0,0,0,0,1,0,0,0,1,0), up.to = TRUE)
time_to_event(c(0,0,0,0,1,0,0,0,1,0), dir="RL")
time_to_event(c(NA,0,0,0,1,0,0,0,1,0), na.action="ignore")

# Continuous threshold instance
( x &lt;- runif(100, 0,7) ) 
time_to_event(x, y = 5, int=TRUE)

# raster example
library(terra)

# Binomial instance
r &lt;- do.call(c, replicate(20,terra::rast(matrix(sample(
             c(0,1), 1000, replace=TRUE), 100, 100))))             
  ( t2e &lt;- app(r, fun=time_to_event) )

# Continuous threshold instance
r &lt;- do.call(c, replicate(20,terra::rast(matrix(
              runif(1000,0,7), 100, 100))))
  ( t2e &lt;- app(r, fun=time_to_event, y=5) )

</code></pre>

<hr>
<h2 id='TM5'>Landsat 5 TM Scene</h2><span id='topic+TM5'></span>

<h3>Description</h3>

<p>Subset of Landsat 5 TM Scene: LT52240631988227CUB02 Contains six bands of surface reflectance
path 224/row 63 acquisition date: 1988-08-14 13:00:47 GMT, EPSG:32622
</p>


<h3>Format</h3>

<p>A tif (inst/extdata/Landsat_TM5.tif) with 30m 6 bands:
</p>

<dl>
<dt>Blue</dt><dd><p>0.45 - 0.52 µm</p>
</dd>
<dt>Green</dt><dd><p>0.52 - 0.60 µm</p>
</dd>
<dt>Red</dt><dd><p>0.63 - 0.69 µm</p>
</dd>
<dt>NIR Near-Infrared</dt><dd><p>0.76 - 0.90 µm</p>
</dd>
<dt>SWIR1 Near-Infrared</dt><dd><p>1.55 - 1.75 µm</p>
</dd>
<dt>SWIR2 Mid-Infrared</dt><dd><p>2.08 - 2.35 µm</p>
</dd>
</dl>


<hr>
<h2 id='topo.distance'>Topographic distance</h2><span id='topic+topo.distance'></span>

<h3>Description</h3>

<p>Calculates topographic corrected distance for a line object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topo.distance(x, r, echo = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topo.distance_+3A_x">x</code></td>
<td>
<p>sf LINESTRING object</p>
</td></tr>
<tr><td><code id="topo.distance_+3A_r">r</code></td>
<td>
<p>terra SpatRaster class elevation raster</p>
</td></tr>
<tr><td><code id="topo.distance_+3A_echo">echo</code></td>
<td>
<p>(FALSE/TRUE) print progress to screen</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function corrects straight-line (euclidean) distances for topographic-slope effect.
</p>


<h3>Value</h3>

<p>Vector of corrected topographic distances same length as nrow(x)
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'> library(sf)
 library(terra)
 
 # create example data
 elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
   names(elev) &lt;- "elev"

 lns &lt;- lapply(1:5, function(i) {
   p &lt;- st_combine(st_as_sf(spatSample(elev, size=2, as.points=TRUE)))
   st_as_sf(st_cast(p, "LINESTRING")) }) 
 lns &lt;- do.call(rbind, lns) 
  
  plot(elev)
    plot(st_geometry(lns), add=TRUE)
      
 # Calculate topographical distance  
 ( tdist &lt;- topo.distance(lns, elev) )
 ( lgt &lt;- as.numeric(st_length(lns)) ) 
 
 # Increase in corrected distance
 tdist - lgt
 
 # Percent increase in corrected distance
 ((tdist - lgt) / lgt) * 100

</code></pre>

<hr>
<h2 id='tpi'>Topographic Position Index (tpi)</h2><span id='topic+tpi'></span>

<h3>Description</h3>

<p>Calculates topographic position using mean deviations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tpi(x, scale = 3, win = "rectangle", normalize = FALSE, zero.correct = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tpi_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="tpi_+3A_scale">scale</code></td>
<td>
<p>focal window size (n-cell x n-cell for rectangle or 
distance for circle)</p>
</td></tr>
<tr><td><code id="tpi_+3A_win">win</code></td>
<td>
<p>Window type. Options are &quot;rectangle&quot; and &quot;circle&quot;</p>
</td></tr>
<tr><td><code id="tpi_+3A_normalize">normalize</code></td>
<td>
<p>Apply deviation correction that normalizes to local 
surface roughness</p>
</td></tr>
<tr><td><code id="tpi_+3A_zero.correct">zero.correct</code></td>
<td>
<p>Apply correction for zero values in matrix weights</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A terra SpatRaster object of tpi A terra SpatRaster object
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>De Reu, J., J. Bourgeois, M. Bats, A. Zwertvaegher, V. Gelorini, et al., (2014) 
Application of the topographic position index to heterogeneous landscapes. 
Geomorphology, 186:39-49.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))

# calculate tpi and plot 
  tpi7 &lt;- tpi(elev, scale=7) 
  tpi025 &lt;- tpi(elev, win = "circle", scale=2500)
  tpi025.zc &lt;- tpi(elev, win = "circle", scale=2500, 
                   zero.correct = TRUE)

opar &lt;- par(no.readonly=TRUE)
    par(mfrow=c(2,2))
      plot(elev, main="original raster")
      plot(tpi7, main="tpi 7x7")
      plot(tpi025, main="tpi Circular window d=2500m")
   plot(tpi025, main="tpi Circular window d=2500m, zero correct")
par(opar)


</code></pre>

<hr>
<h2 id='trasp'>Solar-radiation Aspect Index</h2><span id='topic+trasp'></span>

<h3>Description</h3>

<p>Calculates the Roberts and Cooper (1989) Solar-radiation Aspect Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trasp(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trasp_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster object</p>
</td></tr>
<tr><td><code id="trasp_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::app</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Roberts and Cooper (1989) rotates (transforms) the circular aspect to assign a 
value of zero to land oriented in a north-northeast direction, (typically the 
coolest and wettest orientation), and a value of one on the hotter, dryer 
south-southwesterly slopes. The result is a continuous variable between 0 - 1. 
The metric is defined as: trasp = ( 1 - cos((pi/180)(a-30) ) / 2 
where; a = aspect in degrees
</p>


<h3>Value</h3>

<p>A terra SpatRaster object of Roberts and Cooper (1989) Solar-radiation Aspect Index
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Roberts. D.W., and Cooper, S.V. (1989). Concepts and techniques of vegetation mapping. 
In Land Classifications Based on Vegetation: Applications for Resource Management.
USDA Forest Service GTR INT-257, Ogden, UT, pp 90-96
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  s &lt;- trasp(elev)
    plot(s)
    
</code></pre>

<hr>
<h2 id='trend.line'>trend.line</h2><span id='topic+trend.line'></span>

<h3>Description</h3>

<p>Calculated specified trend line of x,y
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trend.line(x, y, type = "linear", plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trend.line_+3A_x">x</code></td>
<td>
<p>Vector of x</p>
</td></tr>
<tr><td><code id="trend.line_+3A_y">y</code></td>
<td>
<p>Vector of y</p>
</td></tr>
<tr><td><code id="trend.line_+3A_type">type</code></td>
<td>
<p>Trend line types are: 'linear', 'exponential',
'logarithmic', 'polynomial'</p>
</td></tr>
<tr><td><code id="trend.line_+3A_plot">plot</code></td>
<td>
<p>plot results (TRUE/FALSE)</p>
</td></tr>
<tr><td><code id="trend.line_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list class object with the following components:
</p>

<ul>
<li><p> for type = 'linear'  x is slope and y is intercept
</p>
</li>
<li><p> for type = 'exponential', 'logarithmic', or 'polynomial'
x is original x variable and y is vector of fit
regression line
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:10
y &lt;- jitter(x^2)

opar &lt;- par(no.readonly=TRUE)
  par(mfcol=c(2,2))
    trend.line(x,y,type='linear',plot=TRUE,pch=20,main='Linear')
    trend.line(x,y,type='exponential',plot=TRUE,pch=20,main='Exponential')
    trend.line(x,y,type='logarithmic',plot=TRUE,pch=20,main='Logarithmic')
    trend.line(x,y,type='polynomial',plot=TRUE,pch=20,main='Polynomial')
 par(opar)

</code></pre>

<hr>
<h2 id='tri'>Terrain Ruggedness Index</h2><span id='topic+tri'></span>

<h3>Description</h3>

<p>Implementation of the Riley et al (1999) Terrain Ruggedness Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tri(r, s = 3, exact = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tri_+3A_r">r</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="tri_+3A_s">s</code></td>
<td>
<p>Scale of window. Must be odd number, can represent 2
dimensions (eg., s=c(3,5) would represent a 3 x 5 window)</p>
</td></tr>
<tr><td><code id="tri_+3A_exact">exact</code></td>
<td>
<p>Calculate (TRUE/FALSE) the exact TRI or an algebraic
approximation.</p>
</td></tr>
<tr><td><code id="tri_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to terra::focal or terra::app</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algebraic approximation is considerably faster. However, because
inclusion of the center cell, the larger the scale the larger the divergence
of the minimum value. Resuls are driven by local variations so, fixed thresholds
are not very reliable. However there are some reccomended breaks (eg., Riley et al., 1999).
</p>
<p>Riley et al., (1999) ranges for classifying Topographic Ruggedness Index:
</p>

<ul>
<li><p> 0-80 - level terrain surface.
</p>
</li>
<li><p> 81-116 - nearly level surface.
</p>
</li>
<li><p> 117-161 - slightly rugged surface.
</p>
</li>
<li><p> 162-239 - intermediately rugged surface.
</p>
</li>
<li><p> 240-497 - moderately rugged surface.
</p>
</li>
<li><p> 498-958 - highly rugged surface.
</p>
</li>
<li><p> gt 959 - extremely rugged surface.
</p>
</li></ul>



<h3>Value</h3>

<p>A terra SpatRaster class object of the TRI
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  <a href="mailto:jeffrey_evans@tnc.org">jeffrey_evans@tnc.org</a>
</p>


<h3>References</h3>

<p>Riley, S.J., S.D. DeGloria and R. Elliot (1999) A terrain
ruggedness index that quantifies topographic heterogeneity,
Intermountain Journal of Sciences 5(1-4):23-27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
  ( tri.ext &lt;- tri(elev) )
  ( tri.app &lt;- tri(elev, exact = FALSE) )
  plot(c(tri.ext, tri.app))


</code></pre>

<hr>
<h2 id='vrm'>Vector Ruggedness Measure (VRM)</h2><span id='topic+vrm'></span>

<h3>Description</h3>

<p>Implementation of the Sappington et al., (2007) vector 
ruggedness measure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vrm(x, s = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vrm_+3A_x">x</code></td>
<td>
<p>A terra SpatRaster class object</p>
</td></tr>
<tr><td><code id="vrm_+3A_s">s</code></td>
<td>
<p>Scale of window. Must be odd number, can represent 2 dimensions 
(eg., s=c(3,5) would represent a 3 x 5 window)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function measures terrain ruggedness by calculating the vector 
ruggedness measure
</p>


<h3>Value</h3>

<p>A terra SpatRaster class object of the VRI
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Sappington, J.M., K.M. Longshore, D.B. Thomson (2007). Quantifying Landscape 
Ruggedness for Animal Habitat Analysis: A case Study Using Bighorn Sheep in 
the Mojave Desert. Journal of Wildlife Management. 71(5):1419-1426
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(terra)
elev &lt;- rast(system.file("extdata/elev.tif", package="spatialEco"))
   vrm3 &lt;- vrm(elev) 
   vrm5 &lt;- vrm(elev, s=5)
   plot(c(vrm3, vrm5))

</code></pre>

<hr>
<h2 id='winsorize'>Winsorize transformation</h2><span id='topic+winsorize'></span>

<h3>Description</h3>

<p>Removes extreme outliers using a winsorization transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>winsorize(
  x,
  min.value = NULL,
  max.value = NULL,
  p = c(0.05, 0.95),
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="winsorize_+3A_x">x</code></td>
<td>
<p>A numeric vector</p>
</td></tr>
<tr><td><code id="winsorize_+3A_min.value">min.value</code></td>
<td>
<p>A fixed lower bounds, all values lower than this will be 
replaced by this value. The default is set to the 5th-quantile 
of x.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_max.value">max.value</code></td>
<td>
<p>A fixed upper bounds, all values higher than this will be replaced 
by this value. The default is set to the 95th-quantile of x.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_p">p</code></td>
<td>
<p>A numeric vector of 2 representing the probabilities used in the 
quantile function.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_na.rm">na.rm</code></td>
<td>
<p>(FALSE/TRUE) should NAs be omitted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Winsorization is the transformation of a distribution by limiting extreme values 
to reduce the effect of spurious outliers. This is done by shrinking outlying 
observations to the border of the main part of the distribution.
</p>


<h3>Value</h3>

<p>A transformed vector the same length as x, unless na.rm is TRUE, then x is length 
minus number of NA's
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Dixon, W.J. (1960) Simplified Estimation from Censored Normal Samples. Annals of Mathematical 
Statistics. 31(2):385-391
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)     
x &lt;- rnorm(100)     
x[1] &lt;- x[1] * 10  
winsorize(x)       

plot(x, type="l", main="Winsorization transformation")
  lines(winsorize(x), col="red", lwd=2)
    legend("bottomright", legend=c("Original distribution","With outliers removed"),
        lty=c(1,1), col=c("black","red"))

# Behavior with NA value(s)
x[4] &lt;- NA
winsorize(x)             # returns x with original NA's 
winsorize(x, na.rm=TRUE) # removes NA's 

</code></pre>

<hr>
<h2 id='wt.centroid'>Weighted centroid</h2><span id='topic+wt.centroid'></span>

<h3>Description</h3>

<p>Creates centroid of [x,y] coordinates with optional
weights field
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wt.centroid(x, p = NULL, spatial = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wt.centroid_+3A_x">x</code></td>
<td>
<p>sf POINT class object</p>
</td></tr>
<tr><td><code id="wt.centroid_+3A_p">p</code></td>
<td>
<p>Weights column in x</p>
</td></tr>
<tr><td><code id="wt.centroid_+3A_spatial">spatial</code></td>
<td>
<p>(TRUE/FALSE) Output sf POINT object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weighted centroid is calculated as:
[Xw]=[X]*[p], [Yw]=[Y]*[p], [sXw]=SUM[Xw], [sYw]=SUM[Yw], [sP]=SUM[p]
wX=[sXw]/[sP], wY=[sYw]/[sP]    
where; X=X coordinate(S), Y=Y coordinate(S), p=WEIGHT
</p>


<h3>Value</h3>

<p>An x,y coordinate or sf POINT object representing the weighted or unweighted 
coordinate centroid
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = c("sf", "sp")
if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
  m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
  message("Can't run examples, please install ", paste(p[m], collapse = " "))
} else {
invisible(lapply(p, require, character.only=TRUE))

data(meuse, package = "sp")
meuse &lt;- st_as_sf(meuse, coords = c("x", "y"), crs = 28992, 
                  agr = "constant")

wt.copper &lt;- wt.centroid(meuse, p='copper') 
wt.zinc &lt;- wt.centroid(meuse,  p='zinc') 

plot(st_geometry(meuse), pch=20, cex=0.75, 
     main='Weighted centroid(s)')
  plot(st_geometry(wt.copper), pch=19, col='red', 
       cex=1.5, add=TRUE)  
  plot(st_geometry(wt.zinc), pch=19, col='blue', 
       cex=1.5, add=TRUE)
legend('topleft', legend=c('all','copper', 'zinc'), 
       pch=c(20,19,19),col=c('black','red','blue'))
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
