<!DOCTYPE html><html><head><title>Help for package stylo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stylo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#assign.plot.colors'><p>Assign colors to samples</p></a></li>
<li><a href='#change.encoding'>
<p>Change character encoding</p></a></li>
<li><a href='#check.encoding'><p>Check character encoding in corpus folder</p></a></li>
<li><a href='#classify'><p>Machine-learning supervised classification</p></a></li>
<li><a href='#crossv'><p>Function to Perform Cross-Validation</p></a></li>
<li><a href='#define.plot.area'><p>Define area for scatterplots</p></a></li>
<li><a href='#delete.markup'><p>Delete HTML or XML tags</p></a></li>
<li><a href='#delete.stop.words'><p>Exclude stop words (e.g. pronouns, particles, etc.) from a dataset</p></a></li>
<li><a href='#dist.cosine'><p>Cosine Distance</p></a></li>
<li><a href='#dist.delta'><p>Delta Distance</p></a></li>
<li><a href='#dist.entropy'><p>Entropy Distance</p></a></li>
<li><a href='#dist.minmax'><p>Min-Max Distance (aka Ruzicka Distance)</p></a></li>
<li><a href='#dist.simple'><p>Cosine Distance</p></a></li>
<li><a href='#dist.wurzburg'><p>Cosine Delta Distance (aka Wurzburg Distance)</p></a></li>
<li><a href='#galbraith'>
<p>Table of word frequencies (Galbraith, Rowling, Coben, Tolkien, Lewis)</p></a></li>
<li><a href='#gui.classify'><p>GUI for the function classify</p></a></li>
<li><a href='#gui.oppose'><p>GUI for the function oppose</p></a></li>
<li><a href='#gui.stylo'><p>GUI for stylo</p></a></li>
<li><a href='#imposters'><p>Authorship Verification Classifier Known as the Imposters Method</p></a></li>
<li><a href='#imposters.optimize'><p>Tuning Parameters for the Imposters Method</p></a></li>
<li><a href='#lee'>
<p>Table of word frequencies (Lee, Capote, Faulkner, Styron, etc.)</p></a></li>
<li><a href='#load.corpus'><p>Load text files</p></a></li>
<li><a href='#load.corpus.and.parse'><p>Load text files and perform pre-processing</p></a></li>
<li><a href='#make.frequency.list'><p>Make List of the Most Frequent Elements (e.g. Words)</p></a></li>
<li><a href='#make.ngrams'><p>Make text n-grams</p></a></li>
<li><a href='#make.samples'><p>Split text to samples</p></a></li>
<li><a href='#make.table.of.frequencies'><p>Prepare a table of (relative) word frequencies</p></a></li>
<li><a href='#novels'>
<p>A selection of 19th-century English novels</p></a></li>
<li><a href='#oppose'><p>Contrastive analysis of texts</p></a></li>
<li><a href='#parse.corpus'><p>Perform pre-processing (tokenization, n-gram extracting, etc.)</p></a></li>
<li><a href='#parse.pos.tags'><p>Extract POS-tags or Words from Annotated Corpora</p></a></li>
<li><a href='#perform.culling'><p>Exclude variables (e.g. words, n-grams) from a frequency table that are</p>
too characteristic for some samples</a></li>
<li><a href='#perform.delta'><p>Distance-based classifier</p></a></li>
<li><a href='#perform.impostors'><p>An Authorship Verification Classifier Known as the Impostors Method. ATTENTION:</p>
this function is obsolete; refer to a new implementation, aka the imposters() function!</a></li>
<li><a href='#perform.knn'><p>k-Nearest Neighbor classifier</p></a></li>
<li><a href='#perform.naivebayes'><p>Naive Bayes classifier</p></a></li>
<li><a href='#perform.nsc'><p>Nearest Shrunken Centroids classifier</p></a></li>
<li><a href='#perform.svm'><p>Support Vector Machines classifier</p></a></li>
<li><a href='#performance.measures'><p>Accuracy, Precision, Recall, and the F Measure</p></a></li>
<li><a href='#plot.sample.size'><p>Plot Classification Accuracy for Short Text Samples</p></a></li>
<li><a href='#rolling.classify'><p>Sequential machine-learning classification</p></a></li>
<li><a href='#rolling.delta'><p>Sequential stylometric analysis</p></a></li>
<li><a href='#samplesize.penalize'><p>Determining Minimal Sample Size for Text Classification</p></a></li>
<li><a href='#stylo'><p>Stylometric multidimensional analyses</p></a></li>
<li><a href='#stylo.default.settings'><p>Setting variables for the package stylo</p></a></li>
<li><a href='#stylo.network'><p>Bootstrap consensus networks, with D3 visualization</p></a></li>
<li><a href='#stylo.pronouns'><p>List of pronouns</p></a></li>
<li><a href='#txt.to.features'><p>Split string of words or other countable features</p></a></li>
<li><a href='#txt.to.words'><p>Split text into words</p></a></li>
<li><a href='#txt.to.words.ext'><p>Split text into words: extended version</p></a></li>
<li><a href='#zeta.chisquare'><p>Compare two subcorpora using a home-brew variant of Craig's Zeta</p></a></li>
<li><a href='#zeta.craig'><p>Compare two subcorpora using Craig's Zeta</p></a></li>
<li><a href='#zeta.eder'><p>Compare two subcorpora using Eder's Zeta</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Stylometric Multivariate Analyses</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-04-03</td>
</tr>
<tr>
<td>Author:</td>
<td>Maciej Eder [aut, cre],
  Jan Rybicki [aut],
  Mike Kestemont [aut],
  Steffen Pielstroem [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Maciej Eder &lt;maciejeder@gmail.com&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/computationalstylistics/stylo">https://github.com/computationalstylistics/stylo</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>tcltk, tcltk2, ape, pamr, e1071, class, lattice, tsne</td>
</tr>
<tr>
<td>Suggests:</td>
<td>stringi, networkD3, readr</td>
</tr>
<tr>
<td>Description:</td>
<td>Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <a href="https://journal.r-project.org/archive/2016/RJ-2016-007/index.html">https://journal.r-project.org/archive/2016/RJ-2016-007/index.html</a>. You are also encouraged to visit the Computational Stylistics Group's website <a href="https://computationalstylistics.github.io/">https://computationalstylistics.github.io/</a>, where a reasonable amount of information about the package and related projects are provided.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-03 09:03:00 UTC; m</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-03 23:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='assign.plot.colors'>Assign colors to samples</h2><span id='topic+assign.plot.colors'></span>

<h3>Description</h3>

<p>Function that assigns unique colors to each class represented in a corpus: 
used for graph auto-coloring.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assign.plot.colors(labels, col = "colors", opacity = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assign.plot.colors_+3A_labels">labels</code></td>
<td>
<p>a vector containing the names of the samples in a corpus; 
it is obligatory to use an underscore as a class delimiter. Consider 
the following examples: c(&quot;Sterne_Tristram&quot;, &quot;Sterne_Sentimental&quot;,
&quot;Fielding_Tom&quot;, ...), where the classes are the authors' names, and 
c(&quot;M_Joyce_Dubliners&quot;, &quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), 
where the classes are M(ale) and F(emale) according to authors' gender. 
Note that only the part up to the first underscore in the sample's name will be 
included in the class label.</p>
</td></tr>
<tr><td><code id="assign.plot.colors_+3A_col">col</code></td>
<td>
<p>an optional argument specifying the color palette to be used: &quot;colors&quot; for 
full-color output (default), &quot;greyscale&quot; for greyscale (useful 
for preparing publishable pictures), and &quot;black&quot;, if no colors should be used.</p>
</td></tr>
<tr><td><code id="assign.plot.colors_+3A_opacity">opacity</code></td>
<td>
<p>optional argument to set transparency/opacity of the colors. 
0 means full transparency, 1 means full opacity (default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function for graph auto-coloring; depending on the user's choice 
it assigns either colors or greyscale tones to matching strings of 
characters which stand for class identifiers. These metadata will typically 
be encoded in the texts' filenames. (As class delimiter, the underscore 
character should be used). Alternatively, all labels can be plotted in black.
</p>


<h3>Value</h3>

<p>The function returns a vector of colors, using their conventional names 
(e.g. <code>red</code>, <code>maroon4</code>, <code>mediumturquoise</code>, <code>gold4</code>, 
<code>deepskyblue</code>, ...), or numeric values if the greyscale option was 
chosen (e.g. <code>#000000</code>, <code>#000000</code>, <code>#595959</code>, 
<code>#B2B2B2</code>, ...).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>Examples</h3>

<pre><code class='language-R'># in this example, three discrete classes are specified, 
# for Tacitus, Caesar, and Livius
sample.names = c("Tacitus_Annales","Tacitus_Germania","Tacitus_Histories",
                 "Caesar_Civil_wars","Caesar_Gallic_wars",
                 "Livius_Ab_Urbe_Condita")
assign.plot.colors(sample.names)

# as above, but using greyscale:
assign.plot.colors(sample.names, col = "greyscale")
</code></pre>

<hr>
<h2 id='change.encoding'>
Change character encoding
</h2><span id='topic+change.encoding'></span>

<h3>Description</h3>

<p>This function is a wrapper around <code>iconv()</code> that allows for converting 
character encoding of multiple text files in a corpus folder, preferably 
into UTF-8.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>change.encoding(corpus.dir = "corpus/", from, to = "utf-8", 
                keep.original = TRUE, output.dir = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="change.encoding_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>path to the folder containing the corpus.</p>
</td></tr>
<tr><td><code id="change.encoding_+3A_from">from</code></td>
<td>
<p>original character encoding. See the Details section (below) for some hints on how to get the original encoding.</p>
</td></tr>
<tr><td><code id="change.encoding_+3A_to">to</code></td>
<td>
<p>character encoding to convert into.</p>
</td></tr>
<tr><td><code id="change.encoding_+3A_keep.original">keep.original</code></td>
<td>
<p>shall the original files be stored?</p>
</td></tr>
<tr><td><code id="change.encoding_+3A_output.dir">output.dir</code></td>
<td>
<p>folder for the reencoded files.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stylo works on UTF-8-enconded texts by default. This function allows you to convert your corpus, if not yet encoded in UTF-8. To check the current encoding of text files in your corpus folder, you can use the function <code>check.encoding()</code>.
</p>


<h3>Value</h3>

<p>The function saves reencoded text files.
</p>


<h3>Author(s)</h3>

<p>Steffen Pielström
</p>


<h3>See Also</h3>

<p><code><a href="#topic+check.encoding">check.encoding</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# To replace the old versions with the newly encoded, but retain them 
# in another folder:
change.encoding = function(corpus.dir = "~/corpora/example/", 
                           from = "ASCII", to = "utf-8")

# To place the new version in another folder called "utf8/":
change.encoding = function(corpus.dir = "~/corpora/example/",
                           from = "ASCII", 
                           to = "utf-8", 
                           output.dir = "utf8/")
                           
# To simply replace the old version:
change.encoding = function(corpus.dir = "~/corpora/example/", 
                           from = "ASCII", 
                           to = "utf-8",
                           keep.original = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='check.encoding'>Check character encoding in corpus folder</h2><span id='topic+check.encoding'></span>

<h3>Description</h3>

<p>Using non-ASCII characters is never trivial, but sometimes unavoidable.
Specifically, most of the world's languages use non-Latin alphabets or
diacritics added to the standard Latin script. 
The default character encoding in stylo is UTF-8, deviating from it can 
cause problems. This function allows users to check the character 
encoding in a corpus. A summary is returned to the termial and a detailed
list reporting the most probable encodings of all the text files in the
folder can be written to a csv file. The function is basically a wrapper 
around the function <code>guess_encoding()</code> from the 'readr' package by
Wickham et al. (2017). To change the encoding to UTF-8, try the 
<code>change.encoding()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check.encoding(corpus.dir = "corpus/", output.file = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check.encoding_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>path to the folder containing the corpus.</p>
</td></tr>
<tr><td><code id="check.encoding_+3A_output.file">output.file</code></td>
<td>
<p>path to a csv file that reports the most probable encoding
for each text file in the corpus.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If no additional argument is passed, then the function tries to check the
text files in the default subdirectory <code>corpus</code>. 
</p>


<h3>Value</h3>

<p>The function returns a summary message and writes detailed results into a csv file.
</p>


<h3>Author(s)</h3>

<p>Steffen Pielström</p>


<h3>References</h3>

<p>Wickham , H., Hester, J., Francois, R., Jylanki, J., and Jørgensen, M. (2017). Package: 'readr'. &lt;https://cran.r-project.org/web/packages/readr/readr.pdf&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+change.encoding">change.encoding</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage from stylo working directory with a 'corpus' subfolder:
check.encoding()

# specifying another folder:
check.encoding("~/corpora/example1/")

# specifying an output file:
check.encoding(output.file = "~/experiments/charencoding/example1.csv")


## End(Not run)
</code></pre>

<hr>
<h2 id='classify'>Machine-learning supervised classification</h2><span id='topic+classify'></span>

<h3>Description</h3>

<p>Function that performs a number of machine-learning methods
for classification used in computational stylistics: Delta (Burrows, 2002), 
k-Nearest Neighbors, Support Vector Machines, Naive Bayes, 
and Nearest Shrunken Centroids (Jockers and Witten, 2010). Most of the options 
are derived from the <code>stylo</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classify(gui = TRUE, training.frequencies = NULL, test.frequencies = NULL,
         training.corpus = NULL, test.corpus = NULL, features = NULL, 
         path = NULL, training.corpus.dir = "primary_set",
         test.corpus.dir = "secondary_set", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classify_+3A_gui">gui</code></td>
<td>
<p>an optional argument; if switched on, a simple yet effective 
graphical user interface (GUI) will appear. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="classify_+3A_training.frequencies">training.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for several variables, 
e.g. most frequent words, across a number of text samples (for the 
training set). It can be either 
an R object (matrix or data frame), or a filename containing 
tab-delimited data. If you use an R object, make sure that the rows
contain samples, and the columns &ndash; variables (words). If you use
an external file, the variables should go vertically (i.e. in rows):
this is because files containing vertically-oriented tables are far 
more flexible and easily editable using, say, Excel or any text editor. 
To flip your table horizontally/vertically use the generic function 
<code>t()</code>.</p>
</td></tr>
<tr><td><code id="classify_+3A_test.frequencies">test.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for the 
test set. Further details: immediately above.</p>
</td></tr>
<tr><td><code id="classify_+3A_training.corpus">training.corpus</code></td>
<td>
<p>another option is to pass a pre-processed corpus
as an argument (here: the training set). It is assumed that this object 
is a list, each element of which is a vector containing one tokenized 
sample. The example shown below will give you some hints how to prepare 
such a corpus. Also, refer to <code>help(load.corpus.and.parse)</code></p>
</td></tr>
<tr><td><code id="classify_+3A_test.corpus">test.corpus</code></td>
<td>
<p>if <code>training.corpus</code> is used, then you should also 
prepare a similar R object containing the test set.</p>
</td></tr>
<tr><td><code id="classify_+3A_features">features</code></td>
<td>
<p>usually, a number of the most frequent features (words,
word n-grams, character n-grams) are extracted automatically from the
corpus, and they are used as variables for further analysis. However,
in some cases it makes sense to use a set of tailored features, e.g.
the words that are associated with emotions or, say, a specific subset 
of function words. This optional argument allows to pass either a
filename containing your custom list of features, or a vector 
(R object) of features to be assessed.</p>
</td></tr>
<tr><td><code id="classify_+3A_path">path</code></td>
<td>
<p>if not specified, the current directory will be used 
for input/output procedures (reading files, outputting the results).</p>
</td></tr>
<tr><td><code id="classify_+3A_training.corpus.dir">training.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working directory) that
contains the training set, or the collection of texts used to exemplify 
the differences between particular classes (e.g. authors or genres). The discriminating features extracted from this training material will be used during the testing procedure (see below). If not specified, the default subdirectory 
<code>primary_set</code> will be used.</p>
</td></tr>
<tr><td><code id="classify_+3A_test.corpus.dir">test.corpus.dir</code></td>
<td>
<p>the subdirectory (within the working directory) that
contains the test set, or the collection of texts that are used to 
test the effectiveness of the discriminative features extracted from 
the training set. In the case of authorship attribution e.g., 
this set might contain works of non-disputed authorship, in order to check 
whether a classification procedure attribute the tets texts to their correct author. This set contains &lsquo;new&rsquo; or &lsquo;unseen&rsquo; 
data (e.g. anonymous samples or samples of disputed authorship in the case of authorship studies). If not specified, the default subdirectory <code>secondary_set</code> will be used.</p>
</td></tr>
<tr><td><code id="classify_+3A_...">...</code></td>
<td>
<p>any variable as produced by <code>stylo.default.settings()</code>
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are numerous additional options that are passed to 
this function; so far, they are all loaded when <code>stylo.default.settings()</code> 
is executed (it will be invoked automatically from inside this function);
the user can set/change them in the GUI.</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including tables of word frequencies, vector of features 
used, a distance table and some more stuff. Additionally, depending on which 
options have been chosen, the function produces a number of files used to save 
the results, features assessed, generated tables of distances, etc.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont</p>


<h3>References</h3>

<p>Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package 
for computational text analysis. &quot;R Journal&quot;, 8(1): 107-21.
</p>
<p>Burrows, J. F. (2002). &quot;Delta&quot;: a measure of stylistic difference and 
a guide to likely authorship. &quot;Literary and Linguistic Computing&quot;, 
17(3): 267-87.
</p>
<p>Jockers, M. L. and Witten, D. M. (2010). A comparative study of machine 
learning methods for authorship attribution. &quot;Literary and Linguistic
Computing&quot;, 25(2): 215-23.
</p>
<p>Argamon, S. (2008). Interpreting Burrows's Delta: geometric and 
probabilistic foundations. &quot;Literary and Linguistic Computing&quot;, 
23(2): 131-47. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+rolling.delta">rolling.delta</a></code>, <code><a href="#topic+oppose">oppose</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage (it builds a corpus from a collection of text files):
classify()


# loading word frequencies from two tab-delimited files:
classify(training.frequencies = "table_with_training_frequencies.txt",
         test.frequencies = "table_with_test_frequencies.txt")

         
# using two existing sub-corpora (a list containing tokenized texts):
txt1 = c("now", "i", "am", "alone", "o", "what", "a", "slave", "am", "i")
txt2 = c("what", "do", "you", "read", "my", "lord")
  setTRAIN = list(txt1, txt2)
  names(setTRAIN) = c("hamlet_sample1","polonius_sample1")
txt4 = c("to", "be", "or", "not", "to", "be")
txt5 = c("though", "this", "be", "madness", "yet", "there", "is", "method")
txt6 = c("the", "rest", "is", "silence")
  setTEST = list(txt4, txt5, txt6)
  names(setTEST) = c("hamlet_sample2", "polonius_sample2", "uncertain_1")
classify(training.corpus = setTRAIN, test.corpus = setTEST)


# using a custom set of features (words, n-grams) to be analyzed:
my.selection.of.function.words = c("the", "and", "of", "in", "if", "into", 
                                   "within", "on", "upon", "since")
classify(features = my.selection.of.function.words)


# loading a custom set of features (words, n-grams) from a file:
classify(features = "wordlist.txt")


# batch mode, custom name of corpus directories:
my.test = classify(gui = FALSE, training.corpus.dir = "TrainingSet",
       test.corpus.dir = "TestSet")
summary(my.test)


# batch mode, character 3-grams requested:
classify(gui = FALSE, analyzed.features = "c", ngram.size = 3)


## End(Not run)
</code></pre>

<hr>
<h2 id='crossv'>Function to Perform Cross-Validation</h2><span id='topic+crossv'></span>

<h3>Description</h3>

<p>Function for performing a classification iteratively, while in each iteration
the composition of the train set and the test set is re-shuffled. There are
a few cross-validation flavors available; the current function supports (i)
stratified cross-validation, which means that in N iterations, the train/test
sets are assigned randomly, but the exact number of texts representing the 
original classes in the train set are keept unchanged; (ii) leave-one-out
cross-validation, which moves one sample from the train set to the test set, 
performs a classification, and then repeates the same procedure untill the
available samples are exhausted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossv(training.set, test.set = NULL, 
       cv.mode = "leaveoneout", cv.folds = 10, 
       classes.training.set = NULL, classes.test.set = NULL, 
       classification.method = "delta", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossv_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="crossv_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set. If the leave-one-out cross-validation flavor was 
chosen, then the test set is not obligatory: it will be 
created automatically. If the test set is present, however,
it will be used as a &quot;new&quot; dataset for predicting its classes.
It might seem a bit misleading &ndash; new versions will distinguish
more precisely the (i) train set, (ii) validation set and 
(iii) test set in the strict sense.</p>
</td></tr>
<tr><td><code id="crossv_+3A_cv.mode">cv.mode</code></td>
<td>
<p>choose &quot;leaveoneout&quot; to perform leave-one-out 
cross-validation; choose &quot;stratified&quot; to perform random selection
of train samples in N iterations (see the <code>cv.folds</code> parameter 
below) out of the all the available samples, provided that the very
number of samples representing the classes in the original
train set is keept in each iterations.</p>
</td></tr>
<tr><td><code id="crossv_+3A_cv.folds">cv.folds</code></td>
<td>
<p>the number of train/test set swaps, or cross-validation folds.
A standard solution in the exact sciences seems to be a 10-fold
cross-validation. It has been shown, however (Eder and Rybicki
2013) that in text analysis setups, this might be not enough.
This option is immaterial with leave-one-out cross-validation,
since the number of folds is always as high as the number 
of train samples.</p>
</td></tr>
<tr><td><code id="crossv_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="crossv_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="crossv_+3A_classification.method">classification.method</code></td>
<td>
<p>the function invokes one of the classification
methods provided by the package <code>stylo</code>. Choose one of the
following: &quot;delta&quot;, &quot;svm&quot;, &quot;knn&quot;, &quot;nsc&quot;, &quot;naivebayes&quot;.</p>
</td></tr>
<tr><td><code id="crossv_+3A_...">...</code></td>
<td>
<p>further parameters can be passed; they might be needed by 
particular classification methods. See <code>perform.delta</code>,
<code>perform.svm</code>, <code>perform.nsc</code>, <code>perform.knn</code>,
<code>perform.naivebayes</code> for further results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of accuracy scores across specified 
cross-validation folds. The attributes of the vector contain a list of 
misattributed samples (attr &quot;misattributions&quot;) and a list of confusion
matrices for particular cv folds (attr &quot;confusion_matrix&quot;).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+perform.svm">perform.svm</a></code>, 
<code><a href="#topic+perform.nsc">perform.nsc</a></code>, <code><a href="#topic+perform.knn">perform.knn</a></code>, 
<code><a href="#topic+perform.naivebayes">perform.naivebayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## standard usage:
crossv(training.set, test.set)

## End(Not run)

## text categorization

# specify a table with frequencies
data(lee)
# perform a leave-one-out classification using kNN
results = crossv(lee, classification.method = "knn")
# inspect final results
performance.measures(results)



## stratified cross-validation

# specity a table with frequencies
data(galbraith)
freqs = galbraith
# specify class labels:
training.texts = c("coben_breaker", "coben_dropshot", "lewis_battle", 
                   "lewis_caspian", "rowling_casual", "rowling_chamber", 
                   "tolkien_lord1", "tolkien_lord2")
train.classes = match(training.texts,rownames(freqs))

# select the training samples:
training.set = freqs[train.classes,]
# select remaining rows as test samples:
test.set = freqs[-train.classes,]

crossv(training.set, test.set, cv.mode = "stratified")



# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

crossv(train, test, cv.mode = "stratified", cv.folds = 10, 
       train.classes, test.classes)



</code></pre>

<hr>
<h2 id='define.plot.area'>Define area for scatterplots</h2><span id='topic+define.plot.area'></span>

<h3>Description</h3>

<p>Function that determines the size of a scatterplot, taking into consideration 
additional margin to fit longer labels appearing on a graph (if applicable), 
optional margin defined by user, and some space to offset scatterplot labels 
from points (if applicable).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define.plot.area(x.coord, y.coord, xymargins = 2, v.offset = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="define.plot.area_+3A_x.coord">x.coord</code></td>
<td>
<p>a vector of x coordinates, optionally with names.</p>
</td></tr>
<tr><td><code id="define.plot.area_+3A_y.coord">y.coord</code></td>
<td>
<p>a vector of y coordinates.</p>
</td></tr>
<tr><td><code id="define.plot.area_+3A_xymargins">xymargins</code></td>
<td>
<p>additional margins (expressed as a % of the actual plot area).</p>
</td></tr>
<tr><td><code id="define.plot.area_+3A_v.offset">v.offset</code></td>
<td>
<p>label offset (expressed as a % of the actual plot area).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function that finds out the coordinates of scatterplots: it computes 
the extreme x and y values, adds margins, and optionally extends
the top margin if a plot uses sample labels. Automatic margin extension
will only take place if the x coordinates are supplemented by
their names (i.e. labels of points to be shown on scatterplot).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+assign.plot.colors">assign.plot.colors</a></code>, <code><a href="#topic+stylo">stylo</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to determine the plotting area for 4 points:
define.plot.area( c(1,2,3,4), c(-0.001,0.11,-0.023,0.09))

# to determine plot coordinates, taking into consideration 
# the objects' names
my.points = cbind(c(1,2,3,4),c(-0.001,0.11,-0.023,0.09))
rownames(my.points) = c("first","second","third","very_long_fourth")
define.plot.area(my.points[,1], my.points[,2])
</code></pre>

<hr>
<h2 id='delete.markup'>Delete HTML or XML tags</h2><span id='topic+delete.markup'></span>

<h3>Description</h3>

<p>Function for removing markup tags (e.g. HTML, XML) from a string 
of characters. All XML markup is assumed to be compliant with 
the TEI guidelines (<a href="https://tei-c.org/">https://tei-c.org/</a>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete.markup(input.text, markup.type = "plain")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete.markup_+3A_input.text">input.text</code></td>
<td>
<p>any string of characters (e.g. vector) containing markup 
tags that have to be deleted.</p>
</td></tr>
<tr><td><code id="delete.markup_+3A_markup.type">markup.type</code></td>
<td>
<p>any of the following values: <code>plain</code> 
(nothing will happen), <code>html</code> (all &lt;tags&gt; will be deleted as well 
as HTML header), <code>xml</code> (TEI header, all strings between &lt;note&gt; &lt;/note&gt; 
tags, and all the tags will be deleted), <code>xml.drama</code> (as above;
but, additionally, speaker's names will be deleted, or strings within each
the &lt;speaker&gt; &lt;/speaker&gt; tags), <code>xml.notitles</code> (as above; but, 
additionally, all the chapter/section (sub)titles will be deleted, 
or strings within each the &lt;head&gt; &lt;/head&gt; tags).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function needs to be used carefully: while a document formatted in compliance 
with the TEI guidelines will be parsed flawlessly, the cleaning up of an HTML
page harvested randomly on the web might cause some side effects, e.g. the footers, 
disclaimers, etc. will not be removed.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont
</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus">load.corpus</a></code>, <code><a href="#topic+txt.to.words">txt.to.words</a></code>, 
<code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>, <code><a href="#topic+txt.to.features">txt.to.features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  delete.markup("Gallia est omnis &lt;i&gt;divisa&lt;/i&gt; in partes tres", 
           markup.type = "html")

  delete.markup("Gallia&lt;note&gt;Gallia: Gaul.&lt;/note&gt; est omnis 
           &lt;emph&gt;divisa&lt;/emph&gt; in partes tres", markup.type = "xml")

  delete.markup("&lt;speaker&gt;Hamlet&lt;/speaker&gt;Words, words, words...", 
           markup.type = "xml.drama")
</code></pre>

<hr>
<h2 id='delete.stop.words'>Exclude stop words (e.g. pronouns, particles, etc.) from a dataset</h2><span id='topic+delete.stop.words'></span>

<h3>Description</h3>

<p>Function for removing custom words from a dataset: it can be 
the so-called stop words (frequent words without much meaning), 
or personal pronouns, or other custom elements of a dataset. It can 
be used to cull certain words from a vector containing tokenized 
text (particular words as elements of the vector), or to exclude 
unwanted columns (variables) from a table with frequencies. See 
examples below.</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete.stop.words(input.data, stop.words = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete.stop.words_+3A_input.data">input.data</code></td>
<td>
<p>either a vector containing words (actually, any countable 
features), or a data matrix/frame. The former in case of culling 
stop words from running text, the latter for culling them from tables 
of frequencies (then particular columns are excluded). The table should 
be oriented to contain samples in rows, variables in columns, and 
variables' names should be accessible via <code>colnames(input.table)</code>.</p>
</td></tr>
<tr><td><code id="delete.stop.words_+3A_stop.words">stop.words</code></td>
<td>
<p>a vector of words to be excluded.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might be usefull to perform culling, or automatic deletion of 
the words that are too characteristic for particular texts. See 
<code>help(culling)</code> for further details.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo.pronouns">stylo.pronouns</a></code>, <code><a href="#topic+perform.culling">perform.culling</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># (i) excluding stop words from a vector
my.text = c("omnis", "homines", "qui", "sese", "student", "praestare", 
        "ceteris", "animalibus", "summa", "ope", "niti", "decet", "ne",
        "vitam", "silentio", "transeant", "veluti", "pecora", "quae",
        "natura", "prona", "atque", "ventri", "oboedientia", "finxit")
delete.stop.words(my.text, stop.words = c("qui", "quae", "ne", "atque"))


# (ii) excluding stop words from tabular data
#
# assume there is a matrix containing some frequencies
# (be aware that these counts are fictional):
t1 = c(2, 1, 0, 8, 9, 5, 6, 3, 4, 7)
t2 = c(7, 0, 5, 9, 1, 8, 6, 4, 2, 3)
t3 = c(5, 9, 2, 1, 6, 7, 8, 0, 3, 4)
t4 = c(2, 8, 6, 3, 0, 5, 9, 4, 7, 1)
my.data.table = rbind(t1, t2, t3, t4)

# names of the samples:
rownames(my.data.table) = c("text1", "text2", "text3", "text4")
# names of the variables (e.g. words):
colnames(my.data.table) = c("the", "of", "in", "she", "me", "you",
                                    "them", "if", "they", "he")
# the table looks as follows
print(my.data.table)

# now, one might want to get rid of the words "the", "of", "if":
delete.stop.words(my.data.table, stop.words = c("the", "of", "if"))

# also, some pre-defined lists of pronouns can be applied:
delete.stop.words(my.data.table, 
                      stop.words = stylo.pronouns(corpus.lang = "English"))


</code></pre>

<hr>
<h2 id='dist.cosine'>Cosine Distance</h2><span id='topic+dist.cosine'></span>

<h3>Description</h3>

<p>Function for computing a cosine similarity of a matrix of values,
e.g. a table of word frequencies. Recent findings (Jannidis et al. 2015) 
show that this distance outperforms other nearest neighbor approaches 
in the domain of authorship attribution.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.cosine(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.cosine_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielstrom, S., Schoch, C. 
and Vitt, T. (2017). Understanding and explaining Delta measures for authorship 
attribution. Digital Scholarship in the Humanities, 32(suppl. 2): 4-16.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance, in two flavors
        dist.cosine(dataset)
        as.matrix(dist.cosine(dataset))

</code></pre>

<hr>
<h2 id='dist.delta'>Delta Distance</h2><span id='topic+dist.delta'></span><span id='topic+dist.argamon'></span><span id='topic+dist.eder'></span>

<h3>Description</h3>

<p>Function for computing Delta similarity measure of a matrix of values,
e.g. a table of word frequencies. Apart from the Classic Delta, two other 
flavors of the measure are supported: Argamon's Delta and Eder's Delta. 
There are also non-Delta distant measures available: see e.g. 
<code><a href="#topic+dist.cosine">dist.cosine</a></code> and <code><a href="#topic+dist.simple">dist.simple</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.delta(x, scale = TRUE)

dist.argamon(x, scale = TRUE)

dist.eder(x, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.delta_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
<tr><td><code id="dist.delta_+3A_scale">scale</code></td>
<td>
<p>the Delta measure relies on scaled frequencies &ndash; if you have 
your matrix scaled already (i.e. converted to z-scores), switch this option 
off. Default: TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Argamon, S. (2008). Interpreting Burrows's Delta: geometric and 
probabilistic foundations. &quot;Literary and Linguistic Computing&quot;, 
23(2): 131-147.
</p>
<p>Burrows, J. F. (2002). &quot;Delta&quot;: a measure of stylistic difference and 
a guide to likely authorship. &quot;Literary and Linguistic Computing&quot;, 
17(3): 267-287.
</p>
<p>Eder, M. (2015). Taking stylometry to the limits: benchmark study on 5,281 
texts from Patrologia Latina. In: &quot;Digital Humanities 2015: Conference 
Abstracts&quot;. 
</p>
<p>Eder, M. (2022). Boosting word frequencies in authorship attribution. In:
&quot;CHR 2022 Computational Humanities Research 2022&quot;, pp. 387-397. 
<a href="https://ceur-ws.org/Vol-3290/long_paper5362.pdf">https://ceur-ws.org/Vol-3290/long_paper5362.pdf</a>
</p>
<p>Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielstrom, S., Schoch, C. 
and Vitt, T. (2017). Understanding and explaining Delta measures for authorship 
attribution. Digital Scholarship in the Humanities, 32(suppl. 2): 4-16.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+dist.cosine">dist.cosine</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance
        dist.delta(dataset)
        dist.argamon(dataset)
        dist.eder(dataset)

# converting to a regular matrix
        as.matrix(dist.delta(dataset))

</code></pre>

<hr>
<h2 id='dist.entropy'>Entropy Distance</h2><span id='topic+dist.entropy'></span>

<h3>Description</h3>

<p>Function for computing the entropy distance measure between two (or more) 
vectors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.entropy(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.entropy_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Juola, P. and Baayen, H. (2005). A controlled-corpus experiment in authorship 
attribution by cross-entropy. Literary and Linguistic Computing, 20(1): 59-67.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>, <code><a href="#topic+dist.cosine">dist.cosine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance, in two flavors
        dist.entropy(dataset)
        as.matrix(dist.entropy(dataset))

</code></pre>

<hr>
<h2 id='dist.minmax'>Min-Max Distance (aka Ruzicka Distance)</h2><span id='topic+dist.minmax'></span>

<h3>Description</h3>

<p>Function for computing a similarity measure bewteen two (or more) vectors. 
Some scholars (Kestemont et at., 2016) claim that it works well when applied
to authorship attribution problems.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.minmax(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.minmax_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. (2016). 
Authenticating the writings of Julius Caesar. Expert Systems With 
Applications, 63: 86-96.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>, <code><a href="#topic+dist.cosine">dist.cosine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance, in two flavors
        dist.minmax(dataset)
        as.matrix(dist.minmax(dataset))

</code></pre>

<hr>
<h2 id='dist.simple'>Cosine Distance</h2><span id='topic+dist.simple'></span>

<h3>Description</h3>

<p>Function for computing Eder's Simple distance of a matrix of values,
e.g. a table of word frequencies. This is done by normalizing the input
dataset by a square root function, and then applying Manhattan distance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.simple(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.simple_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+dist.delta">dist.delta</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance, in two flavors
        dist.simple(dataset)
        as.matrix(dist.simple(dataset))

</code></pre>

<hr>
<h2 id='dist.wurzburg'>Cosine Delta Distance (aka Wurzburg Distance)</h2><span id='topic+dist.wurzburg'></span>

<h3>Description</h3>

<p>Function for computing a cosine similarity of a scaled (z-scored) matrix 
of values, e.g. a table of word frequencies. Recent findings by 
the briliant guys from Wurzburg (Jannidis et al. 2015) show that 
this distance outperforms other nearest  neighbor approaches  in the domain 
of authorship attribution.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.wurzburg(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.wurzburg_+3A_x">x</code></td>
<td>
<p>a matrix or data table containing at least 2 rows and 2 cols,
the samples (texts) to be compared in rows, the variables in columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>dist</code>, containing distances
between each pair of samples. To convert it to a square matrix instead,
use the generic function <code>as.dist</code>.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielstrom, S., Schoch, C. 
and Vitt, T. (2017). Understanding and explaining Delta measures for authorship 
attribution. Digital Scholarship in the Humanities, 32(suppl. 2): 4-16.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, 
<code><a href="stats.html#topic+as.dist">as.dist</a></code>, <code><a href="#topic+dist.cosine">dist.cosine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first, preparing a table of word frequencies
        Iuvenalis_1 = c(3.939, 0.635, 1.143, 0.762, 0.423)
        Iuvenalis_2 = c(3.733, 0.822, 1.066, 0.933, 0.511)
        Tibullus_1  = c(2.835, 1.302, 0.804, 0.862, 0.881)
        Tibullus_2  = c(2.911, 0.436, 0.400, 0.946, 0.618)
        Tibullus_3  = c(1.893, 1.082, 0.991, 0.879, 1.487)
        dataset = rbind(Iuvenalis_1, Iuvenalis_2, Tibullus_1, Tibullus_2, 
                        Tibullus_3)
        colnames(dataset) = c("et", "non", "in", "est", "nec")

# the table of frequencies looks as follows
        print(dataset)
        
# then, applying a distance, in two flavors
        dist.wurzburg(dataset)
        as.matrix(dist.wurzburg(dataset))

</code></pre>

<hr>
<h2 id='galbraith'>
Table of word frequencies (Galbraith, Rowling, Coben, Tolkien, Lewis)
</h2><span id='topic+galbraith'></span>

<h3>Description</h3>

<p>This dataset contains a table (matrix) of relative frequencies of 3000 most 
frequent words retrieved from 26 books by 5 authors, including the novel 
&quot;Cuckoo's Calling&quot; by a mysterious Robert Galbraith that turned out 
to be J.K. Rowling. The remaining authors are as follows: 
Harlan Coben (&quot;Deal Breaker&quot;, &quot;Drop Shot&quot;, &quot;Fade Away&quot;, &quot;One False Move&quot;, 
&quot;Gone for Good&quot;, &quot;No Second Chance&quot;, &quot;Tell No One&quot;), C.S. Lewis 
(&quot;The Last Battle&quot;, &quot;Prince Caspian: The Return to Narnia&quot;, 
&quot;The Silver Chair&quot;, &quot;The Horse and His Boy&quot;, &quot;The Lion, the Witch and the Wardrobe&quot;, 
&quot;The Magician's Nephew&quot;, &quot;The Voyage of the Dawn Treader&quot;), J.K. Rowling 
(&quot;The Casual Vacancy&quot;, &quot;Harry Potter and the Chamber of Secrets&quot;, 
&quot;Harry Potter and the Goblet of Fire&quot;, &quot;Harry Potter and the Deathly Hallows&quot;, 
&quot;Harry Potter and the Order of the Phoenix&quot;, &quot;Harry Potter and the Half-Blood Prince&quot;, 
&quot;Harry Potter and the Prisoner of Azkaban&quot;, &quot;Harry Potter and the Philosopher's Stone&quot;), 
and J.R.R. Tolkien (&quot;The Fellowship of the Ring&quot;, &quot;The Two Towers&quot;, 
&quot;The Return of the King&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("galbraith")</code></pre>


<h3>Details</h3>

<p>The word frequencies are represented as a two-dimensional table: variables 
(words) in columns, samples (novels) in rows. The frequencies are relative, 
i.e. the number of occurrences of particular word type was divided by 
the total number of tokens in a given text.
</p>


<h3>Source</h3>

<p>The novels represented by this dataset are protected by copyright. 
For that reason, it was not possible to provide the actual texts. Instead, 
the frequences of the most frequent words are obtained &ndash; and those 
can be freely distributed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(galbraith)
rownames(galbraith)

## Not run: 
stylo(frequencies = galbraith, gui = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='gui.classify'>GUI for the function classify</h2><span id='topic+gui.classify'></span>

<h3>Description</h3>

<p>Graphical user interface for <code>classify</code>. Via the GUI, this function can set
most of the variables needed for <code>classify</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gui.classify(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gui.classify_+3A_...">...</code></td>
<td>
<p>any variable as produced by <code>stylo.default.settings</code>
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calls <code>stylo.default.settings</code> to initialize a number
of default variables. Then it reads the file <code>classify_config.txt</code> 
(if the file exists and can be found in the current directory) to 
overwrite any default values. Then a GUI box appears, allowing the variables' 
customization by the user. Refer to HOWTO available at 
<a href="https://sites.google.com/site/computationalstylistics/">https://sites.google.com/site/computationalstylistics/</a> 
for a detailed explanation what the particular variables are for 
and how to use them.
</p>


<h3>Value</h3>

<p>The function returns a list containing ca. 100 variables. 
</p>


<h3>Author(s)</h3>

<p>Jan Rybicki, Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+gui.stylo">gui.stylo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gui.classify()

my.variables = gui.classify()
summary(my.variables)

## End(Not run)
</code></pre>

<hr>
<h2 id='gui.oppose'>GUI for the function oppose</h2><span id='topic+gui.oppose'></span>

<h3>Description</h3>

<p>Graphical user interface for <code>oppose</code>. This function sets most 
of the variables needed for <code>oppose</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gui.oppose(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gui.oppose_+3A_...">...</code></td>
<td>
<p>any variable as produced by <code>stylo.default.settings</code>
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calls <code>stylo.default.settings</code> to initialize a number
of default variables. Then it reads the file <code>oppose_config.txt</code> (if the file
exists and can be found in the current directory) to overwrite any default values.
Then a GUI box appears, allowing the variables' customization by the user. Refer to 
HOWTO available at <a href="https://sites.google.com/site/computationalstylistics/">https://sites.google.com/site/computationalstylistics/</a> 
for a detailed explanation what the particular variables are for 
and how to use them.
</p>


<h3>Value</h3>

<p>The function returns a list containing ca. 100 variables. 
</p>


<h3>Author(s)</h3>

<p>Jan Rybicki, Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+stylo.default.settings">stylo.default.settings</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gui.oppose()

my.variables = gui.oppose()
summary(my.variables)

## End(Not run)
</code></pre>

<hr>
<h2 id='gui.stylo'>GUI for stylo</h2><span id='topic+gui.stylo'></span>

<h3>Description</h3>

<p>Graphical user interface for the function <code>stylo</code>. This function sets 
most of the variables needed for <code>stylo</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gui.stylo(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gui.stylo_+3A_...">...</code></td>
<td>
<p>any variable as produced by <code>stylo.default.settings</code>
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calls <code>stylo.default.settings</code> to initialize a number
of default variables. Then it reads the file <code>stylo_config.txt</code> (if the file
exists and can be found in the current directory) to overwrite any default values.
Then a GUI box appears, allowing the variables' customization by the user. Refer to 
HOWTO available at <a href="https://sites.google.com/site/computationalstylistics/">https://sites.google.com/site/computationalstylistics/</a> 
for a detailed explanation what the particular variables are for 
and how to use them.
</p>


<h3>Value</h3>

<p>The function returns a list containing ca. 100 variables. 
</p>


<h3>Author(s)</h3>

<p>Jan Rybicki, Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+stylo.default.settings">stylo.default.settings</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
gui.stylo()

my.variables = gui.stylo()
summary(my.variables)

## End(Not run)
</code></pre>

<hr>
<h2 id='imposters'>Authorship Verification Classifier Known as the Imposters Method</h2><span id='topic+imposters'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier tailored to assess authorship
verification tasks. This function is an implementation of the 2nd order
verification system known as the General Imposters framework (GI), 
and introduced by Koppel and Winter (2014). The current implementation 
tries to stick &ndash; with some improvements &ndash; to the description provided 
by Kestemont et al. (2016: 88).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imposters(reference.set, 
          test = NULL,
          candidate.set = NULL,
          iterations = 100,
          features = 0.5,
          imposters = 0.5,
          classes.reference.set = NULL,
          classes.candidate.set = NULL,
          ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imposters_+3A_reference.set">reference.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of texts
written by different authors. It is really important to put there
a selection of &quot;imposters&quot;, or the authors that could not have written
the text to be assessed. If no <code>candidate.set</code> is used, then the
table should also contain some texts written by possible candidates to 
authorship, or the authors that are suspected of being the actual author. 
Make sure that the rows contain samples, and the columns &ndash; 
variables (words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="imposters_+3A_test">test</code></td>
<td>
<p>a text to be checked for authorship, represented as a vector
of, say, word frequencies. The variables used (i.e. columns) 
must match the columns of the reference set. If nothing is indicated, 
then the function will try to infer the test text from the 
<code>reference.set</code>; when worse comes to worst, the first text 
in the reference set will be excluded as the test text.</p>
</td></tr>
<tr><td><code id="imposters_+3A_candidate.set">candidate.set</code></td>
<td>
<p>a table containing frequencies/counts for the candidate set.
This set should contain texts written by possible candidates to 
authorship, or the authors that are suspected of being the actual author.
The variables used (i.e. columns) must match the columns of the 
reference set. If no <code>candidate.set</code> is indicated, the function
will test iteratively all the classes (one at a time) from the reference
set.</p>
</td></tr>
<tr><td><code id="imposters_+3A_iterations">iterations</code></td>
<td>
<p>the model is rafined in N iterations. A reasonable number 
of turns is a few dozen or so (see the argument &quot;features&quot; below).</p>
</td></tr>
<tr><td><code id="imposters_+3A_features">features</code></td>
<td>
<p>a proportion of features to be analyzed. The imposters method 
selects randomly, in N iterations, a given subset of features 
(words, n-grams, etc.) and performs a classification. It is assumed that
a large number of iteration, each involving a randomly selected subset
of features, leads to a reliable coverage of features, among which
some outliers might be hidden. The argument specifies the
proportion of features to be randomly chosen; the indicated value 
should lay in the range between 0 and 1 (the default being 0.5).</p>
</td></tr>
<tr><td><code id="imposters_+3A_imposters">imposters</code></td>
<td>
<p>a proportion of text by the imposters to be analyzed. In each 
iteration, a specified number of texts from the comparison set is chosen 
(randomly). See above, for the features' choice. The default value
of this parameter is 0.5.</p>
</td></tr>
<tr><td><code id="imposters_+3A_classes.reference.set">classes.reference.set</code></td>
<td>
<p>a vector containing class identifiers for the
reference set. When missing, the row names of the set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="imposters_+3A_classes.candidate.set">classes.candidate.set</code></td>
<td>
<p>a vector containing class identifiers for the
candidate set. When missing, the row names of the set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="imposters_+3A_...">...</code></td>
<td>
<p>any other argument that can be passed to the classifier; see 
<code>perform.delta</code> for the parameters to be tweaked. In the current
version of the function, only distance measure used for computing 
similarities between texts can be set. Available options so far: &quot;delta&quot; 
(Burrows's Delta, default), &quot;argamon&quot; (Argamon's Linear Delta), 
&quot;eder&quot; (Eder's Delta), &quot;simple&quot; (Eder's Simple Distance), 
&quot;canberra&quot; (Canberra Distance), &quot;manhattan&quot; (Manhattan 
Distance), &quot;euclidean&quot; (Euclidean Distance), &quot;cosine&quot;
(Cosine Distance), &quot;wurzburg&quot; (Cosine Delta), &quot;minmax&quot; 
(Minmax Distance, also known as the Ruzicka measure).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a single score indicating the probability that an
anonymouns sample analyzed was/wasn't written by a candidate author. 
As a proportion, the score lies between 0 and 1 (higher scores indicate a higher 
attribution confidence). If more than one class is assessed, the resulting scores
are returned as a vector.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Koppel, M. , and Winter, Y. (2014). Determining if two documents are 
written by the same author. &quot;Journal of the Association for Information
Science and Technology&quot;, 65(1): 178-187.
</p>
<p>Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. (2016).
Authenticating the writings of Julius Caesar. &quot;Expert Systems With
Applications&quot;, 63: 86-96.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+imposters.optimize">imposters.optimize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# performing the imposters method on the dataset provided by the package:

# activating the datasets with "The Cuckoo's Calling", possibly written by JK Rowling
data(galbraith)

# running the imposters method against all the remaining authorial classes
imposters(galbraith)

# general usage:

# Let's assume there is a table with frequencies, the 8th row of which contains
# the data for a text one wants to verify.

# getting the 8th row from the dataset
text_to_be_tested = dataset[8,]

# building the reference set so that it does not contain the 8th row
remaining_frequencies = dataset[-c(8),]

# launching the imposters method:
imposters(reference.set = remaining_frequencies, test = text_to_be_tested)

## End(Not run)
</code></pre>

<hr>
<h2 id='imposters.optimize'>Tuning Parameters for the Imposters Method</h2><span id='topic+imposters.optimize'></span>

<h3>Description</h3>

<p>A function to optimize hyperparameters used in the General Imposters method
(see <code>link{imposters}</code> for further details). Using a grid search approach,
it tries to define a grey area where the attribution scores are not reliable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imposters.optimize(reference.set,
                   classes.reference.set = NULL,
                   parameter.incr = 0.01,
                   ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imposters.optimize_+3A_reference.set">reference.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of texts
written by different authors. Usually, it is a corpus of known authors
(at least two tests per author) that is used to tune the optimal 
hyperparameters for the imposters method. Such a tuning involves
a leave-one-out procedure of identifying a gray area when the
results returned by the classifier are not particularly reliable.
E.g., if one gets 0.39 and 0.55 as the parameters, one would assume
that any results of the <code>imposters()</code> function that lay within 
this range should be claimed unreliable.
Make sure that the rows contain samples, and the columns &ndash; 
variables (words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="imposters.optimize_+3A_classes.reference.set">classes.reference.set</code></td>
<td>
<p>a vector containing class identifiers for the
reference set. When missing, the row names of the set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
example: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="imposters.optimize_+3A_parameter.incr">parameter.incr</code></td>
<td>
<p>the procedure tries to optimize the hyperparameters
via a grid search &ndash; this means that it tests the range of values
between 0 and 1 incremented by a certain fraction. If this is set 
to 0.01 (default), it test 0, 0.01, 0.02, 0.03, ...</p>
</td></tr>
<tr><td><code id="imposters.optimize_+3A_...">...</code></td>
<td>
<p>any other argument that can be passed to the classifier; see 
<code>perform.delta</code> for the parameters to be tweaked. In the current
version of the function, only the distance measure used for computing 
similarities between texts can be set. Available options so far: &quot;delta&quot; 
(Burrows's Delta, default), &quot;argamon&quot; (Argamon's Linear Delta), 
&quot;eder&quot; (Eder's Delta), &quot;simple&quot; (Eder's Simple Distance), 
&quot;canberra&quot; (Canberra Distance), &quot;manhattan&quot; (Manhattan 
Distance), &quot;euclidean&quot; (Euclidean Distance), &quot;cosine&quot;
(Cosine Distance), &quot;wurzburg&quot; (Cosine Delta), &quot;minmax&quot; 
(Minmax Distance, also known as the Ruzicka measure).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns two scores: the P1 and P2 values.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Koppel, M. , and Winter, Y. (2014). Determining if two documents are 
written by the same author. &quot;Journal of the Association for Information
Science and Technology&quot;, 65(1): 178-187.
</p>
<p>Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. (2016).
Authenticating the writings of Julius Caesar. &quot;Expert Systems With
Applications&quot;, 63: 86-96.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+imposters">imposters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# activating a dummy dataset, in our case: Harper Lee and her Southern colleagues
data(lee)

# running the imposters method against all the remaining authorial classes
imposters.optimize(lee)

## End(Not run)
</code></pre>

<hr>
<h2 id='lee'>
Table of word frequencies (Lee, Capote, Faulkner, Styron, etc.)
</h2><span id='topic+lee'></span>

<h3>Description</h3>

<p>This dataset contains a table (matrix) of relative frequencies of 3000 most 
frequent words retrieved from 28 books by 8 authors, including both novels 
by Harper Lee, namely &quot;To Kill a Mockingbird&quot; and &quot;Go Set a Watchman&quot;. 
The remaining authors are as follows: Truman Capote (&quot;In Cold Blood&quot;, 
&quot;Breakfast at Tiffany's&quot;, &quot;Summer Crossing&quot;, &quot;The Grass Harp&quot;, 
&quot;Other Voices, Other Rooms&quot;), William Faulkner (&quot;Absalom, Absalom!&quot;, 
&quot;As I Lay Dying&quot;, &quot;Light in August&quot;, &quot;Go down, Moses&quot;, 
&quot;The Sound and the Fury&quot;), Ellen Glasgow (&quot;Phases of an Inferior Planet&quot;, 
&quot;Vein of Iron&quot;, &quot;Virginia&quot;), Carson McCullers (&quot;The Heart is a Lonely Hunter&quot;, 
&quot;The Member of the Wedding&quot;, &quot;Reflections in a Golden Eye&quot;), 
Flannery O'Connor (&quot;Everything That Rises Must Converge&quot;, 
&quot;The Compete Stories&quot;, &quot;Wise Blood&quot;), William Styron (&quot;Sophie's Choice&quot;, 
&quot;Set This House on Fire&quot;, &quot;The Confessions of Nat Turner&quot;), Eudora Welty 
(&quot;Delta Wedding&quot;, &quot;Losing Battles&quot;, &quot;The Optimist's Dauther&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("lee")</code></pre>


<h3>Details</h3>

<p>The word frequencies are represented as a two-dimensional table: variables 
(words) in columns, samples (novels) in rows. The frequencies are relative, 
i.e. the number of occurrences of particular word type was divided by 
the total number of tokens in a given text.
</p>


<h3>Source</h3>

<p>The novels represented by this dataset are protected by copyright. For that 
reason, it was not possible to provide the actual texts. Instead, 
the frequences of the most frequent words are obtained &ndash; and those 
can be freely distributed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lee)
rownames(lee)

## Not run: 
stylo(frequencies = lee, gui = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='load.corpus'>Load text files</h2><span id='topic+load.corpus'></span>

<h3>Description</h3>

<p>Function for loading text files from a specified directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load.corpus(files = "all", corpus.dir = "", encoding = "UTF-8")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load.corpus_+3A_files">files</code></td>
<td>
<p>a vector of file names. The default value <code>all</code> is an 
equivalent to <code>list.files()</code>.</p>
</td></tr>
<tr><td><code id="load.corpus_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>a directory containing the text files to be loaded; if
not specified, the current working directory will be used.</p>
</td></tr>
<tr><td><code id="load.corpus_+3A_encoding">encoding</code></td>
<td>
<p>useful if you use Windows and non-ASCII alphabets: French,
Polish, Hebrew, etc. In such a situation, it is quite convenient to 
convert your text files into Unicode and to set this option to
<code>encoding = "UTF-8"</code>. In Linux and Mac, you are always expected
to use Unicode, thus you don't need to set anything.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.corpus</code>. It is a list
containing as elements the texts loaded.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+rolling.classify">rolling.classify</a></code>, 
<code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+txt.to.words">txt.to.words</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# to load file1.txt and file2.txt, stored in the subdirectory my.files:
my.corpus = load.corpus(corpus.dir = "my.files",
                        files = c("file1.txt", "file2.txt") )

# to load all XML files from the current directory:
my.corpus = load.corpus(files = list.files(pattern="[.]xml$") )

## End(Not run)
</code></pre>

<hr>
<h2 id='load.corpus.and.parse'>Load text files and perform pre-processing</h2><span id='topic+load.corpus.and.parse'></span>

<h3>Description</h3>

<p>A high-level function that controls a number of other functions responsible 
for loading texts from files, deleting markup, sampling from texts, 
converting samples to n-grams, etc. It is build on top of a number of functions 
and thus it requires a large number of arguments. The only obligatory
argument, however, is a vector containing the names of the files to be loaded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load.corpus.and.parse(files = "all", corpus.dir = "", markup.type= "plain",
                      corpus.lang = "English", splitting.rule = NULL,
                      sample.size = 10000, sampling = "no.sampling",
                      sample.overlap = 0, number.of.samples = 1,
                      sampling.with.replacement = FALSE, features = "w", 
                      ngram.size = 1, preserve.case = FALSE,
                      encoding = "UTF-8", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load.corpus.and.parse_+3A_files">files</code></td>
<td>
<p>a vector of file names. The default value <code>all</code> is an 
equivalent to <code>list.files()</code>.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>the directory containing the text files to be loaded; if
not specified, the current directory will be used.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_markup.type">markup.type</code></td>
<td>
<p>choose one of the following values: <code>plain</code> 
(nothing will happen), <code>html</code> (all tags will be deleted as well 
as HTML header), <code>xml</code> (TEI header, any text between &lt;note&gt; &lt;/note&gt; 
tags, and all the tags will be deleted), <code>xml.drama</code> (as above;
additionally, speaker's names will be deleted, or strings within the 
&lt;speaker&gt; &lt;/speaker&gt; tags), <code>xml.notitles</code> (as above; but, 
additionally, all the chapter/section (sub)titles will be deleted, 
or strings within each the &lt;head&gt; &lt;/head&gt; tags); 
see <code>delete.markup</code> for further details.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_corpus.lang">corpus.lang</code></td>
<td>
<p>an optional argument indicating the language of the texts 
analyzed; the values that will affect the function's behavior are: 
<code>English.contr</code>, <code>English.all</code>, <code>Latin.corr</code> (type 
<code>help(txt.to.words.ext)</code> for explanation). The default value 
is <code>English</code>.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_splitting.rule">splitting.rule</code></td>
<td>
<p>if you are not satisfied with the default language
settings (or your input string of characters is not a regular text,
but a sequence of, say, dance movements represented using symbolic signs),
you can indicate your custom splitting regular expression here. This
option will overwrite the above language settings. For further details,
refer to <code>help(txt.to.words)</code>.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_sample.size">sample.size</code></td>
<td>
<p>desired size of samples, expressed in number of words;
default value is 10,000.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_sampling">sampling</code></td>
<td>
<p>one of three values: <code>no.sampling</code> (default), 
<code>normal.sampling</code>, <code>random.sampling</code>. See <code>make.samples</code>
for explanation.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_sample.overlap">sample.overlap</code></td>
<td>
<p>if this opion is used, a reference text is segmented 
into consecutive, equal-sized samples that are allowed to partially 
overlap. If one specifies the <code>sample.size</code> parameter of 5,000 and 
the <code>sample.overlap</code> of 1,000, for example, the first sample of a text 
contains words 1&ndash;5,000, the second 4001&ndash;9,000, the third sample 8001&ndash;13,000, 
and so forth.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_number.of.samples">number.of.samples</code></td>
<td>
<p>optional argument which will be used only if 
<code>random.sampling</code> was chosen; it is self-evident.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_sampling.with.replacement">sampling.with.replacement</code></td>
<td>
<p>optional argument which will be used only
if <code>random.sampling</code> was chosen; it specifies the method used to 
randomly harvest words from texts.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_features">features</code></td>
<td>
<p>an option for specifying the desired type of features:
<code>w</code> for words, <code>c</code> for characters (default: <code>w</code>). See
<code>txt.to.features</code> for further details.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_ngram.size">ngram.size</code></td>
<td>
<p>an optional argument (integer) specifying the value of <em>n</em>, 
or the size of n-grams to be produced. If this argument is missing, 
the default value of 1 is used. See <code>txt.to.features</code> for further 
details.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_preserve.case">preserve.case</code></td>
<td>
<p>whether ot not to lowercase all characters in the corpus 
(default = F).</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_encoding">encoding</code></td>
<td>
<p>useful if you use Windows and non-ASCII alphabets: French,
Polish, Hebrew, etc. In such a situation, it is quite convenient to 
convert your text files into Unicode and to set this option to
<code>encoding = "UTF-8"</code>. In Linux and Mac, you are always expected
to use Unicode, thus you don't need to set anything. In Windows, 
consider using UTF-8 but don't forget about the way of analyzing native 
ANSI encoded files: set this option to <code>encoding = "native.enc"</code>.</p>
</td></tr>
<tr><td><code id="load.corpus.and.parse_+3A_...">...</code></td>
<td>
<p>option not used; introduced here for compatibility reasons.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.corpus</code>. It is a list
containing as elements the samples (entire texts or sampled subsets) split into 
words/characters and combined into n-grams (if applicable).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus">load.corpus</a></code>, <code><a href="#topic+delete.markup">delete.markup</a></code>, 
<code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>, 
<code><a href="#topic+txt.to.features">txt.to.features</a></code>, <code><a href="#topic+make.samples">make.samples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# to load file1.txt and file2.txt, stored in the subdirectory my.files:
my.corpus = load.corpus.and.parse(files = c("file1.txt", "file2.txt"),
                        corpus.dir = "my.files")

# to load all XML files from the current directory, while getting rid of
# all markup tags in the file, and split the texts into consecutive 
# word pairs (2-grams):
my.corpus = load.corpus.and.parse(files = list.files(pattern = "[.]xml$"),
                        markup.type = "xml", ngram.size = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='make.frequency.list'>Make List of the Most Frequent Elements (e.g. Words)</h2><span id='topic+make.frequency.list'></span>

<h3>Description</h3>

<p>Function for generating a frequency list of words or other (linguistic)
features. It basically counts the elements of a vector and returns a vector
of these elements in descending order of frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.frequency.list(data, value = FALSE, head = NULL, relative = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.frequency.list_+3A_data">data</code></td>
<td>
<p>either a vector of elements (e.g. words, letter n-grams),
or an object of a class <code>stylo.corpus</code> as produced by the 
function <code>load.corpus.and.parse</code>.</p>
</td></tr>
<tr><td><code id="make.frequency.list_+3A_value">value</code></td>
<td>
<p>if this function is switched on, not only the most frequent
elements are returned, but also their frequencies. Default: FALSE.</p>
</td></tr>
<tr><td><code id="make.frequency.list_+3A_head">head</code></td>
<td>
<p>this option is meant to limit the number of the most frequent
features to be returned. Default value is NULL, which means
that the entire range of frequent and unfrequent features is 
returned.</p>
</td></tr>
<tr><td><code id="make.frequency.list_+3A_relative">relative</code></td>
<td>
<p>if you've switched on the option <code>value</code> (see above),
you might want to convert your frequencies into relative frequencies,
i.e. the counted occurrences divided by the length of the input 
vector &ndash; in a vast majority of cases you should use it, in order 
to neutralize different sample sizes. Default: TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of features (usually, words) in a descending
order of their frequency. Alternatively, when the option <code>value</code> is set
TRUE, it returns a vector of frequencies instead, and the features themselves
might be accessed using the generic <code>names</code> function.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus.and.parse">load.corpus.and.parse</a></code>, <code><a href="#topic+make.table.of.frequencies">make.table.of.frequencies</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># assume there is a text:
text = "Mr. Sherlock Holmes, who was usually very late in the mornings, 
       save upon those not infrequent occasions when he was up all night, 
       was seated at the breakfast table. I stood upon the hearth-rug and 
       picked up the stick which our visitor had left behind him the night 
       before. It was a fine, thick piece of wood, bulbous-headed, of the 
       sort which is known as a \"Penang lawyer.\""

# this text can be converted into vector of words:
words = txt.to.words(text)

# an avanced tokenizer is available via the function 'txt.to.words.ext':
words2 = txt.to.words.ext(text, corpus.lang = "English.all")

# a frequency list (just words):
make.frequency.list(words)
make.frequency.list(words2)

# a frequency list with the numeric values
make.frequency.list(words2, value = TRUE)

## Not run: 
# #####################################
# using the function with large text collections

# first, load and pre-process a corpus from 3 text files:
dataset = load.corpus.and.parse(files = c("1.txt", "2.txt", "3.txt"))
#       
# then, return 100 the most frequent words of the entire corpus:
make.frequency.list(dataset, head = 100)

## End(Not run)
</code></pre>

<hr>
<h2 id='make.ngrams'>Make text n-grams</h2><span id='topic+make.ngrams'></span>

<h3>Description</h3>

<p>Function that combines a vector of text units (words, 
characters, POS-tags, other features) into pairs, triplets, or 
longer sequences, commonly referred to as n-grams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.ngrams(input.text, ngram.size = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.ngrams_+3A_input.text">input.text</code></td>
<td>
<p>a vector containing words or characters to be parsed
into n-grams.</p>
</td></tr>
<tr><td><code id="make.ngrams_+3A_ngram.size">ngram.size</code></td>
<td>
<p>an optional argument (integer) indicating the value 
of <em>n</em>, or the size of n-grams to be produced. If this argument 
is missing, default value of 1 is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function for combining series of items (e.g. words or characters) into n-grams, 
or strings of <em>n</em> elements. E.g. character 2-grams of the sentence &quot;This is 
a sentence&quot; are as follows: &quot;th&quot;, &quot;hi&quot;, &quot;is&quot;, &quot;s &quot;, &quot; i&quot;, &quot;is&quot;, &quot;s &quot;, 
&quot; a&quot;, &quot;a &quot;, &quot; s&quot;, &quot;se&quot;, &quot;en&quot;, &quot;nt&quot;, &quot;te&quot;, &quot;en&quot;, &quot;nc&quot;, &quot;ce&quot;. Character
4-grams would be, of course: &quot;this&quot;, &quot;his &quot;, &quot;is a&quot;, &quot;s a &quot;, &quot; a s&quot;, etc.
Word 2-grams: &quot;this is&quot;, &quot;is a&quot;, &quot;a sentence&quot;. The issue whether using 
n-grams of items increases the accuracy of stylometric procedures has been 
heavily debated in the secondary literature (see the reference section 
for further reading). Eder (2013) e.g. shows that character n-grams 
are suprisingly robust for dealing with noisy corpora (in terms of a high 
number of misspelled characters).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Alexis, A., Craig, H., and Elliot, J. (2014). Language chunking, 
data sparseness, and the value of a long marker list: explorations with 
word n-grams and authorial attribution. &quot;Literary and Linguistic
Computing&quot;, 29, advanced access (doi: 10.1093/llc/fqt028).
</p>
<p>Eder, M. (2011). Style-markers in authorship attribution: a cross-language 
study of the authorial fingerprint. &quot;Studies in Polish Linguistics&quot;, 
6: 99-114. <a href="https://www.ejournals.eu/SPL/2011/SPL-vol-6-2011/">https://www.ejournals.eu/SPL/2011/SPL-vol-6-2011/</a>.
</p>
<p>Eder, M. (2013). Mind your corpus: systematic errors in authorship
attribution. &quot;Literary and Linguistic Computing&quot;, 28(4): 603-14.
</p>
<p>Hoover, D. L. (2002). Frequent word sequences and statistical stylistics.
&quot;Literary and Linguistic Computing&quot;, 17: 157-80.
</p>
<p>Hoover, D. L. (2003). Frequent collocations and authorial style.
&quot;Literary and Linguistic Computing&quot;, 18: 261-86.
</p>
<p>Hoover, D. L. (2012). The rarer they are, the more they are, the less
they matter. In: Digital Humanities 2012: Conference Abstracts,
Hamburg University, Hamburg, pp. 218-21.
</p>
<p>Koppel, M., Schler, J. and Argamon, S. (2009). Computational methods
in authorship attribution. &quot;Journal of the American Society for 
Information Science and Technology&quot;, 60(1): 9-26.
</p>
<p>Stamatatos, E. (2009). A survey of modern authorship attribution methods.
&quot;Journal of the American Society for Information Science and Technology&quot;, 
60(3): 538-56.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>,  
<code><a href="#topic+txt.to.features">txt.to.features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Consider the string my.text:
my.text = "Quousque tandem abutere, Catilina, patientia nostra?"
# which can be split into a vector of consecutive words:
my.vector.of.words = txt.to.words(my.text)
# now, we create a vector of word 2-grams:
make.ngrams(my.vector.of.words, ngram.size = 2)

# similarly, you can produce character n-grams:
my.vector.of.chars = txt.to.features(my.vector.of.words, features = "c")
make.ngrams(my.vector.of.chars, ngram.size = 4)
</code></pre>

<hr>
<h2 id='make.samples'>Split text to samples</h2><span id='topic+make.samples'></span>

<h3>Description</h3>

<p>Function that either splits an input text (a vector of linguistic 
items, such as words, word n-grams, character n-grams, etc.) into 
equal-sized samples of a desired length (expressed in words), 
or excerpts randomly a number of words from the original text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.samples(tokenized.text, sample.size = 10000, 
             sampling = "no.sampling", sample.overlap = 0,
             number.of.samples = 1, sampling.with.replacement = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.samples_+3A_tokenized.text">tokenized.text</code></td>
<td>
<p>input textual data stored either in a form of vector
(single text), or as a list of vectors (whole corpus); 
particular vectors should contain tokenized data, i.e. words, 
word n-grams, or other features, as elements.</p>
</td></tr>
<tr><td><code id="make.samples_+3A_sample.size">sample.size</code></td>
<td>
<p>desired size of sample expressed in number of words;
default value is 10,000.</p>
</td></tr>
<tr><td><code id="make.samples_+3A_sampling">sampling</code></td>
<td>
<p>one of three values: <code>no.sampling</code> (default), 
<code>normal.sampling</code>, <code>random.sampling</code>.</p>
</td></tr>
<tr><td><code id="make.samples_+3A_sample.overlap">sample.overlap</code></td>
<td>
<p>if this opion is used, a reference text is segmented 
into consecutive, equal-sized samples that are allowed to partially 
overlap. If one specifies the <code>sample.size</code> parameter of 5,000 and 
the <code>sample.overlap</code> of 1,000, for example, the first sample of a text 
contains words 1&ndash;5,000, the second 4001&ndash;9,000, the third sample 8001&ndash;13,000, 
and so forth.</p>
</td></tr>
<tr><td><code id="make.samples_+3A_number.of.samples">number.of.samples</code></td>
<td>
<p>optional argument which will be used only if 
<code>random.sampling</code> was chosen; it is self-evident.</p>
</td></tr>
<tr><td><code id="make.samples_+3A_sampling.with.replacement">sampling.with.replacement</code></td>
<td>
<p>optional argument which will be used only 
if <code>random.sampling</code> was chosen; it specifies the method to randomly 
harvest words from texts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Normal sampling is probably a good choice when the input texts are 
long: the advantage is that one gets a bigger number of samples which,
in a way, validate the results (when several independent samples excerpted
from one text are clustered together).
When the analyzed texts are significantly unequal in length, it is not 
a bad idea to prepare samples as randomly chosen &quot;bags of words&quot;. For this, 
set the <code>sampling</code> variable to <code>random.sampling</code>. The desired 
size of the sample should be specified via the <code>sample.size</code> variable.
Sampling with and without replacement is also available. It has been shown
by Eder (2010) that harvesting random samples from original texts improves
the performance of authorship attribution methods.
</p>


<h3>Author(s)</h3>

<p>Mike Kestemont, Maciej Eder
</p>


<h3>References</h3>

<p>Eder, M. (2015). Does size matter? Authorship attribution, small samples, 
big problem. &quot;Digital Scholarship in the Humanities&quot;, 30(2): 167-182.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>,
<code><a href="#topic+txt.to.features">txt.to.features</a></code>,  <code><a href="#topic+make.ngrams">make.ngrams</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my.text = "Arma virumque cano, Troiae qui primus ab oris
           Italiam fato profugus Laviniaque venit
           litora, multum ille et terris iactatus et alto
           vi superum, saevae memorem Iunonis ob iram,
           multa quoque et bello passus, dum conderet urbem
           inferretque deos Latio; genus unde Latinum
           Albanique patres atque altae moenia Romae.
           Musa, mihi causas memora, quo numine laeso
           quidve dolens regina deum tot volvere casus
           insignem pietate virum, tot adire labores
           impulerit. tantaene animis caelestibus irae?"
my.words = txt.to.words(my.text)

# split the above text into samples of 20 words:
make.samples(my.words, sampling = "normal.sampling", sample.size = 20)

# excerpt randomly 50 words from the above text:
make.samples(my.words, sampling = "random.sampling", sample.size = 50)

# excerpt 5 random samples from the above text:
make.samples(my.words, sampling = "random.sampling", sample.size = 50,
             number.of.samples = 5)
</code></pre>

<hr>
<h2 id='make.table.of.frequencies'>Prepare a table of (relative) word frequencies</h2><span id='topic+make.table.of.frequencies'></span>

<h3>Description</h3>

<p>Function that collects several frequency lists and combines them 
into a single frequency table. To this end a number of rearrangements 
inside particular lists are carried out. The table is produced using 
a reference list of words/features (passed as an argument).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.table.of.frequencies(corpus, features, absent.sensitive = TRUE, 
                          relative = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.table.of.frequencies_+3A_corpus">corpus</code></td>
<td>
<p>textual data: either a corpus (represented as a list), or 
a single text (represented as a vector); the data have to be split into words
(or other features, such as character n-grams or word pairs).</p>
</td></tr>
<tr><td><code id="make.table.of.frequencies_+3A_features">features</code></td>
<td>
<p>a vector containing a reference feature list that will be used
to build the table of frequencies (it is assumed that the reference list 
contains the same type of features as the <code>corpus</code> list, e.g. words, 
character n-grams, word pairs, etc.; otherwise, an empty table will be build).</p>
</td></tr>
<tr><td><code id="make.table.of.frequencies_+3A_absent.sensitive">absent.sensitive</code></td>
<td>
<p>this optional argument is used to prevent building 
tables of words/features that never occur in the corpus. When switched on 
(default), variables containing 0 values across all samples, will be excluded. 
However, in some cases this is important to keep all the variables regardless 
of their values. This is e.g. the case when comparing two corpora: even if 
a given word did not occur in corpus A, it might be present in corpus B. 
In short: whenever you perform any analysis involving two or multiple sets 
of texts, switch this option to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="make.table.of.frequencies_+3A_relative">relative</code></td>
<td>
<p>when this argument is switched to <code>TRUE</code> (default), 
relative frequencies are computed instead of raw frequencies.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus">load.corpus</a></code>, <code><a href="#topic+load.corpus.and.parse">load.corpus.and.parse</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to get frequencies of the words "a", "the" and "of" from a text:

sample.txt = txt.to.words("My father had a small estate 
             in Nottinghamshire: I was the third of five sons.")
make.table.of.frequencies(sample.txt, c("a", "the", "of"))



# to get a table of frequencies across several texts:

txt.1 = "Gallia est omnis divisa in partes tres, quarum unam incolunt 
    Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra 
    Galli appellantur."
txt.2 = "Si quis antea, iudices, mirabatur quid esset quod, pro tantis 
    opibus rei publicae tantaque dignitate imperi, nequaquam satis multi 
    cives forti et magno animo invenirentur qui auderent se et salutem 
    suam in discrimen offerre pro statu civitatis et pro communi 
    libertate, ex hoc tempore miretur potius si quem bonum et fortem 
    civem viderit, quam si quem aut timidum aut sibi potius quam rei 
    publicae consulentem."
txt.3 = "Nam mores et instituta vitae resque domesticas ac familiaris 
    nos profecto et melius tuemur et lautius, rem vero publicam nostri 
    maiores certe melioribus temperaverunt et institutis et legibus."
my.corpus.raw = list(txt.1, txt.2, txt.3)
my.corpus.clean = lapply(my.corpus.raw, txt.to.words)
my.favorite.words = c("et", "in", "se", "rara", "avis")
make.table.of.frequencies(my.corpus.clean, my.favorite.words)


# to include all words in the reference list, no matter if they 
# occurred in the corpus:
make.table.of.frequencies(my.corpus.clean, my.favorite.words,
    absent.sensitive=FALSE)


# to prepare a table of frequencies of all the words represented in 
# a corpus, in descendent occurence order, one needs to make the frequency
# list first, via the function 'make.frequency.list'
complete.word.list = make.frequency.list(my.corpus.clean)
make.table.of.frequencies(my.corpus.clean, complete.word.list)


# to create a table of frequencies of word pairs (word 2-grams):
my.word.pairs = lapply(my.corpus.clean, txt.to.features, ngram.size=2)
make.table.of.frequencies(my.word.pairs, c("et legibus", "hoc tempore"))
</code></pre>

<hr>
<h2 id='novels'>
A selection of 19th-century English novels
</h2><span id='topic+novels'></span>

<h3>Description</h3>

<p>This dataset contains a selection of 9 novels in English, written by Jane Austen 
(&quot;Emma&quot;, &quot;Pride and Prejudice&quot;, &quot;Sense and Sensibility&quot;), Anne Bronte 
(&quot;Agnes Grey&quot;, &quot;The Tenant of Wildfell Hall&quot;), Charlotte Bronte (&quot;Jane Eyre&quot;, 
&quot;The Professor&quot;, &quot;Villette&quot;), and Emily Bronte (&quot;Wuthering Heights&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("novels")</code></pre>


<h3>Details</h3>

<p>The novels are represented as elements of a class <code>stylo.corpus</code>, i.e. 
a list containing particular texts as its elements. The texts are not tokenized.
</p>


<h3>Source</h3>

<p>The texts are harvested from open-access resources, e.g. the Gutenberg Project.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(novels)

print(novels)
summary(novels)

</code></pre>

<hr>
<h2 id='oppose'>Contrastive analysis of texts</h2><span id='topic+oppose'></span>

<h3>Description</h3>

<p>Function that performs a contrastive analysis between two given sets 
of texts. It generates a list of words significantly preferred by 
a tested author (or, a collection of authors), and another list containing 
the words significantly avoided by the former when compared to another set 
of texts. Some visualizations are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oppose(gui = TRUE, path = NULL, 
         primary.corpus = NULL,
         secondary.corpus = NULL,
         test.corpus = NULL,
         primary.corpus.dir = "primary_set",
         secondary.corpus.dir = "secondary_set",
         test.corpus.dir = "test_set", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oppose_+3A_gui">gui</code></td>
<td>
<p>an optional argument; if switched on, a simple yet effective 
graphical interface (GUI) will appear. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="oppose_+3A_path">path</code></td>
<td>
<p>if not specified, the current working directory will be used 
for input/output procedures (reading files, outputting the results, etc.).</p>
</td></tr>
<tr><td><code id="oppose_+3A_primary.corpus.dir">primary.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working 
directory) that contains one or more texts to be compared to a comparison 
corpus. These texts can e.g. be the oeuvre by author A (to be compared 
to the oeuvre of another author B) or a collection of texts by female 
authors (to be contrasted with texts by male authors). If not specified, 
the default subdirectory <code>primary_set</code> will be used.</p>
</td></tr>
<tr><td><code id="oppose_+3A_secondary.corpus.dir">secondary.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working 
directory) that contains a comparison corpus: a pool of texts to be 
contrasted with texts from the <code>primary.corpus</code>. If not specified, 
the default subdirectory <code>secondary_set</code> will be used.</p>
</td></tr>
<tr><td><code id="oppose_+3A_test.corpus.dir">test.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working directory)
that contains texts to verify the discriminatory strength of the features
extracted from the <code>primary.set</code> and <code>secondary.sets</code>. Ideally, 
the <code>test.corpus.dir</code> should contain texts known to belong to both 
classes (e.g. texts written by female and male authors in the case of 
a gender-oriented study). If not specified, the default subdirectory 
<code>test_set</code> will be used. If the default subdirectory does not exist 
or does not contain any texts, the validation test will not be performed.</p>
</td></tr>
<tr><td><code id="oppose_+3A_primary.corpus">primary.corpus</code></td>
<td>
<p>another option is to pass a pre-processed corpus
as an argument (here: the primary set). It is assumed that this object 
is a list, each element of which is a vector containing one tokenized 
sample. Refer to <code>help(load.corpus.and.parse)</code> to get some hints 
how to prepare such a corpus.</p>
</td></tr>
<tr><td><code id="oppose_+3A_secondary.corpus">secondary.corpus</code></td>
<td>
<p>if <code>primary.corpus</code> is used, then you should also 
prepare a similar R object containing the secondary set.</p>
</td></tr>
<tr><td><code id="oppose_+3A_test.corpus">test.corpus</code></td>
<td>
<p>if you decide to use test corpus, you can pass it as 
a pre-processed R object using this argument.</p>
</td></tr>
<tr><td><code id="oppose_+3A_...">...</code></td>
<td>
<p>any variable produced by <code>stylo.default.settings</code> can be set
here, in order to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a contrastive analysis between two given sets of texts, 
using Burrows's Zeta (2007) in its different flavors, including Craig's 
extensions (Craig and Kinney, 2009). Also, the Whitney-Wilcoxon procedure
as introduced by Kilgariff (2001) is available. The function generates 
a vector of words significantly preferred by a tested author, and another 
vector containing the words significantly avoided.
</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including a list of words significantly preferred in the 
primary set, words significantly avoided (or, preferred in the secondary set), 
and possibly some other results, if applicable.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont</p>


<h3>References</h3>

<p>Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package 
for computational text analysis. &quot;R Journal&quot;, 8(1): 107-21.
</p>
<p>Burrows, J. F. (2007). All the way through: testing for authorship 
in different frequency strata. &quot;Literary and Linguistic Computing&quot;, 
22(1): 27-48.
</p>
<p>Craig, H. and Kinney, A. F., eds. (2009). Shakespeare, Computers, and the 
Mystery of Authorship. Cambridge: Cambridge University Press.
</p>
<p>Hoover, D. (2010). Teasing out authorship and style with t-tests and
Zeta. In: &quot;Digital Humanities 2010: Conference Abstracts&quot;. 
King's College London, pp. 168-170.
</p>
<p>Kilgariff A. (2001). Comparing Corpora. &quot;International Journal of Corpus
Linguistics&quot; 6(1): 1-37.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+rolling.classify">rolling.classify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage:
oppose()

# batch mode, custom name of corpus directories:
oppose(gui = FALSE, primary.corpus.dir = "ShakespeareCanon",
       secondary.corpus.dir = "MarloweSamples")

## End(Not run)
</code></pre>

<hr>
<h2 id='parse.corpus'>Perform pre-processing (tokenization, n-gram extracting, etc.)</h2><span id='topic+parse.corpus'></span>

<h3>Description</h3>

<p>A high-level function that controls a number of other functions responsible 
for dealing with a raw corpus stored as list, including deleting markup, 
sampling from texts, converting samples to n-grams, etc. It is build on top 
of a number of functions  and thus it requires a large number of arguments. 
The only obligatory argument, however, is an R object containing a raw corpus:
it is either an object of the class <code>sylo.corpus</code>, or a list of vectors, 
their elements being particular texts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse.corpus(input.data, markup.type = "plain",
                      corpus.lang = "English", splitting.rule = NULL,
                      sample.size = 10000, sampling = "no.sampling",
                      sample.overlap = 0, number.of.samples = 1,
                      sampling.with.replacement = FALSE, features = "w", 
                      ngram.size = 1, preserve.case = FALSE,
                      encoding = "UTF-8")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse.corpus_+3A_input.data">input.data</code></td>
<td>
<p>a list (preferably of the class <code>stylo.corpus</code>)
containing a raw corpus, i.e. a vector of texts.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_markup.type">markup.type</code></td>
<td>
<p>choose one of the following values: <code>plain</code> 
(nothing will happen), <code>html</code> (all tags will be deleted as well 
as HTML header), <code>xml</code> (TEI header, any text between &lt;note&gt; &lt;/note&gt; 
tags, and all the tags will be deleted), <code>xml.drama</code> (as above;
additionally, speaker's names will be deleted, or strings within the 
&lt;speaker&gt; &lt;/speaker&gt; tags), <code>xml.notitles</code> (as above; but, 
additionally, all the chapter/section (sub)titles will be deleted, 
or strings within each the &lt;head&gt; &lt;/head&gt; tags); 
see <code>delete.markup</code> for further details.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_corpus.lang">corpus.lang</code></td>
<td>
<p>an optional argument indicating the language of the texts 
analyzed; the values that will affect the function's behavior are: 
<code>English.contr</code>, <code>English.all</code>, <code>Latin.corr</code> (type 
<code>help(txt.to.words.ext)</code> for explanation). The default value 
is <code>English</code>.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_splitting.rule">splitting.rule</code></td>
<td>
<p>if you are not satisfied with the default language
settings (or your input string of characters is not a regular text,
but a sequence of, say, dance movements represented using symbolic signs),
you can indicate your custom splitting regular expression here. This
option will overwrite the above language settings. For further details,
refer to <code>help(txt.to.words)</code>.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_sample.size">sample.size</code></td>
<td>
<p>desired size of samples, expressed in number of words;
default value is 10,000.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_sampling">sampling</code></td>
<td>
<p>one of three values: <code>no.sampling</code> (default), 
<code>normal.sampling</code>, <code>random.sampling</code>. See <code>make.samples</code>
for explanation.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_sample.overlap">sample.overlap</code></td>
<td>
<p>if this opion is used, a reference text is segmented 
into consecutive, equal-sized samples that are allowed to partially 
overlap. If one specifies the <code>sample.size</code> parameter of 5,000 and 
the <code>sample.overlap</code> of 1,000, for example, the first sample of a text 
contains words 1&ndash;5,000, the second 4001&ndash;9,000, the third sample 8001&ndash;13,000, 
and so forth.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_number.of.samples">number.of.samples</code></td>
<td>
<p>optional argument which will be used only if 
<code>random.sampling</code> was chosen; it is self-evident.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_sampling.with.replacement">sampling.with.replacement</code></td>
<td>
<p>optional argument which will be used only
if <code>random.sampling</code> was chosen; it specifies the method used to 
randomly harvest words from texts.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_features">features</code></td>
<td>
<p>an option for specifying the desired type of features:
<code>w</code> for words, <code>c</code> for characters (default: <code>w</code>). See
<code>txt.to.features</code> for further details.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_ngram.size">ngram.size</code></td>
<td>
<p>an optional argument (integer) specifying the value of <em>n</em>, 
or the size of n-grams to be produced. If this argument is missing, 
the default value of 1 is used. See <code>txt.to.features</code> for further 
details.</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_preserve.case">preserve.case</code></td>
<td>
<p>whether ot not to lowercase all characters in the corpus 
(default = F).</p>
</td></tr>
<tr><td><code id="parse.corpus_+3A_encoding">encoding</code></td>
<td>
<p>useful if you use Windows and non-ASCII alphabets: French,
Polish, Hebrew, etc. In such a situation, it is quite convenient to 
convert your text files into Unicode and to set this option to
<code>encoding = "UTF-8"</code>. In Linux and Mac, you are always expected
to use Unicode, thus you don't need to set anything.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.corpus</code>. It is a list
containing as elements the samples (entire texts or sampled subsets) split into 
words/characters and combined into n-grams (if applicable).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus.and.parse">load.corpus.and.parse</a></code>, <code><a href="#topic+delete.markup">delete.markup</a></code>, 
<code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>, 
<code><a href="#topic+txt.to.features">txt.to.features</a></code>, <code><a href="#topic+make.samples">make.samples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(novels)
# depending on the size of the corpus, it might take a while:
parse.corpus(novels)

## End(Not run)
</code></pre>

<hr>
<h2 id='parse.pos.tags'>Extract POS-tags or Words from Annotated Corpora</h2><span id='topic+parse.pos.tags'></span>

<h3>Description</h3>

<p>Function for extracting textual data from annotated corpora.
It uderstands Stanford Tagger, TreeTagger TaKIPI (a tagger for Polish),
and Alpino (a tagger for Dutch) output formats. Either part-of-speech tags, 
or words, or lemmata can be extracted.</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse.pos.tags(input.text, tagger = "stanford", feature = "pos")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse.pos.tags_+3A_input.text">input.text</code></td>
<td>
<p>any string of characters (e.g. vector) containing markup 
tags that have to be deleted.</p>
</td></tr>
<tr><td><code id="parse.pos.tags_+3A_tagger">tagger</code></td>
<td>
<p>choose the input format: &quot;stanford&quot; for Stanford Tagger, 
&quot;treetagger&quot; for TreeTagger, &quot;takipi&quot; for TaKIPI.</p>
</td></tr>
<tr><td><code id="parse.pos.tags_+3A_feature">feature</code></td>
<td>
<p>choose &quot;pos&quot; (default), &quot;word&quot;, or &quot;lemma&quot; (this one is not
available for the Stanford-formatted input).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the function is applied to a single text, then a vector of extracted
features is returned. If it is applied to a corpus (a list, preferably
of a class &quot;stylo.corpus&quot;), then a list of preprocessed texts are returned.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+load.corpus">load.corpus</a></code>, <code><a href="#topic+txt.to.words">txt.to.words</a></code>, 
<code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>, <code><a href="#topic+txt.to.features">txt.to.features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = "I_PRP have_VBP just_RB returned_VBN from_IN a_DT visit_NN 
  to_TO my_PRP$ landlord_NN -_: the_DT solitary_JJ neighbor_NN  that_IN 
  I_PRP shall_MD be_VB troubled_VBN with_IN ._. This_DT is_VBZ certainly_RB 
  a_DT beautiful_JJ country_NN !_. In_IN all_DT England_NNP ,_, I_PRP do_VBP 
  not_RB believe_VB that_IN I_PRP could_MD have_VB fixed_VBN on_IN a_DT 
  situation_NN so_RB completely_RB removed_VBN from_IN the_DT stir_VB of_IN 
  society_NN ._."

parse.pos.tags(text, tagger = "stanford", feature = "word")
parse.pos.tags(text, tagger = "stanford", feature = "pos")
  
</code></pre>

<hr>
<h2 id='perform.culling'>Exclude variables (e.g. words, n-grams) from a frequency table that are 
too characteristic for some samples</h2><span id='topic+perform.culling'></span>

<h3>Description</h3>

<p>Culling refers to the automatic manipulation of the wordlist 
(proposed by Hoover 2004a, 2004b). The culling values specify the degree 
to which words that do not appear in all the texts of a corpus will be removed. 
A culling value of 20 indicates that words that appear in at least 20% of 
the texts in the corpus will be considered in the analysis. A culling 
setting of 0 means that no words will be removed; a culling setting of 100 
means that only those words will be used in the analysis that appear 
in all texts of the corpus at least once.</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.culling(input.table, culling.level = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.culling_+3A_input.table">input.table</code></td>
<td>
<p>a matrix or data frame containing frequencies of words 
or any other countable features; the table should be oriented to contain 
samples in rows, variables in columns, and variables' names should be 
accessible via <code>colnames(input.table)</code>.</p>
</td></tr>
<tr><td><code id="perform.culling_+3A_culling.level">culling.level</code></td>
<td>
<p>percentage of samples that need to have a given word 
in order to prevent this word from being culled (see the description above).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

   
<p>Hoover, D. (2004a). Testing Burrows's Delta. &quot;Literary and Linguistic 
Computing&quot;, 19(4): 453-75.
</p>
<p>Hoover, D. (2004b). Delta prime. &quot;Literary and Linguistic Computing&quot;,
19(4): 477-95.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+delete.stop.words">delete.stop.words</a></code>, <code><a href="#topic+stylo.pronouns">stylo.pronouns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># assume there is a matrix containing some frequencies
# (be aware that these counts are entirely fictional):
t1 = c(2, 1, 0, 2, 9, 1, 0, 0, 2, 0)
t2 = c(1, 0, 4, 2, 1, 0, 3, 0, 1, 3)
t3 = c(5, 2, 2, 0, 6, 0, 1, 0, 0, 0)
t4 = c(1, 4, 1, 0, 0, 0, 0, 3, 0, 1)
my.data.table = rbind(t1, t2, t3, t4)

# names of the samples:
rownames(my.data.table) = c("text1", "text2", "text3", "text4")
# names of the variables (e.g. words):
colnames(my.data.table) = c("the", "of", "in", "she", "me", "you",
                                    "them", "if", "they", "he")
# the table looks as follows
print(my.data.table)

# selecting the words that appeared in at laest 50% of samples:
perform.culling(my.data.table, 50)        

</code></pre>

<hr>
<h2 id='perform.delta'>Distance-based classifier</h2><span id='topic+perform.delta'></span>

<h3>Description</h3>

<p>Delta: a simple yet effective machine-learning method of supervised classification,
introduced by Burrows (2002). It computes a table of distances between samples, 
and compares each sample from the test set against training samples, in order 
to find its nearest neighbor. Apart from classic Delta, a number of alternative 
distance measures are supported by this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.delta(training.set, test.set, 
              classes.training.set = NULL, 
              classes.test.set = NULL, 
              distance = "delta", no.of.candidates = 3, 
              z.scores.both.sets = TRUE) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.delta_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set.</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_distance">distance</code></td>
<td>
<p>a kernel (i.e. a distance measure) used for computing 
similarities between texts. Available options so far: &quot;delta&quot; 
(Burrow's Delta, default), &quot;argamon&quot; (Argamon's Linear Delta), 
&quot;eder&quot; (Eder's Delta), &quot;simple&quot; (Eder's Simple Distance), 
&quot;canberra&quot; (Canberra Distance), &quot;manhattan&quot; (Manhattan 
Distance), &quot;euclidean&quot; (Euclidean Distance), &quot;cosine&quot;
(Cosine Distance).</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_no.of.candidates">no.of.candidates</code></td>
<td>
<p>how many nearest neighbors will be computed for
each test sample (default = 3).</p>
</td></tr>
<tr><td><code id="perform.delta_+3A_z.scores.both.sets">z.scores.both.sets</code></td>
<td>
<p>many distance measures convert input variables into 
z-scores before computing any distances. Such a variable
weighting is highly dependent on the number of input texts. One might
choose either training set only to scale the variables, or the entire
corpus (both sets). The latter is default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of &quot;guessed&quot; classes: each test sample is
linked with one of the classes represented in the training set. Additionally,
final scores and final rankings of candidates are returned as attributes.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Argamon, S. (2008). Interpreting Burrows's Delta: geometric and 
probabilistic foundations. &quot;Literary and Linguistic Computing&quot;, 
23(2): 131-47. 
</p>
<p>Burrows, J. F. (2002). &quot;Delta&quot;: a measure of stylistic difference and 
a guide to likely authorship. &quot;Literary and Linguistic Computing&quot;, 
17(3): 267-87.
</p>
<p>Jockers, M. L. and Witten, D. M. (2010). A comparative study of machine 
learning methods for authorship attribution. &quot;Literary and Linguistic
Computing&quot;, 25(2): 215-23.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.svm">perform.svm</a></code>, <code><a href="#topic+perform.nsc">perform.nsc</a></code>,  
<code><a href="#topic+perform.knn">perform.knn</a></code>, <code><a href="#topic+perform.naivebayes">perform.naivebayes</a></code>, 
<code><a href="#topic+dist.delta">dist.delta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
perform.delta(training.set, test.set)

## End(Not run)

# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

perform.delta(train, test, train.classes, test.classes)
</code></pre>

<hr>
<h2 id='perform.impostors'>An Authorship Verification Classifier Known as the Impostors Method. ATTENTION:
this function is obsolete; refer to a new implementation, aka the imposters() function!</h2><span id='topic+perform.impostors'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier tailored to assess authorship
verification tasks. This function is an implementation of the 2nd order
verification system known as the General Impostors framework (GI), 
and introduced by Koppel and Winter (2014). The current implementation 
tries to stick &ndash; as closely as possible &ndash; to the description provided 
by Kestemont et al. (2016: 88).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.impostors(candidate.set, impostors.set, iterations = 100,
               features = 50, impostors = 30,
               classes.candidate.set = NULL, classes.impostors.set = NULL,
               distance = "delta", z.scores.both.sets = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.impostors_+3A_candidate.set">candidate.set</code></td>
<td>
<p>a table containing	frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of texts
written by a target author (i.e. the candidate to authorship). 
This table should also contain an anonymous sample to be assessed.
Make sure that the rows contain samples, and the columns &ndash; 
variables (words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_impostors.set">impostors.set</code></td>
<td>
<p>a table containing frequencies/counts for the control set.
This set should contain the samples by the impostors, or the 
authors that could not have written the anonymous sample in question.
The variables used (i.e. columns) must match the columns of the 
candidate set.</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_iterations">iterations</code></td>
<td>
<p>the model is rafined in N iterations. A reasonable number 
of turns is a few dozen or so (see the argument &quot;features&quot; below).</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_features">features</code></td>
<td>
<p>the &quot;impostors&quot; method is sometimes referred to as a 2nd order
authorship verification system, since it selects randomly, in N
iterations, a given subset of features (words, n-grams, etc.)
and performs a classification. This argument specifies the
percentage of features to be randomly chosen; the default value
is 50.</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_impostors">impostors</code></td>
<td>
<p>in each iteration, a specified number of texts from the 
control set is chosen (randomly). The default number is 30.</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_classes.candidate.set">classes.candidate.set</code></td>
<td>
<p>a vector containing class identifiers for the
authorial set. When missing, the row names of the set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_classes.impostors.set">classes.impostors.set</code></td>
<td>
<p>a vector containing class identifiers for the
control set. When missing, the row names of the set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_distance">distance</code></td>
<td>
<p>a kernel (i.e. a distance measure) used for computing 
similarities between texts. Available options so far: &quot;delta&quot; 
(Burrow's Delta, default), &quot;argamon&quot; (Argamon's Linear Delta), 
&quot;eder&quot; (Eder's Delta), &quot;simple&quot; (Eder's Simple Distance), 
&quot;canberra&quot; (Canberra Distance), &quot;manhattan&quot; (Manhattan 
Distance), &quot;euclidean&quot; (Euclidean Distance), &quot;cosine&quot;
(Cosine Distance). THIS OPTION WILL BE CHANGED IN NEXT VERSIONS.</p>
</td></tr>
<tr><td><code id="perform.impostors_+3A_z.scores.both.sets">z.scores.both.sets</code></td>
<td>
<p>many distance measures convert input variables into 
z-scores before computing any distances. Such a variable
weighting is highly dependent on the number of input texts. One might
choose either training set only to scale the variables, or the entire
corpus (both sets). The latter is default.
THIS OPTION WILL BE CHANGED (OR DELETED) IN NEXT VERSIONS.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a single score indicating the probability that an
anonymouns sample analyzed was/wasn't written by a candidate author. As a proportion, 
the score lies between 0 and 1 (higher scores indicate a higher attribution confidence).
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Koppel, M. , and Winter, Y. (2014). Determining if two documents are 
written by the same author. &quot;Journal of the Association for Information
Science and Technology&quot;, 65(1): 178-187.
</p>
<p>Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. (2016).
Authenticating the writings of Julius Caesar. &quot;Expert Systems With
Applications&quot;, 63: 86-96.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+imposters">imposters</a></code>
</p>

<hr>
<h2 id='perform.knn'>k-Nearest Neighbor classifier</h2><span id='topic+perform.knn'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier; this function is a wrapper for 
the k-NN procedure provided by the package <code>class</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.knn(training.set, test.set, classes.training.set = NULL, 
            classes.test.set = NULL, k.value = 1) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.knn_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.knn_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set.</p>
</td></tr>
<tr><td><code id="perform.knn_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.knn_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="perform.knn_+3A_k.value">k.value</code></td>
<td>
<p>number of nearest neighbors considered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of &quot;guessed&quot; classes: each test sample is
linked with one of the classes represented in the training set. 
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.svm">perform.svm</a></code>, <code><a href="#topic+perform.nsc">perform.nsc</a></code>,  
<code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+perform.naivebayes">perform.naivebayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
perform.knn(training.set, test.set)

## End(Not run)

# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

perform.knn(train, test, train.classes, test.classes)
</code></pre>

<hr>
<h2 id='perform.naivebayes'>Naive Bayes classifier</h2><span id='topic+perform.naivebayes'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier; this function is a wrapper for 
the Naive Bayes procedure provided by the package <code>e1071</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.naivebayes(training.set, test.set, 
        classes.training.set = NULL, classes.test.set = NULL) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.naivebayes_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.naivebayes_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set.</p>
</td></tr>
<tr><td><code id="perform.naivebayes_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.naivebayes_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of &quot;guessed&quot; classes: each test sample is
linked with one of the classes represented in the training set. 
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.svm">perform.svm</a></code>, <code><a href="#topic+perform.nsc">perform.nsc</a></code>,  
<code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+perform.knn">perform.knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
perform.naivebayes(training.set, test.set)

## End(Not run)

# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

perform.naivebayes(train, test, train.classes, test.classes)
</code></pre>

<hr>
<h2 id='perform.nsc'>Nearest Shrunken Centroids classifier</h2><span id='topic+perform.nsc'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier; this function is a wrapper for 
the Nearest Shrunken Centroids procedure provided by the package <code>pamr</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.nsc(training.set, 
              test.set, 
              classes.training.set = NULL, 
              classes.test.set = NULL, 
              show.features = FALSE,
              no.of.candidates = 3) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.nsc_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.nsc_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set.</p>
</td></tr>
<tr><td><code id="perform.nsc_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.nsc_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="perform.nsc_+3A_show.features">show.features</code></td>
<td>
<p>a logical value (default: FALSE). When the option is 
switched on, the most discriminative features (e.g. words) will 
be shown.</p>
</td></tr>
<tr><td><code id="perform.nsc_+3A_no.of.candidates">no.of.candidates</code></td>
<td>
<p>how many nearest neighbors will be computed for
each test sample (default = 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of &quot;guessed&quot; classes: each test sample is
linked with one of the classes represented in the training set. Additionally,
final scores and final rankings of candidates, as well as the discriminative
features (if applicable) are returned as attributes.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+perform.svm">perform.svm</a></code>,  
<code><a href="#topic+perform.knn">perform.knn</a></code>, <code><a href="#topic+perform.naivebayes">perform.naivebayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
perform.nsc(training.set, test.set)

## End(Not run)

# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

perform.nsc(train, test, train.classes, test.classes)
</code></pre>

<hr>
<h2 id='perform.svm'>Support Vector Machines classifier</h2><span id='topic+perform.svm'></span>

<h3>Description</h3>

<p>A machine-learning supervised classifier; this function is a wrapper for 
the Support Vector Machines procedure provided by the package <code>e1071</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perform.svm(training.set, 
            test.set, 
            classes.training.set = NULL, 
            classes.test.set = NULL, 
            no.of.candidates = 3, 
            tune.parameters = FALSE,
            svm.kernel = "linear",
            svm.degree = 3, 
            svm.coef0 = 0, 
            svm.cost = 1) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perform.svm_+3A_training.set">training.set</code></td>
<td>
<p>a table containing frequencies/counts for several
variables &ndash; e.g. most frequent words &ndash; across a number of text
samples (for the training set). Make sure that
the rows contain samples, and the columns &ndash; variables
(words, n-grams, or whatever needs to be analyzed).</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_test.set">test.set</code></td>
<td>
<p>a table containing frequencies/counts for the training set. 
The variables used (i.e. columns) must match the columns of the 
training set.</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_classes.training.set">classes.training.set</code></td>
<td>
<p>a vector containing class identifiers for the
training set. When missing, the row names of the training set
table will be used; the assumed classes are the strings of 
characters followed by the first underscore. Consider the following 
examples: c(&quot;Sterne_Tristram&quot;,
&quot;Sterne_Sentimental&quot;, &quot;Fielding_Tom&quot;, ...), where the classes
are the authors' names, and c(&quot;M_Joyce_Dubliners&quot;,
&quot;F_Woolf_Night_and_day&quot;, &quot;M_Conrad_Lord_Jim&quot;, ...), where the
classes are M(ale) and F(emale) according to authors' gender.
Note that only the part up to the first underscore in the
sample's name will be included in the class label.</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_classes.test.set">classes.test.set</code></td>
<td>
<p>a vector containing class identifiers for the
test set. When missing, the row names of the test set
table will be used (see above).</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_no.of.candidates">no.of.candidates</code></td>
<td>
<p>how many nearest neighbors will be computed for
each test sample (default = 3).</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>if this argument is used, two parameters, namely
gamma and cost, are tuned using a bootstrap procedure, and
then used to build a SVM model.</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_svm.kernel">svm.kernel</code></td>
<td>
<p>SVM kernel. Available values: &quot;linear&quot;, which is probably 
the best choice in stylometry, since the number of variables 
(e.g. MFWs) is many times bigger than the number of classes;
&quot;polynomial&quot;, and &quot;radial&quot;.</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_svm.degree">svm.degree</code></td>
<td>
<p>parameter needed for kernel of type &quot;polynomial&quot; 
(default: 3).</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_svm.coef0">svm.coef0</code></td>
<td>
<p>parameter needed for kernel of type &quot;polynomial&quot; 
(default: 0).</p>
</td></tr>
<tr><td><code id="perform.svm_+3A_svm.cost">svm.cost</code></td>
<td>
<p>cost of constraints violation (default: 1); it is the 
C-constant of the regularization term in the Lagrange formulation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of &quot;guessed&quot; classes: each test sample is
linked with one of the classes represented in the training set. Additionally,
final scores and final rankings of candidates are returned as attributes.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perform.delta">perform.delta</a></code>, <code><a href="#topic+perform.nsc">perform.nsc</a></code>,  
<code><a href="#topic+perform.knn">perform.knn</a></code>, <code><a href="#topic+perform.naivebayes">perform.naivebayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
perform.svm(training.set, test.set)

## End(Not run)

# classifying the standard 'iris' dataset:
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))

perform.svm(train, test, train.classes, test.classes)
</code></pre>

<hr>
<h2 id='performance.measures'>Accuracy, Precision, Recall, and the F Measure</h2><span id='topic+performance.measures'></span>

<h3>Description</h3>

<p>This function returns a few standard measurments used to test how efficient
a given classifier is, in a supervised machine-learnig classification setup.</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance.measures(predicted_classes, expected_classes = NULL, f_beta = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance.measures_+3A_predicted_classes">predicted_classes</code></td>
<td>
<p>a vector of predictions outputted from a classifier. 
If an object containing results from <code>classify()</code>, <code>crossv</code>,
<code>perform.delta</code>, <code>perform.svm</code> etc. is provided, then no further
input data is required (see below).</p>
</td></tr>
<tr><td><code id="performance.measures_+3A_expected_classes">expected_classes</code></td>
<td>
<p>a vector of expected classes, or the classification
results that we knew in advance. This argument is immaterial when an object
of the class <code>"stylo.results"</code> is provided. In such a case, only the above
parameter <code>predicted_classes</code> is obligatory.</p>
</td></tr>
<tr><td><code id="performance.measures_+3A_f_beta">f_beta</code></td>
<td>
<p>the F score is usually used in its F1 version, but one can
use any other scaling factor, e.g. F(1/2) or F(2); the default value
is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a list containing four performance indexes &ndash; accuracy, precision, 
recall and the F measure &ndash; for each class, as well as an average score for all classes.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+perform.delta">perform.delta</a></code>, 
<code><a href="#topic+perform.svm">perform.svm</a></code>, <code><a href="#topic+perform.nsc">perform.nsc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# classification results aka predictions (or, the classes "guessed" by a classifier)
what_we_got = c("prose", "prose", "prose", "poetry", "prose", "prose")
# expected classes (or, the ground truth)
what_we_expected = c("prose", "prose", "prose", "poetry", "poetry", "poetry")

performance.measures(what_we_got, what_we_expected)


# authorship attribution using the dataset 'lee'
#
data(lee)
results = crossv(training.set = lee, cv.mode = "leaveoneout", 
                 classification.method = "delta")
performance.measures(results)


# classifying the standard 'iris' dataset:
#
data(iris)
x = subset(iris, select = -Species)
train = rbind(x[1:25,], x[51:75,], x[101:125,])
test = rbind(x[26:50,], x[76:100,], x[126:150,])
train.classes = c(rep("s",25), rep("c",25), rep("v",25))
test.classes = c(rep("s",25), rep("c",25), rep("v",25))
results = perform.delta(train, test, train.classes, test.classes)

performance.measures(results)


</code></pre>

<hr>
<h2 id='plot.sample.size'>Plot Classification Accuracy for Short Text Samples</h2><span id='topic+plot.sample.size'></span>

<h3>Description</h3>

<p>Plotting method for objects of the class <code>"stylo.results"</code>, produced by 
the function <code><a href="#topic+samplesize.penalize">samplesize.penalize</a></code>. It can be used to show the behavior 
of short samples in text classification. See the help page of 
<code><a href="#topic+samplesize.penalize">samplesize.penalize</a></code> for further details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sample.size'
plot(x, target = NULL, variable = "diversity",
      trendline = TRUE, observations = FALSE,
      grayscale = FALSE, legend = TRUE, 
      legend_pos = "bottomright", main = "default", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sample.size_+3A_x">x</code></td>
<td>
<p>an object of class <code>"stylo.results"</code> as produced by
the function <code>samplesize.penalize</code>.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_target">target</code></td>
<td>
<p>the number of the text to be plotted, or its name as
stored in the <code>"stylo.results"</code> object (see the examples below). 
Both ways are equivalent, where a numeric value represents the n-th text.
If no target is specified, then the first text is plotted.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_variable">variable</code></td>
<td>
<p>choose either <code>"accuracy"</code> to get the classification 
accuracy, i.e. the ratio of correctly attributed instances to the number 
iterations (usually 100, see the help page of <code><a href="#topic+samplesize.penalize">samplesize.penalize</a></code> 
for further details), or <code>"diversity"</code> to get Simpson's index 
of class imbalance (this is the default value). The index provides you 
with the information how consistent was a classifier in its choices.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_trendline">trendline</code></td>
<td>
<p>since all the observations represented in the plot might be 
difficult to read, one can use a trendline instead (default). The 
trendlines are produced using the generic <code>lowess</code> function.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_observations">observations</code></td>
<td>
<p>particular observations and a trendline (see above) can
be combined. Switch this option on, to do so (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_grayscale">grayscale</code></td>
<td>
<p>using this option, you can switch off colors.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_legend">legend</code></td>
<td>
<p>do you want to have the trendlines and/or observations
explained? Switch this option on (which is default).</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_legend_pos">legend_pos</code></td>
<td>
<p>position of the legend: choose between 
<code>"bottomright"</code>, <code>"bottomleft"</code>, <code>"topright"</code>
and <code>"topleft"</code>.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_main">main</code></td>
<td>
<p>title of the plot; use it as if it was a regular option
of the function <code>plot</code>, or leave it as <code>"default"</code>
to get the name of the sample as automatically extracted from 
the class <code>"stylo.results"</code>.</p>
</td></tr>
<tr><td><code id="plot.sample.size_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An object generated by the <code><a href="#topic+samplesize.penalize">samplesize.penalize</a></code> function can be of course
split into its parts and plotted using any other routine. The method discussed
in this document is a simple shortcut: rather than refine your plot parameters 
from scratch, you can get acceptable results by using one single generic 
function <code>plot</code>; see a few examples below.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>See Also</h3>

<p><code><a href="#topic+samplesize.penalize">samplesize.penalize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# provided that there exists a text collection (text files)
# in the subdirectory 'corpus', perform a test for sample size:
results = samplesize.penalize(corpus.dir = "corpus")

# then plot the first text's classification accuracy:
plot(results)

# plot the results, e.g. for the 5th text:
plot(results, target = 5)

# the 'target' parameter can be set via the text's name, 
# to see which texts are available in the results, type: 
results$test.texts

# plot Simpson's diversity index for the text named 'Woolf_Years_1937':
plot(results_classic, target = "Woolf_Years_1937", variable = "diversity")


## End(Not run)
</code></pre>

<hr>
<h2 id='rolling.classify'>Sequential machine-learning classification</h2><span id='topic+rolling.classify'></span>

<h3>Description</h3>

<p>Function that splits a text into equal-sized consecutive 
blocks (slices) and performs a supervised classification of these blocks 
against a training set. A number of machine-learning methods for 
classification used in computational stylistics are available: Delta, 
k-Nearest Neighbors, Support Vector Machines, Naive Bayes, and Nearest 
Shrunken Centroids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rolling.classify(gui = FALSE, training.corpus.dir = "reference_set",
         test.corpus.dir = "test_set", training.frequencies = NULL, 
         test.frequencies = NULL, training.corpus = NULL, 
         test.corpus = NULL,  features = NULL, path = NULL, 
         slice.size = 5000, slice.overlap = 4500, 
         training.set.sampling = "no.sampling", mfw = 100, culling = 0, 
         milestone.points = NULL, milestone.labels = NULL, 
         plot.legend = TRUE, add.ticks = FALSE, shading = FALSE,
         ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rolling.classify_+3A_gui">gui</code></td>
<td>
<p>an optional argument; if switched on, a simple yet effective 
graphical user interface (GUI) will appear. Default value is <code>FALSE</code>
so far, since GUI is still under development.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_training.frequencies">training.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for several variables, 
e.g. most frequent words, across a number of text samples (for the 
training set). It can be either 
an R object (matrix or data frame), or a filename containing 
tab-delimited data. If you use an R object, make sure that the rows
contain samples, and the columns &ndash; variables (words). If you use
an external file, the variables should go vertically (i.e. in rows):
this is because files containing vertically-oriented tables are far 
more flexible and easily editable using, say, Excel or any text editor. 
To flip your table horizontally/vertically use the generic function 
<code>t()</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_test.frequencies">test.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for the 
test set. Further details: immediately above.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_training.corpus">training.corpus</code></td>
<td>
<p>another option is to pass a pre-processed corpus
as an argument (here: the training set). It is assumed that this object 
is a list, each element of which is a vector containing one tokenized 
sample. The example shown below will give you some hints how to prepare 
such a corpus. Also, refer to <code>help(load.corpus.and.parse)</code></p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_test.corpus">test.corpus</code></td>
<td>
<p>if <code>training.corpus</code> is used, then you should also 
prepare a similar R object containing the test set.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_features">features</code></td>
<td>
<p>usually, a number of the most frequent features (words,
word n-grams, character n-grams) are extracted automatically from the
corpus, and they are used as variables for further analysis. However,
in some cases it makes sense to use a set of tailored features, e.g.
the words that are associated with emotions or, say, a specific subset 
of function words. This optional argument allows to pass either a
filename containing your custom list of features, or a vector 
(R object) of features to be assessed.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_path">path</code></td>
<td>
<p>if not specified, the current directory will be used 
for input/output procedures (reading files, outputting the results).</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_training.corpus.dir">training.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working 
directory) that contains the training set, or the collection of texts 
used to exemplify the differences between particular classes (e.g. authors 
or genres). The discriminating features extracted from this training 
material will be used during the testing procedure (see below). If not 
specified, the default subdirectory <code>reference_set</code> will be used.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_test.corpus.dir">test.corpus.dir</code></td>
<td>
<p>the subdirectory (within the working directory) that
contains a test to be assessed, long enough to be split automatically
into equal-sized slices, or blocks. If not specified, the default 
subdirectory <code>test_set</code> will be used.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_slice.size">slice.size</code></td>
<td>
<p>a text to be analyzed is segmented into consecutive, 
equal-sized samples (slices, windows, or blocks); the slice size
is set using this parameter: default is 5,000 words. The samples 
are allowed to partially overlap (see the next parameter). </p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_slice.overlap">slice.overlap</code></td>
<td>
<p>if one specifies a <code>slice.size</code> of 5,000 
and a <code>slice.overlap</code> of 4,500 (which is default), then 
the first extracted sample contains words 1&ndash;5,000, the second 
501&ndash;5,500, the third sample 1001&ndash;6,000, and so forth.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_training.set.sampling">training.set.sampling</code></td>
<td>
<p>sometimes, it makes sense to split training 
set texts into smaller samples. Available options: &quot;no.sampling&quot;
(default), &quot;normal.sampling&quot;, &quot;random.sampling&quot;. See 
<code>help(make.samples)</code> for further details.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_mfw">mfw</code></td>
<td>
<p>number of the most frequent words (MFWs) to be analyzed.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_culling">culling</code></td>
<td>
<p>culling level; see <code>help(perform.culling)</code> to get
some help on the culling procedure principles.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_milestone.points">milestone.points</code></td>
<td>
<p>sometimes, there is a need to mark one or more 
passages in an analyzed text (e.g. when external evidence 
suggests an authorial takeover at a certain point) to compare if 
the a priori knowledge is confirmed by stylometric evidence.
To this end, one should add into the test file a string
&quot;xmilestone&quot; (when input texts are loaded directly from files),
or specify the break points using this parameter. E.g., to add
two lines at 10,000 words and 15,000 words, use
<code>milestone.points = c(10000, 15000)</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_milestone.labels">milestone.labels</code></td>
<td>
<p>when milestone points are used (see immediately 
above), they are automatically labelled using lowercase letters:
&quot;a&quot;, &quot;b&quot;, &quot;c&quot; etc. However, one can replace them with custom labels,
e.g. <code>milestone.labels = c("Act I", "Act II")</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_plot.legend">plot.legend</code></td>
<td>
<p>self-evident. Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_add.ticks">add.ticks</code></td>
<td>
<p>a graphical parameter: consider adding tiny ticks 
(short horizontal lines) to see the density of sampling. Default:
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_shading">shading</code></td>
<td>
<p>instead of using colors on the final plot, one might 
choose to use shading hatches, which might be an option to toggle 
with greyscale, but also with non-black settings thereby allowing 
for photocopier-friendly charts (even if they may be subjectively 
unattractive). To use this option, switch it to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rolling.classify_+3A_...">...</code></td>
<td>
<p>any variable as produced by <code>stylo.default.settings()</code>
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are numerous additional options that are passed to 
this function; so far, they are all loaded when <code>stylo.default.settings()</code> 
is executed (it will be invoked automatically from inside this function);
the user can set/change them in the GUI.</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including tables of word frequencies, vector of features 
used, a distance table and some more stuff. Additionally, depending on which 
options have been chosen, the function produces a number of files used to save 
the results, features assessed, generated tables of distances, etc.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Eder, M. (2015). Rolling stylometry. &quot;Digital Scholarship in the Humanities&quot;, 
31(3): 457-69.
</p>
<p>Eder, M. (2014). Testing rolling stylometry. <a href="https://goo.gl/f0YlOR">https://goo.gl/f0YlOR</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+rolling.delta">rolling.delta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage (it builds a corpus from a collection of text files):
rolling.classify()

rolling.classify(training.frequencies = "freqs_train.txt",
    test.frequencies = "freqs_test.txt", write.png.file = TRUE,
    classification.method = "nsc")

## End(Not run)
</code></pre>

<hr>
<h2 id='rolling.delta'>Sequential stylometric analysis</h2><span id='topic+rolling.delta'></span>

<h3>Description</h3>

<p>Function that analyses collaborative works and tries to determine
the authorship of their fragments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rolling.delta(gui = TRUE, path = NULL, primary.corpus.dir = "primary_set",
              secondary.corpus.dir = "secondary_set")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rolling.delta_+3A_gui">gui</code></td>
<td>
<p>an optional argument; if switched on, a simple yet effective 
graphical user interface (GUI) will appear. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rolling.delta_+3A_path">path</code></td>
<td>
<p>if not specified, the current working directory will be used 
for input/output procedures (reading files, outputting the results).</p>
</td></tr>
<tr><td><code id="rolling.delta_+3A_primary.corpus.dir">primary.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working 
directory) that contains a collection of texts written by the authorial 
candidates, likely to have been involved in the collaborative work 
analyzed. If not specified, the default subdirectory  <code>primary_set</code> 
will be used.</p>
</td></tr>
<tr><td><code id="rolling.delta_+3A_secondary.corpus.dir">secondary.corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working directory) that
contains the collaborative work to be analyzed. If not specified, 
the default subdirectory <code>secondary_set</code> will be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure provided by this function analyses collaborative works 
and tries to determine the authorship of their fragments. The first step 
involves a &quot;windowing&quot; procedure (Dalen-Oskam and Zundert, 2007) 
in which each reference text is segmented into consecutive, equal-sized 
samples or windows. After &quot;rolling&quot; through the test text, we can plot 
the resulting series of Delta scores for each reference text in a graph.
</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>, and 
produces a final plot.
</p>


<h3>Author(s)</h3>

<p>Mike Kestemont, Maciej Eder, Jan Rybicki</p>


<h3>References</h3>

<p>Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package 
for computational text analysis. &quot;R Journal&quot;, 8(1): 107-21.
</p>
<p>van Dalen-Oskam, K. and van Zundert, J. (2007). Delta for Middle Dutch: 
author and copyist distinction in Walewein. &quot;Literary and Linguistic
Computing&quot;, 22(3): 345-62.
</p>
<p>Hoover, D. (2011). The Tutor's Story: a case study of mixed authorship.
In: &quot;Digital Humanities 2011: Conference Abstracts&quot;. Stanford University,
Stanford, CA, pp. 149-51.
</p>
<p>Rybicki, J., Kestemont, M. and Hoover D. (2014). Collaborative authorship: 
Conrad, Ford and rolling delta. &quot;Literary and Linguistic Computing&quot;,
29(3): 422-31.
</p>
<p>Eder, M. (2015). Rolling stylometry. &quot;Digital Scholarship in the Humanities&quot;, 
31(3): 457-69.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rolling.classify">rolling.classify</a></code>, <code><a href="#topic+stylo">stylo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage:
rolling.delta()

# batch mode, custom name of corpus directories:
rolling.delta(gui = FALSE, primary.corpus.dir = "MySamples",
       secondary.corpus.dir = "ReferenceCorpus")

## End(Not run)
</code></pre>

<hr>
<h2 id='samplesize.penalize'>Determining Minimal Sample Size for Text Classification</h2><span id='topic+samplesize.penalize'></span>

<h3>Description</h3>

<p>This function tests the ability of a given input text (or texts) to be 
correctly classified in a supervised machine-learning setup (e.g. Delta,
SVM or NSC) when its length is limited. The procedure, introduced by Eder 
(2017), involves several iterations in which longer and longer samples 
are drawn from the text in question, and then they are tested against 
a training set. For very short samples, the obtained classification accuracy 
is quite low (obviously), but then it usually increases until it finally 
reaches a point of saturation. The function <code>samplesize.penalize</code> is aimed 
at indentifying such a saturation point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>samplesize.penalize(training.frequencies = NULL, 
              test.frequencies = NULL,
              training.corpus = NULL, test.corpus = NULL,
              mfw = c(100, 200, 500), features = NULL, 
              path = NULL, corpus.dir = "corpus",
              sample.size.coverage = seq(100, 10000, 100),
              sample.with.replacement = FALSE,
              iterations = 100, classification.method = "delta",
              list.cutoff = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="samplesize.penalize_+3A_training.frequencies">training.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for several variables, 
e.g. most frequent words, across a number of text samples (for the 
training set). It can be either 
an R object (matrix or data frame), or a filename containing 
tab-delimited data. If you use an R object, make sure that the rows
contain samples, and the columns &ndash; variables (words). If you use
an external file, the variables should go vertically (i.e. in rows):
this is because files containing vertically-oriented tables are far 
more flexible and easily editable using, say, Excel or any text editor. 
To flip your table horizontally/vertically use the generic function 
<code>t()</code>.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_test.frequencies">test.frequencies</code></td>
<td>
<p>using this optional argument, one can 
load a custom table containing frequencies/counts for the 
test set. Further details: immediately above.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_training.corpus">training.corpus</code></td>
<td>
<p>another option is to pass a pre-processed corpus
as an argument (here: the training set). It is assumed that this object 
is a list, each element of which is a vector containing one tokenized 
sample. The example shown below will give you some hints how to prepare 
such a corpus. Also, refer to <code>help(load.corpus.and.parse)</code></p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_test.corpus">test.corpus</code></td>
<td>
<p>if <code>training.corpus</code> is used, then you should also 
prepare a similar R object containing the test set.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_mfw">mfw</code></td>
<td>
<p>how many most frequent words (or other units) should be used
as features to test the classifier? The default value is 
<code>c(100,200,500)</code>, to assess three different ranges of MFWs.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_features">features</code></td>
<td>
<p>usually, a number of the most frequent features (words,
word n-grams, character n-grams) are extracted automatically from the
corpus, and they are used as variables for further analysis. However,
in some cases it makes sense to use a set of tailored features, e.g.
the words that are associated with emotions or, say, a specific subset
of function words. This optional argument allows to pass either a
filename containing your custom list of features, or a vector
(R object) of features to be assessed.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_path">path</code></td>
<td>
<p>if not specified, the current directory will be used
for input/output procedures (reading files, outputting the results).</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working directory) that
contains the corpus text files. If not specified, the default
subdirectory <code>corpus</code> will be used. This option is immaterial when
an external corpus and/or external tables with frequencies are loaded.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_sample.size.coverage">sample.size.coverage</code></td>
<td>
<p>the procedure iteratively tests classification
accuracy for different sample sizes. Feel free to modify the default
value <code>c(100, 10000, 100)</code>, which tests samples for 100, 200, 300,
..., 10,000 words.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_sample.with.replacement">sample.with.replacement</code></td>
<td>
<p>if a tested sample size is bigger than the 
text to be tested, then the procedure stops, obviously. To prevent
such a situation, you might decide to draw your samples (n words)
with replacement, which means that particular words can be picked 
more than once (default value is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_iterations">iterations</code></td>
<td>
<p>each sample size of a given text is tested by extracting
randomly n words from the text in N iterations (default being 100). Since 
the procedure is random, a large(ish) number of iterations, say 100,
allows for testing an actual behavior of a given sample size.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_classification.method">classification.method</code></td>
<td>
<p>the option invokes one of the classification
methods provided by the package <code>stylo</code>. Choose one of the
following: &quot;delta&quot;, &quot;svm&quot;, &quot;knn&quot;, &quot;nsc&quot;, &quot;naivebayes&quot;.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_list.cutoff">list.cutoff</code></td>
<td>
<p>when texts are loaded from files, tokenized, and counted,
it is all followed by building a table of frequencies. Since it is unlikely
to analyze thousands of most frequent words (rather than 100 or, say, 500),
it saves lots of time when the table of frequencies is trimmed. The default
value is 1000 most frequent words.</p>
</td></tr>
<tr><td><code id="samplesize.penalize_+3A_...">...</code></td>
<td>
<p>any other argument, usually tokenization settings (via the 
parameters <code>corpus.lang</code>, <code>features</code>, <code>ngram.size</code> etc.),
or hyperparameters of different classification methods, such as 
a distanse measure (for Delta), a cost function (for SVM), and so forth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If no additional argument is passed, then the function tries to load
text files from the default subdirectory <code>corpus</code>. The resulting 
object will then contain accuracy and diversity scores for all the texts.
</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including classification accuracy scores for each
tested text and each assessed sample size, Simpson's diversity index scores,
and the names of the texts analyzed. Use the generic function <code>summary</code>
to see the contents of the object. Use the generic function <code>plot</code>
to generate a tailored plot conveniently.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Eder, M. (2017). Short samples in authorship attribution: A new approach. 
&quot;Digital Humanities 2017: Conference Abstracts&quot;. Montreal: McGill 
University, pp. 221–24, <a href="https://dh2017.adho.org/abstracts/341/341.pdf">https://dh2017.adho.org/abstracts/341/341.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.sample.size">plot.sample.size</a></code>, <code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+imposters">imposters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# standard usage (it builds a corpus from a set of text files):
results = samplesize.penalize()
plot(results)


## End(Not run)
</code></pre>

<hr>
<h2 id='stylo'>Stylometric multidimensional analyses</h2><span id='topic+stylo'></span><span id='topic+stylo.package'></span>

<h3>Description</h3>

<p>It is quite a long story what this function does. Basically, it is an
all-in-one tool for a variety of experiments in computational
stylistics. For a more detailed description, refer to HOWTO available at:
<a href="https://sites.google.com/site/computationalstylistics/">https://sites.google.com/site/computationalstylistics/</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stylo(gui = TRUE, frequencies = NULL, parsed.corpus = NULL,
      features = NULL, path = NULL, metadata = NULL, 
      filename.column = "filename", grouping.column = "author",
      corpus.dir = "corpus", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stylo_+3A_gui">gui</code></td>
<td>
<p>an optional argument; if switched on, a simple yet effective
graphical interface (GUI) will appear. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="stylo_+3A_frequencies">frequencies</code></td>
<td>
<p>using this optional argument, one can load a custom
table containing frequencies/counts for several variables, e.g.
most frequent words, across a number of text samples. It can be either
an R object (matrix or data frame), or a filename containing
tab-delimited data. If you use an R object, make sure that the rows
contain samples, and the columns &ndash; variables (words). If you use
an external file, the variables should go vertically (i.e. in rows):
this is because files containing vertically-oriented tables are far
more flexible and easily editable using, say, Excel or any text editor.
To flip your table horizontally/vertically use the generic function t().</p>
</td></tr>
<tr><td><code id="stylo_+3A_parsed.corpus">parsed.corpus</code></td>
<td>
<p>another option is to pass a pre-processed corpus
as an argument. It is assumed that this object is a list, each element
of which is a vector containing one tokenized sample. The example shown
below will give you some hints how to prepare such a corpus.</p>
</td></tr>
<tr><td><code id="stylo_+3A_features">features</code></td>
<td>
<p>usually, a number of the most frequent features (words,
word n-grams, character n-grams) are extracted automatically from the
corpus, and they are used as variables for further analysis. However,
in some cases it makes sense to use a set of tailored features, e.g.
the words that are associated with emotions or, say, a specific subset
of function words. This optional argument allows to pass either a
filename containing your custom list of features, or a vector
(R object) of features to be assessed.</p>
</td></tr>
<tr><td><code id="stylo_+3A_path">path</code></td>
<td>
<p>if not specified, the current directory will be used
for input/output procedures (reading files, outputting the results).</p>
</td></tr>
<tr><td><code id="stylo_+3A_corpus.dir">corpus.dir</code></td>
<td>
<p>the subdirectory (within the current working directory) that
contains the corpus text files. If not specified, the default
subdirectory <code>corpus</code> will be used. This option is immaterial when
an external corpus and/or external table with frequencies is loaded.</p>
</td></tr>
<tr><td><code id="stylo_+3A_metadata">metadata</code></td>
<td>
<p>if not specified, colors for plotting will be assigned
accoding to file names after the usual author_ducument.txt pattern. 
But users can also specify a grouping variable, i.e. a vector of a length equal 
to the number of texts in the corpus, or a csv file, conventionally named 
&quot;metadata.csv&quot; containg matadata for the corpus. This metadata file should 
contain one row per document, a column with the file names in alphabetical order, 
and a calumn containing the grouping varible.</p>
</td></tr>
<tr><td><code id="stylo_+3A_filename.column">filename.column</code></td>
<td>
<p>the column in the metadata.csv containg the file names
of the documents in alphabetical order.</p>
</td></tr>
<tr><td><code id="stylo_+3A_grouping.column">grouping.column</code></td>
<td>
<p>the column in the metadata.csv containg the grouping
variable.</p>
</td></tr>
<tr><td><code id="stylo_+3A_...">...</code></td>
<td>
<p>any variable produced by <code>stylo.default.settings</code>
can be set here, in order to overwrite the default values. An example
of such a variable is <code>network = TRUE</code> (switched off as default)
for producing stylometric bootstrap consensus networks (Eder, forthcoming);
the function saves a csv file, containing a list of nodes that can be
loaded into, say, Gephi.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If no additional argument is passed, then the function tries to load
text files from the default subdirectory <code>corpus</code>.
There are a lot of additional options that should be passed to this
function; they are all loaded when <code>stylo.default.settings</code> is
executed (which is typically called automatically from inside the <code>stylo</code>
function).
</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including a table of word frequencies, vector of features
used, a distance table and some more stuff. Additionally, depending on which
options have been chosen, the function produces a number of files containing
results, plots, tables of distances, etc.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Jan Rybicki, Mike Kestemont, Steffen Pielström</p>


<h3>References</h3>

<p>Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package
for computational text analysis. &quot;R Journal&quot;, 8(1): 107-21.
</p>
<p>Eder, M. (2017). Visualization in stylometry: cluster analysis using networks.
&quot;Digital Scholarship in the Humanities&quot;, 32(1): 50-64.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+rolling.classify">rolling.classify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage (it builds a corpus from a set of text files):
stylo()

# loading word frequencies from a tab-delimited file:
stylo(frequencies = "my_frequencies.txt")

# using an existing corpus (a list containing tokenized texts):
txt1 = c("to", "be", "or", "not", "to", "be")
txt2 = c("now", "i", "am", "alone", "o", "what", "a", "slave", "am", "i")
txt3 = c("though", "this", "be", "madness", "yet", "there", "is", "method")
custom.txt.collection = list(txt1, txt2, txt3)
  names(custom.txt.collection) = c("hamlet_A", "hamlet_B", "polonius_A")
stylo(parsed.corpus = custom.txt.collection)

# using a custom set of features (words, n-grams) to be analyzed:
my.selection.of.function.words = c("the", "and", "of", "in", "if", "into",
                                   "within", "on", "upon", "since")
stylo(features = my.selection.of.function.words)

# loading a custom set of features (words, n-grams) from a file:
stylo(features = "wordlist.txt")

# batch mode, custom name of corpus directory:
my.test = stylo(gui = FALSE, corpus.dir = "ShakespeareCanon")
summary(my.test)

# batch mode, character 3-grams requested:
stylo(gui = FALSE, analyzed.features = "c", ngram.size = 3)


## End(Not run)
</code></pre>

<hr>
<h2 id='stylo.default.settings'>Setting variables for the package stylo</h2><span id='topic+stylo.default.settings'></span>

<h3>Description</h3>

<p>Function that sets a series of global variables to be used by the package
<code>stylo</code> and which can be modified by users via arguments passed to 
the function and/or via <code>gui.stylo</code>, <code>gui.classify</code>, or 
<code>gui.oppose</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stylo.default.settings(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stylo.default.settings_+3A_...">...</code></td>
<td>
<p>any variable as produced by this function
can be set here to overwrite the default values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is typically called from inside <code>stylo</code>, 
<code>classify</code>, <code>oppose</code>, <code>gui.stylo</code>, <code>gui.classify</code>
and <code>gui.oppose</code>.
</p>


<h3>Value</h3>

<p>The function returns a list of a few dozen variables, mostly options
and parameters for different stylometric tests. 
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Jan Rybicki, Mike Kestemont
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>, <code><a href="#topic+gui.stylo">gui.stylo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stylo.default.settings()

# to see which variables have been set:
names(stylo.default.settings())

# to use the elements of the list as if they were independent variables:
my.variables = stylo.default.settings()
attach(my.variables)
</code></pre>

<hr>
<h2 id='stylo.network'>Bootstrap consensus networks, with D3 visualization</h2><span id='topic+stylo.network'></span>

<h3>Description</h3>

<p>A function to perform Bootstrap Consensus Network analysys (Eder, 2017),
supplemented by interactive visualization (this involves javascript D3). 
This is a variant of the function <code><a href="#topic+stylo">stylo</a></code>, except that it 
produces final networks without any external software (e.g. Gephi).
To use this function, one is required to install the package <code>networkD3</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stylo.network(mfw.min = 100, mfw.max = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stylo.network_+3A_mfw.min">mfw.min</code></td>
<td>
<p>the minimal MFW value (e.g. 100 most frequent words) to 
start the bootstrap procedure with.</p>
</td></tr>
<tr><td><code id="stylo.network_+3A_mfw.max">mfw.max</code></td>
<td>
<p>the maximum MFW value (e.g. 1000 most frequent words), 
where procedure should stop. It is required that at least three
iterations are completed.</p>
</td></tr>
<tr><td><code id="stylo.network_+3A_...">...</code></td>
<td>
<p>any variable produced by <code>stylo.default.settings</code>
can be set here, in order to overwrite the default values. An example
of such a variable is <code>network = TRUE</code> (switched off as default)
for producing stylometric bootstrap consensus networks (Eder, forthcoming); 
the function saves a csv file, containing a list of nodes that can be 
loaded into, say, Gephi.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Bootstrap Consensus Network method computes nearest neighborship relations 
between texts, and then tries to represent them in a form of a network 
(Eder, 2017).  Since multidimensional methods are sensitive to input features 
(e.g. most frequent words), the methdod produces a series of networks 
for different MFW settings, and then combines them into a consensus network. 
To do so, it assumes that both the mininum MFW value and the maximum value
is provided.
If no additional argument is passed, then the function tries to load
text files from the default subdirectory <code>corpus</code>. 
There are a lot of additional options that should be passed to this
function; they are all loaded when <code>stylo.default.settings</code> is 
executed (which is typically called automatically from inside the <code>stylo</code> 
function).
</p>


<h3>Value</h3>

<p>The function returns an object of the class <code>stylo.results</code>:
a list of variables, including a table of word frequencies, vector of features 
used, a distance table and some more stuff. Additionally, depending on which 
options have been chosen, the function produces a number of files containing 
results, plots, tables of distances, etc.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder</p>


<h3>References</h3>

<p>Eder, M. (2017). Visualization in stylometry: cluster analysis using networks. 
&quot;Digital Scholarship in the Humanities&quot;, 32(1): 50-64.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# standard usage (it builds a corpus from a set of text files):
stylo.networks()

# to take advantage of a dataset provided by the library 'stylo',
# in this case, a selection of Amarican literature from the South
data(lee)
help(lee) # to see what this dataset actually contains
# 
stylo.network(frequencies = lee)

## End(Not run)
</code></pre>

<hr>
<h2 id='stylo.pronouns'>List of pronouns</h2><span id='topic+stylo.pronouns'></span>

<h3>Description</h3>

<p>This function returns a list of pronouns that can be used
as a stop word list for different stylometric analyses.
It has been shown that pronoun deletion improves, to some extent,
attribution accuracy of stylometric procedures (e.g. in English novels: Hoover 2004a; 2004b).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stylo.pronouns(corpus.lang = "English")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stylo.pronouns_+3A_corpus.lang">corpus.lang</code></td>
<td>
<p>an optional argument specifying the language of the texts 
analyzed: available languages are <code>English</code>, <code>Latin</code>, 
<code>Polish</code>, <code>Dutch</code>, <code>French</code>, <code>German</code>, <code>Spanish</code>, 
<code>Italian</code>, and <code>Hungarian</code> (default is <code>English</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector of pronouns.
</p>


<h3>Author(s)</h3>

<p>Jan Rybicki, Maciej Eder, Mike Kestemont</p>


<h3>References</h3>

<p>Hoover, D. (2004a). Testing Burrows's delta. &quot;Literary and Linguistic 
Computing&quot;, 19(4): 453-75.
</p>
<p>Hoover, D. (2004b). Delta prime?. &quot;Literary and Linguistic Computing&quot;, 
19(4): 477-95.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stylo">stylo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stylo.pronouns()
stylo.pronouns(corpus.lang = "Latin")
my.stop.words = stylo.pronouns(corpus.lang = "German")
</code></pre>

<hr>
<h2 id='txt.to.features'>Split string of words or other countable features</h2><span id='topic+txt.to.features'></span>

<h3>Description</h3>

<p>Function that converts a vector of words into either words, or characters, 
and optionally parses them into n-grams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt.to.features(tokenized.text, features = "w", ngram.size = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt.to.features_+3A_tokenized.text">tokenized.text</code></td>
<td>
<p>a vector of tokinzed words</p>
</td></tr>
<tr><td><code id="txt.to.features_+3A_features">features</code></td>
<td>
<p>an option for specifying the desired type of feature:
<code>w</code> for words, <code>c</code> for characters (default: <code>w</code>).</p>
</td></tr>
<tr><td><code id="txt.to.features_+3A_ngram.size">ngram.size</code></td>
<td>
<p>an optional argument (integer) indicating the value of <em>n</em>, 
or the size of n-grams to be created. If this argument is missing, 
the default value of 1 is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function that carries out the preprocessing steps necessary for 
feature selection: converts an input text into the type of sequences 
needed (n-grams etc.) and returns a new vector of items. The function
invokes <code>make.ngrams</code> to combine single units into pairs, 
triplets or longer n-grams. See <code>help(make.ngrams)</code> for details.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>,  
<code><a href="#topic+make.ngrams">make.ngrams</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># consider the string my.text:
my.text = "Quousque tandem abutere, Catilina, patientia nostra?"

# split it into a vector of consecutive words:
my.vector.of.words = txt.to.words(my.text)

# build a vector of word 2-grams:
txt.to.features(my.vector.of.words, ngram.size = 2)
 
# or produce character n-grams (in this case, character tetragrams):
txt.to.features(my.vector.of.words, features = "c", ngram.size = 4)
</code></pre>

<hr>
<h2 id='txt.to.words'>Split text into words</h2><span id='topic+txt.to.words'></span>

<h3>Description</h3>

<p>Generic tokenization function for splitting a given input text into single words 
(chains of characters delimited by spaces or punctuation marks).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt.to.words(input.text, splitting.rule = NULL, preserve.case = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt.to.words_+3A_input.text">input.text</code></td>
<td>
<p>a string of characters, usually a text.</p>
</td></tr>
<tr><td><code id="txt.to.words_+3A_splitting.rule">splitting.rule</code></td>
<td>
<p>an optional argument indicating an alternative 
splitting regexp. E.g., if your corpus contains no punctuation, you can 
use a very simple splitting sequence: <code>"[ \t\n]+"</code> or 
<code>"[[:space:]]+"</code> (in this case, any whitespace is assumed to be 
a word delimiter). If you deal with non-latin scripts, especially with 
those that are not supported by the <code>stylo</code> package yet (e.g. Chinese,
Japanese, Vietnamese, Georgian), you can indicate your letter characters 
explicitly: for most Cyrillic scripts try the following code
<code>"[^\u0400-\u0482\u048A\u04FF]+"</code>. Remember, however, 
that your texts need to be properly loaded into R (which is quite tricky 
on Windows; see below).</p>
</td></tr>
<tr><td><code id="txt.to.words_+3A_preserve.case">preserve.case</code></td>
<td>
<p>Whether or not to lowercase all characters 
in the corpus (default is <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generic tokenization function for splitting a given input text into 
single words (chains of characters delimited with spaces or punctuation marks).
In obsolete versions of the package <code>stylo</code>, the default splitting 
sequence of chars was <code>"[^[:alpha:]]+"</code> on Mac/Linux, and 
<code>"\\W+_"</code> on Windows. Two different splitting rules were used, because
regular expressions are not entirely platform-independent; type 
<code>help(regexp)</code> for more details. For the sake of compatibility, then, 
in the version &gt;=0.5.6 a lengthy list of dozens of letters in a few  alphabets 
(Latin, Cyrillic, Greek, Hebrew, Arabic so far) has been indicated explicitly:
</p>
<pre>paste("[^A-Za-z",
    # Latin supplement (Western):
    "\U00C0-\U00FF",
    # Latin supplement (Eastern):
    "\U0100-\U01BF",
    # Latin extended (phonetic):
    "\U01C4-\U02AF",
    # modern Greek:
    "\U0386\U0388-\U03FF",
    # Cyrillic:
    "\U0400-\U0481\U048A-\U0527",
    # Hebrew:
    "\U05D0-\U05EA\U05F0-\U05F4",
    # Arabic:
    "\U0620-\U065F\U066E-\U06D3\U06D5\U06DC",
    # extended Latin:
    "\U1E00-\U1EFF",
    # ancient Greek:
    "\U1F00-\U1FBC\U1FC2-\U1FCC\U1FD0-\U1FDB\U1FE0-\U1FEC\U1FF2-\U1FFC",
    # Coptic:
    "\U03E2-\U03EF\U2C80-\U2CF3",
    # Georgian:
    "\U10A0-\U10FF",
    "]+",
    sep="")</pre>
<p>Alternatively, different tokenization rules can be applied through
the option <code>splitting.rule</code> (see above). ATTENTION: this is the only 
piece of coding in the library <code>stylo</code> that might depend on the 
operating system used. While on Mac/Linux the native encoding is Unicode,
on Windows you never know if your text will be loaded proprely. A considerable
solution for Windows users is to convert your texts into Unicode (a variety 
of freeware converters are available on the internet), and to use an
appropriate encoding option when loading the files:
<code>read.table("file.txt", encoding = "UTF-8"</code> or
<code>scan("file.txt", what = "char", encoding = "UTF-8"</code>. If you use 
the functions provided by the library <code>stylo</code>, you should pass this 
option as an argument to your chosen function: 
<code>stylo(encoding = "UTF-8")</code>,
<code>classify(encoding = "UTF-8")</code>, <code>oppose(encoding = "UTF-8")</code>.
</p>


<h3>Value</h3>

<p>The function returns a vector of tokenized words (or other units) as elements.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt.to.words.ext">txt.to.words.ext</a></code>, <code><a href="#topic+txt.to.features">txt.to.features</a></code>,  
<code><a href="#topic+make.ngrams">make.ngrams</a></code>, <code><a href="#topic+load.corpus">load.corpus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt.to.words("And now, Laertes, what's the news with you?")

# retrieving grammatical codes (POS tags) from a tagged text:
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB 
               been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!]+_)|[ \n\t]")
</code></pre>

<hr>
<h2 id='txt.to.words.ext'>Split text into words: extended version</h2><span id='topic+txt.to.words.ext'></span>

<h3>Description</h3>

<p>Function for splitting a string of characters into single words,
removing punctuation etc., and preserving some language-dependent
idiosyncracies, such as common contractions in English.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt.to.words.ext(input.text, corpus.lang = "English", splitting.rule = NULL, 
                 preserve.case = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt.to.words.ext_+3A_input.text">input.text</code></td>
<td>
<p>a string of characters, usually a text.</p>
</td></tr>
<tr><td><code id="txt.to.words.ext_+3A_corpus.lang">corpus.lang</code></td>
<td>
<p>an optional argument specifying the language of the texts 
analyzed. Values that will affect the function's output are: 
<code>English.contr</code>, <code>English.all</code>, <code>Latin.corr</code> (their 
meaning is explained below), <code>JCK</code> for Japanese, Chinese and Korean,
as well as <code>other</code> for a variety of non-Latin scripts, including 
Cyryllic, Greek, Arabic, Hebrew, Coptic, Georgian etc. The default 
value is <code>English</code>.</p>
</td></tr>
<tr><td><code id="txt.to.words.ext_+3A_splitting.rule">splitting.rule</code></td>
<td>
<p>if you are not satisfied with the default language
settings (or your input string of characters is not a regular text,
but a sequence of, say, dance movements represented using symbolic signs),
you can indicate your custom splitting regular expression here. This
option will overwrite the above language settings. For further details,
refer to <code>help(txt.to.words)</code>.</p>
</td></tr>
<tr><td><code id="txt.to.words.ext_+3A_preserve.case">preserve.case</code></td>
<td>
<p>Whether or not to lowercase all character in the corpus 
(default = <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function for splitting a given input text into single words (chains 
of characters delimited with spaces or punctuation marks). It is build on 
top of the function <code>txt.to.words</code> and it is designed to manage some 
language-dependent text features during the tokenization process. In most
languages, this is irrelevant. However, it might be important when with English
or Latin texts: <code>English.contr</code> treats contractions as single, atomary words, 
i.e. strings such as &quot;don't&quot;, &quot;you've&quot; etc. will not be split into two strings;
<code>English.all</code> keeps the contractions (as above), and also prevents
the function from splitting compound words (mother-in-law, double-decker, etc.).
<code>Latin.corr</code>: since some editions do not distinguish the letters v/u,
this setting provides a consistent conversion to &quot;u&quot; in the whole string. The
option <code>preserve.case</code> lets you specify whether you wish to lowercase all
characters in the corpus.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder, Mike Kestemont
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt.to.words">txt.to.words</a></code>, <code><a href="#topic+txt.to.features">txt.to.features</a></code>,  
<code><a href="#topic+make.ngrams">make.ngrams</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt.to.words.ext("Nel mezzo del cammin di nostra vita / mi ritrovai per 
    una selva oscura, che la diritta via era smarrita.")

# to see the difference between particular options for English,
# consider the following sentence from Joseph Conrad's "Nostromo":
sample.text = "That's how your money-making is justified here."
txt.to.words.ext(sample.text, corpus.lang = "English")
txt.to.words.ext(sample.text, corpus.lang = "English.contr")
txt.to.words.ext(sample.text, corpus.lang = "English.all")
</code></pre>

<hr>
<h2 id='zeta.chisquare'>Compare two subcorpora using a home-brew variant of Craig's Zeta</h2><span id='topic+zeta.chisquare'></span>

<h3>Description</h3>

<p>This is a function for comparing two sets of texts; unlike keywords analysis,
it this method the goal is to split input texts into equal-sized slices, 
and to check the appearance of particular words over the slices. Number 
of slices in which a given word appeared in the subcorpus A and B is then
compared using standard chisquare test (if p value exceeds 0.05, a difference 
is considered significant). This method is based on original Zeta as developed
by Burrows and extended by Craig (Burrows 2007, Craig and Kinney 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zeta.chisquare(input.data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zeta.chisquare_+3A_input.data">input.data</code></td>
<td>
<p>a matrix of two columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a list of two elements: the first contains words (or 
other units, like n-grams) statistically preferred by the authors
of the primary subcorpus, while the second element contains avoided words. 
Since the applied measure is symmetrical, the preferred words are ipso facto 
avoided by the secondary authors, and vice versa.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Burrows, J. F. (2007). All the way through: testing for authorship 
in different frequency strata. &quot;Literary and Linguistic Computing&quot;, 
22(1): 27-48.
</p>
<p>Craig, H. and Kinney, A. F., eds. (2009). Shakespeare, Computers, and the 
Mystery of Authorship. Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+zeta.eder">zeta.eder</a></code>,  
<code><a href="#topic+zeta.craig">zeta.craig</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
zeta.chisquare(input.data, filter.threshold)

## End(Not run)
</code></pre>

<hr>
<h2 id='zeta.craig'>Compare two subcorpora using Craig's Zeta</h2><span id='topic+zeta.craig'></span>

<h3>Description</h3>

<p>This is a function for comparing two sets of texts; unlike keywords analysis,
it this method the goal is to split input texts into equal-sized slices, 
and to check the appearance of particular words over the slices. Number 
of slices in which a given word appeared in the subcorpus A and B is then
compared using Craig's formula, which is based on original Zeta as developed
by Burrows (Craig and Kinney 2009, Burrows 2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zeta.craig(input.data, filter.threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zeta.craig_+3A_input.data">input.data</code></td>
<td>
<p>a matrix of two columns.</p>
</td></tr>
<tr><td><code id="zeta.craig_+3A_filter.threshold">filter.threshold</code></td>
<td>
<p>this parameter (default 0.1) gets rid of words of 
weak discrimination strength; the higher the number, the
less words appear in the final wordlists. It does not normally 
exceed 0.5. In original Craig's Zeta, no threshold is used: 
instead, the results contain the fixed number of 500 top avoided 
and 500 top preferred words.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a list of two elements: the first contains words (or 
other units, like n-grams) statistically preferred by the authors
of the primary subcorpus, while the second element contains avoided words. 
Since the applied measure is symmetrical, the preferred words are ipso facto 
avoided by the secondary authors, and vice versa.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Burrows, J. F. (2007). All the way through: testing for authorship 
in different frequency strata. &quot;Literary and Linguistic Computing&quot;, 
22(1): 27-48.
</p>
<p>Craig, H. and Kinney, A. F., eds. (2009). Shakespeare, Computers, and the 
Mystery of Authorship. Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+zeta.eder">zeta.eder</a></code>,  
<code><a href="#topic+zeta.chisquare">zeta.chisquare</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
zeta.craig(input.data, filter.threshold)

## End(Not run)
</code></pre>

<hr>
<h2 id='zeta.eder'>Compare two subcorpora using Eder's Zeta</h2><span id='topic+zeta.eder'></span>

<h3>Description</h3>

<p>This is a function for comparing two sets of texts; unlike keywords analysis,
it this method the goal is to split input texts into equal-sized slices, 
and to check the appearance of particular words over the slices. Number 
of slices in which a given word appeared in the subcorpus A and B is then
compared using a distance derived from Canberra measure of similarity.
Original Zeta was developed by Burrows and extended by Craig 
(Burrows 2007, Craig and Kinney 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zeta.eder(input.data, filter.threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zeta.eder_+3A_input.data">input.data</code></td>
<td>
<p>a matrix of two columns.</p>
</td></tr>
<tr><td><code id="zeta.eder_+3A_filter.threshold">filter.threshold</code></td>
<td>
<p>this parameter (default 0.1) gets rid of words of 
weak discrimination strength; the higher the number, the
less words appear in the final wordlists. It does not normally 
exceed 0.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a list of two elements: the first contains words (or 
other units, like n-grams) statistically preferred by the authors
of the primary subcorpus, while the second element contains avoided words. 
Since the applied measure is symmetrical, the preferred words are ipso facto 
avoided by the secondary authors, and vice versa.
</p>


<h3>Author(s)</h3>

<p>Maciej Eder
</p>


<h3>References</h3>

<p>Burrows, J. F. (2007). All the way through: testing for authorship 
in different frequency strata. &quot;Literary and Linguistic Computing&quot;, 
22(1): 27-48.
</p>
<p>Craig, H. and Kinney, A. F., eds. (2009). Shakespeare, Computers, and the 
Mystery of Authorship. Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+oppose">oppose</a></code>, <code><a href="#topic+zeta.craig">zeta.craig</a></code>,  
<code><a href="#topic+zeta.chisquare">zeta.chisquare</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
zeta.eder(input.data, filter.threshold)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
